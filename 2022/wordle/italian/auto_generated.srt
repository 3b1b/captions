1
00:00:00,000 --> 00:00:02,786
Il gioco Wurdle è diventato piuttosto virale negli ultimi due mesi, 

2
00:00:02,786 --> 00:00:05,694
e per chi non trascura mai l'opportunità di una lezione di matematica, 

3
00:00:05,694 --> 00:00:08,849
mi viene in mente che questo gioco costituisce un ottimo esempio centrale in 

4
00:00:08,849 --> 00:00:12,086
una lezione sulla teoria dell'informazione, e in particolare un argomento noto 

5
00:00:12,086 --> 00:00:12,660
come entropia.

6
00:00:13,920 --> 00:00:16,416
Vedete, come molte persone sono stato risucchiato dal puzzle, 

7
00:00:16,416 --> 00:00:19,356
e come molti programmatori sono stato anche risucchiato nel tentativo di 

8
00:00:19,356 --> 00:00:22,740
scrivere un algoritmo che potesse svolgere il gioco nel modo più ottimale possibile.

9
00:00:23,180 --> 00:00:26,300
E quello che ho pensato di fare qui è semplicemente parlarvi del mio processo, 

10
00:00:26,300 --> 00:00:28,394
e spiegare alcuni dei calcoli che ci sono implicati, 

11
00:00:28,394 --> 00:00:31,080
dato che l'intero algoritmo è incentrato su questa idea di entropia.

12
00:00:38,700 --> 00:00:41,640
Per prima cosa, nel caso non ne avessi sentito parlare, cos'è Wurdle?

13
00:00:42,040 --> 00:00:45,368
E per prendere due piccioni con una fava mentre analizziamo le regole del gioco, 

14
00:00:45,368 --> 00:00:47,587
permettetemi anche di anticipare dove stiamo andando, 

15
00:00:47,587 --> 00:00:51,040
ovvero sviluppare un piccolo algoritmo che sostanzialmente giocherà al posto nostro.

16
00:00:51,360 --> 00:00:55,100
Anche se non ho fatto il Wurdle di oggi, è il 4 febbraio e vedremo come se la cava il bot.

17
00:00:55,480 --> 00:00:57,965
L'obiettivo di Wurdle è indovinare una parola misteriosa di cinque 

18
00:00:57,965 --> 00:01:00,340
lettere e ti vengono date sei diverse possibilità di indovinare.

19
00:01:00,840 --> 00:01:04,379
Ad esempio, il mio bot Wurdle mi suggerisce di iniziare con la gru indovinata.

20
00:01:05,180 --> 00:01:07,835
Ogni volta che fai un'ipotesi, ottieni alcune informazioni 

21
00:01:07,835 --> 00:01:10,220
su quanto la tua ipotesi è vicina alla risposta vera.

22
00:01:10,920 --> 00:01:14,100
Qui la casella grigia mi dice che non c'è C nella risposta effettiva.

23
00:01:14,520 --> 00:01:17,840
La casella gialla mi dice che c'è una R, ma non è in quella posizione.

24
00:01:18,240 --> 00:01:22,240
La casella verde mi dice che la parola segreta ha una A ed è in terza posizione.

25
00:01:22,720 --> 00:01:24,580
E poi non c'è né N né E.

26
00:01:25,200 --> 00:01:27,340
Quindi lasciami entrare e riferire quell'informazione al bot Wurdle.

27
00:01:27,340 --> 00:01:30,320
Abbiamo iniziato con la gru, siamo diventati grigi, gialli, verdi, grigi, grigi.

28
00:01:31,420 --> 00:01:33,900
Non preoccuparti per tutti i dati che vengono mostrati in questo momento, 

29
00:01:33,900 --> 00:01:34,940
te lo spiegherò a tempo debito.

30
00:01:35,460 --> 00:01:38,820
Ma il suo suggerimento principale per la nostra seconda scelta è shtick.

31
00:01:39,560 --> 00:01:42,738
E la tua ipotesi deve essere una vera parola di cinque lettere, ma come vedrai, 

32
00:01:42,738 --> 00:01:45,400
è piuttosto liberale con ciò che ti farà effettivamente indovinare.

33
00:01:46,200 --> 00:01:47,440
In questo caso, proviamo shtick.

34
00:01:48,780 --> 00:01:50,180
E va bene, le cose sembrano piuttosto buone.

35
00:01:50,260 --> 00:01:53,980
Premiamo la S e la H, quindi conosciamo le prime tre lettere, sappiamo che c'è una R.

36
00:01:53,980 --> 00:01:58,700
E quindi sarà come SHA qualcosa R, o SHA R qualcosa.

37
00:01:59,620 --> 00:02:04,240
E sembra che il bot Wurdle sappia che ci sono solo due possibilità, shard o sharp.

38
00:02:05,100 --> 00:02:06,938
È una specie di scelta tra loro a questo punto, 

39
00:02:06,938 --> 00:02:10,080
quindi immagino che probabilmente solo perché è in ordine alfabetico va con shard.

40
00:02:11,220 --> 00:02:12,860
Evviva, è la vera risposta.

41
00:02:12,960 --> 00:02:13,780
Quindi ce l'abbiamo fatta in tre.

42
00:02:14,600 --> 00:02:17,480
Se ti stai chiedendo se va bene, il modo in cui ho sentito dire 

43
00:02:17,480 --> 00:02:20,360
da una persona è che con Wurdle quattro è il par e tre è birdie.

44
00:02:20,680 --> 00:02:22,480
Il che penso sia un'analogia piuttosto appropriata.

45
00:02:22,480 --> 00:02:27,020
Devi essere costantemente in gioco per ottenerne quattro, ma certamente non è pazzesco.

46
00:02:27,180 --> 00:02:29,920
Ma quando lo ottieni in tre, è semplicemente fantastico.

47
00:02:30,880 --> 00:02:33,453
Quindi, se sei d'accordo, quello che vorrei fare qui è semplicemente parlare 

48
00:02:33,453 --> 00:02:35,960
del mio processo di pensiero dall'inizio su come mi avvicino al bot Wurdle.

49
00:02:36,480 --> 00:02:39,440
E come ho detto, in realtà è una scusa per una lezione di teoria dell'informazione.

50
00:02:39,740 --> 00:02:42,820
L’obiettivo principale è spiegare cos’è l’informazione e cos’è l’entropia.

51
00:02:48,220 --> 00:02:51,106
Il mio primo pensiero nell'affrontarlo è stato quello di dare un'occhiata 

52
00:02:51,106 --> 00:02:53,720
alle frequenze relative delle diverse lettere nella lingua inglese.

53
00:02:54,380 --> 00:02:56,718
Quindi ho pensato, ok, esiste un'ipotesi di apertura o una coppia di 

54
00:02:56,718 --> 00:02:59,260
ipotesi di apertura che coincida con molte di queste lettere più frequenti?

55
00:02:59,960 --> 00:03:03,000
E uno a cui ero molto affezionato era farne altri seguiti dai chiodi.

56
00:03:03,760 --> 00:03:06,424
L'idea è che se colpisci una lettera, sai, ottieni un verde o un giallo, 

57
00:03:06,424 --> 00:03:07,520
è sempre una bella sensazione.

58
00:03:07,520 --> 00:03:08,840
Sembra che tu stia ricevendo informazioni.

59
00:03:09,340 --> 00:03:12,177
Ma in questi casi, anche se non colpisci e ottieni sempre dei grigi, 

60
00:03:12,177 --> 00:03:14,850
questo ti dà comunque molte informazioni poiché è piuttosto raro 

61
00:03:14,850 --> 00:03:17,400
trovare una parola che non contenga nessuna di queste lettere.

62
00:03:18,140 --> 00:03:20,536
Ma anche questo non sembra super sistematico, perché, 

63
00:03:20,536 --> 00:03:23,200
ad esempio, non fa nulla considerare l'ordine delle lettere.

64
00:03:23,560 --> 00:03:25,300
Perché scrivere chiodi quando potrei scrivere lumaca?

65
00:03:26,080 --> 00:03:27,500
È meglio avere quella S alla fine?

66
00:03:27,820 --> 00:03:28,680
Non sono veramente sicuro.

67
00:03:29,240 --> 00:03:32,720
Ora, un mio amico ha detto che gli piaceva aprire con la parola stanco, 

68
00:03:32,720 --> 00:03:36,540
il che mi ha sorpreso perché contiene alcune lettere insolite come la W e la Y.

69
00:03:37,120 --> 00:03:39,000
Ma chissà, forse è un'apertura migliore.

70
00:03:39,320 --> 00:03:41,713
Esiste una sorta di punteggio quantitativo che possiamo 

71
00:03:41,713 --> 00:03:44,320
assegnare per giudicare la qualità di una potenziale ipotesi?

72
00:03:45,340 --> 00:03:47,993
Ora, per impostare il modo in cui classificheremo le possibili ipotesi, 

73
00:03:47,993 --> 00:03:51,088
torniamo indietro e aggiungiamo un po' di chiarezza su come è impostato esattamente 

74
00:03:51,088 --> 00:03:51,420
il gioco.

75
00:03:51,420 --> 00:03:54,675
Quindi c'è un elenco di parole che ti permetterà di inserire che 

76
00:03:54,675 --> 00:03:57,880
sono considerate ipotesi valide che è lungo circa 13.000 parole.

77
00:03:58,320 --> 00:04:01,256
Ma quando lo guardi, ci sono un sacco di cose davvero insolite, 

78
00:04:01,256 --> 00:04:05,247
cose come una testa o Ali e ARG, il tipo di parole che provocano discussioni familiari 

79
00:04:05,247 --> 00:04:06,440
in una partita a Scarabeo.

80
00:04:06,960 --> 00:04:10,540
Ma l'atmosfera del gioco è che la risposta sarà sempre una parola abbastanza comune.

81
00:04:10,960 --> 00:04:13,425
E infatti c'è un altro elenco di circa 2300 parole 

82
00:04:13,425 --> 00:04:15,360
che rappresentano le possibili risposte.

83
00:04:15,940 --> 00:04:18,530
E questa è una lista curata da persone umane, penso specificamente 

84
00:04:18,530 --> 00:04:21,160
dalla ragazza del creatore del gioco, il che è piuttosto divertente.

85
00:04:21,820 --> 00:04:24,476
Ma quello che mi piacerebbe fare, la nostra sfida per questo 

86
00:04:24,476 --> 00:04:27,262
progetto è vedere se possiamo scrivere un programma che risolva 

87
00:04:27,262 --> 00:04:30,180
Wordle che non incorpori le conoscenze precedenti su questo elenco.

88
00:04:30,720 --> 00:04:32,773
Per prima cosa, ci sono molte parole di cinque lettere 

89
00:04:32,773 --> 00:04:34,640
piuttosto comuni che non troverai in quell'elenco.

90
00:04:34,940 --> 00:04:38,178
Quindi sarebbe meglio scrivere un programma che sia un po' più resistente 

91
00:04:38,178 --> 00:04:41,460
e faccia giocare Wordle contro chiunque, non solo contro il sito ufficiale.

92
00:04:41,920 --> 00:04:44,460
E anche il motivo per cui sappiamo qual è questo elenco di 

93
00:04:44,460 --> 00:04:47,000
possibili risposte è perché è visibile nel codice sorgente.

94
00:04:47,000 --> 00:04:50,019
Ma il modo in cui è visibile nel codice sorgente è nell'ordine 

95
00:04:50,019 --> 00:04:52,800
specifico in cui le risposte emergono di giorno in giorno.

96
00:04:53,060 --> 00:04:55,840
Quindi potresti sempre cercare quale sarà la risposta di domani.

97
00:04:56,420 --> 00:04:58,880
Quindi, chiaramente, in un certo senso usare la lista è un imbroglio.

98
00:04:59,100 --> 00:05:01,893
E ciò che rende il puzzle più interessante e una lezione di teoria 

99
00:05:01,893 --> 00:05:04,561
dell’informazione più ricca è utilizzare invece alcuni dati più 

100
00:05:04,561 --> 00:05:07,354
universali come le frequenze relative delle parole in generale per 

101
00:05:07,354 --> 00:05:10,440
catturare questa intuizione di avere una preferenza per parole più comuni.

102
00:05:11,600 --> 00:05:15,900
Quindi tra queste 13.000 possibilità, come dovremmo scegliere l'ipotesi di apertura?

103
00:05:16,400 --> 00:05:19,780
Ad esempio, se il mio amico propone stanco, come dovremmo analizzarne la qualità?

104
00:05:20,520 --> 00:05:23,906
Beh, il motivo per cui ha detto che gli piace quell'improbabile W è che 

105
00:05:23,906 --> 00:05:27,340
gli piace la natura a lungo termine di quanto sia bello colpire quella W.

106
00:05:27,920 --> 00:05:31,057
Ad esempio, se il primo schema rivelato fosse qualcosa del genere, 

107
00:05:31,057 --> 00:05:34,850
si scopre che ci sono solo 58 parole in questo lessico gigante che corrispondono 

108
00:05:34,850 --> 00:05:35,600
a quello schema.

109
00:05:36,060 --> 00:05:38,400
Quindi si tratta di un'enorme riduzione rispetto a 13.000.

110
00:05:38,780 --> 00:05:40,990
Ma il rovescio della medaglia, ovviamente, è che 

111
00:05:40,990 --> 00:05:43,020
è molto raro ottenere uno schema come questo.

112
00:05:43,020 --> 00:05:47,230
Nello specifico, se ogni parola avesse la stessa probabilità di essere la risposta, 

113
00:05:47,230 --> 00:05:51,040
la probabilità di ottenere questo schema sarebbe 58 diviso per circa 13.000.

114
00:05:51,580 --> 00:05:53,600
Naturalmente, non è altrettanto probabile che siano risposte.

115
00:05:53,720 --> 00:05:56,220
La maggior parte di queste sono parole molto oscure e persino discutibili.

116
00:05:56,600 --> 00:05:58,466
Ma almeno per il nostro primo passaggio a tutto questo, 

117
00:05:58,466 --> 00:06:01,033
supponiamo che siano tutti ugualmente probabili e poi perfezioniamo il tutto 

118
00:06:01,033 --> 00:06:01,600
un po' più tardi.

119
00:06:02,020 --> 00:06:04,370
Il punto è che un modello con molte informazioni è 

120
00:06:04,370 --> 00:06:06,720
per sua stessa natura improbabile che si verifichi.

121
00:06:07,280 --> 00:06:10,800
In effetti, ciò che significa essere informativo è che è improbabile.

122
00:06:11,720 --> 00:06:16,520
Uno schema molto più probabile da vedere con questa apertura sarebbe qualcosa del genere, 

123
00:06:16,520 --> 00:06:18,120
dove ovviamente non c'è una W.

124
00:06:18,240 --> 00:06:21,400
Forse c'è una E, e forse non c'è A, non c'è R, non c'è Y.

125
00:06:22,080 --> 00:06:24,560
In questo caso ci sono 1400 corrispondenze possibili.

126
00:06:25,080 --> 00:06:27,864
Se tutti fossero ugualmente probabili, la probabilità che 

127
00:06:27,864 --> 00:06:30,600
questo sia lo schema che vedresti sarebbe di circa l’11%.

128
00:06:30,900 --> 00:06:33,340
Quindi i risultati più probabili sono anche quelli meno informativi.

129
00:06:34,240 --> 00:06:37,666
Per avere una visione più globale, lascia che ti mostri la distribuzione 

130
00:06:37,666 --> 00:06:41,140
completa delle probabilità in tutti i diversi modelli che potresti vedere.

131
00:06:41,740 --> 00:06:45,321
Quindi ogni barra che stai guardando corrisponde a un possibile schema di 

132
00:06:45,321 --> 00:06:48,903
colori che potrebbe essere rivelato, di cui ci sono da 3 a 5 possibilità, 

133
00:06:48,903 --> 00:06:52,340
e sono organizzati da sinistra a destra, dal più comune al meno comune.

134
00:06:52,920 --> 00:06:56,000
Quindi la possibilità più comune qui è che ottieni tutti i grigi.

135
00:06:56,100 --> 00:06:58,120
Ciò accade circa il 14% delle volte.

136
00:06:58,580 --> 00:07:02,066
E quello che speri quando fai un'ipotesi è di finire da qualche parte 

137
00:07:02,066 --> 00:07:05,453
in questa lunga coda, come qui dove ci sono solo 18 possibilità per 

138
00:07:05,453 --> 00:07:09,140
ciò che corrisponde a questo schema che evidentemente assomiglia a questo.

139
00:07:09,920 --> 00:07:13,800
O se ci avventuriamo un po' più a sinistra, forse arriviamo fino a qui.

140
00:07:14,940 --> 00:07:16,180
Ok, ecco un bel puzzle per te.

141
00:07:16,540 --> 00:07:19,514
Quali sono le tre parole in lingua inglese che iniziano con una W, 

142
00:07:19,514 --> 00:07:22,000
finiscono con una Y e contengono una R da qualche parte?

143
00:07:22,480 --> 00:07:26,800
Si scopre che le risposte sono, vediamo, prolisse, verminose e ironiche.

144
00:07:27,500 --> 00:07:30,720
Quindi, per giudicare quanto sia buona questa parola nel complesso, 

145
00:07:30,720 --> 00:07:34,745
vogliamo una sorta di misura della quantità prevista di informazioni che otterrai da 

146
00:07:34,745 --> 00:07:35,740
questa distribuzione.

147
00:07:35,740 --> 00:07:40,125
Se esaminiamo ogni modello e moltiplichiamo la sua probabilità che si verifichi per 

148
00:07:40,125 --> 00:07:44,720
qualcosa che misura quanto sia informativo, forse possiamo darci un punteggio oggettivo.

149
00:07:45,960 --> 00:07:47,918
Ora il tuo primo istinto su cosa dovrebbe essere quel 

150
00:07:47,918 --> 00:07:49,840
qualcosa potrebbe essere il numero di corrispondenze.

151
00:07:50,160 --> 00:07:52,400
Desideri un numero medio di partite inferiore.

152
00:07:52,800 --> 00:07:55,633
Ma mi piacerebbe invece usare una misura più universale che spesso 

153
00:07:55,633 --> 00:07:58,466
attribuiamo alle informazioni, e che sarà più flessibile una volta 

154
00:07:58,466 --> 00:08:01,257
che avremo una probabilità diversa assegnata a ciascuna di queste 

155
00:08:01,257 --> 00:08:04,260
13.000 parole per stabilire se siano o meno effettivamente la risposta.

156
00:08:10,320 --> 00:08:14,376
L'unità di informazione standard è il bit, che ha una formula un po' divertente, 

157
00:08:14,376 --> 00:08:16,980
ma è davvero intuitiva se guardiamo solo gli esempi.

158
00:08:17,780 --> 00:08:21,191
Se hai un'osservazione che dimezza il tuo spazio di possibilità, 

159
00:08:21,191 --> 00:08:23,500
diciamo che contiene un bit di informazione.

160
00:08:24,180 --> 00:08:27,479
Nel nostro esempio, lo spazio delle possibilità è composto da tutte le parole possibili, 

161
00:08:27,479 --> 00:08:30,147
e risulta che circa la metà delle parole di cinque lettere hanno una S, 

162
00:08:30,147 --> 00:08:31,260
un po' meno, ma circa la metà.

163
00:08:31,780 --> 00:08:34,320
Quindi quell'osservazione ti darebbe un po' di informazione.

164
00:08:34,880 --> 00:08:39,673
Se invece un fatto nuovo riduce di un fattore quattro quello spazio di possibilità, 

165
00:08:39,673 --> 00:08:41,500
diciamo che ha due informazioni.

166
00:08:41,980 --> 00:08:44,460
Ad esempio, risulta che circa un quarto di queste parole hanno una T.

167
00:08:45,020 --> 00:08:47,822
Se l'osservazione taglia quello spazio di un fattore otto, 

168
00:08:47,822 --> 00:08:50,720
diciamo che si tratta di tre bit di informazione, e così via.

169
00:08:50,900 --> 00:08:53,520
Quattro bit lo tagliano in un sedicesimo, cinque bit lo tagliano in un trentaduesimo.

170
00:08:53,520 --> 00:08:56,717
Quindi ora potresti voler fermarti e chiederti: 

171
00:08:56,717 --> 00:09:01,314
qual è la formula per l'informazione sul numero di bit in termini di 

172
00:09:01,314 --> 00:09:02,980
probabilità di un evento?

173
00:09:03,920 --> 00:09:07,563
Quello che stiamo dicendo qui è che quando prendi la metà del numero di bit, 

174
00:09:07,563 --> 00:09:11,112
è la stessa cosa della probabilità, che è la stessa cosa che dire due alla 

175
00:09:11,112 --> 00:09:13,383
potenza del numero di bit è uno su probabilità, 

176
00:09:13,383 --> 00:09:17,169
che riorganizza ulteriormente dicendo che l'informazione è il logaritmo in base 

177
00:09:17,169 --> 00:09:18,920
due di uno diviso per la probabilità.

178
00:09:19,620 --> 00:09:21,823
E a volte lo vedi con un'ulteriore riorganizzazione, 

179
00:09:21,823 --> 00:09:24,900
dove l'informazione è il logaritmo negativo in base due della probabilità.

180
00:09:25,660 --> 00:09:28,961
Espresso in questo modo, può sembrare un po' strano ai non iniziati, 

181
00:09:28,961 --> 00:09:33,171
ma in realtà è solo l'idea molto intuitiva di chiedersi quante volte hai ridotto a metà 

182
00:09:33,171 --> 00:09:34,080
le tue possibilità.

183
00:09:35,180 --> 00:09:37,134
Ora, se ti stai chiedendo, sai, pensavo stessimo solo facendo un 

184
00:09:37,134 --> 00:09:39,300
divertente gioco di parole, perché i logaritmi stanno entrando in gioco?

185
00:09:39,780 --> 00:09:44,183
Uno dei motivi per cui questa è un'unità più gradevole è che è molto più facile parlare 

186
00:09:44,183 --> 00:09:48,386
di eventi molto improbabili, molto più facile dire che un'osservazione ha 20 bit di 

187
00:09:48,386 --> 00:09:52,439
informazione che dire che la probabilità che si verifichi questo o quell'altro è 

188
00:09:52,439 --> 00:09:52,940
0.0000095.

189
00:09:53,300 --> 00:09:55,900
Ma una ragione più sostanziale per cui questa espressione 

190
00:09:55,900 --> 00:09:58,680
logaritmica si è rivelata un'aggiunta molto utile alla teoria 

191
00:09:58,680 --> 00:10:01,460
della probabilità è il modo in cui le informazioni si sommano.

192
00:10:02,060 --> 00:10:05,322
Ad esempio, se un'osservazione ti fornisce due bit di informazione, 

193
00:10:05,322 --> 00:10:08,920
riducendo il tuo spazio di quattro, e poi una seconda osservazione come la 

194
00:10:08,920 --> 00:10:12,134
tua seconda ipotesi in Wordle ti dà altri tre bit di informazione, 

195
00:10:12,134 --> 00:10:14,629
riducendoti ulteriormente di un altro fattore otto, 

196
00:10:14,629 --> 00:10:16,740
il due insieme ti danno cinque informazioni.

197
00:10:17,160 --> 00:10:19,705
Allo stesso modo in cui le probabilità amano moltiplicarsi, 

198
00:10:19,705 --> 00:10:21,020
le informazioni amano sommarsi.

199
00:10:21,960 --> 00:10:24,674
Quindi non appena siamo nel campo di qualcosa come un valore atteso, 

200
00:10:24,674 --> 00:10:27,980
dove stiamo sommando un sacco di numeri, i log rendono molto più piacevole gestirli.

201
00:10:28,480 --> 00:10:32,364
Torniamo alla nostra distribuzione per Weary e aggiungiamo qui un altro piccolo tracker, 

202
00:10:32,364 --> 00:10:34,940
che ci mostra quante informazioni ci sono per ogni pattern.

203
00:10:35,580 --> 00:10:39,136
La cosa principale che voglio farti notare è che maggiore è la probabilità quando 

204
00:10:39,136 --> 00:10:42,780
arriviamo a quegli schemi più probabili, minore è l'informazione, meno bit guadagni.

205
00:10:43,500 --> 00:10:46,960
Il modo in cui misuriamo la qualità di questa ipotesi sarà quello di prendere 

206
00:10:46,960 --> 00:10:49,844
il valore atteso di queste informazioni, esaminare ogni modello, 

207
00:10:49,844 --> 00:10:53,438
dire quanto è probabile e poi moltiplicarlo per il numero di bit di informazione 

208
00:10:53,438 --> 00:10:54,060
che otteniamo.

209
00:10:54,710 --> 00:10:58,120
E nell'esempio di Weary, risulta essere 4.9 bit.

210
00:10:58,560 --> 00:11:01,975
Quindi, in media, le informazioni che ottieni da questa ipotesi di apertura 

211
00:11:01,975 --> 00:11:05,480
equivalgono a tagliare a metà il tuo spazio di possibilità circa cinque volte.

212
00:11:05,960 --> 00:11:08,565
Al contrario, un esempio di ipotesi con un valore 

213
00:11:08,565 --> 00:11:11,640
informativo atteso più elevato sarebbe qualcosa come Slate.

214
00:11:13,120 --> 00:11:15,620
In questo caso noterai che la distribuzione sembra molto più piatta.

215
00:11:15,940 --> 00:11:20,542
In particolare, l'evento più probabile di tutti i grigi ha solo una probabilità 

216
00:11:20,542 --> 00:11:25,260
del 6% circa, quindi come minimo ne ottieni evidentemente 3.9 bit di informazione.

217
00:11:25,920 --> 00:11:28,560
Ma questo è il minimo, più tipicamente otterresti qualcosa di meglio di così.

218
00:11:29,100 --> 00:11:32,629
E si scopre che quando si calcolano i numeri su questo e si sommano 

219
00:11:32,629 --> 00:11:35,900
tutti i termini rilevanti, l'informazione media è di circa 5.8.

220
00:11:37,360 --> 00:11:40,612
Quindi, a differenza di Weary, il tuo spazio di possibilità 

221
00:11:40,612 --> 00:11:43,540
sarà in media circa la metà dopo questa prima ipotesi.

222
00:11:44,420 --> 00:11:46,282
In realtà c'è una storia divertente sul nome di 

223
00:11:46,282 --> 00:11:48,300
questo valore atteso della quantità di informazioni.

224
00:11:48,300 --> 00:11:51,078
La teoria dell'informazione fu sviluppata da Claude Shannon, 

225
00:11:51,078 --> 00:11:54,950
che lavorava ai Bell Labs negli anni '40, ma stava parlando di alcune delle sue idee 

226
00:11:54,950 --> 00:11:58,594
ancora da pubblicare con John von Neumann, che era questo gigante intellettuale 

227
00:11:58,594 --> 00:12:02,421
dell'epoca, molto importante in matematica e fisica e gli inizi di quella che stava 

228
00:12:02,421 --> 00:12:03,560
diventando l'informatica.

229
00:12:04,100 --> 00:12:07,400
E quando disse che non aveva un buon nome per questo valore atteso 

230
00:12:07,400 --> 00:12:10,701
della quantità di informazioni, von Neumann presumibilmente disse, 

231
00:12:10,701 --> 00:12:14,200
così va la storia, beh, dovresti chiamarla entropia, e per due ragioni.

232
00:12:14,540 --> 00:12:18,551
In primo luogo, la tua funzione di incertezza è stata usata nella meccanica statistica 

233
00:12:18,551 --> 00:12:22,287
con quel nome, quindi ha già un nome, e in secondo luogo, e cosa più importante, 

234
00:12:22,287 --> 00:12:26,298
nessuno sa cosa sia realmente l'entropia, quindi in un dibattito sarai sempre avere il 

235
00:12:26,298 --> 00:12:26,760
vantaggio.

236
00:12:27,700 --> 00:12:31,227
Quindi, se il nome sembra un po' misterioso, e se si deve credere a questa storia, 

237
00:12:31,227 --> 00:12:32,460
è in un certo senso previsto.

238
00:12:33,280 --> 00:12:36,421
Inoltre, se ti stai chiedendo quale sia la sua relazione con tutta quella 

239
00:12:36,421 --> 00:12:38,925
seconda legge della termodinamica, materiale della fisica, 

240
00:12:38,925 --> 00:12:42,066
c'è sicuramente una connessione, ma nelle sue origini Shannon si occupava 

241
00:12:42,066 --> 00:12:44,825
solo di pura teoria della probabilità, e per i nostri scopi qui, 

242
00:12:44,825 --> 00:12:48,136
quando uso la parola entropia, voglio solo che tu pensi al valore informativo 

243
00:12:48,136 --> 00:12:49,580
atteso di una particolare ipotesi.

244
00:12:50,700 --> 00:12:53,780
Puoi pensare all'entropia come alla misurazione di due cose contemporaneamente.

245
00:12:54,240 --> 00:12:56,780
Il primo è quanto piatta è la distribuzione.

246
00:12:57,320 --> 00:13:01,120
Più una distribuzione si avvicina all’uniforme, maggiore sarà l’entropia.

247
00:13:01,580 --> 00:13:04,635
Nel nostro caso, dove ci sono da 3 a 5 modelli totali, 

248
00:13:04,635 --> 00:13:08,412
per una distribuzione uniforme, osservando uno qualsiasi di essi si 

249
00:13:08,412 --> 00:13:11,745
otterrebbe un log delle informazioni in base 2 di 3 alla 5, 

250
00:13:11,745 --> 00:13:15,855
che risulta essere 7.92, quindi questo è il massimo assoluto che potresti 

251
00:13:15,855 --> 00:13:17,300
avere per questa entropia.

252
00:13:17,840 --> 00:13:22,080
Ma l’entropia è anche una sorta di misura di quante possibilità ci sono in primo luogo.

253
00:13:22,320 --> 00:13:27,033
Ad esempio, se ti capita di avere una parola in cui ci sono solo 16 modelli possibili, 

254
00:13:27,033 --> 00:13:29,742
e ognuno è ugualmente probabile, questa entropia, 

255
00:13:29,742 --> 00:13:32,180
questa informazione attesa, sarebbe di 4 bit.

256
00:13:32,580 --> 00:13:36,646
Ma se hai un'altra parola in cui ci sono 64 possibili modelli che potrebbero emergere, 

257
00:13:36,646 --> 00:13:40,480
e sono tutti ugualmente probabili, allora l'entropia risulterebbe essere di 6 bit.

258
00:13:41,500 --> 00:13:45,463
Quindi, se vedi una distribuzione in natura che ha un'entropia di 6 bit, 

259
00:13:45,463 --> 00:13:49,481
è un po' come se dicesse che ci sono tante variazioni e incertezze in ciò 

260
00:13:49,481 --> 00:13:53,500
che sta per accadere come se ci fossero 64 risultati ugualmente probabili.

261
00:13:54,360 --> 00:13:57,960
Per il mio primo passaggio al Wurtelebot, praticamente ho fatto semplicemente questo.

262
00:13:57,960 --> 00:14:02,209
Esamina tutte le possibili ipotesi che potresti avere, tutte le 13.000 parole, 

263
00:14:02,209 --> 00:14:05,597
calcola l'entropia per ciascuna di esse o, più specificamente, 

264
00:14:05,597 --> 00:14:10,169
l'entropia della distribuzione in tutti i modelli che potresti vedere, per ciascuno, 

265
00:14:10,169 --> 00:14:14,741
e sceglie il più alto, poiché è quello che probabilmente ridurrà il più possibile il 

266
00:14:14,741 --> 00:14:16,140
tuo spazio di possibilità.

267
00:14:17,140 --> 00:14:19,330
E anche se qui ho parlato solo della prima ipotesi, 

268
00:14:19,330 --> 00:14:21,100
fa la stessa cosa per le prossime ipotesi.

269
00:14:21,560 --> 00:14:24,170
Ad esempio, dopo aver visto uno schema su quella prima ipotesi, 

270
00:14:24,170 --> 00:14:27,393
che ti limiterebbe a un numero inferiore di parole possibili in base a ciò che 

271
00:14:27,393 --> 00:14:30,902
corrisponde a quello, giochi semplicemente allo stesso gioco rispetto a quell'insieme 

272
00:14:30,902 --> 00:14:31,800
più piccolo di parole.

273
00:14:32,260 --> 00:14:36,205
Per una seconda ipotesi proposta, guardi la distribuzione di tutti i modelli 

274
00:14:36,205 --> 00:14:39,740
che potrebbero verificarsi da quell'insieme di parole più ristretto, 

275
00:14:39,740 --> 00:14:43,840
cerchi tutte le 13.000 possibilità e trovi quello che massimizza quell'entropia.

276
00:14:45,420 --> 00:14:48,724
Per mostrarvi come funziona in azione, lasciatemi semplicemente richiamare una piccola 

277
00:14:48,724 --> 00:14:51,876
variante di Wurtele che ho scritto che mostra i punti salienti di questa analisi a 

278
00:14:51,876 --> 00:14:52,180
margine.

279
00:14:53,680 --> 00:14:56,145
Dopo aver fatto tutti i calcoli dell'entropia, 

280
00:14:56,145 --> 00:14:59,660
qui a destra ci mostra quali hanno le informazioni attese più alte.

281
00:15:00,280 --> 00:15:05,662
Sembra che la risposta migliore, almeno al momento, la perfezioneremo più tardi, 

282
00:15:05,662 --> 00:15:10,580
è Tares, che significa, ehm, ovviamente, una veccia, la veccia più comune.

283
00:15:11,040 --> 00:15:14,385
Ogni volta che facciamo un'ipotesi qui, dove forse ignoro i suoi consigli 

284
00:15:14,385 --> 00:15:17,458
e scelgo lo slate, perché mi piace lo slate, possiamo vedere quante 

285
00:15:17,458 --> 00:15:20,939
informazioni attese aveva, ma poi a destra della parola qui ci mostra quante 

286
00:15:20,939 --> 00:15:24,420
informazioni effettive che abbiamo ottenuto, dato questo modello particolare.

287
00:15:25,000 --> 00:15:28,079
Quindi qui sembra che siamo stati un po' sfortunati, ci aspettavamo di prenderne 5.8, 

288
00:15:28,079 --> 00:15:30,120
ma ci è capitato di ottenere qualcosa con meno di quello.

289
00:15:30,600 --> 00:15:32,862
E poi sul lato sinistro qui ci vengono mostrate tutte le diverse 

290
00:15:32,862 --> 00:15:35,020
parole possibili data la situazione in cui ci troviamo adesso.

291
00:15:35,800 --> 00:15:38,438
Le barre blu ci dicono quanto è probabile che ciascuna parola sia, 

292
00:15:38,438 --> 00:15:41,942
quindi al momento presuppone che ogni parola abbia la stessa probabilità di verificarsi, 

293
00:15:41,942 --> 00:15:43,360
ma lo perfezioneremo tra un momento.

294
00:15:44,060 --> 00:15:48,156
E poi questa misurazione dell'incertezza ci dice l'entropia di questa distribuzione 

295
00:15:48,156 --> 00:15:52,302
tra le parole possibili, che in questo momento, poiché è una distribuzione uniforme, 

296
00:15:52,302 --> 00:15:55,960
è solo un modo inutilmente complicato per contare il numero di possibilità.

297
00:15:56,560 --> 00:15:59,787
Ad esempio, se dovessimo portare 2 alla potenza di 13.66, 

298
00:15:59,787 --> 00:16:02,180
dovrebbero essere circa 13.000 possibilità.

299
00:16:02,900 --> 00:16:06,140
Sono un po' fuori strada, ma solo perché non mostro tutte le cifre decimali.

300
00:16:06,720 --> 00:16:09,833
Al momento potrebbe sembrare ridondante e complicare eccessivamente le cose, 

301
00:16:09,833 --> 00:16:12,340
ma vedrai perché è utile avere entrambi i numeri in un minuto.

302
00:16:12,760 --> 00:16:15,924
Quindi qui sembra che suggerisca che l'entropia più alta per la nostra 

303
00:16:15,924 --> 00:16:19,400
seconda ipotesi sia Ramen, che ancora una volta non sembra proprio una parola.

304
00:16:19,980 --> 00:16:24,060
Quindi, per prendere una posizione morale, andrò avanti e digiterò Rains.

305
00:16:25,440 --> 00:16:27,340
E ancora una volta sembra che siamo stati un po' sfortunati.

306
00:16:27,520 --> 00:16:31,360
Ci aspettavamo 4.3 bit e ne abbiamo solo 3.39 bit di informazioni.

307
00:16:31,940 --> 00:16:33,940
Quindi questo ci porta a 55 possibilità.

308
00:16:34,900 --> 00:16:37,257
E qui forse seguirò semplicemente ciò che suggerisce, 

309
00:16:37,257 --> 00:16:39,440
che è una combinazione, qualunque cosa significhi.

310
00:16:40,040 --> 00:16:42,920
E okay, questa è in realtà una buona occasione per un puzzle.

311
00:16:42,920 --> 00:16:46,380
Ci sta dicendo che questo schema ci dà 4.7 bit di informazione.

312
00:16:47,060 --> 00:16:51,720
Ma a sinistra, prima di vedere lo schema, ce n'erano 5.78 bit di incertezza.

313
00:16:52,420 --> 00:16:56,340
Quindi, come quiz per te, cosa significa riguardo al numero di possibilità rimanenti?

314
00:16:58,040 --> 00:17:01,424
Ebbene, significa che siamo ridotti a un minimo di incertezza, 

315
00:17:01,424 --> 00:17:04,540
il che equivale a dire che ci sono due risposte possibili.

316
00:17:04,700 --> 00:17:05,700
È una scelta 50-50.

317
00:17:06,500 --> 00:17:08,926
E da qui, poiché tu ed io sappiamo quali sono le parole più comuni, 

318
00:17:08,926 --> 00:17:10,640
sappiamo che la risposta dovrebbe essere abisso.

319
00:17:11,180 --> 00:17:13,280
Ma come è scritto proprio ora, il programma non lo sa.

320
00:17:13,540 --> 00:17:17,462
Quindi continua ad andare avanti, cercando di ottenere quante più informazioni possibile, 

321
00:17:17,462 --> 00:17:19,859
finché non rimane solo una possibilità, e poi indovina.

322
00:17:20,380 --> 00:17:22,339
Quindi ovviamente abbiamo bisogno di una migliore strategia di fine gioco.

323
00:17:22,599 --> 00:17:25,582
Ma diciamo che chiamiamo questa versione uno del nostro risolutore di parole, 

324
00:17:25,582 --> 00:17:28,260
e poi andiamo ad eseguire alcune simulazioni per vedere come funziona.

325
00:17:30,360 --> 00:17:34,120
Quindi il modo in cui funziona è giocare a ogni possibile gioco di parole.

326
00:17:34,240 --> 00:17:38,540
Sta esaminando tutte quelle 2315 parole che sono le vere risposte delle parole.

327
00:17:38,540 --> 00:17:40,580
Fondamentalmente lo utilizza come set di test.

328
00:17:41,360 --> 00:17:44,491
E con questo metodo ingenuo di non considerare quanto sia comune una parola, 

329
00:17:44,491 --> 00:17:47,949
e di cercare semplicemente di massimizzare l'informazione in ogni fase del percorso, 

330
00:17:47,949 --> 00:17:49,820
finché non si arriva a una ed una sola scelta.

331
00:17:50,360 --> 00:17:54,300
Alla fine della simulazione, il punteggio medio risulta essere circa 4.124.

332
00:17:55,320 --> 00:17:59,240
Il che non è male, a dire il vero, mi aspettavo di fare di peggio.

333
00:17:59,660 --> 00:18:02,600
Ma le persone che giocano a Wordle ti diranno che di solito riescono a farlo in 4.

334
00:18:02,860 --> 00:18:05,380
La vera sfida è ottenerne il maggior numero possibile in 3.

335
00:18:05,380 --> 00:18:08,080
C'è un salto piuttosto grande tra il punteggio di 4 e il punteggio di 3.

336
00:18:08,860 --> 00:18:12,030
L’ovvio frutto a portata di mano qui è quello di incorporare in qualche 

337
00:18:12,030 --> 00:18:14,980
modo se una parola è comune o meno, e come lo facciamo esattamente.

338
00:18:22,800 --> 00:18:25,226
Il modo in cui mi sono avvicinato è stato quello di ottenere un 

339
00:18:25,226 --> 00:18:27,880
elenco delle frequenze relative per tutte le parole in lingua inglese.

340
00:18:28,220 --> 00:18:31,682
E ho appena utilizzato la funzione dati sulla frequenza delle parole di Mathematica, 

341
00:18:31,682 --> 00:18:34,860
che a sua volta estrae dal set di dati pubblici English Ngram di Google Libri.

342
00:18:35,460 --> 00:18:37,730
Ed è piuttosto divertente da guardare, ad esempio se lo 

343
00:18:37,730 --> 00:18:39,960
ordiniamo dalle parole più comuni a quelle meno comuni.

344
00:18:40,120 --> 00:18:43,080
Evidentemente queste sono le parole di 5 lettere più comuni nella lingua inglese.

345
00:18:43,700 --> 00:18:45,840
O meglio, questi sono gli 8 più comuni.

346
00:18:46,280 --> 00:18:48,880
Il primo è quale, dopodiché c'è lì e là.

347
00:18:49,260 --> 00:18:53,840
Primo in sé non è primo, ma 9°, ed è logico che queste altre parole possano comparire 

348
00:18:53,840 --> 00:18:58,580
più spesso, dove quelle dopo prima sono dopo, dove e quelle sono solo un po' meno comuni.

349
00:18:59,160 --> 00:19:02,663
Ora, utilizzando questi dati per modellare la probabilità che ciascuna di queste 

350
00:19:02,663 --> 00:19:06,340
parole sia la risposta finale, non dovrebbe essere solo proporzionale alla frequenza.

351
00:19:06,700 --> 00:19:11,036
Ad esempio, a cui viene assegnato un punteggio pari a 0.002 in questo set di dati, 

352
00:19:11,036 --> 00:19:15,060
mentre la parola treccia è in un certo senso circa 1000 volte meno probabile.

353
00:19:15,560 --> 00:19:17,186
Ma entrambe queste sono parole abbastanza comuni da valere 

354
00:19:17,186 --> 00:19:18,840
quasi sicuramente la pena di essere prese in considerazione.

355
00:19:19,340 --> 00:19:21,000
Quindi vogliamo più di un taglio binario.

356
00:19:21,860 --> 00:19:25,923
Il modo in cui ho proceduto è stato immaginare di prendere l'intero elenco ordinato 

357
00:19:25,923 --> 00:19:29,890
di parole, quindi disporlo su un asse x, e quindi applicare la funzione sigmoide, 

358
00:19:29,890 --> 00:19:34,147
che è il modo standard per avere una funzione il cui output è fondamentalmente binario, 

359
00:19:34,147 --> 00:19:38,260
è o 0 oppure è 1, ma c'è un livellamento intermedio per quella regione di incertezza.

360
00:19:39,160 --> 00:19:43,992
Quindi, in sostanza, la probabilità che assegno a ciascuna parola di essere nell'elenco 

361
00:19:43,992 --> 00:19:48,440
finale sarà il valore della funzione sigmoide sopra ovunque si trovi sull'asse x.

362
00:19:49,520 --> 00:19:51,801
Ovviamente questo dipende da alcuni parametri, 

363
00:19:51,801 --> 00:19:55,878
ad esempio quanto è ampio lo spazio sull'asse x riempito da quelle parole determina 

364
00:19:55,878 --> 00:19:58,499
quanto gradualmente o ripidamente scendiamo da 1 a 0, 

365
00:19:58,499 --> 00:20:02,140
e il punto in cui le posizioniamo da sinistra a destra determina il limite.

366
00:20:02,980 --> 00:20:04,988
Ad essere onesti, il modo in cui l'ho fatto è stato 

367
00:20:04,988 --> 00:20:06,920
semplicemente leccarmi il dito e alzarlo al vento.

368
00:20:07,140 --> 00:20:10,173
Ho esaminato l'elenco ordinato e ho cercato di trovare una finestra in cui, 

369
00:20:10,173 --> 00:20:13,126
quando l'ho guardata, ho pensato che circa la metà di queste parole fosse 

370
00:20:13,126 --> 00:20:16,120
più probabile che non fossero la risposta finale, e l'ho usata come limite.

371
00:20:17,100 --> 00:20:20,083
Una volta ottenuta una distribuzione come questa tra le parole, 

372
00:20:20,083 --> 00:20:23,860
otteniamo un'altra situazione in cui l'entropia diventa una misura davvero utile.

373
00:20:24,500 --> 00:20:28,020
Ad esempio, supponiamo che stessimo giocando e iniziamo con le mie vecchie aperture, 

374
00:20:28,020 --> 00:20:30,878
che erano una piuma e chiodi, e finiamo con una situazione in cui ci 

375
00:20:30,878 --> 00:20:33,240
sono quattro possibili parole che corrispondono a quella.

376
00:20:33,560 --> 00:20:35,620
E diciamo che li consideriamo tutti ugualmente probabili.

377
00:20:36,220 --> 00:20:38,880
Lascia che ti chieda: qual è l'entropia di questa distribuzione?

378
00:20:41,080 --> 00:20:45,599
Bene, l'informazione associata a ciascuna di queste possibilità 

379
00:20:45,599 --> 00:20:50,260
sarà il logaritmo in base 2 di 4, poiché ognuna è 1 e 4, e cioè 2.

380
00:20:50,640 --> 00:20:52,460
Due informazioni, quattro possibilità.

381
00:20:52,760 --> 00:20:53,580
Tutto molto bello e buono.

382
00:20:54,300 --> 00:20:57,800
E se ti dicessi che in realtà ci sono più di quattro partite?

383
00:20:58,260 --> 00:21:00,901
In realtà, quando esaminiamo l'elenco completo delle parole, 

384
00:21:00,901 --> 00:21:02,460
ci sono 16 parole che corrispondono.

385
00:21:02,580 --> 00:21:05,292
Ma supponiamo che il nostro modello attribuisca una probabilità 

386
00:21:05,292 --> 00:21:08,598
molto bassa alle altre 12 parole di essere effettivamente la risposta finale, 

387
00:21:08,598 --> 00:21:10,760
qualcosa come 1 su 1000 perché sono davvero oscure.

388
00:21:11,500 --> 00:21:14,260
Ora lascia che ti chieda: qual è l'entropia di questa distribuzione?

389
00:21:15,420 --> 00:21:18,879
Se l'entropia misurasse semplicemente il numero di corrispondenze qui, 

390
00:21:18,879 --> 00:21:22,728
allora potresti aspettarti che sia qualcosa come il logaritmo in base 2 di 16, 

391
00:21:22,728 --> 00:21:25,700
che sarebbe 4, due bit di incertezza in più rispetto a prima.

392
00:21:26,180 --> 00:21:29,860
Ma ovviamente l’effettiva incertezza non è poi così diversa da quella che avevamo prima.

393
00:21:30,160 --> 00:21:33,692
Solo perché ci sono queste 12 parole davvero oscure non significa che sarebbe 

394
00:21:33,692 --> 00:21:37,360
ancora più sorprendente apprendere che la risposta finale è fascino, per esempio.

395
00:21:38,180 --> 00:21:40,472
Quindi, quando fai effettivamente il calcolo qui, 

396
00:21:40,472 --> 00:21:43,910
e sommi la probabilità di ogni occorrenza moltiplicata per le informazioni 

397
00:21:43,910 --> 00:21:46,020
corrispondenti, quello che ottieni è 2.11 bit.

398
00:21:46,020 --> 00:21:49,950
Dico solo che sono fondamentalmente due bit, fondamentalmente queste quattro possibilità, 

399
00:21:49,950 --> 00:21:53,705
ma c'è un po' più di incertezza a causa di tutti quegli eventi altamente improbabili, 

400
00:21:53,705 --> 00:21:56,500
anche se se li imparassi ne otterresti un sacco di informazioni.

401
00:21:57,160 --> 00:21:59,348
Quindi, rimpicciolendo, questo fa parte di ciò che rende Wordle 

402
00:21:59,348 --> 00:22:01,400
un bell'esempio per una lezione di teoria dell'informazione.

403
00:22:01,600 --> 00:22:04,640
Abbiamo queste due distinte applicazioni di sensazione per l'entropia.

404
00:22:05,160 --> 00:22:08,610
Il primo ci dice quali sono le informazioni attese che otterremo da 

405
00:22:08,610 --> 00:22:11,908
una determinata ipotesi, e il secondo dice che possiamo misurare 

406
00:22:11,908 --> 00:22:15,460
l'incertezza rimanente tra tutte le parole che abbiamo a disposizione.

407
00:22:16,460 --> 00:22:19,180
E dovrei sottolineare, nel primo caso in cui stiamo esaminando le 

408
00:22:19,180 --> 00:22:22,891
informazioni attese di un'ipotesi, una volta che abbiamo un peso disuguale per le parole, 

409
00:22:22,891 --> 00:22:24,540
ciò influisce sul calcolo dell'entropia.

410
00:22:24,980 --> 00:22:27,823
Ad esempio, vorrei richiamare lo stesso caso che stavamo esaminando 

411
00:22:27,823 --> 00:22:30,040
in precedenza della distribuzione associata a Weary, 

412
00:22:30,040 --> 00:22:33,720
ma questa volta utilizzando una distribuzione non uniforme su tutte le parole possibili.

413
00:22:34,500 --> 00:22:38,280
Quindi vediamo se riesco a trovare una parte qui che lo illustri abbastanza bene.

414
00:22:40,940 --> 00:22:42,360
Ok, ecco, questo è abbastanza buono.

415
00:22:42,360 --> 00:22:45,571
Qui abbiamo due schemi adiacenti che sono quasi altrettanto probabili, 

416
00:22:45,571 --> 00:22:49,100
ma ci viene detto che uno di essi ha 32 possibili parole che lo corrispondono.

417
00:22:49,280 --> 00:22:51,951
E se controlliamo cosa sono, queste sono quelle 32, 

418
00:22:51,951 --> 00:22:55,600
che sono tutte parole molto improbabili mentre le guardi con gli occhi.

419
00:22:55,840 --> 00:22:59,041
È difficile trovare risposte che sembrino plausibili, forse urla, 

420
00:22:59,041 --> 00:23:01,855
ma se guardiamo lo schema dei vicini nella distribuzione, 

421
00:23:01,855 --> 00:23:05,251
che è considerato altrettanto probabile, ci viene detto che ha solo 8 

422
00:23:05,251 --> 00:23:09,520
corrispondenze possibili, quindi un quarto di molte partite, ma è altrettanto probabile.

423
00:23:09,860 --> 00:23:12,140
E quando analizziamo quei fiammiferi, possiamo capire il perché.

424
00:23:12,500 --> 00:23:16,300
Alcune di queste sono risposte realmente plausibili, come ring, o ira, o rap.

425
00:23:17,900 --> 00:23:21,527
Per illustrare come incorporiamo tutto ciò, lasciatemi richiamare qui la versione 2 di 

426
00:23:21,527 --> 00:23:25,280
Wordlebot e ci sono due o tre differenze principali rispetto alla prima che abbiamo visto.

427
00:23:25,860 --> 00:23:29,533
Prima di tutto, come ho appena detto, il modo in cui calcoliamo queste entropie, 

428
00:23:29,533 --> 00:23:32,616
questi valori attesi delle informazioni, ora utilizza distribuzioni 

429
00:23:32,616 --> 00:23:35,609
più raffinate attraverso i modelli che incorporano la probabilità 

430
00:23:35,609 --> 00:23:38,240
che una determinata parola sia effettivamente la risposta.

431
00:23:38,880 --> 00:23:41,517
Si dà il caso che le lacrime siano ancora la numero 1, 

432
00:23:41,517 --> 00:23:43,820
anche se quelle che seguono sono un po' diverse.

433
00:23:44,360 --> 00:23:46,728
In secondo luogo, quando classificherà le scelte migliori, 

434
00:23:46,728 --> 00:23:50,141
manterrà un modello della probabilità che ogni parola sia la risposta effettiva e lo 

435
00:23:50,141 --> 00:23:53,554
incorporerà nella sua decisione, il che è più facile da vedere una volta che abbiamo 

436
00:23:53,554 --> 00:23:55,080
alcune ipotesi sulla risposta. tavolo.

437
00:23:55,860 --> 00:23:57,739
Ancora una volta, ignorando la sua raccomandazione perché 

438
00:23:57,739 --> 00:23:59,780
non possiamo lasciare che le macchine governino le nostre vite.

439
00:24:01,140 --> 00:24:04,203
E suppongo che dovrei menzionare un'altra cosa diversa qui a sinistra, 

440
00:24:04,203 --> 00:24:06,317
che il valore di incertezza, quel numero di bit, 

441
00:24:06,317 --> 00:24:09,640
non è più semplicemente ridondante con il numero di possibili corrispondenze.

442
00:24:10,080 --> 00:24:14,700
Ora se lo tiriamo su e calcoliamo 2^8.02, che è leggermente superiore a 256, 

443
00:24:14,700 --> 00:24:19,020
immagino 259, ciò che dice è che anche se ci sono 526 parole totali che 

444
00:24:19,020 --> 00:24:21,840
effettivamente corrispondono a questo modello, 

445
00:24:21,840 --> 00:24:26,399
la quantità di incertezza che ha è più simile a quella che sarebbe se ce ne 

446
00:24:26,399 --> 00:24:28,980
fossero 259 ugualmente probabili risultati.

447
00:24:29,720 --> 00:24:30,740
Puoi pensarla in questo modo.

448
00:24:31,020 --> 00:24:34,593
Sa che borx non è la risposta, lo stesso con yorts, zorl e zorus, 

449
00:24:34,593 --> 00:24:37,680
quindi è un po' meno incerto rispetto al caso precedente.

450
00:24:37,820 --> 00:24:39,280
Questo numero di bit sarà inferiore.

451
00:24:40,220 --> 00:24:43,297
E se continuo a giocare, lo perfezionerò con un paio di 

452
00:24:43,297 --> 00:24:46,540
ipotesi che sono appropriate a ciò che vorrei spiegare qui.

453
00:24:48,360 --> 00:24:50,814
Alla quarta ipotesi, se guardi le sue scelte migliori, 

454
00:24:50,814 --> 00:24:53,760
puoi vedere che non si tratta più solo di massimizzare l'entropia.

455
00:24:54,460 --> 00:24:57,083
Quindi a questo punto ci sono tecnicamente sette possibilità, 

456
00:24:57,083 --> 00:25:00,300
ma le uniche con una possibilità significativa sono i dormitori e le parole.

457
00:25:00,300 --> 00:25:04,123
E si vede che si colloca scegliendo entrambi al di sopra di questi altri valori, 

458
00:25:04,123 --> 00:25:06,720
che a rigor di termini darebbero maggiori informazioni.

459
00:25:07,240 --> 00:25:10,570
La prima volta che l'ho fatto, ho semplicemente sommato questi due numeri per misurare la 

460
00:25:10,570 --> 00:25:13,900
qualità di ogni ipotesi, che in realtà ha funzionato meglio di quanto potresti sospettare.

461
00:25:14,300 --> 00:25:16,755
Ma in realtà non mi è sembrato sistematico e sono sicuro che ci siano altri 

462
00:25:16,755 --> 00:25:19,340
approcci che le persone potrebbero adottare, ma ecco quello a cui sono arrivato.

463
00:25:19,760 --> 00:25:23,900
Se consideriamo la prospettiva di un'ipotesi successiva, come in questo caso le parole, 

464
00:25:23,900 --> 00:25:27,900
ciò che ci interessa veramente è il punteggio atteso del nostro gioco se lo facciamo.

465
00:25:28,230 --> 00:25:32,011
E per calcolare il punteggio atteso, diciamo qual è la probabilità che 

466
00:25:32,011 --> 00:25:35,900
le parole siano la risposta effettiva, che al momento corrisponde al 58%.

467
00:25:36,040 --> 00:25:39,540
Diciamo che con una probabilità del 58%, il nostro punteggio in questo gioco sarebbe 4.

468
00:25:40,320 --> 00:25:45,640
E poi con la probabilità di 1 meno quel 58%, il nostro punteggio sarà superiore a 4.

469
00:25:46,220 --> 00:25:49,193
Quanto altro non lo sappiamo, ma possiamo stimarlo in base a 

470
00:25:49,193 --> 00:25:52,460
quanta incertezza potrebbe esserci una volta arrivati a quel punto.

471
00:25:52,960 --> 00:25:55,940
Nello specifico, al momento ce n'è 1.44 bit di incertezza.

472
00:25:56,440 --> 00:26:01,120
Se indoviniamo le parole, ci dice che l'informazione prevista che otterremo è 1.27 bit.

473
00:26:01,620 --> 00:26:04,510
Quindi, se indoviniamo le parole, questa differenza rappresenta la 

474
00:26:04,510 --> 00:26:07,660
quantità di incertezza che probabilmente ci resterà dopo che ciò accadrà.

475
00:26:08,260 --> 00:26:10,576
Ciò di cui abbiamo bisogno è una sorta di funzione, 

476
00:26:10,576 --> 00:26:13,740
che qui chiamerò f, che associ questa incertezza a un punteggio atteso.

477
00:26:14,240 --> 00:26:18,282
E il modo in cui è stato fatto è stato semplicemente tracciare una serie di dati dei 

478
00:26:18,282 --> 00:26:21,373
giochi precedenti basati sulla versione 1 del bot per dire, ehi, 

479
00:26:21,373 --> 00:26:25,511
qual era il punteggio effettivo dopo vari punti con determinate quantità di incertezza 

480
00:26:25,511 --> 00:26:26,320
molto misurabili.

481
00:26:27,020 --> 00:26:30,926
Ad esempio, questi punti dati qui si trovano sopra un valore intorno a 

482
00:26:30,926 --> 00:26:35,713
8.Si dice circa 7 per alcune partite dopo un punto in cui erano 8.7 bit di incertezza, 

483
00:26:35,713 --> 00:26:38,960
ci sono volute due ipotesi per ottenere la risposta finale.

484
00:26:39,320 --> 00:26:40,739
Per altri giochi sono state necessarie tre ipotesi, 

485
00:26:40,739 --> 00:26:42,240
per altri giochi sono state necessarie quattro ipotesi.

486
00:26:43,140 --> 00:26:46,709
Se qui ci spostiamo a sinistra, tutti i punti sopra lo zero indicano che ogni 

487
00:26:46,709 --> 00:26:50,644
volta che ci sono zero punti di incertezza, vale a dire che c'è solo una possibilità, 

488
00:26:50,644 --> 00:26:54,260
allora il numero di ipotesi richieste è sempre solo una, il che è rassicurante.

489
00:26:54,780 --> 00:26:57,497
Ogni volta che c'era un po' di incertezza, il che significava 

490
00:26:57,497 --> 00:26:59,776
che essenzialmente si riducevano a due possibilità, 

491
00:26:59,776 --> 00:27:03,020
a volte richiedeva un'altra ipotesi, a volte richiedeva altre due ipotesi.

492
00:27:03,080 --> 00:27:05,240
E chi più ne ha più ne metta qui.

493
00:27:05,740 --> 00:27:08,020
Forse un modo leggermente più semplice per visualizzare 

494
00:27:08,020 --> 00:27:10,220
questi dati è raggrupparli insieme e fare delle medie.

495
00:27:11,000 --> 00:27:15,424
Ad esempio, questa barra qui dice che tra tutti i punti in cui abbiamo avuto un 

496
00:27:15,424 --> 00:27:19,960
po' di incertezza, in media il numero di nuove ipotesi richieste era di circa 1.5.

497
00:27:22,140 --> 00:27:26,803
E la barra qui dice che tra tutti i diversi giochi dove ad un certo punto l'incertezza 

498
00:27:26,803 --> 00:27:31,466
era poco più di quattro bit, che è come restringere il campo a 16 diverse possibilità, 

499
00:27:31,466 --> 00:27:35,380
quindi in media richiede poco più di due ipotesi da quel punto inoltrare.

500
00:27:36,060 --> 00:27:37,793
E da qui ho semplicemente fatto una regressione per 

501
00:27:37,793 --> 00:27:39,460
adattare una funzione che mi sembrava ragionevole.

502
00:27:39,980 --> 00:27:44,156
E ricorda che il punto centrale di tutto ciò è che possiamo quantificare questa 

503
00:27:44,156 --> 00:27:47,132
intuizione che più informazioni otteniamo da una parola, 

504
00:27:47,132 --> 00:27:48,960
più basso sarà il punteggio atteso.

505
00:27:49,680 --> 00:27:54,460
Quindi con questo come versione 2.0, se torniamo indietro ed eseguiamo la stessa serie di 

506
00:27:54,460 --> 00:27:59,240
simulazioni, facendola giocare contro tutte le 2315 possibili risposte di parole, come va?

507
00:28:00,280 --> 00:28:02,669
Beh, a differenza della nostra prima versione è decisamente migliore, 

508
00:28:02,669 --> 00:28:03,420
il che è rassicurante.

509
00:28:04,020 --> 00:28:08,094
Tutto sommato la media è intorno a 3.6, anche se a differenza della prima versione 

510
00:28:08,094 --> 00:28:12,120
ci sono un paio di volte che perde e ne richiede più di sei in questa circostanza.

511
00:28:12,640 --> 00:28:15,210
Presumibilmente perché ci sono momenti in cui è necessario fare quel compromesso 

512
00:28:15,210 --> 00:28:17,940
per raggiungere effettivamente l'obiettivo piuttosto che massimizzare le informazioni.

513
00:28:19,040 --> 00:28:21,000
Quindi possiamo fare meglio di 3.6?

514
00:28:22,080 --> 00:28:22,920
Possiamo sicuramente.

515
00:28:23,280 --> 00:28:26,300
Ora, all'inizio ho detto che è molto divertente provare a non incorporare la 

516
00:28:26,300 --> 00:28:29,360
vera lista delle risposte di Wordle nel modo in cui costruisce il suo modello.

517
00:28:29,880 --> 00:28:34,180
Ma se lo incorporiamo, la prestazione migliore che potrei ottenere è stata di circa 3.43.

518
00:28:35,160 --> 00:28:37,643
Quindi, se proviamo a diventare più sofisticati rispetto al semplice 

519
00:28:37,643 --> 00:28:40,090
utilizzo dei dati sulla frequenza delle parole per scegliere questa 

520
00:28:40,090 --> 00:28:42,753
distribuzione a priori, questo 3.43 probabilmente dà un massimo di quanto 

521
00:28:42,753 --> 00:28:45,740
bene potremmo ottenere con quello, o almeno quanto bene potrei ottenere con quello.

522
00:28:46,240 --> 00:28:49,308
Quella prestazione migliore essenzialmente utilizza semplicemente 

523
00:28:49,308 --> 00:28:51,679
le idee di cui ho parlato qui, ma va un po' oltre, 

524
00:28:51,679 --> 00:28:55,120
come se cercasse le informazioni attese due passi avanti anziché solo uno.

525
00:28:55,620 --> 00:28:57,375
Inizialmente avevo intenzione di parlarne di più, 

526
00:28:57,375 --> 00:29:00,220
ma mi rendo conto che in realtà siamo andati avanti piuttosto a lungo così com'è.

527
00:29:00,580 --> 00:29:03,367
L'unica cosa che dirò è che dopo aver effettuato questa ricerca in due 

528
00:29:03,367 --> 00:29:06,626
passaggi e aver eseguito un paio di simulazioni di esempio sui migliori candidati, 

529
00:29:06,626 --> 00:29:09,100
finora almeno per me sembra che Crane sia il miglior apripista.

530
00:29:09,100 --> 00:29:10,060
Chi l'avrebbe mai detto?

531
00:29:10,920 --> 00:29:15,402
Inoltre, se usi l'elenco delle parole vere per determinare il tuo spazio di possibilità, 

532
00:29:15,402 --> 00:29:17,820
l'incertezza con cui inizi è poco più di 11 bit.

533
00:29:18,300 --> 00:29:21,244
E si scopre che, solo da una ricerca con forza bruta, 

534
00:29:21,244 --> 00:29:25,880
la massima informazione possibile attesa dopo le prime due ipotesi è di circa 10 bit.

535
00:29:26,500 --> 00:29:30,607
Il che suggerisce che, nella migliore delle ipotesi, dopo le prime due ipotesi, 

536
00:29:30,607 --> 00:29:34,560
con un gioco perfettamente ottimale, rimarrai con circa un po' di incertezza.

537
00:29:34,800 --> 00:29:37,320
Il che equivale ad avere solo due possibili ipotesi.

538
00:29:37,740 --> 00:29:40,846
Quindi penso che sia giusto e probabilmente piuttosto prudente dire che 

539
00:29:40,846 --> 00:29:43,953
non potresti mai scrivere un algoritmo che porti questa media fino a 3, 

540
00:29:43,953 --> 00:29:46,973
perché con le parole a tua disposizione, semplicemente non c'è spazio 

541
00:29:46,973 --> 00:29:50,080
per ottenere informazioni sufficienti dopo solo due passaggi per essere 

542
00:29:50,080 --> 00:29:53,360
in grado di garantire la risposta nella terza fascia ogni volta senza fallo.

