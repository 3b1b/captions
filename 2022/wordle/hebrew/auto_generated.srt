1
00:00:00,000 --> 00:00:03,449
המשחק Wurdle הפך די ויראלי בחודש-חודשיים האחרונים,

2
00:00:03,449 --> 00:00:07,777
ואף פעם לא יתעלם מהזדמנות לשיעור מתמטיקה, עולה בדעתי שהמשחק הזה

3
00:00:07,777 --> 00:00:13,120
מהווה דוגמה מרכזית טובה מאוד בשיעור על תורת המידע, ובפרט נושא המכונה אנטרופיה.

4
00:00:13,120 --> 00:00:16,677
אתה מבין, כמו הרבה אנשים קצת נשאבתי לתוך הפאזל,

5
00:00:16,677 --> 00:00:23,200
וכמו הרבה מתכנתים גם נשאבתי לנסות לכתוב אלגוריתם שישחק את המשחק הכי אופטימלי שהוא יכול.

6
00:00:23,200 --> 00:00:26,980
ומה שחשבתי לעשות כאן זה פשוט לדבר איתך על חלק מהתהליך שלי בזה,

7
00:00:26,980 --> 00:00:32,080
ולהסביר חלק מהמתמטיקה שנכנסה לזה, מכיוון שכל האלגוריתם מתרכז ברעיון הזה של אנטרופיה.

8
00:00:32,080 --> 00:00:42,180
דבר ראשון, למקרה שלא שמעתם על זה, מה זה וורדל?

9
00:00:42,180 --> 00:00:45,968
וכדי להרוג כאן שתי ציפורים במכה אחת בזמן שאנחנו עוברים על כללי המשחק,

10
00:00:45,968 --> 00:00:48,565
הרשו לי גם לראות מקדימה לאן אנחנו הולכים עם זה,

11
00:00:48,565 --> 00:00:51,380
כלומר לפתח אלגוריתם קטן שבעצם ישחק את המשחק עבורנו.

12
00:00:51,380 --> 00:00:55,860
למרות שלא עשיתי את הוורדל של היום, זה 4 בפברואר, ונראה איך הבוט יצליח.

13
00:00:55,860 --> 00:01:00,860
המטרה של Wurdle היא לנחש מילה מסתורית של חמש אותיות, וניתנות לך שש הזדמנויות שונות לנחש.

14
00:01:00,860 --> 00:01:05,240
לדוגמה, בוט הוורדל שלי מציע שאתחיל עם מנוף הניחוש.

15
00:01:05,240 --> 00:01:10,940
בכל פעם שאתה מנחש, אתה מקבל מידע על כמה קרוב הניחוש שלך לתשובה האמיתית.

16
00:01:10,940 --> 00:01:14,540
כאן התיבה האפורה אומרת לי שאין C בתשובה האמיתית.

17
00:01:14,540 --> 00:01:18,340
הקופסה הצהובה אומרת לי שיש R, אבל היא לא במצב הזה.

18
00:01:18,340 --> 00:01:22,820
התיבה הירוקה אומרת לי שלמילה הסודית יש א&#39;, והיא נמצאת במיקום השלישי.

19
00:01:22,820 --> 00:01:24,300
ואז אין N ואין E.

20
00:01:24,300 --> 00:01:27,420
אז תן לי פשוט להיכנס ולספר לבוט הוורדל את המידע הזה.

21
00:01:27,420 --> 00:01:31,500
התחלנו עם מנוף, קיבלנו אפור, צהוב, ירוק, אפור, אפור.

22
00:01:31,500 --> 00:01:35,460
אל תדאג לגבי כל הנתונים שהוא מציג עכשיו, אני אסביר את זה בבוא העת.

23
00:01:35,460 --> 00:01:39,700
אבל ההצעה העיקרית שלה לבחירה השנייה שלנו היא shtick.

24
00:01:39,700 --> 00:01:42,452
והניחוש שלך חייב להיות מילה אמיתית של חמש אותיות,

25
00:01:42,452 --> 00:01:45,700
אבל כפי שתראה, זה די ליברלי עם מה שהוא בעצם יאפשר לך לנחש.

26
00:01:45,700 --> 00:01:48,860
במקרה זה, אנו מנסים shtick.

27
00:01:48,860 --> 00:01:50,260
ובסדר, הדברים נראים די טוב.

28
00:01:50,260 --> 00:01:54,580
פגענו ב-S וב-H, אז אנחנו יודעים את שלוש האותיות הראשונות, אנחנו יודעים שיש R.

29
00:01:54,580 --> 00:01:59,740
וכך זה יהיה כמו ש.א. משהו R, או ש.א.ר משהו.

30
00:01:59,740 --> 00:02:05,220
ונראה שהבוט של Wurdle יודע שזה תלוי רק בשתי אפשרויות, או רסיס או חד.

31
00:02:05,220 --> 00:02:11,260
זה סוג של הטלה ביניהם בשלב זה, אז אני מניח שכנראה רק בגלל שהוא אלפביתי זה הולך עם רסיס.

32
00:02:11,260 --> 00:02:13,000
איזה הידד, היא התשובה האמיתית.

33
00:02:13,000 --> 00:02:14,660
אז קיבלנו את זה בשלושה.

34
00:02:14,660 --> 00:02:17,675
אם אתה תוהה אם זה טוב, איך ששמעתי ביטוי של אדם

35
00:02:17,675 --> 00:02:20,820
אחד זה שעם Wurdle ארבע הוא ערך ושלוש הוא ציפורי.

36
00:02:20,820 --> 00:02:22,960
שלדעתי היא אנלוגיה די הולמת.

37
00:02:22,960 --> 00:02:27,560
אתה צריך להיות בעקביות במשחק שלך כדי להשיג ארבע, אבל זה בהחלט לא מטורף.

38
00:02:27,560 --> 00:02:30,000
אבל כשאתה מקבל את זה בשלוש, זה פשוט מרגיש נהדר.

39
00:02:30,000 --> 00:02:33,300
אז אם אתה רוצה לעשות את זה, מה שאני רוצה לעשות כאן זה פשוט

40
00:02:33,300 --> 00:02:36,600
לדבר על תהליך החשיבה שלי מההתחלה איך אני ניגש לבוט Wurdle.

41
00:02:36,600 --> 00:02:39,800
וכמו שאמרתי, באמת שזה תירוץ לשיעור תורת מידע.

42
00:02:39,800 --> 00:02:43,160
המטרה העיקרית היא להסביר מהו מידע ומהי אנטרופיה.

43
00:02:43,160 --> 00:02:53,560
המחשבה הראשונה שלי בגישה לזה הייתה להסתכל על התדרים היחסיים של אותיות שונות בשפה האנגלית.

44
00:02:53,560 --> 00:02:57,069
אז חשבתי, אוקיי, האם יש ניחוש פתיחה או צמד ניחושים

45
00:02:57,069 --> 00:02:59,960
פתיחה שפוגע בהרבה מהאותיות השכיחות ביותר?

46
00:02:59,960 --> 00:03:03,780
ואחד שדי אהבתי היה לעשות אחר ואחריו ציפורניים.

47
00:03:03,780 --> 00:03:07,980
המחשבה היא שאם אתה מכה באות, אתה יודע, אתה מקבל ירוק או צהוב, זה תמיד מרגיש טוב.

48
00:03:07,980 --> 00:03:09,460
זה מרגיש כאילו אתה מקבל מידע.

49
00:03:09,460 --> 00:03:12,637
אבל במקרים אלה, גם אם אתה לא מכה ואתה תמיד מקבל אפור,

50
00:03:12,637 --> 00:03:17,640
זה עדיין נותן לך מידע רב מכיוון שזה די נדיר למצוא מילה שאין בה אף אחת מהאותיות הללו.

51
00:03:17,640 --> 00:03:20,870
אבל אפילו עדיין, זה לא מרגיש סופר שיטתי, כי למשל,

52
00:03:20,870 --> 00:03:23,520
זה לא עושה כלום כדי להתחשב בסדר האותיות.

53
00:03:23,520 --> 00:03:26,080
למה להקליד ציפורניים כשאני יכול להקליד חילזון?

54
00:03:26,080 --> 00:03:27,720
האם עדיף לקבל את ה-S הזה בסוף?

55
00:03:27,720 --> 00:03:28,720
אני לא ממש בטוח.

56
00:03:28,720 --> 00:03:32,861
עכשיו, חבר שלי אמר שהוא אהב לפתוח במילה עייף, מה שדי

57
00:03:32,861 --> 00:03:37,160
הפתיע אותי כי יש בה כמה אותיות לא שכיחות כמו ה-W וה-Y.

58
00:03:37,160 --> 00:03:39,400
אבל מי יודע, אולי זו פתיחה טובה יותר.

59
00:03:39,400 --> 00:03:44,920
האם יש איזשהו ציון כמותי שאנחנו יכולים לתת כדי לשפוט את האיכות של ניחוש פוטנציאלי?

60
00:03:44,920 --> 00:03:48,683
עכשיו כדי להגדיר את הדרך שבה אנחנו הולכים לדרג ניחושים אפשריים,

61
00:03:48,683 --> 00:03:51,800
בואו נחזור ונוסיף קצת בהירות כיצד בדיוק המשחק מוגדר.

62
00:03:51,800 --> 00:03:57,920
אז יש רשימה של מילים שזה יאפשר לך להזין שנחשבות לניחושים תקפים שאורכה רק כ-13,000 מילים.

63
00:03:57,920 --> 00:04:03,150
אבל כשמסתכלים על זה, יש הרבה דברים ממש לא שכיחים, דברים כמו ראש או עלי ו-ARG,

64
00:04:03,150 --> 00:04:07,040
מסוג המילים שמביאות לוויכוחים משפחתיים במשחק של Scrabble.

65
00:04:07,040 --> 00:04:10,600
אבל האווירה של המשחק היא שהתשובה תמיד תהיה מילה נפוצה בהחלט.

66
00:04:10,600 --> 00:04:16,080
ולמעשה, יש עוד רשימה של כ-2300 מילים שהן התשובות האפשריות.

67
00:04:16,080 --> 00:04:21,800
וזו רשימה שנאספה על ידי אדם, אני חושב במיוחד על ידי חברתו של יוצר המשחק, וזה די כיף.

68
00:04:21,800 --> 00:04:26,293
אבל מה שהייתי רוצה לעשות, האתגר שלנו עבור הפרויקט הזה הוא לראות אם

69
00:04:26,293 --> 00:04:30,720
נוכל לכתוב תוכנית לפתרון Wordle שאינה משלבת ידע קודם על רשימה זו.

70
00:04:30,720 --> 00:04:35,560
ראשית, יש הרבה מילים נפוצות למדי של חמש אותיות שלא תמצאו ברשימה הזו.

71
00:04:35,560 --> 00:04:39,848
אז עדיף לכתוב תוכנית שהיא קצת יותר עמידה ותשחק וורדל נגד כל אחד,

72
00:04:39,848 --> 00:04:41,960
לא רק מה שבמקרה הוא האתר הרשמי.

73
00:04:41,960 --> 00:04:47,440
וגם הסיבה שאנחנו יודעים מהי רשימה זו של תשובות אפשריות, היא בגלל שהיא גלויה בקוד המקור.

74
00:04:47,440 --> 00:04:52,840
אבל האופן שבו זה נראה בקוד המקור הוא בסדר הספציפי שבו התשובות עולות מיום ליום.

75
00:04:52,840 --> 00:04:56,400
אז אתה תמיד יכול פשוט לחפש מה תהיה התשובה של מחר.

76
00:04:56,400 --> 00:04:59,140
אז ברור, יש מובן מסוים שהשימוש ברשימה הוא רמאות.

77
00:04:59,140 --> 00:05:03,077
ומה שעושה חידה מעניינת יותר ושיעור תיאוריית מידע עשיר יותר הוא

78
00:05:03,077 --> 00:05:07,327
להשתמש במקום בכמה נתונים אוניברסליים יותר כמו תדרים יחסיים של מילים

79
00:05:07,327 --> 00:05:11,640
באופן כללי כדי ללכוד את האינטואיציה הזו של העדפה למילים נפוצות יותר.

80
00:05:11,640 --> 00:05:16,560
אז מבין 13,000 האפשרויות הללו, איך עלינו לבחור את ניחוש הפתיחה?

81
00:05:16,560 --> 00:05:19,960
לדוגמה, אם חבר שלי מציע נישואים עייפים, כיצד עלינו לנתח את איכותו?

82
00:05:19,960 --> 00:05:23,855
ובכן, הסיבה שהוא אמר שהוא אוהב את ה-W הלא סביר הזה היא שהוא

83
00:05:23,855 --> 00:05:27,880
אוהב את אופי ה-long shot של כמה טוב זה מרגיש אם תפגע ב-W הזה.

84
00:05:27,880 --> 00:05:31,867
לדוגמה, אם הדפוס הראשון שנחשף היה משהו כזה, אז מסתבר

85
00:05:31,867 --> 00:05:36,080
שיש רק 58 מילים בלקסיקון הענק הזה שתואמות את הדפוס הזה.

86
00:05:36,080 --> 00:05:38,900
אז זה הפחתה עצומה מ-13,000.

87
00:05:38,900 --> 00:05:43,320
אבל הצד השני של זה, כמובן, הוא שזה מאוד נדיר לקבל דפוס כזה.

88
00:05:43,320 --> 00:05:47,866
באופן ספציפי, אם כל מילה הייתה בעלת סבירות שווה להיות התשובה,

89
00:05:47,866 --> 00:05:51,680
ההסתברות לפגיעה בתבנית זו תהיה 58 חלקי בערך 13,000.

90
00:05:51,680 --> 00:05:53,880
כמובן, לא סביר להניח שהם יהיו תשובות.

91
00:05:53,880 --> 00:05:56,680
רוב אלו הן מילים מאוד לא ברורות ואפילו מפוקפקות.

92
00:05:56,680 --> 00:05:59,258
אבל לפחות עבור המעבר הראשון שלנו בכל זה, בואו נניח

93
00:05:59,258 --> 00:06:02,040
שכולן סבירות באותה מידה ואז נחדד את זה קצת מאוחר יותר.

94
00:06:02,040 --> 00:06:07,360
הנקודה היא שהדפוס עם הרבה מידע מעצם טבעו לא סביר שיתרחש.

95
00:06:07,360 --> 00:06:11,320
למעשה, המשמעות של להיות אינפורמטיבי הוא שזה לא סביר.

96
00:06:11,320 --> 00:06:18,360
דפוס הרבה יותר סביר לראות עם הפתיחה הזו יהיה משהו כזה, שבו כמובן אין בו W.

97
00:06:18,360 --> 00:06:22,080
אולי יש E, ואולי אין A, אין R, אין Y.

98
00:06:22,080 --> 00:06:24,640
במקרה זה, יש 1400 התאמות אפשריות.

99
00:06:24,640 --> 00:06:30,680
אם כולם היו סבירים באותה מידה, מסתבר שהסתברות של כ-11% היא זו הדפוס שהיית רואה.

100
00:06:30,680 --> 00:06:34,320
אז התוצאות הסבירות ביותר הן גם הפחות אינפורמטיביות.

101
00:06:34,320 --> 00:06:38,160
כדי לקבל תצוגה גלובלית יותר כאן, הרשו לי להראות לכם את ההתפלגות

102
00:06:38,160 --> 00:06:42,000
המלאה של ההסתברויות על פני כל הדפוסים השונים שאתם עשויים לראות.

103
00:06:42,000 --> 00:06:46,895
אז כל פס שאתה מסתכל עליו מתאים לתבנית אפשרית של צבעים שאפשר לחשוף,

104
00:06:46,895 --> 00:06:52,960
מתוכם יש 3 עד 5 אפשרויות, והם מאורגנים משמאל לימין, הנפוצים ביותר עד הפחות נפוצים.

105
00:06:52,960 --> 00:06:56,200
אז האפשרות הנפוצה ביותר כאן היא שאתה מקבל את כל האפורים.

106
00:06:56,200 --> 00:06:58,800
זה קורה בערך 14% מהמקרים.

107
00:06:58,800 --> 00:07:04,516
ומה שאתה מייחל לו כשאתה מנחש זה שאתה בסופו של דבר איפשהו בזנב הארוך הזה,

108
00:07:04,516 --> 00:07:09,920
כמו כאן, שם יש רק 18 אפשרויות למה שתואם את הדפוס הזה שכנראה נראה כך.

109
00:07:09,920 --> 00:07:14,080
או אם נצא קצת יותר שמאלה, אתה יודע, אולי נלך עד לכאן.

110
00:07:14,080 --> 00:07:16,560
אוקיי, הנה חידה טובה בשבילך.

111
00:07:16,560 --> 00:07:22,040
מהן שלוש המילים בשפה האנגלית שמתחילות ב-W, מסתיימות ב-Y, ויש בהן R איפשהו?

112
00:07:22,040 --> 00:07:27,560
מסתבר שהתשובות הן, בוא נראה, מלאות מילים, תולעות ומתפתלות.

113
00:07:27,560 --> 00:07:31,752
אז כדי לשפוט עד כמה המילה הזו טובה בסך הכל, אנחנו רוצים

114
00:07:31,752 --> 00:07:35,720
איזושהי מידה של כמות המידע הצפויה שתקבלו מההפצה הזו.

115
00:07:35,720 --> 00:07:42,899
אם נעבור על כל דפוס ונכפיל את ההסתברות שלו להתרחש פעמים משהו שמודד כמה הוא אינפורמטיבי,

116
00:07:42,899 --> 00:07:46,000
זה יכול אולי לתת לנו ציון אובייקטיבי.

117
00:07:46,000 --> 00:07:50,280
עכשיו האינסטינקט הראשון שלך לגבי מה המשהו הזה צריך להיות עשוי להיות מספר ההתאמות.

118
00:07:50,280 --> 00:07:52,960
אתה רוצה מספר ממוצע נמוך יותר של התאמות.

119
00:07:52,960 --> 00:07:58,171
אבל במקום זאת, הייתי רוצה להשתמש במדידה אוניברסלית יותר שלעתים קרובות אנו מייחסים למידע,

120
00:07:58,171 --> 00:08:02,153
וכזו שתהיה גמישה יותר ברגע שתוקצו לנו הסתברות שונה לכל אחת מ-13,000

121
00:08:02,153 --> 00:08:04,320
המילים הללו אם הן באמת התשובה או לא.

122
00:08:04,320 --> 00:08:11,626
יחידת המידע הסטנדרטית היא ה-bit, שיש לה נוסחה קצת מצחיקה,

123
00:08:11,626 --> 00:08:17,800
אבל היא ממש אינטואיטיבית אם רק נסתכל על דוגמאות.

124
00:08:17,800 --> 00:08:24,200
אם יש לך תצפית שחותכת את מרחב האפשרויות שלך לחצי, אנו אומרים שיש לה פיסת מידע אחת.

125
00:08:24,200 --> 00:08:27,261
בדוגמה שלנו, מרחב האפשרויות הוא כל המילים האפשריות,

126
00:08:27,261 --> 00:08:31,560
ומסתבר שבערך למחצית ממילות חמש האותיות יש S, קצת פחות מזה, אבל בערך חצי.

127
00:08:31,560 --> 00:08:35,200
אז התצפית הזו תיתן לך קצת מידע.

128
00:08:35,200 --> 00:08:39,733
אם במקום זאת עובדה חדשה מקצצת את מרחב האפשרויות הזה בפקטור של ארבע,

129
00:08:39,733 --> 00:08:42,000
אנו אומרים שיש לה שתי פיסות מידע.

130
00:08:42,000 --> 00:08:45,120
לדוגמה, מסתבר שלכרבע מהמילים הללו יש T.

131
00:08:45,120 --> 00:08:49,795
אם התצפית חותכת את החלל הזה בפקטור של שמונה, אנחנו אומרים שזה שלוש פיסות מידע,

132
00:08:49,795 --> 00:08:50,920
וכן הלאה וכן הלאה.

133
00:08:50,920 --> 00:08:55,000
ארבעה ביטים חותכים אותו ל-16, חמישה ביטים חותכים אותו ל-32.

134
00:08:55,000 --> 00:08:59,946
אז עכשיו אולי תרצו לעצור ולשאול את עצמכם, מהי הנוסחה

135
00:08:59,946 --> 00:09:04,520
למידע עבור מספר הביטים מבחינת ההסתברות להתרחשות?

136
00:09:04,520 --> 00:09:10,261
מה שאנחנו אומרים כאן הוא שכאשר אתה לוקח חצי אחד למספר הסיביות, זה אותו דבר כמו ההסתברות,

137
00:09:10,261 --> 00:09:14,712
שזה אותו דבר כמו לומר שניים בחזקת מספר הסיביות הוא אחד מעל ההסתברות,

138
00:09:14,712 --> 00:09:19,680
אשר מסדר מחדש בהמשך לומר שהמידע הוא בסיס היומן שניים מתוך אחד חלקי ההסתברות.

139
00:09:19,680 --> 00:09:22,555
ולפעמים אתה רואה את זה עם עוד סידור מחדש אחד,

140
00:09:22,555 --> 00:09:25,680
כאשר המידע הוא בסיס היומן השלילי שני של ההסתברות.

141
00:09:25,680 --> 00:09:29,358
בביטוי כך, זה יכול להיראות קצת מוזר למי שלא יודע מה,

142
00:09:29,358 --> 00:09:35,120
אבל זה באמת רק הרעיון האינטואיטיבי של לשאול כמה פעמים צמצמת את האפשרויות שלך בחצי.

143
00:09:35,120 --> 00:09:38,513
עכשיו אם אתה תוהה, אתה יודע, חשבתי שאנחנו סתם משחקים משחק מילים מהנה,

144
00:09:38,513 --> 00:09:39,920
למה לוגריתמים נכנסים לתמונה?

145
00:09:39,920 --> 00:09:44,463
אחת הסיבות לכך שזו יחידה נחמדה יותר היא שפשוט הרבה יותר קל לדבר

146
00:09:44,463 --> 00:09:49,149
על אירועים מאוד לא סבירים, הרבה יותר קל לומר שלתצפית יש 20 סיביות

147
00:09:49,149 --> 00:09:53,480
מידע מאשר לומר שההסתברות להתרחשות כאלה ואחרים היא 0.0000095.

148
00:09:53,480 --> 00:09:57,476
אבל סיבה מהותית יותר לכך שהביטוי הלוגריתמי הזה התברר

149
00:09:57,476 --> 00:10:02,000
כתוספת שימושית מאוד לתורת ההסתברות היא הדרך שבה מידע מתחבר.

150
00:10:02,000 --> 00:10:06,912
לדוגמה, אם תצפית אחת נותנת לך שתי פיסות מידע, מקצצת את השטח שלך בארבע,

151
00:10:06,912 --> 00:10:12,101
ואז תצפית שנייה כמו הניחוש השני שלך ב-Wordle נותנת לך עוד שלוש פיסות מידע,

152
00:10:12,101 --> 00:10:17,360
ומצמצמת אותך עוד יותר בפקטור של שמונה, שניים ביחד נותנים לך חמש פיסות מידע.

153
00:10:17,360 --> 00:10:21,200
באותו אופן שבו הסתברויות אוהבות להכפיל, מידע אוהב להוסיף.

154
00:10:21,200 --> 00:10:25,862
אז ברגע שאנחנו נמצאים בתחום של משהו כמו ערך צפוי, שבו אנחנו מוסיפים חבורה של מספרים,

155
00:10:25,862 --> 00:10:28,660
היומנים הופכים את זה להרבה יותר נחמד להתמודד איתו.

156
00:10:28,660 --> 00:10:35,560
בואו נחזור להפצה שלנו עבור Weary ונוסיף עוד עוקב קטן כאן, שמראה לנו כמה מידע יש לכל דפוס.

157
00:10:35,560 --> 00:10:40,814
הדבר העיקרי שאני רוצה שתשים לב הוא שככל שההסתברות גבוהה יותר כשנגיע לדפוסים הסבירים האלה,

158
00:10:40,814 --> 00:10:43,500
כך המידע נמוך יותר, כך אתה מרוויח פחות ביטים.

159
00:10:43,500 --> 00:10:48,860
הדרך שבה אנו מודדים את האיכות של הניחוש הזה תהיה לקחת את הערך הצפוי של המידע הזה,

160
00:10:48,860 --> 00:10:52,390
שם אנחנו עוברים על כל דפוס, אנחנו אומרים כמה זה סביר,

161
00:10:52,390 --> 00:10:54,940
ואז נכפיל את זה בכמה סיביות מידע נקבל.

162
00:10:54,940 --> 00:10:58,480
ובדוגמה של Weary, מסתבר שזה 4.9 ביטים.

163
00:10:58,480 --> 00:11:01,929
אז בממוצע, המידע שאתה מקבל מניחוש הפתיחה הזה טוב

164
00:11:01,929 --> 00:11:05,660
כמו לחתוך את מרחב האפשרויות שלך לחצי בערך חמש פעמים.

165
00:11:05,660 --> 00:11:13,220
לעומת זאת, דוגמה לניחוש עם ערך מידע צפוי גבוה יותר תהיה משהו כמו Slate.

166
00:11:13,220 --> 00:11:16,180
במקרה זה תבחין שההפצה נראית הרבה יותר שטוחה.

167
00:11:16,180 --> 00:11:22,280
בפרט, להתרחשות הסבירה ביותר של כל האפורים יש רק סיכוי של כ-6% להתרחש,

168
00:11:22,280 --> 00:11:25,940
כך שלפחות אתה מקבל כנראה 3.9 סיביות מידע.

169
00:11:25,940 --> 00:11:29,140
אבל זה מינימום, בדרך כלל תקבל משהו יותר טוב מזה.

170
00:11:29,140 --> 00:11:34,414
ומסתבר שכאשר אתה מכתש את המספרים על זה ומחבר את כל המונחים הרלוונטיים,

171
00:11:34,414 --> 00:11:36,420
המידע הממוצע הוא בערך 5.8.

172
00:11:36,420 --> 00:11:43,940
אז בניגוד ל-Weary, מרחב האפשרויות שלך יהיה גדול בערך בחצי לאחר הניחוש הראשון הזה, בממוצע.

173
00:11:43,940 --> 00:11:49,540
למעשה יש סיפור מהנה על השם של הערך הצפוי הזה של כמות מידע.

174
00:11:49,540 --> 00:11:53,555
תורת המידע פותחה על ידי קלוד שאנון, שעבד ב-Bell Labs בשנות ה-40,

175
00:11:53,555 --> 00:11:57,817
אבל הוא דיבר על כמה מהרעיונות שלו שטרם פורסמו עם ג&#39;ון פון נוימן,

176
00:11:57,817 --> 00:12:01,091
שהיה הענק האינטלקטואלי הזה של אותה תקופה, בולט מאוד.

177
00:12:01,091 --> 00:12:04,180
במתמטיקה ובפיזיקה ותחילתו של מה שהפך למדעי המחשב.

178
00:12:04,180 --> 00:12:10,079
וכשהזכיר שאין לו שם טוב לערך הצפוי הזה של כמות מידע, כביכול פון נוימן אמר,

179
00:12:10,079 --> 00:12:14,720
אז הסיפור אומר, ובכן כדאי לקרוא לזה אנטרופיה, ומשתי סיבות.

180
00:12:14,720 --> 00:12:20,626
מלכתחילה, פונקציית אי הוודאות שלך שימשה במכניקה סטטיסטית תחת השם הזה, אז כבר יש לה שם,

181
00:12:20,626 --> 00:12:24,631
ובמקום השני, ויותר חשוב, אף אחד לא יודע מהי באמת אנטרופיה,

182
00:12:24,631 --> 00:12:26,940
אז בוויכוח אתה תמיד יש את היתרון.

183
00:12:26,940 --> 00:12:33,420
אז אם השם נראה קצת מסתורי, ואם אפשר להאמין לסיפור הזה, זה סוג של תכנון.

184
00:12:33,420 --> 00:12:39,401
כמו כן, אם אתה תוהה לגבי הקשר שלו לכל החומר השני של החוק השני של התרמודינמיקה מהפיזיקה,

185
00:12:39,401 --> 00:12:44,838
בהחלט יש קשר, אבל במקורותיו שאנון עסק רק בתורת ההסתברות הטהורה, ולמטרותינו כאן,

186
00:12:44,838 --> 00:12:50,820
כשאני משתמש ב- אנטרופיה של מילים, אני רק רוצה שתחשוב על ערך המידע הצפוי של ניחוש מסוים.

187
00:12:50,820 --> 00:12:54,380
אתה יכול לחשוב על אנטרופיה כמדידת שני דברים בו זמנית.

188
00:12:54,380 --> 00:12:57,420
הראשון הוא עד כמה שטוחה ההתפלגות.

189
00:12:57,420 --> 00:13:01,700
ככל שהתפלגות קרובה יותר לאחידות, כך האנטרופיה תהיה גבוהה יותר.

190
00:13:01,700 --> 00:13:07,114
במקרה שלנו, כאשר יש 3 עד ה-5 דפוסים הכוללים, עבור התפלגות אחידה,

191
00:13:07,114 --> 00:13:13,111
צפייה בכל אחד מהם תהיה בסיס יומן מידע 2 מתוך 3 עד ה-5, שהוא במקרה 7.92,

192
00:13:13,111 --> 00:13:17,860
אז זה המקסימום המוחלט שיכול להיות לך עבור האנטרופיה הזו.

193
00:13:17,860 --> 00:13:22,900
אבל אנטרופיה היא גם סוג של מדד לכמה אפשרויות יש מלכתחילה.

194
00:13:22,900 --> 00:13:27,467
לדוגמה, אם במקרה יש לך מילה כלשהי שבה יש רק 16 דפוסים אפשריים,

195
00:13:27,467 --> 00:13:32,760
וכל אחת מהן בסבירות שווה, האנטרופיה הזו, המידע הצפוי הזה, תהיה 4 סיביות.

196
00:13:32,760 --> 00:13:37,012
אבל אם יש לך מילה אחרת שבה יש 64 דפוסים אפשריים שיכולים להופיע,

197
00:13:37,012 --> 00:13:41,000
וכולם סבירים באותה מידה, אז האנטרופיה תסתבר להיות 6 סיביות.

198
00:13:41,000 --> 00:13:46,261
אז אם אתה רואה איזושהי התפלגות בטבע שיש לה אנטרופיה של 6 ביטים,

199
00:13:46,261 --> 00:13:52,838
זה בערך כאילו זה אומר שיש שונות וחוסר ודאות במה שעומד לקרות כאילו היו 64 תוצאות

200
00:13:52,838 --> 00:13:54,400
סבירות באותה מידה.

201
00:13:54,400 --> 00:13:58,360
עבור המעבר הראשון שלי ב-Wurtelebot, בעצם הייתי צריך לעשות את זה.

202
00:13:58,360 --> 00:14:03,192
הוא עובר על כל הניחושים האפשריים שיכולים להיות לך, כל 13,000 המילים,

203
00:14:03,192 --> 00:14:09,355
מחשב את האנטרופיה עבור כל אחת, או ליתר דיוק, את האנטרופיה של ההתפלגות על פני כל הדפוסים

204
00:14:09,355 --> 00:14:13,067
שאתה עשוי לראות, עבור כל אחת, ובוחר את הגבוהה ביותר,

205
00:14:13,067 --> 00:14:17,200
מכיוון שזהו זה שצפוי לקצץ את מרחב האפשרויות שלך ככל האפשר.

206
00:14:17,200 --> 00:14:21,680
ולמרות שדיברתי רק על הניחוש הראשון כאן, זה עושה את אותו הדבר עבור הניחושים הבאים.

207
00:14:21,680 --> 00:14:24,571
לדוגמה, לאחר שתראה דפוס כלשהו בניחוש הראשון הזה,

208
00:14:24,571 --> 00:14:28,642
שיגביל אותך למספר קטן יותר של מילים אפשריות בהתבסס על מה שמתאים לזה,

209
00:14:28,642 --> 00:14:32,300
אתה פשוט משחק באותו משחק ביחס לאותה קבוצה קטנה יותר של מילים.

210
00:14:32,300 --> 00:14:39,005
לניחוש שני מוצע, אתה מסתכל על ההתפלגות של כל הדפוסים שיכולים להתרחש מאותה קבוצה מוגבלת

211
00:14:39,005 --> 00:14:45,480
יותר של מילים, אתה מחפש בכל 13,000 האפשרויות, ומוצא את זו שממקסמת את האנטרופיה הזו.

212
00:14:45,480 --> 00:14:49,627
כדי להראות לכם איך זה עובד בפעולה, הרשו לי רק להעלות גרסה

213
00:14:49,627 --> 00:14:54,060
קטנה של Wurtele שכתבתי שמראה את הדגשים של הניתוח הזה בשוליים.

214
00:14:54,060 --> 00:14:57,264
לאחר ביצוע כל חישובי האנטרופיה שלו, מימין כאן הוא

215
00:14:57,264 --> 00:15:00,340
מראה לנו למי מהם יש את המידע הצפוי הגבוה ביותר.

216
00:15:00,340 --> 00:15:05,543
מסתבר שהתשובה העליונה, לפחות כרגע, נחדד את זה בהמשך,

217
00:15:05,543 --> 00:15:11,140
היא טארס, שפירושו, אממ, כמובן, בקיה, הקיבה הנפוצה ביותר.

218
00:15:11,140 --> 00:15:15,914
בכל פעם שאנחנו מנחשים כאן, איפה אולי אני קצת מתעלם מההמלצות שלו והולך עם צפחה,

219
00:15:15,914 --> 00:15:19,480
כי אני אוהב צפחה, אנחנו יכולים לראות כמה מידע צפוי היה לו,

220
00:15:19,480 --> 00:15:23,408
אבל אז בצד ימין של המילה כאן זה מראה לנו כמה מידע אמיתי שקיבלנו,

221
00:15:23,408 --> 00:15:24,980
בהתחשב בדפוס הספציפי הזה.

222
00:15:24,980 --> 00:15:28,672
אז כאן זה נראה כאילו היה לנו קצת חסר מזל, היה צפוי לנו לקבל 5.8,

223
00:15:28,672 --> 00:15:30,660
אבל במקרה קיבלנו משהו עם פחות מזה.

224
00:15:30,660 --> 00:15:35,860
ואז בצד שמאל כאן זה מראה לכולנו את המילים האפשריות השונות שבהן אנחנו נמצאים עכשיו.

225
00:15:35,860 --> 00:15:39,310
הפסים הכחולים אומרים לנו עד כמה הוא חושב שכל מילה היא,

226
00:15:39,310 --> 00:15:44,140
אז כרגע זה מניח שכל מילה בסבירות שווה להתרחש, אבל אנחנו נחדד את זה בעוד רגע.

227
00:15:44,140 --> 00:15:50,212
ואז מדידת אי הוודאות הזו אומרת לנו את האנטרופיה של ההתפלגות הזו על פני המילים האפשריות,

228
00:15:50,212 --> 00:15:55,940
שכרגע, מכיוון שזו התפלגות אחידה, היא רק דרך מסובכת מיותרת לספור את מספר האפשרויות.

229
00:15:55,940 --> 00:16:02,700
לדוגמה, אם היינו לוקחים 2 בחזקת 13.66, זה אמור להיות בסביבות 13,000 האפשרויות.

230
00:16:02,700 --> 00:16:06,780
אני קצת בחוץ כאן, אבל רק בגלל שאני לא מראה את כל האותיות העשרוניות.

231
00:16:06,780 --> 00:16:09,616
כרגע זה עשוי להרגיש מיותר וכאילו זה מסבך דברים מדי,

232
00:16:09,616 --> 00:16:12,780
אבל אתה תראה למה זה שימושי להחזיק את שני המספרים תוך דקה.

233
00:16:12,780 --> 00:16:18,015
אז כאן זה נראה כאילו זה מרמז על האנטרופיה הגבוהה ביותר עבור הניחוש השני שלנו הוא ראמן,

234
00:16:18,015 --> 00:16:19,700
ששוב ממש לא מרגיש כמו מילה.

235
00:16:19,700 --> 00:16:25,660
אז כדי לקחת את הרמה המוסרית כאן, אני מתכוון להמשיך ולהקליד את Rains.

236
00:16:25,660 --> 00:16:27,540
ושוב זה נראה כאילו היה לנו קצת חסר מזל.

237
00:16:27,540 --> 00:16:32,100
ציפינו ל-4.3 ביטים וקיבלנו רק 3.39 סיביות מידע.

238
00:16:32,100 --> 00:16:35,060
אז זה מוריד אותנו ל-55 אפשרויות.

239
00:16:35,060 --> 00:16:40,200
וכאן אולי אני פשוט אלך עם מה שזה מציע, שזה שילוב, מה שזה לא אומר.

240
00:16:40,200 --> 00:16:43,300
ובסדר, זו בעצם הזדמנות טובה לפאזל.

241
00:16:43,300 --> 00:16:47,020
זה אומר לנו שהדפוס הזה נותן לנו 4.7 סיביות מידע.

242
00:16:47,020 --> 00:16:52,400
אבל בצד שמאל, לפני שאנחנו רואים את הדפוס הזה, היו 5.78 פיסות של אי ודאות.

243
00:16:52,400 --> 00:16:56,860
אז בתור חידון בשבילך, מה זה אומר לגבי מספר האפשרויות שנותרו?

244
00:16:56,860 --> 00:17:01,020
ובכן, זה אומר שאנחנו מצטמצמים לחלק אחד של אי ודאות,

245
00:17:01,020 --> 00:17:04,700
וזה אותו דבר כמו לומר שיש שתי תשובות אפשריות.

246
00:17:04,700 --> 00:17:06,520
זו בחירה של 50-50.

247
00:17:06,520 --> 00:17:09,227
ומכאן, בגלל שאתה ואני יודעים אילו מילים נפוצות יותר,

248
00:17:09,227 --> 00:17:11,220
אנחנו יודעים שהתשובה צריכה להיות תהום.

249
00:17:11,220 --> 00:17:13,540
אבל כמו שזה נכתב עכשיו, התוכנית לא יודעת את זה.

250
00:17:13,540 --> 00:17:20,360
אז זה פשוט ממשיך, מנסה להשיג כמה שיותר מידע, עד שנותרה רק אפשרות אחת, ואז הוא מנחש את זה.

251
00:17:20,360 --> 00:17:22,700
אז ברור שאנחנו צריכים אסטרטגיית סוף משחק טובה יותר.

252
00:17:22,700 --> 00:17:26,551
אבל נניח שאנחנו קוראים לגרסה הזו אחת מפותרי המילים שלנו,

253
00:17:26,551 --> 00:17:30,740
ואז אנחנו הולכים ומריצים כמה סימולציות כדי לראות איך זה עובד.

254
00:17:30,740 --> 00:17:34,240
אז הדרך שבה זה עובד היא שהוא משחק בכל משחק מילים אפשרי.

255
00:17:34,240 --> 00:17:38,780
זה עובר על כל 2315 המילים האלה שהן התשובות המילוליות בפועל.

256
00:17:38,780 --> 00:17:41,340
זה בעצם משתמש בזה כמערכת בדיקות.

257
00:17:41,340 --> 00:17:45,288
ועם השיטה הנאיבית הזו של לא להתחשב עד כמה מילה נפוצה,

258
00:17:45,288 --> 00:17:50,480
ורק לנסות למקסם את המידע בכל שלב בדרך, עד שהוא מגיע לבחירה אחת ויחידה.

259
00:17:50,480 --> 00:17:55,100
בסוף הסימולציה, הציון הממוצע מתברר כ-4.124.

260
00:17:55,100 --> 00:17:59,780
וזה לא רע, למען האמת, די ציפיתי לעשות יותר גרוע.

261
00:17:59,780 --> 00:18:03,040
אבל האנשים שמנגנים wordle יגידו לך שבדרך כלל הם יכולים להשיג את זה ב-4.

262
00:18:03,040 --> 00:18:05,260
האתגר האמיתי הוא להשיג כמה שיותר ב-3.

263
00:18:05,260 --> 00:18:08,920
זה קפיצה די גדולה בין הציון 4 לציון 3.

264
00:18:08,920 --> 00:18:23,160
הפרי התלוי הנמוך כאן הוא איכשהו לשלב האם מילה נפוצה או לא, ואיך בדיוק אנחנו עושים את זה.

265
00:18:23,160 --> 00:18:28,560
הדרך שבה ניגשתי היא לקבל רשימה של התדרים היחסיים של כל המילים בשפה האנגלית.

266
00:18:28,560 --> 00:18:31,874
והרגע השתמשתי בפונקציית נתוני תדירות המילים של Mathematica,

267
00:18:31,874 --> 00:18:35,520
שבעצמה שואבת ממאגר הנתונים הציבורי של Google Books English Ngram.

268
00:18:35,520 --> 00:18:40,120
וזה די כיף להסתכל עליו, למשל אם נמיין את זה מהמילים הנפוצות ביותר למילים הפחות נפוצות.

269
00:18:40,120 --> 00:18:43,740
ברור שאלו הן המילים הנפוצות ביותר, 5 אותיות בשפה האנגלית.

270
00:18:43,740 --> 00:18:46,480
או ליתר דיוק, אלו הם ה-8 הנפוצים ביותר.

271
00:18:46,480 --> 00:18:49,440
ראשון זה איזה, אחרי זה יש שם ושם.

272
00:18:49,440 --> 00:18:54,161
הראשון עצמו הוא לא הראשון, אלא התשיעי, והגיוני שהמילים האחרות הללו יכולות להופיע

273
00:18:54,161 --> 00:18:59,000
לעתים קרובות יותר, כאשר אלה שאחרי הראשונים הם אחרי, איפה, ואלו רק קצת פחות נפוצות.

274
00:18:59,000 --> 00:19:04,134
כעת, בשימוש בנתונים האלה כדי להדגים את הסבירות שכל אחת מהמילים הללו תהיה התשובה הסופית,

275
00:19:04,134 --> 00:19:06,760
היא לא צריכה להיות רק פרופורציונלית לתדירות.

276
00:19:06,760 --> 00:19:11,249
למשל, שניתן לו ציון 0.002 במערך הנתונים הזה, בעוד

277
00:19:11,249 --> 00:19:15,200
שהמילה צמה היא בסבירות מסוימת פי 1000 פחות.

278
00:19:15,200 --> 00:19:19,400
אבל שתיהן מילים מספיק נפוצות שכדאי לשקול אותן.

279
00:19:19,400 --> 00:19:21,900
אז אנחנו רוצים יותר חתך בינארי.

280
00:19:21,900 --> 00:19:27,997
הדרך שבה הלכתי היא לדמיין לקחת את כל הרשימה הממוינת הזו של מילים, ואז לסדר אותה על ציר x,

281
00:19:27,997 --> 00:19:33,553
ואז להחיל את הפונקציה הסיגמואידית, שהיא הדרך הסטנדרטית לקבל פונקציה שהפלט שלה הוא

282
00:19:33,553 --> 00:19:38,500
בעצם בינארי, זה או 0 או שזה 1, אבל יש החלקה ביניהם לאזור זה של אי ודאות.

283
00:19:38,500 --> 00:19:44,182
אז בעצם, ההסתברות שאני מקצה לכל מילה להימצאות ברשימה הסופית תהיה הערך

284
00:19:44,182 --> 00:19:49,540
של הפונקציה הסיגמואידית למעלה בכל מקום שבו היא ממוקמת על ציר ה-x.

285
00:19:49,540 --> 00:19:54,026
עכשיו ברור שזה תלוי בכמה פרמטרים, למשל כמה רחב הרווח בציר ה-x

286
00:19:54,026 --> 00:19:59,019
ממלאות המילים האלה קובע באיזו הדרגתית או תלולה אנחנו יורדים מ-1 ל-0,

287
00:19:59,019 --> 00:20:03,000
והמקום שבו אנחנו ממקמים אותן משמאל לימין קובע את החתך.

288
00:20:03,000 --> 00:20:07,340
למען האמת, הדרך שעשיתי את זה הייתה פשוט ללקק את האצבע שלי ולהכניס אותה לרוח.

289
00:20:07,340 --> 00:20:12,373
עיינתי ברשימה הממוינת וניסיתי למצוא חלון שבו כשהסתכלתי עליו הבנתי שכמחצית

290
00:20:12,373 --> 00:20:17,680
מהמילים האלה צפויות יותר מאשר לא להיות התשובה הסופית, והשתמשתי בזה בתור החתך.

291
00:20:17,680 --> 00:20:20,969
ברגע שיש לנו התפלגות כזו בין המילים, זה נותן לנו

292
00:20:20,969 --> 00:20:24,460
מצב נוסף שבו האנטרופיה הופכת למדידה ממש שימושית זו.

293
00:20:24,460 --> 00:20:29,704
לדוגמה, נניח ששיחקנו משחק ונתחיל עם הפותחים הישנים שלי, שהיו נוצה ומסמרים,

294
00:20:29,704 --> 00:20:33,760
ובסופו של דבר יש מצב שיש ארבע מילים אפשריות שמתאימות לזה.

295
00:20:33,760 --> 00:20:36,440
ונניח שאנו רואים בכולם סבירים באותה מידה.

296
00:20:36,440 --> 00:20:40,000
הרשו לי לשאול אתכם, מהי האנטרופיה של התפלגות זו?

297
00:20:40,000 --> 00:20:46,994
ובכן, המידע הקשור לכל אחת מהאפשרויות הללו יהיה בסיס היומן 2 מתוך 4,

298
00:20:46,994 --> 00:20:50,800
מכיוון שכל אחד מהם הוא 1 ו-4, וזה 2.

299
00:20:50,800 --> 00:20:52,780
שתי פיסות מידע, ארבע אפשרויות.

300
00:20:52,780 --> 00:20:54,360
הכל טוב מאוד וטוב.

301
00:20:54,360 --> 00:20:58,320
אבל מה אם אגיד לך שבעצם יש יותר מארבע גפרורים?

302
00:20:58,320 --> 00:21:02,600
במציאות, כשאנחנו מסתכלים ברשימת המילים המלאה, יש 16 מילים שמתאימות לה.

303
00:21:02,600 --> 00:21:06,986
אבל נניח שהמודל שלנו שם סבירות ממש נמוכה ל-12 המילים האחרות האלה

304
00:21:06,986 --> 00:21:11,440
להיות למעשה התשובה הסופית, משהו כמו 1 ל-1000 כי הן ממש לא ברורות.

305
00:21:11,440 --> 00:21:15,480
עכשיו הרשו לי לשאול אתכם, מהי האנטרופיה של התפלגות זו?

306
00:21:15,480 --> 00:21:20,738
אם האנטרופיה מדדה אך ורק את מספר ההתאמות כאן, אז אתה עשוי לצפות שזה יהיה משהו

307
00:21:20,738 --> 00:21:26,200
כמו בסיס היומן 2 מתוך 16, שיהיה 4, שתי פיסות יותר של אי ודאות ממה שהיה לנו קודם.

308
00:21:26,200 --> 00:21:30,320
אבל כמובן שאי הוודאות בפועל לא באמת שונה ממה שהיה לנו קודם.

309
00:21:30,320 --> 00:21:34,222
רק בגלל שיש את 12 המילים המעורפלות האלה לא אומר שזה

310
00:21:34,222 --> 00:21:38,200
יהיה כל כך מפתיע ללמוד שהתשובה הסופית היא קסם, למשל.

311
00:21:38,200 --> 00:21:41,983
אז כאשר אתה באמת עושה את החישוב כאן, ואתה מחבר את ההסתברות

312
00:21:41,983 --> 00:21:45,960
של כל התרחשות כפול המידע המתאים, מה שאתה מקבל הוא 2.11 ביטים.

313
00:21:45,960 --> 00:21:49,931
אני רק אומר, זה בעצם שני ביטים, בעצם ארבע האפשרויות האלה,

314
00:21:49,931 --> 00:21:54,107
אבל יש קצת יותר אי ודאות בגלל כל האירועים הבלתי סבירים האלה,

315
00:21:54,107 --> 00:21:57,120
אם כי אם תלמד אותם היית מקבל מזה המון מידע.

316
00:21:57,120 --> 00:22:01,800
אז בהתקרבות, זה חלק ממה שהופך את Wordle לדוגמא כל כך נחמדה לשיעור תורת מידע.

317
00:22:01,800 --> 00:22:05,280
יש לנו את שני יישומי ההרגשה המובהקים הללו עבור אנטרופיה.

318
00:22:05,280 --> 00:22:09,908
הראשון אומר לנו מה המידע הצפוי שנקבל מניחוש נתון,

319
00:22:09,908 --> 00:22:16,480
והשני אומר האם נוכל למדוד את אי הוודאות שנותרה בין כל המילים האפשריות.

320
00:22:16,480 --> 00:22:21,048
ואני צריך להדגיש, במקרה הראשון שבו אנחנו מסתכלים על המידע הצפוי של ניחוש,

321
00:22:21,048 --> 00:22:25,000
ברגע שיש לנו שקלול לא שווה למילים, זה משפיע על חישוב האנטרופיה.

322
00:22:25,000 --> 00:22:30,394
לדוגמה, הרשו לי להעלות את אותו מקרה שבדקנו קודם לכן של התפלגות הקשורה ל-Weary,

323
00:22:30,394 --> 00:22:34,560
אבל הפעם באמצעות התפלגות לא אחידה על פני כל המילים האפשריות.

324
00:22:34,560 --> 00:22:39,360
אז תן לי לראות אם אני יכול למצוא כאן חלק שממחיש את זה די טוב.

325
00:22:39,360 --> 00:22:42,480
אוקיי, כאן זה די טוב.

326
00:22:42,480 --> 00:22:45,980
כאן יש לנו שני דפוסים צמודים שסבירים בערך באותה מידה,

327
00:22:45,980 --> 00:22:49,480
אבל לאחד מהם נאמר לנו יש 32 מילים אפשריות שתואמות לה.

328
00:22:49,480 --> 00:22:52,443
ואם נבדוק מה הם, אלה 32 אלה, שכולן פשוט מילים

329
00:22:52,443 --> 00:22:55,600
מאוד לא סבירות כשאתה סורק את העיניים שלך מעליהן.

330
00:22:55,600 --> 00:22:59,767
קשה למצוא כאלה שמרגישות כמו תשובות סבירות, אולי צעקות,

331
00:22:59,767 --> 00:23:04,237
אבל אם נסתכל על התבנית השכנה בהתפלגות, שנחשבת כסבירה בערך,

332
00:23:04,237 --> 00:23:09,920
נאמר לנו שיש לה רק 8 התאמות אפשריות, אז רבע התאמות רבות, אבל זה סביר בערך.

333
00:23:09,920 --> 00:23:12,520
וכשאנחנו מוציאים את הגפרורים האלה, אנחנו יכולים לראות למה.

334
00:23:12,520 --> 00:23:17,840
חלק מהן תשובות סבירות ממש, כמו צלצול, או זעם, או ראפ.

335
00:23:17,840 --> 00:23:22,884
כדי להמחיש כיצד אנו משלבים את כל זה, הרשו לי להעלות כאן את גרסה 2 של ה-Wordlebot,

336
00:23:22,884 --> 00:23:25,960
ויש שניים או שלושה הבדלים עיקריים מהראשון שראינו.

337
00:23:25,960 --> 00:23:30,593
ראשית, כמו שאמרתי זה עתה, הדרך שבה אנו מחשבים את האנטרופיות הללו,

338
00:23:30,593 --> 00:23:34,946
הערכים הצפויים של המידע, משתמשת כעת בהתפלגות המעודנות יותר על

339
00:23:34,946 --> 00:23:39,300
פני הדפוסים שמשלבת את ההסתברות שמילה נתונה תהיה למעשה התשובה.

340
00:23:39,300 --> 00:23:44,160
כפי שזה קורה, הדמעות עדיין מספר 1, אם כי אלה שאחריו קצת שונים.

341
00:23:44,160 --> 00:23:47,845
שנית, כאשר היא תדרג את הבחירות המובילות שלה, היא עתידה לשמור

342
00:23:47,845 --> 00:23:52,196
מודל של ההסתברות שכל מילה היא התשובה האמיתית, והיא תשלב זאת בהחלטה שלה,

343
00:23:52,196 --> 00:23:55,520
שקל יותר לראות אותה ברגע שיש לנו כמה ניחושים על שולחן.

344
00:23:55,520 --> 00:24:01,120
שוב, מתעלמים מהמלצתו כי אנחנו לא יכולים לתת למכונות לשלוט בחיינו.

345
00:24:01,120 --> 00:24:05,746
ואני מניח שעלי להזכיר עוד דבר שונה כאן משמאל, שערך אי הוודאות,

346
00:24:05,746 --> 00:24:10,080
מספר הביטים הזה, כבר לא רק מיותר עם מספר ההתאמות האפשריות.

347
00:24:10,080 --> 00:24:16,731
עכשיו אם נמשוך אותו למעלה ונחשב 2 ל-8.02, שזה קצת מעל 256, אני מניח 259,

348
00:24:16,731 --> 00:24:22,471
מה שזה אומר זה למרות שיש 526 מילים בסך הכל שמתאימות לדפוס הזה,

349
00:24:22,471 --> 00:24:29,760
כמות אי הוודאות שיש לה דומה יותר למה שהיא הייתה אם היו 259 בסבירות שווה תוצאות.

350
00:24:29,760 --> 00:24:31,100
אתה יכול לחשוב על זה ככה.

351
00:24:31,100 --> 00:24:34,978
הוא יודע שבורקס הוא לא התשובה, אותו דבר עם יורט וזורל וזרוס,

352
00:24:34,978 --> 00:24:37,840
אז זה קצת פחות לא ודאי ממה שהיה במקרה הקודם.

353
00:24:37,840 --> 00:24:40,220
מספר ביטים זה יהיה קטן יותר.

354
00:24:40,220 --> 00:24:44,319
ואם אני ממשיך לשחק את המשחק, אני מחדד את זה עם

355
00:24:44,319 --> 00:24:48,680
כמה ניחושים שהם בהתאם למה שהייתי רוצה להסביר כאן.

356
00:24:48,680 --> 00:24:51,444
לפי הניחוש הרביעי, אם תסתכלו על הבחירות המובילות שלה,

357
00:24:51,444 --> 00:24:53,800
תוכלו לראות שזה כבר לא רק ממקסם את האנטרופיה.

358
00:24:53,800 --> 00:25:00,780
אז בשלב זה, מבחינה טכנית יש שבע אפשרויות, אבל היחידות עם סיכוי משמעותי הן מעונות ומילים.

359
00:25:00,780 --> 00:25:05,399
ואתה יכול לראות את הבחירה בשני אלה מעל כל הערכים האחרים האלה,

360
00:25:05,399 --> 00:25:07,560
שבאופן קפדני ייתן יותר מידע.

361
00:25:07,560 --> 00:25:10,885
בפעם הראשונה שעשיתי את זה, פשוט הוספתי את שני המספרים האלה כדי

362
00:25:10,885 --> 00:25:14,580
למדוד את האיכות של כל ניחוש, שלמעשה עבד טוב יותר ממה שאתה עשוי לחשוד.

363
00:25:14,580 --> 00:25:17,425
אבל זה ממש לא הרגיש שיטתי, ואני בטוח שיש עוד גישות

364
00:25:17,425 --> 00:25:19,880
שאנשים יכולים לנקוט אבל הנה זו שנחתתי עליה.

365
00:25:19,880 --> 00:25:24,019
אם אנחנו שוקלים את הסיכוי לניחוש הבא, כמו במקרה הזה מילים,

366
00:25:24,019 --> 00:25:28,440
מה שבאמת אכפת לנו הוא הציון הצפוי של המשחק שלנו אם נעשה את זה.

367
00:25:28,440 --> 00:25:33,962
וכדי לחשב את הציון הצפוי הזה, אנחנו אומרים מה ההסתברות שמילים הן התשובה בפועל,

368
00:25:33,962 --> 00:25:35,640
שכרגע היא מתארת לה 58%.

369
00:25:35,640 --> 00:25:40,400
אנחנו אומרים שעם סיכוי של 58%, הציון שלנו במשחק הזה יהיה 4.

370
00:25:40,400 --> 00:25:46,240
ואז עם הסתברות של 1 פחות 58%, הציון שלנו יהיה יותר מ-4.

371
00:25:46,240 --> 00:25:49,580
כמה עוד אנחנו לא יודעים, אבל אנחנו יכולים להעריך את זה

372
00:25:49,580 --> 00:25:52,920
על סמך כמה אי ודאות צפויה להיות ברגע שנגיע לנקודה הזו.

373
00:25:52,920 --> 00:25:56,600
ספציפית, כרגע יש 1.44 פיסות של אי ודאות.

374
00:25:56,600 --> 00:26:01,560
אם ננחש מילים, זה אומר לנו שהמידע הצפוי שנקבל הוא 1.27 ביטים.

375
00:26:01,560 --> 00:26:08,280
אז אם ננחש מילים, ההבדל הזה מייצג כמה חוסר ודאות סביר שנישאר עם אחרי שזה יקרה.

376
00:26:08,280 --> 00:26:11,556
מה שאנחנו צריכים זה איזושהי פונקציה, שאני קורא לה כאן,

377
00:26:11,556 --> 00:26:13,880
שקושרת את אי הוודאות הזו עם ציון צפוי.

378
00:26:13,880 --> 00:26:18,102
והדרך שבה זה התנהל הייתה פשוט לשרטט חבורה של נתונים ממשחקים

379
00:26:18,102 --> 00:26:22,324
קודמים המבוססים על גרסה 1 של הבוט כדי לומר היי מה היה הציון

380
00:26:22,324 --> 00:26:27,040
בפועל אחרי נקודות שונות עם כמויות מסוימות מאוד מדידות של אי ודאות.

381
00:26:27,040 --> 00:26:32,880
לדוגמה, נקודות הנתונים האלה כאן שנמצאות מעל ערך שהוא בערך כמו 8.7 בערך אומרים על כמה

382
00:26:32,880 --> 00:26:36,247
משחקים אחרי נקודה שבה היו 8.7 פיסות של אי ודאות,

383
00:26:36,247 --> 00:26:39,340
נדרשו שני ניחושים כדי לקבל את התשובה הסופית.

384
00:26:39,340 --> 00:26:43,180
למשחקים אחרים נדרשו שלושה ניחושים, למשחקים אחרים נדרשו ארבעה ניחושים.

385
00:26:43,180 --> 00:26:49,353
אם נעבור שמאלה כאן, כל הנקודות מעל האפס אומרות בכל פעם שיש אפס פיסות של אי ודאות,

386
00:26:49,353 --> 00:26:55,000
כלומר יש רק אפשרות אחת, אז מספר הניחושים הנדרש הוא תמיד רק אחד, וזה מרגיע.

387
00:26:55,000 --> 00:26:59,435
בכל פעם שהייתה קצת אי ודאות, כלומר זה היה בעצם רק בשתי אפשרויות,

388
00:26:59,435 --> 00:27:03,940
אז לפעמים זה דרש עוד ניחוש אחד, לפעמים זה דרש שני ניחושים נוספים.

389
00:27:03,940 --> 00:27:05,980
וכן הלאה וכן הלאה כאן.

390
00:27:05,980 --> 00:27:11,020
אולי דרך קצת יותר קלה לדמיין את הנתונים האלה היא לרכז אותם יחד ולקחת ממוצעים.

391
00:27:11,020 --> 00:27:17,506
לדוגמה, הסרגל הזה אומר שבין כל הנקודות שבהן היה לנו קצת אי ודאות,

392
00:27:17,506 --> 00:27:22,420
בממוצע מספר הניחושים החדשים הנדרשים היה בערך 1.5.

393
00:27:22,420 --> 00:27:28,898
והסרגל כאן אומר בין כל המשחקים השונים שבו בשלב מסוים חוסר הוודאות היה קצת מעל ארבע ביטים,

394
00:27:28,898 --> 00:27:33,360
שזה כמו לצמצם אותו ל-16 אפשרויות שונות, אז בממוצע זה דורש קצת

395
00:27:33,360 --> 00:27:36,240
יותר משני ניחושים מאותה נקודה קָדִימָה.

396
00:27:36,240 --> 00:27:40,080
ומכאן פשוט עשיתי רגרסיה כדי להתאים לפונקציה שנראתה סבירה לזה.

397
00:27:40,080 --> 00:27:44,975
ותזכרו שכל העניין בעשיית כל זה הוא כדי שנוכל לכמת את האינטואיציה

398
00:27:44,975 --> 00:27:49,720
הזו שככל שנשיג יותר מידע ממילה, כך הציון הצפוי יהיה נמוך יותר.

399
00:27:49,720 --> 00:27:55,052
אז עם זה כגרסה 2.0, אם נחזור אחורה ונריץ את אותה סט של סימולציות,

400
00:27:55,052 --> 00:27:59,820
כשזה ישחק מול כל 2315 תשובות המילה האפשריות, איך זה מסתדר?

401
00:27:59,820 --> 00:28:04,060
ובכן בניגוד לגרסה הראשונה שלנו זה בהחלט טוב יותר, וזה מרגיע.

402
00:28:04,060 --> 00:28:08,217
הכל אמר ועשה הממוצע הוא בסביבות 3.6, אם כי בניגוד לגרסה

403
00:28:08,217 --> 00:28:12,820
הראשונה יש כמה פעמים שהיא מפסידה ודורשת יותר משש בנסיבות אלה.

404
00:28:12,820 --> 00:28:18,980
ככל הנראה בגלל שיש מקרים שבהם זה עושה את הפשרה הזו ללכת על המטרה במקום למקסם את המידע.

405
00:28:18,980 --> 00:28:22,140
אז אנחנו יכולים לעשות יותר טוב מ-3.6?

406
00:28:22,140 --> 00:28:23,260
אנחנו בהחלט יכולים.

407
00:28:23,260 --> 00:28:26,408
עכשיו אמרתי בהתחלה שהכי כיף לנסות לא לשלב את הרשימה

408
00:28:26,408 --> 00:28:29,980
האמיתית של תשובות מילוליות בדרך שבה היא בונה את המודל שלה.

409
00:28:29,980 --> 00:28:35,180
אבל אם נשלב את זה, הביצועים הטובים ביותר שיכולתי לקבל היו בסביבות 3.43.

410
00:28:35,180 --> 00:28:38,926
אז אם ננסה להיות מתוחכמים יותר מסתם שימוש בנתוני תדירות מילים

411
00:28:38,926 --> 00:28:44,124
כדי לבחור את ההפצה הקודמת הזו, זה 3.43 כנראה נותן מקסימום כמה טוב יכולנו להגיע עם זה,

412
00:28:44,124 --> 00:28:46,360
או לפחות כמה טוב יכולתי להגיע עם זה.

413
00:28:46,360 --> 00:28:50,592
הביצועים הטובים ביותר האלה בעצם רק משתמשים ברעיונות שדיברתי עליהם כאן,

414
00:28:50,592 --> 00:28:55,660
אבל זה הולך קצת יותר רחוק, כאילו הוא מחפש את המידע הצפוי שני צעדים קדימה ולא רק אחד.

415
00:28:55,660 --> 00:29:00,580
במקור תכננתי לדבר על זה יותר, אבל אני מבין שלמעשה עברנו די הרבה זמן כמו שזה.

416
00:29:00,580 --> 00:29:04,960
הדבר היחיד שאני אגיד הוא לאחר ביצוע חיפוש דו-שלבי זה ולאחר מכן הפעלת כמה סימולציות

417
00:29:04,960 --> 00:29:09,500
לדוגמא במועמדים המובילים, עד כה עבורי לפחות זה נראה כאילו קריין הוא הפותח הטוב ביותר.

418
00:29:09,500 --> 00:29:11,080
מי היה מנחש?

419
00:29:11,080 --> 00:29:15,048
כמו כן, אם אתה משתמש ברשימת המילים האמיתית כדי לקבוע את מרחב האפשרויות שלך,

420
00:29:15,048 --> 00:29:17,920
אז אי הוודאות שאתה מתחיל איתה היא קצת יותר מ-11 ביטים.

421
00:29:17,920 --> 00:29:21,955
ומסתבר, רק מחיפוש כוח גס, המידע המקסימלי האפשרי

422
00:29:21,955 --> 00:29:26,580
הצפוי לאחר שני הניחושים הראשונים הוא בסביבות 10 ביטים.

423
00:29:26,580 --> 00:29:31,272
מה שמרמז על התרחיש הטוב ביותר, לאחר שני הניחושים הראשונים שלך,

424
00:29:31,272 --> 00:29:35,220
עם משחק אופטימלי לחלוטין, תישאר עם קצת אי ודאות אחת.

425
00:29:35,220 --> 00:29:37,400
וזה אותו דבר כמו לרדת לשני ניחושים אפשריים.

426
00:29:37,400 --> 00:29:42,781
אז אני חושב שזה הוגן וכנראה די שמרני לומר שלעולם לא תוכל לכתוב אלגוריתם שמקבל

427
00:29:42,781 --> 00:29:46,299
את הממוצע הזה נמוך כמו 3, כי עם המילים הזמינות לך,

428
00:29:46,299 --> 00:29:49,818
פשוט אין מקום לקבל מספיק מידע לאחר שני שלבים בלבד.

429
00:29:49,818 --> 00:29:53,820
מסוגל להבטיח את התשובה במשבצת השלישית בכל פעם בלי להיכשל.

