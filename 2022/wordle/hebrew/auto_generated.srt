1
00:00:00,000 --> 00:00:04,192
המשחק Wurdle הפך די ויראלי בחודש-חודשיים האחרונים, ואף פעם לא

2
00:00:04,192 --> 00:00:08,588
יתעלם מהזדמנות לשיעור מתמטיקה, עולה בדעתי שהמשחק הזה מהווה דוגמה

3
00:00:08,588 --> 00:00:13,120
מרכזית טובה מאוד בשיעור על תורת המידע, ובפרט נושא המכונה אנטרופיה.

4
00:00:13,120 --> 00:00:18,234
אתה מבין, כמו הרבה אנשים קצת נשאבתי לתוך הפאזל, וכמו הרבה מתכנתים גם

5
00:00:18,234 --> 00:00:23,200
נשאבתי לנסות לכתוב אלגוריתם שישחק את המשחק הכי אופטימלי שהוא יכול.

6
00:00:23,200 --> 00:00:27,700
ומה שחשבתי לעשות כאן זה פשוט לדבר איתך על חלק מהתהליך שלי בזה, ולהסביר חלק

7
00:00:27,700 --> 00:00:32,080
מהמתמטיקה שנכנסה לזה, מכיוון שכל האלגוריתם מתרכז ברעיון הזה של אנטרופיה.

8
00:00:32,080 --> 00:00:42,180
דבר ראשון, למקרה שלא שמעתם על זה, מה זה וורדל?

9
00:00:42,180 --> 00:00:46,888
וכדי להרוג כאן שתי ציפורים במכה אחת בזמן שאנחנו עוברים על כללי המשחק, הרשו לי גם לראות

10
00:00:46,888 --> 00:00:51,380
מקדימה לאן אנחנו הולכים עם זה, כלומר לפתח אלגוריתם קטן שבעצם ישחק את המשחק עבורנו.

11
00:00:51,380 --> 00:00:55,860
למרות שלא עשיתי את הוורדל של היום, זה 4 בפברואר, ונראה איך הבוט יצליח.

12
00:00:55,860 --> 00:01:00,860
המטרה של Wurdle היא לנחש מילה מסתורית של חמש אותיות, וניתנות לך שש הזדמנויות שונות לנחש.

13
00:01:00,860 --> 00:01:05,240
לדוגמה, בוט הוורדל שלי מציע שאתחיל עם מנוף הניחוש.

14
00:01:05,240 --> 00:01:10,940
בכל פעם שאתה מנחש, אתה מקבל מידע על כמה קרוב הניחוש שלך לתשובה האמיתית.

15
00:01:10,940 --> 00:01:14,540
כאן התיבה האפורה אומרת לי שאין C בתשובה האמיתית.

16
00:01:14,540 --> 00:01:18,340
הקופסה הצהובה אומרת לי שיש R, אבל היא לא במצב הזה.

17
00:01:18,340 --> 00:01:22,820
התיבה הירוקה אומרת לי שלמילה הסודית יש א&#39;, והיא נמצאת במיקום השלישי.

18
00:01:22,820 --> 00:01:24,300
ואז אין N ואין E.

19
00:01:24,300 --> 00:01:27,420
אז תן לי פשוט להיכנס ולספר לבוט הוורדל את המידע הזה.

20
00:01:27,420 --> 00:01:31,500
התחלנו עם מנוף, קיבלנו אפור, צהוב, ירוק, אפור, אפור.

21
00:01:31,500 --> 00:01:35,460
אל תדאג לגבי כל הנתונים שהוא מציג עכשיו, אני אסביר את זה בבוא העת.

22
00:01:35,460 --> 00:01:39,700
אבל ההצעה העיקרית שלה לבחירה השנייה שלנו היא shtick.

23
00:01:39,700 --> 00:01:42,672
והניחוש שלך חייב להיות מילה אמיתית של חמש אותיות, אבל

24
00:01:42,672 --> 00:01:45,700
כפי שתראה, זה די ליברלי עם מה שהוא בעצם יאפשר לך לנחש.

25
00:01:45,700 --> 00:01:48,860
במקרה זה, אנו מנסים shtick.

26
00:01:48,860 --> 00:01:50,260
ובסדר, הדברים נראים די טוב.

27
00:01:50,260 --> 00:01:54,580
פגענו ב-S וב-H, אז אנחנו יודעים את שלוש האותיות הראשונות, אנחנו יודעים שיש R.

28
00:01:54,580 --> 00:01:59,740
וכך זה יהיה כמו ש.א. משהו R, או ש.א.ר משהו.

29
00:01:59,740 --> 00:02:05,220
ונראה שהבוט של Wurdle יודע שזה תלוי רק בשתי אפשרויות, או רסיס או חד.

30
00:02:05,220 --> 00:02:11,260
זה סוג של הטלה ביניהם בשלב זה, אז אני מניח שכנראה רק בגלל שהוא אלפביתי זה הולך עם רסיס.

31
00:02:11,260 --> 00:02:13,000
איזה הידד, היא התשובה האמיתית.

32
00:02:13,000 --> 00:02:14,660
אז קיבלנו את זה בשלושה.

33
00:02:14,660 --> 00:02:17,675
אם אתה תוהה אם זה טוב, איך ששמעתי ביטוי של אדם

34
00:02:17,675 --> 00:02:20,820
אחד זה שעם Wurdle ארבע הוא ערך ושלוש הוא ציפורי.

35
00:02:20,820 --> 00:02:22,960
שלדעתי היא אנלוגיה די הולמת.

36
00:02:22,960 --> 00:02:27,560
אתה צריך להיות בעקביות במשחק שלך כדי להשיג ארבע, אבל זה בהחלט לא מטורף.

37
00:02:27,560 --> 00:02:30,000
אבל כשאתה מקבל את זה בשלוש, זה פשוט מרגיש נהדר.

38
00:02:30,000 --> 00:02:33,300
אז אם אתה רוצה לעשות את זה, מה שאני רוצה לעשות כאן זה פשוט

39
00:02:33,300 --> 00:02:36,600
לדבר על תהליך החשיבה שלי מההתחלה איך אני ניגש לבוט Wurdle.

40
00:02:36,600 --> 00:02:39,800
וכמו שאמרתי, באמת שזה תירוץ לשיעור תורת מידע.

41
00:02:39,800 --> 00:02:43,160
המטרה העיקרית היא להסביר מהו מידע ומהי אנטרופיה.

42
00:02:43,160 --> 00:02:53,560
המחשבה הראשונה שלי בגישה לזה הייתה להסתכל על התדרים היחסיים של אותיות שונות בשפה האנגלית.

43
00:02:53,560 --> 00:02:57,069
אז חשבתי, אוקיי, האם יש ניחוש פתיחה או צמד ניחושים

44
00:02:57,069 --> 00:02:59,960
פתיחה שפוגע בהרבה מהאותיות השכיחות ביותר?

45
00:02:59,960 --> 00:03:03,780
ואחד שדי אהבתי היה לעשות אחר ואחריו ציפורניים.

46
00:03:03,780 --> 00:03:07,980
המחשבה היא שאם אתה מכה באות, אתה יודע, אתה מקבל ירוק או צהוב, זה תמיד מרגיש טוב.

47
00:03:07,980 --> 00:03:09,460
זה מרגיש כאילו אתה מקבל מידע.

48
00:03:09,460 --> 00:03:13,461
אבל במקרים אלה, גם אם אתה לא מכה ואתה תמיד מקבל אפור, זה עדיין נותן

49
00:03:13,461 --> 00:03:17,640
לך מידע רב מכיוון שזה די נדיר למצוא מילה שאין בה אף אחת מהאותיות הללו.

50
00:03:17,640 --> 00:03:20,870
אבל אפילו עדיין, זה לא מרגיש סופר שיטתי, כי למשל,

51
00:03:20,870 --> 00:03:23,520
זה לא עושה כלום כדי להתחשב בסדר האותיות.

52
00:03:23,520 --> 00:03:26,080
למה להקליד ציפורניים כשאני יכול להקליד חילזון?

53
00:03:26,080 --> 00:03:27,720
האם עדיף לקבל את ה-S הזה בסוף?

54
00:03:27,720 --> 00:03:28,720
אני לא ממש בטוח.

55
00:03:28,720 --> 00:03:32,861
עכשיו, חבר שלי אמר שהוא אהב לפתוח במילה עייף, מה שדי

56
00:03:32,861 --> 00:03:37,160
הפתיע אותי כי יש בה כמה אותיות לא שכיחות כמו ה-W וה-Y.

57
00:03:37,160 --> 00:03:39,400
אבל מי יודע, אולי זו פתיחה טובה יותר.

58
00:03:39,400 --> 00:03:44,920
האם יש איזשהו ציון כמותי שאנחנו יכולים לתת כדי לשפוט את האיכות של ניחוש פוטנציאלי?

59
00:03:44,920 --> 00:03:48,154
עכשיו כדי להגדיר את הדרך שבה אנחנו הולכים לדרג ניחושים

60
00:03:48,154 --> 00:03:51,800
אפשריים, בואו נחזור ונוסיף קצת בהירות כיצד בדיוק המשחק מוגדר.

61
00:03:51,800 --> 00:03:57,920
אז יש רשימה של מילים שזה יאפשר לך להזין שנחשבות לניחושים תקפים שאורכה רק כ-13,000 מילים.

62
00:03:57,920 --> 00:04:02,412
אבל כשמסתכלים על זה, יש הרבה דברים ממש לא שכיחים, דברים כמו ראש או

63
00:04:02,412 --> 00:04:07,040
עלי ו-ARG, מסוג המילים שמביאות לוויכוחים משפחתיים במשחק של Scrabble.

64
00:04:07,040 --> 00:04:10,600
אבל האווירה של המשחק היא שהתשובה תמיד תהיה מילה נפוצה בהחלט.

65
00:04:10,600 --> 00:04:16,080
ולמעשה, יש עוד רשימה של כ-2300 מילים שהן התשובות האפשריות.

66
00:04:16,080 --> 00:04:21,800
וזו רשימה שנאספה על ידי אדם, אני חושב במיוחד על ידי חברתו של יוצר המשחק, וזה די כיף.

67
00:04:21,800 --> 00:04:26,293
אבל מה שהייתי רוצה לעשות, האתגר שלנו עבור הפרויקט הזה הוא לראות אם

68
00:04:26,293 --> 00:04:30,720
נוכל לכתוב תוכנית לפתרון Wordle שאינה משלבת ידע קודם על רשימה זו.

69
00:04:30,720 --> 00:04:35,560
ראשית, יש הרבה מילים נפוצות למדי של חמש אותיות שלא תמצאו ברשימה הזו.

70
00:04:35,560 --> 00:04:38,661
אז עדיף לכתוב תוכנית שהיא קצת יותר עמידה ותשחק

71
00:04:38,661 --> 00:04:41,960
וורדל נגד כל אחד, לא רק מה שבמקרה הוא האתר הרשמי.

72
00:04:41,960 --> 00:04:47,440
וגם הסיבה שאנחנו יודעים מהי רשימה זו של תשובות אפשריות, היא בגלל שהיא גלויה בקוד המקור.

73
00:04:47,440 --> 00:04:52,840
אבל האופן שבו זה נראה בקוד המקור הוא בסדר הספציפי שבו התשובות עולות מיום ליום.

74
00:04:52,840 --> 00:04:56,400
אז אתה תמיד יכול פשוט לחפש מה תהיה התשובה של מחר.

75
00:04:56,400 --> 00:04:59,140
אז ברור, יש מובן מסוים שהשימוש ברשימה הוא רמאות.

76
00:04:59,140 --> 00:05:03,077
ומה שעושה חידה מעניינת יותר ושיעור תיאוריית מידע עשיר יותר הוא

77
00:05:03,077 --> 00:05:07,327
להשתמש במקום בכמה נתונים אוניברסליים יותר כמו תדרים יחסיים של מילים

78
00:05:07,327 --> 00:05:11,640
באופן כללי כדי ללכוד את האינטואיציה הזו של העדפה למילים נפוצות יותר.

79
00:05:11,640 --> 00:05:16,560
אז מבין 13,000 האפשרויות הללו, איך עלינו לבחור את ניחוש הפתיחה?

80
00:05:16,560 --> 00:05:19,960
לדוגמה, אם חבר שלי מציע נישואים עייפים, כיצד עלינו לנתח את איכותו?

81
00:05:19,960 --> 00:05:23,855
ובכן, הסיבה שהוא אמר שהוא אוהב את ה-W הלא סביר הזה היא שהוא

82
00:05:23,855 --> 00:05:27,880
אוהב את אופי ה-long shot של כמה טוב זה מרגיש אם תפגע ב-W הזה.

83
00:05:27,880 --> 00:05:31,867
לדוגמה, אם הדפוס הראשון שנחשף היה משהו כזה, אז מסתבר

84
00:05:31,867 --> 00:05:36,080
שיש רק 58 מילים בלקסיקון הענק הזה שתואמות את הדפוס הזה.

85
00:05:36,080 --> 00:05:38,900
אז זה הפחתה עצומה מ-13,000.

86
00:05:38,900 --> 00:05:43,320
אבל הצד השני של זה, כמובן, הוא שזה מאוד נדיר לקבל דפוס כזה.

87
00:05:43,320 --> 00:05:47,280
באופן ספציפי, אם כל מילה הייתה בעלת סבירות שווה להיות

88
00:05:47,280 --> 00:05:51,680
התשובה, ההסתברות לפגיעה בתבנית זו תהיה 58 חלקי בערך 13,000.

89
00:05:51,680 --> 00:05:53,880
כמובן, לא סביר להניח שהם יהיו תשובות.

90
00:05:53,880 --> 00:05:56,680
רוב אלו הן מילים מאוד לא ברורות ואפילו מפוקפקות.

91
00:05:56,680 --> 00:05:59,258
אבל לפחות עבור המעבר הראשון שלנו בכל זה, בואו נניח

92
00:05:59,258 --> 00:06:02,040
שכולן סבירות באותה מידה ואז נחדד את זה קצת מאוחר יותר.

93
00:06:02,040 --> 00:06:07,360
הנקודה היא שהדפוס עם הרבה מידע מעצם טבעו לא סביר שיתרחש.

94
00:06:07,360 --> 00:06:11,320
למעשה, המשמעות של להיות אינפורמטיבי הוא שזה לא סביר.

95
00:06:11,320 --> 00:06:18,360
דפוס הרבה יותר סביר לראות עם הפתיחה הזו יהיה משהו כזה, שבו כמובן אין בו W.

96
00:06:18,360 --> 00:06:22,080
אולי יש E, ואולי אין A, אין R, אין Y.

97
00:06:22,080 --> 00:06:24,640
במקרה זה, יש 1400 התאמות אפשריות.

98
00:06:24,640 --> 00:06:30,680
אם כולם היו סבירים באותה מידה, מסתבר שהסתברות של כ-11% היא זו הדפוס שהיית רואה.

99
00:06:30,680 --> 00:06:34,320
אז התוצאות הסבירות ביותר הן גם הפחות אינפורמטיביות.

100
00:06:34,320 --> 00:06:38,160
כדי לקבל תצוגה גלובלית יותר כאן, הרשו לי להראות לכם את ההתפלגות

101
00:06:38,160 --> 00:06:42,000
המלאה של ההסתברויות על פני כל הדפוסים השונים שאתם עשויים לראות.

102
00:06:42,000 --> 00:06:47,553
אז כל פס שאתה מסתכל עליו מתאים לתבנית אפשרית של צבעים שאפשר לחשוף, מתוכם יש

103
00:06:47,553 --> 00:06:52,960
3 עד 5 אפשרויות, והם מאורגנים משמאל לימין, הנפוצים ביותר עד הפחות נפוצים.

104
00:06:52,960 --> 00:06:56,200
אז האפשרות הנפוצה ביותר כאן היא שאתה מקבל את כל האפורים.

105
00:06:56,200 --> 00:06:58,800
זה קורה בערך 14% מהמקרים.

106
00:06:58,800 --> 00:07:04,516
ומה שאתה מייחל לו כשאתה מנחש זה שאתה בסופו של דבר איפשהו בזנב הארוך הזה,

107
00:07:04,516 --> 00:07:09,920
כמו כאן, שם יש רק 18 אפשרויות למה שתואם את הדפוס הזה שכנראה נראה כך.

108
00:07:09,920 --> 00:07:14,080
או אם נצא קצת יותר שמאלה, אתה יודע, אולי נלך עד לכאן.

109
00:07:14,080 --> 00:07:16,560
אוקיי, הנה חידה טובה בשבילך.

110
00:07:16,560 --> 00:07:22,040
מהן שלוש המילים בשפה האנגלית שמתחילות ב-W, מסתיימות ב-Y, ויש בהן R איפשהו?

111
00:07:22,040 --> 00:07:27,560
מסתבר שהתשובות הן, בוא נראה, מלאות מילים, תולעות ומתפתלות.

112
00:07:27,560 --> 00:07:31,752
אז כדי לשפוט עד כמה המילה הזו טובה בסך הכל, אנחנו רוצים

113
00:07:31,752 --> 00:07:35,720
איזושהי מידה של כמות המידע הצפויה שתקבלו מההפצה הזו.

114
00:07:35,720 --> 00:07:40,696
אם נעבור על כל דפוס ונכפיל את ההסתברות שלו להתרחש פעמים משהו

115
00:07:40,696 --> 00:07:46,000
שמודד כמה הוא אינפורמטיבי, זה יכול אולי לתת לנו ציון אובייקטיבי.

116
00:07:46,000 --> 00:07:50,280
עכשיו האינסטינקט הראשון שלך לגבי מה המשהו הזה צריך להיות עשוי להיות מספר ההתאמות.

117
00:07:50,280 --> 00:07:52,960
אתה רוצה מספר ממוצע נמוך יותר של התאמות.

118
00:07:52,960 --> 00:07:56,649
אבל במקום זאת, הייתי רוצה להשתמש במדידה אוניברסלית יותר שלעתים

119
00:07:56,649 --> 00:08:00,396
קרובות אנו מייחסים למידע, וכזו שתהיה גמישה יותר ברגע שתוקצו לנו

120
00:08:00,396 --> 00:08:04,320
הסתברות שונה לכל אחת מ-13,000 המילים הללו אם הן באמת התשובה או לא.

121
00:08:04,320 --> 00:08:10,619
יחידת המידע הסטנדרטית היא ה-bit, שיש לה נוסחה קצת

122
00:08:10,619 --> 00:08:17,800
מצחיקה, אבל היא ממש אינטואיטיבית אם רק נסתכל על דוגמאות.

123
00:08:17,800 --> 00:08:24,200
אם יש לך תצפית שחותכת את מרחב האפשרויות שלך לחצי, אנו אומרים שיש לה פיסת מידע אחת.

124
00:08:24,200 --> 00:08:27,673
בדוגמה שלנו, מרחב האפשרויות הוא כל המילים האפשריות, ומסתבר

125
00:08:27,673 --> 00:08:31,560
שבערך למחצית ממילות חמש האותיות יש S, קצת פחות מזה, אבל בערך חצי.

126
00:08:31,560 --> 00:08:35,200
אז התצפית הזו תיתן לך קצת מידע.

127
00:08:35,200 --> 00:08:38,666
אם במקום זאת עובדה חדשה מקצצת את מרחב האפשרויות הזה

128
00:08:38,666 --> 00:08:42,000
בפקטור של ארבע, אנו אומרים שיש לה שתי פיסות מידע.

129
00:08:42,000 --> 00:08:45,120
לדוגמה, מסתבר שלכרבע מהמילים הללו יש T.

130
00:08:45,120 --> 00:08:48,138
אם התצפית חותכת את החלל הזה בפקטור של שמונה, אנחנו

131
00:08:48,138 --> 00:08:50,920
אומרים שזה שלוש פיסות מידע, וכן הלאה וכן הלאה.

132
00:08:50,920 --> 00:08:55,000
ארבעה ביטים חותכים אותו ל-16, חמישה ביטים חותכים אותו ל-32.

133
00:08:55,000 --> 00:08:59,946
אז עכשיו אולי תרצו לעצור ולשאול את עצמכם, מהי הנוסחה

134
00:08:59,946 --> 00:09:04,520
למידע עבור מספר הביטים מבחינת ההסתברות להתרחשות?

135
00:09:04,520 --> 00:09:09,616
מה שאנחנו אומרים כאן הוא שכאשר אתה לוקח חצי אחד למספר הסיביות, זה אותו דבר כמו

136
00:09:09,616 --> 00:09:14,712
ההסתברות, שזה אותו דבר כמו לומר שניים בחזקת מספר הסיביות הוא אחד מעל ההסתברות,

137
00:09:14,712 --> 00:09:19,680
אשר מסדר מחדש בהמשך לומר שהמידע הוא בסיס היומן שניים מתוך אחד חלקי ההסתברות.

138
00:09:19,680 --> 00:09:22,555
ולפעמים אתה רואה את זה עם עוד סידור מחדש אחד,

139
00:09:22,555 --> 00:09:25,680
כאשר המידע הוא בסיס היומן השלילי שני של ההסתברות.

140
00:09:25,680 --> 00:09:30,400
בביטוי כך, זה יכול להיראות קצת מוזר למי שלא יודע מה, אבל זה באמת רק

141
00:09:30,400 --> 00:09:35,120
הרעיון האינטואיטיבי של לשאול כמה פעמים צמצמת את האפשרויות שלך בחצי.

142
00:09:35,120 --> 00:09:37,350
עכשיו אם אתה תוהה, אתה יודע, חשבתי שאנחנו סתם

143
00:09:37,350 --> 00:09:39,920
משחקים משחק מילים מהנה, למה לוגריתמים נכנסים לתמונה?

144
00:09:39,920 --> 00:09:44,463
אחת הסיבות לכך שזו יחידה נחמדה יותר היא שפשוט הרבה יותר קל לדבר

145
00:09:44,463 --> 00:09:49,149
על אירועים מאוד לא סבירים, הרבה יותר קל לומר שלתצפית יש 20 סיביות

146
00:09:49,149 --> 00:09:53,480
מידע מאשר לומר שההסתברות להתרחשות כאלה ואחרים היא 0.0000095.

147
00:09:53,480 --> 00:09:57,476
אבל סיבה מהותית יותר לכך שהביטוי הלוגריתמי הזה התברר

148
00:09:57,476 --> 00:10:02,000
כתוספת שימושית מאוד לתורת ההסתברות היא הדרך שבה מידע מתחבר.

149
00:10:02,000 --> 00:10:07,189
לדוגמה, אם תצפית אחת נותנת לך שתי פיסות מידע, מקצצת את השטח שלך בארבע, ואז

150
00:10:07,189 --> 00:10:12,101
תצפית שנייה כמו הניחוש השני שלך ב-Wordle נותנת לך עוד שלוש פיסות מידע,

151
00:10:12,101 --> 00:10:17,360
ומצמצמת אותך עוד יותר בפקטור של שמונה, שניים ביחד נותנים לך חמש פיסות מידע.

152
00:10:17,360 --> 00:10:21,200
באותו אופן שבו הסתברויות אוהבות להכפיל, מידע אוהב להוסיף.

153
00:10:21,200 --> 00:10:24,930
אז ברגע שאנחנו נמצאים בתחום של משהו כמו ערך צפוי, שבו אנחנו מוסיפים

154
00:10:24,930 --> 00:10:28,660
חבורה של מספרים, היומנים הופכים את זה להרבה יותר נחמד להתמודד איתו.

155
00:10:28,660 --> 00:10:35,560
בואו נחזור להפצה שלנו עבור Weary ונוסיף עוד עוקב קטן כאן, שמראה לנו כמה מידע יש לכל דפוס.

156
00:10:35,560 --> 00:10:39,530
הדבר העיקרי שאני רוצה שתשים לב הוא שככל שההסתברות גבוהה יותר כשנגיע

157
00:10:39,530 --> 00:10:43,500
לדפוסים הסבירים האלה, כך המידע נמוך יותר, כך אתה מרוויח פחות ביטים.

158
00:10:43,500 --> 00:10:49,056
הדרך שבה אנו מודדים את האיכות של הניחוש הזה תהיה לקחת את הערך הצפוי של המידע הזה, שם

159
00:10:49,056 --> 00:10:54,940
אנחנו עוברים על כל דפוס, אנחנו אומרים כמה זה סביר, ואז נכפיל את זה בכמה סיביות מידע נקבל.

160
00:10:54,940 --> 00:10:58,480
ובדוגמה של Weary, מסתבר שזה 4.9 ביטים.

161
00:10:58,480 --> 00:11:01,929
אז בממוצע, המידע שאתה מקבל מניחוש הפתיחה הזה טוב

162
00:11:01,929 --> 00:11:05,660
כמו לחתוך את מרחב האפשרויות שלך לחצי בערך חמש פעמים.

163
00:11:05,660 --> 00:11:13,220
לעומת זאת, דוגמה לניחוש עם ערך מידע צפוי גבוה יותר תהיה משהו כמו Slate.

164
00:11:13,220 --> 00:11:16,180
במקרה זה תבחין שההפצה נראית הרבה יותר שטוחה.

165
00:11:16,180 --> 00:11:21,147
בפרט, להתרחשות הסבירה ביותר של כל האפורים יש רק סיכוי של

166
00:11:21,147 --> 00:11:25,940
כ-6% להתרחש, כך שלפחות אתה מקבל כנראה 3.9 סיביות מידע.

167
00:11:25,940 --> 00:11:29,140
אבל זה מינימום, בדרך כלל תקבל משהו יותר טוב מזה.

168
00:11:29,140 --> 00:11:32,705
ומסתבר שכאשר אתה מכתש את המספרים על זה ומחבר את

169
00:11:32,705 --> 00:11:36,420
כל המונחים הרלוונטיים, המידע הממוצע הוא בערך 5.8.

170
00:11:36,420 --> 00:11:43,940
אז בניגוד ל-Weary, מרחב האפשרויות שלך יהיה גדול בערך בחצי לאחר הניחוש הראשון הזה, בממוצע.

171
00:11:43,940 --> 00:11:49,540
למעשה יש סיפור מהנה על השם של הערך הצפוי הזה של כמות מידע.

172
00:11:49,540 --> 00:11:54,358
תורת המידע פותחה על ידי קלוד שאנון, שעבד ב-Bell Labs בשנות ה-40, אבל הוא דיבר

173
00:11:54,358 --> 00:11:59,238
על כמה מהרעיונות שלו שטרם פורסמו עם ג&#39;ון פון נוימן, שהיה הענק האינטלקטואלי

174
00:11:59,238 --> 00:12:04,180
הזה של אותה תקופה, בולט מאוד. במתמטיקה ובפיזיקה ותחילתו של מה שהפך למדעי המחשב.

175
00:12:04,180 --> 00:12:09,214
וכשהזכיר שאין לו שם טוב לערך הצפוי הזה של כמות מידע, כביכול פון

176
00:12:09,214 --> 00:12:14,720
נוימן אמר, אז הסיפור אומר, ובכן כדאי לקרוא לזה אנטרופיה, ומשתי סיבות.

177
00:12:14,720 --> 00:12:20,626
מלכתחילה, פונקציית אי הוודאות שלך שימשה במכניקה סטטיסטית תחת השם הזה, אז כבר יש לה שם,

178
00:12:20,626 --> 00:12:26,396
ובמקום השני, ויותר חשוב, אף אחד לא יודע מהי באמת אנטרופיה, אז בוויכוח אתה תמיד יש את

179
00:12:26,396 --> 00:12:26,940
היתרון.

180
00:12:26,940 --> 00:12:33,420
אז אם השם נראה קצת מסתורי, ואם אפשר להאמין לסיפור הזה, זה סוג של תכנון.

181
00:12:33,420 --> 00:12:39,401
כמו כן, אם אתה תוהה לגבי הקשר שלו לכל החומר השני של החוק השני של התרמודינמיקה מהפיזיקה,

182
00:12:39,401 --> 00:12:45,246
בהחלט יש קשר, אבל במקורותיו שאנון עסק רק בתורת ההסתברות הטהורה, ולמטרותינו כאן, כשאני

183
00:12:45,246 --> 00:12:50,820
משתמש ב- אנטרופיה של מילים, אני רק רוצה שתחשוב על ערך המידע הצפוי של ניחוש מסוים.

184
00:12:50,820 --> 00:12:54,380
אתה יכול לחשוב על אנטרופיה כמדידת שני דברים בו זמנית.

185
00:12:54,380 --> 00:12:57,420
הראשון הוא עד כמה שטוחה ההתפלגות.

186
00:12:57,420 --> 00:13:01,700
ככל שהתפלגות קרובה יותר לאחידות, כך האנטרופיה תהיה גבוהה יותר.

187
00:13:01,700 --> 00:13:07,114
במקרה שלנו, כאשר יש 3 עד ה-5 דפוסים הכוללים, עבור התפלגות אחידה,

188
00:13:07,114 --> 00:13:12,612
צפייה בכל אחד מהם תהיה בסיס יומן מידע 2 מתוך 3 עד ה-5, שהוא במקרה

189
00:13:12,612 --> 00:13:17,860
7.92, אז זה המקסימום המוחלט שיכול להיות לך עבור האנטרופיה הזו.

190
00:13:17,860 --> 00:13:22,900
אבל אנטרופיה היא גם סוג של מדד לכמה אפשרויות יש מלכתחילה.

191
00:13:22,900 --> 00:13:27,757
לדוגמה, אם במקרה יש לך מילה כלשהי שבה יש רק 16 דפוסים אפשריים, וכל

192
00:13:27,757 --> 00:13:32,760
אחת מהן בסבירות שווה, האנטרופיה הזו, המידע הצפוי הזה, תהיה 4 סיביות.

193
00:13:32,760 --> 00:13:37,012
אבל אם יש לך מילה אחרת שבה יש 64 דפוסים אפשריים שיכולים להופיע,

194
00:13:37,012 --> 00:13:41,000
וכולם סבירים באותה מידה, אז האנטרופיה תסתבר להיות 6 סיביות.

195
00:13:41,000 --> 00:13:47,658
אז אם אתה רואה איזושהי התפלגות בטבע שיש לה אנטרופיה של 6 ביטים, זה בערך כאילו זה

196
00:13:47,658 --> 00:13:54,400
אומר שיש שונות וחוסר ודאות במה שעומד לקרות כאילו היו 64 תוצאות סבירות באותה מידה.

197
00:13:54,400 --> 00:13:58,360
עבור המעבר הראשון שלי ב-Wurtelebot, בעצם הייתי צריך לעשות את זה.

198
00:13:58,360 --> 00:14:04,453
הוא עובר על כל הניחושים האפשריים שיכולים להיות לך, כל 13,000 המילים, מחשב את האנטרופיה

199
00:14:04,453 --> 00:14:10,546
עבור כל אחת, או ליתר דיוק, את האנטרופיה של ההתפלגות על פני כל הדפוסים שאתה עשוי לראות,

200
00:14:10,546 --> 00:14:16,709
עבור כל אחת, ובוחר את הגבוהה ביותר, מכיוון שזהו זה שצפוי לקצץ את מרחב האפשרויות שלך ככל

201
00:14:16,709 --> 00:14:17,200
האפשר.

202
00:14:17,200 --> 00:14:21,680
ולמרות שדיברתי רק על הניחוש הראשון כאן, זה עושה את אותו הדבר עבור הניחושים הבאים.

203
00:14:21,680 --> 00:14:26,695
לדוגמה, לאחר שתראה דפוס כלשהו בניחוש הראשון הזה, שיגביל אותך למספר קטן יותר של מילים

204
00:14:26,695 --> 00:14:31,887
אפשריות בהתבסס על מה שמתאים לזה, אתה פשוט משחק באותו משחק ביחס לאותה קבוצה קטנה יותר של

205
00:14:31,887 --> 00:14:32,300
מילים.

206
00:14:32,300 --> 00:14:39,005
לניחוש שני מוצע, אתה מסתכל על ההתפלגות של כל הדפוסים שיכולים להתרחש מאותה קבוצה מוגבלת

207
00:14:39,005 --> 00:14:45,480
יותר של מילים, אתה מחפש בכל 13,000 האפשרויות, ומוצא את זו שממקסמת את האנטרופיה הזו.

208
00:14:45,480 --> 00:14:49,627
כדי להראות לכם איך זה עובד בפעולה, הרשו לי רק להעלות גרסה

209
00:14:49,627 --> 00:14:54,060
קטנה של Wurtele שכתבתי שמראה את הדגשים של הניתוח הזה בשוליים.

210
00:14:54,060 --> 00:14:57,264
לאחר ביצוע כל חישובי האנטרופיה שלו, מימין כאן הוא

211
00:14:57,264 --> 00:15:00,340
מראה לנו למי מהם יש את המידע הצפוי הגבוה ביותר.

212
00:15:00,340 --> 00:15:05,543
מסתבר שהתשובה העליונה, לפחות כרגע, נחדד את זה בהמשך,

213
00:15:05,543 --> 00:15:11,140
היא טארס, שפירושו, אממ, כמובן, בקיה, הקיבה הנפוצה ביותר.

214
00:15:11,140 --> 00:15:15,551
בכל פעם שאנחנו מנחשים כאן, איפה אולי אני קצת מתעלם מההמלצות שלו והולך עם

215
00:15:15,551 --> 00:15:20,145
צפחה, כי אני אוהב צפחה, אנחנו יכולים לראות כמה מידע צפוי היה לו, אבל אז בצד

216
00:15:20,145 --> 00:15:24,980
ימין של המילה כאן זה מראה לנו כמה מידע אמיתי שקיבלנו, בהתחשב בדפוס הספציפי הזה.

217
00:15:24,980 --> 00:15:27,876
אז כאן זה נראה כאילו היה לנו קצת חסר מזל, היה צפוי

218
00:15:27,876 --> 00:15:30,660
לנו לקבל 5.8, אבל במקרה קיבלנו משהו עם פחות מזה.

219
00:15:30,660 --> 00:15:35,860
ואז בצד שמאל כאן זה מראה לכולנו את המילים האפשריות השונות שבהן אנחנו נמצאים עכשיו.

220
00:15:35,860 --> 00:15:40,000
הפסים הכחולים אומרים לנו עד כמה הוא חושב שכל מילה היא, אז כרגע זה

221
00:15:40,000 --> 00:15:44,140
מניח שכל מילה בסבירות שווה להתרחש, אבל אנחנו נחדד את זה בעוד רגע.

222
00:15:44,140 --> 00:15:50,212
ואז מדידת אי הוודאות הזו אומרת לנו את האנטרופיה של ההתפלגות הזו על פני המילים האפשריות,

223
00:15:50,212 --> 00:15:55,940
שכרגע, מכיוון שזו התפלגות אחידה, היא רק דרך מסובכת מיותרת לספור את מספר האפשרויות.

224
00:15:55,940 --> 00:16:02,700
לדוגמה, אם היינו לוקחים 2 בחזקת 13.66, זה אמור להיות בסביבות 13,000 האפשרויות.

225
00:16:02,700 --> 00:16:06,780
אני קצת בחוץ כאן, אבל רק בגלל שאני לא מראה את כל האותיות העשרוניות.

226
00:16:06,780 --> 00:16:09,834
כרגע זה עשוי להרגיש מיותר וכאילו זה מסבך דברים מדי, אבל

227
00:16:09,834 --> 00:16:12,780
אתה תראה למה זה שימושי להחזיק את שני המספרים תוך דקה.

228
00:16:12,780 --> 00:16:16,089
אז כאן זה נראה כאילו זה מרמז על האנטרופיה הגבוהה ביותר

229
00:16:16,089 --> 00:16:19,700
עבור הניחוש השני שלנו הוא ראמן, ששוב ממש לא מרגיש כמו מילה.

230
00:16:19,700 --> 00:16:25,660
אז כדי לקחת את הרמה המוסרית כאן, אני מתכוון להמשיך ולהקליד את Rains.

231
00:16:25,660 --> 00:16:27,540
ושוב זה נראה כאילו היה לנו קצת חסר מזל.

232
00:16:27,540 --> 00:16:32,100
ציפינו ל-4.3 ביטים וקיבלנו רק 3.39 סיביות מידע.

233
00:16:32,100 --> 00:16:35,060
אז זה מוריד אותנו ל-55 אפשרויות.

234
00:16:35,060 --> 00:16:40,200
וכאן אולי אני פשוט אלך עם מה שזה מציע, שזה שילוב, מה שזה לא אומר.

235
00:16:40,200 --> 00:16:43,300
ובסדר, זו בעצם הזדמנות טובה לפאזל.

236
00:16:43,300 --> 00:16:47,020
זה אומר לנו שהדפוס הזה נותן לנו 4.7 סיביות מידע.

237
00:16:47,020 --> 00:16:52,400
אבל בצד שמאל, לפני שאנחנו רואים את הדפוס הזה, היו 5.78 פיסות של אי ודאות.

238
00:16:52,400 --> 00:16:56,860
אז בתור חידון בשבילך, מה זה אומר לגבי מספר האפשרויות שנותרו?

239
00:16:56,860 --> 00:17:01,020
ובכן, זה אומר שאנחנו מצטמצמים לחלק אחד של אי ודאות,

240
00:17:01,020 --> 00:17:04,700
וזה אותו דבר כמו לומר שיש שתי תשובות אפשריות.

241
00:17:04,700 --> 00:17:06,520
זו בחירה של 50-50.

242
00:17:06,520 --> 00:17:08,921
ומכאן, בגלל שאתה ואני יודעים אילו מילים נפוצות

243
00:17:08,921 --> 00:17:11,220
יותר, אנחנו יודעים שהתשובה צריכה להיות תהום.

244
00:17:11,220 --> 00:17:13,540
אבל כמו שזה נכתב עכשיו, התוכנית לא יודעת את זה.

245
00:17:13,540 --> 00:17:20,360
אז זה פשוט ממשיך, מנסה להשיג כמה שיותר מידע, עד שנותרה רק אפשרות אחת, ואז הוא מנחש את זה.

246
00:17:20,360 --> 00:17:22,700
אז ברור שאנחנו צריכים אסטרטגיית סוף משחק טובה יותר.

247
00:17:22,700 --> 00:17:26,551
אבל נניח שאנחנו קוראים לגרסה הזו אחת מפותרי המילים שלנו,

248
00:17:26,551 --> 00:17:30,740
ואז אנחנו הולכים ומריצים כמה סימולציות כדי לראות איך זה עובד.

249
00:17:30,740 --> 00:17:34,240
אז הדרך שבה זה עובד היא שהוא משחק בכל משחק מילים אפשרי.

250
00:17:34,240 --> 00:17:38,780
זה עובר על כל 2315 המילים האלה שהן התשובות המילוליות בפועל.

251
00:17:38,780 --> 00:17:41,340
זה בעצם משתמש בזה כמערכת בדיקות.

252
00:17:41,340 --> 00:17:46,019
ועם השיטה הנאיבית הזו של לא להתחשב עד כמה מילה נפוצה, ורק לנסות

253
00:17:46,019 --> 00:17:50,480
למקסם את המידע בכל שלב בדרך, עד שהוא מגיע לבחירה אחת ויחידה.

254
00:17:50,480 --> 00:17:55,100
בסוף הסימולציה, הציון הממוצע מתברר כ-4.124.

255
00:17:55,100 --> 00:17:59,780
וזה לא רע, למען האמת, די ציפיתי לעשות יותר גרוע.

256
00:17:59,780 --> 00:18:03,040
אבל האנשים שמנגנים wordle יגידו לך שבדרך כלל הם יכולים להשיג את זה ב-4.

257
00:18:03,040 --> 00:18:05,260
האתגר האמיתי הוא להשיג כמה שיותר ב-3.

258
00:18:05,260 --> 00:18:08,920
זה קפיצה די גדולה בין הציון 4 לציון 3.

259
00:18:08,920 --> 00:18:23,160
הפרי התלוי הנמוך כאן הוא איכשהו לשלב האם מילה נפוצה או לא, ואיך בדיוק אנחנו עושים את זה.

260
00:18:23,160 --> 00:18:28,560
הדרך שבה ניגשתי היא לקבל רשימה של התדרים היחסיים של כל המילים בשפה האנגלית.

261
00:18:28,560 --> 00:18:31,874
והרגע השתמשתי בפונקציית נתוני תדירות המילים של Mathematica,

262
00:18:31,874 --> 00:18:35,520
שבעצמה שואבת ממאגר הנתונים הציבורי של Google Books English Ngram.

263
00:18:35,520 --> 00:18:40,120
וזה די כיף להסתכל עליו, למשל אם נמיין את זה מהמילים הנפוצות ביותר למילים הפחות נפוצות.

264
00:18:40,120 --> 00:18:43,740
ברור שאלו הן המילים הנפוצות ביותר, 5 אותיות בשפה האנגלית.

265
00:18:43,740 --> 00:18:46,480
או ליתר דיוק, אלו הם ה-8 הנפוצים ביותר.

266
00:18:46,480 --> 00:18:49,440
ראשון זה איזה, אחרי זה יש שם ושם.

267
00:18:49,440 --> 00:18:54,161
הראשון עצמו הוא לא הראשון, אלא התשיעי, והגיוני שהמילים האחרות הללו יכולות להופיע

268
00:18:54,161 --> 00:18:59,000
לעתים קרובות יותר, כאשר אלה שאחרי הראשונים הם אחרי, איפה, ואלו רק קצת פחות נפוצות.

269
00:18:59,000 --> 00:19:02,967
כעת, בשימוש בנתונים האלה כדי להדגים את הסבירות שכל אחת מהמילים הללו

270
00:19:02,967 --> 00:19:06,760
תהיה התשובה הסופית, היא לא צריכה להיות רק פרופורציונלית לתדירות.

271
00:19:06,760 --> 00:19:11,249
למשל, שניתן לו ציון 0.002 במערך הנתונים הזה, בעוד

272
00:19:11,249 --> 00:19:15,200
שהמילה צמה היא בסבירות מסוימת פי 1000 פחות.

273
00:19:15,200 --> 00:19:19,400
אבל שתיהן מילים מספיק נפוצות שכדאי לשקול אותן.

274
00:19:19,400 --> 00:19:21,900
אז אנחנו רוצים יותר חתך בינארי.

275
00:19:21,900 --> 00:19:27,320
הדרך שבה הלכתי היא לדמיין לקחת את כל הרשימה הממוינת הזו של מילים, ואז לסדר אותה

276
00:19:27,320 --> 00:19:32,605
על ציר x, ואז להחיל את הפונקציה הסיגמואידית, שהיא הדרך הסטנדרטית לקבל פונקציה

277
00:19:32,605 --> 00:19:38,500
שהפלט שלה הוא בעצם בינארי, זה או 0 או שזה 1, אבל יש החלקה ביניהם לאזור זה של אי ודאות.

278
00:19:38,500 --> 00:19:44,182
אז בעצם, ההסתברות שאני מקצה לכל מילה להימצאות ברשימה הסופית תהיה הערך

279
00:19:44,182 --> 00:19:49,540
של הפונקציה הסיגמואידית למעלה בכל מקום שבו היא ממוקמת על ציר ה-x.

280
00:19:49,540 --> 00:19:54,026
עכשיו ברור שזה תלוי בכמה פרמטרים, למשל כמה רחב הרווח בציר ה-x

281
00:19:54,026 --> 00:19:58,368
ממלאות המילים האלה קובע באיזו הדרגתית או תלולה אנחנו יורדים

282
00:19:58,368 --> 00:20:03,000
מ-1 ל-0, והמקום שבו אנחנו ממקמים אותן משמאל לימין קובע את החתך.

283
00:20:03,000 --> 00:20:07,340
למען האמת, הדרך שעשיתי את זה הייתה פשוט ללקק את האצבע שלי ולהכניס אותה לרוח.

284
00:20:07,340 --> 00:20:12,373
עיינתי ברשימה הממוינת וניסיתי למצוא חלון שבו כשהסתכלתי עליו הבנתי שכמחצית

285
00:20:12,373 --> 00:20:17,680
מהמילים האלה צפויות יותר מאשר לא להיות התשובה הסופית, והשתמשתי בזה בתור החתך.

286
00:20:17,680 --> 00:20:20,969
ברגע שיש לנו התפלגות כזו בין המילים, זה נותן לנו

287
00:20:20,969 --> 00:20:24,460
מצב נוסף שבו האנטרופיה הופכת למדידה ממש שימושית זו.

288
00:20:24,460 --> 00:20:29,075
לדוגמה, נניח ששיחקנו משחק ונתחיל עם הפותחים הישנים שלי, שהיו נוצה

289
00:20:29,075 --> 00:20:33,760
ומסמרים, ובסופו של דבר יש מצב שיש ארבע מילים אפשריות שמתאימות לזה.

290
00:20:33,760 --> 00:20:36,440
ונניח שאנו רואים בכולם סבירים באותה מידה.

291
00:20:36,440 --> 00:20:40,000
הרשו לי לשאול אתכם, מהי האנטרופיה של התפלגות זו?

292
00:20:40,000 --> 00:20:45,348
ובכן, המידע הקשור לכל אחת מהאפשרויות הללו יהיה בסיס

293
00:20:45,348 --> 00:20:50,800
היומן 2 מתוך 4, מכיוון שכל אחד מהם הוא 1 ו-4, וזה 2.

294
00:20:50,800 --> 00:20:52,780
שתי פיסות מידע, ארבע אפשרויות.

295
00:20:52,780 --> 00:20:54,360
הכל טוב מאוד וטוב.

296
00:20:54,360 --> 00:20:58,320
אבל מה אם אגיד לך שבעצם יש יותר מארבע גפרורים?

297
00:20:58,320 --> 00:21:02,600
במציאות, כשאנחנו מסתכלים ברשימת המילים המלאה, יש 16 מילים שמתאימות לה.

298
00:21:02,600 --> 00:21:06,986
אבל נניח שהמודל שלנו שם סבירות ממש נמוכה ל-12 המילים האחרות האלה

299
00:21:06,986 --> 00:21:11,440
להיות למעשה התשובה הסופית, משהו כמו 1 ל-1000 כי הן ממש לא ברורות.

300
00:21:11,440 --> 00:21:15,480
עכשיו הרשו לי לשאול אתכם, מהי האנטרופיה של התפלגות זו?

301
00:21:15,480 --> 00:21:20,738
אם האנטרופיה מדדה אך ורק את מספר ההתאמות כאן, אז אתה עשוי לצפות שזה יהיה משהו

302
00:21:20,738 --> 00:21:26,200
כמו בסיס היומן 2 מתוך 16, שיהיה 4, שתי פיסות יותר של אי ודאות ממה שהיה לנו קודם.

303
00:21:26,200 --> 00:21:30,320
אבל כמובן שאי הוודאות בפועל לא באמת שונה ממה שהיה לנו קודם.

304
00:21:30,320 --> 00:21:34,222
רק בגלל שיש את 12 המילים המעורפלות האלה לא אומר שזה

305
00:21:34,222 --> 00:21:38,200
יהיה כל כך מפתיע ללמוד שהתשובה הסופית היא קסם, למשל.

306
00:21:38,200 --> 00:21:41,983
אז כאשר אתה באמת עושה את החישוב כאן, ואתה מחבר את ההסתברות

307
00:21:41,983 --> 00:21:45,960
של כל התרחשות כפול המידע המתאים, מה שאתה מקבל הוא 2.11 ביטים.

308
00:21:45,960 --> 00:21:51,642
אני רק אומר, זה בעצם שני ביטים, בעצם ארבע האפשרויות האלה, אבל יש קצת יותר אי ודאות

309
00:21:51,642 --> 00:21:57,120
בגלל כל האירועים הבלתי סבירים האלה, אם כי אם תלמד אותם היית מקבל מזה המון מידע.

310
00:21:57,120 --> 00:22:01,800
אז בהתקרבות, זה חלק ממה שהופך את Wordle לדוגמא כל כך נחמדה לשיעור תורת מידע.

311
00:22:01,800 --> 00:22:05,280
יש לנו את שני יישומי ההרגשה המובהקים הללו עבור אנטרופיה.

312
00:22:05,280 --> 00:22:10,926
הראשון אומר לנו מה המידע הצפוי שנקבל מניחוש נתון, והשני אומר

313
00:22:10,926 --> 00:22:16,480
האם נוכל למדוד את אי הוודאות שנותרה בין כל המילים האפשריות.

314
00:22:16,480 --> 00:22:20,616
ואני צריך להדגיש, במקרה הראשון שבו אנחנו מסתכלים על המידע הצפוי של

315
00:22:20,616 --> 00:22:25,000
ניחוש, ברגע שיש לנו שקלול לא שווה למילים, זה משפיע על חישוב האנטרופיה.

316
00:22:25,000 --> 00:22:29,780
לדוגמה, הרשו לי להעלות את אותו מקרה שבדקנו קודם לכן של התפלגות הקשורה

317
00:22:29,780 --> 00:22:34,560
ל-Weary, אבל הפעם באמצעות התפלגות לא אחידה על פני כל המילים האפשריות.

318
00:22:34,560 --> 00:22:39,360
אז תן לי לראות אם אני יכול למצוא כאן חלק שממחיש את זה די טוב.

319
00:22:39,360 --> 00:22:42,480
אוקיי, כאן זה די טוב.

320
00:22:42,480 --> 00:22:45,980
כאן יש לנו שני דפוסים צמודים שסבירים בערך באותה מידה,

321
00:22:45,980 --> 00:22:49,480
אבל לאחד מהם נאמר לנו יש 32 מילים אפשריות שתואמות לה.

322
00:22:49,480 --> 00:22:52,443
ואם נבדוק מה הם, אלה 32 אלה, שכולן פשוט מילים

323
00:22:52,443 --> 00:22:55,600
מאוד לא סבירות כשאתה סורק את העיניים שלך מעליהן.

324
00:22:55,600 --> 00:23:00,297
קשה למצוא כאלה שמרגישות כמו תשובות סבירות, אולי צעקות, אבל אם

325
00:23:00,297 --> 00:23:04,919
נסתכל על התבנית השכנה בהתפלגות, שנחשבת כסבירה בערך, נאמר לנו

326
00:23:04,919 --> 00:23:09,920
שיש לה רק 8 התאמות אפשריות, אז רבע התאמות רבות, אבל זה סביר בערך.

327
00:23:09,920 --> 00:23:12,520
וכשאנחנו מוציאים את הגפרורים האלה, אנחנו יכולים לראות למה.

328
00:23:12,520 --> 00:23:17,840
חלק מהן תשובות סבירות ממש, כמו צלצול, או זעם, או ראפ.

329
00:23:17,840 --> 00:23:21,900
כדי להמחיש כיצד אנו משלבים את כל זה, הרשו לי להעלות כאן את גרסה 2

330
00:23:21,900 --> 00:23:25,960
של ה-Wordlebot, ויש שניים או שלושה הבדלים עיקריים מהראשון שראינו.

331
00:23:25,960 --> 00:23:30,172
ראשית, כמו שאמרתי זה עתה, הדרך שבה אנו מחשבים את האנטרופיות

332
00:23:30,172 --> 00:23:34,736
הללו, הערכים הצפויים של המידע, משתמשת כעת בהתפלגות המעודנות יותר

333
00:23:34,736 --> 00:23:39,300
על פני הדפוסים שמשלבת את ההסתברות שמילה נתונה תהיה למעשה התשובה.

334
00:23:39,300 --> 00:23:44,160
כפי שזה קורה, הדמעות עדיין מספר 1, אם כי אלה שאחריו קצת שונים.

335
00:23:44,160 --> 00:23:47,845
שנית, כאשר היא תדרג את הבחירות המובילות שלה, היא עתידה לשמור

336
00:23:47,845 --> 00:23:51,471
מודל של ההסתברות שכל מילה היא התשובה האמיתית, והיא תשלב זאת

337
00:23:51,471 --> 00:23:55,520
בהחלטה שלה, שקל יותר לראות אותה ברגע שיש לנו כמה ניחושים על שולחן.

338
00:23:55,520 --> 00:24:01,120
שוב, מתעלמים מהמלצתו כי אנחנו לא יכולים לתת למכונות לשלוט בחיינו.

339
00:24:01,120 --> 00:24:05,746
ואני מניח שעלי להזכיר עוד דבר שונה כאן משמאל, שערך אי הוודאות,

340
00:24:05,746 --> 00:24:10,080
מספר הביטים הזה, כבר לא רק מיותר עם מספר ההתאמות האפשריות.

341
00:24:10,080 --> 00:24:16,731
עכשיו אם נמשוך אותו למעלה ונחשב 2 ל-8.02, שזה קצת מעל 256, אני מניח 259,

342
00:24:16,731 --> 00:24:23,200
מה שזה אומר זה למרות שיש 526 מילים בסך הכל שמתאימות לדפוס הזה, כמות אי

343
00:24:23,200 --> 00:24:29,760
הוודאות שיש לה דומה יותר למה שהיא הייתה אם היו 259 בסבירות שווה תוצאות.

344
00:24:29,760 --> 00:24:31,100
אתה יכול לחשוב על זה ככה.

345
00:24:31,100 --> 00:24:34,533
הוא יודע שבורקס הוא לא התשובה, אותו דבר עם יורט וזורל

346
00:24:34,533 --> 00:24:37,840
וזרוס, אז זה קצת פחות לא ודאי ממה שהיה במקרה הקודם.

347
00:24:37,840 --> 00:24:40,220
מספר ביטים זה יהיה קטן יותר.

348
00:24:40,220 --> 00:24:44,319
ואם אני ממשיך לשחק את המשחק, אני מחדד את זה עם

349
00:24:44,319 --> 00:24:48,680
כמה ניחושים שהם בהתאם למה שהייתי רוצה להסביר כאן.

350
00:24:48,680 --> 00:24:51,188
לפי הניחוש הרביעי, אם תסתכלו על הבחירות המובילות

351
00:24:51,188 --> 00:24:53,800
שלה, תוכלו לראות שזה כבר לא רק ממקסם את האנטרופיה.

352
00:24:53,800 --> 00:25:00,780
אז בשלב זה, מבחינה טכנית יש שבע אפשרויות, אבל היחידות עם סיכוי משמעותי הן מעונות ומילים.

353
00:25:00,780 --> 00:25:04,430
ואתה יכול לראות את הבחירה בשני אלה מעל כל הערכים

354
00:25:04,430 --> 00:25:07,560
האחרים האלה, שבאופן קפדני ייתן יותר מידע.

355
00:25:07,560 --> 00:25:10,885
בפעם הראשונה שעשיתי את זה, פשוט הוספתי את שני המספרים האלה כדי

356
00:25:10,885 --> 00:25:14,580
למדוד את האיכות של כל ניחוש, שלמעשה עבד טוב יותר ממה שאתה עשוי לחשוד.

357
00:25:14,580 --> 00:25:17,425
אבל זה ממש לא הרגיש שיטתי, ואני בטוח שיש עוד גישות

358
00:25:17,425 --> 00:25:19,880
שאנשים יכולים לנקוט אבל הנה זו שנחתתי עליה.

359
00:25:19,880 --> 00:25:24,230
אם אנחנו שוקלים את הסיכוי לניחוש הבא, כמו במקרה הזה מילים, מה

360
00:25:24,230 --> 00:25:28,440
שבאמת אכפת לנו הוא הציון הצפוי של המשחק שלנו אם נעשה את זה.

361
00:25:28,440 --> 00:25:32,284
וכדי לחשב את הציון הצפוי הזה, אנחנו אומרים מה ההסתברות

362
00:25:32,284 --> 00:25:35,640
שמילים הן התשובה בפועל, שכרגע היא מתארת לה 58%.

363
00:25:35,640 --> 00:25:40,400
אנחנו אומרים שעם סיכוי של 58%, הציון שלנו במשחק הזה יהיה 4.

364
00:25:40,400 --> 00:25:46,240
ואז עם הסתברות של 1 פחות 58%, הציון שלנו יהיה יותר מ-4.

365
00:25:46,240 --> 00:25:49,580
כמה עוד אנחנו לא יודעים, אבל אנחנו יכולים להעריך את זה

366
00:25:49,580 --> 00:25:52,920
על סמך כמה אי ודאות צפויה להיות ברגע שנגיע לנקודה הזו.

367
00:25:52,920 --> 00:25:56,600
ספציפית, כרגע יש 1.44 פיסות של אי ודאות.

368
00:25:56,600 --> 00:26:01,560
אם ננחש מילים, זה אומר לנו שהמידע הצפוי שנקבל הוא 1.27 ביטים.

369
00:26:01,560 --> 00:26:08,280
אז אם ננחש מילים, ההבדל הזה מייצג כמה חוסר ודאות סביר שנישאר עם אחרי שזה יקרה.

370
00:26:08,280 --> 00:26:11,080
מה שאנחנו צריכים זה איזושהי פונקציה, שאני קורא

371
00:26:11,080 --> 00:26:13,880
לה כאן, שקושרת את אי הוודאות הזו עם ציון צפוי.

372
00:26:13,880 --> 00:26:18,102
והדרך שבה זה התנהל הייתה פשוט לשרטט חבורה של נתונים ממשחקים

373
00:26:18,102 --> 00:26:22,324
קודמים המבוססים על גרסה 1 של הבוט כדי לומר היי מה היה הציון

374
00:26:22,324 --> 00:26:27,040
בפועל אחרי נקודות שונות עם כמויות מסוימות מאוד מדידות של אי ודאות.

375
00:26:27,040 --> 00:26:32,880
לדוגמה, נקודות הנתונים האלה כאן שנמצאות מעל ערך שהוא בערך כמו 8.7 בערך אומרים על כמה

376
00:26:32,880 --> 00:26:38,790
משחקים אחרי נקודה שבה היו 8.7 פיסות של אי ודאות, נדרשו שני ניחושים כדי לקבל את התשובה

377
00:26:38,790 --> 00:26:39,340
הסופית.

378
00:26:39,340 --> 00:26:43,180
למשחקים אחרים נדרשו שלושה ניחושים, למשחקים אחרים נדרשו ארבעה ניחושים.

379
00:26:43,180 --> 00:26:48,826
אם נעבור שמאלה כאן, כל הנקודות מעל האפס אומרות בכל פעם שיש אפס פיסות של אי

380
00:26:48,826 --> 00:26:55,000
ודאות, כלומר יש רק אפשרות אחת, אז מספר הניחושים הנדרש הוא תמיד רק אחד, וזה מרגיע.

381
00:26:55,000 --> 00:26:59,435
בכל פעם שהייתה קצת אי ודאות, כלומר זה היה בעצם רק בשתי אפשרויות,

382
00:26:59,435 --> 00:27:03,940
אז לפעמים זה דרש עוד ניחוש אחד, לפעמים זה דרש שני ניחושים נוספים.

383
00:27:03,940 --> 00:27:05,980
וכן הלאה וכן הלאה כאן.

384
00:27:05,980 --> 00:27:11,020
אולי דרך קצת יותר קלה לדמיין את הנתונים האלה היא לרכז אותם יחד ולקחת ממוצעים.

385
00:27:11,020 --> 00:27:16,818
לדוגמה, הסרגל הזה אומר שבין כל הנקודות שבהן היה לנו קצת אי

386
00:27:16,818 --> 00:27:22,420
ודאות, בממוצע מספר הניחושים החדשים הנדרשים היה בערך 1.5.

387
00:27:22,420 --> 00:27:27,170
והסרגל כאן אומר בין כל המשחקים השונים שבו בשלב מסוים חוסר הוודאות

388
00:27:27,170 --> 00:27:31,777
היה קצת מעל ארבע ביטים, שזה כמו לצמצם אותו ל-16 אפשרויות שונות,

389
00:27:31,777 --> 00:27:36,240
אז בממוצע זה דורש קצת יותר משני ניחושים מאותה נקודה קָדִימָה.

390
00:27:36,240 --> 00:27:40,080
ומכאן פשוט עשיתי רגרסיה כדי להתאים לפונקציה שנראתה סבירה לזה.

391
00:27:40,080 --> 00:27:44,975
ותזכרו שכל העניין בעשיית כל זה הוא כדי שנוכל לכמת את האינטואיציה

392
00:27:44,975 --> 00:27:49,720
הזו שככל שנשיג יותר מידע ממילה, כך הציון הצפוי יהיה נמוך יותר.

393
00:27:49,720 --> 00:27:55,052
אז עם זה כגרסה 2.0, אם נחזור אחורה ונריץ את אותה סט של סימולציות,

394
00:27:55,052 --> 00:27:59,820
כשזה ישחק מול כל 2315 תשובות המילה האפשריות, איך זה מסתדר?

395
00:27:59,820 --> 00:28:04,060
ובכן בניגוד לגרסה הראשונה שלנו זה בהחלט טוב יותר, וזה מרגיע.

396
00:28:04,060 --> 00:28:08,217
הכל אמר ועשה הממוצע הוא בסביבות 3.6, אם כי בניגוד לגרסה

397
00:28:08,217 --> 00:28:12,820
הראשונה יש כמה פעמים שהיא מפסידה ודורשת יותר משש בנסיבות אלה.

398
00:28:12,820 --> 00:28:18,980
ככל הנראה בגלל שיש מקרים שבהם זה עושה את הפשרה הזו ללכת על המטרה במקום למקסם את המידע.

399
00:28:18,980 --> 00:28:22,140
אז אנחנו יכולים לעשות יותר טוב מ-3.6?

400
00:28:22,140 --> 00:28:23,260
אנחנו בהחלט יכולים.

401
00:28:23,260 --> 00:28:26,408
עכשיו אמרתי בהתחלה שהכי כיף לנסות לא לשלב את הרשימה

402
00:28:26,408 --> 00:28:29,980
האמיתית של תשובות מילוליות בדרך שבה היא בונה את המודל שלה.

403
00:28:29,980 --> 00:28:35,180
אבל אם נשלב את זה, הביצועים הטובים ביותר שיכולתי לקבל היו בסביבות 3.43.

404
00:28:35,180 --> 00:28:38,926
אז אם ננסה להיות מתוחכמים יותר מסתם שימוש בנתוני תדירות מילים

405
00:28:38,926 --> 00:28:42,673
כדי לבחור את ההפצה הקודמת הזו, זה 3.43 כנראה נותן מקסימום כמה

406
00:28:42,673 --> 00:28:46,360
טוב יכולנו להגיע עם זה, או לפחות כמה טוב יכולתי להגיע עם זה.

407
00:28:46,360 --> 00:28:51,010
הביצועים הטובים ביותר האלה בעצם רק משתמשים ברעיונות שדיברתי עליהם כאן, אבל זה

408
00:28:51,010 --> 00:28:55,660
הולך קצת יותר רחוק, כאילו הוא מחפש את המידע הצפוי שני צעדים קדימה ולא רק אחד.

409
00:28:55,660 --> 00:29:00,580
במקור תכננתי לדבר על זה יותר, אבל אני מבין שלמעשה עברנו די הרבה זמן כמו שזה.

410
00:29:00,580 --> 00:29:04,960
הדבר היחיד שאני אגיד הוא לאחר ביצוע חיפוש דו-שלבי זה ולאחר מכן הפעלת כמה סימולציות

411
00:29:04,960 --> 00:29:09,500
לדוגמא במועמדים המובילים, עד כה עבורי לפחות זה נראה כאילו קריין הוא הפותח הטוב ביותר.

412
00:29:09,500 --> 00:29:11,080
מי היה מנחש?

413
00:29:11,080 --> 00:29:14,265
כמו כן, אם אתה משתמש ברשימת המילים האמיתית כדי לקבוע את מרחב

414
00:29:14,265 --> 00:29:17,920
האפשרויות שלך, אז אי הוודאות שאתה מתחיל איתה היא קצת יותר מ-11 ביטים.

415
00:29:17,920 --> 00:29:21,955
ומסתבר, רק מחיפוש כוח גס, המידע המקסימלי האפשרי

416
00:29:21,955 --> 00:29:26,580
הצפוי לאחר שני הניחושים הראשונים הוא בסביבות 10 ביטים.

417
00:29:26,580 --> 00:29:30,900
מה שמרמז על התרחיש הטוב ביותר, לאחר שני הניחושים הראשונים

418
00:29:30,900 --> 00:29:35,220
שלך, עם משחק אופטימלי לחלוטין, תישאר עם קצת אי ודאות אחת.

419
00:29:35,220 --> 00:29:37,400
וזה אותו דבר כמו לרדת לשני ניחושים אפשריים.

420
00:29:37,400 --> 00:29:42,781
אז אני חושב שזה הוגן וכנראה די שמרני לומר שלעולם לא תוכל לכתוב אלגוריתם שמקבל

421
00:29:42,781 --> 00:29:48,369
את הממוצע הזה נמוך כמו 3, כי עם המילים הזמינות לך, פשוט אין מקום לקבל מספיק מידע

422
00:29:48,369 --> 00:29:53,820
לאחר שני שלבים בלבד. מסוגל להבטיח את התשובה במשבצת השלישית בכל פעם בלי להיכשל.

