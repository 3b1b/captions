1
00:00:00,000 --> 00:00:03,328
המשחק Wurdle הפך די ויראלי בחודש-חודשיים האחרונים, 

2
00:00:03,328 --> 00:00:07,504
ואף פעם לא יתעלם מהזדמנות לשיעור מתמטיקה, עולה בדעתי שהמשחק הזה 

3
00:00:07,504 --> 00:00:12,660
מהווה דוגמה מרכזית טובה מאוד בשיעור על תורת המידע, ובפרט נושא המכונה אנטרופיה. 

4
00:00:13,920 --> 00:00:17,032
אתה מבין, כמו הרבה אנשים קצת נשאבתי לתוך הפאזל, 

5
00:00:17,032 --> 00:00:22,740
וכמו הרבה מתכנתים גם נשאבתי לנסות לכתוב אלגוריתם שישחק את המשחק הכי אופטימלי שהוא יכול. 

6
00:00:23,180 --> 00:00:26,542
ומה שחשבתי לעשות כאן זה פשוט לדבר איתך על חלק מהתהליך שלי בזה, 

7
00:00:26,542 --> 00:00:31,080
ולהסביר חלק מהמתמטיקה שנכנסה לזה, מכיוון שכל האלגוריתם מתרכז ברעיון הזה של אנטרופיה. 

8
00:00:38,700 --> 00:00:41,640
דבר ראשון, למקרה שלא שמעתם על זה, מה זה וורדל? 

9
00:00:42,040 --> 00:00:45,745
וכדי להרוג כאן שתי ציפורים במכה אחת בזמן שאנחנו עוברים על כללי המשחק, 

10
00:00:45,745 --> 00:00:48,287
הרשו לי גם לראות מקדימה לאן אנחנו הולכים עם זה, 

11
00:00:48,287 --> 00:00:51,040
כלומר לפתח אלגוריתם קטן שבעצם ישחק את המשחק עבורנו. 

12
00:00:51,360 --> 00:00:55,100
למרות שלא עשיתי את הוורדל של היום, זה 4 בפברואר, ונראה איך הבוט יצליח. 

13
00:00:55,480 --> 00:01:00,340
המטרה של Wurdle היא לנחש מילה מסתורית של חמש אותיות, וניתנות לך שש הזדמנויות שונות לנחש. 

14
00:01:00,840 --> 00:01:04,379
לדוגמה, בוט הוורדל שלי מציע שאתחיל עם מנוף הניחוש. 

15
00:01:05,180 --> 00:01:10,220
בכל פעם שאתה מנחש, אתה מקבל מידע על כמה קרוב הניחוש שלך לתשובה האמיתית. 

16
00:01:10,920 --> 00:01:14,100
כאן התיבה האפורה אומרת לי שאין C בתשובה האמיתית. 

17
00:01:14,520 --> 00:01:17,840
הקופסה הצהובה אומרת לי שיש R, אבל היא לא במצב הזה. 

18
00:01:18,240 --> 00:01:22,240
התיבה הירוקה אומרת לי שלמילה הסודית יש א', והיא נמצאת במיקום השלישי. 

19
00:01:22,720 --> 00:01:24,580
ואז אין N ואין E. 

20
00:01:25,200 --> 00:01:27,340
אז תן לי פשוט להיכנס ולספר לבוט הוורדל את המידע הזה. 

21
00:01:27,340 --> 00:01:30,320
התחלנו עם מנוף, קיבלנו אפור, צהוב, ירוק, אפור, אפור. 

22
00:01:31,420 --> 00:01:34,940
אל תדאג לגבי כל הנתונים שהוא מציג עכשיו, אני אסביר את זה בבוא העת. 

23
00:01:35,460 --> 00:01:38,820
אבל ההצעה העיקרית שלה לבחירה השנייה שלנו היא shtick. 

24
00:01:39,560 --> 00:01:42,238
והניחוש שלך חייב להיות מילה אמיתית של חמש אותיות, 

25
00:01:42,238 --> 00:01:45,400
אבל כפי שתראה, זה די ליברלי עם מה שהוא בעצם יאפשר לך לנחש. 

26
00:01:46,200 --> 00:01:47,440
במקרה זה, אנו מנסים shtick. 

27
00:01:48,780 --> 00:01:50,180
ובסדר, הדברים נראים די טוב. 

28
00:01:50,260 --> 00:01:53,980
פגענו ב-S וב-H, אז אנחנו יודעים את שלוש האותיות הראשונות, אנחנו יודעים שיש R. 

29
00:01:53,980 --> 00:01:58,700
וכך זה יהיה כמו ש.א. משהו R, או ש.א.ר משהו. 

30
00:01:59,620 --> 00:02:04,240
ונראה שהבוט של Wurdle יודע שזה תלוי רק בשתי אפשרויות, או רסיס או חד. 

31
00:02:05,100 --> 00:02:10,080
זה סוג של הטלה ביניהם בשלב זה, אז אני מניח שכנראה רק בגלל שהוא אלפביתי זה הולך עם רסיס. 

32
00:02:11,220 --> 00:02:12,860
איזה הידד, היא התשובה האמיתית. 

33
00:02:12,960 --> 00:02:13,780
אז קיבלנו את זה בשלושה. 

34
00:02:14,600 --> 00:02:17,420
אם אתה תוהה אם זה טוב, איך ששמעתי ביטוי של אדם 

35
00:02:17,420 --> 00:02:20,360
אחד זה שעם Wurdle ארבע הוא ערך ושלוש הוא ציפורי. 

36
00:02:20,680 --> 00:02:22,480
שלדעתי היא אנלוגיה די הולמת. 

37
00:02:22,480 --> 00:02:27,020
אתה צריך להיות בעקביות במשחק שלך כדי להשיג ארבע, אבל זה בהחלט לא מטורף. 

38
00:02:27,180 --> 00:02:29,920
אבל כשאתה מקבל את זה בשלוש, זה פשוט מרגיש נהדר. 

39
00:02:30,880 --> 00:02:33,420
אז אם אתה רוצה לעשות את זה, מה שאני רוצה לעשות כאן זה פשוט 

40
00:02:33,420 --> 00:02:35,960
לדבר על תהליך החשיבה שלי מההתחלה איך אני ניגש לבוט Wurdle. 

41
00:02:36,480 --> 00:02:39,440
וכמו שאמרתי, באמת שזה תירוץ לשיעור תורת מידע. 

42
00:02:39,740 --> 00:02:42,820
המטרה העיקרית היא להסביר מהו מידע ומהי אנטרופיה. 

43
00:02:48,220 --> 00:02:53,720
המחשבה הראשונה שלי בגישה לזה הייתה להסתכל על התדרים היחסיים של אותיות שונות בשפה האנגלית. 

44
00:02:54,380 --> 00:02:57,056
אז חשבתי, אוקיי, האם יש ניחוש פתיחה או צמד ניחושים 

45
00:02:57,056 --> 00:02:59,260
פתיחה שפוגע בהרבה מהאותיות השכיחות ביותר? 

46
00:02:59,960 --> 00:03:03,000
ואחד שדי אהבתי היה לעשות אחר ואחריו ציפורניים. 

47
00:03:03,760 --> 00:03:07,520
המחשבה היא שאם אתה מכה באות, אתה יודע, אתה מקבל ירוק או צהוב, זה תמיד מרגיש טוב. 

48
00:03:07,520 --> 00:03:08,840
זה מרגיש כאילו אתה מקבל מידע. 

49
00:03:09,340 --> 00:03:12,471
אבל במקרים אלה, גם אם אתה לא מכה ואתה תמיד מקבל אפור, 

50
00:03:12,471 --> 00:03:17,400
זה עדיין נותן לך מידע רב מכיוון שזה די נדיר למצוא מילה שאין בה אף אחת מהאותיות הללו. 

51
00:03:18,140 --> 00:03:20,920
אבל אפילו עדיין, זה לא מרגיש סופר שיטתי, כי למשל, 

52
00:03:20,920 --> 00:03:23,200
זה לא עושה כלום כדי להתחשב בסדר האותיות. 

53
00:03:23,560 --> 00:03:25,300
למה להקליד ציפורניים כשאני יכול להקליד חילזון? 

54
00:03:26,080 --> 00:03:27,500
האם עדיף לקבל את ה-S הזה בסוף? 

55
00:03:27,820 --> 00:03:28,680
אני לא ממש בטוח. 

56
00:03:29,240 --> 00:03:32,822
עכשיו, חבר שלי אמר שהוא אהב לפתוח במילה עייף, מה שדי 

57
00:03:32,822 --> 00:03:36,540
הפתיע אותי כי יש בה כמה אותיות לא שכיחות כמו ה-W וה-Y. 

58
00:03:37,120 --> 00:03:39,000
אבל מי יודע, אולי זו פתיחה טובה יותר. 

59
00:03:39,320 --> 00:03:44,320
האם יש איזשהו ציון כמותי שאנחנו יכולים לתת כדי לשפוט את האיכות של ניחוש פוטנציאלי? 

60
00:03:45,340 --> 00:03:48,665
עכשיו כדי להגדיר את הדרך שבה אנחנו הולכים לדרג ניחושים אפשריים, 

61
00:03:48,665 --> 00:03:51,420
בואו נחזור ונוסיף קצת בהירות כיצד בדיוק המשחק מוגדר. 

62
00:03:51,420 --> 00:03:57,880
אז יש רשימה של מילים שזה יאפשר לך להזין שנחשבות לניחושים תקפים שאורכה רק כ-13,000 מילים. 

63
00:03:58,320 --> 00:04:02,977
אבל כשמסתכלים על זה, יש הרבה דברים ממש לא שכיחים, דברים כמו ראש או עלי ו-ARG, 

64
00:04:02,977 --> 00:04:06,440
מסוג המילים שמביאות לוויכוחים משפחתיים במשחק של Scrabble. 

65
00:04:06,960 --> 00:04:10,540
אבל האווירה של המשחק היא שהתשובה תמיד תהיה מילה נפוצה בהחלט. 

66
00:04:10,960 --> 00:04:15,360
ולמעשה, יש עוד רשימה של כ-2300 מילים שהן התשובות האפשריות. 

67
00:04:15,940 --> 00:04:21,160
וזו רשימה שנאספה על ידי אדם, אני חושב במיוחד על ידי חברתו של יוצר המשחק, וזה די כיף. 

68
00:04:21,820 --> 00:04:26,031
אבל מה שהייתי רוצה לעשות, האתגר שלנו עבור הפרויקט הזה הוא לראות אם 

69
00:04:26,031 --> 00:04:30,180
נוכל לכתוב תוכנית לפתרון Wordle שאינה משלבת ידע קודם על רשימה זו. 

70
00:04:30,720 --> 00:04:34,640
ראשית, יש הרבה מילים נפוצות למדי של חמש אותיות שלא תמצאו ברשימה הזו. 

71
00:04:34,940 --> 00:04:39,309
אז עדיף לכתוב תוכנית שהיא קצת יותר עמידה ותשחק וורדל נגד כל אחד, 

72
00:04:39,309 --> 00:04:41,460
לא רק מה שבמקרה הוא האתר הרשמי. 

73
00:04:41,920 --> 00:04:47,000
וגם הסיבה שאנחנו יודעים מהי רשימה זו של תשובות אפשריות, היא בגלל שהיא גלויה בקוד המקור. 

74
00:04:47,000 --> 00:04:53,260
אבל האופן שבו זה נראה בקוד המקור הוא בסדר הספציפי שבו התשובות עולות מיום ליום. 

75
00:04:53,260 --> 00:04:55,840
אז אתה תמיד יכול פשוט לחפש מה תהיה התשובה של מחר. 

76
00:04:56,420 --> 00:04:58,880
אז ברור, יש מובן מסוים שהשימוש ברשימה הוא רמאות. 

77
00:04:59,100 --> 00:05:02,672
ומה שעושה חידה מעניינת יותר ושיעור תיאוריית מידע עשיר יותר הוא 

78
00:05:02,672 --> 00:05:06,527
להשתמש במקום בכמה נתונים אוניברסליים יותר כמו תדרים יחסיים של מילים 

79
00:05:06,527 --> 00:05:10,440
באופן כללי כדי ללכוד את האינטואיציה הזו של העדפה למילים נפוצות יותר. 

80
00:05:11,600 --> 00:05:15,900
אז מבין 13,000 האפשרויות הללו, איך עלינו לבחור את ניחוש הפתיחה? 

81
00:05:16,400 --> 00:05:19,780
לדוגמה, אם חבר שלי מציע נישואים עייפים, כיצד עלינו לנתח את איכותו? 

82
00:05:20,520 --> 00:05:23,874
ובכן, הסיבה שהוא אמר שהוא אוהב את ה-W הלא סביר הזה היא שהוא 

83
00:05:23,874 --> 00:05:27,340
אוהב את אופי ה-long shot של כמה טוב זה מרגיש אם תפגע ב-W הזה. 

84
00:05:27,920 --> 00:05:31,654
לדוגמה, אם הדפוס הראשון שנחשף היה משהו כזה, אז מסתבר 

85
00:05:31,654 --> 00:05:35,600
שיש רק 58 מילים בלקסיקון הענק הזה שתואמות את הדפוס הזה. 

86
00:05:36,060 --> 00:05:38,400
אז זה הפחתה עצומה מ-13,000. 

87
00:05:38,780 --> 00:05:43,020
אבל הצד השני של זה, כמובן, הוא שזה מאוד נדיר לקבל דפוס כזה. 

88
00:05:43,020 --> 00:05:47,381
באופן ספציפי, אם כל מילה הייתה בעלת סבירות שווה להיות התשובה, 

89
00:05:47,381 --> 00:05:51,040
ההסתברות לפגיעה בתבנית זו תהיה 58 חלקי בערך 13,000. 

90
00:05:51,580 --> 00:05:53,600
כמובן, לא סביר להניח שהם יהיו תשובות. 

91
00:05:53,720 --> 00:05:56,220
רוב אלו הן מילים מאוד לא ברורות ואפילו מפוקפקות. 

92
00:05:56,600 --> 00:05:59,005
אבל לפחות עבור המעבר הראשון שלנו בכל זה, בואו נניח 

93
00:05:59,005 --> 00:06:01,600
שכולן סבירות באותה מידה ואז נחדד את זה קצת מאוחר יותר. 

94
00:06:02,020 --> 00:06:06,720
הנקודה היא שהדפוס עם הרבה מידע מעצם טבעו לא סביר שיתרחש. 

95
00:06:07,280 --> 00:06:10,800
למעשה, המשמעות של להיות אינפורמטיבי הוא שזה לא סביר. 

96
00:06:11,719 --> 00:06:18,120
דפוס הרבה יותר סביר לראות עם הפתיחה הזו יהיה משהו כזה, שבו כמובן אין בו W. 

97
00:06:18,240 --> 00:06:21,400
אולי יש E, ואולי אין A, אין R, אין Y. 

98
00:06:22,080 --> 00:06:24,560
במקרה זה, יש 1400 התאמות אפשריות. 

99
00:06:25,080 --> 00:06:30,600
אם כולם היו סבירים באותה מידה, מסתבר שהסתברות של כ-11% היא זו הדפוס שהיית רואה. 

100
00:06:30,900 --> 00:06:33,340
אז התוצאות הסבירות ביותר הן גם הפחות אינפורמטיביות. 

101
00:06:34,240 --> 00:06:37,690
כדי לקבל תצוגה גלובלית יותר כאן, הרשו לי להראות לכם את ההתפלגות 

102
00:06:37,690 --> 00:06:41,140
המלאה של ההסתברויות על פני כל הדפוסים השונים שאתם עשויים לראות. 

103
00:06:41,740 --> 00:06:46,474
אז כל פס שאתה מסתכל עליו מתאים לתבנית אפשרית של צבעים שאפשר לחשוף, 

104
00:06:46,474 --> 00:06:52,340
מתוכם יש 3 עד 5 אפשרויות, והם מאורגנים משמאל לימין, הנפוצים ביותר עד הפחות נפוצים. 

105
00:06:52,920 --> 00:06:56,000
אז האפשרות הנפוצה ביותר כאן היא שאתה מקבל את כל האפורים. 

106
00:06:56,100 --> 00:06:58,120
זה קורה בערך 14% מהמקרים. 

107
00:06:58,580 --> 00:07:04,008
ומה שאתה מייחל לו כשאתה מנחש זה שאתה בסופו של דבר איפשהו בזנב הארוך הזה, 

108
00:07:04,008 --> 00:07:09,140
כמו כאן, שם יש רק 18 אפשרויות למה שתואם את הדפוס הזה שכנראה נראה כך. 

109
00:07:09,920 --> 00:07:13,800
או אם נצא קצת יותר שמאלה, אתה יודע, אולי נלך עד לכאן. 

110
00:07:14,940 --> 00:07:16,180
אוקיי, הנה חידה טובה בשבילך. 

111
00:07:16,540 --> 00:07:22,000
מהן שלוש המילים בשפה האנגלית שמתחילות ב-W, מסתיימות ב-Y, ויש בהן R איפשהו? 

112
00:07:22,480 --> 00:07:26,800
מסתבר שהתשובות הן, בוא נראה, מלאות מילים, תולעות ומתפתלות. 

113
00:07:27,500 --> 00:07:31,733
אז כדי לשפוט עד כמה המילה הזו טובה בסך הכל, אנחנו רוצים 

114
00:07:31,733 --> 00:07:35,740
איזושהי מידה של כמות המידע הצפויה שתקבלו מההפצה הזו. 

115
00:07:35,740 --> 00:07:42,011
אם נעבור על כל דפוס ונכפיל את ההסתברות שלו להתרחש פעמים משהו שמודד כמה הוא אינפורמטיבי, 

116
00:07:42,011 --> 00:07:44,720
זה יכול אולי לתת לנו ציון אובייקטיבי. 

117
00:07:45,960 --> 00:07:49,840
עכשיו האינסטינקט הראשון שלך לגבי מה המשהו הזה צריך להיות עשוי להיות מספר ההתאמות. 

118
00:07:50,160 --> 00:07:52,400
אתה רוצה מספר ממוצע נמוך יותר של התאמות. 

119
00:07:52,800 --> 00:07:58,057
אבל במקום זאת, הייתי רוצה להשתמש במדידה אוניברסלית יותר שלעתים קרובות אנו מייחסים למידע, 

120
00:07:58,057 --> 00:08:02,074
וכזו שתהיה גמישה יותר ברגע שתוקצו לנו הסתברות שונה לכל אחת מ-13,000 

121
00:08:02,074 --> 00:08:04,260
המילים הללו אם הן באמת התשובה או לא. 

122
00:08:10,320 --> 00:08:13,930
יחידת המידע הסטנדרטית היא ה-bit, שיש לה נוסחה קצת מצחיקה, 

123
00:08:13,930 --> 00:08:16,980
אבל היא ממש אינטואיטיבית אם רק נסתכל על דוגמאות. 

124
00:08:17,780 --> 00:08:23,500
אם יש לך תצפית שחותכת את מרחב האפשרויות שלך לחצי, אנו אומרים שיש לה פיסת מידע אחת. 

125
00:08:24,180 --> 00:08:27,125
בדוגמה שלנו, מרחב האפשרויות הוא כל המילים האפשריות, 

126
00:08:27,125 --> 00:08:31,260
ומסתבר שבערך למחצית ממילות חמש האותיות יש S, קצת פחות מזה, אבל בערך חצי. 

127
00:08:31,780 --> 00:08:34,320
אז התצפית הזו תיתן לך קצת מידע. 

128
00:08:34,880 --> 00:08:39,293
אם במקום זאת עובדה חדשה מקצצת את מרחב האפשרויות הזה בפקטור של ארבע, 

129
00:08:39,293 --> 00:08:41,500
אנו אומרים שיש לה שתי פיסות מידע. 

130
00:08:41,980 --> 00:08:44,460
לדוגמה, מסתבר שלכרבע מהמילים הללו יש T. 

131
00:08:45,020 --> 00:08:49,614
אם התצפית חותכת את החלל הזה בפקטור של שמונה, אנחנו אומרים שזה שלוש פיסות מידע, 

132
00:08:49,614 --> 00:08:50,720
וכן הלאה וכן הלאה. 

133
00:08:50,900 --> 00:08:55,060
ארבעה ביטים חותכים אותו ל-16, חמישה ביטים חותכים אותו ל-32. 

134
00:08:55,060 --> 00:08:59,175
אז עכשיו אולי תרצו לעצור ולשאול את עצמכם, מהי הנוסחה 

135
00:08:59,175 --> 00:09:02,980
למידע עבור מספר הביטים מבחינת ההסתברות להתרחשות? 

136
00:09:03,920 --> 00:09:09,600
מה שאנחנו אומרים כאן הוא שכאשר אתה לוקח חצי אחד למספר הסיביות, זה אותו דבר כמו ההסתברות, 

137
00:09:09,600 --> 00:09:14,005
שזה אותו דבר כמו לומר שניים בחזקת מספר הסיביות הוא אחד מעל ההסתברות, 

138
00:09:14,005 --> 00:09:18,920
אשר מסדר מחדש בהמשך לומר שהמידע הוא בסיס היומן שניים מתוך אחד חלקי ההסתברות. 

139
00:09:19,620 --> 00:09:22,149
ולפעמים אתה רואה את זה עם עוד סידור מחדש אחד, 

140
00:09:22,149 --> 00:09:24,900
כאשר המידע הוא בסיס היומן השלילי שני של ההסתברות. 

141
00:09:25,660 --> 00:09:28,941
בביטוי כך, זה יכול להיראות קצת מוזר למי שלא יודע מה, 

142
00:09:28,941 --> 00:09:34,080
אבל זה באמת רק הרעיון האינטואיטיבי של לשאול כמה פעמים צמצמת את האפשרויות שלך בחצי. 

143
00:09:35,180 --> 00:09:38,093
עכשיו אם אתה תוהה, אתה יודע, חשבתי שאנחנו סתם משחקים משחק מילים מהנה, 

144
00:09:38,093 --> 00:09:39,300
למה לוגריתמים נכנסים לתמונה? 

145
00:09:39,780 --> 00:09:44,189
אחת הסיבות לכך שזו יחידה נחמדה יותר היא שפשוט הרבה יותר קל לדבר 

146
00:09:44,189 --> 00:09:48,737
על אירועים מאוד לא סבירים, הרבה יותר קל לומר שלתצפית יש 20 סיביות 

147
00:09:48,737 --> 00:09:52,940
מידע מאשר לומר שההסתברות להתרחשות כאלה ואחרים היא 0.0000095. 

148
00:09:53,300 --> 00:09:57,127
אבל סיבה מהותית יותר לכך שהביטוי הלוגריתמי הזה התברר 

149
00:09:57,127 --> 00:10:01,460
כתוספת שימושית מאוד לתורת ההסתברות היא הדרך שבה מידע מתחבר. 

150
00:10:02,060 --> 00:10:06,754
לדוגמה, אם תצפית אחת נותנת לך שתי פיסות מידע, מקצצת את השטח שלך בארבע, 

151
00:10:06,754 --> 00:10:11,714
ואז תצפית שנייה כמו הניחוש השני שלך ב-Wordle נותנת לך עוד שלוש פיסות מידע, 

152
00:10:11,714 --> 00:10:16,740
ומצמצמת אותך עוד יותר בפקטור של שמונה, שניים ביחד נותנים לך חמש פיסות מידע. 

153
00:10:17,160 --> 00:10:21,020
באותו אופן שבו הסתברויות אוהבות להכפיל, מידע אוהב להוסיף. 

154
00:10:21,960 --> 00:10:25,722
אז ברגע שאנחנו נמצאים בתחום של משהו כמו ערך צפוי, שבו אנחנו מוסיפים חבורה של מספרים, 

155
00:10:25,722 --> 00:10:27,980
היומנים הופכים את זה להרבה יותר נחמד להתמודד איתו. 

156
00:10:28,480 --> 00:10:34,940
בואו נחזור להפצה שלנו עבור Weary ונוסיף עוד עוקב קטן כאן, שמראה לנו כמה מידע יש לכל דפוס. 

157
00:10:35,580 --> 00:10:40,344
הדבר העיקרי שאני רוצה שתשים לב הוא שככל שההסתברות גבוהה יותר כשנגיע לדפוסים הסבירים האלה, 

158
00:10:40,344 --> 00:10:42,780
כך המידע נמוך יותר, כך אתה מרוויח פחות ביטים. 

159
00:10:43,500 --> 00:10:48,448
הדרך שבה אנו מודדים את האיכות של הניחוש הזה תהיה לקחת את הערך הצפוי של המידע הזה, 

160
00:10:48,448 --> 00:10:51,706
שם אנחנו עוברים על כל דפוס, אנחנו אומרים כמה זה סביר, 

161
00:10:51,706 --> 00:10:54,060
ואז נכפיל את זה בכמה סיביות מידע נקבל. 

162
00:10:54,710 --> 00:10:58,120
ובדוגמה של Weary, מסתבר שזה 4.9 ביטים. 

163
00:10:58,560 --> 00:11:01,884
אז בממוצע, המידע שאתה מקבל מניחוש הפתיחה הזה טוב 

164
00:11:01,884 --> 00:11:05,480
כמו לחתוך את מרחב האפשרויות שלך לחצי בערך חמש פעמים. 

165
00:11:05,960 --> 00:11:11,640
לעומת זאת, דוגמה לניחוש עם ערך מידע צפוי גבוה יותר תהיה משהו כמו Slate. 

166
00:11:13,120 --> 00:11:15,620
במקרה זה תבחין שההפצה נראית הרבה יותר שטוחה. 

167
00:11:15,940 --> 00:11:21,765
בפרט, להתרחשות הסבירה ביותר של כל האפורים יש רק סיכוי של כ-6% להתרחש, 

168
00:11:21,765 --> 00:11:25,260
כך שלפחות אתה מקבל כנראה 3.9 סיביות מידע. 

169
00:11:25,920 --> 00:11:28,560
אבל זה מינימום, בדרך כלל תקבל משהו יותר טוב מזה. 

170
00:11:29,100 --> 00:11:34,026
ומסתבר שכאשר אתה מכתש את המספרים על זה ומחבר את כל המונחים הרלוונטיים, 

171
00:11:34,026 --> 00:11:35,900
המידע הממוצע הוא בערך 5.8. 

172
00:11:37,360 --> 00:11:43,540
אז בניגוד ל-Weary, מרחב האפשרויות שלך יהיה גדול בערך בחצי לאחר הניחוש הראשון הזה, בממוצע. 

173
00:11:44,420 --> 00:11:49,120
למעשה יש סיפור מהנה על השם של הערך הצפוי הזה של כמות מידע. 

174
00:11:49,200 --> 00:11:53,138
תורת המידע פותחה על ידי קלוד שאנון, שעבד ב-Bell Labs בשנות ה-40, 

175
00:11:53,138 --> 00:11:57,319
אבל הוא דיבר על כמה מהרעיונות שלו שטרם פורסמו עם ג'ון פון נוימן, 

176
00:11:57,319 --> 00:12:00,530
שהיה הענק האינטלקטואלי הזה של אותה תקופה, בולט מאוד. 

177
00:12:00,530 --> 00:12:03,560
במתמטיקה ובפיזיקה ותחילתו של מה שהפך למדעי המחשב. 

178
00:12:04,100 --> 00:12:09,752
וכשהזכיר שאין לו שם טוב לערך הצפוי הזה של כמות מידע, כביכול פון נוימן אמר, 

179
00:12:09,752 --> 00:12:14,200
אז הסיפור אומר, ובכן כדאי לקרוא לזה אנטרופיה, ומשתי סיבות. 

180
00:12:14,540 --> 00:12:20,446
מלכתחילה, פונקציית אי הוודאות שלך שימשה במכניקה סטטיסטית תחת השם הזה, אז כבר יש לה שם, 

181
00:12:20,446 --> 00:12:24,451
ובמקום השני, ויותר חשוב, אף אחד לא יודע מהי באמת אנטרופיה, 

182
00:12:24,451 --> 00:12:26,760
אז בוויכוח אתה תמיד יש את היתרון. 

183
00:12:27,700 --> 00:12:32,460
אז אם השם נראה קצת מסתורי, ואם אפשר להאמין לסיפור הזה, זה סוג של תכנון. 

184
00:12:33,280 --> 00:12:38,883
כמו כן, אם אתה תוהה לגבי הקשר שלו לכל החומר השני של החוק השני של התרמודינמיקה מהפיזיקה, 

185
00:12:38,883 --> 00:12:43,976
בהחלט יש קשר, אבל במקורותיו שאנון עסק רק בתורת ההסתברות הטהורה, ולמטרותינו כאן, 

186
00:12:43,976 --> 00:12:49,580
כשאני משתמש ב- אנטרופיה של מילים, אני רק רוצה שתחשוב על ערך המידע הצפוי של ניחוש מסוים. 

187
00:12:50,700 --> 00:12:53,780
אתה יכול לחשוב על אנטרופיה כמדידת שני דברים בו זמנית. 

188
00:12:54,240 --> 00:12:56,780
הראשון הוא עד כמה שטוחה ההתפלגות. 

189
00:12:57,320 --> 00:13:01,120
ככל שהתפלגות קרובה יותר לאחידות, כך האנטרופיה תהיה גבוהה יותר. 

190
00:13:01,580 --> 00:13:06,847
במקרה שלנו, כאשר יש 3 עד ה-5 דפוסים הכוללים, עבור התפלגות אחידה, 

191
00:13:06,847 --> 00:13:12,681
צפייה בכל אחד מהם תהיה בסיס יומן מידע 2 מתוך 3 עד ה-5, שהוא במקרה 7.92, 

192
00:13:12,681 --> 00:13:17,300
אז זה המקסימום המוחלט שיכול להיות לך עבור האנטרופיה הזו. 

193
00:13:17,840 --> 00:13:22,080
אבל אנטרופיה היא גם סוג של מדד לכמה אפשרויות יש מלכתחילה. 

194
00:13:22,320 --> 00:13:26,887
לדוגמה, אם במקרה יש לך מילה כלשהי שבה יש רק 16 דפוסים אפשריים, 

195
00:13:26,887 --> 00:13:32,180
וכל אחת מהן בסבירות שווה, האנטרופיה הזו, המידע הצפוי הזה, תהיה 4 סיביות. 

196
00:13:32,579 --> 00:13:36,657
אבל אם יש לך מילה אחרת שבה יש 64 דפוסים אפשריים שיכולים להופיע, 

197
00:13:36,657 --> 00:13:40,480
וכולם סבירים באותה מידה, אז האנטרופיה תסתבר להיות 6 סיביות. 

198
00:13:41,500 --> 00:13:46,211
אז אם אתה רואה איזושהי התפלגות בטבע שיש לה אנטרופיה של 6 ביטים, 

199
00:13:46,211 --> 00:13:52,101
זה בערך כאילו זה אומר שיש שונות וחוסר ודאות במה שעומד לקרות כאילו היו 64 תוצאות 

200
00:13:52,101 --> 00:13:53,500
סבירות באותה מידה. 

201
00:13:54,360 --> 00:13:59,320
עבור המעבר הראשון שלי ב-Wurtelebot, בעצם הייתי צריך לעשות את זה. 

202
00:13:59,320 --> 00:14:03,634
הוא עובר על כל הניחושים האפשריים שיכולים להיות לך, כל 13,000 המילים, 

203
00:14:03,634 --> 00:14:09,136
מחשב את האנטרופיה עבור כל אחת, או ליתר דיוק, את האנטרופיה של ההתפלגות על פני כל הדפוסים 

204
00:14:09,136 --> 00:14:12,450
שאתה עשוי לראות, עבור כל אחת, ובוחר את הגבוהה ביותר, 

205
00:14:12,450 --> 00:14:16,140
מכיוון שזהו זה שצפוי לקצץ את מרחב האפשרויות שלך ככל האפשר. 

206
00:14:17,140 --> 00:14:21,100
ולמרות שדיברתי רק על הניחוש הראשון כאן, זה עושה את אותו הדבר עבור הניחושים הבאים. 

207
00:14:21,560 --> 00:14:24,347
לדוגמה, לאחר שתראה דפוס כלשהו בניחוש הראשון הזה, 

208
00:14:24,347 --> 00:14:28,272
שיגביל אותך למספר קטן יותר של מילים אפשריות בהתבסס על מה שמתאים לזה, 

209
00:14:28,272 --> 00:14:31,800
אתה פשוט משחק באותו משחק ביחס לאותה קבוצה קטנה יותר של מילים. 

210
00:14:32,260 --> 00:14:38,151
לניחוש שני מוצע, אתה מסתכל על ההתפלגות של כל הדפוסים שיכולים להתרחש מאותה קבוצה מוגבלת 

211
00:14:38,151 --> 00:14:43,840
יותר של מילים, אתה מחפש בכל 13,000 האפשרויות, ומוצא את זו שממקסמת את האנטרופיה הזו. 

212
00:14:45,420 --> 00:14:49,605
כדי להראות לכם איך זה עובד בפעולה, הרשו לי רק להעלות גרסה 

213
00:14:49,605 --> 00:14:54,080
קטנה של Wurtele שכתבתי שמראה את הדגשים של הניתוח הזה בשוליים. 

214
00:14:54,080 --> 00:14:56,926
לאחר ביצוע כל חישובי האנטרופיה שלו, מימין כאן הוא 

215
00:14:56,926 --> 00:14:59,660
מראה לנו למי מהם יש את המידע הצפוי הגבוה ביותר. 

216
00:15:00,280 --> 00:15:05,242
מסתבר שהתשובה העליונה, לפחות כרגע, נחדד את זה בהמשך, 

217
00:15:05,242 --> 00:15:10,580
היא טארס, שפירושו, אממ, כמובן, בקיה, הקיבה הנפוצה ביותר. 

218
00:15:11,040 --> 00:15:15,655
בכל פעם שאנחנו מנחשים כאן, איפה אולי אני קצת מתעלם מההמלצות שלו והולך עם צפחה, 

219
00:15:15,655 --> 00:15:19,103
כי אני אוהב צפחה, אנחנו יכולים לראות כמה מידע צפוי היה לו, 

220
00:15:19,103 --> 00:15:22,900
אבל אז בצד ימין של המילה כאן זה מראה לנו כמה מידע אמיתי שקיבלנו, 

221
00:15:22,900 --> 00:15:24,420
בהתחשב בדפוס הספציפי הזה. 

222
00:15:25,000 --> 00:15:28,328
אז כאן זה נראה כאילו היה לנו קצת חסר מזל, היה צפוי לנו לקבל 5.8, 

223
00:15:28,328 --> 00:15:30,120
אבל במקרה קיבלנו משהו עם פחות מזה. 

224
00:15:30,600 --> 00:15:35,020
ואז בצד שמאל כאן זה מראה לכולנו את המילים האפשריות השונות שבהן אנחנו נמצאים עכשיו. 

225
00:15:35,800 --> 00:15:38,949
הפסים הכחולים אומרים לנו עד כמה הוא חושב שכל מילה היא, 

226
00:15:38,949 --> 00:15:43,360
אז כרגע זה מניח שכל מילה בסבירות שווה להתרחש, אבל אנחנו נחדד את זה בעוד רגע. 

227
00:15:44,060 --> 00:15:50,183
ואז מדידת אי הוודאות הזו אומרת לנו את האנטרופיה של ההתפלגות הזו על פני המילים האפשריות, 

228
00:15:50,183 --> 00:15:55,960
שכרגע, מכיוון שזו התפלגות אחידה, היא רק דרך מסובכת מיותרת לספור את מספר האפשרויות. 

229
00:15:56,560 --> 00:16:02,180
לדוגמה, אם היינו לוקחים 2 בחזקת 13.66, זה אמור להיות בסביבות 13,000 האפשרויות. 

230
00:16:02,900 --> 00:16:06,140
אני קצת בחוץ כאן, אבל רק בגלל שאני לא מראה את כל האותיות העשרוניות. 

231
00:16:06,720 --> 00:16:09,376
כרגע זה עשוי להרגיש מיותר וכאילו זה מסבך דברים מדי, 

232
00:16:09,376 --> 00:16:12,340
אבל אתה תראה למה זה שימושי להחזיק את שני המספרים תוך דקה. 

233
00:16:12,760 --> 00:16:17,783
אז כאן זה נראה כאילו זה מרמז על האנטרופיה הגבוהה ביותר עבור הניחוש השני שלנו הוא ראמן, 

234
00:16:17,783 --> 00:16:19,400
ששוב ממש לא מרגיש כמו מילה. 

235
00:16:19,980 --> 00:16:24,060
אז כדי לקחת את הרמה המוסרית כאן, אני מתכוון להמשיך ולהקליד את Rains. 

236
00:16:25,440 --> 00:16:27,340
ושוב זה נראה כאילו היה לנו קצת חסר מזל. 

237
00:16:27,520 --> 00:16:31,360
ציפינו ל-4.3 ביטים וקיבלנו רק 3.39 סיביות מידע. 

238
00:16:31,940 --> 00:16:33,940
אז זה מוריד אותנו ל-55 אפשרויות. 

239
00:16:34,900 --> 00:16:39,440
וכאן אולי אני פשוט אלך עם מה שזה מציע, שזה שילוב, מה שזה לא אומר. 

240
00:16:40,040 --> 00:16:42,920
ובסדר, זו בעצם הזדמנות טובה לפאזל. 

241
00:16:42,920 --> 00:16:46,380
זה אומר לנו שהדפוס הזה נותן לנו 4.7 סיביות מידע. 

242
00:16:47,060 --> 00:16:51,720
אבל בצד שמאל, לפני שאנחנו רואים את הדפוס הזה, היו 5.78 פיסות של אי ודאות. 

243
00:16:52,420 --> 00:16:56,340
אז בתור חידון בשבילך, מה זה אומר לגבי מספר האפשרויות שנותרו? 

244
00:16:58,040 --> 00:17:01,488
ובכן, זה אומר שאנחנו מצטמצמים לחלק אחד של אי ודאות, 

245
00:17:01,488 --> 00:17:04,540
וזה אותו דבר כמו לומר שיש שתי תשובות אפשריות. 

246
00:17:04,700 --> 00:17:05,700
זו בחירה של 50-50. 

247
00:17:06,500 --> 00:17:08,885
ומכאן, בגלל שאתה ואני יודעים אילו מילים נפוצות יותר, 

248
00:17:08,885 --> 00:17:10,640
אנחנו יודעים שהתשובה צריכה להיות תהום. 

249
00:17:11,180 --> 00:17:13,280
אבל כמו שזה נכתב עכשיו, התוכנית לא יודעת את זה. 

250
00:17:13,540 --> 00:17:19,859
אז זה פשוט ממשיך, מנסה להשיג כמה שיותר מידע, עד שנותרה רק אפשרות אחת, ואז הוא מנחש את זה. 

251
00:17:20,380 --> 00:17:22,339
אז ברור שאנחנו צריכים אסטרטגיית סוף משחק טובה יותר. 

252
00:17:22,599 --> 00:17:25,311
אבל נניח שאנחנו קוראים לגרסה הזו אחת מפותרי המילים שלנו, 

253
00:17:25,311 --> 00:17:28,260
ואז אנחנו הולכים ומריצים כמה סימולציות כדי לראות איך זה עובד. 

254
00:17:30,360 --> 00:17:34,120
אז הדרך שבה זה עובד היא שהוא משחק בכל משחק מילים אפשרי. 

255
00:17:34,240 --> 00:17:38,540
זה עובר על כל 2315 המילים האלה שהן התשובות המילוליות בפועל. 

256
00:17:38,540 --> 00:17:40,580
זה בעצם משתמש בזה כמערכת בדיקות. 

257
00:17:41,360 --> 00:17:45,014
ועם השיטה הנאיבית הזו של לא להתחשב עד כמה מילה נפוצה, 

258
00:17:45,014 --> 00:17:49,820
ורק לנסות למקסם את המידע בכל שלב בדרך, עד שהוא מגיע לבחירה אחת ויחידה. 

259
00:17:50,360 --> 00:17:54,300
בסוף הסימולציה, הציון הממוצע מתברר כ-4.124. 

260
00:17:55,319 --> 00:17:59,240
וזה לא רע, למען האמת, די ציפיתי לעשות יותר גרוע. 

261
00:17:59,660 --> 00:18:02,600
אבל האנשים שמנגנים wordle יגידו לך שבדרך כלל הם יכולים להשיג את זה ב-4. 

262
00:18:02,860 --> 00:18:05,380
האתגר האמיתי הוא להשיג כמה שיותר ב-3. 

263
00:18:05,380 --> 00:18:08,080
זה קפיצה די גדולה בין הציון 4 לציון 3. 

264
00:18:08,860 --> 00:18:14,980
הפרי התלוי הנמוך כאן הוא איכשהו לשלב האם מילה נפוצה או לא, ואיך בדיוק אנחנו עושים את זה. 

265
00:18:22,800 --> 00:18:27,880
הדרך שבה ניגשתי היא לקבל רשימה של התדרים היחסיים של כל המילים בשפה האנגלית. 

266
00:18:28,220 --> 00:18:31,381
והרגע השתמשתי בפונקציית נתוני תדירות המילים של Mathematica, 

267
00:18:31,381 --> 00:18:34,860
שבעצמה שואבת ממאגר הנתונים הציבורי של Google Books English Ngram. 

268
00:18:35,460 --> 00:18:39,960
וזה די כיף להסתכל עליו, למשל אם נמיין את זה מהמילים הנפוצות ביותר למילים הפחות נפוצות. 

269
00:18:40,120 --> 00:18:43,080
ברור שאלו הן המילים הנפוצות ביותר, 5 אותיות בשפה האנגלית. 

270
00:18:43,700 --> 00:18:45,840
או ליתר דיוק, אלו הם ה-8 הנפוצים ביותר. 

271
00:18:46,280 --> 00:18:48,880
ראשון זה איזה, אחרי זה יש שם ושם. 

272
00:18:49,260 --> 00:18:53,863
הראשון עצמו הוא לא הראשון, אלא התשיעי, והגיוני שהמילים האחרות הללו יכולות להופיע 

273
00:18:53,863 --> 00:18:58,580
לעתים קרובות יותר, כאשר אלה שאחרי הראשונים הם אחרי, איפה, ואלו רק קצת פחות נפוצות. 

274
00:18:59,160 --> 00:19:04,254
כעת, בשימוש בנתונים האלה כדי להדגים את הסבירות שכל אחת מהמילים הללו תהיה התשובה הסופית, 

275
00:19:04,254 --> 00:19:06,860
היא לא צריכה להיות רק פרופורציונלית לתדירות. 

276
00:19:06,860 --> 00:19:11,221
למשל, שניתן לו ציון 0.002 במערך הנתונים הזה, בעוד 

277
00:19:11,221 --> 00:19:15,060
שהמילה צמה היא בסבירות מסוימת פי 1000 פחות. 

278
00:19:15,560 --> 00:19:18,840
אבל שתיהן מילים מספיק נפוצות שכדאי לשקול אותן. 

279
00:19:19,340 --> 00:19:21,000
אז אנחנו רוצים יותר חתך בינארי. 

280
00:19:21,860 --> 00:19:27,884
הדרך שבה הלכתי היא לדמיין לקחת את כל הרשימה הממוינת הזו של מילים, ואז לסדר אותה על ציר x, 

281
00:19:27,884 --> 00:19:33,373
ואז להחיל את הפונקציה הסיגמואידית, שהיא הדרך הסטנדרטית לקבל פונקציה שהפלט שלה הוא 

282
00:19:33,373 --> 00:19:38,260
בעצם בינארי, זה או 0 או שזה 1, אבל יש החלקה ביניהם לאזור זה של אי ודאות. 

283
00:19:39,160 --> 00:19:43,936
אז בעצם, ההסתברות שאני מקצה לכל מילה להימצאות ברשימה הסופית תהיה הערך 

284
00:19:43,936 --> 00:19:48,440
של הפונקציה הסיגמואידית למעלה בכל מקום שבו היא ממוקמת על ציר ה-x. 

285
00:19:49,520 --> 00:19:54,093
עכשיו ברור שזה תלוי בכמה פרמטרים, למשל כמה רחב הרווח בציר ה-x 

286
00:19:54,093 --> 00:19:59,183
ממלאות המילים האלה קובע באיזו הדרגתית או תלולה אנחנו יורדים מ-1 ל-0, 

287
00:19:59,183 --> 00:20:03,240
והמקום שבו אנחנו ממקמים אותן משמאל לימין קובע את החתך. 

288
00:20:03,240 --> 00:20:06,920
למען האמת, הדרך שעשיתי את זה הייתה פשוט ללקק את האצבע שלי ולהכניס אותה לרוח. 

289
00:20:07,140 --> 00:20:12,066
עיינתי ברשימה הממוינת וניסיתי למצוא חלון שבו כשהסתכלתי עליו הבנתי שכמחצית 

290
00:20:12,066 --> 00:20:17,260
מהמילים האלה צפויות יותר מאשר לא להיות התשובה הסופית, והשתמשתי בזה בתור החתך. 

291
00:20:17,260 --> 00:20:20,461
ברגע שיש לנו התפלגות כזו בין המילים, זה נותן לנו 

292
00:20:20,461 --> 00:20:23,860
מצב נוסף שבו האנטרופיה הופכת למדידה ממש שימושית זו. 

293
00:20:24,500 --> 00:20:29,428
לדוגמה, נניח ששיחקנו משחק ונתחיל עם הפותחים הישנים שלי, שהיו נוצה ומסמרים, 

294
00:20:29,428 --> 00:20:33,240
ובסופו של דבר יש מצב שיש ארבע מילים אפשריות שמתאימות לזה. 

295
00:20:33,560 --> 00:20:35,620
ונניח שאנו רואים בכולם סבירים באותה מידה. 

296
00:20:36,220 --> 00:20:38,880
הרשו לי לשאול אתכם, מהי האנטרופיה של התפלגות זו? 

297
00:20:41,080 --> 00:20:46,882
ובכן, המידע הקשור לכל אחת מהאפשרויות הללו יהיה בסיס היומן 2 מתוך 4, 

298
00:20:46,882 --> 00:20:50,040
מכיוון שכל אחד מהם הוא 1 ו-4, וזה 2. 

299
00:20:50,040 --> 00:20:52,460
שתי פיסות מידע, ארבע אפשרויות. 

300
00:20:52,760 --> 00:20:53,580
הכל טוב מאוד וטוב. 

301
00:20:54,300 --> 00:20:57,800
אבל מה אם אגיד לך שבעצם יש יותר מארבע גפרורים? 

302
00:20:58,260 --> 00:21:02,460
במציאות, כשאנחנו מסתכלים ברשימת המילים המלאה, יש 16 מילים שמתאימות לה. 

303
00:21:02,580 --> 00:21:06,638
אבל נניח שהמודל שלנו שם סבירות ממש נמוכה ל-12 המילים האחרות האלה 

304
00:21:06,638 --> 00:21:10,760
להיות למעשה התשובה הסופית, משהו כמו 1 ל-1000 כי הן ממש לא ברורות. 

305
00:21:11,500 --> 00:21:14,260
עכשיו הרשו לי לשאול אתכם, מהי האנטרופיה של התפלגות זו? 

306
00:21:15,420 --> 00:21:20,463
אם האנטרופיה מדדה אך ורק את מספר ההתאמות כאן, אז אתה עשוי לצפות שזה יהיה משהו 

307
00:21:20,463 --> 00:21:25,700
כמו בסיס היומן 2 מתוך 16, שיהיה 4, שתי פיסות יותר של אי ודאות ממה שהיה לנו קודם. 

308
00:21:26,180 --> 00:21:29,860
אבל כמובן שאי הוודאות בפועל לא באמת שונה ממה שהיה לנו קודם. 

309
00:21:30,160 --> 00:21:33,725
רק בגלל שיש את 12 המילים המעורפלות האלה לא אומר שזה 

310
00:21:33,725 --> 00:21:37,360
יהיה כל כך מפתיע ללמוד שהתשובה הסופית היא קסם, למשל. 

311
00:21:38,180 --> 00:21:41,778
אז כאשר אתה באמת עושה את החישוב כאן, ואתה מחבר את ההסתברות 

312
00:21:41,778 --> 00:21:45,560
של כל התרחשות כפול המידע המתאים, מה שאתה מקבל הוא 2.11 ביטים. 

313
00:21:45,560 --> 00:21:49,452
אני רק אומר, זה בעצם שני ביטים, בעצם ארבע האפשרויות האלה, 

314
00:21:49,452 --> 00:21:53,546
אבל יש קצת יותר אי ודאות בגלל כל האירועים הבלתי סבירים האלה, 

315
00:21:53,546 --> 00:21:56,500
אם כי אם תלמד אותם היית מקבל מזה המון מידע. 

316
00:21:57,160 --> 00:22:01,400
אז בהתקרבות, זה חלק ממה שהופך את Wordle לדוגמא כל כך נחמדה לשיעור תורת מידע. 

317
00:22:01,600 --> 00:22:04,640
יש לנו את שני יישומי ההרגשה המובהקים הללו עבור אנטרופיה. 

318
00:22:05,160 --> 00:22:09,416
הראשון אומר לנו מה המידע הצפוי שנקבל מניחוש נתון, 

319
00:22:09,416 --> 00:22:15,460
והשני אומר האם נוכל למדוד את אי הוודאות שנותרה בין כל המילים האפשריות. 

320
00:22:16,460 --> 00:22:20,792
ואני צריך להדגיש, במקרה הראשון שבו אנחנו מסתכלים על המידע הצפוי של ניחוש, 

321
00:22:20,792 --> 00:22:24,540
ברגע שיש לנו שקלול לא שווה למילים, זה משפיע על חישוב האנטרופיה. 

322
00:22:24,980 --> 00:22:29,911
לדוגמה, הרשו לי להעלות את אותו מקרה שבדקנו קודם לכן של התפלגות הקשורה ל-Weary, 

323
00:22:29,911 --> 00:22:33,720
אבל הפעם באמצעות התפלגות לא אחידה על פני כל המילים האפשריות. 

324
00:22:34,500 --> 00:22:38,280
אז תן לי לראות אם אני יכול למצוא כאן חלק שממחיש את זה די טוב. 

325
00:22:40,940 --> 00:22:42,360
אוקיי, כאן זה די טוב. 

326
00:22:42,360 --> 00:22:45,730
כאן יש לנו שני דפוסים צמודים שסבירים בערך באותה מידה, 

327
00:22:45,730 --> 00:22:49,100
אבל לאחד מהם נאמר לנו יש 32 מילים אפשריות שתואמות לה. 

328
00:22:49,280 --> 00:22:52,340
ואם נבדוק מה הם, אלה 32 אלה, שכולן פשוט מילים 

329
00:22:52,340 --> 00:22:55,600
מאוד לא סבירות כשאתה סורק את העיניים שלך מעליהן. 

330
00:22:55,840 --> 00:22:59,820
קשה למצוא כאלה שמרגישות כמו תשובות סבירות, אולי צעקות, 

331
00:22:59,820 --> 00:23:04,091
אבל אם נסתכל על התבנית השכנה בהתפלגות, שנחשבת כסבירה בערך, 

332
00:23:04,091 --> 00:23:09,520
נאמר לנו שיש לה רק 8 התאמות אפשריות, אז רבע התאמות רבות, אבל זה סביר בערך. 

333
00:23:09,860 --> 00:23:12,140
וכשאנחנו מוציאים את הגפרורים האלה, אנחנו יכולים לראות למה. 

334
00:23:12,500 --> 00:23:16,300
חלק מהן תשובות סבירות ממש, כמו צלצול, או זעם, או ראפ. 

335
00:23:17,900 --> 00:23:22,484
כדי להמחיש כיצד אנו משלבים את כל זה, הרשו לי להעלות כאן את גרסה 2 של ה-Wordlebot, 

336
00:23:22,484 --> 00:23:25,280
ויש שניים או שלושה הבדלים עיקריים מהראשון שראינו. 

337
00:23:25,860 --> 00:23:30,160
ראשית, כמו שאמרתי זה עתה, הדרך שבה אנו מחשבים את האנטרופיות הללו, 

338
00:23:30,160 --> 00:23:34,200
הערכים הצפויים של המידע, משתמשת כעת בהתפלגות המעודנות יותר על 

339
00:23:34,200 --> 00:23:38,240
פני הדפוסים שמשלבת את ההסתברות שמילה נתונה תהיה למעשה התשובה. 

340
00:23:38,879 --> 00:23:43,820
כפי שזה קורה, הדמעות עדיין מספר 1, אם כי אלה שאחריו קצת שונים. 

341
00:23:44,360 --> 00:23:47,838
שנית, כאשר היא תדרג את הבחירות המובילות שלה, היא עתידה לשמור 

342
00:23:47,838 --> 00:23:51,943
מודל של ההסתברות שכל מילה היא התשובה האמיתית, והיא תשלב זאת בהחלטה שלה, 

343
00:23:51,943 --> 00:23:55,080
שקל יותר לראות אותה ברגע שיש לנו כמה ניחושים על שולחן. 

344
00:23:55,860 --> 00:23:59,780
שוב, מתעלמים מהמלצתו כי אנחנו לא יכולים לתת למכונות לשלוט בחיינו. 

345
00:24:01,140 --> 00:24:05,529
ואני מניח שעלי להזכיר עוד דבר שונה כאן משמאל, שערך אי הוודאות, 

346
00:24:05,529 --> 00:24:09,640
מספר הביטים הזה, כבר לא רק מיותר עם מספר ההתאמות האפשריות. 

347
00:24:10,080 --> 00:24:16,467
עכשיו אם נמשוך אותו למעלה ונחשב 2 ל-8.02, שזה קצת מעל 256, אני מניח 259, 

348
00:24:16,467 --> 00:24:21,980
מה שזה אומר זה למרות שיש 526 מילים בסך הכל שמתאימות לדפוס הזה, 

349
00:24:21,980 --> 00:24:28,980
כמות אי הוודאות שיש לה דומה יותר למה שהיא הייתה אם היו 259 בסבירות שווה תוצאות. 

350
00:24:29,720 --> 00:24:30,740
אתה יכול לחשוב על זה ככה. 

351
00:24:31,020 --> 00:24:34,852
הוא יודע שבורקס הוא לא התשובה, אותו דבר עם יורט וזורל וזרוס, 

352
00:24:34,852 --> 00:24:37,680
אז זה קצת פחות לא ודאי ממה שהיה במקרה הקודם. 

353
00:24:37,820 --> 00:24:39,280
מספר ביטים זה יהיה קטן יותר. 

354
00:24:40,220 --> 00:24:43,282
ואם אני ממשיך לשחק את המשחק, אני מחדד את זה עם 

355
00:24:43,282 --> 00:24:46,540
כמה ניחושים שהם בהתאם למה שהייתי רוצה להסביר כאן. 

356
00:24:48,360 --> 00:24:51,275
לפי הניחוש הרביעי, אם תסתכלו על הבחירות המובילות שלה, 

357
00:24:51,275 --> 00:24:53,760
תוכלו לראות שזה כבר לא רק ממקסם את האנטרופיה. 

358
00:24:54,460 --> 00:25:00,300
אז בשלב זה, מבחינה טכנית יש שבע אפשרויות, אבל היחידות עם סיכוי משמעותי הן מעונות ומילים. 

359
00:25:00,300 --> 00:25:04,674
ואתה יכול לראות את הבחירה בשני אלה מעל כל הערכים האחרים האלה, 

360
00:25:04,674 --> 00:25:06,720
שבאופן קפדני ייתן יותר מידע. 

361
00:25:07,240 --> 00:25:10,394
בפעם הראשונה שעשיתי את זה, פשוט הוספתי את שני המספרים האלה כדי 

362
00:25:10,394 --> 00:25:13,900
למדוד את האיכות של כל ניחוש, שלמעשה עבד טוב יותר ממה שאתה עשוי לחשוד. 

363
00:25:14,300 --> 00:25:17,005
אבל זה ממש לא הרגיש שיטתי, ואני בטוח שיש עוד גישות 

364
00:25:17,005 --> 00:25:19,340
שאנשים יכולים לנקוט אבל הנה זו שנחתתי עליה. 

365
00:25:19,760 --> 00:25:23,696
אם אנחנו שוקלים את הסיכוי לניחוש הבא, כמו במקרה הזה מילים, 

366
00:25:23,696 --> 00:25:27,900
מה שבאמת אכפת לנו הוא הציון הצפוי של המשחק שלנו אם נעשה את זה. 

367
00:25:28,230 --> 00:25:34,112
וכדי לחשב את הציון הצפוי הזה, אנחנו אומרים מה ההסתברות שמילים הן התשובה בפועל, 

368
00:25:34,112 --> 00:25:35,900
שכרגע היא מתארת לה 58%. 

369
00:25:36,040 --> 00:25:39,540
אנחנו אומרים שעם סיכוי של 58%, הציון שלנו במשחק הזה יהיה 4. 

370
00:25:40,320 --> 00:25:45,640
ואז עם הסתברות של 1 פחות 58%, הציון שלנו יהיה יותר מ-4. 

371
00:25:46,220 --> 00:25:49,340
כמה עוד אנחנו לא יודעים, אבל אנחנו יכולים להעריך את זה 

372
00:25:49,340 --> 00:25:52,460
על סמך כמה אי ודאות צפויה להיות ברגע שנגיע לנקודה הזו. 

373
00:25:52,960 --> 00:25:55,940
ספציפית, כרגע יש 1.44 פיסות של אי ודאות. 

374
00:25:56,440 --> 00:26:01,120
אם ננחש מילים, זה אומר לנו שהמידע הצפוי שנקבל הוא 1.27 ביטים. 

375
00:26:01,620 --> 00:26:07,660
אז אם ננחש מילים, ההבדל הזה מייצג כמה חוסר ודאות סביר שנישאר עם אחרי שזה יקרה. 

376
00:26:08,260 --> 00:26:11,466
מה שאנחנו צריכים זה איזושהי פונקציה, שאני קורא לה כאן, 

377
00:26:11,466 --> 00:26:13,740
שקושרת את אי הוודאות הזו עם ציון צפוי. 

378
00:26:14,240 --> 00:26:18,115
והדרך שבה זה התנהל הייתה פשוט לשרטט חבורה של נתונים ממשחקים 

379
00:26:18,115 --> 00:26:21,991
קודמים המבוססים על גרסה 1 של הבוט כדי לומר היי מה היה הציון 

380
00:26:21,991 --> 00:26:26,320
בפועל אחרי נקודות שונות עם כמויות מסוימות מאוד מדידות של אי ודאות. 

381
00:26:27,020 --> 00:26:32,689
לדוגמה, נקודות הנתונים האלה כאן שנמצאות מעל ערך שהוא בערך כמו 8.7 בערך אומרים על כמה 

382
00:26:32,689 --> 00:26:35,958
משחקים אחרי נקודה שבה היו 8.7 פיסות של אי ודאות, 

383
00:26:35,958 --> 00:26:38,960
נדרשו שני ניחושים כדי לקבל את התשובה הסופית. 

384
00:26:39,320 --> 00:26:42,240
למשחקים אחרים נדרשו שלושה ניחושים, למשחקים אחרים נדרשו ארבעה ניחושים. 

385
00:26:43,140 --> 00:26:48,947
אם נעבור שמאלה כאן, כל הנקודות מעל האפס אומרות בכל פעם שיש אפס פיסות של אי ודאות, 

386
00:26:48,947 --> 00:26:54,260
כלומר יש רק אפשרות אחת, אז מספר הניחושים הנדרש הוא תמיד רק אחד, וזה מרגיע. 

387
00:26:54,780 --> 00:26:58,868
בכל פעם שהייתה קצת אי ודאות, כלומר זה היה בעצם רק בשתי אפשרויות, 

388
00:26:58,868 --> 00:27:03,020
אז לפעמים זה דרש עוד ניחוש אחד, לפעמים זה דרש שני ניחושים נוספים. 

389
00:27:03,080 --> 00:27:05,240
וכן הלאה וכן הלאה כאן. 

390
00:27:05,740 --> 00:27:10,220
אולי דרך קצת יותר קלה לדמיין את הנתונים האלה היא לרכז אותם יחד ולקחת ממוצעים. 

391
00:27:11,000 --> 00:27:16,097
לדוגמה, הסרגל הזה אומר שבין כל הנקודות שבהן היה לנו קצת אי ודאות, 

392
00:27:16,097 --> 00:27:19,960
בממוצע מספר הניחושים החדשים הנדרשים היה בערך 1.5. 

393
00:27:22,140 --> 00:27:28,346
והסרגל כאן אומר בין כל המשחקים השונים שבו בשלב מסוים חוסר הוודאות היה קצת מעל ארבע ביטים, 

394
00:27:28,346 --> 00:27:32,621
שזה כמו לצמצם אותו ל-16 אפשרויות שונות, אז בממוצע זה דורש קצת 

395
00:27:32,621 --> 00:27:35,380
יותר משני ניחושים מאותה נקודה קָדִימָה. 

396
00:27:36,060 --> 00:27:39,460
ומכאן פשוט עשיתי רגרסיה כדי להתאים לפונקציה שנראתה סבירה לזה. 

397
00:27:39,980 --> 00:27:44,540
ותזכרו שכל העניין בעשיית כל זה הוא כדי שנוכל לכמת את האינטואיציה 

398
00:27:44,540 --> 00:27:48,960
הזו שככל שנשיג יותר מידע ממילה, כך הציון הצפוי יהיה נמוך יותר. 

399
00:27:49,680 --> 00:27:54,727
אז עם זה כגרסה 2.0, אם נחזור אחורה ונריץ את אותה סט של סימולציות, 

400
00:27:54,727 --> 00:27:59,240
כשזה ישחק מול כל 2315 תשובות המילה האפשריות, איך זה מסתדר? 

401
00:28:00,280 --> 00:28:03,420
ובכן בניגוד לגרסה הראשונה שלנו זה בהחלט טוב יותר, וזה מרגיע. 

402
00:28:04,020 --> 00:28:07,864
הכל אמר ועשה הממוצע הוא בסביבות 3.6, אם כי בניגוד לגרסה 

403
00:28:07,864 --> 00:28:12,120
הראשונה יש כמה פעמים שהיא מפסידה ודורשת יותר משש בנסיבות אלה. 

404
00:28:12,639 --> 00:28:17,940
ככל הנראה בגלל שיש מקרים שבהם זה עושה את הפשרה הזו ללכת על המטרה במקום למקסם את המידע. 

405
00:28:19,040 --> 00:28:21,000
אז אנחנו יכולים לעשות יותר טוב מ-3.6? 

406
00:28:22,080 --> 00:28:22,920
אנחנו בהחלט יכולים. 

407
00:28:23,280 --> 00:28:26,128
עכשיו אמרתי בהתחלה שהכי כיף לנסות לא לשלב את הרשימה 

408
00:28:26,128 --> 00:28:29,360
האמיתית של תשובות מילוליות בדרך שבה היא בונה את המודל שלה. 

409
00:28:29,880 --> 00:28:34,180
אבל אם נשלב את זה, הביצועים הטובים ביותר שיכולתי לקבל היו בסביבות 3.43. 

410
00:28:35,160 --> 00:28:38,705
אז אם ננסה להיות מתוחכמים יותר מסתם שימוש בנתוני תדירות מילים 

411
00:28:38,705 --> 00:28:43,624
כדי לבחור את ההפצה הקודמת הזו, זה 3.43 כנראה נותן מקסימום כמה טוב יכולנו להגיע עם זה, 

412
00:28:43,624 --> 00:28:45,740
או לפחות כמה טוב יכולתי להגיע עם זה. 

413
00:28:46,240 --> 00:28:50,281
הביצועים הטובים ביותר האלה בעצם רק משתמשים ברעיונות שדיברתי עליהם כאן, 

414
00:28:50,281 --> 00:28:55,120
אבל זה הולך קצת יותר רחוק, כאילו הוא מחפש את המידע הצפוי שני צעדים קדימה ולא רק אחד. 

415
00:28:55,620 --> 00:29:00,220
במקור תכננתי לדבר על זה יותר, אבל אני מבין שלמעשה עברנו די הרבה זמן כמו שזה. 

416
00:29:00,580 --> 00:29:04,764
הדבר היחיד שאני אגיד הוא לאחר ביצוע חיפוש דו-שלבי זה ולאחר מכן הפעלת כמה סימולציות 

417
00:29:04,764 --> 00:29:09,100
לדוגמא במועמדים המובילים, עד כה עבורי לפחות זה נראה כאילו קריין הוא הפותח הטוב ביותר. 

418
00:29:09,100 --> 00:29:10,060
מי היה מנחש? 

419
00:29:10,920 --> 00:29:14,923
כמו כן, אם אתה משתמש ברשימת המילים האמיתית כדי לקבוע את מרחב האפשרויות שלך, 

420
00:29:14,923 --> 00:29:17,820
אז אי הוודאות שאתה מתחיל איתה היא קצת יותר מ-11 ביטים. 

421
00:29:18,300 --> 00:29:21,832
ומסתבר, רק מחיפוש כוח גס, המידע המקסימלי האפשרי 

422
00:29:21,832 --> 00:29:25,880
הצפוי לאחר שני הניחושים הראשונים הוא בסביבות 10 ביטים. 

423
00:29:26,500 --> 00:29:30,877
מה שמרמז על התרחיש הטוב ביותר, לאחר שני הניחושים הראשונים שלך, 

424
00:29:30,877 --> 00:29:34,560
עם משחק אופטימלי לחלוטין, תישאר עם קצת אי ודאות אחת. 

425
00:29:34,800 --> 00:29:37,960
וזה אותו דבר כמו לרדת לשני ניחושים אפשריים. 

426
00:29:37,960 --> 00:29:43,007
אז אני חושב שזה הוגן וכנראה די שמרני לומר שלעולם לא תוכל לכתוב אלגוריתם שמקבל 

427
00:29:43,007 --> 00:29:46,307
את הממוצע הזה נמוך כמו 3, כי עם המילים הזמינות לך, 

428
00:29:46,307 --> 00:29:49,607
פשוט אין מקום לקבל מספיק מידע לאחר שני שלבים בלבד. 

429
00:29:49,607 --> 00:29:53,360
מסוגל להבטיח את התשובה במשבצת השלישית בכל פעם בלי להיכשל. 

