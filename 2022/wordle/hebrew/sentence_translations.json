[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy. ",
  "translatedText": "המשחק Wurdle הפך די ויראלי בחודש-חודשיים האחרונים, ואף פעם לא יתעלם מהזדמנות לשיעור מתמטיקה, עולה בדעתי שהמשחק הזה מהווה דוגמה מרכזית טובה מאוד בשיעור על תורת המידע, ובפרט נושא המכונה אנטרופיה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could. ",
  "translatedText": "אתה מבין, כמו הרבה אנשים קצת נשאבתי לתוך הפאזל, וכמו הרבה מתכנתים גם נשאבתי לנסות לכתוב אלגוריתם שישחק את המשחק הכי אופטימלי שהוא יכול. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy. ",
  "translatedText": "ומה שחשבתי לעשות כאן זה פשוט לדבר איתך על חלק מהתהליך שלי בזה, ולהסביר חלק מהמתמטיקה שנכנסה לזה, מכיוון שכל האלגוריתם מתרכז ברעיון הזה של אנטרופיה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle? ",
  "translatedText": "דבר ראשון, למקרה שלא שמעתם על זה, מה זה וורדל? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us. ",
  "translatedText": "וכדי להרוג כאן שתי ציפורים במכה אחת בזמן שאנחנו עוברים על כללי המשחק, הרשו לי גם לראות מקדימה לאן אנחנו הולכים עם זה, כלומר לפתח אלגוריתם קטן שבעצם ישחק את המשחק עבורנו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does. ",
  "translatedText": "למרות שלא עשיתי את הוורדל של היום, זה 4 בפברואר, ונראה איך הבוט יצליח. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess. ",
  "translatedText": "המטרה של Wurdle היא לנחש מילה מסתורית של חמש אותיות, וניתנות לך שש הזדמנויות שונות לנחש. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane. ",
  "translatedText": "לדוגמה, בוט הוורדל שלי מציע שאתחיל עם מנוף הניחוש. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer. ",
  "translatedText": "בכל פעם שאתה מנחש, אתה מקבל מידע על כמה קרוב הניחוש שלך לתשובה האמיתית. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer. ",
  "translatedText": "כאן התיבה האפורה אומרת לי שאין C בתשובה האמיתית. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position. ",
  "translatedText": "הקופסה הצהובה אומרת לי שיש R, אבל היא לא במצב הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position. ",
  "translatedText": "התיבה הירוקה אומרת לי שלמילה הסודית יש א', והיא נמצאת במיקום השלישי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E. ",
  "translatedText": "ואז אין N ואין E. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information. ",
  "translatedText": "אז תן לי פשוט להיכנס ולספר לבוט הוורדל את המידע הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey. ",
  "translatedText": "התחלנו עם מנוף, קיבלנו אפור, צהוב, ירוק, אפור, אפור. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time. ",
  "translatedText": "אל תדאג לגבי כל הנתונים שהוא מציג עכשיו, אני אסביר את זה בבוא העת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick. ",
  "translatedText": "אבל ההצעה העיקרית שלה לבחירה השנייה שלנו היא shtick. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess. ",
  "translatedText": "והניחוש שלך חייב להיות מילה אמיתית של חמש אותיות, אבל כפי שתראה, זה די ליברלי עם מה שהוא בעצם יאפשר לך לנחש. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick. ",
  "translatedText": "במקרה זה, אנו מנסים shtick. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good. ",
  "translatedText": "ובסדר, הדברים נראים די טוב. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R. ",
  "translatedText": "פגענו ב-S וב-H, אז אנחנו יודעים את שלוש האותיות הראשונות, אנחנו יודעים שיש R. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something. ",
  "translatedText": "וכך זה יהיה כמו ש.א. משהו R, או ש.א.ר משהו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp. ",
  "translatedText": "ונראה שהבוט של Wurdle יודע שזה תלוי רק בשתי אפשרויות, או רסיס או חד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard. ",
  "translatedText": "זה סוג של הטלה ביניהם בשלב זה, אז אני מניח שכנראה רק בגלל שהוא אלפביתי זה הולך עם רסיס. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer. ",
  "translatedText": "איזה הידד, היא התשובה האמיתית. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three. ",
  "translatedText": "אז קיבלנו את זה בשלושה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie. ",
  "translatedText": "אם אתה תוהה אם זה טוב, איך ששמעתי ביטוי של אדם אחד זה שעם Wurdle ארבע הוא ערך ושלוש הוא ציפורי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy. ",
  "translatedText": "שלדעתי היא אנלוגיה די הולמת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy. ",
  "translatedText": "אתה צריך להיות בעקביות במשחק שלך כדי להשיג ארבע, אבל זה בהחלט לא מטורף. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great. ",
  "translatedText": "אבל כשאתה מקבל את זה בשלוש, זה פשוט מרגיש נהדר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot. ",
  "translatedText": "אז אם אתה רוצה לעשות את זה, מה שאני רוצה לעשות כאן זה פשוט לדבר על תהליך החשיבה שלי מההתחלה איך אני ניגש לבוט Wurdle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson. ",
  "translatedText": "וכמו שאמרתי, באמת שזה תירוץ לשיעור תורת מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy. ",
  "translatedText": "המטרה העיקרית היא להסביר מהו מידע ומהי אנטרופיה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language. ",
  "translatedText": "המחשבה הראשונה שלי בגישה לזה הייתה להסתכל על התדרים היחסיים של אותיות שונות בשפה האנגלית. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters? ",
  "translatedText": "אז חשבתי, אוקיי, האם יש ניחוש פתיחה או צמד ניחושים פתיחה שפוגע בהרבה מהאותיות השכיחות ביותר? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails. ",
  "translatedText": "ואחד שדי אהבתי היה לעשות אחר ואחריו ציפורניים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good. ",
  "translatedText": "המחשבה היא שאם אתה מכה באות, אתה יודע, אתה מקבל ירוק או צהוב, זה תמיד מרגיש טוב. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information. ",
  "translatedText": "זה מרגיש כאילו אתה מקבל מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters. ",
  "translatedText": "אבל במקרים אלה, גם אם אתה לא מכה ואתה תמיד מקבל אפור, זה עדיין נותן לך מידע רב מכיוון שזה די נדיר למצוא מילה שאין בה אף אחת מהאותיות הללו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters. ",
  "translatedText": "אבל אפילו עדיין, זה לא מרגיש סופר שיטתי, כי למשל, זה לא עושה כלום כדי להתחשב בסדר האותיות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail? ",
  "translatedText": "למה להקליד ציפורניים כשאני יכול להקליד חילזון? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end? ",
  "translatedText": "האם עדיף לקבל את ה-S הזה בסוף? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure. ",
  "translatedText": "אני לא ממש בטוח. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y. ",
  "translatedText": "עכשיו, חבר שלי אמר שהוא אהב לפתוח במילה עייף, מה שדי הפתיע אותי כי יש בה כמה אותיות לא שכיחות כמו ה-W וה-Y. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener. ",
  "translatedText": "אבל מי יודע, אולי זו פתיחה טובה יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess? ",
  "translatedText": "האם יש איזשהו ציון כמותי שאנחנו יכולים לתת כדי לשפוט את האיכות של ניחוש פוטנציאלי? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up. ",
  "translatedText": "עכשיו כדי להגדיר את הדרך שבה אנחנו הולכים לדרג ניחושים אפשריים, בואו נחזור ונוסיף קצת בהירות כיצד בדיוק המשחק מוגדר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long. ",
  "translatedText": "אז יש רשימה של מילים שזה יאפשר לך להזין שנחשבות לניחושים תקפים שאורכה רק כ-13,000 מילים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble. ",
  "translatedText": "אבל כשמסתכלים על זה, יש הרבה דברים ממש לא שכיחים, דברים כמו ראש או עלי ו-ARG, מסוג המילים שמביאות לוויכוחים משפחתיים במשחק של Scrabble. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word. ",
  "translatedText": "אבל האווירה של המשחק היא שהתשובה תמיד תהיה מילה נפוצה בהחלט. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers. ",
  "translatedText": "ולמעשה, יש עוד רשימה של כ-2300 מילים שהן התשובות האפשריות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun. ",
  "translatedText": "וזו רשימה שנאספה על ידי אדם, אני חושב במיוחד על ידי חברתו של יוצר המשחק, וזה די כיף. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list. ",
  "translatedText": "אבל מה שהייתי רוצה לעשות, האתגר שלנו עבור הפרויקט הזה הוא לראות אם נוכל לכתוב תוכנית לפתרון Wordle שאינה משלבת ידע קודם על רשימה זו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list. ",
  "translatedText": "ראשית, יש הרבה מילים נפוצות למדי של חמש אותיות שלא תמצאו ברשימה הזו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website. ",
  "translatedText": "אז עדיף לכתוב תוכנית שהיא קצת יותר עמידה ותשחק וורדל נגד כל אחד, לא רק מה שבמקרה הוא האתר הרשמי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code. ",
  "translatedText": "וגם הסיבה שאנחנו יודעים מהי רשימה זו של תשובות אפשריות, היא בגלל שהיא גלויה בקוד המקור. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day. ",
  "translatedText": "אבל האופן שבו זה נראה בקוד המקור הוא בסדר הספציפי שבו התשובות עולות מיום ליום. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be. ",
  "translatedText": "אז אתה תמיד יכול פשוט לחפש מה תהיה התשובה של מחר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating. ",
  "translatedText": "אז ברור, יש מובן מסוים שהשימוש ברשימה הוא רמאות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words. ",
  "translatedText": "ומה שעושה חידה מעניינת יותר ושיעור תיאוריית מידע עשיר יותר הוא להשתמש במקום בכמה נתונים אוניברסליים יותר כמו תדרים יחסיים של מילים באופן כללי כדי ללכוד את האינטואיציה הזו של העדפה למילים נפוצות יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess? ",
  "translatedText": "אז מבין 13,000 האפשרויות הללו, איך עלינו לבחור את ניחוש הפתיחה? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality? ",
  "translatedText": "לדוגמה, אם חבר שלי מציע נישואים עייפים, כיצד עלינו לנתח את איכותו? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W. ",
  "translatedText": "ובכן, הסיבה שהוא אמר שהוא אוהב את ה-W הלא סביר הזה היא שהוא אוהב את אופי ה-long shot של כמה טוב זה מרגיש אם תפגע ב-W הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern. ",
  "translatedText": "לדוגמה, אם הדפוס הראשון שנחשף היה משהו כזה, אז מסתבר שיש רק 58 מילים בלקסיקון הענק הזה שתואמות את הדפוס הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000. ",
  "translatedText": "אז זה הפחתה עצומה מ-13,000. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this. ",
  "translatedText": "אבל הצד השני של זה, כמובן, הוא שזה מאוד נדיר לקבל דפוס כזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000. ",
  "translatedText": "באופן ספציפי, אם כל מילה הייתה בעלת סבירות שווה להיות התשובה, ההסתברות לפגיעה בתבנית זו תהיה 58 חלקי בערך 13,000. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers. ",
  "translatedText": "כמובן, לא סביר להניח שהם יהיו תשובות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words. ",
  "translatedText": "רוב אלו הן מילים מאוד לא ברורות ואפילו מפוקפקות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later. ",
  "translatedText": "אבל לפחות עבור המעבר הראשון שלנו בכל זה, בואו נניח שכולן סבירות באותה מידה ואז נחדד את זה קצת מאוחר יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur. ",
  "translatedText": "הנקודה היא שהדפוס עם הרבה מידע מעצם טבעו לא סביר שיתרחש. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely. ",
  "translatedText": "למעשה, המשמעות של להיות אינפורמטיבי הוא שזה לא סביר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it. ",
  "translatedText": "דפוס הרבה יותר סביר לראות עם הפתיחה הזו יהיה משהו כזה, שבו כמובן אין בו W. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y. ",
  "translatedText": "אולי יש E, ואולי אין A, אין R, אין Y. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches. ",
  "translatedText": "במקרה זה, יש 1400 התאמות אפשריות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see. ",
  "translatedText": "אם כולם היו סבירים באותה מידה, מסתבר שהסתברות של כ-11% היא זו הדפוס שהיית רואה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative. ",
  "translatedText": "אז התוצאות הסבירות ביותר הן גם הפחות אינפורמטיביות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see. ",
  "translatedText": "כדי לקבל תצוגה גלובלית יותר כאן, הרשו לי להראות לכם את ההתפלגות המלאה של ההסתברויות על פני כל הדפוסים השונים שאתם עשויים לראות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common. ",
  "translatedText": "אז כל פס שאתה מסתכל עליו מתאים לתבנית אפשרית של צבעים שאפשר לחשוף, מתוכם יש 3 עד 5 אפשרויות, והם מאורגנים משמאל לימין, הנפוצים ביותר עד הפחות נפוצים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays. ",
  "translatedText": "אז האפשרות הנפוצה ביותר כאן היא שאתה מקבל את כל האפורים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time. ",
  "translatedText": "זה קורה בערך 14% מהמקרים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this. ",
  "translatedText": "ומה שאתה מייחל לו כשאתה מנחש זה שאתה בסופו של דבר איפשהו בזנב הארוך הזה, כמו כאן, שם יש רק 18 אפשרויות למה שתואם את הדפוס הזה שכנראה נראה כך. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here. ",
  "translatedText": "או אם נצא קצת יותר שמאלה, אתה יודע, אולי נלך עד לכאן. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you. ",
  "translatedText": "אוקיי, הנה חידה טובה בשבילך. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them? ",
  "translatedText": "מהן שלוש המילים בשפה האנגלית שמתחילות ב-W, מסתיימות ב-Y, ויש בהן R איפשהו? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly. ",
  "translatedText": "מסתבר שהתשובות הן, בוא נראה, מלאות מילים, תולעות ומתפתלות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution. ",
  "translatedText": "אז כדי לשפוט עד כמה המילה הזו טובה בסך הכל, אנחנו רוצים איזושהי מידה של כמות המידע הצפויה שתקבלו מההפצה הזו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score. ",
  "translatedText": "אם נעבור על כל דפוס ונכפיל את ההסתברות שלו להתרחש פעמים משהו שמודד כמה הוא אינפורמטיבי, זה יכול אולי לתת לנו ציון אובייקטיבי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches. ",
  "translatedText": "עכשיו האינסטינקט הראשון שלך לגבי מה המשהו הזה צריך להיות עשוי להיות מספר ההתאמות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches. ",
  "translatedText": "אתה רוצה מספר ממוצע נמוך יותר של התאמות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer. ",
  "translatedText": "אבל במקום זאת, הייתי רוצה להשתמש במדידה אוניברסלית יותר שלעתים קרובות אנו מייחסים למידע, וכזו שתהיה גמישה יותר ברגע שתוקצו לנו הסתברות שונה לכל אחת מ-13,000 המילים הללו אם הן באמת התשובה או לא. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples. ",
  "translatedText": "יחידת המידע הסטנדרטית היא ה-bit, שיש לה נוסחה קצת מצחיקה, אבל היא ממש אינטואיטיבית אם רק נסתכל על דוגמאות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information. ",
  "translatedText": "אם יש לך תצפית שחותכת את מרחב האפשרויות שלך לחצי, אנו אומרים שיש לה פיסת מידע אחת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half. ",
  "translatedText": "בדוגמה שלנו, מרחב האפשרויות הוא כל המילים האפשריות, ומסתבר שבערך למחצית ממילות חמש האותיות יש S, קצת פחות מזה, אבל בערך חצי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information. ",
  "translatedText": "אז התצפית הזו תיתן לך קצת מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information. ",
  "translatedText": "אם במקום זאת עובדה חדשה מקצצת את מרחב האפשרויות הזה בפקטור של ארבע, אנו אומרים שיש לה שתי פיסות מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T. ",
  "translatedText": "לדוגמה, מסתבר שלכרבע מהמילים הללו יש T. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth. ",
  "translatedText": "אם התצפית חותכת את החלל הזה בפקטור של שמונה, אנחנו אומרים שזה שלוש פיסות מידע, וכן הלאה וכן הלאה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd. ",
  "translatedText": "ארבעה ביטים חותכים אותו ל-16, חמישה ביטים חותכים אותו ל-32. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence? ",
  "translatedText": "אז עכשיו אולי תרצו לעצור ולשאול את עצמכם, מהי הנוסחה למידע עבור מספר הביטים מבחינת ההסתברות להתרחשות? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability. ",
  "translatedText": "מה שאנחנו אומרים כאן הוא שכאשר אתה לוקח חצי אחד למספר הסיביות, זה אותו דבר כמו ההסתברות, שזה אותו דבר כמו לומר שניים בחזקת מספר הסיביות הוא אחד מעל ההסתברות, אשר מסדר מחדש בהמשך לומר שהמידע הוא בסיס היומן שניים מתוך אחד חלקי ההסתברות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability. ",
  "translatedText": "ולפעמים אתה רואה את זה עם עוד סידור מחדש אחד, כאשר המידע הוא בסיס היומן השלילי שני של ההסתברות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half. ",
  "translatedText": "בביטוי כך, זה יכול להיראות קצת מוזר למי שלא יודע מה, אבל זה באמת רק הרעיון האינטואיטיבי של לשאול כמה פעמים צמצמת את האפשרויות שלך בחצי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture? ",
  "translatedText": "עכשיו אם אתה תוהה, אתה יודע, חשבתי שאנחנו סתם משחקים משחק מילים מהנה, למה לוגריתמים נכנסים לתמונה? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095. ",
  "translatedText": "אחת הסיבות לכך שזו יחידה נחמדה יותר היא שפשוט הרבה יותר קל לדבר על אירועים מאוד לא סבירים, הרבה יותר קל לומר שלתצפית יש 20 סיביות מידע מאשר לומר שההסתברות להתרחשות כאלה ואחרים היא 0.0000095. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together. ",
  "translatedText": "אבל סיבה מהותית יותר לכך שהביטוי הלוגריתמי הזה התברר כתוספת שימושית מאוד לתורת ההסתברות היא הדרך שבה מידע מתחבר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information. ",
  "translatedText": "לדוגמה, אם תצפית אחת נותנת לך שתי פיסות מידע, מקצצת את השטח שלך בארבע, ואז תצפית שנייה כמו הניחוש השני שלך ב-Wordle נותנת לך עוד שלוש פיסות מידע, ומצמצמת אותך עוד יותר בפקטור של שמונה, שניים ביחד נותנים לך חמש פיסות מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add. ",
  "translatedText": "באותו אופן שבו הסתברויות אוהבות להכפיל, מידע אוהב להוסיף. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with. ",
  "translatedText": "אז ברגע שאנחנו נמצאים בתחום של משהו כמו ערך צפוי, שבו אנחנו מוסיפים חבורה של מספרים, היומנים הופכים את זה להרבה יותר נחמד להתמודד איתו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern. ",
  "translatedText": "בואו נחזור להפצה שלנו עבור Weary ונוסיף עוד עוקב קטן כאן, שמראה לנו כמה מידע יש לכל דפוס. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain. ",
  "translatedText": "הדבר העיקרי שאני רוצה שתשים לב הוא שככל שההסתברות גבוהה יותר כשנגיע לדפוסים הסבירים האלה, כך המידע נמוך יותר, כך אתה מרוויח פחות ביטים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get. ",
  "translatedText": "הדרך שבה אנו מודדים את האיכות של הניחוש הזה תהיה לקחת את הערך הצפוי של המידע הזה, שם אנחנו עוברים על כל דפוס, אנחנו אומרים כמה זה סביר, ואז נכפיל את זה בכמה סיביות מידע נקבל. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits. ",
  "translatedText": "ובדוגמה של Weary, מסתבר שזה 4.9 ביטים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times. ",
  "translatedText": "אז בממוצע, המידע שאתה מקבל מניחוש הפתיחה הזה טוב כמו לחתוך את מרחב האפשרויות שלך לחצי בערך חמש פעמים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate. ",
  "translatedText": "לעומת זאת, דוגמה לניחוש עם ערך מידע צפוי גבוה יותר תהיה משהו כמו Slate. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter. ",
  "translatedText": "במקרה זה תבחין שההפצה נראית הרבה יותר שטוחה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information. ",
  "translatedText": "בפרט, להתרחשות הסבירה ביותר של כל האפורים יש רק סיכוי של כ-6% להתרחש, כך שלפחות אתה מקבל כנראה 3.9 סיביות מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that. ",
  "translatedText": "אבל זה מינימום, בדרך כלל תקבל משהו יותר טוב מזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8. ",
  "translatedText": "ומסתבר שכאשר אתה מכתש את המספרים על זה ומחבר את כל המונחים הרלוונטיים, המידע הממוצע הוא בערך 5.8. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average. ",
  "translatedText": "אז בניגוד ל-Weary, מרחב האפשרויות שלך יהיה גדול בערך בחצי לאחר הניחוש הראשון הזה, בממוצע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity. ",
  "translatedText": "למעשה יש סיפור מהנה על השם של הערך הצפוי הזה של כמות מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science. ",
  "translatedText": "תורת המידע פותחה על ידי קלוד שאנון, שעבד ב-Bell Labs בשנות ה-40, אבל הוא דיבר על כמה מהרעיונות שלו שטרם פורסמו עם ג'ון פון נוימן, שהיה הענק האינטלקטואלי הזה של אותה תקופה, בולט מאוד. במתמטיקה ובפיזיקה ותחילתו של מה שהפך למדעי המחשב. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons. ",
  "translatedText": "וכשהזכיר שאין לו שם טוב לערך הצפוי הזה של כמות מידע, כביכול פון נוימן אמר, אז הסיפור אומר, ובכן כדאי לקרוא לזה אנטרופיה, ומשתי סיבות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage. ",
  "translatedText": "מלכתחילה, פונקציית אי הוודאות שלך שימשה במכניקה סטטיסטית תחת השם הזה, אז כבר יש לה שם, ובמקום השני, ויותר חשוב, אף אחד לא יודע מהי באמת אנטרופיה, אז בוויכוח אתה תמיד יש את היתרון. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design. ",
  "translatedText": "אז אם השם נראה קצת מסתורי, ואם אפשר להאמין לסיפור הזה, זה סוג של תכנון. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess. ",
  "translatedText": "כמו כן, אם אתה תוהה לגבי הקשר שלו לכל החומר השני של החוק השני של התרמודינמיקה מהפיזיקה, בהחלט יש קשר, אבל במקורותיו שאנון עסק רק בתורת ההסתברות הטהורה, ולמטרותינו כאן, כשאני משתמש ב- אנטרופיה של מילים, אני רק רוצה שתחשוב על ערך המידע הצפוי של ניחוש מסוים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously. ",
  "translatedText": "אתה יכול לחשוב על אנטרופיה כמדידת שני דברים בו זמנית. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution. ",
  "translatedText": "הראשון הוא עד כמה שטוחה ההתפלגות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be. ",
  "translatedText": "ככל שהתפלגות קרובה יותר לאחידות, כך האנטרופיה תהיה גבוהה יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy. ",
  "translatedText": "במקרה שלנו, כאשר יש 3 עד ה-5 דפוסים הכוללים, עבור התפלגות אחידה, צפייה בכל אחד מהם תהיה בסיס יומן מידע 2 מתוך 3 עד ה-5, שהוא במקרה 7.92, אז זה המקסימום המוחלט שיכול להיות לך עבור האנטרופיה הזו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place. ",
  "translatedText": "אבל אנטרופיה היא גם סוג של מדד לכמה אפשרויות יש מלכתחילה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits. ",
  "translatedText": "לדוגמה, אם במקרה יש לך מילה כלשהי שבה יש רק 16 דפוסים אפשריים, וכל אחת מהן בסבירות שווה, האנטרופיה הזו, המידע הצפוי הזה, תהיה 4 סיביות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits. ",
  "translatedText": "אבל אם יש לך מילה אחרת שבה יש 64 דפוסים אפשריים שיכולים להופיע, וכולם סבירים באותה מידה, אז האנטרופיה תסתבר להיות 6 סיביות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes. ",
  "translatedText": "אז אם אתה רואה איזושהי התפלגות בטבע שיש לה אנטרופיה של 6 ביטים, זה בערך כאילו זה אומר שיש שונות וחוסר ודאות במה שעומד לקרות כאילו היו 64 תוצאות סבירות באותה מידה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this. ",
  "translatedText": "עבור המעבר הראשון שלי ב-Wurtelebot, בעצם הייתי צריך לעשות את זה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible. ",
  "translatedText": "הוא עובר על כל הניחושים האפשריים שיכולים להיות לך, כל 13,000 המילים, מחשב את האנטרופיה עבור כל אחת, או ליתר דיוק, את האנטרופיה של ההתפלגות על פני כל הדפוסים שאתה עשוי לראות, עבור כל אחת, ובוחר את הגבוהה ביותר, מכיוון שזהו זה שצפוי לקצץ את מרחב האפשרויות שלך ככל האפשר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses. ",
  "translatedText": "ולמרות שדיברתי רק על הניחוש הראשון כאן, זה עושה את אותו הדבר עבור הניחושים הבאים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words. ",
  "translatedText": "לדוגמה, לאחר שתראה דפוס כלשהו בניחוש הראשון הזה, שיגביל אותך למספר קטן יותר של מילים אפשריות בהתבסס על מה שמתאים לזה, אתה פשוט משחק באותו משחק ביחס לאותה קבוצה קטנה יותר של מילים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy. ",
  "translatedText": "לניחוש שני מוצע, אתה מסתכל על ההתפלגות של כל הדפוסים שיכולים להתרחש מאותה קבוצה מוגבלת יותר של מילים, אתה מחפש בכל 13,000 האפשרויות, ומוצא את זו שממקסמת את האנטרופיה הזו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins. ",
  "translatedText": "כדי להראות לכם איך זה עובד בפעולה, הרשו לי רק להעלות גרסה קטנה של Wurtele שכתבתי שמראה את הדגשים של הניתוח הזה בשוליים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information. ",
  "translatedText": "לאחר ביצוע כל חישובי האנטרופיה שלו, מימין כאן הוא מראה לנו למי מהם יש את המידע הצפוי הגבוה ביותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch. ",
  "translatedText": "מסתבר שהתשובה העליונה, לפחות כרגע, נחדד את זה בהמשך, היא טארס, שפירושו, אממ, כמובן, בקיה, הקיבה הנפוצה ביותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern. ",
  "translatedText": "בכל פעם שאנחנו מנחשים כאן, איפה אולי אני קצת מתעלם מההמלצות שלו והולך עם צפחה, כי אני אוהב צפחה, אנחנו יכולים לראות כמה מידע צפוי היה לו, אבל אז בצד ימין של המילה כאן זה מראה לנו כמה מידע אמיתי שקיבלנו, בהתחשב בדפוס הספציפי הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that. ",
  "translatedText": "אז כאן זה נראה כאילו היה לנו קצת חסר מזל, היה צפוי לנו לקבל 5.8, אבל במקרה קיבלנו משהו עם פחות מזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now. ",
  "translatedText": "ואז בצד שמאל כאן זה מראה לכולנו את המילים האפשריות השונות שבהן אנחנו נמצאים עכשיו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment. ",
  "translatedText": "הפסים הכחולים אומרים לנו עד כמה הוא חושב שכל מילה היא, אז כרגע זה מניח שכל מילה בסבירות שווה להתרחש, אבל אנחנו נחדד את זה בעוד רגע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities. ",
  "translatedText": "ואז מדידת אי הוודאות הזו אומרת לנו את האנטרופיה של ההתפלגות הזו על פני המילים האפשריות, שכרגע, מכיוון שזו התפלגות אחידה, היא רק דרך מסובכת מיותרת לספור את מספר האפשרויות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities. ",
  "translatedText": "לדוגמה, אם היינו לוקחים 2 בחזקת 13.66, זה אמור להיות בסביבות 13,000 האפשרויות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places. ",
  "translatedText": "אני קצת בחוץ כאן, אבל רק בגלל שאני לא מראה את כל האותיות העשרוניות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute. ",
  "translatedText": "כרגע זה עשוי להרגיש מיותר וכאילו זה מסבך דברים מדי, אבל אתה תראה למה זה שימושי להחזיק את שני המספרים תוך דקה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word. ",
  "translatedText": "אז כאן זה נראה כאילו זה מרמז על האנטרופיה הגבוהה ביותר עבור הניחוש השני שלנו הוא ראמן, ששוב ממש לא מרגיש כמו מילה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains. ",
  "translatedText": "אז כדי לקחת את הרמה המוסרית כאן, אני מתכוון להמשיך ולהקליד את Rains. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky. ",
  "translatedText": "ושוב זה נראה כאילו היה לנו קצת חסר מזל. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information. ",
  "translatedText": "ציפינו ל-4.3 ביטים וקיבלנו רק 3.39 סיביות מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities. ",
  "translatedText": "אז זה מוריד אותנו ל-55 אפשרויות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means. ",
  "translatedText": "וכאן אולי אני פשוט אלך עם מה שזה מציע, שזה שילוב, מה שזה לא אומר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle. ",
  "translatedText": "ובסדר, זו בעצם הזדמנות טובה לפאזל. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information. ",
  "translatedText": "זה אומר לנו שהדפוס הזה נותן לנו 4.7 סיביות מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty. ",
  "translatedText": "אבל בצד שמאל, לפני שאנחנו רואים את הדפוס הזה, היו 5.78 פיסות של אי ודאות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities? ",
  "translatedText": "אז בתור חידון בשבילך, מה זה אומר לגבי מספר האפשרויות שנותרו? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers. ",
  "translatedText": "ובכן, זה אומר שאנחנו מצטמצמים לחלק אחד של אי ודאות, וזה אותו דבר כמו לומר שיש שתי תשובות אפשריות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice. ",
  "translatedText": "זו בחירה של 50-50. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss. ",
  "translatedText": "ומכאן, בגלל שאתה ואני יודעים אילו מילים נפוצות יותר, אנחנו יודעים שהתשובה צריכה להיות תהום. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that. ",
  "translatedText": "אבל כמו שזה נכתב עכשיו, התוכנית לא יודעת את זה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it. ",
  "translatedText": "אז זה פשוט ממשיך, מנסה להשיג כמה שיותר מידע, עד שנותרה רק אפשרות אחת, ואז הוא מנחש את זה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy. ",
  "translatedText": "אז ברור שאנחנו צריכים אסטרטגיית סוף משחק טובה יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does. ",
  "translatedText": "אבל נניח שאנחנו קוראים לגרסה הזו אחת מפותרי המילים שלנו, ואז אנחנו הולכים ומריצים כמה סימולציות כדי לראות איך זה עובד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game. ",
  "translatedText": "אז הדרך שבה זה עובד היא שהוא משחק בכל משחק מילים אפשרי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers. ",
  "translatedText": "זה עובר על כל 2315 המילים האלה שהן התשובות המילוליות בפועל. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set. ",
  "translatedText": "זה בעצם משתמש בזה כמערכת בדיקות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice. ",
  "translatedText": "ועם השיטה הנאיבית הזו של לא להתחשב עד כמה מילה נפוצה, ורק לנסות למקסם את המידע בכל שלב בדרך, עד שהוא מגיע לבחירה אחת ויחידה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124. ",
  "translatedText": "בסוף הסימולציה, הציון הממוצע מתברר כ-4.124. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse. ",
  "translatedText": "וזה לא רע, למען האמת, די ציפיתי לעשות יותר גרוע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4. ",
  "translatedText": "אבל האנשים שמנגנים wordle יגידו לך שבדרך כלל הם יכולים להשיג את זה ב-4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can. ",
  "translatedText": "האתגר האמיתי הוא להשיג כמה שיותר ב-3. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3. ",
  "translatedText": "זה קפיצה די גדולה בין הציון 4 לציון 3. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that. ",
  "translatedText": "הפרי התלוי הנמוך כאן הוא איכשהו לשלב האם מילה נפוצה או לא, ואיך בדיוק אנחנו עושים את זה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language. ",
  "translatedText": "הדרך שבה ניגשתי היא לקבל רשימה של התדרים היחסיים של כל המילים בשפה האנגלית. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset. ",
  "translatedText": "והרגע השתמשתי בפונקציית נתוני תדירות המילים של Mathematica, שבעצמה שואבת ממאגר הנתונים הציבורי של Google Books English Ngram. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words. ",
  "translatedText": "וזה די כיף להסתכל עליו, למשל אם נמיין את זה מהמילים הנפוצות ביותר למילים הפחות נפוצות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language. ",
  "translatedText": "ברור שאלו הן המילים הנפוצות ביותר, 5 אותיות בשפה האנגלית. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common. ",
  "translatedText": "או ליתר דיוק, אלו הם ה-8 הנפוצים ביותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there. ",
  "translatedText": "ראשון זה איזה, אחרי זה יש שם ושם. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common. ",
  "translatedText": "הראשון עצמו הוא לא הראשון, אלא התשיעי, והגיוני שהמילים האחרות הללו יכולות להופיע לעתים קרובות יותר, כאשר אלה שאחרי הראשונים הם אחרי, איפה, ואלו רק קצת פחות נפוצות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency. ",
  "translatedText": "כעת, בשימוש בנתונים האלה כדי להדגים את הסבירות שכל אחת מהמילים הללו תהיה התשובה הסופית, היא לא צריכה להיות רק פרופורציונלית לתדירות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely. ",
  "translatedText": "למשל, שניתן לו ציון 0.002 במערך הנתונים הזה, בעוד שהמילה צמה היא בסבירות מסוימת פי 1000 פחות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering. ",
  "translatedText": "אבל שתיהן מילים מספיק נפוצות שכדאי לשקול אותן. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff. ",
  "translatedText": "אז אנחנו רוצים יותר חתך בינארי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty. ",
  "translatedText": "הדרך שבה הלכתי היא לדמיין לקחת את כל הרשימה הממוינת הזו של מילים, ואז לסדר אותה על ציר x, ואז להחיל את הפונקציה הסיגמואידית, שהיא הדרך הסטנדרטית לקבל פונקציה שהפלט שלה הוא בעצם בינארי, זה או 0 או שזה 1, אבל יש החלקה ביניהם לאזור זה של אי ודאות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis. ",
  "translatedText": "אז בעצם, ההסתברות שאני מקצה לכל מילה להימצאות ברשימה הסופית תהיה הערך של הפונקציה הסיגמואידית למעלה בכל מקום שבו היא ממוקמת על ציר ה-x. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff. ",
  "translatedText": "עכשיו ברור שזה תלוי בכמה פרמטרים, למשל כמה רחב הרווח בציר ה-x ממלאות המילים האלה קובע באיזו הדרגתית או תלולה אנחנו יורדים מ-1 ל-0, והמקום שבו אנחנו ממקמים אותן משמאל לימין קובע את החתך. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind. ",
  "translatedText": "למען האמת, הדרך שעשיתי את זה הייתה פשוט ללקק את האצבע שלי ולהכניס אותה לרוח. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff. ",
  "translatedText": "עיינתי ברשימה הממוינת וניסיתי למצוא חלון שבו כשהסתכלתי עליו הבנתי שכמחצית מהמילים האלה צפויות יותר מאשר לא להיות התשובה הסופית, והשתמשתי בזה בתור החתך. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement. ",
  "translatedText": "ברגע שיש לנו התפלגות כזו בין המילים, זה נותן לנו מצב נוסף שבו האנטרופיה הופכת למדידה ממש שימושית זו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it. ",
  "translatedText": "לדוגמה, נניח ששיחקנו משחק ונתחיל עם הפותחים הישנים שלי, שהיו נוצה ומסמרים, ובסופו של דבר יש מצב שיש ארבע מילים אפשריות שמתאימות לזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely. ",
  "translatedText": "ונניח שאנו רואים בכולם סבירים באותה מידה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution? ",
  "translatedText": "הרשו לי לשאול אתכם, מהי האנטרופיה של התפלגות זו? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2. ",
  "translatedText": "ובכן, המידע הקשור לכל אחת מהאפשרויות הללו יהיה בסיס היומן 2 מתוך 4, מכיוון שכל אחד מהם הוא 1 ו-4, וזה 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities. ",
  "translatedText": "שתי פיסות מידע, ארבע אפשרויות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good. ",
  "translatedText": "הכל טוב מאוד וטוב. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches? ",
  "translatedText": "אבל מה אם אגיד לך שבעצם יש יותר מארבע גפרורים? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it. ",
  "translatedText": "במציאות, כשאנחנו מסתכלים ברשימת המילים המלאה, יש 16 מילים שמתאימות לה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure. ",
  "translatedText": "אבל נניח שהמודל שלנו שם סבירות ממש נמוכה ל-12 המילים האחרות האלה להיות למעשה התשובה הסופית, משהו כמו 1 ל-1000 כי הן ממש לא ברורות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution? ",
  "translatedText": "עכשיו הרשו לי לשאול אתכם, מהי האנטרופיה של התפלגות זו? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before. ",
  "translatedText": "אם האנטרופיה מדדה אך ורק את מספר ההתאמות כאן, אז אתה עשוי לצפות שזה יהיה משהו כמו בסיס היומן 2 מתוך 16, שיהיה 4, שתי פיסות יותר של אי ודאות ממה שהיה לנו קודם. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before. ",
  "translatedText": "אבל כמובן שאי הוודאות בפועל לא באמת שונה ממה שהיה לנו קודם. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example. ",
  "translatedText": "רק בגלל שיש את 12 המילים המעורפלות האלה לא אומר שזה יהיה כל כך מפתיע ללמוד שהתשובה הסופית היא קסם, למשל. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits. ",
  "translatedText": "אז כאשר אתה באמת עושה את החישוב כאן, ואתה מחבר את ההסתברות של כל התרחשות כפול המידע המתאים, מה שאתה מקבל הוא 2.11 ביטים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it. ",
  "translatedText": "אני רק אומר, זה בעצם שני ביטים, בעצם ארבע האפשרויות האלה, אבל יש קצת יותר אי ודאות בגלל כל האירועים הבלתי סבירים האלה, אם כי אם תלמד אותם היית מקבל מזה המון מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson. ",
  "translatedText": "אז בהתקרבות, זה חלק ממה שהופך את Wordle לדוגמא כל כך נחמדה לשיעור תורת מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy. ",
  "translatedText": "יש לנו את שני יישומי ההרגשה המובהקים הללו עבור אנטרופיה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible. ",
  "translatedText": "הראשון אומר לנו מה המידע הצפוי שנקבל מניחוש נתון, והשני אומר האם נוכל למדוד את אי הוודאות שנותרה בין כל המילים האפשריות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation. ",
  "translatedText": "ואני צריך להדגיש, במקרה הראשון שבו אנחנו מסתכלים על המידע הצפוי של ניחוש, ברגע שיש לנו שקלול לא שווה למילים, זה משפיע על חישוב האנטרופיה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words. ",
  "translatedText": "לדוגמה, הרשו לי להעלות את אותו מקרה שבדקנו קודם לכן של התפלגות הקשורה ל-Weary, אבל הפעם באמצעות התפלגות לא אחידה על פני כל המילים האפשריות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well. ",
  "translatedText": "אז תן לי לראות אם אני יכול למצוא כאן חלק שממחיש את זה די טוב. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good. ",
  "translatedText": "אוקיי, כאן זה די טוב. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it. ",
  "translatedText": "כאן יש לנו שני דפוסים צמודים שסבירים בערך באותה מידה, אבל לאחד מהם נאמר לנו יש 32 מילים אפשריות שתואמות לה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them. ",
  "translatedText": "ואם נבדוק מה הם, אלה 32 אלה, שכולן פשוט מילים מאוד לא סבירות כשאתה סורק את העיניים שלך מעליהן. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely. ",
  "translatedText": "קשה למצוא כאלה שמרגישות כמו תשובות סבירות, אולי צעקות, אבל אם נסתכל על התבנית השכנה בהתפלגות, שנחשבת כסבירה בערך, נאמר לנו שיש לה רק 8 התאמות אפשריות, אז רבע התאמות רבות, אבל זה סביר בערך. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why. ",
  "translatedText": "וכשאנחנו מוציאים את הגפרורים האלה, אנחנו יכולים לראות למה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps. ",
  "translatedText": "חלק מהן תשובות סבירות ממש, כמו צלצול, או זעם, או ראפ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw. ",
  "translatedText": "כדי להמחיש כיצד אנו משלבים את כל זה, הרשו לי להעלות כאן את גרסה 2 של ה-Wordlebot, ויש שניים או שלושה הבדלים עיקריים מהראשון שראינו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer. ",
  "translatedText": "ראשית, כמו שאמרתי זה עתה, הדרך שבה אנו מחשבים את האנטרופיות הללו, הערכים הצפויים של המידע, משתמשת כעת בהתפלגות המעודנות יותר על פני הדפוסים שמשלבת את ההסתברות שמילה נתונה תהיה למעשה התשובה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different. ",
  "translatedText": "כפי שזה קורה, הדמעות עדיין מספר 1, אם כי אלה שאחריו קצת שונים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table. ",
  "translatedText": "שנית, כאשר היא תדרג את הבחירות המובילות שלה, היא עתידה לשמור מודל של ההסתברות שכל מילה היא התשובה האמיתית, והיא תשלב זאת בהחלטה שלה, שקל יותר לראות אותה ברגע שיש לנו כמה ניחושים על שולחן. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives. ",
  "translatedText": "שוב, מתעלמים מהמלצתו כי אנחנו לא יכולים לתת למכונות לשלוט בחיינו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches. ",
  "translatedText": "ואני מניח שעלי להזכיר עוד דבר שונה כאן משמאל, שערך אי הוודאות, מספר הביטים הזה, כבר לא רק מיותר עם מספר ההתאמות האפשריות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes. ",
  "translatedText": "עכשיו אם נמשוך אותו למעלה ונחשב 2 ל-8.02, שזה קצת מעל 256, אני מניח 259, מה שזה אומר זה למרות שיש 526 מילים בסך הכל שמתאימות לדפוס הזה, כמות אי הוודאות שיש לה דומה יותר למה שהיא הייתה אם היו 259 בסבירות שווה תוצאות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this. ",
  "translatedText": "אתה יכול לחשוב על זה ככה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case. ",
  "translatedText": "הוא יודע שבורקס הוא לא התשובה, אותו דבר עם יורט וזורל וזרוס, אז זה קצת פחות לא ודאי ממה שהיה במקרה הקודם. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller. ",
  "translatedText": "מספר ביטים זה יהיה קטן יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here. ",
  "translatedText": "ואם אני ממשיך לשחק את המשחק, אני מחדד את זה עם כמה ניחושים שהם בהתאם למה שהייתי רוצה להסביר כאן. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy. ",
  "translatedText": "לפי הניחוש הרביעי, אם תסתכלו על הבחירות המובילות שלה, תוכלו לראות שזה כבר לא רק ממקסם את האנטרופיה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words. ",
  "translatedText": "אז בשלב זה, מבחינה טכנית יש שבע אפשרויות, אבל היחידות עם סיכוי משמעותי הן מעונות ומילים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information. ",
  "translatedText": "ואתה יכול לראות את הבחירה בשני אלה מעל כל הערכים האחרים האלה, שבאופן קפדני ייתן יותר מידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect. ",
  "translatedText": "בפעם הראשונה שעשיתי את זה, פשוט הוספתי את שני המספרים האלה כדי למדוד את האיכות של כל ניחוש, שלמעשה עבד טוב יותר ממה שאתה עשוי לחשוד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on. ",
  "translatedText": "אבל זה ממש לא הרגיש שיטתי, ואני בטוח שיש עוד גישות שאנשים יכולים לנקוט אבל הנה זו שנחתתי עליה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that. ",
  "translatedText": "אם אנחנו שוקלים את הסיכוי לניחוש הבא, כמו במקרה הזה מילים, מה שבאמת אכפת לנו הוא הציון הצפוי של המשחק שלנו אם נעשה את זה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to. ",
  "translatedText": "וכדי לחשב את הציון הצפוי הזה, אנחנו אומרים מה ההסתברות שמילים הן התשובה בפועל, שכרגע היא מתארת לה 58%. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4. ",
  "translatedText": "אנחנו אומרים שעם סיכוי של 58%, הציון שלנו במשחק הזה יהיה 4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4. ",
  "translatedText": "ואז עם הסתברות של 1 פחות 58%, הציון שלנו יהיה יותר מ-4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point. ",
  "translatedText": "כמה עוד אנחנו לא יודעים, אבל אנחנו יכולים להעריך את זה על סמך כמה אי ודאות צפויה להיות ברגע שנגיע לנקודה הזו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty. ",
  "translatedText": "ספציפית, כרגע יש 1.44 פיסות של אי ודאות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits. ",
  "translatedText": "אם ננחש מילים, זה אומר לנו שהמידע הצפוי שנקבל הוא 1.27 ביטים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens. ",
  "translatedText": "אז אם ננחש מילים, ההבדל הזה מייצג כמה חוסר ודאות סביר שנישאר עם אחרי שזה יקרה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score. ",
  "translatedText": "מה שאנחנו צריכים זה איזושהי פונקציה, שאני קורא לה כאן, שקושרת את אי הוודאות הזו עם ציון צפוי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty. ",
  "translatedText": "והדרך שבה זה התנהל הייתה פשוט לשרטט חבורה של נתונים ממשחקים קודמים המבוססים על גרסה 1 של הבוט כדי לומר היי מה היה הציון בפועל אחרי נקודות שונות עם כמויות מסוימות מאוד מדידות של אי ודאות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer. ",
  "translatedText": "לדוגמה, נקודות הנתונים האלה כאן שנמצאות מעל ערך שהוא בערך כמו 8.7 בערך אומרים על כמה משחקים אחרי נקודה שבה היו 8.7 פיסות של אי ודאות, נדרשו שני ניחושים כדי לקבל את התשובה הסופית. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses. ",
  "translatedText": "למשחקים אחרים נדרשו שלושה ניחושים, למשחקים אחרים נדרשו ארבעה ניחושים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring. ",
  "translatedText": "אם נעבור שמאלה כאן, כל הנקודות מעל האפס אומרות בכל פעם שיש אפס פיסות של אי ודאות, כלומר יש רק אפשרות אחת, אז מספר הניחושים הנדרש הוא תמיד רק אחד, וזה מרגיע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses. ",
  "translatedText": "בכל פעם שהייתה קצת אי ודאות, כלומר זה היה בעצם רק בשתי אפשרויות, אז לפעמים זה דרש עוד ניחוש אחד, לפעמים זה דרש שני ניחושים נוספים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here. ",
  "translatedText": "וכן הלאה וכן הלאה כאן. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages. ",
  "translatedText": "אולי דרך קצת יותר קלה לדמיין את הנתונים האלה היא לרכז אותם יחד ולקחת ממוצעים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5. ",
  "translatedText": "לדוגמה, הסרגל הזה אומר שבין כל הנקודות שבהן היה לנו קצת אי ודאות, בממוצע מספר הניחושים החדשים הנדרשים היה בערך 1.5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward. ",
  "translatedText": "והסרגל כאן אומר בין כל המשחקים השונים שבו בשלב מסוים חוסר הוודאות היה קצת מעל ארבע ביטים, שזה כמו לצמצם אותו ל-16 אפשרויות שונות, אז בממוצע זה דורש קצת יותר משני ניחושים מאותה נקודה קָדִימָה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this. ",
  "translatedText": "ומכאן פשוט עשיתי רגרסיה כדי להתאים לפונקציה שנראתה סבירה לזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be. ",
  "translatedText": "ותזכרו שכל העניין בעשיית כל זה הוא כדי שנוכל לכמת את האינטואיציה הזו שככל שנשיג יותר מידע ממילה, כך הציון הצפוי יהיה נמוך יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do? ",
  "translatedText": "אז עם זה כגרסה 2.0, אם נחזור אחורה ונריץ את אותה סט של סימולציות, כשזה ישחק מול כל 2315 תשובות המילה האפשריות, איך זה מסתדר? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring. ",
  "translatedText": "ובכן בניגוד לגרסה הראשונה שלנו זה בהחלט טוב יותר, וזה מרגיע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance. ",
  "translatedText": "הכל אמר ועשה הממוצע הוא בסביבות 3.6, אם כי בניגוד לגרסה הראשונה יש כמה פעמים שהיא מפסידה ודורשת יותר משש בנסיבות אלה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information. ",
  "translatedText": "ככל הנראה בגלל שיש מקרים שבהם זה עושה את הפשרה הזו ללכת על המטרה במקום למקסם את המידע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6? ",
  "translatedText": "אז אנחנו יכולים לעשות יותר טוב מ-3.6? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can. ",
  "translatedText": "אנחנו בהחלט יכולים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model. ",
  "translatedText": "עכשיו אמרתי בהתחלה שהכי כיף לנסות לא לשלב את הרשימה האמיתית של תשובות מילוליות בדרך שבה היא בונה את המודל שלה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43. ",
  "translatedText": "אבל אם נשלב את זה, הביצועים הטובים ביותר שיכולתי לקבל היו בסביבות 3.43. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that. ",
  "translatedText": "אז אם ננסה להיות מתוחכמים יותר מסתם שימוש בנתוני תדירות מילים כדי לבחור את ההפצה הקודמת הזו, זה 3.43 כנראה נותן מקסימום כמה טוב יכולנו להגיע עם זה, או לפחות כמה טוב יכולתי להגיע עם זה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one. ",
  "translatedText": "הביצועים הטובים ביותר האלה בעצם רק משתמשים ברעיונות שדיברתי עליהם כאן, אבל זה הולך קצת יותר רחוק, כאילו הוא מחפש את המידע הצפוי שני צעדים קדימה ולא רק אחד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is. ",
  "translatedText": "במקור תכננתי לדבר על זה יותר, אבל אני מבין שלמעשה עברנו די הרבה זמן כמו שזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener. ",
  "translatedText": "הדבר היחיד שאני אגיד הוא לאחר ביצוע חיפוש דו-שלבי זה ולאחר מכן הפעלת כמה סימולציות לדוגמא במועמדים המובילים, עד כה עבורי לפחות זה נראה כאילו קריין הוא הפותח הטוב ביותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed? ",
  "translatedText": "מי היה מנחש? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits. ",
  "translatedText": "כמו כן, אם אתה משתמש ברשימת המילים האמיתית כדי לקבוע את מרחב האפשרויות שלך, אז אי הוודאות שאתה מתחיל איתה היא קצת יותר מ-11 ביטים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits. ",
  "translatedText": "ומסתבר, רק מחיפוש כוח גס, המידע המקסימלי האפשרי הצפוי לאחר שני הניחושים הראשונים הוא בסביבות 10 ביטים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty. ",
  "translatedText": "מה שמרמז על התרחיש הטוב ביותר, לאחר שני הניחושים הראשונים שלך, עם משחק אופטימלי לחלוטין, תישאר עם קצת אי ודאות אחת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses. ",
  "translatedText": "וזה אותו דבר כמו לרדת לשני ניחושים אפשריים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail. ",
  "translatedText": "אז אני חושב שזה הוגן וכנראה די שמרני לומר שלעולם לא תוכל לכתוב אלגוריתם שמקבל את הממוצע הזה נמוך כמו 3, כי עם המילים הזמינות לך, פשוט אין מקום לקבל מספיק מידע לאחר שני שלבים בלבד. מסוגל להבטיח את התשובה במשבצת השלישית בכל פעם בלי להיכשל. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]