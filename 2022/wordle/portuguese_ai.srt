1
00:00:00,000 --> 00:00:04,040
O jogo Wurdle se tornou bastante viral nos últimos dois meses e,

2
00:00:04,040 --> 00:00:07,880
como nunca desperdiço uma oportunidade de uma aula de matemática, me ocorre

3
00:00:07,880 --> 00:00:12,120
que este jogo é um exemplo central muito bom em uma aula

4
00:00:12,120 --> 00:00:13,120
sobre teoria da informação e, em particular um tópico conhecido como entropia.

5
00:00:13,120 --> 00:00:17,120
Veja, como muitas pessoas, fui sugado pelo quebra-cabeça e, como

6
00:00:17,120 --> 00:00:21,200
muitos programadores, também fui sugado por tentar escrever um algoritmo

7
00:00:21,200 --> 00:00:23,200
que jogasse o jogo da maneira mais otimizada possível.

8
00:00:23,200 --> 00:00:26,400
E o que pensei em fazer aqui é apenas conversar com vocês

9
00:00:26,400 --> 00:00:29,980
sobre meu processo nisso e explicar um pouco da matemática envolvida,

10
00:00:29,980 --> 00:00:32,080
já que todo o algoritmo está centrado nessa ideia de entropia.

11
00:00:32,080 --> 00:00:42,180
Primeiramente, caso você ainda não tenha ouvido falar, o que é Wurdle?

12
00:00:42,180 --> 00:00:45,380
E para matar dois coelhos com uma cajadada só enquanto analisamos as

13
00:00:45,380 --> 00:00:48,980
regras do jogo, deixe-me também prever onde estamos indo com isso, que

14
00:00:48,980 --> 00:00:51,380
é desenvolver um pequeno algoritmo que basicamente jogará o jogo para nós.

15
00:00:51,380 --> 00:00:54,860
Embora eu não tenha feito o Wurdle de hoje, é

16
00:00:54,860 --> 00:00:55,860
4 de fevereiro e veremos como o bot se sai.

17
00:00:55,860 --> 00:00:59,580
O objetivo do Wurdle é adivinhar uma palavra misteriosa de

18
00:00:59,580 --> 00:01:00,860
cinco letras, e você terá seis chances diferentes de adivinhar.

19
00:01:00,860 --> 00:01:05,240
Por exemplo, meu bot Wurdle sugere que eu comece com o guindaste de adivinhação.

20
00:01:05,240 --> 00:01:09,300
Cada vez que você dá um palpite, você obtém algumas informações

21
00:01:09,300 --> 00:01:10,940
sobre o quão próximo seu palpite está da resposta verdadeira.

22
00:01:10,940 --> 00:01:14,540
Aqui, a caixa cinza me diz que não há C na resposta real.

23
00:01:14,540 --> 00:01:18,340
A caixa amarela está me dizendo que existe um R, mas não está nessa posição.

24
00:01:18,340 --> 00:01:21,820
A caixa verde está me dizendo que a palavra

25
00:01:21,820 --> 00:01:22,820
secreta tem um A e está na terceira posição.

26
00:01:22,820 --> 00:01:24,300
E então não há N e não há E.

27
00:01:24,300 --> 00:01:27,420
Então deixe-me entrar e contar essa informação ao bot Wurdle.

28
00:01:27,420 --> 00:01:31,500
Começamos com guindaste, ficamos cinza, amarelo, verde, cinza, cinza.

29
00:01:31,500 --> 00:01:35,460
Não se preocupe com todos os dados que estão mostrando agora, explicarei isso no devido tempo.

30
00:01:35,460 --> 00:01:39,700
Mas sua principal sugestão para nossa segunda escolha é uma besteira.

31
00:01:39,700 --> 00:01:43,500
E seu palpite precisa ser uma palavra real de cinco letras, mas como

32
00:01:43,500 --> 00:01:45,700
você verá, é bastante liberal com o que realmente permitirá que você adivinhe.

33
00:01:45,700 --> 00:01:48,860
Neste caso, tentamos shtick.

34
00:01:48,860 --> 00:01:50,260
E tudo bem, as coisas estão parecendo muito boas.

35
00:01:50,260 --> 00:01:54,580
Atingimos o S e o H, então conhecemos as três primeiras letras, sabemos que existe um R.

36
00:01:54,740 --> 00:01:59,740
E então será como SHA algo R, ou SHA R alguma coisa.

37
00:01:59,740 --> 00:02:03,200
E parece que o bot Wurdle sabe que

38
00:02:03,200 --> 00:02:05,220
existem apenas duas possibilidades: fragmento ou afiado.

39
00:02:05,220 --> 00:02:08,620
Isso é uma espécie de confusão entre eles neste momento, então acho

40
00:02:08,620 --> 00:02:11,260
que provavelmente só porque está em ordem alfabética, vai com o fragmento.

41
00:02:11,260 --> 00:02:13,000
Qual, viva, é a resposta real.

42
00:02:13,000 --> 00:02:14,660
Então conseguimos isso em três.

43
00:02:14,660 --> 00:02:17,740
Se você está se perguntando se isso é bom, a maneira como ouvi uma

44
00:02:17,740 --> 00:02:20,820
pessoa dizer é que com Wurdle quatro é par e três é birdie.

45
00:02:20,820 --> 00:02:22,960
O que considero uma analogia bastante adequada.

46
00:02:22,960 --> 00:02:27,560
Você tem que estar consistentemente no seu jogo para conseguir quatro, mas certamente não é uma loucura.

47
00:02:27,560 --> 00:02:30,000
Mas quando você consegue isso em três, é ótimo.

48
00:02:30,000 --> 00:02:33,800
Então, se você quiser, o que eu gostaria de fazer aqui é apenas falar

49
00:02:33,800 --> 00:02:36,600
sobre meu processo de pensamento desde o início sobre como abordo o bot Wurdle.

50
00:02:36,600 --> 00:02:39,800
E como eu disse, na verdade é uma desculpa para uma aula de teoria da informação.

51
00:02:39,800 --> 00:02:43,160
O objetivo principal é explicar o que é informação e o que é entropia.

52
00:02:48,560 --> 00:02:52,080
Meu primeiro pensamento ao abordar isso foi dar uma olhada

53
00:02:52,080 --> 00:02:53,560
nas frequências relativas de diferentes letras na língua inglesa.

54
00:02:53,560 --> 00:02:57,800
Então pensei, ok, existe um palpite inicial ou um par

55
00:02:57,800 --> 00:02:59,960
inicial de palpites que acerta muitas dessas letras mais frequentes?

56
00:02:59,960 --> 00:03:03,780
E uma que eu gostava muito era fazer outra seguida de unhas.

57
00:03:03,780 --> 00:03:06,980
A ideia é que se você acertar uma letra, você sabe,

58
00:03:06,980 --> 00:03:07,980
você ganha um verde ou um amarelo, isso sempre é bom.

59
00:03:07,980 --> 00:03:09,460
Parece que você está obtendo informações.

60
00:03:09,460 --> 00:03:13,140
Mas nesses casos, mesmo que você não acerte e sempre fique

61
00:03:13,140 --> 00:03:16,640
cinza, isso ainda lhe dá muita informação, já que é muito

62
00:03:16,640 --> 00:03:17,640
raro encontrar uma palavra que não tenha nenhuma dessas letras.

63
00:03:17,640 --> 00:03:21,840
Mas mesmo assim, isso não parece super sistemático, porque, por

64
00:03:21,840 --> 00:03:23,520
exemplo, não faz nada considerar a ordem das letras.

65
00:03:23,520 --> 00:03:26,080
Por que digitar pregos quando eu poderia digitar caracol?

66
00:03:26,080 --> 00:03:27,720
É melhor ter aquele S no final?

67
00:03:27,720 --> 00:03:28,720
Eu não tenho certeza.

68
00:03:28,720 --> 00:03:33,500
Agora, um amigo meu disse que gostava de começar com a palavra cansado, o

69
00:03:33,500 --> 00:03:37,160
que me surpreendeu porque tem algumas letras incomuns, como o W e o Y.

70
00:03:37,160 --> 00:03:39,400
Mas quem sabe, talvez seja uma abertura melhor.

71
00:03:39,400 --> 00:03:43,920
Existe algum tipo de pontuação quantitativa que podemos atribuir

72
00:03:43,920 --> 00:03:44,920
para julgar a qualidade de uma possível estimativa?

73
00:03:44,920 --> 00:03:48,640
Agora, para definir a maneira como classificaremos as possíveis suposições, vamos voltar e

74
00:03:48,640 --> 00:03:51,800
adicionar um pouco de clareza sobre como exatamente o jogo está configurado.

75
00:03:51,800 --> 00:03:55,880
Portanto, há uma lista de palavras que permitirá que você insira

76
00:03:55,880 --> 00:03:57,920
e que sejam consideradas suposições válidas, com cerca de 13.000 palavras.

77
00:03:57,920 --> 00:04:01,560
Mas quando você olha para isso, há muitas coisas realmente incomuns, coisas como uma cabeça ou

78
00:04:01,560 --> 00:04:07,040
Ali e ARG, o tipo de palavras que provocam discussões familiares em um jogo de Scrabble.

79
00:04:07,040 --> 00:04:10,600
Mas a vibração do jogo é que a resposta sempre será uma palavra decentemente comum.

80
00:04:10,600 --> 00:04:16,080
E, de fato, há outra lista de cerca de 2.300 palavras que são as respostas possíveis.

81
00:04:16,080 --> 00:04:20,320
E esta é uma lista com curadoria humana, acho que especificamente

82
00:04:20,320 --> 00:04:21,800
da namorada do criador do jogo, o que é divertido.

83
00:04:21,800 --> 00:04:25,560
Mas o que eu gostaria de fazer, nosso desafio para este projeto é ver se

84
00:04:25,560 --> 00:04:30,720
conseguimos escrever um programa resolvendo Wordle que não incorpore conhecimento prévio sobre esta lista.

85
00:04:30,720 --> 00:04:34,560
Por um lado, há muitas palavras de cinco letras

86
00:04:34,560 --> 00:04:35,560
bastante comuns que você não encontrará nessa lista.

87
00:04:35,560 --> 00:04:38,360
Portanto, seria melhor escrever um programa que fosse um pouco mais resiliente e

88
00:04:38,360 --> 00:04:41,960
que jogasse Wordle contra qualquer um, não apenas contra o site oficial.

89
00:04:41,960 --> 00:04:45,900
E também a razão pela qual sabemos qual é essa lista

90
00:04:45,900 --> 00:04:47,440
de respostas possíveis é porque ela está visível no código-fonte.

91
00:04:47,440 --> 00:04:51,620
Mas a forma como isso fica visível no código-fonte está na

92
00:04:51,620 --> 00:04:52,840
ordem específica em que as respostas surgem no dia a dia.

93
00:04:52,840 --> 00:04:56,400
Então você pode sempre procurar qual será a resposta de amanhã.

94
00:04:56,400 --> 00:04:59,140
Então, claramente, há algum sentido em que usar a lista é trapaça.

95
00:04:59,140 --> 00:05:02,900
E o que torna um quebra-cabeça mais interessante e uma lição de teoria da informação

96
00:05:02,900 --> 00:05:07,640
mais rica é, em vez disso, usar alguns dados mais universais, como frequências relativas de

97
00:05:07,640 --> 00:05:11,640
palavras em geral, para capturar essa intuição de ter preferência por palavras mais comuns.

98
00:05:11,640 --> 00:05:16,560
Então, destas 13.000 possibilidades, como devemos escolher o palpite inicial?

99
00:05:16,560 --> 00:05:19,960
Por exemplo, se meu amigo propõe cansado, como devemos analisar sua qualidade?

100
00:05:19,960 --> 00:05:25,040
Bem, a razão pela qual ele disse que gosta daquele W improvável é que

101
00:05:25,040 --> 00:05:27,880
ele gosta da natureza remota de como é bom se você acertar aquele W.

102
00:05:27,880 --> 00:05:31,400
Por exemplo, se o primeiro padrão revelado for algo assim, então acontece que

103
00:05:31,400 --> 00:05:36,080
existem apenas 58 palavras neste léxico gigante que correspondem a esse padrão.

104
00:05:36,080 --> 00:05:38,900
Portanto, é uma redução enorme em relação aos 13.000.

105
00:05:38,900 --> 00:05:43,320
Mas o outro lado disso, claro, é que é muito incomum obter um padrão como este.

106
00:05:43,360 --> 00:05:47,600
Especificamente, se cada palavra tivesse a mesma probabilidade de ser a resposta, a

107
00:05:47,600 --> 00:05:51,680
probabilidade de atingir esse padrão seria de 58 dividido por cerca de 13.000.

108
00:05:51,680 --> 00:05:53,880
É claro que não têm a mesma probabilidade de serem respostas.

109
00:05:53,880 --> 00:05:56,680
A maioria destas são palavras muito obscuras e até questionáveis.

110
00:05:56,680 --> 00:05:59,560
Mas pelo menos para a nossa primeira passagem por tudo isso, vamos supor que

111
00:05:59,560 --> 00:06:02,040
todas elas sejam igualmente prováveis e então refinar isso um pouco mais tarde.

112
00:06:02,040 --> 00:06:07,360
A questão é que o padrão com muitas informações é, por sua própria natureza, improvável de ocorrer.

113
00:06:07,360 --> 00:06:11,320
Na verdade, o que significa ser informativo é que é improvável.

114
00:06:11,920 --> 00:06:16,720
Um padrão muito mais provável de se ver nesta abertura seria

115
00:06:16,720 --> 00:06:18,360
algo assim, onde é claro que não há um W nela.

116
00:06:18,360 --> 00:06:22,080
Talvez haja um E, e talvez não haja A, não haja R, não haja Y.

117
00:06:22,080 --> 00:06:24,640
Neste caso, existem 1.400 correspondências possíveis.

118
00:06:24,640 --> 00:06:29,600
Se todos fossem igualmente prováveis, haveria uma probabilidade de cerca de

119
00:06:29,600 --> 00:06:30,680
11% de que esse fosse o padrão que você veria.

120
00:06:30,680 --> 00:06:34,320
Portanto, os resultados mais prováveis são também os menos informativos.

121
00:06:34,320 --> 00:06:38,440
Para obter uma visão mais global aqui, deixe-me mostrar a distribuição completa

122
00:06:38,440 --> 00:06:42,000
de probabilidades em todos os diferentes padrões que você pode ver.

123
00:06:42,000 --> 00:06:46,000
Então cada barra que você está olhando corresponde a um possível padrão de cores

124
00:06:46,000 --> 00:06:50,500
que podem ser reveladas, das quais existem 3 a 5 possibilidades, e estão

125
00:06:50,500 --> 00:06:52,960
organizadas da esquerda para a direita, da mais comum para a menos comum.

126
00:06:52,960 --> 00:06:56,200
Portanto, a possibilidade mais comum aqui é que você obtenha todos os tons de cinza.

127
00:06:56,200 --> 00:06:58,800
Isso acontece cerca de 14% das vezes.

128
00:06:58,800 --> 00:07:02,040
E o que você espera quando faz uma suposição é que você acabe

129
00:07:02,040 --> 00:07:06,360
em algum lugar nesta cauda longa, como aqui, onde há apenas 18 possibilidades

130
00:07:06,360 --> 00:07:09,920
para o que corresponde a esse padrão que evidentemente se parece com este.

131
00:07:09,920 --> 00:07:14,080
Ou se nos aventurarmos um pouco mais para a esquerda, você sabe, talvez possamos ir até aqui.

132
00:07:14,080 --> 00:07:16,560
Ok, aqui está um bom quebra-cabeça para você.

133
00:07:16,560 --> 00:07:20,600
Quais são as três palavras da língua inglesa que começam com

134
00:07:20,600 --> 00:07:22,040
W, terminam com Y e têm um R em algum lugar?

135
00:07:22,040 --> 00:07:27,560
Acontece que as respostas são, vejamos, prolixas, minhocas e ironicamente.

136
00:07:27,560 --> 00:07:32,720
Então, para avaliar o quão boa esta palavra é em geral, queremos algum

137
00:07:32,720 --> 00:07:35,720
tipo de medida da quantidade esperada de informação que você obterá desta distribuição.

138
00:07:36,360 --> 00:07:41,080
Se analisarmos cada padrão e multiplicarmos sua probabilidade de ocorrência por algo que meça

139
00:07:41,080 --> 00:07:46,000
o quão informativo ele é, isso talvez possa nos dar uma pontuação objetiva.

140
00:07:46,000 --> 00:07:50,280
Agora, seu primeiro instinto sobre o que deveria ser esse algo pode ser o número de correspondências.

141
00:07:50,280 --> 00:07:52,960
Você deseja um número médio menor de correspondências.

142
00:07:52,960 --> 00:07:57,400
Mas, em vez disso, gostaria de usar uma medida mais universal que frequentemente atribuímos

143
00:07:57,400 --> 00:08:01,040
à informação, e que será mais flexível quando tivermos uma probabilidade diferente atribuída a

144
00:08:01,040 --> 00:08:04,320
cada uma destas 13.000 palavras para determinar se são ou não a resposta.

145
00:08:10,600 --> 00:08:14,760
A unidade padrão de informação é o bit, que tem uma fórmula

146
00:08:14,760 --> 00:08:17,800
um pouco engraçada, mas é muito intuitiva se olharmos apenas os exemplos.

147
00:08:17,800 --> 00:08:21,880
Se você tem uma observação que reduz pela metade o seu

148
00:08:21,880 --> 00:08:24,200
espaço de possibilidades, dizemos que ela contém um bit de informação.

149
00:08:24,200 --> 00:08:27,680
No nosso exemplo, o espaço de possibilidades são todas as palavras possíveis, e acontece que cerca de metade

150
00:08:27,760 --> 00:08:31,560
das palavras de cinco letras têm um S, um pouco menos que isso, mas cerca de metade.

151
00:08:31,560 --> 00:08:35,200
Portanto, essa observação lhe daria um pouco de informação.

152
00:08:35,200 --> 00:08:39,640
Se, em vez disso, um facto novo reduzir esse espaço de possibilidades

153
00:08:39,640 --> 00:08:42,000
por um factor de quatro, dizemos que tem dois bits de informação.

154
00:08:42,000 --> 00:08:45,120
Por exemplo, cerca de um quarto dessas palavras tem T.

155
00:08:45,120 --> 00:08:49,720
Se a observação reduzir esse espaço por um fator de oito,

156
00:08:49,720 --> 00:08:50,920
dizemos que são três bits de informação, e assim por diante.

157
00:08:50,920 --> 00:08:55,000
Quatro bits equivalem a um 16º, cinco bits equivalem a um 32º.

158
00:08:55,000 --> 00:09:00,160
Então agora você pode querer fazer uma pausa e se perguntar: qual é a fórmula

159
00:09:00,160 --> 00:09:04,520
da informação para o número de bits em termos da probabilidade de uma ocorrência?

160
00:09:04,520 --> 00:09:07,920
O que estamos dizendo aqui é que quando você eleva metade do número de bits, isso

161
00:09:07,920 --> 00:09:11,680
é a mesma coisa que a probabilidade, que é a mesma coisa que dizer que dois

162
00:09:11,680 --> 00:09:16,200
elevado à potência do número de bits é um sobre a probabilidade, que reorganiza ainda mais

163
00:09:16,200 --> 00:09:19,680
para dizer que a informação é o log de base dois de um dividido pela probabilidade.

164
00:09:19,680 --> 00:09:23,200
E às vezes você vê isso com mais um rearranjo ainda, onde

165
00:09:23,200 --> 00:09:25,680
a informação é o log negativo na base dois da probabilidade.

166
00:09:25,680 --> 00:09:29,120
Expressado desta forma, pode parecer um pouco estranho para os não

167
00:09:29,120 --> 00:09:33,400
iniciados, mas na verdade é apenas a ideia muito intuitiva

168
00:09:33,400 --> 00:09:35,120
de perguntar quantas vezes você reduziu suas possibilidades pela metade.

169
00:09:35,120 --> 00:09:37,840
Agora, se você está se perguntando, você sabe, pensei que estávamos apenas jogando

170
00:09:37,840 --> 00:09:39,920
um divertido jogo de palavras, por que os logaritmos estão entrando em cena?

171
00:09:39,920 --> 00:09:43,920
Uma razão pela qual esta unidade é melhor é que é muito mais fácil falar sobre

172
00:09:43,920 --> 00:09:48,120
eventos muito improváveis, muito mais fácil dizer que uma observação tem 20 bits de informação

173
00:09:48,120 --> 00:09:53,480
do que dizer que a probabilidade de tal ou tal ocorrência é 0. 0000095.

174
00:09:53,480 --> 00:09:57,360
Mas uma razão mais substantiva pela qual esta expressão logarítmica se revelou um acréscimo

175
00:09:57,360 --> 00:10:02,000
muito útil à teoria da probabilidade é a forma como a informação se soma.

176
00:10:02,000 --> 00:10:05,560
Por exemplo, se uma observação fornece dois bits de informação, reduzindo seu

177
00:10:05,560 --> 00:10:10,120
espaço em quatro, e então uma segunda observação, como sua segunda

178
00:10:10,120 --> 00:10:14,480
estimativa no Wordle, fornece outros três bits de informação, reduzindo ainda mais

179
00:10:14,480 --> 00:10:17,360
por outro fator de oito, o dois juntos fornecem cinco informações.

180
00:10:17,360 --> 00:10:21,200
Da mesma forma que as probabilidades gostam de se multiplicar, a informação gosta de somar.

181
00:10:21,200 --> 00:10:24,920
Então, assim que estamos no reino de algo como um valor esperado, onde estamos

182
00:10:24,920 --> 00:10:28,660
somando um monte de números, os logs tornam muito mais fácil lidar com isso.

183
00:10:28,660 --> 00:10:32,600
Vamos voltar à nossa distribuição para Weary e adicionar outro

184
00:10:32,600 --> 00:10:35,560
pequeno rastreador aqui, mostrando quanta informação existe para cada padrão.

185
00:10:35,560 --> 00:10:38,760
A principal coisa que quero que você observe é que quanto maior a probabilidade à medida

186
00:10:38,760 --> 00:10:43,500
que chegamos a esses padrões mais prováveis, quanto menor a informação, menos bits você ganha.

187
00:10:43,500 --> 00:10:47,360
A forma como medimos a qualidade dessa suposição será pegar o

188
00:10:47,360 --> 00:10:51,620
valor esperado dessa informação, onde percorremos cada padrão, dizemos quão provável

189
00:10:51,620 --> 00:10:54,940
é, e então multiplicamos por quantos bits de informação obtemos.

190
00:10:54,940 --> 00:10:58,480
E no exemplo de Weary, isso resulta em 4. 9 bits.

191
00:10:58,480 --> 00:11:02,800
Então, em média, as informações que você obtém com essa estimativa inicial são tão

192
00:11:02,800 --> 00:11:05,660
boas quanto cortar seu espaço de possibilidades pela metade, cerca de cinco vezes.

193
00:11:05,660 --> 00:11:10,260
Por outro lado, um exemplo de suposição com um valor

194
00:11:10,260 --> 00:11:13,220
de informação esperado mais alto seria algo como Slate.

195
00:11:13,220 --> 00:11:16,180
Neste caso você notará que a distribuição parece muito mais plana.

196
00:11:16,180 --> 00:11:20,780
Em particular, a ocorrência mais provável de todos os tons de cinza tem apenas cerca de 6%

197
00:11:20,780 --> 00:11:25,940
de chance de ocorrer, então, no mínimo, você obtém evidentemente 3. 9 bits de informação.

198
00:11:25,940 --> 00:11:29,140
Mas isso é o mínimo, mais normalmente você conseguiria algo melhor que isso.

199
00:11:29,140 --> 00:11:33,380
E acontece que quando você analisa os números deste aqui e soma todos

200
00:11:33,380 --> 00:11:36,420
os termos relevantes, a informação média é de cerca de 5. 8.

201
00:11:36,420 --> 00:11:42,140
Portanto, em contraste com Weary, seu espaço de possibilidades será cerca

202
00:11:42,140 --> 00:11:43,940
de metade do tamanho após essa primeira estimativa, em média.

203
00:11:43,940 --> 00:11:49,540
Na verdade, há uma história divertida sobre o nome desse valor esperado da quantidade de informação.

204
00:11:49,540 --> 00:11:52,580
A teoria da informação foi desenvolvida por Claude Shannon, que trabalhava no Bell Labs na

205
00:11:52,580 --> 00:11:57,620
década de 1940, mas ele estava conversando sobre algumas de suas ideias ainda a serem

206
00:11:57,620 --> 00:12:01,500
publicadas com John von Neumann, que era um gigante intelectual da época, muito proeminente. em

207
00:12:01,500 --> 00:12:04,180
matemática e física e o início do que estava se tornando a ciência da computação.

208
00:12:04,180 --> 00:12:07,260
E quando ele mencionou que não tinha realmente um bom nome para esse

209
00:12:07,260 --> 00:12:12,540
valor esperado da quantidade de informação, von Neumann supostamente disse, assim continua a

210
00:12:12,540 --> 00:12:14,720
história, bem, você deveria chamar isso de entropia, e por duas razões.

211
00:12:14,720 --> 00:12:18,400
Em primeiro lugar, a sua função de incerteza tem sido usada na mecânica estatística com esse

212
00:12:18,400 --> 00:12:23,100
nome, por isso já tem um nome, e em segundo lugar, e mais importante, ninguém

213
00:12:23,100 --> 00:12:26,940
sabe o que realmente é entropia, por isso num debate você sempre tem a vantagem.

214
00:12:26,940 --> 00:12:31,420
Então, se o nome parece um pouco misterioso, e

215
00:12:31,420 --> 00:12:33,420
se é para acreditar nessa história, isso é intencional.

216
00:12:33,420 --> 00:12:36,740
Além disso, se você está se perguntando sobre sua relação com toda a

217
00:12:36,740 --> 00:12:40,820
segunda lei da termodinâmica da física, definitivamente há uma conexão, mas em suas

218
00:12:40,820 --> 00:12:44,780
origens Shannon estava apenas lidando com a teoria pura da probabilidade, e

219
00:12:44,780 --> 00:12:49,340
para nossos propósitos aqui, quando eu uso o entropia de palavra, só quero

220
00:12:49,340 --> 00:12:50,820
que você pense no valor de informação esperado de uma suposição específica.

221
00:12:50,820 --> 00:12:54,380
Você pode pensar na entropia como uma medida de duas coisas simultaneamente.

222
00:12:54,380 --> 00:12:57,420
A primeira é quão plana é a distribuição.

223
00:12:57,420 --> 00:13:01,700
Quanto mais próxima a distribuição estiver da uniformidade, maior será a entropia.

224
00:13:01,700 --> 00:13:06,340
No nosso caso, onde há 3 elevado a 5 padrões totais, para uma distribuição uniforme, a observação de qualquer

225
00:13:06,340 --> 00:13:11,340
um deles teria log de informações de base 2 de 3 elevado a 5, que passa a ser

226
00:13:11,340 --> 00:13:17,860
7. 92, então esse é o máximo absoluto que você poderia ter para esta entropia.

227
00:13:17,860 --> 00:13:21,900
Mas a entropia também é uma espécie

228
00:13:21,900 --> 00:13:22,900
de medida de quantas possibilidades existem.

229
00:13:22,900 --> 00:13:26,980
Por exemplo, se acontecer de você ter alguma palavra onde há apenas 16 padrões possíveis,

230
00:13:26,980 --> 00:13:32,760
e cada um é igualmente provável, essa entropia, essa informação esperada, seria de 4 bits.

231
00:13:32,760 --> 00:13:36,880
Mas se você tiver outra palavra onde há 64 padrões possíveis que poderiam surgir,

232
00:13:36,880 --> 00:13:41,000
e todos eles são igualmente prováveis, então a entropia resultaria em 6 bits.

233
00:13:41,000 --> 00:13:45,800
Então, se você vir alguma distribuição que tenha uma entropia de 6 bits,

234
00:13:45,800 --> 00:13:50,000
é como se estivesse dizendo que há tanta variação e incerteza no

235
00:13:50,000 --> 00:13:54,400
que está prestes a acontecer como se houvesse 64 resultados igualmente prováveis.

236
00:13:54,400 --> 00:13:58,360
Para minha primeira passagem no Wurtelebot, basicamente fiz isso.

237
00:13:58,360 --> 00:14:03,560
Ele analisa todas as suposições possíveis que você poderia ter, todas as 13.000 palavras,

238
00:14:03,560 --> 00:14:08,580
calcula a entropia de cada uma, ou mais especificamente, a entropia da distribuição em

239
00:14:08,580 --> 00:14:13,040
todos os padrões que você pode ver, para cada uma, e escolhe o mais

240
00:14:13,040 --> 00:14:17,200
alto, já que é aquele que provavelmente reduzirá ao máximo seu espaço de possibilidades.

241
00:14:17,200 --> 00:14:20,120
E mesmo que eu tenha falado apenas sobre a primeira

242
00:14:20,120 --> 00:14:21,680
suposição aqui, acontece o mesmo com as próximas suposições.

243
00:14:21,680 --> 00:14:25,100
Por exemplo, depois de ver algum padrão nessa primeira suposição, que o restringiria a

244
00:14:25,100 --> 00:14:29,300
um número menor de palavras possíveis com base no que corresponde a isso,

245
00:14:29,300 --> 00:14:32,300
basta jogar o mesmo jogo em relação a esse conjunto menor de palavras.

246
00:14:32,300 --> 00:14:36,500
Para uma segunda suposição proposta, você olha para a distribuição de todos os

247
00:14:36,500 --> 00:14:41,540
padrões que podem ocorrer a partir desse conjunto mais restrito de palavras,

248
00:14:41,540 --> 00:14:45,480
pesquisa todas as 13.000 possibilidades e encontra aquela que maximiza essa entropia.

249
00:14:45,480 --> 00:14:48,980
Para mostrar como isso funciona em ação, deixe-me apenas apresentar uma pequena variante

250
00:14:48,980 --> 00:14:54,060
de Wurtele que escrevi, que mostra os destaques desta análise nas margens.

251
00:14:54,460 --> 00:14:57,820
Depois de fazer todos os cálculos de entropia, aqui à

252
00:14:57,820 --> 00:15:00,340
direita ele nos mostra quais possuem a maior informação esperada.

253
00:15:00,340 --> 00:15:04,940
Acontece que a resposta principal, pelo menos no momento, vamos refinar isso

254
00:15:04,940 --> 00:15:11,140
mais tarde, é Tares, que significa, claro, ervilhaca, a ervilhaca mais comum.

255
00:15:11,140 --> 00:15:14,180
Cada vez que fazemos um palpite aqui, onde talvez eu ignore suas

256
00:15:14,180 --> 00:15:19,220
recomendações e opte pelo slate, porque gosto do slate, podemos ver quanta

257
00:15:19,220 --> 00:15:23,300
informação esperada ele tinha, mas à direita da palavra aqui está nos

258
00:15:23,340 --> 00:15:24,980
mostrando o quanto informações reais que obtivemos, dado esse padrão específico.

259
00:15:24,980 --> 00:15:28,660
Então aqui parece que tivemos um pouco de azar, era esperado que tivéssemos 5. 8, mas

260
00:15:28,660 --> 00:15:30,660
conseguimos algo com menos que isso.

261
00:15:30,660 --> 00:15:34,020
E então no lado esquerdo aqui está nos mostrando

262
00:15:34,020 --> 00:15:35,860
todas as diferentes palavras possíveis dadas onde estamos agora.

263
00:15:35,860 --> 00:15:39,820
As barras azuis nos dizem a probabilidade de cada palavra ser considerada, portanto, no momento, estamos

264
00:15:39,820 --> 00:15:44,140
assumindo que cada palavra tem a mesma probabilidade de ocorrer, mas refinaremos isso em um momento.

265
00:15:44,140 --> 00:15:48,580
E então esta medição de incerteza está a dizer-nos a entropia desta distribuição

266
00:15:48,580 --> 00:15:53,220
entre as palavras possíveis, que neste momento, por ser uma distribuição uniforme,

267
00:15:53,300 --> 00:15:55,940
é apenas uma forma desnecessariamente complicada de contar o número de possibilidades.

268
00:15:55,940 --> 00:16:01,700
Por exemplo, se elevarmos 2 elevado a 13. 66, isso deveria

269
00:16:01,700 --> 00:16:02,700
estar em torno das 13.000 possibilidades.

270
00:16:02,700 --> 00:16:06,780
Estou um pouco errado aqui, mas só porque não estou mostrando todas as casas decimais.

271
00:16:06,780 --> 00:16:10,260
No momento, isso pode parecer redundante e complicar demais as coisas, mas você

272
00:16:10,260 --> 00:16:12,780
verá por que é útil ter os dois números em um minuto.

273
00:16:12,780 --> 00:16:16,780
Então aqui parece que está sugerindo que a entropia mais alta para

274
00:16:16,780 --> 00:16:19,700
nosso segundo palpite é Ramen, o que novamente não parece uma palavra.

275
00:16:19,700 --> 00:16:25,660
Então, para ter uma moral elevada aqui, vou prosseguir e digitar Rains.

276
00:16:25,660 --> 00:16:27,540
E novamente parece que tivemos um pouco de azar.

277
00:16:27,540 --> 00:16:32,100
Estávamos esperando 4. 3 bits e só temos 3. 39 bits de informação.

278
00:16:32,100 --> 00:16:35,060
Então isso nos leva a 55 possibilidades.

279
00:16:35,060 --> 00:16:38,860
E aqui talvez eu siga o que está sugerindo,

280
00:16:38,860 --> 00:16:40,200
que é combo, seja lá o que isso signifique.

281
00:16:40,200 --> 00:16:43,300
E tudo bem, esta é realmente uma boa chance para um quebra-cabeça.

282
00:16:43,300 --> 00:16:47,020
Está nos dizendo que esse padrão nos dá 4. 7 bits de informação.

283
00:16:47,020 --> 00:16:52,400
Mas à esquerda, antes de vermos esse padrão, havia 5. 78 bits de incerteza.

284
00:16:52,400 --> 00:16:56,860
Então, como um teste para você, o que isso significa sobre o número de possibilidades restantes?

285
00:16:56,860 --> 00:17:02,280
Bem, isso significa que estamos reduzidos a um pouco de incerteza, o

286
00:17:02,280 --> 00:17:04,700
que é o mesmo que dizer que há duas respostas possíveis.

287
00:17:04,700 --> 00:17:06,520
É uma escolha 50-50.

288
00:17:06,520 --> 00:17:09,860
E a partir daqui, porque você e eu sabemos quais palavras

289
00:17:09,860 --> 00:17:11,220
são mais comuns, sabemos que a resposta deveria ser abismo.

290
00:17:11,220 --> 00:17:13,540
Mas como está escrito agora, o programa não sabe disso.

291
00:17:13,540 --> 00:17:17,560
Então ele continua tentando obter o máximo de informações possível,

292
00:17:17,560 --> 00:17:20,360
até que reste apenas uma possibilidade, e então ele adivinha.

293
00:17:20,360 --> 00:17:22,700
Então, obviamente, precisamos de uma estratégia de final de jogo melhor.

294
00:17:22,700 --> 00:17:26,540
Mas digamos que chamamos esta versão de nosso solucionador de

295
00:17:26,540 --> 00:17:30,740
palavras e então executamos algumas simulações para ver como funciona.

296
00:17:30,740 --> 00:17:34,240
Então, a maneira como isso funciona é jogando todos os jogos de palavras possíveis.

297
00:17:34,240 --> 00:17:38,780
Ele está passando por todas aquelas 2.315 palavras que são as verdadeiras respostas do wordle.

298
00:17:38,780 --> 00:17:41,340
Basicamente, estamos usando isso como um conjunto de testes.

299
00:17:41,340 --> 00:17:45,820
E com esse método ingênuo de não considerar o quão comum uma palavra é, e apenas tentar

300
00:17:45,820 --> 00:17:50,480
maximizar a informação a cada passo do caminho, até chegar a uma e apenas uma escolha.

301
00:17:50,480 --> 00:17:55,100
Ao final da simulação, a pontuação média é de cerca de 4. 124.

302
00:17:55,100 --> 00:17:59,780
O que não é ruim, para ser honesto, eu esperava fazer pior.

303
00:17:59,780 --> 00:18:03,040
Mas as pessoas que jogam wordle dirão que geralmente conseguem em 4.

304
00:18:03,040 --> 00:18:05,260
O verdadeiro desafio é conseguir o máximo possível em 3.

305
00:18:05,260 --> 00:18:08,920
É um salto muito grande entre a pontuação de 4 e a pontuação de 3.

306
00:18:08,920 --> 00:18:13,300
O objetivo óbvio aqui é incorporar de alguma forma se uma

307
00:18:13,300 --> 00:18:23,160
palavra é comum ou não e como exatamente fazemos isso.

308
00:18:23,160 --> 00:18:26,860
A forma como abordei isso foi obter uma lista das

309
00:18:26,860 --> 00:18:28,560
frequências relativas de todas as palavras da língua inglesa.

310
00:18:28,560 --> 00:18:32,560
E acabei de usar a função de dados de frequência de palavras do

311
00:18:32,560 --> 00:18:35,520
Mathematica, que extrai do conjunto de dados público Ngram do Google Books English.

312
00:18:35,520 --> 00:18:38,680
E é divertido de ver, por exemplo, se classificarmos

313
00:18:38,680 --> 00:18:40,120
das palavras mais comuns para as menos comuns.

314
00:18:40,120 --> 00:18:43,740
Evidentemente, essas são as palavras de 5 letras mais comuns na língua inglesa.

315
00:18:43,740 --> 00:18:46,480
Ou melhor, estes são os 8º mais comuns.

316
00:18:46,480 --> 00:18:49,440
O primeiro é qual, depois do qual existe ali e ali.

317
00:18:49,440 --> 00:18:53,020
O primeiro em si não é o primeiro, mas o 9º, e faz sentido

318
00:18:53,020 --> 00:18:57,840
que essas outras palavras possam surgir com mais frequência, onde as que vêm

319
00:18:57,840 --> 00:18:59,000
depois do primeiro são depois, onde, e aquelas são um pouco menos comuns.

320
00:18:59,000 --> 00:19:04,400
Agora, ao usar estes dados para modelar a probabilidade de cada uma destas

321
00:19:04,400 --> 00:19:06,760
palavras ser a resposta final, não deve ser apenas proporcional à frequência.

322
00:19:07,020 --> 00:19:12,560
Por exemplo, que recebe uma pontuação de 0. 002 neste conjunto de dados, enquanto a

323
00:19:12,560 --> 00:19:15,200
palavra trança é, em certo sentido, cerca de 1000 vezes menos provável.

324
00:19:15,200 --> 00:19:19,400
Mas ambas são palavras comuns o suficiente para que quase certamente valha a pena considerá-las.

325
00:19:19,400 --> 00:19:21,900
Então, queremos mais um corte binário.

326
00:19:21,900 --> 00:19:26,520
A maneira como fiz isso foi imaginar pegar toda essa lista ordenada de palavras e,

327
00:19:26,520 --> 00:19:31,060
em seguida, organizá-la em um eixo x e, em seguida, aplicar a função sigmóide,

328
00:19:31,060 --> 00:19:35,540
que é a maneira padrão de ter uma função cuja saída é basicamente binária, é

329
00:19:35,540 --> 00:19:38,500
ou 0 ou 1, mas há uma suavização intermediária para essa região de incerteza.

330
00:19:38,500 --> 00:19:43,900
Então, essencialmente, a probabilidade que estou atribuindo a cada palavra por estar na lista final

331
00:19:43,900 --> 00:19:49,540
será o valor da função sigmóide acima, onde quer que ela esteja no eixo x.

332
00:19:49,540 --> 00:19:53,940
Agora, obviamente, isso depende de alguns parâmetros, por exemplo, a largura do espaço no

333
00:19:53,940 --> 00:19:59,660
eixo x que essas palavras preenchem determina quão gradual ou abruptamente caímos de 1

334
00:19:59,660 --> 00:20:03,000
para 0, e onde as situamos da esquerda para a direita determina o corte.

335
00:20:03,160 --> 00:20:07,340
Para ser sincero, fiz isso apenas lambendo o dedo e apontando-o contra o vento.

336
00:20:07,340 --> 00:20:10,800
Examinei a lista classificada e tentei encontrar uma janela onde, quando olhei

337
00:20:10,800 --> 00:20:15,280
para ela, descobri que cerca de metade dessas palavras têm maior probabilidade

338
00:20:15,280 --> 00:20:17,680
de ser a resposta final, e usei isso como ponto de corte.

339
00:20:17,680 --> 00:20:21,840
Uma vez que tenhamos uma distribuição como esta entre as palavras, teremos

340
00:20:21,840 --> 00:20:24,460
outra situação em que a entropia se torna uma medida realmente útil.

341
00:20:24,460 --> 00:20:28,480
Por exemplo, digamos que estamos jogando um jogo e começamos com meus

342
00:20:28,480 --> 00:20:32,480
antigos abridores, que eram penas e pregos, e terminamos com uma

343
00:20:32,480 --> 00:20:33,760
situação em que há quatro palavras possíveis que combinam com ele.

344
00:20:33,760 --> 00:20:36,440
E digamos que consideramos todos igualmente prováveis.

345
00:20:36,440 --> 00:20:40,000
Deixe-me perguntar: qual é a entropia dessa distribuição?

346
00:20:40,000 --> 00:20:45,920
Bem, a informação associada a cada uma dessas possibilidades será o log de base 2

347
00:20:45,920 --> 00:20:50,800
de 4, já que cada uma é 1 e 4, e isso é 2.

348
00:20:50,800 --> 00:20:52,780
Duas informações, quatro possibilidades.

349
00:20:52,780 --> 00:20:54,360
Tudo muito bem e bom.

350
00:20:54,360 --> 00:20:58,320
Mas e se eu te dissesse que na verdade são mais de quatro partidas?

351
00:20:58,320 --> 00:21:02,600
Na realidade, quando olhamos a lista completa de palavras, há 16 palavras que correspondem a ela.

352
00:21:02,600 --> 00:21:07,260
Mas suponha que nosso modelo atribua uma probabilidade muito baixa a essas outras 12 palavras de

353
00:21:07,260 --> 00:21:11,440
serem realmente a resposta final, algo como 1 em 1.000, porque elas são realmente obscuras.

354
00:21:11,440 --> 00:21:15,480
Agora deixe-me perguntar: qual é a entropia desta distribuição?

355
00:21:15,480 --> 00:21:19,600
Se a entropia medisse puramente o número de correspondências aqui, então você poderia

356
00:21:19,600 --> 00:21:24,760
esperar que fosse algo como o log de base 2 de 16, que

357
00:21:24,760 --> 00:21:26,200
seria 4, dois bits a mais de incerteza do que tínhamos antes.

358
00:21:26,200 --> 00:21:30,320
Mas é claro que a incerteza real não é muito diferente daquela que tínhamos antes.

359
00:21:30,320 --> 00:21:33,840
Só porque existem essas 12 palavras realmente obscuras não significa que seria

360
00:21:33,840 --> 00:21:38,200
ainda mais surpreendente saber que a resposta final é charme, por exemplo.

361
00:21:38,200 --> 00:21:42,080
Então, quando você realmente faz o cálculo aqui e soma a probabilidade de cada

362
00:21:42,080 --> 00:21:45,960
ocorrência vezes a informação correspondente, o que você obtém é 2. 11 bits.

363
00:21:45,960 --> 00:21:50,280
Só estou dizendo que são basicamente dois bits, basicamente essas quatro possibilidades, mas

364
00:21:50,280 --> 00:21:54,240
há um pouco mais de incerteza por causa de todos esses eventos altamente

365
00:21:54,240 --> 00:21:57,120
improváveis, embora se você os aprendesse, obteria uma tonelada de informações com isso.

366
00:21:57,120 --> 00:22:00,800
Diminuindo o zoom, isso é parte do que torna o Wordle

367
00:22:00,800 --> 00:22:01,800
um bom exemplo para uma aula de teoria da informação.

368
00:22:01,800 --> 00:22:05,280
Temos essas duas aplicações de sentimento distintas para a entropia.

369
00:22:05,280 --> 00:22:09,640
O primeiro nos diz qual é a informação esperada que obteremos

370
00:22:09,640 --> 00:22:14,560
de uma determinada suposição, e o segundo diz se podemos medir

371
00:22:14,560 --> 00:22:16,480
a incerteza restante entre todas as palavras que temos possíveis.

372
00:22:16,480 --> 00:22:19,800
E devo enfatizar, nesse primeiro caso em que estamos olhando para a informação esperada de um palpite,

373
00:22:19,800 --> 00:22:25,000
uma vez que temos um peso desigual para as palavras, isso afeta o cálculo da entropia.

374
00:22:25,000 --> 00:22:28,600
Por exemplo, deixe-me abordar o mesmo caso que vimos anteriormente

375
00:22:28,600 --> 00:22:33,560
da distribuição associada a Weary, mas desta vez usando

376
00:22:33,560 --> 00:22:34,560
uma distribuição não uniforme em todas as palavras possíveis.

377
00:22:34,560 --> 00:22:39,360
Então deixe-me ver se consigo encontrar uma parte aqui que ilustre isso muito bem.

378
00:22:39,360 --> 00:22:42,480
Ok, aqui isso é muito bom.

379
00:22:42,480 --> 00:22:46,360
Aqui temos dois padrões adjacentes que são igualmente prováveis, mas nos disseram

380
00:22:46,360 --> 00:22:49,480
que um deles tem 32 palavras possíveis que correspondem a ele.

381
00:22:49,480 --> 00:22:54,080
E se verificarmos o que são, estas são aquelas 32, que

382
00:22:54,080 --> 00:22:55,600
são apenas palavras muito improváveis quando você olha para elas.

383
00:22:55,600 --> 00:23:00,400
É difícil encontrar alguma que pareça ser uma resposta plausível, talvez gritos,

384
00:23:00,400 --> 00:23:04,440
mas se olharmos para o padrão vizinho na distribuição, que é considerado

385
00:23:04,440 --> 00:23:08,920
quase tão provável, somos informados de que ele só tem 8 correspondências

386
00:23:08,920 --> 00:23:09,920
possíveis, então um quarto como muitas partidas, mas é quase tão provável.

387
00:23:09,920 --> 00:23:12,520
E quando puxamos esses fósforos, podemos ver porquê.

388
00:23:12,520 --> 00:23:17,840
Algumas delas são respostas realmente plausíveis, como anel, ou ira, ou batidas.

389
00:23:17,840 --> 00:23:22,000
Para ilustrar como incorporamos tudo isso, deixe-me trazer a versão 2 do Wordlebot aqui,

390
00:23:22,000 --> 00:23:25,960
e há duas ou três diferenças principais em relação à primeira que vimos.

391
00:23:25,960 --> 00:23:29,460
Em primeiro lugar, como acabei de dizer, a forma como calculamos estas entropias,

392
00:23:29,460 --> 00:23:34,800
estes valores esperados de informação, utiliza agora distribuições mais refinadas entre os padrões

393
00:23:34,800 --> 00:23:39,300
que incorporam a probabilidade de uma determinada palavra ser realmente a resposta.

394
00:23:39,300 --> 00:23:44,160
Acontece que as lágrimas ainda são o número 1, embora as seguintes sejam um pouco diferentes.

395
00:23:44,160 --> 00:23:47,920
Em segundo lugar, quando classificar as suas principais escolhas, irá manter um modelo da probabilidade

396
00:23:47,920 --> 00:23:52,600
de cada palavra ser a resposta real, e irá incorporar isso na sua decisão, o

397
00:23:52,600 --> 00:23:55,520
que é mais fácil de ver quando tivermos algumas suposições sobre a resposta. mesa.

398
00:23:55,520 --> 00:24:01,120
Mais uma vez, ignorando a sua recomendação porque não podemos permitir que as máquinas governem as nossas vidas.

399
00:24:01,120 --> 00:24:05,160
E suponho que devo mencionar outra coisa diferente aqui à esquerda, que o valor da incerteza,

400
00:24:05,160 --> 00:24:10,080
esse número de bits, não é mais apenas redundante com o número de correspondências possíveis.

401
00:24:10,080 --> 00:24:16,520
Agora, se puxarmos para cima e calcularmos 2 elevado a 8. 02, que está um pouco acima de

402
00:24:16,520 --> 00:24:22,640
256, acho que 259, o que está dizendo é que embora haja um total de

403
00:24:22,640 --> 00:24:26,400
526 palavras que realmente correspondam a esse padrão, a quantidade de incerteza que ele

404
00:24:26,400 --> 00:24:29,760
tem é mais parecida com o que seria se houvesse 259 igualmente prováveis resultados.

405
00:24:29,760 --> 00:24:31,100
Você pode pensar assim.

406
00:24:31,100 --> 00:24:35,560
Ele sabe que borx não é a resposta, o mesmo acontece com yorts, zorl

407
00:24:35,560 --> 00:24:37,840
e zorus, então é um pouco menos incerto do que no caso anterior.

408
00:24:37,840 --> 00:24:40,220
Este número de bits será menor.

409
00:24:40,220 --> 00:24:44,040
E se eu continuar jogando, estou refinando isso com algumas

410
00:24:44,040 --> 00:24:48,680
suposições que são pertinentes ao que gostaria de explicar aqui.

411
00:24:48,680 --> 00:24:52,520
Na quarta estimativa, se você olhar as principais opções, verá

412
00:24:52,520 --> 00:24:53,800
que não se trata mais apenas de maximizar a entropia.

413
00:24:53,800 --> 00:24:58,480
Então, neste ponto, existem tecnicamente sete possibilidades, mas as

414
00:24:58,480 --> 00:25:00,780
únicas com uma chance significativa são dormitórios e palavras.

415
00:25:00,780 --> 00:25:04,760
E você pode ver que ele classifica escolhendo ambos acima de

416
00:25:04,760 --> 00:25:07,560
todos esses outros valores, que estritamente falando dariam mais informações.

417
00:25:07,560 --> 00:25:11,200
Na primeira vez que fiz isso, apenas somei esses dois números para medir a

418
00:25:11,200 --> 00:25:14,580
qualidade de cada palpite, o que na verdade funcionou melhor do que você imagina.

419
00:25:14,580 --> 00:25:17,600
Mas realmente não parecia sistemático, e tenho certeza de que há outras

420
00:25:17,600 --> 00:25:19,880
abordagens que as pessoas poderiam adotar, mas aqui está a que encontrei.

421
00:25:19,880 --> 00:25:24,200
Se estivermos considerando a perspectiva de um próximo palpite, como neste caso palavras, o

422
00:25:24,200 --> 00:25:28,440
que realmente nos importa é a pontuação esperada do nosso jogo se fizermos isso.

423
00:25:28,440 --> 00:25:32,880
E para calcular a pontuação esperada, dizemos qual é a probabilidade de

424
00:25:32,880 --> 00:25:35,640
as palavras serem a resposta real, que no momento descreve 58%.

425
00:25:36,080 --> 00:25:40,400
Dizemos que com 58% de chance nossa pontuação neste jogo seria 4.

426
00:25:40,400 --> 00:25:46,240
E então, com a probabilidade de 1 menos 58%, nossa pontuação será maior que 4.

427
00:25:46,240 --> 00:25:50,640
Quanto mais não sabemos, mas podemos estimá-lo com base na quantidade

428
00:25:50,640 --> 00:25:52,920
de incerteza que provavelmente haverá quando chegarmos a esse ponto.

429
00:25:52,920 --> 00:25:56,600
Especificamente, no momento há 1. 44 bits de incerteza.

430
00:25:56,600 --> 00:26:01,560
Se adivinharmos as palavras, isso nos diz que a informação esperada que obteremos é 1. 27 bits.

431
00:26:01,560 --> 00:26:06,280
Portanto, se adivinharmos as palavras, esta diferença representa quanta

432
00:26:06,280 --> 00:26:08,280
incerteza provavelmente nos restará depois que isso acontecer.

433
00:26:08,280 --> 00:26:12,500
O que precisamos é de algum tipo de função, que chamo

434
00:26:12,500 --> 00:26:13,880
de f aqui, que associe essa incerteza a uma pontuação esperada.

435
00:26:13,880 --> 00:26:18,040
E a maneira como isso aconteceu foi apenas traçar um monte de dados de

436
00:26:18,040 --> 00:26:23,920
jogos anteriores com base na versão 1 do bot para dizer, ei, qual

437
00:26:23,920 --> 00:26:27,040
foi a pontuação real após vários pontos com certas quantidades mensuráveis de incerteza.

438
00:26:27,040 --> 00:26:31,120
Por exemplo, esses pontos de dados aqui estão acima de um valor próximo a 8. 7

439
00:26:31,120 --> 00:26:36,840
ou mais são o que dizem para alguns jogos depois de um ponto em que havia 8. 7 bits de

440
00:26:36,840 --> 00:26:39,340
incerteza, foram necessárias duas tentativas para obter a resposta final.

441
00:26:39,340 --> 00:26:43,180
Para outros jogos foram necessários três palpites, para outros jogos foram necessários quatro palpites.

442
00:26:43,180 --> 00:26:46,920
Se mudarmos para a esquerda aqui, todos os pontos acima de zero dizem que sempre

443
00:26:46,920 --> 00:26:51,620
que há zero bits de incerteza, o que significa que há apenas uma possibilidade,

444
00:26:51,620 --> 00:26:55,000
então o número de suposições necessárias é sempre apenas um, o que é reconfortante.

445
00:26:55,000 --> 00:26:59,020
Sempre que havia um pouco de incerteza, o que significa que

446
00:26:59,020 --> 00:27:02,360
se resumia essencialmente a duas possibilidades, às vezes era necessário

447
00:27:02,360 --> 00:27:03,940
mais um palpite, às vezes era necessário mais dois palpites.

448
00:27:03,940 --> 00:27:05,980
E assim por diante aqui.

449
00:27:05,980 --> 00:27:11,020
Talvez uma maneira um pouco mais fácil de visualizar esses dados seja agrupá-los e tirar médias.

450
00:27:11,020 --> 00:27:15,940
Por exemplo, esta barra aqui diz que entre todos os pontos onde tivemos um pouco de

451
00:27:15,940 --> 00:27:22,420
incerteza, em média o número de novas suposições necessárias foi de cerca de 1. 5.

452
00:27:22,420 --> 00:27:25,920
E a barra aqui dizendo entre todos os jogos diferentes onde em

453
00:27:25,920 --> 00:27:30,480
algum momento a incerteza estava um pouco acima de quatro bits, o

454
00:27:30,480 --> 00:27:35,120
que é como reduzi-la a 16 possibilidades diferentes, então, em média, requer

455
00:27:35,120 --> 00:27:36,240
um pouco mais de duas suposições a partir desse ponto avançar.

456
00:27:36,240 --> 00:27:40,080
E a partir daqui fiz apenas uma regressão para ajustar uma função que parecesse razoável para isso.

457
00:27:40,080 --> 00:27:44,160
E lembre-se que o objetivo de fazer isso é para que possamos quantificar essa intuição

458
00:27:44,160 --> 00:27:49,720
de que quanto mais informações obtivermos de uma palavra, menor será a pontuação esperada.

459
00:27:49,720 --> 00:27:54,380
Então, com isso como versão 2. 0, se voltarmos e executarmos o mesmo conjunto de simulações,

460
00:27:54,380 --> 00:27:59,820
fazendo-o jogar contra todas as 2.315 respostas possíveis do Wordle, como funciona?

461
00:27:59,820 --> 00:28:04,060
Bem, em contraste com a nossa primeira versão, é definitivamente melhor, o que é reconfortante.

462
00:28:04,060 --> 00:28:08,780
Tudo dito e feito, a média é de cerca de 3. 6, embora ao contrário da primeira versão

463
00:28:08,780 --> 00:28:12,820
haja algumas vezes que perde e requer mais de seis nesta circunstância.

464
00:28:12,820 --> 00:28:15,980
Presumivelmente porque há momentos em que é preciso fazer essa troca

465
00:28:15,980 --> 00:28:18,980
para realmente atingir o objetivo, em vez de maximizar as informações.

466
00:28:18,980 --> 00:28:22,140
Então, podemos fazer melhor que 3. 6?

467
00:28:22,140 --> 00:28:23,260
Nós definitivamente podemos.

468
00:28:23,260 --> 00:28:27,120
Agora eu disse no início que é mais divertido tentar não incorporar a

469
00:28:27,120 --> 00:28:29,980
verdadeira lista de respostas do Wordle na maneira como ela constrói seu modelo.

470
00:28:29,980 --> 00:28:35,180
Mas se incorporarmos isso, o melhor desempenho que consegui foi em torno de 3. 43.

471
00:28:35,180 --> 00:28:39,520
Então, se tentarmos ser mais sofisticados do que apenas usar dados de frequência de palavras para escolher

472
00:28:39,520 --> 00:28:44,220
esta distribuição anterior, este 3. 43 provavelmente dá um máximo de quão bom poderíamos ser

473
00:28:44,220 --> 00:28:46,360
com isso, ou pelo menos quão bom eu poderia ser com isso.

474
00:28:46,360 --> 00:28:50,240
Esse melhor desempenho utiliza essencialmente as ideias de que falei aqui,

475
00:28:50,240 --> 00:28:53,400
mas vai um pouco mais longe, como se procurasse a informação

476
00:28:53,400 --> 00:28:55,660
esperada dois passos à frente em vez de apenas um.

477
00:28:55,660 --> 00:28:58,720
Originalmente eu estava planejando falar mais sobre

478
00:28:58,720 --> 00:29:00,580
isso, mas percebo que já demoramos bastante.

479
00:29:00,580 --> 00:29:03,520
A única coisa que direi é que depois de fazer essa pesquisa em

480
00:29:03,520 --> 00:29:07,720
duas etapas e, em seguida, executar algumas simulações de amostra nos principais candidatos,

481
00:29:07,720 --> 00:29:09,500
até agora, pelo menos para mim, parece que Crane é o melhor abridor.

482
00:29:09,500 --> 00:29:11,080
Quem teria adivinhado?

483
00:29:11,080 --> 00:29:15,680
Além disso, se você usar a verdadeira lista de palavras para determinar seu espaço de

484
00:29:15,680 --> 00:29:17,920
possibilidades, a incerteza com a qual você começa será de pouco mais de 11 bits.

485
00:29:18,160 --> 00:29:22,760
E acontece que, apenas a partir de uma pesquisa de força bruta, o máximo possível

486
00:29:22,760 --> 00:29:26,580
de informações esperadas após as duas primeiras tentativas é de cerca de 10 bits.

487
00:29:26,580 --> 00:29:31,720
O que sugere que, na melhor das hipóteses, após suas duas primeiras suposições,

488
00:29:31,720 --> 00:29:35,220
com um jogo perfeitamente ideal, você ficará com um pouco de incerteza.

489
00:29:35,220 --> 00:29:37,400
O que é o mesmo que ter duas suposições possíveis.

490
00:29:37,400 --> 00:29:41,440
Então eu acho que é justo e provavelmente bastante conservador dizer que você nunca poderia

491
00:29:41,440 --> 00:29:45,620
escrever um algoritmo que obtivesse essa média tão baixa quanto 3, porque com as

492
00:29:45,620 --> 00:29:50,460
palavras disponíveis, simplesmente não há espaço para obter informações suficientes depois de apenas duas

493
00:29:50,460 --> 00:29:53,820
etapas. capaz de garantir a resposta no terceiro slot todas as vezes, sem falhar.

