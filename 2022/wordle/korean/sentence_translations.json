[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy.",
  "translatedText": "",
  "from_community_srt": "Wordle 게임은 지난 한두 달 사이에 입소문이 났고 수학 수업의 기회를 놓칠 수 없는 게임입니다. 이 게임은 정보 이론, 특히 엔트로피라는 주제에 대한 수업에서 매우 좋은 중심적인 예시를 만든다는 생각이 듭니다.",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could.",
  "translatedText": "",
  "from_community_srt": "많은 사람들이 이 퍼즐에 빠져들었다는 것을 알 수 있습니다. 그리고 많은 프로그래머들처럼 저도 최대한 최상으로 게임을 할 수 있는 알고리즘을 만들려고 노력했습니다.",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy.",
  "translatedText": "",
  "from_community_srt": "제가 이번에 할 것은 여러분에게 제 과정 중 일부를 이야기하고, 거기에 관련된 수학에 대해 설명하는 것입니다. 왜냐하면 이 전체 알고리즘이 엔트로피에 대한 아이디어를 중심으로 하기 때문입니다.",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle?",
  "translatedText": "",
  "from_community_srt": "먼저, 여러분이 아직 들어보지 못하셨을지 모르겠지만,",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us.",
  "translatedText": "",
  "from_community_srt": "Wordle 이 무엇인지 그리고 게임의 규칙을 살펴보는 동안 일석이조로, 또한 이 게임의 진행 방향을 알려드리겠습니다. 그니까, 기본적으로 게임을 플레이할 수 있는 작은 알고리즘을 개발하는 것입니다.",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does.",
  "translatedText": "",
  "from_community_srt": "전 \"지금\" 2월 4일인데, Wordle을 아직 안 해 봤고, 봇이 어떻게 돌아가는지 봅시다.",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess.",
  "translatedText": "",
  "from_community_srt": "Wordle의 목표는 다섯 글자로 된 미스터리 단어를 추측하는 것이고, 당신은 여섯 번의 추측 기회가 주어집니다. 예시로,",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane.",
  "translatedText": "",
  "from_community_srt": "저의 wordlebot은 저한테 \"crane\"으로 추측을 시작하라고 제안합니다.",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer.",
  "translatedText": "",
  "from_community_srt": "여러분이 추측할 때마다, 여러분은 추측이 얼마나 답에 가까운지에 대한 정보를 얻게 됩니다.",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer.",
  "translatedText": "",
  "from_community_srt": "여기 회색 박스는 실제 답에 'c'가 없다고 알려 주고 노란색 박스는 'r'이 있다고 표시 되는데 그 위치가 아니라고 알려줍니다.",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position.",
  "translatedText": "",
  "from_community_srt": "녹색 박스는 단어에 'a'가 있고 세 번째 위치에 있다고 알려줍니다.",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E.",
  "translatedText": "",
  "from_community_srt": "그리고 여기에 'n'도 없고 'e'도 없습니다.",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information.",
  "translatedText": "",
  "from_community_srt": "그리고 wordlebot에게 이 정보를 전하겠습니다.",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey.",
  "translatedText": "",
  "from_community_srt": "\"crane\"이 회색 노란색 초록 회색 회색으로 됐죠.",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time.",
  "translatedText": "",
  "from_community_srt": "지금 보여지는 데이터는 신경쓰지 마세요, 때가 되면 설명해 드릴게요.",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick.",
  "translatedText": "",
  "from_community_srt": "두 번째 제안은 \"shtik\" 입니다.",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess.",
  "translatedText": "",
  "from_community_srt": "여러분의 추측은 실제 5글자여야 합니다. 하지만 보시다시피 여러분이 추측할 수 있는 단어들은 꽤 자유분방 합니다.",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick.",
  "translatedText": "",
  "from_community_srt": "이 경우엔 \"shtik\"를 써보죠.",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good.",
  "translatedText": "",
  "from_community_srt": "그리고... 좋아요! 상황이 꽤 좋아 보입니다.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R.",
  "translatedText": "",
  "from_community_srt": "우리는 's'와 'h'를 맞혔습니다. 이와 같이 우리는 3번째 글자까지 알게 됐고, 'r' 도 있다는 것을 압니다.",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something.",
  "translatedText": "",
  "from_community_srt": "그러면 이 단어는 's-h-a-무언가-r' 또는 's-h-a-r-무언가' 가 됩니다.",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp.",
  "translatedText": "",
  "from_community_srt": "Wordle-bot은 두 가지 가능성, 즉 \"shard\"와 \"sharp\"를 알고 있는 것 같습니다.",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a tossup between them at this point, so I guess probably just because it's alphabetical it goes with shard.",
  "translatedText": "",
  "from_community_srt": "이 경우엔 반반이죠. 알파벳 순으로 정렬하니까'shard'로 해보고, 그리고... 와!",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer, so we got it in three.",
  "translatedText": "",
  "from_community_srt": "진짜 답입니다. 우린 3번만에 해냈습니다.",
  "n_reviews": 0,
  "start": 131.22,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle, four is par and three is birdie.",
  "translatedText": "",
  "from_community_srt": "이게 과연 좋은 것인지 궁금하다면, 제가 어떤 사람 말을 들어보면 Wordle에서는 4는 파,",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy.",
  "translatedText": "",
  "from_community_srt": "3은 버디라는 표현하는데 꽤 적절한 비유라고 생각합니다. (골프 용어: 파,",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy.",
  "translatedText": "",
  "from_community_srt": "버디) 꾸준히 게임을 해야 4점을 받을 수 있지만 그렇게 어려운 것은 아닙니다.",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great.",
  "translatedText": "",
  "from_community_srt": "하지만 3번만에 성공하면, 그냥 기분이 좋아요.",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot.",
  "translatedText": "",
  "from_community_srt": "여러분이 괜찮으시다면 제가 여기서 하고 싶은 것은 제가 wordlebot에 어떻게 접근하는지에 대한 제 생각의 과정을 처음부터 쭉 살펴보는 것입니다.",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson.",
  "translatedText": "",
  "from_community_srt": "그리고 제가 정말 정보이론 수업을 위한 것이라고 변명했듯이, 주요 목표는 정보란 무엇이고 엔트로피는 무엇인지를 설명하는 것입니다.",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy.",
  "translatedText": "",
  "from_community_srt": "36 00:02:48,320 --> 00:02:52,240 이것을 만들면서 제가 처음 생각한 것은 알파벳의 상대적인 빈도를 살펴보는 것이었습니다.",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters?",
  "translatedText": "",
  "from_community_srt": "저는 가장 빈번한 알파벳들이 쓰이는, 첫 번째의 추측 단어나 그 추측의 쌍이 있는지 생각했습니다.",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails.",
  "translatedText": "",
  "from_community_srt": "제가 좀 끌리는 것은 \"other\" 다음에 \"nails\"을 하는 것 입니다.",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good, it feels like you're getting information.",
  "translatedText": "",
  "from_community_srt": "글자를 치면 녹색이나 노란색이 나오는데 이것이 정보를 얻는거 같아서, 기분이 언제나 좋습니다.",
  "n_reviews": 0,
  "start": 183.76,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information, since it's pretty rare to find a word that doesn't have any of these letters.",
  "translatedText": "",
  "from_community_srt": "하지만 이런 경우엔, 다 못 맞추고 항상 회색을 얻는다 해도 여전히 많은 정보를 얻을 수 있습니다. 왜냐하면 이런 글자들이 없는 단어를 찾는 것은 매우 드물기 때문입니다.",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters.",
  "translatedText": "",
  "from_community_srt": "하지만 여전히 뭔가 체계적인 느낌이 들지 않습니다. 예를 들어, 글자의 순서를 고려하는 것은 쓸모 없기 때문입니다.",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail?",
  "translatedText": "",
  "from_community_srt": "왜 제가 \"snail\"을 쓸 수 있었는데 왜 \"nails\"를 할까요? 끝에 's'가 있는 것이 더 나을까요?",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure.",
  "translatedText": "",
  "from_community_srt": "솔직히 저도 잘 몰라요.",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y.",
  "translatedText": "",
  "from_community_srt": "제 친구가 말하길 \"weary\"라는 단어로 시작하는 걸 좋아한다고 하더군요. 저는 그 안에 'w'나 'y'같은 흔치 않은 글자가 들어있어서 좀 놀랐습니다.",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener.",
  "translatedText": "",
  "from_community_srt": "하지만 누가 알겠어요, 어쩌면 그게 더 나은 추측일 수도 있습니다.",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess?",
  "translatedText": "",
  "from_community_srt": "잠재적인 추측의 양질을 판단하기 위해,",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up.",
  "translatedText": "",
  "from_community_srt": "우리가 줄 수 있는 양적 점수가 있을까요? 가능한 추측의 순위를 매기는 방식을 설정하려면, 뒤로 돌아가서 게임이 정확히 어떻게 설계되었는지에 대해 조금 더 명확히 설명하겠습니다.",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long.",
  "translatedText": "",
  "from_community_srt": "여기에 여러분이 입력이 가능한 52 00:03:54,000 --> 00:03:58,640 약 13,000개의 단어 목록이 있는데, 모두 유효한 추측으로 여겨집니다.",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble.",
  "translatedText": "",
  "from_community_srt": "하지만 이것을 보면 \"aahed\"나 \"aalii\" 그리고 \"aargh\"과 같은 정말 흔치 않은 것들이 많이 있습니다. 이런 종류의 단어들은 Scrabble(보드게임)에서 가족끼리 말다툼을 불러오는 단어입니다.",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers.",
  "translatedText": "",
  "from_community_srt": "하지만 게임의 분위기로 봐서는 정답은 항상 꽤 흔한 단어일 것이고, 실제로 답이 될 수 있는 2300개의 단어 목록이 있습니다.",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creators girlfriend, which is kind of fun.",
  "translatedText": "",
  "from_community_srt": "이것은 인간이 만든 목록들인데, 특히 게임 제작자의 여자친구가 재미있다고 생각합니다. (어어...",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving wordle that doesn't incorporate previous knowledge about this list.",
  "translatedText": "",
  "from_community_srt": "여친이 있어?) 하지만 제가 하고 싶은 것은, 이 과제에 대한 우리의 도전은, 이 목록에 대한 지식을 포함하지 않고 Wordle을 해결하는 프로그램을 만들 수 있는지 알아보는 것입니다.",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list.",
  "translatedText": "",
  "from_community_srt": "한 가지 예로, 여러분이 그 목록에서 찾을 수 없는 꽤 흔한 다섯 글자의 단어들이 많이 있습니다.",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play wordle against anyone, not just what happens to be the official website.",
  "translatedText": "",
  "from_community_srt": "그래서 단지 공식 웹사이트가 아닌, 조금 더 탄력적이고 누구와도 wordle을 플레이 할 수 있는 프로그램을 작성하는 것이 더 나을 것입니다.",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also, the reason that we know what this list of possible answers is, is because it's visible in the source code.",
  "translatedText": "",
  "from_community_srt": "그리고 우리가 이 가능한 답들의 목록을 알고 있는 이유는, 소스 코드에서 볼 수 있기 때문입니다.",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day, that you could always just look up what tomorrow's answer will be.",
  "translatedText": "",
  "from_community_srt": "그래서 소스 코드를 보면, 매일매일 답이 특정한 순서로 되어 있기 때문에 내일의 답이 무엇일지 항상 알 수 있습니다.",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating.",
  "translatedText": "",
  "from_community_srt": "그러지만 분명히 목록을 사용하는 것은 부정행위이고,",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words.",
  "translatedText": "",
  "from_community_srt": "더 흥미로운 퍼즐과 더 풍부한 정보이론의 수업을 만들기 위해 더욱 보편적인 데이터를 사용하는 것입니다. 예를 들어, 상대적인 단어 빈도와 같은, 보다 일반적인 단어들을 선호하는 이 직관을 갖기 위해서 말이죠.",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess?",
  "translatedText": "",
  "from_community_srt": "그래서! 이 13,000개의 가능성 중에서, 첫 추측을 어떻게 선택해야 할까요? 예를 들어,",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality?",
  "translatedText": "",
  "from_community_srt": "만약 제 친구가 \"weary\"을 제안한다면,",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W.",
  "translatedText": "",
  "from_community_srt": "그것의 양질을 어떻게 분석해야 할까요? 글쎄, 그가 그렇게까지 좋아 할 거 같지 않은 'w'를 좋아한다고 말한 이유는, 그 승산이 거의 없는 'w'를 맞추면 기분이 아주 좋기 때문입니다.",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern.",
  "translatedText": "",
  "from_community_srt": "예를 들어, 만약 처음 패턴이 이렇게 생기면, 이 거대한 단어 목록에서 그 패턴과 일치하는 단어는 58개만 있습니다.",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000.",
  "translatedText": "",
  "from_community_srt": "이것은 13,000개에서 엄청나게 줄어든 것입니다.",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this.",
  "translatedText": "",
  "from_community_srt": "하지만 물론 이런 패턴이 나타나는 것은 매우 드문 일입니다.",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000.",
  "translatedText": "",
  "from_community_srt": "구체적으로, 각 단어가 답이 될 확률이 같다면 이 패턴을 맞출 확률은 58을 13,000 정도로 나눈 것입니다.",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers.",
  "translatedText": "",
  "from_community_srt": "물론, 이것들이 똑같이 답이 되지는 않고,",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words.",
  "translatedText": "",
  "from_community_srt": "대부분 매우 불명확하고, 심지어 의문스러운 단어들이지만,",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later.",
  "translatedText": "",
  "from_community_srt": "적어도 첫 번째 통과에 대해서는 모두 동등하다고 가정하고,",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is, the pattern with a lot of information is, by its very nature, unlikely to occur.",
  "translatedText": "",
  "from_community_srt": "조금 후에 다시 다듬어 봅시다. 요점은 정보가 많은 패턴은 본질적으로 발생할 가능성이 낮다는 것입니다.",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely.",
  "translatedText": "",
  "from_community_srt": "사실 \"유용한 정보\"가 의미하는 바는 \"그러지 않을 경우\" 입니다.",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where, of course, there's not a W in it.",
  "translatedText": "",
  "from_community_srt": "이런 시작점에서 볼 수 있는 훨씬 더 가능성 있는 패턴은 이런 것입니다. 물론 그 안에 'w'가 없고,",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y.",
  "translatedText": "",
  "from_community_srt": "'e'가 없을 수도 있고, 'a'가 없을 수도 있고, 'r'가 없을 수도 있고,",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches.",
  "translatedText": "",
  "from_community_srt": "'y'가 없을 수도 있습니다.",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see.",
  "translatedText": "",
  "from_community_srt": "이 경우 1,400개의 가능한 일치하는 항목이 있습니다. 만약 모든 것이 동등하다면 11%의 확률로 여러분이 볼 수 있는 패턴입니다.",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative.",
  "translatedText": "",
  "from_community_srt": "그래서 가장 가능성이 높은 결과이더라도 가장 정보가 적습니다.",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see.",
  "translatedText": "",
  "from_community_srt": "좀 더 포괄적인 시야를 얻기 위해 다양한 모든 패턴의 확률 분포를 보여드리겠습니다.",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common.",
  "translatedText": "",
  "from_community_srt": "여러분이 보고 있는 각각의 막대는 나타날 수 있는 색깔의 패턴들과 일치하는데, 3⁵개의 가능성이 있습니다. 그리고 그것들은 왼쪽에서 오른쪽으로, 즉 가장 흔한 것에서 가장 흔하지 않는 것으로 구성되어 있습니다. 여기서,",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays.",
  "translatedText": "",
  "from_community_srt": "모두 회색을 얻는 가장 일반적인 가능성은 14% 정도 발생합니다.",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here, where there's only 18 possibilities for what matches this pattern, that evidently look like this.",
  "translatedText": "",
  "from_community_srt": "여러분이 희망하는 것은 이 긴 꼬리 어딘가에 있다는 것입니다. 이 패턴과 일치하는 것은 18개의 가능성 밖에 없는, 이런 단어들이 있습니다.",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here.",
  "translatedText": "",
  "from_community_srt": "아니면 조금 더 오른쪽으로 간다면... 여기까지 가본다면... 좋아요,",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you.",
  "translatedText": "",
  "from_community_srt": "여기 여러분한테 좋은 퍼즐이 있습니다.",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them?",
  "translatedText": "",
  "from_community_srt": "영어에서 'w'로 시작하고 'y'로 끝나며 어딘가에 'r'가 있는 세 단어는 무엇일까요? 답을 드러내보고...",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly.",
  "translatedText": "",
  "from_community_srt": "한 번 보도록 하죠... 'wordy', 'wormy', 'wrily' 이네요.",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution.",
  "translatedText": "",
  "from_community_srt": "이 단어가 전체적으로 얼마나 좋은지 판단하기 위해서, 우리는 이 분포에서 얻게 될 정보의 양에 대한 일종의 측정값이 필요합니다.",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score.",
  "translatedText": "",
  "from_community_srt": "만약 우리가 각각의 패턴을 살펴보고 이 패턴이 얼마나 유용한지 측정하는 발생 확률과 \"무언가\"를 곱한다면, 그 값은 우리에게 객관적인 점수로 될 것입니다.",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches.",
  "translatedText": "",
  "from_community_srt": "이제,",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches.",
  "translatedText": "",
  "from_community_srt": "어떤 것이 일치해야 하는지에 대한 첫 번째 직관은 여러분은 일치 횟수의 평균값을 낮추기 이겠지만, 저는 그 대신 정보에 결과를 두는,",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer.",
  "translatedText": "",
  "from_community_srt": "더욱 보편적인 측정을 사용하고 싶습니다. 이 13,000개의 단어 각각에 실제로 답인지 아닌지에 대해 각각 다른 확률이 할당되면 더 웅통성이 있을 것입니다.",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but is really intuitive if we just look at examples.",
  "translatedText": "",
  "from_community_srt": "정보의 표준 단위는 비트인데, 조금은 재미있는 공식을 가지고 있고, 예시를 보면 정말 직관적입니다.",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information.",
  "translatedText": "",
  "from_community_srt": "만약 여러분이 가능성의 공간을 반으로 줄인다면, 우리는 이것을 하나의 정보가 있다고 할 수 있습니다.",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about half of the five letter words have an S, a little less than that, but about half.",
  "translatedText": "",
  "from_community_srt": "이 예시에서 가능성의 공간은 모두 가능한 단어이며, 다섯 글자의 절반 가량이 's'로 되어 있는데, 사실 절반보다 조금 적지만 절반 정도입니다.",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information.",
  "translatedText": "",
  "from_community_srt": "이 관측은 여러분에게 1bit의 정보를 줄 것입니다.",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information.",
  "translatedText": "",
  "from_community_srt": "만약 새로운 사실이 그 공간을 4개로 나눈다면, 우리는 그것을 2 bits의 정보를 가지고 있다고 생각할 수 있습니다. 예를 들어,",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T.",
  "translatedText": "",
  "from_community_srt": "이 단어들의 약 4분의 1은 't'를 가지고 있습니다.",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth.",
  "translatedText": "",
  "from_community_srt": "만약 관측이 이 공간을 8배로 줄인다면, 우리는 그 공간에 3bits의 정보가 있다고 말할 수 있고, 그리고 계속 이어집니다.",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a sixteenth, five bits cuts it into a thirty second.",
  "translatedText": "",
  "from_community_srt": "4bits는 16으로 나누고 5bits는 32로 나눕니다.",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.88
 },
 {
  "input": "So now's when you might want to take a moment and pause and ask for yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence?",
  "translatedText": "",
  "from_community_srt": "자, 지금은 잠시 스스로에게 물어보고 싶을 수 있습니다.",
  "n_reviews": 0,
  "start": 534.96,
  "end": 542.98
 },
 {
  "input": "Well, what we're saying here is basically that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability.",
  "translatedText": "",
  "from_community_srt": "비트 수에 관련된 발생 확률과 정보의 개수 공식은 무엇일까요? 여기서 말하는 것은 기본적으로 비트 수의 절반을 취할 때, 그것은 확률과 같은 것입니다. 즉, 2의 비트 개수 거듭제곱은 (1/가능성)과 같습니다. 이는 더 나아가 정보는 log₂ (1/가능성)과 같습니다.",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still where the information is the negative log base two of the probability.",
  "translatedText": "",
  "from_community_srt": "그리고 여러분은 여전히 이것을 다른 형태로 바꿀 수 있다는 것을 알 수 있습니다. 여기서 정보는 -log₂(확률) 입니다.",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half.",
  "translatedText": "",
  "from_community_srt": "이렇게 표현하면 처음 보는 사람들에게는 조금 이상해 보일 수 있지만, 이것은 그냥 얼마나 많은 확률들을 반으로 줄였는지를 알 수 있는 매우 직관적인 발상일 뿐입니다.",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture?",
  "translatedText": "",
  "from_community_srt": "이제 여러분은 궁금할 수도 있습니다, 우린 그냥 재미있는 단어 게임을 하는 줄 알았는데, 왜 로그가 이 그림에 들어올까요?",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095.",
  "translatedText": "",
  "from_community_srt": "로그가 좋은 단위인 한 가지 이유는 매우 있을 법하지 않은 사건에 대해 이야기하는 것이 훨씬 쉽기 때문입니다. 예시로, 어떤 관측이 20비트의 정보를 가지고 있다고 말하는 것은 이런 일이 일어날 확률이 0.000095라고 말하는 것보다 훨씬 쉽습니다.",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together.",
  "translatedText": "",
  "from_community_srt": "하지만 이 로그 식이 확률론에 매우 유용한 표현이라고 알려진 것 보다 더욱 타당한 이유는 로그의 덧셈 법칙 때문입니다.",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information.",
  "translatedText": "",
  "from_community_srt": "예를 들어, 한 관측이 2bits의 정보를 제공하고 즉, 공간을 네 개 나누고, 그 다음 두 번째 관측이 Wordle에서의 두 번째 추측처럼, 다른 3bits의 정보를 제공하면, 그러니까 그 구역을 8개로 한번 더 나누면, 두 관측은 총 다섯 개의 정보를 제공합니다.",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add.",
  "translatedText": "",
  "from_community_srt": "확률끼리 서로 곱한 것은 정보끼리 더한 것과 같습니다.",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with.",
  "translatedText": "",
  "from_community_srt": "그래서 우리가 어떤 기대값과 같은 영역에서 우리가 더 많은 숫자들을 더할 때, 로그들이 훨씬 보기 좋게 만듭니다.",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for weary and add another little tracker on here, showing us how much information there is for each pattern.",
  "translatedText": "",
  "from_community_srt": "다시 'weary'에 대한 분포표로 돌아가서 각 패턴에 대해 얼마나 많은 정보가 있는지 보여주는 작은 추적기를 여기에 하나 더 추가해 보겠습니다.",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain.",
  "translatedText": "",
  "from_community_srt": "여러분께서 주목해 주셨으면 하는 것은, 확률이 높은 패턴에 도달할수록, 139 00:10:38,080 --> 00:10:42,640 더 낮은 정보를 얻을 수 있고, 더 적은 bits를 얻을 수 있습니다.",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information.",
  "translatedText": "",
  "from_community_srt": "우리가 이 추측의 양질을 측정하는 방법은 이 정보의 기대값을 구하는 것입니다.",
  "n_reviews": 0,
  "start": 643.5,
  "end": 648.02
 },
 {
  "input": "When we go through each pattern, we say how probable is it and then we multiply that by how many bits of information do we get.",
  "translatedText": "",
  "from_community_srt": "우리가 각 패턴이 있는 곳에서 얼마나 가능성 있는지 고려하고, 그 가능성에서 그 정보를 얼마나 많이 얻을 수 있는지를 곱하는 것입니다.",
  "n_reviews": 0,
  "start": 648.42,
  "end": 654.06
 },
 {
  "input": "And in the example of weary, that turns out to be 4.9 bits.",
  "translatedText": "",
  "from_community_srt": "그리고 \"weary\"의 예시에서는 4.9bits로 나타났습니다.",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times.",
  "translatedText": "",
  "from_community_srt": "그래서 평균적으로, 이 시작점을 통해 얻은 정보는 가능성의 공간을 5번 반으로 줄이는 것과 같습니다.",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like slate.",
  "translatedText": "",
  "from_community_srt": "반대로 정보의 기대값이 더 높은 추측의 예시로는 \"slate\"가 있습니다.",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case, you'll notice the distribution looks a lot flatter.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information.",
  "translatedText": "",
  "from_community_srt": "이 경우 분포표가 훨씬 평평하게 보이며, 특히 모든 부분이 회색일 확률은 약 6% 정도입니다. 그래서 여러분은 최소 3.9bits의 정보를 얻을 수 있습니다.",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that.",
  "translatedText": "",
  "from_community_srt": "하지만 그건 최소고, 일반적으로 여러분은 그것보다 더 나은 것을 얻을 수 있습니다.",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8.",
  "translatedText": "",
  "from_community_srt": "그리고 이 수치값을 계산해서 모든 구간들을 더하면 평균 정보는 약 5.8개 입니다.",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with weary, your space of possibilities will be about half as big after this first guess, on average.",
  "translatedText": "",
  "from_community_srt": "따라서 \"weary\"와는 대조적으로, 확률의 공간은 이 첫 번째 추측 이후에 평균적으로 약 절반 정도 클 것 입니다.",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity.",
  "translatedText": "",
  "from_community_srt": "이 정보량의 기댓값에 대한 이름에 재미있는 이야기가 있습니다.",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "You see, information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science.",
  "translatedText": "",
  "from_community_srt": "정보 이론은 1940년대에 Bell(전화기 만든 사람) 연구소에서 일하던 Calaude Shannon (클로드 섀넌)에 의해 개발되었습니다. Shannon는 그 당시 지적 괴물이고 수학과 물리학에서 저명하고 컴퓨터 과학의 시초인 John von Neumann (폰 노이만)과 아직 공개되지 않은 아이디어에 대해 이야기를 나누었습니다.",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well, you should call it entropy, and for two reasons.",
  "translatedText": "",
  "from_community_srt": "And when he mentioned that he 그리고 그가 \"정보량의 기대값\"에 대한 좋은 이름이 없다고 말했을 때, von Neumann은 아마도 이렇게 말했을 것입니다. \"흠, 당신은 그것을 엔트로피라고 불러야 할 것입니다. 두 가지 이유로요.",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage.",
  "translatedText": "",
  "from_community_srt": "우선 당신의 불확실성 함수는 통계 역학에서 그 이름으로 사용되어 왔기 때문에, 이미 이름을 가지고 있습니다. 그리고 두 번째, 그리고 더 중요한 것은 아무도 엔트로피가 실제로 무엇인지 모른다는 것입니다. 따라서 토론에서 항상 유리할 것입니다.\" (...어라?) 그래서 만약 이름이 약간 신비롭게 느껴진다면,",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design.",
  "translatedText": "",
  "from_community_srt": "그리고 이 이야기가 믿겨진다면, 그건 일종의 \"설계\"입니다.",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess.",
  "translatedText": "",
  "from_community_srt": "또한, 만약 여러분이 물리학에서 나온 열역학 제2법칙과의 관계가 궁금한다 해도, 분명 연관성이 있긴 하지만, Shannon은 그냥 순수한 확률론을 다룹니다. 그래서 우리의 용도로써, 제가 엔트로피라는 단어를 사용할 때, 여러분은 특정한 추측의 정보의 기대값으로 생각하기를 바랍니다.",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously.",
  "translatedText": "",
  "from_community_srt": "엔트로피는 두 가지를 동시에 측정하는 것으로 생각할 수 있습니다.",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution?",
  "translatedText": "",
  "from_community_srt": "첫 번째는 분포표가 얼마나 평평한지입니다.",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be.",
  "translatedText": "",
  "from_community_srt": "분포가 균일해질수록 엔트로피는 더욱 높아집니다.",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy.",
  "translatedText": "",
  "from_community_srt": "패턴이 3⁵개인 경우, 총 균일한 분포에서 그 중 하나를 관찰하면 정보가 log₂(3⁵)가 되는데, 이는 7.92입니다. 이것이 이 엔트로피가 가질 수 있는 절대적인 최대치입니다.",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place.",
  "translatedText": "",
  "from_community_srt": "하지만 엔트로피는 애초에 얼마나 많은 가능성이 있는지를 나타내는 척도이기도 합니다.",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits.",
  "translatedText": "",
  "from_community_srt": "예를 들어, 16개의 패턴만 있는 단어를 가지고 있다면, 그리고 각각의 패턴은 동등할 가능성이 동일하다면, 이 엔트로피, 즉 예상 정보는 4bits가 될 것입니다.",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits.",
  "translatedText": "",
  "from_community_srt": "또한 64개의 가능한 패턴이 있고 있고 모두 동일할 가능성이 있는 단어의 경우,",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes.",
  "translatedText": "",
  "from_community_srt": "엔트로피는 6bits가 됩니다. 그래서 만약 여러분이 엔트로피가 6bits인 분포표를 본다면, 이것은 마치 64개의 동일한 결과가 있는 것처럼 앞으로 일어날 일에 많은 변화와 불확실성이 있다는 것을 말하는 것과 같습니다.",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this.",
  "translatedText": "",
  "from_community_srt": "Wordlebot에서의 첫번 째 수행에서는, 기본적으로 이런 작업을 했습니다.",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the different possible guesses that you could have, all 13,000 words, it computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns that you might see for each one, and then it picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible.",
  "translatedText": "",
  "from_community_srt": "13,000단어를 모두 포함하여 여러분이 가질 수 있는 다양한 추측을 모두 살펴봅니다. 그리고 각각의 엔트로피, 더 구체적으로는 여러분이 볼 수 있는 모든 패턴에 걸친 분포표의 엔트로피를 계산하고 가장 높은 것을 선택합니다. 왜냐하면, 그렇게 한다면 가능성의 공간을 최대한 줄일 수 있기 때문입니다.",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses.",
  "translatedText": "",
  "from_community_srt": "제가 여기서 첫 번째 추측에 대해서만 이야기했지만, 다음 몇 가지 추측에도 똑같은 역할을 합니다. 예를 들어,",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words.",
  "translatedText": "",
  "from_community_srt": "첫 번째 추측에서 패턴을 보고 나면, 일치하는 단어에 따라 가능한 단어의 개수가 제한됩니다. 그 후에 더 작아진 단어 집합안에서 똑같이 게임을 하면 됩니다.",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy.",
  "translatedText": "",
  "from_community_srt": "두 번째 추측을 위해 제한된 단어 집합에서 발생할 수 있는 모든 패턴의 분포를 살펴봅시다. 13,000개의 모든 가능성을 찾아보면 엔트로피를 최대화하는 것을 찾을 수 있습니다.",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins.",
  "translatedText": "",
  "from_community_srt": "이것이 어떻게 작동하는지를 보여드리기 위해 제가 쓴 Wordle의 변형된 부분을 여백에 표시해 보겠습니다.",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "So after doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information.",
  "translatedText": "",
  "from_community_srt": "모든 엔트로피를 계산한 후, 오른쪽에는 어떤 것이 가장 높은 예상 정보를 가지고 있는지 나와 있습니다.",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch.",
  "translatedText": "",
  "from_community_srt": "적어도 나중에 이것을 구체화할 시점에서 최고의 대답은 \"tares\"입니다. 이것은...음... 가장 흔한 살갈퀴(콩과의 식물)를 의미합니다.",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got given this particular pattern.",
  "translatedText": "",
  "from_community_srt": "우리가 여기에 추측을 할 때마다, 전 옆에 있는 추천들을 무시할거고 \"slate\"를 하겠습니다. 왜냐하면 그냥 저는 \"slate\"가 좋습니다. 그리고 예상 정보들이 보입니다. 하지만 또 여기 단어 오른쪽에 우리가 얼마나 많은 정보를 얻었는지 보여줍니다.",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that.",
  "translatedText": "",
  "from_community_srt": "여기선 우리가 조금 운이 없었던 것 같아요. 5.8 정도를 얻을 것으로 예상했는데, 5.8 보다 적게 나왔습니다.",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now.",
  "translatedText": "",
  "from_community_srt": "그리고 여기 왼쪽에는 우리가 현재 있는 단계에서 가능한 모든 단어가 표시되어 있습니다.",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment.",
  "translatedText": "",
  "from_community_srt": "파란 막대는 각 단어가 얼마나 그럴 가능성이 있다고 생각하는지 알려주기 때문에 이 순간에는 각 단어가 똑같이 발생할 가능성이 있다고 가정하지만, 잠시 후에 이를 재규정 할 것입니다.",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities.",
  "translatedText": "",
  "from_community_srt": "그리고 이 불확실성 값은 가능한 단어들에 대한 엔트로피 분포를 알려주고 있습니다. 하지만 현재 이 분포는 균일한 분포이기 때문에, 이것은 그냥 가능성의 수를 계산하는 쓸데없이 복잡한 방법입니다.",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities.",
  "translatedText": "",
  "from_community_srt": "예를 들어, 만약 우리가 2의 13.66 거듭제곱을 한다면, 이건 약 13,000개의 가능성일 것입니다.",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "Um, a little bit off here, but only because I'm not showing all the decimal places.",
  "translatedText": "",
  "from_community_srt": "여기가 좀 이상하게 보이긴 한데, 소수자리를 안 나타내서 그래요.",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute.",
  "translatedText": "",
  "from_community_srt": "불필요한 것처럼 느껴질 수도 있고, 너무 복잡한 것처럼 느껴질 수도 있지만, 여러분은 왜 두 숫자를 모두 가지고 있는 것이 유용한지 1분 안에 알게 될 것입니다.",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Raman, which again just really doesn't feel like a word.",
  "translatedText": "",
  "from_community_srt": "두번째 추측에서 가장 높은 엔트로피는 \"ramin\" 인 것 같습니다. 또 다시.. 이런건 전혀 단어처럼 느껴지지 않아요.",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here I'm going to go ahead and type in Rains.",
  "translatedText": "",
  "from_community_srt": "(ㅋㅋ 사전에도 안 나옴) 그래서 조금 도덕적으로 저는 \"rains\"를 입력하겠습니다.",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky.",
  "translatedText": "",
  "from_community_srt": "또 운이 안 좋은거 같네요.",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information.",
  "translatedText": "",
  "from_community_srt": "우리는 4.3bits를 예상했지만 오직 3.39bits 정도 얻었습니다.",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means.",
  "translatedText": "",
  "from_community_srt": "이제 55가지 가능성으로 줄여집니다. 그리고 여기서 저는 뭘 의미하든 간에 제안하고 있는 \"kombu\"를 쓰겠습니다..",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And, okay, this is actually a good chance for a puzzle.",
  "translatedText": "",
  "from_community_srt": "좋아요! 이번엔 좋은 기회에요.",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty.",
  "translatedText": "",
  "from_community_srt": "이 패턴은 4.78 bits의 정보를 제공하지만, 일단 이 패턴을 보기 전에 왼쪽 위에 5.78bits 의 불확실성이 있었다는 것을 나타냅니다.",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities?",
  "translatedText": "",
  "from_community_srt": "잠시 퀴즈로,",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well it means that we're reduced down to 1 bit of uncertainty, which is the same thing as saying that there's 2 possible answers.",
  "translatedText": "",
  "from_community_srt": "남은 가능성의 수는 무엇을 의미할까요? 이것은 우리가 불확실성을 1bit로 줄였다는 것을 의미합니다. 즉, 두 가지 가능한 답이 있다는 것과 같습니다.",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice.",
  "translatedText": "",
  "from_community_srt": "50대 50의 선택입니다.",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss.",
  "translatedText": "",
  "from_community_srt": "그리고 여기에서, 우리는 어떤 단어가 더 흔한지 알고 있기 때문에, 우리는 그 답이 \"abyss\"라는 것을 알고 있습니다.",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that.",
  "translatedText": "",
  "from_community_srt": "하지만 지금 보여져 있는 것처럼 프로그램은 그걸 모르고 있습니다.",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it.",
  "translatedText": "",
  "from_community_srt": "그래서 가능한 한 많은 정보를 얻으려고 노력하죠. 단 한 가지 가능성만 남아있을 때까지요. 그리고 나서 추측하죠.",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy, but let's say we call this version 1 of our wordle solver, and then we go and run some simulations to see how it does.",
  "translatedText": "",
  "from_community_srt": "따라서 우린 더 나은 끝판왕 전략이 필요합니다. 일단 이 버전을 Wordle Solver로 부르겠습니다. 그런 다음 시뮬레이션을 실행해 보겠습니다.",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game.",
  "translatedText": "",
  "from_community_srt": "이 방법은 가능한 모든 Wordle 게임을 하는 것입니다. Wordle의 답인 2,315개의 모든 단어를 살펴보는 것입니다.",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice.",
  "translatedText": "",
  "from_community_srt": "기본적으로 이를 테스트 세트로 사용하고 있으며 단어가 얼마나 흔한지 고려하지 않는 이 순진한 방법으로 그리고 단 하나의 선택이 될 때까지 각 단계에서 정보를 최대화하려고 노력하면",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124.",
  "translatedText": "",
  "from_community_srt": "시뮬레이션이 끝날 때까지 평균 점수는 약 4.124가 됩니다.",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expect it to do worse.",
  "translatedText": "",
  "from_community_srt": "움... 그렇겐 나쁘진 않네요. (...엥?) 솔직히 더 나쁠 줄 알았어요.",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4.",
  "translatedText": "",
  "from_community_srt": "하지만 Wordle 게임을 하는 사람들은 보통 4개 안에 맞춘다고 말합니다.",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can.",
  "translatedText": "",
  "from_community_srt": "우리의 진짜 도전은 가능한 한 3번으로 많이 맞추는 것입니다.",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3.",
  "translatedText": "",
  "from_community_srt": "4점과 3점 사이에서는 꽤 큰 도약입니다.",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low-hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that.",
  "translatedText": "",
  "from_community_srt": "여기서 쉬운 목표는 어떤 단어가 흔한지 아닌지를 어떻게든 통합시키는 것입니다.",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language, and I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset.",
  "translatedText": "",
  "from_community_srt": "그러면 우리는 어떻게 정확하게 할 수 있을까요? 제가 접근한 방법은 영어의 모든 단어에 대한 상대 빈도 목록을 얻는 것입니다. 전 Mathematica의 단어 빈도 데이터 기능을 사용했는데, 이 기능은 자체적으로 Google Books English n-gram 공개 데이터 세트에 있는 겁니다.",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words.",
  "translatedText": "",
  "from_community_srt": "예를 들어 가장 흔한 단어에서 가장 덜 흔한 단어로 분류하면, 분명 이 단어들이 영어에서 가장 흔한 다섯 글자 단어들입니다.",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5-letter words in the English language.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common.",
  "translatedText": "",
  "from_community_srt": "\"there\"가 8번째로 흔합니다. 첫 번째는 \"which\"이고,",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there.",
  "translatedText": "",
  "from_community_srt": "그 뒤에 \"their\"과 \"there\"이 있습니다.",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common.",
  "translatedText": "",
  "from_community_srt": "\"First\"는 첫 번째가 아니라 아홉 번째이고, \"First\" 뒤에 오는 단어들이 \"after\" \"where\" 그리고 \"those\" 인데, 이 단어들이 조금 덜 흔하게 나오는 것은 타당합니다.",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency, because for example which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely.",
  "translatedText": "",
  "from_community_srt": "자, 이 데이터를 이용하여 각각의 단어가 최종 정답이 될 가능성을 모델링할 때, 그저 빈도에만 비례해서는 안 됩니다. 예를 들어 이 데이터 세트에서 \"which\"에 0.002의 점수가 부여되는 반면 \"braid\"라는 단어는 어떤 의미에서는 가능성이 약 천 배 적습니다.",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering, so we want more of a binary cutoff.",
  "translatedText": "",
  "from_community_srt": "하지만 이 두 단어 모두 충분히 흔하기 때문에 충분히 고려할 가치가 있습니다. 그래서 우리는 좀 더 이분법적으로 골라내기를 할 필요가 있습니다.",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty.",
  "translatedText": "",
  "from_community_srt": "제가 한 방법은 이 모든 분류된 단어 목록을 x축에 배열하고 시그모이드 함수를 적용하는 것입니다. 시그모이드 함수는 기본적으로 2진법이고 0이거나 1이긴 하지만 불확실성의 영역 사이에는 매끄러운 부분이 있습니다.",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis.",
  "translatedText": "",
  "from_community_srt": "그래서 제가 각 단어를 최종 목록에 배치할 확률은 x축 위에 있는 시그모이드 함수의 값이 될 것입니다.",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff.",
  "translatedText": "",
  "from_community_srt": "분명히 이것은 몇 가지 매개변수에 따라 달라집니다. 예를 들어, x축의 공백이 얼마나 넓어지면 1에서 0까지 얼마나 천천히, 얼마나 가파르게 떨어지는지가 결정되고, 왼쪽에서 오른쪽으로 어디에 되느냐에 따라 골라내기가 결정됩니다.",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "And to be honest the way I did this was kind of just licking my finger and sticking it into the wind.",
  "translatedText": "",
  "from_community_srt": "그리고 솔직히 제가 한 방법은 그냥 손가락을 핥고 훑어보는 겁니다.",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff.",
  "translatedText": "",
  "from_community_srt": "(직--관) 전 분류된 목록을 훑어보고 제가 봤을 때 이 단어들의 절반 정도가 최종 정답이 아닐 가능성이 높은 구간을 찾으려고 했습니다. 그리고 전 이 방법으로 골라냈습니다.",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Now once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement.",
  "translatedText": "",
  "from_community_srt": "이렇게 단어 전체에 걸쳐 이와 같은 분포를 갖게 되면 엔트로피가 매우 유용한 측정이 되는 상황이 됩니다.",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were other and nails, and we end up with a situation where there's four possible words that match it.",
  "translatedText": "",
  "from_community_srt": "예를 들어, 게임을 하고 있는데 \"other\"과 \"nails\"로 시작해서 일치하는 단어가 4개 있는 상황으로 끝난다고 가정해 보겠습니다.",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely, let me ask you, what is the entropy of this distribution?",
  "translatedText": "",
  "from_community_srt": "그리고 우리가 이 단어들이 서로 동등하다고 가정해 봅시다. 여러분한테 하나 물어보겠습니다,",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2.",
  "translatedText": "",
  "from_community_srt": "이 분포에서 엔트로피는 얼마일까요? 이러한 각각의 가능성과 관련된 정보는 1/4이고 log₂(4)를 하면 2가 됩니다.",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "2 bits of information, 4 possibilities.",
  "translatedText": "",
  "from_community_srt": "이것은 2bits의 정보, 4가지 가능성,",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good.",
  "translatedText": "",
  "from_community_srt": "모두 아주 좋고 좋은 정보입니다.",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than 4 matches?",
  "translatedText": "",
  "from_community_srt": "하지만!",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it.",
  "translatedText": "",
  "from_community_srt": "만약 제가 여려분에게 4개 이상의 일치가 있다고 말한다면 어떻게 될까요? 실제로 전체 단어 목록을 살펴보면 16개의 단어가 일치합니다.",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure.",
  "translatedText": "",
  "from_community_srt": "하지만 우리의 모델이 실제로 최종 정답이 되는 나머지 12개의 단어들에 대한 확률을 1/1000 정도로 낮다고 합니다. 왜냐하면 그 단어들은 잘 알려지지 않은 단어이기 때문입니다.",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution?",
  "translatedText": "",
  "from_community_srt": "한번 더 물어보겠습니다,",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before.",
  "translatedText": "",
  "from_community_srt": "그럼 이 분포에서 엔트로피는 얼마일까요? 여기서 엔트로피가 단순히 일치 항목 수를 측정하는 것이라면, 여러분은 log₂(16)인 4 라고 예상할 수 있습니다. 우리가 그전에 얻은 것 보다, 2bits 정도가 더 불확실한 상황입니다.",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before.",
  "translatedText": "",
  "from_community_srt": "그러나 물론, 실제적인 불확실성은 이전과 크게 다르지 않습니다. 예를 들어,",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example.",
  "translatedText": "",
  "from_community_srt": "이 잘 알려지지 않은 12개의 단어들이 있다고 해서, 우리가 최종 답이 \"charm\"이라는 것을 그렇게 놀랍게 생각하지 않습니다.",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits.",
  "translatedText": "",
  "from_community_srt": "여기서 실제로 계산을 하고 각 발생 확률에 해당 정보 수를 곱하면 2.11bits가 나옵니다.",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "Just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it.",
  "translatedText": "",
  "from_community_srt": "이것은 기본적으로 2비트이고, 기본적으로 4가지 가능성입니다. 하지만 매우 가능성이 없는 모든 사건들까지 포함하기 때문에 조금 더 불확실성이 있습니다. 하지만 만약 여러분이 이것들을 안다면, 여러분은 많은 정보를 얻을 수 있을 것입니다.",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson.",
  "translatedText": "",
  "from_community_srt": "그래서 멀리 본다면, 이것은 Wordle이 정보이론 수업에서 좋은 예시라는 것입니다.",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy.",
  "translatedText": "",
  "from_community_srt": "우린 엔트로피에 대한 이 두 가지 다른 느낌의 응용 프로그램을 가지고 있습니다.",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words we have possible.",
  "translatedText": "",
  "from_community_srt": "첫 번째는 주어진 추측으로부터 얻을 수 있는 예상 정보를 알려주고, 두 번째는 가능한 모든 단어들 중에서 불확실성을 측정하는 것 입니다.",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation.",
  "translatedText": "",
  "from_community_srt": "제가 강조하는 것은, 우리가 추측에 대한 예상 정보를 볼 때, 단어에 대한 불균등한 가중치가 있다면 그것이 엔트로피 계산에 영향을 미친다는 것입니다.",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words.",
  "translatedText": "",
  "from_community_srt": "앞에서 살펴본 \"weary\"의 분포를 예로 하고, 그러나 이번에는 가능한 모든 단어에 대해 불균등분포를 사용합니다.",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well.",
  "translatedText": "",
  "from_community_srt": "그럼 제가 잘 표현된 부분을 찾을 수 있을지...",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here, this is pretty good.",
  "translatedText": "",
  "from_community_srt": "어... 좋아요, 여기가 괜찮네요.",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it.",
  "translatedText": "",
  "from_community_srt": "여기 두 개의 이웃한 패턴이 있는데, 그 중 하나는 32개의 가능한 단어를 가지고 있다고 합니다.",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them.",
  "translatedText": "",
  "from_community_srt": "그리고 무엇인지 확인한다면, 이것들이 그 32개의 단어들입니다. 모두 매우 가능성이 낮은 단어들입니다. 여러분이 빠르게 훑어보면,",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches.",
  "translatedText": "",
  "from_community_srt": "그럴듯한 답을 찾기가 힘듭니다. 어쩌면 \"yells\" 일 수도? (아님말고!) 하지만 분포에서 옆에 있는 패턴을 보면, 일치 가능한 단어가 8개밖에 없다고 합니다.",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1386.66
 },
 {
  "input": "So a quarter as many matches, but it's about as likely.",
  "translatedText": "",
  "from_community_srt": "그래서 4분의 1정도만 일치하지만, 거의 그럴 것 같습니다.",
  "n_reviews": 0,
  "start": 1386.88,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why.",
  "translatedText": "",
  "from_community_srt": "그리고 우리가 그 항목들을 본다면 우리는 왜 그런지 알 수 있습니다.",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers like ring or wrath or raps.",
  "translatedText": "",
  "from_community_srt": "이들 중 \"wring\", \"wrath\", \"wraps\"와 같은 일부는 실제적인 그럴듯한 대답입니다.",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version two of the Wordlebot here.",
  "translatedText": "",
  "from_community_srt": "이 모든 것을 통합하는 것을 보여주기 위해, Wordlebot 버전 2를 보겠습니다.",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1402.3
 },
 {
  "input": "And there are two or three main differences from the first one that we saw.",
  "translatedText": "",
  "from_community_srt": "우리가 처음 본 것 보다 크게 2~3 가지 차이점이 있습니다.",
  "n_reviews": 0,
  "start": 1402.56,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer.",
  "translatedText": "",
  "from_community_srt": "첫 번째로, 그전에 말했듯이, 우리가 엔트로피, 즉 정보의 기대값을 계산하는 방법은 300 00:23:30,960 --> 00:23:36,160 주어진 단어가 실제로 답이 될 확률을 포함하는 모든 패턴에 관해서, 보다 정교한 분포를 사용하는 것입니다.",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number one, though the ones following are a bit different.",
  "translatedText": "",
  "from_community_srt": "그리고, 다른 것들은 조금 다르지만, \"tares\"가 여전히 1위입니다.",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table.",
  "translatedText": "",
  "from_community_srt": "둘째로, 상위권 순위를 매길 때, 몇 가지 추측을 더 쉽게 하기 위해 각 단어가 실제 답일 304 00:23:50,880 --> 00:23:54,960 확률의 모델을 유지하고 이를 결정에 포함합니다.",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives.",
  "translatedText": "",
  "from_community_srt": "기계가 시키는 대로 살아가면 안되기 때문에, 추천을 또 무시해보겠습니다.",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches.",
  "translatedText": "",
  "from_community_srt": "그리고 여기 왼쪽에 있는 또 다른 점을 말해야 할 것 같습니다. 불확실성 값, bit 수는 더 이상 가능한 일치 수와 중복되지 않습니다. 자,",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which would be a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes.",
  "translatedText": "",
  "from_community_srt": "이제 계산해보면, 2의 8.02 거듭제곱을 하면, 256을 조금 넘습니다. 259 인거 같네요. 이것이 의미하는 것은 실제로 이 패턴과 일치하는 총 단어가 526개 있음에도 불구하고, 그 불확실성의 양은 259개의 동일한 결과가 있는 것과 비슷하다는 것입니다.",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borks is not the answer, same with yorts and zorl and zorus.",
  "translatedText": "",
  "from_community_srt": "여러분은 \"borks\", \"yortz\", \"zoril\",",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1474.66
 },
 {
  "input": "So it's a little less uncertain than it was in the previous case.",
  "translatedText": "",
  "from_community_srt": "\"zorus\"는 답이 아니라는 것을 알고 있습니다.",
  "n_reviews": 0,
  "start": 1474.66,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller.",
  "translatedText": "",
  "from_community_srt": "그래서 이전의 경우보다 조금 덜 불확실합니다, 즉,",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here.",
  "translatedText": "",
  "from_community_srt": "이 bits 수는 더 적을 것입니다. 제가 게임을 진행하고, 2개의 추측을 통해 제가 설명하고자 하는 것을 다듬어 보겠습니다.",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy.",
  "translatedText": "",
  "from_community_srt": "네 번째 추측에서, 저 위에 상위 선택들을 본다면, 더 이상 엔트로피를 최대화만 하는 것이 아니라는 것을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words.",
  "translatedText": "",
  "from_community_srt": "이 시점에서, 정확히 7개의 단어가 있지만 의미 있는 것은 \"dorms\"와 \"words\" 뿐이고, 엄밀히 말하자면,",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values that strictly speaking would give more information.",
  "translatedText": "",
  "from_community_srt": "다른 단어들도 정보를 더 줄 수 있게 순위에 있습니다.",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect.",
  "translatedText": "",
  "from_community_srt": "제가 이걸 처음 했을 때는, 저는 각각의 추측의 양질을 측정하기 위해 이 두 숫자를 더했습니다.",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic.",
  "translatedText": "",
  "from_community_srt": "실제로 여러분이 의심했던 거에 비해 잘 됐습니다.",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1515.9
 },
 {
  "input": "And I'm sure there's other approaches people could take.",
  "translatedText": "",
  "from_community_srt": "하지만 이건 좀 체계적이지 않게 느껴집니다. 사람들이 할 수 있는 몇 가지의 방법들이 있다는 걸 알지만,",
  "n_reviews": 0,
  "start": 1516.1,
  "end": 1517.88
 },
 {
  "input": "But here's the one I landed on.",
  "translatedText": "",
  "from_community_srt": "여기 제가 선택한 방법이 있습니다.",
  "n_reviews": 0,
  "start": 1517.9,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that.",
  "translatedText": "",
  "from_community_srt": "만약 우리가 이 \"words\"의 경우와 같이 다음 추측을 한다면, 우리가 정말로 관심을 갖는 것은 \"words\"로 할 경우의 예상 점수입니다.",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to.",
  "translatedText": "",
  "from_community_srt": "그리고 그 예상 점수를 계산하기 위해, 우리는 \"'words'가 실제 답일 확률이 얼마나 될까? 라고 생각 할 수 있고, 현재 58% 정도로 됩니다.",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be four.",
  "translatedText": "",
  "from_community_srt": "그래서 이 게임에서 58%의 확률로 우리는 4점을 얻을 수 있고,",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of one minus that 58%, our score will be more than that four.",
  "translatedText": "",
  "from_community_srt": "1에서 58%를 뺀 확률 (42%)이, 점수가 4점 초과가 됩니다.",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point.",
  "translatedText": "",
  "from_community_srt": "얼마나 그럴까요? 우리도 잘 모르지만, 일단 그 지점에 도달했을 때, 얼마나 많은 불확실성이 있을 것인지를 기반으로 예측할 수 있습니다.",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment, there's 1.44 bits of uncertainty.",
  "translatedText": "",
  "from_community_srt": "구체적으로, 현재 1.44 bits의 불확실성이 존재하는데, 만약 우리가 \"words\"를 추측한다면,",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits.",
  "translatedText": "",
  "from_community_srt": "예상 정보가 1.27 bits로 나옵니다. 결국,",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens.",
  "translatedText": "",
  "from_community_srt": "이 차이는 이 시점 이후에 얼마나 많은 불확실성이 남겨져 있는지를 나타냅니다.",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score.",
  "translatedText": "",
  "from_community_srt": "우리에게 필요한 것은 이 불확실성을 예상 점수와 연관시키는 일종의 함수, 𝒇 입니다.",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version one of the bot to say, hey, what was the actual score after various points with certain very measurable amounts of uncertainty?",
  "translatedText": "",
  "from_community_srt": "제가 이 문제를 해결하는 방법은, 버전 1의 봇에 있는 이전 게임에 버전 1의 봇에 있는 이전 게임에 관한 데이터를 표시하는 것 입니다. 이렇게 말이죠 \"님아, 불확실성이 있는 다양한 지점에서 그 실제 점수가 몇점이었니?\" 예를 들어,",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games, after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer.",
  "translatedText": "",
  "from_community_srt": "여기에 약 8.7 정도에 있는 데이터들은 다음과 같이 말 할 수 있습니다. \"일부 게임의 경우 8.7 bits의 불확실성이 있었던 지점 이후에 최종 답을 얻기 위해선 두 번의 추측이 필요했습니다. 다른 게임에서는 세 번의 추측이,",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games, it took three guesses.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1600.66
 },
 {
  "input": "For other games, it took four guesses.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1600.82,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring.",
  "translatedText": "",
  "from_community_srt": "또 다른 게임에서는 네 번의 추측이 필요했습니다.\" 여기서 왼쪽으로 이동하면, 0을 넘는 모든 점들은 \"0bit의 불확실성이 있을 때, 즉, 단 하나의 가능성만 있다고 할 때, 필요한 추측의 수는 항상 하나입니다.\" 라고 말할 수 있습니다. 문제 될게 없죠.",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses, and so on and so forth here.",
  "translatedText": "",
  "from_community_srt": "1bit의 불확실성이 있을 때마다, 즉 2개의 가능성으로 의미되며, 때로는 한 가지 추측이 더 필요할 때도, 두 가지 추측이 더 필요할 때도 있습니다.",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages.",
  "translatedText": "",
  "from_community_srt": "여기서 이 데이터를 시각화하는 좀 더 쉬운 방법은 데이터를 묶음으로 묶어서 평균을 구하는 것입니다.",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example, this bar here is saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5.",
  "translatedText": "",
  "from_community_srt": "예를 들어, 이 막대는 \"1 bit의 불확실성이 있었던 모든 지점 중에서 평균적으로 필요한 새로운 추측의 수는 약 1.5개였습니다.\" 라고 말할 수 있습니다.",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here is saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward.",
  "translatedText": "",
  "from_community_srt": "그리고 저기 있는 막대는 \"4 bits을 약간 넘는 불확실성이 있었던 모든 지점 중에서, 이는 16개의 다른 가능성으로 좁혀진 것과 같으며, 그 시점에서 평균적으로 2개 이상의 추측이 필요합니다.\" 라고 말할 수 있습니다.",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this.",
  "translatedText": "",
  "from_community_srt": "그리고 여기서 저는 이것에 합당한 함수를 맞추기 위해 회귀 분석을 했습니다.",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember, the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be.",
  "translatedText": "",
  "from_community_srt": "그리고 이 모든 것을 할 때 중요한 점은 우리가 단어로부터 더 많은 정보를 얻을수록 예상 점수는 더 낮아진다는 직감을 숫자로 나타낼 수 있다는 것입니다.",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So, with this as version 2.0, if we go back and run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do?",
  "translatedText": "",
  "from_community_srt": "버전 2.0 봇으로 돌아가서, 2,315개의 Wordle 답이 있는 똑같은 시뮬레이션을 실행하면 어떻게 될까요?",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version, it's definitely better, which is reassuring.",
  "translatedText": "",
  "from_community_srt": "첫 번째 버전과는 달리 확실히 더 좋습니다,",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done, the average is around 3.6.",
  "translatedText": "",
  "from_community_srt": "안심이 되네요. 종합해 보면 평균은 3.6 정도입니다.",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1686.18
 },
 {
  "input": "Although unlike the first version, there are a couple times that it loses, and requires more than six in this circumstance.",
  "translatedText": "",
  "from_community_srt": "비록 첫 번째 버전과 달리, 이 상황에서는 6번 이상 필요한 경우가 몇 번 있는데,",
  "n_reviews": 0,
  "start": 1686.54,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information.",
  "translatedText": "",
  "from_community_srt": "아마도 정보를 극대화하기 보다는 실제로 목표를 달성하기 위해 균형을 맞추는 경우가 있기 때문일 것입니다.",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6?",
  "translatedText": "",
  "from_community_srt": "그럼 3.6점 보다 더 잘할 수 있을까요?",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can.",
  "translatedText": "",
  "from_community_srt": "당연히 할 수 있죠.",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now, I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model.",
  "translatedText": "",
  "from_community_srt": "저는 처음에 모델을 만들 때 Wordle의 답 목록을 포함하지 않는 것이 가장 재미있다고 말했습니다. 361 00:28:29,680 --> 00:28:35,760 하지만 만약 목록을 포함시킨다면, 제가 얻을 수 있는 최고의 점수는 3.43 정도였습니다.",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that.",
  "translatedText": "",
  "from_community_srt": "그래서 만약 우리가 단어의 빈도 데이터를 사용해서 이 사전 분포를 선택하는 것보다 더 정교하게 하려고 한다면, 아마도 3.43 점이 우리가 얻을 수 있는 최대값일 겁니다. 적어도 제가 잘하면 최소 3.43 정도를 얻을 수 있죠.",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one.",
  "translatedText": "",
  "from_community_srt": "그 최고의 성능은 본질적으로 제가 지금까지 말했던 아이디어를 사용하지만 조금 더 진행해야합니다. 예상 정보를 한 단계 보다, 두 단계를 검색하는 것처럼요.",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is.",
  "translatedText": "",
  "from_community_srt": "원래는 저는 이거에 대해 더 말하려고 했는데, 시간이 꽤 많이 흘렀다는 것을 압니다.",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least, it's looking like Crane is the best opener.",
  "translatedText": "",
  "from_community_srt": "(벌써 29분 정도 흐름 ㄷㄷ) 한 가지 말씀드리고 싶은 것은 이 두 단계 검색을 한 후 상위 후보들로 샘플 시뮬레이션을 몇 번 하면 \"crane\" 이 최고의 시작 단어인 것 같습니다.",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits.",
  "translatedText": "",
  "from_community_srt": "누가 상상이나 해봤을까요? 또한 어려분이 정답 목록을 사용하여 가능성의 공간을 결정한다면, 당신이 시작하는 불확실성은 11비트가 조금 넘습니다.",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits.",
  "translatedText": "",
  "from_community_srt": "무차별 대입으로 처음 두 번 추측한 후에 예상되는 최대 정보는 약 10비트입니다.",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty.",
  "translatedText": "",
  "from_community_srt": "즉, 처음 두 번 추측한 후에 최상의 플레이를 할 경우 약 1 bit의 불확실성이 남는데, 이는 두 번 추측한 것과 같습니다.",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "But I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as three, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail.",
  "translatedText": "",
  "from_community_srt": "따라서 이 평균을 3으로 낮추는 알고리즘을 결코 작성할 수 없다고 말하는 것이 맞다고 생각합니다. 왜냐하면 단 두 번의 단계만으로 세 번째 슬롯에서 실패 없이 답을 맞출 수 있는 충분한 정보를 얻을 수 있는 공간이 없기 때문입니다.",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]