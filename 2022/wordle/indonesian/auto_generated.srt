1
00:00:00,000 --> 00:00:03,194
Permainan Wurdle telah menjadi sangat viral dalam satu atau dua bulan terakhir, 

2
00:00:03,194 --> 00:00:06,389
dan tidak ada seorangpun yang melewatkan kesempatan untuk pelajaran matematika, 

3
00:00:06,389 --> 00:00:09,704
menurut saya permainan ini menjadi contoh sentral yang sangat baik dalam pelajaran 

4
00:00:09,704 --> 00:00:12,660
tentang teori informasi, dan khususnya topik yang dikenal sebagai entropi.

5
00:00:13,920 --> 00:00:16,918
Anda tahu, seperti kebanyakan orang, saya terjebak dalam teka-teki, 

6
00:00:16,918 --> 00:00:19,829
dan seperti kebanyakan programmer, saya juga terjebak dalam upaya 

7
00:00:19,829 --> 00:00:22,740
menulis algoritma yang akan memainkan permainan seoptimal mungkin.

8
00:00:23,180 --> 00:00:25,802
Dan apa yang saya pikir akan saya lakukan di sini hanyalah membicarakan dengan Anda 

9
00:00:25,802 --> 00:00:28,363
beberapa proses saya di dalamnya, dan menjelaskan beberapa perhitungan matematika 

10
00:00:28,363 --> 00:00:31,080
yang masuk ke dalamnya, karena keseluruhan algoritma berpusat pada gagasan entropi ini.

11
00:00:38,700 --> 00:00:41,640
Hal pertama yang pertama, jika Anda belum pernah mendengarnya, apa itu Wurdle?

12
00:00:42,040 --> 00:00:44,994
Dan untuk membunuh dua burung dengan satu batu di sini sementara kita membahas aturan 

13
00:00:44,994 --> 00:00:47,604
permainannya, izinkan saya juga melihat ke mana tujuan kita dengan hal ini, 

14
00:00:47,604 --> 00:00:50,662
yaitu mengembangkan algoritma kecil yang pada dasarnya akan memainkan permainan tersebut 

15
00:00:50,662 --> 00:00:51,040
untuk kita.

16
00:00:51,360 --> 00:00:53,518
Meskipun saya belum melakukan Wurdle hari ini, ini tanggal 4 Februari, 

17
00:00:53,518 --> 00:00:55,100
dan kita akan melihat bagaimana botnya melakukannya.

18
00:00:55,480 --> 00:00:57,979
Tujuan Wurdle adalah menebak kata misteri lima huruf, 

19
00:00:57,979 --> 00:01:00,340
dan Anda diberi enam peluang berbeda untuk menebak.

20
00:01:00,840 --> 00:01:04,379
Misalnya, bot Wurdle saya menyarankan agar saya memulai dengan tebakan derek.

21
00:01:05,180 --> 00:01:07,502
Setiap kali Anda menebak, Anda mendapatkan informasi 

22
00:01:07,502 --> 00:01:10,220
tentang seberapa dekat tebakan Anda dengan jawaban sebenarnya.

23
00:01:10,920 --> 00:01:14,100
Di sini kotak abu-abu memberi tahu saya bahwa tidak ada C pada jawaban sebenarnya.

24
00:01:14,520 --> 00:01:17,840
Kotak kuning memberitahu saya ada huruf R, tapi posisinya tidak seperti itu.

25
00:01:18,240 --> 00:01:21,163
Kotak hijau memberitahu saya bahwa kata rahasianya memang memiliki nilai A, 

26
00:01:21,163 --> 00:01:22,240
dan berada di posisi ketiga.

27
00:01:22,720 --> 00:01:24,580
Dan kemudian tidak ada N dan tidak ada E.

28
00:01:25,200 --> 00:01:27,340
Jadi izinkan saya masuk dan memberi tahu bot Wurdle informasi itu.

29
00:01:27,340 --> 00:01:30,320
Kami mulai dengan crane, kami mendapat warna abu-abu, kuning, hijau, abu-abu, abu-abu.

30
00:01:31,420 --> 00:01:33,580
Jangan khawatir tentang semua data yang ditampilkan saat ini, 

31
00:01:33,580 --> 00:01:34,940
saya akan menjelaskannya pada waktunya.

32
00:01:35,460 --> 00:01:38,820
Tapi saran utamanya untuk pilihan kedua kami adalah buruk.

33
00:01:39,560 --> 00:01:42,033
Dan tebakan Anda memang harus berupa kata yang terdiri dari lima huruf, 

34
00:01:42,033 --> 00:01:45,022
tetapi seperti yang akan Anda lihat, tebakannya cukup liberal karena memungkinkan Anda 

35
00:01:45,022 --> 00:01:45,400
menebaknya.

36
00:01:46,200 --> 00:01:47,440
Dalam hal ini, kami mencoba shtick.

37
00:01:48,780 --> 00:01:50,180
Dan baiklah, semuanya terlihat cukup bagus.

38
00:01:50,260 --> 00:01:53,980
Kita tekan S dan H, jadi kita tahu tiga huruf pertama, kita tahu ada R.

39
00:01:53,980 --> 00:01:58,700
Jadi itu akan menjadi seperti SHA sesuatu R, atau SHA R sesuatu.

40
00:01:59,620 --> 00:02:03,024
Dan sepertinya bot Wurdle mengetahui bahwa hanya ada dua kemungkinan, 

41
00:02:03,024 --> 00:02:04,240
yaitu pecahan atau tajam.

42
00:02:05,100 --> 00:02:07,047
Saat ini ada semacam perselisihan di antara mereka, 

43
00:02:07,047 --> 00:02:10,080
jadi saya kira mungkin hanya karena sesuai abjad, maka itu sesuai dengan pecahan.

44
00:02:11,220 --> 00:02:12,860
Yang mana hore, itulah jawaban sebenarnya.

45
00:02:12,960 --> 00:02:13,780
Jadi kami mendapatkannya dalam tiga.

46
00:02:14,600 --> 00:02:17,501
Jika Anda bertanya-tanya apakah itu bagus, saya mendengar satu orang 

47
00:02:17,501 --> 00:02:20,360
berkata bahwa dengan Wurdle empat adalah par dan tiga adalah birdie.

48
00:02:20,680 --> 00:02:22,480
Yang menurut saya merupakan analogi yang cukup tepat.

49
00:02:22,480 --> 00:02:25,750
Anda harus konsisten dalam permainan Anda untuk mendapatkan empat, 

50
00:02:25,750 --> 00:02:27,020
tapi itu jelas tidak gila.

51
00:02:27,180 --> 00:02:29,920
Tapi ketika Anda mendapatkannya dalam tiga, rasanya luar biasa.

52
00:02:30,880 --> 00:02:33,303
Jadi jika Anda menginginkannya, yang ingin saya lakukan di sini hanyalah 

53
00:02:33,303 --> 00:02:35,960
membahas proses pemikiran saya dari awal tentang cara saya mendekati bot Wurdle.

54
00:02:36,480 --> 00:02:39,440
Dan seperti yang saya katakan, ini sebenarnya alasan untuk pelajaran teori informasi.

55
00:02:39,740 --> 00:02:42,820
Tujuan utamanya adalah menjelaskan apa itu informasi dan apa itu entropi.

56
00:02:48,220 --> 00:02:50,901
Pikiran pertama saya dalam melakukan pendekatan ini adalah 

57
00:02:50,901 --> 00:02:53,720
melihat frekuensi relatif berbagai huruf dalam bahasa Inggris.

58
00:02:54,380 --> 00:02:56,641
Jadi saya berpikir, oke, apakah ada tebakan pembuka atau 

59
00:02:56,641 --> 00:02:59,260
tebakan pembuka yang banyak mengenai huruf yang paling sering ini?

60
00:02:59,960 --> 00:03:01,561
Dan salah satu hal yang sangat saya sukai adalah 

61
00:03:01,561 --> 00:03:03,000
melakukan hal lain yang diikuti dengan paku.

62
00:03:03,760 --> 00:03:05,501
Pemikirannya adalah jika Anda menekan sebuah huruf, Anda tahu, 

63
00:03:05,501 --> 00:03:07,520
Anda mendapatkan warna hijau atau kuning, itu selalu terasa menyenangkan.

64
00:03:07,520 --> 00:03:08,840
Rasanya seperti Anda mendapatkan informasi.

65
00:03:09,340 --> 00:03:12,039
Namun dalam kasus ini, bahkan jika Anda tidak memukul dan Anda selalu 

66
00:03:12,039 --> 00:03:14,623
mendapatkan warna abu-abu, itu tetap memberi Anda banyak informasi 

67
00:03:14,623 --> 00:03:17,400
karena sangat jarang menemukan kata yang tidak memiliki huruf-huruf ini.

68
00:03:18,140 --> 00:03:20,891
Namun tetap saja, hal tersebut tidak terasa super sistematis, 

69
00:03:20,891 --> 00:03:23,200
karena misalnya, tidak memperhitungkan urutan huruf.

70
00:03:23,560 --> 00:03:25,300
Mengapa mengetik paku ketika saya bisa mengetik siput?

71
00:03:26,080 --> 00:03:27,500
Apakah lebih baik memiliki S di akhir?

72
00:03:27,820 --> 00:03:28,680
Saya tidak begitu yakin.

73
00:03:29,240 --> 00:03:32,641
Sekarang, seorang teman saya mengatakan bahwa dia suka membuka dengan kata letih, 

74
00:03:32,641 --> 00:03:36,291
yang membuat saya terkejut karena ada beberapa huruf yang tidak biasa di sana seperti W 

75
00:03:36,291 --> 00:03:36,540
dan Y.

76
00:03:37,120 --> 00:03:39,000
Tapi siapa tahu, mungkin itu pembuka yang lebih baik.

77
00:03:39,320 --> 00:03:44,320
Adakah skor kuantitatif yang bisa kita berikan untuk menilai kualitas tebakan potensial?

78
00:03:45,340 --> 00:03:48,060
Sekarang untuk mempersiapkan cara kita menentukan peringkat kemungkinan tebakan, 

79
00:03:48,060 --> 00:03:50,076
mari kita kembali dan menambahkan sedikit kejelasan tentang 

80
00:03:50,076 --> 00:03:51,420
bagaimana tepatnya permainan ini diatur.

81
00:03:51,420 --> 00:03:54,545
Jadi ada daftar kata yang dapat Anda masukkan yang dianggap 

82
00:03:54,545 --> 00:03:57,880
sebagai tebakan valid yang panjangnya hanya sekitar 13.000 kata.

83
00:03:58,320 --> 00:04:01,417
Tapi kalau dilihat-lihat, ada banyak hal yang sangat tidak biasa, 

84
00:04:01,417 --> 00:04:05,548
seperti kepala atau Ali dan ARG, kata-kata yang menimbulkan pertengkaran keluarga dalam 

85
00:04:05,548 --> 00:04:06,440
permainan Scrabble.

86
00:04:06,960 --> 00:04:10,540
Namun inti dari permainan ini adalah bahwa jawabannya akan selalu berupa kata yang umum.

87
00:04:10,960 --> 00:04:13,182
Dan faktanya, ada daftar lain yang berisi sekitar 

88
00:04:13,182 --> 00:04:15,360
2.300 kata yang merupakan kemungkinan jawabannya.

89
00:04:15,940 --> 00:04:18,028
Dan ini adalah daftar yang dikurasi oleh manusia, 

90
00:04:18,028 --> 00:04:21,160
menurut saya khususnya oleh pacar pembuat game, dan itu cukup menyenangkan.

91
00:04:21,820 --> 00:04:24,491
Tapi yang ingin saya lakukan, tantangan kita untuk proyek ini 

92
00:04:24,491 --> 00:04:27,120
adalah melihat apakah kita bisa menulis program penyelesaian 

93
00:04:27,120 --> 00:04:30,180
Wordle yang tidak memasukkan pengetahuan sebelumnya tentang daftar ini.

94
00:04:30,720 --> 00:04:32,736
Untuk satu hal, ada banyak kata lima huruf yang cukup 

95
00:04:32,736 --> 00:04:34,640
umum yang tidak akan Anda temukan dalam daftar itu.

96
00:04:34,940 --> 00:04:38,276
Jadi akan lebih baik untuk menulis sebuah program yang sedikit lebih tangguh dan dapat 

97
00:04:38,276 --> 00:04:41,460
memainkan Wordle melawan siapa pun, bukan hanya apa yang terjadi di situs resminya.

98
00:04:41,920 --> 00:04:44,675
Dan juga alasan kita mengetahui daftar kemungkinan jawaban ini, 

99
00:04:44,675 --> 00:04:47,000
adalah karena daftar tersebut terlihat di kode sumber.

100
00:04:47,000 --> 00:04:50,198
Namun tampilannya di kode sumber sesuai dengan 

101
00:04:50,198 --> 00:04:53,260
urutan jawaban yang muncul dari hari ke hari.

102
00:04:53,260 --> 00:04:55,840
Jadi, Anda selalu bisa mencari tahu apa jawaban besok.

103
00:04:56,420 --> 00:04:58,880
Jelas sekali, ada kesan bahwa menggunakan daftar itu curang.

104
00:04:59,100 --> 00:05:02,005
Dan apa yang menjadikan teka-teki ini lebih menarik dan pelajaran teori 

105
00:05:02,005 --> 00:05:04,870
informasi yang lebih kaya adalah dengan menggunakan beberapa data yang 

106
00:05:04,870 --> 00:05:07,494
lebih universal seperti frekuensi kata relatif secara umum untuk 

107
00:05:07,494 --> 00:05:10,440
menangkap intuisi mengenai preferensi terhadap kata-kata yang lebih umum.

108
00:05:11,600 --> 00:05:15,900
Jadi dari 13.000 kemungkinan ini, bagaimana sebaiknya kita memilih tebakan pembuka?

109
00:05:16,400 --> 00:05:19,780
Misalnya, jika teman saya melamar dengan letih, bagaimana kita menganalisis kualitasnya?

110
00:05:20,520 --> 00:05:22,780
Nah, alasan dia mengatakan dia menyukai W yang tidak mungkin 

111
00:05:22,780 --> 00:05:25,153
itu adalah karena dia menyukai sifat pukulan jarak jauh tentang 

112
00:05:25,153 --> 00:05:27,340
betapa nikmatnya rasanya jika Anda berhasil mencapai W itu.

113
00:05:27,920 --> 00:05:31,148
Misalnya, jika pola pertama yang terungkap kira-kira seperti ini, 

114
00:05:31,148 --> 00:05:34,915
maka ternyata hanya ada 58 kata dalam leksikon raksasa ini yang cocok dengan 

115
00:05:34,915 --> 00:05:35,600
pola tersebut.

116
00:05:36,060 --> 00:05:38,400
Jadi itu pengurangan yang sangat besar dari 13.000.

117
00:05:38,780 --> 00:05:43,020
Namun sisi sebaliknya tentu saja sangat jarang mendapatkan pola seperti ini.

118
00:05:43,020 --> 00:05:47,503
Secara khusus, jika setiap kata mempunyai kemungkinan yang sama untuk menjadi jawabannya, 

119
00:05:47,503 --> 00:05:51,040
kemungkinan untuk mendapatkan pola ini adalah 58 dibagi sekitar 13.000.

120
00:05:51,580 --> 00:05:53,600
Tentu saja, kemungkinan jawaban tersebut tidak sama.

121
00:05:53,720 --> 00:05:56,220
Kebanyakan dari kata-kata ini sangat tidak jelas dan bahkan dipertanyakan.

122
00:05:56,600 --> 00:05:58,516
Tapi setidaknya untuk langkah pertama kita dalam semua hal ini, 

123
00:05:58,516 --> 00:06:00,881
mari kita asumsikan bahwa semuanya memiliki kemungkinan yang sama dan kemudian 

124
00:06:00,881 --> 00:06:01,600
menyempurnakannya nanti.

125
00:06:02,020 --> 00:06:06,720
Intinya adalah pola dengan banyak informasi pada dasarnya tidak mungkin terjadi.

126
00:06:07,280 --> 00:06:10,800
Faktanya, yang dimaksud dengan informatif adalah bahwa hal itu tidak mungkin.

127
00:06:11,719 --> 00:06:15,625
Pola yang lebih mungkin terlihat pada pembukaan ini adalah seperti ini, 

128
00:06:15,625 --> 00:06:18,120
yang tentu saja tidak ada huruf W di dalamnya.

129
00:06:18,240 --> 00:06:21,400
Mungkin ada E, dan mungkin tidak ada A, tidak ada R, tidak ada Y.

130
00:06:22,080 --> 00:06:24,560
Dalam hal ini, ada 1400 kemungkinan kecocokan.

131
00:06:25,080 --> 00:06:28,027
Jika semua memiliki kemungkinan yang sama, maka kemungkinannya 

132
00:06:28,027 --> 00:06:30,600
sekitar 11% bahwa ini adalah pola yang akan Anda lihat.

133
00:06:30,900 --> 00:06:33,340
Jadi hasil yang paling mungkin juga paling tidak informatif.

134
00:06:34,240 --> 00:06:36,471
Untuk mendapatkan gambaran yang lebih global di sini, 

135
00:06:36,471 --> 00:06:39,817
izinkan saya menunjukkan kepada Anda distribusi penuh probabilitas di semua pola 

136
00:06:39,817 --> 00:06:41,140
berbeda yang mungkin Anda lihat.

137
00:06:41,740 --> 00:06:45,175
Jadi tiap batang yang Anda lihat sesuai dengan kemungkinan pola warna 

138
00:06:45,175 --> 00:06:48,463
yang dapat ditampilkan, yang mana terdapat 3 hingga 5 kemungkinan, 

139
00:06:48,463 --> 00:06:52,340
dan disusun dari kiri ke kanan, yang paling umum hingga yang paling tidak umum.

140
00:06:52,920 --> 00:06:56,000
Jadi kemungkinan paling umum di sini adalah Anda mendapatkan warna abu-abu.

141
00:06:56,100 --> 00:06:58,120
Itu terjadi sekitar 14% dari seluruh kasus.

142
00:06:58,580 --> 00:07:02,047
Dan yang Anda harapkan saat menebak adalah Anda berakhir di suatu 

143
00:07:02,047 --> 00:07:05,462
tempat di ekor panjang ini, seperti di sini di mana hanya ada 18 

144
00:07:05,462 --> 00:07:09,140
kemungkinan yang cocok dengan pola yang ternyata terlihat seperti ini.

145
00:07:09,920 --> 00:07:13,800
Atau jika kita melangkah lebih jauh ke kiri, mungkin kita akan terus ke sini.

146
00:07:14,940 --> 00:07:16,180
Oke, ini teka-teki yang bagus untuk Anda.

147
00:07:16,540 --> 00:07:19,072
Apa tiga kata dalam bahasa Inggris yang dimulai dengan huruf W, 

148
00:07:19,072 --> 00:07:22,000
diakhiri dengan huruf Y, dan memiliki huruf R di suatu tempat di dalamnya?

149
00:07:22,480 --> 00:07:26,800
Ternyata jawabannya adalah, mari kita lihat, bertele-tele, cacingan, dan masam.

150
00:07:27,500 --> 00:07:30,892
Jadi untuk menilai seberapa bagus kata ini secara keseluruhan, 

151
00:07:30,892 --> 00:07:35,740
kami ingin mengukur perkiraan jumlah informasi yang akan Anda peroleh dari distribusi ini.

152
00:07:35,740 --> 00:07:38,668
Jika kita menelusuri setiap pola dan mengalikan kemungkinan 

153
00:07:38,668 --> 00:07:42,328
terjadinya dengan sesuatu yang mengukur seberapa informatif pola tersebut, 

154
00:07:42,328 --> 00:07:44,720
hal itu mungkin dapat memberi kita skor objektif.

155
00:07:45,960 --> 00:07:47,765
Sekarang insting pertama Anda tentang apa yang 

156
00:07:47,765 --> 00:07:49,840
seharusnya terjadi mungkin adalah jumlah pertandingan.

157
00:07:50,160 --> 00:07:52,400
Anda menginginkan jumlah rata-rata kecocokan yang lebih rendah.

158
00:07:52,800 --> 00:07:55,610
Namun saya ingin menggunakan pengukuran yang lebih universal yang sering kita 

159
00:07:55,610 --> 00:07:58,493
anggap berasal dari informasi, dan pengukuran yang akan lebih fleksibel setelah 

160
00:07:58,493 --> 00:08:01,485
kita menetapkan probabilitas berbeda untuk masing-masing dari 13.000 kata tersebut 

161
00:08:01,485 --> 00:08:04,260
untuk mengetahui apakah kata-kata tersebut benar-benar jawabannya atau tidak.

162
00:08:10,320 --> 00:08:14,154
Satuan standar informasi adalah bit, yang memiliki rumus yang sedikit lucu, 

163
00:08:14,154 --> 00:08:16,980
namun sangat intuitif jika kita hanya melihat contohnya.

164
00:08:17,780 --> 00:08:20,702
Jika Anda mempunyai pengamatan yang membagi dua kemungkinan yang ada, 

165
00:08:20,702 --> 00:08:23,500
kami katakan bahwa pengamatan tersebut mempunyai sedikit informasi.

166
00:08:24,180 --> 00:08:26,744
Dalam contoh kita, ruang kemungkinannya adalah semua kemungkinan kata, 

167
00:08:26,744 --> 00:08:29,273
dan ternyata sekitar setengah dari lima huruf kata mempunyai huruf S, 

168
00:08:29,273 --> 00:08:31,260
sedikit lebih kecil dari itu, tapi sekitar setengahnya.

169
00:08:31,780 --> 00:08:34,320
Sehingga pengamatan itu akan memberi Anda sedikit informasi.

170
00:08:34,880 --> 00:08:38,870
Jika suatu fakta baru memperkecil ruang kemungkinan tersebut sebanyak empat kali lipat, 

171
00:08:38,870 --> 00:08:41,500
kita katakan bahwa fakta tersebut mempunyai dua informasi.

172
00:08:41,980 --> 00:08:44,460
Misalnya, ternyata sekitar seperempat dari kata-kata ini memiliki huruf T.

173
00:08:45,020 --> 00:08:47,749
Jika observasi memotong ruang tersebut sebanyak delapan kali lipat, 

174
00:08:47,749 --> 00:08:50,720
kita katakan itu adalah tiga bit informasi, dan seterusnya dan seterusnya.

175
00:08:50,900 --> 00:08:55,060
Empat bit memotongnya menjadi 16, lima bit memotongnya menjadi 32.

176
00:08:55,060 --> 00:08:58,859
Jadi sekarang Anda mungkin ingin berhenti sejenak dan bertanya pada diri sendiri, 

177
00:08:58,859 --> 00:09:02,660
apa rumus informasi jumlah bit dalam kaitannya dengan probabilitas suatu kejadian?

178
00:09:02,660 --> 00:09:06,725
Apa yang ingin kami katakan di sini adalah ketika Anda mengambil setengah dari jumlah 

179
00:09:06,725 --> 00:09:10,837
bit, itu sama dengan probabilitas, yang sama dengan mengatakan dua pangkat dari jumlah 

180
00:09:10,837 --> 00:09:14,996
bit adalah satu di atas probabilitas, yaitu menyusun ulang lebih jauh dengan mengatakan 

181
00:09:14,996 --> 00:09:18,920
bahwa informasi tersebut adalah basis log dua dari satu dibagi dengan probabilitas.

182
00:09:19,620 --> 00:09:22,260
Dan kadang-kadang Anda melihat ini dengan satu penataan ulang lagi, 

183
00:09:22,260 --> 00:09:24,900
di mana informasinya adalah log negatif basis dua dari probabilitas.

184
00:09:25,660 --> 00:09:29,217
Jika diungkapkan seperti ini, hal ini mungkin terlihat sedikit aneh bagi yang belum tahu, 

185
00:09:29,217 --> 00:09:32,024
namun sebenarnya ini hanyalah gagasan intuitif untuk menanyakan berapa 

186
00:09:32,024 --> 00:09:34,080
kali Anda telah mengurangi separuh kemungkinan Anda.

187
00:09:35,180 --> 00:09:37,294
Sekarang jika Anda bertanya-tanya, Anda tahu, saya pikir kita hanya memainkan 

188
00:09:37,294 --> 00:09:39,300
permainan kata yang menyenangkan, mengapa logaritma masuk ke dalam gambar?

189
00:09:39,780 --> 00:09:43,058
Salah satu alasan unit ini lebih bagus adalah karena lebih mudah untuk 

190
00:09:43,058 --> 00:09:45,736
membicarakan peristiwa yang sangat tidak mungkin terjadi, 

191
00:09:45,736 --> 00:09:48,922
lebih mudah untuk mengatakan bahwa suatu pengamatan mempunyai 20 bit 

192
00:09:48,922 --> 00:09:52,940
informasi daripada mengatakan bahwa probabilitas kejadian ini dan itu adalah 0.0000095.

193
00:09:53,300 --> 00:09:57,108
Namun alasan yang lebih substantif mengapa ekspresi logaritmik ini ternyata menjadi 

194
00:09:57,108 --> 00:10:00,915
tambahan yang sangat berguna bagi teori probabilitas adalah cara informasi tersebut 

195
00:10:00,915 --> 00:10:01,460
dijumlahkan.

196
00:10:02,060 --> 00:10:04,958
Misalnya, jika satu observasi memberi Anda dua bit informasi, 

197
00:10:04,958 --> 00:10:08,418
mengurangi ruang Anda menjadi empat, dan kemudian observasi kedua seperti 

198
00:10:08,418 --> 00:10:11,550
tebakan kedua Anda di Wordle memberi Anda tiga bit informasi lagi, 

199
00:10:11,550 --> 00:10:14,402
mengurangi Anda lebih jauh lagi sebanyak delapan kali lipat, 

200
00:10:14,402 --> 00:10:16,740
maka dua bersama-sama memberi Anda lima informasi.

201
00:10:17,160 --> 00:10:21,020
Sama seperti probabilitas yang berlipat ganda, informasi juga suka bertambah.

202
00:10:21,960 --> 00:10:24,534
Jadi segera setelah kita berada di bidang nilai yang diharapkan, 

203
00:10:24,534 --> 00:10:27,980
di mana kita menambahkan banyak angka, log akan membuatnya lebih mudah untuk ditangani.

204
00:10:28,480 --> 00:10:31,911
Mari kita kembali ke distribusi Weary dan menambahkan pelacak kecil lainnya di sini, 

205
00:10:31,911 --> 00:10:34,940
menunjukkan kepada kita berapa banyak informasi yang ada untuk setiap pola.

206
00:10:35,580 --> 00:10:38,070
Hal utama yang saya ingin Anda perhatikan adalah semakin tinggi 

207
00:10:38,070 --> 00:10:40,172
kemungkinan kita mendapatkan pola yang lebih mungkin, 

208
00:10:40,172 --> 00:10:42,780
semakin rendah informasinya, semakin sedikit bit yang Anda peroleh.

209
00:10:43,500 --> 00:10:47,075
Cara kita mengukur kualitas tebakan ini adalah dengan mengambil nilai yang diharapkan 

210
00:10:47,075 --> 00:10:49,403
dari informasi tersebut, dengan menelusuri setiap pola, 

211
00:10:49,403 --> 00:10:52,895
kita tentukan seberapa besar kemungkinannya, lalu kita kalikan dengan berapa banyak 

212
00:10:52,895 --> 00:10:54,060
informasi yang kita peroleh.

213
00:10:54,710 --> 00:10:58,120
Dan dalam contoh Weary, ternyata menjadi 4.9 bit.

214
00:10:58,560 --> 00:11:02,020
Jadi rata-rata, informasi yang Anda dapatkan dari tebakan pembuka ini sama baiknya 

215
00:11:02,020 --> 00:11:05,480
dengan memotong ruang kemungkinan Anda menjadi setengahnya sekitar lima kali lipat.

216
00:11:05,960 --> 00:11:08,800
Sebaliknya, contoh tebakan dengan nilai informasi 

217
00:11:08,800 --> 00:11:11,640
yang diharapkan lebih tinggi adalah seperti Slate.

218
00:11:13,120 --> 00:11:15,620
Dalam hal ini Anda akan melihat distribusinya terlihat lebih datar.

219
00:11:15,940 --> 00:11:20,515
Secara khusus, kemunculan semua warna abu-abu yang paling mungkin hanya memiliki 

220
00:11:20,515 --> 00:11:25,260
peluang sekitar 6% untuk muncul, jadi setidaknya Anda mendapatkan 3.9 bit informasi.

221
00:11:25,920 --> 00:11:28,560
Tapi itu jumlah minimum, biasanya Anda akan mendapatkan sesuatu yang lebih baik dari itu.

222
00:11:29,100 --> 00:11:32,475
Dan ternyata ketika Anda menghitung angka-angka ini dan menjumlahkan 

223
00:11:32,475 --> 00:11:35,900
semua istilah yang relevan, rata-rata informasinya adalah sekitar 5.8.

224
00:11:37,360 --> 00:11:40,424
Jadi berbeda dengan Weary, rata-rata ruang kemungkinan Anda 

225
00:11:40,424 --> 00:11:43,540
akan menjadi sekitar setengahnya setelah tebakan pertama ini.

226
00:11:44,420 --> 00:11:49,120
Sebenarnya ada cerita menarik tentang nama nilai kuantitas informasi yang diharapkan ini.

227
00:11:49,200 --> 00:11:51,299
Teori informasi dikembangkan oleh Claude Shannon, 

228
00:11:51,299 --> 00:11:54,910
yang bekerja di Bell Labs pada tahun 1940-an, tetapi dia membicarakan beberapa idenya 

229
00:11:54,910 --> 00:11:57,051
yang belum dipublikasikan dengan John von Neumann, 

230
00:11:57,051 --> 00:11:59,907
yang merupakan raksasa intelektual pada saat itu, sangat terkemuka. 

231
00:11:59,907 --> 00:12:03,560
dalam matematika dan fisika dan permulaan dari apa yang kemudian menjadi ilmu komputer.

232
00:12:04,100 --> 00:12:07,363
Dan ketika dia menyebutkan bahwa dia tidak benar-benar memiliki nama yang 

233
00:12:07,363 --> 00:12:11,112
bagus untuk nilai yang diharapkan dari kuantitas informasi ini, von Neumann berkata, 

234
00:12:11,112 --> 00:12:14,200
jadi ceritanya, Anda harus menyebutnya entropi, dan karena dua alasan.

235
00:12:14,540 --> 00:12:18,613
Pertama, fungsi ketidakpastian Anda telah digunakan dalam mekanika statistik dengan nama 

236
00:12:18,613 --> 00:12:22,045
tersebut, sehingga sudah memiliki nama, dan kedua, dan yang lebih penting, 

237
00:12:22,045 --> 00:12:25,844
tidak ada yang tahu apa sebenarnya entropi, jadi dalam perdebatan Anda akan selalu 

238
00:12:25,844 --> 00:12:26,760
memiliki keuntungan.

239
00:12:27,700 --> 00:12:31,480
Jadi jika namanya tampak sedikit misterius, dan jika cerita ini dapat dipercaya, 

240
00:12:31,480 --> 00:12:32,460
itu memang disengaja.

241
00:12:33,280 --> 00:12:36,369
Juga jika Anda bertanya-tanya tentang hubungannya dengan semua hukum 

242
00:12:36,369 --> 00:12:38,877
kedua termodinamika dari fisika, pasti ada hubungannya, 

243
00:12:38,877 --> 00:12:42,325
tetapi pada awalnya Shannon hanya berurusan dengan teori probabilitas murni, 

244
00:12:42,325 --> 00:12:45,684
dan untuk tujuan kita di sini, ketika saya menggunakan teori kata entropi, 

245
00:12:45,684 --> 00:12:49,580
saya hanya ingin Anda memikirkan nilai informasi yang diharapkan dari tebakan tertentu.

246
00:12:50,700 --> 00:12:53,780
Anda dapat menganggap entropi sebagai mengukur dua hal secara bersamaan.

247
00:12:54,240 --> 00:12:56,780
Yang pertama adalah seberapa datar distribusinya.

248
00:12:57,320 --> 00:13:01,120
Semakin dekat suatu distribusi ke seragam, semakin tinggi entropinya.

249
00:13:01,580 --> 00:13:06,095
Dalam kasus kita, jika terdapat 3 hingga 5 pola total, untuk distribusi seragam, 

250
00:13:06,095 --> 00:13:10,220
mengamati salah satu dari pola tersebut akan memiliki basis log informasi 

251
00:13:10,220 --> 00:13:13,007
2 dari 3 hingga 5, yang kebetulan berjumlah 7.92, 

252
00:13:13,007 --> 00:13:17,300
jadi itulah nilai maksimum mutlak yang mungkin Anda miliki untuk entropi ini.

253
00:13:17,840 --> 00:13:22,080
Namun entropi juga merupakan ukuran seberapa banyak kemungkinan yang ada.

254
00:13:22,320 --> 00:13:26,523
Misalnya, jika Anda memiliki suatu kata yang hanya memiliki 16 kemungkinan pola, 

255
00:13:26,523 --> 00:13:29,689
dan setiap pola memiliki kemungkinan yang sama, entropi ini, 

256
00:13:29,689 --> 00:13:32,180
informasi yang diharapkan, akan berjumlah 4 bit.

257
00:13:32,579 --> 00:13:36,578
Namun jika Anda memiliki kata lain yang memiliki 64 kemungkinan pola yang muncul, 

258
00:13:36,578 --> 00:13:40,480
dan semuanya memiliki kemungkinan yang sama, maka entropinya akan menjadi 6 bit.

259
00:13:41,500 --> 00:13:45,667
Jadi, jika Anda melihat suatu distribusi di alam liar yang memiliki entropi 6 bit, 

260
00:13:45,667 --> 00:13:49,533
hal ini seperti menyatakan bahwa ada banyak variasi dan ketidakpastian dalam 

261
00:13:49,533 --> 00:13:53,500
apa yang akan terjadi, seolah-olah ada 64 kemungkinan hasil yang sama besarnya.

262
00:13:54,360 --> 00:13:59,320
Untuk pass pertama saya di Wurtelebot, pada dasarnya saya menyuruhnya melakukan ini saja.

263
00:13:59,320 --> 00:14:03,031
Ia menelusuri semua kemungkinan tebakan yang Anda miliki, seluruh 13.000 kata, 

264
00:14:03,031 --> 00:14:06,273
menghitung entropi untuk masing-masing kata, atau lebih khusus lagi, 

265
00:14:06,273 --> 00:14:10,220
entropi distribusi di semua pola yang mungkin Anda lihat, untuk masing-masing kata, 

266
00:14:10,220 --> 00:14:14,260
dan memilih yang tertinggi, karena itulah salah satu yang kemungkinan akan mengurangi 

267
00:14:14,260 --> 00:14:16,140
ruang kemungkinan Anda sebanyak mungkin.

268
00:14:17,140 --> 00:14:19,220
Dan meskipun saya hanya membicarakan tebakan pertama di sini, 

269
00:14:19,220 --> 00:14:21,100
hal yang sama berlaku untuk beberapa tebakan berikutnya.

270
00:14:21,560 --> 00:14:24,576
Misalnya, setelah Anda melihat beberapa pola pada tebakan pertama tersebut, 

271
00:14:24,576 --> 00:14:27,672
yang akan membatasi Anda pada kemungkinan kata yang lebih sedikit berdasarkan 

272
00:14:27,672 --> 00:14:30,966
kecocokannya, Anda cukup memainkan permainan yang sama terhadap kumpulan kata yang 

273
00:14:30,966 --> 00:14:31,800
lebih kecil tersebut.

274
00:14:32,260 --> 00:14:35,937
Untuk usulan tebakan kedua, Anda melihat distribusi semua pola yang dapat 

275
00:14:35,937 --> 00:14:38,770
terjadi dari kumpulan kata yang lebih terbatas tersebut, 

276
00:14:38,770 --> 00:14:42,299
Anda menelusuri 13.000 kemungkinan, dan Anda menemukan salah satu yang 

277
00:14:42,299 --> 00:14:43,840
memaksimalkan entropi tersebut.

278
00:14:45,420 --> 00:14:48,195
Untuk menunjukkan kepada Anda cara kerjanya, izinkan saya 

279
00:14:48,195 --> 00:14:50,874
menampilkan sedikit varian Wurtele yang saya tulis yang 

280
00:14:50,874 --> 00:14:54,080
menunjukkan hal-hal penting dari analisis ini di bagian pinggirnya.

281
00:14:54,080 --> 00:14:56,020
Setelah melakukan semua perhitungan entropinya, 

282
00:14:56,020 --> 00:14:59,660
di sini ia menunjukkan kepada kita mana yang memiliki informasi yang diharapkan tertinggi.

283
00:15:00,280 --> 00:15:05,598
Ternyata jawaban teratas, setidaknya untuk saat ini, akan kita perbaiki nanti, 

284
00:15:05,598 --> 00:15:10,580
adalah Taras, yang artinya, um, tentu saja, vetch, vetch yang paling umum.

285
00:15:11,040 --> 00:15:13,435
Tiap kali kita membuat tebakan di sini, mungkin saya mengabaikan 

286
00:15:13,435 --> 00:15:15,573
rekomendasinya dan memilih slate, karena saya suka slate, 

287
00:15:15,573 --> 00:15:18,043
kita bisa melihat berapa banyak informasi yang diharapkan darinya, 

288
00:15:18,043 --> 00:15:20,549
tapi kemudian di sebelah kanan kata ini, ia menunjukkan kepada kita 

289
00:15:20,549 --> 00:15:23,461
seberapa banyak informasi yang diharapkan. informasi aktual yang kami peroleh, 

290
00:15:23,461 --> 00:15:24,420
mengingat pola khusus ini.

291
00:15:25,000 --> 00:15:26,852
Jadi di sini sepertinya kami sedikit kurang beruntung, 

292
00:15:26,852 --> 00:15:29,412
kami diharapkan mendapat nilai 5.8, tapi kebetulan kami mendapatkan sesuatu 

293
00:15:29,412 --> 00:15:30,120
yang kurang dari itu.

294
00:15:30,600 --> 00:15:32,756
Dan kemudian di sisi kiri ini menunjukkan kepada kita semua 

295
00:15:32,756 --> 00:15:35,020
kemungkinan kata yang berbeda berdasarkan posisi kita sekarang.

296
00:15:35,800 --> 00:15:38,725
Bilah biru memberi tahu kita seberapa besar kemungkinannya untuk memikirkan setiap kata, 

297
00:15:38,725 --> 00:15:41,157
sehingga saat ini diasumsikan bahwa setiap kata memiliki kemungkinan yang 

298
00:15:41,157 --> 00:15:43,360
sama untuk muncul, namun kami akan menyempurnakannya sebentar lagi.

299
00:15:44,060 --> 00:15:47,791
Dan kemudian pengukuran ketidakpastian ini memberi tahu kita entropi dari 

300
00:15:47,791 --> 00:15:52,279
distribusi ini di seluruh kemungkinan kata, yang saat ini, karena distribusinya seragam, 

301
00:15:52,279 --> 00:15:55,960
hanyalah cara rumit yang tidak perlu untuk menghitung jumlah kemungkinan.

302
00:15:56,560 --> 00:16:02,180
Misalnya, jika kita ingin 2 dipangkatkan 13.66, itu seharusnya sekitar 13.000 kemungkinan.

303
00:16:02,900 --> 00:16:04,655
Saya sedikit melenceng di sini, tetapi hanya karena 

304
00:16:04,655 --> 00:16:06,140
saya tidak menampilkan semua tempat desimal.

305
00:16:06,720 --> 00:16:09,333
Saat ini hal itu mungkin terasa berlebihan dan sepertinya terlalu rumit, 

306
00:16:09,333 --> 00:16:12,340
tetapi Anda akan mengerti mengapa ada gunanya memiliki kedua angka dalam satu menit.

307
00:16:12,760 --> 00:16:16,865
Jadi di sini sepertinya entropi tertinggi untuk tebakan kedua kita adalah Ramen, 

308
00:16:16,865 --> 00:16:19,400
yang sekali lagi tidak terasa seperti sebuah kata.

309
00:16:19,980 --> 00:16:22,353
Jadi untuk mengambil landasan moral yang tinggi di sini, 

310
00:16:22,353 --> 00:16:24,060
saya akan melanjutkan dan mengetik Rains.

311
00:16:25,440 --> 00:16:27,340
Dan sekali lagi sepertinya kami sedikit kurang beruntung.

312
00:16:27,520 --> 00:16:31,360
Kami mengharapkan 4.3 bit dan kami hanya mendapat 3.39 bit informasi.

313
00:16:31,940 --> 00:16:33,940
Jadi itu membawa kita ke 55 kemungkinan.

314
00:16:34,900 --> 00:16:38,024
Dan di sini mungkin saya akan mengikuti apa yang disarankannya, 

315
00:16:38,024 --> 00:16:39,440
yaitu kombo, apa pun artinya.

316
00:16:40,040 --> 00:16:42,920
Dan oke, ini sebenarnya kesempatan bagus untuk membuat teka-teki.

317
00:16:42,920 --> 00:16:46,380
Ini memberi tahu kita bahwa pola ini memberi kita 4.7 bit informasi.

318
00:16:47,060 --> 00:16:51,720
Tapi di sebelah kiri, sebelum kita melihat pola itu, ada 5.78 bit ketidakpastian.

319
00:16:52,420 --> 00:16:56,340
Jadi sebagai kuis untuk Anda, apa maksudnya dengan jumlah kemungkinan yang tersisa?

320
00:16:58,040 --> 00:17:01,110
Artinya, kita hanya dihadapkan pada sedikit ketidakpastian, 

321
00:17:01,110 --> 00:17:04,540
yang sama saja dengan mengatakan bahwa ada dua kemungkinan jawaban.

322
00:17:04,700 --> 00:17:05,700
Itu adalah pilihan 50-50.

323
00:17:06,500 --> 00:17:08,969
Dan dari sini, karena Anda dan saya tahu kata mana yang lebih umum, 

324
00:17:08,969 --> 00:17:10,640
kita tahu bahwa jawabannya pasti sangat buruk.

325
00:17:11,180 --> 00:17:13,280
Namun seperti yang tertulis sekarang, program tersebut tidak mengetahui hal itu.

326
00:17:13,540 --> 00:17:17,103
Jadi ia terus berjalan, berusaha mendapatkan informasi sebanyak-banyaknya, 

327
00:17:17,103 --> 00:17:19,859
hingga hanya tersisa satu kemungkinan, lalu ia menebaknya.

328
00:17:20,380 --> 00:17:22,339
Jadi jelas kita memerlukan strategi akhir yang lebih baik.

329
00:17:22,599 --> 00:17:25,429
Tapi katakanlah kita menyebut versi ini sebagai salah satu pemecah kata-kata kita, 

330
00:17:25,429 --> 00:17:28,260
dan kemudian kita menjalankan beberapa simulasi untuk melihat bagaimana kinerjanya.

331
00:17:30,360 --> 00:17:34,120
Jadi cara kerjanya adalah dengan memainkan setiap permainan kata yang memungkinkan.

332
00:17:34,240 --> 00:17:38,540
Itu melewati 2.315 kata yang merupakan jawaban kata yang sebenarnya.

333
00:17:38,540 --> 00:17:40,580
Ini pada dasarnya menggunakannya sebagai set pengujian.

334
00:17:41,360 --> 00:17:45,395
Dan dengan metode naif ini dengan tidak mempertimbangkan seberapa umum suatu kata, 

335
00:17:45,395 --> 00:17:48,361
dan hanya mencoba memaksimalkan informasi di setiap langkah, 

336
00:17:48,361 --> 00:17:49,820
hingga hanya ada satu pilihan.

337
00:17:50,360 --> 00:17:54,300
Pada akhir simulasi, skor rata-rata menjadi sekitar 4.124.

338
00:17:55,319 --> 00:17:59,240
Itu tidak buruk, sejujurnya, saya berharap untuk melakukan yang lebih buruk.

339
00:17:59,660 --> 00:18:01,143
Tetapi orang-orang yang bermain wordle akan memberitahu 

340
00:18:01,143 --> 00:18:02,600
Anda bahwa mereka biasanya bisa mendapatkannya dalam 4.

341
00:18:02,860 --> 00:18:05,380
Tantangan sebenarnya adalah mendapatkan sebanyak 3 buah sebanyak yang Anda bisa.

342
00:18:05,380 --> 00:18:08,080
Ini merupakan lompatan yang cukup besar antara skor 4 dan skor 3.

343
00:18:08,860 --> 00:18:11,987
Hal yang jelas terlihat di sini adalah dengan memasukkan apakah suatu 

344
00:18:11,987 --> 00:18:14,980
kata itu umum atau tidak, dan bagaimana tepatnya kita melakukannya.

345
00:18:22,800 --> 00:18:25,340
Cara saya mendekatinya adalah dengan mendapatkan daftar 

346
00:18:25,340 --> 00:18:27,880
frekuensi relatif untuk semua kata dalam bahasa Inggris.

347
00:18:28,220 --> 00:18:31,516
Dan saya baru saja menggunakan fungsi data frekuensi kata Mathematica, 

348
00:18:31,516 --> 00:18:34,860
yang diambil dari kumpulan data publik Google Buku Bahasa Inggris Ngram.

349
00:18:35,460 --> 00:18:37,846
Dan itu menyenangkan untuk dilihat, misalnya jika kita mengurutkannya 

350
00:18:37,846 --> 00:18:39,960
dari kata yang paling umum hingga kata yang paling tidak umum.

351
00:18:40,120 --> 00:18:43,080
Rupanya ini adalah kata 5 huruf yang paling umum dalam bahasa Inggris.

352
00:18:43,700 --> 00:18:45,840
Atau lebih tepatnya, ini adalah yang paling umum ke-8.

353
00:18:46,280 --> 00:18:48,880
Pertama yang mana, setelah itu ada disana dan disana.

354
00:18:49,260 --> 00:18:51,719
Yang pertama bukanlah yang pertama, melainkan yang ke-9, 

355
00:18:51,719 --> 00:18:54,308
dan masuk akal jika kata-kata lain ini muncul lebih sering, 

356
00:18:54,308 --> 00:18:56,724
jika kata setelah yang pertama adalah setelah, di mana, 

357
00:18:56,724 --> 00:18:58,580
dan kata-kata tersebut menjadi kurang umum.

358
00:18:59,160 --> 00:19:01,726
Sekarang, dalam menggunakan data ini untuk memodelkan seberapa 

359
00:19:01,726 --> 00:19:04,334
besar kemungkinan masing-masing kata ini menjadi jawaban akhir, 

360
00:19:04,334 --> 00:19:06,860
data tersebut tidak boleh hanya sebanding dengan frekuensinya.

361
00:19:06,860 --> 00:19:10,000
Misal yang diberi skor 0.002 dalam kumpulan data ini, 

362
00:19:10,000 --> 00:19:15,060
sedangkan kata jalinan dalam beberapa hal kemungkinannya sekitar 1000 kali lebih kecil.

363
00:19:15,560 --> 00:19:17,166
Namun keduanya adalah kata-kata yang cukup umum 

364
00:19:17,166 --> 00:19:18,840
sehingga hampir pasti layak untuk dipertimbangkan.

365
00:19:19,340 --> 00:19:21,000
Jadi kami ingin lebih banyak pemutusan biner.

366
00:19:21,860 --> 00:19:25,775
Cara saya melakukannya adalah dengan membayangkan mengambil seluruh daftar kata yang 

367
00:19:25,775 --> 00:19:28,263
diurutkan ini, dan kemudian menyusunnya pada sumbu x, 

368
00:19:28,263 --> 00:19:32,409
dan kemudian menerapkan fungsi sigmoid, yang merupakan cara standar untuk memiliki fungsi 

369
00:19:32,409 --> 00:19:35,127
yang keluarannya pada dasarnya biner, itu's baik 0 atau 1, 

370
00:19:35,127 --> 00:19:38,260
namun terdapat kelancaran di antara wilayah ketidakpastian tersebut.

371
00:19:39,160 --> 00:19:43,694
Jadi pada dasarnya, probabilitas yang saya tetapkan untuk setiap kata untuk berada di 

372
00:19:43,694 --> 00:19:48,440
daftar akhir akan menjadi nilai fungsi sigmoid di atas di mana pun ia berada pada sumbu x.

373
00:19:49,520 --> 00:19:52,234
Tentu saja hal ini bergantung pada beberapa parameter, 

374
00:19:52,234 --> 00:19:55,639
misalnya seberapa lebar spasi pada sumbu x kata-kata tersebut terisi 

375
00:19:55,639 --> 00:19:58,946
menentukan seberapa bertahap atau tajamnya kita turun dari 1 ke 0, 

376
00:19:58,946 --> 00:20:03,240
dan di mana kita menempatkan kata-kata tersebut dari kiri ke kanan menentukan batasnya.

377
00:20:03,240 --> 00:20:06,920
Sejujurnya, caraku melakukan ini hanyalah menjilat jariku dan menempelkannya ke angin.

378
00:20:07,140 --> 00:20:10,544
Saya melihat daftar yang diurutkan dan mencoba menemukan jendela di mana 

379
00:20:10,544 --> 00:20:13,855
ketika saya melihatnya, saya pikir sekitar setengah dari kata-kata ini 

380
00:20:13,855 --> 00:20:17,260
lebih mungkin menjadi jawaban akhir, dan menggunakannya sebagai batasnya.

381
00:20:17,260 --> 00:20:20,120
Setelah kita mendapatkan distribusi seperti ini di seluruh kata, 

382
00:20:20,120 --> 00:20:23,860
ini memberi kita situasi lain di mana entropi menjadi pengukuran yang sangat berguna.

383
00:20:24,500 --> 00:20:27,370
Sebagai contoh, katakanlah kita sedang bermain game dan kita mulai 

384
00:20:27,370 --> 00:20:29,384
dengan pembuka lama saya, yaitu bulu dan paku, 

385
00:20:29,384 --> 00:20:33,240
dan kita berakhir dengan situasi di mana ada empat kemungkinan kata yang cocok dengan itu.

386
00:20:33,560 --> 00:20:35,620
Dan katakanlah kita menganggap semuanya memiliki kemungkinan yang sama.

387
00:20:36,220 --> 00:20:38,880
Izinkan saya bertanya, berapa entropi distribusi ini?

388
00:20:41,080 --> 00:20:45,677
Nah, informasi yang terkait dengan masing-masing kemungkinan ini akan menjadi 

389
00:20:45,677 --> 00:20:50,040
basis log 2 dari 4, karena masing-masing adalah 1 dan 4, dan itu adalah 2.

390
00:20:50,040 --> 00:20:52,460
Dua informasi, empat kemungkinan.

391
00:20:52,760 --> 00:20:53,580
Semuanya sangat baik dan bagus.

392
00:20:54,300 --> 00:20:57,800
Tapi bagaimana jika saya bilang sebenarnya ada lebih dari empat pertandingan?

393
00:20:58,260 --> 00:21:02,460
Kenyataannya, jika kita melihat daftar kata selengkapnya, ada 16 kata yang cocok.

394
00:21:02,580 --> 00:21:06,670
Namun misalkan model kita memberikan probabilitas yang sangat rendah pada 12 kata lainnya 

395
00:21:06,670 --> 00:21:10,760
untuk menjadi jawaban akhir, sekitar 1 dalam 1000 karena kata tersebut sangat tidak jelas.

396
00:21:11,500 --> 00:21:14,260
Sekarang izinkan saya bertanya, berapa entropi distribusi ini?

397
00:21:15,420 --> 00:21:18,281
Jika entropi hanya mengukur jumlah kecocokan di sini, 

398
00:21:18,281 --> 00:21:21,831
Anda mungkin mengharapkannya menjadi basis log 2 dari 16, yaitu 4, 

399
00:21:21,831 --> 00:21:25,700
dua bit ketidakpastian lebih banyak daripada yang kita miliki sebelumnya.

400
00:21:26,180 --> 00:21:28,072
Namun tentu saja ketidakpastian yang sebenarnya tidak 

401
00:21:28,072 --> 00:21:29,860
jauh berbeda dengan apa yang kita alami sebelumnya.

402
00:21:30,160 --> 00:21:33,830
Hanya karena ada 12 kata yang sangat tidak jelas ini tidak berarti akan lebih 

403
00:21:33,830 --> 00:21:37,360
mengejutkan jika mengetahui bahwa jawaban akhirnya adalah pesona, misalnya.

404
00:21:38,180 --> 00:21:40,612
Jadi ketika Anda benar-benar melakukan perhitungan di sini, 

405
00:21:40,612 --> 00:21:44,140
dan Anda menjumlahkan probabilitas setiap kejadian dikalikan dengan informasi terkait, 

406
00:21:44,140 --> 00:21:45,560
yang Anda dapatkan adalah 2.11 bit.

407
00:21:45,560 --> 00:21:49,164
Maksud saya, pada dasarnya ada dua bagian, pada dasarnya empat kemungkinan tersebut, 

408
00:21:49,164 --> 00:21:52,598
namun ada sedikit ketidakpastian karena semua kejadian yang sangat tidak mungkin 

409
00:21:52,598 --> 00:21:56,160
tersebut, meskipun jika Anda mempelajarinya, Anda akan mendapatkan banyak informasi 

410
00:21:56,160 --> 00:21:56,500
darinya.

411
00:21:57,160 --> 00:21:59,242
Jadi jika diperkecil, inilah bagian yang membuat Wordle 

412
00:21:59,242 --> 00:22:01,400
menjadi contoh yang bagus untuk pelajaran teori informasi.

413
00:22:01,600 --> 00:22:04,640
Kami memiliki dua penerapan perasaan yang berbeda untuk entropi.

414
00:22:05,160 --> 00:22:08,578
Yang pertama memberi tahu kita informasi apa yang diharapkan yang akan kita 

415
00:22:08,578 --> 00:22:12,131
peroleh dari tebakan tertentu, dan yang kedua mengatakan bisakah kita mengukur 

416
00:22:12,131 --> 00:22:15,460
ketidakpastian yang tersisa di antara semua kata yang mungkin kita miliki.

417
00:22:16,460 --> 00:22:18,989
Dan saya harus menekankan, dalam kasus pertama ketika kita melihat 

418
00:22:18,989 --> 00:22:20,764
informasi yang diharapkan dari sebuah tebakan, 

419
00:22:20,764 --> 00:22:23,142
setelah kita memiliki bobot yang tidak sama pada kata-katanya, 

420
00:22:23,142 --> 00:22:24,540
itu mempengaruhi perhitungan entropi.

421
00:22:24,980 --> 00:22:27,920
Sebagai contoh, izinkan saya mengambil kasus yang sama yang kita lihat 

422
00:22:27,920 --> 00:22:30,281
sebelumnya tentang distribusi yang terkait dengan Weary, 

423
00:22:30,281 --> 00:22:33,720
namun kali ini menggunakan distribusi yang tidak seragam di semua kemungkinan kata.

424
00:22:34,500 --> 00:22:36,372
Jadi izinkan saya melihat apakah saya dapat menemukan 

425
00:22:36,372 --> 00:22:38,280
bagian di sini yang menggambarkannya dengan cukup baik.

426
00:22:40,940 --> 00:22:42,360
Oke, ini cukup bagus.

427
00:22:42,360 --> 00:22:45,822
Di sini kita memiliki dua pola berdekatan yang kemungkinannya hampir sama, 

428
00:22:45,822 --> 00:22:49,100
namun salah satu pola tersebut memiliki 32 kemungkinan kata yang cocok.

429
00:22:49,280 --> 00:22:51,895
Dan jika kita periksa apa itu, ini adalah 32 kata tersebut, 

430
00:22:51,895 --> 00:22:55,600
yang semuanya merupakan kata-kata yang sangat tidak mungkin ketika Anda mengamatinya.

431
00:22:55,840 --> 00:22:59,332
Sulit untuk menemukan jawaban yang terasa masuk akal, mungkin teriakan, 

432
00:22:59,332 --> 00:23:02,097
tetapi jika kita melihat pola tetangga dalam distribusi, 

433
00:23:02,097 --> 00:23:06,366
yang dianggap sama mungkinnya, kita diberitahu bahwa hanya ada 8 kemungkinan kecocokan, 

434
00:23:06,366 --> 00:23:09,520
jadi seperempatnya banyak pertandingan, tapi kemungkinannya sama.

435
00:23:09,860 --> 00:23:12,140
Dan saat kami menghentikan pertandingan tersebut, kami dapat mengetahui alasannya.

436
00:23:12,500 --> 00:23:15,059
Beberapa di antaranya adalah jawaban yang benar-benar masuk akal, 

437
00:23:15,059 --> 00:23:16,300
seperti dering, murka, atau rap.

438
00:23:17,900 --> 00:23:20,386
Untuk mengilustrasikan bagaimana kami menggabungkan semua itu, 

439
00:23:20,386 --> 00:23:22,438
izinkan saya menampilkan Wordlebot versi 2 di sini, 

440
00:23:22,438 --> 00:23:25,280
dan ada dua atau tiga perbedaan utama dari yang pertama yang kami lihat.

441
00:23:25,860 --> 00:23:29,728
Pertama, seperti yang baru saja saya katakan, cara kita menghitung entropi ini, 

442
00:23:29,728 --> 00:23:33,645
nilai informasi yang diharapkan, kini menggunakan distribusi yang lebih halus di 

443
00:23:33,645 --> 00:23:37,708
seluruh pola yang menggabungkan probabilitas bahwa suatu kata tertentu akan menjadi 

444
00:23:37,708 --> 00:23:38,240
jawabannya.

445
00:23:38,879 --> 00:23:43,820
Ternyata, air mata tetaplah nomor 1, meski air mata berikut ini sedikit berbeda.

446
00:23:44,360 --> 00:23:46,577
Kedua, ketika ia memberi peringkat pada pilihan teratasnya, 

447
00:23:46,577 --> 00:23:49,202
sekarang ia akan menyimpan model probabilitas bahwa setiap kata adalah 

448
00:23:49,202 --> 00:23:51,753
jawaban sebenarnya, dan ia akan memasukkannya ke dalam keputusannya, 

449
00:23:51,753 --> 00:23:55,080
yang lebih mudah dilihat setelah kita mempunyai beberapa tebakan pada kata tersebut. meja.

450
00:23:55,860 --> 00:23:57,820
Sekali lagi, abaikan rekomendasinya karena kita 

451
00:23:57,820 --> 00:23:59,780
tidak bisa membiarkan mesin mengatur hidup kita.

452
00:24:01,140 --> 00:24:04,917
Dan saya kira saya harus menyebutkan hal lain yang berbeda di sebelah kiri, 

453
00:24:04,917 --> 00:24:09,142
bahwa nilai ketidakpastian, jumlah bit, tidak lagi mubazir dengan jumlah kemungkinan 

454
00:24:09,142 --> 00:24:09,640
kecocokan.

455
00:24:10,080 --> 00:24:15,571
Sekarang jika kita menariknya dan menghitung 2 sampai 8.02, yang sedikit di atas 256, 

456
00:24:15,571 --> 00:24:20,104
saya rasa 259, maksudnya adalah meskipun ada 526 kata yang benar-benar 

457
00:24:20,104 --> 00:24:24,957
cocok dengan pola ini, jumlah ketidakpastiannya lebih mirip dengan apa yang 

458
00:24:24,957 --> 00:24:28,980
akan terjadi jika ada 259 kata yang sama kemungkinannya. hasil.

459
00:24:29,720 --> 00:24:30,740
Anda bisa memikirkannya seperti ini.

460
00:24:31,020 --> 00:24:34,226
Ia mengetahui bahwa borx bukanlah jawabannya, sama halnya dengan yorts, zorl, 

461
00:24:34,226 --> 00:24:37,680
dan zorus, jadi ketidakpastiannya tidak terlalu besar dibandingkan kasus sebelumnya.

462
00:24:37,820 --> 00:24:39,280
Jumlah bit ini akan lebih kecil.

463
00:24:40,220 --> 00:24:43,235
Dan jika saya terus memainkan permainan ini, saya akan menyempurnakannya 

464
00:24:43,235 --> 00:24:46,540
dengan beberapa tebakan yang sesuai dengan apa yang ingin saya jelaskan di sini.

465
00:24:48,360 --> 00:24:50,891
Pada tebakan keempat, jika Anda melihat pilihan teratasnya, 

466
00:24:50,891 --> 00:24:53,760
Anda dapat melihat bahwa ini tidak lagi hanya memaksimalkan entropi.

467
00:24:54,460 --> 00:24:56,692
Jadi saat ini, secara teknis ada tujuh kemungkinan, 

468
00:24:56,692 --> 00:25:00,300
tapi satu-satunya peluang yang memiliki peluang berarti adalah asrama dan kata-kata.

469
00:25:00,300 --> 00:25:03,592
Dan Anda dapat melihat peringkatnya dengan memilih kedua nilai tersebut di atas 

470
00:25:03,592 --> 00:25:06,720
semua nilai lainnya, yang sebenarnya akan memberikan lebih banyak informasi.

471
00:25:07,240 --> 00:25:10,728
Pertama kali saya melakukan ini, saya hanya menjumlahkan kedua angka ini untuk mengukur 

472
00:25:10,728 --> 00:25:13,900
kualitas setiap tebakan, yang sebenarnya bekerja lebih baik dari yang Anda duga.

473
00:25:14,300 --> 00:25:16,801
Namun hal ini terasa tidak sistematis, dan saya yakin ada pendekatan 

474
00:25:16,801 --> 00:25:19,340
lain yang bisa diambil orang, namun inilah pendekatan yang saya pilih.

475
00:25:19,760 --> 00:25:22,219
Jika kita mempertimbangkan prospek tebakan berikutnya, 

476
00:25:22,219 --> 00:25:24,992
seperti dalam hal ini, yang benar-benar kita pedulikan adalah 

477
00:25:24,992 --> 00:25:27,900
skor yang diharapkan dari permainan kita jika kita melakukan itu.

478
00:25:28,230 --> 00:25:31,947
Dan untuk menghitung skor yang diharapkan, kami mengatakan berapa probabilitas 

479
00:25:31,947 --> 00:25:35,900
bahwa kata-kata tersebut adalah jawaban sebenarnya, yang saat ini menggambarkan 58%.

480
00:25:36,040 --> 00:25:39,540
Kami katakan dengan peluang 58%, skor kami dalam permainan ini adalah 4.

481
00:25:40,320 --> 00:25:45,640
Dan kemudian dengan probabilitas 1 dikurangi 58% itu, skor kita akan lebih dari 4 itu.

482
00:25:46,220 --> 00:25:49,375
Berapa banyak lagi yang belum kita ketahui, namun kita dapat memperkirakannya berdasarkan 

483
00:25:49,375 --> 00:25:52,460
seberapa besar ketidakpastian yang mungkin terjadi setelah kita mencapai titik tersebut.

484
00:25:52,960 --> 00:25:55,940
Secara khusus, saat ini ada 1.44 bit ketidakpastian.

485
00:25:56,440 --> 00:25:58,608
Jika kita menebak kata-kata, itu memberi tahu kita bahwa 

486
00:25:58,608 --> 00:26:01,120
informasi yang diharapkan yang akan kita dapatkan adalah 1.27 bit.

487
00:26:01,620 --> 00:26:04,705
Jadi jika kita menebak-nebak, perbedaan ini menunjukkan seberapa besar 

488
00:26:04,705 --> 00:26:07,660
ketidakpastian yang mungkin kita alami setelah hal tersebut terjadi.

489
00:26:08,260 --> 00:26:11,020
Yang kita butuhkan adalah suatu fungsi, yang saya sebut f di sini, 

490
00:26:11,020 --> 00:26:13,740
yang menghubungkan ketidakpastian ini dengan skor yang diharapkan.

491
00:26:14,240 --> 00:26:18,229
Dan caranya adalah dengan memplot sekumpulan data dari game sebelumnya 

492
00:26:18,229 --> 00:26:22,274
berdasarkan versi 1 bot untuk mengatakan berapa skor sebenarnya setelah 

493
00:26:22,274 --> 00:26:26,320
berbagai poin dengan jumlah ketidakpastian tertentu yang sangat terukur.

494
00:26:27,020 --> 00:26:31,056
Misalnya, titik data di sini yang berada di atas nilai sekitar 8.7 atau 

495
00:26:31,056 --> 00:26:35,036
lebih dikatakan untuk beberapa permainan setelah titik di mana ada 8.7 

496
00:26:35,036 --> 00:26:38,960
bit ketidakpastian, butuh dua tebakan untuk mendapatkan jawaban akhir.

497
00:26:39,320 --> 00:26:40,780
Untuk permainan lainnya membutuhkan tiga kali tebakan, 

498
00:26:40,780 --> 00:26:42,240
untuk permainan lainnya membutuhkan empat kali tebakan.

499
00:26:43,140 --> 00:26:46,667
Jika kita menggeser ke kiri di sini, semua titik di atas nol menyatakan 

500
00:26:46,667 --> 00:26:50,781
kapan pun tidak ada sedikit pun ketidakpastian, artinya hanya ada satu kemungkinan, 

501
00:26:50,781 --> 00:26:54,260
maka jumlah tebakan yang diperlukan selalu hanya satu, yang meyakinkan.

502
00:26:54,780 --> 00:26:59,124
Kapan pun ada sedikit ketidakpastian, artinya pada dasarnya hanya ada dua kemungkinan, 

503
00:26:59,124 --> 00:27:03,020
terkadang diperlukan satu tebakan lagi, terkadang diperlukan dua tebakan lagi.

504
00:27:03,080 --> 00:27:05,240
Dan seterusnya dan seterusnya di sini.

505
00:27:05,740 --> 00:27:07,893
Mungkin cara yang sedikit lebih mudah untuk memvisualisasikan 

506
00:27:07,893 --> 00:27:10,220
data ini adalah dengan menggabungkannya dan mengambil rata-ratanya.

507
00:27:11,000 --> 00:27:15,639
Misalnya bilah di sini menyatakan di antara semua titik di mana kita mempunyai sedikit 

508
00:27:15,639 --> 00:27:19,960
ketidakpastian, rata-rata jumlah tebakan baru yang diperlukan adalah sekitar 1.5.

509
00:27:22,140 --> 00:27:25,522
Dan batasan di sini mengatakan di antara semua permainan yang berbeda 

510
00:27:25,522 --> 00:27:29,049
di mana pada titik tertentu ketidakpastiannya sedikit di atas empat bit, 

511
00:27:29,049 --> 00:27:31,997
yang seperti mempersempitnya menjadi 16 kemungkinan berbeda, 

512
00:27:31,997 --> 00:27:35,380
maka rata-rata memerlukan lebih dari dua tebakan dari titik itu. maju.

513
00:27:36,060 --> 00:27:37,682
Dan dari sini saya baru saja melakukan regresi agar 

514
00:27:37,682 --> 00:27:39,460
sesuai dengan fungsi yang tampaknya masuk akal untuk ini.

515
00:27:39,980 --> 00:27:43,020
Dan ingat, inti dari melakukan semua itu adalah agar kita dapat 

516
00:27:43,020 --> 00:27:47,011
mengukur intuisi bahwa semakin banyak informasi yang kita peroleh dari sebuah kata, 

517
00:27:47,011 --> 00:27:48,960
semakin rendah pula skor yang diharapkan.

518
00:27:49,680 --> 00:27:54,460
Jadi dengan ini sebagai versi 2.0, jika kita kembali dan menjalankan rangkaian simulasi 

519
00:27:54,460 --> 00:27:59,240
yang sama, memainkannya melawan semua 2315 kemungkinan jawaban, bagaimana cara kerjanya?

520
00:28:00,280 --> 00:28:03,420
Berbeda dengan versi pertama kami, ini pasti lebih baik, dan itu meyakinkan.

521
00:28:04,020 --> 00:28:06,778
Semua dikatakan dan dilakukan rata-ratanya adalah sekitar 3.6, 

522
00:28:06,778 --> 00:28:09,492
meskipun tidak seperti versi pertama, ada beberapa kali versi 

523
00:28:09,492 --> 00:28:12,120
ini kalah dan membutuhkan lebih dari enam dalam keadaan ini.

524
00:28:12,639 --> 00:28:15,268
Mungkin karena ada kalanya kita melakukan pengorbanan untuk 

525
00:28:15,268 --> 00:28:17,940
benar-benar mencapai tujuan daripada memaksimalkan informasi.

526
00:28:19,040 --> 00:28:21,000
Jadi bisakah kita melakukan lebih baik dari 3.6?

527
00:28:22,080 --> 00:28:22,920
Kami pasti bisa.

528
00:28:23,280 --> 00:28:26,261
Sekarang saya katakan di awal bahwa paling menyenangkan adalah mencoba tidak 

529
00:28:26,261 --> 00:28:29,360
memasukkan daftar jawaban kata yang sebenarnya ke dalam cara membangun modelnya.

530
00:28:29,880 --> 00:28:32,188
Namun jika kami menggabungkannya, performa terbaik 

531
00:28:32,188 --> 00:28:34,180
yang bisa saya dapatkan adalah sekitar 3.43.

532
00:28:35,160 --> 00:28:37,740
Jadi jika kita mencoba menjadi lebih canggih dari sekedar menggunakan 

533
00:28:37,740 --> 00:28:39,841
data frekuensi kata untuk memilih distribusi sebelumnya, 

534
00:28:39,841 --> 00:28:42,532
ini 3.43 mungkin memberi gambaran maksimal seberapa bagus yang bisa kita 

535
00:28:42,532 --> 00:28:45,740
dapatkan dengan itu, atau setidaknya seberapa bagus yang bisa saya dapatkan dengan itu.

536
00:28:46,240 --> 00:28:49,252
Performa terbaik tersebut pada dasarnya hanya menggunakan ide-ide yang telah 

537
00:28:49,252 --> 00:28:51,090
saya bicarakan di sini, namun lebih jauh lagi, 

538
00:28:51,090 --> 00:28:54,142
seperti melakukan penelusuran informasi yang diharapkan dua langkah ke depan, 

539
00:28:54,142 --> 00:28:55,120
bukan hanya satu langkah.

540
00:28:55,620 --> 00:28:57,952
Awalnya saya berencana untuk membicarakan lebih banyak tentang hal itu, 

541
00:28:57,952 --> 00:29:00,220
tetapi saya menyadari bahwa kita sebenarnya sudah berjalan cukup lama.

542
00:29:00,580 --> 00:29:03,367
Satu hal yang akan saya katakan adalah setelah melakukan pencarian dua 

543
00:29:03,367 --> 00:29:06,704
langkah ini dan kemudian menjalankan beberapa contoh simulasi pada kandidat teratas, 

544
00:29:06,704 --> 00:29:09,100
sejauh ini bagi saya setidaknya Crane adalah pembuka terbaik.

545
00:29:09,100 --> 00:29:10,060
Siapa sangka?

546
00:29:10,920 --> 00:29:14,242
Juga jika Anda menggunakan daftar kata yang sebenarnya untuk menentukan ruang 

547
00:29:14,242 --> 00:29:17,820
kemungkinan Anda, maka ketidakpastian yang Anda mulai adalah sedikit di atas 11 bit.

548
00:29:18,300 --> 00:29:21,035
Dan ternyata, hanya dari pencarian brute force, 

549
00:29:21,035 --> 00:29:25,880
informasi maksimal yang diharapkan setelah dua tebakan pertama adalah sekitar 10 bit.

550
00:29:26,500 --> 00:29:30,556
Hal ini menunjukkan bahwa skenario terbaik, setelah dua tebakan pertama Anda, 

551
00:29:30,556 --> 00:29:34,560
dengan permainan optimal sempurna, Anda akan memiliki sedikit ketidakpastian.

552
00:29:34,800 --> 00:29:37,960
Yang sama dengan membuat dua kemungkinan tebakan.

553
00:29:37,960 --> 00:29:41,072
Jadi menurut saya adil dan mungkin cukup konservatif untuk mengatakan bahwa 

554
00:29:41,072 --> 00:29:44,554
Anda tidak akan pernah bisa menulis algoritma yang mendapatkan rata-rata serendah 3, 

555
00:29:44,554 --> 00:29:46,602
karena dengan kata-kata yang tersedia untuk Anda, 

556
00:29:46,602 --> 00:29:49,591
tidak ada ruang untuk mendapatkan informasi yang cukup setelah hanya dua 

557
00:29:49,591 --> 00:29:52,663
langkah yang harus dilakukan. mampu menjamin jawaban di slot ketiga setiap 

558
00:29:52,663 --> 00:29:53,360
saat tanpa gagal.

