1
00:00:00,000 --> 00:00:04,040
Гра Wurdle стала досить вірусною за останній місяць чи два,

2
00:00:04,040 --> 00:00:07,880
і я ніколи не пропускав можливість уроку математики, мені спадає

3
00:00:07,880 --> 00:00:12,120
на думку, що ця гра є дуже хорошим центральним прикладом

4
00:00:12,120 --> 00:00:13,120
уроку з теорії інформації, зокрема тема, відома як ентропія.

5
00:00:13,120 --> 00:00:17,120
Розумієте, як і багатьох людей, мене затягнуло головоломкою, і, як

6
00:00:17,120 --> 00:00:21,200
і багатьох програмістів, я також був затягнутий у спробі

7
00:00:21,200 --> 00:00:23,200
написати алгоритм, який би грав у гру якомога оптимальніше.

8
00:00:23,200 --> 00:00:26,400
І те, що я думав зробити тут, це просто поговорити з

9
00:00:26,400 --> 00:00:29,980
вами про мій процес і пояснити деякі математичні обчислення, які в

10
00:00:29,980 --> 00:00:32,080
нього входять, оскільки весь алгоритм зосереджений на цій ідеї ентропії.

11
00:00:32,080 --> 00:00:42,180
Перш за все, якщо ви не чули про це, що таке Wurdle?

12
00:00:42,180 --> 00:00:45,380
І щоб убити двох зайців одним пострілом, поки ми проходимо правила

13
00:00:45,380 --> 00:00:48,980
гри, дозвольте мені також переглянути, куди ми йдемо з цим,

14
00:00:48,980 --> 00:00:51,380
тобто розробити маленький алгоритм, який, в основному, гратиме за нас.

15
00:00:51,380 --> 00:00:54,860
Хоча я не робив сьогодні Wurdle, це 4

16
00:00:54,860 --> 00:00:55,860
лютого, і ми побачимо, як бот впорається.

17
00:00:55,860 --> 00:00:59,580
Мета Wurdle — вгадати таємниче слово з п’яти

18
00:00:59,580 --> 00:01:00,860
літер, і вам дається шість різних шансів вгадати.

19
00:01:00,860 --> 00:01:05,240
Наприклад, мій бот Wurdle пропонує мені почати з журавля.

20
00:01:05,240 --> 00:01:09,300
Кожного разу, коли ви припускаєте, ви отримуєте певну інформацію

21
00:01:09,300 --> 00:01:10,940
про те, наскільки близькі ваші припущення до істинної відповіді.

22
00:01:10,940 --> 00:01:14,540
Тут сірий квадрат говорить мені, що у фактичній відповіді немає C.

23
00:01:14,540 --> 00:01:18,340
Жовте поле говорить мені, що є R, але воно не в цьому положенні.

24
00:01:18,340 --> 00:01:21,820
Зелений квадрат говорить мені, що секретне слово дійсно має

25
00:01:21,820 --> 00:01:22,820
літеру А, і воно стоїть на третій позиції.

26
00:01:22,820 --> 00:01:24,300
І тоді немає ні N, ні E.

27
00:01:24,300 --> 00:01:27,420
Тож дозвольте мені просто зайти та повідомити боту Wurdle цю інформацію.

28
00:01:27,420 --> 00:01:31,500
Ми почали з крана, ми отримали сірий, жовтий, зелений, сірий, сірий.

29
00:01:31,500 --> 00:01:35,460
Не турбуйтеся про всі дані, які він зараз показує, я поясню це свого часу.

30
00:01:35,460 --> 00:01:39,700
Але його найкраща пропозиція для нашого другого вибору є химерною.

31
00:01:39,700 --> 00:01:43,500
І ваше припущення має бути справжнім словом із п’яти літер, але, як

32
00:01:43,500 --> 00:01:45,700
ви побачите, це досить ліберально щодо того, що насправді дозволить вам вгадати.

33
00:01:45,700 --> 00:01:48,860
У цьому випадку ми намагаємося shtick.

34
00:01:48,860 --> 00:01:50,260
І добре, все виглядає досить добре.

35
00:01:50,260 --> 00:01:54,580
Ми натиснули S і H, тому ми знаємо перші три літери, ми знаємо, що є R.

36
00:01:54,740 --> 00:01:59,740
Тож це буде як SHA щось R або SHA R щось.

37
00:01:59,740 --> 00:02:03,200
І, схоже, бот Wurdle знає, що він

38
00:02:03,200 --> 00:02:05,220
має лише дві можливості: shard або sharp.

39
00:02:05,220 --> 00:02:08,620
На даний момент це щось на кшталт суперечки між ними, тож я

40
00:02:08,620 --> 00:02:11,260
думаю, що, мабуть, лише тому, що він алфавітний, він поєднується з фрагментом.

41
00:02:11,260 --> 00:02:13,000
Ура, ось справжня відповідь.

42
00:02:13,000 --> 00:02:14,660
Тож ми отримали це за три.

43
00:02:14,660 --> 00:02:17,740
Якщо вам цікаво, чи це добре, я почув фразу однієї людини, що

44
00:02:17,740 --> 00:02:20,820
з Wurdle чотири — це рівномірно, а три — це пташка.

45
00:02:20,820 --> 00:02:22,960
Що, на мою думку, є досить влучною аналогією.

46
00:02:22,960 --> 00:02:27,560
Ви повинні бути постійно в своїй грі, щоб отримати чотири, але це точно не божевілля.

47
00:02:27,560 --> 00:02:30,000
Але коли ви отримуєте це за три, це просто чудово.

48
00:02:30,000 --> 00:02:33,800
Отже, якщо ви не за це, я хотів би просто поговорити про мій

49
00:02:33,800 --> 00:02:36,600
процес мислення з самого початку про те, як я підходжу до бота Wurdle.

50
00:02:36,600 --> 00:02:39,800
І, як я вже сказав, це дійсно привід для уроку теорії інформації.

51
00:02:39,800 --> 00:02:43,160
Основна мета – пояснити, що таке інформація, а що таке ентропія.

52
00:02:48,560 --> 00:02:52,080
Моєю першою думкою при підході до цього було поглянути

53
00:02:52,080 --> 00:02:53,560
на відносні частоти різних літер в англійській мові.

54
00:02:53,560 --> 00:02:57,800
Тож я подумав: гаразд, чи є початкове припущення чи

55
00:02:57,800 --> 00:02:59,960
початкова пара припущень, яка вражає багато цих найчастіших літер?

56
00:02:59,960 --> 00:03:03,780
І один, який я дуже любив, робив інший, а потім цвяхи.

57
00:03:03,780 --> 00:03:06,980
Думка полягає в тому, що якщо ви натискаєте букву, ви

58
00:03:06,980 --> 00:03:07,980
знаєте, ви отримуєте зелену або жовту, це завжди добре.

59
00:03:07,980 --> 00:03:09,460
Таке відчуття, що ви отримуєте інформацію.

60
00:03:09,460 --> 00:03:13,140
Але в цих випадках, навіть якщо ви не влучаєте і завжди

61
00:03:13,140 --> 00:03:16,640
отримуєте сірі, це все одно дає вам багато інформації, оскільки досить

62
00:03:16,640 --> 00:03:17,640
рідко можна знайти слово, у якому немає жодної з цих букв.

63
00:03:17,640 --> 00:03:21,840
Але все одно це не виглядає надсистематичним,

64
00:03:21,840 --> 00:03:23,520
тому що, наприклад, не враховується порядок літер.

65
00:03:23,520 --> 00:03:26,080
Навіщо друкувати цвяхи, коли я можу надрукувати равлика?

66
00:03:26,080 --> 00:03:27,720
Чи краще мати це S в кінці?

67
00:03:27,720 --> 00:03:28,720
Я не дуже впевнений.

68
00:03:28,720 --> 00:03:33,500
Тепер мій друг сказав, що йому подобається починати словом weary, що мене

69
00:03:33,500 --> 00:03:37,160
дещо здивувало, оскільки там є деякі незвичайні літери, як-от W та Y.

70
00:03:37,160 --> 00:03:39,400
Але хто знає, можливо, це кращий відкривач.

71
00:03:39,400 --> 00:03:43,920
Чи існує якась кількісна оцінка, яку ми

72
00:03:43,920 --> 00:03:44,920
можемо надати, щоб оцінити якість потенційного припущення?

73
00:03:44,920 --> 00:03:48,640
Тепер, щоб підготуватися до того, як ми будемо ранжувати можливі припущення, давайте

74
00:03:48,640 --> 00:03:51,800
повернемося назад і внесемо трохи ясності в те, як саме налаштована гра.

75
00:03:51,800 --> 00:03:55,880
Отже, є список слів, які можна ввести, які вважаються

76
00:03:55,880 --> 00:03:57,920
дійсними припущеннями, і складається лише з 13 000 слів.

77
00:03:57,920 --> 00:04:01,560
Але якщо ви подивитесь на це, ви побачите багато справді незвичайних речей, таких як голова

78
00:04:01,560 --> 00:04:07,040
або Алі та ARG, тип слів, які викликають сімейні суперечки в грі в Скрабл.

79
00:04:07,040 --> 00:04:10,600
Але настрій гри полягає в тому, що відповіддю завжди буде досить поширене слово.

80
00:04:10,600 --> 00:04:16,080
І насправді, є ще один список із приблизно 2300 слів, які є можливими відповідями.

81
00:04:16,080 --> 00:04:20,320
І це список, складений людьми, я думаю,

82
00:04:20,320 --> 00:04:21,800
саме дівчиною творця гри, що дуже весело.

83
00:04:21,800 --> 00:04:25,560
Але що я хотів би зробити, наше завдання для цього проекту полягає в тому, щоб побачити,

84
00:04:25,560 --> 00:04:30,720
чи зможемо ми написати програму, що розв’язує Wordle, яка не включатиме попередні знання про цей список.

85
00:04:30,720 --> 00:04:34,560
По-перше, є багато досить поширених слів із п’яти

86
00:04:34,560 --> 00:04:35,560
літер, яких ви не знайдете в цьому списку.

87
00:04:35,560 --> 00:04:38,360
Тож було б краще написати програму, яка була б трішки стійкішою та

88
00:04:38,360 --> 00:04:41,960
грала б у Wordle проти будь-кого, а не лише проти офіційного веб-сайту.

89
00:04:41,960 --> 00:04:45,900
А також причина, чому ми знаємо, що таке цей список можливих

90
00:04:45,900 --> 00:04:47,440
відповідей, полягає в тому, що він видимий у вихідному коді.

91
00:04:47,440 --> 00:04:51,620
Але те, як це видно у вихідному коді, — це

92
00:04:51,620 --> 00:04:52,840
певний порядок, у якому відповіді з’являються день у день.

93
00:04:52,840 --> 00:04:56,400
Тож ви завжди можете просто подивитися, якою буде завтрашня відповідь.

94
00:04:56,400 --> 00:04:59,140
Очевидно, що використання списку є обманом.

95
00:04:59,140 --> 00:05:02,900
І те, що робить головоломку цікавішою та насиченішим уроком теорії інформації, полягає в тому,

96
00:05:02,900 --> 00:05:07,640
що натомість можна використовувати деякі більш універсальні дані, такі як відносна частота слів у

97
00:05:07,640 --> 00:05:11,640
цілому, щоб вловити цю інтуїцію про те, що ми надаємо перевагу більш поширеним словам.

98
00:05:11,640 --> 00:05:16,560
Тож як із цих 13 000 можливостей вибрати початкове припущення?

99
00:05:16,560 --> 00:05:19,960
Наприклад, якщо мій друг пропонує втомленого, як ми повинні проаналізувати його якість?

100
00:05:19,960 --> 00:05:25,040
Що ж, причина, чому він сказав, що йому подобається це малоймовірне W, полягає в тому, що

101
00:05:25,040 --> 00:05:27,880
йому подобається довгострокова природа того, наскільки добре це відчуваєш, якщо ти влучив у це W.

102
00:05:27,880 --> 00:05:31,400
Наприклад, якщо перша виявлена закономірність була приблизно такою, то виявиться, що

103
00:05:31,400 --> 00:05:36,080
в цьому гігантському лексиконі лише 58 слів відповідають цій моделі.

104
00:05:36,080 --> 00:05:38,900
Тож це величезне зниження з 13 000.

105
00:05:38,900 --> 00:05:43,320
Але зворотна сторона цього, звичайно, полягає в тому, що дуже рідко отримати такий візерунок.

106
00:05:43,360 --> 00:05:47,600
Зокрема, якби кожне слово з однаковою ймовірністю було відповіддю, ймовірність досягнення

107
00:05:47,600 --> 00:05:51,680
цього шаблону дорівнювала б 58 поділеним приблизно на 13 000.

108
00:05:51,680 --> 00:05:53,880
Звичайно, вони не однаково ймовірні відповіді.

109
00:05:53,880 --> 00:05:56,680
Більшість із них дуже незрозумілі та навіть сумнівні слова.

110
00:05:56,680 --> 00:05:59,560
Але принаймні для нашого першого проходження всього цього, давайте припустимо, що

111
00:05:59,560 --> 00:06:02,040
всі вони однаково ймовірні, а потім уточнимо це трохи пізніше.

112
00:06:02,040 --> 00:06:07,360
Справа в тому, що шаблон із великою кількістю інформації за своєю природою малоймовірний.

113
00:06:07,360 --> 00:06:11,320
Насправді бути інформативним означає те, що це малоймовірно.

114
00:06:11,920 --> 00:06:16,720
Набагато більш вірогідною схемою для цього відкриття буде щось

115
00:06:16,720 --> 00:06:18,360
на кшталт цього, де, звичайно, немає букви W.

116
00:06:18,360 --> 00:06:22,080
Можливо, там є E, а можливо, немає A, немає R, немає Y.

117
00:06:22,080 --> 00:06:24,640
У цьому випадку є 1400 можливих збігів.

118
00:06:24,640 --> 00:06:29,600
Якби все було однаково вірогідним, виходить, що ймовірність приблизно

119
00:06:29,600 --> 00:06:30,680
11% того, що ви б побачили саме цю модель.

120
00:06:30,680 --> 00:06:34,320
Таким чином, найімовірніші результати також є найменш інформативними.

121
00:06:34,320 --> 00:06:38,440
Щоб отримати більш глобальне уявлення, дозвольте мені показати вам повний

122
00:06:38,440 --> 00:06:42,000
розподіл ймовірностей за всіма різними шаблонами, які ви можете побачити.

123
00:06:42,000 --> 00:06:46,000
Таким чином, кожна смужка, на яку ви дивитеся, відповідає можливому шаблону

124
00:06:46,000 --> 00:06:50,500
кольорів, який можна виявити, з яких є від 3 до 5

125
00:06:50,500 --> 00:06:52,960
варіантів, і вони організовані зліва направо, від найпоширенішого до найменш поширеного.

126
00:06:52,960 --> 00:06:56,200
Отже, найпоширенішою можливістю є те, що ви отримаєте всі сірі.

127
00:06:56,200 --> 00:06:58,800
Це відбувається приблизно в 14% випадків.

128
00:06:58,800 --> 00:07:02,040
І те, на що ви сподіваєтеся, коли ви припускаєте, це те, що ви

129
00:07:02,040 --> 00:07:06,360
опинитеся десь у цьому довгому хвості, як тут, де є лише 18

130
00:07:06,360 --> 00:07:09,920
можливостей для того, що відповідає цьому шаблону, який, очевидно, виглядає ось так.

131
00:07:09,920 --> 00:07:14,080
Або якщо ми підемо трохи ліворуч, то, можливо, ми підемо сюди.

132
00:07:14,080 --> 00:07:16,560
Добре, ось вам гарна головоломка.

133
00:07:16,560 --> 00:07:20,600
Які три слова в англійській мові починаються з літери

134
00:07:20,600 --> 00:07:22,040
W, закінчуються літерою Y і десь містять букву R?

135
00:07:22,040 --> 00:07:27,560
Виявляється, відповіді, давайте подивимося, багатослівні, червиві та іронічні.

136
00:07:27,560 --> 00:07:32,720
Отже, щоб оцінити, наскільки добре це слово в цілому, ми хочемо якийсь

137
00:07:32,720 --> 00:07:35,720
вимір очікуваної кількості інформації, яку ви збираєтеся отримати від цього розподілу.

138
00:07:36,360 --> 00:07:41,080
Якщо ми переглянемо кожен шаблон і помножимо його ймовірність появи на

139
00:07:41,080 --> 00:07:46,000
те, що вимірює його інформативність, це може дати нам об’єктивну оцінку.

140
00:07:46,000 --> 00:07:50,280
Тепер вашим першим інстинктом щодо того, що це має бути, може бути кількість збігів.

141
00:07:50,280 --> 00:07:52,960
Вам потрібна менша середня кількість збігів.

142
00:07:52,960 --> 00:07:57,400
Але натомість я хотів би використовувати більш універсальне вимірювання, яке ми часто приписуємо

143
00:07:57,400 --> 00:08:01,040
інформації, і таке, яке буде більш гнучким, коли ми матимемо різну ймовірність, призначену

144
00:08:01,040 --> 00:08:04,320
кожному з цих 13 000 слів щодо того, чи є вони справді відповіддю.

145
00:08:10,600 --> 00:08:14,760
Стандартною одиницею інформації є біт, який має трохи кумедну формулу, але

146
00:08:14,760 --> 00:08:17,800
вона справді інтуїтивно зрозуміла, якщо ми просто подивимося на приклади.

147
00:08:17,800 --> 00:08:21,880
Якщо у вас є спостереження, яке вдвічі скорочує ваш простір

148
00:08:21,880 --> 00:08:24,200
можливостей, ми кажемо, що воно містить один біт інформації.

149
00:08:24,200 --> 00:08:27,680
У нашому прикладі простір можливостей — це всі можливі слова, і виявляється, що

150
00:08:27,760 --> 00:08:31,560
близько половини слів із п’яти літер мають S, трохи менше, але приблизно половина.

151
00:08:31,560 --> 00:08:35,200
Таким чином, це спостереження дасть вам трохи інформації.

152
00:08:35,200 --> 00:08:39,640
Якщо натомість новий факт скорочує цей простір можливостей у чотири

153
00:08:39,640 --> 00:08:42,000
рази, ми говоримо, що він містить два біти інформації.

154
00:08:42,000 --> 00:08:45,120
Наприклад, виявилося, що приблизно чверть цих слів мають Т.

155
00:08:45,120 --> 00:08:49,720
Якщо спостереження скорочує цей простір у вісім, ми говоримо, що

156
00:08:49,720 --> 00:08:50,920
це три біти інформації, і так далі і так далі.

157
00:08:50,920 --> 00:08:55,000
Чотири біти перетворюють його на 16-й, п’ять бітів — на 32-й.

158
00:08:55,000 --> 00:09:00,160
Тож тепер ви можете зупинитись і запитати себе, яка формула

159
00:09:00,160 --> 00:09:04,520
для інформації для кількості бітів у термінах ймовірності появи?

160
00:09:04,520 --> 00:09:07,920
Ми маємо на увазі те, що коли ви берете одну половину на

161
00:09:07,920 --> 00:09:11,680
кількість бітів, це те саме, що ймовірність, що те саме, що сказати,

162
00:09:11,680 --> 00:09:16,200
що два в степені кількості бітів є одиницею над ймовірністю, що далі

163
00:09:16,200 --> 00:09:19,680
переставляє, кажучи, що інформація є двома логарифмами одиниці, поділеними на ймовірність.

164
00:09:19,680 --> 00:09:23,200
І іноді ви бачите це з ще одним перевпорядкуванням,

165
00:09:23,200 --> 00:09:25,680
де інформація є від’ємним логарифмом за основою два ймовірності.

166
00:09:25,680 --> 00:09:29,120
У такому вигляді це може здатися трохи дивним для

167
00:09:29,120 --> 00:09:33,400
непосвячених, але насправді це просто дуже інтуїтивна ідея

168
00:09:33,400 --> 00:09:35,120
запитати, скільки разів ви скоротили свої можливості вдвічі.

169
00:09:35,120 --> 00:09:37,840
А тепер, якщо вам цікаво, знаєте, я думав, що ми

170
00:09:37,840 --> 00:09:39,920
просто граємо у веселу гру слів, чому логарифми з’являються?

171
00:09:39,920 --> 00:09:43,920
Одна з причин, чому це краща одиниця, полягає в тому, що набагато простіше

172
00:09:43,920 --> 00:09:48,120
говорити про дуже малоймовірні події, набагато легше сказати, що спостереження містить 20

173
00:09:48,120 --> 00:09:53,480
біт інформації, ніж сказати, що ймовірність такого-то виникнення дорівнює 0. 0000095.

174
00:09:53,480 --> 00:09:57,360
Але більш суттєвою причиною того, що цей логарифмічний вираз виявився

175
00:09:57,360 --> 00:10:02,000
дуже корисним доповненням до теорії ймовірності, є спосіб додавання інформації.

176
00:10:02,000 --> 00:10:05,560
Наприклад, якщо одне спостереження дає вам два біти інформації, скорочуючи ваш

177
00:10:05,560 --> 00:10:10,120
простір учетверо, а потім друге спостереження, подібне до вашого другого припущення

178
00:10:10,120 --> 00:10:14,480
в Wordle, дає вам ще три біти інформації, скорочуючи вас ще

179
00:10:14,480 --> 00:10:17,360
на один коефіцієнт вісім, два разом дають п’ять біт інформації.

180
00:10:17,360 --> 00:10:21,200
Подібно до того, як ймовірності люблять множитися, інформація любить додаватися.

181
00:10:21,200 --> 00:10:24,920
Отже, як тільки ми потрапляємо в область чогось на зразок очікуваного значення,

182
00:10:24,920 --> 00:10:28,660
де ми додаємо купу чисел, журнали роблять це набагато зручнішим для роботи.

183
00:10:28,660 --> 00:10:32,600
Давайте повернемося до нашого розподілу для Weary і додамо сюди ще один

184
00:10:32,600 --> 00:10:35,560
маленький трекер, який показуватиме нам, скільки інформації є для кожного шаблону.

185
00:10:35,560 --> 00:10:38,760
Головне, на що я хочу, щоб ви звернули увагу, це те, що чим вища ймовірність, коли

186
00:10:38,760 --> 00:10:43,500
ми дійдемо до тих більш вірогідних моделей, тим менше інформації, тим менше бітів ви отримуєте.

187
00:10:43,500 --> 00:10:47,360
Спосіб вимірювання якості цього припущення полягає в тому, щоб взяти очікуване значення

188
00:10:47,360 --> 00:10:51,620
цієї інформації, де ми проходимо кожен шаблон, говоримо, наскільки це ймовірно, а

189
00:10:51,620 --> 00:10:54,940
потім ми множимо це на кількість біт інформації, яку ми отримуємо.

190
00:10:54,940 --> 00:10:58,480
А у прикладі Вірі це виявляється 4. 9 біт.

191
00:10:58,480 --> 00:11:02,800
Отже, у середньому інформація, яку ви отримуєте з цього початкового припущення, настільки

192
00:11:02,800 --> 00:11:05,660
ж хороша, як скорочення вашого простору можливостей навпіл приблизно в п’ять разів.

193
00:11:05,660 --> 00:11:10,260
Навпаки, прикладом припущення з вищим очікуваним інформаційним

194
00:11:10,260 --> 00:11:13,220
значенням може бути щось на зразок Slate.

195
00:11:13,220 --> 00:11:16,180
У цьому випадку ви помітите, що розподіл виглядає набагато більш плоским.

196
00:11:16,180 --> 00:11:20,780
Зокрема, найімовірніша поява всіх сірих має лише близько 6% ймовірності

197
00:11:20,780 --> 00:11:25,940
появи, тому ви отримуєте мінімум 3. 9 біт інформації.

198
00:11:25,940 --> 00:11:29,140
Але це мінімум, зазвичай ви отримаєте щось краще за це.

199
00:11:29,140 --> 00:11:33,380
І виявляється, коли ви обчислюєте цифри на цьому місці та

200
00:11:33,380 --> 00:11:36,420
додаєте всі відповідні терміни, середня інформація становить близько 5. 8.

201
00:11:36,420 --> 00:11:42,140
Отже, на відміну від Вірі, ваш простір можливостей буде

202
00:11:42,140 --> 00:11:43,940
в середньому приблизно вдвічі менший після цього першого припущення.

203
00:11:43,940 --> 00:11:49,540
Насправді є весела історія про назву цього очікуваного значення кількості інформації.

204
00:11:49,540 --> 00:11:52,580
Теорію інформації розробив Клод Шеннон, який працював у Bell Labs у 1940-х роках,

205
00:11:52,580 --> 00:11:57,620
але він говорив про деякі зі своїх ідей, які ще не були

206
00:11:57,620 --> 00:12:01,500
опубліковані, з Джоном фон Нейманом, який був цим інтелектуальним гігантом того часу,

207
00:12:01,500 --> 00:12:04,180
дуже видатним у математиці та фізиці та початках того, що ставало інформатикою.

208
00:12:04,180 --> 00:12:07,260
І коли він згадав, що він насправді не має вдалого імені

209
00:12:07,260 --> 00:12:12,540
для цього очікуваного значення кількості інформації, фон Нейман нібито сказав,

210
00:12:12,540 --> 00:12:14,720
отже, ви повинні назвати це ентропією, і з двох причин.

211
00:12:14,720 --> 00:12:18,400
По-перше, ваша функція невизначеності використовувалася в статистичній механіці під такою назвою, тож

212
00:12:18,400 --> 00:12:23,100
вона вже має назву, а по-друге, що ще важливіше, ніхто не знає,

213
00:12:23,100 --> 00:12:26,940
що таке ентропія насправді, тому в дебатах ви завжди будете мають перевагу.

214
00:12:26,940 --> 00:12:31,420
Отже, якщо назва здається трохи загадковою, і

215
00:12:31,420 --> 00:12:33,420
якщо вірити цій історії, це начебто задум.

216
00:12:33,420 --> 00:12:36,740
Крім того, якщо вам цікаво його відношення до всього другого закону

217
00:12:36,740 --> 00:12:40,820
термодинаміки з фізики, зв’язок точно є, але в його витоках

218
00:12:40,820 --> 00:12:44,780
Шеннон мав справу лише з чистою теорією ймовірностей, і для

219
00:12:44,780 --> 00:12:49,340
наших цілей тут, коли я використовую слово ентропія, я просто

220
00:12:49,340 --> 00:12:50,820
хочу, щоб ви подумали про очікувану інформаційну цінність конкретного припущення.

221
00:12:50,820 --> 00:12:54,380
Ви можете думати про ентропію як про вимірювання двох речей одночасно.

222
00:12:54,380 --> 00:12:57,420
Перше — це те, наскільки плоским є розподіл.

223
00:12:57,420 --> 00:13:01,700
Чим ближче рівномірний розподіл, тим вищою буде ентропія.

224
00:13:01,700 --> 00:13:06,340
У нашому випадку, коли існує від 3 до 5-го загальних шаблонів, для рівномірного розподілу, спостереження

225
00:13:06,340 --> 00:13:11,340
за будь-яким із них матиме інформаційну базу журналу 2 з 3 до 5-го, що дорівнює

226
00:13:11,340 --> 00:13:17,860
7. 92, тож це абсолютний максимум, який ви могли б отримати для цієї ентропії.

227
00:13:17,860 --> 00:13:21,900
Але ентропія також є своєрідним

228
00:13:21,900 --> 00:13:22,900
показником того, скільки можливостей існує.

229
00:13:22,900 --> 00:13:26,980
Наприклад, якщо у вас є якесь слово, де є лише 16 можливих шаблонів, і

230
00:13:26,980 --> 00:13:32,760
кожен з них однаково вірогідний, ця ентропія, ця очікувана інформація буде 4 біти.

231
00:13:32,760 --> 00:13:36,880
Але якщо у вас є інше слово, де є 64 можливі шаблони, які

232
00:13:36,880 --> 00:13:41,000
можуть виникнути, і всі вони однаково вірогідні, тоді ентропія буде складати 6 біт.

233
00:13:41,000 --> 00:13:45,800
Отже, якщо ви бачите якийсь розподіл у дикій природі, який має ентропію 6

234
00:13:45,800 --> 00:13:50,000
біт, це ніби говорить про те, що в тому, що має статися, існує

235
00:13:50,000 --> 00:13:54,400
стільки варіацій і невизначеності, як якщо б було 64 однаково ймовірні результати.

236
00:13:54,400 --> 00:13:58,360
Під час мого першого проходу в Wurtelebot я просто зробив це.

237
00:13:58,360 --> 00:14:03,560
Він переглядає всі можливі припущення, які ви можете мати, усі 13 000 слів,

238
00:14:03,560 --> 00:14:08,580
обчислює ентропію для кожного з них, або, точніше, ентропію розподілу за всіма

239
00:14:08,580 --> 00:14:13,040
шаблонами, які ви можете побачити, для кожного з них, і вибирає найвище,

240
00:14:13,040 --> 00:14:17,200
оскільки це той, який, швидше за все, максимально скоротить ваш простір можливостей.

241
00:14:17,200 --> 00:14:20,120
І незважаючи на те, що я говорив тут лише про

242
00:14:20,120 --> 00:14:21,680
перше припущення, воно робить те саме для кількох наступних припущень.

243
00:14:21,680 --> 00:14:25,100
Наприклад, після того, як ви бачите певний шаблон у цій першій здогадці, яка

244
00:14:25,100 --> 00:14:29,300
обмежила б вас меншою кількістю можливих слів на основі того, що з цим

245
00:14:29,300 --> 00:14:32,300
збігається, ви просто граєте в ту саму гру щодо цього меншого набору слів.

246
00:14:32,300 --> 00:14:36,500
Для запропонованого другого припущення ви дивитеся на розподіл усіх шаблонів, які

247
00:14:36,500 --> 00:14:41,540
можуть виникати з цього більш обмеженого набору слів, ви шукаєте всі

248
00:14:41,540 --> 00:14:45,480
13 000 можливостей і знаходите ту, яка максимізує цю ентропію.

249
00:14:45,480 --> 00:14:48,980
Щоб показати вам, як це працює в дії, дозвольте мені просто витягнути невеликий

250
00:14:48,980 --> 00:14:54,060
варіант Wurtele, який я написав, який показує основні моменти цього аналізу на полях.

251
00:14:54,460 --> 00:14:57,820
Після виконання всіх обчислень ентропії, тут праворуч він показує

252
00:14:57,820 --> 00:15:00,340
нам, які з них мають найбільшу очікувану інформацію.

253
00:15:00,340 --> 00:15:04,940
Виявляється, головною відповіддю, принаймні на даний момент, ми уточнимо це

254
00:15:04,940 --> 00:15:11,140
пізніше, є Тарес, що означає, гм, звичайно, вика, найпоширеніша вика.

255
00:15:11,140 --> 00:15:14,180
Кожного разу, коли ми робимо припущення тут, де, можливо, я ігнорую його

256
00:15:14,180 --> 00:15:19,220
рекомендації та вибираю шифер, тому що мені подобається шифер, ми можемо

257
00:15:19,220 --> 00:15:23,300
побачити, скільки очікуваної інформації він містить, але праворуч від слова тут

258
00:15:23,340 --> 00:15:24,980
показано, скільки фактичну інформацію, яку ми отримали, враховуючи цю конкретну модель.

259
00:15:24,980 --> 00:15:28,660
Тож тут, схоже, нам трохи не пощастило, очікували, що ми отримаємо 5. 8, але

260
00:15:28,660 --> 00:15:30,660
випадково ми отримали щось менше, ніж це.

261
00:15:30,660 --> 00:15:34,020
А потім ліворуч тут показано всі різні

262
00:15:34,020 --> 00:15:35,860
можливі слова, де ми зараз знаходимося.

263
00:15:35,860 --> 00:15:39,820
Сині смужки вказують на те, наскільки вірогідним є кожне слово, тому наразі припускається,

264
00:15:39,820 --> 00:15:44,140
що кожне слово буде однаково ймовірно, але ми уточнимо це за мить.

265
00:15:44,140 --> 00:15:48,580
І тоді це вимірювання невизначеності говорить нам про ентропію цього

266
00:15:48,580 --> 00:15:53,220
розподілу між можливими словами, що зараз, оскільки це рівномірний

267
00:15:53,300 --> 00:15:55,940
розподіл, є просто непотрібно складним способом підрахувати кількість можливостей.

268
00:15:55,940 --> 00:16:01,700
Наприклад, якби ми взяли 2 у ступінь 13. 66, це має

269
00:16:01,700 --> 00:16:02,700
бути приблизно 13 000 можливостей.

270
00:16:02,700 --> 00:16:06,780
Я трохи відхилився, але лише тому, що не показую всі десяткові знаки.

271
00:16:06,780 --> 00:16:10,260
На даний момент це може здатися зайвим і занадто складним,

272
00:16:10,260 --> 00:16:12,780
але за хвилину ви зрозумієте, чому корисно мати обидва номери.

273
00:16:12,780 --> 00:16:16,780
Отже, тут виглядає так, ніби найвища ентропія для нашого другого припущення

274
00:16:16,780 --> 00:16:19,700
– Рамен, що знову ж таки не схоже на слово.

275
00:16:19,700 --> 00:16:25,660
Отже, щоб підвищити моральну позицію, я введу Rains.

276
00:16:25,660 --> 00:16:27,540
І знову схоже, що нам трохи не пощастило.

277
00:16:27,540 --> 00:16:32,100
Ми чекали 4. 3 біти, а ми отримали лише 3. 39 біт інформації.

278
00:16:32,100 --> 00:16:35,060
Отже, ми маємо 55 можливостей.

279
00:16:35,060 --> 00:16:38,860
І тут, можливо, я просто прийму те, що він

280
00:16:38,860 --> 00:16:40,200
пропонує, тобто комбо, що б це не означало.

281
00:16:40,200 --> 00:16:43,300
І гаразд, це насправді хороший шанс для головоломки.

282
00:16:43,300 --> 00:16:47,020
Це говорить нам, що цей шаблон дає нам 4. 7 біт інформації.

283
00:16:47,020 --> 00:16:52,400
Але ліворуч, перш ніж ми побачимо цей шаблон, їх було 5. 78 біт невизначеності.

284
00:16:52,400 --> 00:16:56,860
Отже, як вікторина для вас, що це означає щодо кількості можливостей, що залишилися?

285
00:16:56,860 --> 00:17:02,280
Ну, це означає, що ми зведені до однієї частки невизначеності,

286
00:17:02,280 --> 00:17:04,700
що те саме, що сказати, що є дві можливі відповіді.

287
00:17:04,700 --> 00:17:06,520
Це вибір 50 на 50.

288
00:17:06,520 --> 00:17:09,860
І звідси, оскільки ми з вами знаємо, які слова є

289
00:17:09,860 --> 00:17:11,220
більш поширеними, ми знаємо, що відповідь має бути безодня.

290
00:17:11,220 --> 00:17:13,540
Але, як зараз написано, програма цього не знає.

291
00:17:13,540 --> 00:17:17,560
Тож він просто продовжує, намагаючись отримати якомога більше інформації, доки

292
00:17:17,560 --> 00:17:20,360
не залишиться лише одна можливість, а потім здогадується про це.

293
00:17:20,360 --> 00:17:22,700
Отже, очевидно, нам потрібна краща стратегія завершення гри.

294
00:17:22,700 --> 00:17:26,540
Але, скажімо, ми назвемо цю версію одним із наших розв’язувачів Wordle,

295
00:17:26,540 --> 00:17:30,740
а потім запустимо кілька симуляцій, щоб побачити, як це працює.

296
00:17:30,740 --> 00:17:34,240
Отже, як це працює, це гра в усі можливі ігри зі словами.

297
00:17:34,240 --> 00:17:38,780
Він переглядає всі ці 2315 слів, які є фактичними відповідями Wordle.

298
00:17:38,780 --> 00:17:41,340
В основному це використовується як набір для тестування.

299
00:17:41,340 --> 00:17:45,820
І з цим наївним методом не брати до уваги, наскільки поширене слово, і просто намагатися максимізувати

300
00:17:45,820 --> 00:17:50,480
інформацію на кожному кроці на цьому шляху, доки не дійде до одного й лише одного вибору.

301
00:17:50,480 --> 00:17:55,100
Наприкінці симуляції середній бал виходить близько 4. 124.

302
00:17:55,100 --> 00:17:59,780
Що непогано, чесно кажучи, я очікував, що буде гірше.

303
00:17:59,780 --> 00:18:03,040
Але люди, які грають у wordle, скажуть вам, що вони зазвичай можуть отримати його за 4.

304
00:18:03,040 --> 00:18:05,260
Справжнє завдання — отримати якомога більше за 3.

305
00:18:05,260 --> 00:18:08,920
Це досить великий стрибок між рахунком 4 і рахунком 3.

306
00:18:08,920 --> 00:18:13,300
Очевидний низький плід тут полягає в тому, щоб якимось чином врахувати,

307
00:18:13,300 --> 00:18:23,160
чи є слово загальним, і як саме ми це робимо.

308
00:18:23,160 --> 00:18:26,860
Я підійшов до цього, щоб отримати список відносних

309
00:18:26,860 --> 00:18:28,560
частот для всіх слів в англійській мові.

310
00:18:28,560 --> 00:18:32,560
І я щойно скористався функцією даних частоти слів Mathematica, яка

311
00:18:32,560 --> 00:18:35,520
сама одержує загальнодоступний набір даних Google Books English Ngram.

312
00:18:35,520 --> 00:18:38,680
І на це цікаво дивитися, наприклад, якщо ми відсортуємо

313
00:18:38,680 --> 00:18:40,120
його від найбільш поширених слів до найменш поширених слів.

314
00:18:40,120 --> 00:18:43,740
Очевидно, це найпоширеніші слова з 5 букв в англійській мові.

315
00:18:43,740 --> 00:18:46,480
А точніше, це 8 місце за поширеністю.

316
00:18:46,480 --> 00:18:49,440
Спочатку який, потім там і там.

317
00:18:49,440 --> 00:18:53,020
Перший сам по собі не перший, а 9-й, і цілком зрозуміло,

318
00:18:53,020 --> 00:18:57,840
що ці інші слова можуть зустрічатися частіше, де ті, що стоять

319
00:18:57,840 --> 00:18:59,000
після першого, є після, де, а ті, які є трохи рідше.

320
00:18:59,000 --> 00:19:04,400
Тепер, використовуючи ці дані для моделювання того, наскільки ймовірно кожне з цих

321
00:19:04,400 --> 00:19:06,760
слів буде остаточною відповіддю, це не повинно бути просто пропорційним частоті.

322
00:19:07,020 --> 00:19:12,560
Наприклад, який отримав оцінку 0. 002 у цьому наборі даних, тоді як

323
00:19:12,560 --> 00:19:15,200
слово braid у певному сенсі приблизно в 1000 разів менш імовірне.

324
00:19:15,200 --> 00:19:19,400
Але обидва ці слова досить поширені, тому їх майже напевно варто розглянути.

325
00:19:19,400 --> 00:19:21,900
Отже, ми хочемо більше двійкового відсікання.

326
00:19:21,900 --> 00:19:26,520
Я пішов з цього приводу: уявити весь цей відсортований список слів, а потім

327
00:19:26,520 --> 00:19:31,060
розташувати його на осі х, а потім застосувати функцію sigmoid, яка є

328
00:19:31,060 --> 00:19:35,540
стандартним способом отримання функції, вихід якої в основному є двійковим, це або

329
00:19:35,540 --> 00:19:38,500
0, або 1, але між ними є згладжування для цієї області невизначеності.

330
00:19:38,500 --> 00:19:43,900
Таким чином, по суті, ймовірність того, що я призначаю кожному слову для того, щоб потрапити в

331
00:19:43,900 --> 00:19:49,540
остаточний список, буде значенням сигмоїдної функції вище, де б воно не знаходилося на осі х.

332
00:19:49,540 --> 00:19:53,940
Тепер, очевидно, це залежить від кількох параметрів, наприклад, наскільки широкий простір на осі

333
00:19:53,940 --> 00:19:59,660
х заповнюють ці слова, визначає, наскільки поступово або круто ми знижуємося від 1

334
00:19:59,660 --> 00:20:03,000
до 0, і те, де ми їх розташовуємо зліва направо, визначає межу.

335
00:20:03,160 --> 00:20:07,340
Чесно кажучи, те, як я це зробив, було просто облизати палець і тицьнути його на вітер.

336
00:20:07,340 --> 00:20:10,800
Я переглянув відсортований список і спробував знайти вікно, у якому, дивлячись

337
00:20:10,800 --> 00:20:15,280
на нього, я вирішив, що приблизно половина цих слів, швидше

338
00:20:15,280 --> 00:20:17,680
за все, є остаточною відповіддю, і використав це як межу.

339
00:20:17,680 --> 00:20:21,840
Коли ми маємо подібний розподіл між словами, це дає нам

340
00:20:21,840 --> 00:20:24,460
іншу ситуацію, коли ентропія стає цим дійсно корисним вимірюванням.

341
00:20:24,460 --> 00:20:28,480
Наприклад, скажімо, ми граємо в гру, і ми починаємо з

342
00:20:28,480 --> 00:20:32,480
моїх старих відкривачів, які були пером і цвяхами, і закінчуємо

343
00:20:32,480 --> 00:20:33,760
ситуацією, коли є чотири можливі слова, які відповідають цьому.

344
00:20:33,760 --> 00:20:36,440
І припустимо, ми вважаємо їх усіх однаково ймовірними.

345
00:20:36,440 --> 00:20:40,000
Дозвольте запитати вас, яка ентропія цього розподілу?

346
00:20:40,000 --> 00:20:45,920
Що ж, інформація, пов’язана з кожною з цих можливостей, буде базою журналу 2

347
00:20:45,920 --> 00:20:50,800
з 4, оскільки кожна з них є 1 і 4, і це 2.

348
00:20:50,800 --> 00:20:52,780
Два біти інформації, чотири можливості.

349
00:20:52,780 --> 00:20:54,360
Все дуже добре і добре.

350
00:20:54,360 --> 00:20:58,320
Але що, якби я сказав вам, що насправді є більше чотирьох збігів?

351
00:20:58,320 --> 00:21:02,600
Насправді, коли ми переглядаємо повний список слів, ми знаходимо 16 слів, які йому відповідають.

352
00:21:02,600 --> 00:21:07,260
Але припустімо, що наша модель надає справді низьку ймовірність того, що ці інші 12

353
00:21:07,260 --> 00:21:11,440
слів є остаточною відповіддю, приблизно 1 з 1000, тому що вони дійсно незрозумілі.

354
00:21:11,440 --> 00:21:15,480
Тепер дозвольте запитати вас, яка ентропія цього розподілу?

355
00:21:15,480 --> 00:21:19,600
Якби ентропія вимірювала лише кількість збігів, тоді можна було б очікувати, що

356
00:21:19,600 --> 00:21:24,760
це буде щось на кшталт логарифмічної бази 2 із 16, що

357
00:21:24,760 --> 00:21:26,200
дорівнюватиме 4, на два біти невизначеності більше, ніж ми мали раніше.

358
00:21:26,200 --> 00:21:30,320
Але, звісно, фактична невизначеність не дуже відрізняється від того, що ми мали раніше.

359
00:21:30,320 --> 00:21:33,840
Те, що є ці 12 справді незрозумілих слів, не означає, що було

360
00:21:33,840 --> 00:21:38,200
б ще більш дивно дізнатися, що остаточною відповіддю є, наприклад, чарівність.

361
00:21:38,200 --> 00:21:42,080
Отже, коли ви фактично виконуєте обчислення тут і додаєте ймовірність кожного

362
00:21:42,080 --> 00:21:45,960
випадку, помножену на відповідну інформацію, ви отримуєте 2. 11 біт.

363
00:21:45,960 --> 00:21:50,280
Я просто кажу, що в основному це два біти, в основному ці чотири

364
00:21:50,280 --> 00:21:54,240
можливості, але є трохи більше невизначеності через усі ці малоймовірні події, хоча

365
00:21:54,240 --> 00:21:57,120
якби ви дізналися про них, ви б отримали з цього масу інформації.

366
00:21:57,120 --> 00:22:00,800
Отже, зменшуючи масштаб, це частина того, що робить

367
00:22:00,800 --> 00:22:01,800
Wordle таким гарним прикладом для уроку теорії інформації.

368
00:22:01,800 --> 00:22:05,280
Ми маємо ці два різні застосування ентропії.

369
00:22:05,280 --> 00:22:09,640
Перше говорить нам, яку інформацію ми очікуємо отримати

370
00:22:09,640 --> 00:22:14,560
від певного припущення, а друге говорить, чи можемо

371
00:22:14,560 --> 00:22:16,480
ми виміряти залишкову невизначеність серед усіх можливих слів.

372
00:22:16,480 --> 00:22:19,800
І я маю підкреслити, що в першому випадку, коли ми розглядаємо очікувану інформацію про

373
00:22:19,800 --> 00:22:25,000
припущення, як тільки ми маємо нерівну вагу слів, це впливає на обчислення ентропії.

374
00:22:25,000 --> 00:22:28,600
Наприклад, дозвольте мені знайти той самий випадок, який ми

375
00:22:28,600 --> 00:22:33,560
розглядали раніше, розподілу, пов’язаного з Weary, але цього разу

376
00:22:33,560 --> 00:22:34,560
з використанням нерівномірного розподілу між усіма можливими словами.

377
00:22:34,560 --> 00:22:39,360
Тож дозвольте мені поглянути, чи зможу я знайти тут частину, яка б це добре ілюструвала.

378
00:22:39,360 --> 00:22:42,480
Гаразд, ось це досить добре.

379
00:22:42,480 --> 00:22:46,360
Тут ми маємо два суміжних шаблони, які приблизно однаково вірогідні, але один

380
00:22:46,360 --> 00:22:49,480
із них, як нам сказали, містить 32 можливі слова, які йому відповідають.

381
00:22:49,480 --> 00:22:54,080
І якщо ми перевіримо, що це таке, це ті 32,

382
00:22:54,080 --> 00:22:55,600
які є дуже малоймовірними словами, якщо ви переглядаєте їх очима.

383
00:22:55,600 --> 00:23:00,400
Важко знайти будь-яку правдоподібну відповідь, можливо, крики, але якщо ми подивимося

384
00:23:00,400 --> 00:23:04,440
на сусідній шаблон у розподілі, який вважається приблизно таким же

385
00:23:04,440 --> 00:23:08,920
вірогідним, нам скажуть, що він має лише 8 можливих збігів, тобто

386
00:23:08,920 --> 00:23:09,920
чверть як багато збігів, але це приблизно так само ймовірно.

387
00:23:09,920 --> 00:23:12,520
І коли ми знайдемо ці сірники, ми зрозуміємо чому.

388
00:23:12,520 --> 00:23:17,840
Деякі з них є реальними правдоподібними відповідями, наприклад, кільце, гнів або стукіт.

389
00:23:17,840 --> 00:23:22,000
Щоб проілюструвати, як ми все це об’єднуємо, дозвольте мені витягнути тут версію 2 Wordlebot,

390
00:23:22,000 --> 00:23:25,960
і там є дві або три основні відмінності від першої, яку ми бачили.

391
00:23:25,960 --> 00:23:29,460
По-перше, як я щойно сказав, спосіб, у який ми обчислюємо ці

392
00:23:29,460 --> 00:23:34,800
ентропії, ці очікувані значення інформації, тепер використовує точніший розподіл між шаблонами,

393
00:23:34,800 --> 00:23:39,300
який включає ймовірність того, що дане слово насправді буде відповіддю.

394
00:23:39,300 --> 00:23:44,160
Як це сталося, сльози все ще залишаються номером 1, хоча наступні трохи інші.

395
00:23:44,160 --> 00:23:47,920
По-друге, коли він ранжує свої найпопулярніші варіанти, він тепер зберігатиме модель ймовірності

396
00:23:47,920 --> 00:23:52,600
того, що кожне слово є фактичною відповіддю, і він включатиме це у

397
00:23:52,600 --> 00:23:55,520
своє рішення, яке легше побачити, коли ми матимемо кілька припущень щодо стіл.

398
00:23:55,520 --> 00:24:01,120
Знову ж таки, ігноруємо його рекомендацію, тому що ми не можемо дозволити машинам керувати нашим життям.

399
00:24:01,120 --> 00:24:05,160
І я вважаю, що я повинен згадати ще одну іншу річ, яка закінчилася ліворуч,

400
00:24:05,160 --> 00:24:10,080
що значення невизначеності, ця кількість бітів, більше не просто надлишкова з кількістю можливих збігів.

401
00:24:10,080 --> 00:24:16,520
Тепер, якщо ми витягнемо це вгору і порахуємо 2 до 8. 02, що трохи вище 256, я

402
00:24:16,520 --> 00:24:22,640
думаю, 259, це говорить про те, що, незважаючи на те, що

403
00:24:22,640 --> 00:24:26,400
всього 526 слів відповідають цьому шаблону, рівень невизначеності більше схожий на

404
00:24:26,400 --> 00:24:29,760
той, який був би, якби було 259 однаково ймовірних результати.

405
00:24:29,760 --> 00:24:31,100
Ви можете думати про це так.

406
00:24:31,100 --> 00:24:35,560
Він знає, що borx не є відповіддю, те саме з yorts, zorl

407
00:24:35,560 --> 00:24:37,840
і zorus, тому це трохи менш невизначено, ніж у попередньому випадку.

408
00:24:37,840 --> 00:24:40,220
Ця кількість бітів буде меншою.

409
00:24:40,220 --> 00:24:44,040
І якщо я продовжу грати в гру, я уточню це парою

410
00:24:44,040 --> 00:24:48,680
припущень, які стосуються того, що я хотів би пояснити тут.

411
00:24:48,680 --> 00:24:52,520
За четвертим припущенням, якщо ви подивитеся на його найкращі варіанти,

412
00:24:52,520 --> 00:24:53,800
ви побачите, що це вже не просто максимізація ентропії.

413
00:24:53,800 --> 00:24:58,480
Отже, на даний момент технічно є сім можливостей, але

414
00:24:58,480 --> 00:25:00,780
єдині, які мають значний шанс, це гуртожиток і слова.

415
00:25:00,780 --> 00:25:04,760
І ви можете бачити, що він класифікує обидва вище за

416
00:25:04,760 --> 00:25:07,560
всі ці інші значення, що, строго кажучи, дасть більше інформації.

417
00:25:07,560 --> 00:25:11,200
У перший раз, коли я зробив це, я просто склав ці два числа,

418
00:25:11,200 --> 00:25:14,580
щоб виміряти якість кожного припущення, яке насправді спрацювало краще, ніж ви могли підозрювати.

419
00:25:14,580 --> 00:25:17,600
Але це справді не здавалося систематичним, і я впевнений, що є інші підходи,

420
00:25:17,600 --> 00:25:19,880
які люди могли б використати, але ось той, на який я зупинився.

421
00:25:19,880 --> 00:25:24,200
Якщо ми розглядаємо перспективу наступного припущення, як у цьому випадку слова, те, що

422
00:25:24,200 --> 00:25:28,440
нас справді хвилює, це очікуваний рахунок нашої гри, якщо ми це зробимо.

423
00:25:28,440 --> 00:25:32,880
І щоб обчислити цей очікуваний бал, ми кажемо, яка ймовірність того,

424
00:25:32,880 --> 00:25:35,640
що слова є фактичною відповіддю, яка на даний момент відповідає 58%.

425
00:25:36,080 --> 00:25:40,400
Ми кажемо, що з імовірністю 58% наш рахунок у цій грі буде 4.

426
00:25:40,400 --> 00:25:46,240
І тоді з імовірністю 1 мінус ці 58% наша оцінка буде більшою за ці 4.

427
00:25:46,240 --> 00:25:50,640
Скільки ще ми не знаємо, але ми можемо оцінити це на

428
00:25:50,640 --> 00:25:52,920
основі того, скільки невизначеності буде, коли ми дійдемо до цієї точки.

429
00:25:52,920 --> 00:25:56,600
Зокрема, на даний момент є 1. 44 біти невизначеності.

430
00:25:56,600 --> 00:26:01,560
Якщо ми вгадуємо слова, це означає, що очікувана інформація, яку ми отримаємо, дорівнює 1. 27 біт.

431
00:26:01,560 --> 00:26:06,280
Отже, якщо ми вгадуємо слова, ця різниця показує, скільки

432
00:26:06,280 --> 00:26:08,280
невизначеності ми, ймовірно, залишимо після того, як це станеться.

433
00:26:08,280 --> 00:26:12,500
Нам потрібна якась функція, яку я тут називаю

434
00:26:12,500 --> 00:26:13,880
f, яка пов’язує цю невизначеність із очікуваною оцінкою.

435
00:26:13,880 --> 00:26:18,040
І те, як це було зроблено, полягало в тому, що просто побудували групу

436
00:26:18,040 --> 00:26:23,920
даних із попередніх ігор на основі версії 1 бота, щоб сказати, яким

437
00:26:23,920 --> 00:26:27,040
був фактичний рахунок після різних моментів з певною дуже вимірною кількістю невизначеності.

438
00:26:27,040 --> 00:26:31,120
Наприклад, ці точки даних тут знаходяться вище значення приблизно 8. 7

439
00:26:31,120 --> 00:26:36,840
або близько того кажуть для деяких ігор після моменту, коли було 8. 7 біт

440
00:26:36,840 --> 00:26:39,340
невизначеності, знадобилося дві здогадки, щоб отримати остаточну відповідь.

441
00:26:39,340 --> 00:26:43,180
Для інших ігор потрібно було три відгадки, для інших ігор – чотири.

442
00:26:43,180 --> 00:26:46,920
Якщо ми перемістимося вліво тут, усі точки над нулем говорять про

443
00:26:46,920 --> 00:26:51,620
те, що будь-коли є нуль біт невизначеності, тобто є лише одна

444
00:26:51,620 --> 00:26:55,000
можливість, тоді необхідна кількість припущень завжди дорівнює лише одній, що заспокоює.

445
00:26:55,000 --> 00:26:59,020
Щоразу, коли була трішка невизначеності, тобто, по суті,

446
00:26:59,020 --> 00:27:02,360
зводилася лише до двох можливостей, іноді вимагалося

447
00:27:02,360 --> 00:27:03,940
ще одне припущення, іноді ще два припущення.

448
00:27:03,940 --> 00:27:05,980
І так далі і так далі тут.

449
00:27:05,980 --> 00:27:11,020
Можливо, дещо простіший спосіб візуалізувати ці дані — об’єднати їх разом і взяти середні значення.

450
00:27:11,020 --> 00:27:15,940
Наприклад, ця смужка тут говорить, що серед усіх пунктів, де ми мали хоча

451
00:27:15,940 --> 00:27:22,420
б трохи невизначеності, в середньому необхідна кількість нових припущень була приблизно 1. 5.

452
00:27:22,420 --> 00:27:25,920
І смужка тут говорить, що серед усіх різних ігор, де в

453
00:27:25,920 --> 00:27:30,480
якийсь момент невизначеність була трохи вище чотирьох бітів, що схоже

454
00:27:30,480 --> 00:27:35,120
на звуження до 16 різних можливостей, то в середньому для

455
00:27:35,120 --> 00:27:36,240
цього потрібно трохи більше двох припущень з цієї точки вперед.

456
00:27:36,240 --> 00:27:40,080
І звідси я просто зробив регресію, щоб відповідати функції, яка здавалася розумною для цього.

457
00:27:40,080 --> 00:27:44,160
І пам’ятайте, що весь сенс будь-чого з цього полягає в тому, щоб ми могли кількісно

458
00:27:44,160 --> 00:27:49,720
оцінити цю інтуїцію: що більше інформації ми отримуємо зі слова, то нижчим буде очікуваний бал.

459
00:27:49,720 --> 00:27:54,380
Отже, це як версія 2. 0, якщо ми повернемося назад і запустимо той самий набір

460
00:27:54,380 --> 00:27:59,820
симуляцій, зіставивши його з усіма 2315 можливими відповідями Wordle, як це вийде?

461
00:27:59,820 --> 00:28:04,060
Що ж, на відміну від нашої першої версії, вона точно краща, що заспокоює.

462
00:28:04,060 --> 00:28:08,780
Усе сказане та зроблене середній бал становить близько 3. 6, хоча, на відміну від першої версії,

463
00:28:08,780 --> 00:28:12,820
вона кілька разів програє, і за цієї обставини вимагає більше шести.

464
00:28:12,820 --> 00:28:15,980
Мабуть тому, що бувають моменти, коли потрібно

465
00:28:15,980 --> 00:28:18,980
досягти мети, а не максимізувати інформацію.

466
00:28:18,980 --> 00:28:22,140
Отже, ми можемо зробити краще, ніж 3. 6?

467
00:28:22,140 --> 00:28:23,260
Ми точно можемо.

468
00:28:23,260 --> 00:28:27,120
На початку я сказав, що найцікавіше спробувати не включати

469
00:28:27,120 --> 00:28:29,980
справжній список відповідей Wordle у спосіб побудови своєї моделі.

470
00:28:29,980 --> 00:28:35,180
Але якщо ми все-таки це включимо, найкраща продуктивність, яку я міг отримати, була приблизно 3. 43.

471
00:28:35,180 --> 00:28:39,520
Отже, якщо ми спробуємо вийти більш складним, ніж просто використовувати дані про частоту слів, щоб вибрати цей

472
00:28:39,520 --> 00:28:44,220
попередній розподіл, це 3. 43, ймовірно, дає максимум того, наскільки добре ми можемо

473
00:28:44,220 --> 00:28:46,360
отримати з цим, або принаймні, наскільки добре я можу отримати з цим.

474
00:28:46,360 --> 00:28:50,240
Ця найкраща продуктивність, по суті, просто використовує ідеї, про які

475
00:28:50,240 --> 00:28:53,400
я тут говорив, але йде трохи далі, ніби шукає очікувану

476
00:28:53,400 --> 00:28:55,660
інформацію на два кроки вперед, а не лише на один.

477
00:28:55,660 --> 00:28:58,720
Спочатку я планував більше поговорити про це, але

478
00:28:58,720 --> 00:29:00,580
я розумію, що насправді ми пройшли досить довго.

479
00:29:00,580 --> 00:29:03,520
Єдине, що я скажу: після цього двоетапного пошуку, а

480
00:29:03,520 --> 00:29:07,720
потім запустивши пару зразків симуляцій у найкращих кандидатах, наразі,

481
00:29:07,720 --> 00:29:09,500
принаймні для мене, здається, що Crane є найкращим відкривачем.

482
00:29:09,500 --> 00:29:11,080
Хто б міг здогадатися?

483
00:29:11,080 --> 00:29:15,680
Крім того, якщо ви використовуєте справжній список слів для визначення простору можливостей,

484
00:29:15,680 --> 00:29:17,920
тоді невизначеність, з якої ви починаєте, становить трохи більше 11 біт.

485
00:29:18,160 --> 00:29:22,760
І виявилося, що лише під час грубого пошуку максимально можлива

486
00:29:22,760 --> 00:29:26,580
очікувана інформація після перших двох припущень становить приблизно 10 біт.

487
00:29:26,580 --> 00:29:31,720
Це свідчить про те, що в найкращому випадку, після ваших перших

488
00:29:31,720 --> 00:29:35,220
двох припущень, з ідеально оптимальною грою, ви залишитеся з дещицею невизначеності.

489
00:29:35,220 --> 00:29:37,400
Це те саме, що мати два можливі припущення.

490
00:29:37,400 --> 00:29:41,440
Тож я вважаю справедливим і, ймовірно, досить консервативним сказати, що ви ніколи не

491
00:29:41,440 --> 00:29:45,620
зможете написати алгоритм, який отримає це середнє значення до 3, тому що з

492
00:29:45,620 --> 00:29:50,460
доступними вам словами просто не буде місця для отримання достатньої інформації лише за

493
00:29:50,460 --> 00:29:53,820
два кроки здатний гарантувати відповідь у третьому слоті кожного разу без збоїв.

