1
00:00:00,000 --> 00:00:04,554
Гра Wurdle стала досить вірусною за останній місяць чи два, і я ніколи не пропускав

2
00:00:04,554 --> 00:00:08,674
можливість уроку математики, мені спадає на думку, що ця гра є дуже хорошим

3
00:00:08,674 --> 00:00:13,120
центральним прикладом уроку з теорії інформації, зокрема тема, відома як ентропія.

4
00:00:13,120 --> 00:00:16,480
Розумієте, як і багатьох людей, мене затягнуло головоломкою,

5
00:00:16,480 --> 00:00:19,950
і, як і багатьох програмістів, я також був затягнутий у спробі

6
00:00:19,950 --> 00:00:23,200
написати алгоритм, який би грав у гру якомога оптимальніше.

7
00:00:23,200 --> 00:00:26,067
І те, що я думав зробити тут, це просто поговорити з вами про

8
00:00:26,067 --> 00:00:29,027
мій процес і пояснити деякі математичні обчислення, які в нього

9
00:00:29,027 --> 00:00:32,080
входять, оскільки весь алгоритм зосереджений на цій ідеї ентропії.

10
00:00:32,080 --> 00:00:42,180
Перш за все, якщо ви не чули про це, що таке Wurdle?

11
00:00:42,180 --> 00:00:45,308
І щоб убити двох зайців одним пострілом, поки ми проходимо правила

12
00:00:45,308 --> 00:00:48,391
гри, дозвольте мені також переглянути, куди ми йдемо з цим, тобто

13
00:00:48,391 --> 00:00:51,380
розробити маленький алгоритм, який, в основному, гратиме за нас.

14
00:00:51,380 --> 00:00:55,860
Хоча я не робив сьогодні Wurdle, це 4 лютого, і ми побачимо, як бот впорається.

15
00:00:55,860 --> 00:00:58,625
Мета Wurdle — вгадати таємниче слово з п’яти літер,

16
00:00:58,625 --> 00:01:00,860
і вам дається шість різних шансів вгадати.

17
00:01:00,860 --> 00:01:05,240
Наприклад, мій бот Wurdle пропонує мені почати з журавля.

18
00:01:05,240 --> 00:01:08,112
Кожного разу, коли ви припускаєте, ви отримуєте певну інформацію

19
00:01:08,112 --> 00:01:10,940
про те, наскільки близькі ваші припущення до істинної відповіді.

20
00:01:10,940 --> 00:01:14,540
Тут сірий квадрат говорить мені, що у фактичній відповіді немає C.

21
00:01:14,540 --> 00:01:18,340
Жовте поле говорить мені, що є R, але воно не в цьому положенні.

22
00:01:18,340 --> 00:01:20,492
Зелений квадрат говорить мені, що секретне слово

23
00:01:20,492 --> 00:01:22,820
дійсно має літеру А, і воно стоїть на третій позиції.

24
00:01:22,820 --> 00:01:24,300
І тоді немає ні N, ні E.

25
00:01:24,300 --> 00:01:27,420
Тож дозвольте мені просто зайти та повідомити боту Wurdle цю інформацію.

26
00:01:27,420 --> 00:01:31,500
Ми почали з крана, ми отримали сірий, жовтий, зелений, сірий, сірий.

27
00:01:31,500 --> 00:01:35,460
Не турбуйтеся про всі дані, які він зараз показує, я поясню це свого часу.

28
00:01:35,460 --> 00:01:39,700
Але його найкраща пропозиція для нашого другого вибору є химерною.

29
00:01:39,700 --> 00:01:42,578
І ваше припущення має бути справжнім словом із п’яти літер, але, як ви

30
00:01:42,578 --> 00:01:45,700
побачите, це досить ліберально щодо того, що насправді дозволить вам вгадати.

31
00:01:45,700 --> 00:01:48,860
У цьому випадку ми намагаємося shtick.

32
00:01:48,860 --> 00:01:50,260
І добре, все виглядає досить добре.

33
00:01:50,260 --> 00:01:54,740
Ми натиснули S і H, тому ми знаємо перші три літери, ми знаємо, що є R.

34
00:01:54,740 --> 00:01:59,740
Тож це буде як SHA щось R або SHA R щось.

35
00:01:59,740 --> 00:02:05,220
І, схоже, бот Wurdle знає, що він має лише дві можливості: shard або sharp.

36
00:02:05,220 --> 00:02:08,152
На даний момент це щось на кшталт суперечки між ними, тож я думаю,

37
00:02:08,152 --> 00:02:11,260
що, мабуть, лише тому, що він алфавітний, він поєднується з фрагментом.

38
00:02:11,260 --> 00:02:13,000
Ура, ось справжня відповідь.

39
00:02:13,000 --> 00:02:14,660
Тож ми отримали це за три.

40
00:02:14,660 --> 00:02:17,876
Якщо вам цікаво, чи це добре, я почув фразу однієї людини,

41
00:02:17,876 --> 00:02:20,820
що з Wurdle чотири — це рівномірно, а три — це пташка.

42
00:02:20,820 --> 00:02:22,960
Що, на мою думку, є досить влучною аналогією.

43
00:02:22,960 --> 00:02:27,560
Ви повинні бути постійно в своїй грі, щоб отримати чотири, але це точно не божевілля.

44
00:02:27,560 --> 00:02:30,000
Але коли ви отримуєте це за три, це просто чудово.

45
00:02:30,000 --> 00:02:33,425
Отже, якщо ви не за це, я хотів би просто поговорити про мій процес

46
00:02:33,425 --> 00:02:36,600
мислення з самого початку про те, як я підходжу до бота Wurdle.

47
00:02:36,600 --> 00:02:39,800
І, як я вже сказав, це дійсно привід для уроку теорії інформації.

48
00:02:39,800 --> 00:02:48,560
Основна мета – пояснити, що таке інформація, а що таке ентропія.

49
00:02:48,560 --> 00:02:51,130
Моєю першою думкою при підході до цього було поглянути

50
00:02:51,130 --> 00:02:53,560
на відносні частоти різних літер в англійській мові.

51
00:02:53,560 --> 00:02:56,922
Тож я подумав: гаразд, чи є початкове припущення чи початкова

52
00:02:56,922 --> 00:02:59,960
пара припущень, яка вражає багато цих найчастіших літер?

53
00:02:59,960 --> 00:03:03,780
І один, який я дуже любив, робив інший, а потім цвяхи.

54
00:03:03,780 --> 00:03:05,860
Думка полягає в тому, що якщо ви натискаєте букву, ви

55
00:03:05,860 --> 00:03:07,980
знаєте, ви отримуєте зелену або жовту, це завжди добре.

56
00:03:07,980 --> 00:03:09,460
Таке відчуття, що ви отримуєте інформацію.

57
00:03:09,460 --> 00:03:11,936
Але в цих випадках, навіть якщо ви не влучаєте і завжди

58
00:03:11,936 --> 00:03:14,765
отримуєте сірі, це все одно дає вам багато інформації, оскільки

59
00:03:14,765 --> 00:03:17,640
досить рідко можна знайти слово, у якому немає жодної з цих букв.

60
00:03:17,640 --> 00:03:20,457
Але все одно це не виглядає надсистематичним,

61
00:03:20,457 --> 00:03:23,520
тому що, наприклад, не враховується порядок літер.

62
00:03:23,520 --> 00:03:26,080
Навіщо друкувати цвяхи, коли я можу надрукувати равлика?

63
00:03:26,080 --> 00:03:27,720
Чи краще мати це S в кінці?

64
00:03:27,720 --> 00:03:28,720
Я не дуже впевнений.

65
00:03:28,720 --> 00:03:32,792
Тепер мій друг сказав, що йому подобається починати словом weary, що

66
00:03:32,792 --> 00:03:37,160
мене дещо здивувало, оскільки там є деякі незвичайні літери, як-от W та Y.

67
00:03:37,160 --> 00:03:39,400
Але хто знає, можливо, це кращий відкривач.

68
00:03:39,400 --> 00:03:42,047
Чи існує якась кількісна оцінка, яку ми можемо

69
00:03:42,047 --> 00:03:44,920
надати, щоб оцінити якість потенційного припущення?

70
00:03:44,920 --> 00:03:48,294
Тепер, щоб підготуватися до того, як ми будемо ранжувати можливі припущення,

71
00:03:48,294 --> 00:03:51,800
давайте повернемося назад і внесемо трохи ясності в те, як саме налаштована гра.

72
00:03:51,800 --> 00:03:54,804
Отже, є список слів, які можна ввести, які вважаються

73
00:03:54,804 --> 00:03:57,920
дійсними припущеннями, і складається лише з 13 000 слів.

74
00:03:57,920 --> 00:04:02,425
Але якщо ви подивитесь на це, ви побачите багато справді незвичайних речей, таких

75
00:04:02,425 --> 00:04:07,040
як голова або Алі та ARG, тип слів, які викликають сімейні суперечки в грі в Скрабл.

76
00:04:07,040 --> 00:04:10,600
Але настрій гри полягає в тому, що відповіддю завжди буде досить поширене слово.

77
00:04:10,600 --> 00:04:16,080
І насправді, є ще один список із приблизно 2300 слів, які є можливими відповідями.

78
00:04:16,080 --> 00:04:21,800
І це список, складений людьми, я думаю, саме дівчиною творця гри, що дуже весело.

79
00:04:21,800 --> 00:04:24,896
Але що я хотів би зробити, наше завдання для цього проекту полягає

80
00:04:24,896 --> 00:04:28,039
в тому, щоб побачити, чи зможемо ми написати програму, що розв’язує

81
00:04:28,039 --> 00:04:30,720
Wordle, яка не включатиме попередні знання про цей список.

82
00:04:30,720 --> 00:04:33,350
По-перше, є багато досить поширених слів із п’яти

83
00:04:33,350 --> 00:04:35,560
літер, яких ви не знайдете в цьому списку.

84
00:04:35,560 --> 00:04:38,667
Тож було б краще написати програму, яка була б трішки стійкішою та

85
00:04:38,667 --> 00:04:41,960
грала б у Wordle проти будь-кого, а не лише проти офіційного веб-сайту.

86
00:04:41,960 --> 00:04:44,722
А також причина, чому ми знаємо, що таке цей список можливих

87
00:04:44,722 --> 00:04:47,440
відповідей, полягає в тому, що він видимий у вихідному коді.

88
00:04:47,440 --> 00:04:50,113
Але те, як це видно у вихідному коді, — це певний

89
00:04:50,113 --> 00:04:52,840
порядок, у якому відповіді з’являються день у день.

90
00:04:52,840 --> 00:04:56,400
Тож ви завжди можете просто подивитися, якою буде завтрашня відповідь.

91
00:04:56,400 --> 00:04:59,140
Очевидно, що використання списку є обманом.

92
00:04:59,140 --> 00:05:02,141
І те, що робить головоломку цікавішою та насиченішим уроком теорії

93
00:05:02,141 --> 00:05:05,233
інформації, полягає в тому, що натомість можна використовувати деякі

94
00:05:05,233 --> 00:05:08,324
більш універсальні дані, такі як відносна частота слів у цілому, щоб

95
00:05:08,324 --> 00:05:11,640
вловити цю інтуїцію про те, що ми надаємо перевагу більш поширеним словам.

96
00:05:11,640 --> 00:05:16,560
Тож як із цих 13 000 можливостей вибрати початкове припущення?

97
00:05:16,560 --> 00:05:19,960
Наприклад, якщо мій друг пропонує втомленого, як ми повинні проаналізувати його якість?

98
00:05:19,960 --> 00:05:22,840
Що ж, причина, чому він сказав, що йому подобається це малоймовірне

99
00:05:22,840 --> 00:05:25,423
W, полягає в тому, що йому подобається довгострокова природа

100
00:05:25,423 --> 00:05:27,880
того, наскільки добре це відчуваєш, якщо ти влучив у це W.

101
00:05:27,880 --> 00:05:31,685
Наприклад, якщо перша виявлена закономірність була приблизно такою, то

102
00:05:31,685 --> 00:05:36,080
виявиться, що в цьому гігантському лексиконі лише 58 слів відповідають цій моделі.

103
00:05:36,080 --> 00:05:38,900
Тож це величезне зниження з 13 000.

104
00:05:38,900 --> 00:05:41,106
Але зворотна сторона цього, звичайно, полягає

105
00:05:41,106 --> 00:05:43,360
в тому, що дуже рідко отримати такий візерунок.

106
00:05:43,360 --> 00:05:47,712
Зокрема, якби кожне слово з однаковою ймовірністю було відповіддю, ймовірність

107
00:05:47,712 --> 00:05:51,680
досягнення цього шаблону дорівнювала б 58 поділеним приблизно на 13 000.

108
00:05:51,680 --> 00:05:53,880
Звичайно, вони не однаково ймовірні відповіді.

109
00:05:53,880 --> 00:05:56,680
Більшість із них дуже незрозумілі та навіть сумнівні слова.

110
00:05:56,680 --> 00:05:59,173
Але принаймні для нашого першого проходження всього цього, давайте

111
00:05:59,173 --> 00:06:02,040
припустимо, що всі вони однаково ймовірні, а потім уточнимо це трохи пізніше.

112
00:06:02,040 --> 00:06:07,360
Справа в тому, що шаблон із великою кількістю інформації за своєю природою малоймовірний.

113
00:06:07,360 --> 00:06:11,920
Насправді бути інформативним означає те, що це малоймовірно.

114
00:06:11,920 --> 00:06:15,080
Набагато більш вірогідною схемою для цього відкриття

115
00:06:15,080 --> 00:06:18,360
буде щось на кшталт цього, де, звичайно, немає букви W.

116
00:06:18,360 --> 00:06:22,080
Можливо, там є E, а можливо, немає A, немає R, немає Y.

117
00:06:22,080 --> 00:06:24,640
У цьому випадку є 1400 можливих збігів.

118
00:06:24,640 --> 00:06:27,875
Якби все було однаково вірогідним, виходить, що ймовірність

119
00:06:27,875 --> 00:06:30,680
приблизно 11% того, що ви б побачили саме цю модель.

120
00:06:30,680 --> 00:06:34,320
Таким чином, найімовірніші результати також є найменш інформативними.

121
00:06:34,320 --> 00:06:38,186
Щоб отримати більш глобальне уявлення, дозвольте мені показати вам повний

122
00:06:38,186 --> 00:06:42,000
розподіл ймовірностей за всіма різними шаблонами, які ви можете побачити.

123
00:06:42,000 --> 00:06:45,482
Таким чином, кожна смужка, на яку ви дивитеся, відповідає можливому

124
00:06:45,482 --> 00:06:49,118
шаблону кольорів, який можна виявити, з яких є від 3 до 5 варіантів, і

125
00:06:49,118 --> 00:06:52,960
вони організовані зліва направо, від найпоширенішого до найменш поширеного.

126
00:06:52,960 --> 00:06:56,200
Отже, найпоширенішою можливістю є те, що ви отримаєте всі сірі.

127
00:06:56,200 --> 00:06:58,800
Це відбувається приблизно в 14% випадків.

128
00:06:58,800 --> 00:07:02,721
І те, на що ви сподіваєтеся, коли ви припускаєте, це те, що ви опинитеся

129
00:07:02,721 --> 00:07:06,267
десь у цьому довгому хвості, як тут, де є лише 18 можливостей для

130
00:07:06,267 --> 00:07:09,920
того, що відповідає цьому шаблону, який, очевидно, виглядає ось так.

131
00:07:09,920 --> 00:07:14,080
Або якщо ми підемо трохи ліворуч, то, можливо, ми підемо сюди.

132
00:07:14,080 --> 00:07:16,560
Добре, ось вам гарна головоломка.

133
00:07:16,560 --> 00:07:19,433
Які три слова в англійській мові починаються з літери

134
00:07:19,433 --> 00:07:22,040
W, закінчуються літерою Y і десь містять букву R?

135
00:07:22,040 --> 00:07:27,560
Виявляється, відповіді, давайте подивимося, багатослівні, червиві та іронічні.

136
00:07:27,560 --> 00:07:31,875
Отже, щоб оцінити, наскільки добре це слово в цілому, ми хочемо якийсь вимір

137
00:07:31,875 --> 00:07:36,360
очікуваної кількості інформації, яку ви збираєтеся отримати від цього розподілу.

138
00:07:36,360 --> 00:07:41,146
Якщо ми переглянемо кожен шаблон і помножимо його ймовірність появи на

139
00:07:41,146 --> 00:07:46,000
те, що вимірює його інформативність, це може дати нам об’єктивну оцінку.

140
00:07:46,000 --> 00:07:50,280
Тепер вашим першим інстинктом щодо того, що це має бути, може бути кількість збігів.

141
00:07:50,280 --> 00:07:52,960
Вам потрібна менша середня кількість збігів.

142
00:07:52,960 --> 00:07:58,932
Але натомість я хотів би використовувати більш універсальне вимірювання, яке ми часто

143
00:07:58,932 --> 00:08:04,349
приписуємо інформації, і таке, яке буде більш гнучким, коли ми матимемо різну

144
00:08:04,349 --> 00:08:10,600
ймовірність, призначену кожному з цих 13 000 слів щодо того, чи є вони справді відповіддю.

145
00:08:10,600 --> 00:08:14,248
Стандартною одиницею інформації є біт, який має трохи кумедну формулу, але

146
00:08:14,248 --> 00:08:17,800
вона справді інтуїтивно зрозуміла, якщо ми просто подивимося на приклади.

147
00:08:17,800 --> 00:08:21,000
Якщо у вас є спостереження, яке вдвічі скорочує ваш простір

148
00:08:21,000 --> 00:08:24,200
можливостей, ми кажемо, що воно містить один біт інформації.

149
00:08:24,200 --> 00:08:27,834
У нашому прикладі простір можливостей — це всі можливі слова, і виявляється, що

150
00:08:27,834 --> 00:08:31,560
близько половини слів із п’яти літер мають S, трохи менше, але приблизно половина.

151
00:08:31,560 --> 00:08:35,200
Таким чином, це спостереження дасть вам трохи інформації.

152
00:08:35,200 --> 00:08:38,600
Якщо натомість новий факт скорочує цей простір можливостей у

153
00:08:38,600 --> 00:08:42,000
чотири рази, ми говоримо, що він містить два біти інформації.

154
00:08:42,000 --> 00:08:45,120
Наприклад, виявилося, що приблизно чверть цих слів мають Т.

155
00:08:45,120 --> 00:08:47,837
Якщо спостереження скорочує цей простір у вісім, ми

156
00:08:47,837 --> 00:08:50,920
говоримо, що це три біти інформації, і так далі і так далі.

157
00:08:50,920 --> 00:08:55,000
Чотири біти перетворюють його на 16-й, п’ять бітів — на 32-й.

158
00:08:55,000 --> 00:08:59,606
Тож тепер ви можете зупинитись і запитати себе, яка формула

159
00:08:59,606 --> 00:09:04,520
для інформації для кількості бітів у термінах ймовірності появи?

160
00:09:04,520 --> 00:09:08,456
Ми маємо на увазі те, що коли ви берете одну половину на кількість бітів,

161
00:09:08,456 --> 00:09:12,126
це те саме, що ймовірність, що те саме, що сказати, що два в степені

162
00:09:12,126 --> 00:09:16,009
кількості бітів є одиницею над ймовірністю, що далі переставляє, кажучи,

163
00:09:16,009 --> 00:09:19,680
що інформація є двома логарифмами одиниці, поділеними на ймовірність.

164
00:09:19,680 --> 00:09:22,524
І іноді ви бачите це з ще одним перевпорядкуванням, де

165
00:09:22,524 --> 00:09:25,680
інформація є від’ємним логарифмом за основою два ймовірності.

166
00:09:25,680 --> 00:09:30,204
У такому вигляді це може здатися трохи дивним для непосвячених, але насправді це

167
00:09:30,204 --> 00:09:35,120
просто дуже інтуїтивна ідея запитати, скільки разів ви скоротили свої можливості вдвічі.

168
00:09:35,120 --> 00:09:37,586
А тепер, якщо вам цікаво, знаєте, я думав, що ми просто

169
00:09:37,586 --> 00:09:39,920
граємо у веселу гру слів, чому логарифми з’являються?

170
00:09:39,920 --> 00:09:44,404
Одна з причин, чому це краща одиниця, полягає в тому, що набагато простіше говорити

171
00:09:44,404 --> 00:09:48,728
про дуже малоймовірні події, набагато легше сказати, що спостереження містить 20

172
00:09:48,728 --> 00:09:52,785
біт інформації, ніж сказати, що ймовірність такого-то виникнення дорівнює 0.

173
00:09:52,785 --> 00:09:53,480
0000095.

174
00:09:53,480 --> 00:09:57,768
Але більш суттєвою причиною того, що цей логарифмічний вираз виявився дуже

175
00:09:57,768 --> 00:10:02,000
корисним доповненням до теорії ймовірності, є спосіб додавання інформації.

176
00:10:02,000 --> 00:10:06,027
Наприклад, якщо одне спостереження дає вам два біти інформації, скорочуючи

177
00:10:06,027 --> 00:10:09,733
ваш простір учетверо, а потім друге спостереження, подібне до вашого

178
00:10:09,733 --> 00:10:13,600
другого припущення в Wordle, дає вам ще три біти інформації, скорочуючи

179
00:10:13,600 --> 00:10:17,360
вас ще на один коефіцієнт вісім, два разом дають п’ять біт інформації.

180
00:10:17,360 --> 00:10:21,200
Подібно до того, як ймовірності люблять множитися, інформація любить додаватися.

181
00:10:21,200 --> 00:10:25,050
Отже, як тільки ми потрапляємо в область чогось на зразок очікуваного значення,

182
00:10:25,050 --> 00:10:28,660
де ми додаємо купу чисел, журнали роблять це набагато зручнішим для роботи.

183
00:10:28,660 --> 00:10:31,885
Давайте повернемося до нашого розподілу для Weary і додамо сюди ще один

184
00:10:31,885 --> 00:10:35,560
маленький трекер, який показуватиме нам, скільки інформації є для кожного шаблону.

185
00:10:35,560 --> 00:10:39,396
Головне, на що я хочу, щоб ви звернули увагу, це те, що чим вища ймовірність, коли ми

186
00:10:39,396 --> 00:10:43,053
дійдемо до тих більш вірогідних моделей, тим менше інформації, тим менше бітів ви

187
00:10:43,053 --> 00:10:43,500
отримуєте.

188
00:10:43,500 --> 00:10:47,329
Спосіб вимірювання якості цього припущення полягає в тому, щоб взяти очікуване

189
00:10:47,329 --> 00:10:51,158
значення цієї інформації, де ми проходимо кожен шаблон, говоримо, наскільки це

190
00:10:51,158 --> 00:10:54,940
ймовірно, а потім ми множимо це на кількість біт інформації, яку ми отримуємо.

191
00:10:54,940 --> 00:10:58,107
А у прикладі Вірі це виявляється 4.

192
00:10:58,107 --> 00:10:58,480
9 біт.

193
00:10:58,480 --> 00:11:02,215
Отже, у середньому інформація, яку ви отримуєте з цього початкового припущення, настільки

194
00:11:02,215 --> 00:11:05,660
ж хороша, як скорочення вашого простору можливостей навпіл приблизно в п’ять разів.

195
00:11:05,660 --> 00:11:09,221
Навпаки, прикладом припущення з вищим очікуваним

196
00:11:09,221 --> 00:11:13,220
інформаційним значенням може бути щось на зразок Slate.

197
00:11:13,220 --> 00:11:16,180
У цьому випадку ви помітите, що розподіл виглядає набагато більш плоским.

198
00:11:16,180 --> 00:11:19,960
Зокрема, найімовірніша поява всіх сірих має лише

199
00:11:19,960 --> 00:11:24,435
близько 6% ймовірності появи, тому ви отримуєте мінімум 3.

200
00:11:24,435 --> 00:11:25,940
9 біт інформації.

201
00:11:25,940 --> 00:11:29,140
Але це мінімум, зазвичай ви отримаєте щось краще за це.

202
00:11:29,140 --> 00:11:32,820
І виявляється, коли ви обчислюєте цифри на цьому місці та додаєте

203
00:11:32,820 --> 00:11:36,333
всі відповідні терміни, середня інформація становить близько 5.

204
00:11:36,333 --> 00:11:36,420
8.

205
00:11:36,420 --> 00:11:39,937
Отже, на відміну від Вірі, ваш простір можливостей буде в

206
00:11:39,937 --> 00:11:43,940
середньому приблизно вдвічі менший після цього першого припущення.

207
00:11:43,940 --> 00:11:49,540
Насправді є весела історія про назву цього очікуваного значення кількості інформації.

208
00:11:49,540 --> 00:11:53,163
Теорію інформації розробив Клод Шеннон, який працював у Bell Labs у 1940-х

209
00:11:53,163 --> 00:11:56,884
роках, але він говорив про деякі зі своїх ідей, які ще не були опубліковані,

210
00:11:56,884 --> 00:12:00,604
з Джоном фон Нейманом, який був цим інтелектуальним гігантом того часу, дуже

211
00:12:00,604 --> 00:12:04,180
видатним у математиці та фізиці та початках того, що ставало інформатикою.

212
00:12:04,180 --> 00:12:07,546
І коли він згадав, що він насправді не має вдалого імені для

213
00:12:07,546 --> 00:12:10,857
цього очікуваного значення кількості інформації, фон Нейман

214
00:12:10,857 --> 00:12:14,720
нібито сказав, отже, ви повинні назвати це ентропією, і з двох причин.

215
00:12:14,720 --> 00:12:18,776
По-перше, ваша функція невизначеності використовувалася в статистичній механіці

216
00:12:18,776 --> 00:12:22,832
під такою назвою, тож вона вже має назву, а по-друге, що ще важливіше, ніхто не

217
00:12:22,832 --> 00:12:26,940
знає, що таке ентропія насправді, тому в дебатах ви завжди будете мають перевагу.

218
00:12:26,940 --> 00:12:33,420
Отже, якщо назва здається трохи загадковою, і якщо вірити цій історії, це начебто задум.

219
00:12:33,420 --> 00:12:37,730
Крім того, якщо вам цікаво його відношення до всього другого закону термодинаміки

220
00:12:37,730 --> 00:12:41,830
з фізики, зв’язок точно є, але в його витоках Шеннон мав справу лише з чистою

221
00:12:41,830 --> 00:12:46,088
теорією ймовірностей, і для наших цілей тут, коли я використовую слово ентропія,

222
00:12:46,088 --> 00:12:50,820
я просто хочу, щоб ви подумали про очікувану інформаційну цінність конкретного припущення.

223
00:12:50,820 --> 00:12:54,380
Ви можете думати про ентропію як про вимірювання двох речей одночасно.

224
00:12:54,380 --> 00:12:57,420
Перше — це те, наскільки плоским є розподіл.

225
00:12:57,420 --> 00:13:01,700
Чим ближче рівномірний розподіл, тим вищою буде ентропія.

226
00:13:01,700 --> 00:13:05,122
У нашому випадку, коли існує від 3 до 5-го загальних шаблонів,

227
00:13:05,122 --> 00:13:08,490
для рівномірного розподілу, спостереження за будь-яким із них

228
00:13:08,490 --> 00:13:11,858
матиме інформаційну базу журналу 2 з 3 до 5-го, що дорівнює 7.

229
00:13:11,858 --> 00:13:17,860
92, тож це абсолютний максимум, який ви могли б отримати для цієї ентропії.

230
00:13:17,860 --> 00:13:22,900
Але ентропія також є своєрідним показником того, скільки можливостей існує.

231
00:13:22,900 --> 00:13:27,862
Наприклад, якщо у вас є якесь слово, де є лише 16 можливих шаблонів, і кожен

232
00:13:27,862 --> 00:13:32,760
з них однаково вірогідний, ця ентропія, ця очікувана інформація буде 4 біти.

233
00:13:32,760 --> 00:13:36,558
Але якщо у вас є інше слово, де є 64 можливі шаблони, які можуть

234
00:13:36,558 --> 00:13:41,000
виникнути, і всі вони однаково вірогідні, тоді ентропія буде складати 6 біт.

235
00:13:41,000 --> 00:13:45,405
Отже, якщо ви бачите якийсь розподіл у дикій природі, який має ентропію

236
00:13:45,405 --> 00:13:49,872
6 біт, це ніби говорить про те, що в тому, що має статися, існує стільки

237
00:13:49,872 --> 00:13:54,400
варіацій і невизначеності, як якщо б було 64 однаково ймовірні результати.

238
00:13:54,400 --> 00:13:58,360
Під час мого першого проходу в Wurtelebot я просто зробив це.

239
00:13:58,360 --> 00:14:02,949
Він переглядає всі можливі припущення, які ви можете мати, усі 13 000 слів,

240
00:14:02,949 --> 00:14:07,719
обчислює ентропію для кожного з них, або, точніше, ентропію розподілу за всіма

241
00:14:07,719 --> 00:14:12,671
шаблонами, які ви можете побачити, для кожного з них, і вибирає найвище, оскільки

242
00:14:12,671 --> 00:14:17,200
це той, який, швидше за все, максимально скоротить ваш простір можливостей.

243
00:14:17,200 --> 00:14:19,208
І незважаючи на те, що я говорив тут лише про перше

244
00:14:19,208 --> 00:14:21,680
припущення, воно робить те саме для кількох наступних припущень.

245
00:14:21,680 --> 00:14:25,172
Наприклад, після того, як ви бачите певний шаблон у цій першій здогадці,

246
00:14:25,172 --> 00:14:28,760
яка обмежила б вас меншою кількістю можливих слів на основі того, що з цим

247
00:14:28,760 --> 00:14:32,300
збігається, ви просто граєте в ту саму гру щодо цього меншого набору слів.

248
00:14:32,300 --> 00:14:36,507
Для запропонованого другого припущення ви дивитеся на розподіл усіх

249
00:14:36,507 --> 00:14:40,901
шаблонів, які можуть виникати з цього більш обмеженого набору слів, ви

250
00:14:40,901 --> 00:14:45,480
шукаєте всі 13 000 можливостей і знаходите ту, яка максимізує цю ентропію.

251
00:14:45,480 --> 00:14:49,833
Щоб показати вам, як це працює в дії, дозвольте мені просто витягнути невеликий

252
00:14:49,833 --> 00:14:54,460
варіант Wurtele, який я написав, який показує основні моменти цього аналізу на полях.

253
00:14:54,460 --> 00:14:57,350
Після виконання всіх обчислень ентропії, тут праворуч він

254
00:14:57,350 --> 00:15:00,340
показує нам, які з них мають найбільшу очікувану інформацію.

255
00:15:00,340 --> 00:15:05,777
Виявляється, головною відповіддю, принаймні на даний момент, ми уточнимо

256
00:15:05,777 --> 00:15:11,140
це пізніше, є Тарес, що означає, гм, звичайно, вика, найпоширеніша вика.

257
00:15:11,140 --> 00:15:14,367
Кожного разу, коли ми робимо припущення тут, де, можливо, я ігнорую його

258
00:15:14,367 --> 00:15:17,595
рекомендації та вибираю шифер, тому що мені подобається шифер, ми можемо

259
00:15:17,595 --> 00:15:20,956
побачити, скільки очікуваної інформації він містить, але праворуч від слова

260
00:15:20,956 --> 00:15:24,228
тут показано, скільки фактичну інформацію, яку ми отримали, враховуючи цю

261
00:15:24,228 --> 00:15:24,980
конкретну модель.

262
00:15:24,980 --> 00:15:27,932
Тож тут, схоже, нам трохи не пощастило, очікували, що ми отримаємо 5.

263
00:15:27,932 --> 00:15:30,660
8, але випадково ми отримали щось менше, ніж це.

264
00:15:30,660 --> 00:15:35,860
А потім ліворуч тут показано всі різні можливі слова, де ми зараз знаходимося.

265
00:15:35,860 --> 00:15:39,868
Сині смужки вказують на те, наскільки вірогідним є кожне слово, тому наразі

266
00:15:39,868 --> 00:15:44,140
припускається, що кожне слово буде однаково ймовірно, але ми уточнимо це за мить.

267
00:15:44,140 --> 00:15:47,874
І тоді це вимірювання невизначеності говорить нам про ентропію цього

268
00:15:47,874 --> 00:15:51,501
розподілу між можливими словами, що зараз, оскільки це рівномірний

269
00:15:51,501 --> 00:15:55,940
розподіл, є просто непотрібно складним способом підрахувати кількість можливостей.

270
00:15:55,940 --> 00:15:59,343
Наприклад, якби ми взяли 2 у ступінь 13.

271
00:15:59,343 --> 00:16:02,700
66, це має бути приблизно 13 000 можливостей.

272
00:16:02,700 --> 00:16:06,780
Я трохи відхилився, але лише тому, що не показую всі десяткові знаки.

273
00:16:06,780 --> 00:16:09,705
На даний момент це може здатися зайвим і занадто складним,

274
00:16:09,705 --> 00:16:12,780
але за хвилину ви зрозумієте, чому корисно мати обидва номери.

275
00:16:12,780 --> 00:16:16,094
Отже, тут виглядає так, ніби найвища ентропія для нашого

276
00:16:16,094 --> 00:16:19,700
другого припущення – Рамен, що знову ж таки не схоже на слово.

277
00:16:19,700 --> 00:16:25,660
Отже, щоб підвищити моральну позицію, я введу Rains.

278
00:16:25,660 --> 00:16:27,540
І знову схоже, що нам трохи не пощастило.

279
00:16:27,540 --> 00:16:28,872
Ми чекали 4.

280
00:16:28,872 --> 00:16:30,556
3 біти, а ми отримали лише 3.

281
00:16:30,556 --> 00:16:32,100
39 біт інформації.

282
00:16:32,100 --> 00:16:35,060
Отже, ми маємо 55 можливостей.

283
00:16:35,060 --> 00:16:40,200
І тут, можливо, я просто прийму те, що він пропонує, тобто комбо, що б це не означало.

284
00:16:40,200 --> 00:16:43,300
І гаразд, це насправді хороший шанс для головоломки.

285
00:16:43,300 --> 00:16:45,718
Це говорить нам, що цей шаблон дає нам 4.

286
00:16:45,718 --> 00:16:47,020
7 біт інформації.

287
00:16:47,020 --> 00:16:50,990
Але ліворуч, перш ніж ми побачимо цей шаблон, їх було 5.

288
00:16:50,990 --> 00:16:52,400
78 біт невизначеності.

289
00:16:52,400 --> 00:16:56,860
Отже, як вікторина для вас, що це означає щодо кількості можливостей, що залишилися?

290
00:16:56,860 --> 00:17:01,192
Ну, це означає, що ми зведені до однієї частки невизначеності,

291
00:17:01,192 --> 00:17:04,700
що те саме, що сказати, що є дві можливі відповіді.

292
00:17:04,700 --> 00:17:06,520
Це вибір 50 на 50.

293
00:17:06,520 --> 00:17:08,913
І звідси, оскільки ми з вами знаємо, які слова є більш

294
00:17:08,913 --> 00:17:11,220
поширеними, ми знаємо, що відповідь має бути безодня.

295
00:17:11,220 --> 00:17:13,540
Але, як зараз написано, програма цього не знає.

296
00:17:13,540 --> 00:17:17,069
Тож він просто продовжує, намагаючись отримати якомога більше інформації,

297
00:17:17,069 --> 00:17:20,360
доки не залишиться лише одна можливість, а потім здогадується про це.

298
00:17:20,360 --> 00:17:22,700
Отже, очевидно, нам потрібна краща стратегія завершення гри.

299
00:17:22,700 --> 00:17:26,511
Але, скажімо, ми назвемо цю версію одним із наших розв’язувачів

300
00:17:26,511 --> 00:17:30,740
Wordle, а потім запустимо кілька симуляцій, щоб побачити, як це працює.

301
00:17:30,740 --> 00:17:34,240
Отже, як це працює, це гра в усі можливі ігри зі словами.

302
00:17:34,240 --> 00:17:38,780
Він переглядає всі ці 2315 слів, які є фактичними відповідями Wordle.

303
00:17:38,780 --> 00:17:41,340
В основному це використовується як набір для тестування.

304
00:17:41,340 --> 00:17:44,291
І з цим наївним методом не брати до уваги, наскільки поширене

305
00:17:44,291 --> 00:17:47,290
слово, і просто намагатися максимізувати інформацію на кожному

306
00:17:47,290 --> 00:17:50,480
кроці на цьому шляху, доки не дійде до одного й лише одного вибору.

307
00:17:50,480 --> 00:17:54,912
Наприкінці симуляції середній бал виходить близько 4.

308
00:17:54,912 --> 00:17:55,100
124.

309
00:17:55,100 --> 00:17:59,780
Що непогано, чесно кажучи, я очікував, що буде гірше.

310
00:17:59,780 --> 00:18:03,040
Але люди, які грають у wordle, скажуть вам, що вони зазвичай можуть отримати його за 4.

311
00:18:03,040 --> 00:18:05,260
Справжнє завдання — отримати якомога більше за 3.

312
00:18:05,260 --> 00:18:08,920
Це досить великий стрибок між рахунком 4 і рахунком 3.

313
00:18:08,920 --> 00:18:15,614
Очевидний низький плід тут полягає в тому, щоб якимось

314
00:18:15,614 --> 00:18:23,160
чином врахувати, чи є слово загальним, і як саме ми це робимо.

315
00:18:23,160 --> 00:18:26,186
Я підійшов до цього, щоб отримати список відносних

316
00:18:26,186 --> 00:18:28,560
частот для всіх слів в англійській мові.

317
00:18:28,560 --> 00:18:31,963
І я щойно скористався функцією даних частоти слів Mathematica, яка

318
00:18:31,963 --> 00:18:35,520
сама одержує загальнодоступний набір даних Google Books English Ngram.

319
00:18:35,520 --> 00:18:37,760
І на це цікаво дивитися, наприклад, якщо ми відсортуємо

320
00:18:37,760 --> 00:18:40,120
його від найбільш поширених слів до найменш поширених слів.

321
00:18:40,120 --> 00:18:43,740
Очевидно, це найпоширеніші слова з 5 букв в англійській мові.

322
00:18:43,740 --> 00:18:46,480
А точніше, це 8 місце за поширеністю.

323
00:18:46,480 --> 00:18:49,440
Спочатку який, потім там і там.

324
00:18:49,440 --> 00:18:53,942
Перший сам по собі не перший, а 9-й, і цілком зрозуміло, що ці інші слова можуть

325
00:18:53,942 --> 00:18:58,666
зустрічатися частіше, де ті, що стоять після першого, є після, де, а ті, які є трохи

326
00:18:58,666 --> 00:18:59,000
рідше.

327
00:18:59,000 --> 00:19:02,886
Тепер, використовуючи ці дані для моделювання того, наскільки ймовірно кожне з

328
00:19:02,886 --> 00:19:07,020
цих слів буде остаточною відповіддю, це не повинно бути просто пропорційним частоті.

329
00:19:07,020 --> 00:19:09,596
Наприклад, який отримав оцінку 0.

330
00:19:09,596 --> 00:19:12,313
002 у цьому наборі даних, тоді як слово braid у

331
00:19:12,313 --> 00:19:15,200
певному сенсі приблизно в 1000 разів менш імовірне.

332
00:19:15,200 --> 00:19:19,400
Але обидва ці слова досить поширені, тому їх майже напевно варто розглянути.

333
00:19:19,400 --> 00:19:21,900
Отже, ми хочемо більше двійкового відсікання.

334
00:19:21,900 --> 00:19:26,119
Я пішов з цього приводу: уявити весь цей відсортований список слів, а потім

335
00:19:26,119 --> 00:19:30,005
розташувати його на осі х, а потім застосувати функцію sigmoid, яка є

336
00:19:30,005 --> 00:19:34,225
стандартним способом отримання функції, вихід якої в основному є двійковим,

337
00:19:34,225 --> 00:19:38,500
це або 0, або 1, але між ними є згладжування для цієї області невизначеності.

338
00:19:38,500 --> 00:19:42,199
Таким чином, по суті, ймовірність того, що я призначаю кожному

339
00:19:42,199 --> 00:19:46,075
слову для того, щоб потрапити в остаточний список, буде значенням

340
00:19:46,075 --> 00:19:49,540
сигмоїдної функції вище, де б воно не знаходилося на осі х.

341
00:19:49,540 --> 00:19:54,136
Тепер, очевидно, це залежить від кількох параметрів, наприклад, наскільки широкий

342
00:19:54,136 --> 00:19:58,676
простір на осі х заповнюють ці слова, визначає, наскільки поступово або круто ми

343
00:19:58,676 --> 00:20:03,160
знижуємося від 1 до 0, і те, де ми їх розташовуємо зліва направо, визначає межу.

344
00:20:03,160 --> 00:20:07,340
Чесно кажучи, те, як я це зробив, було просто облизати палець і тицьнути його на вітер.

345
00:20:07,340 --> 00:20:10,610
Я переглянув відсортований список і спробував знайти вікно, у

346
00:20:10,610 --> 00:20:13,987
якому, дивлячись на нього, я вирішив, що приблизно половина цих

347
00:20:13,987 --> 00:20:17,680
слів, швидше за все, є остаточною відповіддю, і використав це як межу.

348
00:20:17,680 --> 00:20:21,042
Коли ми маємо подібний розподіл між словами, це дає нам іншу

349
00:20:21,042 --> 00:20:24,460
ситуацію, коли ентропія стає цим дійсно корисним вимірюванням.

350
00:20:24,460 --> 00:20:27,491
Наприклад, скажімо, ми граємо в гру, і ми починаємо з моїх

351
00:20:27,491 --> 00:20:30,522
старих відкривачів, які були пером і цвяхами, і закінчуємо

352
00:20:30,522 --> 00:20:33,760
ситуацією, коли є чотири можливі слова, які відповідають цьому.

353
00:20:33,760 --> 00:20:36,440
І припустимо, ми вважаємо їх усіх однаково ймовірними.

354
00:20:36,440 --> 00:20:40,000
Дозвольте запитати вас, яка ентропія цього розподілу?

355
00:20:40,000 --> 00:20:45,536
Що ж, інформація, пов’язана з кожною з цих можливостей, буде

356
00:20:45,536 --> 00:20:50,800
базою журналу 2 з 4, оскільки кожна з них є 1 і 4, і це 2.

357
00:20:50,800 --> 00:20:52,780
Два біти інформації, чотири можливості.

358
00:20:52,780 --> 00:20:54,360
Все дуже добре і добре.

359
00:20:54,360 --> 00:20:58,320
Але що, якби я сказав вам, що насправді є більше чотирьох збігів?

360
00:20:58,320 --> 00:21:00,370
Насправді, коли ми переглядаємо повний список

361
00:21:00,370 --> 00:21:02,600
слів, ми знаходимо 16 слів, які йому відповідають.

362
00:21:02,600 --> 00:21:07,073
Але припустімо, що наша модель надає справді низьку ймовірність того, що ці інші 12

363
00:21:07,073 --> 00:21:11,440
слів є остаточною відповіддю, приблизно 1 з 1000, тому що вони дійсно незрозумілі.

364
00:21:11,440 --> 00:21:15,480
Тепер дозвольте запитати вас, яка ентропія цього розподілу?

365
00:21:15,480 --> 00:21:18,931
Якби ентропія вимірювала лише кількість збігів, тоді можна було б

366
00:21:18,931 --> 00:21:22,487
очікувати, що це буде щось на кшталт логарифмічної бази 2 із 16, що

367
00:21:22,487 --> 00:21:26,200
дорівнюватиме 4, на два біти невизначеності більше, ніж ми мали раніше.

368
00:21:26,200 --> 00:21:30,320
Але, звісно, фактична невизначеність не дуже відрізняється від того, що ми мали раніше.

369
00:21:30,320 --> 00:21:34,091
Те, що є ці 12 справді незрозумілих слів, не означає, що було б ще

370
00:21:34,091 --> 00:21:38,200
більш дивно дізнатися, що остаточною відповіддю є, наприклад, чарівність.

371
00:21:38,200 --> 00:21:41,910
Отже, коли ви фактично виконуєте обчислення тут і додаєте ймовірність

372
00:21:41,910 --> 00:21:45,514
кожного випадку, помножену на відповідну інформацію, ви отримуєте 2.

373
00:21:45,514 --> 00:21:45,960
11 біт.

374
00:21:45,960 --> 00:21:49,349
Я просто кажу, що в основному це два біти, в основному ці чотири

375
00:21:49,349 --> 00:21:53,104
можливості, але є трохи більше невизначеності через усі ці малоймовірні

376
00:21:53,104 --> 00:21:57,120
події, хоча якби ви дізналися про них, ви б отримали з цього масу інформації.

377
00:21:57,120 --> 00:21:59,332
Отже, зменшуючи масштаб, це частина того, що робить

378
00:21:59,332 --> 00:22:01,800
Wordle таким гарним прикладом для уроку теорії інформації.

379
00:22:01,800 --> 00:22:05,280
Ми маємо ці два різні застосування ентропії.

380
00:22:05,280 --> 00:22:10,682
Перше говорить нам, яку інформацію ми очікуємо отримати від певного припущення, а

381
00:22:10,682 --> 00:22:16,480
друге говорить, чи можемо ми виміряти залишкову невизначеність серед усіх можливих слів.

382
00:22:16,480 --> 00:22:20,640
І я маю підкреслити, що в першому випадку, коли ми розглядаємо очікувану інформацію

383
00:22:20,640 --> 00:22:25,000
про припущення, як тільки ми маємо нерівну вагу слів, це впливає на обчислення ентропії.

384
00:22:25,000 --> 00:22:27,941
Наприклад, дозвольте мені знайти той самий випадок, який ми

385
00:22:27,941 --> 00:22:31,177
розглядали раніше, розподілу, пов’язаного з Weary, але цього разу

386
00:22:31,177 --> 00:22:34,560
з використанням нерівномірного розподілу між усіма можливими словами.

387
00:22:34,560 --> 00:22:39,360
Тож дозвольте мені поглянути, чи зможу я знайти тут частину, яка б це добре ілюструвала.

388
00:22:39,360 --> 00:22:42,480
Гаразд, ось це досить добре.

389
00:22:42,480 --> 00:22:45,886
Тут ми маємо два суміжних шаблони, які приблизно однаково вірогідні, але

390
00:22:45,886 --> 00:22:49,480
один із них, як нам сказали, містить 32 можливі слова, які йому відповідають.

391
00:22:49,480 --> 00:22:52,568
І якщо ми перевіримо, що це таке, це ті 32, які є дуже

392
00:22:52,568 --> 00:22:55,600
малоймовірними словами, якщо ви переглядаєте їх очима.

393
00:22:55,600 --> 00:22:59,038
Важко знайти будь-яку правдоподібну відповідь, можливо, крики, але

394
00:22:59,038 --> 00:23:02,477
якщо ми подивимося на сусідній шаблон у розподілі, який вважається

395
00:23:02,477 --> 00:23:06,121
приблизно таким же вірогідним, нам скажуть, що він має лише 8 можливих

396
00:23:06,121 --> 00:23:09,920
збігів, тобто чверть як багато збігів, але це приблизно так само ймовірно.

397
00:23:09,920 --> 00:23:12,520
І коли ми знайдемо ці сірники, ми зрозуміємо чому.

398
00:23:12,520 --> 00:23:17,840
Деякі з них є реальними правдоподібними відповідями, наприклад, кільце, гнів або стукіт.

399
00:23:17,840 --> 00:23:21,950
Щоб проілюструвати, як ми все це об’єднуємо, дозвольте мені витягнути тут версію

400
00:23:21,950 --> 00:23:25,960
2 Wordlebot, і там є дві або три основні відмінності від першої, яку ми бачили.

401
00:23:25,960 --> 00:23:30,289
По-перше, як я щойно сказав, спосіб, у який ми обчислюємо ці ентропії, ці

402
00:23:30,289 --> 00:23:34,502
очікувані значення інформації, тепер використовує точніший розподіл між

403
00:23:34,502 --> 00:23:39,300
шаблонами, який включає ймовірність того, що дане слово насправді буде відповіддю.

404
00:23:39,300 --> 00:23:44,160
Як це сталося, сльози все ще залишаються номером 1, хоча наступні трохи інші.

405
00:23:44,160 --> 00:23:47,821
По-друге, коли він ранжує свої найпопулярніші варіанти, він тепер зберігатиме

406
00:23:47,821 --> 00:23:51,623
модель ймовірності того, що кожне слово є фактичною відповіддю, і він включатиме

407
00:23:51,623 --> 00:23:55,520
це у своє рішення, яке легше побачити, коли ми матимемо кілька припущень щодо стіл.

408
00:23:55,520 --> 00:23:58,266
Знову ж таки, ігноруємо його рекомендацію, тому що

409
00:23:58,266 --> 00:24:01,120
ми не можемо дозволити машинам керувати нашим життям.

410
00:24:01,120 --> 00:24:05,500
І я вважаю, що я повинен згадати ще одну іншу річ, яка закінчилася ліворуч, що значення

411
00:24:05,500 --> 00:24:09,731
невизначеності, ця кількість бітів, більше не просто надлишкова з кількістю можливих

412
00:24:09,731 --> 00:24:10,080
збігів.

413
00:24:10,080 --> 00:24:13,489
Тепер, якщо ми витягнемо це вгору і порахуємо 2 до 8.

414
00:24:13,489 --> 00:24:18,865
02, що трохи вище 256, я думаю, 259, це говорить про те, що, незважаючи на

415
00:24:18,865 --> 00:24:24,097
те, що всього 526 слів відповідають цьому шаблону, рівень невизначеності

416
00:24:24,097 --> 00:24:29,760
більше схожий на той, який був би, якби було 259 однаково ймовірних результати.

417
00:24:29,760 --> 00:24:31,100
Ви можете думати про це так.

418
00:24:31,100 --> 00:24:34,281
Він знає, що borx не є відповіддю, те саме з yorts, zorl і

419
00:24:34,281 --> 00:24:37,840
zorus, тому це трохи менш невизначено, ніж у попередньому випадку.

420
00:24:37,840 --> 00:24:40,220
Ця кількість бітів буде меншою.

421
00:24:40,220 --> 00:24:44,058
І якщо я продовжу грати в гру, я уточню це парою

422
00:24:44,058 --> 00:24:48,680
припущень, які стосуються того, що я хотів би пояснити тут.

423
00:24:48,680 --> 00:24:51,179
За четвертим припущенням, якщо ви подивитеся на його найкращі

424
00:24:51,179 --> 00:24:53,800
варіанти, ви побачите, що це вже не просто максимізація ентропії.

425
00:24:53,800 --> 00:24:57,322
Отже, на даний момент технічно є сім можливостей, але

426
00:24:57,322 --> 00:25:00,780
єдині, які мають значний шанс, це гуртожиток і слова.

427
00:25:00,780 --> 00:25:04,227
І ви можете бачити, що він класифікує обидва вище за всі ці

428
00:25:04,227 --> 00:25:07,560
інші значення, що, строго кажучи, дасть більше інформації.

429
00:25:07,560 --> 00:25:11,158
У перший раз, коли я зробив це, я просто склав ці два числа, щоб виміряти якість

430
00:25:11,158 --> 00:25:14,580
кожного припущення, яке насправді спрацювало краще, ніж ви могли підозрювати.

431
00:25:14,580 --> 00:25:17,154
Але це справді не здавалося систематичним, і я впевнений, що є інші

432
00:25:17,154 --> 00:25:19,880
підходи, які люди могли б використати, але ось той, на який я зупинився.

433
00:25:19,880 --> 00:25:24,213
Якщо ми розглядаємо перспективу наступного припущення, як у цьому випадку слова,

434
00:25:24,213 --> 00:25:28,440
те, що нас справді хвилює, це очікуваний рахунок нашої гри, якщо ми це зробимо.

435
00:25:28,440 --> 00:25:32,287
І щоб обчислити цей очікуваний бал, ми кажемо, яка ймовірність того,

436
00:25:32,287 --> 00:25:36,080
що слова є фактичною відповіддю, яка на даний момент відповідає 58%.

437
00:25:36,080 --> 00:25:40,400
Ми кажемо, що з імовірністю 58% наш рахунок у цій грі буде 4.

438
00:25:40,400 --> 00:25:46,240
І тоді з імовірністю 1 мінус ці 58% наша оцінка буде більшою за ці 4.

439
00:25:46,240 --> 00:25:49,446
Скільки ще ми не знаємо, але ми можемо оцінити це на основі

440
00:25:49,446 --> 00:25:52,920
того, скільки невизначеності буде, коли ми дійдемо до цієї точки.

441
00:25:52,920 --> 00:25:55,227
Зокрема, на даний момент є 1.

442
00:25:55,227 --> 00:25:56,600
44 біти невизначеності.

443
00:25:56,600 --> 00:26:01,131
Якщо ми вгадуємо слова, це означає, що очікувана інформація, яку ми отримаємо, дорівнює 1.

444
00:26:01,131 --> 00:26:01,560
27 біт.

445
00:26:01,560 --> 00:26:04,703
Отже, якщо ми вгадуємо слова, ця різниця показує, скільки

446
00:26:04,703 --> 00:26:08,280
невизначеності ми, ймовірно, залишимо після того, як це станеться.

447
00:26:08,280 --> 00:26:10,970
Нам потрібна якась функція, яку я тут називаю f,

448
00:26:10,970 --> 00:26:13,880
яка пов’язує цю невизначеність із очікуваною оцінкою.

449
00:26:13,880 --> 00:26:18,266
І те, як це було зроблено, полягало в тому, що просто побудували групу даних

450
00:26:18,266 --> 00:26:22,596
із попередніх ігор на основі версії 1 бота, щоб сказати, яким був фактичний

451
00:26:22,596 --> 00:26:27,040
рахунок після різних моментів з певною дуже вимірною кількістю невизначеності.

452
00:26:27,040 --> 00:26:31,073
Наприклад, ці точки даних тут знаходяться вище значення приблизно 8.

453
00:26:31,073 --> 00:26:35,426
7 або близько того кажуть для деяких ігор після моменту, коли було 8.

454
00:26:35,426 --> 00:26:39,340
7 біт невизначеності, знадобилося дві здогадки, щоб отримати остаточну відповідь.

455
00:26:39,340 --> 00:26:43,180
Для інших ігор потрібно було три відгадки, для інших ігор – чотири.

456
00:26:43,180 --> 00:26:47,174
Якщо ми перемістимося вліво тут, усі точки над нулем говорять про те, що

457
00:26:47,174 --> 00:26:51,114
будь-коли є нуль біт невизначеності, тобто є лише одна можливість, тоді

458
00:26:51,114 --> 00:26:55,000
необхідна кількість припущень завжди дорівнює лише одній, що заспокоює.

459
00:26:55,000 --> 00:26:59,353
Щоразу, коли була трішка невизначеності, тобто, по суті, зводилася лише до

460
00:26:59,353 --> 00:27:03,940
двох можливостей, іноді вимагалося ще одне припущення, іноді ще два припущення.

461
00:27:03,940 --> 00:27:05,980
І так далі і так далі тут.

462
00:27:05,980 --> 00:27:08,423
Можливо, дещо простіший спосіб візуалізувати ці

463
00:27:08,423 --> 00:27:11,020
дані — об’єднати їх разом і взяти середні значення.

464
00:27:11,020 --> 00:27:16,664
Наприклад, ця смужка тут говорить, що серед усіх пунктів, де ми мали хоча б трохи

465
00:27:16,664 --> 00:27:22,308
невизначеності, в середньому необхідна кількість нових припущень була приблизно 1.

466
00:27:22,308 --> 00:27:22,420
5.

467
00:27:22,420 --> 00:27:27,139
І смужка тут говорить, що серед усіх різних ігор, де в якийсь момент невизначеність

468
00:27:27,139 --> 00:27:31,745
була трохи вище чотирьох бітів, що схоже на звуження до 16 різних можливостей, то

469
00:27:31,745 --> 00:27:36,240
в середньому для цього потрібно трохи більше двох припущень з цієї точки вперед.

470
00:27:36,240 --> 00:27:38,345
І звідси я просто зробив регресію, щоб відповідати

471
00:27:38,345 --> 00:27:40,080
функції, яка здавалася розумною для цього.

472
00:27:40,080 --> 00:27:44,603
І пам’ятайте, що весь сенс будь-чого з цього полягає в тому, щоб ми могли кількісно

473
00:27:44,603 --> 00:27:48,912
оцінити цю інтуїцію: що більше інформації ми отримуємо зі слова, то нижчим буде

474
00:27:48,912 --> 00:27:49,720
очікуваний бал.

475
00:27:49,720 --> 00:27:51,043
Отже, це як версія 2.

476
00:27:51,043 --> 00:27:55,337
0, якщо ми повернемося назад і запустимо той самий набір симуляцій,

477
00:27:55,337 --> 00:27:59,820
зіставивши його з усіма 2315 можливими відповідями Wordle, як це вийде?

478
00:27:59,820 --> 00:28:04,060
Що ж, на відміну від нашої першої версії, вона точно краща, що заспокоює.

479
00:28:04,060 --> 00:28:06,284
Усе сказане та зроблене середній бал становить близько 3.

480
00:28:06,284 --> 00:28:09,399
6, хоча, на відміну від першої версії, вона кілька

481
00:28:09,399 --> 00:28:12,820
разів програє, і за цієї обставини вимагає більше шести.

482
00:28:12,820 --> 00:28:16,001
Мабуть тому, що бувають моменти, коли потрібно

483
00:28:16,001 --> 00:28:18,980
досягти мети, а не максимізувати інформацію.

484
00:28:18,980 --> 00:28:22,022
Отже, ми можемо зробити краще, ніж 3.

485
00:28:22,022 --> 00:28:22,140
6?

486
00:28:22,140 --> 00:28:23,260
Ми точно можемо.

487
00:28:23,260 --> 00:28:26,431
На початку я сказав, що найцікавіше спробувати не включати

488
00:28:26,431 --> 00:28:29,980
справжній список відповідей Wordle у спосіб побудови своєї моделі.

489
00:28:29,980 --> 00:28:33,091
Але якщо ми все-таки це включимо, найкраща продуктивність,

490
00:28:33,091 --> 00:28:35,043
яку я міг отримати, була приблизно 3.

491
00:28:35,043 --> 00:28:35,180
43.

492
00:28:35,180 --> 00:28:38,236
Отже, якщо ми спробуємо вийти більш складним, ніж просто використовувати

493
00:28:38,236 --> 00:28:40,957
дані про частоту слів, щоб вибрати цей попередній розподіл, це 3.

494
00:28:40,957 --> 00:28:43,827
43, ймовірно, дає максимум того, наскільки добре ми можемо отримати

495
00:28:43,827 --> 00:28:46,360
з цим, або принаймні, наскільки добре я можу отримати з цим.

496
00:28:46,360 --> 00:28:50,878
Ця найкраща продуктивність, по суті, просто використовує ідеї, про які я тут говорив,

497
00:28:50,878 --> 00:28:55,397
але йде трохи далі, ніби шукає очікувану інформацію на два кроки вперед, а не лише на

498
00:28:55,397 --> 00:28:55,660
один.

499
00:28:55,660 --> 00:28:58,170
Спочатку я планував більше поговорити про це, але

500
00:28:58,170 --> 00:29:00,580
я розумію, що насправді ми пройшли досить довго.

501
00:29:00,580 --> 00:29:03,379
Єдине, що я скажу: після цього двоетапного пошуку, а потім

502
00:29:03,379 --> 00:29:06,131
запустивши пару зразків симуляцій у найкращих кандидатах,

503
00:29:06,131 --> 00:29:09,500
наразі, принаймні для мене, здається, що Crane є найкращим відкривачем.

504
00:29:09,500 --> 00:29:11,080
Хто б міг здогадатися?

505
00:29:11,080 --> 00:29:14,512
Крім того, якщо ви використовуєте справжній список слів для визначення простору

506
00:29:14,512 --> 00:29:18,160
можливостей, тоді невизначеність, з якої ви починаєте, становить трохи більше 11 біт.

507
00:29:18,160 --> 00:29:22,610
І виявилося, що лише під час грубого пошуку максимально можлива очікувана

508
00:29:22,610 --> 00:29:26,580
інформація після перших двох припущень становить приблизно 10 біт.

509
00:29:26,580 --> 00:29:30,608
Це свідчить про те, що в найкращому випадку, після ваших перших двох

510
00:29:30,608 --> 00:29:35,220
припущень, з ідеально оптимальною грою, ви залишитеся з дещицею невизначеності.

511
00:29:35,220 --> 00:29:37,400
Це те саме, що мати два можливі припущення.

512
00:29:37,400 --> 00:29:40,684
Тож я вважаю справедливим і, ймовірно, досить консервативним сказати, що ви ніколи

513
00:29:40,684 --> 00:29:43,811
не зможете написати алгоритм, який отримає це середнє значення до 3, тому що з

514
00:29:43,811 --> 00:29:46,977
доступними вам словами просто не буде місця для отримання достатньої інформації

515
00:29:46,977 --> 00:29:50,460
лише за два кроки здатний гарантувати відповідь у третьому слоті кожного разу без збоїв.

