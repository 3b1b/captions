1
00:00:00,000 --> 00:00:03,252
Гра Wurdle стала досить вірусною за останній місяць чи два,

2
00:00:03,252 --> 00:00:07,319
і я ніколи не пропускав можливість уроку математики, мені спадає на думку,

3
00:00:07,319 --> 00:00:11,330
що ця гра є дуже хорошим центральним прикладом уроку з теорії інформації,

4
00:00:11,330 --> 00:00:13,120
зокрема тема, відома як ентропія.

5
00:00:13,120 --> 00:00:16,480
Розумієте, як і багатьох людей, мене затягнуло головоломкою,

6
00:00:16,480 --> 00:00:20,996
і, як і багатьох програмістів, я також був затягнутий у спробі написати алгоритм,

7
00:00:20,996 --> 00:00:23,200
який би грав у гру якомога оптимальніше.

8
00:00:23,200 --> 00:00:26,067
І те, що я думав зробити тут, це просто поговорити з вами про

9
00:00:26,067 --> 00:00:29,443
мій процес і пояснити деякі математичні обчислення, які в нього входять,

10
00:00:29,443 --> 00:00:32,080
оскільки весь алгоритм зосереджений на цій ідеї ентропії.

11
00:00:32,080 --> 00:00:42,180
Перш за все, якщо ви не чули про це, що таке Wurdle?

12
00:00:42,180 --> 00:00:45,542
І щоб убити двох зайців одним пострілом, поки ми проходимо правила гри,

13
00:00:45,542 --> 00:00:48,110
дозвольте мені також переглянути, куди ми йдемо з цим,

14
00:00:48,110 --> 00:00:51,380
тобто розробити маленький алгоритм, який, в основному, гратиме за нас.

15
00:00:51,380 --> 00:00:55,860
Хоча я не робив сьогодні Wurdle, це 4 лютого, і ми побачимо, як бот впорається.

16
00:00:55,860 --> 00:00:58,625
Мета Wurdle — вгадати таємниче слово з п’яти літер,

17
00:00:58,625 --> 00:01:00,860
і вам дається шість різних шансів вгадати.

18
00:01:00,860 --> 00:01:05,240
Наприклад, мій бот Wurdle пропонує мені почати з журавля.

19
00:01:05,240 --> 00:01:08,465
Кожного разу, коли ви припускаєте, ви отримуєте певну інформацію про те,

20
00:01:08,465 --> 00:01:10,940
наскільки близькі ваші припущення до істинної відповіді.

21
00:01:10,940 --> 00:01:14,540
Тут сірий квадрат говорить мені, що у фактичній відповіді немає C.

22
00:01:14,540 --> 00:01:18,340
Жовте поле говорить мені, що є R, але воно не в цьому положенні.

23
00:01:18,340 --> 00:01:21,414
Зелений квадрат говорить мені, що секретне слово дійсно має літеру А,

24
00:01:21,414 --> 00:01:22,820
і воно стоїть на третій позиції.

25
00:01:22,820 --> 00:01:24,300
І тоді немає ні N, ні E.

26
00:01:24,300 --> 00:01:27,420
Тож дозвольте мені просто зайти та повідомити боту Wurdle цю інформацію.

27
00:01:27,420 --> 00:01:31,500
Ми почали з крана, ми отримали сірий, жовтий, зелений, сірий, сірий.

28
00:01:31,500 --> 00:01:35,460
Не турбуйтеся про всі дані, які він зараз показує, я поясню це свого часу.

29
00:01:35,460 --> 00:01:39,700
Але його найкраща пропозиція для нашого другого вибору є химерною.

30
00:01:39,700 --> 00:01:42,983
І ваше припущення має бути справжнім словом із п’яти літер, але, як ви побачите,

31
00:01:42,983 --> 00:01:45,700
це досить ліберально щодо того, що насправді дозволить вам вгадати.

32
00:01:45,700 --> 00:01:48,860
У цьому випадку ми намагаємося shtick.

33
00:01:48,860 --> 00:01:50,260
І добре, все виглядає досить добре.

34
00:01:50,260 --> 00:01:54,740
Ми натиснули S і H, тому ми знаємо перші три літери, ми знаємо, що є R.

35
00:01:54,740 --> 00:01:59,740
Тож це буде як SHA щось R або SHA R щось.

36
00:01:59,740 --> 00:02:05,220
І, схоже, бот Wurdle знає, що він має лише дві можливості: shard або sharp.

37
00:02:05,220 --> 00:02:08,152
На даний момент це щось на кшталт суперечки між ними, тож я думаю,

38
00:02:08,152 --> 00:02:11,260
що, мабуть, лише тому, що він алфавітний, він поєднується з фрагментом.

39
00:02:11,260 --> 00:02:13,000
Ура, ось справжня відповідь.

40
00:02:13,000 --> 00:02:14,660
Тож ми отримали це за три.

41
00:02:14,660 --> 00:02:17,876
Якщо вам цікаво, чи це добре, я почув фразу однієї людини,

42
00:02:17,876 --> 00:02:20,820
що з Wurdle чотири — це рівномірно, а три — це пташка.

43
00:02:20,820 --> 00:02:22,960
Що, на мою думку, є досить влучною аналогією.

44
00:02:22,960 --> 00:02:27,560
Ви повинні бути постійно в своїй грі, щоб отримати чотири, але це точно не божевілля.

45
00:02:27,560 --> 00:02:30,000
Але коли ви отримуєте це за три, це просто чудово.

46
00:02:30,000 --> 00:02:33,425
Отже, якщо ви не за це, я хотів би просто поговорити про мій процес

47
00:02:33,425 --> 00:02:36,600
мислення з самого початку про те, як я підходжу до бота Wurdle.

48
00:02:36,600 --> 00:02:39,800
І, як я вже сказав, це дійсно привід для уроку теорії інформації.

49
00:02:39,800 --> 00:02:48,560
Основна мета – пояснити, що таке інформація, а що таке ентропія.

50
00:02:48,560 --> 00:02:51,130
Моєю першою думкою при підході до цього було поглянути

51
00:02:51,130 --> 00:02:53,560
на відносні частоти різних літер в англійській мові.

52
00:02:53,560 --> 00:02:57,790
Тож я подумав: гаразд, чи є початкове припущення чи початкова пара припущень,

53
00:02:57,790 --> 00:02:59,960
яка вражає багато цих найчастіших літер?

54
00:02:59,960 --> 00:03:03,780
І один, який я дуже любив, робив інший, а потім цвяхи.

55
00:03:03,780 --> 00:03:05,745
Думка полягає в тому, що якщо ви натискаєте букву,

56
00:03:05,745 --> 00:03:07,980
ви знаєте, ви отримуєте зелену або жовту, це завжди добре.

57
00:03:07,980 --> 00:03:09,460
Таке відчуття, що ви отримуєте інформацію.

58
00:03:09,460 --> 00:03:12,643
Але в цих випадках, навіть якщо ви не влучаєте і завжди отримуєте сірі,

59
00:03:12,643 --> 00:03:16,225
це все одно дає вам багато інформації, оскільки досить рідко можна знайти слово,

60
00:03:16,225 --> 00:03:17,640
у якому немає жодної з цих букв.

61
00:03:17,640 --> 00:03:21,008
Але все одно це не виглядає надсистематичним, тому що,

62
00:03:21,008 --> 00:03:23,520
наприклад, не враховується порядок літер.

63
00:03:23,520 --> 00:03:26,080
Навіщо друкувати цвяхи, коли я можу надрукувати равлика?

64
00:03:26,080 --> 00:03:27,720
Чи краще мати це S в кінці?

65
00:03:27,720 --> 00:03:28,720
Я не дуже впевнений.

66
00:03:28,720 --> 00:03:32,615
Тепер мій друг сказав, що йому подобається починати словом weary,

67
00:03:32,615 --> 00:03:37,160
що мене дещо здивувало, оскільки там є деякі незвичайні літери, як-от W та Y.

68
00:03:37,160 --> 00:03:39,400
Але хто знає, можливо, це кращий відкривач.

69
00:03:39,400 --> 00:03:42,497
Чи існує якась кількісна оцінка, яку ми можемо надати,

70
00:03:42,497 --> 00:03:44,920
щоб оцінити якість потенційного припущення?

71
00:03:44,920 --> 00:03:48,294
Тепер, щоб підготуватися до того, як ми будемо ранжувати можливі припущення,

72
00:03:48,294 --> 00:03:51,800
давайте повернемося назад і внесемо трохи ясності в те, як саме налаштована гра.

73
00:03:51,800 --> 00:03:56,084
Отже, є список слів, які можна ввести, які вважаються дійсними припущеннями,

74
00:03:56,084 --> 00:03:57,920
і складається лише з 13 000 слів.

75
00:03:57,920 --> 00:04:02,095
Але якщо ви подивитесь на це, ви побачите багато справді незвичайних речей,

76
00:04:02,095 --> 00:04:07,040
таких як голова або Алі та ARG, тип слів, які викликають сімейні суперечки в грі в Скрабл.

77
00:04:07,040 --> 00:04:10,600
Але настрій гри полягає в тому, що відповіддю завжди буде досить поширене слово.

78
00:04:10,600 --> 00:04:16,080
І насправді, є ще один список із приблизно 2300 слів, які є можливими відповідями.

79
00:04:16,080 --> 00:04:21,800
І це список, складений людьми, я думаю, саме дівчиною творця гри, що дуже весело.

80
00:04:21,800 --> 00:04:25,266
Але що я хотів би зробити, наше завдання для цього проекту полягає в тому,

81
00:04:25,266 --> 00:04:28,409
щоб побачити, чи зможемо ми написати програму, що розв’язує Wordle,

82
00:04:28,409 --> 00:04:30,720
яка не включатиме попередні знання про цей список.

83
00:04:30,720 --> 00:04:33,718
По-перше, є багато досить поширених слів із п’яти літер,

84
00:04:33,718 --> 00:04:35,560
яких ви не знайдете в цьому списку.

85
00:04:35,560 --> 00:04:38,667
Тож було б краще написати програму, яка була б трішки стійкішою та

86
00:04:38,667 --> 00:04:41,960
грала б у Wordle проти будь-кого, а не лише проти офіційного веб-сайту.

87
00:04:41,960 --> 00:04:45,266
А також причина, чому ми знаємо, що таке цей список можливих відповідей,

88
00:04:45,266 --> 00:04:47,440
полягає в тому, що він видимий у вихідному коді.

89
00:04:47,440 --> 00:04:50,594
Але те, як це видно у вихідному коді, — це певний порядок,

90
00:04:50,594 --> 00:04:52,840
у якому відповіді з’являються день у день.

91
00:04:52,840 --> 00:04:56,400
Тож ви завжди можете просто подивитися, якою буде завтрашня відповідь.

92
00:04:56,400 --> 00:04:59,140
Очевидно, що використання списку є обманом.

93
00:04:59,140 --> 00:05:02,679
І те, що робить головоломку цікавішою та насиченішим уроком теорії інформації,

94
00:05:02,679 --> 00:05:06,353
полягає в тому, що натомість можна використовувати деякі більш універсальні дані,

95
00:05:06,353 --> 00:05:09,579
такі як відносна частота слів у цілому, щоб вловити цю інтуїцію про те,

96
00:05:09,579 --> 00:05:11,640
що ми надаємо перевагу більш поширеним словам.

97
00:05:11,640 --> 00:05:16,560
Тож як із цих 13 000 можливостей вибрати початкове припущення?

98
00:05:16,560 --> 00:05:19,960
Наприклад, якщо мій друг пропонує втомленого, як ми повинні проаналізувати його якість?

99
00:05:19,960 --> 00:05:22,967
Що ж, причина, чому він сказав, що йому подобається це малоймовірне W,

100
00:05:22,967 --> 00:05:25,677
полягає в тому, що йому подобається довгострокова природа того,

101
00:05:25,677 --> 00:05:27,880
наскільки добре це відчуваєш, якщо ти влучив у це W.

102
00:05:27,880 --> 00:05:32,274
Наприклад, якщо перша виявлена закономірність була приблизно такою, то виявиться,

103
00:05:32,274 --> 00:05:36,080
що в цьому гігантському лексиконі лише 58 слів відповідають цій моделі.

104
00:05:36,080 --> 00:05:38,900
Тож це величезне зниження з 13 000.

105
00:05:38,900 --> 00:05:41,489
Але зворотна сторона цього, звичайно, полягає в тому,

106
00:05:41,489 --> 00:05:43,360
що дуже рідко отримати такий візерунок.

107
00:05:43,360 --> 00:05:47,051
Зокрема, якби кожне слово з однаковою ймовірністю було відповіддю,

108
00:05:47,051 --> 00:05:51,680
ймовірність досягнення цього шаблону дорівнювала б 58 поділеним приблизно на 13 000.

109
00:05:51,680 --> 00:05:53,880
Звичайно, вони не однаково ймовірні відповіді.

110
00:05:53,880 --> 00:05:56,680
Більшість із них дуже незрозумілі та навіть сумнівні слова.

111
00:05:56,680 --> 00:05:59,620
Але принаймні для нашого першого проходження всього цього, давайте припустимо,

112
00:05:59,620 --> 00:06:02,040
що всі вони однаково ймовірні, а потім уточнимо це трохи пізніше.

113
00:06:02,040 --> 00:06:07,360
Справа в тому, що шаблон із великою кількістю інформації за своєю природою малоймовірний.

114
00:06:07,360 --> 00:06:11,920
Насправді бути інформативним означає те, що це малоймовірно.

115
00:06:11,920 --> 00:06:16,690
Набагато більш вірогідною схемою для цього відкриття буде щось на кшталт цього,

116
00:06:16,690 --> 00:06:18,360
де, звичайно, немає букви W.

117
00:06:18,360 --> 00:06:22,080
Можливо, там є E, а можливо, немає A, немає R, немає Y.

118
00:06:22,080 --> 00:06:24,640
У цьому випадку є 1400 можливих збігів.

119
00:06:24,640 --> 00:06:28,954
Якби все було однаково вірогідним, виходить, що ймовірність приблизно 11% того,

120
00:06:28,954 --> 00:06:30,680
що ви б побачили саме цю модель.

121
00:06:30,680 --> 00:06:34,320
Таким чином, найімовірніші результати також є найменш інформативними.

122
00:06:34,320 --> 00:06:38,186
Щоб отримати більш глобальне уявлення, дозвольте мені показати вам повний

123
00:06:38,186 --> 00:06:42,000
розподіл ймовірностей за всіма різними шаблонами, які ви можете побачити.

124
00:06:42,000 --> 00:06:46,404
Таким чином, кожна смужка, на яку ви дивитеся, відповідає можливому шаблону кольорів,

125
00:06:46,404 --> 00:06:50,808
який можна виявити, з яких є від 3 до 5 варіантів, і вони організовані зліва направо,

126
00:06:50,808 --> 00:06:52,960
від найпоширенішого до найменш поширеного.

127
00:06:52,960 --> 00:06:56,200
Отже, найпоширенішою можливістю є те, що ви отримаєте всі сірі.

128
00:06:56,200 --> 00:06:58,800
Це відбувається приблизно в 14% випадків.

129
00:06:58,800 --> 00:07:01,862
І те, на що ви сподіваєтеся, коли ви припускаєте, це те,

130
00:07:01,862 --> 00:07:04,709
що ви опинитеся десь у цьому довгому хвості, як тут,

131
00:07:04,709 --> 00:07:08,469
де є лише 18 можливостей для того, що відповідає цьому шаблону, який,

132
00:07:08,469 --> 00:07:09,920
очевидно, виглядає ось так.

133
00:07:09,920 --> 00:07:14,080
Або якщо ми підемо трохи ліворуч, то, можливо, ми підемо сюди.

134
00:07:14,080 --> 00:07:16,560
Добре, ось вам гарна головоломка.

135
00:07:16,560 --> 00:07:19,592
Які три слова в англійській мові починаються з літери W,

136
00:07:19,592 --> 00:07:22,040
закінчуються літерою Y і десь містять букву R?

137
00:07:22,040 --> 00:07:27,560
Виявляється, відповіді, давайте подивимося, багатослівні, червиві та іронічні.

138
00:07:27,560 --> 00:07:30,586
Отже, щоб оцінити, наскільки добре це слово в цілому,

139
00:07:30,586 --> 00:07:33,725
ми хочемо якийсь вимір очікуваної кількості інформації,

140
00:07:33,725 --> 00:07:36,360
яку ви збираєтеся отримати від цього розподілу.

141
00:07:36,360 --> 00:07:41,415
Якщо ми переглянемо кожен шаблон і помножимо його ймовірність появи на те,

142
00:07:41,415 --> 00:07:46,000
що вимірює його інформативність, це може дати нам об’єктивну оцінку.

143
00:07:46,000 --> 00:07:50,280
Тепер вашим першим інстинктом щодо того, що це має бути, може бути кількість збігів.

144
00:07:50,280 --> 00:07:52,960
Вам потрібна менша середня кількість збігів.

145
00:07:52,960 --> 00:07:58,029
Але натомість я хотів би використовувати більш універсальне вимірювання,

146
00:07:58,029 --> 00:08:02,752
яке ми часто приписуємо інформації, і таке, яке буде більш гнучким,

147
00:08:02,752 --> 00:08:08,585
коли ми матимемо різну ймовірність, призначену кожному з цих 13 000 слів щодо того,

148
00:08:08,585 --> 00:08:10,600
чи є вони справді відповіддю.

149
00:08:10,600 --> 00:08:14,054
Стандартною одиницею інформації є біт, який має трохи кумедну формулу,

150
00:08:14,054 --> 00:08:17,800
але вона справді інтуїтивно зрозуміла, якщо ми просто подивимося на приклади.

151
00:08:17,800 --> 00:08:21,693
Якщо у вас є спостереження, яке вдвічі скорочує ваш простір можливостей,

152
00:08:21,693 --> 00:08:24,200
ми кажемо, що воно містить один біт інформації.

153
00:08:24,200 --> 00:08:27,698
У нашому прикладі простір можливостей — це всі можливі слова, і виявляється,

154
00:08:27,698 --> 00:08:31,560
що близько половини слів із п’яти літер мають S, трохи менше, але приблизно половина.

155
00:08:31,560 --> 00:08:35,200
Таким чином, це спостереження дасть вам трохи інформації.

156
00:08:35,200 --> 00:08:39,324
Якщо натомість новий факт скорочує цей простір можливостей у чотири рази,

157
00:08:39,324 --> 00:08:42,000
ми говоримо, що він містить два біти інформації.

158
00:08:42,000 --> 00:08:45,120
Наприклад, виявилося, що приблизно чверть цих слів мають Т.

159
00:08:45,120 --> 00:08:47,680
Якщо спостереження скорочує цей простір у вісім,

160
00:08:47,680 --> 00:08:50,920
ми говоримо, що це три біти інформації, і так далі і так далі.

161
00:08:50,920 --> 00:08:55,000
Чотири біти перетворюють його на 16-й, п’ять бітів — на 32-й.

162
00:08:55,000 --> 00:08:58,685
Тож тепер ви можете зупинитись і запитати себе,

163
00:08:58,685 --> 00:09:04,520
яка формула для інформації для кількості бітів у термінах ймовірності появи?

164
00:09:04,520 --> 00:09:08,456
Ми маємо на увазі те, що коли ви берете одну половину на кількість бітів,

165
00:09:08,456 --> 00:09:11,222
це те саме, що ймовірність, що те саме, що сказати,

166
00:09:11,222 --> 00:09:14,467
що два в степені кількості бітів є одиницею над ймовірністю,

167
00:09:14,467 --> 00:09:18,296
що далі переставляє, кажучи, що інформація є двома логарифмами одиниці,

168
00:09:18,296 --> 00:09:19,680
поділеними на ймовірність.

169
00:09:19,680 --> 00:09:22,369
І іноді ви бачите це з ще одним перевпорядкуванням,

170
00:09:22,369 --> 00:09:25,680
де інформація є від’ємним логарифмом за основою два ймовірності.

171
00:09:25,680 --> 00:09:29,254
У такому вигляді це може здатися трохи дивним для непосвячених,

172
00:09:29,254 --> 00:09:32,327
але насправді це просто дуже інтуїтивна ідея запитати,

173
00:09:32,327 --> 00:09:35,120
скільки разів ви скоротили свої можливості вдвічі.

174
00:09:35,120 --> 00:09:38,731
А тепер, якщо вам цікаво, знаєте, я думав, що ми просто граємо у веселу гру слів,

175
00:09:38,731 --> 00:09:39,920
чому логарифми з’являються?

176
00:09:39,920 --> 00:09:42,860
Одна з причин, чому це краща одиниця, полягає в тому,

177
00:09:42,860 --> 00:09:47,380
що набагато простіше говорити про дуже малоймовірні події, набагато легше сказати,

178
00:09:47,380 --> 00:09:50,484
що спостереження містить 20 біт інформації, ніж сказати,

179
00:09:50,484 --> 00:09:53,480
що ймовірність такого-то виникнення дорівнює 0.0000095.

180
00:09:53,480 --> 00:09:57,768
Але більш суттєвою причиною того, що цей логарифмічний вираз виявився дуже

181
00:09:57,768 --> 00:10:02,000
корисним доповненням до теорії ймовірності, є спосіб додавання інформації.

182
00:10:02,000 --> 00:10:05,437
Наприклад, якщо одне спостереження дає вам два біти інформації,

183
00:10:05,437 --> 00:10:08,766
скорочуючи ваш простір учетверо, а потім друге спостереження,

184
00:10:08,766 --> 00:10:13,009
подібне до вашого другого припущення в Wordle, дає вам ще три біти інформації,

185
00:10:13,009 --> 00:10:17,360
скорочуючи вас ще на один коефіцієнт вісім, два разом дають п’ять біт інформації.

186
00:10:17,360 --> 00:10:21,200
Подібно до того, як ймовірності люблять множитися, інформація любить додаватися.

187
00:10:21,200 --> 00:10:25,050
Отже, як тільки ми потрапляємо в область чогось на зразок очікуваного значення,

188
00:10:25,050 --> 00:10:28,660
де ми додаємо купу чисел, журнали роблять це набагато зручнішим для роботи.

189
00:10:28,660 --> 00:10:32,692
Давайте повернемося до нашого розподілу для Weary і додамо сюди ще один маленький трекер,

190
00:10:32,692 --> 00:10:35,560
який показуватиме нам, скільки інформації є для кожного шаблону.

191
00:10:35,560 --> 00:10:39,039
Головне, на що я хочу, щоб ви звернули увагу, це те, що чим вища ймовірність,

192
00:10:39,039 --> 00:10:42,206
коли ми дійдемо до тих більш вірогідних моделей, тим менше інформації,

193
00:10:42,206 --> 00:10:43,500
тим менше бітів ви отримуєте.

194
00:10:43,500 --> 00:10:46,360
Спосіб вимірювання якості цього припущення полягає в тому,

195
00:10:46,360 --> 00:10:50,044
щоб взяти очікуване значення цієї інформації, де ми проходимо кожен шаблон,

196
00:10:50,044 --> 00:10:54,115
говоримо, наскільки це ймовірно, а потім ми множимо це на кількість біт інформації,

197
00:10:54,115 --> 00:10:54,940
яку ми отримуємо.

198
00:10:54,940 --> 00:10:58,480
А у прикладі Вірі це виявляється 4.9 біт.

199
00:10:58,480 --> 00:11:01,800
Отже, у середньому інформація, яку ви отримуєте з цього початкового припущення,

200
00:11:01,800 --> 00:11:05,410
настільки ж хороша, як скорочення вашого простору можливостей навпіл приблизно в п’ять

201
00:11:05,410 --> 00:11:05,660
разів.

202
00:11:05,660 --> 00:11:09,221
Навпаки, прикладом припущення з вищим очікуваним

203
00:11:09,221 --> 00:11:13,220
інформаційним значенням може бути щось на зразок Slate.

204
00:11:13,220 --> 00:11:16,180
У цьому випадку ви помітите, що розподіл виглядає набагато більш плоским.

205
00:11:16,180 --> 00:11:22,398
Зокрема, найімовірніша поява всіх сірих має лише близько 6% ймовірності появи,

206
00:11:22,398 --> 00:11:25,940
тому ви отримуєте мінімум 3.9 біт інформації.

207
00:11:25,940 --> 00:11:29,140
Але це мінімум, зазвичай ви отримаєте щось краще за це.

208
00:11:29,140 --> 00:11:34,141
І виявляється, коли ви обчислюєте цифри на цьому місці та додаєте всі відповідні терміни,

209
00:11:34,141 --> 00:11:36,420
середня інформація становить близько 5.8.

210
00:11:36,420 --> 00:11:39,937
Отже, на відміну від Вірі, ваш простір можливостей буде в

211
00:11:39,937 --> 00:11:43,940
середньому приблизно вдвічі менший після цього першого припущення.

212
00:11:43,940 --> 00:11:49,540
Насправді є весела історія про назву цього очікуваного значення кількості інформації.

213
00:11:49,540 --> 00:11:53,501
Теорію інформації розробив Клод Шеннон, який працював у Bell Labs у 1940-х роках,

214
00:11:53,501 --> 00:11:56,884
але він говорив про деякі зі своїх ідей, які ще не були опубліковані,

215
00:11:56,884 --> 00:12:00,362
з Джоном фон Нейманом, який був цим інтелектуальним гігантом того часу,

216
00:12:00,362 --> 00:12:04,180
дуже видатним у математиці та фізиці та початках того, що ставало інформатикою.

217
00:12:04,180 --> 00:12:07,546
І коли він згадав, що він насправді не має вдалого імені для

218
00:12:07,546 --> 00:12:11,684
цього очікуваного значення кількості інформації, фон Нейман нібито сказав,

219
00:12:11,684 --> 00:12:14,720
отже, ви повинні назвати це ентропією, і з двох причин.

220
00:12:14,720 --> 00:12:18,776
По-перше, ваша функція невизначеності використовувалася в статистичній механіці

221
00:12:18,776 --> 00:12:23,137
під такою назвою, тож вона вже має назву, а по-друге, що ще важливіше, ніхто не знає,

222
00:12:23,137 --> 00:12:26,940
що таке ентропія насправді, тому в дебатах ви завжди будете мають перевагу.

223
00:12:26,940 --> 00:12:33,420
Отже, якщо назва здається трохи загадковою, і якщо вірити цій історії, це начебто задум.

224
00:12:33,420 --> 00:12:37,730
Крім того, якщо вам цікаво його відношення до всього другого закону термодинаміки

225
00:12:37,730 --> 00:12:41,830
з фізики, зв’язок точно є, але в його витоках Шеннон мав справу лише з чистою

226
00:12:41,830 --> 00:12:46,088
теорією ймовірностей, і для наших цілей тут, коли я використовую слово ентропія,

227
00:12:46,088 --> 00:12:50,820
я просто хочу, щоб ви подумали про очікувану інформаційну цінність конкретного припущення.

228
00:12:50,820 --> 00:12:54,380
Ви можете думати про ентропію як про вимірювання двох речей одночасно.

229
00:12:54,380 --> 00:12:57,420
Перше — це те, наскільки плоским є розподіл.

230
00:12:57,420 --> 00:13:01,700
Чим ближче рівномірний розподіл, тим вищою буде ентропія.

231
00:13:01,700 --> 00:13:05,585
У нашому випадку, коли існує від 3 до 5-го загальних шаблонів,

232
00:13:05,585 --> 00:13:10,951
для рівномірного розподілу, спостереження за будь-яким із них матиме інформаційну базу

233
00:13:10,951 --> 00:13:15,207
журналу 2 з 3 до 5-го, що дорівнює 7.92, тож це абсолютний максимум,

234
00:13:15,207 --> 00:13:17,860
який ви могли б отримати для цієї ентропії.

235
00:13:17,860 --> 00:13:22,900
Але ентропія також є своєрідним показником того, скільки можливостей існує.

236
00:13:22,900 --> 00:13:27,346
Наприклад, якщо у вас є якесь слово, де є лише 16 можливих шаблонів,

237
00:13:27,346 --> 00:13:32,760
і кожен з них однаково вірогідний, ця ентропія, ця очікувана інформація буде 4 біти.

238
00:13:32,760 --> 00:13:37,201
Але якщо у вас є інше слово, де є 64 можливі шаблони, які можуть виникнути,

239
00:13:37,201 --> 00:13:41,000
і всі вони однаково вірогідні, тоді ентропія буде складати 6 біт.

240
00:13:41,000 --> 00:13:45,833
Отже, якщо ви бачите якийсь розподіл у дикій природі, який має ентропію 6 біт,

241
00:13:45,833 --> 00:13:49,015
це ніби говорить про те, що в тому, що має статися,

242
00:13:49,015 --> 00:13:54,400
існує стільки варіацій і невизначеності, як якщо б було 64 однаково ймовірні результати.

243
00:13:54,400 --> 00:13:58,360
Під час мого першого проходу в Wurtelebot я просто зробив це.

244
00:13:58,360 --> 00:14:02,949
Він переглядає всі можливі припущення, які ви можете мати, усі 13 000 слів,

245
00:14:02,949 --> 00:14:08,383
обчислює ентропію для кожного з них, або, точніше, ентропію розподілу за всіма шаблонами,

246
00:14:08,383 --> 00:14:13,154
які ви можете побачити, для кожного з них, і вибирає найвище, оскільки це той,

247
00:14:13,154 --> 00:14:17,200
який, швидше за все, максимально скоротить ваш простір можливостей.

248
00:14:17,200 --> 00:14:19,671
І незважаючи на те, що я говорив тут лише про перше припущення,

249
00:14:19,671 --> 00:14:21,680
воно робить те саме для кількох наступних припущень.

250
00:14:21,680 --> 00:14:25,172
Наприклад, після того, як ви бачите певний шаблон у цій першій здогадці,

251
00:14:25,172 --> 00:14:28,329
яка обмежила б вас меншою кількістю можливих слів на основі того,

252
00:14:28,329 --> 00:14:32,300
що з цим збігається, ви просто граєте в ту саму гру щодо цього меншого набору слів.

253
00:14:32,300 --> 00:14:37,126
Для запропонованого другого припущення ви дивитеся на розподіл усіх шаблонів,

254
00:14:37,126 --> 00:14:40,715
які можуть виникати з цього більш обмеженого набору слів,

255
00:14:40,715 --> 00:14:45,480
ви шукаєте всі 13 000 можливостей і знаходите ту, яка максимізує цю ентропію.

256
00:14:45,480 --> 00:14:49,833
Щоб показати вам, як це працює в дії, дозвольте мені просто витягнути невеликий

257
00:14:49,833 --> 00:14:54,460
варіант Wurtele, який я написав, який показує основні моменти цього аналізу на полях.

258
00:14:54,460 --> 00:14:57,997
Після виконання всіх обчислень ентропії, тут праворуч він показує нам,

259
00:14:57,997 --> 00:15:00,340
які з них мають найбільшу очікувану інформацію.

260
00:15:00,340 --> 00:15:04,883
Виявляється, головною відповіддю, принаймні на даний момент,

261
00:15:04,883 --> 00:15:11,140
ми уточнимо це пізніше, є Тарес, що означає, гм, звичайно, вика, найпоширеніша вика.

262
00:15:11,140 --> 00:15:13,704
Кожного разу, коли ми робимо припущення тут, де, можливо,

263
00:15:13,704 --> 00:15:17,153
я ігнорую його рекомендації та вибираю шифер, тому що мені подобається шифер,

264
00:15:17,153 --> 00:15:19,939
ми можемо побачити, скільки очікуваної інформації він містить,

265
00:15:19,939 --> 00:15:23,609
але праворуч від слова тут показано, скільки фактичну інформацію, яку ми отримали,

266
00:15:23,609 --> 00:15:24,980
враховуючи цю конкретну модель.

267
00:15:24,980 --> 00:15:27,455
Тож тут, схоже, нам трохи не пощастило, очікували,

268
00:15:27,455 --> 00:15:30,660
що ми отримаємо 5.8, але випадково ми отримали щось менше, ніж це.

269
00:15:30,660 --> 00:15:35,860
А потім ліворуч тут показано всі різні можливі слова, де ми зараз знаходимося.

270
00:15:35,860 --> 00:15:39,235
Сині смужки вказують на те, наскільки вірогідним є кожне слово,

271
00:15:39,235 --> 00:15:42,716
тому наразі припускається, що кожне слово буде однаково ймовірно,

272
00:15:42,716 --> 00:15:44,140
але ми уточнимо це за мить.

273
00:15:44,140 --> 00:15:47,874
І тоді це вимірювання невизначеності говорить нам про ентропію цього

274
00:15:47,874 --> 00:15:52,042
розподілу між можливими словами, що зараз, оскільки це рівномірний розподіл,

275
00:15:52,042 --> 00:15:55,940
є просто непотрібно складним способом підрахувати кількість можливостей.

276
00:15:55,940 --> 00:16:02,700
Наприклад, якби ми взяли 2 у ступінь 13.66, це має бути приблизно 13 000 можливостей.

277
00:16:02,700 --> 00:16:06,780
Я трохи відхилився, але лише тому, що не показую всі десяткові знаки.

278
00:16:06,780 --> 00:16:09,705
На даний момент це може здатися зайвим і занадто складним,

279
00:16:09,705 --> 00:16:12,780
але за хвилину ви зрозумієте, чому корисно мати обидва номери.

280
00:16:12,780 --> 00:16:17,722
Отже, тут виглядає так, ніби найвища ентропія для нашого другого припущення – Рамен,

281
00:16:17,722 --> 00:16:19,700
що знову ж таки не схоже на слово.

282
00:16:19,700 --> 00:16:25,660
Отже, щоб підвищити моральну позицію, я введу Rains.

283
00:16:25,660 --> 00:16:27,540
І знову схоже, що нам трохи не пощастило.

284
00:16:27,540 --> 00:16:32,100
Ми чекали 4.3 біти, а ми отримали лише 3.39 біт інформації.

285
00:16:32,100 --> 00:16:35,060
Отже, ми маємо 55 можливостей.

286
00:16:35,060 --> 00:16:40,200
І тут, можливо, я просто прийму те, що він пропонує, тобто комбо, що б це не означало.

287
00:16:40,200 --> 00:16:43,300
І гаразд, це насправді хороший шанс для головоломки.

288
00:16:43,300 --> 00:16:47,020
Це говорить нам, що цей шаблон дає нам 4.7 біт інформації.

289
00:16:47,020 --> 00:16:52,400
Але ліворуч, перш ніж ми побачимо цей шаблон, їх було 5.78 біт невизначеності.

290
00:16:52,400 --> 00:16:56,860
Отже, як вікторина для вас, що це означає щодо кількості можливостей, що залишилися?

291
00:16:56,860 --> 00:17:01,192
Ну, це означає, що ми зведені до однієї частки невизначеності,

292
00:17:01,192 --> 00:17:04,700
що те саме, що сказати, що є дві можливі відповіді.

293
00:17:04,700 --> 00:17:06,520
Це вибір 50 на 50.

294
00:17:06,520 --> 00:17:09,435
І звідси, оскільки ми з вами знаємо, які слова є більш поширеними,

295
00:17:09,435 --> 00:17:11,220
ми знаємо, що відповідь має бути безодня.

296
00:17:11,220 --> 00:17:13,540
Але, як зараз написано, програма цього не знає.

297
00:17:13,540 --> 00:17:17,069
Тож він просто продовжує, намагаючись отримати якомога більше інформації,

298
00:17:17,069 --> 00:17:20,360
доки не залишиться лише одна можливість, а потім здогадується про це.

299
00:17:20,360 --> 00:17:22,700
Отже, очевидно, нам потрібна краща стратегія завершення гри.

300
00:17:22,700 --> 00:17:26,988
Але, скажімо, ми назвемо цю версію одним із наших розв’язувачів Wordle,

301
00:17:26,988 --> 00:17:30,740
а потім запустимо кілька симуляцій, щоб побачити, як це працює.

302
00:17:30,740 --> 00:17:34,240
Отже, як це працює, це гра в усі можливі ігри зі словами.

303
00:17:34,240 --> 00:17:38,780
Він переглядає всі ці 2315 слів, які є фактичними відповідями Wordle.

304
00:17:38,780 --> 00:17:41,340
В основному це використовується як набір для тестування.

305
00:17:41,340 --> 00:17:44,624
І з цим наївним методом не брати до уваги, наскільки поширене слово,

306
00:17:44,624 --> 00:17:48,337
і просто намагатися максимізувати інформацію на кожному кроці на цьому шляху,

307
00:17:48,337 --> 00:17:50,480
доки не дійде до одного й лише одного вибору.

308
00:17:50,480 --> 00:17:55,100
Наприкінці симуляції середній бал виходить близько 4.124.

309
00:17:55,100 --> 00:17:59,780
Що непогано, чесно кажучи, я очікував, що буде гірше.

310
00:17:59,780 --> 00:18:03,040
Але люди, які грають у wordle, скажуть вам, що вони зазвичай можуть отримати його за 4.

311
00:18:03,040 --> 00:18:05,260
Справжнє завдання — отримати якомога більше за 3.

312
00:18:05,260 --> 00:18:08,920
Це досить великий стрибок між рахунком 4 і рахунком 3.

313
00:18:08,920 --> 00:18:17,683
Очевидний низький плід тут полягає в тому, щоб якимось чином врахувати,

314
00:18:17,683 --> 00:18:23,160
чи є слово загальним, і як саме ми це робимо.

315
00:18:23,160 --> 00:18:26,186
Я підійшов до цього, щоб отримати список відносних

316
00:18:26,186 --> 00:18:28,560
частот для всіх слів в англійській мові.

317
00:18:28,560 --> 00:18:31,760
І я щойно скористався функцією даних частоти слів Mathematica,

318
00:18:31,760 --> 00:18:35,520
яка сама одержує загальнодоступний набір даних Google Books English Ngram.

319
00:18:35,520 --> 00:18:37,760
І на це цікаво дивитися, наприклад, якщо ми відсортуємо

320
00:18:37,760 --> 00:18:40,120
його від найбільш поширених слів до найменш поширених слів.

321
00:18:40,120 --> 00:18:43,740
Очевидно, це найпоширеніші слова з 5 букв в англійській мові.

322
00:18:43,740 --> 00:18:46,480
А точніше, це 8 місце за поширеністю.

323
00:18:46,480 --> 00:18:49,440
Спочатку який, потім там і там.

324
00:18:49,440 --> 00:18:52,608
Перший сам по собі не перший, а 9-й, і цілком зрозуміло,

325
00:18:52,608 --> 00:18:57,443
що ці інші слова можуть зустрічатися частіше, де ті, що стоять після першого, є після,

326
00:18:57,443 --> 00:18:59,000
де, а ті, які є трохи рідше.

327
00:18:59,000 --> 00:19:01,558
Тепер, використовуючи ці дані для моделювання того,

328
00:19:01,558 --> 00:19:04,707
наскільки ймовірно кожне з цих слів буде остаточною відповіддю,

329
00:19:04,707 --> 00:19:07,020
це не повинно бути просто пропорційним частоті.

330
00:19:07,020 --> 00:19:10,676
Наприклад, який отримав оцінку 0.002 у цьому наборі даних,

331
00:19:10,676 --> 00:19:15,200
тоді як слово braid у певному сенсі приблизно в 1000 разів менш імовірне.

332
00:19:15,200 --> 00:19:19,400
Але обидва ці слова досить поширені, тому їх майже напевно варто розглянути.

333
00:19:19,400 --> 00:19:21,900
Отже, ми хочемо більше двійкового відсікання.

334
00:19:21,900 --> 00:19:25,675
Я пішов з цього приводу: уявити весь цей відсортований список слів,

335
00:19:25,675 --> 00:19:29,672
а потім розташувати його на осі х, а потім застосувати функцію sigmoid,

336
00:19:29,672 --> 00:19:34,225
яка є стандартним способом отримання функції, вихід якої в основному є двійковим,

337
00:19:34,225 --> 00:19:38,500
це або 0, або 1, але між ними є згладжування для цієї області невизначеності.

338
00:19:38,500 --> 00:19:43,139
Таким чином, по суті, ймовірність того, що я призначаю кожному слову для того,

339
00:19:43,139 --> 00:19:47,543
щоб потрапити в остаточний список, буде значенням сигмоїдної функції вище,

340
00:19:47,543 --> 00:19:49,540
де б воно не знаходилося на осі х.

341
00:19:49,540 --> 00:19:53,127
Тепер, очевидно, це залежить від кількох параметрів, наприклад,

342
00:19:53,127 --> 00:19:56,826
наскільки широкий простір на осі х заповнюють ці слова, визначає,

343
00:19:56,826 --> 00:20:00,301
наскільки поступово або круто ми знижуємося від 1 до 0, і те,

344
00:20:00,301 --> 00:20:03,160
де ми їх розташовуємо зліва направо, визначає межу.

345
00:20:03,160 --> 00:20:07,340
Чесно кажучи, те, як я це зробив, було просто облизати палець і тицьнути його на вітер.

346
00:20:07,340 --> 00:20:10,980
Я переглянув відсортований список і спробував знайти вікно, у якому,

347
00:20:10,980 --> 00:20:14,303
дивлячись на нього, я вирішив, що приблизно половина цих слів,

348
00:20:14,303 --> 00:20:17,680
швидше за все, є остаточною відповіддю, і використав це як межу.

349
00:20:17,680 --> 00:20:21,593
Коли ми маємо подібний розподіл між словами, це дає нам іншу ситуацію,

350
00:20:21,593 --> 00:20:24,460
коли ентропія стає цим дійсно корисним вимірюванням.

351
00:20:24,460 --> 00:20:28,519
Наприклад, скажімо, ми граємо в гру, і ми починаємо з моїх старих відкривачів,

352
00:20:28,519 --> 00:20:31,088
які були пером і цвяхами, і закінчуємо ситуацією,

353
00:20:31,088 --> 00:20:33,760
коли є чотири можливі слова, які відповідають цьому.

354
00:20:33,760 --> 00:20:36,440
І припустимо, ми вважаємо їх усіх однаково ймовірними.

355
00:20:36,440 --> 00:20:40,000
Дозвольте запитати вас, яка ентропія цього розподілу?

356
00:20:40,000 --> 00:20:45,082
Що ж, інформація, пов’язана з кожною з цих можливостей,

357
00:20:45,082 --> 00:20:50,800
буде базою журналу 2 з 4, оскільки кожна з них є 1 і 4, і це 2.

358
00:20:50,800 --> 00:20:52,780
Два біти інформації, чотири можливості.

359
00:20:52,780 --> 00:20:54,360
Все дуже добре і добре.

360
00:20:54,360 --> 00:20:58,320
Але що, якби я сказав вам, що насправді є більше чотирьох збігів?

361
00:20:58,320 --> 00:21:00,638
Насправді, коли ми переглядаємо повний список слів,

362
00:21:00,638 --> 00:21:02,600
ми знаходимо 16 слів, які йому відповідають.

363
00:21:02,600 --> 00:21:06,327
Але припустімо, що наша модель надає справді низьку ймовірність того,

364
00:21:06,327 --> 00:21:09,735
що ці інші 12 слів є остаточною відповіддю, приблизно 1 з 1000,

365
00:21:09,735 --> 00:21:11,440
тому що вони дійсно незрозумілі.

366
00:21:11,440 --> 00:21:15,480
Тепер дозвольте запитати вас, яка ентропія цього розподілу?

367
00:21:15,480 --> 00:21:19,506
Якби ентропія вимірювала лише кількість збігів, тоді можна було б очікувати,

368
00:21:19,506 --> 00:21:23,376
що це буде щось на кшталт логарифмічної бази 2 із 16, що дорівнюватиме 4,

369
00:21:23,376 --> 00:21:26,200
на два біти невизначеності більше, ніж ми мали раніше.

370
00:21:26,200 --> 00:21:30,320
Але, звісно, фактична невизначеність не дуже відрізняється від того, що ми мали раніше.

371
00:21:30,320 --> 00:21:33,359
Те, що є ці 12 справді незрозумілих слів, не означає,

372
00:21:33,359 --> 00:21:38,200
що було б ще більш дивно дізнатися, що остаточною відповіддю є, наприклад, чарівність.

373
00:21:38,200 --> 00:21:42,856
Отже, коли ви фактично виконуєте обчислення тут і додаєте ймовірність кожного випадку,

374
00:21:42,856 --> 00:21:45,960
помножену на відповідну інформацію, ви отримуєте 2.11 біт.

375
00:21:45,960 --> 00:21:49,975
Я просто кажу, що в основному це два біти, в основному ці чотири можливості,

376
00:21:49,975 --> 00:21:53,469
але є трохи більше невизначеності через усі ці малоймовірні події,

377
00:21:53,469 --> 00:21:57,120
хоча якби ви дізналися про них, ви б отримали з цього масу інформації.

378
00:21:57,120 --> 00:21:59,332
Отже, зменшуючи масштаб, це частина того, що робить

379
00:21:59,332 --> 00:22:01,800
Wordle таким гарним прикладом для уроку теорії інформації.

380
00:22:01,800 --> 00:22:05,280
Ми маємо ці два різні застосування ентропії.

381
00:22:05,280 --> 00:22:10,550
Перше говорить нам, яку інформацію ми очікуємо отримати від певного припущення,

382
00:22:10,550 --> 00:22:16,480
а друге говорить, чи можемо ми виміряти залишкову невизначеність серед усіх можливих слів.

383
00:22:16,480 --> 00:22:20,640
І я маю підкреслити, що в першому випадку, коли ми розглядаємо очікувану інформацію

384
00:22:20,640 --> 00:22:25,000
про припущення, як тільки ми маємо нерівну вагу слів, це впливає на обчислення ентропії.

385
00:22:25,000 --> 00:22:27,549
Наприклад, дозвольте мені знайти той самий випадок,

386
00:22:27,549 --> 00:22:30,441
який ми розглядали раніше, розподілу, пов’язаного з Weary,

387
00:22:30,441 --> 00:22:34,560
але цього разу з використанням нерівномірного розподілу між усіма можливими словами.

388
00:22:34,560 --> 00:22:39,360
Тож дозвольте мені поглянути, чи зможу я знайти тут частину, яка б це добре ілюструвала.

389
00:22:39,360 --> 00:22:42,480
Гаразд, ось це досить добре.

390
00:22:42,480 --> 00:22:45,700
Тут ми маємо два суміжних шаблони, які приблизно однаково вірогідні,

391
00:22:45,700 --> 00:22:49,480
але один із них, як нам сказали, містить 32 можливі слова, які йому відповідають.

392
00:22:49,480 --> 00:22:53,915
І якщо ми перевіримо, що це таке, це ті 32, які є дуже малоймовірними словами,

393
00:22:53,915 --> 00:22:55,600
якщо ви переглядаєте їх очима.

394
00:22:55,600 --> 00:22:58,833
Важко знайти будь-яку правдоподібну відповідь, можливо, крики,

395
00:22:58,833 --> 00:23:01,656
але якщо ми подивимося на сусідній шаблон у розподілі,

396
00:23:01,656 --> 00:23:04,736
який вважається приблизно таким же вірогідним, нам скажуть,

397
00:23:04,736 --> 00:23:08,123
що він має лише 8 можливих збігів, тобто чверть як багато збігів,

398
00:23:08,123 --> 00:23:09,920
але це приблизно так само ймовірно.

399
00:23:09,920 --> 00:23:12,520
І коли ми знайдемо ці сірники, ми зрозуміємо чому.

400
00:23:12,520 --> 00:23:17,840
Деякі з них є реальними правдоподібними відповідями, наприклад, кільце, гнів або стукіт.

401
00:23:17,840 --> 00:23:21,950
Щоб проілюструвати, як ми все це об’єднуємо, дозвольте мені витягнути тут версію

402
00:23:21,950 --> 00:23:25,960
2 Wordlebot, і там є дві або три основні відмінності від першої, яку ми бачили.

403
00:23:25,960 --> 00:23:30,114
По-перше, як я щойно сказав, спосіб, у який ми обчислюємо ці ентропії,

404
00:23:30,114 --> 00:23:35,145
ці очікувані значення інформації, тепер використовує точніший розподіл між шаблонами,

405
00:23:35,145 --> 00:23:39,300
який включає ймовірність того, що дане слово насправді буде відповіддю.

406
00:23:39,300 --> 00:23:44,160
Як це сталося, сльози все ще залишаються номером 1, хоча наступні трохи інші.

407
00:23:44,160 --> 00:23:46,788
По-друге, коли він ранжує свої найпопулярніші варіанти,

408
00:23:46,788 --> 00:23:50,825
він тепер зберігатиме модель ймовірності того, що кожне слово є фактичною відповіддю,

409
00:23:50,825 --> 00:23:53,454
і він включатиме це у своє рішення, яке легше побачити,

410
00:23:53,454 --> 00:23:55,520
коли ми матимемо кілька припущень щодо стіл.

411
00:23:55,520 --> 00:23:58,266
Знову ж таки, ігноруємо його рекомендацію, тому що

412
00:23:58,266 --> 00:24:01,120
ми не можемо дозволити машинам керувати нашим життям.

413
00:24:01,120 --> 00:24:04,903
І я вважаю, що я повинен згадати ще одну іншу річ, яка закінчилася ліворуч,

414
00:24:04,903 --> 00:24:07,292
що значення невизначеності, ця кількість бітів,

415
00:24:07,292 --> 00:24:10,080
більше не просто надлишкова з кількістю можливих збігів.

416
00:24:10,080 --> 00:24:15,421
Тепер, якщо ми витягнемо це вгору і порахуємо 2 до 8.02, що трохи вище 256,

417
00:24:15,421 --> 00:24:19,357
я думаю, 259, це говорить про те, що, незважаючи на те,

418
00:24:19,357 --> 00:24:22,661
що всього 526 слів відповідають цьому шаблону,

419
00:24:22,661 --> 00:24:26,667
рівень невизначеності більше схожий на той, який був би,

420
00:24:26,667 --> 00:24:29,760
якби було 259 однаково ймовірних результати.

421
00:24:29,760 --> 00:24:31,100
Ви можете думати про це так.

422
00:24:31,100 --> 00:24:34,658
Він знає, що borx не є відповіддю, те саме з yorts, zorl і zorus,

423
00:24:34,658 --> 00:24:37,840
тому це трохи менш невизначено, ніж у попередньому випадку.

424
00:24:37,840 --> 00:24:40,220
Ця кількість бітів буде меншою.

425
00:24:40,220 --> 00:24:44,920
І якщо я продовжу грати в гру, я уточню це парою припущень,

426
00:24:44,920 --> 00:24:48,680
які стосуються того, що я хотів би пояснити тут.

427
00:24:48,680 --> 00:24:51,582
За четвертим припущенням, якщо ви подивитеся на його найкращі варіанти,

428
00:24:51,582 --> 00:24:53,800
ви побачите, що це вже не просто максимізація ентропії.

429
00:24:53,800 --> 00:24:57,061
Отже, на даний момент технічно є сім можливостей,

430
00:24:57,061 --> 00:25:00,780
але єдині, які мають значний шанс, це гуртожиток і слова.

431
00:25:00,780 --> 00:25:05,089
І ви можете бачити, що він класифікує обидва вище за всі ці інші значення,

432
00:25:05,089 --> 00:25:07,560
що, строго кажучи, дасть більше інформації.

433
00:25:07,560 --> 00:25:10,270
У перший раз, коли я зробив це, я просто склав ці два числа,

434
00:25:10,270 --> 00:25:13,469
щоб виміряти якість кожного припущення, яке насправді спрацювало краще,

435
00:25:13,469 --> 00:25:14,580
ніж ви могли підозрювати.

436
00:25:14,580 --> 00:25:17,495
Але це справді не здавалося систематичним, і я впевнений, що є інші підходи,

437
00:25:17,495 --> 00:25:19,880
які люди могли б використати, але ось той, на який я зупинився.

438
00:25:19,880 --> 00:25:24,213
Якщо ми розглядаємо перспективу наступного припущення, як у цьому випадку слова,

439
00:25:24,213 --> 00:25:28,440
те, що нас справді хвилює, це очікуваний рахунок нашої гри, якщо ми це зробимо.

440
00:25:28,440 --> 00:25:32,287
І щоб обчислити цей очікуваний бал, ми кажемо, яка ймовірність того,

441
00:25:32,287 --> 00:25:36,080
що слова є фактичною відповіддю, яка на даний момент відповідає 58%.

442
00:25:36,080 --> 00:25:40,400
Ми кажемо, що з імовірністю 58% наш рахунок у цій грі буде 4.

443
00:25:40,400 --> 00:25:46,240
І тоді з імовірністю 1 мінус ці 58% наша оцінка буде більшою за ці 4.

444
00:25:46,240 --> 00:25:49,767
Скільки ще ми не знаємо, але ми можемо оцінити це на основі того,

445
00:25:49,767 --> 00:25:52,920
скільки невизначеності буде, коли ми дійдемо до цієї точки.

446
00:25:52,920 --> 00:25:56,600
Зокрема, на даний момент є 1.44 біти невизначеності.

447
00:25:56,600 --> 00:25:59,719
Якщо ми вгадуємо слова, це означає, що очікувана інформація,

448
00:25:59,719 --> 00:26:01,560
яку ми отримаємо, дорівнює 1.27 біт.

449
00:26:01,560 --> 00:26:04,269
Отже, якщо ми вгадуємо слова, ця різниця показує,

450
00:26:04,269 --> 00:26:08,280
скільки невизначеності ми, ймовірно, залишимо після того, як це станеться.

451
00:26:08,280 --> 00:26:10,970
Нам потрібна якась функція, яку я тут називаю f,

452
00:26:10,970 --> 00:26:13,880
яка пов’язує цю невизначеність із очікуваною оцінкою.

453
00:26:13,880 --> 00:26:18,266
І те, як це було зроблено, полягало в тому, що просто побудували групу даних

454
00:26:18,266 --> 00:26:21,513
із попередніх ігор на основі версії 1 бота, щоб сказати,

455
00:26:21,513 --> 00:26:25,615
яким був фактичний рахунок після різних моментів з певною дуже вимірною

456
00:26:25,615 --> 00:26:27,040
кількістю невизначеності.

457
00:26:27,040 --> 00:26:30,989
Наприклад, ці точки даних тут знаходяться вище значення приблизно 8.7

458
00:26:30,989 --> 00:26:34,092
або близько того кажуть для деяких ігор після моменту,

459
00:26:34,092 --> 00:26:37,478
коли було 8.7 біт невизначеності, знадобилося дві здогадки,

460
00:26:37,478 --> 00:26:39,340
щоб отримати остаточну відповідь.

461
00:26:39,340 --> 00:26:43,180
Для інших ігор потрібно було три відгадки, для інших ігор – чотири.

462
00:26:43,180 --> 00:26:47,010
Якщо ми перемістимося вліво тут, усі точки над нулем говорять про те,

463
00:26:47,010 --> 00:26:50,841
що будь-коли є нуль біт невизначеності, тобто є лише одна можливість,

464
00:26:50,841 --> 00:26:55,000
тоді необхідна кількість припущень завжди дорівнює лише одній, що заспокоює.

465
00:26:55,000 --> 00:26:58,308
Щоразу, коли була трішка невизначеності, тобто, по суті,

466
00:26:58,308 --> 00:27:02,546
зводилася лише до двох можливостей, іноді вимагалося ще одне припущення,

467
00:27:02,546 --> 00:27:03,940
іноді ще два припущення.

468
00:27:03,940 --> 00:27:05,980
І так далі і так далі тут.

469
00:27:05,980 --> 00:27:08,423
Можливо, дещо простіший спосіб візуалізувати ці

470
00:27:08,423 --> 00:27:11,020
дані — об’єднати їх разом і взяти середні значення.

471
00:27:11,020 --> 00:27:15,003
Наприклад, ця смужка тут говорить, що серед усіх пунктів,

472
00:27:15,003 --> 00:27:20,428
де ми мали хоча б трохи невизначеності, в середньому необхідна кількість нових

473
00:27:20,428 --> 00:27:22,420
припущень була приблизно 1.5.

474
00:27:22,420 --> 00:27:25,228
І смужка тут говорить, що серед усіх різних ігор,

475
00:27:25,228 --> 00:27:28,936
де в якийсь момент невизначеність була трохи вище чотирьох бітів,

476
00:27:28,936 --> 00:27:31,577
що схоже на звуження до 16 різних можливостей,

477
00:27:31,577 --> 00:27:36,240
то в середньому для цього потрібно трохи більше двох припущень з цієї точки вперед.

478
00:27:36,240 --> 00:27:38,717
І звідси я просто зробив регресію, щоб відповідати функції,

479
00:27:38,717 --> 00:27:40,080
яка здавалася розумною для цього.

480
00:27:40,080 --> 00:27:43,365
І пам’ятайте, що весь сенс будь-чого з цього полягає в тому,

481
00:27:43,365 --> 00:27:48,104
щоб ми могли кількісно оцінити цю інтуїцію: що більше інформації ми отримуємо зі слова,

482
00:27:48,104 --> 00:27:49,720
то нижчим буде очікуваний бал.

483
00:27:49,720 --> 00:27:55,338
Отже, це як версія 2.0, якщо ми повернемося назад і запустимо той самий набір симуляцій,

484
00:27:55,338 --> 00:27:59,820
зіставивши його з усіма 2315 можливими відповідями Wordle, як це вийде?

485
00:27:59,820 --> 00:28:04,060
Що ж, на відміну від нашої першої версії, вона точно краща, що заспокоює.

486
00:28:04,060 --> 00:28:07,585
Усе сказане та зроблене середній бал становить близько 3.6, хоча,

487
00:28:07,585 --> 00:28:10,630
на відміну від першої версії, вона кілька разів програє,

488
00:28:10,630 --> 00:28:12,820
і за цієї обставини вимагає більше шести.

489
00:28:12,820 --> 00:28:16,949
Мабуть тому, що бувають моменти, коли потрібно досягти мети,

490
00:28:16,949 --> 00:28:18,980
а не максимізувати інформацію.

491
00:28:18,980 --> 00:28:22,140
Отже, ми можемо зробити краще, ніж 3.6?

492
00:28:22,140 --> 00:28:23,260
Ми точно можемо.

493
00:28:23,260 --> 00:28:26,431
На початку я сказав, що найцікавіше спробувати не включати

494
00:28:26,431 --> 00:28:29,980
справжній список відповідей Wordle у спосіб побудови своєї моделі.

495
00:28:29,980 --> 00:28:33,078
Але якщо ми все-таки це включимо, найкраща продуктивність,

496
00:28:33,078 --> 00:28:35,180
яку я міг отримати, була приблизно 3.43.

497
00:28:35,180 --> 00:28:38,962
Отже, якщо ми спробуємо вийти більш складним, ніж просто використовувати дані про частоту

498
00:28:38,962 --> 00:28:42,367
слів, щоб вибрати цей попередній розподіл, це 3.43, ймовірно, дає максимум того,

499
00:28:42,367 --> 00:28:44,762
наскільки добре ми можемо отримати з цим, або принаймні,

500
00:28:44,762 --> 00:28:46,360
наскільки добре я можу отримати з цим.

501
00:28:46,360 --> 00:28:50,878
Ця найкраща продуктивність, по суті, просто використовує ідеї, про які я тут говорив,

502
00:28:50,878 --> 00:28:54,714
але йде трохи далі, ніби шукає очікувану інформацію на два кроки вперед,

503
00:28:54,714 --> 00:28:55,660
а не лише на один.

504
00:28:55,660 --> 00:28:58,722
Спочатку я планував більше поговорити про це, але я розумію,

505
00:28:58,722 --> 00:29:00,580
що насправді ми пройшли досить довго.

506
00:29:00,580 --> 00:29:02,999
Єдине, що я скажу: після цього двоетапного пошуку,

507
00:29:02,999 --> 00:29:06,131
а потім запустивши пару зразків симуляцій у найкращих кандидатах,

508
00:29:06,131 --> 00:29:09,500
наразі, принаймні для мене, здається, що Crane є найкращим відкривачем.

509
00:29:09,500 --> 00:29:11,080
Хто б міг здогадатися?

510
00:29:11,080 --> 00:29:14,512
Крім того, якщо ви використовуєте справжній список слів для визначення простору

511
00:29:14,512 --> 00:29:18,160
можливостей, тоді невизначеність, з якої ви починаєте, становить трохи більше 11 біт.

512
00:29:18,160 --> 00:29:22,610
І виявилося, що лише під час грубого пошуку максимально можлива очікувана

513
00:29:22,610 --> 00:29:26,580
інформація після перших двох припущень становить приблизно 10 біт.

514
00:29:26,580 --> 00:29:31,250
Це свідчить про те, що в найкращому випадку, після ваших перших двох припущень,

515
00:29:31,250 --> 00:29:35,220
з ідеально оптимальною грою, ви залишитеся з дещицею невизначеності.

516
00:29:35,220 --> 00:29:37,400
Це те саме, що мати два можливі припущення.

517
00:29:37,400 --> 00:29:40,170
Тож я вважаю справедливим і, ймовірно, досить консервативним сказати,

518
00:29:40,170 --> 00:29:43,415
що ви ніколи не зможете написати алгоритм, який отримає це середнє значення до 3,

519
00:29:43,415 --> 00:29:46,542
тому що з доступними вам словами просто не буде місця для отримання достатньої

520
00:29:46,542 --> 00:29:49,866
інформації лише за два кроки здатний гарантувати відповідь у третьому слоті кожного

521
00:29:49,866 --> 00:29:50,460
разу без збоїв.

