1
00:00:00,000 --> 00:00:03,138
Гра Wurdle стала досить вірусною за останній місяць чи два, 

2
00:00:03,138 --> 00:00:07,062
і я ніколи не пропускав можливість уроку математики, мені спадає на думку, 

3
00:00:07,062 --> 00:00:10,933
що ця гра є дуже хорошим центральним прикладом уроку з теорії інформації, 

4
00:00:10,933 --> 00:00:12,660
зокрема тема, відома як ентропія.

5
00:00:13,920 --> 00:00:16,860
Розумієте, як і багатьох людей, мене затягнуло головоломкою, 

6
00:00:16,860 --> 00:00:20,812
і, як і багатьох програмістів, я також був затягнутий у спробі написати алгоритм, 

7
00:00:20,812 --> 00:00:22,740
який би грав у гру якомога оптимальніше.

8
00:00:23,180 --> 00:00:25,731
І те, що я думав зробити тут, це просто поговорити з вами про 

9
00:00:25,731 --> 00:00:28,734
мій процес і пояснити деякі математичні обчислення, які в нього входять, 

10
00:00:28,734 --> 00:00:31,080
оскільки весь алгоритм зосереджений на цій ідеї ентропії.

11
00:00:38,700 --> 00:00:41,640
Перш за все, якщо ви не чули про це, що таке Wurdle?

12
00:00:42,040 --> 00:00:45,329
І щоб убити двох зайців одним пострілом, поки ми проходимо правила гри, 

13
00:00:45,329 --> 00:00:47,842
дозвольте мені також переглянути, куди ми йдемо з цим, 

14
00:00:47,842 --> 00:00:51,040
тобто розробити маленький алгоритм, який, в основному, гратиме за нас.

15
00:00:51,360 --> 00:00:55,100
Хоча я не робив сьогодні Wurdle, це 4 лютого, і ми побачимо, як бот впорається.

16
00:00:55,480 --> 00:00:58,168
Мета Wurdle — вгадати таємниче слово з п’яти літер, 

17
00:00:58,168 --> 00:01:00,340
і вам дається шість різних шансів вгадати.

18
00:01:00,840 --> 00:01:04,379
Наприклад, мій бот Wurdle пропонує мені почати з журавля.

19
00:01:05,180 --> 00:01:08,032
Кожного разу, коли ви припускаєте, ви отримуєте певну інформацію про те, 

20
00:01:08,032 --> 00:01:10,220
наскільки близькі ваші припущення до істинної відповіді.

21
00:01:10,920 --> 00:01:14,100
Тут сірий квадрат говорить мені, що у фактичній відповіді немає C.

22
00:01:14,520 --> 00:01:17,840
Жовте поле говорить мені, що є R, але воно не в цьому положенні.

23
00:01:18,240 --> 00:01:20,985
Зелений квадрат говорить мені, що секретне слово дійсно має літеру А, 

24
00:01:20,985 --> 00:01:22,240
і воно стоїть на третій позиції.

25
00:01:22,720 --> 00:01:24,580
І тоді немає ні N, ні E.

26
00:01:25,200 --> 00:01:27,340
Тож дозвольте мені просто зайти та повідомити боту Wurdle цю інформацію.

27
00:01:27,340 --> 00:01:30,320
Ми почали з крана, ми отримали сірий, жовтий, зелений, сірий, сірий.

28
00:01:31,420 --> 00:01:34,940
Не турбуйтеся про всі дані, які він зараз показує, я поясню це свого часу.

29
00:01:35,460 --> 00:01:38,820
Але його найкраща пропозиція для нашого другого вибору є химерною.

30
00:01:39,560 --> 00:01:42,756
І ваше припущення має бути справжнім словом із п’яти літер, але, як ви побачите, 

31
00:01:42,756 --> 00:01:45,400
це досить ліберально щодо того, що насправді дозволить вам вгадати.

32
00:01:46,200 --> 00:01:47,440
У цьому випадку ми намагаємося shtick.

33
00:01:48,780 --> 00:01:50,180
І добре, все виглядає досить добре.

34
00:01:50,260 --> 00:01:53,980
Ми натиснули S і H, тому ми знаємо перші три літери, ми знаємо, що є R.

35
00:01:53,980 --> 00:01:58,700
Тож це буде як SHA щось R або SHA R щось.

36
00:01:59,620 --> 00:02:04,240
І, схоже, бот Wurdle знає, що він має лише дві можливості: shard або sharp.

37
00:02:05,100 --> 00:02:07,517
На даний момент це щось на кшталт суперечки між ними, тож я думаю, 

38
00:02:07,517 --> 00:02:10,080
що, мабуть, лише тому, що він алфавітний, він поєднується з фрагментом.

39
00:02:11,220 --> 00:02:12,860
Ура, ось справжня відповідь.

40
00:02:12,960 --> 00:02:13,780
Тож ми отримали це за три.

41
00:02:14,600 --> 00:02:17,607
Якщо вам цікаво, чи це добре, я почув фразу однієї людини, 

42
00:02:17,607 --> 00:02:20,360
що з Wurdle чотири — це рівномірно, а три — це пташка.

43
00:02:20,680 --> 00:02:22,480
Що, на мою думку, є досить влучною аналогією.

44
00:02:22,480 --> 00:02:27,020
Ви повинні бути постійно в своїй грі, щоб отримати чотири, але це точно не божевілля.

45
00:02:27,180 --> 00:02:29,920
Але коли ви отримуєте це за три, це просто чудово.

46
00:02:30,880 --> 00:02:33,516
Отже, якщо ви не за це, я хотів би просто поговорити про мій процес 

47
00:02:33,516 --> 00:02:35,960
мислення з самого початку про те, як я підходжу до бота Wurdle.

48
00:02:36,480 --> 00:02:39,440
І, як я вже сказав, це дійсно привід для уроку теорії інформації.

49
00:02:39,740 --> 00:02:42,820
Основна мета – пояснити, що таке інформація, а що таке ентропія.

50
00:02:48,220 --> 00:02:51,047
Моєю першою думкою при підході до цього було поглянути 

51
00:02:51,047 --> 00:02:53,720
на відносні частоти різних літер в англійській мові.

52
00:02:54,380 --> 00:02:57,605
Тож я подумав: гаразд, чи є початкове припущення чи початкова пара припущень, 

53
00:02:57,605 --> 00:02:59,260
яка вражає багато цих найчастіших літер?

54
00:02:59,960 --> 00:03:03,000
І один, який я дуже любив, робив інший, а потім цвяхи.

55
00:03:03,760 --> 00:03:05,519
Думка полягає в тому, що якщо ви натискаєте букву, 

56
00:03:05,519 --> 00:03:07,520
ви знаєте, ви отримуєте зелену або жовту, це завжди добре.

57
00:03:07,520 --> 00:03:08,840
Таке відчуття, що ви отримуєте інформацію.

58
00:03:09,340 --> 00:03:12,476
Але в цих випадках, навіть якщо ви не влучаєте і завжди отримуєте сірі, 

59
00:03:12,476 --> 00:03:16,005
це все одно дає вам багато інформації, оскільки досить рідко можна знайти слово, 

60
00:03:16,005 --> 00:03:17,400
у якому немає жодної з цих букв.

61
00:03:18,140 --> 00:03:21,038
Але все одно це не виглядає надсистематичним, тому що, 

62
00:03:21,038 --> 00:03:23,200
наприклад, не враховується порядок літер.

63
00:03:23,560 --> 00:03:25,300
Навіщо друкувати цвяхи, коли я можу надрукувати равлика?

64
00:03:26,080 --> 00:03:27,500
Чи краще мати це S в кінці?

65
00:03:27,820 --> 00:03:28,680
Я не дуже впевнений.

66
00:03:29,240 --> 00:03:32,609
Тепер мій друг сказав, що йому подобається починати словом weary, 

67
00:03:32,609 --> 00:03:36,540
що мене дещо здивувало, оскільки там є деякі незвичайні літери, як-от W та Y.

68
00:03:37,120 --> 00:03:39,000
Але хто знає, можливо, це кращий відкривач.

69
00:03:39,320 --> 00:03:42,126
Чи існує якась кількісна оцінка, яку ми можемо надати, 

70
00:03:42,126 --> 00:03:44,320
щоб оцінити якість потенційного припущення?

71
00:03:45,340 --> 00:03:48,321
Тепер, щоб підготуватися до того, як ми будемо ранжувати можливі припущення, 

72
00:03:48,321 --> 00:03:51,420
давайте повернемося назад і внесемо трохи ясності в те, як саме налаштована гра.

73
00:03:51,420 --> 00:03:55,942
Отже, є список слів, які можна ввести, які вважаються дійсними припущеннями, 

74
00:03:55,942 --> 00:03:57,880
і складається лише з 13 000 слів.

75
00:03:58,320 --> 00:04:02,037
Але якщо ви подивитесь на це, ви побачите багато справді незвичайних речей, 

76
00:04:02,037 --> 00:04:06,440
таких як голова або Алі та ARG, тип слів, які викликають сімейні суперечки в грі в Скрабл.

77
00:04:06,960 --> 00:04:10,540
Але настрій гри полягає в тому, що відповіддю завжди буде досить поширене слово.

78
00:04:10,960 --> 00:04:15,360
І насправді, є ще один список із приблизно 2300 слів, які є можливими відповідями.

79
00:04:15,940 --> 00:04:21,160
І це список, складений людьми, я думаю, саме дівчиною творця гри, що дуже весело.

80
00:04:21,820 --> 00:04:25,068
Але що я хотів би зробити, наше завдання для цього проекту полягає в тому, 

81
00:04:25,068 --> 00:04:28,014
щоб побачити, чи зможемо ми написати програму, що розв’язує Wordle, 

82
00:04:28,014 --> 00:04:30,180
яка не включатиме попередні знання про цей список.

83
00:04:30,720 --> 00:04:33,148
По-перше, є багато досить поширених слів із п’яти літер, 

84
00:04:33,148 --> 00:04:34,640
яких ви не знайдете в цьому списку.

85
00:04:34,940 --> 00:04:38,105
Тож було б краще написати програму, яка була б трішки стійкішою та 

86
00:04:38,105 --> 00:04:41,460
грала б у Wordle проти будь-кого, а не лише проти офіційного веб-сайту.

87
00:04:41,920 --> 00:04:44,984
А також причина, чому ми знаємо, що таке цей список можливих відповідей, 

88
00:04:44,984 --> 00:04:47,000
полягає в тому, що він видимий у вихідному коді.

89
00:04:47,000 --> 00:04:50,656
Але те, як це видно у вихідному коді, — це певний порядок, 

90
00:04:50,656 --> 00:04:53,260
у якому відповіді з’являються день у день.

91
00:04:53,260 --> 00:04:55,840
Тож ви завжди можете просто подивитися, якою буде завтрашня відповідь.

92
00:04:56,420 --> 00:04:58,880
Очевидно, що використання списку є обманом.

93
00:04:59,100 --> 00:05:02,310
І те, що робить головоломку цікавішою та насиченішим уроком теорії інформації, 

94
00:05:02,310 --> 00:05:05,643
полягає в тому, що натомість можна використовувати деякі більш універсальні дані, 

95
00:05:05,643 --> 00:05:08,570
такі як відносна частота слів у цілому, щоб вловити цю інтуїцію про те, 

96
00:05:08,570 --> 00:05:10,440
що ми надаємо перевагу більш поширеним словам.

97
00:05:11,600 --> 00:05:15,900
Тож як із цих 13 000 можливостей вибрати початкове припущення?

98
00:05:16,400 --> 00:05:19,780
Наприклад, якщо мій друг пропонує втомленого, як ми повинні проаналізувати його якість?

99
00:05:20,520 --> 00:05:23,109
Що ж, причина, чому він сказав, що йому подобається це малоймовірне W, 

100
00:05:23,109 --> 00:05:25,443
полягає в тому, що йому подобається довгострокова природа того, 

101
00:05:25,443 --> 00:05:27,340
наскільки добре це відчуваєш, якщо ти влучив у це W.

102
00:05:27,920 --> 00:05:32,036
Наприклад, якщо перша виявлена закономірність була приблизно такою, то виявиться, 

103
00:05:32,036 --> 00:05:35,600
що в цьому гігантському лексиконі лише 58 слів відповідають цій моделі.

104
00:05:36,060 --> 00:05:38,400
Тож це величезне зниження з 13 000.

105
00:05:38,780 --> 00:05:41,241
Але зворотна сторона цього, звичайно, полягає в тому, 

106
00:05:41,241 --> 00:05:43,020
що дуже рідко отримати такий візерунок.

107
00:05:43,020 --> 00:05:46,578
Зокрема, якби кожне слово з однаковою ймовірністю було відповіддю, 

108
00:05:46,578 --> 00:05:51,040
ймовірність досягнення цього шаблону дорівнювала б 58 поділеним приблизно на 13 000.

109
00:05:51,580 --> 00:05:53,600
Звичайно, вони не однаково ймовірні відповіді.

110
00:05:53,720 --> 00:05:56,220
Більшість із них дуже незрозумілі та навіть сумнівні слова.

111
00:05:56,600 --> 00:05:59,343
Але принаймні для нашого першого проходження всього цього, давайте припустимо, 

112
00:05:59,343 --> 00:06:01,600
що всі вони однаково ймовірні, а потім уточнимо це трохи пізніше.

113
00:06:02,020 --> 00:06:06,720
Справа в тому, що шаблон із великою кількістю інформації за своєю природою малоймовірний.

114
00:06:07,280 --> 00:06:10,800
Насправді бути інформативним означає те, що це малоймовірно.

115
00:06:11,719 --> 00:06:16,460
Набагато більш вірогідною схемою для цього відкриття буде щось на кшталт цього, 

116
00:06:16,460 --> 00:06:18,120
де, звичайно, немає букви W.

117
00:06:18,240 --> 00:06:21,400
Можливо, там є E, а можливо, немає A, немає R, немає Y.

118
00:06:22,080 --> 00:06:24,560
У цьому випадку є 1400 можливих збігів.

119
00:06:25,080 --> 00:06:29,022
Якби все було однаково вірогідним, виходить, що ймовірність приблизно 11% того, 

120
00:06:29,022 --> 00:06:30,600
що ви б побачили саме цю модель.

121
00:06:30,900 --> 00:06:33,340
Таким чином, найімовірніші результати також є найменш інформативними.

122
00:06:34,240 --> 00:06:37,713
Щоб отримати більш глобальне уявлення, дозвольте мені показати вам повний 

123
00:06:37,713 --> 00:06:41,140
розподіл ймовірностей за всіма різними шаблонами, які ви можете побачити.

124
00:06:41,740 --> 00:06:45,999
Таким чином, кожна смужка, на яку ви дивитеся, відповідає можливому шаблону кольорів, 

125
00:06:45,999 --> 00:06:50,259
який можна виявити, з яких є від 3 до 5 варіантів, і вони організовані зліва направо, 

126
00:06:50,259 --> 00:06:52,340
від найпоширенішого до найменш поширеного.

127
00:06:52,920 --> 00:06:56,000
Отже, найпоширенішою можливістю є те, що ви отримаєте всі сірі.

128
00:06:56,100 --> 00:06:58,120
Це відбувається приблизно в 14% випадків.

129
00:06:58,580 --> 00:07:01,487
І те, на що ви сподіваєтеся, коли ви припускаєте, це те, 

130
00:07:01,487 --> 00:07:04,191
що ви опинитеся десь у цьому довгому хвості, як тут, 

131
00:07:04,191 --> 00:07:07,762
де є лише 18 можливостей для того, що відповідає цьому шаблону, який, 

132
00:07:07,762 --> 00:07:09,140
очевидно, виглядає ось так.

133
00:07:09,920 --> 00:07:13,800
Або якщо ми підемо трохи ліворуч, то, можливо, ми підемо сюди.

134
00:07:14,940 --> 00:07:16,180
Добре, ось вам гарна головоломка.

135
00:07:16,540 --> 00:07:19,561
Які три слова в англійській мові починаються з літери W, 

136
00:07:19,561 --> 00:07:22,000
закінчуються літерою Y і десь містять букву R?

137
00:07:22,480 --> 00:07:26,800
Виявляється, відповіді, давайте подивимося, багатослівні, червиві та іронічні.

138
00:07:27,500 --> 00:07:30,334
Отже, щоб оцінити, наскільки добре це слово в цілому, 

139
00:07:30,334 --> 00:07:33,273
ми хочемо якийсь вимір очікуваної кількості інформації, 

140
00:07:33,273 --> 00:07:35,740
яку ви збираєтеся отримати від цього розподілу.

141
00:07:35,740 --> 00:07:40,449
Якщо ми переглянемо кожен шаблон і помножимо його ймовірність появи на те, 

142
00:07:40,449 --> 00:07:44,720
що вимірює його інформативність, це може дати нам об’єктивну оцінку.

143
00:07:45,960 --> 00:07:49,840
Тепер вашим першим інстинктом щодо того, що це має бути, може бути кількість збігів.

144
00:07:50,160 --> 00:07:52,400
Вам потрібна менша середня кількість збігів.

145
00:07:52,800 --> 00:07:56,093
Але натомість я хотів би використовувати більш універсальне вимірювання, 

146
00:07:56,093 --> 00:07:59,161
яке ми часто приписуємо інформації, і таке, яке буде більш гнучким, 

147
00:07:59,161 --> 00:08:02,951
коли ми матимемо різну ймовірність, призначену кожному з цих 13 000 слів щодо того, 

148
00:08:02,951 --> 00:08:04,260
чи є вони справді відповіддю.

149
00:08:10,320 --> 00:08:13,515
Стандартною одиницею інформації є біт, який має трохи кумедну формулу, 

150
00:08:13,515 --> 00:08:16,980
але вона справді інтуїтивно зрозуміла, якщо ми просто подивимося на приклади.

151
00:08:17,780 --> 00:08:21,259
Якщо у вас є спостереження, яке вдвічі скорочує ваш простір можливостей, 

152
00:08:21,259 --> 00:08:23,500
ми кажемо, що воно містить один біт інформації.

153
00:08:24,180 --> 00:08:27,545
У нашому прикладі простір можливостей — це всі можливі слова, і виявляється, 

154
00:08:27,545 --> 00:08:31,260
що близько половини слів із п’яти літер мають S, трохи менше, але приблизно половина.

155
00:08:31,780 --> 00:08:34,320
Таким чином, це спостереження дасть вам трохи інформації.

156
00:08:34,880 --> 00:08:38,895
Якщо натомість новий факт скорочує цей простір можливостей у чотири рази, 

157
00:08:38,895 --> 00:08:41,500
ми говоримо, що він містить два біти інформації.

158
00:08:41,980 --> 00:08:44,460
Наприклад, виявилося, що приблизно чверть цих слів мають Т.

159
00:08:45,020 --> 00:08:47,536
Якщо спостереження скорочує цей простір у вісім, 

160
00:08:47,536 --> 00:08:50,720
ми говоримо, що це три біти інформації, і так далі і так далі.

161
00:08:50,900 --> 00:08:55,060
Чотири біти перетворюють його на 16-й, п’ять бітів — на 32-й.

162
00:08:55,060 --> 00:08:58,001
Тож тепер ви можете зупинитись і запитати себе, 

163
00:08:58,001 --> 00:09:02,660
яка формула для інформації для кількості бітів у термінах ймовірності появи?

164
00:09:02,660 --> 00:09:06,881
Ми маємо на увазі те, що коли ви берете одну половину на кількість бітів, 

165
00:09:06,881 --> 00:09:09,848
це те саме, що ймовірність, що те саме, що сказати, 

166
00:09:09,848 --> 00:09:13,328
що два в степені кількості бітів є одиницею над ймовірністю, 

167
00:09:13,328 --> 00:09:17,436
що далі переставляє, кажучи, що інформація є двома логарифмами одиниці, 

168
00:09:17,436 --> 00:09:18,920
поділеними на ймовірність.

169
00:09:19,620 --> 00:09:21,986
І іноді ви бачите це з ще одним перевпорядкуванням, 

170
00:09:21,986 --> 00:09:24,900
де інформація є від’ємним логарифмом за основою два ймовірності.

171
00:09:25,660 --> 00:09:28,848
У такому вигляді це може здатися трохи дивним для непосвячених, 

172
00:09:28,848 --> 00:09:31,588
але насправді це просто дуже інтуїтивна ідея запитати, 

173
00:09:31,588 --> 00:09:34,080
скільки разів ви скоротили свої можливості вдвічі.

174
00:09:35,180 --> 00:09:38,279
А тепер, якщо вам цікаво, знаєте, я думав, що ми просто граємо у веселу гру слів, 

175
00:09:38,279 --> 00:09:39,300
чому логарифми з’являються?

176
00:09:39,780 --> 00:09:42,633
Одна з причин, чому це краща одиниця, полягає в тому, 

177
00:09:42,633 --> 00:09:47,020
що набагато простіше говорити про дуже малоймовірні події, набагато легше сказати, 

178
00:09:47,020 --> 00:09:50,033
що спостереження містить 20 біт інформації, ніж сказати, 

179
00:09:50,033 --> 00:09:52,940
що ймовірність такого-то виникнення дорівнює 0.0000095.

180
00:09:53,300 --> 00:09:57,407
Але більш суттєвою причиною того, що цей логарифмічний вираз виявився дуже 

181
00:09:57,407 --> 00:10:01,460
корисним доповненням до теорії ймовірності, є спосіб додавання інформації.

182
00:10:02,060 --> 00:10:05,345
Наприклад, якщо одне спостереження дає вам два біти інформації, 

183
00:10:05,345 --> 00:10:08,527
скорочуючи ваш простір учетверо, а потім друге спостереження, 

184
00:10:08,527 --> 00:10:12,582
подібне до вашого другого припущення в Wordle, дає вам ще три біти інформації, 

185
00:10:12,582 --> 00:10:16,740
скорочуючи вас ще на один коефіцієнт вісім, два разом дають п’ять біт інформації.

186
00:10:17,160 --> 00:10:21,020
Подібно до того, як ймовірності люблять множитися, інформація любить додаватися.

187
00:10:21,960 --> 00:10:25,067
Отже, як тільки ми потрапляємо в область чогось на зразок очікуваного значення, 

188
00:10:25,067 --> 00:10:27,980
де ми додаємо купу чисел, журнали роблять це набагато зручнішим для роботи.

189
00:10:28,480 --> 00:10:32,255
Давайте повернемося до нашого розподілу для Weary і додамо сюди ще один маленький трекер, 

190
00:10:32,255 --> 00:10:34,940
який показуватиме нам, скільки інформації є для кожного шаблону.

191
00:10:35,580 --> 00:10:38,735
Головне, на що я хочу, щоб ви звернули увагу, це те, що чим вища ймовірність, 

192
00:10:38,735 --> 00:10:41,606
коли ми дійдемо до тих більш вірогідних моделей, тим менше інформації, 

193
00:10:41,606 --> 00:10:42,780
тим менше бітів ви отримуєте.

194
00:10:43,500 --> 00:10:46,140
Спосіб вимірювання якості цього припущення полягає в тому, 

195
00:10:46,140 --> 00:10:49,540
щоб взяти очікуване значення цієї інформації, де ми проходимо кожен шаблон, 

196
00:10:49,540 --> 00:10:53,299
говоримо, наскільки це ймовірно, а потім ми множимо це на кількість біт інформації, 

197
00:10:53,299 --> 00:10:54,060
яку ми отримуємо.

198
00:10:54,710 --> 00:10:58,120
А у прикладі Вірі це виявляється 4.9 біт.

199
00:10:58,560 --> 00:11:01,760
Отже, у середньому інформація, яку ви отримуєте з цього початкового припущення, 

200
00:11:01,760 --> 00:11:05,240
настільки ж хороша, як скорочення вашого простору можливостей навпіл приблизно в п’ять 

201
00:11:05,240 --> 00:11:05,480
разів.

202
00:11:05,960 --> 00:11:08,636
Навпаки, прикладом припущення з вищим очікуваним 

203
00:11:08,636 --> 00:11:11,640
інформаційним значенням може бути щось на зразок Slate.

204
00:11:13,120 --> 00:11:15,620
У цьому випадку ви помітите, що розподіл виглядає набагато більш плоским.

205
00:11:15,940 --> 00:11:21,877
Зокрема, найімовірніша поява всіх сірих має лише близько 6% ймовірності появи, 

206
00:11:21,877 --> 00:11:25,260
тому ви отримуєте мінімум 3.9 біт інформації.

207
00:11:25,920 --> 00:11:28,560
Але це мінімум, зазвичай ви отримаєте щось краще за це.

208
00:11:29,100 --> 00:11:33,771
І виявляється, коли ви обчислюєте цифри на цьому місці та додаєте всі відповідні терміни, 

209
00:11:33,771 --> 00:11:35,900
середня інформація становить близько 5.8.

210
00:11:37,360 --> 00:11:40,250
Отже, на відміну від Вірі, ваш простір можливостей буде в 

211
00:11:40,250 --> 00:11:43,540
середньому приблизно вдвічі менший після цього першого припущення.

212
00:11:44,420 --> 00:11:49,120
Насправді є весела історія про назву цього очікуваного значення кількості інформації.

213
00:11:49,200 --> 00:11:53,086
Теорію інформації розробив Клод Шеннон, який працював у Bell Labs у 1940-х роках, 

214
00:11:53,086 --> 00:11:56,403
але він говорив про деякі зі своїх ідей, які ще не були опубліковані, 

215
00:11:56,403 --> 00:11:59,815
з Джоном фон Нейманом, який був цим інтелектуальним гігантом того часу, 

216
00:11:59,815 --> 00:12:03,560
дуже видатним у математиці та фізиці та початках того, що ставало інформатикою.

217
00:12:04,100 --> 00:12:07,325
І коли він згадав, що він насправді не має вдалого імені для 

218
00:12:07,325 --> 00:12:11,291
цього очікуваного значення кількості інформації, фон Нейман нібито сказав, 

219
00:12:11,291 --> 00:12:14,200
отже, ви повинні назвати це ентропією, і з двох причин.

220
00:12:14,540 --> 00:12:18,596
По-перше, ваша функція невизначеності використовувалася в статистичній механіці 

221
00:12:18,596 --> 00:12:22,957
під такою назвою, тож вона вже має назву, а по-друге, що ще важливіше, ніхто не знає, 

222
00:12:22,957 --> 00:12:26,760
що таке ентропія насправді, тому в дебатах ви завжди будете мають перевагу.

223
00:12:27,700 --> 00:12:32,460
Отже, якщо назва здається трохи загадковою, і якщо вірити цій історії, це начебто задум.

224
00:12:33,280 --> 00:12:37,318
Крім того, якщо вам цікаво його відношення до всього другого закону термодинаміки 

225
00:12:37,318 --> 00:12:41,159
з фізики, зв’язок точно є, але в його витоках Шеннон мав справу лише з чистою 

226
00:12:41,159 --> 00:12:45,147
теорією ймовірностей, і для наших цілей тут, коли я використовую слово ентропія, 

227
00:12:45,147 --> 00:12:49,580
я просто хочу, щоб ви подумали про очікувану інформаційну цінність конкретного припущення.

228
00:12:50,700 --> 00:12:53,780
Ви можете думати про ентропію як про вимірювання двох речей одночасно.

229
00:12:54,240 --> 00:12:56,780
Перше — це те, наскільки плоским є розподіл.

230
00:12:57,320 --> 00:13:01,120
Чим ближче рівномірний розподіл, тим вищою буде ентропія.

231
00:13:01,580 --> 00:13:05,360
У нашому випадку, коли існує від 3 до 5-го загальних шаблонів, 

232
00:13:05,360 --> 00:13:10,579
для рівномірного розподілу, спостереження за будь-яким із них матиме інформаційну базу 

233
00:13:10,579 --> 00:13:14,720
журналу 2 з 3 до 5-го, що дорівнює 7.92, тож це абсолютний максимум, 

234
00:13:14,720 --> 00:13:17,300
який ви могли б отримати для цієї ентропії.

235
00:13:17,840 --> 00:13:22,080
Але ентропія також є своєрідним показником того, скільки можливостей існує.

236
00:13:22,320 --> 00:13:26,766
Наприклад, якщо у вас є якесь слово, де є лише 16 можливих шаблонів, 

237
00:13:26,766 --> 00:13:32,180
і кожен з них однаково вірогідний, ця ентропія, ця очікувана інформація буде 4 біти.

238
00:13:32,579 --> 00:13:36,838
Але якщо у вас є інше слово, де є 64 можливі шаблони, які можуть виникнути, 

239
00:13:36,838 --> 00:13:40,480
і всі вони однаково вірогідні, тоді ентропія буде складати 6 біт.

240
00:13:41,500 --> 00:13:45,828
Отже, якщо ви бачите якийсь розподіл у дикій природі, який має ентропію 6 біт, 

241
00:13:45,828 --> 00:13:48,678
це ніби говорить про те, що в тому, що має статися, 

242
00:13:48,678 --> 00:13:53,500
існує стільки варіацій і невизначеності, як якщо б було 64 однаково ймовірні результати.

243
00:13:54,360 --> 00:13:59,320
Під час мого першого проходу в Wurtelebot я просто зробив це.

244
00:13:59,320 --> 00:14:03,417
Він переглядає всі можливі припущення, які ви можете мати, усі 13 000 слів, 

245
00:14:03,417 --> 00:14:08,269
обчислює ентропію для кожного з них, або, точніше, ентропію розподілу за всіма шаблонами, 

246
00:14:08,269 --> 00:14:12,528
які ви можете побачити, для кожного з них, і вибирає найвище, оскільки це той, 

247
00:14:12,528 --> 00:14:16,140
який, швидше за все, максимально скоротить ваш простір можливостей.

248
00:14:17,140 --> 00:14:19,324
І незважаючи на те, що я говорив тут лише про перше припущення, 

249
00:14:19,324 --> 00:14:21,100
воно робить те саме для кількох наступних припущень.

250
00:14:21,560 --> 00:14:24,927
Наприклад, після того, як ви бачите певний шаблон у цій першій здогадці, 

251
00:14:24,927 --> 00:14:27,971
яка обмежила б вас меншою кількістю можливих слів на основі того, 

252
00:14:27,971 --> 00:14:31,800
що з цим збігається, ви просто граєте в ту саму гру щодо цього меншого набору слів.

253
00:14:32,260 --> 00:14:36,500
Для запропонованого другого припущення ви дивитеся на розподіл усіх шаблонів, 

254
00:14:36,500 --> 00:14:39,653
які можуть виникати з цього більш обмеженого набору слів, 

255
00:14:39,653 --> 00:14:43,840
ви шукаєте всі 13 000 можливостей і знаходите ту, яка максимізує цю ентропію.

256
00:14:45,420 --> 00:14:49,618
Щоб показати вам, як це працює в дії, дозвольте мені просто витягнути невеликий 

257
00:14:49,618 --> 00:14:54,080
варіант Wurtele, який я написав, який показує основні моменти цього аналізу на полях.

258
00:14:54,080 --> 00:14:57,437
Після виконання всіх обчислень ентропії, тут праворуч він показує нам, 

259
00:14:57,437 --> 00:14:59,660
які з них мають найбільшу очікувану інформацію.

260
00:15:00,280 --> 00:15:04,613
Виявляється, головною відповіддю, принаймні на даний момент, 

261
00:15:04,613 --> 00:15:10,580
ми уточнимо це пізніше, є Тарес, що означає, гм, звичайно, вика, найпоширеніша вика.

262
00:15:11,040 --> 00:15:13,519
Кожного разу, коли ми робимо припущення тут, де, можливо, 

263
00:15:13,519 --> 00:15:16,853
я ігнорую його рекомендації та вибираю шифер, тому що мені подобається шифер, 

264
00:15:16,853 --> 00:15:19,546
ми можемо побачити, скільки очікуваної інформації він містить, 

265
00:15:19,546 --> 00:15:23,094
але праворуч від слова тут показано, скільки фактичну інформацію, яку ми отримали, 

266
00:15:23,094 --> 00:15:24,420
враховуючи цю конкретну модель.

267
00:15:25,000 --> 00:15:27,231
Тож тут, схоже, нам трохи не пощастило, очікували, 

268
00:15:27,231 --> 00:15:30,120
що ми отримаємо 5.8, але випадково ми отримали щось менше, ніж це.

269
00:15:30,600 --> 00:15:35,020
А потім ліворуч тут показано всі різні можливі слова, де ми зараз знаходимося.

270
00:15:35,800 --> 00:15:38,881
Сині смужки вказують на те, наскільки вірогідним є кожне слово, 

271
00:15:38,881 --> 00:15:42,059
тому наразі припускається, що кожне слово буде однаково ймовірно, 

272
00:15:42,059 --> 00:15:43,360
але ми уточнимо це за мить.

273
00:15:44,060 --> 00:15:47,826
І тоді це вимірювання невизначеності говорить нам про ентропію цього 

274
00:15:47,826 --> 00:15:52,029
розподілу між можливими словами, що зараз, оскільки це рівномірний розподіл, 

275
00:15:52,029 --> 00:15:55,960
є просто непотрібно складним способом підрахувати кількість можливостей.

276
00:15:56,560 --> 00:16:02,180
Наприклад, якби ми взяли 2 у ступінь 13.66, це має бути приблизно 13 000 можливостей.

277
00:16:02,900 --> 00:16:06,140
Я трохи відхилився, але лише тому, що не показую всі десяткові знаки.

278
00:16:06,720 --> 00:16:09,460
На даний момент це може здатися зайвим і занадто складним, 

279
00:16:09,460 --> 00:16:12,340
але за хвилину ви зрозумієте, чому корисно мати обидва номери.

280
00:16:12,760 --> 00:16:17,502
Отже, тут виглядає так, ніби найвища ентропія для нашого другого припущення – Рамен, 

281
00:16:17,502 --> 00:16:19,400
що знову ж таки не схоже на слово.

282
00:16:19,980 --> 00:16:24,060
Отже, щоб підвищити моральну позицію, я введу Rains.

283
00:16:25,440 --> 00:16:27,340
І знову схоже, що нам трохи не пощастило.

284
00:16:27,520 --> 00:16:31,360
Ми чекали 4.3 біти, а ми отримали лише 3.39 біт інформації.

285
00:16:31,940 --> 00:16:33,940
Отже, ми маємо 55 можливостей.

286
00:16:34,900 --> 00:16:39,440
І тут, можливо, я просто прийму те, що він пропонує, тобто комбо, що б це не означало.

287
00:16:40,040 --> 00:16:42,920
І гаразд, це насправді хороший шанс для головоломки.

288
00:16:42,920 --> 00:16:46,380
Це говорить нам, що цей шаблон дає нам 4.7 біт інформації.

289
00:16:47,060 --> 00:16:51,720
Але ліворуч, перш ніж ми побачимо цей шаблон, їх було 5.78 біт невизначеності.

290
00:16:52,420 --> 00:16:56,340
Отже, як вікторина для вас, що це означає щодо кількості можливостей, що залишилися?

291
00:16:58,040 --> 00:17:01,632
Ну, це означає, що ми зведені до однієї частки невизначеності, 

292
00:17:01,632 --> 00:17:04,540
що те саме, що сказати, що є дві можливі відповіді.

293
00:17:04,700 --> 00:17:05,700
Це вибір 50 на 50.

294
00:17:06,500 --> 00:17:09,068
І звідси, оскільки ми з вами знаємо, які слова є більш поширеними, 

295
00:17:09,068 --> 00:17:10,640
ми знаємо, що відповідь має бути безодня.

296
00:17:11,180 --> 00:17:13,280
Але, як зараз написано, програма цього не знає.

297
00:17:13,540 --> 00:17:16,810
Тож він просто продовжує, намагаючись отримати якомога більше інформації, 

298
00:17:16,810 --> 00:17:19,859
доки не залишиться лише одна можливість, а потім здогадується про це.

299
00:17:20,380 --> 00:17:22,339
Отже, очевидно, нам потрібна краща стратегія завершення гри.

300
00:17:22,599 --> 00:17:25,618
Але, скажімо, ми назвемо цю версію одним із наших розв’язувачів Wordle, 

301
00:17:25,618 --> 00:17:28,260
а потім запустимо кілька симуляцій, щоб побачити, як це працює.

302
00:17:30,360 --> 00:17:34,120
Отже, як це працює, це гра в усі можливі ігри зі словами.

303
00:17:34,240 --> 00:17:38,540
Він переглядає всі ці 2315 слів, які є фактичними відповідями Wordle.

304
00:17:38,540 --> 00:17:40,580
В основному це використовується як набір для тестування.

305
00:17:41,360 --> 00:17:44,400
І з цим наївним методом не брати до уваги, наскільки поширене слово, 

306
00:17:44,400 --> 00:17:47,837
і просто намагатися максимізувати інформацію на кожному кроці на цьому шляху, 

307
00:17:47,837 --> 00:17:49,820
доки не дійде до одного й лише одного вибору.

308
00:17:50,360 --> 00:17:54,300
Наприкінці симуляції середній бал виходить близько 4.124.

309
00:17:55,319 --> 00:17:59,240
Що непогано, чесно кажучи, я очікував, що буде гірше.

310
00:17:59,660 --> 00:18:02,600
Але люди, які грають у wordle, скажуть вам, що вони зазвичай можуть отримати його за 4.

311
00:18:02,860 --> 00:18:05,380
Справжнє завдання — отримати якомога більше за 3.

312
00:18:05,380 --> 00:18:08,080
Це досить великий стрибок між рахунком 4 і рахунком 3.

313
00:18:08,860 --> 00:18:12,626
Очевидний низький плід тут полягає в тому, щоб якимось чином врахувати, 

314
00:18:12,626 --> 00:18:14,980
чи є слово загальним, і як саме ми це робимо.

315
00:18:22,800 --> 00:18:25,647
Я підійшов до цього, щоб отримати список відносних 

316
00:18:25,647 --> 00:18:27,880
частот для всіх слів в англійській мові.

317
00:18:28,220 --> 00:18:31,273
І я щойно скористався функцією даних частоти слів Mathematica, 

318
00:18:31,273 --> 00:18:34,860
яка сама одержує загальнодоступний набір даних Google Books English Ngram.

319
00:18:35,460 --> 00:18:37,651
І на це цікаво дивитися, наприклад, якщо ми відсортуємо 

320
00:18:37,651 --> 00:18:39,960
його від найбільш поширених слів до найменш поширених слів.

321
00:18:40,120 --> 00:18:43,080
Очевидно, це найпоширеніші слова з 5 букв в англійській мові.

322
00:18:43,700 --> 00:18:45,840
А точніше, це 8 місце за поширеністю.

323
00:18:46,280 --> 00:18:48,880
Спочатку який, потім там і там.

324
00:18:49,260 --> 00:18:52,348
Перший сам по собі не перший, а 9-й, і цілком зрозуміло, 

325
00:18:52,348 --> 00:18:57,062
що ці інші слова можуть зустрічатися частіше, де ті, що стоять після першого, є після, 

326
00:18:57,062 --> 00:18:58,580
де, а ті, які є трохи рідше.

327
00:18:59,160 --> 00:19:01,616
Тепер, використовуючи ці дані для моделювання того, 

328
00:19:01,616 --> 00:19:04,639
наскільки ймовірно кожне з цих слів буде остаточною відповіддю, 

329
00:19:04,639 --> 00:19:06,860
це не повинно бути просто пропорційним частоті.

330
00:19:06,860 --> 00:19:10,525
Наприклад, який отримав оцінку 0.002 у цьому наборі даних, 

331
00:19:10,525 --> 00:19:15,060
тоді як слово braid у певному сенсі приблизно в 1000 разів менш імовірне.

332
00:19:15,560 --> 00:19:18,840
Але обидва ці слова досить поширені, тому їх майже напевно варто розглянути.

333
00:19:19,340 --> 00:19:21,000
Отже, ми хочемо більше двійкового відсікання.

334
00:19:21,860 --> 00:19:25,589
Я пішов з цього приводу: уявити весь цей відсортований список слів, 

335
00:19:25,589 --> 00:19:29,538
а потім розташувати його на осі х, а потім застосувати функцію sigmoid, 

336
00:19:29,538 --> 00:19:34,036
яка є стандартним способом отримання функції, вихід якої в основному є двійковим, 

337
00:19:34,036 --> 00:19:38,260
це або 0, або 1, але між ними є згладжування для цієї області невизначеності.

338
00:19:39,160 --> 00:19:43,059
Таким чином, по суті, ймовірність того, що я призначаю кожному слову для того, 

339
00:19:43,059 --> 00:19:46,761
щоб потрапити в остаточний список, буде значенням сигмоїдної функції вище, 

340
00:19:46,761 --> 00:19:48,440
де б воно не знаходилося на осі х.

341
00:19:49,520 --> 00:19:53,133
Тепер, очевидно, це залежить від кількох параметрів, наприклад, 

342
00:19:53,133 --> 00:19:56,859
наскільки широкий простір на осі х заповнюють ці слова, визначає, 

343
00:19:56,859 --> 00:20:00,360
наскільки поступово або круто ми знижуємося від 1 до 0, і те, 

344
00:20:00,360 --> 00:20:03,240
де ми їх розташовуємо зліва направо, визначає межу.

345
00:20:03,240 --> 00:20:06,920
Чесно кажучи, те, як я це зробив, було просто облизати палець і тицьнути його на вітер.

346
00:20:07,140 --> 00:20:10,702
Я переглянув відсортований список і спробував знайти вікно, у якому, 

347
00:20:10,702 --> 00:20:13,955
дивлячись на нього, я вирішив, що приблизно половина цих слів, 

348
00:20:13,955 --> 00:20:17,260
швидше за все, є остаточною відповіддю, і використав це як межу.

349
00:20:17,260 --> 00:20:21,069
Коли ми маємо подібний розподіл між словами, це дає нам іншу ситуацію, 

350
00:20:21,069 --> 00:20:23,860
коли ентропія стає цим дійсно корисним вимірюванням.

351
00:20:24,500 --> 00:20:28,314
Наприклад, скажімо, ми граємо в гру, і ми починаємо з моїх старих відкривачів, 

352
00:20:28,314 --> 00:20:30,729
які були пером і цвяхами, і закінчуємо ситуацією, 

353
00:20:30,729 --> 00:20:33,240
коли є чотири можливі слова, які відповідають цьому.

354
00:20:33,560 --> 00:20:35,620
І припустимо, ми вважаємо їх усіх однаково ймовірними.

355
00:20:36,220 --> 00:20:38,880
Дозвольте запитати вас, яка ентропія цього розподілу?

356
00:20:41,080 --> 00:20:45,296
Що ж, інформація, пов’язана з кожною з цих можливостей, 

357
00:20:45,296 --> 00:20:50,040
буде базою журналу 2 з 4, оскільки кожна з них є 1 і 4, і це 2.

358
00:20:50,040 --> 00:20:52,460
Два біти інформації, чотири можливості.

359
00:20:52,760 --> 00:20:53,580
Все дуже добре і добре.

360
00:20:54,300 --> 00:20:57,800
Але що, якби я сказав вам, що насправді є більше чотирьох збігів?

361
00:20:58,260 --> 00:21:00,534
Насправді, коли ми переглядаємо повний список слів, 

362
00:21:00,534 --> 00:21:02,460
ми знаходимо 16 слів, які йому відповідають.

363
00:21:02,580 --> 00:21:06,029
Але припустімо, що наша модель надає справді низьку ймовірність того, 

364
00:21:06,029 --> 00:21:09,183
що ці інші 12 слів є остаточною відповіддю, приблизно 1 з 1000, 

365
00:21:09,183 --> 00:21:10,760
тому що вони дійсно незрозумілі.

366
00:21:11,500 --> 00:21:14,260
Тепер дозвольте запитати вас, яка ентропія цього розподілу?

367
00:21:15,420 --> 00:21:19,281
Якби ентропія вимірювала лише кількість збігів, тоді можна було б очікувати, 

368
00:21:19,281 --> 00:21:22,992
що це буде щось на кшталт логарифмічної бази 2 із 16, що дорівнюватиме 4, 

369
00:21:22,992 --> 00:21:25,700
на два біти невизначеності більше, ніж ми мали раніше.

370
00:21:26,180 --> 00:21:29,860
Але, звісно, фактична невизначеність не дуже відрізняється від того, що ми мали раніше.

371
00:21:30,160 --> 00:21:32,937
Те, що є ці 12 справді незрозумілих слів, не означає, 

372
00:21:32,937 --> 00:21:37,360
що було б ще більш дивно дізнатися, що остаточною відповіддю є, наприклад, чарівність.

373
00:21:38,180 --> 00:21:42,608
Отже, коли ви фактично виконуєте обчислення тут і додаєте ймовірність кожного випадку, 

374
00:21:42,608 --> 00:21:45,560
помножену на відповідну інформацію, ви отримуєте 2.11 біт.

375
00:21:45,560 --> 00:21:49,496
Я просто кажу, що в основному це два біти, в основному ці чотири можливості, 

376
00:21:49,496 --> 00:21:52,921
але є трохи більше невизначеності через усі ці малоймовірні події, 

377
00:21:52,921 --> 00:21:56,500
хоча якби ви дізналися про них, ви б отримали з цього масу інформації.

378
00:21:57,160 --> 00:21:59,164
Отже, зменшуючи масштаб, це частина того, що робить 

379
00:21:59,164 --> 00:22:01,400
Wordle таким гарним прикладом для уроку теорії інформації.

380
00:22:01,600 --> 00:22:04,640
Ми маємо ці два різні застосування ентропії.

381
00:22:05,160 --> 00:22:10,007
Перше говорить нам, яку інформацію ми очікуємо отримати від певного припущення, 

382
00:22:10,007 --> 00:22:15,460
а друге говорить, чи можемо ми виміряти залишкову невизначеність серед усіх можливих слів.

383
00:22:16,460 --> 00:22:20,406
І я маю підкреслити, що в першому випадку, коли ми розглядаємо очікувану інформацію 

384
00:22:20,406 --> 00:22:24,540
про припущення, як тільки ми маємо нерівну вагу слів, це впливає на обчислення ентропії.

385
00:22:24,980 --> 00:22:27,310
Наприклад, дозвольте мені знайти той самий випадок, 

386
00:22:27,310 --> 00:22:29,955
який ми розглядали раніше, розподілу, пов’язаного з Weary, 

387
00:22:29,955 --> 00:22:33,720
але цього разу з використанням нерівномірного розподілу між усіма можливими словами.

388
00:22:34,500 --> 00:22:38,280
Тож дозвольте мені поглянути, чи зможу я знайти тут частину, яка б це добре ілюструвала.

389
00:22:40,940 --> 00:22:42,360
Гаразд, ось це досить добре.

390
00:22:42,360 --> 00:22:45,460
Тут ми маємо два суміжних шаблони, які приблизно однаково вірогідні, 

391
00:22:45,460 --> 00:22:49,100
але один із них, як нам сказали, містить 32 можливі слова, які йому відповідають.

392
00:22:49,280 --> 00:22:53,860
І якщо ми перевіримо, що це таке, це ті 32, які є дуже малоймовірними словами, 

393
00:22:53,860 --> 00:22:55,600
якщо ви переглядаєте їх очима.

394
00:22:55,840 --> 00:22:58,929
Важко знайти будь-яку правдоподібну відповідь, можливо, крики, 

395
00:22:58,929 --> 00:23:01,625
але якщо ми подивимося на сусідній шаблон у розподілі, 

396
00:23:01,625 --> 00:23:04,567
який вважається приблизно таким же вірогідним, нам скажуть, 

397
00:23:04,567 --> 00:23:07,803
що він має лише 8 можливих збігів, тобто чверть як багато збігів, 

398
00:23:07,803 --> 00:23:09,520
але це приблизно так само ймовірно.

399
00:23:09,860 --> 00:23:12,140
І коли ми знайдемо ці сірники, ми зрозуміємо чому.

400
00:23:12,500 --> 00:23:16,300
Деякі з них є реальними правдоподібними відповідями, наприклад, кільце, гнів або стукіт.

401
00:23:17,900 --> 00:23:21,636
Щоб проілюструвати, як ми все це об’єднуємо, дозвольте мені витягнути тут версію 

402
00:23:21,636 --> 00:23:25,280
2 Wordlebot, і там є дві або три основні відмінності від першої, яку ми бачили.

403
00:23:25,860 --> 00:23:29,715
По-перше, як я щойно сказав, спосіб, у який ми обчислюємо ці ентропії, 

404
00:23:29,715 --> 00:23:34,384
ці очікувані значення інформації, тепер використовує точніший розподіл між шаблонами, 

405
00:23:34,384 --> 00:23:38,240
який включає ймовірність того, що дане слово насправді буде відповіддю.

406
00:23:38,879 --> 00:23:43,820
Як це сталося, сльози все ще залишаються номером 1, хоча наступні трохи інші.

407
00:23:44,360 --> 00:23:46,840
По-друге, коли він ранжує свої найпопулярніші варіанти, 

408
00:23:46,840 --> 00:23:50,650
він тепер зберігатиме модель ймовірності того, що кожне слово є фактичною відповіддю, 

409
00:23:50,650 --> 00:23:53,130
і він включатиме це у своє рішення, яке легше побачити, 

410
00:23:53,130 --> 00:23:55,080
коли ми матимемо кілька припущень щодо стіл.

411
00:23:55,860 --> 00:23:57,782
Знову ж таки, ігноруємо його рекомендацію, тому що 

412
00:23:57,782 --> 00:23:59,780
ми не можемо дозволити машинам керувати нашим життям.

413
00:24:01,140 --> 00:24:04,728
І я вважаю, що я повинен згадати ще одну іншу річ, яка закінчилася ліворуч, 

414
00:24:04,728 --> 00:24:06,995
що значення невизначеності, ця кількість бітів, 

415
00:24:06,995 --> 00:24:09,640
більше не просто надлишкова з кількістю можливих збігів.

416
00:24:10,080 --> 00:24:15,210
Тепер, якщо ми витягнемо це вгору і порахуємо 2 до 8.02, що трохи вище 256, 

417
00:24:15,210 --> 00:24:18,989
я думаю, 259, це говорить про те, що, незважаючи на те, 

418
00:24:18,989 --> 00:24:22,162
що всього 526 слів відповідають цьому шаблону, 

419
00:24:22,162 --> 00:24:26,010
рівень невизначеності більше схожий на той, який був би, 

420
00:24:26,010 --> 00:24:28,980
якби було 259 однаково ймовірних результати.

421
00:24:29,720 --> 00:24:30,740
Ви можете думати про це так.

422
00:24:31,020 --> 00:24:34,536
Він знає, що borx не є відповіддю, те саме з yorts, zorl і zorus, 

423
00:24:34,536 --> 00:24:37,680
тому це трохи менш невизначено, ніж у попередньому випадку.

424
00:24:37,820 --> 00:24:39,280
Ця кількість бітів буде меншою.

425
00:24:40,220 --> 00:24:43,731
І якщо я продовжу грати в гру, я уточню це парою припущень, 

426
00:24:43,731 --> 00:24:46,540
які стосуються того, що я хотів би пояснити тут.

427
00:24:48,360 --> 00:24:51,421
За четвертим припущенням, якщо ви подивитеся на його найкращі варіанти, 

428
00:24:51,421 --> 00:24:53,760
ви побачите, що це вже не просто максимізація ентропії.

429
00:24:54,460 --> 00:24:57,188
Отже, на даний момент технічно є сім можливостей, 

430
00:24:57,188 --> 00:25:00,300
але єдині, які мають значний шанс, це гуртожиток і слова.

431
00:25:00,300 --> 00:25:04,380
І ви можете бачити, що він класифікує обидва вище за всі ці інші значення, 

432
00:25:04,380 --> 00:25:06,720
що, строго кажучи, дасть більше інформації.

433
00:25:07,240 --> 00:25:09,811
У перший раз, коли я зробив це, я просто склав ці два числа, 

434
00:25:09,811 --> 00:25:12,846
щоб виміряти якість кожного припущення, яке насправді спрацювало краще, 

435
00:25:12,846 --> 00:25:13,900
ніж ви могли підозрювати.

436
00:25:14,300 --> 00:25:17,072
Але це справді не здавалося систематичним, і я впевнений, що є інші підходи, 

437
00:25:17,072 --> 00:25:19,340
які люди могли б використати, але ось той, на який я зупинився.

438
00:25:19,760 --> 00:25:23,880
Якщо ми розглядаємо перспективу наступного припущення, як у цьому випадку слова, 

439
00:25:23,880 --> 00:25:27,900
те, що нас справді хвилює, це очікуваний рахунок нашої гри, якщо ми це зробимо.

440
00:25:28,230 --> 00:25:32,092
І щоб обчислити цей очікуваний бал, ми кажемо, яка ймовірність того, 

441
00:25:32,092 --> 00:25:35,900
що слова є фактичною відповіддю, яка на даний момент відповідає 58%.

442
00:25:36,040 --> 00:25:39,540
Ми кажемо, що з імовірністю 58% наш рахунок у цій грі буде 4.

443
00:25:40,320 --> 00:25:45,640
І тоді з імовірністю 1 мінус ці 58% наша оцінка буде більшою за ці 4.

444
00:25:46,220 --> 00:25:49,514
Скільки ще ми не знаємо, але ми можемо оцінити це на основі того, 

445
00:25:49,514 --> 00:25:52,460
скільки невизначеності буде, коли ми дійдемо до цієї точки.

446
00:25:52,960 --> 00:25:55,940
Зокрема, на даний момент є 1.44 біти невизначеності.

447
00:25:56,440 --> 00:25:59,383
Якщо ми вгадуємо слова, це означає, що очікувана інформація, 

448
00:25:59,383 --> 00:26:01,120
яку ми отримаємо, дорівнює 1.27 біт.

449
00:26:01,620 --> 00:26:04,055
Отже, якщо ми вгадуємо слова, ця різниця показує, 

450
00:26:04,055 --> 00:26:07,660
скільки невизначеності ми, ймовірно, залишимо після того, як це станеться.

451
00:26:08,260 --> 00:26:10,892
Нам потрібна якась функція, яку я тут називаю f, 

452
00:26:10,892 --> 00:26:13,740
яка пов’язує цю невизначеність із очікуваною оцінкою.

453
00:26:14,240 --> 00:26:18,266
І те, як це було зроблено, полягало в тому, що просто побудували групу даних 

454
00:26:18,266 --> 00:26:21,247
із попередніх ігор на основі версії 1 бота, щоб сказати, 

455
00:26:21,247 --> 00:26:25,012
яким був фактичний рахунок після різних моментів з певною дуже вимірною 

456
00:26:25,012 --> 00:26:26,320
кількістю невизначеності.

457
00:26:27,020 --> 00:26:30,853
Наприклад, ці точки даних тут знаходяться вище значення приблизно 8.7 

458
00:26:30,853 --> 00:26:33,866
або близько того кажуть для деяких ігор після моменту, 

459
00:26:33,866 --> 00:26:37,152
коли було 8.7 біт невизначеності, знадобилося дві здогадки, 

460
00:26:37,152 --> 00:26:38,960
щоб отримати остаточну відповідь.

461
00:26:39,320 --> 00:26:42,240
Для інших ігор потрібно було три відгадки, для інших ігор – чотири.

462
00:26:43,140 --> 00:26:46,743
Якщо ми перемістимося вліво тут, усі точки над нулем говорять про те, 

463
00:26:46,743 --> 00:26:50,347
що будь-коли є нуль біт невизначеності, тобто є лише одна можливість, 

464
00:26:50,347 --> 00:26:54,260
тоді необхідна кількість припущень завжди дорівнює лише одній, що заспокоює.

465
00:26:54,780 --> 00:26:57,829
Щоразу, коли була трішка невизначеності, тобто, по суті, 

466
00:26:57,829 --> 00:27:01,735
зводилася лише до двох можливостей, іноді вимагалося ще одне припущення, 

467
00:27:01,735 --> 00:27:03,020
іноді ще два припущення.

468
00:27:03,080 --> 00:27:05,240
І так далі і так далі тут.

469
00:27:05,740 --> 00:27:07,912
Можливо, дещо простіший спосіб візуалізувати ці 

470
00:27:07,912 --> 00:27:10,220
дані — об’єднати їх разом і взяти середні значення.

471
00:27:11,000 --> 00:27:14,130
Наприклад, ця смужка тут говорить, що серед усіх пунктів, 

472
00:27:14,130 --> 00:27:18,394
де ми мали хоча б трохи невизначеності, в середньому необхідна кількість нових 

473
00:27:18,394 --> 00:27:19,960
припущень була приблизно 1.5.

474
00:27:22,140 --> 00:27:24,831
І смужка тут говорить, що серед усіх різних ігор, 

475
00:27:24,831 --> 00:27:28,383
де в якийсь момент невизначеність була трохи вище чотирьох бітів, 

476
00:27:28,383 --> 00:27:30,912
що схоже на звуження до 16 різних можливостей, 

477
00:27:30,912 --> 00:27:35,380
то в середньому для цього потрібно трохи більше двох припущень з цієї точки вперед.

478
00:27:36,060 --> 00:27:38,253
І звідси я просто зробив регресію, щоб відповідати функції, 

479
00:27:38,253 --> 00:27:39,460
яка здавалася розумною для цього.

480
00:27:39,980 --> 00:27:43,040
І пам’ятайте, що весь сенс будь-чого з цього полягає в тому, 

481
00:27:43,040 --> 00:27:47,454
щоб ми могли кількісно оцінити цю інтуїцію: що більше інформації ми отримуємо зі слова, 

482
00:27:47,454 --> 00:27:48,960
то нижчим буде очікуваний бал.

483
00:27:49,680 --> 00:27:54,997
Отже, це як версія 2.0, якщо ми повернемося назад і запустимо той самий набір симуляцій, 

484
00:27:54,997 --> 00:27:59,240
зіставивши його з усіма 2315 можливими відповідями Wordle, як це вийде?

485
00:28:00,280 --> 00:28:03,420
Що ж, на відміну від нашої першої версії, вона точно краща, що заспокоює.

486
00:28:04,020 --> 00:28:07,279
Усе сказане та зроблене середній бал становить близько 3.6, хоча, 

487
00:28:07,279 --> 00:28:10,094
на відміну від першої версії, вона кілька разів програє, 

488
00:28:10,094 --> 00:28:12,120
і за цієї обставини вимагає більше шести.

489
00:28:12,639 --> 00:28:16,192
Мабуть тому, що бувають моменти, коли потрібно досягти мети, 

490
00:28:16,192 --> 00:28:17,940
а не максимізувати інформацію.

491
00:28:19,040 --> 00:28:21,000
Отже, ми можемо зробити краще, ніж 3.6?

492
00:28:22,080 --> 00:28:22,920
Ми точно можемо.

493
00:28:23,280 --> 00:28:26,149
На початку я сказав, що найцікавіше спробувати не включати 

494
00:28:26,149 --> 00:28:29,360
справжній список відповідей Wordle у спосіб побудови своєї моделі.

495
00:28:29,880 --> 00:28:32,442
Але якщо ми все-таки це включимо, найкраща продуктивність, 

496
00:28:32,442 --> 00:28:34,180
яку я міг отримати, була приблизно 3.43.

497
00:28:35,160 --> 00:28:38,739
Отже, якщо ми спробуємо вийти більш складним, ніж просто використовувати дані про частоту 

498
00:28:38,739 --> 00:28:41,961
слів, щоб вибрати цей попередній розподіл, це 3.43, ймовірно, дає максимум того, 

499
00:28:41,961 --> 00:28:44,228
наскільки добре ми можемо отримати з цим, або принаймні, 

500
00:28:44,228 --> 00:28:45,740
наскільки добре я можу отримати з цим.

501
00:28:46,240 --> 00:28:50,554
Ця найкраща продуктивність, по суті, просто використовує ідеї, про які я тут говорив, 

502
00:28:50,554 --> 00:28:54,216
але йде трохи далі, ніби шукає очікувану інформацію на два кроки вперед, 

503
00:28:54,216 --> 00:28:55,120
а не лише на один.

504
00:28:55,620 --> 00:28:58,483
Спочатку я планував більше поговорити про це, але я розумію, 

505
00:28:58,483 --> 00:29:00,220
що насправді ми пройшли досить довго.

506
00:29:00,580 --> 00:29:02,891
Єдине, що я скажу: після цього двоетапного пошуку, 

507
00:29:02,891 --> 00:29:05,882
а потім запустивши пару зразків симуляцій у найкращих кандидатах, 

508
00:29:05,882 --> 00:29:09,100
наразі, принаймні для мене, здається, що Crane є найкращим відкривачем.

509
00:29:09,100 --> 00:29:10,060
Хто б міг здогадатися?

510
00:29:10,920 --> 00:29:14,265
Крім того, якщо ви використовуєте справжній список слів для визначення простору 

511
00:29:14,265 --> 00:29:17,820
можливостей, тоді невизначеність, з якої ви починаєте, становить трохи більше 11 біт.

512
00:29:18,300 --> 00:29:22,306
І виявилося, що лише під час грубого пошуку максимально можлива очікувана 

513
00:29:22,306 --> 00:29:25,880
інформація після перших двох припущень становить приблизно 10 біт.

514
00:29:26,500 --> 00:29:30,856
Це свідчить про те, що в найкращому випадку, після ваших перших двох припущень, 

515
00:29:30,856 --> 00:29:34,560
з ідеально оптимальною грою, ви залишитеся з дещицею невизначеності.

516
00:29:34,800 --> 00:29:37,960
Це те саме, що мати два можливі припущення.

517
00:29:37,960 --> 00:29:41,226
Тож я вважаю справедливим і, ймовірно, досить консервативним сказати, 

518
00:29:41,226 --> 00:29:45,053
що ви ніколи не зможете написати алгоритм, який отримає це середнє значення до 3, 

519
00:29:45,053 --> 00:29:48,739
тому що з доступними вам словами просто не буде місця для отримання достатньої 

520
00:29:48,739 --> 00:29:52,659
інформації лише за два кроки здатний гарантувати відповідь у третьому слоті кожного 

521
00:29:52,659 --> 00:29:53,360
разу без збоїв.

