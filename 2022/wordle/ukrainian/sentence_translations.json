[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy.",
  "translatedText": "Гра Wurdle стала досить вірусною за останній місяць чи два, і я ніколи не пропускав можливість уроку математики, мені спадає на думку, що ця гра є дуже хорошим центральним прикладом уроку з теорії інформації, зокрема тема, відома як ентропія.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could.",
  "translatedText": "Розумієте, як і багатьох людей, мене затягнуло головоломкою, і, як і багатьох програмістів, я також був затягнутий у спробі написати алгоритм, який би грав у гру якомога оптимальніше.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy.",
  "translatedText": "І те, що я думав зробити тут, це просто поговорити з вами про мій процес і пояснити деякі математичні обчислення, які в нього входять, оскільки весь алгоритм зосереджений на цій ідеї ентропії.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle?",
  "translatedText": "Перш за все, якщо ви не чули про це, що таке Wurdle?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us.",
  "translatedText": "І щоб убити двох зайців одним пострілом, поки ми проходимо правила гри, дозвольте мені також переглянути, куди ми йдемо з цим, тобто розробити маленький алгоритм, який, в основному, гратиме за нас.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does.",
  "translatedText": "Хоча я не робив сьогодні Wurdle, це 4 лютого, і ми побачимо, як бот впорається.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess.",
  "translatedText": "Мета Wurdle — вгадати таємниче слово з п’яти літер, і вам дається шість різних шансів вгадати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane.",
  "translatedText": "Наприклад, мій бот Wurdle пропонує мені почати з журавля.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer.",
  "translatedText": "Кожного разу, коли ви припускаєте, ви отримуєте певну інформацію про те, наскільки близькі ваші припущення до істинної відповіді.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer.",
  "translatedText": "Тут сірий квадрат говорить мені, що у фактичній відповіді немає C.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position.",
  "translatedText": "Жовте поле говорить мені, що є R, але воно не в цьому положенні.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position.",
  "translatedText": "Зелений квадрат говорить мені, що секретне слово дійсно має літеру А, і воно стоїть на третій позиції.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E.",
  "translatedText": "І тоді немає ні N, ні E.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information.",
  "translatedText": "Тож дозвольте мені просто зайти та повідомити боту Wurdle цю інформацію.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey.",
  "translatedText": "Ми почали з крана, ми отримали сірий, жовтий, зелений, сірий, сірий.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time.",
  "translatedText": "Не турбуйтеся про всі дані, які він зараз показує, я поясню це свого часу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick.",
  "translatedText": "Але його найкраща пропозиція для нашого другого вибору є химерною.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess.",
  "translatedText": "І ваше припущення має бути справжнім словом із п’яти літер, але, як ви побачите, це досить ліберально щодо того, що насправді дозволить вам вгадати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick.",
  "translatedText": "У цьому випадку ми намагаємося shtick.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good.",
  "translatedText": "І добре, все виглядає досить добре.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R.",
  "translatedText": "Ми натиснули S і H, тому ми знаємо перші три літери, ми знаємо, що є R.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something.",
  "translatedText": "Тож це буде як SHA щось R або SHA R щось.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp.",
  "translatedText": "І, схоже, бот Wurdle знає, що він має лише дві можливості: shard або sharp.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a tossup between them at this point, so I guess probably just because it's alphabetical it goes with shard.",
  "translatedText": "Наразі між ними важко вибрати, тож, мабуть, просто тому, що це алфавітний шрифт, він більше підходить для shard.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer, so we got it in three.",
  "translatedText": "Що, на щастя, є правильною відповіддю, тож ми впоралися з цим завданням за три хвилини.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.22,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle, four is par and three is birdie.",
  "translatedText": "Якщо вам цікаво, чи добре це, то я чув, як одна людина сказала, що у Вурдлі чотири - це пар, а три - бьорд.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy.",
  "translatedText": "Що, на мою думку, є досить влучною аналогією.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy.",
  "translatedText": "Ви повинні бути постійно в своїй грі, щоб отримати чотири, але це точно не божевілля.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great.",
  "translatedText": "Але коли ви отримуєте це за три, це просто чудово.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot.",
  "translatedText": "Отже, якщо ви не за це, я хотів би просто поговорити про мій процес мислення з самого початку про те, як я підходжу до бота Wurdle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson.",
  "translatedText": "І, як я вже сказав, це дійсно привід для уроку теорії інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy.",
  "translatedText": "Основна мета – пояснити, що таке інформація, а що таке ентропія.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language.",
  "translatedText": "Моєю першою думкою при підході до цього було поглянути на відносні частоти різних літер в англійській мові.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters?",
  "translatedText": "Тож я подумав: гаразд, чи є початкове припущення чи початкова пара припущень, яка вражає багато цих найчастіших літер?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails.",
  "translatedText": "І один, який я дуже любив, робив інший, а потім цвяхи.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good, it feels like you're getting information.",
  "translatedText": "Ідея полягає в тому, що коли ви натискаєте на літеру, ви отримуєте зелений або жовтий колір, це завжди приємно, це означає, що ви отримуєте інформацію.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.76,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information, since it's pretty rare to find a word that doesn't have any of these letters.",
  "translatedText": "Але в цих випадках, навіть якщо ви не влучаєте і завжди отримуєте сірий колір, це все одно дає вам багато інформації, оскільки досить рідко можна знайти слово, в якому немає жодної з цих літер.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters.",
  "translatedText": "Але все одно це не виглядає надсистематичним, тому що, наприклад, не враховується порядок літер.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail?",
  "translatedText": "Навіщо друкувати цвяхи, коли я можу надрукувати равлика?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end?",
  "translatedText": "Чи краще мати це S в кінці?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure.",
  "translatedText": "Я не дуже впевнений.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y.",
  "translatedText": "Тепер мій друг сказав, що йому подобається починати словом weary, що мене дещо здивувало, оскільки там є деякі незвичайні літери, як-от W та Y.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener.",
  "translatedText": "Але хто знає, можливо, це кращий відкривач.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess?",
  "translatedText": "Чи існує якась кількісна оцінка, яку ми можемо надати, щоб оцінити якість потенційного припущення?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up.",
  "translatedText": "Тепер, щоб підготуватися до того, як ми будемо ранжувати можливі припущення, давайте повернемося назад і внесемо трохи ясності в те, як саме налаштована гра.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long.",
  "translatedText": "Отже, є список слів, які можна ввести, які вважаються дійсними припущеннями, і складається лише з 13 000 слів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble.",
  "translatedText": "Але якщо ви подивитесь на це, ви побачите багато справді незвичайних речей, таких як голова або Алі та ARG, тип слів, які викликають сімейні суперечки в грі в Скрабл.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word.",
  "translatedText": "Але настрій гри полягає в тому, що відповіддю завжди буде досить поширене слово.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers.",
  "translatedText": "І насправді, є ще один список із приблизно 2300 слів, які є можливими відповідями.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creators girlfriend, which is kind of fun.",
  "translatedText": "І це список, який курує людина, я думаю, що саме подруга творців гри, і це дуже весело.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving wordle that doesn't incorporate previous knowledge about this list.",
  "translatedText": "Але що я хотів би зробити, наше завдання в цьому проекті - побачити, чи зможемо ми написати програму, яка розв'язує wordle без використання попередніх знань про цей список.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list.",
  "translatedText": "По-перше, є багато досить поширених слів із п’яти літер, яких ви не знайдете в цьому списку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play wordle against anyone, not just what happens to be the official website.",
  "translatedText": "Тому було б краще написати програму, яка була б трохи стійкішою і могла б грати у вордл проти будь-кого, а не лише проти того, що є офіційним сайтом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also, the reason that we know what this list of possible answers is, is because it's visible in the source code.",
  "translatedText": "Крім того, ми знаємо, що це за список можливих відповідей, тому що його видно у вихідному коді.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day, that you could always just look up what tomorrow's answer will be.",
  "translatedText": "Але у вихідному коді це видно в певному порядку, в якому відповіді з'являються з дня на день, так що ви завжди можете просто подивитися, якою буде завтрашня відповідь.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating.",
  "translatedText": "Очевидно, що використання списку є обманом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words.",
  "translatedText": "І те, що робить головоломку цікавішою та насиченішим уроком теорії інформації, полягає в тому, що натомість можна використовувати деякі більш універсальні дані, такі як відносна частота слів у цілому, щоб вловити цю інтуїцію про те, що ми надаємо перевагу більш поширеним словам.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess?",
  "translatedText": "Тож як із цих 13 000 можливостей вибрати початкове припущення?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality?",
  "translatedText": "Наприклад, якщо мій друг пропонує втомленого, як ми повинні проаналізувати його якість?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W.",
  "translatedText": "Що ж, причина, чому він сказав, що йому подобається це малоймовірне W, полягає в тому, що йому подобається довгострокова природа того, наскільки добре це відчуваєш, якщо ти влучив у це W.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern.",
  "translatedText": "Наприклад, якщо перша виявлена закономірність була приблизно такою, то виявиться, що в цьому гігантському лексиконі лише 58 слів відповідають цій моделі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000.",
  "translatedText": "Тож це величезне зниження з 13 000.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this.",
  "translatedText": "Але зворотна сторона цього, звичайно, полягає в тому, що дуже рідко отримати такий візерунок.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000.",
  "translatedText": "Зокрема, якби кожне слово з однаковою ймовірністю було відповіддю, ймовірність досягнення цього шаблону дорівнювала б 58 поділеним приблизно на 13 000.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers.",
  "translatedText": "Звичайно, вони не однаково ймовірні відповіді.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words.",
  "translatedText": "Більшість із них дуже незрозумілі та навіть сумнівні слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later.",
  "translatedText": "Але принаймні для нашого першого проходження всього цього, давайте припустимо, що всі вони однаково ймовірні, а потім уточнимо це трохи пізніше.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is, the pattern with a lot of information is, by its very nature, unlikely to occur.",
  "translatedText": "Справа в тому, що шаблон з великою кількістю інформації, за своєю природою, малоймовірний.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely.",
  "translatedText": "Насправді бути інформативним означає те, що це малоймовірно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where, of course, there's not a W in it.",
  "translatedText": "Набагато більш вірогідним візерунком для цього отвору було б щось на кшталт цього, де, звісно, немає літери \"W\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y.",
  "translatedText": "Можливо, там є E, а можливо, немає A, немає R, немає Y.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches.",
  "translatedText": "У цьому випадку є 1400 можливих збігів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see.",
  "translatedText": "Якби все було однаково вірогідним, виходить, що ймовірність приблизно 11% того, що ви б побачили саме цю модель.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative.",
  "translatedText": "Таким чином, найімовірніші результати також є найменш інформативними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see.",
  "translatedText": "Щоб отримати більш глобальне уявлення, дозвольте мені показати вам повний розподіл ймовірностей за всіма різними шаблонами, які ви можете побачити.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common.",
  "translatedText": "Таким чином, кожна смужка, на яку ви дивитеся, відповідає можливому шаблону кольорів, який можна виявити, з яких є від 3 до 5 варіантів, і вони організовані зліва направо, від найпоширенішого до найменш поширеного.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays.",
  "translatedText": "Отже, найпоширенішою можливістю є те, що ви отримаєте всі сірі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time.",
  "translatedText": "Це відбувається приблизно в 14% випадків.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here, where there's only 18 possibilities for what matches this pattern, that evidently look like this.",
  "translatedText": "І те, на що ви сподіваєтеся, коли вгадуєте, - це те, що ви опинитеся десь в кінці цього довгого хвоста, як тут, де є лише 18 варіантів, які відповідають цьому шаблону, і які, очевидно, виглядають так.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here.",
  "translatedText": "Або якщо ми підемо трохи ліворуч, то, можливо, ми підемо сюди.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you.",
  "translatedText": "Добре, ось вам гарна головоломка.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them?",
  "translatedText": "Які три слова в англійській мові починаються з літери W, закінчуються літерою Y і десь містять букву R?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly.",
  "translatedText": "Виявляється, відповіді, давайте подивимося, багатослівні, червиві та іронічні.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution.",
  "translatedText": "Отже, щоб оцінити, наскільки добре це слово в цілому, ми хочемо якийсь вимір очікуваної кількості інформації, яку ви збираєтеся отримати від цього розподілу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score.",
  "translatedText": "Якщо ми переглянемо кожен шаблон і помножимо його ймовірність появи на те, що вимірює його інформативність, це може дати нам об’єктивну оцінку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches.",
  "translatedText": "Тепер вашим першим інстинктом щодо того, що це має бути, може бути кількість збігів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches.",
  "translatedText": "Вам потрібна менша середня кількість збігів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer.",
  "translatedText": "Але натомість я хотів би використовувати більш універсальне вимірювання, яке ми часто приписуємо інформації, і таке, яке буде більш гнучким, коли ми матимемо різну ймовірність, призначену кожному з цих 13 000 слів щодо того, чи є вони справді відповіддю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but is really intuitive if we just look at examples.",
  "translatedText": "Стандартною одиницею інформації є біт, який має дещо кумедну формулу, але насправді інтуїтивно зрозумілий, якщо ми просто подивимося на приклади.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information.",
  "translatedText": "Якщо у вас є спостереження, яке вдвічі скорочує ваш простір можливостей, ми кажемо, що воно містить один біт інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about half of the five letter words have an S, a little less than that, but about half.",
  "translatedText": "У нашому прикладі простір можливостей - це всі можливі слова, і виходить, що близько половини п'ятилітерних слів мають літеру S, трохи менше, але теж близько половини.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information.",
  "translatedText": "Таким чином, це спостереження дасть вам трохи інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information.",
  "translatedText": "Якщо натомість новий факт скорочує цей простір можливостей у чотири рази, ми говоримо, що він містить два біти інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T.",
  "translatedText": "Наприклад, виявилося, що приблизно чверть цих слів мають Т.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth.",
  "translatedText": "Якщо спостереження скорочує цей простір у вісім, ми говоримо, що це три біти інформації, і так далі і так далі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a sixteenth, five bits cuts it into a thirty second.",
  "translatedText": "Чотири біти розрізають його на шістнадцяті, п'ять бітів - на тридцять другі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.88
 },
 {
  "input": "So now's when you might want to take a moment and pause and ask for yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence?",
  "translatedText": "Отже, зараз саме той момент, коли ви, можливо, захочете зробити паузу і запитати себе, яка формула для кількості бітів інформації з точки зору ймовірності її появи?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 534.96,
  "end": 542.98
 },
 {
  "input": "Well, what we're saying here is basically that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability.",
  "translatedText": "Ми маємо на увазі те, що коли ви берете одну половину на кількість бітів, це те саме, що ймовірність, що те саме, що сказати, що два в степені кількості бітів є одиницею над ймовірністю, що далі переставляє, кажучи, що інформація є двома логарифмами одиниці, поділеними на ймовірність.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still where the information is the negative log base two of the probability.",
  "translatedText": "А іноді ви бачите це ще з однією перестановкою, де інформація є від'ємною основою другого степеня ймовірності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half.",
  "translatedText": "У такому вигляді це може здатися трохи дивним для непосвячених, але насправді це просто дуже інтуїтивна ідея запитати, скільки разів ви скоротили свої можливості вдвічі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture?",
  "translatedText": "А тепер, якщо вам цікаво, знаєте, я думав, що ми просто граємо у веселу гру слів, чому логарифми з’являються?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095.",
  "translatedText": "Одна з причин, чому це краща одиниця, полягає в тому, що набагато простіше говорити про дуже малоймовірні події, набагато легше сказати, що спостереження містить 20 біт інформації, ніж сказати, що ймовірність такого-то виникнення дорівнює 0.0000095.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together.",
  "translatedText": "Але більш суттєвою причиною того, що цей логарифмічний вираз виявився дуже корисним доповненням до теорії ймовірності, є спосіб додавання інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information.",
  "translatedText": "Наприклад, якщо одне спостереження дає вам два біти інформації, скорочуючи ваш простір учетверо, а потім друге спостереження, подібне до вашого другого припущення в Wordle, дає вам ще три біти інформації, скорочуючи вас ще на один коефіцієнт вісім, два разом дають п’ять біт інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add.",
  "translatedText": "Подібно до того, як ймовірності люблять множитися, інформація любить додаватися.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with.",
  "translatedText": "Отже, як тільки ми потрапляємо в область чогось на зразок очікуваного значення, де ми додаємо купу чисел, журнали роблять це набагато зручнішим для роботи.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for weary and add another little tracker on here, showing us how much information there is for each pattern.",
  "translatedText": "Давайте повернемося до нашого дистрибутива для втомлених і додамо сюди ще один маленький трекер, який показує, скільки інформації є для кожного шаблону.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain.",
  "translatedText": "Головне, на що я хочу, щоб ви звернули увагу, це те, що чим вища ймовірність, коли ми дійдемо до тих більш вірогідних моделей, тим менше інформації, тим менше бітів ви отримуєте.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information.",
  "translatedText": "Вимірювання якості цього припущення буде полягати в тому, що ми візьмемо очікуване значення цієї інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 643.5,
  "end": 648.02
 },
 {
  "input": "When we go through each pattern, we say how probable is it and then we multiply that by how many bits of information do we get.",
  "translatedText": "Коли ми переглядаємо кожен шаблон, ми говоримо, наскільки він вірогідний, а потім множимо це на кількість бітів інформації, яку ми отримуємо.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 648.42,
  "end": 654.06
 },
 {
  "input": "And in the example of weary, that turns out to be 4.9 bits.",
  "translatedText": "У прикладі з weary це виходить 4,9 біта.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times.",
  "translatedText": "Отже, у середньому інформація, яку ви отримуєте з цього початкового припущення, настільки ж хороша, як скорочення вашого простору можливостей навпіл приблизно в п’ять разів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like slate.",
  "translatedText": "Навпаки, прикладом здогадки з вищою очікуваною інформаційною цінністю може бути щось на кшталт шиферу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case, you'll notice the distribution looks a lot flatter.",
  "translatedText": "У цьому випадку ви помітите, що розподіл виглядає набагато пласкішим.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information.",
  "translatedText": "Зокрема, найімовірніша поява всіх сірих має лише близько 6% ймовірності появи, тому ви отримуєте мінімум 3.9 біт інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that.",
  "translatedText": "Але це мінімум, зазвичай ви отримаєте щось краще за це.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8.",
  "translatedText": "І виявляється, коли ви обчислюєте цифри на цьому місці та додаєте всі відповідні терміни, середня інформація становить близько 5.8.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with weary, your space of possibilities will be about half as big after this first guess, on average.",
  "translatedText": "Тож, на відміну від втомленого, ваш простір можливостей після цього першого здогаду буде в середньому вдвічі меншим.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity.",
  "translatedText": "Насправді є весела історія про назву цього очікуваного значення кількості інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "You see, information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science.",
  "translatedText": "Теорію інформації розробив Клод Шеннон, який працював у Bell Labs у 1940-х роках, але він говорив про деякі зі своїх ідей, які ще не були опубліковані, з Джоном фон Нейманом, який був цим інтелектуальним гігантом того часу, дуже видатним у математиці та фізиці та початках того, що ставало інформатикою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well, you should call it entropy, and for two reasons.",
  "translatedText": "І коли він згадав, що у нього немає гарної назви для цієї очікуваної величини кількості інформації, фон Нейман нібито сказав, що, мовляв, варто назвати її ентропією, і з двох причин.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage.",
  "translatedText": "По-перше, ваша функція невизначеності використовувалася в статистичній механіці під такою назвою, тож вона вже має назву, а по-друге, що ще важливіше, ніхто не знає, що таке ентропія насправді, тому в дебатах ви завжди будете мають перевагу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design.",
  "translatedText": "Отже, якщо назва здається трохи загадковою, і якщо вірити цій історії, це начебто задум.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess.",
  "translatedText": "Крім того, якщо вам цікаво його відношення до всього другого закону термодинаміки з фізики, зв’язок точно є, але в його витоках Шеннон мав справу лише з чистою теорією ймовірностей, і для наших цілей тут, коли я використовую слово ентропія, я просто хочу, щоб ви подумали про очікувану інформаційну цінність конкретного припущення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously.",
  "translatedText": "Ви можете думати про ентропію як про вимірювання двох речей одночасно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution?",
  "translatedText": "Перше - наскільки рівномірним є розподіл?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be.",
  "translatedText": "Чим ближче рівномірний розподіл, тим вищою буде ентропія.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy.",
  "translatedText": "У нашому випадку, коли існує від 3 до 5-го загальних шаблонів, для рівномірного розподілу, спостереження за будь-яким із них матиме інформаційну базу журналу 2 з 3 до 5-го, що дорівнює 7.92, тож це абсолютний максимум, який ви могли б отримати для цієї ентропії.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place.",
  "translatedText": "Але ентропія також є своєрідним показником того, скільки можливостей існує.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits.",
  "translatedText": "Наприклад, якщо у вас є якесь слово, де є лише 16 можливих шаблонів, і кожен з них однаково вірогідний, ця ентропія, ця очікувана інформація буде 4 біти.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits.",
  "translatedText": "Але якщо у вас є інше слово, де є 64 можливі шаблони, які можуть виникнути, і всі вони однаково вірогідні, тоді ентропія буде складати 6 біт.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes.",
  "translatedText": "Отже, якщо ви бачите якийсь розподіл у дикій природі, який має ентропію 6 біт, це ніби говорить про те, що в тому, що має статися, існує стільки варіацій і невизначеності, як якщо б було 64 однаково ймовірні результати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this.",
  "translatedText": "Під час мого першого проходу в Wurtelebot я просто зробив це.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the different possible guesses that you could have, all 13,000 words, it computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns that you might see for each one, and then it picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible.",
  "translatedText": "Він переглядає всі можливі припущення, які ви можете мати, усі 13 000 слів, обчислює ентропію для кожного з них, або, точніше, ентропію розподілу за всіма шаблонами, які ви можете побачити, для кожного з них, і вибирає найвище, оскільки це той, який, швидше за все, максимально скоротить ваш простір можливостей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses.",
  "translatedText": "І незважаючи на те, що я говорив тут лише про перше припущення, воно робить те саме для кількох наступних припущень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words.",
  "translatedText": "Наприклад, після того, як ви бачите певний шаблон у цій першій здогадці, яка обмежила б вас меншою кількістю можливих слів на основі того, що з цим збігається, ви просто граєте в ту саму гру щодо цього меншого набору слів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy.",
  "translatedText": "Для запропонованого другого припущення ви дивитеся на розподіл усіх шаблонів, які можуть виникати з цього більш обмеженого набору слів, ви шукаєте всі 13 000 можливостей і знаходите ту, яка максимізує цю ентропію.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins.",
  "translatedText": "Щоб показати вам, як це працює в дії, дозвольте мені просто витягнути невеликий варіант Wurtele, який я написав, який показує основні моменти цього аналізу на полях.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "So after doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information.",
  "translatedText": "Отже, після всіх розрахунків ентропії, праворуч ми бачимо, які з них мають найбільшу очікувану інформацію.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch.",
  "translatedText": "Виявляється, головною відповіддю, принаймні на даний момент, ми уточнимо це пізніше, є Тарес, що означає, гм, звичайно, вика, найпоширеніша вика.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got given this particular pattern.",
  "translatedText": "Кожного разу, коли ми робимо припущення, наприклад, що я проігнорував його рекомендації і використовую грифель, тому що мені подобається грифель, ми бачимо, скільки очікуваної інформації було отримано, а потім праворуч від цього слова показується, скільки фактичної інформації ми отримали, враховуючи цей конкретний шаблон.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that.",
  "translatedText": "Тож тут, схоже, нам трохи не пощастило, очікували, що ми отримаємо 5.8, але випадково ми отримали щось менше, ніж це.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now.",
  "translatedText": "А потім ліворуч тут показано всі різні можливі слова, де ми зараз знаходимося.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment.",
  "translatedText": "Сині смужки вказують на те, наскільки вірогідним є кожне слово, тому наразі припускається, що кожне слово буде однаково ймовірно, але ми уточнимо це за мить.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities.",
  "translatedText": "І тоді це вимірювання невизначеності говорить нам про ентропію цього розподілу між можливими словами, що зараз, оскільки це рівномірний розподіл, є просто непотрібно складним способом підрахувати кількість можливостей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities.",
  "translatedText": "Наприклад, якби ми взяли 2 у ступінь 13.66, це має бути приблизно 13 000 можливостей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "Um, a little bit off here, but only because I'm not showing all the decimal places.",
  "translatedText": "Тут трохи не так, але тільки тому, що я не показую всі десяткові знаки.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute.",
  "translatedText": "На даний момент це може здатися зайвим і занадто складним, але за хвилину ви зрозумієте, чому корисно мати обидва номери.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Raman, which again just really doesn't feel like a word.",
  "translatedText": "Тож, схоже, що найвищою ентропією для нашого другого припущення є комбінаційне розсіювання, що знову ж таки не схоже на слово.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here I'm going to go ahead and type in Rains.",
  "translatedText": "Тож, щоб зайняти моральну позицію, я збираюся продовжити і набрати \"Дощі\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky.",
  "translatedText": "І знову схоже, що нам трохи не пощастило.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information.",
  "translatedText": "Ми чекали 4.3 біти, а ми отримали лише 3.39 біт інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities.",
  "translatedText": "Отже, ми маємо 55 можливостей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means.",
  "translatedText": "І тут, можливо, я просто прийму те, що він пропонує, тобто комбо, що б це не означало.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And, okay, this is actually a good chance for a puzzle.",
  "translatedText": "І, гаразд, це насправді хороший шанс для пазла.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information.",
  "translatedText": "Це говорить нам, що цей шаблон дає нам 4.7 біт інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty.",
  "translatedText": "Але ліворуч, перш ніж ми побачимо цей шаблон, їх було 5.78 біт невизначеності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities?",
  "translatedText": "Отже, як вікторина для вас, що це означає щодо кількості можливостей, що залишилися?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well it means that we're reduced down to 1 bit of uncertainty, which is the same thing as saying that there's 2 possible answers.",
  "translatedText": "Це означає, що у нас залишився лише один елемент невизначеності, а це те ж саме, що сказати, що є 2 можливі відповіді.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice.",
  "translatedText": "Це вибір 50 на 50.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss.",
  "translatedText": "І звідси, оскільки ми з вами знаємо, які слова є більш поширеними, ми знаємо, що відповідь має бути безодня.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that.",
  "translatedText": "Але, як зараз написано, програма цього не знає.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it.",
  "translatedText": "Тож він просто продовжує, намагаючись отримати якомога більше інформації, доки не залишиться лише одна можливість, а потім здогадується про це.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy, but let's say we call this version 1 of our wordle solver, and then we go and run some simulations to see how it does.",
  "translatedText": "Очевидно, що нам потрібна краща стратегія в ендшпілі, але, скажімо, ми назвемо це версією 1 нашого вордл-розв'язувача, а потім проведемо кілька симуляцій, щоб побачити, як він працює.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game.",
  "translatedText": "Отже, як це працює, це гра в усі можливі ігри зі словами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers.",
  "translatedText": "Він переглядає всі ці 2315 слів, які є фактичними відповідями Wordle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set.",
  "translatedText": "В основному це використовується як набір для тестування.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice.",
  "translatedText": "І з цим наївним методом не брати до уваги, наскільки поширене слово, і просто намагатися максимізувати інформацію на кожному кроці на цьому шляху, доки не дійде до одного й лише одного вибору.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124.",
  "translatedText": "Наприкінці симуляції середній бал виходить близько 4.124.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expect it to do worse.",
  "translatedText": "Що непогано, якщо чесно, я очікував гіршого.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4.",
  "translatedText": "Але люди, які грають у wordle, скажуть вам, що вони зазвичай можуть отримати його за 4.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can.",
  "translatedText": "Справжнє завдання — отримати якомога більше за 3.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3.",
  "translatedText": "Це досить великий стрибок між рахунком 4 і рахунком 3.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low-hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that.",
  "translatedText": "Очевидно, що найпростіше - це якось врахувати, чи є слово загальновживаним, і як саме ми це робимо.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language, and I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset.",
  "translatedText": "Я вирішив отримати список відносних частот для всіх слів в англійській мові, і я просто використав функцію Mathematica's word frequency data, яка сама бере дані з публічного набору даних Google Books English Ngram.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words.",
  "translatedText": "І на це цікаво дивитися, наприклад, якщо ми відсортуємо його від найбільш поширених слів до найменш поширених слів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5-letter words in the English language.",
  "translatedText": "Очевидно, що це найпоширеніші 5-літерні слова в англійській мові.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common.",
  "translatedText": "А точніше, це 8 місце за поширеністю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there.",
  "translatedText": "Спочатку який, потім там і там.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common.",
  "translatedText": "Перший сам по собі не перший, а 9-й, і цілком зрозуміло, що ці інші слова можуть зустрічатися частіше, де ті, що стоять після першого, є після, де, а ті, які є трохи рідше.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency, because for example which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely.",
  "translatedText": "Тепер, використовуючи ці дані для моделювання ймовірності того, що кожне з цих слів стане остаточною відповіддю, вона не повинна бути просто пропорційною частоті, оскільки, наприклад, у цьому наборі даних слово \"коса\" має оцінку 0,002, тоді як слово \"коса\" в певному сенсі є в 1000 разів менш імовірним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering, so we want more of a binary cutoff.",
  "translatedText": "Але обидва ці слова є досить поширеними, тому їх майже напевно варто розглянути, тому ми хочемо більше двійкового відсікання.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty.",
  "translatedText": "Я пішов з цього приводу: уявити весь цей відсортований список слів, а потім розташувати його на осі х, а потім застосувати функцію sigmoid, яка є стандартним способом отримання функції, вихід якої в основному є двійковим, це або 0, або 1, але між ними є згладжування для цієї області невизначеності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis.",
  "translatedText": "Таким чином, по суті, ймовірність того, що я призначаю кожному слову для того, щоб потрапити в остаточний список, буде значенням сигмоїдної функції вище, де б воно не знаходилося на осі х.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff.",
  "translatedText": "Тепер, очевидно, це залежить від кількох параметрів, наприклад, наскільки широкий простір на осі х заповнюють ці слова, визначає, наскільки поступово або круто ми знижуємося від 1 до 0, і те, де ми їх розташовуємо зліва направо, визначає межу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "And to be honest the way I did this was kind of just licking my finger and sticking it into the wind.",
  "translatedText": "Чесно кажучи, те, як я це зробив, було просто облизати палець і тицьнути його на вітер.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff.",
  "translatedText": "Я переглянув відсортований список і спробував знайти вікно, у якому, дивлячись на нього, я вирішив, що приблизно половина цих слів, швидше за все, є остаточною відповіддю, і використав це як межу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Now once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement.",
  "translatedText": "Тепер, коли ми маємо такий розподіл між словами, це дає нам іншу ситуацію, коли ентропія стає дійсно корисним виміром.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were other and nails, and we end up with a situation where there's four possible words that match it.",
  "translatedText": "Наприклад, скажімо, ми грали в гру, і ми почали з моїх старих відкривачів, якими були \"інші\" та \"цвяхи\", і закінчили ситуацією, коли є чотири можливих слова, які підходять до нього.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely, let me ask you, what is the entropy of this distribution?",
  "translatedText": "І припустімо, що ми вважаємо їх усі однаково ймовірними, дозвольте запитати вас, яка ентропія цього розподілу?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2.",
  "translatedText": "Що ж, інформація, пов’язана з кожною з цих можливостей, буде базою журналу 2 з 4, оскільки кожна з них є 1 і 4, і це 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "2 bits of information, 4 possibilities.",
  "translatedText": "2 біти інформації, 4 можливості.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good.",
  "translatedText": "Все дуже добре і добре.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than 4 matches?",
  "translatedText": "Але що, якщо я скажу вам, що насправді існує більше 4-х збігів?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it.",
  "translatedText": "Насправді, коли ми переглядаємо повний список слів, ми знаходимо 16 слів, які йому відповідають.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure.",
  "translatedText": "Але припустімо, що наша модель надає справді низьку ймовірність того, що ці інші 12 слів є остаточною відповіддю, приблизно 1 з 1000, тому що вони дійсно незрозумілі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution?",
  "translatedText": "Тепер дозвольте запитати вас, яка ентропія цього розподілу?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before.",
  "translatedText": "Якби ентропія вимірювала лише кількість збігів, тоді можна було б очікувати, що це буде щось на кшталт логарифмічної бази 2 із 16, що дорівнюватиме 4, на два біти невизначеності більше, ніж ми мали раніше.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before.",
  "translatedText": "Але, звісно, фактична невизначеність не дуже відрізняється від того, що ми мали раніше.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example.",
  "translatedText": "Те, що є ці 12 справді незрозумілих слів, не означає, що було б ще більш дивно дізнатися, що остаточною відповіддю є, наприклад, чарівність.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits.",
  "translatedText": "Отже, коли ви робите обчислення тут і складаєте ймовірність кожної події, помножену на відповідну інформацію, ви отримуєте 2,11 біта.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "Just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it.",
  "translatedText": "Просто кажучи, це в основному два біти, в основному ці чотири можливості, але є трохи більше невизначеності через всі ці малоймовірні події, хоча, якщо ви вивчите їх, ви отримаєте тонну інформації з цього.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson.",
  "translatedText": "Отже, зменшуючи масштаб, це частина того, що робить Wordle таким гарним прикладом для уроку теорії інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy.",
  "translatedText": "Ми маємо ці два різні застосування ентропії.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words we have possible.",
  "translatedText": "Перший каже нам, яку очікувану інформацію ми отримаємо від даного припущення, а другий - чи можемо ми виміряти невизначеність, що залишилася, серед усіх можливих слів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation.",
  "translatedText": "І я маю підкреслити, що в першому випадку, коли ми розглядаємо очікувану інформацію про припущення, як тільки ми маємо нерівну вагу слів, це впливає на обчислення ентропії.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words.",
  "translatedText": "Наприклад, дозвольте мені знайти той самий випадок, який ми розглядали раніше, розподілу, пов’язаного з Weary, але цього разу з використанням нерівномірного розподілу між усіма можливими словами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well.",
  "translatedText": "Тож дозвольте мені поглянути, чи зможу я знайти тут частину, яка б це добре ілюструвала.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here, this is pretty good.",
  "translatedText": "Гаразд, ось, це досить добре.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it.",
  "translatedText": "Тут ми маємо два суміжних шаблони, які приблизно однаково вірогідні, але один із них, як нам сказали, містить 32 можливі слова, які йому відповідають.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them.",
  "translatedText": "І якщо ми перевіримо, що це таке, це ті 32, які є дуже малоймовірними словами, якщо ви переглядаєте їх очима.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches.",
  "translatedText": "Важко знайти якісь правдоподібні відповіді, можливо, кричущі, але якщо ми подивимося на сусідній патерн у розподілі, який вважається майже таким же ймовірним, ми побачимо, що він має лише 8 можливих збігів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1386.66
 },
 {
  "input": "So a quarter as many matches, but it's about as likely.",
  "translatedText": "Тобто на чверть менше збігів, але приблизно так само ймовірно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1386.88,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why.",
  "translatedText": "І коли ми знайдемо ці сірники, ми зрозуміємо чому.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers like ring or wrath or raps.",
  "translatedText": "Деякі з цих відповідей справді правдоподібні, як-от кільце, гнів чи реп.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version two of the Wordlebot here.",
  "translatedText": "Щоб проілюструвати, як ми все це інтегруємо, дозвольте мені запустити другу версію Wordlebot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1402.3
 },
 {
  "input": "And there are two or three main differences from the first one that we saw.",
  "translatedText": "І є дві-три основні відмінності від першої, яку ми бачили.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1402.56,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer.",
  "translatedText": "По-перше, як я щойно сказав, спосіб, у який ми обчислюємо ці ентропії, ці очікувані значення інформації, тепер використовує точніший розподіл між шаблонами, який включає ймовірність того, що дане слово насправді буде відповіддю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number one, though the ones following are a bit different.",
  "translatedText": "Так сталося, що сльози все ще на першому місці, хоча наступні трохи відрізняються.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table.",
  "translatedText": "По-друге, коли він ранжує свої найпопулярніші варіанти, він тепер зберігатиме модель ймовірності того, що кожне слово є фактичною відповіддю, і він включатиме це у своє рішення, яке легше побачити, коли ми матимемо кілька припущень щодо стіл.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives.",
  "translatedText": "Знову ж таки, ігноруємо його рекомендацію, тому що ми не можемо дозволити машинам керувати нашим життям.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches.",
  "translatedText": "І я вважаю, що я повинен згадати ще одну іншу річ, яка закінчилася ліворуч, що значення невизначеності, ця кількість бітів, більше не просто надлишкова з кількістю можливих збігів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which would be a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes.",
  "translatedText": "Тепер, якщо ми витягнемо його і обчислимо 2 до 8.02, що буде трохи вище 256, я думаю, 259, це говорить про те, що навіть якщо є 526 слів, які фактично відповідають цьому шаблону, рівень невизначеності більше схожий на той, який був би, якби було 259 однаково ймовірних результатів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this.",
  "translatedText": "Ви можете думати про це так.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borks is not the answer, same with yorts and zorl and zorus.",
  "translatedText": "Він знає, що боркс - це не вихід, так само як і йортс, і зорл, і зорус.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1474.66
 },
 {
  "input": "So it's a little less uncertain than it was in the previous case.",
  "translatedText": "Отже, це трохи менш невизначено, ніж у попередньому випадку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.66,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller.",
  "translatedText": "Ця кількість бітів буде меншою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here.",
  "translatedText": "І якщо я продовжу грати в гру, я уточню це парою припущень, які стосуються того, що я хотів би пояснити тут.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy.",
  "translatedText": "За четвертим припущенням, якщо ви подивитеся на його найкращі варіанти, ви побачите, що це вже не просто максимізація ентропії.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words.",
  "translatedText": "Отже, на даний момент технічно є сім можливостей, але єдині, які мають значний шанс, це гуртожиток і слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values that strictly speaking would give more information.",
  "translatedText": "І ви можете бачити, що вона обирає обидві ці цінності вище за всі інші, які, строго кажучи, дають більше інформації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect.",
  "translatedText": "У перший раз, коли я зробив це, я просто склав ці два числа, щоб виміряти якість кожного припущення, яке насправді спрацювало краще, ніж ви могли підозрювати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic.",
  "translatedText": "Але це не відчувалося систематично.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1515.9
 },
 {
  "input": "And I'm sure there's other approaches people could take.",
  "translatedText": "І я впевнений, що є й інші підходи, які люди могли б застосувати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1516.1,
  "end": 1517.88
 },
 {
  "input": "But here's the one I landed on.",
  "translatedText": "Але ось на якому я приземлився.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1517.9,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that.",
  "translatedText": "Якщо ми розглядаємо перспективу наступного припущення, як у цьому випадку слова, те, що нас справді хвилює, це очікуваний рахунок нашої гри, якщо ми це зробимо.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to.",
  "translatedText": "І щоб обчислити цей очікуваний бал, ми кажемо, яка ймовірність того, що слова є фактичною відповіддю, яка на даний момент відповідає 58%.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be four.",
  "translatedText": "Ми говоримо, що з ймовірністю 58% наш рахунок у цій грі буде чотири.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of one minus that 58%, our score will be more than that four.",
  "translatedText": "І тоді з імовірністю один мінус ці 58%, наш рахунок буде більше, ніж чотири.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point.",
  "translatedText": "Скільки ще ми не знаємо, але ми можемо оцінити це на основі того, скільки невизначеності буде, коли ми дійдемо до цієї точки.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment, there's 1.44 bits of uncertainty.",
  "translatedText": "Зокрема, на даний момент існує 1,44 біта невизначеності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits.",
  "translatedText": "Якщо ми вгадуємо слова, це означає, що очікувана інформація, яку ми отримаємо, дорівнює 1.27 біт.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens.",
  "translatedText": "Отже, якщо ми вгадуємо слова, ця різниця показує, скільки невизначеності ми, ймовірно, залишимо після того, як це станеться.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score.",
  "translatedText": "Нам потрібна якась функція, яку я тут називаю f, яка пов’язує цю невизначеність із очікуваною оцінкою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version one of the bot to say, hey, what was the actual score after various points with certain very measurable amounts of uncertainty?",
  "translatedText": "І спосіб, яким це було зроблено, полягав у тому, що ми просто побудували графік даних з попередніх ігор на основі першої версії бота, щоб сказати: \"Агов, яким був фактичний рахунок після різних моментів з певною, дуже вимірюваною кількістю невизначеності?\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games, after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer.",
  "translatedText": "Наприклад, ці точки даних, що знаходяться над значенням, яке приблизно дорівнює 8,7, говорять про те, що в деяких іграх після точки, в якій було 8,7 біт невизначеності, потрібно було дві спроби, щоб отримати остаточну відповідь.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games, it took three guesses.",
  "translatedText": "В інших іграх потрібно було вгадати з трьох разів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1600.66
 },
 {
  "input": "For other games, it took four guesses.",
  "translatedText": "В інших іграх потрібно було вгадати чотири рази.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1600.82,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring.",
  "translatedText": "Якщо ми перемістимося вліво тут, усі точки над нулем говорять про те, що будь-коли є нуль біт невизначеності, тобто є лише одна можливість, тоді необхідна кількість припущень завжди дорівнює лише одній, що заспокоює.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses, and so on and so forth here.",
  "translatedText": "Щоразу, коли була хоч крапля невизначеності, тобто, по суті, залишалося лише дві можливості, іноді потрібно було ще одне припущення, іноді ще два припущення, і так далі, і тому подібне.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages.",
  "translatedText": "Можливо, дещо простіший спосіб візуалізувати ці дані — об’єднати їх разом і взяти середні значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example, this bar here is saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5.",
  "translatedText": "Наприклад, цей стовпчик показує, що серед усіх точок, де ми мали хоч трохи невизначеності, в середньому кількість нових вгадувань становила близько 1,5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here is saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward.",
  "translatedText": "І ось ця смужка показує, що серед усіх різних ігор, де в якийсь момент невизначеність була трохи вищою за чотири біти, що звужує діапазон до 16 різних можливостей, в середньому потрібно трохи більше двох вгадувань, починаючи з цього моменту і далі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this.",
  "translatedText": "І звідси я просто зробив регресію, щоб відповідати функції, яка здавалася розумною для цього.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember, the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be.",
  "translatedText": "І пам'ятайте, що весь сенс усього цього полягає в тому, щоб ми могли кількісно оцінити цю інтуїцію, що чим більше інформації ми отримуємо від слова, тим нижчим буде очікуваний бал.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So, with this as version 2.0, if we go back and run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do?",
  "translatedText": "Отже, якщо ми повернемося до версії 2.0 і запустимо той самий набір симуляцій з усіма 2315 можливими варіантами відповідей, як вона себе покаже?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version, it's definitely better, which is reassuring.",
  "translatedText": "Ну, на відміну від нашої першої версії, вона, безумовно, краща, що не може не радувати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done, the average is around 3.6.",
  "translatedText": "Загалом, середній показник становить близько 3,6.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1686.18
 },
 {
  "input": "Although unlike the first version, there are a couple times that it loses, and requires more than six in this circumstance.",
  "translatedText": "Хоча, на відміну від першої версії, бувають випадки, коли вона програє, і в цьому випадку потрібно більше шести.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1686.54,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information.",
  "translatedText": "Мабуть тому, що бувають моменти, коли потрібно досягти мети, а не максимізувати інформацію.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6?",
  "translatedText": "Отже, ми можемо зробити краще, ніж 3.6?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can.",
  "translatedText": "Ми точно можемо.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now, I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model.",
  "translatedText": "На початку я казав, що найцікавіше спробувати не включати справжній список відповідей wordle в те, як він будує свою модель.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43.",
  "translatedText": "Але якщо ми все-таки це включимо, найкраща продуктивність, яку я міг отримати, була приблизно 3.43.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that.",
  "translatedText": "Отже, якщо ми спробуємо вийти більш складним, ніж просто використовувати дані про частоту слів, щоб вибрати цей попередній розподіл, це 3.43, ймовірно, дає максимум того, наскільки добре ми можемо отримати з цим, або принаймні, наскільки добре я можу отримати з цим.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one.",
  "translatedText": "Ця найкраща продуктивність, по суті, просто використовує ідеї, про які я тут говорив, але йде трохи далі, ніби шукає очікувану інформацію на два кроки вперед, а не лише на один.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is.",
  "translatedText": "Спочатку я планував більше поговорити про це, але я розумію, що насправді ми пройшли досить довго.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least, it's looking like Crane is the best opener.",
  "translatedText": "Єдине, що я можу сказати, це те, що після проведення цього двоетапного пошуку і запуску декількох прикладів симуляцій у найкращих кандидатів, поки що, принаймні для мене, виглядає так, що Crane є найкращим відкривачем.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed?",
  "translatedText": "Хто б міг здогадатися?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits.",
  "translatedText": "Крім того, якщо ви використовуєте справжній список слів для визначення простору можливостей, тоді невизначеність, з якої ви починаєте, становить трохи більше 11 біт.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits.",
  "translatedText": "І виявляється, що тільки з перебору грубою силою максимально можлива очікувана інформація після перших двох здогадок становить близько 10 біт.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty.",
  "translatedText": "Це свідчить про те, що в найкращому випадку, після ваших перших двох припущень, з ідеально оптимальною грою, ви залишитеся з дещицею невизначеності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses.",
  "translatedText": "Це те саме, що мати два можливі припущення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "But I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as three, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail.",
  "translatedText": "Але я думаю, що буде справедливо і, можливо, досить консервативно сказати, що ви ніколи не зможете написати алгоритм, який отримає це середнє значення на рівні трьох, тому що з доступними вам словами просто немає можливості отримати достатньо інформації після двох кроків, щоб мати можливість гарантувати відповідь у третьому слоті кожного разу безпомилково.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]