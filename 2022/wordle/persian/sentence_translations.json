[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy. ",
  "translatedText": "بازی Wurdle در یکی دو ماه اخیر بسیار ویروسی شده است، و هرگز کسی فرصتی برای درس ریاضی را نادیده نمی گیرد، به ذهنم می رسد که این بازی یک مثال مرکزی بسیار خوب در یک درس در مورد تئوری اطلاعات و به طور خاص است. موضوعی که به آنتروپی معروف است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could. ",
  "translatedText": "ببینید، من هم مانند بسیاری از افراد به نوعی در معما جذب شدم، و مانند بسیاری از برنامه نویسان من نیز در تلاش برای نوشتن الگوریتمی که بازی را به بهترین شکل ممکن انجام دهد، جذب شدم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy. ",
  "translatedText": "و آنچه من فکر کردم اینجا انجام دهم این است که فقط با شما بخشی از فرآیند خود را در آن صحبت کنم، و برخی از ریاضیات را توضیح دهم، زیرا کل الگوریتم بر این ایده آنتروپی متمرکز است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle? ",
  "translatedText": "اول از همه، اگر شما در مورد آن نشنیده اید، Wurdle چیست؟ و برای کشتن دو پرنده با یک سنگ در اینجا در حالی که ما قوانین بازی را دنبال می کنیم، اجازه دهید پیش نمایشی هم داشته باشم که با این کار به کجا می رویم، یعنی ایجاد یک الگوریتم کوچک که اساساً بازی را برای ما انجام می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does. ",
  "translatedText": "اگرچه من Wurdle امروز را انجام نداده ام، اما این 4 فوریه است، و خواهیم دید که ربات چگونه عمل می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess. ",
  "translatedText": "هدف Wurdle حدس زدن یک کلمه مرموز پنج حرفی است و شش فرصت مختلف برای حدس زدن به شما داده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane. ",
  "translatedText": "به عنوان مثال، ربات Wurdle من پیشنهاد می کند که با جرثقیل حدس زدن شروع کنم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer. ",
  "translatedText": "هر بار که حدس می زنید، اطلاعاتی در مورد نزدیک بودن حدس شما به پاسخ واقعی به دست می آورید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer. ",
  "translatedText": "در اینجا جعبه خاکستری به من می گوید که در پاسخ واقعی C وجود ندارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position. ",
  "translatedText": "جعبه زرد به من می گوید R وجود دارد، اما در آن موقعیت نیست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position. ",
  "translatedText": "جعبه سبز به من می گوید که کلمه مخفی دارای A است و در جایگاه سوم قرار دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E. ",
  "translatedText": "و سپس نه N و نه E وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information. ",
  "translatedText": "بنابراین اجازه دهید من وارد شوم و این اطلاعات را به ربات Wurdle بگویم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey. ",
  "translatedText": "ما با جرثقیل شروع کردیم، خاکستری، زرد، سبز، خاکستری، خاکستری شدیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time. ",
  "translatedText": "نگران تمام داده هایی که در حال حاضر نشان می دهد نباشید، در زمان مناسب آن را توضیح خواهم داد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick. ",
  "translatedText": "اما بهترین پیشنهاد آن برای انتخاب دوم ما shtick است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess. ",
  "translatedText": "و حدس شما باید یک کلمه پنج حرفی واقعی باشد، اما همانطور که خواهید دید، با آنچه که در واقع به شما اجازه حدس زدن می‌دهد بسیار آزادانه است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick. ",
  "translatedText": "در این مورد ما shtick را امتحان می کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good. ",
  "translatedText": "و خوب، همه چیز خیلی خوب به نظر می رسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R. ",
  "translatedText": "S و H را می زنیم، بنابراین سه حرف اول را می دانیم، می دانیم که R وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something. ",
  "translatedText": "و بنابراین مانند SHA چیزی R، یا SHA R چیزی خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp. ",
  "translatedText": "و به نظر می‌رسد که ربات Wurdle می‌داند که فقط دو احتمال دارد، خرد یا تیز. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard. ",
  "translatedText": "در این مرحله این یک نوع پرت کردن بین آنهاست، بنابراین حدس می‌زنم احتمالاً فقط به دلیل حروف الفبا بودن آن با خرده‌ها همخوانی دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer. ",
  "translatedText": "چه هورا، پاسخ واقعی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three. ",
  "translatedText": "بنابراین ما آن را در سه دریافت کردیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie. ",
  "translatedText": "اگر تعجب می کنید که آیا این چیز خوبی است، روشی که من از یک نفر شنیدم این است که با Wurdle چهار برابر است و سه مرغک است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy. ",
  "translatedText": "که به نظر من تشبیه بسیار مناسبی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy. ",
  "translatedText": "شما باید به طور مداوم در بازی خود باشید تا به 4 برسید، اما مطمئناً دیوانه کننده نیست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great. ",
  "translatedText": "اما وقتی آن را سه تایی دریافت می‌کنید، احساس عالی می‌کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot. ",
  "translatedText": "بنابراین، اگر دوست ندارید، کاری که من می‌خواهم در اینجا انجام دهم این است که از ابتدا در مورد نحوه برخوردم با ربات Wurdle صحبت کنم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson. ",
  "translatedText": "و همانطور که گفتم، واقعاً بهانه ای برای درس تئوری اطلاعات است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy. ",
  "translatedText": "هدف اصلی این است که توضیح دهیم اطلاعات چیست و آنتروپی چیست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language. ",
  "translatedText": "اولین فکر من در نزدیک شدن به این موضوع، نگاهی به بسامدهای نسبی حروف مختلف در زبان انگلیسی بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters? ",
  "translatedText": "بنابراین فکر کردم، خوب، آیا یک حدس اولیه یا یک جفت حدس ابتدایی وجود دارد که به تعداد زیادی از این حروف متداول برخورد کند؟ و یکی که من خیلی به آن علاقه داشتم این بود که کارهای دیگری انجام می دادم و به دنبال آن میخ می زدم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good. ",
  "translatedText": "فکر این است که اگر یک حرف بزنید، می دانید، سبز یا زرد می گیرید، که همیشه حس خوبی دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters. ",
  "translatedText": "انگار داری اطلاعات میگیری اما در این موارد، حتی اگر ضربه ای نزنید و همیشه خاکستری می شوید، باز هم اطلاعات زیادی به شما می دهد زیرا یافتن کلمه ای که هیچ یک از این حروف را نداشته باشد بسیار نادر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters. ",
  "translatedText": "اما حتی با این حال، این امر فوق العاده سیستماتیک به نظر نمی رسد، زیرا برای مثال، ترتیب حروف را در نظر نمی گیرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end? ",
  "translatedText": "وقتی می‌توانم حلزون را تایپ کنم، چرا ناخن بنویسم؟ آیا بهتر است آن S را در انتها داشته باشیم؟ من واقعا اطمینان ندارم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y. ",
  "translatedText": "حالا یکی از دوستانم گفت که دوست دارد با کلمه weary باز کند، که من را متعجب کرد زیرا در آن حروف غیر معمول مانند W و Y وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener. ",
  "translatedText": "اما چه کسی می داند، شاید بازکننده بهتری باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up. ",
  "translatedText": "آیا نوعی امتیاز کمی وجود دارد که بتوانیم برای قضاوت در مورد کیفیت یک حدس احتمالی بدهیم؟ اکنون برای تنظیم روشی که می‌خواهیم حدس‌های احتمالی را رتبه‌بندی کنیم، اجازه دهید به عقب برگردیم و کمی وضوح به نحوه دقیق تنظیم بازی اضافه کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long. ",
  "translatedText": "بنابراین فهرستی از کلمات وجود دارد که به شما امکان می دهد وارد کنید که حدس های معتبری در نظر گرفته می شوند که فقط حدود 13000 کلمه طولانی دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble. ",
  "translatedText": "اما وقتی به آن نگاه می‌کنید، چیزهای واقعاً غیرمعمول زیادی وجود دارد، چیزهایی مانند سر یا علی و ARG، از جمله کلماتی که در بازی Scrabble مشاجره‌های خانوادگی ایجاد می‌کنند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word. ",
  "translatedText": "اما جو بازی این است که پاسخ همیشه یک کلمه معمولی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers. ",
  "translatedText": "و در واقع، فهرست دیگری از حدود 2300 کلمه وجود دارد که پاسخ‌های ممکن است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun. ",
  "translatedText": "و این یک لیست انتخاب شده توسط انسان است، به نظر من به طور خاص توسط دوست دختر سازنده بازی، که به نوعی سرگرم کننده است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list. ",
  "translatedText": "اما کاری که من می‌خواهم انجام دهم، چالش ما برای این پروژه این است که ببینیم آیا می‌توانیم برنامه‌ای برای حل Wordle بنویسیم که دانش قبلی در مورد این لیست را ترکیب نکند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list. ",
  "translatedText": "برای یک چیز، تعداد زیادی از کلمات پنج حرفی بسیار رایج وجود دارد که شما در آن لیست نخواهید یافت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website. ",
  "translatedText": "بنابراین بهتر است برنامه‌ای بنویسید که کمی انعطاف‌پذیرتر باشد و وردل را در برابر هر کسی بازی کند، نه فقط چیزی که اتفاقاً وب‌سایت رسمی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code. ",
  "translatedText": "و همچنین دلیل اینکه ما می دانیم این لیست از پاسخ های ممکن چیست، این است که در کد منبع قابل مشاهده است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day. ",
  "translatedText": "اما روشی که در کد منبع قابل مشاهده است به ترتیب خاصی است که در آن پاسخ ها هر روز ارائه می شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be. ",
  "translatedText": "بنابراین همیشه می توانید به دنبال پاسخ فردا باشید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating. ",
  "translatedText": "بنابراین واضح است که استفاده از فهرست تقلب است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words. ",
  "translatedText": "و آنچه برای یک پازل جالب‌تر و یک درس تئوری اطلاعات غنی‌تر می‌سازد، استفاده از داده‌های جهانی‌تر مانند بسامدهای نسبی کلمات به طور کلی برای درک این شهود ترجیح دادن به کلمات رایج‌تر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality? ",
  "translatedText": "پس از بین این 13000 احتمال، چگونه باید حدس اولیه را انتخاب کنیم؟ به عنوان مثال، اگر دوست من خسته را پیشنهاد دهد، چگونه باید کیفیت آن را تجزیه و تحلیل کنیم؟ خوب، دلیلی که او گفت که W بعید را دوست دارد این است که او از طبیعت لانگ شات خوشش می‌آید که اگر به آن دبلیو ضربه بزنید چقدر حس خوبی دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern. ",
  "translatedText": "به عنوان مثال، اگر اولین الگوی فاش شده چیزی شبیه به این باشد، پس معلوم می شود که تنها 58 کلمه در این واژگان غول پیکر وجود دارد که با آن الگو مطابقت دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000. ",
  "translatedText": "بنابراین این کاهش بسیار زیادی از 13000 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this. ",
  "translatedText": "اما طرف دیگر آن، البته، این است که دریافت چنین الگویی بسیار غیر معمول است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000. ",
  "translatedText": "به طور خاص، اگر هر کلمه به یک اندازه پاسخ باشد، احتمال برخورد با این الگو 58 تقسیم بر حدود 13000 خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers. ",
  "translatedText": "البته، آنها به یک اندازه به احتمال زیاد پاسخ نیستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words. ",
  "translatedText": "بیشتر اینها کلماتی بسیار مبهم و حتی مشکوک هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later. ",
  "translatedText": "اما حداقل برای اولین پاس ما در تمام این موارد، بیایید فرض کنیم که همه آنها به یک اندازه محتمل هستند و سپس آن را کمی بعد اصلاح کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur. ",
  "translatedText": "نکته این است که الگوی با اطلاعات زیاد به دلیل ماهیت خود بعید است که رخ دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely. ",
  "translatedText": "در واقع، معنای آموزنده بودن این است که بعید است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it. ",
  "translatedText": "یک الگوی بسیار محتمل تر برای دیدن با این باز شدن چیزی شبیه به این خواهد بود، که البته W در آن وجود ندارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y. ",
  "translatedText": "شاید E وجود داشته باشد، و شاید A وجود نداشته باشد، R وجود نداشته باشد، Y وجود نداشته باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches. ",
  "translatedText": "در این مورد، 1400 مسابقه احتمالی وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see. ",
  "translatedText": "اگر همه به یک اندازه محتمل بودند، به احتمال حدود 11 درصد این الگویی است که می بینید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative. ",
  "translatedText": "بنابراین محتمل ترین نتایج نیز کمترین اطلاعات را دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see. ",
  "translatedText": "برای دریافت نمای کلی‌تر در اینجا، اجازه دهید توزیع کامل احتمالات را در تمام الگوهای مختلف که ممکن است ببینید را به شما نشان دهم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common. ",
  "translatedText": "بنابراین هر نواری که به آن نگاه می‌کنید مربوط به الگوی احتمالی رنگ‌هایی است که می‌توان آشکار کرد، که از 3 تا 5 احتمال وجود دارد، و آنها از چپ به راست، رایج‌ترین تا کم‌معمول‌ترین، سازمان‌دهی شده‌اند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays. ",
  "translatedText": "بنابراین رایج ترین احتمال در اینجا این است که تمام خاکستری ها را دریافت کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time. ",
  "translatedText": "این در حدود 14 درصد مواقع اتفاق می افتد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this. ",
  "translatedText": "و وقتی حدس می‌زنید به آن امیدوار هستید این است که به جایی در این دم بلند برسید، مانند اینجا که تنها 18 احتمال برای آنچه که با این الگو مطابقت دارد و ظاهراً شبیه این است وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here. ",
  "translatedText": "یا اگر کمی دورتر به سمت چپ برویم، می‌دانی، شاید تا آخر اینجا برویم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you. ",
  "translatedText": "خوب، در اینجا یک پازل خوب برای شما وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly. ",
  "translatedText": "سه کلمه در زبان انگلیسی که با W شروع و با Y ختم می شوند و در جایی R دارند کدامند؟ به نظر می رسد، پاسخ ها، بیایید ببینیم، لفظی، کرمی، و پرخاشگر هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution. ",
  "translatedText": "بنابراین برای قضاوت در مورد اینکه این کلمه به طور کلی چقدر خوب است، ما می خواهیم نوعی اندازه گیری از میزان اطلاعات مورد انتظاری که از این توزیع به دست می آورید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score. ",
  "translatedText": "اگر هر الگو را مرور کنیم و احتمال وقوع آن را در دفعات چیزی که میزان آموزنده بودن آن را می سنجد ضرب کنیم، ممکن است به ما یک امتیاز عینی بدهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches. ",
  "translatedText": "اکنون اولین غریزه شما برای اینکه آن چیزی باید باشد، ممکن است تعداد مسابقات باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches. ",
  "translatedText": "شما میانگین تعداد کمتری از مسابقات را می خواهید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer. ",
  "translatedText": "اما در عوض می‌خواهم از یک اندازه‌گیری جهانی‌تر استفاده کنم که اغلب به اطلاعات نسبت می‌دهیم، و زمانی که احتمال متفاوتی برای هر یک از این 13000 کلمه در نظر بگیریم که آیا واقعاً پاسخ هستند یا نه، انعطاف‌پذیرتر خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples. ",
  "translatedText": "واحد استاندارد اطلاعات بیت است که کمی فرمول خنده‌دار دارد، اما اگر فقط به مثال‌ها نگاه کنیم، واقعاً بصری است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information. ",
  "translatedText": "اگر مشاهده ای دارید که فضای احتمالی شما را نصف می کند، می گوییم که یک بیت اطلاعات دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half. ",
  "translatedText": "در مثال ما، فضای احتمالات همه کلمات ممکن است، و تقریباً نیمی از کلمات پنج حرفی S دارند، کمی کمتر از آن، اما تقریباً نصف. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information. ",
  "translatedText": "بنابراین آن مشاهده یک بیت اطلاعات به شما می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information. ",
  "translatedText": "اگر در عوض یک واقعیت جدید آن فضای احتمالی را با ضریب چهار کاهش دهد، می گوییم که دارای دو بیت اطلاعات است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T. ",
  "translatedText": "به عنوان مثال، معلوم می شود که حدود یک چهارم این کلمات دارای T هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth. ",
  "translatedText": "اگر مشاهدات آن فضا را به ضریب هشت کاهش دهد، می گوییم سه بیت اطلاعات است و غیره و غیره. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd. ",
  "translatedText": "چهار بیت آن را به 16 برش می دهد، پنج بیت آن را به 32 برش می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability. ",
  "translatedText": "بنابراین اکنون ممکن است بخواهید مکث کنید و از خود بپرسید که فرمول اطلاعات برای تعداد بیت ها از نظر احتمال وقوع چیست؟ چیزی که در اینجا می گوییم این است که وقتی یک نصف را به تعداد بیت ها می گیریم، این همان احتمال است، که همان چیزی است که بگوییم دو به توان تعداد بیت ها یک بر احتمال است، که بازآرایی می‌کند و می‌گوید که اطلاعات مبنای گزارش دو از یک تقسیم بر احتمال است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability. ",
  "translatedText": "و گاهی اوقات شما این را با یک بازآرایی دیگر مشاهده می کنید، که در آن اطلاعات، پایه ثبت منفی دو احتمال است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half. ",
  "translatedText": "اینطور بیان می شود، ممکن است برای افراد ناآشنا کمی عجیب به نظر برسد، اما واقعاً این ایده بسیار شهودی است که بپرسید چند بار امکانات خود را به نصف کاهش داده اید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095. ",
  "translatedText": "حالا اگر تعجب می‌کنید، می‌دانید، من فکر می‌کردم که ما داریم یک بازی سرگرم‌کننده با کلمات انجام می‌دهیم، چرا لگاریتم‌ها وارد تصویر می‌شوند؟ یکی از دلایلی که این واحد زیباتر است این است که صحبت کردن در مورد رویدادهای بسیار بعید بسیار ساده تر است، گفتن اینکه یک مشاهده دارای 20 بیت اطلاعات است بسیار آسان تر از این است که بگوییم احتمال وقوع فلان و آن 0 است. 0000095. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together. ",
  "translatedText": "اما دلیل اصلی‌تر اینکه این عبارت لگاریتمی افزوده بسیار مفیدی برای نظریه احتمال است، روشی است که اطلاعات با هم جمع می‌شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information. ",
  "translatedText": "به عنوان مثال، اگر یک مشاهده دو بیت اطلاعات به شما بدهد، فضای شما را چهار بیت کاهش دهد، و سپس مشاهده دوم مانند حدس دوم شما در Wordle، سه بیت اطلاعات دیگر به شما بدهد، و شما را با ضریب هشت بیشتر کاهش دهد، دو با هم پنج بیت اطلاعات را به شما می دهند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add. ",
  "translatedText": "همانطور که احتمالات دوست دارند ضرب شوند، اطلاعات نیز دوست دارند جمع شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with. ",
  "translatedText": "بنابراین به محض اینکه در قلمرو چیزی مانند یک مقدار مورد انتظار قرار می گیریم، جایی که ما در حال اضافه کردن یک سری اعداد به بالا هستیم، گزارش ها رسیدگی به آن را بسیار زیباتر می کنند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern. ",
  "translatedText": "بیایید به توزیع خود برای Weary برگردیم و یک ردیاب کوچک دیگر را در اینجا اضافه کنیم و به ما نشان دهد که برای هر الگو چقدر اطلاعات وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain. ",
  "translatedText": "نکته اصلی که می‌خواهم به آن توجه داشته باشید این است که هرچه احتمال بالاتر رفتن به آن الگوهای محتمل‌تر باشد، اطلاعات کمتر، بیت‌های کمتری به دست می‌آورید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get. ",
  "translatedText": "روشی که کیفیت این حدس را اندازه گیری می کنیم به این صورت است که مقدار مورد انتظار این اطلاعات را می گیریم، جایی که هر الگو را مرور می کنیم، می گوییم چقدر احتمال دارد، و سپس آن را در چند بیت اطلاعاتی که به دست می آوریم ضرب می کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits. ",
  "translatedText": "و در مثال Weary، معلوم می شود که 4 است. 9 بیت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times. ",
  "translatedText": "بنابراین، به طور متوسط، اطلاعاتی که از این حدس آغازین به دست می آورید، به اندازه نصف کردن فضای احتمالی شما در حدود پنج بار است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate. ",
  "translatedText": "در مقابل، نمونه‌ای از حدس با ارزش اطلاعات مورد انتظار بالاتر چیزی شبیه Slate است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter. ",
  "translatedText": "در این مورد متوجه خواهید شد که توزیع بسیار صاف تر به نظر می رسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information. ",
  "translatedText": "به طور خاص، احتمال وقوع همه خاکستری ها تنها حدود 6 درصد است، بنابراین حداقل شما به وضوح 3 می گیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that. ",
  "translatedText": "9 بیت اطلاعات اما این یک حداقل است، معمولاً شما چیزی بهتر از آن را دریافت می کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8. ",
  "translatedText": "و وقتی اعداد را روی این یکی خرد می‌کنید و همه عبارت‌های مربوطه را جمع می‌کنید، میانگین اطلاعات حدود 5 است. 8. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average. ",
  "translatedText": "بنابراین بر خلاف Weary، فضای احتمالی شما پس از اولین حدس به طور متوسط تقریباً نصف خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity. ",
  "translatedText": "در واقع یک داستان سرگرم کننده در مورد نام این مقدار مورد انتظار کمیت اطلاعات وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science. ",
  "translatedText": "نظریه اطلاعات توسط کلود شانون که در دهه 1940 در آزمایشگاه های بل کار می کرد، ایجاد شد، اما او در مورد برخی از ایده های خود که هنوز منتشر نشده بود با جان فون نویمان، که این غول فکری بسیار برجسته آن زمان بود صحبت می کرد. در ریاضیات و فیزیک و آغاز آنچه در حال تبدیل شدن به علم کامپیوتر بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons. ",
  "translatedText": "و هنگامی که او اشاره کرد که واقعاً نام خوبی برای این مقدار مورد انتظار کمیت اطلاعات ندارد، ظاهراً فون نویمان گفت، بنابراین داستان ادامه دارد، خوب شما باید آن را آنتروپی بنامید، و به دو دلیل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage. ",
  "translatedText": "در وهله اول، تابع عدم قطعیت شما در مکانیک آماری با آن نام استفاده شده است، بنابراین قبلاً یک نام دارد، و در وهله دوم، و مهمتر از آن، هیچ کس نمی داند آنتروپی واقعاً چیست، بنابراین در یک بحث همیشه خواهید دید مزیت را دارند بنابراین اگر نام کمی مرموز به نظر می رسد، و اگر این داستان را باید باور کرد، این یک نوع طراحی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess. ",
  "translatedText": "همچنین اگر در مورد رابطه آن با تمام آن قانون دوم ترمودینامیک از فیزیک تعجب می کنید، قطعاً ارتباطی وجود دارد، اما در اصل شانون فقط با نظریه احتمال محض سر و کار داشت، و برای اهداف ما در اینجا، زمانی که من از آنتروپی کلمه، من فقط از شما می خواهم که ارزش اطلاعات مورد انتظار یک حدس خاص را در نظر بگیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously. ",
  "translatedText": "شما می توانید آنتروپی را به عنوان اندازه گیری دو چیز به طور همزمان در نظر بگیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution. ",
  "translatedText": "اولین مورد این است که توزیع چقدر مسطح است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be. ",
  "translatedText": "هر چه یک توزیع به یکنواخت نزدیکتر باشد، آنتروپی بالاتر خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy. ",
  "translatedText": "در مورد ما، جایی که الگوهای کل 3 تا 5 وجود دارد، برای توزیع یکنواخت، مشاهده هر یک از آنها پایه ثبت اطلاعات 2 از 3 تا 5 را خواهد داشت که اتفاقاً 7 است. 92، بنابراین حداکثر مطلقی است که می توانید برای این آنتروپی داشته باشید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place. ",
  "translatedText": "اما آنتروپی همچنین به نوعی معیاری است که در وهله اول چند احتمال وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits. ",
  "translatedText": "به عنوان مثال، اگر شما کلمه ای دارید که در آن فقط 16 الگوی ممکن وجود دارد، و هر کدام به یک اندازه احتمال دارد، این آنتروپی، این اطلاعات مورد انتظار، 4 بیت خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits. ",
  "translatedText": "اما اگر کلمه دیگری داشته باشید که در آن 64 الگوی احتمالی وجود دارد که می‌تواند ظاهر شود، و همه آنها به یک اندازه احتمال دارند، آنتروپی 6 بیت خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes. ",
  "translatedText": "بنابراین، اگر توزیعی را در طبیعت مشاهده کردید که دارای آنتروپی 6 بیتی است، به نوعی می‌گویید که تغییرات و عدم قطعیت در آنچه قرار است اتفاق بیفتد به اندازه‌ای است که 64 نتیجه به همان اندازه محتمل است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this. ",
  "translatedText": "برای اولین پاسم در Wurtelebot، اساساً مجبور شدم این کار را انجام دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible. ",
  "translatedText": "تمام حدس‌های ممکن را که می‌توانید داشته باشید، تمام 13000 کلمه را بررسی می‌کند، آنتروپی را برای هر یک محاسبه می‌کند، یا به طور خاص، آنتروپی توزیع را در همه الگوهایی که ممکن است ببینید، برای هر یک، و بالاترین را انتخاب می‌کند، زیرا این موردی که احتمالاً فضای احتمالی شما را تا حد امکان کاهش می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses. ",
  "translatedText": "و اگرچه من فقط در مورد حدس اول در اینجا صحبت کرده ام، برای چند حدس بعدی همین کار را انجام می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words. ",
  "translatedText": "به عنوان مثال، پس از اینکه الگویی را در اولین حدس مشاهده کردید، که شما را به تعداد کمتری از کلمات ممکن بر اساس آنچه با آن منطبق است محدود می‌کند، فقط همان بازی را با توجه به مجموعه کلمات کوچک‌تر انجام می‌دهید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy. ",
  "translatedText": "برای یک حدس دوم پیشنهادی، به توزیع همه الگوهایی که می‌توانند از آن مجموعه کلمات محدودتر رخ دهند نگاه کنید، تمام 13000 احتمال را جستجو می‌کنید و احتمالی را پیدا می‌کنید که آنتروپی را به حداکثر می‌رساند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins. ",
  "translatedText": "برای اینکه به شما نشان دهم چگونه این در عمل کار می کند، اجازه دهید فقط یک نوع کوچک از Wurtele را که نوشتم که نکات برجسته این تحلیل را در حاشیه نشان می دهد، بیاورم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information. ",
  "translatedText": "پس از انجام تمام محاسبات آنتروپی، در سمت راست اینجا به ما نشان می دهد که کدام یک بالاترین اطلاعات مورد انتظار را دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch. ",
  "translatedText": "به نظر می رسد که پاسخ برتر، حداقل در حال حاضر، ما بعداً آن را اصلاح خواهیم کرد، Tares است، که به معنای، البته، ماشک، رایج ترین ماشک است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern. ",
  "translatedText": "هر بار که در اینجا حدس می زنیم، جایی که شاید به نوعی توصیه های آن را نادیده می گیرم و با تخته سنگ پیش می روم، زیرا من تخته سنگ را دوست دارم، می توانیم ببینیم که چقدر اطلاعات مورد انتظار آن را داشته است، اما در سمت راست کلمه اینجا به ما نشان می دهد که چقدر اطلاعات واقعی ما با توجه به این الگوی خاص به دست آوردیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that. ",
  "translatedText": "بنابراین در اینجا به نظر می رسد که ما کمی بدشانس بودیم، انتظار می رفت که 5 بگیریم. 8، اما اتفاقاً چیزی با کمتر از آن به دست آوردیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now. ",
  "translatedText": "و سپس در سمت چپ اینجا همه کلمات ممکن مختلف را به ما نشان می دهد که در کجا هستیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment. ",
  "translatedText": "نوارهای آبی به ما می‌گویند که احتمال هر کلمه چقدر است، بنابراین در حال حاضر فرض می‌کند هر کلمه به یک اندازه ممکن است رخ دهد، اما ما آن را در یک لحظه اصلاح می‌کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities. ",
  "translatedText": "و سپس این اندازه‌گیری عدم قطعیت آنتروپی این توزیع را در میان کلمات ممکن به ما می‌گوید، که در حال حاضر، به دلیل اینکه توزیع یکنواختی است، فقط یک راه پیچیده برای شمارش تعداد احتمالات است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities. ",
  "translatedText": "مثلاً اگر 2 را به توان 13 بگیریم. 66، که باید حدود 13000 احتمال باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places. ",
  "translatedText": "من از اینجا کمی دور هستم، اما فقط به این دلیل که تمام ارقام اعشار را نشان نمی دهم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute. ",
  "translatedText": "در حال حاضر ممکن است این کار زائد به نظر برسد و چیزها را بیش از حد پیچیده کند، اما خواهید دید که چرا داشتن هر دو عدد در یک دقیقه مفید است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word. ",
  "translatedText": "بنابراین در اینجا به نظر می رسد که بالاترین آنتروپی را برای حدس دوم ما رامن نشان می دهد، که دوباره واقعاً شبیه یک کلمه نیست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains. ",
  "translatedText": "بنابراین برای اینکه در اینجا از اخلاقیات بالاتری برخوردار باشم، می‌روم و Rains را تایپ می‌کنم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky. ",
  "translatedText": "و دوباره به نظر می رسد که ما کمی بدشانس بودیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information. ",
  "translatedText": "ما منتظر 4 بودیم. 3 بیت و ما فقط 3 بیت گرفتیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities. ",
  "translatedText": "39 بیت اطلاعات بنابراین ما را به 55 احتمال کاهش می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means. ",
  "translatedText": "و در اینجا شاید من در واقع به آنچه که پیشنهاد می کند، که ترکیبی است، هر معنایی که دارد، ادامه دهم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle. ",
  "translatedText": "و خوب، این در واقع فرصت خوبی برای یک پازل است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information. ",
  "translatedText": "به ما می گوید که این الگو به ما 4 می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty. ",
  "translatedText": "7 بیت اطلاعات اما در سمت چپ، قبل از اینکه آن الگو را ببینیم، 5 مورد وجود داشت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers. ",
  "translatedText": "۷۸ بیت عدم قطعیت بنابراین به عنوان یک مسابقه برای شما، این به چه معناست در مورد تعداد احتمالات باقی مانده؟ خوب، به این معنی است که ما به یک بیت عدم قطعیت کاهش یافته ایم، که همان چیزی است که بگوییم دو پاسخ ممکن وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice. ",
  "translatedText": "این یک انتخاب 50-50 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss. ",
  "translatedText": "و از اینجا، چون من و شما می دانیم که کدام کلمات رایج تر است، می دانیم که پاسخ باید پرتگاه باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that. ",
  "translatedText": "اما همانطور که در حال حاضر نوشته شده است، برنامه این را نمی داند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it. ",
  "translatedText": "بنابراین فقط به راه خود ادامه می دهد و سعی می کند تا جایی که می تواند اطلاعات بیشتری به دست آورد، تا زمانی که تنها یک احتمال باقی بماند و سپس آن را حدس بزند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy. ",
  "translatedText": "بنابراین بدیهی است که ما به یک استراتژی آخر بازی بهتر نیاز داریم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does. ",
  "translatedText": "اما فرض کنید این نسخه را یکی از حل کننده های wordle خود می نامیم و سپس می رویم و شبیه سازی هایی را اجرا می کنیم تا ببینیم چگونه کار می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game. ",
  "translatedText": "بنابراین روشی که این کار می کند این است که همه بازی های wordle ممکن را انجام می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers. ",
  "translatedText": "در حال بررسی تمام آن 2315 کلمه است که پاسخ های واقعی کلمه هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set. ",
  "translatedText": "این اساساً از آن به عنوان یک مجموعه آزمایشی استفاده می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice. ",
  "translatedText": "و با این روش ساده لوحانه در نظر نگرفتن میزان رایج بودن یک کلمه، و فقط تلاش برای به حداکثر رساندن اطلاعات در هر مرحله از مسیر، تا زمانی که به یک و تنها یک انتخاب می رسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124. ",
  "translatedText": "در پایان شبیه سازی، میانگین امتیاز حدود 4 خواهد بود. 124. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse. ",
  "translatedText": "که بد نیست، صادقانه بگویم، انتظار داشتم بدتر از این کار کنم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4. ",
  "translatedText": "اما افرادی که wordle بازی می کنند به شما خواهند گفت که معمولاً می توانند آن را در 4 دریافت کنند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can. ",
  "translatedText": "چالش واقعی این است که تا می توانید تعداد زیادی در 3 بدست آورید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3. ",
  "translatedText": "این یک جهش بسیار بزرگ بین امتیاز 4 و نمره 3 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that. ",
  "translatedText": "میوه کم آویزان واضح در اینجا این است که به نحوی مشخص کنیم که یک کلمه رایج است یا نه، و دقیقاً چگونه این کار را انجام می دهیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language. ",
  "translatedText": "روشی که من به آن نزدیک شدم دریافت لیستی از بسامدهای نسبی برای همه کلمات در زبان انگلیسی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset. ",
  "translatedText": "و من فقط از تابع داده فرکانس کلمه Mathematica استفاده کردم، که خود از مجموعه داده عمومی Ngram انگلیسی کتاب های گوگل استخراج می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words. ",
  "translatedText": "و نگاه کردن به آن به نوعی سرگرم کننده است، برای مثال اگر آن را از رایج ترین کلمات به کم رایج ترین کلمات مرتب کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language. ",
  "translatedText": "بدیهی است که اینها متداول ترین کلمات 5 حرفی در زبان انگلیسی هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common. ",
  "translatedText": "یا بهتر است بگوییم، اینها هشتمین مورد رایج هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there. ",
  "translatedText": "اول این است که، پس از آن وجود دارد و وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common. ",
  "translatedText": "اولین به خودی خود اول نیست، بلکه نهمین است، و منطقی است که این کلمات دیگر ممکن است بیشتر به وجود بیایند، جایی که کلمات بعد از اولین بعد هستند، کجا، و آنهایی که فقط کمی کمتر رایج هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency. ",
  "translatedText": "حال، در استفاده از این داده‌ها برای مدل‌سازی احتمال اینکه هر یک از این کلمات پاسخ نهایی باشد، نباید فقط با فراوانی متناسب باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely. ",
  "translatedText": "به عنوان مثال، به آن نمره 0 داده می شود. 002 در این مجموعه داده، در حالی که احتمال کلمه braid به نوعی حدود 1000 برابر کمتر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering. ",
  "translatedText": "اما هر دوی اینها به اندازه کافی کلمات رایجی هستند که تقریباً ارزش بررسی را دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff. ",
  "translatedText": "بنابراین ما یک قطع باینری بیشتر می خواهیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty. ",
  "translatedText": "راهی که من در مورد آن رفتم این بود که تصور کنم کل این فهرست مرتب شده از کلمات را بگیرم، و سپس آن را بر روی محور x مرتب کنیم، و سپس تابع sigmoid را اعمال کنیم، که روش استاندارد برای داشتن تابعی است که خروجی آن اساساً باینری است. یا 0 یا 1 است، اما یک هموارسازی در این بین برای آن ناحیه از عدم قطعیت وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis. ",
  "translatedText": "بنابراین، اساساً، احتمالی که من به هر کلمه برای قرار گرفتن در لیست نهایی اختصاص می‌دهم، مقدار تابع سیگموئید در بالا هرجا که روی محور x قرار می‌گیرد، خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff. ",
  "translatedText": "اکنون بدیهی است که این بستگی به چند پارامتر دارد، برای مثال اینکه چقدر فضایی در محور x آن کلمات پر می‌شود، تعیین می‌کند که چقدر تدریجی یا تند از 1 به 0 می‌افتیم، و جایی که آنها را از چپ به راست قرار می‌دهیم، برش را تعیین می‌کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind. ",
  "translatedText": "صادقانه بگویم، روشی که من این کار را کردم فقط لیسیدن انگشتم و چسباندن آن به باد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff. ",
  "translatedText": "من فهرست مرتب شده را نگاه کردم و سعی کردم پنجره ای پیدا کنم که وقتی به آن نگاه کردم متوجه شدم حدود نیمی از این کلمات بیشتر از اینکه پاسخ نهایی نباشند، هستند و از آن به عنوان نقطه برش استفاده کردم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement. ",
  "translatedText": "هنگامی که توزیعی مانند این در بین کلمات داشته باشیم، موقعیت دیگری به ما می دهد که در آن آنتروپی به این اندازه گیری واقعا مفید تبدیل می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it. ",
  "translatedText": "به عنوان مثال، فرض کنید در حال انجام یک بازی بودیم و با بازکن های قدیمی من که یک پر و میخ بودند شروع می کنیم و در نهایت به موقعیتی می رسیم که چهار کلمه ممکن است که با آن مطابقت داشته باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely. ",
  "translatedText": "و بیایید بگوییم که همه آنها را به یک اندازه محتمل در نظر می گیریم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2. ",
  "translatedText": "اجازه دهید از شما بپرسم آنتروپی این توزیع چیست؟ خوب، اطلاعات مرتبط با هر یک از این احتمالات، پایه گزارش 2 از 4 خواهد بود، زیرا هر کدام 1 و 4 هستند و آن 2 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities. ",
  "translatedText": "دو بیت اطلاعات، چهار احتمال. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches? ",
  "translatedText": "همه چیز خیلی خوب و عالی اما اگر به شما بگویم که در واقع بیش از چهار مسابقه وجود دارد، چه؟ در حقیقت، وقتی فهرست کامل کلمات را بررسی می کنیم، 16 کلمه وجود دارد که با آن مطابقت دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure. ",
  "translatedText": "اما فرض کنید مدل ما احتمال بسیار کمی را برای آن 12 کلمه دیگر قرار می دهد که واقعاً پاسخ نهایی باشد، چیزی در حدود 1 در 1000 زیرا آنها واقعا مبهم هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before. ",
  "translatedText": "حالا اجازه بدهید از شما بپرسم که آنتروپی این توزیع چقدر است؟ اگر آنتروپی صرفاً تعداد منطبق‌ها را در اینجا اندازه‌گیری می‌کرد، ممکن است انتظار داشته باشید که چیزی شبیه پایه گزارش 2 از 16 باشد، که 4 بیت خواهد بود، دو بیت عدم قطعیت بیشتر از قبل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before. ",
  "translatedText": "اما مطمئناً عدم قطعیت واقعی واقعاً با آنچه قبلاً داشتیم متفاوت نیست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example. ",
  "translatedText": "فقط به این دلیل که این ۱۲ کلمه واقعا مبهم وجود دارد، به این معنا نیست که برای مثال، دانستن اینکه پاسخ نهایی جذابیت است، بسیار شگفت‌انگیزتر خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits. ",
  "translatedText": "بنابراین وقتی محاسبه را در اینجا انجام می‌دهید و احتمال وقوع هر وقوع را ضربدر اطلاعات مربوطه جمع می‌کنید، چیزی که به دست می‌آورید 2 است. 11 بیت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it. ",
  "translatedText": "من فقط می گویم، اساساً این دو بیت است، اساساً آن چهار احتمال، اما به دلیل همه آن رویدادهای بسیار بعید، کمی عدم اطمینان بیشتر وجود دارد، اگرچه اگر آنها را یاد بگیرید، اطلاعات زیادی از آن به دست خواهید آورد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson. ",
  "translatedText": "بنابراین، با کوچک‌نمایی، این بخشی از چیزی است که Wordle را به یک مثال خوب برای درس تئوری اطلاعات تبدیل می‌کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy. ",
  "translatedText": "ما این دو کاربرد احساسی متمایز را برای آنتروپی داریم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible. ",
  "translatedText": "اولی به ما می‌گوید اطلاعات مورد انتظاری که از یک حدس به دست می‌آوریم چیست، و دومی می‌گوید آیا می‌توانیم عدم قطعیت باقی‌مانده را در بین همه کلماتی که ممکن است اندازه‌گیری کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation. ",
  "translatedText": "و من باید تأکید کنم، در اولین مورد که ما به اطلاعات مورد انتظار یک حدس نگاه می کنیم، زمانی که وزن نابرابر کلمات را داشته باشیم، بر محاسبه آنتروپی تأثیر می گذارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words. ",
  "translatedText": "برای مثال، اجازه دهید همان موردی را که قبلاً در مورد توزیع مرتبط با Weary بررسی می‌کردیم، بیاورم، اما این بار با استفاده از یک توزیع غیریکنواخت در همه کلمات ممکن. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well. ",
  "translatedText": "بنابراین اجازه دهید ببینم آیا می توانم بخشی را در اینجا پیدا کنم که آن را به خوبی نشان دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good. ",
  "translatedText": "خوب، اینجا خیلی خوب است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it. ",
  "translatedText": "در اینجا ما دو الگوی مجاور داریم که احتمال آنها تقریباً یکسان است، اما یکی از آنها که به ما گفته شده است دارای 32 کلمه ممکن است که با آن مطابقت دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them. ",
  "translatedText": "و اگر بررسی کنیم که آنها چه هستند، این 32 مورد هستند، که همه آنها کلمات بسیار بعید هستند که چشمان خود را روی آنها اسکن می کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely. ",
  "translatedText": "به سختی می‌توان پاسخ‌هایی را پیدا کرد که شبیه پاسخ‌های قابل قبول باشد، شاید فریاد بزند، اما اگر به الگوی همسایه در توزیع نگاه کنیم، که تقریباً به همان اندازه محتمل در نظر گرفته می‌شود، به ما گفته می‌شود که فقط 8 مطابقت ممکن دارد، بنابراین یک چهارم بسیاری از مسابقات، اما احتمال آن تقریباً به همان اندازه است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why. ",
  "translatedText": "و وقتی آن مسابقات را انجام دهیم، می‌توانیم دلیل آن را ببینیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps. ",
  "translatedText": "برخی از این ها پاسخ های واقعی هستند، مانند حلقه، خشم، یا رپ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw. ",
  "translatedText": "برای نشان دادن اینکه چگونه همه اینها را ترکیب می کنیم، اجازه دهید نسخه 2 Wordlebot را اینجا بیاورم، و دو یا سه تفاوت اصلی با نسخه اولی که دیدیم وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer. ",
  "translatedText": "اول از همه، همانطور که قبلاً گفتم، روشی که ما این آنتروپی‌ها، این مقادیر مورد انتظار اطلاعات را محاسبه می‌کنیم، اکنون از توزیع‌های دقیق‌تر در سراسر الگوها استفاده می‌کند که احتمال اینکه یک کلمه معین واقعاً پاسخ باشد را در بر می‌گیرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different. ",
  "translatedText": "همانطور که اتفاق می افتد، اشک هنوز شماره 1 است، اگرچه موارد زیر کمی متفاوت هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table. ",
  "translatedText": "دوم، زمانی که انتخاب های برتر خود را رتبه بندی می کند، اکنون مدلی از احتمال اینکه هر کلمه پاسخ واقعی است را حفظ می کند، و آن را در تصمیم خود لحاظ می کند، که وقتی چند حدس در مورد آن داشته باشیم، دیدن آن آسان تر است. جدول. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives. ",
  "translatedText": "باز هم، نادیده گرفتن توصیه آن، زیرا نمی توانیم اجازه دهیم ماشین ها بر زندگی ما حکومت کنند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches. ",
  "translatedText": "و فکر می‌کنم باید به چیز دیگری اشاره کنم که در اینجا در سمت چپ است، این که مقدار عدم قطعیت، آن تعداد بیت‌ها، دیگر فقط با تعداد تطابق‌های ممکن اضافی نیست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes. ",
  "translatedText": "حالا اگر آن را بالا بکشیم و 2 تا 8 را محاسبه کنیم. 02، که کمی بالاتر از 256 است، حدس می‌زنم 259، چیزی که می‌گوید این است که با وجود اینکه در کل 526 کلمه وجود دارد که واقعاً با این الگو مطابقت دارند، میزان عدم قطعیت آن بیشتر شبیه به چیزی است که اگر 259 به یک اندازه احتمال وجود داشت. عواقب. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this. ",
  "translatedText": "شما می توانید آن را اینگونه فکر کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case. ",
  "translatedText": "می‌داند که بورکس جوابگو نیست، همینطور در مورد یورتس و زورل و زوروس، بنابراین نسبت به حالت قبلی نامشخص کمی کمتر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller. ",
  "translatedText": "این تعداد بیت کمتر خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here. ",
  "translatedText": "و اگر به بازی ادامه دهم، این را با چند حدس که در اینجا توضیح می‌دهم مناسب است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy. ",
  "translatedText": "با حدس چهارم، اگر به انتخاب های برتر آن نگاه کنید، می بینید که دیگر فقط آنتروپی را به حداکثر نمی رساند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words. ",
  "translatedText": "بنابراین در این مرحله، از نظر فنی هفت احتمال وجود دارد، اما تنها مواردی که شانس معناداری دارند خوابگاه‌ها و کلمات هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information. ",
  "translatedText": "و می‌توانید ببینید که هر دوی آن‌ها را بالاتر از همه این مقادیر دیگر رتبه‌بندی می‌کند، که به‌طور دقیق‌تر می‌توان اطلاعات بیشتری را ارائه کرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect. ",
  "translatedText": "اولین باری که این کار را انجام دادم، فقط این دو عدد را جمع کردم تا کیفیت هر حدس را اندازه‌گیری کنم، که در واقع بهتر از چیزی که فکر می‌کنید جواب داد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on. ",
  "translatedText": "اما واقعاً سیستماتیک به نظر نمی‌رسید، و من مطمئن هستم که روش‌های دیگری هم وجود دارد که مردم می‌توانند از آن استفاده کنند، اما این همان رویکردی است که من به آن دست یافتم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that. ",
  "translatedText": "اگر به احتمال حدس بعدی فکر می کنیم، مانند کلمات این مورد، چیزی که واقعاً به آن اهمیت می دهد، امتیاز مورد انتظار بازی ما در صورت انجام این کار است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to. ",
  "translatedText": "و برای محاسبه آن امتیاز مورد انتظار، می گوییم احتمال اینکه کلمات پاسخ واقعی باشند چقدر است، که در حال حاضر 58٪ آن را توصیف می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4. ",
  "translatedText": "می گوییم با شانس 58 درصد امتیاز ما در این بازی 4 می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4. ",
  "translatedText": "و سپس با احتمال 1 منهای آن 58 درصد امتیاز ما بیشتر از آن 4 خواهد شد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point. ",
  "translatedText": "ما چقدر بیشتر نمی دانیم، اما می توانیم آن را بر اساس میزان عدم قطعیت احتمالی پس از رسیدن به آن نقطه تخمین بزنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty. ",
  "translatedText": "به طور خاص، در حال حاضر 1 وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits. ",
  "translatedText": "44 بیت عدم قطعیت اگر کلمات را حدس بزنیم، به ما می گوید که اطلاعات مورد انتظاری که به دست می آوریم 1 است. 27 بیت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens. ",
  "translatedText": "بنابراین اگر کلمات را حدس بزنیم، این تفاوت نشان‌دهنده میزان عدم قطعیتی است که بعد از این اتفاق می‌افتد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score. ",
  "translatedText": "آنچه ما نیاز داریم نوعی تابع است که من در اینجا آن را f می نامم، که این عدم قطعیت را با نمره مورد انتظار مرتبط می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty. ",
  "translatedText": "و روشی که در مورد این کار پیش رفت این بود که مجموعه ای از داده های بازی های قبلی را بر اساس نسخه 1 ربات ترسیم کنیم تا بگوییم که امتیاز واقعی بعد از نقاط مختلف با مقادیر بسیار قابل اندازه گیری عدم قطعیت چقدر بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer. ",
  "translatedText": "برای مثال، این داده‌ها در اینجا نقاطی را نشان می‌دهند که بالای یک مقدار تقریباً 8 قرار دارند. 7 یا بیشتر برای برخی از بازی ها بعد از یک نقطه که در آن 8 بود می گویند. 7 بیت عدم قطعیت، دو حدس طول کشید تا به جواب نهایی رسید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses. ",
  "translatedText": "برای بازی‌های دیگر سه حدس و برای بازی‌های دیگر چهار حدس انجام شد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring. ",
  "translatedText": "اگر در اینجا به سمت چپ حرکت کنیم، تمام نقاط بالای صفر می‌گویند که هرگاه بیت‌های عدم قطعیت صفر وجود داشته باشد، یعنی فقط یک احتمال وجود دارد، پس تعداد حدس‌های مورد نیاز همیشه فقط یک است که اطمینان‌بخش است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses. ",
  "translatedText": "هر زمان که یک ذره عدم قطعیت وجود داشت، به این معنی که اساساً فقط به دو احتمال می رسید، سپس گاهی به یک حدس بیشتر نیاز داشت، گاهی اوقات نیاز به دو حدس دیگر داشت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here. ",
  "translatedText": "و غیره و غیره اینجا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages. ",
  "translatedText": "شاید یک راه کمی ساده تر برای تجسم این داده ها این باشد که آنها را با هم جمع کنید و میانگین ها را بگیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5. ",
  "translatedText": "به عنوان مثال، این نوار در اینجا می گوید در بین تمام نقاطی که در آن یک بیت عدم قطعیت داشتیم، به طور متوسط تعداد حدس های جدید مورد نیاز حدود 1 بود. 5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward. ",
  "translatedText": "و نواری که در اینجا در میان همه بازی‌های مختلف می‌گوید که در آن عدم قطعیت کمی بالاتر از چهار بیت بود، که مانند کاهش آن به 16 احتمال مختلف است، سپس به طور متوسط به کمی بیش از دو حدس از آن نقطه نیاز دارد. رو به جلو. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this. ",
  "translatedText": "و از اینجا من فقط یک رگرسیون انجام دادم تا تابعی را که برای این کار معقول به نظر می‌رسید، جا بدهم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be. ",
  "translatedText": "و به یاد داشته باشید که تمام هدف انجام هر یک از این کارها این است که بتوانیم این شهود را کمیت کنیم که هرچه اطلاعات بیشتری از یک کلمه به دست آوریم، امتیاز مورد انتظار کمتر خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do? ",
  "translatedText": "بنابراین با این نسخه 2.0، اگر به عقب برگردیم و همان مجموعه شبیه‌سازی‌ها را اجرا کنیم و آن را با تمام 2315 پاسخ wordle ممکن بازی کنیم، چگونه انجام می‌شود؟ خوب بر خلاف نسخه اول ما، قطعا بهتر است، که اطمینان بخش است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance. ",
  "translatedText": "همه گفته‌ها و انجام‌ها میانگین حدود 3 است. 6، اگرچه بر خلاف نسخه اول چند بار است که از دست می دهد و در این شرایط به بیش از شش نیاز دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information. ",
  "translatedText": "احتمالاً به این دلیل که مواقعی وجود دارد که به جای به حداکثر رساندن اطلاعات، به دنبال هدف است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6? ",
  "translatedText": "پس آیا می توانیم بهتر از 3 کار کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can. ",
  "translatedText": "6 ما قطعا می توانیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model. ",
  "translatedText": "اکنون در ابتدا گفتم که بسیار سرگرم کننده است که سعی کنید لیست واقعی پاسخ های wordle را در روشی که مدل خود را ایجاد می کند ترکیب نکنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43. ",
  "translatedText": "اما اگر آن را در نظر بگیریم، بهترین عملکردی که می توانستم داشته باشم حدود 3 بود. 43. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that. ",
  "translatedText": "بنابراین، اگر سعی کنیم پیچیده‌تر از استفاده از داده‌های بسامد کلمه برای انتخاب این توزیع قبلی، این 3 باشد. 43 احتمالاً حداكثری را نشان می‌دهد كه چقدر می‌توانیم با آن خوب شویم، یا حداقل چقدر می‌توانم با آن خوب شوم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one. ",
  "translatedText": "این بهترین عملکرد اساساً فقط از ایده هایی استفاده می کند که من در اینجا در مورد آنها صحبت کردم، اما کمی فراتر می رود، مانند جستجوی اطلاعات مورد انتظار دو گام به جلو و نه فقط یک قدم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is. ",
  "translatedText": "در ابتدا من قصد داشتم بیشتر در مورد آن صحبت کنم، اما متوجه شدم که ما در واقع تا حدی که هست پیش رفته ایم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener. ",
  "translatedText": "تنها چیزی که من می گویم این است که پس از انجام این جستجوی دو مرحله ای و سپس اجرای چند شبیه سازی نمونه در کاندیداهای برتر، حداقل تا کنون برای من به نظر می رسد که کرین بهترین بازکننده است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits. ",
  "translatedText": "چه کسی حدس می زد؟ همچنین اگر از لیست wordle واقعی برای تعیین فضای احتمالی خود استفاده کنید، عدم قطعیتی که با آن شروع می کنید کمی بیش از 11 بیت است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits. ",
  "translatedText": "و معلوم شد، فقط از یک جستجوی brute force، حداکثر اطلاعات مورد انتظار ممکن پس از دو حدس اول حدود 10 بیت است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty. ",
  "translatedText": "این نشان می دهد که بهترین سناریو، پس از دو حدس اول شما، با بازی کاملاً بهینه، تقریباً با یک بیت عدم اطمینان روبرو خواهید شد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses. ",
  "translatedText": "که همان است که به دو حدس ممکن است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail. ",
  "translatedText": "بنابراین من فکر می کنم منصفانه و احتمالاً محافظه کارانه است که بگوییم شما هرگز نمی توانید الگوریتمی بنویسید که این میانگین را به 3 برساند، زیرا با کلماتی که در دسترس شما هستند، به سادگی پس از دو مرحله دیگر جایی برای به دست آوردن اطلاعات کافی وجود ندارد. قادر به تضمین پاسخ در شکاف سوم هر بار بدون شکست است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]