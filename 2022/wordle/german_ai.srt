1
00:00:00,000 --> 00:00:04,040
Das Spiel Wurdle ist in den letzten ein, zwei Monaten ziemlich viral gegangen,

2
00:00:04,040 --> 00:00:07,880
und da ich nie eine Gelegenheit für eine Mathematikstunde ausgelassen habe, kommt mir

3
00:00:07,880 --> 00:00:12,120
der Gedanke, dass dieses Spiel ein sehr gutes zentrales Beispiel in einer Lektion

4
00:00:12,120 --> 00:00:13,120
über Informationstheorie und insbesondere Informationstheorie darstellt ein Thema, das als Entropie bekannt ist.

5
00:00:13,120 --> 00:00:17,120
Sehen Sie, wie viele Leute wurde auch ich in das Rätsel hineingezogen,

6
00:00:17,120 --> 00:00:21,200
und wie viele Programmierer wurde auch ich in den Versuch hineingezogen, einen

7
00:00:21,200 --> 00:00:23,200
Algorithmus zu schreiben, der das Spiel so optimal wie möglich abspielt.

8
00:00:23,200 --> 00:00:26,400
Und ich dachte, ich würde hier einfach mit Ihnen einige

9
00:00:26,400 --> 00:00:29,980
meiner Prozesse besprechen und einige der darin enthaltenen Mathematik erklären,

10
00:00:29,980 --> 00:00:32,080
da der gesamte Algorithmus auf dieser Idee der Entropie basiert.

11
00:00:32,080 --> 00:00:42,180
Das Wichtigste zuerst: Falls Sie noch nichts davon gehört haben: Was ist Wurdle?

12
00:00:42,180 --> 00:00:45,380
Und um hier zwei Fliegen mit einer Klappe zu schlagen, während wir die Spielregeln

13
00:00:45,380 --> 00:00:48,980
durchgehen, möchte ich auch eine Vorschau darauf geben, wohin wir damit gehen, nämlich

14
00:00:48,980 --> 00:00:51,380
einen kleinen Algorithmus zu entwickeln, der das Spiel im Grunde für uns spielt.

15
00:00:51,380 --> 00:00:54,860
Obwohl ich das heutige Wurdle noch nicht gemacht habe, ist es der

16
00:00:54,860 --> 00:00:55,860
4. Februar und wir werden sehen, wie sich der Bot schlägt.

17
00:00:55,860 --> 00:00:59,580
Das Ziel von Wurdle ist es, ein geheimnisvolles Wort mit fünf Buchstaben

18
00:00:59,580 --> 00:01:00,860
zu erraten, und Sie haben sechs verschiedene Möglichkeiten, es zu erraten.

19
00:01:00,860 --> 00:01:05,240
Beispielsweise schlägt mein Wurdle-Bot vor, dass ich mit dem Ratekran beginne.

20
00:01:05,240 --> 00:01:09,300
Jedes Mal, wenn Sie eine Vermutung anstellen, erhalten Sie Informationen

21
00:01:09,300 --> 00:01:10,940
darüber, wie nah Ihre Vermutung an der wahren Antwort ist.

22
00:01:10,940 --> 00:01:14,540
Hier sagt mir das graue Kästchen, dass die eigentliche Antwort kein C enthält.

23
00:01:14,540 --> 00:01:18,340
Das gelbe Kästchen sagt mir, dass es ein R gibt, aber es befindet sich nicht an dieser Position.

24
00:01:18,340 --> 00:01:21,820
Das grüne Kästchen sagt mir, dass das geheime Wort

25
00:01:21,820 --> 00:01:22,820
ein A hat und an dritter Stelle steht.

26
00:01:22,820 --> 00:01:24,300
Und dann gibt es kein N und kein E.

27
00:01:24,300 --> 00:01:27,420
Lassen Sie mich einfach hineingehen und dem Wurdle-Bot diese Informationen mitteilen.

28
00:01:27,420 --> 00:01:31,500
Wir fingen mit dem Kranich an, wir bekamen Grau, Gelb, Grün, Grau, Grau.

29
00:01:31,500 --> 00:01:35,460
Machen Sie sich keine Sorgen wegen all der Daten, die gerade angezeigt werden, ich werde das zu gegebener Zeit erklären.

30
00:01:35,460 --> 00:01:39,700
Aber sein Top-Vorschlag für unsere zweite Wahl ist shtick.

31
00:01:39,700 --> 00:01:43,500
Und Ihre Vermutung muss tatsächlich ein Wort mit fünf Buchstaben sein, aber wie

32
00:01:43,500 --> 00:01:45,700
Sie sehen werden, ist es ziemlich großzügig, was Sie tatsächlich erraten lässt.

33
00:01:45,700 --> 00:01:48,860
In diesem Fall versuchen wir es mit shtick.

34
00:01:48,860 --> 00:01:50,260
Und gut, es sieht ziemlich gut aus.

35
00:01:50,260 --> 00:01:54,580
Wir drücken das S und das H, damit wir die ersten drei Buchstaben kennen und wissen, dass es ein R gibt.

36
00:01:54,740 --> 00:01:59,740
Und so wird es wie SHA irgendetwas R oder SHA R irgendetwas sein.

37
00:01:59,740 --> 00:02:03,200
Und es sieht so aus, als ob der Wurdle-Bot weiß, dass

38
00:02:03,200 --> 00:02:05,220
es nur auf zwei Möglichkeiten ankommt: entweder Shard oder Sharp.

39
00:02:05,220 --> 00:02:08,620
Das ist im Moment eine Art Streit zwischen ihnen, also denke ich,

40
00:02:08,620 --> 00:02:11,260
dass es wahrscheinlich nur, weil es alphabetisch ist, mit Shard zusammenhängt.

41
00:02:11,260 --> 00:02:13,000
Hurra, ist die eigentliche Antwort.

42
00:02:13,000 --> 00:02:14,660
Also haben wir es in drei geschafft.

43
00:02:14,660 --> 00:02:17,740
Wenn Sie sich fragen, ob das etwas nützt: Ich habe jemanden

44
00:02:17,740 --> 00:02:20,820
sagen hören, dass bei Wurdle vier Par und drei Birdie sind.

45
00:02:20,820 --> 00:02:22,960
Was meiner Meinung nach eine ziemlich treffende Analogie ist.

46
00:02:22,960 --> 00:02:27,560
Um vier zu erreichen, muss man konstant sein Spiel halten, aber verrückt ist das sicher nicht.

47
00:02:27,560 --> 00:02:30,000
Aber wenn man es in drei Teilen bekommt, fühlt es sich einfach großartig an.

48
00:02:30,000 --> 00:02:33,800
Wenn Sie also Lust darauf haben, möchte ich hier einfach meinen

49
00:02:33,800 --> 00:02:36,600
Denkprozess von Anfang an besprechen, wie ich an den Wurdle-Bot herangehe.

50
00:02:36,600 --> 00:02:39,800
Und wie ich schon sagte, es ist eigentlich ein Vorwand für eine Lektion in Informationstheorie.

51
00:02:39,800 --> 00:02:43,160
Das Hauptziel besteht darin, zu erklären, was Information und was Entropie ist.

52
00:02:48,560 --> 00:02:52,080
Mein erster Gedanke bei der Annäherung an dieses Thema war, einen Blick

53
00:02:52,080 --> 00:02:53,560
auf die relative Häufigkeit verschiedener Buchstaben in der englischen Sprache zu werfen.

54
00:02:53,560 --> 00:02:57,800
Also dachte ich: Okay, gibt es eine Eröffnungsschätzung oder

55
00:02:57,800 --> 00:02:59,960
ein Eröffnungspaar, das viele dieser häufigsten Buchstaben trifft?

56
00:02:59,960 --> 00:03:03,780
Und eines, das mir sehr gefiel, war die Arbeit mit anderen gefolgt von Nägeln.

57
00:03:03,780 --> 00:03:06,980
Der Gedanke ist, dass wenn man einen Buchstaben trifft, man einen

58
00:03:06,980 --> 00:03:07,980
grünen oder einen gelben bekommt, das fühlt sich immer gut an.

59
00:03:07,980 --> 00:03:09,460
Es fühlt sich an, als würden Sie Informationen erhalten.

60
00:03:09,460 --> 00:03:13,140
Aber selbst wenn Sie in diesen Fällen nicht klicken und immer

61
00:03:13,140 --> 00:03:16,640
Grautöne erhalten, erhalten Sie dennoch viele Informationen, da es ziemlich selten

62
00:03:16,640 --> 00:03:17,640
ist, ein Wort zu finden, das keinen dieser Buchstaben enthält.

63
00:03:17,640 --> 00:03:21,840
Aber auch das fühlt sich nicht besonders systematisch an, weil es

64
00:03:21,840 --> 00:03:23,520
beispielsweise nichts mit der Reihenfolge der Buchstaben zu tun hat.

65
00:03:23,520 --> 00:03:26,080
Warum Nägel tippen, wenn ich Schnecke tippen könnte?

66
00:03:26,080 --> 00:03:27,720
Ist es besser, das S am Ende zu haben?

67
00:03:27,720 --> 00:03:28,720
Ich bin mir nicht wirklich sicher.

68
00:03:28,720 --> 00:03:33,500
Nun sagte ein Freund von mir, dass er gerne mit dem Wort „müde“ beginnt, was mich

69
00:03:33,500 --> 00:03:37,160
irgendwie überraschte, weil darin einige ungewöhnliche Buchstaben wie das W und das Y enthalten sind.

70
00:03:37,160 --> 00:03:39,400
Aber wer weiß, vielleicht ist das ein besserer Auftakt.

71
00:03:39,400 --> 00:03:43,920
Gibt es eine Art quantitative Bewertung, mit der

72
00:03:43,920 --> 00:03:44,920
wir die Qualität einer möglichen Vermutung beurteilen können?

73
00:03:44,920 --> 00:03:48,640
Um nun die Art und Weise festzulegen, wie wir mögliche Vermutungen einordnen werden, gehen wir

74
00:03:48,640 --> 00:03:51,800
noch einmal zurück und bringen ein wenig Klarheit darüber, wie das Spiel genau aufgebaut ist.

75
00:03:51,800 --> 00:03:55,880
Es gibt also eine Liste von Wörtern, die Sie eingeben können und die

76
00:03:55,880 --> 00:03:57,920
als gültige Vermutungen gelten und die nur etwa 13.000 Wörter lang sind.

77
00:03:57,920 --> 00:04:01,560
Aber wenn man es sich anschaut, sieht man da eine Menge wirklich ungewöhnlicher Dinge, Dinge wie

78
00:04:01,560 --> 00:04:07,040
einen Kopf oder Ali und ARG, die Art von Wörtern, die bei einem Scrabble-Spiel Familienstreitigkeiten auslösen.

79
00:04:07,040 --> 00:04:10,600
Aber die Atmosphäre des Spiels ist, dass die Antwort immer ein einigermaßen gebräuchliches Wort sein wird.

80
00:04:10,600 --> 00:04:16,080
Und tatsächlich gibt es noch eine weitere Liste mit rund 2300 Wörtern, die mögliche Antworten darstellen.

81
00:04:16,080 --> 00:04:20,320
Und das ist eine von Menschen kuratierte Liste, ich glaube,

82
00:04:20,320 --> 00:04:21,800
speziell von der Freundin des Spieleentwicklers, was irgendwie Spaß macht.

83
00:04:21,800 --> 00:04:25,560
Aber was ich gerne tun würde, unsere Herausforderung für dieses Projekt besteht darin, zu sehen, ob

84
00:04:25,560 --> 00:04:30,720
wir ein Programm schreiben können, das Wordle löst, das keine Vorkenntnisse über diese Liste enthält.

85
00:04:30,720 --> 00:04:34,560
Zum einen gibt es viele ziemlich gebräuchliche Wörter mit

86
00:04:34,560 --> 00:04:35,560
fünf Buchstaben, die Sie in dieser Liste nicht finden.

87
00:04:35,560 --> 00:04:38,360
Daher wäre es besser, ein Programm zu schreiben, das etwas widerstandsfähiger ist und Wordle gegen

88
00:04:38,360 --> 00:04:41,960
jeden spielen kann, nicht nur gegen das, was zufällig auf der offiziellen Website steht.

89
00:04:41,960 --> 00:04:45,900
Und wir kennen diese Liste möglicher Antworten auch

90
00:04:45,900 --> 00:04:47,440
deshalb, weil sie im Quellcode sichtbar ist.

91
00:04:47,440 --> 00:04:51,620
Aber die Art und Weise, wie es im Quellcode sichtbar ist, liegt in

92
00:04:51,620 --> 00:04:52,840
der spezifischen Reihenfolge, in der die Antworten von Tag zu Tag kommen.

93
00:04:52,840 --> 00:04:56,400
Sie können also jederzeit nachschauen, wie die Antwort morgen lautet.

94
00:04:56,400 --> 00:04:59,140
Es ist also klar, dass die Verwendung der Liste in gewisser Weise Betrug ist.

95
00:04:59,140 --> 00:05:02,900
Und was zu einem interessanteren Rätsel und einer reichhaltigeren Informationstheorie-Lektion führt,

96
00:05:02,900 --> 00:05:07,640
ist stattdessen die Verwendung einiger universellerer Daten wie relativer Worthäufigkeiten im

97
00:05:07,640 --> 00:05:11,640
Allgemeinen, um die Intuition einer Vorliebe für gebräuchlichere Wörter zu erfassen.

98
00:05:11,640 --> 00:05:16,560
Wie sollten wir also aus diesen 13.000 Möglichkeiten den Eröffnungstipp wählen?

99
00:05:16,560 --> 00:05:19,960
Wenn mein Freund beispielsweise müde einen Heiratsantrag macht, wie sollten wir dessen Qualität analysieren?

100
00:05:19,960 --> 00:05:25,040
Nun, der Grund, warum er sagte, dass er dieses unwahrscheinliche W mag, ist, dass

101
00:05:25,040 --> 00:05:27,880
ihm die Weitsicht gefällt, wie gut es sich anfühlt, wenn man dieses W trifft.

102
00:05:27,880 --> 00:05:31,400
Wenn zum Beispiel das erste Muster, das aufgedeckt wurde, ungefähr so aussah, dann stellt sich

103
00:05:31,400 --> 00:05:36,080
heraus, dass es in diesem riesigen Lexikon nur 58 Wörter gibt, die diesem Muster entsprechen.

104
00:05:36,080 --> 00:05:38,900
Das ist also eine enorme Reduzierung gegenüber 13.000.

105
00:05:38,900 --> 00:05:43,320
Aber die Kehrseite davon ist natürlich, dass es sehr ungewöhnlich ist, ein solches Muster zu erhalten.

106
00:05:43,360 --> 00:05:47,600
Wenn jedes Wort mit gleicher Wahrscheinlichkeit die Antwort wäre, wäre die

107
00:05:47,600 --> 00:05:51,680
Wahrscheinlichkeit, dieses Muster zu treffen, 58 geteilt durch etwa 13.000.

108
00:05:51,680 --> 00:05:53,880
Natürlich ist es nicht gleichermaßen wahrscheinlich, dass es sich dabei um Antworten handelt.

109
00:05:53,880 --> 00:05:56,680
Die meisten davon sind sehr obskure und sogar fragwürdige Wörter.

110
00:05:56,680 --> 00:05:59,560
Aber zumindest für unseren ersten Versuch gehen wir davon aus, dass

111
00:05:59,560 --> 00:06:02,040
sie alle gleich wahrscheinlich sind, und verfeinern das dann etwas später.

112
00:06:02,040 --> 00:06:07,360
Der Punkt ist, dass es von Natur aus unwahrscheinlich ist, dass ein Muster mit vielen Informationen auftritt.

113
00:06:07,360 --> 00:06:11,320
Tatsächlich bedeutet es, informativ zu sein, dass es unwahrscheinlich ist.

114
00:06:11,920 --> 00:06:16,720
Ein viel wahrscheinlicheres Muster, das man bei dieser Eröffnung sehen

115
00:06:16,720 --> 00:06:18,360
könnte, wäre so etwas, wo natürlich kein W drin ist.

116
00:06:18,360 --> 00:06:22,080
Vielleicht gibt es ein E, vielleicht gibt es kein A, es gibt kein R, es gibt kein Y.

117
00:06:22,080 --> 00:06:24,640
In diesem Fall gibt es 1400 mögliche Übereinstimmungen.

118
00:06:24,640 --> 00:06:29,600
Wenn alle gleich wahrscheinlich wären, errechnet sich eine Wahrscheinlichkeit von etwa 11

119
00:06:29,600 --> 00:06:30,680
%, dass es sich um das Muster handelt, das Sie sehen würden.

120
00:06:30,680 --> 00:06:34,320
Daher sind die wahrscheinlichsten Ergebnisse auch die am wenigsten aussagekräftigen.

121
00:06:34,320 --> 00:06:38,440
Um hier einen umfassenderen Überblick zu erhalten, möchte ich Ihnen die vollständige Verteilung

122
00:06:38,440 --> 00:06:42,000
der Wahrscheinlichkeiten über alle verschiedenen Muster hinweg zeigen, die Sie möglicherweise sehen.

123
00:06:42,000 --> 00:06:46,000
Jeder Balken, den Sie betrachten, entspricht also einem möglichen Farbmuster, das aufgedeckt

124
00:06:46,000 --> 00:06:50,500
werden könnte, von denen es 3 bis 5 Möglichkeiten gibt, und sie

125
00:06:50,500 --> 00:06:52,960
sind von links nach rechts geordnet, am häufigsten bis am seltensten.

126
00:06:52,960 --> 00:06:56,200
Die häufigste Möglichkeit besteht hier also darin, dass Sie nur Grautöne erhalten.

127
00:06:56,200 --> 00:06:58,800
Das passiert in etwa 14 % der Fälle.

128
00:06:58,800 --> 00:07:02,040
Und wenn Sie eine Vermutung anstellen, hoffen Sie, dass Sie irgendwo

129
00:07:02,040 --> 00:07:06,360
in diesem langen Schwanz landen, wie hier, wo es nur 18

130
00:07:06,360 --> 00:07:09,920
Möglichkeiten gibt, was zu diesem Muster passt und offensichtlich so aussieht.

131
00:07:09,920 --> 00:07:14,080
Oder wenn wir uns etwas weiter nach links wagen, wissen Sie, vielleicht kommen wir bis hierher.

132
00:07:14,080 --> 00:07:16,560
Okay, hier ist ein gutes Rätsel für dich.

133
00:07:16,560 --> 00:07:20,600
Welche drei Wörter in der englischen Sprache beginnen mit einem

134
00:07:20,600 --> 00:07:22,040
W, enden mit einem Y und enthalten irgendwo ein R?

135
00:07:22,040 --> 00:07:27,560
Es stellt sich heraus, dass die Antworten, mal sehen, wortreich, wurmig und ironisch sind.

136
00:07:27,560 --> 00:07:32,720
Um zu beurteilen, wie gut dieses Wort insgesamt ist, benötigen wir eine Art

137
00:07:32,720 --> 00:07:35,720
Maß für die erwartete Menge an Informationen, die Sie von dieser Distribution erhalten.

138
00:07:36,360 --> 00:07:41,080
Wenn wir jedes Muster durchgehen und seine Auftrittswahrscheinlichkeit mit etwas multiplizieren, das misst,

139
00:07:41,080 --> 00:07:46,000
wie informativ es ist, kann uns das vielleicht eine objektive Bewertung geben.

140
00:07:46,000 --> 00:07:50,280
Ihr erster Instinkt dafür, was das sein sollte, könnte nun die Anzahl der Übereinstimmungen sein.

141
00:07:50,280 --> 00:07:52,960
Sie möchten eine geringere durchschnittliche Anzahl von Übereinstimmungen.

142
00:07:52,960 --> 00:07:57,400
Aber stattdessen möchte ich ein universelleres Maß verwenden, das wir oft Informationen zuschreiben,

143
00:07:57,400 --> 00:08:01,040
und eines, das flexibler ist, wenn wir jedem dieser 13.000 Wörter eine

144
00:08:01,040 --> 00:08:04,320
andere Wahrscheinlichkeit dafür zuordnen, ob es tatsächlich die Antwort ist oder nicht.

145
00:08:10,600 --> 00:08:14,760
Die Standardinformationseinheit ist das Bit, dessen Formel etwas komisch ist,

146
00:08:14,760 --> 00:08:17,800
aber wirklich intuitiv ist, wenn wir uns nur Beispiele ansehen.

147
00:08:17,800 --> 00:08:21,880
Wenn Sie eine Beobachtung haben, die Ihren Möglichkeitenraum

148
00:08:21,880 --> 00:08:24,200
halbiert, sagen wir, dass sie eine Information enthält.

149
00:08:24,200 --> 00:08:27,680
In unserem Beispiel besteht der Raum der Möglichkeiten aus allen möglichen Wörtern, und es stellt sich heraus, dass

150
00:08:27,760 --> 00:08:31,560
etwa die Hälfte der Wörter mit fünf Buchstaben ein S hat, etwas weniger, aber etwa die Hälfte.

151
00:08:31,560 --> 00:08:35,200
Diese Beobachtung würde Ihnen also eine kleine Information geben.

152
00:08:35,200 --> 00:08:39,640
Wenn stattdessen eine neue Tatsache den Raum der Möglichkeiten um den

153
00:08:39,640 --> 00:08:42,000
Faktor vier verkleinert, sagen wir, dass sie zwei Informationsbits enthält.

154
00:08:42,000 --> 00:08:45,120
Es stellt sich beispielsweise heraus, dass etwa ein Viertel dieser Wörter ein T hat.

155
00:08:45,120 --> 00:08:49,720
Wenn die Beobachtung diesen Raum um den Faktor acht verkleinert, sagen wir, dass

156
00:08:49,720 --> 00:08:50,920
es sich um drei Informationsbits handelt, und so weiter und so fort.

157
00:08:50,920 --> 00:08:55,000
Vier Bits zerschneiden es in ein Sechzehntel, fünf Bits zerschneiden es in ein 32tel.

158
00:08:55,000 --> 00:09:00,160
Jetzt möchten Sie vielleicht innehalten und sich fragen: Wie lautet die Formel für

159
00:09:00,160 --> 00:09:04,520
Informationen über die Anzahl der Bits im Hinblick auf die Wahrscheinlichkeit eines Auftretens?

160
00:09:04,520 --> 00:09:07,920
Was wir hier sagen ist, dass wenn man die Hälfte der Anzahl der Bits hochnimmt, das

161
00:09:07,920 --> 00:09:11,680
dasselbe ist wie die Wahrscheinlichkeit, was dasselbe ist, als würde man sagen, dass zwei hoch

162
00:09:11,680 --> 00:09:16,200
die Anzahl der Bits eins über der Wahrscheinlichkeit ist, was ordnet sich weiter um und

163
00:09:16,200 --> 00:09:19,680
sagt, dass die Informationen die logarithmische Basis zwei von eins dividiert durch die Wahrscheinlichkeit sind.

164
00:09:19,680 --> 00:09:23,200
Und manchmal sieht man das noch bei einer weiteren Neuordnung, bei

165
00:09:23,200 --> 00:09:25,680
der die Information die negative logarithmische Basis zwei der Wahrscheinlichkeit ist.

166
00:09:25,680 --> 00:09:29,120
So ausgedrückt, mag es für den Uneingeweihten etwas seltsam aussehen,

167
00:09:29,120 --> 00:09:33,400
aber es ist eigentlich nur die sehr intuitive Idee,

168
00:09:33,400 --> 00:09:35,120
zu fragen, wie oft man seine Möglichkeiten halbiert hat.

169
00:09:35,120 --> 00:09:37,840
Wenn Sie sich jetzt fragen: Ich dachte, wir spielen nur

170
00:09:37,840 --> 00:09:39,920
ein lustiges Wortspiel, warum kommen dann Logarithmen ins Spiel?

171
00:09:39,920 --> 00:09:43,920
Ein Grund dafür, dass dies eine schönere Einheit ist, ist, dass es einfach viel einfacher ist, über

172
00:09:43,920 --> 00:09:48,120
sehr unwahrscheinliche Ereignisse zu sprechen, viel einfacher zu sagen, dass eine Beobachtung 20 Bits an Informationen

173
00:09:48,120 --> 00:09:53,480
enthält, als zu sagen, dass die Wahrscheinlichkeit, dass dieses oder jenes eintritt, 0 ist. 0000095.

174
00:09:53,480 --> 00:09:57,360
Aber ein substanziellerer Grund dafür, dass sich dieser logarithmische Ausdruck als sehr nützliche

175
00:09:57,360 --> 00:10:02,000
Ergänzung zur Wahrscheinlichkeitstheorie erwies, ist die Art und Weise, wie Informationen addiert werden.

176
00:10:02,000 --> 00:10:05,560
Wenn Ihnen beispielsweise eine Beobachtung zwei Informationsbits liefert, wodurch Ihr Platz um vier

177
00:10:05,560 --> 00:10:10,120
reduziert wird, und wenn Ihnen dann eine zweite Beobachtung wie Ihre zweite Schätzung

178
00:10:10,120 --> 00:10:14,480
in Wordle weitere drei Informationsbits liefert, wodurch Sie noch einmal um den Faktor

179
00:10:14,480 --> 00:10:17,360
acht reduziert werden, dann ist das der Fall zwei zusammen ergeben fünf Informationen.

180
00:10:17,360 --> 00:10:21,200
So wie sich Wahrscheinlichkeiten gerne vervielfachen, fügen sich auch Informationen gerne hinzu.

181
00:10:21,200 --> 00:10:24,920
Sobald wir uns also im Bereich eines erwarteten Werts befinden, bei dem wir

182
00:10:24,920 --> 00:10:28,660
eine Reihe von Zahlen addieren, ist der Umgang mit den Protokollen viel einfacher.

183
00:10:28,660 --> 00:10:32,600
Kehren wir zu unserer Verteilung für Weary zurück und fügen hier einen weiteren kleinen

184
00:10:32,600 --> 00:10:35,560
Tracker hinzu, der uns zeigt, wie viele Informationen für jedes Muster vorhanden sind.

185
00:10:35,560 --> 00:10:38,760
Ich möchte Sie vor allem darauf aufmerksam machen, dass je höher die Wahrscheinlichkeit ist, wenn wir

186
00:10:38,760 --> 00:10:43,500
zu diesen wahrscheinlicheren Mustern gelangen, je niedriger die Informationen sind, desto weniger Bits gewinnen Sie.

187
00:10:43,500 --> 00:10:47,360
Wir messen die Qualität dieser Vermutung, indem wir den erwarteten Wert dieser

188
00:10:47,360 --> 00:10:51,620
Informationen nehmen, indem wir jedes Muster durchgehen, sagen, wie wahrscheinlich es ist,

189
00:10:51,620 --> 00:10:54,940
und diesen dann mit der Anzahl der Informationen multiplizieren, die wir erhalten.

190
00:10:54,940 --> 00:10:58,480
Und im Beispiel von Weary sind es 4. 9 Bit.

191
00:10:58,480 --> 00:11:02,800
Im Durchschnitt sind die Informationen, die Sie aus dieser Eröffnungsvermutung erhalten, so

192
00:11:02,800 --> 00:11:05,660
gut, als würden Sie Ihren Raum an Möglichkeiten etwa fünfmal halbieren.

193
00:11:05,660 --> 00:11:10,260
Im Gegensatz dazu wäre ein Beispiel für eine Schätzung

194
00:11:10,260 --> 00:11:13,220
mit einem höheren erwarteten Informationswert so etwas wie Slate.

195
00:11:13,220 --> 00:11:16,180
In diesem Fall werden Sie feststellen, dass die Verteilung viel flacher aussieht.

196
00:11:16,180 --> 00:11:20,780
Insbesondere beträgt die Eintrittswahrscheinlichkeit des wahrscheinlichsten aller Grautöne nur etwa 6

197
00:11:20,780 --> 00:11:25,940
%, Sie erhalten also offensichtlich mindestens 3. 9 Bits an Informationen.

198
00:11:25,940 --> 00:11:29,140
Aber das ist ein Minimum, normalerweise bekommt man etwas Besseres.

199
00:11:29,140 --> 00:11:33,380
Und es stellt sich heraus, dass die durchschnittliche Information, wenn man die Zahlen zu

200
00:11:33,380 --> 00:11:36,420
diesem Thema auswertet und alle relevanten Begriffe addiert, bei etwa 5 liegt. 8.

201
00:11:36,420 --> 00:11:42,140
Im Gegensatz zu Weary wird Ihr Spielraum an Möglichkeiten also nach

202
00:11:42,140 --> 00:11:43,940
dieser ersten Vermutung im Durchschnitt etwa halb so groß sein.

203
00:11:43,940 --> 00:11:49,540
Es gibt tatsächlich eine lustige Geschichte zum Namen für diesen erwarteten Wert der Informationsmenge.

204
00:11:49,540 --> 00:11:52,580
Die Informationstheorie wurde von Claude Shannon entwickelt, der in den 1940er Jahren an

205
00:11:52,580 --> 00:11:57,620
den Bell Labs arbeitete, aber er sprach über einige seiner noch nicht

206
00:11:57,620 --> 00:12:01,500
veröffentlichten Ideen mit John von Neumann, dem damals prominenten intellektuellen Giganten in

207
00:12:01,500 --> 00:12:04,180
Mathematik und Physik und die Anfänge dessen, was später zur Informatik wurde.

208
00:12:04,180 --> 00:12:07,260
Und als er erwähnte, dass er keinen wirklich guten Namen für

209
00:12:07,260 --> 00:12:12,540
diesen erwarteten Wert der Informationsmenge hatte, sagte von Neumann angeblich,

210
00:12:12,540 --> 00:12:14,720
man sollte es Entropie nennen, und das aus zwei Gründen.

211
00:12:14,720 --> 00:12:18,400
Erstens wurde Ihre Unsicherheitsfunktion in der statistischen Mechanik unter diesem Namen verwendet, sie hat also

212
00:12:18,400 --> 00:12:23,100
bereits einen Namen, und zweitens, und was noch wichtiger ist, weiß niemand, was Entropie

213
00:12:23,100 --> 00:12:26,940
wirklich ist, also werden Sie es in einer Debatte immer tun den Vorteil haben.

214
00:12:26,940 --> 00:12:31,420
Wenn der Name also etwas mysteriös erscheint und man dieser

215
00:12:31,420 --> 00:12:33,420
Geschichte Glauben schenken darf, dann ist das sozusagen Absicht.

216
00:12:33,420 --> 00:12:36,740
Auch wenn Sie sich über seine Beziehung zu all dem zweiten Hauptsatz

217
00:12:36,740 --> 00:12:40,820
der Thermodynamik aus der Physik wundern, gibt es definitiv einen Zusammenhang, aber

218
00:12:40,820 --> 00:12:44,780
in seinen Ursprüngen beschäftigte sich Shannon nur mit der reinen Wahrscheinlichkeitstheorie,

219
00:12:44,780 --> 00:12:49,340
und für unsere Zwecke hier, wenn ich das verwende Beim Wort Entropie

220
00:12:49,340 --> 00:12:50,820
möchte ich Ihnen nur den erwarteten Informationswert einer bestimmten Vermutung vorstellen.

221
00:12:50,820 --> 00:12:54,380
Man kann sich Entropie als die gleichzeitige Messung zweier Dinge vorstellen.

222
00:12:54,380 --> 00:12:57,420
Die erste Frage ist, wie flach die Verteilung ist.

223
00:12:57,420 --> 00:13:01,700
Je näher eine Verteilung an der Gleichmäßigkeit liegt, desto höher ist die Entropie.

224
00:13:01,700 --> 00:13:06,340
In unserem Fall, in dem es insgesamt 3 bis 5 Muster gibt, würde die Beobachtung eines beliebigen

225
00:13:06,340 --> 00:13:11,340
Musters für eine gleichmäßige Verteilung die Informationsprotokollbasis 2 von 3 bis 5 ergeben, was zufällig 7

226
00:13:11,340 --> 00:13:17,860
ist. 92, das ist also das absolute Maximum, das man für diese Entropie erreichen könnte.

227
00:13:17,860 --> 00:13:21,900
Aber die Entropie ist auch eine Art Maß

228
00:13:21,900 --> 00:13:22,900
dafür, wie viele Möglichkeiten es überhaupt gibt.

229
00:13:22,900 --> 00:13:26,980
Wenn Sie beispielsweise ein Wort haben, bei dem es nur 16 mögliche Muster gibt

230
00:13:26,980 --> 00:13:32,760
und jedes davon gleich wahrscheinlich ist, beträgt diese Entropie, diese erwartete Information, 4 Bits.

231
00:13:32,760 --> 00:13:36,880
Aber wenn Sie ein anderes Wort haben, bei dem 64 mögliche Muster auftauchen

232
00:13:36,880 --> 00:13:41,000
könnten und alle gleich wahrscheinlich sind, dann würde die Entropie 6 Bit betragen.

233
00:13:41,000 --> 00:13:45,800
Wenn Sie also irgendwo in freier Wildbahn eine Verteilung sehen, die eine Entropie von 6 Bit

234
00:13:45,800 --> 00:13:50,000
hat, dann ist das so, als ob das so wäre, als ob es so viel Variation

235
00:13:50,000 --> 00:13:54,400
und Ungewissheit darüber gibt, was passieren wird, als ob es 64 gleich wahrscheinliche Ergebnisse gäbe.

236
00:13:54,400 --> 00:13:58,360
Bei meinem ersten Durchgang beim Wurtelebot ließ ich es im Grunde einfach so machen.

237
00:13:58,360 --> 00:14:03,560
Es geht alle möglichen Vermutungen durch, alle 13.000 Wörter, berechnet die Entropie für jedes

238
00:14:03,560 --> 00:14:08,580
einzelne, oder genauer gesagt, die Entropie der Verteilung über alle Muster, die Sie

239
00:14:08,580 --> 00:14:13,040
möglicherweise sehen, für jedes einzelne und wählt das höchste aus, denn das ist

240
00:14:13,040 --> 00:14:17,200
so diejenige, die Ihren Raum an Möglichkeiten wahrscheinlich so weit wie möglich einschränkt.

241
00:14:17,200 --> 00:14:20,120
Und obwohl ich hier nur über die erste Vermutung gesprochen

242
00:14:20,120 --> 00:14:21,680
habe, gilt das Gleiche auch für die nächsten paar Vermutungen.

243
00:14:21,680 --> 00:14:25,100
Wenn Sie beispielsweise bei dieser ersten Vermutung ein Muster erkennen, das Sie auf

244
00:14:25,100 --> 00:14:29,300
eine kleinere Anzahl möglicher Wörter beschränkt, je nachdem, was damit übereinstimmt, spielen

245
00:14:29,300 --> 00:14:32,300
Sie einfach dasselbe Spiel mit Bezug auf diese kleinere Gruppe von Wörtern.

246
00:14:32,300 --> 00:14:36,500
Für eine vorgeschlagene zweite Vermutung betrachten Sie die Verteilung aller Muster,

247
00:14:36,500 --> 00:14:41,540
die aus dieser eingeschränkteren Menge von Wörtern auftreten könnten, durchsuchen

248
00:14:41,540 --> 00:14:45,480
alle 13.000 Möglichkeiten und finden diejenige, die diese Entropie maximiert.

249
00:14:45,480 --> 00:14:48,980
Um Ihnen zu zeigen, wie das in der Praxis funktioniert, möchte ich einfach eine kleine Variante

250
00:14:48,980 --> 00:14:54,060
von Wurtele aufrufen, die ich geschrieben habe und die am Rand die Höhepunkte dieser Analyse zeigt.

251
00:14:54,460 --> 00:14:57,820
Nachdem wir alle Entropieberechnungen durchgeführt haben, zeigt es uns

252
00:14:57,820 --> 00:15:00,340
hier rechts, welche die höchsten erwarteten Informationen haben.

253
00:15:00,340 --> 00:15:04,940
Es stellt sich heraus, dass die beste Antwort, zumindest im Moment, wir werden das

254
00:15:04,940 --> 00:15:11,140
später verfeinern, Tares ist, was, ähm, natürlich, eine Wicke bedeutet, die häufigste Wicke.

255
00:15:11,140 --> 00:15:14,180
Jedes Mal, wenn wir hier eine Vermutung anstellen, bei der ich vielleicht die Empfehlungen

256
00:15:14,180 --> 00:15:19,220
ignoriere und mich für Slate entscheide, weil ich Slate mag, können wir sehen,

257
00:15:19,220 --> 00:15:23,300
wie viele erwartete Informationen es hatte, aber rechts vom Wort wird uns dann

258
00:15:23,340 --> 00:15:24,980
angezeigt, wie viele Tatsächliche Informationen, die wir aufgrund dieses besonderen Musters erhalten haben.

259
00:15:24,980 --> 00:15:28,660
Hier sieht es also so aus, als hätten wir etwas Pech gehabt, man hatte erwartet, dass wir 5 bekommen. 8, aber

260
00:15:28,660 --> 00:15:30,660
wir haben zufällig etwas mit weniger bekommen.

261
00:15:30,660 --> 00:15:34,020
Und dann zeigt es uns auf der linken Seite alle

262
00:15:34,020 --> 00:15:35,860
möglichen Wörter, je nachdem, wo wir uns gerade befinden.

263
00:15:35,860 --> 00:15:39,820
Die blauen Balken geben an, wie wahrscheinlich es ist, dass jedes Wort vorkommt. Im Moment geht es

264
00:15:39,820 --> 00:15:44,140
also davon aus, dass jedes Wort mit gleicher Wahrscheinlichkeit vorkommt, aber wir werden das gleich verfeinern.

265
00:15:44,140 --> 00:15:48,580
Und dann sagt uns diese Unsicherheitsmessung die Entropie dieser Verteilung über die

266
00:15:48,580 --> 00:15:53,220
möglichen Wörter, was im Moment, weil es eine gleichmäßige Verteilung ist, nur

267
00:15:53,300 --> 00:15:55,940
eine unnötig komplizierte Methode ist, die Anzahl der Möglichkeiten zu zählen.

268
00:15:55,940 --> 00:16:01,700
Wenn wir zum Beispiel 2 hoch 13 nehmen würden. 66, das dürften

269
00:16:01,700 --> 00:16:02,700
etwa 13.000 Möglichkeiten sein.

270
00:16:02,700 --> 00:16:06,780
Ich bin hier etwas daneben, aber nur, weil ich nicht alle Dezimalstellen zeige.

271
00:16:06,780 --> 00:16:10,260
Im Moment mag das überflüssig wirken und die Sache zu kompliziert machen, aber Sie

272
00:16:10,260 --> 00:16:12,780
werden sehen, warum es nützlich ist, beide Zahlen in einer Minute zu haben.

273
00:16:12,780 --> 00:16:16,780
Hier scheint es also darauf hinzudeuten, dass die höchste Entropie für unsere zweite

274
00:16:16,780 --> 00:16:19,700
Vermutung Ramen ist, was sich wiederum einfach nicht wie ein Wort anfühlt.

275
00:16:19,700 --> 00:16:25,660
Um hier also den moralischen Standpunkt zu vertreten, tippe ich Rains ein.

276
00:16:25,660 --> 00:16:27,540
Und wieder sieht es so aus, als hätten wir etwas Pech gehabt.

277
00:16:27,540 --> 00:16:32,100
Wir hatten 4 erwartet. 3 Bits und wir haben nur 3. 39 Bit Informationen.

278
00:16:32,100 --> 00:16:35,060
Damit kommen wir auf 55 Möglichkeiten.

279
00:16:35,060 --> 00:16:38,860
Und hier werde ich vielleicht einfach dem folgen, was

280
00:16:38,860 --> 00:16:40,200
es vorschlägt, nämlich Combo, was auch immer das bedeutet.

281
00:16:40,200 --> 00:16:43,300
Und okay, das ist tatsächlich eine gute Chance für ein Rätsel.

282
00:16:43,300 --> 00:16:47,020
Es sagt uns, dass dieses Muster uns 4 gibt. 7 Bits an Informationen.

283
00:16:47,020 --> 00:16:52,400
Aber bevor wir dieses Muster sehen, waren es auf der linken Seite fünf. 78 Bit Unsicherheit.

284
00:16:52,400 --> 00:16:56,860
Als Quiz für Sie: Was bedeutet das über die Anzahl der verbleibenden Möglichkeiten?

285
00:16:56,860 --> 00:17:02,280
Nun, es bedeutet, dass wir auf ein bisschen Unsicherheit reduziert sind, was

286
00:17:02,280 --> 00:17:04,700
dasselbe ist, als würde man sagen, dass es zwei mögliche Antworten gibt.

287
00:17:04,700 --> 00:17:06,520
Es ist eine 50:50-Wahl.

288
00:17:06,520 --> 00:17:09,860
Und da Sie und ich wissen, welche Wörter gebräuchlicher

289
00:17:09,860 --> 00:17:11,220
sind, wissen wir, dass die Antwort „Abgrund“ lauten sollte.

290
00:17:11,220 --> 00:17:13,540
Aber so wie es gerade geschrieben steht, weiß das Programm das nicht.

291
00:17:13,540 --> 00:17:17,560
Also macht es einfach weiter und versucht, so viele Informationen wie möglich zu

292
00:17:17,560 --> 00:17:20,360
sammeln, bis nur noch eine Möglichkeit übrig ist, und dann errät es es.

293
00:17:20,360 --> 00:17:22,700
Wir brauchen also offensichtlich eine bessere Endspielstrategie.

294
00:17:22,700 --> 00:17:26,540
Aber nehmen wir an, wir nennen diese Version einen unserer Wortlöser und

295
00:17:26,540 --> 00:17:30,740
führen dann einige Simulationen durch, um zu sehen, wie es funktioniert.

296
00:17:30,740 --> 00:17:34,240
Das funktioniert also so, dass jedes mögliche Weltspiel gespielt wird.

297
00:17:34,240 --> 00:17:38,780
Es geht darum, alle 2315 Wörter durchzugehen, die die eigentlichen Wortantworten sind.

298
00:17:38,780 --> 00:17:41,340
Im Grunde wird es als Testset verwendet.

299
00:17:41,340 --> 00:17:45,820
Und mit dieser naiven Methode, nicht darüber nachzudenken, wie häufig ein Wort ist, und einfach zu versuchen, die

300
00:17:45,820 --> 00:17:50,480
Informationen bei jedem Schritt auf dem Weg zu maximieren, bis es nur noch eine einzige Wahlmöglichkeit gibt.

301
00:17:50,480 --> 00:17:55,100
Am Ende der Simulation liegt die durchschnittliche Punktzahl bei etwa 4. 124.

302
00:17:55,100 --> 00:17:59,780
Was nicht schlecht ist, um ehrlich zu sein, ich hatte irgendwie damit gerechnet, dass es schlechter abschneiden würde.

303
00:17:59,780 --> 00:18:03,040
Aber die Leute, die Wordle spielen, werden Ihnen sagen, dass sie es normalerweise in 4 Minuten schaffen.

304
00:18:03,040 --> 00:18:05,260
Die eigentliche Herausforderung besteht darin, in 3 so viele wie möglich zu bekommen.

305
00:18:05,260 --> 00:18:08,920
Es ist ein ziemlich großer Sprung zwischen der Punktzahl 4 und der Punktzahl 3.

306
00:18:08,920 --> 00:18:13,300
Die offensichtliche, niedrig hängende Frucht besteht hier darin, irgendwie einzubeziehen, ob ein

307
00:18:13,300 --> 00:18:23,160
Wort gebräuchlich ist oder nicht, und wie wir das genau machen.

308
00:18:23,160 --> 00:18:26,860
Mein Ansatz besteht darin, eine Liste der relativen Häufigkeiten

309
00:18:26,860 --> 00:18:28,560
aller Wörter in der englischen Sprache zu erhalten.

310
00:18:28,560 --> 00:18:32,560
Und ich habe gerade die Worthäufigkeitsdatenfunktion von Mathematica verwendet, die ihrerseits

311
00:18:32,560 --> 00:18:35,520
aus dem öffentlichen Ngram-Datensatz für Englisch von Google Books stammt.

312
00:18:35,520 --> 00:18:38,680
Und es macht irgendwie Spaß, es anzusehen, wenn wir es zum Beispiel

313
00:18:38,680 --> 00:18:40,120
von den häufigsten Wörtern zu den am wenigsten häufigen Wörtern sortieren.

314
00:18:40,120 --> 00:18:43,740
Offensichtlich sind dies die häufigsten Wörter mit fünf Buchstaben in der englischen Sprache.

315
00:18:43,740 --> 00:18:46,480
Oder besser gesagt, dies ist die achthäufigste.

316
00:18:46,480 --> 00:18:49,440
Zuerst ist which, danach gibt es there und there.

317
00:18:49,440 --> 00:18:53,020
First selbst ist nicht first, sondern 9th, und es macht Sinn,

318
00:18:53,020 --> 00:18:57,840
dass diese anderen Wörter häufiger vorkommen könnten, wobei die Worte nach

319
00:18:57,840 --> 00:18:59,000
first nach, where sind und jene nur etwas seltener vorkommen.

320
00:18:59,000 --> 00:19:04,400
Wenn wir diese Daten nun verwenden, um zu modellieren, wie wahrscheinlich es ist, dass jedes

321
00:19:04,400 --> 00:19:06,760
dieser Wörter die endgültige Antwort ist, sollten sie nicht nur proportional zur Häufigkeit sein.

322
00:19:07,020 --> 00:19:12,560
Zum Beispiel, was mit einer Punktzahl von 0 bewertet wird. 002 in diesem Datensatz, während das

323
00:19:12,560 --> 00:19:15,200
Wort „zopf“ in gewissem Sinne etwa 1000-mal unwahrscheinlicher ist.

324
00:19:15,200 --> 00:19:19,400
Aber beide Wörter sind so häufig, dass sie mit ziemlicher Sicherheit eine Überlegung wert sind.

325
00:19:19,400 --> 00:19:21,900
Wir wollen also eher einen binären Cutoff.

326
00:19:21,900 --> 00:19:26,520
Meine Vorgehensweise besteht darin, mir vorzustellen, dass ich diese gesamte sortierte Liste von

327
00:19:26,520 --> 00:19:31,060
Wörtern nehme, sie dann auf einer x-Achse anordne und dann die Sigmoidfunktion anwende,

328
00:19:31,060 --> 00:19:35,540
was die Standardmethode für eine Funktion ist, deren Ausgabe grundsätzlich binär ist entweder

329
00:19:35,540 --> 00:19:38,500
0 oder 1, aber dazwischen gibt es für diesen Unsicherheitsbereich eine Glättung.

330
00:19:38,500 --> 00:19:43,900
Im Wesentlichen ist die Wahrscheinlichkeit, die ich jedem Wort für die Aufnahme in die endgültige Liste

331
00:19:43,900 --> 00:19:49,540
zuordne, der Wert der Sigmoidfunktion oben, wo auch immer sie sich auf der x-Achse befindet.

332
00:19:49,540 --> 00:19:53,940
Dies hängt natürlich von einigen Parametern ab. Beispielsweise bestimmt die Breite des Raums auf

333
00:19:53,940 --> 00:19:59,660
der X-Achse, den diese Wörter ausfüllen, wie allmählich oder steil wir von 1 auf

334
00:19:59,660 --> 00:20:03,000
0 abfallen, und wo wir sie von links nach rechts positionieren, bestimmt die Grenze.

335
00:20:03,160 --> 00:20:07,340
Um ehrlich zu sein, habe ich das einfach so gemacht, indem ich meinen Finger abgeleckt und ihn in den Wind gehalten habe.

336
00:20:07,340 --> 00:20:10,800
Ich habe die sortierte Liste durchgesehen und versucht, ein Fenster zu finden, in dem

337
00:20:10,800 --> 00:20:15,280
ich beim Betrachten davon ausgegangen bin, dass etwa die Hälfte dieser Wörter mit

338
00:20:15,280 --> 00:20:17,680
größerer Wahrscheinlichkeit die endgültige Antwort sein werden, und habe dies als Grenzwert verwendet.

339
00:20:17,680 --> 00:20:21,840
Sobald wir eine solche Verteilung über die Wörter haben, ergibt sich eine

340
00:20:21,840 --> 00:20:24,460
weitere Situation, in der die Entropie zu diesem wirklich nützlichen Maß wird.

341
00:20:24,460 --> 00:20:28,480
Nehmen wir zum Beispiel an, wir spielen ein Spiel und beginnen mit

342
00:20:28,480 --> 00:20:32,480
meinen alten Eröffnungsworten, die eine Feder und Nägel waren, und enden in

343
00:20:32,480 --> 00:20:33,760
einer Situation, in der es vier mögliche Wörter gibt, die dazu passen.

344
00:20:33,760 --> 00:20:36,440
Nehmen wir an, wir halten sie alle für gleich wahrscheinlich.

345
00:20:36,440 --> 00:20:40,000
Ich frage Sie: Wie groß ist die Entropie dieser Verteilung?

346
00:20:40,000 --> 00:20:45,920
Nun, die Informationen, die jeder dieser Möglichkeiten zugeordnet sind, werden die Logbasis 2 von

347
00:20:45,920 --> 00:20:50,800
4 sein, da jede davon 1 und 4 ist, und das ist 2.

348
00:20:50,800 --> 00:20:52,780
Zwei Informationen, vier Möglichkeiten.

349
00:20:52,780 --> 00:20:54,360
Alles sehr schön und gut.

350
00:20:54,360 --> 00:20:58,320
Aber was wäre, wenn ich Ihnen sagen würde, dass es tatsächlich mehr als vier Spiele sind?

351
00:20:58,320 --> 00:21:02,600
Wenn wir die vollständige Wortliste durchsehen, finden wir in Wirklichkeit 16 Wörter, die dazu passen.

352
00:21:02,600 --> 00:21:07,260
Aber nehmen wir an, dass unser Modell den anderen 12 Wörtern eine sehr geringe Wahrscheinlichkeit zuordnet,

353
00:21:07,260 --> 00:21:11,440
tatsächlich die endgültige Antwort zu sein, etwa 1 zu 1000, weil sie wirklich unklar sind.

354
00:21:11,440 --> 00:21:15,480
Nun möchte ich Sie fragen: Wie hoch ist die Entropie dieser Verteilung?

355
00:21:15,480 --> 00:21:19,600
Wenn die Entropie hier nur die Anzahl der Übereinstimmungen messen würde,

356
00:21:19,600 --> 00:21:24,760
könnte man erwarten, dass sie etwa der Logarithmusbasis 2 von 16

357
00:21:24,760 --> 00:21:26,200
entspricht, was 4 wäre, zwei Bits mehr Unsicherheit als zuvor.

358
00:21:26,200 --> 00:21:30,320
Aber natürlich unterscheidet sich die tatsächliche Unsicherheit nicht wirklich von der, die wir zuvor hatten.

359
00:21:30,320 --> 00:21:33,840
Nur weil es diese 12 wirklich obskuren Wörter gibt, heißt das nicht, dass es

360
00:21:33,840 --> 00:21:38,200
umso überraschender wäre, zu erfahren, dass die endgültige Antwort zum Beispiel Charme ist.

361
00:21:38,200 --> 00:21:42,080
Wenn Sie also die Berechnung hier tatsächlich durchführen und die Wahrscheinlichkeit jedes

362
00:21:42,080 --> 00:21:45,960
Auftretens mal die entsprechenden Informationen addieren, erhalten Sie 2. 11 Bit.

363
00:21:45,960 --> 00:21:50,280
Ich sage nur, es sind im Grunde genommen zwei Teile, im Grunde genommen diese

364
00:21:50,280 --> 00:21:54,240
vier Möglichkeiten, aber aufgrund all dieser höchst unwahrscheinlichen Ereignisse gibt es etwas mehr Unsicherheit,

365
00:21:54,240 --> 00:21:57,120
obwohl man, wenn man sie erfahren würde, eine Menge Informationen daraus gewinnen würde.

366
00:21:57,120 --> 00:22:00,800
Wenn man also herauszoomt, ist dies ein Teil dessen, was Wordle

367
00:22:00,800 --> 00:22:01,800
zu einem so schönen Beispiel für eine Lektion in Informationstheorie macht.

368
00:22:01,800 --> 00:22:05,280
Wir haben diese beiden unterschiedlichen Gefühlsanwendungen für Entropie.

369
00:22:05,280 --> 00:22:09,640
Der erste sagt uns, welche Informationen wir von einer gegebenen

370
00:22:09,640 --> 00:22:14,560
Vermutung erwarten, und der zweite sagt, können wir die verbleibende

371
00:22:14,560 --> 00:22:16,480
Unsicherheit unter allen Wörtern messen, die uns zur Verfügung stehen.

372
00:22:16,480 --> 00:22:19,800
Und ich sollte betonen: Im ersten Fall, in dem wir die erwarteten Informationen einer Vermutung betrachten,

373
00:22:19,800 --> 00:22:25,000
wirkt sich dies auf die Entropieberechnung aus, sobald wir eine ungleiche Gewichtung der Wörter haben.

374
00:22:25,000 --> 00:22:28,600
Lassen Sie mich zum Beispiel den gleichen Fall der mit

375
00:22:28,600 --> 00:22:33,560
„Weary“ verbundenen Verteilung aufgreifen, den wir zuvor betrachtet haben, diesmal

376
00:22:33,560 --> 00:22:34,560
jedoch unter Verwendung einer ungleichmäßigen Verteilung über alle möglichen Wörter.

377
00:22:34,560 --> 00:22:39,360
Lassen Sie mich sehen, ob ich hier einen Teil finde, der es ziemlich gut veranschaulicht.

378
00:22:39,360 --> 00:22:42,480
Okay, hier ist das ziemlich gut.

379
00:22:42,480 --> 00:22:46,360
Hier haben wir zwei benachbarte Muster, die ungefähr gleich wahrscheinlich sind, aber für eines

380
00:22:46,360 --> 00:22:49,480
davon haben wir erfahren, dass es 32 mögliche Wörter gibt, die dazu passen.

381
00:22:49,480 --> 00:22:54,080
Und wenn wir nachsehen, was sie sind, sind das diese 32, die allesamt

382
00:22:54,080 --> 00:22:55,600
nur sehr unwahrscheinliche Wörter sind, wenn man sie mit den Augen überfliegt.

383
00:22:55,600 --> 00:23:00,400
Es ist schwer, irgendwelche zu finden, die sich wie plausible Antworten anfühlen, vielleicht

384
00:23:00,400 --> 00:23:04,440
Schreie, aber wenn wir uns das benachbarte Muster in der Verteilung ansehen, das

385
00:23:04,440 --> 00:23:08,920
als ungefähr genauso wahrscheinlich gilt, wird uns gesagt, dass es nur 8 mögliche

386
00:23:08,920 --> 00:23:09,920
Übereinstimmungen gibt, also ein Viertel viele Übereinstimmungen, aber es ist ungefähr genauso wahrscheinlich.

387
00:23:09,920 --> 00:23:12,520
Und wenn wir diese Übereinstimmungen abrufen, können wir sehen, warum.

388
00:23:12,520 --> 00:23:17,840
Einige davon sind tatsächlich plausible Antworten, wie „Ring“, „Wrath“ oder „Raps“.

389
00:23:17,840 --> 00:23:22,000
Um zu veranschaulichen, wie wir das alles integrieren, möchte ich hier Version 2 des Wordlebot

390
00:23:22,000 --> 00:23:25,960
aufrufen. Es gibt zwei oder drei Hauptunterschiede zur ersten Version, die wir gesehen haben.

391
00:23:25,960 --> 00:23:29,460
Zunächst einmal verwendet die Art und Weise, wie wir diese Entropien, diese erwarteten

392
00:23:29,460 --> 00:23:34,800
Informationswerte, berechnen, wie ich gerade sagte, jetzt die verfeinerten Verteilungen über die Muster

393
00:23:34,800 --> 00:23:39,300
hinweg, die die Wahrscheinlichkeit berücksichtigen, dass ein bestimmtes Wort tatsächlich die Antwort wäre.

394
00:23:39,300 --> 00:23:44,160
Zufällig sind Tränen immer noch die Nummer 1, obwohl die folgenden etwas anders sind.

395
00:23:44,160 --> 00:23:47,920
Zweitens wird es bei der Rangfolge seiner Top-Tipps nun ein Modell der Wahrscheinlichkeit behalten,

396
00:23:47,920 --> 00:23:52,600
dass jedes Wort die tatsächliche Antwort ist, und es wird dies in seine Entscheidung

397
00:23:52,600 --> 00:23:55,520
einbeziehen, was leichter zu erkennen ist, wenn wir ein paar Vermutungen dazu haben Tisch.

398
00:23:55,520 --> 00:24:01,120
Auch hier ignorieren wir die Empfehlung, weil wir nicht zulassen können, dass Maschinen unser Leben bestimmen.

399
00:24:01,120 --> 00:24:05,160
Und ich denke, ich sollte noch etwas erwähnen, das hier links anders ist: Der Unsicherheitswert,

400
00:24:05,160 --> 00:24:10,080
diese Anzahl von Bits, ist nicht mehr nur redundant mit der Anzahl möglicher Übereinstimmungen.

401
00:24:10,080 --> 00:24:16,520
Wenn wir es jetzt hochziehen und 2 hoch 8 berechnen. 02, was etwas über 256 liegt,

402
00:24:16,520 --> 00:24:22,640
ich schätze 259. Was damit gesagt wird, ist, dass, obwohl es insgesamt 526

403
00:24:22,640 --> 00:24:26,400
Wörter gibt, die diesem Muster tatsächlich entsprechen, das Ausmaß der Unsicherheit eher dem

404
00:24:26,400 --> 00:24:29,760
entspricht, das es wäre, wenn es 259 mit gleicher Wahrscheinlichkeit gäbe Ergebnisse.

405
00:24:29,760 --> 00:24:31,100
Man kann es sich so vorstellen.

406
00:24:31,100 --> 00:24:35,560
Es weiß, dass Borx nicht die Antwort ist, das Gleiche gilt für Yorts,

407
00:24:35,560 --> 00:24:37,840
Zorl und Zorus, daher ist es etwas weniger unsicher als im vorherigen Fall.

408
00:24:37,840 --> 00:24:40,220
Diese Anzahl von Bits wird kleiner sein.

409
00:24:40,220 --> 00:24:44,040
Und wenn ich das Spiel weiter spiele, verfeinere ich dies mit ein

410
00:24:44,040 --> 00:24:48,680
paar Vermutungen, die zu dem passen, was ich hier erklären möchte.

411
00:24:48,680 --> 00:24:52,520
Bei der vierten Vermutung, wenn Sie einen Blick auf die Top-Picks werfen, können

412
00:24:52,520 --> 00:24:53,800
Sie erkennen, dass es nicht mehr nur um die Maximierung der Entropie geht.

413
00:24:53,800 --> 00:24:58,480
An diesem Punkt gibt es also technisch gesehen sieben Möglichkeiten, aber

414
00:24:58,480 --> 00:25:00,780
die einzigen mit einer sinnvollen Chance sind Schlafsäle und Worte.

415
00:25:00,780 --> 00:25:04,760
Und Sie können sehen, dass es wichtiger ist, diese beiden Werte zu

416
00:25:04,760 --> 00:25:07,560
wählen als alle anderen Werte, die streng genommen mehr Informationen liefern würden.

417
00:25:07,560 --> 00:25:11,200
Als ich das zum ersten Mal gemacht habe, habe ich einfach diese beiden Zahlen addiert, um

418
00:25:11,200 --> 00:25:14,580
die Qualität jeder Vermutung zu messen, was tatsächlich besser funktioniert hat, als Sie vielleicht vermuten.

419
00:25:14,580 --> 00:25:17,600
Aber es fühlte sich wirklich nicht systematisch an, und ich bin mir sicher, dass es andere

420
00:25:17,600 --> 00:25:19,880
Ansätze gibt, die die Leute verfolgen könnten, aber hier ist der, bei dem ich gelandet bin.

421
00:25:19,880 --> 00:25:24,200
Wenn wir die Aussicht auf eine nächste Vermutung in Betracht ziehen, wie in diesem Fall Wörter,

422
00:25:24,200 --> 00:25:28,440
ist das, was uns wirklich interessiert, das erwartete Ergebnis unseres Spiels, wenn wir das tun.

423
00:25:28,440 --> 00:25:32,880
Und um diesen erwarteten Wert zu berechnen, sagen wir, wie hoch die Wahrscheinlichkeit

424
00:25:32,880 --> 00:25:35,640
ist, dass Wörter die tatsächliche Antwort sind, was derzeit 58 % entspricht.

425
00:25:36,080 --> 00:25:40,400
Wir gehen davon aus, dass unser Punktestand in diesem Spiel bei einer Chance von 58 % 4 wäre.

426
00:25:40,400 --> 00:25:46,240
Und dann, wenn die Wahrscheinlichkeit 1 minus 58 % beträgt, wird unser Ergebnis mehr als 4 betragen.

427
00:25:46,240 --> 00:25:50,640
Wie viel mehr wissen wir nicht, aber wir können es anhand

428
00:25:50,640 --> 00:25:52,920
der voraussichtlichen Unsicherheit abschätzen, sobald wir diesen Punkt erreicht haben.

429
00:25:52,920 --> 00:25:56,600
Konkret gibt es im Moment 1. 44 Bit Unsicherheit.

430
00:25:56,600 --> 00:26:01,560
Wenn wir Wörter erraten, sagt uns das, dass die erwartete Information, die wir erhalten, 1 ist. 27 Bit.

431
00:26:01,560 --> 00:26:06,280
Wenn wir also Wörter erraten, stellt dieser Unterschied

432
00:26:06,280 --> 00:26:08,280
dar, wie viel Unsicherheit uns danach wahrscheinlich bleibt.

433
00:26:08,280 --> 00:26:12,500
Was wir brauchen, ist eine Art Funktion, die ich hier

434
00:26:12,500 --> 00:26:13,880
f nenne, die diese Unsicherheit mit einem erwarteten Ergebnis verknüpft.

435
00:26:13,880 --> 00:26:18,040
Und die Vorgehensweise bestand darin, einfach eine Reihe von Daten aus früheren Spielen

436
00:26:18,040 --> 00:26:23,920
basierend auf Version 1 des Bots aufzuzeichnen, um zu sagen, wie hoch

437
00:26:23,920 --> 00:26:27,040
der tatsächliche Punktestand nach verschiedenen Punkten war, mit gewissen, sehr messbaren Unsicherheiten.

438
00:26:27,040 --> 00:26:31,120
Zum Beispiel diese Datenpunkte hier, die über einem Wert von etwa 8 liegen. 7

439
00:26:31,120 --> 00:26:36,840
oder so sagen für einige Spiele nach einem Zeitpunkt, an dem es 8 waren. 7 Bits Unsicherheit,

440
00:26:36,840 --> 00:26:39,340
es waren zwei Vermutungen erforderlich, um die endgültige Antwort zu erhalten.

441
00:26:39,340 --> 00:26:43,180
Bei anderen Spielen waren drei Schätzungen erforderlich, bei anderen Spielen waren vier Schätzungen erforderlich.

442
00:26:43,180 --> 00:26:46,920
Wenn wir hier nach links wechseln, sagen alle Punkte über Null, dass immer

443
00:26:46,920 --> 00:26:51,620
dann, wenn es null Unsicherheitsbits gibt, das heißt, dass es nur eine Möglichkeit

444
00:26:51,620 --> 00:26:55,000
gibt, die Anzahl der erforderlichen Schätzungen immer nur eins beträgt, was beruhigend ist.

445
00:26:55,000 --> 00:26:59,020
Wann immer es ein bisschen Unsicherheit gab, was bedeutete, dass es

446
00:26:59,020 --> 00:27:02,360
sich im Wesentlichen nur um zwei Möglichkeiten handelte, war manchmal

447
00:27:02,360 --> 00:27:03,940
eine weitere Vermutung erforderlich, manchmal waren zwei weitere Vermutungen erforderlich.

448
00:27:03,940 --> 00:27:05,980
Und so weiter und so fort hier.

449
00:27:05,980 --> 00:27:11,020
Eine vielleicht etwas einfachere Möglichkeit, diese Daten zu visualisieren, besteht darin, sie zusammenzufassen und Durchschnittswerte zu bilden.

450
00:27:11,020 --> 00:27:15,940
Zum Beispiel dieser Balken hier, der besagt, dass von allen Punkten, bei denen wir eine

451
00:27:15,940 --> 00:27:22,420
gewisse Unsicherheit hatten, die Anzahl der erforderlichen neuen Vermutungen im Durchschnitt etwa 1 betrug. 5.

452
00:27:22,420 --> 00:27:25,920
Und der Balken hier besagt, dass bei all den verschiedenen Spielen,

453
00:27:25,920 --> 00:27:30,480
bei denen die Unsicherheit irgendwann etwas über vier Bit lag, was

454
00:27:30,480 --> 00:27:35,120
einer Eingrenzung auf 16 verschiedene Möglichkeiten entspricht, ab diesem Zeitpunkt im

455
00:27:35,120 --> 00:27:36,240
Durchschnitt etwas mehr als zwei Vermutungen erforderlich sind nach vorne.

456
00:27:36,240 --> 00:27:40,080
Und von hier aus habe ich einfach eine Regression durchgeführt, um eine Funktion anzupassen, die hier sinnvoll erschien.

457
00:27:40,080 --> 00:27:44,160
Und bedenken Sie, dass der Sinn all dessen darin besteht, dass wir die Intuition quantifizieren können,

458
00:27:44,160 --> 00:27:49,720
dass die erwartete Punktzahl umso niedriger sein wird, je mehr Informationen wir aus einem Wort gewinnen.

459
00:27:49,720 --> 00:27:54,380
Also hiermit als Version 2. 0, wenn wir zurückgehen und den gleichen Satz Simulationen durchführen

460
00:27:54,380 --> 00:27:59,820
und ihn gegen alle 2315 möglichen Wortantworten spielen lassen, wie funktioniert das?

461
00:27:59,820 --> 00:28:04,060
Nun, im Gegensatz zu unserer ersten Version ist es definitiv besser, was beruhigend ist.

462
00:28:04,060 --> 00:28:08,780
Alles in allem liegt der Durchschnitt bei etwa 3. 6, obwohl es im Gegensatz zur ersten Version

463
00:28:08,780 --> 00:28:12,820
ein paar Mal Verluste gibt und in diesem Fall mehr als sechs erforderlich sind.

464
00:28:12,820 --> 00:28:15,980
Vermutlich, weil es Zeiten gibt, in denen es darum geht, diesen Kompromiss

465
00:28:15,980 --> 00:28:18,980
einzugehen, um tatsächlich das Ziel zu erreichen, anstatt die Informationen zu maximieren.

466
00:28:18,980 --> 00:28:22,140
Können wir es also besser machen als 3? 6?

467
00:28:22,140 --> 00:28:23,260
Das können wir auf jeden Fall.

468
00:28:23,260 --> 00:28:27,120
Nun habe ich zu Beginn gesagt, dass es am meisten Spaß macht, zu versuchen, nicht die

469
00:28:27,120 --> 00:28:29,980
wahre Liste der Wort-Antworten in die Art und Weise zu integrieren, wie das Modell erstellt wird.

470
00:28:29,980 --> 00:28:35,180
Aber wenn wir es integrieren, lag die beste Leistung, die ich erzielen konnte, bei etwa 3. 43.

471
00:28:35,180 --> 00:28:39,520
Wenn wir also versuchen, bei der Auswahl dieser vorherigen Verteilung, dieser 3, anspruchsvoller zu werden, als

472
00:28:39,520 --> 00:28:44,220
nur Worthäufigkeitsdaten zu verwenden. 43 gibt wahrscheinlich einen Höchstwert dafür, wie gut wir

473
00:28:44,220 --> 00:28:46,360
damit werden könnten, oder zumindest, wie gut ich damit werden könnte.

474
00:28:46,360 --> 00:28:50,240
Diese beste Leistung nutzt im Wesentlichen nur die Ideen, über die ich

475
00:28:50,240 --> 00:28:53,400
hier gesprochen habe, geht aber noch ein wenig weiter, als würde die

476
00:28:53,400 --> 00:28:55,660
Suche nach den erwarteten Informationen zwei Schritte vorwärts statt nur einen durchführen.

477
00:28:55,660 --> 00:28:58,720
Ursprünglich hatte ich vor, mehr darüber zu reden, aber mir

478
00:28:58,720 --> 00:29:00,580
ist klar, dass wir ohnehin schon ziemlich weit gekommen sind.

479
00:29:00,580 --> 00:29:03,520
Das Einzige, was ich sagen möchte, ist, dass nach dieser zweistufigen Suche

480
00:29:03,520 --> 00:29:07,720
und dem anschließenden Ausführen einiger Beispielsimulationen bei den Top-Kandidaten es für

481
00:29:07,720 --> 00:29:09,500
mich zumindest so aussieht, als ob Crane der beste Opener ist.

482
00:29:09,500 --> 00:29:11,080
Wer hätte es gedacht?

483
00:29:11,080 --> 00:29:15,680
Auch wenn Sie die wahre Wortliste verwenden, um Ihren Möglichkeitenraum zu bestimmen,

484
00:29:15,680 --> 00:29:17,920
beträgt die Unsicherheit, mit der Sie beginnen, etwas mehr als 11 Bit.

485
00:29:18,160 --> 00:29:22,760
Und es stellt sich heraus, dass allein bei einer Brute-Force-Suche die maximal

486
00:29:22,760 --> 00:29:26,580
mögliche erwartete Information nach den ersten beiden Schätzungen etwa 10 Bit beträgt.

487
00:29:26,580 --> 00:29:31,720
Das deutet darauf hin, dass Sie im besten Fall nach Ihren ersten

488
00:29:31,720 --> 00:29:35,220
beiden Vermutungen und vollkommen optimalem Spiel mit etwa einem kleinen Unsicherheitsfaktor zurückbleiben.

489
00:29:35,220 --> 00:29:37,400
Das ist das Gleiche, als ob man auf zwei mögliche Vermutungen angewiesen wäre.

490
00:29:37,400 --> 00:29:41,440
Daher denke ich, dass es fair und wahrscheinlich ziemlich konservativ ist, zu sagen, dass Sie niemals

491
00:29:41,440 --> 00:29:45,620
einen Algorithmus schreiben könnten, der diesen Durchschnitt auf 3 reduziert, denn mit den Wörtern, die

492
00:29:45,620 --> 00:29:50,460
Ihnen zur Verfügung stehen, gibt es einfach keinen Platz, um nach nur zwei Schritten genügend Informationen

493
00:29:50,460 --> 00:29:53,820
zu erhalten in der Lage, die Antwort im dritten Slot jedes Mal fehlerfrei zu garantieren.

