1
00:00:00,000 --> 00:00:04,040
Permainan Wurdle telah menjadi sangat viral dalam satu atau dua bulan

2
00:00:04,040 --> 00:00:07,880
terakhir, dan tidak ada seorangpun yang melewatkan kesempatan untuk pelajaran matematika,

3
00:00:07,880 --> 00:00:12,120
menurut saya permainan ini menjadi contoh sentral yang sangat baik dalam

4
00:00:12,120 --> 00:00:13,120
pelajaran tentang teori informasi, dan khususnya topik yang dikenal sebagai entropi.

5
00:00:13,120 --> 00:00:17,120
Anda tahu, seperti kebanyakan orang, saya terjebak dalam teka-teki,

6
00:00:17,120 --> 00:00:21,200
dan seperti kebanyakan programmer, saya juga terjebak dalam upaya

7
00:00:21,200 --> 00:00:23,200
menulis algoritma yang akan memainkan permainan seoptimal mungkin.

8
00:00:23,200 --> 00:00:26,400
Dan apa yang saya pikir akan saya lakukan di sini hanyalah membicarakan

9
00:00:26,400 --> 00:00:29,980
dengan Anda beberapa proses saya di dalamnya, dan menjelaskan beberapa perhitungan matematika

10
00:00:29,980 --> 00:00:32,080
yang masuk ke dalamnya, karena keseluruhan algoritma berpusat pada gagasan entropi ini.

11
00:00:32,080 --> 00:00:42,180
Hal pertama yang pertama, jika Anda belum pernah mendengarnya, apa itu Wurdle?

12
00:00:42,180 --> 00:00:45,380
Dan untuk membunuh dua burung dengan satu batu di sini sementara kita membahas

13
00:00:45,380 --> 00:00:48,980
aturan permainannya, izinkan saya juga melihat ke mana tujuan kita dengan hal ini,

14
00:00:48,980 --> 00:00:51,380
yaitu mengembangkan algoritma kecil yang pada dasarnya akan memainkan permainan tersebut untuk kita.

15
00:00:51,380 --> 00:00:54,860
Meskipun saya belum melakukan Wurdle hari ini, ini tanggal

16
00:00:54,860 --> 00:00:55,860
4 Februari, dan kita akan melihat bagaimana botnya melakukannya.

17
00:00:55,860 --> 00:00:59,580
Tujuan Wurdle adalah menebak kata misteri lima huruf,

18
00:00:59,580 --> 00:01:00,860
dan Anda diberi enam peluang berbeda untuk menebak.

19
00:01:00,860 --> 00:01:05,240
Misalnya, bot Wurdle saya menyarankan agar saya memulai dengan tebakan derek.

20
00:01:05,240 --> 00:01:09,300
Setiap kali Anda menebak, Anda mendapatkan informasi tentang

21
00:01:09,300 --> 00:01:10,940
seberapa dekat tebakan Anda dengan jawaban sebenarnya.

22
00:01:10,940 --> 00:01:14,540
Di sini kotak abu-abu memberi tahu saya bahwa tidak ada C pada jawaban sebenarnya.

23
00:01:14,540 --> 00:01:18,340
Kotak kuning memberitahu saya ada huruf R, tapi posisinya tidak seperti itu.

24
00:01:18,340 --> 00:01:21,820
Kotak hijau memberitahu saya bahwa kata rahasianya memang

25
00:01:21,820 --> 00:01:22,820
memiliki nilai A, dan berada di posisi ketiga.

26
00:01:22,820 --> 00:01:24,300
Dan kemudian tidak ada N dan tidak ada E.

27
00:01:24,300 --> 00:01:27,420
Jadi izinkan saya masuk dan memberi tahu bot Wurdle informasi itu.

28
00:01:27,420 --> 00:01:31,500
Kami mulai dengan crane, kami mendapat warna abu-abu, kuning, hijau, abu-abu, abu-abu.

29
00:01:31,500 --> 00:01:35,460
Jangan khawatir tentang semua data yang ditampilkan saat ini, saya akan menjelaskannya pada waktunya.

30
00:01:35,460 --> 00:01:39,700
Tapi saran utamanya untuk pilihan kedua kami adalah buruk.

31
00:01:39,700 --> 00:01:43,500
Dan tebakan Anda memang harus berupa kata yang terdiri dari lima huruf, tetapi

32
00:01:43,500 --> 00:01:45,700
seperti yang akan Anda lihat, tebakannya cukup liberal karena memungkinkan Anda menebaknya.

33
00:01:45,700 --> 00:01:48,860
Dalam hal ini, kami mencoba shtick.

34
00:01:48,860 --> 00:01:50,260
Dan baiklah, semuanya terlihat cukup bagus.

35
00:01:50,260 --> 00:01:54,580
Kita tekan S dan H, jadi kita tahu tiga huruf pertama, kita tahu ada R.

36
00:01:54,740 --> 00:01:59,740
Jadi itu akan menjadi seperti SHA sesuatu R, atau SHA R sesuatu.

37
00:01:59,740 --> 00:02:03,200
Dan sepertinya bot Wurdle mengetahui bahwa hanya

38
00:02:03,200 --> 00:02:05,220
ada dua kemungkinan, yaitu pecahan atau tajam.

39
00:02:05,220 --> 00:02:08,620
Saat ini ada semacam perselisihan di antara mereka, jadi saya kira

40
00:02:08,620 --> 00:02:11,260
mungkin hanya karena sesuai abjad, maka itu sesuai dengan pecahan.

41
00:02:11,260 --> 00:02:13,000
Yang mana hore, itulah jawaban sebenarnya.

42
00:02:13,000 --> 00:02:14,660
Jadi kami mendapatkannya dalam tiga.

43
00:02:14,660 --> 00:02:17,740
Jika Anda bertanya-tanya apakah itu bagus, saya mendengar satu orang berkata

44
00:02:17,740 --> 00:02:20,820
bahwa dengan Wurdle empat adalah par dan tiga adalah birdie.

45
00:02:20,820 --> 00:02:22,960
Yang menurut saya merupakan analogi yang cukup tepat.

46
00:02:22,960 --> 00:02:27,560
Anda harus konsisten dalam permainan Anda untuk mendapatkan empat, tapi itu jelas tidak gila.

47
00:02:27,560 --> 00:02:30,000
Tapi ketika Anda mendapatkannya dalam tiga, rasanya luar biasa.

48
00:02:30,000 --> 00:02:33,800
Jadi jika Anda menginginkannya, yang ingin saya lakukan di sini hanyalah membahas

49
00:02:33,800 --> 00:02:36,600
proses pemikiran saya dari awal tentang cara saya mendekati bot Wurdle.

50
00:02:36,600 --> 00:02:39,800
Dan seperti yang saya katakan, ini sebenarnya alasan untuk pelajaran teori informasi.

51
00:02:39,800 --> 00:02:43,160
Tujuan utamanya adalah menjelaskan apa itu informasi dan apa itu entropi.

52
00:02:48,560 --> 00:02:52,080
Pikiran pertama saya dalam melakukan pendekatan ini adalah

53
00:02:52,080 --> 00:02:53,560
melihat frekuensi relatif berbagai huruf dalam bahasa Inggris.

54
00:02:53,560 --> 00:02:57,800
Jadi saya berpikir, oke, apakah ada tebakan pembuka atau tebakan

55
00:02:57,800 --> 00:02:59,960
pembuka yang banyak mengenai huruf yang paling sering ini?

56
00:02:59,960 --> 00:03:03,780
Dan salah satu hal yang sangat saya sukai adalah melakukan hal lain yang diikuti dengan paku.

57
00:03:03,780 --> 00:03:06,980
Pemikirannya adalah jika Anda menekan sebuah huruf, Anda tahu, Anda

58
00:03:06,980 --> 00:03:07,980
mendapatkan warna hijau atau kuning, itu selalu terasa menyenangkan.

59
00:03:07,980 --> 00:03:09,460
Rasanya seperti Anda mendapatkan informasi.

60
00:03:09,460 --> 00:03:13,140
Namun dalam kasus ini, bahkan jika Anda tidak memukul dan Anda

61
00:03:13,140 --> 00:03:16,640
selalu mendapatkan warna abu-abu, itu tetap memberi Anda banyak informasi

62
00:03:16,640 --> 00:03:17,640
karena sangat jarang menemukan kata yang tidak memiliki huruf-huruf ini.

63
00:03:17,640 --> 00:03:21,840
Namun tetap saja, hal tersebut tidak terasa super

64
00:03:21,840 --> 00:03:23,520
sistematis, karena misalnya, tidak memperhitungkan urutan huruf.

65
00:03:23,520 --> 00:03:26,080
Mengapa mengetik paku ketika saya bisa mengetik siput?

66
00:03:26,080 --> 00:03:27,720
Apakah lebih baik memiliki S di akhir?

67
00:03:27,720 --> 00:03:28,720
Saya tidak begitu yakin.

68
00:03:28,720 --> 00:03:33,500
Sekarang, seorang teman saya mengatakan bahwa dia suka membuka dengan kata letih, yang membuat saya

69
00:03:33,500 --> 00:03:37,160
terkejut karena ada beberapa huruf yang tidak biasa di sana seperti W dan Y.

70
00:03:37,160 --> 00:03:39,400
Tapi siapa tahu, mungkin itu pembuka yang lebih baik.

71
00:03:39,400 --> 00:03:43,920
Adakah skor kuantitatif yang bisa kita

72
00:03:43,920 --> 00:03:44,920
berikan untuk menilai kualitas tebakan potensial?

73
00:03:44,920 --> 00:03:48,640
Sekarang untuk mempersiapkan cara kita menentukan peringkat kemungkinan tebakan, mari kita

74
00:03:48,640 --> 00:03:51,800
kembali dan menambahkan sedikit kejelasan tentang bagaimana tepatnya permainan ini diatur.

75
00:03:51,800 --> 00:03:55,880
Jadi ada daftar kata yang dapat Anda masukkan yang dianggap

76
00:03:55,880 --> 00:03:57,920
sebagai tebakan valid yang panjangnya hanya sekitar 13.000 kata.

77
00:03:57,920 --> 00:04:01,560
Tapi kalau dilihat-lihat, ada banyak hal yang sangat tidak biasa, seperti kepala

78
00:04:01,560 --> 00:04:07,040
atau Ali dan ARG, kata-kata yang menimbulkan pertengkaran keluarga dalam permainan Scrabble.

79
00:04:07,040 --> 00:04:10,600
Namun inti dari permainan ini adalah bahwa jawabannya akan selalu berupa kata yang umum.

80
00:04:10,600 --> 00:04:16,080
Dan faktanya, ada daftar lain yang berisi sekitar 2.300 kata yang merupakan kemungkinan jawabannya.

81
00:04:16,080 --> 00:04:20,320
Dan ini adalah daftar yang dikurasi oleh manusia, menurut saya

82
00:04:20,320 --> 00:04:21,800
khususnya oleh pacar pembuat game, dan itu cukup menyenangkan.

83
00:04:21,800 --> 00:04:25,560
Tapi yang ingin saya lakukan, tantangan kita untuk proyek ini adalah melihat apakah kita

84
00:04:25,560 --> 00:04:30,720
bisa menulis program penyelesaian Wordle yang tidak memasukkan pengetahuan sebelumnya tentang daftar ini.

85
00:04:30,720 --> 00:04:34,560
Untuk satu hal, ada banyak kata lima huruf yang cukup

86
00:04:34,560 --> 00:04:35,560
umum yang tidak akan Anda temukan dalam daftar itu.

87
00:04:35,560 --> 00:04:38,360
Jadi akan lebih baik untuk menulis sebuah program yang sedikit lebih tangguh dan dapat

88
00:04:38,360 --> 00:04:41,960
memainkan Wordle melawan siapa pun, bukan hanya apa yang terjadi di situs resminya.

89
00:04:41,960 --> 00:04:45,900
Dan juga alasan kita mengetahui daftar kemungkinan jawaban ini,

90
00:04:45,900 --> 00:04:47,440
adalah karena daftar tersebut terlihat di kode sumber.

91
00:04:47,440 --> 00:04:51,620
Namun tampilannya di kode sumber sesuai dengan urutan

92
00:04:51,620 --> 00:04:52,840
jawaban yang muncul dari hari ke hari.

93
00:04:52,840 --> 00:04:56,400
Jadi, Anda selalu bisa mencari tahu apa jawaban besok.

94
00:04:56,400 --> 00:04:59,140
Jelas sekali, ada kesan bahwa menggunakan daftar itu curang.

95
00:04:59,140 --> 00:05:02,900
Dan apa yang menjadikan teka-teki ini lebih menarik dan pelajaran teori informasi yang

96
00:05:02,900 --> 00:05:07,640
lebih kaya adalah dengan menggunakan beberapa data yang lebih universal seperti frekuensi kata

97
00:05:07,640 --> 00:05:11,640
relatif secara umum untuk menangkap intuisi mengenai preferensi terhadap kata-kata yang lebih umum.

98
00:05:11,640 --> 00:05:16,560
Jadi dari 13.000 kemungkinan ini, bagaimana sebaiknya kita memilih tebakan pembuka?

99
00:05:16,560 --> 00:05:19,960
Misalnya, jika teman saya melamar dengan letih, bagaimana kita menganalisis kualitasnya?

100
00:05:19,960 --> 00:05:25,040
Nah, alasan dia mengatakan dia menyukai W yang tidak mungkin itu adalah karena dia menyukai

101
00:05:25,040 --> 00:05:27,880
sifat pukulan jarak jauh tentang betapa nikmatnya rasanya jika Anda berhasil mencapai W itu.

102
00:05:27,880 --> 00:05:31,400
Misalnya, jika pola pertama yang terungkap kira-kira seperti ini, maka ternyata hanya

103
00:05:31,400 --> 00:05:36,080
ada 58 kata dalam leksikon raksasa ini yang cocok dengan pola tersebut.

104
00:05:36,080 --> 00:05:38,900
Jadi itu pengurangan yang sangat besar dari 13.000.

105
00:05:38,900 --> 00:05:43,320
Namun sisi sebaliknya tentu saja sangat jarang mendapatkan pola seperti ini.

106
00:05:43,360 --> 00:05:47,600
Secara khusus, jika setiap kata mempunyai kemungkinan yang sama untuk menjadi

107
00:05:47,600 --> 00:05:51,680
jawabannya, kemungkinan untuk mendapatkan pola ini adalah 58 dibagi sekitar 13.000.

108
00:05:51,680 --> 00:05:53,880
Tentu saja, kemungkinan jawaban tersebut tidak sama.

109
00:05:53,880 --> 00:05:56,680
Kebanyakan dari kata-kata ini sangat tidak jelas dan bahkan dipertanyakan.

110
00:05:56,680 --> 00:05:59,560
Tapi setidaknya untuk langkah pertama kita dalam semua hal ini, mari kita

111
00:05:59,560 --> 00:06:02,040
asumsikan bahwa semuanya memiliki kemungkinan yang sama dan kemudian menyempurnakannya nanti.

112
00:06:02,040 --> 00:06:07,360
Intinya adalah pola dengan banyak informasi pada dasarnya tidak mungkin terjadi.

113
00:06:07,360 --> 00:06:11,320
Faktanya, yang dimaksud dengan informatif adalah bahwa hal itu tidak mungkin.

114
00:06:11,920 --> 00:06:16,720
Pola yang lebih mungkin terlihat pada pembukaan ini adalah seperti

115
00:06:16,720 --> 00:06:18,360
ini, yang tentu saja tidak ada huruf W di dalamnya.

116
00:06:18,360 --> 00:06:22,080
Mungkin ada E, dan mungkin tidak ada A, tidak ada R, tidak ada Y.

117
00:06:22,080 --> 00:06:24,640
Dalam hal ini, ada 1400 kemungkinan kecocokan.

118
00:06:24,640 --> 00:06:29,600
Jika semua memiliki kemungkinan yang sama, maka kemungkinannya sekitar

119
00:06:29,600 --> 00:06:30,680
11% bahwa ini adalah pola yang akan Anda lihat.

120
00:06:30,680 --> 00:06:34,320
Jadi hasil yang paling mungkin juga paling tidak informatif.

121
00:06:34,320 --> 00:06:38,440
Untuk mendapatkan gambaran yang lebih global di sini, izinkan saya menunjukkan kepada

122
00:06:38,440 --> 00:06:42,000
Anda distribusi penuh probabilitas di semua pola berbeda yang mungkin Anda lihat.

123
00:06:42,000 --> 00:06:46,000
Jadi tiap batang yang Anda lihat sesuai dengan kemungkinan pola warna yang

124
00:06:46,000 --> 00:06:50,500
dapat ditampilkan, yang mana terdapat 3 hingga 5 kemungkinan, dan disusun dari

125
00:06:50,500 --> 00:06:52,960
kiri ke kanan, yang paling umum hingga yang paling tidak umum.

126
00:06:52,960 --> 00:06:56,200
Jadi kemungkinan paling umum di sini adalah Anda mendapatkan warna abu-abu.

127
00:06:56,200 --> 00:06:58,800
Itu terjadi sekitar 14% dari seluruh kasus.

128
00:06:58,800 --> 00:07:02,040
Dan yang Anda harapkan saat menebak adalah Anda berakhir di suatu tempat

129
00:07:02,040 --> 00:07:06,360
di ekor panjang ini, seperti di sini di mana hanya ada

130
00:07:06,360 --> 00:07:09,920
18 kemungkinan yang cocok dengan pola yang ternyata terlihat seperti ini.

131
00:07:09,920 --> 00:07:14,080
Atau jika kita melangkah lebih jauh ke kiri, mungkin kita akan terus ke sini.

132
00:07:14,080 --> 00:07:16,560
Oke, ini teka-teki yang bagus untuk Anda.

133
00:07:16,560 --> 00:07:20,600
Apa tiga kata dalam bahasa Inggris yang dimulai dengan huruf W, diakhiri

134
00:07:20,600 --> 00:07:22,040
dengan huruf Y, dan memiliki huruf R di suatu tempat di dalamnya?

135
00:07:22,040 --> 00:07:27,560
Ternyata jawabannya adalah, mari kita lihat, bertele-tele, cacingan, dan masam.

136
00:07:27,560 --> 00:07:32,720
Jadi untuk menilai seberapa bagus kata ini secara keseluruhan, kami ingin

137
00:07:32,720 --> 00:07:35,720
mengukur perkiraan jumlah informasi yang akan Anda peroleh dari distribusi ini.

138
00:07:36,360 --> 00:07:41,080
Jika kita menelusuri setiap pola dan mengalikan kemungkinan terjadinya dengan sesuatu yang mengukur

139
00:07:41,080 --> 00:07:46,000
seberapa informatif pola tersebut, hal itu mungkin dapat memberi kita skor objektif.

140
00:07:46,000 --> 00:07:50,280
Sekarang insting pertama Anda tentang apa yang seharusnya terjadi mungkin adalah jumlah pertandingan.

141
00:07:50,280 --> 00:07:52,960
Anda menginginkan jumlah rata-rata kecocokan yang lebih rendah.

142
00:07:52,960 --> 00:07:57,400
Namun saya ingin menggunakan pengukuran yang lebih universal yang sering kita anggap berasal dari

143
00:07:57,400 --> 00:08:01,040
informasi, dan pengukuran yang akan lebih fleksibel setelah kita menetapkan probabilitas berbeda untuk masing-masing

144
00:08:01,040 --> 00:08:04,320
dari 13.000 kata tersebut untuk mengetahui apakah kata-kata tersebut benar-benar jawabannya atau tidak.

145
00:08:10,600 --> 00:08:14,760
Satuan standar informasi adalah bit, yang memiliki rumus yang sedikit

146
00:08:14,760 --> 00:08:17,800
lucu, namun sangat intuitif jika kita hanya melihat contohnya.

147
00:08:17,800 --> 00:08:21,880
Jika Anda mempunyai pengamatan yang membagi dua kemungkinan yang

148
00:08:21,880 --> 00:08:24,200
ada, kami katakan bahwa pengamatan tersebut mempunyai sedikit informasi.

149
00:08:24,200 --> 00:08:27,680
Dalam contoh kita, ruang kemungkinannya adalah semua kemungkinan kata, dan ternyata sekitar setengah dari

150
00:08:27,760 --> 00:08:31,560
lima huruf kata mempunyai huruf S, sedikit lebih kecil dari itu, tapi sekitar setengahnya.

151
00:08:31,560 --> 00:08:35,200
Sehingga pengamatan itu akan memberi Anda sedikit informasi.

152
00:08:35,200 --> 00:08:39,640
Jika suatu fakta baru memperkecil ruang kemungkinan tersebut sebanyak empat

153
00:08:39,640 --> 00:08:42,000
kali lipat, kita katakan bahwa fakta tersebut mempunyai dua informasi.

154
00:08:42,000 --> 00:08:45,120
Misalnya, ternyata sekitar seperempat dari kata-kata ini memiliki huruf T.

155
00:08:45,120 --> 00:08:49,720
Jika observasi memotong ruang tersebut sebanyak delapan kali lipat, kita

156
00:08:49,720 --> 00:08:50,920
katakan itu adalah tiga bit informasi, dan seterusnya dan seterusnya.

157
00:08:50,920 --> 00:08:55,000
Empat bit memotongnya menjadi 16, lima bit memotongnya menjadi 32.

158
00:08:55,000 --> 00:09:00,160
Jadi sekarang Anda mungkin ingin berhenti sejenak dan bertanya pada diri sendiri,

159
00:09:00,160 --> 00:09:04,520
apa rumus informasi jumlah bit dalam kaitannya dengan probabilitas suatu kejadian?

160
00:09:04,520 --> 00:09:07,920
Apa yang ingin kami katakan di sini adalah ketika Anda mengambil setengah dari

161
00:09:07,920 --> 00:09:11,680
jumlah bit, itu sama dengan probabilitas, yang sama dengan mengatakan dua pangkat dari

162
00:09:11,680 --> 00:09:16,200
jumlah bit adalah satu di atas probabilitas, yaitu menyusun ulang lebih jauh dengan

163
00:09:16,200 --> 00:09:19,680
mengatakan bahwa informasi tersebut adalah basis log dua dari satu dibagi dengan probabilitas.

164
00:09:19,680 --> 00:09:23,200
Dan kadang-kadang Anda melihat ini dengan satu penataan ulang lagi,

165
00:09:23,200 --> 00:09:25,680
di mana informasinya adalah log negatif basis dua dari probabilitas.

166
00:09:25,680 --> 00:09:29,120
Jika diungkapkan seperti ini, hal ini mungkin terlihat sedikit aneh

167
00:09:29,120 --> 00:09:33,400
bagi yang belum tahu, namun sebenarnya ini hanyalah gagasan intuitif

168
00:09:33,400 --> 00:09:35,120
untuk menanyakan berapa kali Anda telah mengurangi separuh kemungkinan Anda.

169
00:09:35,120 --> 00:09:37,840
Sekarang jika Anda bertanya-tanya, Anda tahu, saya pikir kita hanya memainkan

170
00:09:37,840 --> 00:09:39,920
permainan kata yang menyenangkan, mengapa logaritma masuk ke dalam gambar?

171
00:09:39,920 --> 00:09:43,920
Salah satu alasan unit ini lebih bagus adalah karena lebih mudah untuk membicarakan peristiwa

172
00:09:43,920 --> 00:09:48,120
yang sangat tidak mungkin terjadi, lebih mudah untuk mengatakan bahwa suatu pengamatan mempunyai 20

173
00:09:48,120 --> 00:09:53,480
bit informasi daripada mengatakan bahwa probabilitas kejadian ini dan itu adalah 0. 0000095.

174
00:09:53,480 --> 00:09:57,360
Namun alasan yang lebih substantif mengapa ekspresi logaritmik ini ternyata menjadi tambahan

175
00:09:57,360 --> 00:10:02,000
yang sangat berguna bagi teori probabilitas adalah cara informasi tersebut dijumlahkan.

176
00:10:02,000 --> 00:10:05,560
Misalnya, jika satu observasi memberi Anda dua bit informasi, mengurangi ruang Anda

177
00:10:05,560 --> 00:10:10,120
menjadi empat, dan kemudian observasi kedua seperti tebakan kedua Anda di

178
00:10:10,120 --> 00:10:14,480
Wordle memberi Anda tiga bit informasi lagi, mengurangi Anda lebih jauh lagi

179
00:10:14,480 --> 00:10:17,360
sebanyak delapan kali lipat, maka dua bersama-sama memberi Anda lima informasi.

180
00:10:17,360 --> 00:10:21,200
Sama seperti probabilitas yang berlipat ganda, informasi juga suka bertambah.

181
00:10:21,200 --> 00:10:24,920
Jadi segera setelah kita berada di bidang nilai yang diharapkan, di mana

182
00:10:24,920 --> 00:10:28,660
kita menambahkan banyak angka, log akan membuatnya lebih mudah untuk ditangani.

183
00:10:28,660 --> 00:10:32,600
Mari kita kembali ke distribusi Weary dan menambahkan pelacak kecil lainnya di

184
00:10:32,600 --> 00:10:35,560
sini, menunjukkan kepada kita berapa banyak informasi yang ada untuk setiap pola.

185
00:10:35,560 --> 00:10:38,760
Hal utama yang saya ingin Anda perhatikan adalah semakin tinggi kemungkinan kita mendapatkan

186
00:10:38,760 --> 00:10:43,500
pola yang lebih mungkin, semakin rendah informasinya, semakin sedikit bit yang Anda peroleh.

187
00:10:43,500 --> 00:10:47,360
Cara kita mengukur kualitas tebakan ini adalah dengan mengambil nilai yang diharapkan

188
00:10:47,360 --> 00:10:51,620
dari informasi tersebut, dengan menelusuri setiap pola, kita tentukan seberapa besar

189
00:10:51,620 --> 00:10:54,940
kemungkinannya, lalu kita kalikan dengan berapa banyak informasi yang kita peroleh.

190
00:10:54,940 --> 00:10:58,480
Dan dalam contoh Weary, ternyata menjadi 4. 9 bit.

191
00:10:58,480 --> 00:11:02,800
Jadi rata-rata, informasi yang Anda dapatkan dari tebakan pembuka ini sama baiknya

192
00:11:02,800 --> 00:11:05,660
dengan memotong ruang kemungkinan Anda menjadi setengahnya sekitar lima kali lipat.

193
00:11:05,660 --> 00:11:10,260
Sebaliknya, contoh tebakan dengan nilai informasi yang

194
00:11:10,260 --> 00:11:13,220
diharapkan lebih tinggi adalah seperti Slate.

195
00:11:13,220 --> 00:11:16,180
Dalam hal ini Anda akan melihat distribusinya terlihat lebih datar.

196
00:11:16,180 --> 00:11:20,780
Secara khusus, kemunculan semua warna abu-abu yang paling mungkin hanya memiliki peluang sekitar

197
00:11:20,780 --> 00:11:25,940
6% untuk muncul, jadi setidaknya Anda mendapatkan 3. 9 bit informasi.

198
00:11:25,940 --> 00:11:29,140
Tapi itu jumlah minimum, biasanya Anda akan mendapatkan sesuatu yang lebih baik dari itu.

199
00:11:29,140 --> 00:11:33,380
Dan ternyata ketika Anda menghitung angka-angka ini dan menjumlahkan semua

200
00:11:33,380 --> 00:11:36,420
istilah yang relevan, rata-rata informasinya adalah sekitar 5. 8.

201
00:11:36,420 --> 00:11:42,140
Jadi berbeda dengan Weary, rata-rata ruang kemungkinan Anda

202
00:11:42,140 --> 00:11:43,940
akan menjadi sekitar setengahnya setelah tebakan pertama ini.

203
00:11:43,940 --> 00:11:49,540
Sebenarnya ada cerita menarik tentang nama nilai kuantitas informasi yang diharapkan ini.

204
00:11:49,540 --> 00:11:52,580
Teori informasi dikembangkan oleh Claude Shannon, yang bekerja di Bell Labs pada

205
00:11:52,580 --> 00:11:57,620
tahun 1940-an, tetapi dia membicarakan beberapa idenya yang belum dipublikasikan dengan John

206
00:11:57,620 --> 00:12:01,500
von Neumann, yang merupakan raksasa intelektual pada saat itu, sangat terkemuka. dalam

207
00:12:01,500 --> 00:12:04,180
matematika dan fisika dan permulaan dari apa yang kemudian menjadi ilmu komputer.

208
00:12:04,180 --> 00:12:07,260
Dan ketika dia menyebutkan bahwa dia tidak benar-benar memiliki nama yang

209
00:12:07,260 --> 00:12:12,540
bagus untuk nilai yang diharapkan dari kuantitas informasi ini, von Neumann

210
00:12:12,540 --> 00:12:14,720
berkata, jadi ceritanya, Anda harus menyebutnya entropi, dan karena dua alasan.

211
00:12:14,720 --> 00:12:18,400
Pertama, fungsi ketidakpastian Anda telah digunakan dalam mekanika statistik dengan nama tersebut, sehingga

212
00:12:18,400 --> 00:12:23,100
sudah memiliki nama, dan kedua, dan yang lebih penting, tidak ada yang

213
00:12:23,100 --> 00:12:26,940
tahu apa sebenarnya entropi, jadi dalam perdebatan Anda akan selalu memiliki keuntungan.

214
00:12:26,940 --> 00:12:31,420
Jadi jika namanya tampak sedikit misterius, dan jika

215
00:12:31,420 --> 00:12:33,420
cerita ini dapat dipercaya, itu memang disengaja.

216
00:12:33,420 --> 00:12:36,740
Juga jika Anda bertanya-tanya tentang hubungannya dengan semua hukum kedua

217
00:12:36,740 --> 00:12:40,820
termodinamika dari fisika, pasti ada hubungannya, tetapi pada awalnya Shannon

218
00:12:40,820 --> 00:12:44,780
hanya berurusan dengan teori probabilitas murni, dan untuk tujuan kita

219
00:12:44,780 --> 00:12:49,340
di sini, ketika saya menggunakan teori kata entropi, saya hanya

220
00:12:49,340 --> 00:12:50,820
ingin Anda memikirkan nilai informasi yang diharapkan dari tebakan tertentu.

221
00:12:50,820 --> 00:12:54,380
Anda dapat menganggap entropi sebagai mengukur dua hal secara bersamaan.

222
00:12:54,380 --> 00:12:57,420
Yang pertama adalah seberapa datar distribusinya.

223
00:12:57,420 --> 00:13:01,700
Semakin dekat suatu distribusi ke seragam, semakin tinggi entropinya.

224
00:13:01,700 --> 00:13:06,340
Dalam kasus kita, jika terdapat 3 hingga 5 pola total, untuk distribusi seragam, mengamati salah satu

225
00:13:06,340 --> 00:13:11,340
dari pola tersebut akan memiliki basis log informasi 2 dari 3 hingga 5, yang kebetulan berjumlah

226
00:13:11,340 --> 00:13:17,860
7. 92, jadi itulah nilai maksimum mutlak yang mungkin Anda miliki untuk entropi ini.

227
00:13:17,860 --> 00:13:21,900
Namun entropi juga merupakan ukuran

228
00:13:21,900 --> 00:13:22,900
seberapa banyak kemungkinan yang ada.

229
00:13:22,900 --> 00:13:26,980
Misalnya, jika Anda memiliki suatu kata yang hanya memiliki 16 kemungkinan pola, dan setiap

230
00:13:26,980 --> 00:13:32,760
pola memiliki kemungkinan yang sama, entropi ini, informasi yang diharapkan, akan berjumlah 4 bit.

231
00:13:32,760 --> 00:13:36,880
Namun jika Anda memiliki kata lain yang memiliki 64 kemungkinan pola yang muncul,

232
00:13:36,880 --> 00:13:41,000
dan semuanya memiliki kemungkinan yang sama, maka entropinya akan menjadi 6 bit.

233
00:13:41,000 --> 00:13:45,800
Jadi, jika Anda melihat suatu distribusi di alam liar yang memiliki entropi 6

234
00:13:45,800 --> 00:13:50,000
bit, hal ini seperti menyatakan bahwa ada banyak variasi dan ketidakpastian dalam

235
00:13:50,000 --> 00:13:54,400
apa yang akan terjadi, seolah-olah ada 64 kemungkinan hasil yang sama besarnya.

236
00:13:54,400 --> 00:13:58,360
Untuk pass pertama saya di Wurtelebot, pada dasarnya saya menyuruhnya melakukan ini saja.

237
00:13:58,360 --> 00:14:03,560
Ia menelusuri semua kemungkinan tebakan yang Anda miliki, seluruh 13.000 kata, menghitung entropi

238
00:14:03,560 --> 00:14:08,580
untuk masing-masing kata, atau lebih khusus lagi, entropi distribusi di semua pola

239
00:14:08,580 --> 00:14:13,040
yang mungkin Anda lihat, untuk masing-masing kata, dan memilih yang tertinggi, karena

240
00:14:13,040 --> 00:14:17,200
itulah salah satu yang kemungkinan akan mengurangi ruang kemungkinan Anda sebanyak mungkin.

241
00:14:17,200 --> 00:14:20,120
Dan meskipun saya hanya membicarakan tebakan pertama di sini,

242
00:14:20,120 --> 00:14:21,680
hal yang sama berlaku untuk beberapa tebakan berikutnya.

243
00:14:21,680 --> 00:14:25,100
Misalnya, setelah Anda melihat beberapa pola pada tebakan pertama tersebut, yang akan

244
00:14:25,100 --> 00:14:29,300
membatasi Anda pada kemungkinan kata yang lebih sedikit berdasarkan kecocokannya, Anda cukup

245
00:14:29,300 --> 00:14:32,300
memainkan permainan yang sama terhadap kumpulan kata yang lebih kecil tersebut.

246
00:14:32,300 --> 00:14:36,500
Untuk usulan tebakan kedua, Anda melihat distribusi semua pola yang dapat

247
00:14:36,500 --> 00:14:41,540
terjadi dari kumpulan kata yang lebih terbatas tersebut, Anda menelusuri 13.000

248
00:14:41,540 --> 00:14:45,480
kemungkinan, dan Anda menemukan salah satu yang memaksimalkan entropi tersebut.

249
00:14:45,480 --> 00:14:48,980
Untuk menunjukkan kepada Anda cara kerjanya, izinkan saya menampilkan sedikit varian Wurtele yang

250
00:14:48,980 --> 00:14:54,060
saya tulis yang menunjukkan hal-hal penting dari analisis ini di bagian pinggirnya.

251
00:14:54,460 --> 00:14:57,820
Setelah melakukan semua perhitungan entropinya, di sini ia menunjukkan

252
00:14:57,820 --> 00:15:00,340
kepada kita mana yang memiliki informasi yang diharapkan tertinggi.

253
00:15:00,340 --> 00:15:04,940
Ternyata jawaban teratas, setidaknya untuk saat ini, akan kita perbaiki nanti, adalah

254
00:15:04,940 --> 00:15:11,140
Taras, yang artinya, um, tentu saja, vetch, vetch yang paling umum.

255
00:15:11,140 --> 00:15:14,180
Tiap kali kita membuat tebakan di sini, mungkin saya mengabaikan rekomendasinya dan memilih

256
00:15:14,180 --> 00:15:19,220
slate, karena saya suka slate, kita bisa melihat berapa banyak informasi yang diharapkan

257
00:15:19,220 --> 00:15:23,300
darinya, tapi kemudian di sebelah kanan kata ini, ia menunjukkan kepada kita seberapa

258
00:15:23,340 --> 00:15:24,980
banyak informasi yang diharapkan. informasi aktual yang kami peroleh, mengingat pola khusus ini.

259
00:15:24,980 --> 00:15:28,660
Jadi di sini sepertinya kami sedikit kurang beruntung, kami diharapkan mendapat nilai 5. 8, tapi

260
00:15:28,660 --> 00:15:30,660
kebetulan kami mendapatkan sesuatu yang kurang dari itu.

261
00:15:30,660 --> 00:15:34,020
Dan kemudian di sisi kiri ini menunjukkan kepada kita

262
00:15:34,020 --> 00:15:35,860
semua kemungkinan kata yang berbeda berdasarkan posisi kita sekarang.

263
00:15:35,860 --> 00:15:39,820
Bilah biru memberi tahu kita seberapa besar kemungkinannya untuk memikirkan setiap kata, sehingga saat ini diasumsikan

264
00:15:39,820 --> 00:15:44,140
bahwa setiap kata memiliki kemungkinan yang sama untuk muncul, namun kami akan menyempurnakannya sebentar lagi.

265
00:15:44,140 --> 00:15:48,580
Dan kemudian pengukuran ketidakpastian ini memberi tahu kita entropi dari distribusi

266
00:15:48,580 --> 00:15:53,220
ini di seluruh kemungkinan kata, yang saat ini, karena distribusinya seragam,

267
00:15:53,300 --> 00:15:55,940
hanyalah cara rumit yang tidak perlu untuk menghitung jumlah kemungkinan.

268
00:15:55,940 --> 00:16:01,700
Misalnya, jika kita ingin 2 dipangkatkan 13. 66, itu

269
00:16:01,700 --> 00:16:02,700
seharusnya sekitar 13.000 kemungkinan.

270
00:16:02,700 --> 00:16:06,780
Saya sedikit melenceng di sini, tetapi hanya karena saya tidak menampilkan semua tempat desimal.

271
00:16:06,780 --> 00:16:10,260
Saat ini hal itu mungkin terasa berlebihan dan sepertinya terlalu rumit, tetapi

272
00:16:10,260 --> 00:16:12,780
Anda akan mengerti mengapa ada gunanya memiliki kedua angka dalam satu menit.

273
00:16:12,780 --> 00:16:16,780
Jadi di sini sepertinya entropi tertinggi untuk tebakan kedua kita

274
00:16:16,780 --> 00:16:19,700
adalah Ramen, yang sekali lagi tidak terasa seperti sebuah kata.

275
00:16:19,700 --> 00:16:25,660
Jadi untuk mengambil landasan moral yang tinggi di sini, saya akan melanjutkan dan mengetik Rains.

276
00:16:25,660 --> 00:16:27,540
Dan sekali lagi sepertinya kami sedikit kurang beruntung.

277
00:16:27,540 --> 00:16:32,100
Kami mengharapkan 4. 3 bit dan kami hanya mendapat 3. 39 bit informasi.

278
00:16:32,100 --> 00:16:35,060
Jadi itu membawa kita ke 55 kemungkinan.

279
00:16:35,060 --> 00:16:38,860
Dan di sini mungkin saya akan mengikuti apa

280
00:16:38,860 --> 00:16:40,200
yang disarankannya, yaitu kombo, apa pun artinya.

281
00:16:40,200 --> 00:16:43,300
Dan oke, ini sebenarnya kesempatan bagus untuk membuat teka-teki.

282
00:16:43,300 --> 00:16:47,020
Ini memberi tahu kita bahwa pola ini memberi kita 4. 7 bit informasi.

283
00:16:47,020 --> 00:16:52,400
Tapi di sebelah kiri, sebelum kita melihat pola itu, ada 5. 78 bit ketidakpastian.

284
00:16:52,400 --> 00:16:56,860
Jadi sebagai kuis untuk Anda, apa maksudnya dengan jumlah kemungkinan yang tersisa?

285
00:16:56,860 --> 00:17:02,280
Artinya, kita hanya dihadapkan pada sedikit ketidakpastian, yang sama

286
00:17:02,280 --> 00:17:04,700
saja dengan mengatakan bahwa ada dua kemungkinan jawaban.

287
00:17:04,700 --> 00:17:06,520
Itu adalah pilihan 50-50.

288
00:17:06,520 --> 00:17:09,860
Dan dari sini, karena Anda dan saya tahu kata mana

289
00:17:09,860 --> 00:17:11,220
yang lebih umum, kita tahu bahwa jawabannya pasti sangat buruk.

290
00:17:11,220 --> 00:17:13,540
Namun seperti yang tertulis sekarang, program tersebut tidak mengetahui hal itu.

291
00:17:13,540 --> 00:17:17,560
Jadi ia terus berjalan, berusaha mendapatkan informasi sebanyak-banyaknya,

292
00:17:17,560 --> 00:17:20,360
hingga hanya tersisa satu kemungkinan, lalu ia menebaknya.

293
00:17:20,360 --> 00:17:22,700
Jadi jelas kita memerlukan strategi akhir yang lebih baik.

294
00:17:22,700 --> 00:17:26,540
Tapi katakanlah kita menyebut versi ini sebagai salah satu pemecah kata-kata

295
00:17:26,540 --> 00:17:30,740
kita, dan kemudian kita menjalankan beberapa simulasi untuk melihat bagaimana kinerjanya.

296
00:17:30,740 --> 00:17:34,240
Jadi cara kerjanya adalah dengan memainkan setiap permainan kata yang memungkinkan.

297
00:17:34,240 --> 00:17:38,780
Itu melewati 2.315 kata yang merupakan jawaban kata yang sebenarnya.

298
00:17:38,780 --> 00:17:41,340
Ini pada dasarnya menggunakannya sebagai set pengujian.

299
00:17:41,340 --> 00:17:45,820
Dan dengan metode naif ini dengan tidak mempertimbangkan seberapa umum suatu kata, dan

300
00:17:45,820 --> 00:17:50,480
hanya mencoba memaksimalkan informasi di setiap langkah, hingga hanya ada satu pilihan.

301
00:17:50,480 --> 00:17:55,100
Pada akhir simulasi, skor rata-rata menjadi sekitar 4. 124.

302
00:17:55,100 --> 00:17:59,780
Itu tidak buruk, sejujurnya, saya berharap untuk melakukan yang lebih buruk.

303
00:17:59,780 --> 00:18:03,040
Tetapi orang-orang yang bermain wordle akan memberitahu Anda bahwa mereka biasanya bisa mendapatkannya dalam 4.

304
00:18:03,040 --> 00:18:05,260
Tantangan sebenarnya adalah mendapatkan sebanyak 3 buah sebanyak yang Anda bisa.

305
00:18:05,260 --> 00:18:08,920
Ini merupakan lompatan yang cukup besar antara skor 4 dan skor 3.

306
00:18:08,920 --> 00:18:13,300
Hal yang jelas terlihat di sini adalah dengan memasukkan apakah suatu

307
00:18:13,300 --> 00:18:23,160
kata itu umum atau tidak, dan bagaimana tepatnya kita melakukannya.

308
00:18:23,160 --> 00:18:26,860
Cara saya mendekatinya adalah dengan mendapatkan daftar frekuensi

309
00:18:26,860 --> 00:18:28,560
relatif untuk semua kata dalam bahasa Inggris.

310
00:18:28,560 --> 00:18:32,560
Dan saya baru saja menggunakan fungsi data frekuensi kata Mathematica, yang

311
00:18:32,560 --> 00:18:35,520
diambil dari kumpulan data publik Google Buku Bahasa Inggris Ngram.

312
00:18:35,520 --> 00:18:38,680
Dan itu menyenangkan untuk dilihat, misalnya jika kita mengurutkannya dari

313
00:18:38,680 --> 00:18:40,120
kata yang paling umum hingga kata yang paling tidak umum.

314
00:18:40,120 --> 00:18:43,740
Rupanya ini adalah kata 5 huruf yang paling umum dalam bahasa Inggris.

315
00:18:43,740 --> 00:18:46,480
Atau lebih tepatnya, ini adalah yang paling umum ke-8.

316
00:18:46,480 --> 00:18:49,440
Pertama yang mana, setelah itu ada disana dan disana.

317
00:18:49,440 --> 00:18:53,020
Yang pertama bukanlah yang pertama, melainkan yang ke-9, dan masuk akal

318
00:18:53,020 --> 00:18:57,840
jika kata-kata lain ini muncul lebih sering, jika kata setelah yang

319
00:18:57,840 --> 00:18:59,000
pertama adalah setelah, di mana, dan kata-kata tersebut menjadi kurang umum.

320
00:18:59,000 --> 00:19:04,400
Sekarang, dalam menggunakan data ini untuk memodelkan seberapa besar kemungkinan masing-masing kata

321
00:19:04,400 --> 00:19:06,760
ini menjadi jawaban akhir, data tersebut tidak boleh hanya sebanding dengan frekuensinya.

322
00:19:07,020 --> 00:19:12,560
Misal yang diberi skor 0. 002 dalam kumpulan data ini, sedangkan

323
00:19:12,560 --> 00:19:15,200
kata jalinan dalam beberapa hal kemungkinannya sekitar 1000 kali lebih kecil.

324
00:19:15,200 --> 00:19:19,400
Namun keduanya adalah kata-kata yang cukup umum sehingga hampir pasti layak untuk dipertimbangkan.

325
00:19:19,400 --> 00:19:21,900
Jadi kami ingin lebih banyak pemutusan biner.

326
00:19:21,900 --> 00:19:26,520
Cara saya melakukannya adalah dengan membayangkan mengambil seluruh daftar kata yang diurutkan ini,

327
00:19:26,520 --> 00:19:31,060
dan kemudian menyusunnya pada sumbu x, dan kemudian menerapkan fungsi sigmoid, yang

328
00:19:31,060 --> 00:19:35,540
merupakan cara standar untuk memiliki fungsi yang keluarannya pada dasarnya biner, itu's

329
00:19:35,540 --> 00:19:38,500
baik 0 atau 1, namun terdapat kelancaran di antara wilayah ketidakpastian tersebut.

330
00:19:38,500 --> 00:19:43,900
Jadi pada dasarnya, probabilitas yang saya tetapkan untuk setiap kata untuk berada di daftar akhir

331
00:19:43,900 --> 00:19:49,540
akan menjadi nilai fungsi sigmoid di atas di mana pun ia berada pada sumbu x.

332
00:19:49,540 --> 00:19:53,940
Tentu saja hal ini bergantung pada beberapa parameter, misalnya seberapa lebar spasi pada sumbu

333
00:19:53,940 --> 00:19:59,660
x kata-kata tersebut terisi menentukan seberapa bertahap atau tajamnya kita turun dari 1 ke

334
00:19:59,660 --> 00:20:03,000
0, dan di mana kita menempatkan kata-kata tersebut dari kiri ke kanan menentukan batasnya.

335
00:20:03,160 --> 00:20:07,340
Sejujurnya, caraku melakukan ini hanyalah menjilat jariku dan menempelkannya ke angin.

336
00:20:07,340 --> 00:20:10,800
Saya melihat daftar yang diurutkan dan mencoba menemukan jendela di

337
00:20:10,800 --> 00:20:15,280
mana ketika saya melihatnya, saya pikir sekitar setengah dari kata-kata

338
00:20:15,280 --> 00:20:17,680
ini lebih mungkin menjadi jawaban akhir, dan menggunakannya sebagai batasnya.

339
00:20:17,680 --> 00:20:21,840
Setelah kita mendapatkan distribusi seperti ini di seluruh kata, ini memberi

340
00:20:21,840 --> 00:20:24,460
kita situasi lain di mana entropi menjadi pengukuran yang sangat berguna.

341
00:20:24,460 --> 00:20:28,480
Sebagai contoh, katakanlah kita sedang bermain game dan kita mulai dengan

342
00:20:28,480 --> 00:20:32,480
pembuka lama saya, yaitu bulu dan paku, dan kita berakhir dengan

343
00:20:32,480 --> 00:20:33,760
situasi di mana ada empat kemungkinan kata yang cocok dengan itu.

344
00:20:33,760 --> 00:20:36,440
Dan katakanlah kita menganggap semuanya memiliki kemungkinan yang sama.

345
00:20:36,440 --> 00:20:40,000
Izinkan saya bertanya, berapa entropi distribusi ini?

346
00:20:40,000 --> 00:20:45,920
Nah, informasi yang terkait dengan masing-masing kemungkinan ini akan menjadi basis log 2

347
00:20:45,920 --> 00:20:50,800
dari 4, karena masing-masing adalah 1 dan 4, dan itu adalah 2.

348
00:20:50,800 --> 00:20:52,780
Dua informasi, empat kemungkinan.

349
00:20:52,780 --> 00:20:54,360
Semuanya sangat baik dan bagus.

350
00:20:54,360 --> 00:20:58,320
Tapi bagaimana jika saya bilang sebenarnya ada lebih dari empat pertandingan?

351
00:20:58,320 --> 00:21:02,600
Kenyataannya, jika kita melihat daftar kata selengkapnya, ada 16 kata yang cocok.

352
00:21:02,600 --> 00:21:07,260
Namun misalkan model kita memberikan probabilitas yang sangat rendah pada 12 kata lainnya untuk

353
00:21:07,260 --> 00:21:11,440
menjadi jawaban akhir, sekitar 1 dalam 1000 karena kata tersebut sangat tidak jelas.

354
00:21:11,440 --> 00:21:15,480
Sekarang izinkan saya bertanya, berapa entropi distribusi ini?

355
00:21:15,480 --> 00:21:19,600
Jika entropi hanya mengukur jumlah kecocokan di sini, Anda mungkin

356
00:21:19,600 --> 00:21:24,760
mengharapkannya menjadi basis log 2 dari 16, yaitu 4, dua

357
00:21:24,760 --> 00:21:26,200
bit ketidakpastian lebih banyak daripada yang kita miliki sebelumnya.

358
00:21:26,200 --> 00:21:30,320
Namun tentu saja ketidakpastian yang sebenarnya tidak jauh berbeda dengan apa yang kita alami sebelumnya.

359
00:21:30,320 --> 00:21:33,840
Hanya karena ada 12 kata yang sangat tidak jelas ini tidak berarti

360
00:21:33,840 --> 00:21:38,200
akan lebih mengejutkan jika mengetahui bahwa jawaban akhirnya adalah pesona, misalnya.

361
00:21:38,200 --> 00:21:42,080
Jadi ketika Anda benar-benar melakukan perhitungan di sini, dan Anda menjumlahkan probabilitas setiap

362
00:21:42,080 --> 00:21:45,960
kejadian dikalikan dengan informasi terkait, yang Anda dapatkan adalah 2. 11 bit.

363
00:21:45,960 --> 00:21:50,280
Maksud saya, pada dasarnya ada dua bagian, pada dasarnya empat kemungkinan tersebut,

364
00:21:50,280 --> 00:21:54,240
namun ada sedikit ketidakpastian karena semua kejadian yang sangat tidak mungkin

365
00:21:54,240 --> 00:21:57,120
tersebut, meskipun jika Anda mempelajarinya, Anda akan mendapatkan banyak informasi darinya.

366
00:21:57,120 --> 00:22:00,800
Jadi jika diperkecil, inilah bagian yang membuat Wordle

367
00:22:00,800 --> 00:22:01,800
menjadi contoh yang bagus untuk pelajaran teori informasi.

368
00:22:01,800 --> 00:22:05,280
Kami memiliki dua penerapan perasaan yang berbeda untuk entropi.

369
00:22:05,280 --> 00:22:09,640
Yang pertama memberi tahu kita informasi apa yang diharapkan yang akan kita

370
00:22:09,640 --> 00:22:14,560
peroleh dari tebakan tertentu, dan yang kedua mengatakan bisakah kita mengukur

371
00:22:14,560 --> 00:22:16,480
ketidakpastian yang tersisa di antara semua kata yang mungkin kita miliki.

372
00:22:16,480 --> 00:22:19,800
Dan saya harus menekankan, dalam kasus pertama ketika kita melihat informasi yang diharapkan dari sebuah

373
00:22:19,800 --> 00:22:25,000
tebakan, setelah kita memiliki bobot yang tidak sama pada kata-katanya, itu mempengaruhi perhitungan entropi.

374
00:22:25,000 --> 00:22:28,600
Sebagai contoh, izinkan saya mengambil kasus yang sama yang kita

375
00:22:28,600 --> 00:22:33,560
lihat sebelumnya tentang distribusi yang terkait dengan Weary, namun kali

376
00:22:33,560 --> 00:22:34,560
ini menggunakan distribusi yang tidak seragam di semua kemungkinan kata.

377
00:22:34,560 --> 00:22:39,360
Jadi izinkan saya melihat apakah saya dapat menemukan bagian di sini yang menggambarkannya dengan cukup baik.

378
00:22:39,360 --> 00:22:42,480
Oke, ini cukup bagus.

379
00:22:42,480 --> 00:22:46,360
Di sini kita memiliki dua pola berdekatan yang kemungkinannya hampir sama,

380
00:22:46,360 --> 00:22:49,480
namun salah satu pola tersebut memiliki 32 kemungkinan kata yang cocok.

381
00:22:49,480 --> 00:22:54,080
Dan jika kita periksa apa itu, ini adalah 32 kata tersebut,

382
00:22:54,080 --> 00:22:55,600
yang semuanya merupakan kata-kata yang sangat tidak mungkin ketika Anda mengamatinya.

383
00:22:55,600 --> 00:23:00,400
Sulit untuk menemukan jawaban yang terasa masuk akal, mungkin teriakan,

384
00:23:00,400 --> 00:23:04,440
tetapi jika kita melihat pola tetangga dalam distribusi, yang

385
00:23:04,440 --> 00:23:08,920
dianggap sama mungkinnya, kita diberitahu bahwa hanya ada 8

386
00:23:08,920 --> 00:23:09,920
kemungkinan kecocokan, jadi seperempatnya banyak pertandingan, tapi kemungkinannya sama.

387
00:23:09,920 --> 00:23:12,520
Dan saat kami menghentikan pertandingan tersebut, kami dapat mengetahui alasannya.

388
00:23:12,520 --> 00:23:17,840
Beberapa di antaranya adalah jawaban yang benar-benar masuk akal, seperti dering, murka, atau rap.

389
00:23:17,840 --> 00:23:22,000
Untuk mengilustrasikan bagaimana kami menggabungkan semua itu, izinkan saya menampilkan Wordlebot versi 2 di

390
00:23:22,000 --> 00:23:25,960
sini, dan ada dua atau tiga perbedaan utama dari yang pertama yang kami lihat.

391
00:23:25,960 --> 00:23:29,460
Pertama, seperti yang baru saja saya katakan, cara kita menghitung entropi ini,

392
00:23:29,460 --> 00:23:34,800
nilai informasi yang diharapkan, kini menggunakan distribusi yang lebih halus di seluruh

393
00:23:34,800 --> 00:23:39,300
pola yang menggabungkan probabilitas bahwa suatu kata tertentu akan menjadi jawabannya.

394
00:23:39,300 --> 00:23:44,160
Ternyata, air mata tetaplah nomor 1, meski air mata berikut ini sedikit berbeda.

395
00:23:44,160 --> 00:23:47,920
Kedua, ketika ia memberi peringkat pada pilihan teratasnya, sekarang ia akan menyimpan model probabilitas

396
00:23:47,920 --> 00:23:52,600
bahwa setiap kata adalah jawaban sebenarnya, dan ia akan memasukkannya ke dalam keputusannya,

397
00:23:52,600 --> 00:23:55,520
yang lebih mudah dilihat setelah kita mempunyai beberapa tebakan pada kata tersebut. meja.

398
00:23:55,520 --> 00:24:01,120
Sekali lagi, abaikan rekomendasinya karena kita tidak bisa membiarkan mesin mengatur hidup kita.

399
00:24:01,120 --> 00:24:05,160
Dan saya kira saya harus menyebutkan hal lain yang berbeda di sebelah kiri,

400
00:24:05,160 --> 00:24:10,080
bahwa nilai ketidakpastian, jumlah bit, tidak lagi mubazir dengan jumlah kemungkinan kecocokan.

401
00:24:10,080 --> 00:24:16,520
Sekarang jika kita menariknya dan menghitung 2 sampai 8. 02, yang sedikit di atas 256,

402
00:24:16,520 --> 00:24:22,640
saya rasa 259, maksudnya adalah meskipun ada 526 kata yang benar-benar

403
00:24:22,640 --> 00:24:26,400
cocok dengan pola ini, jumlah ketidakpastiannya lebih mirip dengan apa yang

404
00:24:26,400 --> 00:24:29,760
akan terjadi jika ada 259 kata yang sama kemungkinannya. hasil.

405
00:24:29,760 --> 00:24:31,100
Anda bisa memikirkannya seperti ini.

406
00:24:31,100 --> 00:24:35,560
Ia mengetahui bahwa borx bukanlah jawabannya, sama halnya dengan yorts, zorl,

407
00:24:35,560 --> 00:24:37,840
dan zorus, jadi ketidakpastiannya tidak terlalu besar dibandingkan kasus sebelumnya.

408
00:24:37,840 --> 00:24:40,220
Jumlah bit ini akan lebih kecil.

409
00:24:40,220 --> 00:24:44,040
Dan jika saya terus memainkan permainan ini, saya akan menyempurnakannya dengan beberapa

410
00:24:44,040 --> 00:24:48,680
tebakan yang sesuai dengan apa yang ingin saya jelaskan di sini.

411
00:24:48,680 --> 00:24:52,520
Pada tebakan keempat, jika Anda melihat pilihan teratasnya, Anda

412
00:24:52,520 --> 00:24:53,800
dapat melihat bahwa ini tidak lagi hanya memaksimalkan entropi.

413
00:24:53,800 --> 00:24:58,480
Jadi saat ini, secara teknis ada tujuh kemungkinan, tapi satu-satunya

414
00:24:58,480 --> 00:25:00,780
peluang yang memiliki peluang berarti adalah asrama dan kata-kata.

415
00:25:00,780 --> 00:25:04,760
Dan Anda dapat melihat peringkatnya dengan memilih kedua nilai tersebut di

416
00:25:04,760 --> 00:25:07,560
atas semua nilai lainnya, yang sebenarnya akan memberikan lebih banyak informasi.

417
00:25:07,560 --> 00:25:11,200
Pertama kali saya melakukan ini, saya hanya menjumlahkan kedua angka ini untuk mengukur

418
00:25:11,200 --> 00:25:14,580
kualitas setiap tebakan, yang sebenarnya bekerja lebih baik dari yang Anda duga.

419
00:25:14,580 --> 00:25:17,600
Namun hal ini terasa tidak sistematis, dan saya yakin ada pendekatan

420
00:25:17,600 --> 00:25:19,880
lain yang bisa diambil orang, namun inilah pendekatan yang saya pilih.

421
00:25:19,880 --> 00:25:24,200
Jika kita mempertimbangkan prospek tebakan berikutnya, seperti dalam hal ini, yang benar-benar kita

422
00:25:24,200 --> 00:25:28,440
pedulikan adalah skor yang diharapkan dari permainan kita jika kita melakukan itu.

423
00:25:28,440 --> 00:25:32,880
Dan untuk menghitung skor yang diharapkan, kami mengatakan berapa probabilitas bahwa

424
00:25:32,880 --> 00:25:35,640
kata-kata tersebut adalah jawaban sebenarnya, yang saat ini menggambarkan 58%.

425
00:25:36,080 --> 00:25:40,400
Kami katakan dengan peluang 58%, skor kami dalam permainan ini adalah 4.

426
00:25:40,400 --> 00:25:46,240
Dan kemudian dengan probabilitas 1 dikurangi 58% itu, skor kita akan lebih dari 4 itu.

427
00:25:46,240 --> 00:25:50,640
Berapa banyak lagi yang belum kita ketahui, namun kita dapat memperkirakannya berdasarkan

428
00:25:50,640 --> 00:25:52,920
seberapa besar ketidakpastian yang mungkin terjadi setelah kita mencapai titik tersebut.

429
00:25:52,920 --> 00:25:56,600
Secara khusus, saat ini ada 1. 44 bit ketidakpastian.

430
00:25:56,600 --> 00:26:01,560
Jika kita menebak kata-kata, itu memberi tahu kita bahwa informasi yang diharapkan yang akan kita dapatkan adalah 1. 27 bit.

431
00:26:01,560 --> 00:26:06,280
Jadi jika kita menebak-nebak, perbedaan ini menunjukkan seberapa besar

432
00:26:06,280 --> 00:26:08,280
ketidakpastian yang mungkin kita alami setelah hal tersebut terjadi.

433
00:26:08,280 --> 00:26:12,500
Yang kita butuhkan adalah suatu fungsi, yang saya sebut f

434
00:26:12,500 --> 00:26:13,880
di sini, yang menghubungkan ketidakpastian ini dengan skor yang diharapkan.

435
00:26:13,880 --> 00:26:18,040
Dan caranya adalah dengan memplot sekumpulan data dari game sebelumnya

436
00:26:18,040 --> 00:26:23,920
berdasarkan versi 1 bot untuk mengatakan berapa skor sebenarnya setelah

437
00:26:23,920 --> 00:26:27,040
berbagai poin dengan jumlah ketidakpastian tertentu yang sangat terukur.

438
00:26:27,040 --> 00:26:31,120
Misalnya, titik data di sini yang berada di atas nilai sekitar 8. 7

439
00:26:31,120 --> 00:26:36,840
atau lebih dikatakan untuk beberapa permainan setelah titik di mana ada 8. 7 bit

440
00:26:36,840 --> 00:26:39,340
ketidakpastian, butuh dua tebakan untuk mendapatkan jawaban akhir.

441
00:26:39,340 --> 00:26:43,180
Untuk permainan lainnya membutuhkan tiga kali tebakan, untuk permainan lainnya membutuhkan empat kali tebakan.

442
00:26:43,180 --> 00:26:46,920
Jika kita menggeser ke kiri di sini, semua titik di atas nol

443
00:26:46,920 --> 00:26:51,620
menyatakan kapan pun tidak ada sedikit pun ketidakpastian, artinya hanya ada satu

444
00:26:51,620 --> 00:26:55,000
kemungkinan, maka jumlah tebakan yang diperlukan selalu hanya satu, yang meyakinkan.

445
00:26:55,000 --> 00:26:59,020
Kapan pun ada sedikit ketidakpastian, artinya pada dasarnya

446
00:26:59,020 --> 00:27:02,360
hanya ada dua kemungkinan, terkadang diperlukan satu

447
00:27:02,360 --> 00:27:03,940
tebakan lagi, terkadang diperlukan dua tebakan lagi.

448
00:27:03,940 --> 00:27:05,980
Dan seterusnya dan seterusnya di sini.

449
00:27:05,980 --> 00:27:11,020
Mungkin cara yang sedikit lebih mudah untuk memvisualisasikan data ini adalah dengan menggabungkannya dan mengambil rata-ratanya.

450
00:27:11,020 --> 00:27:15,940
Misalnya bilah di sini menyatakan di antara semua titik di mana kita mempunyai

451
00:27:15,940 --> 00:27:22,420
sedikit ketidakpastian, rata-rata jumlah tebakan baru yang diperlukan adalah sekitar 1. 5.

452
00:27:22,420 --> 00:27:25,920
Dan batasan di sini mengatakan di antara semua permainan yang

453
00:27:25,920 --> 00:27:30,480
berbeda di mana pada titik tertentu ketidakpastiannya sedikit di atas

454
00:27:30,480 --> 00:27:35,120
empat bit, yang seperti mempersempitnya menjadi 16 kemungkinan berbeda, maka

455
00:27:35,120 --> 00:27:36,240
rata-rata memerlukan lebih dari dua tebakan dari titik itu. maju.

456
00:27:36,240 --> 00:27:40,080
Dan dari sini saya baru saja melakukan regresi agar sesuai dengan fungsi yang tampaknya masuk akal untuk ini.

457
00:27:40,080 --> 00:27:44,160
Dan ingat, inti dari melakukan semua itu adalah agar kita dapat mengukur intuisi bahwa semakin

458
00:27:44,160 --> 00:27:49,720
banyak informasi yang kita peroleh dari sebuah kata, semakin rendah pula skor yang diharapkan.

459
00:27:49,720 --> 00:27:54,380
Jadi dengan ini sebagai versi 2. 0, jika kita kembali dan menjalankan rangkaian simulasi

460
00:27:54,380 --> 00:27:59,820
yang sama, memainkannya melawan semua 2315 kemungkinan jawaban, bagaimana cara kerjanya?

461
00:27:59,820 --> 00:28:04,060
Berbeda dengan versi pertama kami, ini pasti lebih baik, dan itu meyakinkan.

462
00:28:04,060 --> 00:28:08,780
Semua dikatakan dan dilakukan rata-ratanya adalah sekitar 3. 6, meskipun tidak seperti versi pertama, ada

463
00:28:08,780 --> 00:28:12,820
beberapa kali versi ini kalah dan membutuhkan lebih dari enam dalam keadaan ini.

464
00:28:12,820 --> 00:28:15,980
Mungkin karena ada kalanya kita melakukan pengorbanan

465
00:28:15,980 --> 00:28:18,980
untuk benar-benar mencapai tujuan daripada memaksimalkan informasi.

466
00:28:18,980 --> 00:28:22,140
Jadi bisakah kita melakukan lebih baik dari 3. 6?

467
00:28:22,140 --> 00:28:23,260
Kami pasti bisa.

468
00:28:23,260 --> 00:28:27,120
Sekarang saya katakan di awal bahwa paling menyenangkan adalah mencoba tidak

469
00:28:27,120 --> 00:28:29,980
memasukkan daftar jawaban kata yang sebenarnya ke dalam cara membangun modelnya.

470
00:28:29,980 --> 00:28:35,180
Namun jika kami menggabungkannya, performa terbaik yang bisa saya dapatkan adalah sekitar 3. 43.

471
00:28:35,180 --> 00:28:39,520
Jadi jika kita mencoba menjadi lebih canggih dari sekedar menggunakan data frekuensi kata untuk memilih

472
00:28:39,520 --> 00:28:44,220
distribusi sebelumnya, ini 3. 43 mungkin memberi gambaran maksimal seberapa bagus yang bisa kita

473
00:28:44,220 --> 00:28:46,360
dapatkan dengan itu, atau setidaknya seberapa bagus yang bisa saya dapatkan dengan itu.

474
00:28:46,360 --> 00:28:50,240
Performa terbaik tersebut pada dasarnya hanya menggunakan ide-ide yang telah saya

475
00:28:50,240 --> 00:28:53,400
bicarakan di sini, namun lebih jauh lagi, seperti melakukan penelusuran informasi

476
00:28:53,400 --> 00:28:55,660
yang diharapkan dua langkah ke depan, bukan hanya satu langkah.

477
00:28:55,660 --> 00:28:58,720
Awalnya saya berencana untuk membicarakan lebih banyak tentang hal itu,

478
00:28:58,720 --> 00:29:00,580
tetapi saya menyadari bahwa kita sebenarnya sudah berjalan cukup lama.

479
00:29:00,580 --> 00:29:03,520
Satu hal yang akan saya katakan adalah setelah melakukan pencarian dua

480
00:29:03,520 --> 00:29:07,720
langkah ini dan kemudian menjalankan beberapa contoh simulasi pada kandidat

481
00:29:07,720 --> 00:29:09,500
teratas, sejauh ini bagi saya setidaknya Crane adalah pembuka terbaik.

482
00:29:09,500 --> 00:29:11,080
Siapa sangka?

483
00:29:11,080 --> 00:29:15,680
Juga jika Anda menggunakan daftar kata yang sebenarnya untuk menentukan ruang kemungkinan

484
00:29:15,680 --> 00:29:17,920
Anda, maka ketidakpastian yang Anda mulai adalah sedikit di atas 11 bit.

485
00:29:18,160 --> 00:29:22,760
Dan ternyata, hanya dari pencarian brute force, informasi maksimal yang

486
00:29:22,760 --> 00:29:26,580
diharapkan setelah dua tebakan pertama adalah sekitar 10 bit.

487
00:29:26,580 --> 00:29:31,720
Hal ini menunjukkan bahwa skenario terbaik, setelah dua tebakan pertama

488
00:29:31,720 --> 00:29:35,220
Anda, dengan permainan optimal sempurna, Anda akan memiliki sedikit ketidakpastian.

489
00:29:35,220 --> 00:29:37,400
Yang sama dengan membuat dua kemungkinan tebakan.

490
00:29:37,400 --> 00:29:41,440
Jadi menurut saya adil dan mungkin cukup konservatif untuk mengatakan bahwa Anda tidak akan

491
00:29:41,440 --> 00:29:45,620
pernah bisa menulis algoritma yang mendapatkan rata-rata serendah 3, karena dengan kata-kata yang tersedia

492
00:29:45,620 --> 00:29:50,460
untuk Anda, tidak ada ruang untuk mendapatkan informasi yang cukup setelah hanya dua langkah

493
00:29:50,460 --> 00:29:53,820
yang harus dilakukan. mampu menjamin jawaban di slot ketiga setiap saat tanpa gagal.

