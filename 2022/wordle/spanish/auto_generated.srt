1
00:00:00,000 --> 00:00:03,180
El juego Wurdle se ha vuelto bastante viral en los últimos dos meses y,

2
00:00:03,180 --> 00:00:06,449
como nunca desperdiciaré la oportunidad de una lección de matemáticas, se

3
00:00:06,449 --> 00:00:09,585
me ocurre que este juego es un muy buen ejemplo central en una lección

4
00:00:09,585 --> 00:00:13,120
sobre teoría de la información y, en particular, un tema conocido como entropía.

5
00:00:13,120 --> 00:00:16,463
Verá, como mucha gente, me quedé atrapado en el rompecabezas, y como

6
00:00:16,463 --> 00:00:19,613
muchos programadores, también me quedé atrapado en el intento de

7
00:00:19,613 --> 00:00:23,200
escribir un algoritmo que jugara el juego de la manera más óptima posible.

8
00:00:23,200 --> 00:00:26,096
Y lo que pensé que haría aquí es simplemente hablarles sobre

9
00:00:26,096 --> 00:00:29,230
mi proceso y explicarles algunas de las matemáticas que implican,

10
00:00:29,230 --> 00:00:32,080
ya que todo el algoritmo se centra en esta idea de entropía.

11
00:00:32,080 --> 00:00:42,180
Lo primero es lo primero, en caso de que no hayas oído hablar de él, ¿qué es Wurdle?

12
00:00:42,180 --> 00:00:45,370
Y para matar dos pájaros de un tiro mientras repasamos las reglas del juego,

13
00:00:45,370 --> 00:00:48,644
permítanme también un avance de hacia dónde vamos con esto, que es desarrollar

14
00:00:48,644 --> 00:00:51,380
un pequeño algoritmo que básicamente jugará el juego por nosotros.

15
00:00:51,380 --> 00:00:55,860
Aunque no he hecho el Wurdle de hoy, es el 4 de febrero y veremos cómo le va al robot.

16
00:00:55,860 --> 00:00:58,240
El objetivo de Wurdle es adivinar una palabra misteriosa de

17
00:00:58,240 --> 00:01:00,860
cinco letras y tienes seis oportunidades diferentes para adivinar.

18
00:01:00,860 --> 00:01:05,240
Por ejemplo, mi robot Wurdle me sugiere que empiece con la grúa de adivinanzas.

19
00:01:05,240 --> 00:01:08,160
Cada vez que haces una suposición, obtienes información sobre

20
00:01:08,160 --> 00:01:10,940
qué tan cerca está tu suposición de la respuesta verdadera.

21
00:01:10,940 --> 00:01:14,540
Aquí el cuadro gris me dice que no hay una C en la respuesta real.

22
00:01:14,540 --> 00:01:18,340
El cuadro amarillo me dice que hay una R, pero no está en esa posición.

23
00:01:18,340 --> 00:01:22,820
El cuadro verde me dice que la palabra secreta tiene una A y está en la tercera posición.

24
00:01:22,820 --> 00:01:24,300
Y luego no hay N y no hay E.

25
00:01:24,300 --> 00:01:27,420
Así que déjame entrar y decirle esa información al robot Wurdle.

26
00:01:27,420 --> 00:01:31,500
Empezamos con grúa, obtuvimos gris, amarillo, verde, gris, gris.

27
00:01:31,500 --> 00:01:33,632
No te preocupes por todos los datos que están mostrando

28
00:01:33,632 --> 00:01:35,460
ahora mismo, te lo explicaré a su debido tiempo.

29
00:01:35,460 --> 00:01:39,700
Pero su principal sugerencia para nuestra segunda elección es un truco.

30
00:01:39,700 --> 00:01:42,783
Y tu suposición tiene que ser una palabra real de cinco letras, pero como

31
00:01:42,783 --> 00:01:45,700
verás, es bastante liberal con lo que realmente te permitirá adivinar.

32
00:01:45,700 --> 00:01:48,860
En este caso, intentamos stick.

33
00:01:48,860 --> 00:01:50,260
Y bueno, la cosa pinta bastante bien.

34
00:01:50,260 --> 00:01:54,740
Pulsamos la S y la H, así conocemos las tres primeras letras, sabemos que hay una R.

35
00:01:54,740 --> 00:01:59,740
Y entonces será como SHA algo R, o SHA R algo.

36
00:01:59,740 --> 00:02:05,220
Y parece que el robot Wurdle sabe que solo hay dos posibilidades: fragmentar o filoso.

37
00:02:05,220 --> 00:02:08,218
Eso es una especie de volatilidad entre ellos en este momento, así que

38
00:02:08,218 --> 00:02:11,260
supongo que probablemente solo porque es alfabético va con el fragmento.

39
00:02:11,260 --> 00:02:13,000
Qué hurra, es la respuesta real.

40
00:02:13,000 --> 00:02:14,660
Entonces lo tenemos en tres.

41
00:02:14,660 --> 00:02:17,614
Si te preguntas si eso es bueno, la forma en que escuché a

42
00:02:17,614 --> 00:02:20,820
una persona decir que con Wurdle cuatro es par y tres es birdie.

43
00:02:20,820 --> 00:02:22,960
Lo cual creo que es una analogía bastante adecuada.

44
00:02:22,960 --> 00:02:25,504
Tienes que ser constante en tu juego para conseguir

45
00:02:25,504 --> 00:02:27,560
cuatro, pero ciertamente no es una locura.

46
00:02:27,560 --> 00:02:30,000
Pero cuando lo obtienes en tres, se siente genial.

47
00:02:30,000 --> 00:02:33,169
Entonces, si estás dispuesto a hacerlo, lo que me gustaría hacer aquí es simplemente

48
00:02:33,169 --> 00:02:36,338
hablar sobre mi proceso de pensamiento desde el principio sobre cómo abordo el robot

49
00:02:36,338 --> 00:02:36,600
Wurdle.

50
00:02:36,600 --> 00:02:39,800
Y como dije, en realidad es una excusa para una lección de teoría de la información.

51
00:02:39,800 --> 00:02:48,560
El objetivo principal es explicar qué es información y qué es entropía.

52
00:02:48,560 --> 00:02:50,867
Lo primero que pensé al abordar esto fue observar las

53
00:02:50,867 --> 00:02:53,560
frecuencias relativas de diferentes letras en el idioma inglés.

54
00:02:53,560 --> 00:02:56,503
Entonces pensé, bueno, ¿hay una suposición inicial o un par de

55
00:02:56,503 --> 00:02:59,960
suposiciones iniciales que acierten muchas de estas letras más frecuentes?

56
00:02:59,960 --> 00:03:03,780
Y una que me gustaba mucho era hacer otras seguidas de uñas.

57
00:03:03,780 --> 00:03:05,940
La idea es que si tocas una letra, ya sabes, obtienes

58
00:03:05,940 --> 00:03:07,980
un verde o un amarillo, eso siempre se siente bien.

59
00:03:07,980 --> 00:03:09,460
Parece que estás recibiendo información.

60
00:03:09,460 --> 00:03:12,291
Pero en estos casos, incluso si no aciertas y siempre aparecen

61
00:03:12,291 --> 00:03:14,988
grises, eso te da mucha información ya que es bastante raro

62
00:03:14,988 --> 00:03:17,640
encontrar una palabra que no tenga ninguna de estas letras.

63
00:03:17,640 --> 00:03:20,606
Pero aun así, esto no parece súper sistemático, porque,

64
00:03:20,606 --> 00:03:23,520
por ejemplo, no tiene en cuenta el orden de las letras.

65
00:03:23,520 --> 00:03:26,080
¿Por qué escribir uñas cuando puedo escribir caracol?

66
00:03:26,080 --> 00:03:27,720
¿Es mejor tener esa S al final?

67
00:03:27,720 --> 00:03:28,720
No estoy realmente seguro.

68
00:03:28,720 --> 00:03:32,940
Ahora, un amigo mío dijo que le gustaba comenzar con la palabra cansado, lo que

69
00:03:32,940 --> 00:03:37,160
me sorprendió un poco porque tiene algunas letras poco comunes como la W y la Y.

70
00:03:37,160 --> 00:03:39,400
Pero quién sabe, tal vez ese sea un mejor comienzo.

71
00:03:39,400 --> 00:03:42,284
¿Existe algún tipo de puntuación cuantitativa que podamos

72
00:03:42,284 --> 00:03:44,920
dar para juzgar la calidad de una posible suposición?

73
00:03:44,920 --> 00:03:48,086
Ahora, para preparar la forma en que vamos a clasificar las posibles conjeturas,

74
00:03:48,086 --> 00:03:51,565
retrocedamos y agreguemos un poco de claridad sobre cómo está configurado exactamente el

75
00:03:51,565 --> 00:03:51,800
juego.

76
00:03:51,800 --> 00:03:54,796
Entonces, hay una lista de palabras que le permitirá ingresar y que se

77
00:03:54,796 --> 00:03:57,920
consideran conjeturas válidas y que tiene aproximadamente 13,000 palabras.

78
00:03:57,920 --> 00:04:02,377
Pero cuando lo miras, hay muchas cosas realmente poco comunes, cosas como una cabeza o

79
00:04:02,377 --> 00:04:06,578
Ali y ARG, el tipo de palabras que provocan discusiones familiares en un juego de

80
00:04:06,578 --> 00:04:07,040
Scrabble.

81
00:04:07,040 --> 00:04:10,600
Pero la sensación del juego es que la respuesta siempre será una palabra bastante común.

82
00:04:10,600 --> 00:04:16,080
Y de hecho, hay otra lista de alrededor de 2300 palabras que son las posibles respuestas.

83
00:04:16,080 --> 00:04:19,044
Y esta es una lista seleccionada por humanos, creo que específicamente

84
00:04:19,044 --> 00:04:21,800
por la novia del creador del juego, lo cual es bastante divertido.

85
00:04:21,800 --> 00:04:25,962
Pero lo que me gustaría hacer, nuestro desafío para este proyecto es ver si podemos

86
00:04:25,962 --> 00:04:30,422
escribir un programa resolviendo Wordle que no incorpore conocimientos previos sobre esta

87
00:04:30,422 --> 00:04:30,720
lista.

88
00:04:30,720 --> 00:04:33,140
Por un lado, hay muchas palabras de cinco letras

89
00:04:33,140 --> 00:04:35,560
bastante comunes que no encontrarás en esa lista.

90
00:04:35,560 --> 00:04:38,720
Por lo tanto, sería mejor escribir un programa que sea un poco más resistente y

91
00:04:38,720 --> 00:04:41,960
que pueda jugar con Wordle contra cualquiera, no solo contra el sitio web oficial.

92
00:04:41,960 --> 00:04:44,677
Y también la razón por la que sabemos cuál es esta lista de

93
00:04:44,677 --> 00:04:47,440
posibles respuestas es porque es visible en el código fuente.

94
00:04:47,440 --> 00:04:50,140
Pero la forma en que es visible en el código fuente es en el

95
00:04:50,140 --> 00:04:52,840
orden específico en el que aparecen las respuestas día a día.

96
00:04:52,840 --> 00:04:56,400
Así que siempre puedes buscar cuál será la respuesta de mañana.

97
00:04:56,400 --> 00:04:59,140
Claramente, en cierto sentido usar la lista es hacer trampa.

98
00:04:59,140 --> 00:05:02,287
Y lo que hace que el rompecabezas sea más interesante y una lección de

99
00:05:02,287 --> 00:05:05,124
teoría de la información más rica es utilizar algunos datos más

100
00:05:05,124 --> 00:05:08,315
universales, como las frecuencias relativas de las palabras en general,

101
00:05:08,315 --> 00:05:11,640
para capturar esta intuición de tener preferencia por palabras más comunes.

102
00:05:11,640 --> 00:05:16,560
Entonces, de estas 13.000 posibilidades, ¿cómo deberíamos elegir la suposición inicial?

103
00:05:16,560 --> 00:05:19,960
Por ejemplo, si mi amigo me propone cansancio, ¿cómo debemos analizar su calidad?

104
00:05:19,960 --> 00:05:23,975
Bueno, la razón por la que dijo que le gusta esa improbable W es que le

105
00:05:23,975 --> 00:05:27,880
gusta la naturaleza remota de lo bien que se siente si aciertas esa W.

106
00:05:27,880 --> 00:05:32,032
Por ejemplo, si el primer patrón revelado fue algo como este, entonces resulta

107
00:05:32,032 --> 00:05:36,080
que solo hay 58 palabras en este léxico gigante que coinciden con ese patrón.

108
00:05:36,080 --> 00:05:38,900
Entonces esa es una gran reducción de 13.000.

109
00:05:38,900 --> 00:05:41,107
Pero la otra cara de la moneda, por supuesto, es

110
00:05:41,107 --> 00:05:43,360
que es muy poco común obtener un patrón como este.

111
00:05:43,360 --> 00:05:47,544
Específicamente, si cada palabra tuviera la misma probabilidad de ser la respuesta,

112
00:05:47,544 --> 00:05:51,680
la probabilidad de encontrar este patrón sería 58 dividido por alrededor de 13.000.

113
00:05:51,680 --> 00:05:53,880
Por supuesto, no es igualmente probable que sean respuestas.

114
00:05:53,880 --> 00:05:56,680
La mayoría de ellas son palabras muy oscuras e incluso cuestionables.

115
00:05:56,680 --> 00:05:59,395
Pero al menos para nuestra primera aproximación a todo esto, supongamos que

116
00:05:59,395 --> 00:06:02,040
todas son igualmente probables y luego refinemos eso un poco más adelante.

117
00:06:02,040 --> 00:06:04,628
La cuestión es que, por su propia naturaleza, es poco

118
00:06:04,628 --> 00:06:07,360
probable que se produzca un patrón con mucha información.

119
00:06:07,360 --> 00:06:11,920
De hecho, lo que significa ser informativo es que es poco probable.

120
00:06:11,920 --> 00:06:15,140
Un patrón mucho más probable de ver con esta apertura

121
00:06:15,140 --> 00:06:18,360
sería algo como esto, donde por supuesto no hay una W.

122
00:06:18,360 --> 00:06:22,080
Tal vez haya una E, y tal vez no haya una A, no haya una R, no haya una Y.

123
00:06:22,080 --> 00:06:24,640
En este caso, hay 1400 coincidencias posibles.

124
00:06:24,640 --> 00:06:27,355
Si todas fueran igualmente probables, resulta que hay una

125
00:06:27,355 --> 00:06:30,680
probabilidad de alrededor del 11% de que este sea el patrón que verías.

126
00:06:30,680 --> 00:06:34,320
De modo que los resultados más probables son también los menos informativos.

127
00:06:34,320 --> 00:06:38,107
Para obtener una visión más global, permítame mostrarle la distribución

128
00:06:38,107 --> 00:06:42,000
completa de probabilidades en todos los diferentes patrones que pueda ver.

129
00:06:42,000 --> 00:06:45,670
Entonces, cada barra que estás viendo corresponde a un posible patrón de

130
00:06:45,670 --> 00:06:49,289
colores que podría revelarse, de los cuales hay de 3 a 5 posibilidades,

131
00:06:49,289 --> 00:06:52,960
y están organizados de izquierda a derecha, del más común al menos común.

132
00:06:52,960 --> 00:06:56,200
Entonces, la posibilidad más común aquí es que obtengas todos los grises.

133
00:06:56,200 --> 00:06:58,800
Esto sucede aproximadamente el 14% del tiempo.

134
00:06:58,800 --> 00:07:02,488
Y lo que esperas cuando haces una suposición es terminar en algún

135
00:07:02,488 --> 00:07:06,287
lugar de esta larga cola, como aquí donde solo hay 18 posibilidades

136
00:07:06,287 --> 00:07:09,920
para lo que coincide con este patrón que evidentemente se ve así.

137
00:07:09,920 --> 00:07:12,316
O si nos aventuramos un poco más hacia la izquierda,

138
00:07:12,316 --> 00:07:14,080
ya sabes, tal vez lleguemos hasta aquí.

139
00:07:14,080 --> 00:07:16,560
Bien, aquí tienes un buen rompecabezas.

140
00:07:16,560 --> 00:07:19,325
¿Cuáles son las tres palabras en inglés que comienzan

141
00:07:19,325 --> 00:07:22,040
con W, terminan con Y y tienen una R en alguna parte?

142
00:07:22,040 --> 00:07:27,560
Resulta que las respuestas son, veamos, prolijas, llenas de gusanos e irónicas.

143
00:07:27,560 --> 00:07:31,986
Entonces, para juzgar qué tan buena es esta palabra en general, queremos algún tipo

144
00:07:31,986 --> 00:07:36,360
de medida de la cantidad esperada de información que obtendrá de esta distribución.

145
00:07:36,360 --> 00:07:41,059
Si analizamos cada patrón y multiplicamos su probabilidad de ocurrir por algo

146
00:07:41,059 --> 00:07:46,000
que mida qué tan informativo es, eso tal vez pueda darnos una puntuación objetiva.

147
00:07:46,000 --> 00:07:48,029
Ahora tu primer instinto sobre lo que debería

148
00:07:48,029 --> 00:07:50,280
ser ese algo podría ser el número de coincidencias.

149
00:07:50,280 --> 00:07:52,960
Quieres un número promedio más bajo de coincidencias.

150
00:07:52,960 --> 00:07:58,931
Pero en lugar de eso me gustaría usar una medida más universal que a menudo atribuimos

151
00:07:58,931 --> 00:08:04,422
a la información, y una que será más flexible una vez que tengamos asignada una

152
00:08:04,422 --> 00:08:10,600
probabilidad diferente a cada una de estas 13.000 palabras sobre si son o no la respuesta.

153
00:08:10,600 --> 00:08:14,225
La unidad de información estándar es el bit, que tiene una fórmula un

154
00:08:14,225 --> 00:08:17,800
poco divertida, pero es realmente intuitiva si solo miramos ejemplos.

155
00:08:17,800 --> 00:08:21,027
Si tienes una observación que reduce a la mitad tu espacio

156
00:08:21,027 --> 00:08:24,200
de posibilidades, decimos que tiene un bit de información.

157
00:08:24,200 --> 00:08:26,586
En nuestro ejemplo, el espacio de posibilidades son todas las palabras

158
00:08:26,586 --> 00:08:29,073
posibles, y resulta que aproximadamente la mitad de las palabras de cinco

159
00:08:29,073 --> 00:08:31,560
letras tienen una S, un poco menos que eso, pero aproximadamente la mitad.

160
00:08:31,560 --> 00:08:35,200
Entonces esa observación les daría un poco de información.

161
00:08:35,200 --> 00:08:38,547
Si en cambio un hecho nuevo reduce ese espacio de posibilidades

162
00:08:38,547 --> 00:08:42,000
en un factor de cuatro, decimos que tiene dos bits de información.

163
00:08:42,000 --> 00:08:45,120
Por ejemplo, resulta que aproximadamente una cuarta parte de estas palabras tienen una T.

164
00:08:45,120 --> 00:08:47,948
Si la observación corta ese espacio por un factor de ocho,

165
00:08:47,948 --> 00:08:50,920
decimos que son tres bits de información, y así sucesivamente.

166
00:08:50,920 --> 00:08:55,000
Cuatro bits lo cortan en un 16, cinco bits lo cortan en un 32.

167
00:08:55,000 --> 00:08:59,732
Así que ahora quizás quieras hacer una pausa y preguntarte: ¿cuál es la fórmula para

168
00:08:59,732 --> 00:09:04,520
obtener información sobre el número de bits en términos de probabilidad de que ocurra?

169
00:09:04,520 --> 00:09:08,299
Lo que estamos diciendo aquí es que cuando se le suma la mitad al número de bits, eso

170
00:09:08,299 --> 00:09:12,209
es lo mismo que la probabilidad, que es lo mismo que decir que dos elevado a la potencia

171
00:09:12,209 --> 00:09:16,032
del número de bits es uno sobre la probabilidad, lo cual se reordena además para decir

172
00:09:16,032 --> 00:09:19,680
que la información es el logaritmo en base dos de uno dividido por la probabilidad.

173
00:09:19,680 --> 00:09:22,416
Y a veces se ve esto con un reordenamiento más, donde la

174
00:09:22,416 --> 00:09:25,680
información es el logaritmo negativo en base dos de la probabilidad.

175
00:09:25,680 --> 00:09:28,604
Expresado así, puede parecer un poco extraño para los no

176
00:09:28,604 --> 00:09:31,733
iniciados, pero en realidad es sólo la idea muy intuitiva de

177
00:09:31,733 --> 00:09:35,120
preguntar cuántas veces has reducido tus posibilidades a la mitad.

178
00:09:35,120 --> 00:09:37,599
Ahora, si te lo estás preguntando, ya sabes, pensé que solo estábamos jugando

179
00:09:37,599 --> 00:09:39,920
un divertido juego de palabras, ¿por qué los logaritmos entran en escena?

180
00:09:39,920 --> 00:09:44,060
Una de las razones por las que esta es una unidad más agradable es que es mucho más

181
00:09:44,060 --> 00:09:48,447
fácil hablar de eventos muy improbables, mucho más fácil decir que una observación tiene

182
00:09:48,447 --> 00:09:52,785
20 bits de información que decir que la probabilidad de que ocurra tal o cual cosa es 0.

183
00:09:52,785 --> 00:09:53,480
0000095.

184
00:09:53,480 --> 00:09:57,593
Pero una razón más sustancial por la que esta expresión logarítmica resultó ser una

185
00:09:57,593 --> 00:10:02,000
adición muy útil a la teoría de la probabilidad es la forma en que se suma la información.

186
00:10:02,000 --> 00:10:05,768
Por ejemplo, si una observación le proporciona dos bits de información, lo que

187
00:10:05,768 --> 00:10:09,489
reduce su espacio en cuatro, y luego una segunda observación, como su segunda

188
00:10:09,489 --> 00:10:13,257
suposición en Wordle, le proporciona otros tres bits de información, lo que lo

189
00:10:13,257 --> 00:10:17,360
reduce aún más en otro factor de ocho, la dos juntos te dan cinco bits de información.

190
00:10:17,360 --> 00:10:19,184
De la misma manera que a las probabilidades les

191
00:10:19,184 --> 00:10:21,200
gusta multiplicarse, a la información le gusta sumar.

192
00:10:21,200 --> 00:10:23,592
Entonces, tan pronto como estamos en el ámbito de algo así

193
00:10:23,592 --> 00:10:26,024
como un valor esperado, donde sumamos un montón de números,

194
00:10:26,024 --> 00:10:28,660
los registros hacen que sea mucho más agradable tratar con ellos.

195
00:10:28,660 --> 00:10:31,962
Volvamos a nuestra distribución de Weary y agreguemos otro pequeño

196
00:10:31,962 --> 00:10:35,560
rastreador aquí, que nos muestra cuánta información hay para cada patrón.

197
00:10:35,560 --> 00:10:39,506
Lo principal que quiero que note es que cuanto mayor es la probabilidad a medida que

198
00:10:39,506 --> 00:10:43,500
llegamos a esos patrones más probables, menor es la información y menos bits se ganan.

199
00:10:43,500 --> 00:10:47,263
La forma en que medimos la calidad de esta suposición será tomando el valor

200
00:10:47,263 --> 00:10:51,027
esperado de esta información, donde analizamos cada patrón, decimos qué tan

201
00:10:51,027 --> 00:10:54,940
probable es y luego lo multiplicamos por cuántos bits de información obtenemos.

202
00:10:54,940 --> 00:10:58,107
Y en el ejemplo de Weary, resulta ser 4.

203
00:10:58,107 --> 00:10:58,480
9 bits.

204
00:10:58,480 --> 00:11:02,002
Entonces, en promedio, la información que obtienes de esta suposición inicial

205
00:11:02,002 --> 00:11:05,660
es tan buena como cortar tu espacio de posibilidades a la mitad unas cinco veces.

206
00:11:05,660 --> 00:11:09,209
Por el contrario, un ejemplo de una suposición con un

207
00:11:09,209 --> 00:11:13,220
valor de información esperado más alto sería algo como Slate.

208
00:11:13,220 --> 00:11:16,180
En este caso notarás que la distribución luce mucho más plana.

209
00:11:16,180 --> 00:11:20,283
En particular, la aparición más probable de todos los grises solo tiene alrededor de

210
00:11:20,283 --> 00:11:24,435
un 6% de posibilidades de ocurrir, por lo que, como mínimo, evidentemente obtendrás 3.

211
00:11:24,435 --> 00:11:25,940
9 bits de información.

212
00:11:25,940 --> 00:11:29,140
Pero eso es un mínimo, normalmente obtendrás algo mejor que eso.

213
00:11:29,140 --> 00:11:32,683
Y resulta que cuando haces cálculos en este caso y sumas todos los

214
00:11:32,683 --> 00:11:36,333
términos relevantes, la información promedio es de aproximadamente 5.

215
00:11:36,333 --> 00:11:36,420
8.

216
00:11:36,420 --> 00:11:39,990
Entonces, a diferencia de Weary, su espacio de posibilidades será

217
00:11:39,990 --> 00:11:43,940
aproximadamente la mitad después de esta primera suposición, en promedio.

218
00:11:43,940 --> 00:11:46,821
De hecho, hay una historia divertida sobre el nombre

219
00:11:46,821 --> 00:11:49,540
de este valor esperado de cantidad de información.

220
00:11:49,540 --> 00:11:53,066
La teoría de la información fue desarrollada por Claude Shannon, que trabajaba en los

221
00:11:53,066 --> 00:11:56,716
Laboratorios Bell en la década de 1940, pero estaba hablando de algunas de sus ideas aún

222
00:11:56,716 --> 00:12:00,202
por publicar con John von Neumann, que era este gigante intelectual de la época, muy

223
00:12:00,202 --> 00:12:03,687
destacado. en matemáticas y física y los inicios de lo que se estaba convirtiendo en

224
00:12:03,687 --> 00:12:04,180
informática.

225
00:12:04,180 --> 00:12:07,583
Y cuando mencionó que realmente no tenía un buen nombre para este valor

226
00:12:07,583 --> 00:12:10,986
esperado de la cantidad de información, supuestamente von Neumann dijo,

227
00:12:10,986 --> 00:12:14,720
según cuenta la historia, bueno, deberías llamarlo entropía, y por dos razones.

228
00:12:14,720 --> 00:12:18,715
En primer lugar, su función de incertidumbre se ha utilizado en mecánica estadística

229
00:12:18,715 --> 00:12:22,710
con ese nombre, por lo que ya tiene un nombre, y en segundo lugar, y más importante,

230
00:12:22,710 --> 00:12:26,940
nadie sabe qué es realmente la entropía, por lo que en un debate siempre tener la ventaja.

231
00:12:26,940 --> 00:12:30,152
Entonces, si el nombre parece un poco misterioso, y si hay

232
00:12:30,152 --> 00:12:33,420
que creer en esta historia, es más o menos intencionalmente.

233
00:12:33,420 --> 00:12:36,936
Además, si te preguntas acerca de su relación con toda esa segunda ley de la

234
00:12:36,936 --> 00:12:40,316
termodinámica de la física, definitivamente hay una conexión, pero en sus

235
00:12:40,316 --> 00:12:43,832
orígenes Shannon solo estaba tratando con la teoría de la probabilidad pura,

236
00:12:43,832 --> 00:12:47,349
y para nuestros propósitos aquí, cuando uso la palabra entropía, solo quiero

237
00:12:47,349 --> 00:12:50,820
que piense en el valor de información esperado de una suposición particular.

238
00:12:50,820 --> 00:12:54,380
Puedes pensar que la entropía mide dos cosas simultáneamente.

239
00:12:54,380 --> 00:12:57,420
El primero es qué tan plana es la distribución.

240
00:12:57,420 --> 00:13:01,700
Cuanto más cercana a la uniformidad sea una distribución, mayor será la entropía.

241
00:13:01,700 --> 00:13:05,139
En nuestro caso, donde hay de 3 a 5 patrones en total, para una

242
00:13:05,139 --> 00:13:08,526
distribución uniforme, observar cualquiera de ellos tendría un

243
00:13:08,526 --> 00:13:11,858
registro de información en base 2 de 3 a 5, que resulta ser 7.

244
00:13:11,858 --> 00:13:17,860
92, por lo que ese es el máximo absoluto que podrías tener para esta entropía.

245
00:13:17,860 --> 00:13:20,512
Pero la entropía también es una especie de medida

246
00:13:20,512 --> 00:13:22,900
de cuántas posibilidades hay en primer lugar.

247
00:13:22,900 --> 00:13:27,857
Por ejemplo, si tienes una palabra en la que sólo hay 16 patrones posibles y cada uno de

248
00:13:27,857 --> 00:13:32,760
ellos es igualmente probable, esta entropía, esta información esperada, sería de 4 bits.

249
00:13:32,760 --> 00:13:36,854
Pero si tienes otra palabra donde hay 64 patrones posibles que podrían surgir,

250
00:13:36,854 --> 00:13:41,000
y todos son igualmente probables, entonces la entropía resultaría ser de 6 bits.

251
00:13:41,000 --> 00:13:45,411
Entonces, si ves alguna distribución en la naturaleza que tiene una entropía de

252
00:13:45,411 --> 00:13:49,878
6 bits, es como si estuviera diciendo que hay tanta variación e incertidumbre en

253
00:13:49,878 --> 00:13:54,400
lo que está a punto de suceder como si hubiera 64 resultados igualmente probables.

254
00:13:54,400 --> 00:13:58,360
Para mi primera pasada por el Wurtelebot, básicamente le pedí que hiciera esto.

255
00:13:58,360 --> 00:14:03,184
Revisa todas las conjeturas posibles que puedas tener, las 13.000 palabras, calcula

256
00:14:03,184 --> 00:14:07,894
la entropía de cada una, o más específicamente, la entropía de la distribución en

257
00:14:07,894 --> 00:14:12,547
todos los patrones que puedas ver, para cada una, y elige la más alta, ya que es

258
00:14:12,547 --> 00:14:17,200
el que probablemente reducirá su espacio de posibilidades tanto como sea posible.

259
00:14:17,200 --> 00:14:19,280
Y aunque aquí solo he estado hablando de la primera

260
00:14:19,280 --> 00:14:21,680
suposición, ocurre lo mismo con las siguientes suposiciones.

261
00:14:21,680 --> 00:14:25,403
Por ejemplo, después de ver algún patrón en esa primera suposición, que lo restringiría

262
00:14:25,403 --> 00:14:28,703
a un número menor de palabras posibles en función de lo que coincide con eso,

263
00:14:28,703 --> 00:14:32,300
simplemente juega el mismo juego con respecto a ese conjunto más pequeño de palabras.

264
00:14:32,300 --> 00:14:36,549
Para una segunda suposición propuesta, observamos la distribución de todos los

265
00:14:36,549 --> 00:14:41,014
patrones que podrían ocurrir a partir de ese conjunto más restringido de palabras,

266
00:14:41,014 --> 00:14:45,480
buscamos entre las 13.000 posibilidades y encontramos la que maximiza esa entropía.

267
00:14:45,480 --> 00:14:48,217
Para mostrarles cómo funciona esto en acción, permítanme

268
00:14:48,217 --> 00:14:51,146
mostrarles una pequeña variante de Wurtele que escribí y que

269
00:14:51,146 --> 00:14:54,460
muestra los aspectos más destacados de este análisis en los márgenes.

270
00:14:54,460 --> 00:14:57,213
Después de hacer todos los cálculos de entropía, aquí a la

271
00:14:57,213 --> 00:15:00,340
derecha nos muestra cuáles tienen la información esperada más alta.

272
00:15:00,340 --> 00:15:05,568
Resulta que la respuesta principal, al menos por el momento, lo refinaremos

273
00:15:05,568 --> 00:15:11,140
más adelante, es Tares, que significa, por supuesto, arveja, la arveja más común.

274
00:15:11,140 --> 00:15:14,315
Cada vez que hacemos una suposición aquí, donde tal vez ignoro sus

275
00:15:14,315 --> 00:15:17,775
recomendaciones y elijo slate, porque me gusta slate, podemos ver cuánta

276
00:15:17,775 --> 00:15:21,235
información esperada tenía, pero luego, a la derecha de la palabra aquí,

277
00:15:21,235 --> 00:15:24,980
nos muestra cuánta información real que obtuvimos, dado este patrón particular.

278
00:15:24,980 --> 00:15:27,932
Así que aquí parece que tuvimos un poco de mala suerte, se esperaba que obtuviéramos 5.

279
00:15:27,932 --> 00:15:30,660
8, pero obtuvimos algo con menos que eso.

280
00:15:30,660 --> 00:15:33,420
Y luego, en el lado izquierdo, aquí nos muestra todas las diferentes

281
00:15:33,420 --> 00:15:35,860
palabras posibles según el lugar donde nos encontramos ahora.

282
00:15:35,860 --> 00:15:38,452
Las barras azules nos dicen qué tan probable cree que es cada

283
00:15:38,452 --> 00:15:41,296
palabra, por lo que por el momento suponemos que cada palabra tiene

284
00:15:41,296 --> 00:15:44,140
la misma probabilidad de ocurrir, pero lo refinaremos en un momento.

285
00:15:44,140 --> 00:15:48,136
Y luego esta medida de incertidumbre nos dice la entropía de esta distribución entre

286
00:15:48,136 --> 00:15:52,038
las palabras posibles, que ahora mismo, debido a que es una distribución uniforme,

287
00:15:52,038 --> 00:15:55,940
es solo una forma innecesariamente complicada de contar el número de posibilidades.

288
00:15:55,940 --> 00:15:59,343
Por ejemplo, si tuviéramos que elevar 2 a la potencia de 13.

289
00:15:59,343 --> 00:16:02,700
66, eso debería rondar las 13.000 posibilidades.

290
00:16:02,700 --> 00:16:06,780
Estoy un poco fuera de lugar aquí, pero sólo porque no muestro todos los decimales.

291
00:16:06,780 --> 00:16:09,736
Por el momento, esto puede parecer redundante y complicar demasiado

292
00:16:09,736 --> 00:16:12,780
las cosas, pero verá por qué es útil tener ambos números en un minuto.

293
00:16:12,780 --> 00:16:16,341
Así que aquí parece que sugiere que la entropía más alta para nuestra

294
00:16:16,341 --> 00:16:19,700
segunda suposición es Ramen, que nuevamente no parece una palabra.

295
00:16:19,700 --> 00:16:25,660
Entonces, para tener autoridad moral aquí, seguiré adelante y escribiré Rains.

296
00:16:25,660 --> 00:16:27,540
Y nuevamente parece que tuvimos un poco de mala suerte.

297
00:16:27,540 --> 00:16:28,872
Esperábamos 4.

298
00:16:28,872 --> 00:16:30,556
3 bits y solo tenemos 3.

299
00:16:30,556 --> 00:16:32,100
39 bits de información.

300
00:16:32,100 --> 00:16:35,060
Eso nos lleva a 55 posibilidades.

301
00:16:35,060 --> 00:16:37,741
Y aquí tal vez me quede con lo que sugiere, que

302
00:16:37,741 --> 00:16:40,200
es combo, sea lo que sea que eso signifique.

303
00:16:40,200 --> 00:16:43,300
Y está bien, esta es en realidad una buena oportunidad para resolver un rompecabezas.

304
00:16:43,300 --> 00:16:45,718
Nos dice que este patrón nos da 4.

305
00:16:45,718 --> 00:16:47,020
7 bits de información.

306
00:16:47,020 --> 00:16:50,990
Pero a la izquierda, antes de que veamos ese patrón, había 5.

307
00:16:50,990 --> 00:16:52,400
78 bits de incertidumbre.

308
00:16:52,400 --> 00:16:56,860
Entonces, a modo de prueba, ¿qué significa eso sobre el número de posibilidades restantes?

309
00:16:56,860 --> 00:17:01,062
Bueno, significa que estamos reducidos a un poco de incertidumbre,

310
00:17:01,062 --> 00:17:04,700
que es lo mismo que decir que hay dos respuestas posibles.

311
00:17:04,700 --> 00:17:06,520
Es una elección 50-50.

312
00:17:06,520 --> 00:17:08,769
Y a partir de aquí, porque tú y yo sabemos qué palabras

313
00:17:08,769 --> 00:17:11,220
son más comunes, sabemos que la respuesta debería ser abismo.

314
00:17:11,220 --> 00:17:13,540
Pero tal como está escrito ahora, el programa no lo sabe.

315
00:17:13,540 --> 00:17:17,028
Así que sigue adelante, tratando de obtener tanta información como

316
00:17:17,028 --> 00:17:20,360
puede, hasta que sólo queda una posibilidad, y luego la adivina.

317
00:17:20,360 --> 00:17:22,700
Obviamente necesitamos una mejor estrategia para el final del juego.

318
00:17:22,700 --> 00:17:26,720
Pero digamos que llamamos a esta versión uno de nuestro solucionador de

319
00:17:26,720 --> 00:17:30,740
palabras y luego ejecutamos algunas simulaciones para ver cómo funciona.

320
00:17:30,740 --> 00:17:34,240
Entonces, la forma en que esto funciona es jugando todos los juegos de palabras posibles.

321
00:17:34,240 --> 00:17:38,780
Está repasando todas esas 2315 palabras que son las respuestas reales.

322
00:17:38,780 --> 00:17:41,340
Básicamente se trata de utilizarlo como un conjunto de pruebas.

323
00:17:41,340 --> 00:17:44,336
Y con este método ingenuo de no considerar qué tan común es

324
00:17:44,336 --> 00:17:47,383
una palabra y simplemente tratar de maximizar la información

325
00:17:47,383 --> 00:17:50,480
en cada paso del camino, hasta llegar a una y sólo una opción.

326
00:17:50,480 --> 00:17:54,912
Al final de la simulación, la puntuación media resulta ser de aproximadamente 4.

327
00:17:54,912 --> 00:17:55,100
124.

328
00:17:55,100 --> 00:17:59,780
Lo cual no está mal, para ser honesto, esperaba hacerlo peor.

329
00:17:59,780 --> 00:18:03,040
Pero la gente que juega wordle te dirá que normalmente lo consiguen en 4.

330
00:18:03,040 --> 00:18:05,260
El verdadero desafío es conseguir tantos en 3 como puedas.

331
00:18:05,260 --> 00:18:08,920
Es un salto bastante grande entre la puntuación de 4 y la puntuación de 3.

332
00:18:08,920 --> 00:18:15,972
Lo obvio aquí es incorporar de alguna manera si una

333
00:18:15,972 --> 00:18:23,160
palabra es común o no, y cómo lo hacemos exactamente.

334
00:18:23,160 --> 00:18:25,655
La forma en que lo acerqué es obtener una lista de las

335
00:18:25,655 --> 00:18:28,560
frecuencias relativas de todas las palabras en el idioma inglés.

336
00:18:28,560 --> 00:18:31,957
Y acabo de utilizar la función de datos de frecuencia de palabras de Mathematica,

337
00:18:31,957 --> 00:18:35,520
que a su vez se extrae del conjunto de datos públicos Ngram en inglés de Google Books.

338
00:18:35,520 --> 00:18:37,798
Y es divertido verlo, por ejemplo, si lo clasificamos

339
00:18:37,798 --> 00:18:40,120
desde las palabras más comunes hasta las menos comunes.

340
00:18:40,120 --> 00:18:43,740
Evidentemente estas son las palabras de cinco letras más comunes en el idioma inglés.

341
00:18:43,740 --> 00:18:46,480
O mejor dicho, este es el octavo más común.

342
00:18:46,480 --> 00:18:49,440
Primero es cuál, después está ahí y ahí.

343
00:18:49,440 --> 00:18:52,658
Primero en sí no es primero, sino noveno, y tiene sentido que estas

344
00:18:52,658 --> 00:18:55,734
otras palabras puedan aparecer con más frecuencia, donde las que

345
00:18:55,734 --> 00:18:59,000
siguen a primero son después, dónde, y que son un poco menos comunes.

346
00:18:59,000 --> 00:19:02,916
Ahora bien, al utilizar estos datos para modelar la probabilidad de que cada una de

347
00:19:02,916 --> 00:19:07,020
estas palabras sea la respuesta final, no debería ser sólo proporcional a la frecuencia.

348
00:19:07,020 --> 00:19:09,596
Por ejemplo, a la que se le da una puntuación de 0.

349
00:19:09,596 --> 00:19:12,332
002 en este conjunto de datos, mientras que la palabra trenza

350
00:19:12,332 --> 00:19:15,200
es, en cierto sentido, aproximadamente 1000 veces menos probable.

351
00:19:15,200 --> 00:19:17,507
Pero ambas son palabras bastante comunes que casi

352
00:19:17,507 --> 00:19:19,400
con seguridad vale la pena considerarlas.

353
00:19:19,400 --> 00:19:21,900
Por eso queremos más un límite binario.

354
00:19:21,900 --> 00:19:26,197
La forma en que lo hice es imaginar tomar toda esta lista ordenada de palabras,

355
00:19:26,197 --> 00:19:30,388
y luego organizarla en un eje x, y luego aplicar la función sigmoidea, que es

356
00:19:30,388 --> 00:19:34,578
la forma estándar de tener una función cuya salida es básicamente binaria, es

357
00:19:34,578 --> 00:19:38,500
0 o 1, pero hay un suavizado intermedio para esa región de incertidumbre.

358
00:19:38,500 --> 00:19:43,801
Básicamente, la probabilidad que estoy asignando a cada palabra de estar en la lista

359
00:19:43,801 --> 00:19:49,165
final será el valor de la función sigmoidea arriba dondequiera que se encuentre en el

360
00:19:49,165 --> 00:19:49,540
eje x.

361
00:19:49,540 --> 00:19:54,080
Ahora bien, obviamente, esto depende de algunos parámetros, por ejemplo, qué tan ancho

362
00:19:54,080 --> 00:19:58,202
es el espacio en el eje x que ocupan esas palabras determina qué tan gradual o

363
00:19:58,202 --> 00:20:02,794
abruptamente bajamos de 1 a 0, y dónde las ubicamos de izquierda a derecha determina el

364
00:20:02,794 --> 00:20:03,160
límite.

365
00:20:03,160 --> 00:20:05,250
Para ser honesto, la forma en que hice esto fue

366
00:20:05,250 --> 00:20:07,340
simplemente lamerme el dedo y pegarlo al viento.

367
00:20:07,340 --> 00:20:10,650
Revisé la lista ordenada y traté de encontrar una ventana donde,

368
00:20:10,650 --> 00:20:14,012
cuando la miré, pensé que era más probable que aproximadamente la

369
00:20:14,012 --> 00:20:17,680
mitad de estas palabras fueran la respuesta final, y la usé como límite.

370
00:20:17,680 --> 00:20:20,959
Una vez que tenemos una distribución como esta entre las palabras, nos da

371
00:20:20,959 --> 00:20:24,460
otra situación en la que la entropía se convierte en una medida realmente útil.

372
00:20:24,460 --> 00:20:27,494
Por ejemplo, digamos que estamos jugando y comenzamos con mis

373
00:20:27,494 --> 00:20:30,480
viejos abridores, que eran plumas y clavos, y terminamos con

374
00:20:30,480 --> 00:20:33,760
una situación en la que hay cuatro palabras posibles que coinciden.

375
00:20:33,760 --> 00:20:36,440
Y digamos que los consideramos todos igualmente probables.

376
00:20:36,440 --> 00:20:40,000
Déjame preguntarte, ¿cuál es la entropía de esta distribución?

377
00:20:40,000 --> 00:20:45,400
Bueno, la información asociada a cada una de estas posibilidades va a

378
00:20:45,400 --> 00:20:50,800
ser el logaritmo en base 2 de 4, ya que cada una es 1 y 4, y eso es 2.

379
00:20:50,800 --> 00:20:52,780
Dos bits de información, cuatro posibilidades.

380
00:20:52,780 --> 00:20:54,360
Todo muy bien y bueno.

381
00:20:54,360 --> 00:20:58,320
Pero ¿y si te dijera que en realidad hay más de cuatro coincidencias?

382
00:20:58,320 --> 00:21:02,600
En realidad, cuando miramos la lista completa de palabras, hay 16 palabras que coinciden.

383
00:21:02,600 --> 00:21:05,374
Pero supongamos que nuestro modelo asigna una probabilidad

384
00:21:05,374 --> 00:21:08,289
realmente baja a que esas otras 12 palabras sean realmente la

385
00:21:08,289 --> 00:21:11,440
respuesta final, algo así como 1 entre 1000 porque son muy oscuras.

386
00:21:11,440 --> 00:21:15,480
Ahora déjame preguntarte, ¿cuál es la entropía de esta distribución?

387
00:21:15,480 --> 00:21:19,249
Si la entropía simplemente midiera el número de coincidencias aquí, entonces

388
00:21:19,249 --> 00:21:22,773
se podría esperar que fuera algo así como el logaritmo en base 2 de 16,

389
00:21:22,773 --> 00:21:26,200
que sería 4, dos bits más de incertidumbre que los que teníamos antes.

390
00:21:26,200 --> 00:21:30,320
Pero, por supuesto, la incertidumbre real no es tan diferente de la que teníamos antes.

391
00:21:30,320 --> 00:21:34,136
El hecho de que existan estas 12 palabras realmente oscuras no significa que

392
00:21:34,136 --> 00:21:38,200
sería mucho más sorprendente saber que la respuesta final es encanto, por ejemplo.

393
00:21:38,200 --> 00:21:41,857
Entonces, cuando realmente haces el cálculo aquí y sumas la probabilidad de cada

394
00:21:41,857 --> 00:21:45,514
ocurrencia multiplicada por la información correspondiente, lo que obtienes es 2.

395
00:21:45,514 --> 00:21:45,960
11 bits.

396
00:21:45,960 --> 00:21:49,776
Solo digo que son básicamente dos bits, básicamente esas cuatro posibilidades,

397
00:21:49,776 --> 00:21:53,448
pero hay un poco más de incertidumbre debido a todos esos eventos altamente

398
00:21:53,448 --> 00:21:57,120
improbables, aunque si los aprendieras, obtendrías un montón de información.

399
00:21:57,120 --> 00:21:59,440
Entonces, alejarnos, esto es parte de lo que hace de Wordle

400
00:21:59,440 --> 00:22:01,800
un buen ejemplo para una lección de teoría de la información.

401
00:22:01,800 --> 00:22:05,280
Tenemos estas dos aplicaciones de sentimiento distintas para la entropía.

402
00:22:05,280 --> 00:22:08,977
El primero nos dice cuál es la información esperada que obtendremos

403
00:22:08,977 --> 00:22:12,728
de una suposición determinada, y el segundo dice si podemos medir la

404
00:22:12,728 --> 00:22:16,480
incertidumbre restante entre todas las palabras que tenemos posibles.

405
00:22:16,480 --> 00:22:19,166
Y debo enfatizar que, en el primer caso en el que observamos la

406
00:22:19,166 --> 00:22:21,852
información esperada de una suposición, una vez que tenemos una

407
00:22:21,852 --> 00:22:25,000
ponderación desigual de las palabras, eso afecta el cálculo de la entropía.

408
00:22:25,000 --> 00:22:27,887
Por ejemplo, permítanme mencionar el mismo caso que vimos

409
00:22:27,887 --> 00:22:31,024
anteriormente de la distribución asociada con Weary, pero esta

410
00:22:31,024 --> 00:22:34,560
vez usando una distribución no uniforme en todas las palabras posibles.

411
00:22:34,560 --> 00:22:39,360
Déjame ver si puedo encontrar una parte aquí que lo ilustre bastante bien.

412
00:22:39,360 --> 00:22:42,480
Bien, aquí esto está bastante bien.

413
00:22:42,480 --> 00:22:46,080
Aquí tenemos dos patrones adyacentes que son igualmente probables, pero

414
00:22:46,080 --> 00:22:49,480
nos dicen que uno de ellos tiene 32 palabras posibles que coinciden.

415
00:22:49,480 --> 00:22:52,486
Y si comprobamos cuáles son, estas son esas 32, que son

416
00:22:52,486 --> 00:22:55,600
palabras muy improbables cuando las examinas con la vista.

417
00:22:55,600 --> 00:22:59,250
Es difícil encontrar respuestas que parezcan plausibles, tal vez gritos, pero

418
00:22:59,250 --> 00:23:02,806
si miramos el patrón vecino en la distribución, que se considera casi igual

419
00:23:02,806 --> 00:23:06,316
de probable, nos dicen que solo tiene 8 coincidencias posibles, por lo que

420
00:23:06,316 --> 00:23:09,920
una cuarta parte de Hay muchas coincidencias, pero es casi igual de probable.

421
00:23:09,920 --> 00:23:12,520
Y cuando analizamos esas coincidencias, podemos ver por qué.

422
00:23:12,520 --> 00:23:17,840
Algunas de estas son respuestas realmente plausibles, como timbre, ira o golpes.

423
00:23:17,840 --> 00:23:21,781
Para ilustrar cómo incorporamos todo eso, permítanme mostrar aquí la versión 2 del

424
00:23:21,781 --> 00:23:25,960
Wordlebot, y hay dos o tres diferencias principales con respecto a la primera que vimos.

425
00:23:25,960 --> 00:23:29,136
En primer lugar, como acabo de decir, la forma en que calculamos

426
00:23:29,136 --> 00:23:32,605
estas entropías, estos valores esperados de información, ahora utiliza

427
00:23:32,605 --> 00:23:35,830
distribuciones más refinadas entre los patrones que incorporan la

428
00:23:35,830 --> 00:23:39,300
probabilidad de que una palabra determinada sea realmente la respuesta.

429
00:23:39,300 --> 00:23:41,707
Da la casualidad de que las lágrimas siguen siendo el

430
00:23:41,707 --> 00:23:44,160
número 1, aunque las siguientes son un poco diferentes.

431
00:23:44,160 --> 00:23:46,832
En segundo lugar, cuando clasifique sus mejores opciones, ahora

432
00:23:46,832 --> 00:23:49,589
mantendrá un modelo de la probabilidad de que cada palabra sea la

433
00:23:49,589 --> 00:23:52,512
respuesta real, y lo incorporará en su decisión, lo cual es más fácil

434
00:23:52,512 --> 00:23:55,520
de ver una vez que tengamos algunas conjeturas sobre la respuesta. mesa.

435
00:23:55,520 --> 00:23:58,133
Nuevamente, ignorando su recomendación porque no

436
00:23:58,133 --> 00:24:01,120
podemos dejar que las máquinas gobiernen nuestras vidas.

437
00:24:01,120 --> 00:24:03,939
Y supongo que debería mencionar otra cosa diferente aquí a la

438
00:24:03,939 --> 00:24:06,850
izquierda, ese valor de incertidumbre, esa cantidad de bits, ya

439
00:24:06,850 --> 00:24:10,080
no es simplemente redundante con la cantidad de coincidencias posibles.

440
00:24:10,080 --> 00:24:13,489
Ahora si lo levantamos y calculamos 2 elevado a 8.

441
00:24:13,489 --> 00:24:17,632
02, que está un poco por encima de 256, supongo que 259, lo que dice

442
00:24:17,632 --> 00:24:21,654
es que aunque hay un total de 526 palabras que realmente coinciden

443
00:24:21,654 --> 00:24:25,437
con este patrón, la cantidad de incertidumbre que tiene es más

444
00:24:25,437 --> 00:24:29,760
parecida a la que sería si hubiera 259 igualmente probables. resultados.

445
00:24:29,760 --> 00:24:31,100
Puedes pensar en ello así.

446
00:24:31,100 --> 00:24:34,470
Sabe que borx no es la respuesta, lo mismo ocurre con yorts, zorl y

447
00:24:34,470 --> 00:24:37,840
zorus, por lo que es un poco menos incierto que en el caso anterior.

448
00:24:37,840 --> 00:24:40,220
Este número de bits será menor.

449
00:24:40,220 --> 00:24:44,486
Y si sigo jugando, lo refinaré con un par de suposiciones

450
00:24:44,486 --> 00:24:48,680
que son apropiadas para lo que me gustaría explicar aquí.

451
00:24:48,680 --> 00:24:51,322
En la cuarta suposición, si observa sus mejores opciones, podrá

452
00:24:51,322 --> 00:24:53,800
ver que ya no se trata simplemente de maximizar la entropía.

453
00:24:53,800 --> 00:24:57,218
Entonces, en este punto, técnicamente hay siete posibilidades, pero las

454
00:24:57,218 --> 00:25:00,780
únicas con posibilidades significativas son los dormitorios y las palabras.

455
00:25:00,780 --> 00:25:04,218
Y puede ver que se clasifica al elegir ambos por encima de todos estos

456
00:25:04,218 --> 00:25:07,560
otros valores, que estrictamente hablando brindarían más información.

457
00:25:07,560 --> 00:25:11,026
La primera vez que hice esto, simplemente sumé estos dos números para medir la

458
00:25:11,026 --> 00:25:14,580
calidad de cada suposición, lo que en realidad funcionó mejor de lo que imaginas.

459
00:25:14,580 --> 00:25:17,096
Pero realmente no lo sentí sistemático, y estoy seguro de que hay

460
00:25:17,096 --> 00:25:19,880
otros enfoques que la gente podría adoptar, pero este es el que encontré.

461
00:25:19,880 --> 00:25:23,874
Si estamos considerando la posibilidad de una próxima suposición, como en este caso

462
00:25:23,874 --> 00:25:28,059
palabras, lo que realmente nos importa es la puntuación esperada de nuestro juego si lo

463
00:25:28,059 --> 00:25:28,440
hacemos.

464
00:25:28,440 --> 00:25:32,235
Y para calcular esa puntuación esperada, decimos cuál es la probabilidad de

465
00:25:32,235 --> 00:25:36,080
que las palabras sean la respuesta real, que en este momento describe el 58%.

466
00:25:36,080 --> 00:25:40,400
Decimos que con un 58% de posibilidades, nuestra puntuación en este juego sería 4.

467
00:25:40,400 --> 00:25:46,240
Y luego, con la probabilidad de 1 menos ese 58%, nuestra puntuación será mayor que ese 4.

468
00:25:46,240 --> 00:25:49,555
No sabemos cuánto más, pero podemos estimarlo en función de cuánta

469
00:25:49,555 --> 00:25:52,920
incertidumbre probablemente habrá una vez que lleguemos a ese punto.

470
00:25:52,920 --> 00:25:55,227
En concreto, de momento hay 1.

471
00:25:55,227 --> 00:25:56,600
44 bits de incertidumbre.

472
00:25:56,600 --> 00:26:01,131
Si adivinamos palabras, nos dice que la información esperada que obtendremos es 1.

473
00:26:01,131 --> 00:26:01,560
27 bits.

474
00:26:01,560 --> 00:26:04,970
Entonces, si adivinamos palabras, esta diferencia representa cuánta

475
00:26:04,970 --> 00:26:08,280
incertidumbre es probable que nos quede después de que eso suceda.

476
00:26:08,280 --> 00:26:11,168
Lo que necesitamos es algún tipo de función, a la que aquí llamo

477
00:26:11,168 --> 00:26:13,880
f, que asocie esta incertidumbre con una puntuación esperada.

478
00:26:13,880 --> 00:26:18,303
Y la forma en que lo hicimos fue simplemente trazar un montón de datos de juegos

479
00:26:18,303 --> 00:26:22,616
anteriores basados en la versión 1 del bot para decir cuál fue el puntaje real

480
00:26:22,616 --> 00:26:27,040
después de varios puntos con ciertas cantidades de incertidumbre muy mensurables.

481
00:26:27,040 --> 00:26:29,078
Por ejemplo, estos puntos de datos aquí que se

482
00:26:29,078 --> 00:26:31,073
encuentran por encima de un valor cercano a 8.

483
00:26:31,073 --> 00:26:35,426
Aproximadamente 7, dicen para algunos juegos después de un punto en el que había 8.

484
00:26:35,426 --> 00:26:39,340
7 bits de incertidumbre, fueron necesarias dos conjeturas para obtener la respuesta final.

485
00:26:39,340 --> 00:26:41,242
Para otros juegos fueron necesarias tres conjeturas,

486
00:26:41,242 --> 00:26:43,180
para otros juegos fueron necesarias cuatro conjeturas.

487
00:26:43,180 --> 00:26:46,936
Si aquí nos desplazamos hacia la izquierda, todos los puntos sobre cero dicen que

488
00:26:46,936 --> 00:26:50,830
siempre que haya cero bits de incertidumbre, es decir, que solo hay una posibilidad,

489
00:26:50,830 --> 00:26:54,312
entonces el número de conjeturas requeridas es siempre solo uno, lo cual es

490
00:26:54,312 --> 00:26:55,000
tranquilizador.

491
00:26:55,000 --> 00:26:57,964
Cada vez que había un poco de incertidumbre, lo que significaba

492
00:26:57,964 --> 00:27:00,790
que esencialmente se reducía a dos posibilidades, a veces se

493
00:27:00,790 --> 00:27:03,940
requería una conjetura más, a veces se requerían dos conjeturas más.

494
00:27:03,940 --> 00:27:05,980
Y así sucesivamente aquí.

495
00:27:05,980 --> 00:27:08,681
Quizás una forma un poco más sencilla de visualizar

496
00:27:08,681 --> 00:27:11,020
estos datos sea agruparlos y tomar promedios.

497
00:27:11,020 --> 00:27:16,537
Por ejemplo, esta barra dice que entre todos los puntos en los que teníamos un poco de

498
00:27:16,537 --> 00:27:22,181
incertidumbre, en promedio el número de nuevas conjeturas requeridas fue aproximadamente

499
00:27:22,181 --> 00:27:22,308
1.

500
00:27:22,308 --> 00:27:22,420
5.

501
00:27:22,420 --> 00:27:25,712
Y la barra de aquí dice que entre todos los diferentes juegos donde en

502
00:27:25,712 --> 00:27:29,005
algún momento la incertidumbre estuvo un poco por encima de los cuatro

503
00:27:29,005 --> 00:27:32,483
bits, lo que es como reducirla a 16 posibilidades diferentes, entonces, en

504
00:27:32,483 --> 00:27:36,240
promedio, requiere un poco más de dos conjeturas a partir de ese punto. adelante.

505
00:27:36,240 --> 00:27:38,142
Y a partir de aquí simplemente hice una regresión para

506
00:27:38,142 --> 00:27:40,080
ajustarme a una función que parecía razonable para esto.

507
00:27:40,080 --> 00:27:44,819
Y recuerde que el objetivo de hacer todo esto es que podamos cuantificar esta intuición

508
00:27:44,819 --> 00:27:49,235
de que cuanta más información obtengamos de una palabra, menor será la puntuación

509
00:27:49,235 --> 00:27:49,720
esperada.

510
00:27:49,720 --> 00:27:51,043
Entonces con esto como versión 2.

511
00:27:51,043 --> 00:27:55,431
0, si volvemos atrás y ejecutamos el mismo conjunto de simulaciones,

512
00:27:55,431 --> 00:27:59,820
haciéndolo jugar contra las 2315 respuestas posibles, ¿cómo funciona?

513
00:27:59,820 --> 00:28:01,982
Bueno, a diferencia de nuestra primera versión, es

514
00:28:01,982 --> 00:28:04,060
definitivamente mejor, lo cual es tranquilizador.

515
00:28:04,060 --> 00:28:06,284
Todo dicho y hecho, la media ronda los 3.

516
00:28:06,284 --> 00:28:09,471
6, aunque a diferencia de la primera versión hay un par de

517
00:28:09,471 --> 00:28:12,820
veces que pierde y requiere más de seis en esta circunstancia.

518
00:28:12,820 --> 00:28:15,819
Presumiblemente porque hay momentos en los que se trata

519
00:28:15,819 --> 00:28:18,980
de buscar el objetivo en lugar de maximizar la información.

520
00:28:18,980 --> 00:28:22,022
Entonces, ¿podemos hacerlo mejor que 3?

521
00:28:22,022 --> 00:28:22,140
6?

522
00:28:22,140 --> 00:28:23,260
Definitivamente podemos.

523
00:28:23,260 --> 00:28:26,709
Ahora dije al principio que es muy divertido intentar no incorporar la lista

524
00:28:26,709 --> 00:28:29,980
verdadera de respuestas de Wordle en la forma en que construye su modelo.

525
00:28:29,980 --> 00:28:35,043
Pero si lo incorporamos, el mejor rendimiento que pude obtener fue alrededor de 3.

526
00:28:35,043 --> 00:28:35,180
43.

527
00:28:35,180 --> 00:28:38,089
Entonces, si intentamos ser más sofisticados que simplemente usar datos

528
00:28:38,089 --> 00:28:40,957
de frecuencia de palabras para elegir esta distribución previa, este 3.

529
00:28:40,957 --> 00:28:43,618
43 probablemente da un máximo de lo buenos que podríamos llegar a

530
00:28:43,618 --> 00:28:46,360
ser con eso, o al menos de lo bueno que podría llegar a ser con eso.

531
00:28:46,360 --> 00:28:49,413
Ese mejor rendimiento esencialmente solo utiliza las ideas de las

532
00:28:49,413 --> 00:28:52,328
que he estado hablando aquí, pero va un poco más allá, como si

533
00:28:52,328 --> 00:28:55,660
buscara la información esperada dos pasos adelante en lugar de solo uno.

534
00:28:55,660 --> 00:28:58,167
Originalmente estaba planeando hablar más sobre eso,

535
00:28:58,167 --> 00:29:00,580
pero me doy cuenta de que ya hemos durado bastante.

536
00:29:00,580 --> 00:29:03,553
Lo único que diré es que después de hacer esta búsqueda de dos pasos y

537
00:29:03,553 --> 00:29:06,736
luego ejecutar un par de simulaciones de muestra en los mejores candidatos,

538
00:29:06,736 --> 00:29:09,500
hasta ahora al menos para mí parece que Crane es el mejor abridor.

539
00:29:09,500 --> 00:29:11,080
¿Quién lo hubiera adivinado?

540
00:29:11,080 --> 00:29:14,493
Además, si utiliza la lista de palabras verdaderas para determinar su espacio de

541
00:29:14,493 --> 00:29:18,160
posibilidades, entonces la incertidumbre con la que comienza es un poco más de 11 bits.

542
00:29:18,160 --> 00:29:22,395
Y resulta que, sólo a partir de una búsqueda de fuerza bruta, la máxima información

543
00:29:22,395 --> 00:29:26,580
esperada posible después de las dos primeras conjeturas es de alrededor de 10 bits.

544
00:29:26,580 --> 00:29:31,141
Lo que sugiere que en el mejor de los casos, después de tus dos primeras conjeturas,

545
00:29:31,141 --> 00:29:35,220
con un juego perfectamente óptimo, te quedarás con un poco de incertidumbre.

546
00:29:35,220 --> 00:29:37,400
Lo que es lo mismo que limitarse a dos posibles conjeturas.

547
00:29:37,400 --> 00:29:40,004
Así que creo que es justo y probablemente bastante conservador decir que

548
00:29:40,004 --> 00:29:42,574
nunca sería posible escribir un algoritmo que consiga este promedio tan

549
00:29:42,574 --> 00:29:45,036
bajo como 3, porque con las palabras disponibles, simplemente no hay

550
00:29:45,036 --> 00:29:47,712
espacio para obtener suficiente información después de sólo dos pasos para

551
00:29:47,712 --> 00:29:50,460
ser capaz de garantizar la respuesta en el tercer espacio cada vez sin falta.

