1
00:00:00,000 --> 00:00:03,069
El juego Wurdle se ha vuelto bastante viral en los últimos dos meses y, 

2
00:00:03,069 --> 00:00:06,095
como nunca desperdiciaré la oportunidad de una lección de matemáticas, 

3
00:00:06,095 --> 00:00:09,249
se me ocurre que este juego es un muy buen ejemplo central en una lección 

4
00:00:09,249 --> 00:00:12,660
sobre teoría de la información y, en particular, un tema conocido como entropía.

5
00:00:13,920 --> 00:00:16,549
Verá, como mucha gente, me quedé atrapado en el rompecabezas, 

6
00:00:16,549 --> 00:00:19,474
y como muchos programadores, también me quedé atrapado en el intento 

7
00:00:19,474 --> 00:00:22,740
de escribir un algoritmo que jugara el juego de la manera más óptima posible.

8
00:00:23,180 --> 00:00:25,757
Y lo que pensé que haría aquí es simplemente hablarles sobre 

9
00:00:25,757 --> 00:00:28,545
mi proceso y explicarles algunas de las matemáticas que implican, 

10
00:00:28,545 --> 00:00:31,080
ya que todo el algoritmo se centra en esta idea de entropía.

11
00:00:38,700 --> 00:00:41,640
Lo primero es lo primero, en caso de que no hayas oído hablar de él, ¿qué es Wurdle?

12
00:00:42,040 --> 00:00:45,161
Y para matar dos pájaros de un tiro mientras repasamos las reglas del juego, 

13
00:00:45,161 --> 00:00:47,594
permítanme también un avance de hacia dónde vamos con esto, 

14
00:00:47,594 --> 00:00:51,040
que es desarrollar un pequeño algoritmo que básicamente jugará el juego por nosotros.

15
00:00:51,360 --> 00:00:55,100
Aunque no he hecho el Wurdle de hoy, es el 4 de febrero y veremos cómo le va al robot.

16
00:00:55,480 --> 00:00:57,794
El objetivo de Wurdle es adivinar una palabra misteriosa de 

17
00:00:57,794 --> 00:01:00,340
cinco letras y tienes seis oportunidades diferentes para adivinar.

18
00:01:00,840 --> 00:01:04,379
Por ejemplo, mi robot Wurdle me sugiere que empiece con la grúa de adivinanzas.

19
00:01:05,180 --> 00:01:07,762
Cada vez que haces una suposición, obtienes información sobre 

20
00:01:07,762 --> 00:01:10,220
qué tan cerca está tu suposición de la respuesta verdadera.

21
00:01:10,920 --> 00:01:14,100
Aquí el cuadro gris me dice que no hay una C en la respuesta real.

22
00:01:14,520 --> 00:01:17,840
El cuadro amarillo me dice que hay una R, pero no está en esa posición.

23
00:01:18,240 --> 00:01:22,240
El cuadro verde me dice que la palabra secreta tiene una A y está en la tercera posición.

24
00:01:22,720 --> 00:01:24,580
Y luego no hay N y no hay E.

25
00:01:25,200 --> 00:01:27,340
Así que déjame entrar y decirle esa información al robot Wurdle.

26
00:01:27,340 --> 00:01:30,320
Empezamos con grúa, obtuvimos gris, amarillo, verde, gris, gris.

27
00:01:31,420 --> 00:01:33,755
No te preocupes por todos los datos que están mostrando ahora mismo, 

28
00:01:33,755 --> 00:01:34,940
te lo explicaré a su debido tiempo.

29
00:01:35,460 --> 00:01:38,820
Pero su principal sugerencia para nuestra segunda elección es un truco.

30
00:01:39,560 --> 00:01:42,155
Y tu suposición tiene que ser una palabra real de cinco letras, 

31
00:01:42,155 --> 00:01:45,400
pero como verás, es bastante liberal con lo que realmente te permitirá adivinar.

32
00:01:46,200 --> 00:01:47,440
En este caso, intentamos stick.

33
00:01:48,780 --> 00:01:50,180
Y bueno, la cosa pinta bastante bien.

34
00:01:50,260 --> 00:01:53,980
Pulsamos la S y la H, así conocemos las tres primeras letras, sabemos que hay una R.

35
00:01:53,980 --> 00:01:58,700
Y entonces será como SHA algo R, o SHA R algo.

36
00:01:59,620 --> 00:02:04,240
Y parece que el robot Wurdle sabe que solo hay dos posibilidades: fragmentar o filoso.

37
00:02:05,100 --> 00:02:07,293
Eso es una especie de volatilidad entre ellos en este momento, 

38
00:02:07,293 --> 00:02:10,080
así que supongo que probablemente solo porque es alfabético va con el fragmento.

39
00:02:11,220 --> 00:02:12,860
Qué hurra, es la respuesta real.

40
00:02:12,960 --> 00:02:13,780
Entonces lo tenemos en tres.

41
00:02:14,600 --> 00:02:17,362
Si te preguntas si eso es bueno, la forma en que escuché a 

42
00:02:17,362 --> 00:02:20,360
una persona decir que con Wurdle cuatro es par y tres es birdie.

43
00:02:20,680 --> 00:02:22,480
Lo cual creo que es una analogía bastante adecuada.

44
00:02:22,480 --> 00:02:25,377
Tienes que ser constante en tu juego para conseguir cuatro, 

45
00:02:25,377 --> 00:02:27,020
pero ciertamente no es una locura.

46
00:02:27,180 --> 00:02:29,920
Pero cuando lo obtienes en tres, se siente genial.

47
00:02:30,880 --> 00:02:33,319
Entonces, si estás dispuesto a hacerlo, lo que me gustaría hacer aquí es simplemente 

48
00:02:33,319 --> 00:02:35,759
hablar sobre mi proceso de pensamiento desde el principio sobre cómo abordo el robot 

49
00:02:35,759 --> 00:02:35,960
Wurdle.

50
00:02:36,480 --> 00:02:39,440
Y como dije, en realidad es una excusa para una lección de teoría de la información.

51
00:02:39,740 --> 00:02:42,820
El objetivo principal es explicar qué es información y qué es entropía.

52
00:02:48,220 --> 00:02:50,758
Lo primero que pensé al abordar esto fue observar las 

53
00:02:50,758 --> 00:02:53,720
frecuencias relativas de diferentes letras en el idioma inglés.

54
00:02:54,380 --> 00:02:56,624
Entonces pensé, bueno, ¿hay una suposición inicial o un par de 

55
00:02:56,624 --> 00:02:59,260
suposiciones iniciales que acierten muchas de estas letras más frecuentes?

56
00:02:59,960 --> 00:03:03,000
Y una que me gustaba mucho era hacer otras seguidas de uñas.

57
00:03:03,760 --> 00:03:06,553
La idea es que si tocas una letra, ya sabes, obtienes un verde o un amarillo, 

58
00:03:06,553 --> 00:03:07,520
eso siempre se siente bien.

59
00:03:07,520 --> 00:03:08,840
Parece que estás recibiendo información.

60
00:03:09,340 --> 00:03:12,484
Pero en estos casos, incluso si no aciertas y siempre aparecen grises, 

61
00:03:12,484 --> 00:03:15,230
eso te da mucha información ya que es bastante raro encontrar 

62
00:03:15,230 --> 00:03:17,400
una palabra que no tenga ninguna de estas letras.

63
00:03:18,140 --> 00:03:20,692
Pero aun así, esto no parece súper sistemático, porque, 

64
00:03:20,692 --> 00:03:23,200
por ejemplo, no tiene en cuenta el orden de las letras.

65
00:03:23,560 --> 00:03:25,300
¿Por qué escribir uñas cuando puedo escribir caracol?

66
00:03:26,080 --> 00:03:27,500
¿Es mejor tener esa S al final?

67
00:03:27,820 --> 00:03:28,680
No estoy realmente seguro.

68
00:03:29,240 --> 00:03:32,570
Ahora, un amigo mío dijo que le gustaba comenzar con la palabra cansado, 

69
00:03:32,570 --> 00:03:36,540
lo que me sorprendió un poco porque tiene algunas letras poco comunes como la W y la Y.

70
00:03:37,120 --> 00:03:39,000
Pero quién sabe, tal vez ese sea un mejor comienzo.

71
00:03:39,320 --> 00:03:41,932
¿Existe algún tipo de puntuación cuantitativa que podamos 

72
00:03:41,932 --> 00:03:44,320
dar para juzgar la calidad de una posible suposición?

73
00:03:45,340 --> 00:03:48,138
Ahora, para preparar la forma en que vamos a clasificar las posibles conjeturas, 

74
00:03:48,138 --> 00:03:51,212
retrocedamos y agreguemos un poco de claridad sobre cómo está configurado exactamente el 

75
00:03:51,212 --> 00:03:51,420
juego.

76
00:03:51,420 --> 00:03:54,583
Entonces, hay una lista de palabras que le permitirá ingresar y que se 

77
00:03:54,583 --> 00:03:57,880
consideran conjeturas válidas y que tiene aproximadamente 13,000 palabras.

78
00:03:58,320 --> 00:04:01,193
Pero cuando lo miras, hay muchas cosas realmente poco comunes, 

79
00:04:01,193 --> 00:04:04,843
cosas como una cabeza o Ali y ARG, el tipo de palabras que provocan discusiones 

80
00:04:04,843 --> 00:04:06,440
familiares en un juego de Scrabble.

81
00:04:06,960 --> 00:04:10,540
Pero la sensación del juego es que la respuesta siempre será una palabra bastante común.

82
00:04:10,960 --> 00:04:15,360
Y de hecho, hay otra lista de alrededor de 2300 palabras que son las posibles respuestas.

83
00:04:15,940 --> 00:04:18,645
Y esta es una lista seleccionada por humanos, creo que específicamente 

84
00:04:18,645 --> 00:04:21,160
por la novia del creador del juego, lo cual es bastante divertido.

85
00:04:21,820 --> 00:04:25,721
Pero lo que me gustaría hacer, nuestro desafío para este proyecto es ver si podemos 

86
00:04:25,721 --> 00:04:29,901
escribir un programa resolviendo Wordle que no incorpore conocimientos previos sobre esta 

87
00:04:29,901 --> 00:04:30,180
lista.

88
00:04:30,720 --> 00:04:32,680
Por un lado, hay muchas palabras de cinco letras 

89
00:04:32,680 --> 00:04:34,640
bastante comunes que no encontrarás en esa lista.

90
00:04:34,940 --> 00:04:38,159
Por lo tanto, sería mejor escribir un programa que sea un poco más resistente y 

91
00:04:38,159 --> 00:04:41,460
que pueda jugar con Wordle contra cualquiera, no solo contra el sitio web oficial.

92
00:04:41,920 --> 00:04:44,439
Y también la razón por la que sabemos cuál es esta lista de 

93
00:04:44,439 --> 00:04:47,000
posibles respuestas es porque es visible en el código fuente.

94
00:04:47,000 --> 00:04:50,130
Pero la forma en que es visible en el código fuente es en el 

95
00:04:50,130 --> 00:04:53,260
orden específico en el que aparecen las respuestas día a día.

96
00:04:53,260 --> 00:04:55,840
Así que siempre puedes buscar cuál será la respuesta de mañana.

97
00:04:56,420 --> 00:04:58,880
Claramente, en cierto sentido usar la lista es hacer trampa.

98
00:04:59,100 --> 00:05:01,955
Y lo que hace que el rompecabezas sea más interesante y una lección de 

99
00:05:01,955 --> 00:05:05,051
teoría de la información más rica es utilizar algunos datos más universales, 

100
00:05:05,051 --> 00:05:07,424
como las frecuencias relativas de las palabras en general, 

101
00:05:07,424 --> 00:05:10,440
para capturar esta intuición de tener preferencia por palabras más comunes.

102
00:05:11,600 --> 00:05:15,900
Entonces, de estas 13.000 posibilidades, ¿cómo deberíamos elegir la suposición inicial?

103
00:05:16,400 --> 00:05:19,780
Por ejemplo, si mi amigo me propone cansancio, ¿cómo debemos analizar su calidad?

104
00:05:20,520 --> 00:05:23,978
Bueno, la razón por la que dijo que le gusta esa improbable W es que le 

105
00:05:23,978 --> 00:05:27,340
gusta la naturaleza remota de lo bien que se siente si aciertas esa W.

106
00:05:27,920 --> 00:05:30,972
Por ejemplo, si el primer patrón revelado fue algo como este, 

107
00:05:30,972 --> 00:05:34,861
entonces resulta que solo hay 58 palabras en este léxico gigante que coinciden 

108
00:05:34,861 --> 00:05:35,600
con ese patrón.

109
00:05:36,060 --> 00:05:38,400
Entonces esa es una gran reducción de 13.000.

110
00:05:38,780 --> 00:05:40,878
Pero la otra cara de la moneda, por supuesto, es 

111
00:05:40,878 --> 00:05:43,020
que es muy poco común obtener un patrón como este.

112
00:05:43,020 --> 00:05:47,054
Específicamente, si cada palabra tuviera la misma probabilidad de ser la respuesta, 

113
00:05:47,054 --> 00:05:51,040
la probabilidad de encontrar este patrón sería 58 dividido por alrededor de 13.000.

114
00:05:51,580 --> 00:05:53,600
Por supuesto, no es igualmente probable que sean respuestas.

115
00:05:53,720 --> 00:05:56,220
La mayoría de ellas son palabras muy oscuras e incluso cuestionables.

116
00:05:56,600 --> 00:05:58,633
Pero al menos para nuestra primera aproximación a todo esto, 

117
00:05:58,633 --> 00:06:01,600
supongamos que todas son igualmente probables y luego refinemos eso un poco más adelante.

118
00:06:02,020 --> 00:06:04,306
La cuestión es que, por su propia naturaleza, es poco 

119
00:06:04,306 --> 00:06:06,720
probable que se produzca un patrón con mucha información.

120
00:06:07,280 --> 00:06:10,800
De hecho, lo que significa ser informativo es que es poco probable.

121
00:06:11,719 --> 00:06:16,223
Un patrón mucho más probable de ver con esta apertura sería algo como esto, 

122
00:06:16,223 --> 00:06:18,120
donde por supuesto no hay una W.

123
00:06:18,240 --> 00:06:21,400
Tal vez haya una E, y tal vez no haya una A, no haya una R, no haya una Y.

124
00:06:22,080 --> 00:06:24,560
En este caso, hay 1400 coincidencias posibles.

125
00:06:25,080 --> 00:06:27,561
Si todas fueran igualmente probables, resulta que hay una 

126
00:06:27,561 --> 00:06:30,600
probabilidad de alrededor del 11% de que este sea el patrón que verías.

127
00:06:30,900 --> 00:06:33,340
De modo que los resultados más probables son también los menos informativos.

128
00:06:34,240 --> 00:06:37,642
Para obtener una visión más global, permítame mostrarle la distribución 

129
00:06:37,642 --> 00:06:41,140
completa de probabilidades en todos los diferentes patrones que pueda ver.

130
00:06:41,740 --> 00:06:45,289
Entonces, cada barra que estás viendo corresponde a un posible patrón de 

131
00:06:45,289 --> 00:06:48,790
colores que podría revelarse, de los cuales hay de 3 a 5 posibilidades, 

132
00:06:48,790 --> 00:06:52,340
y están organizados de izquierda a derecha, del más común al menos común.

133
00:06:52,920 --> 00:06:56,000
Entonces, la posibilidad más común aquí es que obtengas todos los grises.

134
00:06:56,100 --> 00:06:58,120
Esto sucede aproximadamente el 14% del tiempo.

135
00:06:58,580 --> 00:07:02,082
Y lo que esperas cuando haces una suposición es terminar en algún 

136
00:07:02,082 --> 00:07:05,690
lugar de esta larga cola, como aquí donde solo hay 18 posibilidades 

137
00:07:05,690 --> 00:07:09,140
para lo que coincide con este patrón que evidentemente se ve así.

138
00:07:09,920 --> 00:07:12,155
O si nos aventuramos un poco más hacia la izquierda, 

139
00:07:12,155 --> 00:07:13,800
ya sabes, tal vez lleguemos hasta aquí.

140
00:07:14,940 --> 00:07:16,180
Bien, aquí tienes un buen rompecabezas.

141
00:07:16,540 --> 00:07:19,652
¿Cuáles son las tres palabras en inglés que comienzan con W, 

142
00:07:19,652 --> 00:07:22,000
terminan con Y y tienen una R en alguna parte?

143
00:07:22,480 --> 00:07:26,800
Resulta que las respuestas son, veamos, prolijas, llenas de gusanos e irónicas.

144
00:07:27,500 --> 00:07:30,657
Entonces, para juzgar qué tan buena es esta palabra en general, 

145
00:07:30,657 --> 00:07:34,703
queremos algún tipo de medida de la cantidad esperada de información que obtendrá 

146
00:07:34,703 --> 00:07:35,740
de esta distribución.

147
00:07:35,740 --> 00:07:40,117
Si analizamos cada patrón y multiplicamos su probabilidad de ocurrir por algo 

148
00:07:40,117 --> 00:07:44,720
que mida qué tan informativo es, eso tal vez pueda darnos una puntuación objetiva.

149
00:07:45,960 --> 00:07:47,799
Ahora tu primer instinto sobre lo que debería 

150
00:07:47,799 --> 00:07:49,840
ser ese algo podría ser el número de coincidencias.

151
00:07:50,160 --> 00:07:52,400
Quieres un número promedio más bajo de coincidencias.

152
00:07:52,800 --> 00:07:56,679
Pero en lugar de eso me gustaría usar una medida más universal que a menudo atribuimos 

153
00:07:56,679 --> 00:08:00,246
a la información, y una que será más flexible una vez que tengamos asignada una 

154
00:08:00,246 --> 00:08:04,260
probabilidad diferente a cada una de estas 13.000 palabras sobre si son o no la respuesta.

155
00:08:10,320 --> 00:08:14,440
La unidad de información estándar es el bit, que tiene una fórmula un poco divertida, 

156
00:08:14,440 --> 00:08:16,980
pero es realmente intuitiva si solo miramos ejemplos.

157
00:08:17,780 --> 00:08:21,544
Si tienes una observación que reduce a la mitad tu espacio de posibilidades, 

158
00:08:21,544 --> 00:08:23,500
decimos que tiene un bit de información.

159
00:08:24,180 --> 00:08:26,798
En nuestro ejemplo, el espacio de posibilidades son todas las palabras posibles, 

160
00:08:26,798 --> 00:08:29,546
y resulta que aproximadamente la mitad de las palabras de cinco letras tienen una S, 

161
00:08:29,546 --> 00:08:31,260
un poco menos que eso, pero aproximadamente la mitad.

162
00:08:31,780 --> 00:08:34,320
Entonces esa observación les daría un poco de información.

163
00:08:34,880 --> 00:08:39,361
Si en cambio un hecho nuevo reduce ese espacio de posibilidades en un factor de cuatro, 

164
00:08:39,361 --> 00:08:41,500
decimos que tiene dos bits de información.

165
00:08:41,980 --> 00:08:44,460
Por ejemplo, resulta que aproximadamente una cuarta parte de estas palabras tienen una T.

166
00:08:45,020 --> 00:08:47,799
Si la observación corta ese espacio por un factor de ocho, 

167
00:08:47,799 --> 00:08:50,720
decimos que son tres bits de información, y así sucesivamente.

168
00:08:50,900 --> 00:08:55,060
Cuatro bits lo cortan en un 16, cinco bits lo cortan en un 32.

169
00:08:55,060 --> 00:08:57,726
Así que ahora quizás quieras hacer una pausa y preguntarte: 

170
00:08:57,726 --> 00:09:01,460
¿cuál es la fórmula para obtener información sobre el número de bits en términos de 

171
00:09:01,460 --> 00:09:02,660
probabilidad de que ocurra?

172
00:09:02,660 --> 00:09:06,524
Lo que estamos diciendo aquí es que cuando se le suma la mitad al número de bits, 

173
00:09:06,524 --> 00:09:10,483
eso es lo mismo que la probabilidad, que es lo mismo que decir que dos elevado a la 

174
00:09:10,483 --> 00:09:13,217
potencia del número de bits es uno sobre la probabilidad, 

175
00:09:13,217 --> 00:09:17,223
lo cual se reordena además para decir que la información es el logaritmo en base dos 

176
00:09:17,223 --> 00:09:18,920
de uno dividido por la probabilidad.

177
00:09:19,620 --> 00:09:21,647
Y a veces se ve esto con un reordenamiento más, 

178
00:09:21,647 --> 00:09:24,900
donde la información es el logaritmo negativo en base dos de la probabilidad.

179
00:09:25,660 --> 00:09:28,771
Expresado así, puede parecer un poco extraño para los no iniciados, 

180
00:09:28,771 --> 00:09:31,517
pero en realidad es sólo la idea muy intuitiva de preguntar 

181
00:09:31,517 --> 00:09:34,080
cuántas veces has reducido tus posibilidades a la mitad.

182
00:09:35,180 --> 00:09:37,308
Ahora, si te lo estás preguntando, ya sabes, pensé que solo estábamos jugando 

183
00:09:37,308 --> 00:09:39,300
un divertido juego de palabras, ¿por qué los logaritmos entran en escena?

184
00:09:39,780 --> 00:09:44,182
Una de las razones por las que esta es una unidad más agradable es que es mucho más fácil 

185
00:09:44,182 --> 00:09:48,390
hablar de eventos muy improbables, mucho más fácil decir que una observación tiene 20 

186
00:09:48,390 --> 00:09:52,450
bits de información que decir que la probabilidad de que ocurra tal o cual cosa es 

187
00:09:52,450 --> 00:09:52,940
0.0000095.

188
00:09:53,300 --> 00:09:57,239
Pero una razón más sustancial por la que esta expresión logarítmica resultó ser una 

189
00:09:57,239 --> 00:10:01,460
adición muy útil a la teoría de la probabilidad es la forma en que se suma la información.

190
00:10:02,060 --> 00:10:05,342
Por ejemplo, si una observación le proporciona dos bits de información, 

191
00:10:05,342 --> 00:10:08,488
lo que reduce su espacio en cuatro, y luego una segunda observación, 

192
00:10:08,488 --> 00:10:12,363
como su segunda suposición en Wordle, le proporciona otros tres bits de información, 

193
00:10:12,363 --> 00:10:14,597
lo que lo reduce aún más en otro factor de ocho, 

194
00:10:14,597 --> 00:10:16,740
la dos juntos te dan cinco bits de información.

195
00:10:17,160 --> 00:10:19,797
De la misma manera que a las probabilidades les gusta multiplicarse, 

196
00:10:19,797 --> 00:10:21,020
a la información le gusta sumar.

197
00:10:21,960 --> 00:10:24,675
Entonces, tan pronto como estamos en el ámbito de algo así como un valor esperado, 

198
00:10:24,675 --> 00:10:26,638
donde sumamos un montón de números, los registros hacen que 

199
00:10:26,638 --> 00:10:27,980
sea mucho más agradable tratar con ellos.

200
00:10:28,480 --> 00:10:32,356
Volvamos a nuestra distribución de Weary y agreguemos otro pequeño rastreador aquí, 

201
00:10:32,356 --> 00:10:34,940
que nos muestra cuánta información hay para cada patrón.

202
00:10:35,580 --> 00:10:39,158
Lo principal que quiero que note es que cuanto mayor es la probabilidad a medida que 

203
00:10:39,158 --> 00:10:42,780
llegamos a esos patrones más probables, menor es la información y menos bits se ganan.

204
00:10:43,500 --> 00:10:46,974
La forma en que medimos la calidad de esta suposición será tomando el valor 

205
00:10:46,974 --> 00:10:49,717
esperado de esta información, donde analizamos cada patrón, 

206
00:10:49,717 --> 00:10:53,054
decimos qué tan probable es y luego lo multiplicamos por cuántos bits de 

207
00:10:53,054 --> 00:10:54,060
información obtenemos.

208
00:10:54,710 --> 00:10:58,120
Y en el ejemplo de Weary, resulta ser 4.9 bits.

209
00:10:58,560 --> 00:11:01,954
Entonces, en promedio, la información que obtienes de esta suposición inicial 

210
00:11:01,954 --> 00:11:05,480
es tan buena como cortar tu espacio de posibilidades a la mitad unas cinco veces.

211
00:11:05,960 --> 00:11:08,627
Por el contrario, un ejemplo de una suposición con un 

212
00:11:08,627 --> 00:11:11,640
valor de información esperado más alto sería algo como Slate.

213
00:11:13,120 --> 00:11:15,620
En este caso notarás que la distribución luce mucho más plana.

214
00:11:15,940 --> 00:11:19,127
En particular, la aparición más probable de todos los grises solo 

215
00:11:19,127 --> 00:11:22,314
tiene alrededor de un 6% de posibilidades de ocurrir, por lo que, 

216
00:11:22,314 --> 00:11:25,260
como mínimo, evidentemente obtendrás 3.9 bits de información.

217
00:11:25,920 --> 00:11:28,560
Pero eso es un mínimo, normalmente obtendrás algo mejor que eso.

218
00:11:29,100 --> 00:11:33,436
Y resulta que cuando haces cálculos en este caso y sumas todos los términos relevantes, 

219
00:11:33,436 --> 00:11:35,900
la información promedio es de aproximadamente 5.8.

220
00:11:37,360 --> 00:11:40,294
Entonces, a diferencia de Weary, su espacio de posibilidades será 

221
00:11:40,294 --> 00:11:43,540
aproximadamente la mitad después de esta primera suposición, en promedio.

222
00:11:44,420 --> 00:11:46,838
De hecho, hay una historia divertida sobre el nombre 

223
00:11:46,838 --> 00:11:49,120
de este valor esperado de cantidad de información.

224
00:11:49,200 --> 00:11:51,814
La teoría de la información fue desarrollada por Claude Shannon, 

225
00:11:51,814 --> 00:11:54,268
que trabajaba en los Laboratorios Bell en la década de 1940, 

226
00:11:54,268 --> 00:11:57,647
pero estaba hablando de algunas de sus ideas aún por publicar con John von Neumann, 

227
00:11:57,647 --> 00:12:00,100
que era este gigante intelectual de la época, muy destacado. 

228
00:12:00,100 --> 00:12:03,560
en matemáticas y física y los inicios de lo que se estaba convirtiendo en informática.

229
00:12:04,100 --> 00:12:07,360
Y cuando mencionó que realmente no tenía un buen nombre para este valor 

230
00:12:07,360 --> 00:12:10,621
esperado de la cantidad de información, supuestamente von Neumann dijo, 

231
00:12:10,621 --> 00:12:14,200
según cuenta la historia, bueno, deberías llamarlo entropía, y por dos razones.

232
00:12:14,540 --> 00:12:18,535
En primer lugar, su función de incertidumbre se ha utilizado en mecánica estadística 

233
00:12:18,535 --> 00:12:22,530
con ese nombre, por lo que ya tiene un nombre, y en segundo lugar, y más importante, 

234
00:12:22,530 --> 00:12:26,760
nadie sabe qué es realmente la entropía, por lo que en un debate siempre tener la ventaja.

235
00:12:27,700 --> 00:12:29,700
Entonces, si el nombre parece un poco misterioso, 

236
00:12:29,700 --> 00:12:32,460
y si hay que creer en esta historia, es más o menos intencionalmente.

237
00:12:33,280 --> 00:12:36,574
Además, si te preguntas acerca de su relación con toda esa segunda ley de la 

238
00:12:36,574 --> 00:12:39,226
termodinámica de la física, definitivamente hay una conexión, 

239
00:12:39,226 --> 00:12:43,034
pero en sus orígenes Shannon solo estaba tratando con la teoría de la probabilidad pura, 

240
00:12:43,034 --> 00:12:45,815
y para nuestros propósitos aquí, cuando uso la palabra entropía, 

241
00:12:45,815 --> 00:12:49,580
solo quiero que piense en el valor de información esperado de una suposición particular.

242
00:12:50,700 --> 00:12:53,780
Puedes pensar que la entropía mide dos cosas simultáneamente.

243
00:12:54,240 --> 00:12:56,780
El primero es qué tan plana es la distribución.

244
00:12:57,320 --> 00:13:01,120
Cuanto más cercana a la uniformidad sea una distribución, mayor será la entropía.

245
00:13:01,580 --> 00:13:06,702
En nuestro caso, donde hay de 3 a 5 patrones en total, para una distribución uniforme, 

246
00:13:06,702 --> 00:13:11,647
observar cualquiera de ellos tendría un registro de información en base 2 de 3 a 5, 

247
00:13:11,647 --> 00:13:16,770
que resulta ser 7.92, por lo que ese es el máximo absoluto que podrías tener para esta 

248
00:13:16,770 --> 00:13:17,300
entropía.

249
00:13:17,840 --> 00:13:20,071
Pero la entropía también es una especie de medida 

250
00:13:20,071 --> 00:13:22,080
de cuántas posibilidades hay en primer lugar.

251
00:13:22,320 --> 00:13:27,277
Por ejemplo, si tienes una palabra en la que sólo hay 16 patrones posibles y cada uno de 

252
00:13:27,277 --> 00:13:32,180
ellos es igualmente probable, esta entropía, esta información esperada, sería de 4 bits.

253
00:13:32,579 --> 00:13:36,505
Pero si tienes otra palabra donde hay 64 patrones posibles que podrían surgir, 

254
00:13:36,505 --> 00:13:40,480
y todos son igualmente probables, entonces la entropía resultaría ser de 6 bits.

255
00:13:41,500 --> 00:13:45,845
Entonces, si ves alguna distribución en la naturaleza que tiene una entropía de 6 bits, 

256
00:13:45,845 --> 00:13:49,796
es como si estuviera diciendo que hay tanta variación e incertidumbre en lo que 

257
00:13:49,796 --> 00:13:53,500
está a punto de suceder como si hubiera 64 resultados igualmente probables.

258
00:13:54,360 --> 00:13:59,320
Para mi primera pasada por el Wurtelebot, básicamente le pedí que hiciera esto.

259
00:13:59,320 --> 00:14:03,217
Revisa todas las conjeturas posibles que puedas tener, las 13.000 palabras, 

260
00:14:03,217 --> 00:14:06,089
calcula la entropía de cada una, o más específicamente, 

261
00:14:06,089 --> 00:14:10,396
la entropía de la distribución en todos los patrones que puedas ver, para cada una, 

262
00:14:10,396 --> 00:14:14,242
y elige la más alta, ya que es el que probablemente reducirá su espacio de 

263
00:14:14,242 --> 00:14:16,140
posibilidades tanto como sea posible.

264
00:14:17,140 --> 00:14:19,402
Y aunque aquí solo he estado hablando de la primera suposición, 

265
00:14:19,402 --> 00:14:21,100
ocurre lo mismo con las siguientes suposiciones.

266
00:14:21,560 --> 00:14:24,334
Por ejemplo, después de ver algún patrón en esa primera suposición, 

267
00:14:24,334 --> 00:14:27,597
que lo restringiría a un número menor de palabras posibles en función de lo que 

268
00:14:27,597 --> 00:14:30,984
coincide con eso, simplemente juega el mismo juego con respecto a ese conjunto más 

269
00:14:30,984 --> 00:14:31,800
pequeño de palabras.

270
00:14:32,260 --> 00:14:35,993
Para una segunda suposición propuesta, observamos la distribución de todos los 

271
00:14:35,993 --> 00:14:39,916
patrones que podrían ocurrir a partir de ese conjunto más restringido de palabras, 

272
00:14:39,916 --> 00:14:43,840
buscamos entre las 13.000 posibilidades y encontramos la que maximiza esa entropía.

273
00:14:45,420 --> 00:14:48,059
Para mostrarles cómo funciona esto en acción, permítanme 

274
00:14:48,059 --> 00:14:50,884
mostrarles una pequeña variante de Wurtele que escribí y que 

275
00:14:50,884 --> 00:14:54,080
muestra los aspectos más destacados de este análisis en los márgenes.

276
00:14:54,080 --> 00:14:56,250
Después de hacer todos los cálculos de entropía, 

277
00:14:56,250 --> 00:14:59,660
aquí a la derecha nos muestra cuáles tienen la información esperada más alta.

278
00:15:00,280 --> 00:15:06,184
Resulta que la respuesta principal, al menos por el momento, lo refinaremos más adelante, 

279
00:15:06,184 --> 00:15:10,580
es Tares, que significa, por supuesto, arveja, la arveja más común.

280
00:15:11,040 --> 00:15:14,110
Cada vez que hacemos una suposición aquí, donde tal vez ignoro sus 

281
00:15:14,110 --> 00:15:16,584
recomendaciones y elijo slate, porque me gusta slate, 

282
00:15:16,584 --> 00:15:19,287
podemos ver cuánta información esperada tenía, pero luego, 

283
00:15:19,287 --> 00:15:23,136
a la derecha de la palabra aquí, nos muestra cuánta información real que obtuvimos, 

284
00:15:23,136 --> 00:15:24,420
dado este patrón particular.

285
00:15:25,000 --> 00:15:27,240
Así que aquí parece que tuvimos un poco de mala suerte, 

286
00:15:27,240 --> 00:15:30,120
se esperaba que obtuviéramos 5.8, pero obtuvimos algo con menos que eso.

287
00:15:30,600 --> 00:15:32,946
Y luego, en el lado izquierdo, aquí nos muestra todas las diferentes 

288
00:15:32,946 --> 00:15:35,020
palabras posibles según el lugar donde nos encontramos ahora.

289
00:15:35,800 --> 00:15:38,510
Las barras azules nos dicen qué tan probable cree que es cada palabra, 

290
00:15:38,510 --> 00:15:41,107
por lo que por el momento suponemos que cada palabra tiene la misma 

291
00:15:41,107 --> 00:15:43,360
probabilidad de ocurrir, pero lo refinaremos en un momento.

292
00:15:44,060 --> 00:15:48,089
Y luego esta medida de incertidumbre nos dice la entropía de esta distribución entre 

293
00:15:48,089 --> 00:15:52,024
las palabras posibles, que ahora mismo, debido a que es una distribución uniforme, 

294
00:15:52,024 --> 00:15:55,960
es solo una forma innecesariamente complicada de contar el número de posibilidades.

295
00:15:56,560 --> 00:15:59,890
Por ejemplo, si tuviéramos que elevar 2 a la potencia de 13.66, 

296
00:15:59,890 --> 00:16:02,180
eso debería rondar las 13.000 posibilidades.

297
00:16:02,900 --> 00:16:06,140
Estoy un poco fuera de lugar aquí, pero sólo porque no muestro todos los decimales.

298
00:16:06,720 --> 00:16:09,937
Por el momento, esto puede parecer redundante y complicar demasiado las cosas, 

299
00:16:09,937 --> 00:16:12,340
pero verá por qué es útil tener ambos números en un minuto.

300
00:16:12,760 --> 00:16:16,177
Así que aquí parece que sugiere que la entropía más alta para nuestra 

301
00:16:16,177 --> 00:16:19,400
segunda suposición es Ramen, que nuevamente no parece una palabra.

302
00:16:19,980 --> 00:16:24,060
Entonces, para tener autoridad moral aquí, seguiré adelante y escribiré Rains.

303
00:16:25,440 --> 00:16:27,340
Y nuevamente parece que tuvimos un poco de mala suerte.

304
00:16:27,520 --> 00:16:31,360
Esperábamos 4.3 bits y solo tenemos 3.39 bits de información.

305
00:16:31,940 --> 00:16:33,940
Eso nos lleva a 55 posibilidades.

306
00:16:34,900 --> 00:16:37,762
Y aquí tal vez me quede con lo que sugiere, que es combo, 

307
00:16:37,762 --> 00:16:39,440
sea lo que sea que eso signifique.

308
00:16:40,040 --> 00:16:42,920
Y está bien, esta es en realidad una buena oportunidad para resolver un rompecabezas.

309
00:16:42,920 --> 00:16:46,380
Nos dice que este patrón nos da 4.7 bits de información.

310
00:16:47,060 --> 00:16:51,720
Pero a la izquierda, antes de que veamos ese patrón, había 5.78 bits de incertidumbre.

311
00:16:52,420 --> 00:16:56,340
Entonces, a modo de prueba, ¿qué significa eso sobre el número de posibilidades restantes?

312
00:16:58,040 --> 00:17:01,524
Bueno, significa que estamos reducidos a un poco de incertidumbre, 

313
00:17:01,524 --> 00:17:04,540
que es lo mismo que decir que hay dos respuestas posibles.

314
00:17:04,700 --> 00:17:05,700
Es una elección 50-50.

315
00:17:06,500 --> 00:17:09,083
Y a partir de aquí, porque tú y yo sabemos qué palabras son más comunes, 

316
00:17:09,083 --> 00:17:10,640
sabemos que la respuesta debería ser abismo.

317
00:17:11,180 --> 00:17:13,280
Pero tal como está escrito ahora, el programa no lo sabe.

318
00:17:13,540 --> 00:17:17,110
Así que sigue adelante, tratando de obtener tanta información como puede, 

319
00:17:17,110 --> 00:17:19,859
hasta que sólo queda una posibilidad, y luego la adivina.

320
00:17:20,380 --> 00:17:22,339
Obviamente necesitamos una mejor estrategia para el final del juego.

321
00:17:22,599 --> 00:17:25,429
Pero digamos que llamamos a esta versión uno de nuestro solucionador de 

322
00:17:25,429 --> 00:17:28,260
palabras y luego ejecutamos algunas simulaciones para ver cómo funciona.

323
00:17:30,360 --> 00:17:34,120
Entonces, la forma en que esto funciona es jugando todos los juegos de palabras posibles.

324
00:17:34,240 --> 00:17:38,540
Está repasando todas esas 2315 palabras que son las respuestas reales.

325
00:17:38,540 --> 00:17:40,580
Básicamente se trata de utilizarlo como un conjunto de pruebas.

326
00:17:41,360 --> 00:17:44,133
Y con este método ingenuo de no considerar qué tan común es 

327
00:17:44,133 --> 00:17:48,109
una palabra y simplemente tratar de maximizar la información en cada paso del camino, 

328
00:17:48,109 --> 00:17:49,820
hasta llegar a una y sólo una opción.

329
00:17:50,360 --> 00:17:54,300
Al final de la simulación, la puntuación media resulta ser de aproximadamente 4.124.

330
00:17:55,319 --> 00:17:59,240
Lo cual no está mal, para ser honesto, esperaba hacerlo peor.

331
00:17:59,660 --> 00:18:02,600
Pero la gente que juega wordle te dirá que normalmente lo consiguen en 4.

332
00:18:02,860 --> 00:18:05,380
El verdadero desafío es conseguir tantos en 3 como puedas.

333
00:18:05,380 --> 00:18:08,080
Es un salto bastante grande entre la puntuación de 4 y la puntuación de 3.

334
00:18:08,860 --> 00:18:13,231
Lo obvio aquí es incorporar de alguna manera si una palabra es común o no, 

335
00:18:13,231 --> 00:18:14,980
y cómo lo hacemos exactamente.

336
00:18:22,800 --> 00:18:25,147
La forma en que lo acerqué es obtener una lista de las 

337
00:18:25,147 --> 00:18:27,880
frecuencias relativas de todas las palabras en el idioma inglés.

338
00:18:28,220 --> 00:18:31,460
Y acabo de utilizar la función de datos de frecuencia de palabras de Mathematica, 

339
00:18:31,460 --> 00:18:34,860
que a su vez se extrae del conjunto de datos públicos Ngram en inglés de Google Books.

340
00:18:35,460 --> 00:18:37,689
Y es divertido verlo, por ejemplo, si lo clasificamos 

341
00:18:37,689 --> 00:18:39,960
desde las palabras más comunes hasta las menos comunes.

342
00:18:40,120 --> 00:18:43,080
Evidentemente estas son las palabras de cinco letras más comunes en el idioma inglés.

343
00:18:43,700 --> 00:18:45,840
O mejor dicho, este es el octavo más común.

344
00:18:46,280 --> 00:18:48,880
Primero es cuál, después está ahí y ahí.

345
00:18:49,260 --> 00:18:52,397
Primero en sí no es primero, sino noveno, y tiene sentido que estas 

346
00:18:52,397 --> 00:18:54,750
otras palabras puedan aparecer con más frecuencia, 

347
00:18:54,750 --> 00:18:58,580
donde las que siguen a primero son después, dónde, y que son un poco menos comunes.

348
00:18:59,160 --> 00:19:02,920
Ahora bien, al utilizar estos datos para modelar la probabilidad de que cada una de 

349
00:19:02,920 --> 00:19:06,860
estas palabras sea la respuesta final, no debería ser sólo proporcional a la frecuencia.

350
00:19:06,860 --> 00:19:10,637
Por ejemplo, a la que se le da una puntuación de 0.002 en este conjunto de datos, 

351
00:19:10,637 --> 00:19:13,125
mientras que la palabra trenza es, en cierto sentido, 

352
00:19:13,125 --> 00:19:15,060
aproximadamente 1000 veces menos probable.

353
00:19:15,560 --> 00:19:17,362
Pero ambas son palabras bastante comunes que casi 

354
00:19:17,362 --> 00:19:18,840
con seguridad vale la pena considerarlas.

355
00:19:19,340 --> 00:19:21,000
Por eso queremos más un límite binario.

356
00:19:21,860 --> 00:19:26,105
La forma en que lo hice es imaginar tomar toda esta lista ordenada de palabras, 

357
00:19:26,105 --> 00:19:29,874
y luego organizarla en un eje x, y luego aplicar la función sigmoidea, 

358
00:19:29,874 --> 00:19:34,226
que es la forma estándar de tener una función cuya salida es básicamente binaria, 

359
00:19:34,226 --> 00:19:38,260
es 0 o 1, pero hay un suavizado intermedio para esa región de incertidumbre.

360
00:19:39,160 --> 00:19:43,616
Básicamente, la probabilidad que estoy asignando a cada palabra de estar en la lista 

361
00:19:43,616 --> 00:19:48,125
final será el valor de la función sigmoidea arriba dondequiera que se encuentre en el 

362
00:19:48,125 --> 00:19:48,440
eje x.

363
00:19:49,520 --> 00:19:53,357
Ahora bien, obviamente, esto depende de algunos parámetros, por ejemplo, 

364
00:19:53,357 --> 00:19:57,720
qué tan ancho es el espacio en el eje x que ocupan esas palabras determina qué tan 

365
00:19:57,720 --> 00:20:02,188
gradual o abruptamente bajamos de 1 a 0, y dónde las ubicamos de izquierda a derecha 

366
00:20:02,188 --> 00:20:03,240
determina el límite.

367
00:20:03,240 --> 00:20:05,080
Para ser honesto, la forma en que hice esto fue 

368
00:20:05,080 --> 00:20:06,920
simplemente lamerme el dedo y pegarlo al viento.

369
00:20:07,140 --> 00:20:10,380
Revisé la lista ordenada y traté de encontrar una ventana donde, 

370
00:20:10,380 --> 00:20:13,670
cuando la miré, pensé que era más probable que aproximadamente la 

371
00:20:13,670 --> 00:20:17,260
mitad de estas palabras fueran la respuesta final, y la usé como límite.

372
00:20:17,260 --> 00:20:20,150
Una vez que tenemos una distribución como esta entre las palabras, 

373
00:20:20,150 --> 00:20:23,860
nos da otra situación en la que la entropía se convierte en una medida realmente útil.

374
00:20:24,500 --> 00:20:28,179
Por ejemplo, digamos que estamos jugando y comenzamos con mis viejos abridores, 

375
00:20:28,179 --> 00:20:31,078
que eran plumas y clavos, y terminamos con una situación en la 

376
00:20:31,078 --> 00:20:33,240
que hay cuatro palabras posibles que coinciden.

377
00:20:33,560 --> 00:20:35,620
Y digamos que los consideramos todos igualmente probables.

378
00:20:36,220 --> 00:20:38,880
Déjame preguntarte, ¿cuál es la entropía de esta distribución?

379
00:20:41,080 --> 00:20:45,560
Bueno, la información asociada a cada una de estas posibilidades va a 

380
00:20:45,560 --> 00:20:50,040
ser el logaritmo en base 2 de 4, ya que cada una es 1 y 4, y eso es 2.

381
00:20:50,040 --> 00:20:52,460
Dos bits de información, cuatro posibilidades.

382
00:20:52,760 --> 00:20:53,580
Todo muy bien y bueno.

383
00:20:54,300 --> 00:20:57,800
Pero ¿y si te dijera que en realidad hay más de cuatro coincidencias?

384
00:20:58,260 --> 00:21:02,460
En realidad, cuando miramos la lista completa de palabras, hay 16 palabras que coinciden.

385
00:21:02,580 --> 00:21:05,147
Pero supongamos que nuestro modelo asigna una probabilidad 

386
00:21:05,147 --> 00:21:08,584
realmente baja a que esas otras 12 palabras sean realmente la respuesta final, 

387
00:21:08,584 --> 00:21:10,760
algo así como 1 entre 1000 porque son muy oscuras.

388
00:21:11,500 --> 00:21:14,260
Ahora déjame preguntarte, ¿cuál es la entropía de esta distribución?

389
00:21:15,420 --> 00:21:18,611
Si la entropía simplemente midiera el número de coincidencias aquí, 

390
00:21:18,611 --> 00:21:22,414
entonces se podría esperar que fuera algo así como el logaritmo en base 2 de 16, 

391
00:21:22,414 --> 00:21:25,700
que sería 4, dos bits más de incertidumbre que los que teníamos antes.

392
00:21:26,180 --> 00:21:29,860
Pero, por supuesto, la incertidumbre real no es tan diferente de la que teníamos antes.

393
00:21:30,160 --> 00:21:33,646
El hecho de que existan estas 12 palabras realmente oscuras no significa que 

394
00:21:33,646 --> 00:21:37,360
sería mucho más sorprendente saber que la respuesta final es encanto, por ejemplo.

395
00:21:38,180 --> 00:21:41,696
Entonces, cuando realmente haces el cálculo aquí y sumas la probabilidad de cada 

396
00:21:41,696 --> 00:21:45,560
ocurrencia multiplicada por la información correspondiente, lo que obtienes es 2.11 bits.

397
00:21:45,560 --> 00:21:49,301
Solo digo que son básicamente dos bits, básicamente esas cuatro posibilidades, 

398
00:21:49,301 --> 00:21:53,516
pero hay un poco más de incertidumbre debido a todos esos eventos altamente improbables, 

399
00:21:53,516 --> 00:21:56,500
aunque si los aprendieras, obtendrías un montón de información.

400
00:21:57,160 --> 00:21:59,262
Entonces, alejarnos, esto es parte de lo que hace de Wordle 

401
00:21:59,262 --> 00:22:01,400
un buen ejemplo para una lección de teoría de la información.

402
00:22:01,600 --> 00:22:04,640
Tenemos estas dos aplicaciones de sentimiento distintas para la entropía.

403
00:22:05,160 --> 00:22:08,560
El primero nos dice cuál es la información esperada que obtendremos 

404
00:22:08,560 --> 00:22:12,010
de una suposición determinada, y el segundo dice si podemos medir la 

405
00:22:12,010 --> 00:22:15,460
incertidumbre restante entre todas las palabras que tenemos posibles.

406
00:22:16,460 --> 00:22:19,007
Y debo enfatizar que, en el primer caso en el que observamos la 

407
00:22:19,007 --> 00:22:21,554
información esperada de una suposición, una vez que tenemos una 

408
00:22:21,554 --> 00:22:24,540
ponderación desigual de las palabras, eso afecta el cálculo de la entropía.

409
00:22:24,980 --> 00:22:27,620
Por ejemplo, permítanme mencionar el mismo caso que vimos 

410
00:22:27,620 --> 00:22:30,032
anteriormente de la distribución asociada con Weary, 

411
00:22:30,032 --> 00:22:33,720
pero esta vez usando una distribución no uniforme en todas las palabras posibles.

412
00:22:34,500 --> 00:22:38,280
Déjame ver si puedo encontrar una parte aquí que lo ilustre bastante bien.

413
00:22:40,940 --> 00:22:42,360
Bien, aquí esto está bastante bien.

414
00:22:42,360 --> 00:22:45,585
Aquí tenemos dos patrones adyacentes que son igualmente probables, 

415
00:22:45,585 --> 00:22:49,100
pero nos dicen que uno de ellos tiene 32 palabras posibles que coinciden.

416
00:22:49,280 --> 00:22:51,941
Y si comprobamos cuáles son, estas son esas 32, 

417
00:22:51,941 --> 00:22:55,600
que son palabras muy improbables cuando las examinas con la vista.

418
00:22:55,840 --> 00:22:59,103
Es difícil encontrar respuestas que parezcan plausibles, tal vez gritos, 

419
00:22:59,103 --> 00:23:01,472
pero si miramos el patrón vecino en la distribución, 

420
00:23:01,472 --> 00:23:05,138
que se considera casi igual de probable, nos dicen que solo tiene 8 coincidencias 

421
00:23:05,138 --> 00:23:08,134
posibles, por lo que una cuarta parte de Hay muchas coincidencias, 

422
00:23:08,134 --> 00:23:09,520
pero es casi igual de probable.

423
00:23:09,860 --> 00:23:12,140
Y cuando analizamos esas coincidencias, podemos ver por qué.

424
00:23:12,500 --> 00:23:16,300
Algunas de estas son respuestas realmente plausibles, como timbre, ira o golpes.

425
00:23:17,900 --> 00:23:21,482
Para ilustrar cómo incorporamos todo eso, permítanme mostrar aquí la versión 2 del 

426
00:23:21,482 --> 00:23:25,280
Wordlebot, y hay dos o tres diferencias principales con respecto a la primera que vimos.

427
00:23:25,860 --> 00:23:29,578
En primer lugar, como acabo de decir, la forma en que calculamos estas entropías, 

428
00:23:29,578 --> 00:23:32,707
estos valores esperados de información, ahora utiliza distribuciones 

429
00:23:32,707 --> 00:23:35,745
más refinadas entre los patrones que incorporan la probabilidad de 

430
00:23:35,745 --> 00:23:38,240
que una palabra determinada sea realmente la respuesta.

431
00:23:38,879 --> 00:23:41,780
Da la casualidad de que las lágrimas siguen siendo el número 1, 

432
00:23:41,780 --> 00:23:43,820
aunque las siguientes son un poco diferentes.

433
00:23:44,360 --> 00:23:46,645
En segundo lugar, cuando clasifique sus mejores opciones, 

434
00:23:46,645 --> 00:23:50,114
ahora mantendrá un modelo de la probabilidad de que cada palabra sea la respuesta real, 

435
00:23:50,114 --> 00:23:52,833
y lo incorporará en su decisión, lo cual es más fácil de ver una vez 

436
00:23:52,833 --> 00:23:55,080
que tengamos algunas conjeturas sobre la respuesta. mesa.

437
00:23:55,860 --> 00:23:57,689
Nuevamente, ignorando su recomendación porque no 

438
00:23:57,689 --> 00:23:59,780
podemos dejar que las máquinas gobiernen nuestras vidas.

439
00:24:01,140 --> 00:24:04,289
Y supongo que debería mencionar otra cosa diferente aquí a la izquierda, 

440
00:24:04,289 --> 00:24:06,447
ese valor de incertidumbre, esa cantidad de bits, 

441
00:24:06,447 --> 00:24:09,640
ya no es simplemente redundante con la cantidad de coincidencias posibles.

442
00:24:10,080 --> 00:24:15,379
Ahora si lo levantamos y calculamos 2 elevado a 8.02, que está un poco por encima de 256, 

443
00:24:15,379 --> 00:24:19,853
supongo que 259, lo que dice es que aunque hay un total de 526 palabras que 

444
00:24:19,853 --> 00:24:24,505
realmente coinciden con este patrón, la cantidad de incertidumbre que tiene es 

445
00:24:24,505 --> 00:24:28,980
más parecida a la que sería si hubiera 259 igualmente probables. resultados.

446
00:24:29,720 --> 00:24:30,740
Puedes pensar en ello así.

447
00:24:31,020 --> 00:24:34,007
Sabe que borx no es la respuesta, lo mismo ocurre con yorts, 

448
00:24:34,007 --> 00:24:37,680
zorl y zorus, por lo que es un poco menos incierto que en el caso anterior.

449
00:24:37,820 --> 00:24:39,280
Este número de bits será menor.

450
00:24:40,220 --> 00:24:43,407
Y si sigo jugando, lo refinaré con un par de suposiciones 

451
00:24:43,407 --> 00:24:46,540
que son apropiadas para lo que me gustaría explicar aquí.

452
00:24:48,360 --> 00:24:50,885
En la cuarta suposición, si observa sus mejores opciones, 

453
00:24:50,885 --> 00:24:53,760
podrá ver que ya no se trata simplemente de maximizar la entropía.

454
00:24:54,460 --> 00:24:56,962
Entonces, en este punto, técnicamente hay siete posibilidades, 

455
00:24:56,962 --> 00:25:00,300
pero las únicas con posibilidades significativas son los dormitorios y las palabras.

456
00:25:00,300 --> 00:25:04,243
Y puede ver que se clasifica al elegir ambos por encima de todos estos otros valores, 

457
00:25:04,243 --> 00:25:06,720
que estrictamente hablando brindarían más información.

458
00:25:07,240 --> 00:25:10,528
La primera vez que hice esto, simplemente sumé estos dos números para medir la 

459
00:25:10,528 --> 00:25:13,900
calidad de cada suposición, lo que en realidad funcionó mejor de lo que imaginas.

460
00:25:14,300 --> 00:25:16,693
Pero realmente no lo sentí sistemático, y estoy seguro de que hay 

461
00:25:16,693 --> 00:25:19,340
otros enfoques que la gente podría adoptar, pero este es el que encontré.

462
00:25:19,760 --> 00:25:22,744
Si estamos considerando la posibilidad de una próxima suposición, 

463
00:25:22,744 --> 00:25:26,633
como en este caso palabras, lo que realmente nos importa es la puntuación esperada de 

464
00:25:26,633 --> 00:25:27,900
nuestro juego si lo hacemos.

465
00:25:28,230 --> 00:25:32,039
Y para calcular esa puntuación esperada, decimos cuál es la probabilidad de 

466
00:25:32,039 --> 00:25:35,900
que las palabras sean la respuesta real, que en este momento describe el 58%.

467
00:25:36,040 --> 00:25:39,540
Decimos que con un 58% de posibilidades, nuestra puntuación en este juego sería 4.

468
00:25:40,320 --> 00:25:45,640
Y luego, con la probabilidad de 1 menos ese 58%, nuestra puntuación será mayor que ese 4.

469
00:25:46,220 --> 00:25:49,316
No sabemos cuánto más, pero podemos estimarlo en función de cuánta 

470
00:25:49,316 --> 00:25:52,460
incertidumbre probablemente habrá una vez que lleguemos a ese punto.

471
00:25:52,960 --> 00:25:55,940
En concreto, de momento hay 1.44 bits de incertidumbre.

472
00:25:56,440 --> 00:26:01,120
Si adivinamos palabras, nos dice que la información esperada que obtendremos es 1.27 bits.

473
00:26:01,620 --> 00:26:04,685
Entonces, si adivinamos palabras, esta diferencia representa cuánta 

474
00:26:04,685 --> 00:26:07,660
incertidumbre es probable que nos quede después de que eso suceda.

475
00:26:08,260 --> 00:26:11,217
Lo que necesitamos es algún tipo de función, a la que aquí llamo f, 

476
00:26:11,217 --> 00:26:13,740
que asocie esta incertidumbre con una puntuación esperada.

477
00:26:14,240 --> 00:26:18,300
Y la forma en que lo hicimos fue simplemente trazar un montón de datos de juegos 

478
00:26:18,300 --> 00:26:22,259
anteriores basados en la versión 1 del bot para decir cuál fue el puntaje real 

479
00:26:22,259 --> 00:26:26,320
después de varios puntos con ciertas cantidades de incertidumbre muy mensurables.

480
00:26:27,020 --> 00:26:31,014
Por ejemplo, estos puntos de datos aquí que se encuentran por encima de un valor cercano 

481
00:26:31,014 --> 00:26:35,009
a 8.Aproximadamente 7, dicen para algunos juegos después de un punto en el que había 8.7 

482
00:26:35,009 --> 00:26:38,960
bits de incertidumbre, fueron necesarias dos conjeturas para obtener la respuesta final.

483
00:26:39,320 --> 00:26:40,766
Para otros juegos fueron necesarias tres conjeturas, 

484
00:26:40,766 --> 00:26:42,240
para otros juegos fueron necesarias cuatro conjeturas.

485
00:26:43,140 --> 00:26:46,674
Si aquí nos desplazamos hacia la izquierda, todos los puntos sobre cero dicen que 

486
00:26:46,674 --> 00:26:50,337
siempre que haya cero bits de incertidumbre, es decir, que solo hay una posibilidad, 

487
00:26:50,337 --> 00:26:53,139
entonces el número de conjeturas requeridas es siempre solo uno, 

488
00:26:53,139 --> 00:26:54,260
lo cual es tranquilizador.

489
00:26:54,780 --> 00:26:57,512
Cada vez que había un poco de incertidumbre, lo que significaba 

490
00:26:57,512 --> 00:26:59,647
que esencialmente se reducía a dos posibilidades, 

491
00:26:59,647 --> 00:27:03,020
a veces se requería una conjetura más, a veces se requerían dos conjeturas más.

492
00:27:03,080 --> 00:27:05,240
Y así sucesivamente aquí.

493
00:27:05,740 --> 00:27:08,141
Quizás una forma un poco más sencilla de visualizar 

494
00:27:08,141 --> 00:27:10,220
estos datos sea agruparlos y tomar promedios.

495
00:27:11,000 --> 00:27:15,330
Por ejemplo, esta barra dice que entre todos los puntos en los que teníamos un poco de 

496
00:27:15,330 --> 00:27:19,760
incertidumbre, en promedio el número de nuevas conjeturas requeridas fue aproximadamente 

497
00:27:19,760 --> 00:27:19,960
1.5.

498
00:27:22,140 --> 00:27:25,294
Y la barra de aquí dice que entre todos los diferentes juegos donde en 

499
00:27:25,294 --> 00:27:28,715
algún momento la incertidumbre estuvo un poco por encima de los cuatro bits, 

500
00:27:28,715 --> 00:27:32,225
lo que es como reducirla a 16 posibilidades diferentes, entonces, en promedio, 

501
00:27:32,225 --> 00:27:35,380
requiere un poco más de dos conjeturas a partir de ese punto. adelante.

502
00:27:36,060 --> 00:27:37,744
Y a partir de aquí simplemente hice una regresión para 

503
00:27:37,744 --> 00:27:39,460
ajustarme a una función que parecía razonable para esto.

504
00:27:39,980 --> 00:27:44,394
Y recuerde que el objetivo de hacer todo esto es que podamos cuantificar esta intuición 

505
00:27:44,394 --> 00:27:47,254
de que cuanta más información obtengamos de una palabra, 

506
00:27:47,254 --> 00:27:48,960
menor será la puntuación esperada.

507
00:27:49,680 --> 00:27:54,432
Entonces con esto como versión 2.0, si volvemos atrás y ejecutamos el mismo conjunto 

508
00:27:54,432 --> 00:27:59,240
de simulaciones, haciéndolo jugar contra las 2315 respuestas posibles, ¿cómo funciona?

509
00:28:00,280 --> 00:28:01,787
Bueno, a diferencia de nuestra primera versión, 

510
00:28:01,787 --> 00:28:03,420
es definitivamente mejor, lo cual es tranquilizador.

511
00:28:04,020 --> 00:28:07,920
Todo dicho y hecho, la media ronda los 3.6, aunque a diferencia de la primera 

512
00:28:07,920 --> 00:28:12,120
versión hay un par de veces que pierde y requiere más de seis en esta circunstancia.

513
00:28:12,639 --> 00:28:15,220
Presumiblemente porque hay momentos en los que se trata 

514
00:28:15,220 --> 00:28:17,940
de buscar el objetivo en lugar de maximizar la información.

515
00:28:19,040 --> 00:28:21,000
Entonces, ¿podemos hacerlo mejor que 3?6?

516
00:28:22,080 --> 00:28:22,920
Definitivamente podemos.

517
00:28:23,280 --> 00:28:26,401
Ahora dije al principio que es muy divertido intentar no incorporar la lista 

518
00:28:26,401 --> 00:28:29,360
verdadera de respuestas de Wordle en la forma en que construye su modelo.

519
00:28:29,880 --> 00:28:34,180
Pero si lo incorporamos, el mejor rendimiento que pude obtener fue alrededor de 3.43.

520
00:28:35,160 --> 00:28:37,680
Entonces, si intentamos ser más sofisticados que simplemente usar 

521
00:28:37,680 --> 00:28:40,354
datos de frecuencia de palabras para elegir esta distribución previa, 

522
00:28:40,354 --> 00:28:43,639
este 3.43 probablemente da un máximo de lo buenos que podríamos llegar a ser con eso, 

523
00:28:43,639 --> 00:28:45,740
o al menos de lo bueno que podría llegar a ser con eso.

524
00:28:46,240 --> 00:28:49,155
Ese mejor rendimiento esencialmente solo utiliza las ideas de las 

525
00:28:49,155 --> 00:28:51,585
que he estado hablando aquí, pero va un poco más allá, 

526
00:28:51,585 --> 00:28:55,120
como si buscara la información esperada dos pasos adelante en lugar de solo uno.

527
00:28:55,620 --> 00:28:57,964
Originalmente estaba planeando hablar más sobre eso, 

528
00:28:57,964 --> 00:29:00,220
pero me doy cuenta de que ya hemos durado bastante.

529
00:29:00,580 --> 00:29:03,420
Lo único que diré es que después de hacer esta búsqueda de dos pasos y 

530
00:29:03,420 --> 00:29:06,460
luego ejecutar un par de simulaciones de muestra en los mejores candidatos, 

531
00:29:06,460 --> 00:29:09,100
hasta ahora al menos para mí parece que Crane es el mejor abridor.

532
00:29:09,100 --> 00:29:10,060
¿Quién lo hubiera adivinado?

533
00:29:10,920 --> 00:29:14,246
Además, si utiliza la lista de palabras verdaderas para determinar su espacio de 

534
00:29:14,246 --> 00:29:17,820
posibilidades, entonces la incertidumbre con la que comienza es un poco más de 11 bits.

535
00:29:18,300 --> 00:29:21,114
Y resulta que, sólo a partir de una búsqueda de fuerza bruta, 

536
00:29:21,114 --> 00:29:24,926
la máxima información esperada posible después de las dos primeras conjeturas es de 

537
00:29:24,926 --> 00:29:25,880
alrededor de 10 bits.

538
00:29:26,500 --> 00:29:30,755
Lo que sugiere que en el mejor de los casos, después de tus dos primeras conjeturas, 

539
00:29:30,755 --> 00:29:34,560
con un juego perfectamente óptimo, te quedarás con un poco de incertidumbre.

540
00:29:34,800 --> 00:29:37,960
Lo que es lo mismo que limitarse a dos posibles conjeturas.

541
00:29:37,960 --> 00:29:41,031
Así que creo que es justo y probablemente bastante conservador decir que 

542
00:29:41,031 --> 00:29:44,608
nunca sería posible escribir un algoritmo que consiga este promedio tan bajo como 3, 

543
00:29:44,608 --> 00:29:47,511
porque con las palabras disponibles, simplemente no hay espacio para 

544
00:29:47,511 --> 00:29:50,540
obtener suficiente información después de sólo dos pasos para ser capaz 

545
00:29:50,540 --> 00:29:53,360
de garantizar la respuesta en el tercer espacio cada vez sin falta.

