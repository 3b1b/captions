1
00:00:00,000 --> 00:00:03,180
El juego Wurdle se ha vuelto bastante viral en los últimos dos meses y,

2
00:00:03,180 --> 00:00:06,317
como nunca desperdiciaré la oportunidad de una lección de matemáticas,

3
00:00:06,317 --> 00:00:09,585
se me ocurre que este juego es un muy buen ejemplo central en una lección

4
00:00:09,585 --> 00:00:13,120
sobre teoría de la información y, en particular, un tema conocido como entropía.

5
00:00:13,120 --> 00:00:16,124
Verá, como mucha gente, me quedé atrapado en el rompecabezas,

6
00:00:16,124 --> 00:00:19,468
y como muchos programadores, también me quedé atrapado en el intento

7
00:00:19,468 --> 00:00:23,200
de escribir un algoritmo que jugara el juego de la manera más óptima posible.

8
00:00:23,200 --> 00:00:26,096
Y lo que pensé que haría aquí es simplemente hablarles sobre

9
00:00:26,096 --> 00:00:29,230
mi proceso y explicarles algunas de las matemáticas que implican,

10
00:00:29,230 --> 00:00:32,080
ya que todo el algoritmo se centra en esta idea de entropía.

11
00:00:32,080 --> 00:00:42,180
Lo primero es lo primero, en caso de que no hayas oído hablar de él, ¿qué es Wurdle?

12
00:00:42,180 --> 00:00:45,370
Y para matar dos pájaros de un tiro mientras repasamos las reglas del juego,

13
00:00:45,370 --> 00:00:47,857
permítanme también un avance de hacia dónde vamos con esto,

14
00:00:47,857 --> 00:00:51,380
que es desarrollar un pequeño algoritmo que básicamente jugará el juego por nosotros.

15
00:00:51,380 --> 00:00:55,860
Aunque no he hecho el Wurdle de hoy, es el 4 de febrero y veremos cómo le va al robot.

16
00:00:55,860 --> 00:00:58,240
El objetivo de Wurdle es adivinar una palabra misteriosa de

17
00:00:58,240 --> 00:01:00,860
cinco letras y tienes seis oportunidades diferentes para adivinar.

18
00:01:00,860 --> 00:01:05,240
Por ejemplo, mi robot Wurdle me sugiere que empiece con la grúa de adivinanzas.

19
00:01:05,240 --> 00:01:08,160
Cada vez que haces una suposición, obtienes información sobre

20
00:01:08,160 --> 00:01:10,940
qué tan cerca está tu suposición de la respuesta verdadera.

21
00:01:10,940 --> 00:01:14,540
Aquí el cuadro gris me dice que no hay una C en la respuesta real.

22
00:01:14,540 --> 00:01:18,340
El cuadro amarillo me dice que hay una R, pero no está en esa posición.

23
00:01:18,340 --> 00:01:22,820
El cuadro verde me dice que la palabra secreta tiene una A y está en la tercera posición.

24
00:01:22,820 --> 00:01:24,300
Y luego no hay N y no hay E.

25
00:01:24,300 --> 00:01:27,420
Así que déjame entrar y decirle esa información al robot Wurdle.

26
00:01:27,420 --> 00:01:31,500
Empezamos con grúa, obtuvimos gris, amarillo, verde, gris, gris.

27
00:01:31,500 --> 00:01:34,127
No te preocupes por todos los datos que están mostrando ahora mismo,

28
00:01:34,127 --> 00:01:35,460
te lo explicaré a su debido tiempo.

29
00:01:35,460 --> 00:01:39,700
Pero su principal sugerencia para nuestra segunda elección es un truco.

30
00:01:39,700 --> 00:01:42,366
Y tu suposición tiene que ser una palabra real de cinco letras,

31
00:01:42,366 --> 00:01:45,700
pero como verás, es bastante liberal con lo que realmente te permitirá adivinar.

32
00:01:45,700 --> 00:01:48,860
En este caso, intentamos stick.

33
00:01:48,860 --> 00:01:50,260
Y bueno, la cosa pinta bastante bien.

34
00:01:50,260 --> 00:01:54,740
Pulsamos la S y la H, así conocemos las tres primeras letras, sabemos que hay una R.

35
00:01:54,740 --> 00:01:59,740
Y entonces será como SHA algo R, o SHA R algo.

36
00:01:59,740 --> 00:02:05,220
Y parece que el robot Wurdle sabe que solo hay dos posibilidades: fragmentar o filoso.

37
00:02:05,220 --> 00:02:07,880
Eso es una especie de volatilidad entre ellos en este momento,

38
00:02:07,880 --> 00:02:11,260
así que supongo que probablemente solo porque es alfabético va con el fragmento.

39
00:02:11,260 --> 00:02:13,000
Qué hurra, es la respuesta real.

40
00:02:13,000 --> 00:02:14,660
Entonces lo tenemos en tres.

41
00:02:14,660 --> 00:02:17,614
Si te preguntas si eso es bueno, la forma en que escuché a

42
00:02:17,614 --> 00:02:20,820
una persona decir que con Wurdle cuatro es par y tres es birdie.

43
00:02:20,820 --> 00:02:22,960
Lo cual creo que es una analogía bastante adecuada.

44
00:02:22,960 --> 00:02:25,896
Tienes que ser constante en tu juego para conseguir cuatro,

45
00:02:25,896 --> 00:02:27,560
pero ciertamente no es una locura.

46
00:02:27,560 --> 00:02:30,000
Pero cuando lo obtienes en tres, se siente genial.

47
00:02:30,000 --> 00:02:33,169
Entonces, si estás dispuesto a hacerlo, lo que me gustaría hacer aquí es simplemente

48
00:02:33,169 --> 00:02:36,338
hablar sobre mi proceso de pensamiento desde el principio sobre cómo abordo el robot

49
00:02:36,338 --> 00:02:36,600
Wurdle.

50
00:02:36,600 --> 00:02:39,800
Y como dije, en realidad es una excusa para una lección de teoría de la información.

51
00:02:39,800 --> 00:02:48,560
El objetivo principal es explicar qué es información y qué es entropía.

52
00:02:48,560 --> 00:02:50,867
Lo primero que pensé al abordar esto fue observar las

53
00:02:50,867 --> 00:02:53,560
frecuencias relativas de diferentes letras en el idioma inglés.

54
00:02:53,560 --> 00:02:56,503
Entonces pensé, bueno, ¿hay una suposición inicial o un par de

55
00:02:56,503 --> 00:02:59,960
suposiciones iniciales que acierten muchas de estas letras más frecuentes?

56
00:02:59,960 --> 00:03:03,780
Y una que me gustaba mucho era hacer otras seguidas de uñas.

57
00:03:03,780 --> 00:03:06,900
La idea es que si tocas una letra, ya sabes, obtienes un verde o un amarillo,

58
00:03:06,900 --> 00:03:07,980
eso siempre se siente bien.

59
00:03:07,980 --> 00:03:09,460
Parece que estás recibiendo información.

60
00:03:09,460 --> 00:03:12,651
Pero en estos casos, incluso si no aciertas y siempre aparecen grises,

61
00:03:12,651 --> 00:03:15,437
eso te da mucha información ya que es bastante raro encontrar

62
00:03:15,437 --> 00:03:17,640
una palabra que no tenga ninguna de estas letras.

63
00:03:17,640 --> 00:03:20,606
Pero aun así, esto no parece súper sistemático, porque,

64
00:03:20,606 --> 00:03:23,520
por ejemplo, no tiene en cuenta el orden de las letras.

65
00:03:23,520 --> 00:03:26,080
¿Por qué escribir uñas cuando puedo escribir caracol?

66
00:03:26,080 --> 00:03:27,720
¿Es mejor tener esa S al final?

67
00:03:27,720 --> 00:03:28,720
No estoy realmente seguro.

68
00:03:28,720 --> 00:03:32,570
Ahora, un amigo mío dijo que le gustaba comenzar con la palabra cansado,

69
00:03:32,570 --> 00:03:37,160
lo que me sorprendió un poco porque tiene algunas letras poco comunes como la W y la Y.

70
00:03:37,160 --> 00:03:39,400
Pero quién sabe, tal vez ese sea un mejor comienzo.

71
00:03:39,400 --> 00:03:42,284
¿Existe algún tipo de puntuación cuantitativa que podamos

72
00:03:42,284 --> 00:03:44,920
dar para juzgar la calidad de una posible suposición?

73
00:03:44,920 --> 00:03:48,086
Ahora, para preparar la forma en que vamos a clasificar las posibles conjeturas,

74
00:03:48,086 --> 00:03:51,565
retrocedamos y agreguemos un poco de claridad sobre cómo está configurado exactamente el

75
00:03:51,565 --> 00:03:51,800
juego.

76
00:03:51,800 --> 00:03:54,796
Entonces, hay una lista de palabras que le permitirá ingresar y que se

77
00:03:54,796 --> 00:03:57,920
consideran conjeturas válidas y que tiene aproximadamente 13,000 palabras.

78
00:03:57,920 --> 00:04:01,147
Pero cuando lo miras, hay muchas cosas realmente poco comunes,

79
00:04:01,147 --> 00:04:05,246
cosas como una cabeza o Ali y ARG, el tipo de palabras que provocan discusiones

80
00:04:05,246 --> 00:04:07,040
familiares en un juego de Scrabble.

81
00:04:07,040 --> 00:04:10,600
Pero la sensación del juego es que la respuesta siempre será una palabra bastante común.

82
00:04:10,600 --> 00:04:16,080
Y de hecho, hay otra lista de alrededor de 2300 palabras que son las posibles respuestas.

83
00:04:16,080 --> 00:04:19,044
Y esta es una lista seleccionada por humanos, creo que específicamente

84
00:04:19,044 --> 00:04:21,800
por la novia del creador del juego, lo cual es bastante divertido.

85
00:04:21,800 --> 00:04:25,962
Pero lo que me gustaría hacer, nuestro desafío para este proyecto es ver si podemos

86
00:04:25,962 --> 00:04:30,422
escribir un programa resolviendo Wordle que no incorpore conocimientos previos sobre esta

87
00:04:30,422 --> 00:04:30,720
lista.

88
00:04:30,720 --> 00:04:33,140
Por un lado, hay muchas palabras de cinco letras

89
00:04:33,140 --> 00:04:35,560
bastante comunes que no encontrarás en esa lista.

90
00:04:35,560 --> 00:04:38,720
Por lo tanto, sería mejor escribir un programa que sea un poco más resistente y

91
00:04:38,720 --> 00:04:41,960
que pueda jugar con Wordle contra cualquiera, no solo contra el sitio web oficial.

92
00:04:41,960 --> 00:04:44,677
Y también la razón por la que sabemos cuál es esta lista de

93
00:04:44,677 --> 00:04:47,440
posibles respuestas es porque es visible en el código fuente.

94
00:04:47,440 --> 00:04:50,140
Pero la forma en que es visible en el código fuente es en el

95
00:04:50,140 --> 00:04:52,840
orden específico en el que aparecen las respuestas día a día.

96
00:04:52,840 --> 00:04:56,400
Así que siempre puedes buscar cuál será la respuesta de mañana.

97
00:04:56,400 --> 00:04:59,140
Claramente, en cierto sentido usar la lista es hacer trampa.

98
00:04:59,140 --> 00:05:02,287
Y lo que hace que el rompecabezas sea más interesante y una lección de

99
00:05:02,287 --> 00:05:05,700
teoría de la información más rica es utilizar algunos datos más universales,

100
00:05:05,700 --> 00:05:08,315
como las frecuencias relativas de las palabras en general,

101
00:05:08,315 --> 00:05:11,640
para capturar esta intuición de tener preferencia por palabras más comunes.

102
00:05:11,640 --> 00:05:16,560
Entonces, de estas 13.000 posibilidades, ¿cómo deberíamos elegir la suposición inicial?

103
00:05:16,560 --> 00:05:19,960
Por ejemplo, si mi amigo me propone cansancio, ¿cómo debemos analizar su calidad?

104
00:05:19,960 --> 00:05:23,975
Bueno, la razón por la que dijo que le gusta esa improbable W es que le

105
00:05:23,975 --> 00:05:27,880
gusta la naturaleza remota de lo bien que se siente si aciertas esa W.

106
00:05:27,880 --> 00:05:31,138
Por ejemplo, si el primer patrón revelado fue algo como este,

107
00:05:31,138 --> 00:05:35,291
entonces resulta que solo hay 58 palabras en este léxico gigante que coinciden

108
00:05:35,291 --> 00:05:36,080
con ese patrón.

109
00:05:36,080 --> 00:05:38,900
Entonces esa es una gran reducción de 13.000.

110
00:05:38,900 --> 00:05:41,107
Pero la otra cara de la moneda, por supuesto, es

111
00:05:41,107 --> 00:05:43,360
que es muy poco común obtener un patrón como este.

112
00:05:43,360 --> 00:05:47,544
Específicamente, si cada palabra tuviera la misma probabilidad de ser la respuesta,

113
00:05:47,544 --> 00:05:51,680
la probabilidad de encontrar este patrón sería 58 dividido por alrededor de 13.000.

114
00:05:51,680 --> 00:05:53,880
Por supuesto, no es igualmente probable que sean respuestas.

115
00:05:53,880 --> 00:05:56,680
La mayoría de ellas son palabras muy oscuras e incluso cuestionables.

116
00:05:56,680 --> 00:05:58,859
Pero al menos para nuestra primera aproximación a todo esto,

117
00:05:58,859 --> 00:06:02,040
supongamos que todas son igualmente probables y luego refinemos eso un poco más adelante.

118
00:06:02,040 --> 00:06:04,628
La cuestión es que, por su propia naturaleza, es poco

119
00:06:04,628 --> 00:06:07,360
probable que se produzca un patrón con mucha información.

120
00:06:07,360 --> 00:06:11,920
De hecho, lo que significa ser informativo es que es poco probable.

121
00:06:11,920 --> 00:06:16,451
Un patrón mucho más probable de ver con esta apertura sería algo como esto,

122
00:06:16,451 --> 00:06:18,360
donde por supuesto no hay una W.

123
00:06:18,360 --> 00:06:22,080
Tal vez haya una E, y tal vez no haya una A, no haya una R, no haya una Y.

124
00:06:22,080 --> 00:06:24,640
En este caso, hay 1400 coincidencias posibles.

125
00:06:24,640 --> 00:06:27,355
Si todas fueran igualmente probables, resulta que hay una

126
00:06:27,355 --> 00:06:30,680
probabilidad de alrededor del 11% de que este sea el patrón que verías.

127
00:06:30,680 --> 00:06:34,320
De modo que los resultados más probables son también los menos informativos.

128
00:06:34,320 --> 00:06:38,107
Para obtener una visión más global, permítame mostrarle la distribución

129
00:06:38,107 --> 00:06:42,000
completa de probabilidades en todos los diferentes patrones que pueda ver.

130
00:06:42,000 --> 00:06:45,670
Entonces, cada barra que estás viendo corresponde a un posible patrón de

131
00:06:45,670 --> 00:06:49,289
colores que podría revelarse, de los cuales hay de 3 a 5 posibilidades,

132
00:06:49,289 --> 00:06:52,960
y están organizados de izquierda a derecha, del más común al menos común.

133
00:06:52,960 --> 00:06:56,200
Entonces, la posibilidad más común aquí es que obtengas todos los grises.

134
00:06:56,200 --> 00:06:58,800
Esto sucede aproximadamente el 14% del tiempo.

135
00:06:58,800 --> 00:07:02,488
Y lo que esperas cuando haces una suposición es terminar en algún

136
00:07:02,488 --> 00:07:06,287
lugar de esta larga cola, como aquí donde solo hay 18 posibilidades

137
00:07:06,287 --> 00:07:09,920
para lo que coincide con este patrón que evidentemente se ve así.

138
00:07:09,920 --> 00:07:12,316
O si nos aventuramos un poco más hacia la izquierda,

139
00:07:12,316 --> 00:07:14,080
ya sabes, tal vez lleguemos hasta aquí.

140
00:07:14,080 --> 00:07:16,560
Bien, aquí tienes un buen rompecabezas.

141
00:07:16,560 --> 00:07:19,684
¿Cuáles son las tres palabras en inglés que comienzan con W,

142
00:07:19,684 --> 00:07:22,040
terminan con Y y tienen una R en alguna parte?

143
00:07:22,040 --> 00:07:27,560
Resulta que las respuestas son, veamos, prolijas, llenas de gusanos e irónicas.

144
00:07:27,560 --> 00:07:30,932
Entonces, para juzgar qué tan buena es esta palabra en general,

145
00:07:30,932 --> 00:07:35,253
queremos algún tipo de medida de la cantidad esperada de información que obtendrá

146
00:07:35,253 --> 00:07:36,360
de esta distribución.

147
00:07:36,360 --> 00:07:41,059
Si analizamos cada patrón y multiplicamos su probabilidad de ocurrir por algo

148
00:07:41,059 --> 00:07:46,000
que mida qué tan informativo es, eso tal vez pueda darnos una puntuación objetiva.

149
00:07:46,000 --> 00:07:48,029
Ahora tu primer instinto sobre lo que debería

150
00:07:48,029 --> 00:07:50,280
ser ese algo podría ser el número de coincidencias.

151
00:07:50,280 --> 00:07:52,960
Quieres un número promedio más bajo de coincidencias.

152
00:07:52,960 --> 00:07:58,931
Pero en lugar de eso me gustaría usar una medida más universal que a menudo atribuimos

153
00:07:58,931 --> 00:08:04,422
a la información, y una que será más flexible una vez que tengamos asignada una

154
00:08:04,422 --> 00:08:10,600
probabilidad diferente a cada una de estas 13.000 palabras sobre si son o no la respuesta.

155
00:08:10,600 --> 00:08:15,054
La unidad de información estándar es el bit, que tiene una fórmula un poco divertida,

156
00:08:15,054 --> 00:08:17,800
pero es realmente intuitiva si solo miramos ejemplos.

157
00:08:17,800 --> 00:08:22,011
Si tienes una observación que reduce a la mitad tu espacio de posibilidades,

158
00:08:22,011 --> 00:08:24,200
decimos que tiene un bit de información.

159
00:08:24,200 --> 00:08:26,922
En nuestro ejemplo, el espacio de posibilidades son todas las palabras posibles,

160
00:08:26,922 --> 00:08:29,778
y resulta que aproximadamente la mitad de las palabras de cinco letras tienen una S,

161
00:08:29,778 --> 00:08:31,560
un poco menos que eso, pero aproximadamente la mitad.

162
00:08:31,560 --> 00:08:35,200
Entonces esa observación les daría un poco de información.

163
00:08:35,200 --> 00:08:39,803
Si en cambio un hecho nuevo reduce ese espacio de posibilidades en un factor de cuatro,

164
00:08:39,803 --> 00:08:42,000
decimos que tiene dos bits de información.

165
00:08:42,000 --> 00:08:45,120
Por ejemplo, resulta que aproximadamente una cuarta parte de estas palabras tienen una T.

166
00:08:45,120 --> 00:08:47,948
Si la observación corta ese espacio por un factor de ocho,

167
00:08:47,948 --> 00:08:50,920
decimos que son tres bits de información, y así sucesivamente.

168
00:08:50,920 --> 00:08:55,000
Cuatro bits lo cortan en un 16, cinco bits lo cortan en un 32.

169
00:08:55,000 --> 00:08:58,340
Así que ahora quizás quieras hacer una pausa y preguntarte:

170
00:08:58,340 --> 00:09:03,016
¿cuál es la fórmula para obtener información sobre el número de bits en términos de

171
00:09:03,016 --> 00:09:04,520
probabilidad de que ocurra?

172
00:09:04,520 --> 00:09:08,123
Lo que estamos diciendo aquí es que cuando se le suma la mitad al número de bits,

173
00:09:08,123 --> 00:09:11,814
eso es lo mismo que la probabilidad, que es lo mismo que decir que dos elevado a la

174
00:09:11,814 --> 00:09:14,363
potencia del número de bits es uno sobre la probabilidad,

175
00:09:14,363 --> 00:09:18,098
lo cual se reordena además para decir que la información es el logaritmo en base dos

176
00:09:18,098 --> 00:09:19,680
de uno dividido por la probabilidad.

177
00:09:19,680 --> 00:09:21,984
Y a veces se ve esto con un reordenamiento más,

178
00:09:21,984 --> 00:09:25,680
donde la información es el logaritmo negativo en base dos de la probabilidad.

179
00:09:25,680 --> 00:09:29,168
Expresado así, puede parecer un poco extraño para los no iniciados,

180
00:09:29,168 --> 00:09:32,246
pero en realidad es sólo la idea muy intuitiva de preguntar

181
00:09:32,246 --> 00:09:35,120
cuántas veces has reducido tus posibilidades a la mitad.

182
00:09:35,120 --> 00:09:37,599
Ahora, si te lo estás preguntando, ya sabes, pensé que solo estábamos jugando

183
00:09:37,599 --> 00:09:39,920
un divertido juego de palabras, ¿por qué los logaritmos entran en escena?

184
00:09:39,920 --> 00:09:44,456
Una de las razones por las que esta es una unidad más agradable es que es mucho más fácil

185
00:09:44,456 --> 00:09:48,791
hablar de eventos muy improbables, mucho más fácil decir que una observación tiene 20

186
00:09:48,791 --> 00:09:52,975
bits de información que decir que la probabilidad de que ocurra tal o cual cosa es

187
00:09:52,975 --> 00:09:53,480
0.0000095.

188
00:09:53,480 --> 00:09:57,593
Pero una razón más sustancial por la que esta expresión logarítmica resultó ser una

189
00:09:57,593 --> 00:10:02,000
adición muy útil a la teoría de la probabilidad es la forma en que se suma la información.

190
00:10:02,000 --> 00:10:05,434
Por ejemplo, si una observación le proporciona dos bits de información,

191
00:10:05,434 --> 00:10:08,725
lo que reduce su espacio en cuatro, y luego una segunda observación,

192
00:10:08,725 --> 00:10:12,780
como su segunda suposición en Wordle, le proporciona otros tres bits de información,

193
00:10:12,780 --> 00:10:15,118
lo que lo reduce aún más en otro factor de ocho,

194
00:10:15,118 --> 00:10:17,360
la dos juntos te dan cinco bits de información.

195
00:10:17,360 --> 00:10:19,983
De la misma manera que a las probabilidades les gusta multiplicarse,

196
00:10:19,983 --> 00:10:21,200
a la información le gusta sumar.

197
00:10:21,200 --> 00:10:24,565
Entonces, tan pronto como estamos en el ámbito de algo así como un valor esperado,

198
00:10:24,565 --> 00:10:26,997
donde sumamos un montón de números, los registros hacen que

199
00:10:26,997 --> 00:10:28,660
sea mucho más agradable tratar con ellos.

200
00:10:28,660 --> 00:10:32,800
Volvamos a nuestra distribución de Weary y agreguemos otro pequeño rastreador aquí,

201
00:10:32,800 --> 00:10:35,560
que nos muestra cuánta información hay para cada patrón.

202
00:10:35,560 --> 00:10:39,506
Lo principal que quiero que note es que cuanto mayor es la probabilidad a medida que

203
00:10:39,506 --> 00:10:43,500
llegamos a esos patrones más probables, menor es la información y menos bits se ganan.

204
00:10:43,500 --> 00:10:47,263
La forma en que medimos la calidad de esta suposición será tomando el valor

205
00:10:47,263 --> 00:10:50,235
esperado de esta información, donde analizamos cada patrón,

206
00:10:50,235 --> 00:10:53,850
decimos qué tan probable es y luego lo multiplicamos por cuántos bits de

207
00:10:53,850 --> 00:10:54,940
información obtenemos.

208
00:10:54,940 --> 00:10:58,480
Y en el ejemplo de Weary, resulta ser 4.9 bits.

209
00:10:58,480 --> 00:11:02,002
Entonces, en promedio, la información que obtienes de esta suposición inicial

210
00:11:02,002 --> 00:11:05,660
es tan buena como cortar tu espacio de posibilidades a la mitad unas cinco veces.

211
00:11:05,660 --> 00:11:09,209
Por el contrario, un ejemplo de una suposición con un

212
00:11:09,209 --> 00:11:13,220
valor de información esperado más alto sería algo como Slate.

213
00:11:13,220 --> 00:11:16,180
En este caso notarás que la distribución luce mucho más plana.

214
00:11:16,180 --> 00:11:19,517
En particular, la aparición más probable de todos los grises solo

215
00:11:19,517 --> 00:11:22,855
tiene alrededor de un 6% de posibilidades de ocurrir, por lo que,

216
00:11:22,855 --> 00:11:25,940
como mínimo, evidentemente obtendrás 3.9 bits de información.

217
00:11:25,940 --> 00:11:29,140
Pero eso es un mínimo, normalmente obtendrás algo mejor que eso.

218
00:11:29,140 --> 00:11:33,782
Y resulta que cuando haces cálculos en este caso y sumas todos los términos relevantes,

219
00:11:33,782 --> 00:11:36,420
la información promedio es de aproximadamente 5.8.

220
00:11:36,420 --> 00:11:39,990
Entonces, a diferencia de Weary, su espacio de posibilidades será

221
00:11:39,990 --> 00:11:43,940
aproximadamente la mitad después de esta primera suposición, en promedio.

222
00:11:43,940 --> 00:11:46,821
De hecho, hay una historia divertida sobre el nombre

223
00:11:46,821 --> 00:11:49,540
de este valor esperado de cantidad de información.

224
00:11:49,540 --> 00:11:52,205
La teoría de la información fue desarrollada por Claude Shannon,

225
00:11:52,205 --> 00:11:54,707
que trabajaba en los Laboratorios Bell en la década de 1940,

226
00:11:54,707 --> 00:11:58,151
pero estaba hablando de algunas de sus ideas aún por publicar con John von Neumann,

227
00:11:58,151 --> 00:12:00,653
que era este gigante intelectual de la época, muy destacado.

228
00:12:00,653 --> 00:12:04,180
en matemáticas y física y los inicios de lo que se estaba convirtiendo en informática.

229
00:12:04,180 --> 00:12:07,583
Y cuando mencionó que realmente no tenía un buen nombre para este valor

230
00:12:07,583 --> 00:12:10,986
esperado de la cantidad de información, supuestamente von Neumann dijo,

231
00:12:10,986 --> 00:12:14,720
según cuenta la historia, bueno, deberías llamarlo entropía, y por dos razones.

232
00:12:14,720 --> 00:12:18,715
En primer lugar, su función de incertidumbre se ha utilizado en mecánica estadística

233
00:12:18,715 --> 00:12:22,710
con ese nombre, por lo que ya tiene un nombre, y en segundo lugar, y más importante,

234
00:12:22,710 --> 00:12:26,940
nadie sabe qué es realmente la entropía, por lo que en un debate siempre tener la ventaja.

235
00:12:26,940 --> 00:12:29,662
Entonces, si el nombre parece un poco misterioso,

236
00:12:29,662 --> 00:12:33,420
y si hay que creer en esta historia, es más o menos intencionalmente.

237
00:12:33,420 --> 00:12:36,936
Además, si te preguntas acerca de su relación con toda esa segunda ley de la

238
00:12:36,936 --> 00:12:39,768
termodinámica de la física, definitivamente hay una conexión,

239
00:12:39,768 --> 00:12:43,832
pero en sus orígenes Shannon solo estaba tratando con la teoría de la probabilidad pura,

240
00:12:43,832 --> 00:12:46,801
y para nuestros propósitos aquí, cuando uso la palabra entropía,

241
00:12:46,801 --> 00:12:50,820
solo quiero que piense en el valor de información esperado de una suposición particular.

242
00:12:50,820 --> 00:12:54,380
Puedes pensar que la entropía mide dos cosas simultáneamente.

243
00:12:54,380 --> 00:12:57,420
El primero es qué tan plana es la distribución.

244
00:12:57,420 --> 00:13:01,700
Cuanto más cercana a la uniformidad sea una distribución, mayor será la entropía.

245
00:13:01,700 --> 00:13:06,965
En nuestro caso, donde hay de 3 a 5 patrones en total, para una distribución uniforme,

246
00:13:06,965 --> 00:13:12,049
observar cualquiera de ellos tendría un registro de información en base 2 de 3 a 5,

247
00:13:12,049 --> 00:13:17,315
que resulta ser 7.92, por lo que ese es el máximo absoluto que podrías tener para esta

248
00:13:17,315 --> 00:13:17,860
entropía.

249
00:13:17,860 --> 00:13:20,512
Pero la entropía también es una especie de medida

250
00:13:20,512 --> 00:13:22,900
de cuántas posibilidades hay en primer lugar.

251
00:13:22,900 --> 00:13:27,857
Por ejemplo, si tienes una palabra en la que sólo hay 16 patrones posibles y cada uno de

252
00:13:27,857 --> 00:13:32,760
ellos es igualmente probable, esta entropía, esta información esperada, sería de 4 bits.

253
00:13:32,760 --> 00:13:36,854
Pero si tienes otra palabra donde hay 64 patrones posibles que podrían surgir,

254
00:13:36,854 --> 00:13:41,000
y todos son igualmente probables, entonces la entropía resultaría ser de 6 bits.

255
00:13:41,000 --> 00:13:45,852
Entonces, si ves alguna distribución en la naturaleza que tiene una entropía de 6 bits,

256
00:13:45,852 --> 00:13:50,264
es como si estuviera diciendo que hay tanta variación e incertidumbre en lo que

257
00:13:50,264 --> 00:13:54,400
está a punto de suceder como si hubiera 64 resultados igualmente probables.

258
00:13:54,400 --> 00:13:58,360
Para mi primera pasada por el Wurtelebot, básicamente le pedí que hiciera esto.

259
00:13:58,360 --> 00:14:02,725
Revisa todas las conjeturas posibles que puedas tener, las 13.000 palabras,

260
00:14:02,725 --> 00:14:05,941
calcula la entropía de cada una, o más específicamente,

261
00:14:05,941 --> 00:14:10,766
la entropía de la distribución en todos los patrones que puedas ver, para cada una,

262
00:14:10,766 --> 00:14:15,074
y elige la más alta, ya que es el que probablemente reducirá su espacio de

263
00:14:15,074 --> 00:14:17,200
posibilidades tanto como sea posible.

264
00:14:17,200 --> 00:14:19,760
Y aunque aquí solo he estado hablando de la primera suposición,

265
00:14:19,760 --> 00:14:21,680
ocurre lo mismo con las siguientes suposiciones.

266
00:14:21,680 --> 00:14:24,557
Por ejemplo, después de ver algún patrón en esa primera suposición,

267
00:14:24,557 --> 00:14:27,941
que lo restringiría a un número menor de palabras posibles en función de lo que

268
00:14:27,941 --> 00:14:31,453
coincide con eso, simplemente juega el mismo juego con respecto a ese conjunto más

269
00:14:31,453 --> 00:14:32,300
pequeño de palabras.

270
00:14:32,300 --> 00:14:36,549
Para una segunda suposición propuesta, observamos la distribución de todos los

271
00:14:36,549 --> 00:14:41,014
patrones que podrían ocurrir a partir de ese conjunto más restringido de palabras,

272
00:14:41,014 --> 00:14:45,480
buscamos entre las 13.000 posibilidades y encontramos la que maximiza esa entropía.

273
00:14:45,480 --> 00:14:48,217
Para mostrarles cómo funciona esto en acción, permítanme

274
00:14:48,217 --> 00:14:51,146
mostrarles una pequeña variante de Wurtele que escribí y que

275
00:14:51,146 --> 00:14:54,460
muestra los aspectos más destacados de este análisis en los márgenes.

276
00:14:54,460 --> 00:14:56,746
Después de hacer todos los cálculos de entropía,

277
00:14:56,746 --> 00:15:00,340
aquí a la derecha nos muestra cuáles tienen la información esperada más alta.

278
00:15:00,340 --> 00:15:06,531
Resulta que la respuesta principal, al menos por el momento, lo refinaremos más adelante,

279
00:15:06,531 --> 00:15:11,140
es Tares, que significa, por supuesto, arveja, la arveja más común.

280
00:15:11,140 --> 00:15:14,315
Cada vez que hacemos una suposición aquí, donde tal vez ignoro sus

281
00:15:14,315 --> 00:15:16,875
recomendaciones y elijo slate, porque me gusta slate,

282
00:15:16,875 --> 00:15:19,671
podemos ver cuánta información esperada tenía, pero luego,

283
00:15:19,671 --> 00:15:23,652
a la derecha de la palabra aquí, nos muestra cuánta información real que obtuvimos,

284
00:15:23,652 --> 00:15:24,980
dado este patrón particular.

285
00:15:24,980 --> 00:15:27,465
Así que aquí parece que tuvimos un poco de mala suerte,

286
00:15:27,465 --> 00:15:30,660
se esperaba que obtuviéramos 5.8, pero obtuvimos algo con menos que eso.

287
00:15:30,660 --> 00:15:33,420
Y luego, en el lado izquierdo, aquí nos muestra todas las diferentes

288
00:15:33,420 --> 00:15:35,860
palabras posibles según el lugar donde nos encontramos ahora.

289
00:15:35,860 --> 00:15:38,829
Las barras azules nos dicen qué tan probable cree que es cada palabra,

290
00:15:38,829 --> 00:15:41,672
por lo que por el momento suponemos que cada palabra tiene la misma

291
00:15:41,672 --> 00:15:44,140
probabilidad de ocurrir, pero lo refinaremos en un momento.

292
00:15:44,140 --> 00:15:48,136
Y luego esta medida de incertidumbre nos dice la entropía de esta distribución entre

293
00:15:48,136 --> 00:15:52,038
las palabras posibles, que ahora mismo, debido a que es una distribución uniforme,

294
00:15:52,038 --> 00:15:55,940
es solo una forma innecesariamente complicada de contar el número de posibilidades.

295
00:15:55,940 --> 00:15:59,945
Por ejemplo, si tuviéramos que elevar 2 a la potencia de 13.66,

296
00:15:59,945 --> 00:16:02,700
eso debería rondar las 13.000 posibilidades.

297
00:16:02,700 --> 00:16:06,780
Estoy un poco fuera de lugar aquí, pero sólo porque no muestro todos los decimales.

298
00:16:06,780 --> 00:16:10,214
Por el momento, esto puede parecer redundante y complicar demasiado las cosas,

299
00:16:10,214 --> 00:16:12,780
pero verá por qué es útil tener ambos números en un minuto.

300
00:16:12,780 --> 00:16:16,341
Así que aquí parece que sugiere que la entropía más alta para nuestra

301
00:16:16,341 --> 00:16:19,700
segunda suposición es Ramen, que nuevamente no parece una palabra.

302
00:16:19,700 --> 00:16:25,660
Entonces, para tener autoridad moral aquí, seguiré adelante y escribiré Rains.

303
00:16:25,660 --> 00:16:27,540
Y nuevamente parece que tuvimos un poco de mala suerte.

304
00:16:27,540 --> 00:16:32,100
Esperábamos 4.3 bits y solo tenemos 3.39 bits de información.

305
00:16:32,100 --> 00:16:35,060
Eso nos lleva a 55 posibilidades.

306
00:16:35,060 --> 00:16:38,300
Y aquí tal vez me quede con lo que sugiere, que es combo,

307
00:16:38,300 --> 00:16:40,200
sea lo que sea que eso signifique.

308
00:16:40,200 --> 00:16:43,300
Y está bien, esta es en realidad una buena oportunidad para resolver un rompecabezas.

309
00:16:43,300 --> 00:16:47,020
Nos dice que este patrón nos da 4.7 bits de información.

310
00:16:47,020 --> 00:16:52,400
Pero a la izquierda, antes de que veamos ese patrón, había 5.78 bits de incertidumbre.

311
00:16:52,400 --> 00:16:56,860
Entonces, a modo de prueba, ¿qué significa eso sobre el número de posibilidades restantes?

312
00:16:56,860 --> 00:17:01,062
Bueno, significa que estamos reducidos a un poco de incertidumbre,

313
00:17:01,062 --> 00:17:04,700
que es lo mismo que decir que hay dos respuestas posibles.

314
00:17:04,700 --> 00:17:06,520
Es una elección 50-50.

315
00:17:06,520 --> 00:17:09,452
Y a partir de aquí, porque tú y yo sabemos qué palabras son más comunes,

316
00:17:09,452 --> 00:17:11,220
sabemos que la respuesta debería ser abismo.

317
00:17:11,220 --> 00:17:13,540
Pero tal como está escrito ahora, el programa no lo sabe.

318
00:17:13,540 --> 00:17:17,392
Así que sigue adelante, tratando de obtener tanta información como puede,

319
00:17:17,392 --> 00:17:20,360
hasta que sólo queda una posibilidad, y luego la adivina.

320
00:17:20,360 --> 00:17:22,700
Obviamente necesitamos una mejor estrategia para el final del juego.

321
00:17:22,700 --> 00:17:26,720
Pero digamos que llamamos a esta versión uno de nuestro solucionador de

322
00:17:26,720 --> 00:17:30,740
palabras y luego ejecutamos algunas simulaciones para ver cómo funciona.

323
00:17:30,740 --> 00:17:34,240
Entonces, la forma en que esto funciona es jugando todos los juegos de palabras posibles.

324
00:17:34,240 --> 00:17:38,780
Está repasando todas esas 2315 palabras que son las respuestas reales.

325
00:17:38,780 --> 00:17:41,340
Básicamente se trata de utilizarlo como un conjunto de pruebas.

326
00:17:41,340 --> 00:17:44,336
Y con este método ingenuo de no considerar qué tan común es

327
00:17:44,336 --> 00:17:48,632
una palabra y simplemente tratar de maximizar la información en cada paso del camino,

328
00:17:48,632 --> 00:17:50,480
hasta llegar a una y sólo una opción.

329
00:17:50,480 --> 00:17:55,100
Al final de la simulación, la puntuación media resulta ser de aproximadamente 4.124.

330
00:17:55,100 --> 00:17:59,780
Lo cual no está mal, para ser honesto, esperaba hacerlo peor.

331
00:17:59,780 --> 00:18:03,040
Pero la gente que juega wordle te dirá que normalmente lo consiguen en 4.

332
00:18:03,040 --> 00:18:05,260
El verdadero desafío es conseguir tantos en 3 como puedas.

333
00:18:05,260 --> 00:18:08,920
Es un salto bastante grande entre la puntuación de 4 y la puntuación de 3.

334
00:18:08,920 --> 00:18:19,091
Lo obvio aquí es incorporar de alguna manera si una palabra es común o no,

335
00:18:19,091 --> 00:18:23,160
y cómo lo hacemos exactamente.

336
00:18:23,160 --> 00:18:25,655
La forma en que lo acerqué es obtener una lista de las

337
00:18:25,655 --> 00:18:28,560
frecuencias relativas de todas las palabras en el idioma inglés.

338
00:18:28,560 --> 00:18:31,957
Y acabo de utilizar la función de datos de frecuencia de palabras de Mathematica,

339
00:18:31,957 --> 00:18:35,520
que a su vez se extrae del conjunto de datos públicos Ngram en inglés de Google Books.

340
00:18:35,520 --> 00:18:37,798
Y es divertido verlo, por ejemplo, si lo clasificamos

341
00:18:37,798 --> 00:18:40,120
desde las palabras más comunes hasta las menos comunes.

342
00:18:40,120 --> 00:18:43,740
Evidentemente estas son las palabras de cinco letras más comunes en el idioma inglés.

343
00:18:43,740 --> 00:18:46,480
O mejor dicho, este es el octavo más común.

344
00:18:46,480 --> 00:18:49,440
Primero es cuál, después está ahí y ahí.

345
00:18:49,440 --> 00:18:52,658
Primero en sí no es primero, sino noveno, y tiene sentido que estas

346
00:18:52,658 --> 00:18:55,071
otras palabras puedan aparecer con más frecuencia,

347
00:18:55,071 --> 00:18:59,000
donde las que siguen a primero son después, dónde, y que son un poco menos comunes.

348
00:18:59,000 --> 00:19:02,916
Ahora bien, al utilizar estos datos para modelar la probabilidad de que cada una de

349
00:19:02,916 --> 00:19:07,020
estas palabras sea la respuesta final, no debería ser sólo proporcional a la frecuencia.

350
00:19:07,020 --> 00:19:10,788
Por ejemplo, a la que se le da una puntuación de 0.002 en este conjunto de datos,

351
00:19:10,788 --> 00:19:13,269
mientras que la palabra trenza es, en cierto sentido,

352
00:19:13,269 --> 00:19:15,200
aproximadamente 1000 veces menos probable.

353
00:19:15,200 --> 00:19:17,507
Pero ambas son palabras bastante comunes que casi

354
00:19:17,507 --> 00:19:19,400
con seguridad vale la pena considerarlas.

355
00:19:19,400 --> 00:19:21,900
Por eso queremos más un límite binario.

356
00:19:21,900 --> 00:19:26,197
La forma en que lo hice es imaginar tomar toda esta lista ordenada de palabras,

357
00:19:26,197 --> 00:19:30,011
y luego organizarla en un eje x, y luego aplicar la función sigmoidea,

358
00:19:30,011 --> 00:19:34,417
que es la forma estándar de tener una función cuya salida es básicamente binaria,

359
00:19:34,417 --> 00:19:38,500
es 0 o 1, pero hay un suavizado intermedio para esa región de incertidumbre.

360
00:19:38,500 --> 00:19:43,801
Básicamente, la probabilidad que estoy asignando a cada palabra de estar en la lista

361
00:19:43,801 --> 00:19:49,165
final será el valor de la función sigmoidea arriba dondequiera que se encuentre en el

362
00:19:49,165 --> 00:19:49,540
eje x.

363
00:19:49,540 --> 00:19:53,349
Ahora bien, obviamente, esto depende de algunos parámetros, por ejemplo,

364
00:19:53,349 --> 00:19:57,680
qué tan ancho es el espacio en el eje x que ocupan esas palabras determina qué tan

365
00:19:57,680 --> 00:20:02,116
gradual o abruptamente bajamos de 1 a 0, y dónde las ubicamos de izquierda a derecha

366
00:20:02,116 --> 00:20:03,160
determina el límite.

367
00:20:03,160 --> 00:20:05,250
Para ser honesto, la forma en que hice esto fue

368
00:20:05,250 --> 00:20:07,340
simplemente lamerme el dedo y pegarlo al viento.

369
00:20:07,340 --> 00:20:10,650
Revisé la lista ordenada y traté de encontrar una ventana donde,

370
00:20:10,650 --> 00:20:14,012
cuando la miré, pensé que era más probable que aproximadamente la

371
00:20:14,012 --> 00:20:17,680
mitad de estas palabras fueran la respuesta final, y la usé como límite.

372
00:20:17,680 --> 00:20:20,649
Una vez que tenemos una distribución como esta entre las palabras,

373
00:20:20,649 --> 00:20:24,460
nos da otra situación en la que la entropía se convierte en una medida realmente útil.

374
00:20:24,460 --> 00:20:28,375
Por ejemplo, digamos que estamos jugando y comenzamos con mis viejos abridores,

375
00:20:28,375 --> 00:20:31,459
que eran plumas y clavos, y terminamos con una situación en la

376
00:20:31,459 --> 00:20:33,760
que hay cuatro palabras posibles que coinciden.

377
00:20:33,760 --> 00:20:36,440
Y digamos que los consideramos todos igualmente probables.

378
00:20:36,440 --> 00:20:40,000
Déjame preguntarte, ¿cuál es la entropía de esta distribución?

379
00:20:40,000 --> 00:20:45,400
Bueno, la información asociada a cada una de estas posibilidades va a

380
00:20:45,400 --> 00:20:50,800
ser el logaritmo en base 2 de 4, ya que cada una es 1 y 4, y eso es 2.

381
00:20:50,800 --> 00:20:52,780
Dos bits de información, cuatro posibilidades.

382
00:20:52,780 --> 00:20:54,360
Todo muy bien y bueno.

383
00:20:54,360 --> 00:20:58,320
Pero ¿y si te dijera que en realidad hay más de cuatro coincidencias?

384
00:20:58,320 --> 00:21:02,600
En realidad, cuando miramos la lista completa de palabras, hay 16 palabras que coinciden.

385
00:21:02,600 --> 00:21:05,374
Pero supongamos que nuestro modelo asigna una probabilidad

386
00:21:05,374 --> 00:21:09,088
realmente baja a que esas otras 12 palabras sean realmente la respuesta final,

387
00:21:09,088 --> 00:21:11,440
algo así como 1 entre 1000 porque son muy oscuras.

388
00:21:11,440 --> 00:21:15,480
Ahora déjame preguntarte, ¿cuál es la entropía de esta distribución?

389
00:21:15,480 --> 00:21:18,808
Si la entropía simplemente midiera el número de coincidencias aquí,

390
00:21:18,808 --> 00:21:22,773
entonces se podría esperar que fuera algo así como el logaritmo en base 2 de 16,

391
00:21:22,773 --> 00:21:26,200
que sería 4, dos bits más de incertidumbre que los que teníamos antes.

392
00:21:26,200 --> 00:21:30,320
Pero, por supuesto, la incertidumbre real no es tan diferente de la que teníamos antes.

393
00:21:30,320 --> 00:21:34,136
El hecho de que existan estas 12 palabras realmente oscuras no significa que

394
00:21:34,136 --> 00:21:38,200
sería mucho más sorprendente saber que la respuesta final es encanto, por ejemplo.

395
00:21:38,200 --> 00:21:41,897
Entonces, cuando realmente haces el cálculo aquí y sumas la probabilidad de cada

396
00:21:41,897 --> 00:21:45,960
ocurrencia multiplicada por la información correspondiente, lo que obtienes es 2.11 bits.

397
00:21:45,960 --> 00:21:49,776
Solo digo que son básicamente dos bits, básicamente esas cuatro posibilidades,

398
00:21:49,776 --> 00:21:54,076
pero hay un poco más de incertidumbre debido a todos esos eventos altamente improbables,

399
00:21:54,076 --> 00:21:57,120
aunque si los aprendieras, obtendrías un montón de información.

400
00:21:57,120 --> 00:21:59,440
Entonces, alejarnos, esto es parte de lo que hace de Wordle

401
00:21:59,440 --> 00:22:01,800
un buen ejemplo para una lección de teoría de la información.

402
00:22:01,800 --> 00:22:05,280
Tenemos estas dos aplicaciones de sentimiento distintas para la entropía.

403
00:22:05,280 --> 00:22:08,977
El primero nos dice cuál es la información esperada que obtendremos

404
00:22:08,977 --> 00:22:12,728
de una suposición determinada, y el segundo dice si podemos medir la

405
00:22:12,728 --> 00:22:16,480
incertidumbre restante entre todas las palabras que tenemos posibles.

406
00:22:16,480 --> 00:22:19,166
Y debo enfatizar que, en el primer caso en el que observamos la

407
00:22:19,166 --> 00:22:21,852
información esperada de una suposición, una vez que tenemos una

408
00:22:21,852 --> 00:22:25,000
ponderación desigual de las palabras, eso afecta el cálculo de la entropía.

409
00:22:25,000 --> 00:22:27,887
Por ejemplo, permítanme mencionar el mismo caso que vimos

410
00:22:27,887 --> 00:22:30,526
anteriormente de la distribución asociada con Weary,

411
00:22:30,526 --> 00:22:34,560
pero esta vez usando una distribución no uniforme en todas las palabras posibles.

412
00:22:34,560 --> 00:22:39,360
Déjame ver si puedo encontrar una parte aquí que lo ilustre bastante bien.

413
00:22:39,360 --> 00:22:42,480
Bien, aquí esto está bastante bien.

414
00:22:42,480 --> 00:22:45,830
Aquí tenemos dos patrones adyacentes que son igualmente probables,

415
00:22:45,830 --> 00:22:49,480
pero nos dicen que uno de ellos tiene 32 palabras posibles que coinciden.

416
00:22:49,480 --> 00:22:52,056
Y si comprobamos cuáles son, estas son esas 32,

417
00:22:52,056 --> 00:22:55,600
que son palabras muy improbables cuando las examinas con la vista.

418
00:22:55,600 --> 00:22:59,016
Es difícil encontrar respuestas que parezcan plausibles, tal vez gritos,

419
00:22:59,016 --> 00:23:01,496
pero si miramos el patrón vecino en la distribución,

420
00:23:01,496 --> 00:23:05,333
que se considera casi igual de probable, nos dicen que solo tiene 8 coincidencias

421
00:23:05,333 --> 00:23:08,469
posibles, por lo que una cuarta parte de Hay muchas coincidencias,

422
00:23:08,469 --> 00:23:09,920
pero es casi igual de probable.

423
00:23:09,920 --> 00:23:12,520
Y cuando analizamos esas coincidencias, podemos ver por qué.

424
00:23:12,520 --> 00:23:17,840
Algunas de estas son respuestas realmente plausibles, como timbre, ira o golpes.

425
00:23:17,840 --> 00:23:21,781
Para ilustrar cómo incorporamos todo eso, permítanme mostrar aquí la versión 2 del

426
00:23:21,781 --> 00:23:25,960
Wordlebot, y hay dos o tres diferencias principales con respecto a la primera que vimos.

427
00:23:25,960 --> 00:23:29,966
En primer lugar, como acabo de decir, la forma en que calculamos estas entropías,

428
00:23:29,966 --> 00:23:33,338
estos valores esperados de información, ahora utiliza distribuciones

429
00:23:33,338 --> 00:23:36,612
más refinadas entre los patrones que incorporan la probabilidad de

430
00:23:36,612 --> 00:23:39,300
que una palabra determinada sea realmente la respuesta.

431
00:23:39,300 --> 00:23:42,153
Da la casualidad de que las lágrimas siguen siendo el número 1,

432
00:23:42,153 --> 00:23:44,160
aunque las siguientes son un poco diferentes.

433
00:23:44,160 --> 00:23:46,582
En segundo lugar, cuando clasifique sus mejores opciones,

434
00:23:46,582 --> 00:23:50,257
ahora mantendrá un modelo de la probabilidad de que cada palabra sea la respuesta real,

435
00:23:50,257 --> 00:23:53,139
y lo incorporará en su decisión, lo cual es más fácil de ver una vez

436
00:23:53,139 --> 00:23:55,520
que tengamos algunas conjeturas sobre la respuesta. mesa.

437
00:23:55,520 --> 00:23:58,133
Nuevamente, ignorando su recomendación porque no

438
00:23:58,133 --> 00:24:01,120
podemos dejar que las máquinas gobiernen nuestras vidas.

439
00:24:01,120 --> 00:24:04,440
Y supongo que debería mencionar otra cosa diferente aquí a la izquierda,

440
00:24:04,440 --> 00:24:06,714
ese valor de incertidumbre, esa cantidad de bits,

441
00:24:06,714 --> 00:24:10,080
ya no es simplemente redundante con la cantidad de coincidencias posibles.

442
00:24:10,080 --> 00:24:15,597
Ahora si lo levantamos y calculamos 2 elevado a 8.02, que está un poco por encima de 256,

443
00:24:15,597 --> 00:24:20,257
supongo que 259, lo que dice es que aunque hay un total de 526 palabras que

444
00:24:20,257 --> 00:24:25,100
realmente coinciden con este patrón, la cantidad de incertidumbre que tiene es

445
00:24:25,100 --> 00:24:29,760
más parecida a la que sería si hubiera 259 igualmente probables. resultados.

446
00:24:29,760 --> 00:24:31,100
Puedes pensar en ello así.

447
00:24:31,100 --> 00:24:34,123
Sabe que borx no es la respuesta, lo mismo ocurre con yorts,

448
00:24:34,123 --> 00:24:37,840
zorl y zorus, por lo que es un poco menos incierto que en el caso anterior.

449
00:24:37,840 --> 00:24:40,220
Este número de bits será menor.

450
00:24:40,220 --> 00:24:44,486
Y si sigo jugando, lo refinaré con un par de suposiciones

451
00:24:44,486 --> 00:24:48,680
que son apropiadas para lo que me gustaría explicar aquí.

452
00:24:48,680 --> 00:24:51,074
En la cuarta suposición, si observa sus mejores opciones,

453
00:24:51,074 --> 00:24:53,800
podrá ver que ya no se trata simplemente de maximizar la entropía.

454
00:24:53,800 --> 00:24:56,791
Entonces, en este punto, técnicamente hay siete posibilidades,

455
00:24:56,791 --> 00:25:00,780
pero las únicas con posibilidades significativas son los dormitorios y las palabras.

456
00:25:00,780 --> 00:25:04,944
Y puede ver que se clasifica al elegir ambos por encima de todos estos otros valores,

457
00:25:04,944 --> 00:25:07,560
que estrictamente hablando brindarían más información.

458
00:25:07,560 --> 00:25:11,026
La primera vez que hice esto, simplemente sumé estos dos números para medir la

459
00:25:11,026 --> 00:25:14,580
calidad de cada suposición, lo que en realidad funcionó mejor de lo que imaginas.

460
00:25:14,580 --> 00:25:17,096
Pero realmente no lo sentí sistemático, y estoy seguro de que hay

461
00:25:17,096 --> 00:25:19,880
otros enfoques que la gente podría adoptar, pero este es el que encontré.

462
00:25:19,880 --> 00:25:23,018
Si estamos considerando la posibilidad de una próxima suposición,

463
00:25:23,018 --> 00:25:27,108
como en este caso palabras, lo que realmente nos importa es la puntuación esperada de

464
00:25:27,108 --> 00:25:28,440
nuestro juego si lo hacemos.

465
00:25:28,440 --> 00:25:32,235
Y para calcular esa puntuación esperada, decimos cuál es la probabilidad de

466
00:25:32,235 --> 00:25:36,080
que las palabras sean la respuesta real, que en este momento describe el 58%.

467
00:25:36,080 --> 00:25:40,400
Decimos que con un 58% de posibilidades, nuestra puntuación en este juego sería 4.

468
00:25:40,400 --> 00:25:46,240
Y luego, con la probabilidad de 1 menos ese 58%, nuestra puntuación será mayor que ese 4.

469
00:25:46,240 --> 00:25:49,555
No sabemos cuánto más, pero podemos estimarlo en función de cuánta

470
00:25:49,555 --> 00:25:52,920
incertidumbre probablemente habrá una vez que lleguemos a ese punto.

471
00:25:52,920 --> 00:25:56,600
En concreto, de momento hay 1.44 bits de incertidumbre.

472
00:25:56,600 --> 00:26:01,560
Si adivinamos palabras, nos dice que la información esperada que obtendremos es 1.27 bits.

473
00:26:01,560 --> 00:26:04,970
Entonces, si adivinamos palabras, esta diferencia representa cuánta

474
00:26:04,970 --> 00:26:08,280
incertidumbre es probable que nos quede después de que eso suceda.

475
00:26:08,280 --> 00:26:11,302
Lo que necesitamos es algún tipo de función, a la que aquí llamo f,

476
00:26:11,302 --> 00:26:13,880
que asocie esta incertidumbre con una puntuación esperada.

477
00:26:13,880 --> 00:26:18,303
Y la forma en que lo hicimos fue simplemente trazar un montón de datos de juegos

478
00:26:18,303 --> 00:26:22,616
anteriores basados en la versión 1 del bot para decir cuál fue el puntaje real

479
00:26:22,616 --> 00:26:27,040
después de varios puntos con ciertas cantidades de incertidumbre muy mensurables.

480
00:26:27,040 --> 00:26:31,155
Por ejemplo, estos puntos de datos aquí que se encuentran por encima de un valor cercano

481
00:26:31,155 --> 00:26:35,270
a 8.Aproximadamente 7, dicen para algunos juegos después de un punto en el que había 8.7

482
00:26:35,270 --> 00:26:39,340
bits de incertidumbre, fueron necesarias dos conjeturas para obtener la respuesta final.

483
00:26:39,340 --> 00:26:41,242
Para otros juegos fueron necesarias tres conjeturas,

484
00:26:41,242 --> 00:26:43,180
para otros juegos fueron necesarias cuatro conjeturas.

485
00:26:43,180 --> 00:26:46,936
Si aquí nos desplazamos hacia la izquierda, todos los puntos sobre cero dicen que

486
00:26:46,936 --> 00:26:50,830
siempre que haya cero bits de incertidumbre, es decir, que solo hay una posibilidad,

487
00:26:50,830 --> 00:26:53,808
entonces el número de conjeturas requeridas es siempre solo uno,

488
00:26:53,808 --> 00:26:55,000
lo cual es tranquilizador.

489
00:26:55,000 --> 00:26:57,964
Cada vez que había un poco de incertidumbre, lo que significaba

490
00:26:57,964 --> 00:27:00,280
que esencialmente se reducía a dos posibilidades,

491
00:27:00,280 --> 00:27:03,940
a veces se requería una conjetura más, a veces se requerían dos conjeturas más.

492
00:27:03,940 --> 00:27:05,980
Y así sucesivamente aquí.

493
00:27:05,980 --> 00:27:08,681
Quizás una forma un poco más sencilla de visualizar

494
00:27:08,681 --> 00:27:11,020
estos datos sea agruparlos y tomar promedios.

495
00:27:11,020 --> 00:27:16,530
Por ejemplo, esta barra dice que entre todos los puntos en los que teníamos un poco de

496
00:27:16,530 --> 00:27:22,166
incertidumbre, en promedio el número de nuevas conjeturas requeridas fue aproximadamente

497
00:27:22,166 --> 00:27:22,420
1.5.

498
00:27:22,420 --> 00:27:25,712
Y la barra de aquí dice que entre todos los diferentes juegos donde en

499
00:27:25,712 --> 00:27:29,283
algún momento la incertidumbre estuvo un poco por encima de los cuatro bits,

500
00:27:29,283 --> 00:27:32,947
lo que es como reducirla a 16 posibilidades diferentes, entonces, en promedio,

501
00:27:32,947 --> 00:27:36,240
requiere un poco más de dos conjeturas a partir de ese punto. adelante.

502
00:27:36,240 --> 00:27:38,142
Y a partir de aquí simplemente hice una regresión para

503
00:27:38,142 --> 00:27:40,080
ajustarme a una función que parecía razonable para esto.

504
00:27:40,080 --> 00:27:44,819
Y recuerde que el objetivo de hacer todo esto es que podamos cuantificar esta intuición

505
00:27:44,819 --> 00:27:47,888
de que cuanta más información obtengamos de una palabra,

506
00:27:47,888 --> 00:27:49,720
menor será la puntuación esperada.

507
00:27:49,720 --> 00:27:54,740
Entonces con esto como versión 2.0, si volvemos atrás y ejecutamos el mismo conjunto

508
00:27:54,740 --> 00:27:59,820
de simulaciones, haciéndolo jugar contra las 2315 respuestas posibles, ¿cómo funciona?

509
00:27:59,820 --> 00:28:01,855
Bueno, a diferencia de nuestra primera versión,

510
00:28:01,855 --> 00:28:04,060
es definitivamente mejor, lo cual es tranquilizador.

511
00:28:04,060 --> 00:28:08,277
Todo dicho y hecho, la media ronda los 3.6, aunque a diferencia de la primera

512
00:28:08,277 --> 00:28:12,820
versión hay un par de veces que pierde y requiere más de seis en esta circunstancia.

513
00:28:12,820 --> 00:28:15,819
Presumiblemente porque hay momentos en los que se trata

514
00:28:15,819 --> 00:28:18,980
de buscar el objetivo en lugar de maximizar la información.

515
00:28:18,980 --> 00:28:22,140
Entonces, ¿podemos hacerlo mejor que 3?6?

516
00:28:22,140 --> 00:28:23,260
Definitivamente podemos.

517
00:28:23,260 --> 00:28:26,709
Ahora dije al principio que es muy divertido intentar no incorporar la lista

518
00:28:26,709 --> 00:28:29,980
verdadera de respuestas de Wordle en la forma en que construye su modelo.

519
00:28:29,980 --> 00:28:35,180
Pero si lo incorporamos, el mejor rendimiento que pude obtener fue alrededor de 3.43.

520
00:28:35,180 --> 00:28:37,843
Entonces, si intentamos ser más sofisticados que simplemente usar

521
00:28:37,843 --> 00:28:40,669
datos de frecuencia de palabras para elegir esta distribución previa,

522
00:28:40,669 --> 00:28:44,140
este 3.43 probablemente da un máximo de lo buenos que podríamos llegar a ser con eso,

523
00:28:44,140 --> 00:28:46,360
o al menos de lo bueno que podría llegar a ser con eso.

524
00:28:46,360 --> 00:28:49,413
Ese mejor rendimiento esencialmente solo utiliza las ideas de las

525
00:28:49,413 --> 00:28:51,958
que he estado hablando aquí, pero va un poco más allá,

526
00:28:51,958 --> 00:28:55,660
como si buscara la información esperada dos pasos adelante en lugar de solo uno.

527
00:28:55,660 --> 00:28:58,167
Originalmente estaba planeando hablar más sobre eso,

528
00:28:58,167 --> 00:29:00,580
pero me doy cuenta de que ya hemos durado bastante.

529
00:29:00,580 --> 00:29:03,553
Lo único que diré es que después de hacer esta búsqueda de dos pasos y

530
00:29:03,553 --> 00:29:06,736
luego ejecutar un par de simulaciones de muestra en los mejores candidatos,

531
00:29:06,736 --> 00:29:09,500
hasta ahora al menos para mí parece que Crane es el mejor abridor.

532
00:29:09,500 --> 00:29:11,080
¿Quién lo hubiera adivinado?

533
00:29:11,080 --> 00:29:14,493
Además, si utiliza la lista de palabras verdaderas para determinar su espacio de

534
00:29:14,493 --> 00:29:18,160
posibilidades, entonces la incertidumbre con la que comienza es un poco más de 11 bits.

535
00:29:18,160 --> 00:29:21,285
Y resulta que, sólo a partir de una búsqueda de fuerza bruta,

536
00:29:21,285 --> 00:29:25,521
la máxima información esperada posible después de las dos primeras conjeturas es de

537
00:29:25,521 --> 00:29:26,580
alrededor de 10 bits.

538
00:29:26,580 --> 00:29:31,141
Lo que sugiere que en el mejor de los casos, después de tus dos primeras conjeturas,

539
00:29:31,141 --> 00:29:35,220
con un juego perfectamente óptimo, te quedarás con un poco de incertidumbre.

540
00:29:35,220 --> 00:29:37,400
Lo que es lo mismo que limitarse a dos posibles conjeturas.

541
00:29:37,400 --> 00:29:40,004
Así que creo que es justo y probablemente bastante conservador decir que

542
00:29:40,004 --> 00:29:43,037
nunca sería posible escribir un algoritmo que consiga este promedio tan bajo como 3,

543
00:29:43,037 --> 00:29:45,500
porque con las palabras disponibles, simplemente no hay espacio para

544
00:29:45,500 --> 00:29:48,069
obtener suficiente información después de sólo dos pasos para ser capaz

545
00:29:48,069 --> 00:29:50,460
de garantizar la respuesta en el tercer espacio cada vez sin falta.

