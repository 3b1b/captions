[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy.",
  "model": "nmt",
  "translatedText": "El juego Wurdle se ha vuelto bastante viral en los últimos dos meses y, como nunca desperdiciaré la oportunidad de una lección de matemáticas, se me ocurre que este juego es un muy buen ejemplo central en una lección sobre teoría de la información y, en particular, un tema conocido como entropía.",
  "time_range": [
   0.0,
   12.66
  ]
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could.",
  "model": "nmt",
  "translatedText": "Verá, como mucha gente, me quedé atrapado en el rompecabezas, y como muchos programadores, también me quedé atrapado en el intento de escribir un algoritmo que jugara el juego de la manera más óptima posible.",
  "time_range": [
   13.92,
   22.74
  ]
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy.",
  "model": "nmt",
  "translatedText": "Y lo que pensé que haría aquí es simplemente hablarles sobre mi proceso y explicarles algunas de las matemáticas que implican, ya que todo el algoritmo se centra en esta idea de entropía.",
  "time_range": [
   23.18,
   31.08
  ]
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle?",
  "model": "nmt",
  "translatedText": "Lo primero es lo primero, en caso de que no hayas oído hablar de él, ¿qué es Wurdle?",
  "time_range": [
   38.7,
   41.64
  ]
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us.",
  "model": "nmt",
  "translatedText": "Y para matar dos pájaros de un tiro mientras repasamos las reglas del juego, permítanme también un avance de hacia dónde vamos con esto, que es desarrollar un pequeño algoritmo que básicamente jugará el juego por nosotros.",
  "time_range": [
   42.04,
   51.04
  ]
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does.",
  "model": "nmt",
  "translatedText": "Aunque no he hecho el Wurdle de hoy, es el 4 de febrero y veremos cómo le va al robot.",
  "time_range": [
   51.36,
   55.1
  ]
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess.",
  "model": "nmt",
  "translatedText": "El objetivo de Wurdle es adivinar una palabra misteriosa de cinco letras y tienes seis oportunidades diferentes para adivinar.",
  "time_range": [
   55.48,
   60.34
  ]
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane.",
  "model": "nmt",
  "translatedText": "Por ejemplo, mi robot Wurdle me sugiere que empiece con la grúa de adivinanzas.",
  "time_range": [
   60.84,
   64.38
  ]
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer.",
  "model": "nmt",
  "translatedText": "Cada vez que haces una suposición, obtienes información sobre qué tan cerca está tu suposición de la respuesta verdadera.",
  "time_range": [
   65.18,
   70.22
  ]
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer.",
  "model": "nmt",
  "translatedText": "Aquí el cuadro gris me dice que no hay una C en la respuesta real.",
  "time_range": [
   70.92,
   74.1
  ]
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position.",
  "model": "nmt",
  "translatedText": "El cuadro amarillo me dice que hay una R, pero no está en esa posición.",
  "time_range": [
   74.52,
   77.84
  ]
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position.",
  "model": "nmt",
  "translatedText": "El cuadro verde me dice que la palabra secreta tiene una A y está en la tercera posición.",
  "time_range": [
   78.24,
   82.24
  ]
 },
 {
  "input": "And then there's no N and there's no E.",
  "model": "nmt",
  "translatedText": "Y luego no hay N y no hay E.",
  "time_range": [
   82.72,
   84.58
  ]
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information.",
  "model": "nmt",
  "translatedText": "Así que déjame entrar y decirle esa información al robot Wurdle.",
  "time_range": [
   85.2,
   87.34
  ]
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey.",
  "model": "nmt",
  "translatedText": "Empezamos con grúa, obtuvimos gris, amarillo, verde, gris, gris.",
  "time_range": [
   87.34,
   90.32
  ]
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time.",
  "model": "nmt",
  "translatedText": "No te preocupes por todos los datos que están mostrando ahora mismo, te lo explicaré a su debido tiempo.",
  "time_range": [
   91.42,
   94.94
  ]
 },
 {
  "input": "But its top suggestion for our second pick is shtick.",
  "model": "nmt",
  "translatedText": "Pero su principal sugerencia para nuestra segunda elección es un truco.",
  "time_range": [
   95.46,
   98.82
  ]
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess.",
  "model": "nmt",
  "translatedText": "Y tu suposición tiene que ser una palabra real de cinco letras, pero como verás, es bastante liberal con lo que realmente te permitirá adivinar.",
  "time_range": [
   99.56,
   105.4
  ]
 },
 {
  "input": "In this case, we try shtick.",
  "model": "nmt",
  "translatedText": "En este caso, intentamos stick.",
  "time_range": [
   106.2,
   107.44
  ]
 },
 {
  "input": "And alright, things are looking pretty good.",
  "model": "nmt",
  "translatedText": "Y bueno, la cosa pinta bastante bien.",
  "time_range": [
   108.78,
   110.18
  ]
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R.",
  "model": "nmt",
  "translatedText": "Pulsamos la S y la H, así conocemos las tres primeras letras, sabemos que hay una R.",
  "time_range": [
   110.26,
   113.98
  ]
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something.",
  "model": "nmt",
  "translatedText": "Y entonces será como SHA algo R, o SHA R algo.",
  "time_range": [
   113.98,
   118.7
  ]
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp.",
  "model": "nmt",
  "translatedText": "Y parece que el robot Wurdle sabe que solo hay dos posibilidades: fragmentar o filoso.",
  "time_range": [
   119.62,
   124.24
  ]
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard.",
  "model": "nmt",
  "translatedText": "Eso es una especie de volatilidad entre ellos en este momento, así que supongo que probablemente solo porque es alfabético va con el fragmento.",
  "time_range": [
   125.1,
   130.08
  ]
 },
 {
  "input": "Which hooray, is the actual answer.",
  "model": "nmt",
  "translatedText": "Qué hurra, es la respuesta real.",
  "time_range": [
   131.22,
   132.86
  ]
 },
 {
  "input": "So we got it in three.",
  "model": "nmt",
  "translatedText": "Entonces lo tenemos en tres.",
  "time_range": [
   132.96,
   133.78
  ]
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie.",
  "model": "nmt",
  "translatedText": "Si te preguntas si eso es bueno, la forma en que escuché a una persona decir que con Wurdle cuatro es par y tres es birdie.",
  "time_range": [
   134.6,
   140.36
  ]
 },
 {
  "input": "Which I think is a pretty apt analogy.",
  "model": "nmt",
  "translatedText": "Lo cual creo que es una analogía bastante adecuada.",
  "time_range": [
   140.68,
   142.48
  ]
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy.",
  "model": "nmt",
  "translatedText": "Tienes que ser constante en tu juego para conseguir cuatro, pero ciertamente no es una locura.",
  "time_range": [
   142.48,
   147.02
  ]
 },
 {
  "input": "But when you get it in three, it just feels great.",
  "model": "nmt",
  "translatedText": "Pero cuando lo obtienes en tres, se siente genial.",
  "time_range": [
   147.18,
   149.92
  ]
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot.",
  "model": "nmt",
  "translatedText": "Entonces, si estás dispuesto a hacerlo, lo que me gustaría hacer aquí es simplemente hablar sobre mi proceso de pensamiento desde el principio sobre cómo abordo el robot Wurdle.",
  "time_range": [
   150.88,
   155.96
  ]
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson.",
  "model": "nmt",
  "translatedText": "Y como dije, en realidad es una excusa para una lección de teoría de la información.",
  "time_range": [
   156.48,
   159.44
  ]
 },
 {
  "input": "The main goal is to explain what is information and what is entropy.",
  "model": "nmt",
  "translatedText": "El objetivo principal es explicar qué es información y qué es entropía.",
  "time_range": [
   159.74,
   162.82
  ]
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language.",
  "model": "nmt",
  "translatedText": "Lo primero que pensé al abordar esto fue observar las frecuencias relativas de diferentes letras en el idioma inglés.",
  "time_range": [
   168.22,
   173.72
  ]
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters?",
  "model": "nmt",
  "translatedText": "Entonces pensé, bueno, ¿hay una suposición inicial o un par de suposiciones iniciales que acierten muchas de estas letras más frecuentes?",
  "time_range": [
   174.38,
   179.26
  ]
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails.",
  "model": "nmt",
  "translatedText": "Y una que me gustaba mucho era hacer otras seguidas de uñas.",
  "time_range": [
   179.96,
   183.0
  ]
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good.",
  "model": "nmt",
  "translatedText": "La idea es que si tocas una letra, ya sabes, obtienes un verde o un amarillo, eso siempre se siente bien.",
  "time_range": [
   183.76,
   187.52
  ]
 },
 {
  "input": "It feels like you're getting information.",
  "model": "nmt",
  "translatedText": "Parece que estás recibiendo información.",
  "time_range": [
   187.52,
   188.84
  ]
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters.",
  "model": "nmt",
  "translatedText": "Pero en estos casos, incluso si no aciertas y siempre aparecen grises, eso te da mucha información ya que es bastante raro encontrar una palabra que no tenga ninguna de estas letras.",
  "time_range": [
   189.34,
   197.4
  ]
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters.",
  "model": "nmt",
  "translatedText": "Pero aun así, esto no parece súper sistemático, porque, por ejemplo, no tiene en cuenta el orden de las letras.",
  "time_range": [
   198.14,
   203.2
  ]
 },
 {
  "input": "Why type nails when I could type snail?",
  "model": "nmt",
  "translatedText": "¿Por qué escribir uñas cuando puedo escribir caracol?",
  "time_range": [
   203.56,
   205.3
  ]
 },
 {
  "input": "Is it better to have that S at the end?",
  "model": "nmt",
  "translatedText": "¿Es mejor tener esa S al final?",
  "time_range": [
   206.08,
   207.5
  ]
 },
 {
  "input": "I'm not really sure.",
  "model": "nmt",
  "translatedText": "No estoy realmente seguro.",
  "time_range": [
   207.82,
   208.68
  ]
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y.",
  "model": "nmt",
  "translatedText": "Ahora, un amigo mío dijo que le gustaba comenzar con la palabra cansado, lo que me sorprendió un poco porque tiene algunas letras poco comunes como la W y la Y.",
  "time_range": [
   209.24,
   216.54
  ]
 },
 {
  "input": "But who knows, maybe that is a better opener.",
  "model": "nmt",
  "translatedText": "Pero quién sabe, tal vez ese sea un mejor comienzo.",
  "time_range": [
   217.12,
   219.0
  ]
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess?",
  "model": "nmt",
  "translatedText": "¿Existe algún tipo de puntuación cuantitativa que podamos dar para juzgar la calidad de una posible suposición?",
  "time_range": [
   219.32,
   224.32
  ]
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up.",
  "model": "nmt",
  "translatedText": "Ahora, para preparar la forma en que vamos a clasificar las posibles conjeturas, retrocedamos y agreguemos un poco de claridad sobre cómo está configurado exactamente el juego.",
  "time_range": [
   225.34,
   231.42
  ]
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long.",
  "model": "nmt",
  "translatedText": "Entonces, hay una lista de palabras que le permitirá ingresar y que se consideran conjeturas válidas y que tiene aproximadamente 13,000 palabras.",
  "time_range": [
   231.42,
   237.88
  ]
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble.",
  "model": "nmt",
  "translatedText": "Pero cuando lo miras, hay muchas cosas realmente poco comunes, cosas como una cabeza o Ali y ARG, el tipo de palabras que provocan discusiones familiares en un juego de Scrabble.",
  "time_range": [
   238.32,
   246.44
  ]
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word.",
  "model": "nmt",
  "translatedText": "Pero la sensación del juego es que la respuesta siempre será una palabra bastante común.",
  "time_range": [
   246.96,
   250.54
  ]
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers.",
  "model": "nmt",
  "translatedText": "Y de hecho, hay otra lista de alrededor de 2300 palabras que son las posibles respuestas.",
  "time_range": [
   250.96,
   255.36
  ]
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun.",
  "model": "nmt",
  "translatedText": "Y esta es una lista seleccionada por humanos, creo que específicamente por la novia del creador del juego, lo cual es bastante divertido.",
  "time_range": [
   255.94,
   261.16
  ]
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list.",
  "model": "nmt",
  "translatedText": "Pero lo que me gustaría hacer, nuestro desafío para este proyecto es ver si podemos escribir un programa resolviendo Wordle que no incorpore conocimientos previos sobre esta lista.",
  "time_range": [
   261.82,
   270.18
  ]
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list.",
  "model": "nmt",
  "translatedText": "Por un lado, hay muchas palabras de cinco letras bastante comunes que no encontrarás en esa lista.",
  "time_range": [
   270.72,
   274.64
  ]
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website.",
  "model": "nmt",
  "translatedText": "Por lo tanto, sería mejor escribir un programa que sea un poco más resistente y que pueda jugar con Wordle contra cualquiera, no solo contra el sitio web oficial.",
  "time_range": [
   274.94,
   281.46
  ]
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code.",
  "model": "nmt",
  "translatedText": "Y también la razón por la que sabemos cuál es esta lista de posibles respuestas es porque es visible en el código fuente.",
  "time_range": [
   281.92,
   287.0
  ]
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day.",
  "model": "nmt",
  "translatedText": "Pero la forma en que es visible en el código fuente es en el orden específico en el que aparecen las respuestas día a día.",
  "time_range": [
   287.0,
   293.26
  ]
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be.",
  "model": "nmt",
  "translatedText": "Así que siempre puedes buscar cuál será la respuesta de mañana.",
  "time_range": [
   293.26,
   295.84
  ]
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating.",
  "model": "nmt",
  "translatedText": "Claramente, en cierto sentido usar la lista es hacer trampa.",
  "time_range": [
   296.42,
   298.88
  ]
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words.",
  "model": "nmt",
  "translatedText": "Y lo que hace que el rompecabezas sea más interesante y una lección de teoría de la información más rica es utilizar algunos datos más universales, como las frecuencias relativas de las palabras en general, para capturar esta intuición de tener preferencia por palabras más comunes.",
  "time_range": [
   299.1,
   310.44
  ]
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess?",
  "model": "nmt",
  "translatedText": "Entonces, de estas 13.000 posibilidades, ¿cómo deberíamos elegir la suposición inicial?",
  "time_range": [
   311.6,
   315.9
  ]
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality?",
  "model": "nmt",
  "translatedText": "Por ejemplo, si mi amigo me propone cansancio, ¿cómo debemos analizar su calidad?",
  "time_range": [
   316.4,
   319.78
  ]
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W.",
  "model": "nmt",
  "translatedText": "Bueno, la razón por la que dijo que le gusta esa improbable W es que le gusta la naturaleza remota de lo bien que se siente si aciertas esa W.",
  "time_range": [
   320.52,
   327.34
  ]
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern.",
  "model": "nmt",
  "translatedText": "Por ejemplo, si el primer patrón revelado fue algo como este, entonces resulta que solo hay 58 palabras en este léxico gigante que coinciden con ese patrón.",
  "time_range": [
   327.92,
   335.6
  ]
 },
 {
  "input": "So that's a huge reduction from 13,000.",
  "model": "nmt",
  "translatedText": "Entonces esa es una gran reducción de 13.000.",
  "time_range": [
   336.06,
   338.4
  ]
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this.",
  "model": "nmt",
  "translatedText": "Pero la otra cara de la moneda, por supuesto, es que es muy poco común obtener un patrón como este.",
  "time_range": [
   338.78,
   343.02
  ]
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000.",
  "model": "nmt",
  "translatedText": "Específicamente, si cada palabra tuviera la misma probabilidad de ser la respuesta, la probabilidad de encontrar este patrón sería 58 dividido por alrededor de 13.000.",
  "time_range": [
   343.02,
   351.04
  ]
 },
 {
  "input": "Of course, they're not equally likely to be answers.",
  "model": "nmt",
  "translatedText": "Por supuesto, no es igualmente probable que sean respuestas.",
  "time_range": [
   351.58,
   353.6
  ]
 },
 {
  "input": "Most of these are very obscure and even questionable words.",
  "model": "nmt",
  "translatedText": "La mayoría de ellas son palabras muy oscuras e incluso cuestionables.",
  "time_range": [
   353.72,
   356.22
  ]
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later.",
  "model": "nmt",
  "translatedText": "Pero al menos para nuestra primera aproximación a todo esto, supongamos que todas son igualmente probables y luego refinemos eso un poco más adelante.",
  "time_range": [
   356.6,
   361.6
  ]
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur.",
  "model": "nmt",
  "translatedText": "La cuestión es que, por su propia naturaleza, es poco probable que se produzca un patrón con mucha información.",
  "time_range": [
   362.02,
   366.72
  ]
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely.",
  "model": "nmt",
  "translatedText": "De hecho, lo que significa ser informativo es que es poco probable.",
  "time_range": [
   367.28,
   370.8
  ]
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it.",
  "model": "nmt",
  "translatedText": "Un patrón mucho más probable de ver con esta apertura sería algo como esto, donde por supuesto no hay una W.",
  "time_range": [
   371.71999999999997,
   378.12
  ]
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y.",
  "model": "nmt",
  "translatedText": "Tal vez haya una E, y tal vez no haya una A, no haya una R, no haya una Y.",
  "time_range": [
   378.24,
   381.4
  ]
 },
 {
  "input": "In this case, there are 1400 possible matches.",
  "model": "nmt",
  "translatedText": "En este caso, hay 1400 coincidencias posibles.",
  "time_range": [
   382.08,
   384.56
  ]
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see.",
  "model": "nmt",
  "translatedText": "Si todas fueran igualmente probables, resulta que hay una probabilidad de alrededor del 11% de que este sea el patrón que verías.",
  "time_range": [
   385.08,
   390.6
  ]
 },
 {
  "input": "So the most likely outcomes are also the least informative.",
  "model": "nmt",
  "translatedText": "De modo que los resultados más probables son también los menos informativos.",
  "time_range": [
   390.9,
   393.34
  ]
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see.",
  "model": "nmt",
  "translatedText": "Para obtener una visión más global, permítame mostrarle la distribución completa de probabilidades en todos los diferentes patrones que pueda ver.",
  "time_range": [
   394.24,
   401.14
  ]
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common.",
  "model": "nmt",
  "translatedText": "Entonces, cada barra que estás viendo corresponde a un posible patrón de colores que podría revelarse, de los cuales hay de 3 a 5 posibilidades, y están organizados de izquierda a derecha, del más común al menos común.",
  "time_range": [
   401.74,
   412.34
  ]
 },
 {
  "input": "So the most common possibility here is that you get all grays.",
  "model": "nmt",
  "translatedText": "Entonces, la posibilidad más común aquí es que obtengas todos los grises.",
  "time_range": [
   412.92,
   416.0
  ]
 },
 {
  "input": "That happens about 14% of the time.",
  "model": "nmt",
  "translatedText": "Esto sucede aproximadamente el 14% del tiempo.",
  "time_range": [
   416.1,
   418.12
  ]
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this.",
  "model": "nmt",
  "translatedText": "Y lo que esperas cuando haces una suposición es terminar en algún lugar de esta larga cola, como aquí donde solo hay 18 posibilidades para lo que coincide con este patrón que evidentemente se ve así.",
  "time_range": [
   418.58,
   429.14
  ]
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here.",
  "model": "nmt",
  "translatedText": "O si nos aventuramos un poco más hacia la izquierda, ya sabes, tal vez lleguemos hasta aquí.",
  "time_range": [
   429.92,
   433.8
  ]
 },
 {
  "input": "Okay, here's a good puzzle for you.",
  "model": "nmt",
  "translatedText": "Bien, aquí tienes un buen rompecabezas.",
  "time_range": [
   434.94,
   436.18
  ]
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them?",
  "model": "nmt",
  "translatedText": "¿Cuáles son las tres palabras en inglés que comienzan con W, terminan con Y y tienen una R en alguna parte?",
  "time_range": [
   436.54,
   442.0
  ]
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly.",
  "model": "nmt",
  "translatedText": "Resulta que las respuestas son, veamos, prolijas, llenas de gusanos e irónicas.",
  "time_range": [
   442.48,
   446.8
  ]
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution.",
  "model": "nmt",
  "translatedText": "Entonces, para juzgar qué tan buena es esta palabra en general, queremos algún tipo de medida de la cantidad esperada de información que obtendrá de esta distribución.",
  "time_range": [
   447.5,
   455.74
  ]
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score.",
  "model": "nmt",
  "translatedText": "Si analizamos cada patrón y multiplicamos su probabilidad de ocurrir por algo que mida qué tan informativo es, eso tal vez pueda darnos una puntuación objetiva.",
  "time_range": [
   455.74,
   464.72
  ]
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches.",
  "model": "nmt",
  "translatedText": "Ahora tu primer instinto sobre lo que debería ser ese algo podría ser el número de coincidencias.",
  "time_range": [
   465.96,
   469.84
  ]
 },
 {
  "input": "You want a lower average number of matches.",
  "model": "nmt",
  "translatedText": "Quieres un número promedio más bajo de coincidencias.",
  "time_range": [
   470.16,
   472.4
  ]
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer.",
  "model": "nmt",
  "translatedText": "Pero en lugar de eso me gustaría usar una medida más universal que a menudo atribuimos a la información, y una que será más flexible una vez que tengamos asignada una probabilidad diferente a cada una de estas 13.000 palabras sobre si son o no la respuesta.",
  "time_range": [
   472.8,
   484.26
  ]
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples.",
  "model": "nmt",
  "translatedText": "La unidad de información estándar es el bit, que tiene una fórmula un poco divertida, pero es realmente intuitiva si solo miramos ejemplos.",
  "time_range": [
   490.32,
   496.98
  ]
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information.",
  "model": "nmt",
  "translatedText": "Si tienes una observación que reduce a la mitad tu espacio de posibilidades, decimos que tiene un bit de información.",
  "time_range": [
   497.78,
   503.5
  ]
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half.",
  "model": "nmt",
  "translatedText": "En nuestro ejemplo, el espacio de posibilidades son todas las palabras posibles, y resulta que aproximadamente la mitad de las palabras de cinco letras tienen una S, un poco menos que eso, pero aproximadamente la mitad.",
  "time_range": [
   504.18,
   511.26
  ]
 },
 {
  "input": "So that observation would give you one bit of information.",
  "model": "nmt",
  "translatedText": "Entonces esa observación les daría un poco de información.",
  "time_range": [
   511.78,
   514.32
  ]
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information.",
  "model": "nmt",
  "translatedText": "Si en cambio un hecho nuevo reduce ese espacio de posibilidades en un factor de cuatro, decimos que tiene dos bits de información.",
  "time_range": [
   514.88,
   521.5
  ]
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T.",
  "model": "nmt",
  "translatedText": "Por ejemplo, resulta que aproximadamente una cuarta parte de estas palabras tienen una T.",
  "time_range": [
   521.98,
   524.46
  ]
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth.",
  "model": "nmt",
  "translatedText": "Si la observación corta ese espacio por un factor de ocho, decimos que son tres bits de información, y así sucesivamente.",
  "time_range": [
   525.02,
   530.72
  ]
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd.",
  "model": "nmt",
  "translatedText": "Cuatro bits lo cortan en un 16, cinco bits lo cortan en un 32.",
  "time_range": [
   530.9,
   535.06
  ]
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence?",
  "model": "nmt",
  "translatedText": "Así que ahora quizás quieras hacer una pausa y preguntarte: ¿cuál es la fórmula para obtener información sobre el número de bits en términos de probabilidad de que ocurra?",
  "time_range": [
   535.06,
   542.66
  ]
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability.",
  "model": "nmt",
  "translatedText": "Lo que estamos diciendo aquí es que cuando se le suma la mitad al número de bits, eso es lo mismo que la probabilidad, que es lo mismo que decir que dos elevado a la potencia del número de bits es uno sobre la probabilidad, lo cual se reordena además para decir que la información es el logaritmo en base dos de uno dividido por la probabilidad.",
  "time_range": [
   542.66,
   558.92
  ]
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability.",
  "model": "nmt",
  "translatedText": "Y a veces se ve esto con un reordenamiento más, donde la información es el logaritmo negativo en base dos de la probabilidad.",
  "time_range": [
   559.62,
   564.9
  ]
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half.",
  "model": "nmt",
  "translatedText": "Expresado así, puede parecer un poco extraño para los no iniciados, pero en realidad es sólo la idea muy intuitiva de preguntar cuántas veces has reducido tus posibilidades a la mitad.",
  "time_range": [
   565.66,
   574.08
  ]
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture?",
  "model": "nmt",
  "translatedText": "Ahora, si te lo estás preguntando, ya sabes, pensé que solo estábamos jugando un divertido juego de palabras, ¿por qué los logaritmos entran en escena?",
  "time_range": [
   575.18,
   579.3
  ]
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095.",
  "model": "nmt",
  "translatedText": "Una de las razones por las que esta es una unidad más agradable es que es mucho más fácil hablar de eventos muy improbables, mucho más fácil decir que una observación tiene 20 bits de información que decir que la probabilidad de que ocurra tal o cual cosa es 0.0000095.",
  "time_range": [
   579.78,
   592.94
  ]
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together.",
  "model": "nmt",
  "translatedText": "Pero una razón más sustancial por la que esta expresión logarítmica resultó ser una adición muy útil a la teoría de la probabilidad es la forma en que se suma la información.",
  "time_range": [
   593.3,
   601.46
  ]
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information.",
  "model": "nmt",
  "translatedText": "Por ejemplo, si una observación le proporciona dos bits de información, lo que reduce su espacio en cuatro, y luego una segunda observación, como su segunda suposición en Wordle, le proporciona otros tres bits de información, lo que lo reduce aún más en otro factor de ocho, la dos juntos te dan cinco bits de información.",
  "time_range": [
   602.06,
   616.74
  ]
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add.",
  "model": "nmt",
  "translatedText": "De la misma manera que a las probabilidades les gusta multiplicarse, a la información le gusta sumar.",
  "time_range": [
   617.16,
   621.02
  ]
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with.",
  "model": "nmt",
  "translatedText": "Entonces, tan pronto como estamos en el ámbito de algo así como un valor esperado, donde sumamos un montón de números, los registros hacen que sea mucho más agradable tratar con ellos.",
  "time_range": [
   621.96,
   627.98
  ]
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern.",
  "model": "nmt",
  "translatedText": "Volvamos a nuestra distribución de Weary y agreguemos otro pequeño rastreador aquí, que nos muestra cuánta información hay para cada patrón.",
  "time_range": [
   628.48,
   634.94
  ]
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain.",
  "model": "nmt",
  "translatedText": "Lo principal que quiero que note es que cuanto mayor es la probabilidad a medida que llegamos a esos patrones más probables, menor es la información y menos bits se ganan.",
  "time_range": [
   635.58,
   642.78
  ]
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get.",
  "model": "nmt",
  "translatedText": "La forma en que medimos la calidad de esta suposición será tomando el valor esperado de esta información, donde analizamos cada patrón, decimos qué tan probable es y luego lo multiplicamos por cuántos bits de información obtenemos.",
  "time_range": [
   643.5,
   654.06
  ]
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits.",
  "model": "nmt",
  "translatedText": "Y en el ejemplo de Weary, resulta ser 4.9 bits.",
  "time_range": [
   654.71,
   658.12
  ]
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times.",
  "model": "nmt",
  "translatedText": "Entonces, en promedio, la información que obtienes de esta suposición inicial es tan buena como cortar tu espacio de posibilidades a la mitad unas cinco veces.",
  "time_range": [
   658.56,
   665.48
  ]
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate.",
  "model": "nmt",
  "translatedText": "Por el contrario, un ejemplo de una suposición con un valor de información esperado más alto sería algo como Slate.",
  "time_range": [
   665.96,
   671.64
  ]
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter.",
  "model": "nmt",
  "translatedText": "En este caso notarás que la distribución luce mucho más plana.",
  "time_range": [
   673.12,
   675.62
  ]
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information.",
  "model": "nmt",
  "translatedText": "En particular, la aparición más probable de todos los grises solo tiene alrededor de un 6% de posibilidades de ocurrir, por lo que, como mínimo, evidentemente obtendrás 3.9 bits de información.",
  "time_range": [
   675.94,
   685.26
  ]
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that.",
  "model": "nmt",
  "translatedText": "Pero eso es un mínimo, normalmente obtendrás algo mejor que eso.",
  "time_range": [
   685.92,
   688.56
  ]
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8.",
  "model": "nmt",
  "translatedText": "Y resulta que cuando haces cálculos en este caso y sumas todos los términos relevantes, la información promedio es de aproximadamente 5.8.",
  "time_range": [
   689.1,
   695.9
  ]
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average.",
  "model": "nmt",
  "translatedText": "Entonces, a diferencia de Weary, su espacio de posibilidades será aproximadamente la mitad después de esta primera suposición, en promedio.",
  "time_range": [
   697.36,
   703.54
  ]
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity.",
  "model": "nmt",
  "translatedText": "De hecho, hay una historia divertida sobre el nombre de este valor esperado de cantidad de información.",
  "time_range": [
   704.42,
   709.12
  ]
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science.",
  "model": "nmt",
  "translatedText": "La teoría de la información fue desarrollada por Claude Shannon, que trabajaba en los Laboratorios Bell en la década de 1940, pero estaba hablando de algunas de sus ideas aún por publicar con John von Neumann, que era este gigante intelectual de la época, muy destacado. en matemáticas y física y los inicios de lo que se estaba convirtiendo en informática.",
  "time_range": [
   709.2,
   723.56
  ]
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons.",
  "model": "nmt",
  "translatedText": "Y cuando mencionó que realmente no tenía un buen nombre para este valor esperado de la cantidad de información, supuestamente von Neumann dijo, según cuenta la historia, bueno, deberías llamarlo entropía, y por dos razones.",
  "time_range": [
   724.1,
   734.2
  ]
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage.",
  "model": "nmt",
  "translatedText": "En primer lugar, su función de incertidumbre se ha utilizado en mecánica estadística con ese nombre, por lo que ya tiene un nombre, y en segundo lugar, y más importante, nadie sabe qué es realmente la entropía, por lo que en un debate siempre tener la ventaja.",
  "time_range": [
   734.54,
   746.76
  ]
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design.",
  "model": "nmt",
  "translatedText": "Entonces, si el nombre parece un poco misterioso, y si hay que creer en esta historia, es más o menos intencionalmente.",
  "time_range": [
   747.7,
   752.46
  ]
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess.",
  "model": "nmt",
  "translatedText": "Además, si te preguntas acerca de su relación con toda esa segunda ley de la termodinámica de la física, definitivamente hay una conexión, pero en sus orígenes Shannon solo estaba tratando con la teoría de la probabilidad pura, y para nuestros propósitos aquí, cuando uso la palabra entropía, solo quiero que piense en el valor de información esperado de una suposición particular.",
  "time_range": [
   753.28,
   769.58
  ]
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously.",
  "model": "nmt",
  "translatedText": "Puedes pensar que la entropía mide dos cosas simultáneamente.",
  "time_range": [
   770.7,
   773.78
  ]
 },
 {
  "input": "The first one is how flat is the distribution.",
  "model": "nmt",
  "translatedText": "El primero es qué tan plana es la distribución.",
  "time_range": [
   774.24,
   776.78
  ]
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be.",
  "model": "nmt",
  "translatedText": "Cuanto más cercana a la uniformidad sea una distribución, mayor será la entropía.",
  "time_range": [
   777.32,
   781.12
  ]
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy.",
  "model": "nmt",
  "translatedText": "En nuestro caso, donde hay de 3 a 5 patrones en total, para una distribución uniforme, observar cualquiera de ellos tendría un registro de información en base 2 de 3 a 5, que resulta ser 7.92, por lo que ese es el máximo absoluto que podrías tener para esta entropía.",
  "time_range": [
   781.58,
   797.3
  ]
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place.",
  "model": "nmt",
  "translatedText": "Pero la entropía también es una especie de medida de cuántas posibilidades hay en primer lugar.",
  "time_range": [
   797.84,
   802.08
  ]
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits.",
  "model": "nmt",
  "translatedText": "Por ejemplo, si tienes una palabra en la que sólo hay 16 patrones posibles y cada uno de ellos es igualmente probable, esta entropía, esta información esperada, sería de 4 bits.",
  "time_range": [
   802.32,
   812.18
  ]
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits.",
  "model": "nmt",
  "translatedText": "Pero si tienes otra palabra donde hay 64 patrones posibles que podrían surgir, y todos son igualmente probables, entonces la entropía resultaría ser de 6 bits.",
  "time_range": [
   812.5799999999999,
   820.48
  ]
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes.",
  "model": "nmt",
  "translatedText": "Entonces, si ves alguna distribución en la naturaleza que tiene una entropía de 6 bits, es como si estuviera diciendo que hay tanta variación e incertidumbre en lo que está a punto de suceder como si hubiera 64 resultados igualmente probables.",
  "time_range": [
   821.5,
   833.5
  ]
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this.",
  "model": "nmt",
  "translatedText": "Para mi primera pasada por el Wurtelebot, básicamente le pedí que hiciera esto.",
  "time_range": [
   834.36,
   839.32
  ]
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible.",
  "model": "nmt",
  "translatedText": "Revisa todas las conjeturas posibles que puedas tener, las 13.000 palabras, calcula la entropía de cada una, o más específicamente, la entropía de la distribución en todos los patrones que puedas ver, para cada una, y elige la más alta, ya que es el que probablemente reducirá su espacio de posibilidades tanto como sea posible.",
  "time_range": [
   839.32,
   856.14
  ]
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses.",
  "model": "nmt",
  "translatedText": "Y aunque aquí solo he estado hablando de la primera suposición, ocurre lo mismo con las siguientes suposiciones.",
  "time_range": [
   857.14,
   861.1
  ]
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words.",
  "model": "nmt",
  "translatedText": "Por ejemplo, después de ver algún patrón en esa primera suposición, que lo restringiría a un número menor de palabras posibles en función de lo que coincide con eso, simplemente juega el mismo juego con respecto a ese conjunto más pequeño de palabras.",
  "time_range": [
   861.56,
   871.8
  ]
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy.",
  "model": "nmt",
  "translatedText": "Para una segunda suposición propuesta, observamos la distribución de todos los patrones que podrían ocurrir a partir de ese conjunto más restringido de palabras, buscamos entre las 13.000 posibilidades y encontramos la que maximiza esa entropía.",
  "time_range": [
   872.26,
   883.84
  ]
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins.",
  "model": "nmt",
  "translatedText": "Para mostrarles cómo funciona esto en acción, permítanme mostrarles una pequeña variante de Wurtele que escribí y que muestra los aspectos más destacados de este análisis en los márgenes.",
  "time_range": [
   885.42,
   894.08
  ]
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information.",
  "model": "nmt",
  "translatedText": "Después de hacer todos los cálculos de entropía, aquí a la derecha nos muestra cuáles tienen la información esperada más alta.",
  "time_range": [
   894.08,
   899.66
  ]
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch.",
  "model": "nmt",
  "translatedText": "Resulta que la respuesta principal, al menos por el momento, lo refinaremos más adelante, es Tares, que significa, por supuesto, arveja, la arveja más común.",
  "time_range": [
   900.28,
   910.58
  ]
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern.",
  "model": "nmt",
  "translatedText": "Cada vez que hacemos una suposición aquí, donde tal vez ignoro sus recomendaciones y elijo slate, porque me gusta slate, podemos ver cuánta información esperada tenía, pero luego, a la derecha de la palabra aquí, nos muestra cuánta información real que obtuvimos, dado este patrón particular.",
  "time_range": [
   911.04,
   924.42
  ]
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that.",
  "model": "nmt",
  "translatedText": "Así que aquí parece que tuvimos un poco de mala suerte, se esperaba que obtuviéramos 5.8, pero obtuvimos algo con menos que eso.",
  "time_range": [
   925.0,
   930.12
  ]
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now.",
  "model": "nmt",
  "translatedText": "Y luego, en el lado izquierdo, aquí nos muestra todas las diferentes palabras posibles según el lugar donde nos encontramos ahora.",
  "time_range": [
   930.6,
   935.02
  ]
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment.",
  "model": "nmt",
  "translatedText": "Las barras azules nos dicen qué tan probable cree que es cada palabra, por lo que por el momento suponemos que cada palabra tiene la misma probabilidad de ocurrir, pero lo refinaremos en un momento.",
  "time_range": [
   935.8,
   943.36
  ]
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities.",
  "model": "nmt",
  "translatedText": "Y luego esta medida de incertidumbre nos dice la entropía de esta distribución entre las palabras posibles, que ahora mismo, debido a que es una distribución uniforme, es solo una forma innecesariamente complicada de contar el número de posibilidades.",
  "time_range": [
   944.06,
   955.96
  ]
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities.",
  "model": "nmt",
  "translatedText": "Por ejemplo, si tuviéramos que elevar 2 a la potencia de 13.66, eso debería rondar las 13.000 posibilidades.",
  "time_range": [
   956.56,
   962.18
  ]
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places.",
  "model": "nmt",
  "translatedText": "Estoy un poco fuera de lugar aquí, pero sólo porque no muestro todos los decimales.",
  "time_range": [
   962.9,
   966.14
  ]
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute.",
  "model": "nmt",
  "translatedText": "Por el momento, esto puede parecer redundante y complicar demasiado las cosas, pero verá por qué es útil tener ambos números en un minuto.",
  "time_range": [
   966.72,
   972.34
  ]
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word.",
  "model": "nmt",
  "translatedText": "Así que aquí parece que sugiere que la entropía más alta para nuestra segunda suposición es Ramen, que nuevamente no parece una palabra.",
  "time_range": [
   972.76,
   979.4
  ]
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains.",
  "model": "nmt",
  "translatedText": "Entonces, para tener autoridad moral aquí, seguiré adelante y escribiré Rains.",
  "time_range": [
   979.98,
   984.06
  ]
 },
 {
  "input": "And again it looks like we were a little unlucky.",
  "model": "nmt",
  "translatedText": "Y nuevamente parece que tuvimos un poco de mala suerte.",
  "time_range": [
   985.44,
   987.34
  ]
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information.",
  "model": "nmt",
  "translatedText": "Esperábamos 4.3 bits y solo tenemos 3.39 bits de información.",
  "time_range": [
   987.52,
   991.36
  ]
 },
 {
  "input": "So that takes us down to 55 possibilities.",
  "model": "nmt",
  "translatedText": "Eso nos lleva a 55 posibilidades.",
  "time_range": [
   991.94,
   993.94
  ]
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means.",
  "model": "nmt",
  "translatedText": "Y aquí tal vez me quede con lo que sugiere, que es combo, sea lo que sea que eso signifique.",
  "time_range": [
   994.9,
   999.44
  ]
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle.",
  "model": "nmt",
  "translatedText": "Y está bien, esta es en realidad una buena oportunidad para resolver un rompecabezas.",
  "time_range": [
   1000.04,
   1002.92
  ]
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information.",
  "model": "nmt",
  "translatedText": "Nos dice que este patrón nos da 4.7 bits de información.",
  "time_range": [
   1002.92,
   1006.38
  ]
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty.",
  "model": "nmt",
  "translatedText": "Pero a la izquierda, antes de que veamos ese patrón, había 5.78 bits de incertidumbre.",
  "time_range": [
   1007.06,
   1011.72
  ]
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities?",
  "model": "nmt",
  "translatedText": "Entonces, a modo de prueba, ¿qué significa eso sobre el número de posibilidades restantes?",
  "time_range": [
   1012.42,
   1016.34
  ]
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers.",
  "model": "nmt",
  "translatedText": "Bueno, significa que estamos reducidos a un poco de incertidumbre, que es lo mismo que decir que hay dos respuestas posibles.",
  "time_range": [
   1018.04,
   1024.54
  ]
 },
 {
  "input": "It's a 50-50 choice.",
  "model": "nmt",
  "translatedText": "Es una elección 50-50.",
  "time_range": [
   1024.7,
   1025.7
  ]
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss.",
  "model": "nmt",
  "translatedText": "Y a partir de aquí, porque tú y yo sabemos qué palabras son más comunes, sabemos que la respuesta debería ser abismo.",
  "time_range": [
   1026.5,
   1030.64
  ]
 },
 {
  "input": "But as it's written right now, the program doesn't know that.",
  "model": "nmt",
  "translatedText": "Pero tal como está escrito ahora, el programa no lo sabe.",
  "time_range": [
   1031.18,
   1033.28
  ]
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it.",
  "model": "nmt",
  "translatedText": "Así que sigue adelante, tratando de obtener tanta información como puede, hasta que sólo queda una posibilidad, y luego la adivina.",
  "time_range": [
   1033.54,
   1039.86
  ]
 },
 {
  "input": "So obviously we need a better endgame strategy.",
  "model": "nmt",
  "translatedText": "Obviamente necesitamos una mejor estrategia para el final del juego.",
  "time_range": [
   1040.38,
   1042.34
  ]
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does.",
  "model": "nmt",
  "translatedText": "Pero digamos que llamamos a esta versión uno de nuestro solucionador de palabras y luego ejecutamos algunas simulaciones para ver cómo funciona.",
  "time_range": [
   1042.6,
   1048.26
  ]
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game.",
  "model": "nmt",
  "translatedText": "Entonces, la forma en que esto funciona es jugando todos los juegos de palabras posibles.",
  "time_range": [
   1050.36,
   1054.12
  ]
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers.",
  "model": "nmt",
  "translatedText": "Está repasando todas esas 2315 palabras que son las respuestas reales.",
  "time_range": [
   1054.24,
   1058.54
  ]
 },
 {
  "input": "It's basically using that as a testing set.",
  "model": "nmt",
  "translatedText": "Básicamente se trata de utilizarlo como un conjunto de pruebas.",
  "time_range": [
   1058.54,
   1060.58
  ]
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice.",
  "model": "nmt",
  "translatedText": "Y con este método ingenuo de no considerar qué tan común es una palabra y simplemente tratar de maximizar la información en cada paso del camino, hasta llegar a una y sólo una opción.",
  "time_range": [
   1061.36,
   1069.82
  ]
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124.",
  "model": "nmt",
  "translatedText": "Al final de la simulación, la puntuación media resulta ser de aproximadamente 4.124.",
  "time_range": [
   1070.36,
   1074.3
  ]
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse.",
  "model": "nmt",
  "translatedText": "Lo cual no está mal, para ser honesto, esperaba hacerlo peor.",
  "time_range": [
   1075.3199999999997,
   1079.24
  ]
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4.",
  "model": "nmt",
  "translatedText": "Pero la gente que juega wordle te dirá que normalmente lo consiguen en 4.",
  "time_range": [
   1079.66,
   1082.6
  ]
 },
 {
  "input": "The real challenge is to get as many in 3 as you can.",
  "model": "nmt",
  "translatedText": "El verdadero desafío es conseguir tantos en 3 como puedas.",
  "time_range": [
   1082.86,
   1085.38
  ]
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3.",
  "model": "nmt",
  "translatedText": "Es un salto bastante grande entre la puntuación de 4 y la puntuación de 3.",
  "time_range": [
   1085.38,
   1088.08
  ]
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that.",
  "model": "nmt",
  "translatedText": "Lo obvio aquí es incorporar de alguna manera si una palabra es común o no, y cómo lo hacemos exactamente.",
  "time_range": [
   1088.86,
   1094.98
  ]
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language.",
  "model": "nmt",
  "translatedText": "La forma en que lo acerqué es obtener una lista de las frecuencias relativas de todas las palabras en el idioma inglés.",
  "time_range": [
   1102.8,
   1107.88
  ]
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset.",
  "model": "nmt",
  "translatedText": "Y acabo de utilizar la función de datos de frecuencia de palabras de Mathematica, que a su vez se extrae del conjunto de datos públicos Ngram en inglés de Google Books.",
  "time_range": [
   1108.22,
   1114.86
  ]
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words.",
  "model": "nmt",
  "translatedText": "Y es divertido verlo, por ejemplo, si lo clasificamos desde las palabras más comunes hasta las menos comunes.",
  "time_range": [
   1115.46,
   1119.96
  ]
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language.",
  "model": "nmt",
  "translatedText": "Evidentemente estas son las palabras de cinco letras más comunes en el idioma inglés.",
  "time_range": [
   1120.12,
   1123.08
  ]
 },
 {
  "input": "Or rather, these is the 8th most common.",
  "model": "nmt",
  "translatedText": "O mejor dicho, este es el octavo más común.",
  "time_range": [
   1123.7,
   1125.84
  ]
 },
 {
  "input": "First is which, after which there's there and there.",
  "model": "nmt",
  "translatedText": "Primero es cuál, después está ahí y ahí.",
  "time_range": [
   1126.28,
   1128.88
  ]
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common.",
  "model": "nmt",
  "translatedText": "Primero en sí no es primero, sino noveno, y tiene sentido que estas otras palabras puedan aparecer con más frecuencia, donde las que siguen a primero son después, dónde, y que son un poco menos comunes.",
  "time_range": [
   1129.26,
   1138.58
  ]
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency.",
  "model": "nmt",
  "translatedText": "Ahora bien, al utilizar estos datos para modelar la probabilidad de que cada una de estas palabras sea la respuesta final, no debería ser sólo proporcional a la frecuencia.",
  "time_range": [
   1139.16,
   1146.86
  ]
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely.",
  "model": "nmt",
  "translatedText": "Por ejemplo, a la que se le da una puntuación de 0.002 en este conjunto de datos, mientras que la palabra trenza es, en cierto sentido, aproximadamente 1000 veces menos probable.",
  "time_range": [
   1146.86,
   1155.06
  ]
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering.",
  "model": "nmt",
  "translatedText": "Pero ambas son palabras bastante comunes que casi con seguridad vale la pena considerarlas.",
  "time_range": [
   1155.56,
   1158.84
  ]
 },
 {
  "input": "So we want more of a binary cutoff.",
  "model": "nmt",
  "translatedText": "Por eso queremos más un límite binario.",
  "time_range": [
   1159.34,
   1161.0
  ]
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty.",
  "model": "nmt",
  "translatedText": "La forma en que lo hice es imaginar tomar toda esta lista ordenada de palabras, y luego organizarla en un eje x, y luego aplicar la función sigmoidea, que es la forma estándar de tener una función cuya salida es básicamente binaria, es 0 o 1, pero hay un suavizado intermedio para esa región de incertidumbre.",
  "time_range": [
   1161.86,
   1178.26
  ]
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis.",
  "model": "nmt",
  "translatedText": "Básicamente, la probabilidad que estoy asignando a cada palabra de estar en la lista final será el valor de la función sigmoidea arriba dondequiera que se encuentre en el eje x.",
  "time_range": [
   1179.16,
   1188.44
  ]
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff.",
  "model": "nmt",
  "translatedText": "Ahora bien, obviamente, esto depende de algunos parámetros, por ejemplo, qué tan ancho es el espacio en el eje x que ocupan esas palabras determina qué tan gradual o abruptamente bajamos de 1 a 0, y dónde las ubicamos de izquierda a derecha determina el límite.",
  "time_range": [
   1189.52,
   1203.24
  ]
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind.",
  "model": "nmt",
  "translatedText": "Para ser honesto, la forma en que hice esto fue simplemente lamerme el dedo y pegarlo al viento.",
  "time_range": [
   1203.24,
   1206.92
  ]
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff.",
  "model": "nmt",
  "translatedText": "Revisé la lista ordenada y traté de encontrar una ventana donde, cuando la miré, pensé que era más probable que aproximadamente la mitad de estas palabras fueran la respuesta final, y la usé como límite.",
  "time_range": [
   1207.14,
   1217.26
  ]
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement.",
  "model": "nmt",
  "translatedText": "Una vez que tenemos una distribución como esta entre las palabras, nos da otra situación en la que la entropía se convierte en una medida realmente útil.",
  "time_range": [
   1217.26,
   1223.86
  ]
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it.",
  "model": "nmt",
  "translatedText": "Por ejemplo, digamos que estamos jugando y comenzamos con mis viejos abridores, que eran plumas y clavos, y terminamos con una situación en la que hay cuatro palabras posibles que coinciden.",
  "time_range": [
   1224.5,
   1233.24
  ]
 },
 {
  "input": "And let's say we consider them all equally likely.",
  "model": "nmt",
  "translatedText": "Y digamos que los consideramos todos igualmente probables.",
  "time_range": [
   1233.56,
   1235.62
  ]
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution?",
  "model": "nmt",
  "translatedText": "Déjame preguntarte, ¿cuál es la entropía de esta distribución?",
  "time_range": [
   1236.22,
   1238.88
  ]
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2.",
  "model": "nmt",
  "translatedText": "Bueno, la información asociada a cada una de estas posibilidades va a ser el logaritmo en base 2 de 4, ya que cada una es 1 y 4, y eso es 2.",
  "time_range": [
   1241.08,
   1250.04
  ]
 },
 {
  "input": "Two bits of information, four possibilities.",
  "model": "nmt",
  "translatedText": "Dos bits de información, cuatro posibilidades.",
  "time_range": [
   1250.04,
   1252.46
  ]
 },
 {
  "input": "All very well and good.",
  "model": "nmt",
  "translatedText": "Todo muy bien y bueno.",
  "time_range": [
   1252.76,
   1253.58
  ]
 },
 {
  "input": "But what if I told you that actually there's more than four matches?",
  "model": "nmt",
  "translatedText": "Pero ¿y si te dijera que en realidad hay más de cuatro coincidencias?",
  "time_range": [
   1254.3,
   1257.8
  ]
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it.",
  "model": "nmt",
  "translatedText": "En realidad, cuando miramos la lista completa de palabras, hay 16 palabras que coinciden.",
  "time_range": [
   1258.26,
   1262.46
  ]
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure.",
  "model": "nmt",
  "translatedText": "Pero supongamos que nuestro modelo asigna una probabilidad realmente baja a que esas otras 12 palabras sean realmente la respuesta final, algo así como 1 entre 1000 porque son muy oscuras.",
  "time_range": [
   1262.58,
   1270.76
  ]
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution?",
  "model": "nmt",
  "translatedText": "Ahora déjame preguntarte, ¿cuál es la entropía de esta distribución?",
  "time_range": [
   1271.5,
   1274.26
  ]
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before.",
  "model": "nmt",
  "translatedText": "Si la entropía simplemente midiera el número de coincidencias aquí, entonces se podría esperar que fuera algo así como el logaritmo en base 2 de 16, que sería 4, dos bits más de incertidumbre que los que teníamos antes.",
  "time_range": [
   1275.42,
   1285.7
  ]
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before.",
  "model": "nmt",
  "translatedText": "Pero, por supuesto, la incertidumbre real no es tan diferente de la que teníamos antes.",
  "time_range": [
   1286.18,
   1289.86
  ]
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example.",
  "model": "nmt",
  "translatedText": "El hecho de que existan estas 12 palabras realmente oscuras no significa que sería mucho más sorprendente saber que la respuesta final es encanto, por ejemplo.",
  "time_range": [
   1290.16,
   1297.36
  ]
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits.",
  "model": "nmt",
  "translatedText": "Entonces, cuando realmente haces el cálculo aquí y sumas la probabilidad de cada ocurrencia multiplicada por la información correspondiente, lo que obtienes es 2.11 bits.",
  "time_range": [
   1298.18,
   1305.56
  ]
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it.",
  "model": "nmt",
  "translatedText": "Solo digo que son básicamente dos bits, básicamente esas cuatro posibilidades, pero hay un poco más de incertidumbre debido a todos esos eventos altamente improbables, aunque si los aprendieras, obtendrías un montón de información.",
  "time_range": [
   1305.56,
   1316.5
  ]
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson.",
  "model": "nmt",
  "translatedText": "Entonces, alejarnos, esto es parte de lo que hace de Wordle un buen ejemplo para una lección de teoría de la información.",
  "time_range": [
   1317.16,
   1321.4
  ]
 },
 {
  "input": "We have these two distinct feeling applications for entropy.",
  "model": "nmt",
  "translatedText": "Tenemos estas dos aplicaciones de sentimiento distintas para la entropía.",
  "time_range": [
   1321.6,
   1324.64
  ]
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible.",
  "model": "nmt",
  "translatedText": "El primero nos dice cuál es la información esperada que obtendremos de una suposición determinada, y el segundo dice si podemos medir la incertidumbre restante entre todas las palabras que tenemos posibles.",
  "time_range": [
   1325.16,
   1335.46
  ]
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation.",
  "model": "nmt",
  "translatedText": "Y debo enfatizar que, en el primer caso en el que observamos la información esperada de una suposición, una vez que tenemos una ponderación desigual de las palabras, eso afecta el cálculo de la entropía.",
  "time_range": [
   1336.46,
   1344.54
  ]
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words.",
  "model": "nmt",
  "translatedText": "Por ejemplo, permítanme mencionar el mismo caso que vimos anteriormente de la distribución asociada con Weary, pero esta vez usando una distribución no uniforme en todas las palabras posibles.",
  "time_range": [
   1344.98,
   1353.72
  ]
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well.",
  "model": "nmt",
  "translatedText": "Déjame ver si puedo encontrar una parte aquí que lo ilustre bastante bien.",
  "time_range": [
   1354.5,
   1358.28
  ]
 },
 {
  "input": "Okay, here this is pretty good.",
  "model": "nmt",
  "translatedText": "Bien, aquí esto está bastante bien.",
  "time_range": [
   1360.94,
   1362.36
  ]
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it.",
  "model": "nmt",
  "translatedText": "Aquí tenemos dos patrones adyacentes que son igualmente probables, pero nos dicen que uno de ellos tiene 32 palabras posibles que coinciden.",
  "time_range": [
   1362.36,
   1369.1
  ]
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them.",
  "model": "nmt",
  "translatedText": "Y si comprobamos cuáles son, estas son esas 32, que son palabras muy improbables cuando las examinas con la vista.",
  "time_range": [
   1369.28,
   1375.6
  ]
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely.",
  "model": "nmt",
  "translatedText": "Es difícil encontrar respuestas que parezcan plausibles, tal vez gritos, pero si miramos el patrón vecino en la distribución, que se considera casi igual de probable, nos dicen que solo tiene 8 coincidencias posibles, por lo que una cuarta parte de Hay muchas coincidencias, pero es casi igual de probable.",
  "time_range": [
   1375.84,
   1389.52
  ]
 },
 {
  "input": "And when we pull up those matches, we can see why.",
  "model": "nmt",
  "translatedText": "Y cuando analizamos esas coincidencias, podemos ver por qué.",
  "time_range": [
   1389.86,
   1392.14
  ]
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps.",
  "model": "nmt",
  "translatedText": "Algunas de estas son respuestas realmente plausibles, como timbre, ira o golpes.",
  "time_range": [
   1392.5,
   1396.3
  ]
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw.",
  "model": "nmt",
  "translatedText": "Para ilustrar cómo incorporamos todo eso, permítanme mostrar aquí la versión 2 del Wordlebot, y hay dos o tres diferencias principales con respecto a la primera que vimos.",
  "time_range": [
   1397.9,
   1405.28
  ]
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer.",
  "model": "nmt",
  "translatedText": "En primer lugar, como acabo de decir, la forma en que calculamos estas entropías, estos valores esperados de información, ahora utiliza distribuciones más refinadas entre los patrones que incorporan la probabilidad de que una palabra determinada sea realmente la respuesta.",
  "time_range": [
   1405.86,
   1418.24
  ]
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different.",
  "model": "nmt",
  "translatedText": "Da la casualidad de que las lágrimas siguen siendo el número 1, aunque las siguientes son un poco diferentes.",
  "time_range": [
   1418.8799999999999,
   1423.82
  ]
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table.",
  "model": "nmt",
  "translatedText": "En segundo lugar, cuando clasifique sus mejores opciones, ahora mantendrá un modelo de la probabilidad de que cada palabra sea la respuesta real, y lo incorporará en su decisión, lo cual es más fácil de ver una vez que tengamos algunas conjeturas sobre la respuesta. mesa.",
  "time_range": [
   1424.36,
   1435.08
  ]
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives.",
  "model": "nmt",
  "translatedText": "Nuevamente, ignorando su recomendación porque no podemos dejar que las máquinas gobiernen nuestras vidas.",
  "time_range": [
   1435.86,
   1439.78
  ]
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches.",
  "model": "nmt",
  "translatedText": "Y supongo que debería mencionar otra cosa diferente aquí a la izquierda, ese valor de incertidumbre, esa cantidad de bits, ya no es simplemente redundante con la cantidad de coincidencias posibles.",
  "time_range": [
   1441.14,
   1449.64
  ]
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes.",
  "model": "nmt",
  "translatedText": "Ahora si lo levantamos y calculamos 2 elevado a 8.02, que está un poco por encima de 256, supongo que 259, lo que dice es que aunque hay un total de 526 palabras que realmente coinciden con este patrón, la cantidad de incertidumbre que tiene es más parecida a la que sería si hubiera 259 igualmente probables. resultados.",
  "time_range": [
   1450.08,
   1468.98
  ]
 },
 {
  "input": "You can think of it like this.",
  "model": "nmt",
  "translatedText": "Puedes pensar en ello así.",
  "time_range": [
   1469.72,
   1470.74
  ]
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case.",
  "model": "nmt",
  "translatedText": "Sabe que borx no es la respuesta, lo mismo ocurre con yorts, zorl y zorus, por lo que es un poco menos incierto que en el caso anterior.",
  "time_range": [
   1471.02,
   1477.68
  ]
 },
 {
  "input": "This number of bits will be smaller.",
  "model": "nmt",
  "translatedText": "Este número de bits será menor.",
  "time_range": [
   1477.82,
   1479.28
  ]
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here.",
  "model": "nmt",
  "translatedText": "Y si sigo jugando, lo refinaré con un par de suposiciones que son apropiadas para lo que me gustaría explicar aquí.",
  "time_range": [
   1480.22,
   1486.54
  ]
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy.",
  "model": "nmt",
  "translatedText": "En la cuarta suposición, si observa sus mejores opciones, podrá ver que ya no se trata simplemente de maximizar la entropía.",
  "time_range": [
   1488.36,
   1493.76
  ]
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words.",
  "model": "nmt",
  "translatedText": "Entonces, en este punto, técnicamente hay siete posibilidades, pero las únicas con posibilidades significativas son los dormitorios y las palabras.",
  "time_range": [
   1494.46,
   1500.3
  ]
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information.",
  "model": "nmt",
  "translatedText": "Y puede ver que se clasifica al elegir ambos por encima de todos estos otros valores, que estrictamente hablando brindarían más información.",
  "time_range": [
   1500.3,
   1506.72
  ]
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect.",
  "model": "nmt",
  "translatedText": "La primera vez que hice esto, simplemente sumé estos dos números para medir la calidad de cada suposición, lo que en realidad funcionó mejor de lo que imaginas.",
  "time_range": [
   1507.24,
   1513.9
  ]
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on.",
  "model": "nmt",
  "translatedText": "Pero realmente no lo sentí sistemático, y estoy seguro de que hay otros enfoques que la gente podría adoptar, pero este es el que encontré.",
  "time_range": [
   1514.3,
   1519.34
  ]
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that.",
  "model": "nmt",
  "translatedText": "Si estamos considerando la posibilidad de una próxima suposición, como en este caso palabras, lo que realmente nos importa es la puntuación esperada de nuestro juego si lo hacemos.",
  "time_range": [
   1519.76,
   1527.9
  ]
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to.",
  "model": "nmt",
  "translatedText": "Y para calcular esa puntuación esperada, decimos cuál es la probabilidad de que las palabras sean la respuesta real, que en este momento describe el 58%.",
  "time_range": [
   1528.23,
   1535.9
  ]
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4.",
  "model": "nmt",
  "translatedText": "Decimos que con un 58% de posibilidades, nuestra puntuación en este juego sería 4.",
  "time_range": [
   1536.04,
   1539.54
  ]
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4.",
  "model": "nmt",
  "translatedText": "Y luego, con la probabilidad de 1 menos ese 58%, nuestra puntuación será mayor que ese 4.",
  "time_range": [
   1540.32,
   1545.64
  ]
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point.",
  "model": "nmt",
  "translatedText": "No sabemos cuánto más, pero podemos estimarlo en función de cuánta incertidumbre probablemente habrá una vez que lleguemos a ese punto.",
  "time_range": [
   1546.22,
   1552.46
  ]
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty.",
  "model": "nmt",
  "translatedText": "En concreto, de momento hay 1.44 bits de incertidumbre.",
  "time_range": [
   1552.96,
   1555.94
  ]
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits.",
  "model": "nmt",
  "translatedText": "Si adivinamos palabras, nos dice que la información esperada que obtendremos es 1.27 bits.",
  "time_range": [
   1556.44,
   1561.12
  ]
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens.",
  "model": "nmt",
  "translatedText": "Entonces, si adivinamos palabras, esta diferencia representa cuánta incertidumbre es probable que nos quede después de que eso suceda.",
  "time_range": [
   1561.62,
   1567.66
  ]
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score.",
  "model": "nmt",
  "translatedText": "Lo que necesitamos es algún tipo de función, a la que aquí llamo f, que asocie esta incertidumbre con una puntuación esperada.",
  "time_range": [
   1568.26,
   1573.74
  ]
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty.",
  "model": "nmt",
  "translatedText": "Y la forma en que lo hicimos fue simplemente trazar un montón de datos de juegos anteriores basados en la versión 1 del bot para decir cuál fue el puntaje real después de varios puntos con ciertas cantidades de incertidumbre muy mensurables.",
  "time_range": [
   1574.24,
   1586.32
  ]
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer.",
  "model": "nmt",
  "translatedText": "Por ejemplo, estos puntos de datos aquí que se encuentran por encima de un valor cercano a 8.Aproximadamente 7, dicen para algunos juegos después de un punto en el que había 8.7 bits de incertidumbre, fueron necesarias dos conjeturas para obtener la respuesta final.",
  "time_range": [
   1587.02,
   1598.96
  ]
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses.",
  "model": "nmt",
  "translatedText": "Para otros juegos fueron necesarias tres conjeturas, para otros juegos fueron necesarias cuatro conjeturas.",
  "time_range": [
   1599.32,
   1602.24
  ]
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring.",
  "model": "nmt",
  "translatedText": "Si aquí nos desplazamos hacia la izquierda, todos los puntos sobre cero dicen que siempre que haya cero bits de incertidumbre, es decir, que solo hay una posibilidad, entonces el número de conjeturas requeridas es siempre solo uno, lo cual es tranquilizador.",
  "time_range": [
   1603.14,
   1614.26
  ]
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses.",
  "model": "nmt",
  "translatedText": "Cada vez que había un poco de incertidumbre, lo que significaba que esencialmente se reducía a dos posibilidades, a veces se requería una conjetura más, a veces se requerían dos conjeturas más.",
  "time_range": [
   1614.78,
   1623.02
  ]
 },
 {
  "input": "And so on and so forth here.",
  "model": "nmt",
  "translatedText": "Y así sucesivamente aquí.",
  "time_range": [
   1623.08,
   1625.24
  ]
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages.",
  "model": "nmt",
  "translatedText": "Quizás una forma un poco más sencilla de visualizar estos datos sea agruparlos y tomar promedios.",
  "time_range": [
   1625.74,
   1630.22
  ]
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5.",
  "model": "nmt",
  "translatedText": "Por ejemplo, esta barra dice que entre todos los puntos en los que teníamos un poco de incertidumbre, en promedio el número de nuevas conjeturas requeridas fue aproximadamente 1.5.",
  "time_range": [
   1631.0,
   1639.96
  ]
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward.",
  "model": "nmt",
  "translatedText": "Y la barra de aquí dice que entre todos los diferentes juegos donde en algún momento la incertidumbre estuvo un poco por encima de los cuatro bits, lo que es como reducirla a 16 posibilidades diferentes, entonces, en promedio, requiere un poco más de dos conjeturas a partir de ese punto. adelante.",
  "time_range": [
   1642.14,
   1655.38
  ]
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this.",
  "model": "nmt",
  "translatedText": "Y a partir de aquí simplemente hice una regresión para ajustarme a una función que parecía razonable para esto.",
  "time_range": [
   1656.06,
   1659.46
  ]
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be.",
  "model": "nmt",
  "translatedText": "Y recuerde que el objetivo de hacer todo esto es que podamos cuantificar esta intuición de que cuanta más información obtengamos de una palabra, menor será la puntuación esperada.",
  "time_range": [
   1659.98,
   1668.96
  ]
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do?",
  "model": "nmt",
  "translatedText": "Entonces con esto como versión 2.0, si volvemos atrás y ejecutamos el mismo conjunto de simulaciones, haciéndolo jugar contra las 2315 respuestas posibles, ¿cómo funciona?",
  "time_range": [
   1669.68,
   1679.24
  ]
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring.",
  "model": "nmt",
  "translatedText": "Bueno, a diferencia de nuestra primera versión, es definitivamente mejor, lo cual es tranquilizador.",
  "time_range": [
   1680.28,
   1683.42
  ]
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance.",
  "model": "nmt",
  "translatedText": "Todo dicho y hecho, la media ronda los 3.6, aunque a diferencia de la primera versión hay un par de veces que pierde y requiere más de seis en esta circunstancia.",
  "time_range": [
   1684.02,
   1692.12
  ]
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information.",
  "model": "nmt",
  "translatedText": "Presumiblemente porque hay momentos en los que se trata de buscar el objetivo en lugar de maximizar la información.",
  "time_range": [
   1692.6399999999999,
   1697.94
  ]
 },
 {
  "input": "So can we do better than 3.6?",
  "model": "nmt",
  "translatedText": "Entonces, ¿podemos hacerlo mejor que 3?6?",
  "time_range": [
   1699.04,
   1701.0
  ]
 },
 {
  "input": "We definitely can.",
  "model": "nmt",
  "translatedText": "Definitivamente podemos.",
  "time_range": [
   1702.08,
   1702.92
  ]
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model.",
  "model": "nmt",
  "translatedText": "Ahora dije al principio que es muy divertido intentar no incorporar la lista verdadera de respuestas de Wordle en la forma en que construye su modelo.",
  "time_range": [
   1703.28,
   1709.36
  ]
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43.",
  "model": "nmt",
  "translatedText": "Pero si lo incorporamos, el mejor rendimiento que pude obtener fue alrededor de 3.43.",
  "time_range": [
   1709.88,
   1714.18
  ]
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that.",
  "model": "nmt",
  "translatedText": "Entonces, si intentamos ser más sofisticados que simplemente usar datos de frecuencia de palabras para elegir esta distribución previa, este 3.43 probablemente da un máximo de lo buenos que podríamos llegar a ser con eso, o al menos de lo bueno que podría llegar a ser con eso.",
  "time_range": [
   1715.16,
   1725.74
  ]
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one.",
  "model": "nmt",
  "translatedText": "Ese mejor rendimiento esencialmente solo utiliza las ideas de las que he estado hablando aquí, pero va un poco más allá, como si buscara la información esperada dos pasos adelante en lugar de solo uno.",
  "time_range": [
   1726.24,
   1735.12
  ]
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is.",
  "model": "nmt",
  "translatedText": "Originalmente estaba planeando hablar más sobre eso, pero me doy cuenta de que ya hemos durado bastante.",
  "time_range": [
   1735.62,
   1740.22
  ]
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener.",
  "model": "nmt",
  "translatedText": "Lo único que diré es que después de hacer esta búsqueda de dos pasos y luego ejecutar un par de simulaciones de muestra en los mejores candidatos, hasta ahora al menos para mí parece que Crane es el mejor abridor.",
  "time_range": [
   1740.58,
   1749.1
  ]
 },
 {
  "input": "Who would have guessed?",
  "model": "nmt",
  "translatedText": "¿Quién lo hubiera adivinado?",
  "time_range": [
   1749.1,
   1750.06
  ]
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits.",
  "model": "nmt",
  "translatedText": "Además, si utiliza la lista de palabras verdaderas para determinar su espacio de posibilidades, entonces la incertidumbre con la que comienza es un poco más de 11 bits.",
  "time_range": [
   1750.92,
   1757.82
  ]
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits.",
  "model": "nmt",
  "translatedText": "Y resulta que, sólo a partir de una búsqueda de fuerza bruta, la máxima información esperada posible después de las dos primeras conjeturas es de alrededor de 10 bits.",
  "time_range": [
   1758.3,
   1765.88
  ]
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty.",
  "model": "nmt",
  "translatedText": "Lo que sugiere que en el mejor de los casos, después de tus dos primeras conjeturas, con un juego perfectamente óptimo, te quedarás con un poco de incertidumbre.",
  "time_range": [
   1766.5,
   1774.56
  ]
 },
 {
  "input": "Which is the same as being down to two possible guesses.",
  "model": "nmt",
  "translatedText": "Lo que es lo mismo que limitarse a dos posibles conjeturas.",
  "time_range": [
   1774.8,
   1777.96
  ]
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail.",
  "model": "nmt",
  "translatedText": "Así que creo que es justo y probablemente bastante conservador decir que nunca sería posible escribir un algoritmo que consiga este promedio tan bajo como 3, porque con las palabras disponibles, simplemente no hay espacio para obtener suficiente información después de sólo dos pasos para ser capaz de garantizar la respuesta en el tercer espacio cada vez sin falta.",
  "time_range": [
   1777.96,
   1793.36
  ]
 }
]