1
00:00:00,000 --> 00:00:04,040
El juego Wurdle se ha vuelto bastante viral en los últimos dos meses

2
00:00:04,040 --> 00:00:07,880
y, como nunca desperdiciaré la oportunidad de una lección de matemáticas, se me

3
00:00:07,880 --> 00:00:12,120
ocurre que este juego es un muy buen ejemplo central en una lección

4
00:00:12,120 --> 00:00:13,120
sobre teoría de la información y, en particular, un tema conocido como entropía.

5
00:00:13,120 --> 00:00:17,120
Verá, como mucha gente, me quedé atrapado en el rompecabezas, y como

6
00:00:17,120 --> 00:00:21,200
muchos programadores, también me quedé atrapado en el intento de escribir un

7
00:00:21,200 --> 00:00:23,200
algoritmo que jugara el juego de la manera más óptima posible.

8
00:00:23,200 --> 00:00:26,400
Y lo que pensé que haría aquí es simplemente hablarles sobre

9
00:00:26,400 --> 00:00:29,980
mi proceso y explicarles algunas de las matemáticas que implican, ya

10
00:00:29,980 --> 00:00:32,080
que todo el algoritmo se centra en esta idea de entropía.

11
00:00:32,080 --> 00:00:42,180
Lo primero es lo primero, en caso de que no hayas oído hablar de él, ¿qué es Wurdle?

12
00:00:42,180 --> 00:00:45,380
Y para matar dos pájaros de un tiro mientras repasamos las reglas del

13
00:00:45,380 --> 00:00:48,980
juego, permítanme también un avance de hacia dónde vamos con esto, que

14
00:00:48,980 --> 00:00:51,380
es desarrollar un pequeño algoritmo que básicamente jugará el juego por nosotros.

15
00:00:51,380 --> 00:00:54,860
Aunque no he hecho el Wurdle de hoy, es el

16
00:00:54,860 --> 00:00:55,860
4 de febrero y veremos cómo le va al robot.

17
00:00:55,860 --> 00:00:59,580
El objetivo de Wurdle es adivinar una palabra misteriosa de

18
00:00:59,580 --> 00:01:00,860
cinco letras y tienes seis oportunidades diferentes para adivinar.

19
00:01:00,860 --> 00:01:05,240
Por ejemplo, mi robot Wurdle me sugiere que empiece con la grúa de adivinanzas.

20
00:01:05,240 --> 00:01:09,300
Cada vez que haces una suposición, obtienes información sobre qué

21
00:01:09,300 --> 00:01:10,940
tan cerca está tu suposición de la respuesta verdadera.

22
00:01:10,940 --> 00:01:14,540
Aquí el cuadro gris me dice que no hay una C en la respuesta real.

23
00:01:14,540 --> 00:01:18,340
El cuadro amarillo me dice que hay una R, pero no está en esa posición.

24
00:01:18,340 --> 00:01:21,820
El cuadro verde me dice que la palabra secreta

25
00:01:21,820 --> 00:01:22,820
tiene una A y está en la tercera posición.

26
00:01:22,820 --> 00:01:24,300
Y luego no hay N y no hay E.

27
00:01:24,300 --> 00:01:27,420
Así que déjame entrar y decirle esa información al robot Wurdle.

28
00:01:27,420 --> 00:01:31,500
Empezamos con grúa, obtuvimos gris, amarillo, verde, gris, gris.

29
00:01:31,500 --> 00:01:35,460
No te preocupes por todos los datos que están mostrando ahora mismo, te lo explicaré a su debido tiempo.

30
00:01:35,460 --> 00:01:39,700
Pero su principal sugerencia para nuestra segunda elección es un truco.

31
00:01:39,700 --> 00:01:43,500
Y tu suposición tiene que ser una palabra real de cinco letras, pero

32
00:01:43,500 --> 00:01:45,700
como verás, es bastante liberal con lo que realmente te permitirá adivinar.

33
00:01:45,700 --> 00:01:48,860
En este caso, intentamos stick.

34
00:01:48,860 --> 00:01:50,260
Y bueno, la cosa pinta bastante bien.

35
00:01:50,260 --> 00:01:54,580
Pulsamos la S y la H, así conocemos las tres primeras letras, sabemos que hay una R.

36
00:01:54,740 --> 00:01:59,740
Y entonces será como SHA algo R, o SHA R algo.

37
00:01:59,740 --> 00:02:03,200
Y parece que el robot Wurdle sabe que

38
00:02:03,200 --> 00:02:05,220
solo hay dos posibilidades: fragmentar o filoso.

39
00:02:05,220 --> 00:02:08,620
Eso es una especie de volatilidad entre ellos en este momento, así

40
00:02:08,620 --> 00:02:11,260
que supongo que probablemente solo porque es alfabético va con el fragmento.

41
00:02:11,260 --> 00:02:13,000
Qué hurra, es la respuesta real.

42
00:02:13,000 --> 00:02:14,660
Entonces lo tenemos en tres.

43
00:02:14,660 --> 00:02:17,740
Si te preguntas si eso es bueno, la forma en que escuché a

44
00:02:17,740 --> 00:02:20,820
una persona decir que con Wurdle cuatro es par y tres es birdie.

45
00:02:20,820 --> 00:02:22,960
Lo cual creo que es una analogía bastante adecuada.

46
00:02:22,960 --> 00:02:27,560
Tienes que ser constante en tu juego para conseguir cuatro, pero ciertamente no es una locura.

47
00:02:27,560 --> 00:02:30,000
Pero cuando lo obtienes en tres, se siente genial.

48
00:02:30,000 --> 00:02:33,800
Entonces, si estás dispuesto a hacerlo, lo que me gustaría hacer aquí es simplemente hablar

49
00:02:33,800 --> 00:02:36,600
sobre mi proceso de pensamiento desde el principio sobre cómo abordo el robot Wurdle.

50
00:02:36,600 --> 00:02:39,800
Y como dije, en realidad es una excusa para una lección de teoría de la información.

51
00:02:39,800 --> 00:02:43,160
El objetivo principal es explicar qué es información y qué es entropía.

52
00:02:48,560 --> 00:02:52,080
Lo primero que pensé al abordar esto fue observar las

53
00:02:52,080 --> 00:02:53,560
frecuencias relativas de diferentes letras en el idioma inglés.

54
00:02:53,560 --> 00:02:57,800
Entonces pensé, bueno, ¿hay una suposición inicial o un par de

55
00:02:57,800 --> 00:02:59,960
suposiciones iniciales que acierten muchas de estas letras más frecuentes?

56
00:02:59,960 --> 00:03:03,780
Y una que me gustaba mucho era hacer otras seguidas de uñas.

57
00:03:03,780 --> 00:03:06,980
La idea es que si tocas una letra, ya sabes, obtienes

58
00:03:06,980 --> 00:03:07,980
un verde o un amarillo, eso siempre se siente bien.

59
00:03:07,980 --> 00:03:09,460
Parece que estás recibiendo información.

60
00:03:09,460 --> 00:03:13,140
Pero en estos casos, incluso si no aciertas y siempre aparecen

61
00:03:13,140 --> 00:03:16,640
grises, eso te da mucha información ya que es bastante raro

62
00:03:16,640 --> 00:03:17,640
encontrar una palabra que no tenga ninguna de estas letras.

63
00:03:17,640 --> 00:03:21,840
Pero aun así, esto no parece súper sistemático, porque, por

64
00:03:21,840 --> 00:03:23,520
ejemplo, no tiene en cuenta el orden de las letras.

65
00:03:23,520 --> 00:03:26,080
¿Por qué escribir uñas cuando puedo escribir caracol?

66
00:03:26,080 --> 00:03:27,720
¿Es mejor tener esa S al final?

67
00:03:27,720 --> 00:03:28,720
No estoy realmente seguro.

68
00:03:28,720 --> 00:03:33,500
Ahora, un amigo mío dijo que le gustaba comenzar con la palabra cansado, lo que me

69
00:03:33,500 --> 00:03:37,160
sorprendió un poco porque tiene algunas letras poco comunes como la W y la Y.

70
00:03:37,160 --> 00:03:39,400
Pero quién sabe, tal vez ese sea un mejor comienzo.

71
00:03:39,400 --> 00:03:43,920
¿Existe algún tipo de puntuación cuantitativa que podamos dar

72
00:03:43,920 --> 00:03:44,920
para juzgar la calidad de una posible suposición?

73
00:03:44,920 --> 00:03:48,640
Ahora, para preparar la forma en que vamos a clasificar las posibles conjeturas, retrocedamos

74
00:03:48,640 --> 00:03:51,800
y agreguemos un poco de claridad sobre cómo está configurado exactamente el juego.

75
00:03:51,800 --> 00:03:55,880
Entonces, hay una lista de palabras que le permitirá ingresar y

76
00:03:55,880 --> 00:03:57,920
que se consideran conjeturas válidas y que tiene aproximadamente 13,000 palabras.

77
00:03:57,920 --> 00:04:01,560
Pero cuando lo miras, hay muchas cosas realmente poco comunes, cosas como una cabeza o Ali

78
00:04:01,560 --> 00:04:07,040
y ARG, el tipo de palabras que provocan discusiones familiares en un juego de Scrabble.

79
00:04:07,040 --> 00:04:10,600
Pero la sensación del juego es que la respuesta siempre será una palabra bastante común.

80
00:04:10,600 --> 00:04:16,080
Y de hecho, hay otra lista de alrededor de 2300 palabras que son las posibles respuestas.

81
00:04:16,080 --> 00:04:20,320
Y esta es una lista seleccionada por humanos, creo que específicamente por

82
00:04:20,320 --> 00:04:21,800
la novia del creador del juego, lo cual es bastante divertido.

83
00:04:21,800 --> 00:04:25,560
Pero lo que me gustaría hacer, nuestro desafío para este proyecto es ver si

84
00:04:25,560 --> 00:04:30,720
podemos escribir un programa resolviendo Wordle que no incorpore conocimientos previos sobre esta lista.

85
00:04:30,720 --> 00:04:34,560
Por un lado, hay muchas palabras de cinco letras

86
00:04:34,560 --> 00:04:35,560
bastante comunes que no encontrarás en esa lista.

87
00:04:35,560 --> 00:04:38,360
Por lo tanto, sería mejor escribir un programa que sea un poco más resistente y

88
00:04:38,360 --> 00:04:41,960
que pueda jugar con Wordle contra cualquiera, no solo contra el sitio web oficial.

89
00:04:41,960 --> 00:04:45,900
Y también la razón por la que sabemos cuál es esta lista

90
00:04:45,900 --> 00:04:47,440
de posibles respuestas es porque es visible en el código fuente.

91
00:04:47,440 --> 00:04:51,620
Pero la forma en que es visible en el código fuente es en

92
00:04:51,620 --> 00:04:52,840
el orden específico en el que aparecen las respuestas día a día.

93
00:04:52,840 --> 00:04:56,400
Así que siempre puedes buscar cuál será la respuesta de mañana.

94
00:04:56,400 --> 00:04:59,140
Claramente, en cierto sentido usar la lista es hacer trampa.

95
00:04:59,140 --> 00:05:02,900
Y lo que hace que el rompecabezas sea más interesante y una lección de teoría de

96
00:05:02,900 --> 00:05:07,640
la información más rica es utilizar algunos datos más universales, como las frecuencias relativas de

97
00:05:07,640 --> 00:05:11,640
las palabras en general, para capturar esta intuición de tener preferencia por palabras más comunes.

98
00:05:11,640 --> 00:05:16,560
Entonces, de estas 13.000 posibilidades, ¿cómo deberíamos elegir la suposición inicial?

99
00:05:16,560 --> 00:05:19,960
Por ejemplo, si mi amigo me propone cansancio, ¿cómo debemos analizar su calidad?

100
00:05:19,960 --> 00:05:25,040
Bueno, la razón por la que dijo que le gusta esa improbable W es que

101
00:05:25,040 --> 00:05:27,880
le gusta la naturaleza remota de lo bien que se siente si aciertas esa W.

102
00:05:27,880 --> 00:05:31,400
Por ejemplo, si el primer patrón revelado fue algo como este, entonces resulta que

103
00:05:31,400 --> 00:05:36,080
solo hay 58 palabras en este léxico gigante que coinciden con ese patrón.

104
00:05:36,080 --> 00:05:38,900
Entonces esa es una gran reducción de 13.000.

105
00:05:38,900 --> 00:05:43,320
Pero la otra cara de la moneda, por supuesto, es que es muy poco común obtener un patrón como este.

106
00:05:43,360 --> 00:05:47,600
Específicamente, si cada palabra tuviera la misma probabilidad de ser la respuesta, la

107
00:05:47,600 --> 00:05:51,680
probabilidad de encontrar este patrón sería 58 dividido por alrededor de 13.000.

108
00:05:51,680 --> 00:05:53,880
Por supuesto, no es igualmente probable que sean respuestas.

109
00:05:53,880 --> 00:05:56,680
La mayoría de ellas son palabras muy oscuras e incluso cuestionables.

110
00:05:56,680 --> 00:05:59,560
Pero al menos para nuestra primera aproximación a todo esto, supongamos que

111
00:05:59,560 --> 00:06:02,040
todas son igualmente probables y luego refinemos eso un poco más adelante.

112
00:06:02,040 --> 00:06:07,360
La cuestión es que, por su propia naturaleza, es poco probable que se produzca un patrón con mucha información.

113
00:06:07,360 --> 00:06:11,320
De hecho, lo que significa ser informativo es que es poco probable.

114
00:06:11,920 --> 00:06:16,720
Un patrón mucho más probable de ver con esta apertura sería

115
00:06:16,720 --> 00:06:18,360
algo como esto, donde por supuesto no hay una W.

116
00:06:18,360 --> 00:06:22,080
Tal vez haya una E, y tal vez no haya una A, no haya una R, no haya una Y.

117
00:06:22,080 --> 00:06:24,640
En este caso, hay 1400 coincidencias posibles.

118
00:06:24,640 --> 00:06:29,600
Si todas fueran igualmente probables, resulta que hay una probabilidad de

119
00:06:29,600 --> 00:06:30,680
alrededor del 11% de que este sea el patrón que verías.

120
00:06:30,680 --> 00:06:34,320
De modo que los resultados más probables son también los menos informativos.

121
00:06:34,320 --> 00:06:38,440
Para obtener una visión más global, permítame mostrarle la distribución completa

122
00:06:38,440 --> 00:06:42,000
de probabilidades en todos los diferentes patrones que pueda ver.

123
00:06:42,000 --> 00:06:46,000
Entonces, cada barra que estás viendo corresponde a un posible patrón de colores

124
00:06:46,000 --> 00:06:50,500
que podría revelarse, de los cuales hay de 3 a 5 posibilidades, y

125
00:06:50,500 --> 00:06:52,960
están organizados de izquierda a derecha, del más común al menos común.

126
00:06:52,960 --> 00:06:56,200
Entonces, la posibilidad más común aquí es que obtengas todos los grises.

127
00:06:56,200 --> 00:06:58,800
Esto sucede aproximadamente el 14% del tiempo.

128
00:06:58,800 --> 00:07:02,040
Y lo que esperas cuando haces una suposición es terminar en algún

129
00:07:02,040 --> 00:07:06,360
lugar de esta larga cola, como aquí donde solo hay 18 posibilidades

130
00:07:06,360 --> 00:07:09,920
para lo que coincide con este patrón que evidentemente se ve así.

131
00:07:09,920 --> 00:07:14,080
O si nos aventuramos un poco más hacia la izquierda, ya sabes, tal vez lleguemos hasta aquí.

132
00:07:14,080 --> 00:07:16,560
Bien, aquí tienes un buen rompecabezas.

133
00:07:16,560 --> 00:07:20,600
¿Cuáles son las tres palabras en inglés que comienzan con W,

134
00:07:20,600 --> 00:07:22,040
terminan con Y y tienen una R en alguna parte?

135
00:07:22,040 --> 00:07:27,560
Resulta que las respuestas son, veamos, prolijas, llenas de gusanos e irónicas.

136
00:07:27,560 --> 00:07:32,720
Entonces, para juzgar qué tan buena es esta palabra en general, queremos algún tipo

137
00:07:32,720 --> 00:07:35,720
de medida de la cantidad esperada de información que obtendrá de esta distribución.

138
00:07:36,360 --> 00:07:41,080
Si analizamos cada patrón y multiplicamos su probabilidad de ocurrir por algo que

139
00:07:41,080 --> 00:07:46,000
mida qué tan informativo es, eso tal vez pueda darnos una puntuación objetiva.

140
00:07:46,000 --> 00:07:50,280
Ahora tu primer instinto sobre lo que debería ser ese algo podría ser el número de coincidencias.

141
00:07:50,280 --> 00:07:52,960
Quieres un número promedio más bajo de coincidencias.

142
00:07:52,960 --> 00:07:57,400
Pero en lugar de eso me gustaría usar una medida más universal que a menudo atribuimos

143
00:07:57,400 --> 00:08:01,040
a la información, y una que será más flexible una vez que tengamos asignada una probabilidad

144
00:08:01,040 --> 00:08:04,320
diferente a cada una de estas 13.000 palabras sobre si son o no la respuesta.

145
00:08:10,600 --> 00:08:14,760
La unidad de información estándar es el bit, que tiene una fórmula

146
00:08:14,760 --> 00:08:17,800
un poco divertida, pero es realmente intuitiva si solo miramos ejemplos.

147
00:08:17,800 --> 00:08:21,880
Si tienes una observación que reduce a la mitad tu

148
00:08:21,880 --> 00:08:24,200
espacio de posibilidades, decimos que tiene un bit de información.

149
00:08:24,200 --> 00:08:27,680
En nuestro ejemplo, el espacio de posibilidades son todas las palabras posibles, y resulta que aproximadamente la mitad

150
00:08:27,760 --> 00:08:31,560
de las palabras de cinco letras tienen una S, un poco menos que eso, pero aproximadamente la mitad.

151
00:08:31,560 --> 00:08:35,200
Entonces esa observación les daría un poco de información.

152
00:08:35,200 --> 00:08:39,640
Si en cambio un hecho nuevo reduce ese espacio de posibilidades en

153
00:08:39,640 --> 00:08:42,000
un factor de cuatro, decimos que tiene dos bits de información.

154
00:08:42,000 --> 00:08:45,120
Por ejemplo, resulta que aproximadamente una cuarta parte de estas palabras tienen una T.

155
00:08:45,120 --> 00:08:49,720
Si la observación corta ese espacio por un factor de ocho,

156
00:08:49,720 --> 00:08:50,920
decimos que son tres bits de información, y así sucesivamente.

157
00:08:50,920 --> 00:08:55,000
Cuatro bits lo cortan en un 16, cinco bits lo cortan en un 32.

158
00:08:55,000 --> 00:09:00,160
Así que ahora quizás quieras hacer una pausa y preguntarte: ¿cuál es la fórmula para

159
00:09:00,160 --> 00:09:04,520
obtener información sobre el número de bits en términos de probabilidad de que ocurra?

160
00:09:04,520 --> 00:09:07,920
Lo que estamos diciendo aquí es que cuando se le suma la mitad al número de bits,

161
00:09:07,920 --> 00:09:11,680
eso es lo mismo que la probabilidad, que es lo mismo que decir que dos elevado a

162
00:09:11,680 --> 00:09:16,200
la potencia del número de bits es uno sobre la probabilidad, lo cual se reordena además para

163
00:09:16,200 --> 00:09:19,680
decir que la información es el logaritmo en base dos de uno dividido por la probabilidad.

164
00:09:19,680 --> 00:09:23,200
Y a veces se ve esto con un reordenamiento más, donde la

165
00:09:23,200 --> 00:09:25,680
información es el logaritmo negativo en base dos de la probabilidad.

166
00:09:25,680 --> 00:09:29,120
Expresado así, puede parecer un poco extraño para los no iniciados,

167
00:09:29,120 --> 00:09:33,400
pero en realidad es sólo la idea muy intuitiva de

168
00:09:33,400 --> 00:09:35,120
preguntar cuántas veces has reducido tus posibilidades a la mitad.

169
00:09:35,120 --> 00:09:37,840
Ahora, si te lo estás preguntando, ya sabes, pensé que solo estábamos jugando

170
00:09:37,840 --> 00:09:39,920
un divertido juego de palabras, ¿por qué los logaritmos entran en escena?

171
00:09:39,920 --> 00:09:43,920
Una de las razones por las que esta es una unidad más agradable es que es mucho más

172
00:09:43,920 --> 00:09:48,120
fácil hablar de eventos muy improbables, mucho más fácil decir que una observación tiene 20 bits de

173
00:09:48,120 --> 00:09:53,480
información que decir que la probabilidad de que ocurra tal o cual cosa es 0. 0000095.

174
00:09:53,480 --> 00:09:57,360
Pero una razón más sustancial por la que esta expresión logarítmica resultó ser una adición muy

175
00:09:57,360 --> 00:10:02,000
útil a la teoría de la probabilidad es la forma en que se suma la información.

176
00:10:02,000 --> 00:10:05,560
Por ejemplo, si una observación le proporciona dos bits de información, lo que reduce

177
00:10:05,560 --> 00:10:10,120
su espacio en cuatro, y luego una segunda observación, como su segunda suposición en

178
00:10:10,120 --> 00:10:14,480
Wordle, le proporciona otros tres bits de información, lo que lo reduce aún más

179
00:10:14,480 --> 00:10:17,360
en otro factor de ocho, la dos juntos te dan cinco bits de información.

180
00:10:17,360 --> 00:10:21,200
De la misma manera que a las probabilidades les gusta multiplicarse, a la información le gusta sumar.

181
00:10:21,200 --> 00:10:24,920
Entonces, tan pronto como estamos en el ámbito de algo así como un valor esperado, donde

182
00:10:24,920 --> 00:10:28,660
sumamos un montón de números, los registros hacen que sea mucho más agradable tratar con ellos.

183
00:10:28,660 --> 00:10:32,600
Volvamos a nuestra distribución de Weary y agreguemos otro pequeño rastreador

184
00:10:32,600 --> 00:10:35,560
aquí, que nos muestra cuánta información hay para cada patrón.

185
00:10:35,560 --> 00:10:38,760
Lo principal que quiero que note es que cuanto mayor es la probabilidad a medida que

186
00:10:38,760 --> 00:10:43,500
llegamos a esos patrones más probables, menor es la información y menos bits se ganan.

187
00:10:43,500 --> 00:10:47,360
La forma en que medimos la calidad de esta suposición será tomando el

188
00:10:47,360 --> 00:10:51,620
valor esperado de esta información, donde analizamos cada patrón, decimos qué tan

189
00:10:51,620 --> 00:10:54,940
probable es y luego lo multiplicamos por cuántos bits de información obtenemos.

190
00:10:54,940 --> 00:10:58,480
Y en el ejemplo de Weary, resulta ser 4. 9 bits.

191
00:10:58,480 --> 00:11:02,800
Entonces, en promedio, la información que obtienes de esta suposición inicial es tan

192
00:11:02,800 --> 00:11:05,660
buena como cortar tu espacio de posibilidades a la mitad unas cinco veces.

193
00:11:05,660 --> 00:11:10,260
Por el contrario, un ejemplo de una suposición con un

194
00:11:10,260 --> 00:11:13,220
valor de información esperado más alto sería algo como Slate.

195
00:11:13,220 --> 00:11:16,180
En este caso notarás que la distribución luce mucho más plana.

196
00:11:16,180 --> 00:11:20,780
En particular, la aparición más probable de todos los grises solo tiene alrededor de un 6% de

197
00:11:20,780 --> 00:11:25,940
posibilidades de ocurrir, por lo que, como mínimo, evidentemente obtendrás 3. 9 bits de información.

198
00:11:25,940 --> 00:11:29,140
Pero eso es un mínimo, normalmente obtendrás algo mejor que eso.

199
00:11:29,140 --> 00:11:33,380
Y resulta que cuando haces cálculos en este caso y sumas todos

200
00:11:33,380 --> 00:11:36,420
los términos relevantes, la información promedio es de aproximadamente 5. 8.

201
00:11:36,420 --> 00:11:42,140
Entonces, a diferencia de Weary, su espacio de posibilidades será

202
00:11:42,140 --> 00:11:43,940
aproximadamente la mitad después de esta primera suposición, en promedio.

203
00:11:43,940 --> 00:11:49,540
De hecho, hay una historia divertida sobre el nombre de este valor esperado de cantidad de información.

204
00:11:49,540 --> 00:11:52,580
La teoría de la información fue desarrollada por Claude Shannon, que trabajaba en los Laboratorios Bell

205
00:11:52,580 --> 00:11:57,620
en la década de 1940, pero estaba hablando de algunas de sus ideas aún por

206
00:11:57,620 --> 00:12:01,500
publicar con John von Neumann, que era este gigante intelectual de la época, muy destacado.

207
00:12:01,500 --> 00:12:04,180
en matemáticas y física y los inicios de lo que se estaba convirtiendo en informática.

208
00:12:04,180 --> 00:12:07,260
Y cuando mencionó que realmente no tenía un buen nombre para este

209
00:12:07,260 --> 00:12:12,540
valor esperado de la cantidad de información, supuestamente von Neumann dijo, según

210
00:12:12,540 --> 00:12:14,720
cuenta la historia, bueno, deberías llamarlo entropía, y por dos razones.

211
00:12:14,720 --> 00:12:18,400
En primer lugar, su función de incertidumbre se ha utilizado en mecánica estadística con ese nombre,

212
00:12:18,400 --> 00:12:23,100
por lo que ya tiene un nombre, y en segundo lugar, y más importante, nadie sabe

213
00:12:23,100 --> 00:12:26,940
qué es realmente la entropía, por lo que en un debate siempre tener la ventaja.

214
00:12:26,940 --> 00:12:31,420
Entonces, si el nombre parece un poco misterioso, y si hay

215
00:12:31,420 --> 00:12:33,420
que creer en esta historia, es más o menos intencionalmente.

216
00:12:33,420 --> 00:12:36,740
Además, si te preguntas acerca de su relación con toda esa segunda ley

217
00:12:36,740 --> 00:12:40,820
de la termodinámica de la física, definitivamente hay una conexión, pero en

218
00:12:40,820 --> 00:12:44,780
sus orígenes Shannon solo estaba tratando con la teoría de la probabilidad pura,

219
00:12:44,780 --> 00:12:49,340
y para nuestros propósitos aquí, cuando uso la palabra entropía, solo quiero

220
00:12:49,340 --> 00:12:50,820
que piense en el valor de información esperado de una suposición particular.

221
00:12:50,820 --> 00:12:54,380
Puedes pensar que la entropía mide dos cosas simultáneamente.

222
00:12:54,380 --> 00:12:57,420
El primero es qué tan plana es la distribución.

223
00:12:57,420 --> 00:13:01,700
Cuanto más cercana a la uniformidad sea una distribución, mayor será la entropía.

224
00:13:01,700 --> 00:13:06,340
En nuestro caso, donde hay de 3 a 5 patrones en total, para una distribución uniforme, observar cualquiera

225
00:13:06,340 --> 00:13:11,340
de ellos tendría un registro de información en base 2 de 3 a 5, que resulta ser

226
00:13:11,340 --> 00:13:17,860
7. 92, por lo que ese es el máximo absoluto que podrías tener para esta entropía.

227
00:13:17,860 --> 00:13:21,900
Pero la entropía también es una especie de

228
00:13:21,900 --> 00:13:22,900
medida de cuántas posibilidades hay en primer lugar.

229
00:13:22,900 --> 00:13:26,980
Por ejemplo, si tienes una palabra en la que sólo hay 16 patrones posibles y cada

230
00:13:26,980 --> 00:13:32,760
uno de ellos es igualmente probable, esta entropía, esta información esperada, sería de 4 bits.

231
00:13:32,760 --> 00:13:36,880
Pero si tienes otra palabra donde hay 64 patrones posibles que podrían surgir,

232
00:13:36,880 --> 00:13:41,000
y todos son igualmente probables, entonces la entropía resultaría ser de 6 bits.

233
00:13:41,000 --> 00:13:45,800
Entonces, si ves alguna distribución en la naturaleza que tiene una entropía de 6

234
00:13:45,800 --> 00:13:50,000
bits, es como si estuviera diciendo que hay tanta variación e incertidumbre en lo

235
00:13:50,000 --> 00:13:54,400
que está a punto de suceder como si hubiera 64 resultados igualmente probables.

236
00:13:54,400 --> 00:13:58,360
Para mi primera pasada por el Wurtelebot, básicamente le pedí que hiciera esto.

237
00:13:58,360 --> 00:14:03,560
Revisa todas las conjeturas posibles que puedas tener, las 13.000 palabras, calcula la entropía

238
00:14:03,560 --> 00:14:08,580
de cada una, o más específicamente, la entropía de la distribución en todos los

239
00:14:08,580 --> 00:14:13,040
patrones que puedas ver, para cada una, y elige la más alta, ya que

240
00:14:13,040 --> 00:14:17,200
es el que probablemente reducirá su espacio de posibilidades tanto como sea posible.

241
00:14:17,200 --> 00:14:20,120
Y aunque aquí solo he estado hablando de la

242
00:14:20,120 --> 00:14:21,680
primera suposición, ocurre lo mismo con las siguientes suposiciones.

243
00:14:21,680 --> 00:14:25,100
Por ejemplo, después de ver algún patrón en esa primera suposición, que lo restringiría a

244
00:14:25,100 --> 00:14:29,300
un número menor de palabras posibles en función de lo que coincide con eso,

245
00:14:29,300 --> 00:14:32,300
simplemente juega el mismo juego con respecto a ese conjunto más pequeño de palabras.

246
00:14:32,300 --> 00:14:36,500
Para una segunda suposición propuesta, observamos la distribución de todos los patrones

247
00:14:36,500 --> 00:14:41,540
que podrían ocurrir a partir de ese conjunto más restringido de palabras,

248
00:14:41,540 --> 00:14:45,480
buscamos entre las 13.000 posibilidades y encontramos la que maximiza esa entropía.

249
00:14:45,480 --> 00:14:48,980
Para mostrarles cómo funciona esto en acción, permítanme mostrarles una pequeña variante de Wurtele que

250
00:14:48,980 --> 00:14:54,060
escribí y que muestra los aspectos más destacados de este análisis en los márgenes.

251
00:14:54,460 --> 00:14:57,820
Después de hacer todos los cálculos de entropía, aquí a la

252
00:14:57,820 --> 00:15:00,340
derecha nos muestra cuáles tienen la información esperada más alta.

253
00:15:00,340 --> 00:15:04,940
Resulta que la respuesta principal, al menos por el momento, lo refinaremos más

254
00:15:04,940 --> 00:15:11,140
adelante, es Tares, que significa, por supuesto, arveja, la arveja más común.

255
00:15:11,140 --> 00:15:14,180
Cada vez que hacemos una suposición aquí, donde tal vez ignoro sus

256
00:15:14,180 --> 00:15:19,220
recomendaciones y elijo slate, porque me gusta slate, podemos ver cuánta

257
00:15:19,220 --> 00:15:23,300
información esperada tenía, pero luego, a la derecha de la palabra aquí,

258
00:15:23,340 --> 00:15:24,980
nos muestra cuánta información real que obtuvimos, dado este patrón particular.

259
00:15:24,980 --> 00:15:28,660
Así que aquí parece que tuvimos un poco de mala suerte, se esperaba que obtuviéramos 5. 8, pero

260
00:15:28,660 --> 00:15:30,660
obtuvimos algo con menos que eso.

261
00:15:30,660 --> 00:15:34,020
Y luego, en el lado izquierdo, aquí nos muestra todas las

262
00:15:34,020 --> 00:15:35,860
diferentes palabras posibles según el lugar donde nos encontramos ahora.

263
00:15:35,860 --> 00:15:39,820
Las barras azules nos dicen qué tan probable cree que es cada palabra, por lo que por el

264
00:15:39,820 --> 00:15:44,140
momento suponemos que cada palabra tiene la misma probabilidad de ocurrir, pero lo refinaremos en un momento.

265
00:15:44,140 --> 00:15:48,580
Y luego esta medida de incertidumbre nos dice la entropía de esta distribución

266
00:15:48,580 --> 00:15:53,220
entre las palabras posibles, que ahora mismo, debido a que es una distribución

267
00:15:53,300 --> 00:15:55,940
uniforme, es solo una forma innecesariamente complicada de contar el número de posibilidades.

268
00:15:55,940 --> 00:16:01,700
Por ejemplo, si tuviéramos que elevar 2 a la potencia de 13. 66, eso debería

269
00:16:01,700 --> 00:16:02,700
rondar las 13.000 posibilidades.

270
00:16:02,700 --> 00:16:06,780
Estoy un poco fuera de lugar aquí, pero sólo porque no muestro todos los decimales.

271
00:16:06,780 --> 00:16:10,260
Por el momento, esto puede parecer redundante y complicar demasiado las cosas,

272
00:16:10,260 --> 00:16:12,780
pero verá por qué es útil tener ambos números en un minuto.

273
00:16:12,780 --> 00:16:16,780
Así que aquí parece que sugiere que la entropía más alta para

274
00:16:16,780 --> 00:16:19,700
nuestra segunda suposición es Ramen, que nuevamente no parece una palabra.

275
00:16:19,700 --> 00:16:25,660
Entonces, para tener autoridad moral aquí, seguiré adelante y escribiré Rains.

276
00:16:25,660 --> 00:16:27,540
Y nuevamente parece que tuvimos un poco de mala suerte.

277
00:16:27,540 --> 00:16:32,100
Esperábamos 4. 3 bits y solo tenemos 3. 39 bits de información.

278
00:16:32,100 --> 00:16:35,060
Eso nos lleva a 55 posibilidades.

279
00:16:35,060 --> 00:16:38,860
Y aquí tal vez me quede con lo que sugiere,

280
00:16:38,860 --> 00:16:40,200
que es combo, sea lo que sea que eso signifique.

281
00:16:40,200 --> 00:16:43,300
Y está bien, esta es en realidad una buena oportunidad para resolver un rompecabezas.

282
00:16:43,300 --> 00:16:47,020
Nos dice que este patrón nos da 4. 7 bits de información.

283
00:16:47,020 --> 00:16:52,400
Pero a la izquierda, antes de que veamos ese patrón, había 5. 78 bits de incertidumbre.

284
00:16:52,400 --> 00:16:56,860
Entonces, a modo de prueba, ¿qué significa eso sobre el número de posibilidades restantes?

285
00:16:56,860 --> 00:17:02,280
Bueno, significa que estamos reducidos a un poco de incertidumbre, que

286
00:17:02,280 --> 00:17:04,700
es lo mismo que decir que hay dos respuestas posibles.

287
00:17:04,700 --> 00:17:06,520
Es una elección 50-50.

288
00:17:06,520 --> 00:17:09,860
Y a partir de aquí, porque tú y yo sabemos qué

289
00:17:09,860 --> 00:17:11,220
palabras son más comunes, sabemos que la respuesta debería ser abismo.

290
00:17:11,220 --> 00:17:13,540
Pero tal como está escrito ahora, el programa no lo sabe.

291
00:17:13,540 --> 00:17:17,560
Así que sigue adelante, tratando de obtener tanta información como puede,

292
00:17:17,560 --> 00:17:20,360
hasta que sólo queda una posibilidad, y luego la adivina.

293
00:17:20,360 --> 00:17:22,700
Obviamente necesitamos una mejor estrategia para el final del juego.

294
00:17:22,700 --> 00:17:26,540
Pero digamos que llamamos a esta versión uno de nuestro solucionador

295
00:17:26,540 --> 00:17:30,740
de palabras y luego ejecutamos algunas simulaciones para ver cómo funciona.

296
00:17:30,740 --> 00:17:34,240
Entonces, la forma en que esto funciona es jugando todos los juegos de palabras posibles.

297
00:17:34,240 --> 00:17:38,780
Está repasando todas esas 2315 palabras que son las respuestas reales.

298
00:17:38,780 --> 00:17:41,340
Básicamente se trata de utilizarlo como un conjunto de pruebas.

299
00:17:41,340 --> 00:17:45,820
Y con este método ingenuo de no considerar qué tan común es una palabra y simplemente tratar

300
00:17:45,820 --> 00:17:50,480
de maximizar la información en cada paso del camino, hasta llegar a una y sólo una opción.

301
00:17:50,480 --> 00:17:55,100
Al final de la simulación, la puntuación media resulta ser de aproximadamente 4. 124.

302
00:17:55,100 --> 00:17:59,780
Lo cual no está mal, para ser honesto, esperaba hacerlo peor.

303
00:17:59,780 --> 00:18:03,040
Pero la gente que juega wordle te dirá que normalmente lo consiguen en 4.

304
00:18:03,040 --> 00:18:05,260
El verdadero desafío es conseguir tantos en 3 como puedas.

305
00:18:05,260 --> 00:18:08,920
Es un salto bastante grande entre la puntuación de 4 y la puntuación de 3.

306
00:18:08,920 --> 00:18:13,300
Lo obvio aquí es incorporar de alguna manera si una

307
00:18:13,300 --> 00:18:23,160
palabra es común o no, y cómo lo hacemos exactamente.

308
00:18:23,160 --> 00:18:26,860
La forma en que lo acerqué es obtener una lista de

309
00:18:26,860 --> 00:18:28,560
las frecuencias relativas de todas las palabras en el idioma inglés.

310
00:18:28,560 --> 00:18:32,560
Y acabo de utilizar la función de datos de frecuencia de palabras de Mathematica, que a

311
00:18:32,560 --> 00:18:35,520
su vez se extrae del conjunto de datos públicos Ngram en inglés de Google Books.

312
00:18:35,520 --> 00:18:38,680
Y es divertido verlo, por ejemplo, si lo clasificamos

313
00:18:38,680 --> 00:18:40,120
desde las palabras más comunes hasta las menos comunes.

314
00:18:40,120 --> 00:18:43,740
Evidentemente estas son las palabras de cinco letras más comunes en el idioma inglés.

315
00:18:43,740 --> 00:18:46,480
O mejor dicho, este es el octavo más común.

316
00:18:46,480 --> 00:18:49,440
Primero es cuál, después está ahí y ahí.

317
00:18:49,440 --> 00:18:53,020
Primero en sí no es primero, sino noveno, y tiene sentido que

318
00:18:53,020 --> 00:18:57,840
estas otras palabras puedan aparecer con más frecuencia, donde las que siguen

319
00:18:57,840 --> 00:18:59,000
a primero son después, dónde, y que son un poco menos comunes.

320
00:18:59,000 --> 00:19:04,400
Ahora bien, al utilizar estos datos para modelar la probabilidad de que cada una de

321
00:19:04,400 --> 00:19:06,760
estas palabras sea la respuesta final, no debería ser sólo proporcional a la frecuencia.

322
00:19:07,020 --> 00:19:12,560
Por ejemplo, a la que se le da una puntuación de 0. 002 en este conjunto de datos, mientras

323
00:19:12,560 --> 00:19:15,200
que la palabra trenza es, en cierto sentido, aproximadamente 1000 veces menos probable.

324
00:19:15,200 --> 00:19:19,400
Pero ambas son palabras bastante comunes que casi con seguridad vale la pena considerarlas.

325
00:19:19,400 --> 00:19:21,900
Por eso queremos más un límite binario.

326
00:19:21,900 --> 00:19:26,520
La forma en que lo hice es imaginar tomar toda esta lista ordenada de

327
00:19:26,520 --> 00:19:31,060
palabras, y luego organizarla en un eje x, y luego aplicar la función sigmoidea,

328
00:19:31,060 --> 00:19:35,540
que es la forma estándar de tener una función cuya salida es básicamente binaria,

329
00:19:35,540 --> 00:19:38,500
es 0 o 1, pero hay un suavizado intermedio para esa región de incertidumbre.

330
00:19:38,500 --> 00:19:43,900
Básicamente, la probabilidad que estoy asignando a cada palabra de estar en la lista final será

331
00:19:43,900 --> 00:19:49,540
el valor de la función sigmoidea arriba dondequiera que se encuentre en el eje x.

332
00:19:49,540 --> 00:19:53,940
Ahora bien, obviamente, esto depende de algunos parámetros, por ejemplo, qué tan ancho es el espacio

333
00:19:53,940 --> 00:19:59,660
en el eje x que ocupan esas palabras determina qué tan gradual o abruptamente bajamos

334
00:19:59,660 --> 00:20:03,000
de 1 a 0, y dónde las ubicamos de izquierda a derecha determina el límite.

335
00:20:03,160 --> 00:20:07,340
Para ser honesto, la forma en que hice esto fue simplemente lamerme el dedo y pegarlo al viento.

336
00:20:07,340 --> 00:20:10,800
Revisé la lista ordenada y traté de encontrar una ventana donde, cuando

337
00:20:10,800 --> 00:20:15,280
la miré, pensé que era más probable que aproximadamente la mitad de

338
00:20:15,280 --> 00:20:17,680
estas palabras fueran la respuesta final, y la usé como límite.

339
00:20:17,680 --> 00:20:21,840
Una vez que tenemos una distribución como esta entre las palabras, nos da otra

340
00:20:21,840 --> 00:20:24,460
situación en la que la entropía se convierte en una medida realmente útil.

341
00:20:24,460 --> 00:20:28,480
Por ejemplo, digamos que estamos jugando y comenzamos con mis viejos

342
00:20:28,480 --> 00:20:32,480
abridores, que eran plumas y clavos, y terminamos con una

343
00:20:32,480 --> 00:20:33,760
situación en la que hay cuatro palabras posibles que coinciden.

344
00:20:33,760 --> 00:20:36,440
Y digamos que los consideramos todos igualmente probables.

345
00:20:36,440 --> 00:20:40,000
Déjame preguntarte, ¿cuál es la entropía de esta distribución?

346
00:20:40,000 --> 00:20:45,920
Bueno, la información asociada a cada una de estas posibilidades va a ser el logaritmo en

347
00:20:45,920 --> 00:20:50,800
base 2 de 4, ya que cada una es 1 y 4, y eso es 2.

348
00:20:50,800 --> 00:20:52,780
Dos bits de información, cuatro posibilidades.

349
00:20:52,780 --> 00:20:54,360
Todo muy bien y bueno.

350
00:20:54,360 --> 00:20:58,320
Pero ¿y si te dijera que en realidad hay más de cuatro coincidencias?

351
00:20:58,320 --> 00:21:02,600
En realidad, cuando miramos la lista completa de palabras, hay 16 palabras que coinciden.

352
00:21:02,600 --> 00:21:07,260
Pero supongamos que nuestro modelo asigna una probabilidad realmente baja a que esas otras 12 palabras

353
00:21:07,260 --> 00:21:11,440
sean realmente la respuesta final, algo así como 1 entre 1000 porque son muy oscuras.

354
00:21:11,440 --> 00:21:15,480
Ahora déjame preguntarte, ¿cuál es la entropía de esta distribución?

355
00:21:15,480 --> 00:21:19,600
Si la entropía simplemente midiera el número de coincidencias aquí, entonces se podría

356
00:21:19,600 --> 00:21:24,760
esperar que fuera algo así como el logaritmo en base 2 de 16,

357
00:21:24,760 --> 00:21:26,200
que sería 4, dos bits más de incertidumbre que los que teníamos antes.

358
00:21:26,200 --> 00:21:30,320
Pero, por supuesto, la incertidumbre real no es tan diferente de la que teníamos antes.

359
00:21:30,320 --> 00:21:33,840
El hecho de que existan estas 12 palabras realmente oscuras no significa que

360
00:21:33,840 --> 00:21:38,200
sería mucho más sorprendente saber que la respuesta final es encanto, por ejemplo.

361
00:21:38,200 --> 00:21:42,080
Entonces, cuando realmente haces el cálculo aquí y sumas la probabilidad de cada

362
00:21:42,080 --> 00:21:45,960
ocurrencia multiplicada por la información correspondiente, lo que obtienes es 2. 11 bits.

363
00:21:45,960 --> 00:21:50,280
Solo digo que son básicamente dos bits, básicamente esas cuatro posibilidades, pero

364
00:21:50,280 --> 00:21:54,240
hay un poco más de incertidumbre debido a todos esos eventos

365
00:21:54,240 --> 00:21:57,120
altamente improbables, aunque si los aprendieras, obtendrías un montón de información.

366
00:21:57,120 --> 00:22:00,800
Entonces, alejarnos, esto es parte de lo que hace de Wordle

367
00:22:00,800 --> 00:22:01,800
un buen ejemplo para una lección de teoría de la información.

368
00:22:01,800 --> 00:22:05,280
Tenemos estas dos aplicaciones de sentimiento distintas para la entropía.

369
00:22:05,280 --> 00:22:09,640
El primero nos dice cuál es la información esperada que obtendremos

370
00:22:09,640 --> 00:22:14,560
de una suposición determinada, y el segundo dice si podemos medir

371
00:22:14,560 --> 00:22:16,480
la incertidumbre restante entre todas las palabras que tenemos posibles.

372
00:22:16,480 --> 00:22:19,800
Y debo enfatizar que, en el primer caso en el que observamos la información esperada de una suposición,

373
00:22:19,800 --> 00:22:25,000
una vez que tenemos una ponderación desigual de las palabras, eso afecta el cálculo de la entropía.

374
00:22:25,000 --> 00:22:28,600
Por ejemplo, permítanme mencionar el mismo caso que vimos anteriormente

375
00:22:28,600 --> 00:22:33,560
de la distribución asociada con Weary, pero esta vez usando

376
00:22:33,560 --> 00:22:34,560
una distribución no uniforme en todas las palabras posibles.

377
00:22:34,560 --> 00:22:39,360
Déjame ver si puedo encontrar una parte aquí que lo ilustre bastante bien.

378
00:22:39,360 --> 00:22:42,480
Bien, aquí esto está bastante bien.

379
00:22:42,480 --> 00:22:46,360
Aquí tenemos dos patrones adyacentes que son igualmente probables, pero nos

380
00:22:46,360 --> 00:22:49,480
dicen que uno de ellos tiene 32 palabras posibles que coinciden.

381
00:22:49,480 --> 00:22:54,080
Y si comprobamos cuáles son, estas son esas 32, que

382
00:22:54,080 --> 00:22:55,600
son palabras muy improbables cuando las examinas con la vista.

383
00:22:55,600 --> 00:23:00,400
Es difícil encontrar respuestas que parezcan plausibles, tal vez gritos, pero si miramos

384
00:23:00,400 --> 00:23:04,440
el patrón vecino en la distribución, que se considera casi igual de

385
00:23:04,440 --> 00:23:08,920
probable, nos dicen que solo tiene 8 coincidencias posibles, por lo que una

386
00:23:08,920 --> 00:23:09,920
cuarta parte de Hay muchas coincidencias, pero es casi igual de probable.

387
00:23:09,920 --> 00:23:12,520
Y cuando analizamos esas coincidencias, podemos ver por qué.

388
00:23:12,520 --> 00:23:17,840
Algunas de estas son respuestas realmente plausibles, como timbre, ira o golpes.

389
00:23:17,840 --> 00:23:22,000
Para ilustrar cómo incorporamos todo eso, permítanme mostrar aquí la versión 2 del Wordlebot,

390
00:23:22,000 --> 00:23:25,960
y hay dos o tres diferencias principales con respecto a la primera que vimos.

391
00:23:25,960 --> 00:23:29,460
En primer lugar, como acabo de decir, la forma en que calculamos estas entropías,

392
00:23:29,460 --> 00:23:34,800
estos valores esperados de información, ahora utiliza distribuciones más refinadas entre los patrones

393
00:23:34,800 --> 00:23:39,300
que incorporan la probabilidad de que una palabra determinada sea realmente la respuesta.

394
00:23:39,300 --> 00:23:44,160
Da la casualidad de que las lágrimas siguen siendo el número 1, aunque las siguientes son un poco diferentes.

395
00:23:44,160 --> 00:23:47,920
En segundo lugar, cuando clasifique sus mejores opciones, ahora mantendrá un modelo de la probabilidad de

396
00:23:47,920 --> 00:23:52,600
que cada palabra sea la respuesta real, y lo incorporará en su decisión, lo cual

397
00:23:52,600 --> 00:23:55,520
es más fácil de ver una vez que tengamos algunas conjeturas sobre la respuesta. mesa.

398
00:23:55,520 --> 00:24:01,120
Nuevamente, ignorando su recomendación porque no podemos dejar que las máquinas gobiernen nuestras vidas.

399
00:24:01,120 --> 00:24:05,160
Y supongo que debería mencionar otra cosa diferente aquí a la izquierda, ese valor de incertidumbre,

400
00:24:05,160 --> 00:24:10,080
esa cantidad de bits, ya no es simplemente redundante con la cantidad de coincidencias posibles.

401
00:24:10,080 --> 00:24:16,520
Ahora si lo levantamos y calculamos 2 elevado a 8. 02, que está un poco por encima

402
00:24:16,520 --> 00:24:22,640
de 256, supongo que 259, lo que dice es que aunque hay un total

403
00:24:22,640 --> 00:24:26,400
de 526 palabras que realmente coinciden con este patrón, la cantidad de incertidumbre que

404
00:24:26,400 --> 00:24:29,760
tiene es más parecida a la que sería si hubiera 259 igualmente probables. resultados.

405
00:24:29,760 --> 00:24:31,100
Puedes pensar en ello así.

406
00:24:31,100 --> 00:24:35,560
Sabe que borx no es la respuesta, lo mismo ocurre con yorts, zorl y

407
00:24:35,560 --> 00:24:37,840
zorus, por lo que es un poco menos incierto que en el caso anterior.

408
00:24:37,840 --> 00:24:40,220
Este número de bits será menor.

409
00:24:40,220 --> 00:24:44,040
Y si sigo jugando, lo refinaré con un par de suposiciones

410
00:24:44,040 --> 00:24:48,680
que son apropiadas para lo que me gustaría explicar aquí.

411
00:24:48,680 --> 00:24:52,520
En la cuarta suposición, si observa sus mejores opciones, podrá ver

412
00:24:52,520 --> 00:24:53,800
que ya no se trata simplemente de maximizar la entropía.

413
00:24:53,800 --> 00:24:58,480
Entonces, en este punto, técnicamente hay siete posibilidades, pero las

414
00:24:58,480 --> 00:25:00,780
únicas con posibilidades significativas son los dormitorios y las palabras.

415
00:25:00,780 --> 00:25:04,760
Y puede ver que se clasifica al elegir ambos por encima

416
00:25:04,760 --> 00:25:07,560
de todos estos otros valores, que estrictamente hablando brindarían más información.

417
00:25:07,560 --> 00:25:11,200
La primera vez que hice esto, simplemente sumé estos dos números para medir la

418
00:25:11,200 --> 00:25:14,580
calidad de cada suposición, lo que en realidad funcionó mejor de lo que imaginas.

419
00:25:14,580 --> 00:25:17,600
Pero realmente no lo sentí sistemático, y estoy seguro de que hay otros

420
00:25:17,600 --> 00:25:19,880
enfoques que la gente podría adoptar, pero este es el que encontré.

421
00:25:19,880 --> 00:25:24,200
Si estamos considerando la posibilidad de una próxima suposición, como en este caso palabras, lo

422
00:25:24,200 --> 00:25:28,440
que realmente nos importa es la puntuación esperada de nuestro juego si lo hacemos.

423
00:25:28,440 --> 00:25:32,880
Y para calcular esa puntuación esperada, decimos cuál es la probabilidad de que

424
00:25:32,880 --> 00:25:35,640
las palabras sean la respuesta real, que en este momento describe el 58%.

425
00:25:36,080 --> 00:25:40,400
Decimos que con un 58% de posibilidades, nuestra puntuación en este juego sería 4.

426
00:25:40,400 --> 00:25:46,240
Y luego, con la probabilidad de 1 menos ese 58%, nuestra puntuación será mayor que ese 4.

427
00:25:46,240 --> 00:25:50,640
No sabemos cuánto más, pero podemos estimarlo en función de cuánta

428
00:25:50,640 --> 00:25:52,920
incertidumbre probablemente habrá una vez que lleguemos a ese punto.

429
00:25:52,920 --> 00:25:56,600
En concreto, de momento hay 1. 44 bits de incertidumbre.

430
00:25:56,600 --> 00:26:01,560
Si adivinamos palabras, nos dice que la información esperada que obtendremos es 1. 27 bits.

431
00:26:01,560 --> 00:26:06,280
Entonces, si adivinamos palabras, esta diferencia representa cuánta incertidumbre es

432
00:26:06,280 --> 00:26:08,280
probable que nos quede después de que eso suceda.

433
00:26:08,280 --> 00:26:12,500
Lo que necesitamos es algún tipo de función, a la que

434
00:26:12,500 --> 00:26:13,880
aquí llamo f, que asocie esta incertidumbre con una puntuación esperada.

435
00:26:13,880 --> 00:26:18,040
Y la forma en que lo hicimos fue simplemente trazar un montón de datos

436
00:26:18,040 --> 00:26:23,920
de juegos anteriores basados en la versión 1 del bot para decir cuál fue

437
00:26:23,920 --> 00:26:27,040
el puntaje real después de varios puntos con ciertas cantidades de incertidumbre muy mensurables.

438
00:26:27,040 --> 00:26:31,120
Por ejemplo, estos puntos de datos aquí que se encuentran por encima de un valor cercano a 8. Aproximadamente

439
00:26:31,120 --> 00:26:36,840
7, dicen para algunos juegos después de un punto en el que había 8. 7 bits de

440
00:26:36,840 --> 00:26:39,340
incertidumbre, fueron necesarias dos conjeturas para obtener la respuesta final.

441
00:26:39,340 --> 00:26:43,180
Para otros juegos fueron necesarias tres conjeturas, para otros juegos fueron necesarias cuatro conjeturas.

442
00:26:43,180 --> 00:26:46,920
Si aquí nos desplazamos hacia la izquierda, todos los puntos sobre cero dicen que

443
00:26:46,920 --> 00:26:51,620
siempre que haya cero bits de incertidumbre, es decir, que solo hay una posibilidad,

444
00:26:51,620 --> 00:26:55,000
entonces el número de conjeturas requeridas es siempre solo uno, lo cual es tranquilizador.

445
00:26:55,000 --> 00:26:59,020
Cada vez que había un poco de incertidumbre, lo que significaba

446
00:26:59,020 --> 00:27:02,360
que esencialmente se reducía a dos posibilidades, a veces se requería

447
00:27:02,360 --> 00:27:03,940
una conjetura más, a veces se requerían dos conjeturas más.

448
00:27:03,940 --> 00:27:05,980
Y así sucesivamente aquí.

449
00:27:05,980 --> 00:27:11,020
Quizás una forma un poco más sencilla de visualizar estos datos sea agruparlos y tomar promedios.

450
00:27:11,020 --> 00:27:15,940
Por ejemplo, esta barra dice que entre todos los puntos en los que teníamos un

451
00:27:15,940 --> 00:27:22,420
poco de incertidumbre, en promedio el número de nuevas conjeturas requeridas fue aproximadamente 1. 5.

452
00:27:22,420 --> 00:27:25,920
Y la barra de aquí dice que entre todos los diferentes juegos donde

453
00:27:25,920 --> 00:27:30,480
en algún momento la incertidumbre estuvo un poco por encima de los cuatro

454
00:27:30,480 --> 00:27:35,120
bits, lo que es como reducirla a 16 posibilidades diferentes, entonces, en promedio,

455
00:27:35,120 --> 00:27:36,240
requiere un poco más de dos conjeturas a partir de ese punto. adelante.

456
00:27:36,240 --> 00:27:40,080
Y a partir de aquí simplemente hice una regresión para ajustarme a una función que parecía razonable para esto.

457
00:27:40,080 --> 00:27:44,160
Y recuerde que el objetivo de hacer todo esto es que podamos cuantificar esta intuición

458
00:27:44,160 --> 00:27:49,720
de que cuanta más información obtengamos de una palabra, menor será la puntuación esperada.

459
00:27:49,720 --> 00:27:54,380
Entonces con esto como versión 2. 0, si volvemos atrás y ejecutamos el mismo conjunto

460
00:27:54,380 --> 00:27:59,820
de simulaciones, haciéndolo jugar contra las 2315 respuestas posibles, ¿cómo funciona?

461
00:27:59,820 --> 00:28:04,060
Bueno, a diferencia de nuestra primera versión, es definitivamente mejor, lo cual es tranquilizador.

462
00:28:04,060 --> 00:28:08,780
Todo dicho y hecho, la media ronda los 3. 6, aunque a diferencia de la primera versión

463
00:28:08,780 --> 00:28:12,820
hay un par de veces que pierde y requiere más de seis en esta circunstancia.

464
00:28:12,820 --> 00:28:15,980
Presumiblemente porque hay momentos en los que se trata de

465
00:28:15,980 --> 00:28:18,980
buscar el objetivo en lugar de maximizar la información.

466
00:28:18,980 --> 00:28:22,140
Entonces, ¿podemos hacerlo mejor que 3? 6?

467
00:28:22,140 --> 00:28:23,260
Definitivamente podemos.

468
00:28:23,260 --> 00:28:27,120
Ahora dije al principio que es muy divertido intentar no incorporar la lista

469
00:28:27,120 --> 00:28:29,980
verdadera de respuestas de Wordle en la forma en que construye su modelo.

470
00:28:29,980 --> 00:28:35,180
Pero si lo incorporamos, el mejor rendimiento que pude obtener fue alrededor de 3. 43.

471
00:28:35,180 --> 00:28:39,520
Entonces, si intentamos ser más sofisticados que simplemente usar datos de frecuencia de palabras para elegir esta

472
00:28:39,520 --> 00:28:44,220
distribución previa, este 3. 43 probablemente da un máximo de lo buenos que podríamos llegar a

473
00:28:44,220 --> 00:28:46,360
ser con eso, o al menos de lo bueno que podría llegar a ser con eso.

474
00:28:46,360 --> 00:28:50,240
Ese mejor rendimiento esencialmente solo utiliza las ideas de las que he

475
00:28:50,240 --> 00:28:53,400
estado hablando aquí, pero va un poco más allá, como si buscara

476
00:28:53,400 --> 00:28:55,660
la información esperada dos pasos adelante en lugar de solo uno.

477
00:28:55,660 --> 00:28:58,720
Originalmente estaba planeando hablar más sobre eso, pero me

478
00:28:58,720 --> 00:29:00,580
doy cuenta de que ya hemos durado bastante.

479
00:29:00,580 --> 00:29:03,520
Lo único que diré es que después de hacer esta búsqueda de dos pasos

480
00:29:03,520 --> 00:29:07,720
y luego ejecutar un par de simulaciones de muestra en los mejores candidatos,

481
00:29:07,720 --> 00:29:09,500
hasta ahora al menos para mí parece que Crane es el mejor abridor.

482
00:29:09,500 --> 00:29:11,080
¿Quién lo hubiera adivinado?

483
00:29:11,080 --> 00:29:15,680
Además, si utiliza la lista de palabras verdaderas para determinar su espacio de posibilidades,

484
00:29:15,680 --> 00:29:17,920
entonces la incertidumbre con la que comienza es un poco más de 11 bits.

485
00:29:18,160 --> 00:29:22,760
Y resulta que, sólo a partir de una búsqueda de fuerza bruta, la máxima información

486
00:29:22,760 --> 00:29:26,580
esperada posible después de las dos primeras conjeturas es de alrededor de 10 bits.

487
00:29:26,580 --> 00:29:31,720
Lo que sugiere que en el mejor de los casos, después de tus dos

488
00:29:31,720 --> 00:29:35,220
primeras conjeturas, con un juego perfectamente óptimo, te quedarás con un poco de incertidumbre.

489
00:29:35,220 --> 00:29:37,400
Lo que es lo mismo que limitarse a dos posibles conjeturas.

490
00:29:37,400 --> 00:29:41,440
Así que creo que es justo y probablemente bastante conservador decir que nunca sería posible

491
00:29:41,440 --> 00:29:45,620
escribir un algoritmo que consiga este promedio tan bajo como 3, porque con las palabras

492
00:29:45,620 --> 00:29:50,460
disponibles, simplemente no hay espacio para obtener suficiente información después de sólo dos pasos para

493
00:29:50,460 --> 00:29:53,820
ser capaz de garantizar la respuesta en el tercer espacio cada vez sin falta.

