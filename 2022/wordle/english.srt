1
00:00:00,000 --> 00:00:04,040
The game Wurdle has gone pretty viral in the last month or two, and never one to overlook

2
00:00:04,040 --> 00:00:07,880
an opportunity for a math lesson, it occurs to me that this game makes for a very good

3
00:00:07,880 --> 00:00:12,120
central example in a lesson about information theory, and in particular a topic known as

4
00:00:12,120 --> 00:00:13,120
entropy.

5
00:00:13,120 --> 00:00:17,120
You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of

6
00:00:17,120 --> 00:00:21,200
programmers I also got sucked into trying to write an algorithm that would play the

7
00:00:21,200 --> 00:00:23,200
game as optimally as it could.

8
00:00:23,200 --> 00:00:26,400
And what I thought I'd do here is just talk through with you some of my process in that,

9
00:00:26,400 --> 00:00:29,980
and explain some of the math that went into it, since the whole algorithm centers on this

10
00:00:29,980 --> 00:00:32,080
idea of entropy.

11
00:00:32,080 --> 00:00:42,180
First things first, in case you haven't heard of it, what is Wurdle?

12
00:00:42,180 --> 00:00:45,380
And to kill two birds with one stone here while we go through the rules of the game,

13
00:00:45,380 --> 00:00:48,980
let me also preview where we're going with this, which is to develop a little algorithm

14
00:00:48,980 --> 00:00:51,380
that will basically play the game for us.

15
00:00:51,380 --> 00:00:54,860
Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot

16
00:00:54,860 --> 00:00:55,860
does.

17
00:00:55,860 --> 00:00:59,580
The goal of Wurdle is to guess a mystery five letter word, and you're given six different

18
00:00:59,580 --> 00:01:00,860
chances to guess.

19
00:01:00,860 --> 00:01:05,240
For example, my Wurdle bot suggests that I start with the guess crane.

20
00:01:05,240 --> 00:01:09,300
Each time that you make a guess, you get some information about how close your guess is

21
00:01:09,300 --> 00:01:10,940
to the true answer.

22
00:01:10,940 --> 00:01:14,540
Here the grey box is telling me there's no C in the actual answer.

23
00:01:14,540 --> 00:01:18,340
The yellow box is telling me there is an R, but it's not in that position.

24
00:01:18,340 --> 00:01:21,820
The green box is telling me that the secret word does have an A, and it's in the third

25
00:01:21,820 --> 00:01:22,820
position.

26
00:01:22,820 --> 00:01:24,300
And then there's no N and there's no E.

27
00:01:24,300 --> 00:01:27,420
So let me just go in and tell the Wurdle bot that information.

28
00:01:27,420 --> 00:01:31,500
We started with crane, we got grey, yellow, green, grey, grey.

29
00:01:31,500 --> 00:01:35,460
Don't worry about all the data that it's showing right now, I'll explain that in due time.

30
00:01:35,460 --> 00:01:39,700
But its top suggestion for our second pick is shtick.

31
00:01:39,700 --> 00:01:43,500
And your guess does have to be an actual five letter word, but as you'll see, it's pretty

32
00:01:43,500 --> 00:01:45,700
liberal with what it will actually let you guess.

33
00:01:45,700 --> 00:01:48,860
In this case, we try shtick.

34
00:01:48,860 --> 00:01:50,260
And alright, things are looking pretty good.

35
00:01:50,260 --> 00:01:54,580
We hit the S and the H, so we know the first three letters, we know that there's an R.

36
00:01:54,740 --> 00:01:59,740
And so it's going to be like S-H-A something R, or S-H-A R something.

37
00:01:59,740 --> 00:02:03,200
And it looks like the Wurdle bot knows that it's down to just two possibilities, either

38
00:02:03,200 --> 00:02:05,220
shard or sharp.

39
00:02:05,220 --> 00:02:08,620
That's kind of a toss up between them at this point, so I guess probably just because it's

40
00:02:08,620 --> 00:02:11,260
alphabetical it goes with shard.

41
00:02:11,260 --> 00:02:13,000
Which hooray, is the actual answer.

42
00:02:13,000 --> 00:02:14,660
So we got it in three.

43
00:02:14,660 --> 00:02:17,740
If you're wondering if that's any good, the way I heard one person phrase it is that with

44
00:02:17,740 --> 00:02:20,820
Wurdle four is par and three is birdie.

45
00:02:20,820 --> 00:02:22,960
Which I think is a pretty apt analogy.

46
00:02:22,960 --> 00:02:27,560
You have to be consistently on your game to be getting four, but it's certainly not crazy.

47
00:02:27,560 --> 00:02:30,000
But when you get it in three, it just feels great.

48
00:02:30,000 --> 00:02:33,800
So if you're down for it, what I'd like to do here is just talk through my thought process

49
00:02:33,800 --> 00:02:36,600
from the beginning for how I approach the Wurdle bot.

50
00:02:36,600 --> 00:02:39,800
And like I said, really it's an excuse for an information theory lesson.

51
00:02:39,800 --> 00:02:43,160
The main goal is to explain what is information and what is entropy.

52
00:02:48,560 --> 00:02:52,080
My first thought in approaching this was to take a look at the relative frequencies of

53
00:02:52,080 --> 00:02:53,560
different letters in the English language.

54
00:02:53,560 --> 00:02:57,800
So I thought, okay, is there an opening guess or an opening pair of guesses that hits a

55
00:02:57,800 --> 00:02:59,960
lot of these most frequent letters?

56
00:02:59,960 --> 00:03:03,780
And one that I was pretty fond of was doing other followed by nails.

57
00:03:03,780 --> 00:03:06,980
The thought is that if you hit a letter, you know, you get a green or a yellow, that always

58
00:03:06,980 --> 00:03:07,980
feels good.

59
00:03:07,980 --> 00:03:09,460
It feels like you're getting information.

60
00:03:09,460 --> 00:03:13,140
But in these cases, even if you don't hit and you always get grays, that's still giving

61
00:03:13,140 --> 00:03:16,640
you a lot of information since it's pretty rare to find a word that doesn't have any

62
00:03:16,640 --> 00:03:17,640
of these letters.

63
00:03:17,640 --> 00:03:21,840
But even still, that doesn't feel super systematic, because for example, it does nothing to

64
00:03:21,840 --> 00:03:23,520
consider the order of the letters.

65
00:03:23,520 --> 00:03:26,080
Why type nails when I could type snail?

66
00:03:26,080 --> 00:03:27,720
Is it better to have that S at the end?

67
00:03:27,720 --> 00:03:28,720
I'm not really sure.

68
00:03:28,720 --> 00:03:33,500
Now, a friend of mine said that he liked to open with the word weary, which kind of surprised

69
00:03:33,500 --> 00:03:37,160
me because it has some uncommon letters in there like the W and the Y.

70
00:03:37,160 --> 00:03:39,400
But who knows, maybe that is a better opener.

71
00:03:39,400 --> 00:03:43,920
Is there some kind of quantitative score that we can give to judge the quality of a potential

72
00:03:43,920 --> 00:03:44,920
guess?

73
00:03:44,920 --> 00:03:48,640
Now to set up for the way that we're going to rank possible guesses, let's go back and

74
00:03:48,640 --> 00:03:51,800
add a little clarity to how exactly the game is set up.

75
00:03:51,800 --> 00:03:55,880
So there's a list of words that it will allow you to enter that are considered valid guesses

76
00:03:55,880 --> 00:03:57,920
that's just about 13,000 words long.

77
00:03:57,920 --> 00:04:01,560
But when you look at it, there's a lot of really uncommon things, things like a head

78
00:04:01,560 --> 00:04:07,040
or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble.

79
00:04:07,040 --> 00:04:10,600
But the vibe of the game is that the answer is always going to be a decently common word.

80
00:04:10,600 --> 00:04:16,080
And in fact, there's another list of around 2300 words that are the possible answers.

81
00:04:16,080 --> 00:04:20,320
And this is a human curated list, I think specifically by the game creator's girlfriend,

82
00:04:20,320 --> 00:04:21,800
which is kind of fun.

83
00:04:21,800 --> 00:04:25,560
But what I would like to do, our challenge for this project is to see if we can write

84
00:04:25,560 --> 00:04:30,720
a program solving Wordle that doesn't incorporate previous knowledge about this list.

85
00:04:30,720 --> 00:04:34,560
For one thing, there's plenty of pretty common five letter words that you won't find in that

86
00:04:34,560 --> 00:04:35,560
list.

87
00:04:35,560 --> 00:04:38,360
So it would be better to write a program that's a little more resilient and would play Wordle

88
00:04:38,360 --> 00:04:41,960
against anyone, not just what happens to be the official website.

89
00:04:41,960 --> 00:04:45,900
And also the reason that we know what this list of possible answers is, is because it's

90
00:04:45,900 --> 00:04:47,440
visible in the source code.

91
00:04:47,440 --> 00:04:51,620
But the way that it's visible in the source code is in the specific order in which answers

92
00:04:51,620 --> 00:04:52,840
come up from day to day.

93
00:04:52,840 --> 00:04:56,400
So you could always just look up what tomorrow's answer will be.

94
00:04:56,400 --> 00:04:59,140
So clearly, there's some sense in which using the list is cheating.

95
00:04:59,140 --> 00:05:02,900
And what makes for a more interesting puzzle and a richer information theory lesson is

96
00:05:02,900 --> 00:05:07,640
to instead use some more universal data like relative word frequencies in general to capture

97
00:05:07,640 --> 00:05:11,640
this intuition of having a preference for more common words.

98
00:05:11,640 --> 00:05:16,560
So of these 13,000 possibilities, how should we choose the opening guess?

99
00:05:16,560 --> 00:05:19,960
For example, if my friend proposes weary, how should we analyze its quality?

100
00:05:19,960 --> 00:05:25,040
Well, the reason he said he likes that unlikely W is that he likes the long shot nature of

101
00:05:25,040 --> 00:05:27,880
just how good it feels if you do hit that W.

102
00:05:27,880 --> 00:05:31,400
For example, if the first pattern revealed was something like this, then it turns out

103
00:05:31,400 --> 00:05:36,080
there are only 58 words in this giant lexicon that match that pattern.

104
00:05:36,080 --> 00:05:38,900
So that's a huge reduction from 13,000.

105
00:05:38,900 --> 00:05:43,320
But the flip side of that, of course, is that it's very uncommon to get a pattern like this.

106
00:05:43,360 --> 00:05:47,600
Specifically, if each word was equally likely to be the answer, the probability of hitting

107
00:05:47,600 --> 00:05:51,680
this pattern would be 58 divided by around 13,000.

108
00:05:51,680 --> 00:05:53,880
Of course, they're not equally likely to be answers.

109
00:05:53,880 --> 00:05:56,680
Most of these are very obscure and even questionable words.

110
00:05:56,680 --> 00:05:59,560
But at least for our first pass at all of this, let's assume that they're all equally

111
00:05:59,560 --> 00:06:02,040
likely and then refine that a bit later.

112
00:06:02,040 --> 00:06:07,360
The point is the pattern with a lot of information is by its very nature unlikely to occur.

113
00:06:07,360 --> 00:06:11,320
In fact, what it means to be informative is that it's unlikely.

114
00:06:11,920 --> 00:06:16,720
A much more probable pattern to see with this opening would be something like this, where

115
00:06:16,720 --> 00:06:18,360
of course there's not a W in it.

116
00:06:18,360 --> 00:06:22,080
Maybe there's an E, and maybe there's no A, there's no R, there's no Y.

117
00:06:22,080 --> 00:06:24,640
In this case, there are 1400 possible matches.

118
00:06:24,640 --> 00:06:29,600
If all were equally likely, it works out to be a probability of about 11% that this is

119
00:06:29,600 --> 00:06:30,680
the pattern you would see.

120
00:06:30,680 --> 00:06:34,320
So the most likely outcomes are also the least informative.

121
00:06:34,320 --> 00:06:38,440
To get a more global view here, let me show you the full distribution of probabilities

122
00:06:38,440 --> 00:06:42,000
across all of the different patterns that you might see.

123
00:06:42,000 --> 00:06:46,000
So each bar that you're looking at corresponds to a possible pattern of colors that could

124
00:06:46,000 --> 00:06:50,500
be revealed, of which there are 3 to the 5th possibilities, and they're organized from

125
00:06:50,500 --> 00:06:52,960
left to right, most common to least common.

126
00:06:52,960 --> 00:06:56,200
So the most common possibility here is that you get all grays.

127
00:06:56,200 --> 00:06:58,800
That happens about 14% of the time.

128
00:06:58,800 --> 00:07:02,040
And what you're hoping for when you make a guess is that you end up somewhere out in

129
00:07:02,040 --> 00:07:06,360
this long tail, like over here where there's only 18 possibilities for what matches this

130
00:07:06,360 --> 00:07:09,920
pattern that evidently look like this.

131
00:07:09,920 --> 00:07:14,080
Or if we venture a little farther to the left, you know, maybe we go all the way over here.

132
00:07:14,080 --> 00:07:16,560
Okay, here's a good puzzle for you.

133
00:07:16,560 --> 00:07:20,600
What are the three words in the English language that start with a W, end with a Y, and have

134
00:07:20,600 --> 00:07:22,040
an R somewhere in them?

135
00:07:22,040 --> 00:07:27,560
Turns out, the answers are, let's see, wordy, wormy, and wryly.

136
00:07:27,560 --> 00:07:32,720
So to judge how good this word is overall, we want some kind of measure of the expected

137
00:07:32,720 --> 00:07:35,720
amount of information that you're going to get from this distribution.

138
00:07:36,360 --> 00:07:41,080
If we go through each pattern and we multiply its probability of occurring times something

139
00:07:41,080 --> 00:07:46,000
that measures how informative it is, that can maybe give us an objective score.

140
00:07:46,000 --> 00:07:50,280
Now your first instinct for what that something should be might be the number of matches.

141
00:07:50,280 --> 00:07:52,960
You want a lower average number of matches.

142
00:07:52,960 --> 00:07:57,400
But instead I'd like to use a more universal measurement that we often ascribe to information,

143
00:07:57,400 --> 00:08:01,040
and one that will be more flexible once we have a different probability assigned to each

144
00:08:01,040 --> 00:08:04,320
of these 13,000 words for whether or not they're actually the answer.

145
00:08:10,600 --> 00:08:14,760
The standard unit of information is the bit, which has a little bit of a funny formula,

146
00:08:14,760 --> 00:08:17,800
but it's really intuitive if we just look at examples.

147
00:08:17,800 --> 00:08:21,880
If you have an observation that cuts your space of possibilities in half, we say that

148
00:08:21,880 --> 00:08:24,200
it has one bit of information.

149
00:08:24,200 --> 00:08:27,680
In our example, the space of possibilities is all possible words, and it turns out about

150
00:08:27,760 --> 00:08:31,560
Half of the five letter words have an S, a little less than that, but about half.

151
00:08:31,560 --> 00:08:35,200
So that observation would give you one bit of information.

152
00:08:35,200 --> 00:08:39,640
If instead a new fact chops down that space of possibilities by a factor of four, we say

153
00:08:39,640 --> 00:08:42,000
that it has two bits of information.

154
00:08:42,000 --> 00:08:45,120
For example, it turns out about a quarter of these words have a T.

155
00:08:45,120 --> 00:08:49,720
If the observation cuts that space by a factor of eight, we say it's three bits of information,

156
00:08:49,720 --> 00:08:50,920
and so on and so forth.

157
00:08:50,920 --> 00:08:55,000
Four bits cuts it into a 16th, five bits cuts it into a 32nd.

158
00:08:55,000 --> 00:09:00,160
So now you might want to pause and ask yourself, what is the formula for information for the

159
00:09:00,160 --> 00:09:04,520
number of bits in terms of the probability of an occurrence?

160
00:09:04,520 --> 00:09:07,920
What we're saying here is that when you take one half to the number of bits, that's the

161
00:09:07,920 --> 00:09:11,680
same thing as the probability, which is the same thing as saying two to the power of the

162
00:09:11,680 --> 00:09:16,200
number of bits is one over the probability, which rearranges further to saying the information

163
00:09:16,200 --> 00:09:19,680
is the log base two of one divided by the probability.

164
00:09:19,680 --> 00:09:23,200
And sometimes you see this with one more rearrangement still, where the information is the negative

165
00:09:23,200 --> 00:09:25,680
log base two of the probability.

166
00:09:25,680 --> 00:09:29,120
Expressed like this, it can look a little bit weird to the uninitiated, but it really

167
00:09:29,120 --> 00:09:33,400
is just the very intuitive idea of asking how many times you've cut down your possibilities

168
00:09:33,400 --> 00:09:35,120
in half.

169
00:09:35,120 --> 00:09:37,840
Now if you're wondering, you know, I thought we were just playing a fun word game, why

170
00:09:37,840 --> 00:09:39,920
are logarithms entering the picture?

171
00:09:39,920 --> 00:09:43,920
One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events,

172
00:09:43,920 --> 00:09:48,120
much easier to say that an observation has 20 bits of information than it is to say that

173
00:09:48,120 --> 00:09:53,480
the probability of such and such occurring is 0.0000095.

174
00:09:53,480 --> 00:09:57,360
But a more substantive reason that this logarithmic expression turned out to be a very useful

175
00:09:57,360 --> 00:10:02,000
addition to the theory of probability is the way that information adds together.

176
00:10:02,000 --> 00:10:05,560
For example, if one observation gives you two bits of information, cutting your space

177
00:10:05,560 --> 00:10:10,120
down by four, and then a second observation like your second guess in Wordle gives you

178
00:10:10,120 --> 00:10:14,480
another three bits of information, chopping you down further by another factor of eight,

179
00:10:14,480 --> 00:10:17,360
the two together give you five bits of information.

180
00:10:17,360 --> 00:10:21,200
In the same way that probabilities like to multiply, information likes to add.

181
00:10:21,200 --> 00:10:24,920
So as soon as we're in the realm of something like an expected value, where we're adding

182
00:10:24,920 --> 00:10:28,660
a bunch of numbers up, the logs make it a lot nicer to deal with.

183
00:10:28,660 --> 00:10:32,600
Let's go back to our distribution for Weary and add another little tracker on here, showing

184
00:10:32,600 --> 00:10:35,560
us how much information there is for each pattern.

185
00:10:35,560 --> 00:10:38,760
The main thing I want you to notice is that the higher the probability as we get to those

186
00:10:38,760 --> 00:10:43,500
more likely patterns, the lower the information, the fewer bits you gain.

187
00:10:43,500 --> 00:10:47,360
The way we measure the quality of this guess will be to take the expected value of this

188
00:10:47,360 --> 00:10:51,620
information, where we go through each pattern, we say how probable is it, and then we multiply

189
00:10:51,620 --> 00:10:54,940
that by how many bits of information do we get.

190
00:10:54,940 --> 00:10:58,480
And in the example of Weary, that turns out to be 4.9 bits.

191
00:10:58,480 --> 00:11:02,800
So on average, the information you get from this opening guess is as good as chopping

192
00:11:02,800 --> 00:11:05,660
your space of possibilities in half about five times.

193
00:11:05,660 --> 00:11:10,260
By contrast, an example of a guess with a higher expected information value would be

194
00:11:10,260 --> 00:11:13,220
something like Slate.

195
00:11:13,220 --> 00:11:16,180
In this case you'll notice the distribution looks a lot flatter.

196
00:11:16,180 --> 00:11:20,780
In particular, the most probable occurrence of all grays only has about a 6% chance of

197
00:11:20,780 --> 00:11:25,940
occurring, so at minimum you're getting evidently 3.9 bits of information.

198
00:11:25,940 --> 00:11:29,140
But that's a minimum, more typically you'd get something better than that.

199
00:11:29,140 --> 00:11:33,380
And it turns out when you crunch the numbers on this one and add up all the relevant terms,

200
00:11:33,380 --> 00:11:36,420
the average information is about 5.8.

201
00:11:36,420 --> 00:11:42,140
So in contrast with Weary, your space of possibilities will be about half as big after this first

202
00:11:42,140 --> 00:11:43,940
guess, on average.

203
00:11:43,940 --> 00:11:49,540
There's actually a fun story about the name for this expected value of information quantity.

204
00:11:49,540 --> 00:11:52,580
Information theory was developed by Claude Shannon, who was working at Bell Labs in the

205
00:11:52,580 --> 00:11:57,620
1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann,

206
00:11:57,620 --> 00:12:01,500
who was this intellectual giant of the time, very prominent in math and physics and the

207
00:12:01,500 --> 00:12:04,180
beginnings of what was becoming computer science.

208
00:12:04,180 --> 00:12:07,260
And when he mentioned that he didn't really have a good name for this expected value of

209
00:12:07,260 --> 00:12:12,540
information quantity, von Neumann supposedly said, so the story goes, well you should call

210
00:12:12,540 --> 00:12:14,720
it entropy, and for two reasons.

211
00:12:14,720 --> 00:12:18,400
In the first place, your uncertainty function has been used in statistical mechanics under

212
00:12:18,400 --> 00:12:23,100
that name, so it already has a name, and in the second place, and more important, nobody

213
00:12:23,100 --> 00:12:26,940
knows what entropy really is, so in a debate you'll always have the advantage.

214
00:12:26,940 --> 00:12:31,420
So if the name seems a little bit mysterious, and if this story is to be believed, that's

215
00:12:31,420 --> 00:12:33,420
kind of by design.

216
00:12:33,420 --> 00:12:36,740
Also if you're wondering about its relation to all of that second law of thermodynamics

217
00:12:36,740 --> 00:12:40,820
stuff from physics, there definitely is a connection, but in its origins Shannon was

218
00:12:40,820 --> 00:12:44,780
just dealing with pure probability theory, and for our purposes here, when I use the

219
00:12:44,780 --> 00:12:49,340
word entropy, I just want you to think the expected information value of a particular

220
00:12:49,340 --> 00:12:50,820
guess.

221
00:12:50,820 --> 00:12:54,380
You can think of entropy as measuring two things simultaneously.

222
00:12:54,380 --> 00:12:57,420
The first one is how flat is the distribution.

223
00:12:57,420 --> 00:13:01,700
The closer a distribution is to uniform, the higher that entropy will be.

224
00:13:01,700 --> 00:13:06,340
In our case, where there are 3 to the 5th total patterns, for a uniform distribution,

225
00:13:06,340 --> 00:13:11,340
observing any one of them would have information log base 2 of 3 to the 5th, which happens

226
00:13:11,340 --> 00:13:17,860
to be 7.92, so that is the absolute maximum that you could possibly have for this entropy.

227
00:13:17,860 --> 00:13:21,900
But entropy is also kind of a measure of how many possibilities there are in the first

228
00:13:21,900 --> 00:13:22,900
place.

229
00:13:22,900 --> 00:13:26,980
For example, if you happen to have some word where there's only 16 possible patterns, and

230
00:13:26,980 --> 00:13:32,760
each one is equally likely, this entropy, this expected information, would be 4 bits.

231
00:13:32,760 --> 00:13:36,880
But if you have another word where there's 64 possible patterns that could come up, and

232
00:13:36,880 --> 00:13:41,000
they're all equally likely, then the entropy would work out to be 6 bits.

233
00:13:41,000 --> 00:13:45,800
So if you see some distribution out in the wild that has an entropy of 6 bits, it's

234
00:13:45,800 --> 00:13:50,000
sort of like it's saying there's as much variation and uncertainty in what's about

235
00:13:50,000 --> 00:13:54,400
to happen as if there were 64 equally likely outcomes.

236
00:13:54,400 --> 00:13:58,360
For my first pass at the Wurtelebot, I basically had it just do this.

237
00:13:58,360 --> 00:14:03,560
It goes through all of the possible guesses you could have, all 13,000 words, computes

238
00:14:03,560 --> 00:14:08,580
the entropy for each one, or more specifically, the entropy of the distribution across all

239
00:14:08,580 --> 00:14:13,040
patterns you might see, for each one, and picks the highest, since that's the one

240
00:14:13,040 --> 00:14:17,200
that's likely to chop down your space of possibilities as much as possible.

241
00:14:17,200 --> 00:14:20,120
And even though I've only been talking about the first guess here, it does the same thing

242
00:14:20,120 --> 00:14:21,680
for the next few guesses.

243
00:14:21,680 --> 00:14:25,100
For example, after you see some pattern on that first guess, which would restrict you

244
00:14:25,100 --> 00:14:29,300
to a smaller number of possible words based on what matches with that, you just play the

245
00:14:29,300 --> 00:14:32,300
same game with respect to that smaller set of words.

246
00:14:32,300 --> 00:14:36,500
For a proposed second guess, you look at the distribution of all patterns that could occur

247
00:14:36,500 --> 00:14:41,540
from that more restricted set of words, you search through all 13,000 possibilities, and

248
00:14:41,540 --> 00:14:45,480
you find the one that maximizes that entropy.

249
00:14:45,480 --> 00:14:48,980
To show you how this works in action, let me just pull up a little variant of Wurtele

250
00:14:48,980 --> 00:14:54,060
that I wrote that shows the highlights of this analysis in the margins.

251
00:14:54,460 --> 00:14:57,820
After doing all its entropy calculations, on the right here it's showing us which ones

252
00:14:57,820 --> 00:15:00,340
have the highest expected information.

253
00:15:00,340 --> 00:15:04,940
Turns out the top answer, at least at the moment, we'll refine this later, is Tares,

254
00:15:04,940 --> 00:15:11,140
which means, um, of course, a vetch, the most common vetch.

255
00:15:11,140 --> 00:15:14,180
Each time we make a guess here, where maybe I kind of ignore its recommendations and go

256
00:15:14,180 --> 00:15:19,220
with slate, because I like slate, we can see how much expected information it had, but

257
00:15:19,220 --> 00:15:23,300
then on the right of the word here it's showing us how much actual information we got, given

258
00:15:23,340 --> 00:15:24,980
this particular pattern.

259
00:15:24,980 --> 00:15:28,660
So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened

260
00:15:28,660 --> 00:15:30,660
to get something with less than that.

261
00:15:30,660 --> 00:15:34,020
And then on the left side here it's showing us all of the different possible words given

262
00:15:34,020 --> 00:15:35,860
where we are now.

263
00:15:35,860 --> 00:15:39,820
The blue bars are telling us how likely it thinks each word is, so at the moment it's

264
00:15:39,820 --> 00:15:44,140
assuming each word is equally likely to occur, but we'll refine that in a moment.

265
00:15:44,140 --> 00:15:48,580
And then this uncertainty measurement is telling us the entropy of this distribution across

266
00:15:48,580 --> 00:15:53,220
the possible words, which right now, because it's a uniform distribution, is just a needlessly

267
00:15:53,300 --> 00:15:55,940
complicated way to count the number of possibilities.

268
00:15:55,940 --> 00:16:01,700
For example, if we were to take 2 to the power of 13.66, that should be around the 13,000

269
00:16:01,700 --> 00:16:02,700
possibilities.

270
00:16:02,700 --> 00:16:06,780
I'm a little bit off here, but only because I'm not showing all the decimal places.

271
00:16:06,780 --> 00:16:10,260
At the moment that might feel redundant and like it's overly complicating things, but

272
00:16:10,260 --> 00:16:12,780
you'll see why it's useful to have both numbers in a minute.

273
00:16:12,780 --> 00:16:16,780
So here it looks like it's suggesting the highest entropy for our second guess is Ramen,

274
00:16:16,780 --> 00:16:19,700
which again just really doesn't feel like a word.

275
00:16:19,700 --> 00:16:25,660
So to take the moral high ground here, I'm going to go ahead and type in Rains.

276
00:16:25,660 --> 00:16:27,540
And again it looks like we were a little unlucky.

277
00:16:27,540 --> 00:16:32,100
We were expecting 4.3 bits and we only got 3.39 bits of information.

278
00:16:32,100 --> 00:16:35,060
So that takes us down to 55 possibilities.

279
00:16:35,060 --> 00:16:38,860
And here maybe I'll just actually go with what it's suggesting, which is combo, whatever

280
00:16:38,860 --> 00:16:40,200
that means.

281
00:16:40,200 --> 00:16:43,300
And okay, this is actually a good chance for a puzzle.

282
00:16:43,300 --> 00:16:47,020
It's telling us this pattern gives us 4.7 bits of information.

283
00:16:47,020 --> 00:16:52,400
But over on the left, before we see that pattern, there were 5.78 bits of uncertainty.

284
00:16:52,400 --> 00:16:56,860
So as a quiz for you, what does that mean about the number of remaining possibilities?

285
00:16:56,860 --> 00:17:02,280
Well, it means that we're reduced down to one bit of uncertainty, which is the same

286
00:17:02,280 --> 00:17:04,700
thing as saying that there's two possible answers.

287
00:17:04,700 --> 00:17:06,520
It's a 50-50 choice.

288
00:17:06,520 --> 00:17:09,860
And from here, because you and I know which words are more common, we know that the answer

289
00:17:09,860 --> 00:17:11,220
should be abyss.

290
00:17:11,220 --> 00:17:13,540
But as it's written right now, the program doesn't know that.

291
00:17:13,540 --> 00:17:17,560
So it just keeps going, trying to gain as much information as it can, until it's only

292
00:17:17,560 --> 00:17:20,360
one possibility left, and then it guesses it.

293
00:17:20,360 --> 00:17:22,700
So obviously we need a better endgame strategy.

294
00:17:22,700 --> 00:17:26,540
But let's say we call this version one of our wordle solver, and then we go and run

295
00:17:26,540 --> 00:17:30,740
some simulations to see how it does.

296
00:17:30,740 --> 00:17:34,240
So the way this is working is it's playing every possible wordle game.

297
00:17:34,240 --> 00:17:38,780
It's going through all of those 2315 words that are the actual wordle answers.

298
00:17:38,780 --> 00:17:41,340
It's basically using that as a testing set.

299
00:17:41,340 --> 00:17:45,820
And with this naive method of not considering how common a word is, and just trying to maximize

300
00:17:45,820 --> 00:17:50,480
the information at each step along the way, until it gets down to one and only one choice.

301
00:17:50,480 --> 00:17:55,100
By the end of the simulation, the average score works out to be about 4.124.

302
00:17:55,100 --> 00:17:59,780
Which is not bad, to be honest, I kind of expected to do worse.

303
00:17:59,780 --> 00:18:03,040
But the people who play wordle will tell you that they can usually get it in 4.

304
00:18:03,040 --> 00:18:05,260
The real challenge is to get as many in 3 as you can.

305
00:18:05,260 --> 00:18:08,920
It's a pretty big jump between the score of 4 and the score of 3.

306
00:18:08,920 --> 00:18:13,300
The obvious low hanging fruit here is to somehow incorporate whether or not a word is common,

307
00:18:13,300 --> 00:18:23,160
and how exactly do we do that.

308
00:18:23,160 --> 00:18:26,860
The way I approached it is to get a list of the relative frequencies for all of the words

309
00:18:26,860 --> 00:18:28,560
in the English language.

310
00:18:28,560 --> 00:18:32,560
And I just used Mathematica's word frequency data function, which itself pulls from the

311
00:18:32,560 --> 00:18:35,520
Google Books English Ngram public dataset.

312
00:18:35,520 --> 00:18:38,680
And it's kind of fun to look at, for example if we sort it from the most common words to

313
00:18:38,680 --> 00:18:40,120
the least common words.

314
00:18:40,120 --> 00:18:43,740
Evidently these are the most common, 5 letter words in the English language.

315
00:18:43,740 --> 00:18:46,480
Or rather, these is the 8th most common.

316
00:18:46,480 --> 00:18:49,440
First is which, after which there's there and there.

317
00:18:49,440 --> 00:18:53,020
First itself is not first, but 9th, and it makes sense that these other words could come

318
00:18:53,020 --> 00:18:57,840
about more often, where those after first are after, where, and those being just a little

319
00:18:57,840 --> 00:18:59,000
bit less common.

320
00:18:59,000 --> 00:19:04,400
Now, in using this data to model how likely each of these words is to be the final answer,

321
00:19:04,400 --> 00:19:06,760
it shouldn't just be proportional to the frequency.

322
00:19:07,020 --> 00:19:12,560
For example, which is given a score of 0.002 in this dataset, whereas the word braid is

323
00:19:12,560 --> 00:19:15,200
in some sense about 1000 times less likely.

324
00:19:15,200 --> 00:19:19,400
But both of these are common enough words that they're almost certainly worth considering.

325
00:19:19,400 --> 00:19:21,900
So we want more of a binary cutoff.

326
00:19:21,900 --> 00:19:26,520
The way I went about it is to imagine taking this whole sorted list of words, and then

327
00:19:26,520 --> 00:19:31,060
arranging it on an x-axis, and then applying the sigmoid function, which is the standard

328
00:19:31,060 --> 00:19:35,540
way to have a function whose output is basically binary, it's either 0 or it's 1, but there's

329
00:19:35,540 --> 00:19:38,500
a smoothing in between for that region of uncertainty.

330
00:19:38,500 --> 00:19:43,900
So essentially, the probability that I'm assigning to each word for being in the final list will

331
00:19:43,900 --> 00:19:49,540
be the value of the sigmoid function above wherever it sits on the x-axis.

332
00:19:49,540 --> 00:19:53,940
Now obviously this depends on a few parameters, for example how wide a space on the x-axis

333
00:19:53,940 --> 00:19:59,660
those words fill determines how gradually or steeply we drop off from 1 to 0, and where

334
00:19:59,660 --> 00:20:03,000
we situate them left to right determines the cutoff.

335
00:20:03,160 --> 00:20:07,340
To be honest, the way I did this was just licking my finger and sticking it into the wind.

336
00:20:07,340 --> 00:20:10,800
I looked through the sorted list and tried to find a window where when I looked at it

337
00:20:10,800 --> 00:20:15,280
I figured about half of these words are more likely than not to be the final answer, and

338
00:20:15,280 --> 00:20:17,680
used that as the cutoff.

339
00:20:17,680 --> 00:20:21,840
Once we have a distribution like this across the words, it gives us another situation where

340
00:20:21,840 --> 00:20:24,460
entropy becomes this really useful measurement.

341
00:20:24,460 --> 00:20:28,480
For example, let's say we were playing a game and we start with my old openers, which were

342
00:20:28,480 --> 00:20:32,480
a feather and nails, and we end up with a situation where there's four possible words

343
00:20:32,480 --> 00:20:33,760
that match it.

344
00:20:33,760 --> 00:20:36,440
And let's say we consider them all equally likely.

345
00:20:36,440 --> 00:20:40,000
Let me ask you, what is the entropy of this distribution?

346
00:20:40,000 --> 00:20:45,920
Well, the information associated with each one of these possibilities is going to be

347
00:20:45,920 --> 00:20:50,800
the log base 2 of 4, since each one is 1 and 4, and that's 2.

348
00:20:50,800 --> 00:20:52,780
Two bits of information, four possibilities.

349
00:20:52,780 --> 00:20:54,360
All very well and good.

350
00:20:54,360 --> 00:20:58,320
But what if I told you that actually there's more than four matches?

351
00:20:58,320 --> 00:21:02,600
In reality, when we look through the full word list, there are 16 words that match it.

352
00:21:02,600 --> 00:21:07,260
But suppose our model puts a really low probability on those other 12 words of actually being

353
00:21:07,260 --> 00:21:11,440
the final answer, something like 1 in 1000 because they're really obscure.

354
00:21:11,440 --> 00:21:15,480
Now let me ask you, what is the entropy of this distribution?

355
00:21:15,480 --> 00:21:19,600
If entropy was purely measuring the number of matches here, then you might expect it

356
00:21:19,600 --> 00:21:24,760
to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty

357
00:21:24,760 --> 00:21:26,200
than we had before.

358
00:21:26,200 --> 00:21:30,320
But of course the actual uncertainty is not really that different from what we had before.

359
00:21:30,320 --> 00:21:33,840
Just because there's these 12 really obscure words doesn't mean that it would be all that

360
00:21:33,840 --> 00:21:38,200
more surprising to learn that the final answer is charm, for example.

361
00:21:38,200 --> 00:21:42,080
So when you actually do the calculation here, and you add up the probability of each occurrence

362
00:21:42,080 --> 00:21:45,960
times the corresponding information, what you get is 2.11 bits.

363
00:21:45,960 --> 00:21:50,280
I'm just saying, it's basically two bits, basically those four possibilities, but there's

364
00:21:50,280 --> 00:21:54,240
a little more uncertainty because of all of those highly unlikely events, though if you

365
00:21:54,240 --> 00:21:57,120
did learn them you'd get a ton of information from it.

366
00:21:57,120 --> 00:22:00,800
So zooming out, this is part of what makes Wordle such a nice example for an information

367
00:22:00,800 --> 00:22:01,800
theory lesson.

368
00:22:01,800 --> 00:22:05,280
We have these two distinct feeling applications for entropy.

369
00:22:05,280 --> 00:22:09,640
The first one telling us what's the expected information we'll get from a given guess,

370
00:22:09,640 --> 00:22:14,560
and the second one saying can we measure the remaining uncertainty among all of the words

371
00:22:14,560 --> 00:22:16,480
that we have possible.

372
00:22:16,480 --> 00:22:19,800
And I should emphasize, in that first case where we're looking at the expected information

373
00:22:19,800 --> 00:22:25,000
of a guess, once we have an unequal weighting to the words, that affects the entropy calculation.

374
00:22:25,000 --> 00:22:28,600
For example, let me pull up that same case we were looking at earlier of the distribution

375
00:22:28,600 --> 00:22:33,560
associated with Weary, but this time using a non-uniform distribution across all possible

376
00:22:33,560 --> 00:22:34,560
words.

377
00:22:34,560 --> 00:22:39,360
So let me see if I can find a part here that illustrates it pretty well.

378
00:22:39,360 --> 00:22:42,480
Okay, here this is pretty good.

379
00:22:42,480 --> 00:22:46,360
Here we have two adjacent patterns that are about equally likely, but one of them we're

380
00:22:46,360 --> 00:22:49,480
told has 32 possible words that match it.

381
00:22:49,480 --> 00:22:54,080
And if we check what they are, these are those 32, which are all just very unlikely words

382
00:22:54,080 --> 00:22:55,600
as you scan your eyes over them.

383
00:22:55,600 --> 00:23:00,400
It's hard to find any that feel like plausible answers, maybe yells, but if we look at the

384
00:23:00,400 --> 00:23:04,440
neighboring pattern in the distribution, which is considered just about as likely, we're

385
00:23:04,440 --> 00:23:08,920
told that it only has 8 possible matches, so a quarter as many matches, but it's about

386
00:23:08,920 --> 00:23:09,920
as likely.

387
00:23:09,920 --> 00:23:12,520
And when we pull up those matches, we can see why.

388
00:23:12,520 --> 00:23:17,840
Some of these are actual plausible answers, like ring, or wrath, or raps.

389
00:23:17,840 --> 00:23:22,000
To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot

390
00:23:22,000 --> 00:23:25,960
here, and there are two or three main differences from the first one that we saw.

391
00:23:25,960 --> 00:23:29,460
First off, like I just said, the way that we're computing these entropies, these expected

392
00:23:29,460 --> 00:23:34,800
values of information, is now using the more refined distributions across the patterns

393
00:23:34,800 --> 00:23:39,300
that incorporates the probability that a given word would actually be the answer.

394
00:23:39,300 --> 00:23:44,160
As it happens, tears is still number 1, though the ones following are a bit different.

395
00:23:44,160 --> 00:23:47,920
Second, when it ranks its top picks, it's now going to keep a model of the probability

396
00:23:47,920 --> 00:23:52,600
that each word is the actual answer, and it'll incorporate that into its decision, which

397
00:23:52,600 --> 00:23:55,520
is easier to see once we have a few guesses on the table.

398
00:23:55,520 --> 00:24:01,120
Again, ignoring its recommendation because we can't let machines rule our lives.

399
00:24:01,120 --> 00:24:05,160
And I suppose I should mention another thing different here is over on the left, that uncertainty

400
00:24:05,160 --> 00:24:10,080
value, that number of bits, is no longer just redundant with the number of possible matches.

401
00:24:10,080 --> 00:24:16,520
Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess

402
00:24:16,520 --> 00:24:22,640
259, what it's saying is even though there are 526 total words that actually match this

403
00:24:22,640 --> 00:24:26,400
pattern, the amount of uncertainty it has is more akin to what it would be if there

404
00:24:26,400 --> 00:24:29,760
were 259 equally likely outcomes.

405
00:24:29,760 --> 00:24:31,100
You can think of it like this.

406
00:24:31,100 --> 00:24:35,560
It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little

407
00:24:35,560 --> 00:24:37,840
less uncertain than it was in the previous case.

408
00:24:37,840 --> 00:24:40,220
This number of bits will be smaller.

409
00:24:40,220 --> 00:24:44,040
And if I keep playing the game, I'm refining this down with a couple guesses that are apropos

410
00:24:44,040 --> 00:24:48,680
of what I would like to explain here.

411
00:24:48,680 --> 00:24:52,520
By the fourth guess, if you look over at its top picks, you can see it's no longer just

412
00:24:52,520 --> 00:24:53,800
maximizing the entropy.

413
00:24:53,800 --> 00:24:58,480
So at this point, there's technically seven possibilities, but the only ones with a meaningful

414
00:24:58,480 --> 00:25:00,780
chance are dorms and words.

415
00:25:00,780 --> 00:25:04,760
And you can see it ranks choosing both of those above all of these other values, that

416
00:25:04,760 --> 00:25:07,560
strictly speaking would give more information.

417
00:25:07,560 --> 00:25:11,200
The very first time I did this, I just added up these two numbers to measure the quality

418
00:25:11,200 --> 00:25:14,580
of each guess, which actually worked better than you might suspect.

419
00:25:14,580 --> 00:25:17,600
But it really didn't feel systematic, and I'm sure there's other approaches people could

420
00:25:17,600 --> 00:25:19,880
take but here's the one I landed on.

421
00:25:19,880 --> 00:25:24,200
If we're considering the prospect of a next guess, like in this case words, what we really

422
00:25:24,200 --> 00:25:28,440
care about is the expected score of our game if we do that.

423
00:25:28,440 --> 00:25:32,880
And to calculate that expected score, we say what's the probability that words is the actual

424
00:25:32,880 --> 00:25:35,640
answer, which at the moment it describes 58% to.

425
00:25:36,080 --> 00:25:40,400
We say with a 58% chance, our score in this game would be 4.

426
00:25:40,400 --> 00:25:46,240
And then with the probability of 1 minus that 58%, our score will be more than that 4.

427
00:25:46,240 --> 00:25:50,640
How much more we don't know, but we can estimate it based on how much uncertainty there's likely

428
00:25:50,640 --> 00:25:52,920
to be once we get to that point.

429
00:25:52,920 --> 00:25:56,600
Specifically, at the moment there's 1.44 bits of uncertainty.

430
00:25:56,600 --> 00:26:01,560
If we guess words, it's telling us the expected information we'll get is 1.27 bits.

431
00:26:01,560 --> 00:26:06,280
So if we guess words, this difference represents how much uncertainty we're likely to be left

432
00:26:06,280 --> 00:26:08,280
with after that happens.

433
00:26:08,280 --> 00:26:12,500
What we need is some kind of function, which I'm calling f here, that associates this uncertainty

434
00:26:12,500 --> 00:26:13,880
with an expected score.

435
00:26:13,880 --> 00:26:18,040
And the way it went about this was to just plot a bunch of the data from previous games

436
00:26:18,040 --> 00:26:23,920
based on version 1 of the bot to say hey what was the actual score after various points

437
00:26:23,920 --> 00:26:27,040
with certain very measurable amounts of uncertainty.

438
00:26:27,040 --> 00:26:31,120
For example, these data points here that are sitting above a value that's around like 8.7

439
00:26:31,120 --> 00:26:36,840
or so are saying for some games after a point at which there were 8.7 bits of uncertainty,

440
00:26:36,840 --> 00:26:39,340
it took two guesses to get the final answer.

441
00:26:39,340 --> 00:26:43,180
For other games it took three guesses, for other games it took four guesses.

442
00:26:43,180 --> 00:26:46,920
If we shift over to the left here, all the points over zero are saying whenever there's

443
00:26:46,920 --> 00:26:51,620
zero bits of uncertainty, which is to say there's only one possibility, then the number

444
00:26:51,620 --> 00:26:55,000
of guesses required is always just one, which is reassuring.

445
00:26:55,000 --> 00:26:59,020
Whenever there was one bit of uncertainty, meaning it was essentially just down to two

446
00:26:59,020 --> 00:27:02,360
possibilities, then sometimes it required one more guess, sometimes it required two

447
00:27:02,360 --> 00:27:03,940
more guesses.

448
00:27:03,940 --> 00:27:05,980
And so on and so forth here.

449
00:27:05,980 --> 00:27:11,020
Maybe a slightly easier way to visualize this data is to bucket it together and take averages.

450
00:27:11,020 --> 00:27:15,940
For example this bar here saying among all the points where we had one bit of uncertainty,

451
00:27:15,940 --> 00:27:22,420
on average the number of new guesses required was about 1.5.

452
00:27:22,420 --> 00:27:25,920
And the bar over here saying among all of the different games where at some point the

453
00:27:25,920 --> 00:27:30,480
uncertainty was a little above four bits, which is like narrowing it down to 16 different

454
00:27:30,480 --> 00:27:35,120
possibilities, then on average it requires a little more than two guesses from that point

455
00:27:35,120 --> 00:27:36,240
forward.

456
00:27:36,240 --> 00:27:40,080
And from here I just did a regression to fit a function that seemed reasonable to this.

457
00:27:40,080 --> 00:27:44,160
And remember the whole point of doing any of that is so that we can quantify this intuition

458
00:27:44,160 --> 00:27:49,720
that the more information we gain from a word, the lower the expected score will be.

459
00:27:49,720 --> 00:27:54,380
So with this as version 2.0, if we go back and we run the same set of simulations, having

460
00:27:54,380 --> 00:27:59,820
it play against all 2315 possible wordle answers, how does it do?

461
00:27:59,820 --> 00:28:04,060
Well in contrast to our first version it's definitely better, which is reassuring.

462
00:28:04,060 --> 00:28:08,780
All said and done the average is around 3.6, although unlike the first version there are

463
00:28:08,780 --> 00:28:12,820
a couple times that it loses and requires more than six in this circumstance.

464
00:28:12,820 --> 00:28:15,980
Presumably because there's times when it's making that tradeoff to actually go for the

465
00:28:15,980 --> 00:28:18,980
goal rather than maximizing information.

466
00:28:18,980 --> 00:28:22,140
So can we do better than 3.6?

467
00:28:22,140 --> 00:28:23,260
We definitely can.

468
00:28:23,260 --> 00:28:27,120
Now I said at the start that it's most fun to try not incorporating the true list of

469
00:28:27,120 --> 00:28:29,980
wordle answers into the way that it builds its model.

470
00:28:29,980 --> 00:28:35,180
But if we do incorporate it, the best performance I could get was around 3.43.

471
00:28:35,180 --> 00:28:39,520
So if we try to get more sophisticated than just using word frequency data to choose this

472
00:28:39,520 --> 00:28:44,220
prior distribution, this 3.43 probably gives a max at how good we could get with that,

473
00:28:44,220 --> 00:28:46,360
or at least how good I could get with that.

474
00:28:46,360 --> 00:28:50,240
That best performance essentially just uses the ideas that I've been talking about here,

475
00:28:50,240 --> 00:28:53,400
but it goes a little farther, like it does a search for the expected information two

476
00:28:53,400 --> 00:28:55,660
steps forward rather than just one.

477
00:28:55,660 --> 00:28:58,720
Originally I was planning on talking more about that, but I realize we've actually

478
00:28:58,720 --> 00:29:00,580
gone quite long as it is.

479
00:29:00,580 --> 00:29:03,520
The one thing I'll say is after doing this two-step search and then running a couple

480
00:29:03,520 --> 00:29:07,720
sample simulations in the top candidates, so far for me at least it's looking like

481
00:29:07,720 --> 00:29:09,500
Crane is the best opener.

482
00:29:09,500 --> 00:29:11,080
Who would have guessed?

483
00:29:11,080 --> 00:29:15,680
Also if you use the true wordle list to determine your space of possibilities, then the uncertainty

484
00:29:15,680 --> 00:29:17,920
you start with is a little over 11 bits.

485
00:29:18,160 --> 00:29:22,760
And it turns out, just from a brute force search, the maximum possible expected information

486
00:29:22,760 --> 00:29:26,580
after the first two guesses is around 10 bits.

487
00:29:26,580 --> 00:29:31,720
Which suggests that best case scenario, after your first two guesses, with perfectly optimal

488
00:29:31,720 --> 00:29:35,220
play, you'll be left with around one bit of uncertainty.

489
00:29:35,220 --> 00:29:37,400
Which is the same as being down to two possible guesses.

490
00:29:37,400 --> 00:29:41,440
So I think it's fair and probably pretty conservative to say that you could never possibly

491
00:29:41,440 --> 00:29:45,620
write an algorithm that gets this average as low as 3, because with the words available

492
00:29:45,620 --> 00:29:50,460
to you, there's simply not room to get enough information after only two steps to be able

493
00:29:50,460 --> 00:29:53,820
to guarantee the answer in the third slot every single time without fail.

