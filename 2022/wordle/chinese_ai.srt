1
00:00:00,000 --> 00:00:04,040
Wurdle

2
00:00:04,040 --> 00:00:07,880


3
00:00:07,880 --> 00:00:12,120
游戏在过去一两个月里非常火爆，从来没有人会忽视数学课的机会，我觉得这个游戏是信息论课程中一个非常好的中心例子，特别是一个称为熵的主题。

4
00:00:12,120 --> 00:00:13,120


5
00:00:13,120 --> 00:00:17,120
你看，就像很多人一样，我有点陷入了这个谜题，而且像很多程序员一样，我也陷入了尝试编写一种算法来尽可能最佳地玩游戏。

6
00:00:17,120 --> 00:00:21,200


7
00:00:21,200 --> 00:00:23,200


8
00:00:23,200 --> 00:00:26,400
我想我在这里要做的只是与你们讨论我的一些过程，并解释其中的一些数学，因为整个算法以熵的概念为中心。

9
00:00:26,400 --> 00:00:29,980


10
00:00:29,980 --> 00:00:32,080


11
00:00:32,080 --> 00:00:42,180
首先，如果您还没有听说过，那么什么是 Wurdle？

12
00:00:42,180 --> 00:00:45,380
在我们介绍游戏规则的同时，让我也预览一下我们要做什么，即开发一个基本上可以为我们玩游戏的小算法。

13
00:00:45,380 --> 00:00:48,980


14
00:00:48,980 --> 00:00:51,380


15
00:00:51,380 --> 00:00:54,860
虽然我还没有完成今天的 Wurdle，但现在是 2

16
00:00:54,860 --> 00:00:55,860
月 4 日，我们将看看机器人的表现。

17
00:00:55,860 --> 00:00:59,580
Wurdle

18
00:00:59,580 --> 00:01:00,860
的目标是猜测一个神秘的五个字母单词，您有六次不同的猜测机会。

19
00:01:00,860 --> 00:01:05,240
例如，我的 Wurdle 机器人建议我从猜测起重机开始。

20
00:01:05,240 --> 00:01:09,300
每次您进行猜测时，您都会获得一些有关您的猜测与真实答案的接近程度的信息。

21
00:01:09,300 --> 00:01:10,940


22
00:01:10,940 --> 00:01:14,540
灰色框告诉我实际答案中没有 C。

23
00:01:14,540 --> 00:01:18,340
黄色框告诉我有一个 R，但它不在那个位置。

24
00:01:18,340 --> 00:01:21,820
绿色框告诉我秘密词确实有一个

25
00:01:21,820 --> 00:01:22,820
A，而且位于第三个位置。

26
00:01:22,820 --> 00:01:24,300
然后就没有N了，也没有E了。

27
00:01:24,300 --> 00:01:27,420
那么让我进去告诉 Wurdle 机器人该信息。

28
00:01:27,420 --> 00:01:31,500
我们从起重机开始，有灰色、黄色、绿色、灰色、灰色。

29
00:01:31,500 --> 00:01:35,460
不要担心它现在显示的所有数据，我会在适当的时候解释这一点。

30
00:01:35,460 --> 00:01:39,700
但对于我们的第二个选择来说，它的首要建议是小技巧。

31
00:01:39,700 --> 00:01:43,500
你的猜测确实必须是一个实际的五个字母的单词，但正如你将看到的，它实际上让你猜测的内容相当自由。

32
00:01:43,500 --> 00:01:45,700


33
00:01:45,700 --> 00:01:48,860
在这种情况下，我们尝试一下shtic。

34
00:01:48,860 --> 00:01:50,260
好吧，事情看起来相当不错。

35
00:01:50,260 --> 00:01:54,580
我们按了 S 和 H，所以我们知道前三个字母，我们知道有一个 R。

36
00:01:54,740 --> 00:01:59,740
所以它会像 SHA 某些 R 或 SHA R 某些东西。

37
00:01:59,740 --> 00:02:03,200
看起来 Wurdle

38
00:02:03,200 --> 00:02:05,220
机器人知道它只有两种可能性，要么是碎片，要么是锋利的。

39
00:02:05,220 --> 00:02:08,620
在这一点上，他们之间存在着一种折腾，所以我想可能只是因为它是按字母顺序排列的，所以它与分片相匹配。

40
00:02:08,620 --> 00:02:11,260


41
00:02:11,260 --> 00:02:13,000
万岁，这才是真正的答案。

42
00:02:13,000 --> 00:02:14,660
所以我们三分就搞定了。

43
00:02:14,660 --> 00:02:17,740
如果你想知道这是否有好处，我听到一个人的说法是，对于Wurdle来说，四杆是标准杆，三杆是小鸟球。

44
00:02:17,740 --> 00:02:20,820


45
00:02:20,820 --> 00:02:22,960
我认为这是一个非常恰当的比喻。

46
00:02:22,960 --> 00:02:27,560
你必须坚持自己的比赛才能获得四分，但这当然并不疯狂。

47
00:02:27,560 --> 00:02:30,000
但当你拿到三分的时候，感觉棒极了。

48
00:02:30,000 --> 00:02:33,800
因此，如果您愿意的话，我在这里想做的就是从一开始就谈谈我如何处理 Wurdle

49
00:02:33,800 --> 00:02:36,600
机器人的思考过程。

50
00:02:36,600 --> 00:02:39,800
就像我说的，这确实是信息论课程的借口。

51
00:02:39,800 --> 00:02:43,160
主要目标是解释什么是信息和什么是熵。

52
00:02:48,560 --> 00:02:52,080
在解决这个问题时，我的第一个想法是查看英语中不同字母的相对频率。

53
00:02:52,080 --> 00:02:53,560


54
00:02:53,560 --> 00:02:57,800
所以我想，好吧，是否有一个开局猜测或一对开局猜测可以命中这些最常见的字母？

55
00:02:57,800 --> 00:02:59,960


56
00:02:59,960 --> 00:03:03,780
我非常喜欢做其他事情，然后做指甲。

57
00:03:03,780 --> 00:03:06,980
我们的想法是，如果你击中一个字母，你知道，你会得到绿色或黄色，这总是感觉很好。

58
00:03:06,980 --> 00:03:07,980


59
00:03:07,980 --> 00:03:09,460
感觉就像你正在获取信息。

60
00:03:09,460 --> 00:03:13,140
但在这些情况下，即使你没有击中并且总是得到灰色，这仍然为你提供了大量信息，因为很难找到没有任何这些字母的单词。

61
00:03:13,140 --> 00:03:16,640


62
00:03:16,640 --> 00:03:17,640


63
00:03:17,640 --> 00:03:21,840
但即便如此，这仍然感觉不是超级系统，因为例如，它没有考虑字母的顺序。

64
00:03:21,840 --> 00:03:23,520


65
00:03:23,520 --> 00:03:26,080
当我可以输入蜗牛时，为什么还要输入指甲？

66
00:03:26,080 --> 00:03:27,720
最后加个S是不是更好？

67
00:03:27,720 --> 00:03:28,720
我不太确定。

68
00:03:28,720 --> 00:03:33,500
现在，我的一个朋友说他喜欢用“weary”这个词开头，这让我有点惊讶，因为里面有一些不常见的字母，比如 W

69
00:03:33,500 --> 00:03:37,160
和 Y。

70
00:03:37,160 --> 00:03:39,400
但谁知道呢，也许这是一个更好的开局。

71
00:03:39,400 --> 00:03:43,920
我们是否可以给出某种定量评分来判断潜在猜测的质量？

72
00:03:43,920 --> 00:03:44,920


73
00:03:44,920 --> 00:03:48,640
现在，为了设置我们对可能的猜测进行排名的方式，让我们回顾一下游戏的具体设置方式。

74
00:03:48,640 --> 00:03:51,800


75
00:03:51,800 --> 00:03:55,880
因此，您可以输入一个单词列表，这些单词被视为有效的猜测，长度约为 13,000

76
00:03:55,880 --> 00:03:57,920
个单词。

77
00:03:57,920 --> 00:04:01,560
但当你仔细观察时，你会发现有很多非常不常见的东西，比如头像、阿里和ARG，以及拼字游戏中引发家庭争论的词语。

78
00:04:01,560 --> 00:04:07,040


79
00:04:07,040 --> 00:04:10,600
但游戏的氛围是答案总是一个相当常见的词。

80
00:04:10,600 --> 00:04:16,080
事实上，还有另一个大约 2300 个单词的列表，它们是可能的答案。

81
00:04:16,080 --> 00:04:20,320
这是一个人工策划的列表，我认为是由游戏创建者的女朋友专门制作的，这很有趣。

82
00:04:20,320 --> 00:04:21,800


83
00:04:21,800 --> 00:04:25,560
但我想做的是，我们对这个项目的挑战是看看我们是否可以编写一个解决 Wordle

84
00:04:25,560 --> 00:04:30,720
的程序，该程序不包含有关此列表的先前知识。

85
00:04:30,720 --> 00:04:34,560
一方面，有很多非常常见的五个字母单词您在该列表中找不到。

86
00:04:34,560 --> 00:04:35,560


87
00:04:35,560 --> 00:04:38,360
因此，最好编写一个更具弹性的程序，并且可以与任何人玩

88
00:04:38,360 --> 00:04:41,960
Wordle，而不仅仅是官方网站。

89
00:04:41,960 --> 00:04:45,900
我们之所以知道这个可能答案的列表是什么，是因为它在源代码中可见。

90
00:04:45,900 --> 00:04:47,440


91
00:04:47,440 --> 00:04:51,620
但它在源代码中可见的方式是按照每天出现答案的特定顺序。

92
00:04:51,620 --> 00:04:52,840


93
00:04:52,840 --> 00:04:56,400
所以你总是可以查找明天的答案。

94
00:04:56,400 --> 00:04:59,140
很明显，使用该列表在某种程度上是作弊行为。

95
00:04:59,140 --> 00:05:02,900
使谜题更有趣、信息论课程更丰富的方法是使用一些更通用的数据，例如相对词频，来捕捉对更常见词的偏好的直觉。

96
00:05:02,900 --> 00:05:07,640


97
00:05:07,640 --> 00:05:11,640


98
00:05:11,640 --> 00:05:16,560
那么在这13000种可能性中，我们应该如何选择开局猜测呢？

99
00:05:16,560 --> 00:05:19,960
例如，如果我的朋友提出厌烦，我们应该如何分析它的质量？

100
00:05:19,960 --> 00:05:25,040
好吧，他说他喜欢那个不太可能的 W

101
00:05:25,040 --> 00:05:27,880
的原因是他喜欢远射的性质，如果你击中了那个 W，那感觉是多么好。

102
00:05:27,880 --> 00:05:31,400
例如，如果第一个揭示的模式是这样的，那么这个庞大的词典中只有 58

103
00:05:31,400 --> 00:05:36,080
个单词与该模式匹配。

104
00:05:36,080 --> 00:05:38,900
与 13,000 人相比，这是一个巨大的减少。

105
00:05:38,900 --> 00:05:43,320
但当然，另一方面是，获得这样的模式非常罕见。

106
00:05:43,360 --> 00:05:47,600
具体来说，如果每个单词作为答案的可能性相同，则达到此模式的概率将为 58

107
00:05:47,600 --> 00:05:51,680
除以大约 13,000。

108
00:05:51,680 --> 00:05:53,880
当然，它们成为答案的可能性并不相同。

109
00:05:53,880 --> 00:05:56,680
其中大部分都是非常晦涩甚至有问题的词语。

110
00:05:56,680 --> 00:05:59,560
但至少对于我们第一次通过这一切，我们假设它们的可能性相同，然后稍后再对其进行完善。

111
00:05:59,560 --> 00:06:02,040


112
00:06:02,040 --> 00:06:07,360
关键是，包含大量信息的模式本质上不太可能发生。

113
00:06:07,360 --> 00:06:11,320
事实上，提供信息意味着这是不可能的。

114
00:06:11,920 --> 00:06:16,720
在这个开口中看到的更可能的模式是这样的，当然其中没有

115
00:06:16,720 --> 00:06:18,360
W。

116
00:06:18,360 --> 00:06:22,080
也许有E，也许没有A，没有R，没有Y。

117
00:06:22,080 --> 00:06:24,640
在本例中，有 1400 个可能的匹配项。

118
00:06:24,640 --> 00:06:29,600
如果所有可能性均等，则您看到的模式的概率约为

119
00:06:29,600 --> 00:06:30,680
11%。

120
00:06:30,680 --> 00:06:34,320
因此，最可能的结果也是信息最少的。

121
00:06:34,320 --> 00:06:38,440
为了获得更全面的视角，让我向您展示您可能看到的所有不同模式的概率的完整分布。

122
00:06:38,440 --> 00:06:42,000


123
00:06:42,000 --> 00:06:46,000
因此，您看到的每个条形都对应于可能显示的颜色模式，其中有 3

124
00:06:46,000 --> 00:06:50,500
到 5

125
00:06:50,500 --> 00:06:52,960
种可能性，并且它们从左到右、最常见到最不常见进行组织。

126
00:06:52,960 --> 00:06:56,200
所以这里最常见的可能性是你得到的都是灰色的。

127
00:06:56,200 --> 00:06:58,800
这种情况发生的概率约为 14%。

128
00:06:58,800 --> 00:07:02,040
当你进行猜测时，你所希望的是你最终会出现在这条长尾中的某个地方，就像在这里，与这个显然看起来像这样的模式相匹配的可能性只有

129
00:07:02,040 --> 00:07:06,360
18

130
00:07:06,360 --> 00:07:09,920
种。

131
00:07:09,920 --> 00:07:14,080
或者，如果我们冒险向左走一点，你知道，也许我们会一直走到这里。

132
00:07:14,080 --> 00:07:16,560
好的，这是给你的一个很好的谜题。

133
00:07:16,560 --> 00:07:20,600
英语中以 W 开头、以 Y

134
00:07:20,600 --> 00:07:22,040
结尾、其中某个位置有 R 的三个单词是什么？

135
00:07:22,040 --> 00:07:27,560
事实证明，让我们看看，答案是罗嗦、狡猾和讽刺的。

136
00:07:27,560 --> 00:07:32,720
因此，为了判断这个词的整体效果如何，我们需要某种方式来衡量您将从该分布中获得的预期信息量。

137
00:07:32,720 --> 00:07:35,720


138
00:07:36,360 --> 00:07:41,080
如果我们检查每个模式，并将其发生的概率乘以衡量其信息量的因素，这也许可以给我们一个客观的分数。

139
00:07:41,080 --> 00:07:46,000


140
00:07:46,000 --> 00:07:50,280
现在，您对某事物应该是什么的第一直觉可能是匹配的数量。

141
00:07:50,280 --> 00:07:52,960
您想要较低的平均匹配数。

142
00:07:52,960 --> 00:07:57,400
但相反，我想使用一种更通用的衡量标准，我们通常将其归因于信息，并且一旦我们为这

143
00:07:57,400 --> 00:08:01,040
13,000

144
00:08:01,040 --> 00:08:04,320
个单词中的每一个分配不同的概率来判断它们是否真正是答案，这种衡量标准就会更加灵活。

145
00:08:10,600 --> 00:08:14,760
信息的标准单位是位，它的公式有点有趣，但如果我们只看例子，它真的很直观。

146
00:08:14,760 --> 00:08:17,800


147
00:08:17,800 --> 00:08:21,880
如果你的观察结果将你的可能性空间减少了一半，我们就说它只有一点信息。

148
00:08:21,880 --> 00:08:24,200


149
00:08:24,200 --> 00:08:27,680
在我们的示例中，可能性空间是所有可能的单词，结果表明，五个字母的单词中大约一半有

150
00:08:27,760 --> 00:08:31,560
S，比这个少一点，但大约一半。

151
00:08:31,560 --> 00:08:35,200
这样观察就会给你一点信息。

152
00:08:35,200 --> 00:08:39,640
相反，如果一个新事实将可能性空间减少了四倍，我们就说它有两位信息。

153
00:08:39,640 --> 00:08:42,000


154
00:08:42,000 --> 00:08:45,120
例如，事实证明这些单词中大约四分之一有 T。

155
00:08:45,120 --> 00:08:49,720
如果观察将该空间缩小八分之一，我们就说它是三位信息，依此类推。

156
00:08:49,720 --> 00:08:50,920


157
00:08:50,920 --> 00:08:55,000
四位将其切割为 16 度，五位将其切割为 32 度。

158
00:08:55,000 --> 00:09:00,160
所以现在您可能想停下来问自己，就发生概率而言，比特数的信息公式是什么？

159
00:09:00,160 --> 00:09:04,520


160
00:09:04,520 --> 00:09:07,920
我们在这里所说的是，当你取位数的二分之一时，这与概率是一样的，这与说 2

161
00:09:07,920 --> 00:09:11,680
的位数次方等于概率的

162
00:09:11,680 --> 00:09:16,200
1

163
00:09:16,200 --> 00:09:19,680
是一样的，重新排列进一步表示该信息是一的对数基数除以概率。

164
00:09:19,680 --> 00:09:23,200
有时您还会看到这种情况，还需要进行一次重新排列，其中信息是以概率的负对数为底的二。

165
00:09:23,200 --> 00:09:25,680


166
00:09:25,680 --> 00:09:29,120
这样表达对于外行来说可能有点奇怪，但这确实是一个非常直观的想法，即询问您已将可能性减少了一半的次数。

167
00:09:29,120 --> 00:09:33,400


168
00:09:33,400 --> 00:09:35,120


169
00:09:35,120 --> 00:09:37,840
现在，如果您想知道，您知道，我以为我们只是在玩一个有趣的文字游戏，为什么要使用对数？

170
00:09:37,840 --> 00:09:39,920


171
00:09:39,920 --> 00:09:43,920
这是一个更好的单元的原因之一是，谈论非常不可能的事件要容易得多，说一个观察有 20

172
00:09:43,920 --> 00:09:48,120
位信息比说这样那样发生的概率为 0

173
00:09:48,120 --> 00:09:53,480
容易得多。 0000095。

174
00:09:53,480 --> 00:09:57,360
但这种对数表达式被证明是对概率论非常有用的补充，更实质性的原因是信息相加的方式。

175
00:09:57,360 --> 00:10:02,000


176
00:10:02,000 --> 00:10:05,560
例如，如果一个观察结果为您提供了两位信息，将您的空间缩小了四分之二，然后第二个观察结果（如

177
00:10:05,560 --> 00:10:10,120
Wordle

178
00:10:10,120 --> 00:10:14,480
中的第二次猜测）为您提供了另外三位信息，将您的空间进一步缩小了八倍，则两个一起给你五位信息。

179
00:10:14,480 --> 00:10:17,360


180
00:10:17,360 --> 00:10:21,200
就像概率喜欢乘法一样，信息喜欢增加。

181
00:10:21,200 --> 00:10:24,920
因此，一旦我们处于预期值之类的领域，我们将一堆数字相加，日志就会使其更容易处理。

182
00:10:24,920 --> 00:10:28,660


183
00:10:28,660 --> 00:10:32,600
让我们回到 Weary

184
00:10:32,600 --> 00:10:35,560
的发行版，并在此处添加另一个小跟踪器，向我们展示每种模式有多少信息。

185
00:10:35,560 --> 00:10:38,760
我想让你注意到的主要一点是，我们获得那些更有可能的模式的概率越高，信息越少，你获得的位数就越少。

186
00:10:38,760 --> 00:10:43,500


187
00:10:43,500 --> 00:10:47,360
我们衡量这种猜测质量的方法是获取该信息的期望值，我们遍历每个模式，我们说它的可能性有多大，然后我们将其乘以我们获得的信息位数。

188
00:10:47,360 --> 00:10:51,620


189
00:10:51,620 --> 00:10:54,940


190
00:10:54,940 --> 00:10:58,480
在 Weary 的例子中，结果是 4。 9 位。

191
00:10:58,480 --> 00:11:02,800
因此，平均而言，您从这个开局猜测中获得的信息相当于将您的可能性空间切成两半（大约五倍）。

192
00:11:02,800 --> 00:11:05,660


193
00:11:05,660 --> 00:11:10,260
相比之下，具有较高预期信息值的猜测的示例类似于

194
00:11:10,260 --> 00:11:13,220
Slate。

195
00:11:13,220 --> 00:11:16,180
在这种情况下，您会注意到分布看起来更加平坦。

196
00:11:16,180 --> 00:11:20,780
特别是，所有灰色中最有可能出现的概率只有 6% 左右，因此至少明显会得到

197
00:11:20,780 --> 00:11:25,940
3。 9位信息。

198
00:11:25,940 --> 00:11:29,140
但这是最低限度，更常见的是你会得到比这更好的东西。

199
00:11:29,140 --> 00:11:33,380
事实证明，当你计算这个数字并将所有相关术语加起来时，平均信息约为 5。

200
00:11:33,380 --> 00:11:36,420
8.

201
00:11:36,420 --> 00:11:42,140
因此，与《Weary》相比，平均而言，在第一次猜测之后，你的可能性空间大约只有一半大。

202
00:11:42,140 --> 00:11:43,940


203
00:11:43,940 --> 00:11:49,540
关于信息量期望值的名称，实际上有一个有趣的故事。

204
00:11:49,540 --> 00:11:52,580
信息论是由 20 世纪

205
00:11:52,580 --> 00:11:57,620
40 年代在贝尔实验室工作的克劳德·香农 (Claude

206
00:11:57,620 --> 00:12:01,500
Shannon) 提出的，但他正在与约翰·冯·诺依曼 (John

207
00:12:01,500 --> 00:12:04,180
von Neumann) 谈论他尚未发表的一些想法，约翰·冯·诺依曼是当时非常杰出的知识巨人。数学和物理以及计算机科学的开端。

208
00:12:04,180 --> 00:12:07,260
当冯·诺依曼提到他对于信息量的期望值并没有一个好名字时，据说，所以故事是这样的，你应该称之为熵，有两个原因。

209
00:12:07,260 --> 00:12:12,540


210
00:12:12,540 --> 00:12:14,720


211
00:12:14,720 --> 00:12:18,400
首先，你的不确定性函数已经在统计力学中以这个名字使用了，所以它已经有一个名字了，其次，更重要的是，没有人知道熵到底是什么，所以在辩论中你总是会知道熵是什么。有优势。

212
00:12:18,400 --> 00:12:23,100


213
00:12:23,100 --> 00:12:26,940


214
00:12:26,940 --> 00:12:31,420
因此，如果这个名字看起来有点神秘，并且如果这个故事可信的话，那就是有意为之。

215
00:12:31,420 --> 00:12:33,420


216
00:12:33,420 --> 00:12:36,740
另外，如果您想知道它与物理学中所有热力学第二定律的关系，那么肯定存在某种联系，但在其起源中，香农只是处理纯概率论，并且出于我们的目的，当我使用熵这个词，我只是想让你思考一个特定猜测的预期信息值。

217
00:12:36,740 --> 00:12:40,820


218
00:12:40,820 --> 00:12:44,780


219
00:12:44,780 --> 00:12:49,340


220
00:12:49,340 --> 00:12:50,820


221
00:12:50,820 --> 00:12:54,380
您可以将熵视为同时测量两个事物。

222
00:12:54,380 --> 00:12:57,420
第一个是分布的平坦程度。

223
00:12:57,420 --> 00:13:01,700
分布越接近均匀，熵就越高。

224
00:13:01,700 --> 00:13:06,340
在我们的例子中，总共有 3 到 5 个模式，对于均匀分布，观察其中任何一个模式都会有 3

225
00:13:06,340 --> 00:13:11,340
到 5 个的信息对数基数 2，恰好是 7。

226
00:13:11,340 --> 00:13:17,860
92，所以这是该熵可能具有的绝对最大值。

227
00:13:17,860 --> 00:13:21,900
但熵首先也是一种衡量可能性的方法。

228
00:13:21,900 --> 00:13:22,900


229
00:13:22,900 --> 00:13:26,980
例如，如果您碰巧有某个单词，其中只有 16 种可能的模式，并且每种模式的可能性相同，则该熵（即该预期信息）将是

230
00:13:26,980 --> 00:13:32,760
4 位。

231
00:13:32,760 --> 00:13:36,880
但如果你有另一个词，其中可能出现 64 种可能的模式，并且它们的可能性相同，那么熵将是

232
00:13:36,880 --> 00:13:41,000
6 位。

233
00:13:41,000 --> 00:13:45,800
因此，如果您在野外看到某个分布的熵为 6

234
00:13:45,800 --> 00:13:50,000
位，这就有点像是在说即将发生的事情存在同样多的变化和不确定性，就好像有 64

235
00:13:50,000 --> 00:13:54,400
个同样可能的结果一样。

236
00:13:54,400 --> 00:13:58,360
对于我第一次使用 Wurtelebot，我基本上就是让它这样做。

237
00:13:58,360 --> 00:14:03,560
它会遍历所有可能的猜测，即所有

238
00:14:03,560 --> 00:14:08,580
13,000

239
00:14:08,580 --> 00:14:13,040
个单词，计算每个单词的熵，或者更具体地说，计算您可能看到的所有模式中每个单词的分布熵，并选择最高的，因为这是一个可能会尽可能地削减你的可能性空间的人。

240
00:14:13,040 --> 00:14:17,200


241
00:14:17,200 --> 00:14:20,120
尽管我在这里只讨论了第一个猜测，但它对于接下来的几次猜测也起到了同样的作用。

242
00:14:20,120 --> 00:14:21,680


243
00:14:21,680 --> 00:14:25,100
例如，在您看到第一个猜测的某些模式后，这将根据与之匹配的内容将您限制为较少数量的可能单词，您只需针对该较小的单词集玩相同的游戏即可。

244
00:14:25,100 --> 00:14:29,300


245
00:14:29,300 --> 00:14:32,300


246
00:14:32,300 --> 00:14:36,500
对于建议的第二个猜测，您会查看从一组更受限制的单词中可能出现的所有模式的分布，搜索所有

247
00:14:36,500 --> 00:14:41,540
13,000

248
00:14:41,540 --> 00:14:45,480
种可能性，然后找到使熵最大化的一种。

249
00:14:45,480 --> 00:14:48,980
为了向您展示这是如何实际工作的，让我拿出我编写的 Wurtele

250
00:14:48,980 --> 00:14:54,060
的一个小变体，它在页边空白处显示了此分析的亮点。

251
00:14:54,460 --> 00:14:57,820
完成所有熵计算后，右侧向我们展示了哪些具有最高的预期信息。

252
00:14:57,820 --> 00:15:00,340


253
00:15:00,340 --> 00:15:04,940
事实证明，至少目前最重要的答案是稗子，我们稍后会对此进行完善，这意味着，嗯，当然是野豌豆，最常见的野豌豆。

254
00:15:04,940 --> 00:15:11,140


255
00:15:11,140 --> 00:15:14,180
每次我们在这里进行猜测时，也许我会忽略它的建议并选择

256
00:15:14,180 --> 00:15:19,220
slate，因为我喜欢

257
00:15:19,220 --> 00:15:23,300
slate，我们可以看到它有多少预期信息，但是在这个词的右侧，它向我们展示了多少信息考虑到这种特定的模式，我们得到的实际信息。

258
00:15:23,340 --> 00:15:24,980


259
00:15:24,980 --> 00:15:28,660
所以看起来我们有点不走运，我们预计会得到 5 个。 8，但我们碰巧得到的东西比这个少。

260
00:15:28,660 --> 00:15:30,660


261
00:15:30,660 --> 00:15:34,020
然后在左侧，它向我们展示了当前所处位置的所有不同可能的单词。

262
00:15:34,020 --> 00:15:35,860


263
00:15:35,860 --> 00:15:39,820
蓝色条告诉我们它认为每个单词出现的可能性有多大，因此目前它假设每个单词出现的可能性相同，但我们稍后会对其进行改进。

264
00:15:39,820 --> 00:15:44,140


265
00:15:44,140 --> 00:15:48,580
然后，这种不确定性测量告诉我们可能单词的分布熵，因为它是均匀分布，所以现在只是一种不必要的复杂方法来计算可能性的数量。

266
00:15:48,580 --> 00:15:53,220


267
00:15:53,300 --> 00:15:55,940


268
00:15:55,940 --> 00:16:01,700
例如，如果我们要计算 2 的 13 次方。 66，这应该是大约

269
00:16:01,700 --> 00:16:02,700
13,000 种可能性。

270
00:16:02,700 --> 00:16:06,780
我在这里有点偏离，但这只是因为我没有显示所有小数位。

271
00:16:06,780 --> 00:16:10,260
目前，这可能感觉多余，而且好像事情过于复杂，但您很快就会明白为什么同时拥有这两个数字是有用的。

272
00:16:10,260 --> 00:16:12,780


273
00:16:12,780 --> 00:16:16,780
所以这里看起来它表明我们第二个猜测的最高熵是拉面，这又感觉不像一个词。

274
00:16:16,780 --> 00:16:19,700


275
00:16:19,700 --> 00:16:25,660
因此，为了占据道德制高点，我将继续输入 Rains。

276
00:16:25,660 --> 00:16:27,540
看来我们又有点不走运了。

277
00:16:27,540 --> 00:16:32,100
我们本来期待4。 3 位，但我们只得到了 3 位。 39位信息。

278
00:16:32,100 --> 00:16:35,060
这样一来，我们就有 55 种可能性。

279
00:16:35,060 --> 00:16:38,860
在这里，也许我实际上会遵循它的建议，即组合，无论这意味着什么。

280
00:16:38,860 --> 00:16:40,200


281
00:16:40,200 --> 00:16:43,300
好吧，这实际上是一个解谜的好机会。

282
00:16:43,300 --> 00:16:47,020
它告诉我们这个模式给了我们 4。 7 位信息。

283
00:16:47,020 --> 00:16:52,400
但在左边，在我们看到该模式之前，有 5 个。 78 位不确定性。

284
00:16:52,400 --> 00:16:56,860
那么作为对你的一个测验，剩余可能性的数量意味着什么？

285
00:16:56,860 --> 00:17:02,280
嗯，这意味着我们将不确定性减少到一点点，这与说有两个可能的答案是一样的。

286
00:17:02,280 --> 00:17:04,700


287
00:17:04,700 --> 00:17:06,520
这是50-50的选择。

288
00:17:06,520 --> 00:17:09,860
从这里开始，因为你和我知道哪些词更常见，所以我们知道答案应该是深渊。

289
00:17:09,860 --> 00:17:11,220


290
00:17:11,220 --> 00:17:13,540
但正如现在所写的，程序并不知道这一点。

291
00:17:13,540 --> 00:17:17,560
所以它会继续前进，尝试获取尽可能多的信息，直到只剩下一种可能性，然后它就会猜测它。

292
00:17:17,560 --> 00:17:20,360


293
00:17:20,360 --> 00:17:22,700
显然我们需要更好的残局策略。

294
00:17:22,700 --> 00:17:26,540
但是，假设我们将此版本称为我们的 wordle

295
00:17:26,540 --> 00:17:30,740
求解器之一，然后我们运行一些模拟来看看它是如何工作的。

296
00:17:30,740 --> 00:17:34,240
所以它的工作方式是玩所有可能的文字游戏。

297
00:17:34,240 --> 00:17:38,780
它会检查所有 2315 个单词，这些单词是实际的单词答案。

298
00:17:38,780 --> 00:17:41,340
它基本上使用它作为测试集。

299
00:17:41,340 --> 00:17:45,820
采用这种天真的方法，不考虑一个词的常见程度，只是试图在每一步中最大化信息，直到它只剩下一个选择。

300
00:17:45,820 --> 00:17:50,480


301
00:17:50,480 --> 00:17:55,100
模拟结束时，平均得分约为 4。 124.

302
00:17:55,100 --> 00:17:59,780
老实说，这还不错，我本来以为会做得更糟。

303
00:17:59,780 --> 00:18:03,040
但玩wordle的人会告诉你，他们通常可以在4内得到它。

304
00:18:03,040 --> 00:18:05,260
真正的挑战是尽可能多地获得三分。

305
00:18:05,260 --> 00:18:08,920
4分和3分之间的差距相当大。

306
00:18:08,920 --> 00:18:13,300
这里显而易见的容易实现的目标是以某种方式纳入一个单词是否常见，以及我们到底如何做到这一点。

307
00:18:13,300 --> 00:18:23,160


308
00:18:23,160 --> 00:18:26,860
我的方法是获取英语中所有单词的相对频率列表。

309
00:18:26,860 --> 00:18:28,560


310
00:18:28,560 --> 00:18:32,560
我刚刚使用了 Mathematica 的词频数据函数，它本身是从 Google

311
00:18:32,560 --> 00:18:35,520
Books English Ngram 公共数据集中提取的。

312
00:18:35,520 --> 00:18:38,680
看起来很有趣，例如，如果我们将其从最常见的单词到最不常见的单词进行排序。

313
00:18:38,680 --> 00:18:40,120


314
00:18:40,120 --> 00:18:43,740
显然，这些是英语中最常见的 5 个字母单词。

315
00:18:43,740 --> 00:18:46,480
或者更确切地说，这些是第八个最常见的。

316
00:18:46,480 --> 00:18:49,440
首先是which，然后是there和there。

317
00:18:49,440 --> 00:18:53,020
First本身不是first，而是9th，并且这些其他词可能更频繁地出现是有道理的，其中first之后的词是after、where，而那些词则不太常见。

318
00:18:53,020 --> 00:18:57,840


319
00:18:57,840 --> 00:18:59,000


320
00:18:59,000 --> 00:19:04,400
现在，在使用这些数据来模拟每个单词成为最终答案的可能性时，它不应该仅仅与频率成正比。

321
00:19:04,400 --> 00:19:06,760


322
00:19:07,020 --> 00:19:12,560
例如，得分为 0。 002 在此数据集中，而“braid”一词在某种意义上的可能性要小

323
00:19:12,560 --> 00:19:15,200
1000 倍。

324
00:19:15,200 --> 00:19:19,400
但这两个词都很常见，几乎肯定值得考虑。

325
00:19:19,400 --> 00:19:21,900
所以我们想要更多的二元截止。

326
00:19:21,900 --> 00:19:26,520
我的方法是想象一下将整个排序的单词列表，然后将其排列在 x

327
00:19:26,520 --> 00:19:31,060
轴上，然后应用 sigmoid

328
00:19:31,060 --> 00:19:35,540
函数，这是输出基本上是二进制的函数的标准方法，它是要么是 0，要么是

329
00:19:35,540 --> 00:19:38,500
1，但对于该不确定区域，中间有一个平滑。

330
00:19:38,500 --> 00:19:43,900
所以本质上，我分配给每个单词出现在最终列表中的概率将是上面的 sigmoid 函数的值，无论它位于

331
00:19:43,900 --> 00:19:49,540
x 轴上。

332
00:19:49,540 --> 00:19:53,940
显然，这取决于几个参数，例如，这些单词在 x 轴上填充的空间有多宽，决定了我们从

333
00:19:53,940 --> 00:19:59,660
1 到

334
00:19:59,660 --> 00:20:03,000
0 的逐渐或陡峭程度，以及我们将它们从左到右放置的位置决定了截止值。

335
00:20:03,160 --> 00:20:07,340
说实话，我的做法就是舔手指然后把它插到风里。

336
00:20:07,340 --> 00:20:10,800
我查看了排序后的列表，并试图找到一个窗口，当我查看它时，我认为这些单词中的一半更有可能是最终答案，并将其用作截止值。

337
00:20:10,800 --> 00:20:15,280


338
00:20:15,280 --> 00:20:17,680


339
00:20:17,680 --> 00:20:21,840
一旦我们在单词之间有了这样的分布，它就会给我们带来另一种情况，即熵成为这种真正有用的度量。

340
00:20:21,840 --> 00:20:24,460


341
00:20:24,460 --> 00:20:28,480
例如，假设我们正在玩一个游戏，我们从我的旧开场白开始，即羽毛和指甲，我们最终会遇到有四个可能的单词与之匹配的情况。

342
00:20:28,480 --> 00:20:32,480


343
00:20:32,480 --> 00:20:33,760


344
00:20:33,760 --> 00:20:36,440
假设我们认为它们都有相同的可能性。

345
00:20:36,440 --> 00:20:40,000
我问你，这个分布的熵是多少？

346
00:20:40,000 --> 00:20:45,920
嗯，与这些可能性中的每一种相关的信息将是 4 的以 2 为底的对数，因为每一种都是

347
00:20:45,920 --> 00:20:50,800
1 和 4，那就是 2。

348
00:20:50,800 --> 00:20:52,780
两位信息，四种可能性。

349
00:20:52,780 --> 00:20:54,360
一切都很好。

350
00:20:54,360 --> 00:20:58,320
但如果我告诉你实际上有超过四场比赛呢？

351
00:20:58,320 --> 00:21:02,600
事实上，当我们查看完整的单词列表时，有 16 个单词与其匹配。

352
00:21:02,600 --> 00:21:07,260
但假设我们的模型对其他 12

353
00:21:07,260 --> 00:21:11,440
个单词实际成为最终答案的概率非常低，大约是千分之一，因为它们真的很晦涩。

354
00:21:11,440 --> 00:21:15,480
现在我问你，这个分布的熵是多少？

355
00:21:15,480 --> 00:21:19,600
如果熵在这里纯粹测量匹配的数量，那么您可能会期望它类似于 16

356
00:21:19,600 --> 00:21:24,760
的以 2

357
00:21:24,760 --> 00:21:26,200
为底的对数，即 4，比我们之前的不确定性多了两位。

358
00:21:26,200 --> 00:21:30,320
但当然，实际的不确定性与我们之前的情况并没有太大不同。

359
00:21:30,320 --> 00:21:33,840
例如，仅仅因为有这 12

360
00:21:33,840 --> 00:21:38,200
个非常晦涩的单词并不意味着当得知最终答案是“魅力”时会更加令人惊讶。

361
00:21:38,200 --> 00:21:42,080
所以当你在这里实际进行计算时，将每次出现的概率乘以相应的信息相加，得到的就是 2。

362
00:21:42,080 --> 00:21:45,960
11 位。

363
00:21:45,960 --> 00:21:50,280
我只是说，它基本上是两位，基本上是这四种可能性，但是由于所有这些极不可能发生的事件，存在更多的不确定性，尽管如果你确实了解了它们，你会从中获得大量信息。

364
00:21:50,280 --> 00:21:54,240


365
00:21:54,240 --> 00:21:57,120


366
00:21:57,120 --> 00:22:00,800
缩小范围，这就是 Wordle

367
00:22:00,800 --> 00:22:01,800
成为信息论课程的一个很好的例子的部分原因。

368
00:22:01,800 --> 00:22:05,280
我们对熵有两种不同的感觉应用。

369
00:22:05,280 --> 00:22:09,640
第一个告诉我们从给定的猜测中得到的预期信息是什么，第二个告诉我们我们是否可以衡量所有可能的单词中剩余的不确定性。

370
00:22:09,640 --> 00:22:14,560


371
00:22:14,560 --> 00:22:16,480


372
00:22:16,480 --> 00:22:19,800
我应该强调，在第一种情况下，我们正在查看猜测的预期信息，一旦我们对单词的权重不相等，就会影响熵计算。

373
00:22:19,800 --> 00:22:25,000


374
00:22:25,000 --> 00:22:28,600
例如，让我拿出我们之前查看的与

375
00:22:28,600 --> 00:22:33,560
Weary

376
00:22:33,560 --> 00:22:34,560
相关的分布的相同案例，但这次在所有可能的单词中使用非均匀分布。

377
00:22:34,560 --> 00:22:39,360
所以让我看看是否可以在这里找到一个很好地说明它的部分。

378
00:22:39,360 --> 00:22:42,480
好吧，这里这很好。

379
00:22:42,480 --> 00:22:46,360
这里我们有两个相邻的模式，它们的可能性大致相同，但我们被告知其中一个模式有 32

380
00:22:46,360 --> 00:22:49,480
个可能的单词与其匹配。

381
00:22:49,480 --> 00:22:54,080
如果我们检查它们是什么，那就是 32

382
00:22:54,080 --> 00:22:55,600
个，当你扫视它们时，它们都只是非常不可能的单词。

383
00:22:55,600 --> 00:23:00,400
很难找到任何看似合理的答案，也许会大喊大叫，但如果我们查看分布中的相邻模式，这被认为是同样可能的，我们被告知它只有

384
00:23:00,400 --> 00:23:04,440
8

385
00:23:04,440 --> 00:23:08,920
个可能的匹配，所以四分之一很多比赛，但可能性差不多。

386
00:23:08,920 --> 00:23:09,920


387
00:23:09,920 --> 00:23:12,520
当我们拿出这些匹配项时，我们就能明白原因了。

388
00:23:12,520 --> 00:23:17,840
其中一些是实际合理的答案，例如戒指、愤怒或说唱。

389
00:23:17,840 --> 00:23:22,000
为了说明我们如何整合所有这些，让我在这里列出 Wordlebot 的第

390
00:23:22,000 --> 00:23:25,960
2 版，它与我们看到的第一个版本有两三个主要区别。

391
00:23:25,960 --> 00:23:29,460
首先，就像我刚才说的，我们计算这些熵、这些信息的预期值的方式现在正在使用跨模式的更精细的分布，其中包含给定单词实际上是答案的概率。

392
00:23:29,460 --> 00:23:34,800


393
00:23:34,800 --> 00:23:39,300


394
00:23:39,300 --> 00:23:44,160
碰巧，眼泪仍然是第一位，尽管接下来的有点不同。

395
00:23:44,160 --> 00:23:47,920
其次，当它对首选进行排名时，它现在将保留每个单词是实际答案的概率模型，并将其纳入其决策中，一旦我们对答案有了一些猜测，就更容易看到这一点。桌子。

396
00:23:47,920 --> 00:23:52,600


397
00:23:52,600 --> 00:23:55,520


398
00:23:55,520 --> 00:24:01,120
再次忽略它的建议，因为我们不能让机器统治我们的生活。

399
00:24:01,120 --> 00:24:05,160
我想我应该提到另一件不同的事情是在左边，不确定性值，即位数，不再只是与可能匹配的数量冗余。

400
00:24:05,160 --> 00:24:10,080


401
00:24:10,080 --> 00:24:16,520
现在，如果我们将其拉高并计算 2 的 8。 02，略高于

402
00:24:16,520 --> 00:24:22,640
256，我猜是 259，它的意思是，尽管总共有

403
00:24:22,640 --> 00:24:26,400
526 个单词实际上与此模式匹配，但它所具有的不确定性量更类似于如果有

404
00:24:26,400 --> 00:24:29,760
259 个同样可能的单词结果。

405
00:24:29,760 --> 00:24:31,100
你可以这样想。

406
00:24:31,100 --> 00:24:35,560
它知道 borx 不是答案，与 yorts、zorl

407
00:24:35,560 --> 00:24:37,840
和 zorus 一样，因此它的不确定性比之前的情况要小一些。

408
00:24:37,840 --> 00:24:40,220
这个位数会更小。

409
00:24:40,220 --> 00:24:44,040
如果我继续玩这个游戏，我会用一些与我想在这里解释的内容相符的猜测来完善这个游戏。

410
00:24:44,040 --> 00:24:48,680


411
00:24:48,680 --> 00:24:52,520
通过第四种猜测，如果你看一下它的首选，你会发现它不再只是最大化熵。

412
00:24:52,520 --> 00:24:53,800


413
00:24:53,800 --> 00:24:58,480
所以在这一点上，技术上有七种可能性，但唯一有意义的机会是宿舍和单词。

414
00:24:58,480 --> 00:25:00,780


415
00:25:00,780 --> 00:25:04,760
您可以看到它对选择这两个值的排名高于所有其他值，严格来说，这会提供更多信息。

416
00:25:04,760 --> 00:25:07,560


417
00:25:07,560 --> 00:25:11,200
我第一次这样做时，我只是将这两个数字相加来衡量每个猜测的质量，这实际上比你想象的要好。

418
00:25:11,200 --> 00:25:14,580


419
00:25:14,580 --> 00:25:17,600
但它确实感觉不系统，而且我确信人们可以采取其他方法，但这是我找到的方法。

420
00:25:17,600 --> 00:25:19,880


421
00:25:19,880 --> 00:25:24,200
如果我们正在考虑下一次猜测的前景，就像在这种情况下的话，我们真正关心的是如果我们这样做的话我们游戏的预期得分。

422
00:25:24,200 --> 00:25:28,440


423
00:25:28,440 --> 00:25:32,880
为了计算预期分数，我们会计算单词是实际答案的概率是多少，目前描述为

424
00:25:32,880 --> 00:25:35,640
58%。

425
00:25:36,080 --> 00:25:40,400
我们说，有 58% 的机会，我们在这场比赛中得分为 4。

426
00:25:40,400 --> 00:25:46,240
然后，以 1 减去 58% 的概率，我们的分数将大于 4。

427
00:25:46,240 --> 00:25:50,640
我们不知道还有多少，但我们可以根据到达这一点后可能存在的不确定性来估计。

428
00:25:50,640 --> 00:25:52,920


429
00:25:52,920 --> 00:25:56,600
具体来说，目前有1。 44 位不确定性。

430
00:25:56,600 --> 00:26:01,560
如果我们猜测单词，它会告诉我们预期得到的信息是 1。 27 位。

431
00:26:01,560 --> 00:26:06,280
因此，如果我们猜测单词，这种差异代表了在这种情况发生后我们可能会留下多少不确定性。

432
00:26:06,280 --> 00:26:08,280


433
00:26:08,280 --> 00:26:12,500
我们需要的是某种函数，我在这里称之为

434
00:26:12,500 --> 00:26:13,880
f，它将这种不确定性与预期分数联系起来。

435
00:26:13,880 --> 00:26:18,040
它的处理方式是根据机器人的版本

436
00:26:18,040 --> 00:26:23,920
1

437
00:26:23,920 --> 00:26:27,040
绘制之前游戏的一堆数据，以说明在具有某些非常可测量的不确定性的各个点之后的实际得分是多少。

438
00:26:27,040 --> 00:26:31,120
例如，这里的这些数据点位于 8 左右的值之上。 在8分之后的一些比赛中，大约有7分左右。

439
00:26:31,120 --> 00:26:36,840
7位不确定性，经过两次猜测才得到最终答案。

440
00:26:36,840 --> 00:26:39,340


441
00:26:39,340 --> 00:26:43,180
对于其他游戏需要猜测三次，对于其他游戏需要猜测四次。

442
00:26:43,180 --> 00:26:46,920
如果我们在这里向左移动，所有超过零的点都表明，每当不确定性为零时，也就是说只有一种可能性，那么所需的猜测次数总是只有一次，这是令人放心的。

443
00:26:46,920 --> 00:26:51,620


444
00:26:51,620 --> 00:26:55,000


445
00:26:55,000 --> 00:26:59,020
每当有一点不确定性时，意味着基本上只有两种可能性，那么有时需要再进行一次猜测，有时则需要再进行两次猜测。

446
00:26:59,020 --> 00:27:02,360


447
00:27:02,360 --> 00:27:03,940


448
00:27:03,940 --> 00:27:05,980
这里等等等等。

449
00:27:05,980 --> 00:27:11,020
也许可视化这些数据的一种稍微简单的方法是将其放在一起并取平均值。

450
00:27:11,020 --> 00:27:15,940
例如，这里的这个条表示，在我们有一点不确定性的所有点中，平均所需的新猜测数量约为 1。

451
00:27:15,940 --> 00:27:22,420
5.

452
00:27:22,420 --> 00:27:25,920
这里的栏表示在所有不同的游戏中，在某些时候不确定性略高于 4

453
00:27:25,920 --> 00:27:30,480
位，这就像将其缩小到

454
00:27:30,480 --> 00:27:35,120
16

455
00:27:35,120 --> 00:27:36,240
种不同的可能性，然后从该点开始平均需要两个以上的猜测向前。

456
00:27:36,240 --> 00:27:40,080
从这里开始，我只是做了一个回归来拟合一个看起来合理的函数。

457
00:27:40,080 --> 00:27:44,160
请记住，这样做的全部目的是为了我们可以量化这种直觉，即我们从单词中获得的信息越多，预期得分就越低。

458
00:27:44,160 --> 00:27:49,720


459
00:27:49,720 --> 00:27:54,380
所以将此作为版本 2。 0，如果我们返回并运行相同的一组模拟，让它与所有 2315

460
00:27:54,380 --> 00:27:59,820
个可能的单词答案进行比较，结果如何？

461
00:27:59,820 --> 00:28:04,060
与我们的第一个版本相比，它肯定更好，这令人放心。

462
00:28:04,060 --> 00:28:08,780
综上所述，平均值约为 3。 6，尽管与第一个版本不同，它有几次丢失并且在这种情况下需要超过 6

463
00:28:08,780 --> 00:28:12,820
个。

464
00:28:12,820 --> 00:28:15,980
大概是因为有时需要进行权衡以实际实现目标，而不是最大化信息。

465
00:28:15,980 --> 00:28:18,980


466
00:28:18,980 --> 00:28:22,140
那么我们能做得比 3 更好吗？ 6？

467
00:28:22,140 --> 00:28:23,260
我们绝对可以。

468
00:28:23,260 --> 00:28:27,120
现在我在一开始就说过，尝试不将单词答案的真实列表合并到构建模型的方式中是最有趣的。

469
00:28:27,120 --> 00:28:29,980


470
00:28:29,980 --> 00:28:35,180
但如果我们确实将其合并，我可以获得的最佳性能约为 3。 43.

471
00:28:35,180 --> 00:28:39,520
因此，如果我们尝试比仅使用词频数据更复杂地选择此先验分布，则这 3.

472
00:28:39,520 --> 00:28:44,220
43

473
00:28:44,220 --> 00:28:46,360
可能给出了我们可以做到什么程度的最大值，或者至少是我可以做到什么程度。

474
00:28:46,360 --> 00:28:50,240
最佳性能本质上只是使用了我在这里讨论的想法，但它更进一步，就像它向前两步而不是一步搜索预期信息一样。

475
00:28:50,240 --> 00:28:53,400


476
00:28:53,400 --> 00:28:55,660


477
00:28:55,660 --> 00:28:58,720
本来我打算更多地讨论这个问题，但我意识到我们实际上已经讨论了很长时间了。

478
00:28:58,720 --> 00:29:00,580


479
00:29:00,580 --> 00:29:03,520
我要说的一件事是，在进行了两步搜索，然后在顶级候选者中运行了几个样本模拟之后，至少到目前为止对我来说，Crane

480
00:29:03,520 --> 00:29:07,720
看起来是最好的开局者。

481
00:29:07,720 --> 00:29:09,500


482
00:29:09,500 --> 00:29:11,080
谁能想到呢？

483
00:29:11,080 --> 00:29:15,680
此外，如果您使用真实的单词列表来确定您的可能性空间，那么您开始的不确定性将略高于 11

484
00:29:15,680 --> 00:29:17,920
位。

485
00:29:18,160 --> 00:29:22,760
事实证明，仅通过强力搜索，前两次猜测后最大可能的预期信息约为 10

486
00:29:22,760 --> 00:29:26,580
位。

487
00:29:26,580 --> 00:29:31,720
这表明，在最好的情况下，在您进行前两次猜测之后，如果有完美的最佳玩法，您将留下大约一点不确定性。

488
00:29:31,720 --> 00:29:35,220


489
00:29:35,220 --> 00:29:37,400
这与归结为两种可能的猜测相同。

490
00:29:37,400 --> 00:29:41,440
因此，我认为公平且可能相当保守地说，您永远不可能编写一个使平均值低至

491
00:29:41,440 --> 00:29:45,620
3

492
00:29:45,620 --> 00:29:50,460
的算法，因为根据您可用的单词，在仅执行两个步骤后根本没有空间获得足够的信息。能够保证每次都在第三个槽中得到答案，不会失败。

493
00:29:50,460 --> 00:29:53,820


