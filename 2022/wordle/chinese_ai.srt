1
00:00:00,000 --> 00:00:04,040
Wurdle 游戏在过去一两个月里非常

2
00:00:04,040 --> 00:00:07,880
火爆，从来没有人会忽视数学课的机会，

3
00:00:07,880 --> 00:00:12,120
我觉得这个游戏是信息论课程中一个非常好

4
00:00:12,120 --> 00:00:13,120
的中心例子，特别是一个称为熵的主题。

5
00:00:13,120 --> 00:00:17,120
你看，就像很多人一样，我有点陷入了这个

6
00:00:17,120 --> 00:00:21,200
谜题，而且像很多程序员一样，我也陷入了

7
00:00:21,200 --> 00:00:23,200
尝试编写一种算法来尽可能最佳地玩游戏。

8
00:00:23,200 --> 00:00:26,400
我想我在这里要做的只是与你们讨论我

9
00:00:26,400 --> 00:00:29,980
的一些过程，并解释其中的一些数学

10
00:00:29,980 --> 00:00:32,080
，因为整个算法以熵的概念为中心。

11
00:00:32,080 --> 00:00:42,180
首先，如果您还没有听说过，那么什么是 Wurdle？

12
00:00:42,180 --> 00:00:45,380
在我们介绍游戏规则的同时，让我也

13
00:00:45,380 --> 00:00:48,980
预览一下我们要做什么，即开发一个

14
00:00:48,980 --> 00:00:51,380
基本上可以为我们玩游戏的小算法。

15
00:00:51,380 --> 00:00:54,860
虽然我还没有完成今天的 Wurdle，但现在

16
00:00:54,860 --> 00:00:55,860
是 2 月 4 日，我们将看看机器人的表现。

17
00:00:55,860 --> 00:00:59,580
Wurdle 的目标是猜测一个神秘的五

18
00:00:59,580 --> 00:01:00,860
个字母单词，您有六次不同的猜测机会。

19
00:01:00,860 --> 00:01:05,240
例如，我的 Wurdle 机器人建议我从猜测起重机开始。

20
00:01:05,240 --> 00:01:09,300
每次您进行猜测时，您都会获得一些有关

21
00:01:09,300 --> 00:01:10,940
您的猜测与真实答案的接近程度的信息。

22
00:01:10,940 --> 00:01:14,540
灰色框告诉我实际答案中没有 C。

23
00:01:14,540 --> 00:01:18,340
黄色框告诉我有一个 R，但它不在那个位置。

24
00:01:18,340 --> 00:01:21,820
绿色框告诉我秘密词确实有一个

25
00:01:21,820 --> 00:01:22,820
A，而且位于第三个位置。

26
00:01:22,820 --> 00:01:24,300
然后就没有N了，也没有E了。

27
00:01:24,300 --> 00:01:27,420
那么让我进去告诉 Wurdle 机器人该信息。

28
00:01:27,420 --> 00:01:31,500
我们从起重机开始，有灰色、黄色、绿色、灰色、灰色。

29
00:01:31,500 --> 00:01:35,460
不要担心它现在显示的所有数据，我会在适当的时候解释这一点。

30
00:01:35,460 --> 00:01:39,700
但对于我们的第二个选择来说，它的首要建议是小技巧。

31
00:01:39,700 --> 00:01:43,500
你的猜测确实必须是一个实际的五个字母的单词，但正

32
00:01:43,500 --> 00:01:45,700
如你将看到的，它实际上让你猜测的内容相当自由。

33
00:01:45,700 --> 00:01:48,860
在这种情况下，我们尝试一下shtic。

34
00:01:48,860 --> 00:01:50,260
好吧，事情看起来相当不错。

35
00:01:50,260 --> 00:01:54,580
我们按了 S 和 H，所以我们知道前三个字母，我们知道有一个 R。

36
00:01:54,740 --> 00:01:59,740
所以它会像 SHA 某些 R 或 SHA R 某些东西。

37
00:01:59,740 --> 00:02:03,200
看起来 Wurdle 机器人知道它只有

38
00:02:03,200 --> 00:02:05,220
两种可能性，要么是碎片，要么是锋利的。

39
00:02:05,220 --> 00:02:08,620
在这一点上，他们之间存在着一种折腾，所以我想可能只

40
00:02:08,620 --> 00:02:11,260
是因为它是按字母顺序排列的，所以它与分片相匹配。

41
00:02:11,260 --> 00:02:13,000
万岁，这才是真正的答案。

42
00:02:13,000 --> 00:02:14,660
所以我们三分就搞定了。

43
00:02:14,660 --> 00:02:17,740
如果你想知道这是否有好处，我听到一个人的说法是，对

44
00:02:17,740 --> 00:02:20,820
于Wurdle来说，四杆是标准杆，三杆是小鸟球。

45
00:02:20,820 --> 00:02:22,960
我认为这是一个非常恰当的比喻。

46
00:02:22,960 --> 00:02:27,560
你必须坚持自己的比赛才能获得四分，但这当然并不疯狂。

47
00:02:27,560 --> 00:02:30,000
但当你拿到三分的时候，感觉棒极了。

48
00:02:30,000 --> 00:02:33,800
因此，如果您愿意的话，我在这里想做的就是从一开始就

49
00:02:33,800 --> 00:02:36,600
谈谈我如何处理 Wurdle 机器人的思考过程。

50
00:02:36,600 --> 00:02:39,800
就像我说的，这确实是信息论课程的借口。

51
00:02:39,800 --> 00:02:43,160
主要目标是解释什么是信息和什么是熵。

52
00:02:48,560 --> 00:02:52,080
在解决这个问题时，我的第一个想法

53
00:02:52,080 --> 00:02:53,560
是看看英语中不同字母的相对频率。

54
00:02:53,560 --> 00:02:57,800
所以我想，好吧，是否有一个开局猜测或一

55
00:02:57,800 --> 00:02:59,960
对开局猜测可以命中这些最常见的字母？

56
00:02:59,960 --> 00:03:03,780
我非常喜欢做其他事情，然后做指甲。

57
00:03:03,780 --> 00:03:06,980
我们的想法是，如果你击中一个字母，你知道

58
00:03:06,980 --> 00:03:07,980
，你会得到绿色或黄色，这总是感觉很好。

59
00:03:07,980 --> 00:03:09,460
感觉就像你正在获取信息。

60
00:03:09,460 --> 00:03:13,140
但在这些情况下，即使你没有击中并且总

61
00:03:13,140 --> 00:03:16,640
是得到灰色，这仍然为你提供了大量信息

62
00:03:16,640 --> 00:03:17,640
，因为很少找到没有这些字母的单词。

63
00:03:17,640 --> 00:03:21,840
但即便如此，这仍然感觉不是超级系统

64
00:03:21,840 --> 00:03:23,520
，因为例如，它没有考虑字母的顺序。

65
00:03:23,520 --> 00:03:26,080
当我可以输入蜗牛时，为什么还要输入指甲？

66
00:03:26,080 --> 00:03:27,720
最后加个S是不是更好？

67
00:03:27,720 --> 00:03:28,720
我不太确定。

68
00:03:28,720 --> 00:03:33,500
现在，我的一个朋友说他喜欢用“weary”这个词开头，这让

69
00:03:33,500 --> 00:03:37,160
我有点惊讶，因为里面有一些不常见的字母，比如 W 和 Y。

70
00:03:37,160 --> 00:03:39,400
但谁知道呢，也许这是一个更好的开局。

71
00:03:39,400 --> 00:03:43,920
我们是否可以给出某种定量评

72
00:03:43,920 --> 00:03:44,920
分来判断潜在猜测的质量？

73
00:03:44,920 --> 00:03:48,640
现在，为了设置我们对可能的猜测进行排名的

74
00:03:48,640 --> 00:03:51,800
方式，让我们回顾一下游戏的具体设置方式。

75
00:03:51,800 --> 00:03:55,880
因此，您可以输入一个单词列表，这些单词被视为

76
00:03:55,880 --> 00:03:57,920
有效的猜测，长度约为 13,000 个单词。

77
00:03:57,920 --> 00:04:01,560
但当你仔细观察时，你会发现有很多非常不常见的东西，比如

78
00:04:01,560 --> 00:04:07,040
头像、阿里和ARG，以及拼字游戏中引发家庭争吵的词语。

79
00:04:07,040 --> 00:04:10,600
但游戏的氛围是答案总是一个相当常见的词。

80
00:04:10,600 --> 00:04:16,080
事实上，还有另一个大约 2300 个单词的列表，它们是可能的答案。

81
00:04:16,080 --> 00:04:20,320
这是一个人工策划的列表，我认为是由游戏

82
00:04:20,320 --> 00:04:21,800
创建者的女朋友专门制作的，这很有趣。

83
00:04:21,800 --> 00:04:25,560
但我想做的是，我们对这个项目的挑战是看看我们是否可以编写一个解

84
00:04:25,560 --> 00:04:30,720
决 Wordle 的程序，该程序不包含有关此列表的先前知识。

85
00:04:30,720 --> 00:04:34,560
一方面，有很多非常常见的五个

86
00:04:34,560 --> 00:04:35,560
字母单词您在该列表中找不到。

87
00:04:35,560 --> 00:04:38,360
因此，最好编写一个更具弹性的程序，并且可以与

88
00:04:38,360 --> 00:04:41,960
任何人玩 Wordle，而不仅仅是官方网站。

89
00:04:41,960 --> 00:04:45,900
我们之所以知道可能答案的列表是

90
00:04:45,900 --> 00:04:47,440
什么，是因为它在源代码中可见。

91
00:04:47,440 --> 00:04:51,620
但它在源代码中可见的方式是按

92
00:04:51,620 --> 00:04:52,840
照每天出现答案的特定顺序。

93
00:04:52,840 --> 00:04:56,400
所以你总是可以查找明天的答案。

94
00:04:56,400 --> 00:04:59,140
很明显，使用该列表在某种程度上是作弊行为。

95
00:04:59,140 --> 00:05:02,900
使谜题更有趣、信息论课程更丰富的方法

96
00:05:02,900 --> 00:05:07,640
是使用一些更通用的数据，例如相对词

97
00:05:07,640 --> 00:05:11,640
频，来捕捉对更常见词的偏好的直觉。

98
00:05:11,640 --> 00:05:16,560
那么在这13000种可能性中，我们应该如何选择开局猜测呢？

99
00:05:16,560 --> 00:05:19,960
例如，如果我的朋友提出厌烦，我们应该如何分析它的质量？

100
00:05:19,960 --> 00:05:25,040
好吧，他说他喜欢那个不太可能的 W 的原因是他喜欢

101
00:05:25,040 --> 00:05:27,880
远射的本质，如果你击中了那个 W，那感觉是多么好。

102
00:05:27,880 --> 00:05:31,400
例如，如果第一个揭示的模式是这样的，那么这个

103
00:05:31,400 --> 00:05:36,080
庞大的词典中只有 58 个单词与该模式匹配。

104
00:05:36,080 --> 00:05:38,900
因此，与 13,000 人相比，这是一个巨大的减少。

105
00:05:38,900 --> 00:05:43,320
但当然，另一方面是，获得这样的模式非常罕见。

106
00:05:43,360 --> 00:05:47,600
具体来说，如果每个单词作为答案的可能性相同，则达到

107
00:05:47,600 --> 00:05:51,680
此模式的概率将为 58 除以大约 13,000。

108
00:05:51,680 --> 00:05:53,880
当然，它们成为答案的可能性并不相同。

109
00:05:53,880 --> 00:05:56,680
其中大部分都是非常晦涩甚至有问题的词语。

110
00:05:56,680 --> 00:05:59,560
但至少对于我们第一次通过这一切，我们假设它

111
00:05:59,560 --> 00:06:02,040
们的可能性相同，然后稍后再对其进行完善。

112
00:06:02,040 --> 00:06:07,360
关键是，包含大量信息的模式本质上不太可能发生。

113
00:06:07,360 --> 00:06:11,320
事实上，提供信息意味着这是不可能的。

114
00:06:11,920 --> 00:06:16,720
在这个开口中看到的更可能的模式

115
00:06:16,720 --> 00:06:18,360
是这样的，当然其中没有 W。

116
00:06:18,360 --> 00:06:22,080
也许有E，也许没有A，没有R，没有Y。

117
00:06:22,080 --> 00:06:24,640
在本例中，有 1400 个可能的匹配项。

118
00:06:24,640 --> 00:06:29,600
如果所有可能性均等，则您看到

119
00:06:29,600 --> 00:06:30,680
的模式的概率约为 11%。

120
00:06:30,680 --> 00:06:34,320
因此，最可能的结果也是信息最少的。

121
00:06:34,320 --> 00:06:38,440
为了获得更全面的视角，让我向您展示您可

122
00:06:38,440 --> 00:06:42,000
能看到的所有不同模式的概率的完整分布。

123
00:06:42,000 --> 00:06:46,000
因此，您看到的每个条形都对应于可能显示的颜

124
00:06:46,000 --> 00:06:50,500
色模式，其中有 3 到 5 种可能性，并且

125
00:06:50,500 --> 00:06:52,960
它们从左到右、最常见到最不常见进行组织。

126
00:06:52,960 --> 00:06:56,200
所以这里最常见的可能性是你得到的都是灰色的。

127
00:06:56,200 --> 00:06:58,800
这种情况发生的概率约为 14%。

128
00:06:58,800 --> 00:07:02,040
当你进行猜测时，你所希望的是你最终会出现在这条

129
00:07:02,040 --> 00:07:06,360
长尾中的某个地方，就像在这里，与这个显然看起

130
00:07:06,360 --> 00:07:09,920
来像这样的模式相匹配的可能性只有 18 种。

131
00:07:09,920 --> 00:07:14,080
或者，如果我们冒险向左走一点，你知道，也许我们会一直走到这里。

132
00:07:14,080 --> 00:07:16,560
好的，这是给你的一个很好的谜题。

133
00:07:16,560 --> 00:07:20,600
英语中以 W 开头、以 Y 结尾、其

134
00:07:20,600 --> 00:07:22,040
中某个位置有 R 的三个单词是什么？

135
00:07:22,040 --> 00:07:27,560
事实证明，让我们看看，答案是罗嗦、狡猾和讽刺的。

136
00:07:27,560 --> 00:07:32,720
因此，为了判断这个词的整体效果如何，我们需要某

137
00:07:32,720 --> 00:07:35,720
种方式来衡量您将从该分布中获得的预期信息量。

138
00:07:36,360 --> 00:07:41,080
如果我们检查每个模式，并将其发生的概率乘以衡量其

139
00:07:41,080 --> 00:07:46,000
信息量的因素，这也许可以给我们一个客观的分数。

140
00:07:46,000 --> 00:07:50,280
现在，您对某事物应该是什么的第一直觉可能是匹配的数量。

141
00:07:50,280 --> 00:07:52,960
您想要较低的平均匹配数。

142
00:07:52,960 --> 00:07:57,400
但相反，我想使用一种更通用的衡量标准，我们通常将其归因于信息

143
00:07:57,400 --> 00:08:01,040
，并且一旦我们为这 13,000 个单词中的每一个分配不同的

144
00:08:01,040 --> 00:08:04,320
概率来判断它们是否实际上是答案，这种衡量标准就会更加灵活。

145
00:08:10,600 --> 00:08:14,760
信息的标准单位是位，它的公式有点有趣

146
00:08:14,760 --> 00:08:17,800
，但如果我们只看例子，它真的很直观。

147
00:08:17,800 --> 00:08:21,880
如果你的观察结果将你的可能性空间减

148
00:08:21,880 --> 00:08:24,200
少了一半，我们就说它只有一点信息。

149
00:08:24,200 --> 00:08:27,680
在我们的示例中，可能性空间是所有可能的单词，结果表明，五

150
00:08:27,760 --> 00:08:31,560
个字母的单词中大约一半有 S，比这个少一点，但大约一半。

151
00:08:31,560 --> 00:08:35,200
这样观察就会给你一点信息。

152
00:08:35,200 --> 00:08:39,640
相反，如果一个新事实将可能性空间减

153
00:08:39,640 --> 00:08:42,000
少了四倍，我们就说它有两位信息。

154
00:08:42,000 --> 00:08:45,120
例如，事实证明这些单词中大约四分之一有 T。

155
00:08:45,120 --> 00:08:49,720
如果观察将该空间缩小八分之一，我

156
00:08:49,720 --> 00:08:50,920
们就说它是三位信息，依此类推。

157
00:08:50,920 --> 00:08:55,000
四位将其切割为 16 度，五位将其切割为 32 度。

158
00:08:55,000 --> 00:09:00,160
所以现在您可能想停下来问自己，就发生

159
00:09:00,160 --> 00:09:04,520
概率而言，比特数的信息公式是什么？

160
00:09:04,520 --> 00:09:07,920
我们在这里所说的是，当你取位数的二分之一

161
00:09:07,920 --> 00:09:11,680
时，这与概率是一样的，这与说 2 的位数

162
00:09:11,680 --> 00:09:16,200
次方等于概率的 1 是一样的，重新排列进

163
00:09:16,200 --> 00:09:19,680
一步表示该信息是一的对数基数除以概率。

164
00:09:19,680 --> 00:09:23,200
有时您还会看到这种情况，还需要进行一次重新

165
00:09:23,200 --> 00:09:25,680
排列，其中信息是以概率的负对数为底的二。

166
00:09:25,680 --> 00:09:29,120
对于外行来说，这样表达可能有点奇怪

167
00:09:29,120 --> 00:09:33,400
，但这确实是一个非常直观的想法，即

168
00:09:33,400 --> 00:09:35,120
询问您已将可能性减少一半的次数。

169
00:09:35,120 --> 00:09:37,840
现在，如果您想知道，您知道，我以为我们只是

170
00:09:37,840 --> 00:09:39,920
在玩一个有趣的文字游戏，为什么要使用对数？

171
00:09:39,920 --> 00:09:43,920
这是一个更好的单元的原因之一是，谈论非常不可能

172
00:09:43,920 --> 00:09:48,120
的事件要容易得多，说一个观察有 20 位信息比

173
00:09:48,120 --> 00:09:53,480
说这样那样发生的概率为 0 容易得多。0000095。

174
00:09:53,480 --> 00:09:57,360
但这种对数表达式被证明是对概率论非常有用

175
00:09:57,360 --> 00:10:02,000
的补充，更实质性的原因是信息相加的方式。

176
00:10:02,000 --> 00:10:05,560
例如，如果一个观察结果为您提供了两位信息，将您的空间

177
00:10:05,560 --> 00:10:10,120
缩小了四分之二，然后第二个观察结果（如您在 Wor

178
00:10:10,120 --> 00:10:14,480
dle 中的第二次猜测）为您提供了另外三位信息，将

179
00:10:14,480 --> 00:10:17,360
您的空间进一步缩小了八倍，则两个一起给你五位信息。

180
00:10:17,360 --> 00:10:21,200
就像概率喜欢乘法一样，信息喜欢增加。

181
00:10:21,200 --> 00:10:24,920
因此，一旦我们处于预期值之类的领域，我们

182
00:10:24,920 --> 00:10:28,660
将一堆数字相加，日志就会使其更容易处理。

183
00:10:28,660 --> 00:10:32,600
让我们回到 Weary 的发行版，并在此处添加

184
00:10:32,600 --> 00:10:35,560
另一个小跟踪器，向我们展示每种模式有多少信息。

185
00:10:35,560 --> 00:10:38,760
我希望您注意的主要事情是，我们获得这些更有可能

186
00:10:38,760 --> 00:10:43,500
的模式的概率越高，信息越少，您获得的位就越少。

187
00:10:43,500 --> 00:10:47,360
我们衡量这种猜测质量的方法是获取该信息的期

188
00:10:47,360 --> 00:10:51,620
望值，我们遍历每个模式，我们说它的可能性有

189
00:10:51,620 --> 00:10:54,940
多大，然后我们将其乘以我们获得的信息位数。

190
00:10:54,940 --> 00:10:58,480
在 Weary 的例子中，结果是 4。9 位。

191
00:10:58,480 --> 00:11:02,800
因此，平均而言，您从这个开局猜测中获得的信息

192
00:11:02,800 --> 00:11:05,660
相当于将您的可能性空间切成两半（大约五倍）。

193
00:11:05,660 --> 00:11:10,260
相比之下，具有较高预期信息值的

194
00:11:10,260 --> 00:11:13,220
猜测的示例类似于 Slate。

195
00:11:13,220 --> 00:11:16,180
在这种情况下，您会注意到分布看起来更加平坦。

196
00:11:16,180 --> 00:11:20,780
特别是，所有灰色中最有可能出现的概率只有 6%

197
00:11:20,780 --> 00:11:25,940
左右，因此至少明显会得到 3。9位信息。

198
00:11:25,940 --> 00:11:29,140
但这是最低限度，更常见的是你会得到比这更好的东西。

199
00:11:29,140 --> 00:11:33,380
事实证明，当你计算这个数字并将所有相

200
00:11:33,380 --> 00:11:36,420
关术语加起来时，平均信息约为 5。8.

201
00:11:36,420 --> 00:11:42,140
因此，与《Weary》相比，平均而言，在第一

202
00:11:42,140 --> 00:11:43,940
次猜测之后，你的可能性空间大约只有一半大。

203
00:11:43,940 --> 00:11:49,540
关于信息量期望值的名称，实际上有一个有趣的故事。

204
00:11:49,540 --> 00:11:52,580
信息论是由 20 世纪 40 年代在贝尔实验室工作的克劳德·香农 (C

205
00:11:52,580 --> 00:11:57,620
laude Shannon) 提出的，但他正在与约翰·冯·诺依曼 (

206
00:11:57,620 --> 00:12:01,500
John von Neumann) 谈论他尚未发表的一些想法，约翰·

207
00:12:01,500 --> 00:12:04,180
冯·诺依曼是当时非常杰出的知识巨人。数学和物理以及计算机科学的开端。

208
00:12:04,180 --> 00:12:07,260
当冯·诺依曼提到他对于信息量的期望值

209
00:12:07,260 --> 00:12:12,540
并没有一个好名字时，据说，所以故事是

210
00:12:12,540 --> 00:12:14,720
这样的，你应该称之为熵，有两个原因。

211
00:12:14,720 --> 00:12:18,400
首先，你的不确定性函数已经在统计力学中以这个名字使

212
00:12:18,400 --> 00:12:23,100
用了，所以它已经有一个名字了，其次，更重要的是，没

213
00:12:23,100 --> 00:12:26,940
有人知道熵到底是什么，所以在辩论中你总是会有优势。

214
00:12:26,940 --> 00:12:31,420
因此，如果这个名字看起来有点神秘，并且

215
00:12:31,420 --> 00:12:33,420
如果这个故事可信的话，那就是有意为之。

216
00:12:33,420 --> 00:12:36,740
另外，如果您想知道它与物理学中所有热力学

217
00:12:36,740 --> 00:12:40,820
第二定律的关系，那么肯定存在某种联系，

218
00:12:40,820 --> 00:12:44,780
但在其起源中，香农只是处理纯概率论，并且

219
00:12:44,780 --> 00:12:49,340
出于我们的目的，当我使用熵这个词，我只

220
00:12:49,340 --> 00:12:50,820
是想让你思考一个特定猜测的预期信息值。

221
00:12:50,820 --> 00:12:54,380
您可以将熵视为同时测量两个事物。

222
00:12:54,380 --> 00:12:57,420
第一个是分布的平坦程度。

223
00:12:57,420 --> 00:13:01,700
分布越接近均匀，熵就越高。

224
00:13:01,700 --> 00:13:06,340
在我们的例子中，总共有 3 到 5 个模式，对于均匀分布，观察其

225
00:13:06,340 --> 00:13:11,340
中任何一个模式都会有 3 到 5 个的信息对数基数 2，恰好是

226
00:13:11,340 --> 00:13:17,860
7。92，所以这是该熵可能具有的绝对最大值。

227
00:13:17,860 --> 00:13:21,900
但熵首先也是一种衡

228
00:13:21,900 --> 00:13:22,900
量可能性的方法。

229
00:13:22,900 --> 00:13:26,980
例如，如果您碰巧有某个单词，其中只有 16 种可能的模式，并

230
00:13:26,980 --> 00:13:32,760
且每种模式的可能性相同，则该熵（即该预期信息）将是 4 位。

231
00:13:32,760 --> 00:13:36,880
但如果你有另一个词，其中可能出现 64 种可能的

232
00:13:36,880 --> 00:13:41,000
模式，并且它们的可能性相同，那么熵将是 6 位。

233
00:13:41,000 --> 00:13:45,800
因此，如果您在野外看到某个分布的熵为 6 位，这

234
00:13:45,800 --> 00:13:50,000
就有点像是在说即将发生的事情存在同样多的变化和不

235
00:13:50,000 --> 00:13:54,400
确定性，就好像有 64 个同样可能的结果一样。

236
00:13:54,400 --> 00:13:58,360
对于我第一次使用 Wurtelebot，我基本上就是让它这样做。

237
00:13:58,360 --> 00:14:03,560
它会遍历所有可能的猜测，即所有 13,000 个单

238
00:14:03,560 --> 00:14:08,580
词，计算每个单词的熵，或者更具体地说，计算您可能

239
00:14:08,580 --> 00:14:13,040
看到的所有模式中每个单词的分布熵，并选择最高的，因

240
00:14:13,040 --> 00:14:17,200
为这是一个可能会尽可能地削减你的可能性空间的人。

241
00:14:17,200 --> 00:14:20,120
尽管我在这里只讨论了第一个猜测，但它对

242
00:14:20,120 --> 00:14:21,680
于接下来的几次猜测也起到了同样的作用。

243
00:14:21,680 --> 00:14:25,100
例如，在您看到第一个猜测的某些模式后，这将根

244
00:14:25,100 --> 00:14:29,300
据与之匹配的内容将您限制为较少数量的可能单词

245
00:14:29,300 --> 00:14:32,300
，您只需针对该较小的单词集玩相同的游戏即可。

246
00:14:32,300 --> 00:14:36,500
对于建议的第二个猜测，您会查看从一组更受限制的

247
00:14:36,500 --> 00:14:41,540
单词中可能出现的所有模式的分布，搜索所有 13

248
00:14:41,540 --> 00:14:45,480
,000 种可能性，然后找到使熵最大化的一种。

249
00:14:45,480 --> 00:14:48,980
为了向您展示这是如何实际工作的，让我拿出我编写的 Wurt

250
00:14:48,980 --> 00:14:54,060
ele 的一个小变体，它在页边空白处显示了此分析的亮点。

251
00:14:54,460 --> 00:14:57,820
完成所有熵计算后，右侧向我们展

252
00:14:57,820 --> 00:15:00,340
示了哪些具有最高的预期信息。

253
00:15:00,340 --> 00:15:04,940
事实证明，至少目前最重要的答案是稗子，我们稍后会对此进

254
00:15:04,940 --> 00:15:11,140
行完善，这意味着，嗯，当然是野豌豆，最常见的野豌豆。

255
00:15:11,140 --> 00:15:14,180
每次我们在这里进行猜测时，也许我会忽略它的建议并选择

256
00:15:14,180 --> 00:15:19,220
slate，因为我喜欢 slate，我们可以看到

257
00:15:19,220 --> 00:15:23,300
它有多少预期信息，但在这个词的右侧，它向我们展示了

258
00:15:23,340 --> 00:15:24,980
多少信息考虑到这种特定的模式，我们得到的实际信息。

259
00:15:24,980 --> 00:15:28,660
所以看起来我们有点不走运，我们预计会得到 5 个。8，但

260
00:15:28,660 --> 00:15:30,660
我们碰巧得到的东西比这个少。

261
00:15:30,660 --> 00:15:34,020
然后在左侧，它向我们展示了当前

262
00:15:34,020 --> 00:15:35,860
所处位置的所有不同可能的单词。

263
00:15:35,860 --> 00:15:39,820
蓝色条告诉我们它认为每个单词出现的可能性有多大，因此目前它

264
00:15:39,820 --> 00:15:44,140
假设每个单词出现的可能性相同，但我们稍后会对其进行改进。

265
00:15:44,140 --> 00:15:48,580
然后，这种不确定性测量告诉我们可能单词的

266
00:15:48,580 --> 00:15:53,220
分布熵，因为它是均匀分布，所以现在只是

267
00:15:53,300 --> 00:15:55,940
一种计算可能性数量的不必要的复杂方法。

268
00:15:55,940 --> 00:16:01,700
例如，如果我们要计算 2 的 13 次方。66，这应该是

269
00:16:01,700 --> 00:16:02,700
大约 13,000 种可能性。

270
00:16:02,700 --> 00:16:06,780
我在这里有点偏离，但这只是因为我没有显示所有小数位。

271
00:16:06,780 --> 00:16:10,260
目前，这可能感觉多余，而且好像事情过于复杂，但您

272
00:16:10,260 --> 00:16:12,780
很快就会明白为什么同时拥有这两个数字是有用的。

273
00:16:12,780 --> 00:16:16,780
所以这里看起来它表明我们第二个猜测的

274
00:16:16,780 --> 00:16:19,700
最高熵是拉面，这又感觉不像一个词。

275
00:16:19,700 --> 00:16:25,660
因此，为了占据道德制高点，我将继续输入 Rains。

276
00:16:25,660 --> 00:16:27,540
看来我们又有点不走运了。

277
00:16:27,540 --> 00:16:32,100
我们本来期待4。3 位，但我们只得到了 3 位。39位信息。

278
00:16:32,100 --> 00:16:35,060
这样一来，我们就有 55 种可能性。

279
00:16:35,060 --> 00:16:38,860
在这里，也许我实际上会遵循它的建

280
00:16:38,860 --> 00:16:40,200
议，即组合，无论这意味着什么。

281
00:16:40,200 --> 00:16:43,300
好吧，这实际上是一个解谜的好机会。

282
00:16:43,300 --> 00:16:47,020
它告诉我们这个模式给了我们 4。7 位信息。

283
00:16:47,020 --> 00:16:52,400
但在左边，在我们看到该模式之前，有 5 个。78 位不确定性。

284
00:16:52,400 --> 00:16:56,860
那么作为对你的一个测验，剩余可能性的数量意味着什么？

285
00:16:56,860 --> 00:17:02,280
嗯，这意味着我们将不确定性减少到一点

286
00:17:02,280 --> 00:17:04,700
点，这与说有两个可能的答案是一样的。

287
00:17:04,700 --> 00:17:06,520
这是50-50的选择。

288
00:17:06,520 --> 00:17:09,860
从这里开始，因为你和我知道哪些词更

289
00:17:09,860 --> 00:17:11,220
常见，所以我们知道答案应该是深渊。

290
00:17:11,220 --> 00:17:13,540
但正如现在所写的，程序并不知道这一点。

291
00:17:13,540 --> 00:17:17,560
所以它会继续前进，尝试获取尽可能多的信息，

292
00:17:17,560 --> 00:17:20,360
直到只剩下一种可能性，然后它就会猜测它。

293
00:17:20,360 --> 00:17:22,700
显然我们需要更好的残局策略。

294
00:17:22,700 --> 00:17:26,540
但是，假设我们将此版本称为我们的 wordle 求解

295
00:17:26,540 --> 00:17:30,740
器之一，然后我们运行一些模拟来看看它是如何工作的。

296
00:17:30,740 --> 00:17:34,240
所以它的工作方式是玩所有可能的文字游戏。

297
00:17:34,240 --> 00:17:38,780
它会检查所有 2315 个单词，这些单词是实际的单词答案。

298
00:17:38,780 --> 00:17:41,340
它基本上使用它作为测试集。

299
00:17:41,340 --> 00:17:45,820
采用这种天真的方法，不考虑一个词的常见程度，只是

300
00:17:45,820 --> 00:17:50,480
试图在每一步中最大化信息，直到它只剩下一个选择。

301
00:17:50,480 --> 00:17:55,100
模拟结束时，平均得分约为 4。124.

302
00:17:55,100 --> 00:17:59,780
老实说，这还不错，我本来以为会做得更糟。

303
00:17:59,780 --> 00:18:03,040
但玩wordle的人会告诉你，他们通常可以在4内得到它。

304
00:18:03,040 --> 00:18:05,260
真正的挑战是尽可能多地获得三分。

305
00:18:05,260 --> 00:18:08,920
4分和3分之间的差距相当大。

306
00:18:08,920 --> 00:18:13,300
这里显而易见的容易实现的目标是以某种方式纳入一

307
00:18:13,300 --> 00:18:23,160
个单词是否常见，以及我们到底如何做到这一点。

308
00:18:23,160 --> 00:18:26,860
我的方法是获取英语中所

309
00:18:26,860 --> 00:18:28,560
有单词的相对频率列表。

310
00:18:28,560 --> 00:18:32,560
我刚刚使用了 Mathematica 的词频数据函数，它本身是从 Go

311
00:18:32,560 --> 00:18:35,520
ogle Books English Ngram 公共数据集中提取的。

312
00:18:35,520 --> 00:18:38,680
看起来很有趣，例如，如果我们将其从最

313
00:18:38,680 --> 00:18:40,120
常见的单词到最不常见的单词进行排序。

314
00:18:40,120 --> 00:18:43,740
显然，这些是英语中最常见的 5 个字母单词。

315
00:18:43,740 --> 00:18:46,480
或者更确切地说，这些是第八个最常见的。

316
00:18:46,480 --> 00:18:49,440
首先是which，然后是there和there。

317
00:18:49,440 --> 00:18:53,020
First本身不是first，而是9th，并且这些其

318
00:18:53,020 --> 00:18:57,840
他词可能更频繁地出现是有道理的，其中first之后

319
00:18:57,840 --> 00:18:59,000
的词是after、where，而那些词则不太常见。

320
00:18:59,000 --> 00:19:04,400
现在，在使用这些数据来模拟每个单词成为最终

321
00:19:04,400 --> 00:19:06,760
答案的可能性时，它不应该仅仅与频率成正比。

322
00:19:07,020 --> 00:19:12,560
例如，得分为 0。002 在此数据集中，而“br

323
00:19:12,560 --> 00:19:15,200
aid”一词在某种意义上的可能性要小 1000 倍。

324
00:19:15,200 --> 00:19:19,400
但这两个词都很常见，几乎肯定值得考虑。

325
00:19:19,400 --> 00:19:21,900
所以我们想要更多的二元截止。

326
00:19:21,900 --> 00:19:26,520
我的方法是想象一下将整个排序的单词列表，然后将其排列

327
00:19:26,520 --> 00:19:31,060
在 x 轴上，然后应用 sigmoid 函数，这是

328
00:19:31,060 --> 00:19:35,540
输出基本上是二进制的函数的标准方法，它是要么是 0

329
00:19:35,540 --> 00:19:38,500
，要么是 1，但对于该不确定区域，中间有一个平滑。

330
00:19:38,500 --> 00:19:43,900
所以本质上，我分配给每个单词出现在最终列表中的概率将是上

331
00:19:43,900 --> 00:19:49,540
面的 sigmoid 函数的值，无论它位于 x 轴上。

332
00:19:49,540 --> 00:19:53,940
显然，这取决于几个参数，例如，这些单词在 x 轴上填充

333
00:19:53,940 --> 00:19:59,660
的空间有多宽，决定了我们从 1 到 0 的逐渐或陡峭

334
00:19:59,660 --> 00:20:03,000
程度，以及我们将它们从左到右放置的位置决定了截止值。

335
00:20:03,160 --> 00:20:07,340
说实话，我的做法就是舔手指然后把它插到风里。

336
00:20:07,340 --> 00:20:10,800
我查看了排序后的列表，并试图找到一个窗口

337
00:20:10,800 --> 00:20:15,280
，当我查看它时，我认为这些单词中的一半

338
00:20:15,280 --> 00:20:17,680
更有可能是最终答案，并将其用作截止值。

339
00:20:17,680 --> 00:20:21,840
一旦我们在单词之间有了这样的分布，它就会给我们

340
00:20:21,840 --> 00:20:24,460
带来另一种情况，即熵成为这种真正有用的度量。

341
00:20:24,460 --> 00:20:28,480
例如，假设我们正在玩一个游戏，我们从我

342
00:20:28,480 --> 00:20:32,480
的旧开场白开始，即羽毛和指甲，我们最终

343
00:20:32,480 --> 00:20:33,760
会遇到有四个可能的单词与之匹配的情况。

344
00:20:33,760 --> 00:20:36,440
假设我们认为它们都有相同的可能性。

345
00:20:36,440 --> 00:20:40,000
我问你，这个分布的熵是多少？

346
00:20:40,000 --> 00:20:45,920
嗯，与这些可能性中的每一种相关的信息将是 4 的以 2

347
00:20:45,920 --> 00:20:50,800
为底的对数，因为每一种都是 1 和 4，那就是 2。

348
00:20:50,800 --> 00:20:52,780
两位信息，四种可能性。

349
00:20:52,780 --> 00:20:54,360
一切都很好。

350
00:20:54,360 --> 00:20:58,320
但如果我告诉你实际上有超过四场比赛呢？

351
00:20:58,320 --> 00:21:02,600
事实上，当我们查看完整的单词列表时，有 16 个单词与其匹配。

352
00:21:02,600 --> 00:21:07,260
但假设我们的模型对其他 12 个单词实际成为最终答案

353
00:21:07,260 --> 00:21:11,440
的概率非常低，大约是千分之一，因为它们真的很晦涩。

354
00:21:11,440 --> 00:21:15,480
现在我问你，这个分布的熵是多少？

355
00:21:15,480 --> 00:21:19,600
如果熵在这里纯粹测量匹配的数量，那么您可能

356
00:21:19,600 --> 00:21:24,760
会期望它类似于 16 的以 2 为底的对数

357
00:21:24,760 --> 00:21:26,200
，即 4，比我们之前的不确定性多了两位。

358
00:21:26,200 --> 00:21:30,320
但当然，实际的不确定性与我们之前的情况并没有太大不同。

359
00:21:30,320 --> 00:21:33,840
例如，仅仅因为有这 12 个非常晦涩的单词并不意

360
00:21:33,840 --> 00:21:38,200
味着当得知最终答案是“魅力”时会更加令人惊讶。

361
00:21:38,200 --> 00:21:42,080
所以当你在这里实际进行计算时，将每次出现的概

362
00:21:42,080 --> 00:21:45,960
率乘以相应的信息相加，得到的就是 2。11 位。

363
00:21:45,960 --> 00:21:50,280
我只是说，它基本上是两位，基本上是这四种可能性，但是

364
00:21:50,280 --> 00:21:54,240
由于所有这些极不可能发生的事件，存在更多的不确定性

365
00:21:54,240 --> 00:21:57,120
，尽管如果你确实了解了它们，你会从中获得大量信息。

366
00:21:57,120 --> 00:22:00,800
缩小范围，这就是 Wordle 成为信

367
00:22:00,800 --> 00:22:01,800
息论课程的一个很好的例子的部分原因。

368
00:22:01,800 --> 00:22:05,280
我们对熵有两种不同的感觉应用。

369
00:22:05,280 --> 00:22:09,640
第一个告诉我们从给定的猜测中得到的预期

370
00:22:09,640 --> 00:22:14,560
信息是什么，第二个告诉我们我们是否可以

371
00:22:14,560 --> 00:22:16,480
衡量所有可能的单词中剩余的不确定性。

372
00:22:16,480 --> 00:22:19,800
我应该强调，在第一种情况下，我们正在查看猜测的预期

373
00:22:19,800 --> 00:22:25,000
信息，一旦我们对单词的权重不相等，就会影响熵计算。

374
00:22:25,000 --> 00:22:28,600
例如，让我拿出我们之前查看的与 We

375
00:22:28,600 --> 00:22:33,560
ary 相关的分布的相同案例，但这次

376
00:22:33,560 --> 00:22:34,560
在所有可能的单词中使用非均匀分布。

377
00:22:34,560 --> 00:22:39,360
所以让我看看是否可以在这里找到一个很好地说明它的部分。

378
00:22:39,360 --> 00:22:42,480
好吧，这里这很好。

379
00:22:42,480 --> 00:22:46,360
这里我们有两个相邻的模式，它们的可能性大致相同，但我

380
00:22:46,360 --> 00:22:49,480
们被告知其中一个模式有 32 个可能的单词与其匹配。

381
00:22:49,480 --> 00:22:54,080
如果我们检查它们是什么，那就是 32 个，当

382
00:22:54,080 --> 00:22:55,600
你扫视它们时，它们都只是非常不可能的单词。

383
00:22:55,600 --> 00:23:00,400
很难找到任何看似合理的答案，也许会大喊大叫，

384
00:23:00,400 --> 00:23:04,440
但如果我们查看分布中的相邻模式，这被认为是

385
00:23:04,440 --> 00:23:08,920
同样可能的，我们被告知它只有 8 个可能的匹

386
00:23:08,920 --> 00:23:09,920
配，所以四分之一很多比赛，但可能性差不多。

387
00:23:09,920 --> 00:23:12,520
当我们拿出这些匹配项时，我们就能明白原因了。

388
00:23:12,520 --> 00:23:17,840
其中一些是实际合理的答案，例如戒指、愤怒或说唱。

389
00:23:17,840 --> 00:23:22,000
为了说明我们如何整合所有这些，让我在这里列出 Wordlebo

390
00:23:22,000 --> 00:23:25,960
t 的第 2 版，它与我们看到的第一个版本有两三个主要区别。

391
00:23:25,960 --> 00:23:29,460
首先，就像我刚才说的，我们计算这些熵、这些信

392
00:23:29,460 --> 00:23:34,800
息的预期值的方式现在正在使用跨模式的更精细的

393
00:23:34,800 --> 00:23:39,300
分布，其中包含给定单词实际上是答案的概率。

394
00:23:39,300 --> 00:23:44,160
碰巧，眼泪仍然是第一位，尽管接下来的有点不同。

395
00:23:44,160 --> 00:23:47,920
其次，当它对首选进行排名时，它现在将保留每个单词

396
00:23:47,920 --> 00:23:52,600
是实际答案的概率模型，并将其纳入其决策中，一旦我

397
00:23:52,600 --> 00:23:55,520
们对答案有了一些猜测，就更容易看到这一点。桌子。

398
00:23:55,520 --> 00:24:01,120
再次忽略它的建议，因为我们不能让机器统治我们的生活。

399
00:24:01,120 --> 00:24:05,160
我想我应该提到另一件不同的事情是在左边，不确定

400
00:24:05,160 --> 00:24:10,080
性值，即位数，不再只是与可能匹配的数量冗余。

401
00:24:10,080 --> 00:24:16,520
现在，如果我们将其拉高并计算 2 的 8。02，略高于 256，我

402
00:24:16,520 --> 00:24:22,640
猜是 259，它的意思是，尽管总共有 526

403
00:24:22,640 --> 00:24:26,400
个单词实际上与此模式匹配，但它所具有的不确定性量

404
00:24:26,400 --> 00:24:29,760
更类似于如果有 259 个同样可能的单词结果。

405
00:24:29,760 --> 00:24:31,100
你可以这样想。

406
00:24:31,100 --> 00:24:35,560
它知道 borx 不是答案，与 yorts、zorl 和

407
00:24:35,560 --> 00:24:37,840
zorus 一样，因此它的不确定性比之前的情况要小一些。

408
00:24:37,840 --> 00:24:40,220
这个位数会更小。

409
00:24:40,220 --> 00:24:44,040
如果我继续玩这个游戏，我会用一些与我想在

410
00:24:44,040 --> 00:24:48,680
这里解释的内容相符的猜测来完善这个游戏。

411
00:24:48,680 --> 00:24:52,520
通过第四种猜测，如果你看一下它的首

412
00:24:52,520 --> 00:24:53,800
选，你会发现它不再只是最大化熵。

413
00:24:53,800 --> 00:24:58,480
所以在这一点上，技术上有七种可能性

414
00:24:58,480 --> 00:25:00,780
，但唯一有意义的机会是宿舍和单词。

415
00:25:00,780 --> 00:25:04,760
您可以看到它对选择这两个值的排名高于所

416
00:25:04,760 --> 00:25:07,560
有其他值，严格来说，这会提供更多信息。

417
00:25:07,560 --> 00:25:11,200
我第一次这样做时，我只是将这两个数字相加来衡

418
00:25:11,200 --> 00:25:14,580
量每个猜测的质量，这实际上比你想象的要好。

419
00:25:14,580 --> 00:25:17,600
但它确实感觉不系统，而且我确信人们可

420
00:25:17,600 --> 00:25:19,880
以采取其他方法，但这是我找到的方法。

421
00:25:19,880 --> 00:25:24,200
如果我们正在考虑下一次猜测的前景，就像在这种情况下的话，

422
00:25:24,200 --> 00:25:28,440
我们真正关心的是如果我们这样做的话我们游戏的预期得分。

423
00:25:28,440 --> 00:25:32,880
为了计算预期分数，我们会计算单词是实际

424
00:25:32,880 --> 00:25:35,640
答案的概率是多少，目前描述为 58%。

425
00:25:36,080 --> 00:25:40,400
我们说，有 58% 的机会，我们在这场比赛中得分为 4。

426
00:25:40,400 --> 00:25:46,240
然后，以 1 减去 58% 的概率，我们的分数将大于 4。

427
00:25:46,240 --> 00:25:50,640
我们不知道还有多少，但我们可以根据到

428
00:25:50,640 --> 00:25:52,920
达这一点后可能存在的不确定性来估计。

429
00:25:52,920 --> 00:25:56,600
具体来说，目前有1。44 位不确定性。

430
00:25:56,600 --> 00:26:01,560
如果我们猜测单词，它会告诉我们预期得到的信息是 1。27 位。

431
00:26:01,560 --> 00:26:06,280
因此，如果我们猜测单词，这种差异代表了在这

432
00:26:06,280 --> 00:26:08,280
种情况发生后我们可能会留下多少不确定性。

433
00:26:08,280 --> 00:26:12,500
我们需要的是某种函数，我在这里称之为 f

434
00:26:12,500 --> 00:26:13,880
，它将这种不确定性与预期分数联系起来。

435
00:26:13,880 --> 00:26:18,040
它的处理方式是根据机器人的版本 1 绘制之

436
00:26:18,040 --> 00:26:23,920
前游戏的一堆数据，以说明在具有某些非常可测

437
00:26:23,920 --> 00:26:27,040
量的不确定性的各个点之后的实际得分是多少。

438
00:26:27,040 --> 00:26:31,120
例如，这里的这些数据点位于 8 左右的值之上。在

439
00:26:31,120 --> 00:26:36,840
8分之后的一些比赛中，大约有7分左右。7位不确定

440
00:26:36,840 --> 00:26:39,340
性，经过两次猜测才得到最终答案。

441
00:26:39,340 --> 00:26:43,180
对于其他游戏需要猜测三次，对于其他游戏需要猜测四次。

442
00:26:43,180 --> 00:26:46,920
如果我们在这里向左移动，所有超过零的点都表明，

443
00:26:46,920 --> 00:26:51,620
每当不确定性为零时，也就是说只有一种可能性，那

444
00:26:51,620 --> 00:26:55,000
么所需的猜测次数总是只有一次，这是令人放心的。

445
00:26:55,000 --> 00:26:59,020
每当有一点不确定性时，意味着基本上只

446
00:26:59,020 --> 00:27:02,360
有两种可能性，那么有时需要再进行一

447
00:27:02,360 --> 00:27:03,940
次猜测，有时则需要再进行两次猜测。

448
00:27:03,940 --> 00:27:05,980
这里等等等等。

449
00:27:05,980 --> 00:27:11,020
也许可视化这些数据的一种稍微简单的方法是将其放在一起并取平均值。

450
00:27:11,020 --> 00:27:15,940
例如，这里的这个条表示，在我们有一点不确定性

451
00:27:15,940 --> 00:27:22,420
的所有点中，平均所需的新猜测数量约为 1。5.

452
00:27:22,420 --> 00:27:25,920
这里的栏表示在所有不同的游戏中，在某些

453
00:27:25,920 --> 00:27:30,480
时候不确定性略高于 4 位，这就像将

454
00:27:30,480 --> 00:27:35,120
其缩小到 16 种不同的可能性，然后从

455
00:27:35,120 --> 00:27:36,240
该点开始平均需要两个以上的猜测向前。

456
00:27:36,240 --> 00:27:40,080
从这里开始，我只是做了一个回归来拟合一个看起来合理的函数。

457
00:27:40,080 --> 00:27:44,160
请记住，这样做的全部目的是为了我们可以量化这种直觉

458
00:27:44,160 --> 00:27:49,720
，即我们从单词中获得的信息越多，预期得分就越低。

459
00:27:49,720 --> 00:27:54,380
所以将此作为版本 2。0，如果我们返回并运行相同的一组模拟，让它

460
00:27:54,380 --> 00:27:59,820
与所有 2315 个可能的单词答案进行比较，结果如何？

461
00:27:59,820 --> 00:28:04,060
与我们的第一个版本相比，它肯定更好，这令人放心。

462
00:28:04,060 --> 00:28:08,780
综上所述，平均值约为 3。6，尽管与第一个版本不同，

463
00:28:08,780 --> 00:28:12,820
它有几次丢失并且在这种情况下需要超过 6 个。

464
00:28:12,820 --> 00:28:15,980
大概是因为有时需要进行权衡以实

465
00:28:15,980 --> 00:28:18,980
际实现目标，而不是最大化信息。

466
00:28:18,980 --> 00:28:22,140
那么我们可以做得比 3 更好吗？6？

467
00:28:22,140 --> 00:28:23,260
我们绝对可以。

468
00:28:23,260 --> 00:28:27,120
现在我在一开始就说过，尝试不将单词答案的真

469
00:28:27,120 --> 00:28:29,980
实列表合并到构建模型的方式中是最有趣的。

470
00:28:29,980 --> 00:28:35,180
但如果我们确实将其合并，我可以获得的最佳性能约为 3。43.

471
00:28:35,180 --> 00:28:39,520
因此，如果我们尝试比仅使用词频数据更复杂地选择此先验分

472
00:28:39,520 --> 00:28:44,220
布，则这 3.43 可能给出了我们可以做到什么

473
00:28:44,220 --> 00:28:46,360
程度的最大值，或者至少是我可以做到什么程度。

474
00:28:46,360 --> 00:28:50,240
最佳性能本质上只是使用了我在这里讨

475
00:28:50,240 --> 00:28:53,400
论的想法，但它更进一步，就像它向前

476
00:28:53,400 --> 00:28:55,660
两步而不是一步搜索预期信息一样。

477
00:28:55,660 --> 00:28:58,720
本来我打算更多地讨论这个问题，但我意

478
00:28:58,720 --> 00:29:00,580
识到我们实际上已经讨论了很长时间了。

479
00:29:00,580 --> 00:29:03,520
我要说的一件事是，在进行了两步搜索，然后在顶级

480
00:29:03,520 --> 00:29:07,720
候选者中运行了几个样本模拟之后，至少到目前为止

481
00:29:07,720 --> 00:29:09,500
对我来说，Crane 看起来是最好的开局者。

482
00:29:09,500 --> 00:29:11,080
谁能想到呢？

483
00:29:11,080 --> 00:29:15,680
此外，如果您使用真实的单词列表来确定您的可能性

484
00:29:15,680 --> 00:29:17,920
空间，那么您开始的不确定性将略高于 11 位。

485
00:29:18,160 --> 00:29:22,760
事实证明，仅通过强力搜索，前两次猜测

486
00:29:22,760 --> 00:29:26,580
后最大可能的预期信息约为 10 位。

487
00:29:26,580 --> 00:29:31,720
这表明，在最好的情况下，在您进行前两次猜测之后，

488
00:29:31,720 --> 00:29:35,220
如果有完美的最佳玩法，您将留下大约一点不确定性。

489
00:29:35,220 --> 00:29:37,400
这与归结为两种可能的猜测相同。

490
00:29:37,400 --> 00:29:41,440
因此，我认为公平且可能相当保守地说，您永远不可能编

491
00:29:41,440 --> 00:29:45,620
写一个使平均值低至 3 的算法，因为根据您可用的

492
00:29:45,620 --> 00:29:50,460
单词，在仅执行两个步骤后根本没有空间获得足够的信息

493
00:29:50,460 --> 00:29:53,820
。能够保证每次都在第三个槽中得到答案，不会失败。

