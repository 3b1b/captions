1
00:00:00,000 --> 00:00:04,220
Suppose I give you two different lists of numbers, or maybe two different functions,

2
00:00:04,700 --> 00:00:08,520
and I ask you to think of all the ways you might combine those two lists to get a new list of

3
00:00:08,520 --> 00:00:14,060
numbers, or combine the two functions to get a new function. Maybe one simple way that comes to mind

4
00:00:14,060 --> 00:00:18,760
is to simply add them together term by term. Likewise with the functions, you can add all

5
00:00:18,760 --> 00:00:23,860
the corresponding outputs. In a similar vein, you could also multiply the two lists term by term

6
00:00:23,860 --> 00:00:28,860
and do the same thing with the functions. But there's another kind of combination just as

7
00:00:28,860 --> 00:00:33,500
fundamental as both of those, but a lot less commonly discussed, known as a convolution.

8
00:00:34,080 --> 00:00:39,820
But unlike the previous two cases, it's not something that's merely inherited from an operation you can do to numbers.

9
00:00:39,980 --> 00:00:44,700
It's something genuinely new for the context of lists of numbers or combining functions.

10
00:00:45,320 --> 00:00:51,620
They show up all over the place, they are ubiquitous in image processing, it's a core construct in the theory of probability,

11
00:00:52,220 --> 00:00:56,880
they're used a lot in solving differential equations, and one context where you've almost certainly seen it,

12
00:00:56,880 --> 00:01:02,840
if not by this name, is multiplying two polynomials together. As someone in the business of visual explanations,

13
00:01:03,300 --> 00:01:10,020
this is an especially great topic, because the formulaic definition in isolation and without context can look kind of intimidating,

14
00:01:10,160 --> 00:01:16,280
but if we take the time to really unpack what it's saying, and before that actually motivate why you would want something like this,

15
00:01:16,540 --> 00:01:23,540
it's an incredibly beautiful operation. And I have to admit, I actually learned a little something while putting together the visuals for this project.

16
00:01:23,540 --> 00:01:29,020
In the case of convolving two different functions, I was trying to think of different ways you might picture what that could mean,

17
00:01:29,340 --> 00:01:36,440
and with one of them I had a little bit of an aha moment for why it is that normal distributions play the role that they do in probability,

18
00:01:36,660 --> 00:01:41,520
why it's such a natural shape for a function. But I'm getting ahead of myself, there's a lot of setup for that one.

19
00:01:41,840 --> 00:01:50,260
In this video, our primary focus is just going to be on the discrete case, and in particular building up to a very unexpected but very clever algorithm for computing these.

20
00:01:50,260 --> 00:01:54,480
And I'll pull out the discussion for the continuous case into a second part.

21
00:01:58,580 --> 00:02:03,660
It's very tempting to open up with the image processing examples, since they're visually the most intriguing,

22
00:02:03,960 --> 00:02:09,020
but there are a couple bits of finickiness that make the image processing case less representative of convolutions overall,

23
00:02:09,320 --> 00:02:16,600
so instead let's kick things off with probability, and in particular one of the simplest examples that I'm sure everyone here has thought about at some point in their life,

24
00:02:16,600 --> 00:02:21,500
which is rolling a pair of dice and figuring out the chances of seeing various different sums.

25
00:02:22,460 --> 00:02:27,160
And you might say, not a problem, not a problem. Each of your two dice has six different possible outcomes,

26
00:02:27,580 --> 00:02:35,860
which gives us a total of 36 distinct possible pairs of outcomes, and if we just look through them all we can count up how many pairs have a given sum.

27
00:02:36,600 --> 00:02:45,440
And arranging all the pairs in a grid like this, one pretty nice thing is that all of the pairs that have a constant sum are visible along one of these different diagonals.

28
00:02:45,440 --> 00:02:52,120
So simply counting how many exist on each of those diagonals will tell you how likely you are to see a particular sum.

29
00:02:53,220 --> 00:02:58,660
And I'd say, very good, very good, but can you think of any other ways that you might visualize the same question?

30
00:02:59,300 --> 00:03:04,060
Other images that can come to mind to think of all the distinct pairs that have a given sum?

31
00:03:04,860 --> 00:03:07,980
And maybe one of you raises your hand and says, yeah, I've got one.

32
00:03:08,280 --> 00:03:13,760
Let's say you picture these two different sets of possibilities each in a row, but you flip around that second row.

33
00:03:13,760 --> 00:03:18,760
That way all of the different pairs which add up to seven line up vertically like this.

34
00:03:19,360 --> 00:03:24,560
And if we slide that bottom row all the way to the right, then the unique pair that adds up to two, the snake eyes,

35
00:03:24,920 --> 00:03:28,880
are the only ones that align, and if I schlunk that over one unit to the right,

36
00:03:29,220 --> 00:03:32,080
the pairs which align are the two different pairs that add up to three.

37
00:03:32,880 --> 00:03:38,540
And in general, different offset values of this lower array, which remember I had to flip around first,

38
00:03:38,980 --> 00:03:41,460
reveal all the distinct pairs that have a given sum.

39
00:03:44,820 --> 00:03:52,640
As far as probability questions go, this still isn't especially interesting because all we're doing is counting how many outcomes there are in each of these categories.

40
00:03:52,980 --> 00:03:58,120
But that is with the implicit assumption that there's an equal chance for each of these faces to come up.

41
00:03:58,360 --> 00:04:01,620
But what if I told you I have a special set of dice that's not uniform?

42
00:04:02,060 --> 00:04:06,580
Maybe the blue die has its own set of numbers describing the probabilities for each face coming up,

43
00:04:06,820 --> 00:04:09,760
and the red die has its own unique distinct set of numbers.

44
00:04:10,300 --> 00:04:13,860
In that case, if you wanted to figure out, say, the probability of seeing a 2,

45
00:04:14,440 --> 00:04:19,860
you would multiply the probability that the blue die is a 1 times the probability that the red die is a 1.

46
00:04:20,280 --> 00:04:24,440
And for the chances of seeing a 3, you look at the two distinct pairs where that's possible,

47
00:04:24,960 --> 00:04:29,680
and again multiply the corresponding probabilities and then add those two products together.

48
00:04:30,100 --> 00:04:36,820
Similarly, the chances of seeing a 4 involves multiplying together three different pairs of possibilities and adding them all together.

49
00:04:36,820 --> 00:04:42,920
And in the spirit of setting up some formulas, let's name these top probabilities a 1, a 2, a 3, and so on,

50
00:04:43,000 --> 00:04:45,960
and name the bottom ones b 1, b 2, b 3, and so on.

51
00:04:46,400 --> 00:04:50,320
And in general, this process where we're taking two different arrays of numbers,

52
00:04:50,640 --> 00:04:57,700
flipping the second one around, and then lining them up at various different offset values, taking a bunch of pairwise products and adding them up,

53
00:04:57,800 --> 00:05:01,620
that's one of the fundamental ways to think about what a convolution is.

54
00:05:04,860 --> 00:05:08,140
So just to spell it out a little more exactly, through this process,

55
00:05:08,280 --> 00:05:12,100
we just generated probabilities for seeing 2, 3, 4, on and on up to 12,

56
00:05:12,460 --> 00:05:16,980
and we got them by mixing together one list of values, a, and another list of values, b.

57
00:05:17,440 --> 00:05:24,660
In the lingo, we'd say the convolution of those two sequences gives us this new sequence, the new sequence of 11 values,

58
00:05:24,780 --> 00:05:27,340
each of which looks like some sum of pairwise products.

59
00:05:27,340 --> 00:05:30,720
If you prefer, another way you could think about the same operation

60
00:05:30,720 --> 00:05:36,980
is to first create a table of all the pairwise products, and then add up along all these diagonals.

61
00:05:37,460 --> 00:05:42,760
Again, that's a way of mixing together these two sequences of numbers to get us a new sequence of 11 numbers.

62
00:05:43,240 --> 00:05:46,460
It's the same operation as the sliding windows thought, just another perspective.

63
00:05:47,140 --> 00:05:49,800
Putting a little notation to it, here's how you might see it written down.

64
00:05:50,220 --> 00:05:54,540
The convolution of a and b, denoted with this little asterisk, is a new list,

65
00:05:54,540 --> 00:05:57,780
and the nth element of that list looks like a sum,

66
00:05:58,520 --> 00:06:01,760
and that sum goes over all different pairs of indices, i and j,

67
00:06:02,260 --> 00:06:07,140
so that the sum of those indices is equal to n. It's kind of a mouthful, but for example,

68
00:06:07,600 --> 00:06:13,920
if n was 6, the pairs we're going over are 1 and 5, 2 and 4, 3 and 3, 4 and 2, 5 and 1,

69
00:06:14,100 --> 00:06:15,800
all the different pairs that add up to 6.

70
00:06:16,620 --> 00:06:22,340
But honestly, however you write it down, the notation is secondary in importance to the visual you might hold in your head for the process.

71
00:06:23,280 --> 00:06:27,040
Here, maybe it helps to do a super simple example, where I might ask you

72
00:06:27,040 --> 00:06:30,780
what's the convolution of the list 1, 2, 3 with the list 4, 5, 6.

73
00:06:31,480 --> 00:06:34,900
You might picture taking both of these lists, flipping around that second one,

74
00:06:35,320 --> 00:06:37,680
and then starting with its lid all the way over to the left.

75
00:06:38,180 --> 00:06:43,540
Then the pair of values which align are 1 and 4, multiply them together, and that gives us our first term of our output.

76
00:06:43,960 --> 00:06:50,400
Slide that bottom array one unit to the right, the pairs which align are 1, 5 and 2 and 4, multiply those pairs,

77
00:06:50,400 --> 00:06:54,460
add them together, and that gives us 13, the next entry in our output.

78
00:06:54,960 --> 00:07:01,560
Slide things over once more, and we'll take 1 times 6 plus 2 times 5 plus 3 times 4, which happens to be 28.

79
00:07:02,020 --> 00:07:07,020
One more slide and we get 2 times 6 plus 3 times 5, and that gives us 27.

80
00:07:07,500 --> 00:07:10,120
And finally the last term will look like 3 times 6.

81
00:07:10,660 --> 00:07:16,860
If you'd like, you can pull up whatever your favorite programming language is and your favorite library that includes various numerical operations,

82
00:07:16,860 --> 00:07:18,980
and you can confirm I'm not lying to you.

83
00:07:18,980 --> 00:07:24,480
If you take the convolution of 1, 2, 3 against 4, 5, 6, this is indeed the result that you'll get.

84
00:07:25,920 --> 00:07:30,840
We've seen one case where this is a natural and desirable operation, adding up to probability distributions,

85
00:07:31,240 --> 00:07:33,660
and another common example would be a moving average.

86
00:07:34,080 --> 00:07:39,760
Imagine you have some long list of numbers, and you take another smaller list of numbers that all add up to 1.

87
00:07:40,100 --> 00:07:44,060
In this case, I just have a little list of 5 values and they're all equal to 1 5th.

88
00:07:44,060 --> 00:07:49,840
Then if we do this sliding window convolution process, and kind of close our eyes and sweep under the rug

89
00:07:49,840 --> 00:07:55,560
what happens at the very beginning of it, once our smaller list of values entirely overlaps with the bigger one,

90
00:07:55,880 --> 00:07:58,700
think about what each term in this convolution really means.

91
00:07:59,400 --> 00:08:05,720
At each iteration, what you're doing is multiplying each of the values from your data by 1 5th and adding them all together,

92
00:08:06,160 --> 00:08:10,520
which is to say you're taking an average of your data inside this little window.

93
00:08:11,100 --> 00:08:14,820
Overall, the process gives you a smoothed out version of the original data,

94
00:08:15,380 --> 00:08:18,160
and you could modify this starting with a different little list of numbers,

95
00:08:18,580 --> 00:08:22,720
and as long as that little list all adds up to 1, you can still interpret it as a moving average.

96
00:08:23,400 --> 00:08:27,760
In the example shown here, that moving average would be giving more weight towards the central value.

97
00:08:28,420 --> 00:08:30,800
This also results in a smoothed out version of the data.

98
00:08:33,140 --> 00:08:38,720
If you do kind of a two-dimensional analog of this, it gives you a fun algorithm for blurring a given image.

99
00:08:38,720 --> 00:08:44,520
And I should say the animations I'm about to show are modified from something I originally made for

100
00:08:44,520 --> 00:08:51,080
part of a set of lectures I did with the Julia lab at MIT for a certain open courseware class that included an image processing unit.

101
00:08:51,560 --> 00:08:54,340
There we did a little bit more to dive into the code behind all of this,

102
00:08:54,340 --> 00:08:58,580
so if you're curious, I'll leave you some links. But focusing back on this blurring example,

103
00:08:58,960 --> 00:09:03,740
what's going on is I've got this little 3x3 grid of values that's marching along our original image,

104
00:09:04,140 --> 00:09:07,280
and if we zoom in, each one of those values is 1 9th,

105
00:09:07,280 --> 00:09:13,620
and what I'm doing at each iteration is multiplying each of those values by the corresponding pixel that it sits on top of.

106
00:09:13,900 --> 00:09:20,200
And of course in computer science, we think of colors as little vectors of three values, representing the red, green, and blue components.

107
00:09:20,560 --> 00:09:23,940
When I multiply all these little values by 1 9th and I add them together,

108
00:09:24,360 --> 00:09:31,200
it gives us an average along each color channel, and the corresponding pixel for the image on the right is defined to be that sum.

109
00:09:31,940 --> 00:09:35,140
The overall effect, as we do this for every single pixel on the image,

110
00:09:35,140 --> 00:09:40,860
is that each one kind of bleeds into all of its neighbors, which gives us a blurrier version than the original.

111
00:09:41,720 --> 00:09:47,740
In the lingo, we'd say that the image on the right is a convolution of our original image with a little grid of values.

112
00:09:48,140 --> 00:09:54,400
Or more technically, maybe I should say that it's the convolution with a 180 degree rotated version of that little grid of values.

113
00:09:54,620 --> 00:09:56,760
Not that it matters when the grid is symmetric,

114
00:09:56,760 --> 00:10:01,980
but it's just worth keeping in mind that the definition of a convolution, as inherited from the pure math context,

115
00:10:01,980 --> 00:10:05,240
should always invite you to think about flipping around that second array.

116
00:10:05,960 --> 00:10:11,100
If we modify this slightly, we can get a much more elegant blurring effect by choosing a different grid of values.

117
00:10:11,440 --> 00:10:13,300
In this case, I have a little 5x5 grid,

118
00:10:13,600 --> 00:10:19,140
but the distinction is not so much its size. If we zoom in, we notice that the value in the middle is a lot bigger

119
00:10:19,140 --> 00:10:25,940
than the value towards the edges. And where this is coming from is they're all sampled from a bell curve, known as a Gaussian distribution.

120
00:10:26,800 --> 00:10:31,480
That way, when we multiply all of these values by the corresponding pixel that they're sitting on top of,

121
00:10:31,480 --> 00:10:36,380
we're giving a lot more weight to that central pixel, and much less towards the ones out at the edge.

122
00:10:36,800 --> 00:10:40,560
And just as before, the corresponding pixel on the right is defined to be this sum.

123
00:10:41,320 --> 00:10:47,360
As we do this process for every single pixel, it gives a blurring effect, which much more authentically simulates the notion of

124
00:10:47,360 --> 00:10:49,720
putting your lens out of focus, or something like that.

125
00:10:49,900 --> 00:10:53,360
But blurring is far from the only thing that you can do with this idea.

126
00:10:53,800 --> 00:10:58,580
For instance, take a look at this little grid of values, which involves some positive numbers on the left,

127
00:10:58,580 --> 00:11:02,900
and some negative numbers on the right, which I'll color with blue and red respectively.

128
00:11:03,640 --> 00:11:08,480
Take a moment to see if you can predict and understand what effect this will have on the final image.

129
00:11:10,720 --> 00:11:14,820
So in this case, I'll just be thinking of the image as grayscale instead of colored,

130
00:11:15,020 --> 00:11:19,340
so each of the pixels is just represented by one number instead of three. And one thing worth noticing

131
00:11:19,340 --> 00:11:23,060
is that as we do this convolution, it's possible to get negative values.

132
00:11:23,060 --> 00:11:28,860
For example, at this point here, if we zoom in, the left half of our little grid sits entirely on top of black pixels,

133
00:11:29,120 --> 00:11:34,040
which would have a value of zero, but the right half of negative values all sit on top of white pixels,

134
00:11:34,200 --> 00:11:35,460
which would have a value of one.

135
00:11:36,180 --> 00:11:40,820
So when we multiply corresponding terms and add them together, the results will be very negative.

136
00:11:41,160 --> 00:11:46,360
And the way I'm displaying this with the image on the right is to color negative values red and positive values blue.

137
00:11:46,880 --> 00:11:49,920
Another thing to notice is that when you're on a patch that's all the same color,

138
00:11:49,920 --> 00:11:54,080
everything goes to zero, since the sum of the values in our little grid is zero.

139
00:11:55,180 --> 00:11:58,980
This is very different from the previous two examples where the sum of our little grid was one,

140
00:11:59,340 --> 00:12:02,180
which let us interpret it as a moving average and hence a blur.

141
00:12:03,640 --> 00:12:07,120
All in all, this little process basically detects wherever there's

142
00:12:07,120 --> 00:12:10,140
variation in the pixel value as you move from left to right,

143
00:12:10,320 --> 00:12:13,920
and so it gives you a kind of way to pick up on all the vertical edges from your image.

144
00:12:16,500 --> 00:12:21,360
And similarly, if we rotated that grid around so that it varies as you move from the top to the bottom,

145
00:12:21,800 --> 00:12:23,920
this will be picking up on all the horizontal edges,

146
00:12:24,720 --> 00:12:29,340
which in the case of our little pie creature image does result in some pretty demonic eyes.

147
00:12:30,400 --> 00:12:35,920
This smaller grid, by the way, is often called a kernel, and the beauty here is how just by choosing a different kernel,

148
00:12:36,040 --> 00:12:40,840
you can get different image processing effects, not just blurring your edge detection, but also things like sharpening.

149
00:12:40,840 --> 00:12:43,820
For those of you who have heard of a convolutional neural network,

150
00:12:44,100 --> 00:12:48,220
the idea there is to use data to figure out what the kernels should be in the first place,

151
00:12:48,520 --> 00:12:51,480
as determined by whatever the neural network wants to detect.

152
00:12:52,760 --> 00:12:57,500
Another thing I should maybe bring up is the length of the output. For something like the moving average example,

153
00:12:57,860 --> 00:13:01,900
you might only want to think about the terms when both of the windows fully align with each other.

154
00:13:02,120 --> 00:13:07,280
Or in the image processing example, maybe you want the final output to have the same size as the original.

155
00:13:07,280 --> 00:13:13,940
Now, convolutions as a pure math operation always produce an array that's bigger than the two arrays that you started with,

156
00:13:14,140 --> 00:13:16,180
at least assuming one of them doesn't have a length of one.

157
00:13:16,720 --> 00:13:21,520
Just know that in certain computer science contexts, you often want to deliberately truncate that output.

158
00:13:24,720 --> 00:13:27,840
Another thing worth highlighting is that in the computer science context,

159
00:13:28,200 --> 00:13:34,940
this notion of flipping around that kernel before you let it march across the original often feels really weird and just uncalled for,

160
00:13:34,940 --> 00:13:40,140
but again, note that that's what's inherited from the pure math context, where like we saw with the probabilities,

161
00:13:40,540 --> 00:13:42,440
it's an incredibly natural thing to do.

162
00:13:43,020 --> 00:13:47,980
And actually, I can show you one more pure math example where even the programmers should care about this one,

163
00:13:48,240 --> 00:13:52,020
because it opens the doors for a much faster algorithm to compute all of these.

164
00:13:52,620 --> 00:13:56,760
To set up what I mean by faster here, let me go back and pull up some Python again,

165
00:13:56,980 --> 00:13:59,780
and I'm going to create two different relatively big arrays.

166
00:13:59,940 --> 00:14:04,540
Each one will have a hundred thousand random elements in it, and I'm going to assess the runtime,

167
00:14:04,940 --> 00:14:07,540
of the convolve function from the NumPy library.

168
00:14:08,180 --> 00:14:13,860
And in this case, it runs it for multiple different iterations, tries to find an average, and it looks like, on this computer at least,

169
00:14:14,320 --> 00:14:21,440
it averages at 4.87 seconds. By contrast, if I use a different function from the SciPy library called fftConvolve,

170
00:14:22,400 --> 00:14:30,160
which is the same thing just implemented differently, that only takes 4.3 milliseconds on average, so three orders of magnitude improvement.

171
00:14:30,160 --> 00:14:32,880
And again, even though it flies under a different name,

172
00:14:33,240 --> 00:14:39,120
it's giving the same output that the other convolve function does, it's just doing something to go about it in a cleverer way.

173
00:14:42,200 --> 00:14:44,140
Remember how with the probability example,

174
00:14:44,540 --> 00:14:49,620
I said another way you could think about the convolution was to create this table of all the pairwise products,

175
00:14:49,880 --> 00:14:52,680
and then add up those pairwise products along the diagonals.

176
00:14:53,660 --> 00:14:59,040
There's of course nothing specific to probability. Any time you're convolving two different lists of numbers, you can think about it this way.

177
00:14:59,040 --> 00:15:06,460
Create this kind of multiplication table with all pairwise products, and then each sum along the diagonal corresponds to one of your final outputs.

178
00:15:07,600 --> 00:15:12,800
One context where this view is especially natural is when you multiply together two polynomials.

179
00:15:13,300 --> 00:15:19,260
For example, let me take the little grid we already have and replace the top terms with 1, 2x, and 3x squared,

180
00:15:19,740 --> 00:15:23,600
and replace the other terms with 4, 5x, and 6x squared.

181
00:15:24,000 --> 00:15:28,840
Now, think about what it means when we're creating all of these different pairwise products between the two lists.

182
00:15:29,040 --> 00:15:34,620
What you're doing is essentially expanding out the full product of the two polynomials I have written down,

183
00:15:34,700 --> 00:15:39,900
and then when you add up along the diagonal, that corresponds to collecting all like terms.

184
00:15:40,600 --> 00:15:46,440
Which is pretty neat. Expanding a polynomial and collecting like terms is exactly the same process as a convolution.

185
00:15:47,740 --> 00:15:52,340
But this allows us to do something that's pretty cool, because think about what we're saying here.

186
00:15:52,340 --> 00:15:58,500
We're saying if you take two different functions and you multiply them together, which is a simple pointwise operation,

187
00:15:58,500 --> 00:16:04,900
that's the same thing as if you had first extracted the coefficients from each one of those, assuming they're polynomials,

188
00:16:05,280 --> 00:16:08,840
and then taken a convolution of those two lists of coefficients.

189
00:16:09,620 --> 00:16:15,360
What makes that so interesting is that convolutions feel, in principle, a lot more complicated than simple multiplication.

190
00:16:15,820 --> 00:16:19,040
And I don't just mean conceptually they're harder to think about. I mean,

191
00:16:19,400 --> 00:16:25,760
computationally, it requires more steps to perform a convolution than it does to perform a pointwise product of two different lists.

192
00:16:26,320 --> 00:16:31,900
For example, let's say I gave you two really big polynomials, say each one with a hundred different coefficients.

193
00:16:32,740 --> 00:16:40,000
Then if the way you multiply them was to expand out this product, you know, filling in this entire 100 by 100 grid of pairwise products,

194
00:16:40,360 --> 00:16:46,680
that would require you to perform 10,000 different products. And then, when you're collecting all the like terms along the diagonals,

195
00:16:47,080 --> 00:16:49,860
that's another set of around 10,000 operations.

196
00:16:50,700 --> 00:16:54,220
More generally, in the lingo, we'd say the algorithm is O of n squared,

197
00:16:54,220 --> 00:17:01,140
meaning for two lists of size n, the way that the number of operations scales is in proportion to the square of n.

198
00:17:01,820 --> 00:17:06,240
On the other hand, if I think of two polynomials in terms of their outputs, for example,

199
00:17:06,360 --> 00:17:11,700
sampling their values at some handful of inputs, then multiplying them only requires as many

200
00:17:11,700 --> 00:17:17,140
operations as the number of samples, since again, it's a pointwise operation. And with polynomials,

201
00:17:17,340 --> 00:17:20,540
you only need finitely many samples to be able to recover the coefficients.

202
00:17:20,540 --> 00:17:25,060
For example, two outputs are enough to uniquely specify a linear polynomial,

203
00:17:25,660 --> 00:17:32,440
three outputs would be enough to uniquely specify a quadratic polynomial, and in general, if you know n distinct outputs,

204
00:17:32,840 --> 00:17:36,740
that's enough to uniquely specify a polynomial that has n different coefficients.

205
00:17:37,440 --> 00:17:40,720
Or, if you prefer, we could phrase this in the language of systems of equations.

206
00:17:41,200 --> 00:17:45,200
Imagine I tell you I have some polynomial, but I don't tell you what the coefficients are.

207
00:17:45,260 --> 00:17:50,180
Those are a mystery to you. In our example, you might think of this as the product that we're trying to figure out.

208
00:17:50,180 --> 00:17:57,580
And then, suppose I say, I'll just tell you what the outputs of this polynomial would be if you inputted various different inputs, like

209
00:17:57,580 --> 00:18:03,460
0, 1, 2, 3, on and on, and I give you enough so that you have as many equations as you have unknowns.

210
00:18:04,140 --> 00:18:10,900
It even happens to be a linear system of equations, so that's nice, and in principle, at least, this should be enough to recover the coefficients.

211
00:18:11,740 --> 00:18:15,940
So the rough algorithm outline then would be, whenever you want to convolve two lists of numbers,

212
00:18:15,940 --> 00:18:21,880
you treat them like they're coefficients of two polynomials, you sample those polynomials at enough outputs,

213
00:18:22,780 --> 00:18:24,460
multiply those samples point-wise,

214
00:18:25,280 --> 00:18:30,560
and then solve this system to recover the coefficients as a sneaky backdoor way to find the convolution.

215
00:18:31,420 --> 00:18:37,340
And, as I've stated it so far, at least, some of you could rightfully complain, grant, that is an idiotic plan,

216
00:18:37,580 --> 00:18:41,720
because, for one thing, just calculating all these samples for one of the polynomials

217
00:18:41,720 --> 00:18:45,120
we know already takes on the order of n-squared operations.

218
00:18:45,600 --> 00:18:48,280
Not to mention, solving that system is certainly going to be

219
00:18:48,280 --> 00:18:52,100
computationally as difficult as just doing the convolution in the first place.

220
00:18:52,600 --> 00:18:56,360
So, like, sure, we have this connection between multiplication and convolutions,

221
00:18:56,480 --> 00:19:00,480
but all of the complexity happens in translating from one viewpoint to the other.

222
00:19:01,600 --> 00:19:07,740
But there is a trick, and those of you who know about Fourier transforms and the FFT algorithm might see where this is going.

223
00:19:07,740 --> 00:19:12,180
If you're unfamiliar with those topics, what I'm about to say might seem completely out of the blue.

224
00:19:12,260 --> 00:19:16,860
Just know that there are certain paths you could have walked in math that make this more of an expected step.

225
00:19:17,720 --> 00:19:20,360
Basically, the idea is that we have a freedom of choice here.

226
00:19:20,540 --> 00:19:27,780
If instead of evaluating at some arbitrary set of inputs, like 0, 1, 2, 3, on and on, you choose to evaluate on a very specially

227
00:19:27,780 --> 00:19:29,700
selected set of complex numbers,

228
00:19:30,240 --> 00:19:36,880
specifically the ones that sit evenly spaced on the unit circle, what are known as the roots of unity, this gives us a friendlier system.

229
00:19:38,360 --> 00:19:43,700
The basic idea is that by finding a number where taking its powers falls into this cycling pattern,

230
00:19:44,100 --> 00:19:49,340
it means that the system we generate is going to have a lot of redundancy in the different terms that you're calculating,

231
00:19:49,840 --> 00:19:54,460
and by being clever about how you leverage that redundancy, you can save yourself a lot of work.

232
00:19:56,020 --> 00:19:58,560
This set of outputs that I've written has a special name.

233
00:19:58,900 --> 00:20:03,720
It's called the discrete Fourier transform of the coefficients, and if you want to learn more,

234
00:20:03,720 --> 00:20:09,140
I actually did another lecture for that same Julia MIT class all about discrete Fourier transforms,

235
00:20:09,220 --> 00:20:14,420
and there's also a really excellent video on the channel Reducible talking about the fast Fourier transform,

236
00:20:14,560 --> 00:20:20,380
which is an algorithm for computing these more quickly. Also Veritasium recently did a really good video on FFTs,

237
00:20:20,400 --> 00:20:24,660
so you've got lots of options. And that fast algorithm really is the point for us.

238
00:20:25,120 --> 00:20:30,500
Again, because of all this redundancy, there exists a method to go from the coefficients to all of these outputs,

239
00:20:30,500 --> 00:20:36,700
where instead of doing on the order of n squared operations, you do on the order of n times the log of n operations,

240
00:20:36,760 --> 00:20:39,960
which is much much better as you scale to big lists. And,

241
00:20:40,180 --> 00:20:47,380
importantly, this FFT algorithm goes both ways. It also lets you go from the outputs to the coefficients. So, bringing it all together,

242
00:20:47,480 --> 00:20:52,460
let's look back at our algorithm outline. Now we can say, whenever you're given two long lists of numbers,

243
00:20:52,620 --> 00:20:57,760
and you want to take their convolution, first compute the fast Fourier transform of each one of them,

244
00:20:57,760 --> 00:21:03,040
which, in the back of your mind, you can just think of as treating them like they're the coefficients of a polynomial,

245
00:21:03,380 --> 00:21:06,380
and evaluating it at a very specially selected set of points.

246
00:21:06,900 --> 00:21:13,660
Then, multiply together the two results that you just got, point-wise, which is nice and fast, and then do an inverse fast Fourier transform,

247
00:21:14,560 --> 00:21:18,900
and what that gives you is the sneaky backdoor way to compute the convolution that we were looking for.

248
00:21:19,040 --> 00:21:22,240
But this time, it only involves O of n log n operations.

249
00:21:23,140 --> 00:21:29,400
That's really cool to me. This very specific context where convolutions show up, multiplying two polynomials,

250
00:21:29,880 --> 00:21:35,740
opens the doors for an algorithm that's relevant everywhere else where convolutions might come up. If you want to add probability

251
00:21:35,740 --> 00:21:42,240
distributions, do some large image processing, whatever it might be, and I just think that's such a good example of why you should be excited

252
00:21:42,240 --> 00:21:47,480
when you see some operation or concept in math show up in a lot of seemingly unrelated areas.

253
00:21:48,480 --> 00:21:51,500
If you want a little homework, here's something that's fun to think about.

254
00:21:51,720 --> 00:21:57,800
Explain why when you multiply two different numbers, just ordinary multiplication the way we all learn in elementary school,

255
00:21:58,160 --> 00:22:01,980
what you're doing is basically a convolution between the digits of those numbers.

256
00:22:02,500 --> 00:22:06,460
There's some added steps with carries and the like, but the core step is a convolution.

257
00:22:07,280 --> 00:22:12,800
In light of the existence of a fast algorithm, what that means is if you have two very large integers,

258
00:22:12,800 --> 00:22:19,620
then there exists a way to find their product that's faster than the method we learn in elementary school, that instead of requiring O of n

259
00:22:19,620 --> 00:22:24,920
squared operations, only requires O of n log n, which doesn't even feel like it should be possible.

260
00:22:25,380 --> 00:22:30,840
The catch is that before this is actually useful in practice, your numbers would have to be absolutely monstrous.

261
00:22:31,220 --> 00:22:33,860
But still, it's cool that such an algorithm exists.

262
00:22:35,160 --> 00:22:40,440
And next up, we'll turn our attention to the continuous case with a special focus on probability distributions.

