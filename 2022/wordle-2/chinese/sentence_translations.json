[
 {
  "input": "Last week I put up this video about solving the game Wordle, or at least trying to solve it, using information theory. ",
  "translatedText": "上周我发布了这段关于解决 Wordle 游戏的视频，或者至少尝试使用信息论来解决它。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.18
 },
 {
  "input": "And I wanted to add a quick, what should we call this, an addendum? ",
  "translatedText": "我想快速添加一个，我们应该称之为什么，附录？ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.58,
  "end": 9.78
 },
 {
  "input": "A confession? ",
  "translatedText": "坦白？ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.08,
  "end": 10.66
 },
 {
  "input": "Basically I just want to explain a place where I made a mistake. ",
  "translatedText": "基本上我只是想解释一下我犯错误的地方。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.02,
  "end": 13.9
 },
 {
  "input": "It turns out there was a very slight bug in the code that I was running to recreate Wordle and then run all of the algorithms to solve it and test their performance. ",
  "translatedText": "事实证明，我运行的重新创建 Wordle 的代码中有一个非常小的错误，然后运行所有算法来解决它并测试它们的性能。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.46,
  "end": 22.0
 },
 {
  "input": "And it's one of those bugs that affects a very small percentage of cases, so it was easy to miss, and it has only a very slight effect that for the most part doesn't really matter. ",
  "translatedText": "它是影响极小 比例案例的错误之一，因此很容易被忽视，而且它的影响 非常轻微，在大多数情况下并不重要。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 22.6,
  "end": 30.5
 },
 {
  "input": "Basically it had to do with how you assign a color to a guess that has multiple different letters in it. ",
  "translatedText": "基本上，它与如 何为包含多个不同字母的猜测分配颜色有关。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.22,
  "end": 36.36
 },
 {
  "input": "For example, if you guess speed and the true answer is abide, how should you color those two e's from the guess? ",
  "translatedText": "例如，如果您 猜测速度并且正确答案是 abide，那么您应该如何根据猜测对这两个 e 进行着色？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 36.52,
  "end": 42.12
 },
 {
  "input": "Well the way that it works with the Wordle conventions is that the first e would be colored yellow, and the second one would be colored gray. ",
  "translatedText": "它与 Wordle 约定的工作方式是第一个 e 为黄 色，第二个 e 为灰色。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.06,
  "end": 49.08
 },
 {
  "input": "You might think of that first one as matching up with something from the true answer, and the grayness is telling you there is no second e. ",
  "translatedText": "您可能会认为第一个 e 与真 实答案中的某些内容相匹配，灰色告诉您没有第二个 e。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 49.6,
  "end": 55.52
 },
 {
  "input": "By contrast, if the answer was something like erase, both of those e's would be colored yellow, telling you that there is a first e in a different location, and there's a second e also in a different location. ",
  "translatedText": "相比之下，如果答案类似于擦除，那么这两个 e 都会被染成黄色 ，告诉您第一个 e 位于不同的位置，并且第二个 e 也位于不 同的位置。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.52,
  "end": 66.78
 },
 {
  "input": "Similarly if one of the e's hits and it's green, then that second one would be gray in the case where the true answer has no second e, but it would be yellow in the case where there is a second e and it's just in a different location. ",
  "translatedText": "类似地，如果其中一个 e 命中并且它是绿色的，那么在真实答 案没有第二个 e 的情况下，第二个 e 将是灰色的，但在有第二个 e 并且只是在不同的情况下，它会是黄色的地点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.3,
  "end": 80.04
 },
 {
  "input": "Long story short, somewhere along the way I accidentally introduced behavior that differs from these conventions slightly. ",
  "translatedText": "长话短说，在这个过程 中，我不小心引入了与这些约定略有不同的行为。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.7,
  "end": 86.4
 },
 {
  "input": "Honestly it was really dumb. ",
  "translatedText": "老实说，这真的很愚蠢。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.1,
  "end": 88.54
 },
 {
  "input": "Basically at some point in the middle of the project I wanted to speed up some of the computations, and I was trying a little trick for how it computed the value for this pattern between any given pair of words, and you know I just didn't really think it through and it introduced this slight change. ",
  "translatedText": "基本上在项目中间的某个时刻，我想加快一些 计算速度，并且我尝试了一些小技巧来计算任何给定单词对之 间的模式值，你知道我只是没有这样做。并没有真正考虑清楚， 它引入了这个微小的变化。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.88,
  "end": 102.72
 },
 {
  "input": "The ironic part is that in the end the actual way to make things fastest is to pre-compute all those patterns so that everything is just a lookup, and so it wouldn't matter how long it takes to do each one, especially if you're writing hard to read buggy code to make it happen. ",
  "translatedText": "具有讽刺意味的是，最终让事情变得最快的 实际方法是预先计算所有这些模式，这样一切都只是一个查找，所以完成每个 模式需要多长时间并不重要，特别是如果你为了实现这一目标，我们正在编写 难以阅读的有缺陷的代码。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.22,
  "end": 115.84
 },
 {
  "input": "You know, you live and you learn. ",
  "translatedText": "你知道，你生活并学习。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 116.4,
  "end": 117.24
 },
 {
  "input": "As far as how this affects the actual video, I mean very little of substance really changes. ",
  "translatedText": "至于这如何影响实际 视频，我的意思是几乎没有实质内容发生变化。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.04,
  "end": 122.34
 },
 {
  "input": "Of course the main lessons about what is information, what is entropy, all that stays the same. ",
  "translatedText": "当然，关于什么是信息 、什么是熵的主要教训都保持不变。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 122.66,
  "end": 126.56
 },
 {
  "input": "Every now and then if I'm showing on screen some distribution associated with a given word, that distribution might actually be a little bit off because some of the buckets associated with various patterns should include either more or fewer true answers. ",
  "translatedText": "如果我时不时地在屏幕 上显示与给定单词相关的一些分布，那么该分布实际上可能会 有点偏差，因为与各种模式相关的一些存储桶应该包含更多 或更少的真实答案。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.86,
  "end": 140.32
 },
 {
  "input": "Even then it doesn't really come up because it was very rare that I would be showing a word that had multiple letters that also hit this edge case. ",
  "translatedText": "即使如此，它也没有真正出现，因为我很少会显 示一个包含多个字母的单词，这些字母也符合这种边缘情况。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.84,
  "end": 146.96
 },
 {
  "input": "But one of the very few things of substance that does change and that arguably does matter a fair bit was the final conclusion around how if we want to find the optimal possible score for the wordle answer list, what opening guess does such an algorithm use? ",
  "translatedText": "但是，确 实发生变化并且可以说确实很重要的极少数实质性事情之一是 最终结论：如果我们想找到单词答案列表的最佳可能得分，这样 的算法会使用什么开局猜测？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.68,
  "end": 162.46
 },
 {
  "input": "In the video I said the best performance that I could find came from opening with the word crane, which was true only in the sense that the algorithms were playing a very slightly different game. ",
  "translatedText": "在视频中，我说我能找到的最佳性 能来自于以“起重机”一词开头，这仅在算法正在玩一个略有 不同的游戏的意义上才是正确的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.08,
  "end": 172.56
 },
 {
  "input": "After fixing it and rerunning it all, there is a different answer for what the theoretically optimal first guess is for this particular list. ",
  "translatedText": "修复并重新运行后，对于这个特 定列表的理论上最佳的第一个猜测是什么，有一个不同的答案。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.16,
  "end": 180.16
 },
 {
  "input": "And look, I know that you know that the point of the video is not to find some technically optimal answer to some random online game. ",
  "translatedText": "看，我知道您知道视频的重点不是为某些随机在线游戏找 到技术上的最佳答案。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 181.0,
  "end": 189.1
 },
 {
  "input": "The point of the video is to shamelessly hop on the bandwagon of an internet trend to sneak attack people with an information theory lesson. ",
  "translatedText": "该视频的目的是厚颜无耻地搭 上互联网潮流的潮流，通过信息论课程偷袭人们。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.46,
  "end": 195.9
 },
 {
  "input": "And that's all good, I stand by that part. ",
  "translatedText": "这一切都很好，我支持这一点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.32,
  "end": 198.0
 },
 {
  "input": "But I know how the internet works, and for a lot of people the one main takeaway was what is the best opener for the game wordle. ",
  "translatedText": "但我知道互联网是如何运作的，对于很多人 来说，一个主要的收获是什么是游戏单词的最佳开场白。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.2,
  "end": 204.6
 },
 {
  "input": "And I get it, I walked into that because I put it in the thumbnail, but presumably you can forgive me if I want to add a little correction here. ",
  "translatedText": "我明白了，我之所 以谈到这一点是因为我把它放在缩略图中，但如果我想在这里添加一点更正，想 必你可以原谅我。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 205.28,
  "end": 211.86
 },
 {
  "input": "And a more meaningful reason to circle back to all this actually is that I never really talked about what went into that final analysis. ",
  "translatedText": "实际上，回到这一切的一个更有意义的原因 是，我从未真正谈论过最终分析的内容。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.98,
  "end": 218.34
 },
 {
  "input": "And it's interesting as a sublesson in its own right, so it's worth doing here. ",
  "translatedText": "作为一门子课， 它本身就很有趣，所以值得在这里做。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.84,
  "end": 222.42
 },
 {
  "input": "Now if you'll recall, most of our time last video was spent on the challenge of trying to write an algorithm to solve wordle that did not use the official list of all possible answers. ",
  "translatedText": "现在，如果您还记得的话，我们 上一个视频的大部分时间都花在尝试编写一个算法来解决 wordle 的挑 战上，该算法没有使用所有可能答案的官方列表。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.14,
  "end": 232.46
 },
 {
  "input": "To my taste, that feels a bit like overfitting to a test set, and what's more fun is building something that's resilient. ",
  "translatedText": "根据我的口味，这感觉有 点像过度拟合测试集，更有趣的是构建具有弹性的东西。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.98,
  "end": 238.48
 },
 {
  "input": "This is why we went through the whole process of looking at relative word frequencies in the English language to come up with some notion of how likely each one would be to be included as a final answer. ",
  "translatedText": "这就是 为什么我们经历了查看英语中相对词频的整个过程，以 得出每个词被包含作为最终答案的可能性的一些概念。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.9,
  "end": 248.76
 },
 {
  "input": "However, for what we're doing here, where we're just trying to find an absolute best performance period, I am incorporating that official list and just shamelessly overfitting to the test set, which is to say we know with certainty whether a word is included or not, and we can assign a uniform probability to each one. ",
  "translatedText": "然而，对于我们在这里所做的事情，我们只是试图找到一个绝对最佳的 性能时期，我将合并该官方列表，并且无耻地过度拟合测试集，也就 是说，我们确定地知道一个单词是否包含或不包含，我们可以为每一个 分配一个统一的概率。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 249.4,
  "end": 265.46
 },
 {
  "input": "If you'll remember, the first step in all of this was to say for a particular opening guess, maybe something like my old favorite, crane, how likely is it that you would see each of the possible patterns? ",
  "translatedText": "如果您还记得的话，所有这一切的第一步是 针对特定的开局猜测，也许像我以前最喜欢的起重机一样，您看到 每种可能模式的可能性有多大？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 266.44,
  "end": 276.18
 },
 {
  "input": "And in this context, where we are shamelessly overfitting to the wordle answer list, all that involves is counting how many of the possible answers give each one of these patterns. ",
  "translatedText": "在这种情况下，我们无耻地过度拟 合单词答案列表，所涉及的只是计算有多少可能的答案给出了 这些模式中的每一种。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 276.68,
  "end": 285.34
 },
 {
  "input": "And then of course most of our time was spent on this kind of funny looking formula to quantify the amount of information that you would get from this guess that basically involves going through each one of those buckets and saying how much information would you gain that has this log expression that is a fanciful way of saying how many times would you cut your space of possibilities in half if you observed a given pattern. ",
  "translatedText": "当然，我们的大部分时间都花在这种 看起来很有趣的公式上，以量化您将从这种猜测中获得的信 息量，该猜测基本上涉及遍历每个桶并说出您将获得多少 信息这个对数表达式是一种奇特的方式，表示如果您观察到 给定的模式，您会将可能性空间减半多少次。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.98,
  "end": 306.84
 },
 {
  "input": "We take a weighted average of all of those and it gives us a measure of how much we expect to learn from this first guess. ",
  "translatedText": "我们对所有这 些进行加权平均，它可以衡量我们期望从第一次猜测中学到多少东西 。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.6,
  "end": 313.18
 },
 {
  "input": "In a moment we'll go deeper than this, but if you simply search through all 13,000 different words that you could start with and you ask which one has the highest expected information, it turns out the best possible answer is soar, which doesn't really look like a real word, but I guess it's an obsolete term for a baby hawk. ",
  "translatedText": "稍后我们将对此进行更深入的探讨，但如果您只是简单地搜索所有 13,00 0 个可以开始使用的不同单词，并询问哪个单词具有最高的预期信息，结果会发 现最好的答案是 soar，而这并不会'看起来确实不像一个真正的词， 但我猜这是一个过时的术语，指的是小鹰。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.56,
  "end": 333.0
 },
 {
  "input": "The top 15 openers by this metric happen to look like this, but these are not necessarily the best opening guesses because they're only looking one step in with the heuristic of expected information to try to estimate what the true score will be. ",
  "translatedText": "按此指标计算的前 15 个开局碰巧看起来像这样，但这些不一定是最好的开局猜测 ，因为它们只是通过预期信息的启发式来尝试估计真实分 数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.04,
  "end": 347.54
 },
 {
  "input": "But there's few enough patterns that we can do an exhaustive search two steps in. ",
  "translatedText": "但我们可以通过两步进行详尽的搜索的模式很少。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 347.92,
  "end": 351.68
 },
 {
  "input": "For example, let's say you opened with soar and the pattern you happen to see was the most likely one, all grays, then you can run identical analysis from that point. ",
  "translatedText": "例如，假设您以飙升打开，并且您碰巧看到的模式是最有可能的模式， 全是灰色，那么您可以从该点开始运行相同的分析。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.16,
  "end": 360.8
 },
 {
  "input": "For a given proposed second guess, something like kitty, what's the distribution across all patterns in that restricted case where we're restricted only to the words that would produce all grays for soar, and then we measure the flatness of that distribution using this expected information formula, and we do that for all 13,000 possible words that we could use as a second guess. ",
  "translatedText": "对于给定的提议的第 二个猜测，比如 kitty，在这种受限情况下，所有模式的分布是什么，在 这种情况下，我们仅限于会产生 soar 的所有灰色的单词，然后我们使用 这个预期来测量该分布的平坦度信息公式，我们对所有 13,000 个可能 的单词都这样做，我们可以将其用作第二次猜测。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.32,
  "end": 381.42
 },
 {
  "input": "Doing this we can find the optimal second guess in that scenario and the amount of information we were expected to get from it, and if we wash rinse and repeat and do this for all of the different possible patterns that you might see, we get a full map of all the best possible second guesses together with the expected information of each. ",
  "translatedText": "通过这样做，我们可以找 到该场景中的最佳第二次猜测以及我们期望从中获得的信息量，如果 我们清洗并重复，并对您可能看到的所有不同的可能模式执行此操 作，我们会得到一个所有可能的最佳第二次猜测的完整地图以及每个 猜测的预期信息。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.12,
  "end": 399.2
 },
 {
  "input": "From there, if you take a weighted average of all those second step values, weighted according to how likely you are to fall into that bucket, it gives you a measure of how much information you're likely to gain from the guess soar after the second step. ",
  "translatedText": "从那里开始，如果您对所有第二步值进行 加权平均值，并根据您落入该桶的可能性进行加权，它可 以衡量您在猜测飙升后可能获得多少信息。第二步。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 416.8
 },
 {
  "input": "When we use this two-step metric as our new means of ranking, the list gets shaken up a bit. ",
  "translatedText": "当我 们使用这个两步指标作为新的排名方法时，列表会发生一些变化。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.38,
  "end": 421.78
 },
 {
  "input": "Soar is no longer first place, it falls back to 14th, and instead what rises to the top is slain. ",
  "translatedText": "Soa r不再是第一名，跌回第14名，而上升的则被淘汰。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.08,
  "end": 427.66
 },
 {
  "input": "Again, doesn't feel very real, and it looks like it is a British term for a spade that's used for cutting turf. ",
  "translatedText": "再说一 次，感觉不太真实，而且看起来它是一个英国术语，指的是用于切割草皮的铁锹 。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.64,
  "end": 436.38
 },
 {
  "input": "Alright, but as you can see it is a really tight race among all of these top contenders for who gains the most information after those two steps. ",
  "translatedText": "好吧，但正如您所看到的，所有这些顶级竞争者之间的竞争非常激烈 ，谁在这两步之后获得了最多的信息。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.2,
  "end": 445.0
 },
 {
  "input": "And even still, these are not necessarily the best opening guesses, because information is just the heuristic, it's not telling us the actual score if you actually play the game. ",
  "translatedText": "即便如此，这些不一定是最好 的开局猜测，因为信息只是启发式的，它并不能告诉我们如果 你真的玩游戏的话实际得分。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.7,
  "end": 454.0
 },
 {
  "input": "What I did is I ran the simulation of playing all 2315 possible wordle games with all possible answers on the top 250 from this list. ",
  "translatedText": "我所做的是模拟玩所有 2315 个 可能的文字游戏，所有可能的答案都在此列表中的前 250 个中。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 454.58,
  "end": 464.62
 },
 {
  "input": "And by doing this, seeing how they actually perform, the one that ends up very marginally with the best possible score turns out to be Salé, which is an alternate spelling for Salé, which is a light medieval helmet. ",
  "translatedText": "通过这样做，看看它们的实际表现，最终以微弱优势获得最高分的结 果是 Salé，它是 Salé 的另一种拼写，Salé 是 一种轻型中世纪头盔。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.46,
  "end": 485.98
 },
 {
  "input": "Alright, if that feels a little bit too fake for you, which it does for me, you'll be happy to know that Trace and Crate give almost identical performance. ",
  "translatedText": "好吧，如果这对您来说有点太假了（对我来说也是如 此），您会很高兴知道 Trace 和 Crate 提供几乎相同的性能。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.98,
  "end": 495.78
 },
 {
  "input": "Each of them has the benefit of obviously being a real word, so there is one day when you get it right on the first guess, since both are actual wordle answers. ",
  "translatedText": "它们中的每一个都有明显是真实单词的好处，所以有一天你第一次猜 测就正确，因为两者都是真实的单词答案。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.3,
  "end": 504.06
 },
 {
  "input": "This move from sorting based on the best two-step entropies to sorting based on the lowest average score also shakes up the list, but not nearly as much. ",
  "translatedText": "从基于最佳两步熵排序 到基于最低平均分数排序的这一转变也对列表产生了影响，但幅度 没有那么大。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 505.02,
  "end": 512.46
 },
 {
  "input": "For example, Salé was previously third place before it bubbles to the top, and Crate and Trace were both fourth and fifth. ",
  "translatedText": "例如，Salé 在跃居榜首之前曾排名第三，而 Crate 和 Trace 分别排名第四和第五。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.66,
  "end": 519.08
 },
 {
  "input": "If you're curious, you can get slightly better performance from here by doing a little brute forcing. ",
  "translatedText": "如果您好奇，您可以通过进行 一些暴力强制来获得稍微更好的性能。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 519.64,
  "end": 523.72
 },
 {
  "input": "There's a very nice blog post by Jonathan Olson, if you're curious about this, where he also lets you explore what the optimal following guesses are for a few of the starting words based on these optimal algorithms. ",
  "translatedText": "如果您对此感到好奇，乔纳 森·奥尔森（Jonathan Olson）有一篇非常好的博客文章 ，他还让您探索基于这些最佳算法的一些起始词的最佳以下猜测是什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.1,
  "end": 533.66
 },
 {
  "input": "Stepping back from all this though, I'm told by some people that it quote ruins the game to overanalyze it like this and try to find an optimal opening guess. ",
  "translatedText": "不过，退一步来说，有些人告诉我，像这样过度 分析并尝试找到最佳的开局猜测会毁掉游戏。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 535.18,
  "end": 543.6
 },
 {
  "input": "You know, it feels kind of dirty if you use that opening guess after learning it, and it feels inefficient if you don't. ",
  "translatedText": "你知道，如果你在学习之后使用这个开局猜测，感觉有点肮脏，如果你 不这样做，感觉效率很低。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.26,
  "end": 549.66
 },
 {
  "input": "But the thing is, I don't actually think this is the best opener for a human playing the game. ",
  "translatedText": "但问题是，我实际上并不认为这对于人类玩 游戏来说是最好的开局。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 549.8,
  "end": 554.4
 },
 {
  "input": "For one thing, you would need to know what the optimal second guess is for each one of the patterns that you see. ",
  "translatedText": "一方面，您需要知道您看到的每种 模式的最佳第二次猜测是什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.88,
  "end": 559.68
 },
 {
  "input": "And more importantly, all of this is in a setting where we are absurdly overfit to the official wordle answer list. ",
  "translatedText": "更重要的是，所有这一切都是在 我们与官方单词答案列表荒谬地过度拟合的情况下发生的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 560.26,
  "end": 566.36
 },
 {
  "input": "The moment that, say, the New York Times chooses to change what that list is under the hood, all of this would go out the window. ",
  "translatedText": "比如说，当 《纽约时报》选择更改该列表的内容时，所有这些都将 消失。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.58,
  "end": 572.88
 },
 {
  "input": "The way that we humans play the game is just very different from what any of these algorithms are doing. ",
  "translatedText": "我们人类玩游戏的方式与这些算法的做法非 常不同。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.58,
  "end": 577.68
 },
 {
  "input": "We don't have the word list memorized, we're not doing exhaustive searches, we get intuition from things like what are the vowels and how are they placed. ",
  "translatedText": "我们没有记住单词列表，我们没有进行详尽的搜索， 我们从诸如元音是什么以及它们如何放置之类的事情中获得直觉。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.02,
  "end": 585.08
 },
 {
  "input": "I would actually be most happy if those of you watching this video promptly forgot what happens to be the technically best opening guess, and instead came out remembering things like how do you quantify information, or the fact that you should look out for when a greedy algorithm falls short of the globally best performance that you would get from a deeper search. ",
  "translatedText": "实际上，如果你们观看此视频的人立即忘记技术上最好 的开局猜测是什么，而是记住诸如如何量化信息之类的 事情，或者当贪婪时您应该注意的事实，我会非常高兴 。算法达不到从更深入的搜索中获得的全局最佳性能。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.64,
  "end": 603.1
 },
 {
  "input": "For my taste at least, the joy of writing algorithms to try to play games actually has very little bearing on how I like to play those games as a human. ",
  "translatedText": "至少就我的口味而言，编写算法来尝试玩游戏的乐趣实际上与我作 为人类如何玩这些游戏几乎没有关系。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 603.7,
  "end": 610.76
 },
 {
  "input": "The point of writing algorithms for all this is not to affect the way that we play the game, it's still just a fun word game. ",
  "translatedText": "为这一切编写算法的目的不 是影响我们玩游戏的方式，它仍然只是一个有趣的文字游戏。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 611.3,
  "end": 616.78
 },
 {
  "input": "It's to hone in our muscles for writing algorithms in more meaningful contexts elsewhere. ",
  "translatedText": "这是 为了磨练我们的肌肉，以便在其他更有意义的环境中编写算法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.1,
  "end": 620.72
 }
]