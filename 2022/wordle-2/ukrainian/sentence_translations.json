[
 {
  "input": "Last week I put up this video about solving the game Wordle, or at least trying to solve it, using information theory. ",
  "translatedText": "Минулого тижня я опублікував це відео про розгадування гри Wordle або, принаймні, спробу розв’язати її за допомогою теорії інформації. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.18
 },
 {
  "input": "And I wanted to add a quick, what should we call this, an addendum? ",
  "translatedText": "І я хотів швидко додати, як ми маємо це назвати, додаток? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.58,
  "end": 9.78
 },
 {
  "input": "A confession? ",
  "translatedText": "зізнання? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.08,
  "end": 10.66
 },
 {
  "input": "Basically I just want to explain a place where I made a mistake. ",
  "translatedText": "По суті, я просто хочу пояснити місце, де я зробив помилку. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.02,
  "end": 13.9
 },
 {
  "input": "It turns out there was a very slight bug in the code that I was running to recreate Wordle and then run all of the algorithms to solve it and test their performance. ",
  "translatedText": "Виявилося, що в коді, який я запускав, щоб відтворити Wordle, а потім запустив усі алгоритми, щоб її вирішити та перевірити їх продуктивність, була дуже невелика помилка. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.46,
  "end": 22.0
 },
 {
  "input": "And it's one of those bugs that affects a very small percentage of cases, so it was easy to miss, and it has only a very slight effect that for the most part doesn't really matter. ",
  "translatedText": "І це одна з тих помилок, яка впливає на дуже невеликий відсоток випадків, тому її було легко пропустити, і вона має лише дуже незначний ефект, який здебільшого не має особливого значення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 22.6,
  "end": 30.5
 },
 {
  "input": "Basically it had to do with how you assign a color to a guess that has multiple different letters in it. ",
  "translatedText": "В основному це було пов’язано з тим, як ви призначаєте колір припущенню, яке містить кілька різних літер. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.22,
  "end": 36.36
 },
 {
  "input": "For example, if you guess speed and the true answer is abide, how should you color those two e's from the guess? ",
  "translatedText": "Наприклад, якщо ви вгадали швидкість, а вірною відповіддю є дотримуватися, як розфарбувати ці два «е» з припущення? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 36.52,
  "end": 42.12
 },
 {
  "input": "Well the way that it works with the Wordle conventions is that the first e would be colored yellow, and the second one would be colored gray. ",
  "translatedText": "Так, як це працює з угодами Wordle, перше e має жовтий колір, а другий — сірий. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.06,
  "end": 49.08
 },
 {
  "input": "You might think of that first one as matching up with something from the true answer, and the grayness is telling you there is no second e. ",
  "translatedText": "Ви можете подумати, що перша відповідь збігається з чимось із правдивої відповіді, а сірість говорить вам, що другого e не існує. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 49.6,
  "end": 55.52
 },
 {
  "input": "By contrast, if the answer was something like erase, both of those e's would be colored yellow, telling you that there is a first e in a different location, and there's a second e also in a different location. ",
  "translatedText": "Навпаки, якби відповіддю було щось на кшталт стерти, обидва ці e були б забарвлені в жовтий колір, повідомляючи вам, що перше e знаходиться в іншому місці, а також є друге e в іншому місці. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.52,
  "end": 66.78
 },
 {
  "input": "Similarly if one of the e's hits and it's green, then that second one would be gray in the case where the true answer has no second e, but it would be yellow in the case where there is a second e and it's just in a different location. ",
  "translatedText": "Подібним чином, якщо один із символів e потрапляє і він зелений, то другий символ буде сірим у випадку, коли у правдивій відповіді немає другого e, але він буде жовтим, якщо є другий e, і він просто в іншому Місцезнаходження. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.3,
  "end": 80.04
 },
 {
  "input": "Long story short, somewhere along the way I accidentally introduced behavior that differs from these conventions slightly. ",
  "translatedText": "Коротше кажучи, десь по дорозі я випадково представив поведінку, яка трохи відрізняється від цих конвенцій. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.7,
  "end": 86.4
 },
 {
  "input": "Honestly it was really dumb. ",
  "translatedText": "Чесно кажучи, це було справді тупо. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.1,
  "end": 88.54
 },
 {
  "input": "Basically at some point in the middle of the project I wanted to speed up some of the computations, and I was trying a little trick for how it computed the value for this pattern between any given pair of words, and you know I just didn't really think it through and it introduced this slight change. ",
  "translatedText": "По суті, в якийсь момент посередині проекту я хотів пришвидшити деякі обчислення, і я спробував невеликий трюк, щоб обчислити значення для цього шаблону між будь-якою даною парою слів, і ви знаєте, що я просто не Я дійсно не продумав це, і це ввело цю невелику зміну. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.88,
  "end": 102.72
 },
 {
  "input": "The ironic part is that in the end the actual way to make things fastest is to pre-compute all those patterns so that everything is just a lookup, and so it wouldn't matter how long it takes to do each one, especially if you're writing hard to read buggy code to make it happen. ",
  "translatedText": "Іронія полягає в тому, що врешті-решт справжній спосіб зробити все найшвидшим — це попередньо обчислити всі ці шаблони, щоб усе було лише пошуком, і тому не матиме значення, скільки часу потрібно для кожного з них, особливо якщо ви ми пишемо код з помилками, який важко читати, щоб це сталося. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.22,
  "end": 115.84
 },
 {
  "input": "You know, you live and you learn. ",
  "translatedText": "Знаєш, живеш і вчишся. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 116.4,
  "end": 117.24
 },
 {
  "input": "As far as how this affects the actual video, I mean very little of substance really changes. ",
  "translatedText": "Щодо того, як це впливає на фактичне відео, я маю на увазі, що істота насправді змінюється дуже мало. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.04,
  "end": 122.34
 },
 {
  "input": "Of course the main lessons about what is information, what is entropy, all that stays the same. ",
  "translatedText": "Звичайно, основні уроки про те, що таке інформація, що таке ентропія, все це залишається незмінним. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 122.66,
  "end": 126.56
 },
 {
  "input": "Every now and then if I'm showing on screen some distribution associated with a given word, that distribution might actually be a little bit off because some of the buckets associated with various patterns should include either more or fewer true answers. ",
  "translatedText": "Час від часу, якщо я показую на екрані якийсь розподіл, пов’язаний із заданим словом, цей розподіл насправді може бути дещо неправильним, оскільки деякі сегменти, пов’язані з різними шаблонами, мають містити або більше, або менше правильних відповідей. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.86,
  "end": 140.32
 },
 {
  "input": "Even then it doesn't really come up because it was very rare that I would be showing a word that had multiple letters that also hit this edge case. ",
  "translatedText": "Навіть тоді це не з’явилося, тому що я дуже рідко показував слово з кількома літерами, які також потрапляли в регістр. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.84,
  "end": 146.96
 },
 {
  "input": "But one of the very few things of substance that does change and that arguably does matter a fair bit was the final conclusion around how if we want to find the optimal possible score for the wordle answer list, what opening guess does such an algorithm use? ",
  "translatedText": "Але однією з небагатьох суттєвих речей, які все-таки змінюються і які, мабуть, мають неабияке значення, був остаточний висновок щодо того, як, якщо ми хочемо знайти оптимальний можливий бал для списку відповідей Wordle, яке початкове припущення використовує такий алгоритм? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.68,
  "end": 162.46
 },
 {
  "input": "In the video I said the best performance that I could find came from opening with the word crane, which was true only in the sense that the algorithms were playing a very slightly different game. ",
  "translatedText": "У відео я сказав, що найкраща ефективність, яку я міг знайти, досягнута від відкриття словом кран, що було правдою лише в тому сенсі, що алгоритми грали в зовсім іншу гру. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.08,
  "end": 172.56
 },
 {
  "input": "After fixing it and rerunning it all, there is a different answer for what the theoretically optimal first guess is for this particular list. ",
  "translatedText": "Після виправлення та повторного запуску всього, є інша відповідь щодо того, яке теоретично оптимальне перше припущення є для цього конкретного списку. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.16,
  "end": 180.16
 },
 {
  "input": "And look, I know that you know that the point of the video is not to find some technically optimal answer to some random online game. ",
  "translatedText": "І подивіться, я знаю, що ви знаєте, що суть відео полягає не в тому, щоб знайти якусь технічно оптимальну відповідь на якусь випадкову онлайн-гру. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 181.0,
  "end": 189.1
 },
 {
  "input": "The point of the video is to shamelessly hop on the bandwagon of an internet trend to sneak attack people with an information theory lesson. ",
  "translatedText": "Сенс відео полягає в тому, щоб безсоромно підхопити тенденцію в Інтернеті потай атакувати людей за допомогою уроку теорії інформації. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.46,
  "end": 195.9
 },
 {
  "input": "And that's all good, I stand by that part. ",
  "translatedText": "І це все добре, я підтримую цю частину. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.32,
  "end": 198.0
 },
 {
  "input": "But I know how the internet works, and for a lot of people the one main takeaway was what is the best opener for the game wordle. ",
  "translatedText": "Але я знаю, як працює Інтернет, і для багатьох людей головним висновком було те, що найкраще відкриває гру Wordle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.2,
  "end": 204.6
 },
 {
  "input": "And I get it, I walked into that because I put it in the thumbnail, but presumably you can forgive me if I want to add a little correction here. ",
  "translatedText": "І я розумію, я ввійшов у це, тому що я розмістив це на мініатюрі, але, мабуть, ви можете мені вибачити, якщо я хочу додати тут невелике виправлення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 205.28,
  "end": 211.86
 },
 {
  "input": "And a more meaningful reason to circle back to all this actually is that I never really talked about what went into that final analysis. ",
  "translatedText": "І більш вагомою причиною повернутися до всього цього насправді є те, що я ніколи насправді не говорив про те, що увійшло в цей остаточний аналіз. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.98,
  "end": 218.34
 },
 {
  "input": "And it's interesting as a sublesson in its own right, so it's worth doing here. ",
  "translatedText": "І це цікаво як самостійний підурок, тому це варто зробити тут. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.84,
  "end": 222.42
 },
 {
  "input": "Now if you'll recall, most of our time last video was spent on the challenge of trying to write an algorithm to solve wordle that did not use the official list of all possible answers. ",
  "translatedText": "Тепер, якщо ви пам’ятаєте, більшу частину нашого останнього відео ми витратили на те, щоб спробувати написати алгоритм для вирішення Wordle, який не використовує офіційний список усіх можливих відповідей. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.14,
  "end": 232.46
 },
 {
  "input": "To my taste, that feels a bit like overfitting to a test set, and what's more fun is building something that's resilient. ",
  "translatedText": "На мій смак, це трохи схоже на переобладнання тестового набору, і що ще цікавіше, це створювати щось стійке. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.98,
  "end": 238.48
 },
 {
  "input": "This is why we went through the whole process of looking at relative word frequencies in the English language to come up with some notion of how likely each one would be to be included as a final answer. ",
  "translatedText": "Ось чому ми пройшли весь процес розгляду відносної частоти слів в англійській мові, щоб отримати деяке уявлення про те, наскільки ймовірно кожне з них буде включено в остаточну відповідь. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.9,
  "end": 248.76
 },
 {
  "input": "However, for what we're doing here, where we're just trying to find an absolute best performance period, I am incorporating that official list and just shamelessly overfitting to the test set, which is to say we know with certainty whether a word is included or not, and we can assign a uniform probability to each one. ",
  "translatedText": "Однак для того, що ми робимо тут, коли ми просто намагаємося знайти абсолютно найкращий період продуктивності, я включаю цей офіційний список і просто безсоромно переобладную тестовий набір, тобто ми точно знаємо, чи слово включено чи ні, і ми можемо призначити однакову ймовірність кожному з них. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 249.4,
  "end": 265.46
 },
 {
  "input": "If you'll remember, the first step in all of this was to say for a particular opening guess, maybe something like my old favorite, crane, how likely is it that you would see each of the possible patterns? ",
  "translatedText": "Якщо ви пам’ятаєте, першим кроком у всьому цьому було сказати для конкретного початкового припущення, можливо, щось на зразок мого старого улюбленого, журавля, наскільки ймовірно, що ви побачите кожен із можливих візерунків? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 266.44,
  "end": 276.18
 },
 {
  "input": "And in this context, where we are shamelessly overfitting to the wordle answer list, all that involves is counting how many of the possible answers give each one of these patterns. ",
  "translatedText": "І в цьому контексті, де ми безсоромно переповнюємо список відповідей wordle, все, що включає, це підрахувати, скільки можливих відповідей дає кожен із цих шаблонів. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 276.68,
  "end": 285.34
 },
 {
  "input": "And then of course most of our time was spent on this kind of funny looking formula to quantify the amount of information that you would get from this guess that basically involves going through each one of those buckets and saying how much information would you gain that has this log expression that is a fanciful way of saying how many times would you cut your space of possibilities in half if you observed a given pattern. ",
  "translatedText": "І, звісно, більшість нашого часу було витрачено на таку кумедну формулу для кількісного визначення обсягу інформації, яку ви отримаєте від цього припущення, яке в основному передбачає перегляд кожного з цих відер і визначення того, скільки інформації ви отримаєте, якщо цей логарифмічний вираз, який є химерним способом сказати, скільки разів ви скоротили б свій простір можливостей наполовину, якби спостерігали за заданою закономірністю. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.98,
  "end": 306.84
 },
 {
  "input": "We take a weighted average of all of those and it gives us a measure of how much we expect to learn from this first guess. ",
  "translatedText": "Ми беремо середнє зважене значення всіх цих показників, і це дає нам міру того, скільки ми очікуємо дізнатися з цього першого припущення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.6,
  "end": 313.18
 },
 {
  "input": "In a moment we'll go deeper than this, but if you simply search through all 13,000 different words that you could start with and you ask which one has the highest expected information, it turns out the best possible answer is soar, which doesn't really look like a real word, but I guess it's an obsolete term for a baby hawk. ",
  "translatedText": "За мить ми заглибимося глибше, але якщо ви просто пошукаєте серед усіх 13 000 різних слів, з яких ви могли б почати, і запитайте, яке з них містить найбільшу очікувану інформацію, виявиться, що найкраща можлива відповідь - це злітати, що не Справді це не схоже на справжнє слово, але я думаю, що це застарілий термін для дитинчати яструба. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.56,
  "end": 333.0
 },
 {
  "input": "The top 15 openers by this metric happen to look like this, but these are not necessarily the best opening guesses because they're only looking one step in with the heuristic of expected information to try to estimate what the true score will be. ",
  "translatedText": "15 найпопулярніших учасників за цим показником виглядають так, але це не обов’язково найкращі початкові припущення, тому що вони розглядають лише один крок із евристикою очікуваної інформації, щоб спробувати оцінити справжній результат. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.04,
  "end": 347.54
 },
 {
  "input": "But there's few enough patterns that we can do an exhaustive search two steps in. ",
  "translatedText": "Але є достатньо шаблонів, щоб ми могли виконати вичерпний пошук у два кроки. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 347.92,
  "end": 351.68
 },
 {
  "input": "For example, let's say you opened with soar and the pattern you happen to see was the most likely one, all grays, then you can run identical analysis from that point. ",
  "translatedText": "Наприклад, припустімо, що ви відкрили з ширянням, і візерунок, який ви випадково побачили, був найімовірнішим, усі сірі, тоді ви можете запустити ідентичний аналіз із цієї точки. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.16,
  "end": 360.8
 },
 {
  "input": "For a given proposed second guess, something like kitty, what's the distribution across all patterns in that restricted case where we're restricted only to the words that would produce all grays for soar, and then we measure the flatness of that distribution using this expected information formula, and we do that for all 13,000 possible words that we could use as a second guess. ",
  "translatedText": "Для заданого запропонованого другого припущення, щось на зразок кошеня, який розподіл між усіма шаблонами в тому обмеженому випадку, де ми обмежені лише словами, які створять усі сірі відтінки для ширяння, а потім ми вимірюємо рівномірність цього розподілу за допомогою цього очікуваного інформаційну формулу, і ми робимо це для всіх 13 000 можливих слів, які ми можемо використати як друге припущення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.32,
  "end": 381.42
 },
 {
  "input": "Doing this we can find the optimal second guess in that scenario and the amount of information we were expected to get from it, and if we wash rinse and repeat and do this for all of the different possible patterns that you might see, we get a full map of all the best possible second guesses together with the expected information of each. ",
  "translatedText": "Роблячи це, ми можемо знайти оптимальне друге припущення в цьому сценарії та кількість інформації, яку ми повинні були отримати від нього, і якщо ми промиємо, промиємо та повторимо, і зробимо це для всіх різних можливих шаблонів, які ви можете побачити, ми отримаємо повна карта всіх найкращих можливих других припущень разом із очікуваною інформацією про кожне. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.12,
  "end": 399.2
 },
 {
  "input": "From there, if you take a weighted average of all those second step values, weighted according to how likely you are to fall into that bucket, it gives you a measure of how much information you're likely to gain from the guess soar after the second step. ",
  "translatedText": "Звідси, якщо ви візьмете середнє зважене значення всіх цих значень другого кроку, зважене відповідно до того, наскільки ймовірно ви потрапите в це відро, це дасть вам міру того, скільки інформації ви, ймовірно, отримаєте від припущень, що злетять після другий крок. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 416.8
 },
 {
  "input": "When we use this two-step metric as our new means of ranking, the list gets shaken up a bit. ",
  "translatedText": "Коли ми використовуємо цей двоетапний показник як наш новий засіб рейтингу, список трохи змінюється. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.38,
  "end": 421.78
 },
 {
  "input": "Soar is no longer first place, it falls back to 14th, and instead what rises to the top is slain. ",
  "translatedText": "Soar більше не займає перше місце, воно повертається до 14-го, а натомість те, що піднімається на вершину, вбивається. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.08,
  "end": 427.66
 },
 {
  "input": "Again, doesn't feel very real, and it looks like it is a British term for a spade that's used for cutting turf. ",
  "translatedText": "Знову ж таки, це не дуже реально, і схоже, що це британський термін для лопати, яка використовується для різання дерну. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.64,
  "end": 436.38
 },
 {
  "input": "Alright, but as you can see it is a really tight race among all of these top contenders for who gains the most information after those two steps. ",
  "translatedText": "Гаразд, але, як бачите, це дуже напружена боротьба між усіма цими головними претендентами за те, хто отримає більше інформації після цих двох кроків. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.2,
  "end": 445.0
 },
 {
  "input": "And even still, these are not necessarily the best opening guesses, because information is just the heuristic, it's not telling us the actual score if you actually play the game. ",
  "translatedText": "І все одно це не обов’язково найкращі початкові припущення, тому що інформація — це лише евристика, вона не повідомляє нам фактичний рахунок, якщо ви дійсно граєте в гру. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.7,
  "end": 454.0
 },
 {
  "input": "What I did is I ran the simulation of playing all 2315 possible wordle games with all possible answers on the top 250 from this list. ",
  "translatedText": "Що я зробив, так це я запустив симуляцію гри у всі 2315 можливих словесних ігор з усіма можливими відповідями в перших 250 із цього списку. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 454.58,
  "end": 464.62
 },
 {
  "input": "And by doing this, seeing how they actually perform, the one that ends up very marginally with the best possible score turns out to be Salé, which is an alternate spelling for Salé, which is a light medieval helmet. ",
  "translatedText": "І роблячи це, дивлячись на те, як вони справді працюють, той, хто в кінцевому підсумку отримав найкращий бал, виявляється Salé, що є альтернативним варіантом написання Salé, який є легким середньовічним шоломом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.46,
  "end": 485.98
 },
 {
  "input": "Alright, if that feels a little bit too fake for you, which it does for me, you'll be happy to know that Trace and Crate give almost identical performance. ",
  "translatedText": "Гаразд, якщо вам це здасться надто фальшивим, а мені це так, ви будете раді дізнатися, що Trace і Crate дають майже однакову продуктивність. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.98,
  "end": 495.78
 },
 {
  "input": "Each of them has the benefit of obviously being a real word, so there is one day when you get it right on the first guess, since both are actual wordle answers. ",
  "translatedText": "Перевага кожного з них полягає в тому, що він, очевидно, є справжнім словом, тому є один день, коли ви зрозумієте його правильно з першого разу, оскільки обидва є фактичними відповідями на основі слів. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.3,
  "end": 504.06
 },
 {
  "input": "This move from sorting based on the best two-step entropies to sorting based on the lowest average score also shakes up the list, but not nearly as much. ",
  "translatedText": "Цей перехід від сортування на основі найкращої двокрокової ентропії до сортування на основі найнижчого середнього балу також похитує список, але не так сильно. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 505.02,
  "end": 512.46
 },
 {
  "input": "For example, Salé was previously third place before it bubbles to the top, and Crate and Trace were both fourth and fifth. ",
  "translatedText": "Наприклад, Salé раніше займав третє місце, перш ніж піднятися на вершину, а Crate і Trace були як четвертим, так і п’ятим. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.66,
  "end": 519.08
 },
 {
  "input": "If you're curious, you can get slightly better performance from here by doing a little brute forcing. ",
  "translatedText": "Якщо вам цікаво, ви можете отримати трохи кращу продуктивність звідси, зробивши невеликий брутфорс. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 519.64,
  "end": 523.72
 },
 {
  "input": "There's a very nice blog post by Jonathan Olson, if you're curious about this, where he also lets you explore what the optimal following guesses are for a few of the starting words based on these optimal algorithms. ",
  "translatedText": "Є дуже гарна публікація в блозі Джонатана Олсона, якщо вам це цікаво, де він також дозволяє вам досліджувати, якими є оптимальні наступні припущення для кількох початкових слів на основі цих оптимальних алгоритмів. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.1,
  "end": 533.66
 },
 {
  "input": "Stepping back from all this though, I'm told by some people that it quote ruins the game to overanalyze it like this and try to find an optimal opening guess. ",
  "translatedText": "Але якщо відійти від усього цього, деякі люди кажуть мені, що ця цитата руйнує гру, якщо надмірно аналізувати її таким чином і намагатися знайти оптимальне початкове припущення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 535.18,
  "end": 543.6
 },
 {
  "input": "You know, it feels kind of dirty if you use that opening guess after learning it, and it feels inefficient if you don't. ",
  "translatedText": "Ви знаєте, це виглядає дещо брудно, якщо ви використовуєте це початкове припущення після того, як його вивчили, і це здається неефективним, якщо ви цього не робите. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.26,
  "end": 549.66
 },
 {
  "input": "But the thing is, I don't actually think this is the best opener for a human playing the game. ",
  "translatedText": "Але справа в тому, що я насправді не думаю, що це найкраще відкриття для людини, яка грає в гру. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 549.8,
  "end": 554.4
 },
 {
  "input": "For one thing, you would need to know what the optimal second guess is for each one of the patterns that you see. ",
  "translatedText": "По-перше, вам потрібно знати, якою є оптимальна друга здогадка для кожного шаблону, який ви бачите. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.88,
  "end": 559.68
 },
 {
  "input": "And more importantly, all of this is in a setting where we are absurdly overfit to the official wordle answer list. ",
  "translatedText": "І що ще важливіше, все це відбувається в умовах, коли ми абсурдно не відповідаємо офіційному списку відповідей Wordle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 560.26,
  "end": 566.36
 },
 {
  "input": "The moment that, say, the New York Times chooses to change what that list is under the hood, all of this would go out the window. ",
  "translatedText": "У той момент, коли, скажімо, New York Times вирішить змінити список під капотом, усе це вилетить у вікно. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.58,
  "end": 572.88
 },
 {
  "input": "The way that we humans play the game is just very different from what any of these algorithms are doing. ",
  "translatedText": "Те, як ми, люди, граємо в гру, просто сильно відрізняється від того, що робить будь-який із цих алгоритмів. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.58,
  "end": 577.68
 },
 {
  "input": "We don't have the word list memorized, we're not doing exhaustive searches, we get intuition from things like what are the vowels and how are they placed. ",
  "translatedText": "Ми не запам’ятали список слів, ми не робимо вичерпних пошуків, ми отримуємо інтуїцію з таких речей, як те, що таке голосні та як вони розміщені. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.02,
  "end": 585.08
 },
 {
  "input": "I would actually be most happy if those of you watching this video promptly forgot what happens to be the technically best opening guess, and instead came out remembering things like how do you quantify information, or the fact that you should look out for when a greedy algorithm falls short of the globally best performance that you would get from a deeper search. ",
  "translatedText": "Насправді я був би дуже радий, якби ті з вас, хто дивиться це відео, одразу забули, що є технічно найкращим початковим припущенням, і натомість згадали речі, наприклад, як ви кількісно оцінюєте інформацію, або той факт, що вам слід остерігатися, коли жадібний алгоритм не досягає найкращої в усьому світі продуктивності, яку ви могли б отримати від глибшого пошуку. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.64,
  "end": 603.1
 },
 {
  "input": "For my taste at least, the joy of writing algorithms to try to play games actually has very little bearing on how I like to play those games as a human. ",
  "translatedText": "Принаймні на мій смак, радість від написання алгоритмів, щоб спробувати грати в ігри, насправді мало впливає на те, як мені подобається грати в ці ігри як людині. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 603.7,
  "end": 610.76
 },
 {
  "input": "The point of writing algorithms for all this is not to affect the way that we play the game, it's still just a fun word game. ",
  "translatedText": "Сенс написання алгоритмів для всього цього полягає не в тому, щоб впливати на те, як ми граємо в гру, це все одно весела гра слів. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 611.3,
  "end": 616.78
 },
 {
  "input": "It's to hone in our muscles for writing algorithms in more meaningful contexts elsewhere. ",
  "translatedText": "Це для того, щоб відточити наші м’язи для написання алгоритмів у більш значущих контекстах деінде. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.1,
  "end": 620.72
 }
]