[
 {
  "input": "Last week I put up this video about solving the game Wordle, or at least trying to solve it, using information theory. ",
  "translatedText": "지난 주에 저는 정보 이론을 사용하여 Wordle 게임을 해결하는 방법, 또는 적어도 해결해 보는 방법에 대한 영상을 올렸습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.18
 },
 {
  "input": "And I wanted to add a quick, what should we call this, an addendum? ",
  "translatedText": "그리고 저는 이것을 간단히 덧붙이고 싶었습니다. 이것을 부록이라 불러야 할까요? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.58,
  "end": 9.78
 },
 {
  "input": "A confession? ",
  "translatedText": "고백? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.08,
  "end": 10.66
 },
 {
  "input": "Basically I just want to explain a place where I made a mistake. ",
  "translatedText": "기본적으로 저는 단지 제가 실수한 부분을 설명하고 싶을 뿐입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.02,
  "end": 13.9
 },
 {
  "input": "It turns out there was a very slight bug in the code that I was running to recreate Wordle and then run all of the algorithms to solve it and test their performance. ",
  "translatedText": "Wordle을 다시 만들고 모든 알고리즘을 실행하여 문제를 해결하고 성능을 테스트하기 위해 실행한 코드에 아주 작은 버그가 있는 것으로 나타났습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.46,
  "end": 22.0
 },
 {
  "input": "And it's one of those bugs that affects a very small percentage of cases, so it was easy to miss, and it has only a very slight effect that for the most part doesn't really matter. ",
  "translatedText": "그리고 이는 매우 적은 비율의 사례에 영향을 미치는 버그 중 하나이므로 놓치기 쉬웠고 대부분의 경우 실제로 중요하지 않은 아주 작은 영향만 미쳤습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 22.6,
  "end": 30.5
 },
 {
  "input": "Basically it had to do with how you assign a color to a guess that has multiple different letters in it. ",
  "translatedText": "기본적으로 이는 여러 개의 서로 다른 문자가 포함된 추측에 색상을 할당하는 방법과 관련이 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.22,
  "end": 36.36
 },
 {
  "input": "For example, if you guess speed and the true answer is abide, how should you color those two e's from the guess? ",
  "translatedText": "예를 들어, 속도를 추측했는데 정답이 '준수'라면 추측에서 나온 두 e에 어떤 색을 칠해야 할까요? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 36.52,
  "end": 42.12
 },
 {
  "input": "Well the way that it works with the Wordle conventions is that the first e would be colored yellow, and the second one would be colored gray. ",
  "translatedText": "Wordle 규칙에 따라 작동하는 방식은 첫 번째 e가 노란색으로 표시되고 두 번째 e가 회색으로 표시된다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.06,
  "end": 49.08
 },
 {
  "input": "You might think of that first one as matching up with something from the true answer, and the grayness is telling you there is no second e. ",
  "translatedText": "당신은 그 첫 번째 것이 참 답의 어떤 것과 일치하는 것으로 생각할 수도 있고, 회색조는 두 번째 e가 없다는 것을 의미합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 49.6,
  "end": 55.52
 },
 {
  "input": "By contrast, if the answer was something like erase, both of those e's would be colored yellow, telling you that there is a first e in a different location, and there's a second e also in a different location. ",
  "translatedText": "대조적으로, 대답이 지우기와 같은 것이라면 두 e는 모두 노란색으로 표시되어 첫 번째 e가 다른 위치에 있고 두 번째 e도 다른 위치에 있음을 나타냅니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.52,
  "end": 66.78
 },
 {
  "input": "Similarly if one of the e's hits and it's green, then that second one would be gray in the case where the true answer has no second e, but it would be yellow in the case where there is a second e and it's just in a different location. ",
  "translatedText": "마찬가지로 e 중 하나가 히트하고 녹색인 경우 실제 답에 두 번째 e가 없는 경우 두 번째 e는 회색이 되지만 두 번째 e가 있고 다른 위치에 있는 경우에는 노란색이 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.3,
  "end": 80.04
 },
 {
  "input": "Long story short, somewhere along the way I accidentally introduced behavior that differs from these conventions slightly. ",
  "translatedText": "위치. 간단히 말해서, 어딘가에서 실수로 이러한 규칙과 약간 다른 동작을 도입했습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.7,
  "end": 86.4
 },
 {
  "input": "Honestly it was really dumb. ",
  "translatedText": "솔직히 정말 멍청했어요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.1,
  "end": 88.54
 },
 {
  "input": "Basically at some point in the middle of the project I wanted to speed up some of the computations, and I was trying a little trick for how it computed the value for this pattern between any given pair of words, and you know I just didn't really think it through and it introduced this slight change. ",
  "translatedText": "기본적으로 프로젝트 중간 어느 시점에서 나는 일부 계산 속도를 높이고 싶었고 주어진 단어 쌍 사이에서 이 패턴의 값을 계산하는 방법에 대한 약간의 트릭을 시도하고 있었지만 방금 그렇게 하지 않았습니다. 실제로 깊이 생각하지 않고 약간의 변화를 도입했습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.88,
  "end": 102.72
 },
 {
  "input": "The ironic part is that in the end the actual way to make things fastest is to pre-compute all those patterns so that everything is just a lookup, and so it wouldn't matter how long it takes to do each one, especially if you're writing hard to read buggy code to make it happen. ",
  "translatedText": "아이러니한 부분은 결국 작업을 가장 빠르게 만드는 실제 방법은 모든 패턴을 미리 계산하여 모든 것이 조회일 뿐이므로 각 작업을 수행하는 데 시간이 얼마나 걸리는지는 중요하지 않다는 것입니다. 특히 버그가 있는 코드를 읽기 위해 열심히 작성하고 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.22,
  "end": 115.84
 },
 {
  "input": "You know, you live and you learn. ",
  "translatedText": "아시다시피, 당신은 살고 배웁니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 116.4,
  "end": 117.24
 },
 {
  "input": "As far as how this affects the actual video, I mean very little of substance really changes. ",
  "translatedText": "이것이 실제 비디오에 어떤 영향을 미치는지에 관해서는 실제로 변경되는 내용이 거의 없다는 것을 의미합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.04,
  "end": 122.34
 },
 {
  "input": "Of course the main lessons about what is information, what is entropy, all that stays the same. ",
  "translatedText": "물론 정보가 무엇인지, 엔트로피가 무엇인지에 대한 주요 교훈은 모두 동일합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 122.66,
  "end": 126.56
 },
 {
  "input": "Every now and then if I'm showing on screen some distribution associated with a given word, that distribution might actually be a little bit off because some of the buckets associated with various patterns should include either more or fewer true answers. ",
  "translatedText": "때때로 특정 단어와 관련된 일부 분포를 화면에 표시하는 경우 다양한 패턴과 관련된 일부 버킷에는 실제 답변이 더 많거나 적으므로 해당 분포가 실제로 약간 다를 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.86,
  "end": 140.32
 },
 {
  "input": "Even then it doesn't really come up because it was very rare that I would be showing a word that had multiple letters that also hit this edge case. ",
  "translatedText": "그럼에도 불구하고 이 극단적인 경우에 부딪히는 여러 글자가 있는 단어를 표시하는 경우는 매우 드물기 때문에 실제로 나타나지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.84,
  "end": 146.96
 },
 {
  "input": "But one of the very few things of substance that does change and that arguably does matter a fair bit was the final conclusion around how if we want to find the optimal possible score for the wordle answer list, what opening guess does such an algorithm use? ",
  "translatedText": "그러나 변하는 몇 안 되는 본질적인 것 중 하나는 틀림없이 상당히 중요한 것 중 하나는 단어 답변 목록에 대해 가능한 최적의 점수를 찾고자 하는 경우 그러한 알고리즘이 어떤 시작 추측을 사용하는지에 대한 최종 결론이었습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.68,
  "end": 162.46
 },
 {
  "input": "In the video I said the best performance that I could find came from opening with the word crane, which was true only in the sense that the algorithms were playing a very slightly different game. ",
  "translatedText": "비디오에서 나는 내가 찾을 수 있는 최고의 성능은 크레인이라는 단어로 시작하는 것에서 나왔다고 말했는데, 이는 알고리즘이 아주 약간 다른 게임을 하고 있다는 점에서만 사실이었습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.08,
  "end": 172.56
 },
 {
  "input": "After fixing it and rerunning it all, there is a different answer for what the theoretically optimal first guess is for this particular list. ",
  "translatedText": "문제를 수정하고 모두 다시 실행한 후에는 이 특정 목록에 대한 이론적으로 최적의 첫 번째 추측이 무엇인지에 대한 다른 대답이 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.16,
  "end": 180.16
 },
 {
  "input": "And look, I know that you know that the point of the video is not to find some technically optimal answer to some random online game. ",
  "translatedText": "그리고 보세요, 이 비디오의 요점이 임의의 온라인 게임에 대한 기술적으로 최적의 답을 찾는 것이 아니라는 것을 알고 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 181.0,
  "end": 189.1
 },
 {
  "input": "The point of the video is to shamelessly hop on the bandwagon of an internet trend to sneak attack people with an information theory lesson. ",
  "translatedText": "영상의 요점은 뻔뻔하게도 인터넷 트렌드에 편승하여 정보 이론 수업을 통해 사람들을 몰래 공격하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.46,
  "end": 195.9
 },
 {
  "input": "And that's all good, I stand by that part. ",
  "translatedText": "그리고 그것은 모두 좋습니다. 나는 그 부분을 지지합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.32,
  "end": 198.0
 },
 {
  "input": "But I know how the internet works, and for a lot of people the one main takeaway was what is the best opener for the game wordle. ",
  "translatedText": "하지만 저는 인터넷이 어떻게 작동하는지 알고 있으며, 많은 사람들이 가장 중요하게 생각하는 점은 게임 단어를 위한 최고의 시작 방법이 무엇인지였습니다. 그리고 알겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.2,
  "end": 204.6
 },
 {
  "input": "And I get it, I walked into that because I put it in the thumbnail, but presumably you can forgive me if I want to add a little correction here. ",
  "translatedText": "미리보기 이미지에 넣었기 때문에 그 부분에 들어갔습니다. 하지만 여기에 약간의 수정 사항을 추가하고 싶다면 용서해 주실 수 있을 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 205.28,
  "end": 211.86
 },
 {
  "input": "And a more meaningful reason to circle back to all this actually is that I never really talked about what went into that final analysis. ",
  "translatedText": "그리고 실제로 이 모든 것을 다시 언급해야 하는 더 의미 있는 이유는 제가 최종 분석에 무엇이 포함되었는지에 대해 실제로 이야기한 적이 없다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.98,
  "end": 218.34
 },
 {
  "input": "And it's interesting as a sublesson in its own right, so it's worth doing here. ",
  "translatedText": "그리고 그것은 그 자체로 하위 레슨으로서 흥미롭기 때문에 여기서 해볼 가치가 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.84,
  "end": 222.42
 },
 {
  "input": "Now if you'll recall, most of our time last video was spent on the challenge of trying to write an algorithm to solve wordle that did not use the official list of all possible answers. ",
  "translatedText": "기억하시겠지만, 지난 비디오의 대부분의 시간은 가능한 모든 답의 공식 목록을 사용하지 않은 단어를 해결하기 위한 알고리즘을 작성하는 데 소비되었습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.14,
  "end": 232.46
 },
 {
  "input": "To my taste, that feels a bit like overfitting to a test set, and what's more fun is building something that's resilient. ",
  "translatedText": "내 취향에는 테스트 세트에 과적합된 것 같은 느낌이 들며, 더 재미있는 것은 탄력적인 것을 구축하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.98,
  "end": 238.48
 },
 {
  "input": "This is why we went through the whole process of looking at relative word frequencies in the English language to come up with some notion of how likely each one would be to be included as a final answer. ",
  "translatedText": "이것이 우리가 영어의 상대적인 단어 빈도를 살펴보는 전체 과정을 거쳐 각 단어가 최종 답변에 포함될 가능성에 대한 개념을 찾아낸 이유입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.9,
  "end": 248.76
 },
 {
  "input": "However, for what we're doing here, where we're just trying to find an absolute best performance period, I am incorporating that official list and just shamelessly overfitting to the test set, which is to say we know with certainty whether a word is included or not, and we can assign a uniform probability to each one. ",
  "translatedText": "그러나 우리가 여기서 하고 있는 일, 즉 절대적인 최고 성능 기간을 찾으려는 경우에는 공식 목록을 통합하고 뻔뻔스럽게 테스트 세트에 과대적합하고 있습니다. 포함 여부에 따라 각 항목에 균일한 확률을 할당할 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 249.4,
  "end": 265.46
 },
 {
  "input": "If you'll remember, the first step in all of this was to say for a particular opening guess, maybe something like my old favorite, crane, how likely is it that you would see each of the possible patterns? ",
  "translatedText": "기억하신다면, 이 모든 것의 첫 번째 단계는 특정한 시작 추측, 아마도 제가 가장 좋아하는 크레인과 같은 것에 대해 가능한 각 패턴을 볼 가능성이 얼마나 되는지 말하는 것이었습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 266.44,
  "end": 276.18
 },
 {
  "input": "And in this context, where we are shamelessly overfitting to the wordle answer list, all that involves is counting how many of the possible answers give each one of these patterns. ",
  "translatedText": "그리고 우리가 단어 답변 목록에 뻔뻔스럽게 과대적합하는 이러한 맥락에서 관련된 모든 것은 이러한 패턴 각각에 대해 가능한 답변 중 얼마나 많은 수를 계산하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 276.68,
  "end": 285.34
 },
 {
  "input": "And then of course most of our time was spent on this kind of funny looking formula to quantify the amount of information that you would get from this guess that basically involves going through each one of those buckets and saying how much information would you gain that has this log expression that is a fanciful way of saying how many times would you cut your space of possibilities in half if you observed a given pattern. ",
  "translatedText": "그리고 물론 우리 시간의 대부분은 기본적으로 각 버킷을 통과하여 얻을 수 있는 정보의 양을 말하는 이 추측에서 얻을 수 있는 정보의 양을 정량화하기 위해 이런 종류의 재미있어 보이는 공식에 소비되었습니다. 주어진 패턴을 관찰하면 가능성의 공간을 몇 번이나 절반으로 줄일 것인지를 표현하는 기발한 방법인 이 로그 표현입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.98,
  "end": 306.84
 },
 {
  "input": "We take a weighted average of all of those and it gives us a measure of how much we expect to learn from this first guess. ",
  "translatedText": "우리는 이들 모두의 가중 평균을 취하여 이 첫 번째 추측에서 얼마나 많은 것을 배울 수 있을지 측정합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.6,
  "end": 313.18
 },
 {
  "input": "In a moment we'll go deeper than this, but if you simply search through all 13,000 different words that you could start with and you ask which one has the highest expected information, it turns out the best possible answer is soar, which doesn't really look like a real word, but I guess it's an obsolete term for a baby hawk. ",
  "translatedText": "잠시 후에 우리는 이것보다 더 깊이 들어갈 것입니다. 그러나 시작할 수 있는 13,000개의 다른 단어를 모두 검색하고 어떤 단어가 가장 기대되는 정보를 가지고 있는지 묻는다면 가능한 최선의 대답은 솟아오르는 것입니다. 실제로는 실제 단어처럼 보이지 않지만 아기 매를 지칭하는 구식 용어인 것 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.56,
  "end": 333.0
 },
 {
  "input": "The top 15 openers by this metric happen to look like this, but these are not necessarily the best opening guesses because they're only looking one step in with the heuristic of expected information to try to estimate what the true score will be. ",
  "translatedText": "이 지표에 따른 상위 15개 오프너는 다음과 같이 보이지만 실제 점수가 무엇인지 추정하기 위해 예상 정보의 휴리스틱을 사용하여 한 단계만 보고 있기 때문에 이것이 반드시 최고의 오프닝 추측은 아닙니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.04,
  "end": 347.54
 },
 {
  "input": "But there's few enough patterns that we can do an exhaustive search two steps in. ",
  "translatedText": "하지만 두 단계로 철저한 검색을 수행할 수 있을 만큼 충분한 패턴이 없습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 347.92,
  "end": 351.68
 },
 {
  "input": "For example, let's say you opened with soar and the pattern you happen to see was the most likely one, all grays, then you can run identical analysis from that point. ",
  "translatedText": "예를 들어, soar로 열었고 우연히 발견한 패턴이 가장 가능성이 높은 패턴인 모두 회색이었다고 가정하고 해당 지점에서 동일한 분석을 실행할 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.16,
  "end": 360.8
 },
 {
  "input": "For a given proposed second guess, something like kitty, what's the distribution across all patterns in that restricted case where we're restricted only to the words that would produce all grays for soar, and then we measure the flatness of that distribution using this expected information formula, and we do that for all 13,000 possible words that we could use as a second guess. ",
  "translatedText": "Kitty와 같이 제안된 두 번째 추측에 대해, soar에 대한 모든 회색을 생성하는 단어로만 제한되는 제한된 경우의 모든 패턴에 대한 분포는 무엇입니까? 그런 다음 예상되는 이 값을 사용하여 해당 분포의 평탄도를 측정합니다. 두 번째 추측으로 사용할 수 있는 13,000개의 가능한 단어 모두에 대해 정보 공식을 적용합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.32,
  "end": 381.42
 },
 {
  "input": "Doing this we can find the optimal second guess in that scenario and the amount of information we were expected to get from it, and if we wash rinse and repeat and do this for all of the different possible patterns that you might see, we get a full map of all the best possible second guesses together with the expected information of each. ",
  "translatedText": "이를 통해 우리는 해당 시나리오에서 최적의 두 번째 추측과 그로부터 얻을 것으로 예상되는 정보의 양을 찾을 수 있습니다. 그리고 볼 수 있는 모든 가능한 패턴에 대해 헹구고 반복하여 이 작업을 수행하면 다음을 얻습니다. 가능한 최선의 두 번째 추측이 모두 포함된 전체 지도와 각각의 예상 정보. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.12,
  "end": 399.2
 },
 {
  "input": "From there, if you take a weighted average of all those second step values, weighted according to how likely you are to fall into that bucket, it gives you a measure of how much information you're likely to gain from the guess soar after the second step. ",
  "translatedText": "거기에서 모든 두 번째 단계 값의 가중 평균을 취하고 해당 버킷에 빠질 가능성에 따라 가중치를 적용하면 추측이 치솟은 후 얻을 수 있는 정보의 양을 측정할 수 있습니다. 두번째 단계. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 416.8
 },
 {
  "input": "When we use this two-step metric as our new means of ranking, the list gets shaken up a bit. ",
  "translatedText": "이 2단계 측정항목을 순위를 정하는 새로운 수단으로 사용하면 목록이 약간 흔들립니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.38,
  "end": 421.78
 },
 {
  "input": "Soar is no longer first place, it falls back to 14th, and instead what rises to the top is slain. ",
  "translatedText": "Soar는 더 이상 1위가 아니며 14위로 다시 떨어지며, 대신 정상에 오른 것은 죽임을 당합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.08,
  "end": 427.66
 },
 {
  "input": "Again, doesn't feel very real, and it looks like it is a British term for a spade that's used for cutting turf. ",
  "translatedText": "다시 말하지만, 그다지 현실감이 없으며 잔디를 자르는 데 사용되는 삽을 가리키는 영국 용어인 것 같습니다. 좋습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.64,
  "end": 436.38
 },
 {
  "input": "Alright, but as you can see it is a really tight race among all of these top contenders for who gains the most information after those two steps. ",
  "translatedText": "하지만 보시다시피 이 두 단계 후에 누가 가장 많은 정보를 얻을 수 있는지에 대한 모든 최고 경쟁자 사이에서는 정말 치열한 경쟁이 벌어지고 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.2,
  "end": 445.0
 },
 {
  "input": "And even still, these are not necessarily the best opening guesses, because information is just the heuristic, it's not telling us the actual score if you actually play the game. ",
  "translatedText": "그럼에도 불구하고 이것이 반드시 최선의 시작 추측은 아닙니다. 정보는 경험적일 뿐이고 실제로 게임을 플레이할 경우 실제 점수를 알려주는 것은 아니기 때문입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.7,
  "end": 454.0
 },
 {
  "input": "What I did is I ran the simulation of playing all 2315 possible wordle games with all possible answers on the top 250 from this list. ",
  "translatedText": "내가 한 일은 이 목록의 상위 250개에 대한 모든 가능한 답을 가지고 2315개의 가능한 단어 게임을 모두 플레이하는 시뮬레이션을 실행한 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 454.58,
  "end": 464.62
 },
 {
  "input": "And by doing this, seeing how they actually perform, the one that ends up very marginally with the best possible score turns out to be Salé, which is an alternate spelling for Salé, which is a light medieval helmet. ",
  "translatedText": "그리고 이렇게 함으로써, 그들이 실제로 어떻게 수행하는지를 보면, 가능한 최고의 점수로 아주 미미하게 끝나는 것은 Salé로 밝혀졌습니다. 이는 가벼운 중세 헬멧인 Salé의 대체 철자인 Salé입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.46,
  "end": 485.98
 },
 {
  "input": "Alright, if that feels a little bit too fake for you, which it does for me, you'll be happy to know that Trace and Crate give almost identical performance. ",
  "translatedText": "좋아요, 저처럼 그것이 여러분에게 너무 가짜라고 느껴지신다면 Trace와 Crate가 거의 동일한 성능을 제공한다는 사실을 알게 되어 기뻐하실 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.98,
  "end": 495.78
 },
 {
  "input": "Each of them has the benefit of obviously being a real word, so there is one day when you get it right on the first guess, since both are actual wordle answers. ",
  "translatedText": "각각은 분명히 실제 단어라는 이점이 있으므로 둘 다 실제 단어 답변이기 때문에 첫 번째 추측에서 정답을 얻을 날이 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.3,
  "end": 504.06
 },
 {
  "input": "This move from sorting based on the best two-step entropies to sorting based on the lowest average score also shakes up the list, but not nearly as much. ",
  "translatedText": "최고의 2단계 엔트로피를 기반으로 한 정렬에서 가장 낮은 평균 점수를 기반으로 한 정렬로의 이동도 목록을 뒤흔들지만 그다지 많지는 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 505.02,
  "end": 512.46
 },
 {
  "input": "For example, Salé was previously third place before it bubbles to the top, and Crate and Trace were both fourth and fifth. ",
  "translatedText": "예를 들어, Salé는 상위권에 오르기 전 이전에 3위였으며 Crate와 Trace는 모두 4위와 5위였습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.66,
  "end": 519.08
 },
 {
  "input": "If you're curious, you can get slightly better performance from here by doing a little brute forcing. ",
  "translatedText": "궁금하다면 여기에서 약간의 무차별 대입을 수행하여 약간 더 나은 성능을 얻을 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 519.64,
  "end": 523.72
 },
 {
  "input": "There's a very nice blog post by Jonathan Olson, if you're curious about this, where he also lets you explore what the optimal following guesses are for a few of the starting words based on these optimal algorithms. ",
  "translatedText": "Jonathan Olson이 작성한 매우 멋진 블로그 게시물이 있습니다. 여기에서 최적의 알고리즘을 기반으로 몇 가지 시작 단어에 대한 최적의 다음 추측이 무엇인지 탐색할 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.1,
  "end": 533.66
 },
 {
  "input": "Stepping back from all this though, I'm told by some people that it quote ruins the game to overanalyze it like this and try to find an optimal opening guess. ",
  "translatedText": "하지만 이 모든 것에서 물러나서, 일부 사람들은 게임을 이렇게 과도하게 분석하고 최적의 오프닝 추측을 찾으려고 노력하는 것이 게임을 망친다고 말했습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 535.18,
  "end": 543.6
 },
 {
  "input": "You know, it feels kind of dirty if you use that opening guess after learning it, and it feels inefficient if you don't. ",
  "translatedText": "아시다시피, 학습한 후 시작 추측을 사용하면 좀 더러운 느낌이 들고, 그렇지 않으면 비효율적인 느낌이 듭니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.26,
  "end": 549.66
 },
 {
  "input": "But the thing is, I don't actually think this is the best opener for a human playing the game. ",
  "translatedText": "하지만 문제는 이것이 인간이 게임을 플레이하기 위한 최고의 오프너라고 생각하지 않는다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 549.8,
  "end": 554.4
 },
 {
  "input": "For one thing, you would need to know what the optimal second guess is for each one of the patterns that you see. ",
  "translatedText": "우선, 표시된 각 패턴에 대한 최적의 두 번째 추측이 무엇인지 알아야 합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.88,
  "end": 559.68
 },
 {
  "input": "And more importantly, all of this is in a setting where we are absurdly overfit to the official wordle answer list. ",
  "translatedText": "그리고 더 중요한 것은 이 모든 것이 공식 단어 답변 목록에 터무니없이 과적합된 환경에 있다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 560.26,
  "end": 566.36
 },
 {
  "input": "The moment that, say, the New York Times chooses to change what that list is under the hood, all of this would go out the window. ",
  "translatedText": "예를 들어 New York Times가 그 목록의 내용을 변경하기로 결정하는 순간 이 모든 것이 창 밖으로 사라질 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.58,
  "end": 572.88
 },
 {
  "input": "The way that we humans play the game is just very different from what any of these algorithms are doing. ",
  "translatedText": "우리 인간이 게임을 플레이하는 방식은 이러한 알고리즘이 수행하는 방식과 매우 다릅니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.58,
  "end": 577.68
 },
 {
  "input": "We don't have the word list memorized, we're not doing exhaustive searches, we get intuition from things like what are the vowels and how are they placed. ",
  "translatedText": "우리는 단어 목록을 기억하지 않고 철저한 검색을 수행하지 않으며 모음이 무엇인지, 모음이 어떻게 배치되는지와 같은 직관을 얻습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.02,
  "end": 585.08
 },
 {
  "input": "I would actually be most happy if those of you watching this video promptly forgot what happens to be the technically best opening guess, and instead came out remembering things like how do you quantify information, or the fact that you should look out for when a greedy algorithm falls short of the globally best performance that you would get from a deeper search. ",
  "translatedText": "이 비디오를 시청하는 분들이 기술적으로 가장 좋은 오프닝 추측이 무엇인지 즉시 잊어버리고 대신 정보를 수량화하는 방법이나 욕심이 많을 때 조심해야 하는 사실 등을 기억해 주신다면 실제로 가장 기쁠 것입니다. 알고리즘은 더 깊은 검색에서 얻을 수 있는 세계 최고의 성능에 미치지 못합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.64,
  "end": 603.1
 },
 {
  "input": "For my taste at least, the joy of writing algorithms to try to play games actually has very little bearing on how I like to play those games as a human. ",
  "translatedText": "적어도 내 취향에 따르면, 게임을 하기 위해 알고리즘을 작성하는 즐거움은 실제로 내가 인간으로서 그 게임을 즐기는 방식에 거의 영향을 미치지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 603.7,
  "end": 610.76
 },
 {
  "input": "The point of writing algorithms for all this is not to affect the way that we play the game, it's still just a fun word game. ",
  "translatedText": "이 모든 것에 대한 알고리즘을 작성하는 목적은 우리가 게임을 하는 방식에 영향을 미치는 것이 아니라 여전히 재미있는 단어 게임일 뿐입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 611.3,
  "end": 616.78
 },
 {
  "input": "It's to hone in our muscles for writing algorithms in more meaningful contexts elsewhere. ",
  "translatedText": "다른 곳에서 보다 의미 있는 맥락에서 알고리즘을 작성하기 위해 근육을 단련하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.1,
  "end": 620.72
 }
]