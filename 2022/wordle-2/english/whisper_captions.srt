1
00:00:00,000 --> 00:00:04,560
Last week I put up this video about solving the game Wordle, or at least trying to solve it,

2
00:00:04,880 --> 00:00:09,780
using information theory. And I wanted to add a quick, what should we call this, an addendum,

3
00:00:10,080 --> 00:00:13,900
a confession, basically I just want to explain a place where I made a mistake.

4
00:00:14,460 --> 00:00:19,020
It turns out there was a very slight bug in the code that I was running to recreate Wordle and

5
00:00:19,020 --> 00:00:23,180
then run all of the algorithms to solve it and test their performance. And it's one of those

6
00:00:23,180 --> 00:00:27,820
bugs that affects a very small percentage of cases, so it was easy to miss, and it has only

7
00:00:27,820 --> 00:00:32,480
a slight effect that for the most part doesn't really matter. Basically it had to do with how

8
00:00:32,480 --> 00:00:37,440
you assign a color to a guess that has multiple different letters in it. For example if you guess

9
00:00:37,440 --> 00:00:42,120
speed and the true answer is abide, how should you color those two e's from the guess?

10
00:00:43,060 --> 00:00:46,660
Well the way that it works with the Wordle conventions is that the first e would be colored

11
00:00:46,660 --> 00:00:51,220
yellow, and the second one would be colored gray. You might think of that first one as matching up

12
00:00:51,220 --> 00:00:55,520
with something from the true answer, and the grayness is telling you there is no second e.

13
00:00:55,520 --> 00:01:00,900
By contrast, if the answer was something like erase, both of those e's would be colored yellow,

14
00:01:01,420 --> 00:01:06,060
telling you that there is a first e in a different location, and there's a second e also in a

15
00:01:06,060 --> 00:01:11,060
different location. Similarly if one of the e's hits and it's green, then that second one would

16
00:01:11,060 --> 00:01:17,420
be gray in the case where the true answer has no second e, but it would be yellow in the case where

17
00:01:17,420 --> 00:01:25,380
there is a second e and it's just in a different location. Long story short, somewhere along the

18
00:01:25,380 --> 00:01:30,640
convention slightly. Honestly it was really dumb. Basically at some point in the middle of

19
00:01:30,640 --> 00:01:34,980
the project I wanted to speed up some of the computations, and I was trying a little trick

20
00:01:34,980 --> 00:01:39,880
for how it computed the value for this pattern between any given pair of words, and you know I

21
00:01:39,880 --> 00:01:43,880
just didn't really think it through, and it introduced this slight change. The ironic part

22
00:01:43,880 --> 00:01:48,600
is that in the end the actual way to make things fastest is to pre-compute all those patterns so

23
00:01:48,600 --> 00:01:52,620
that everything is just a lookup, and so it wouldn't matter how long it takes to do each one,

24
00:01:52,620 --> 00:01:56,880
especially if you're writing hard to read buggy code to make it happen. You know, you live and

25
00:01:56,880 --> 00:02:01,880
you learn. As far as how this affects the actual video, I mean very little of substance really

26
00:02:01,880 --> 00:02:06,080
changes. Of course the main lessons about what is information, what is entropy, all that stays

27
00:02:06,080 --> 00:02:10,960
the same. Every now and then if I'm showing on screen some distribution associated with a given

28
00:02:10,960 --> 00:02:16,060
word, that distribution might actually be a little bit off because some of the buckets associated

29
00:02:16,060 --> 00:02:21,940
with various patterns should include either more or fewer true answers. Even then it doesn't really

30
00:02:21,940 --> 00:02:25,940
come up because it was very rare that I would be showing a word that had multiple letters that also

31
00:02:25,940 --> 00:02:31,840
hit this edge case. But one of the very few things of substance that does change, and that arguably

32
00:02:31,840 --> 00:02:37,860
does matter a fair bit, was the final conclusion around how if we want to find the optimal possible

33
00:02:37,860 --> 00:02:43,680
score for the wordle answer list, what opening guess does such an algorithm use? In the video

34
00:02:43,680 --> 00:02:49,280
I said the best performance that I could find came from opening with the word crane, which was true

35
00:02:49,280 --> 00:02:52,560
only in the sense that the algorithms were playing a very slightly different game.

36
00:02:53,160 --> 00:02:57,900
After fixing it and rerunning it all, there is a different answer for what the theoretically optimal

37
00:02:57,900 --> 00:03:05,160
first guess is for this particular list. And look, I know that you know that the point of the video is

38
00:03:05,160 --> 00:03:10,480
not to find some technically optimal answer to some random online game. The point of the video

39
00:03:10,480 --> 00:03:14,800
is to shamelessly hop on the bandwagon of an internet trend to sneak attack people with an

40
00:03:14,800 --> 00:03:19,120
information theory lesson. And that's all good, I stand by that part. But I know how the internet

41
00:03:19,120 --> 00:03:24,260
works, and for a lot of people the one main takeaway was what is the best opener for the game

42
00:03:24,260 --> 00:03:29,400
wordle. And I get it, I walked into that because I put it in the thumbnail, but presumably you can

43
00:03:29,400 --> 00:03:33,900
forgive me if I want to add a little correction here. And a more meaningful reason to circle back

44
00:03:33,900 --> 00:03:38,340
to all this actually is that I never really talked about what went into that final analysis,

45
00:03:38,840 --> 00:03:42,420
and it's interesting as a sublesson in its own right, so that's worth doing here.

46
00:03:43,140 --> 00:03:47,840
Now if you'll recall, most of our time last video was spent on the challenge of trying to write an

47
00:03:47,840 --> 00:03:52,460
algorithm to solve wordle that did not use the official list of all possible answers.

48
00:03:52,980 --> 00:03:58,100
To my taste that feels a bit like overfitting to a test set, and what's more fun is building something that's

49
00:03:58,100 --> 00:04:02,580
resilient. This is why we went through the whole process of looking at relative word frequencies

50
00:04:02,580 --> 00:04:07,500
in the English language to come up with some notion of how likely each one would be to be

51
00:04:07,500 --> 00:04:12,560
included as a final answer. However, for what we're doing here, where we're just trying to find an

52
00:04:12,560 --> 00:04:18,040
absolute best performance period, I am incorporating that official list and just shamelessly overfitting

53
00:04:18,040 --> 00:04:22,540
to the test set, which is to say we know with certainty whether a word is included or not,

54
00:04:22,540 --> 00:04:28,300
and we can assign a uniform probability to each one. If you'll remember, the first step in all of this

55
00:04:28,300 --> 00:04:33,640
was to say for a particular opening guess, maybe something like my old favorite, crane, how likely

56
00:04:33,640 --> 00:04:38,120
is it that you would see each of the possible patterns? And in this context, where we are

57
00:04:38,120 --> 00:04:43,420
shamelessly overfitting to the wordle answer list, all that involves is counting how many of the possible

58
00:04:43,420 --> 00:04:48,380
answers give each one of these patterns. And then of course most of our time was spent on this kind

59
00:04:48,380 --> 00:04:52,960
of funny looking formula to quantify the amount of information that you would get from this guess

60
00:04:52,960 --> 00:04:57,820
that basically involves going through each one of those buckets and saying how much information would

61
00:04:57,820 --> 00:05:02,940
you gain, that has this log expression that is a fanciful way of saying how many times would you

62
00:05:02,940 --> 00:05:08,540
cut your space of possibilities in half if you observed a given pattern. We take a weighted average

63
00:05:08,540 --> 00:05:13,180
of all of those and it gives us a measure of how much we expect to learn from this first guess.

64
00:05:13,560 --> 00:05:18,340
In a moment we'll go deeper than this, but if you simply search through all 13,000 different words

65
00:05:18,340 --> 00:05:23,580
that you could start with and you ask which one has the highest expected information, it turns out

66
00:05:23,580 --> 00:05:31,240
the best possible answer is soar, which doesn't really look like a real word, but I guess it's an

67
00:05:31,240 --> 00:05:38,440
obsolete term for a baby hawk. The top 15 openers by this metric happen to look like this, but these

68
00:05:38,440 --> 00:05:43,100
are not necessarily the best opening guesses because they're only looking one step in with

69
00:05:43,100 --> 00:05:48,240
the heuristic of expected information to try to estimate what the true score will be. But there's

70
00:05:48,240 --> 00:05:53,080
few enough patterns that we can do an exhaustive search two steps in. For example let's say you

71
00:05:53,080 --> 00:05:58,920
opened with soar and the pattern you happen to see was the most likely one, all grays, then you can run

72
00:05:58,920 --> 00:06:04,840
identical analysis from that point. For a given proposed second guess, something like kitty, what's

73
00:06:04,840 --> 00:06:09,020
the distribution across all patterns in that restricted case where we're restricted only to

74
00:06:09,020 --> 00:06:14,040
the words that would produce all grays for soar, and then we measure the flatness of that distribution

75
00:06:14,040 --> 00:06:20,000
using this expected information formula, and we do that for all 13,000 possible words that we could

76
00:06:20,000 --> 00:06:25,920
use as a second guess. Doing this we can find the optimal second guess in that scenario and the

77
00:06:25,920 --> 00:06:30,520
amount of information we were expected to get from it. And if we wash rinse and repeat and do

78
00:06:30,520 --> 00:06:35,400
this for all of the different possible patterns that you might see, we get a full map of all the

79
00:06:35,400 --> 00:06:39,200
best possible second guesses together with the expected information of each.

80
00:06:43,180 --> 00:06:48,580
From there, if you take a weighted average of all those second step values, weighted according to how

81
00:06:48,580 --> 00:06:53,520
likely you are to fall into that bucket, it gives you a measure of how much information you're likely

82
00:06:53,520 --> 00:06:59,600
to gain from the guess soar after the second step. When we use this two-step metric as our new means

83
00:06:59,600 --> 00:07:05,320
of ranking, the list gets shaken up a bit. Soar is no longer first place, it falls back to 14th,

84
00:07:05,320 --> 00:07:12,680
and instead what rises to the top is slain. Again, doesn't feel very real, and it looks like it is a

85
00:07:12,680 --> 00:07:19,720
British term for a spade that's used for cutting turf. All right, but as you can see, it is a really

86
00:07:19,720 --> 00:07:24,540
tight race among all of these top contenders for who gains the most information after those two

87
00:07:24,540 --> 00:07:30,220
steps. And even still, these are not necessarily the best opening guesses, because information is

88
00:07:30,220 --> 00:07:35,280
just the heuristic, it's not telling us the actual score if you actually play the game. What I did is

89
00:07:35,280 --> 00:07:42,480
I ran the simulation of playing all 2315 possible Wurtle games with all possible answers on the top

90
00:07:42,480 --> 00:07:50,560
250 from this list. And by doing this, seeing how they actually perform, the one that ends up very

91
00:07:50,560 --> 00:07:58,410
marginally with the best possible score turns out to be Salé, which is, let's see, Salé,

92
00:08:00,640 --> 00:08:09,020
an alternate spelling for Salé, which is a light medieval helmet. All right, if that feels a little

93
00:08:09,020 --> 00:08:14,720
too fake for you, which it does for me, you'll be happy to know that trace and crate give almost

94
00:08:14,720 --> 00:08:20,080
identical performance, and each of them has the benefit of obviously being a real word, so there

95
00:08:20,080 --> 00:08:24,060
is one day when you get it right on the first guess, since both are actual Wurtle answers.

96
00:08:25,020 --> 00:08:29,600
This move from sorting based on the best two-step entropies to sorting based on the lowest average

97
00:08:29,600 --> 00:08:35,140
score also shakes up the list, but not nearly as much. For example, Salé was previously third place

98
00:08:35,140 --> 00:08:40,200
before it bubbles to the top, and crate and trace were both fourth and fifth. If you're curious,

99
00:08:40,460 --> 00:08:44,400
you can get slightly better performance from here by doing a little brute forcing. There's a very

100
00:08:44,400 --> 00:08:49,140
nice blog post by Jonathan Olson, if you're curious about this, where he also lets you explore what

101
00:08:49,140 --> 00:08:53,660
the optimal following guesses are for a few of the starting words based on these optimal algorithms.

102
00:08:55,180 --> 00:09:00,000
Stepping back from all this though, I'm told by some people that it quote ruins the game

103
00:09:00,000 --> 00:09:05,500
to overanalyze it like this and try to find an optimal opening guess. It feels kinda dirty if

104
00:09:05,500 --> 00:09:10,160
you use that opening guess after learning it, and it feels inefficient if you don't, but the thing

105
00:09:10,160 --> 00:09:15,640
is, I don't actually think this is the best opener for a human playing the game. For one thing, you

106
00:09:15,640 --> 00:09:20,500
would need to know what the optimal second guess is for each one of the patterns you see. And more

107
00:09:20,500 --> 00:09:26,000
importantly, all of this is in a setting where we are absurdly overfit to the official Wurtle answer

108
00:09:26,000 --> 00:09:30,860
list. The moment that, say, the New York Times chooses to change what that list is under the

109
00:09:30,860 --> 00:09:35,720
hood, all of this would go out the window. The way that we humans play the game is just very

110
00:09:35,720 --> 00:09:39,640
different from what any of these algorithms are doing. We don't have the word list memorized,

111
00:09:39,700 --> 00:09:44,100
we're not doing exhaustive searches. We get intuition from things like what are the vowels,

112
00:09:44,180 --> 00:09:48,300
and how are they placed. I would actually be most happy if those of you watching this video

113
00:09:48,300 --> 00:09:53,640
promptly forgot what happens to be the technically best opening guess, and instead came out

114
00:09:53,640 --> 00:09:57,660
remembering things like how do you quantify information, or the fact that you should look

115
00:09:57,660 --> 00:10:02,140
out for when a greedy algorithm falls short of the globally best performance that you would get

116
00:10:02,140 --> 00:10:07,140
from a deeper search. For my taste at least, the joy of writing algorithms to try to play games

117
00:10:07,140 --> 00:10:12,140
actually has very little bearing on how I like to play those games as a human. The point of writing

118
00:10:12,140 --> 00:10:16,460
algorithms for all this is not to affect the way that we play the game, it's still just a fun word

119
00:10:16,460 --> 00:10:21,240
game. It's to hone in our muscles for writing algorithms in more meaningful contexts elsewhere.

