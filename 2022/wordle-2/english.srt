1
00:00:00,000 --> 00:00:04,800
Last week I put up this video about solving the game Wordle, or at least trying to solve it,

2
00:00:04,800 --> 00:00:09,920
using information theory. And I wanted to add a quick, what should we call this, an addendum,

3
00:00:09,920 --> 00:00:14,240
a confession, basically I just want to explain a place where I made a mistake.

4
00:00:14,240 --> 00:00:18,880
It turns out there was a very slight bug in the code that I was running to recreate Wordle and

5
00:00:18,880 --> 00:00:23,040
then run all of the algorithms to solve it and test their performance. And it's one of those

6
00:00:23,040 --> 00:00:27,760
bugs that affects a very small percentage of cases, so it was easy to miss, and it has only

7
00:00:27,760 --> 00:00:32,080
a very slight effect that for the most part doesn't really matter. Basically it had to do

8
00:00:32,080 --> 00:00:36,880
with how you assign a color to a guess that has multiple different letters in it. For example,

9
00:00:36,880 --> 00:00:42,080
if you guess speed and the true answer is abide, how should you color those two e's from the guess?

10
00:00:42,800 --> 00:00:46,640
Well the way that it works with the Wordle conventions is that the first e would be colored

11
00:00:46,640 --> 00:00:51,120
yellow, and the second one would be colored gray. You might think of that first one as matching up

12
00:00:51,120 --> 00:00:56,000
with something from the true answer, and the grayness is telling you there is no second e.

13
00:00:56,000 --> 00:01:01,200
By contrast, if the answer was something like erase, both of those e's would be colored yellow,

14
00:01:01,200 --> 00:01:05,920
telling you that there is a first e in a different location, and there's a second e also in a

15
00:01:05,920 --> 00:01:10,960
different location. Similarly if one of the e's hits and it's green, then that second one would

16
00:01:10,960 --> 00:01:17,280
be gray in the case where the true answer has no second e, but it would be yellow in the case where

17
00:01:17,280 --> 00:01:21,920
there is a second e and it's just in a different location. Long story short, somewhere along the

18
00:01:21,920 --> 00:01:26,240
way I accidentally introduced behavior that differs from these conventions slightly.

19
00:01:26,960 --> 00:01:31,680
Honestly it was really dumb. Basically at some point in the middle of the project I wanted to

20
00:01:31,680 --> 00:01:35,840
speed up some of the computations, and I was trying a little trick for how it computed the

21
00:01:35,840 --> 00:01:40,640
value for this pattern between any given pair of words, and you know I just didn't really think it

22
00:01:40,640 --> 00:01:45,600
through and it introduced this slight change. The ironic part is that in the end the actual way to

23
00:01:45,600 --> 00:01:50,080
make things fastest is to pre-compute all those patterns so that everything is just a lookup,

24
00:01:50,080 --> 00:01:54,000
and so it wouldn't matter how long it takes to do each one, especially if you're writing hard

25
00:01:54,000 --> 00:01:59,040
to read buggy code to make it happen. You know, you live and you learn. As far as how this affects

26
00:01:59,040 --> 00:02:03,760
the actual video, I mean very little of substance really changes. Of course the main lessons about

27
00:02:03,760 --> 00:02:08,160
what is information, what is entropy, all that stays the same. Every now and then if I'm showing

28
00:02:08,160 --> 00:02:13,360
on screen some distribution associated with a given word, that distribution might actually be

29
00:02:13,360 --> 00:02:18,000
a little bit off because some of the buckets associated with various patterns should include

30
00:02:18,000 --> 00:02:22,960
either more or fewer true answers. Even then it doesn't really come up because it was very rare

31
00:02:22,960 --> 00:02:28,400
that I would be showing a word that had multiple letters that also hit this edge case. But one of

32
00:02:28,400 --> 00:02:33,680
the very few things of substance that does change and that arguably does matter a fair bit was the

33
00:02:33,680 --> 00:02:40,240
final conclusion around how if we want to find the optimal possible score for the wordle answer list,

34
00:02:40,240 --> 00:02:45,120
what opening guess does such an algorithm use? In the video I said the best performance that I

35
00:02:45,120 --> 00:02:50,160
could find came from opening with the word crane, which was true only in the sense that the

36
00:02:50,160 --> 00:02:55,120
algorithms were playing a very slightly different game. After fixing it and rerunning it all, there

37
00:02:55,120 --> 00:03:00,000
is a different answer for what the theoretically optimal first guess is for this particular list.

38
00:03:00,800 --> 00:03:06,560
And look, I know that you know that the point of the video is not to find some technically

39
00:03:06,560 --> 00:03:11,760
optimal answer to some random online game. The point of the video is to shamelessly hop on the

40
00:03:11,760 --> 00:03:16,240
bandwagon of an internet trend to sneak attack people with an information theory lesson.

41
00:03:16,240 --> 00:03:20,160
And that's all good, I stand by that part. But I know how the internet works, and for a lot of

42
00:03:20,160 --> 00:03:26,160
people the one main takeaway was what is the best opener for the game wordle. And I get it, I walked

43
00:03:26,160 --> 00:03:30,480
into that because I put it in the thumbnail, but presumably you can forgive me if I want to add a

44
00:03:30,480 --> 00:03:35,120
little correction here. And a more meaningful reason to circle back to all this actually

45
00:03:35,120 --> 00:03:39,440
is that I never really talked about what went into that final analysis. And it's interesting

46
00:03:39,440 --> 00:03:44,560
as a sublesson in its own right, so it's worth doing here. Now if you'll recall, most of our

47
00:03:44,560 --> 00:03:49,120
time last video was spent on the challenge of trying to write an algorithm to solve wordle

48
00:03:49,120 --> 00:03:54,320
that did not use the official list of all possible answers. To my taste, that feels a bit like

49
00:03:54,320 --> 00:03:59,280
overfitting to a test set, and what's more fun is building something that's resilient. This is why

50
00:03:59,280 --> 00:04:03,920
we went through the whole process of looking at relative word frequencies in the English language

51
00:04:03,920 --> 00:04:09,120
to come up with some notion of how likely each one would be to be included as a final answer.

52
00:04:09,120 --> 00:04:13,680
However, for what we're doing here, where we're just trying to find an absolute best performance

53
00:04:13,680 --> 00:04:19,120
period, I am incorporating that official list and just shamelessly overfitting to the test set,

54
00:04:19,120 --> 00:04:23,520
which is to say we know with certainty whether a word is included or not, and we can assign a

55
00:04:23,520 --> 00:04:28,560
uniform probability to each one. If you'll remember, the first step in all of this was to

56
00:04:28,560 --> 00:04:34,080
say for a particular opening guess, maybe something like my old favorite, crane, how likely is it that

57
00:04:34,080 --> 00:04:38,560
you would see each of the possible patterns? And in this context, where we are shamelessly

58
00:04:38,560 --> 00:04:43,440
overfitting to the wordle answer list, all that involves is counting how many of the possible

59
00:04:43,440 --> 00:04:48,240
answers give each one of these patterns. And then of course most of our time was spent on this kind

60
00:04:48,240 --> 00:04:53,040
of funny looking formula to quantify the amount of information that you would get from this guess

61
00:04:53,040 --> 00:04:57,520
that basically involves going through each one of those buckets and saying how much information

62
00:04:57,520 --> 00:05:02,720
would you gain that has this log expression that is a fanciful way of saying how many times would

63
00:05:02,720 --> 00:05:08,160
you cut your space of possibilities in half if you observed a given pattern. We take a weighted

64
00:05:08,160 --> 00:05:12,800
average of all of those and it gives us a measure of how much we expect to learn from this first

65
00:05:12,800 --> 00:05:17,920
guess. In a moment we'll go deeper than this, but if you simply search through all 13,000 different

66
00:05:17,920 --> 00:05:22,880
words that you could start with and you ask which one has the highest expected information,

67
00:05:22,880 --> 00:05:28,400
it turns out the best possible answer is soar, which doesn't really look like a real word,

68
00:05:28,400 --> 00:05:36,640
but I guess it's an obsolete term for a baby hawk. The top 15 openers by this metric happen

69
00:05:36,640 --> 00:05:41,680
to look like this, but these are not necessarily the best opening guesses because they're only

70
00:05:41,680 --> 00:05:46,960
looking one step in with the heuristic of expected information to try to estimate what the true score

71
00:05:46,960 --> 00:05:52,000
will be. But there's few enough patterns that we can do an exhaustive search two steps in.

72
00:05:52,000 --> 00:05:56,640
For example, let's say you opened with soar and the pattern you happen to see was the most likely

73
00:05:56,640 --> 00:06:02,240
one, all grays, then you can run identical analysis from that point. For a given proposed

74
00:06:02,240 --> 00:06:07,360
second guess, something like kitty, what's the distribution across all patterns in that restricted

75
00:06:07,360 --> 00:06:11,920
case where we're restricted only to the words that would produce all grays for soar, and then we

76
00:06:11,920 --> 00:06:17,440
measure the flatness of that distribution using this expected information formula, and we do that

77
00:06:17,440 --> 00:06:23,680
for all 13,000 possible words that we could use as a second guess. Doing this we can find the optimal

78
00:06:23,680 --> 00:06:28,080
second guess in that scenario and the amount of information we were expected to get from it,

79
00:06:28,640 --> 00:06:32,880
and if we wash rinse and repeat and do this for all of the different possible patterns that you

80
00:06:32,880 --> 00:06:37,680
might see, we get a full map of all the best possible second guesses together with the

81
00:06:37,680 --> 00:06:46,640
expected information of each. From there, if you take a weighted average of all those second step

82
00:06:46,640 --> 00:06:51,760
values, weighted according to how likely you are to fall into that bucket, it gives you a measure

83
00:06:51,760 --> 00:06:57,440
of how much information you're likely to gain from the guess soar after the second step. When

84
00:06:57,440 --> 00:07:02,400
we use this two-step metric as our new means of ranking, the list gets shaken up a bit. Soar is

85
00:07:02,400 --> 00:07:09,040
no longer first place, it falls back to 14th, and instead what rises to the top is slain. Again,

86
00:07:09,040 --> 00:07:16,000
doesn't feel very real, and it looks like it is a British term for a spade that's used for cutting

87
00:07:16,000 --> 00:07:22,320
turf. Alright, but as you can see it is a really tight race among all of these top contenders for

88
00:07:22,320 --> 00:07:27,600
who gains the most information after those two steps. And even still, these are not necessarily

89
00:07:27,600 --> 00:07:32,000
the best opening guesses, because information is just the heuristic, it's not telling us the

90
00:07:32,000 --> 00:07:37,120
actual score if you actually play the game. What I did is I ran the simulation of playing

91
00:07:37,120 --> 00:07:44,400
all 2315 possible wordle games with all possible answers on the top 250 from this list.

92
00:07:46,160 --> 00:07:51,120
And by doing this, seeing how they actually perform, the one that ends up very marginally

93
00:07:51,120 --> 00:08:03,280
with the best possible score turns out to be Salé, which is an alternate spelling for Salé,

94
00:08:04,240 --> 00:08:10,080
which is a light medieval helmet. Alright, if that feels a little bit too fake for you,

95
00:08:10,080 --> 00:08:15,760
which it does for me, you'll be happy to know that Trace and Crate give almost identical performance.

96
00:08:16,240 --> 00:08:21,040
Each of them has the benefit of obviously being a real word, so there is one day when you get it

97
00:08:21,040 --> 00:08:26,480
right on the first guess, since both are actual wordle answers. This move from sorting based on

98
00:08:26,480 --> 00:08:31,200
the best two-step entropies to sorting based on the lowest average score also shakes up the list,

99
00:08:31,200 --> 00:08:36,000
but not nearly as much. For example, Salé was previously third place before it bubbles to the

100
00:08:36,000 --> 00:08:41,200
top, and Crate and Trace were both fourth and fifth. If you're curious, you can get slightly

101
00:08:41,200 --> 00:08:45,120
better performance from here by doing a little brute forcing. There's a very nice blog post by

102
00:08:45,120 --> 00:08:49,600
Jonathan Olson, if you're curious about this, where he also lets you explore what the optimal

103
00:08:49,600 --> 00:08:53,600
following guesses are for a few of the starting words based on these optimal algorithms.

104
00:08:55,040 --> 00:08:59,040
Stepping back from all this though, I'm told by some people that it quote

105
00:08:59,040 --> 00:09:03,920
ruins the game to overanalyze it like this and try to find an optimal opening guess.

106
00:09:03,920 --> 00:09:07,680
You know, it feels kind of dirty if you use that opening guess after learning it,

107
00:09:07,680 --> 00:09:12,400
and it feels inefficient if you don't. But the thing is, I don't actually think this is the best

108
00:09:12,400 --> 00:09:16,880
opener for a human playing the game. For one thing, you would need to know what the optimal

109
00:09:16,880 --> 00:09:22,160
second guess is for each one of the patterns that you see. And more importantly, all of this is in a

110
00:09:22,160 --> 00:09:27,440
setting where we are absurdly overfit to the official wordle answer list. The moment that,

111
00:09:27,440 --> 00:09:32,240
say, the New York Times chooses to change what that list is under the hood, all of this would go

112
00:09:32,240 --> 00:09:36,720
out the window. The way that we humans play the game is just very different from what any of these

113
00:09:36,720 --> 00:09:41,440
algorithms are doing. We don't have the word list memorized, we're not doing exhaustive searches,

114
00:09:41,440 --> 00:09:45,520
we get intuition from things like what are the vowels and how are they placed.

115
00:09:45,520 --> 00:09:50,080
I would actually be most happy if those of you watching this video promptly forgot what happens

116
00:09:50,080 --> 00:09:54,880
to be the technically best opening guess, and instead came out remembering things like how

117
00:09:54,880 --> 00:09:59,440
do you quantify information, or the fact that you should look out for when a greedy algorithm

118
00:09:59,440 --> 00:10:02,960
falls short of the globally best performance that you would get from a deeper search.

119
00:10:03,520 --> 00:10:07,920
For my taste at least, the joy of writing algorithms to try to play games actually has very

120
00:10:07,920 --> 00:10:12,800
little bearing on how I like to play those games as a human. The point of writing algorithms for

121
00:10:12,800 --> 00:10:17,280
all this is not to affect the way that we play the game, it's still just a fun word game. It's

122
00:10:17,280 --> 00:10:22,160
to hone in our muscles for writing algorithms in more meaningful contexts elsewhere.

123
00:10:37,920 --> 00:10:38,420
you

