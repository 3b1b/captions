[
 {
  "input": "Last week I put up this video about solving the game Wordle, or at least trying to solve it, using information theory. ",
  "translatedText": "先週、私は情報理論を使用して Wordle ゲームを解く、または少なくともそれを解こうとする ビデオを投稿しました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.18
 },
 {
  "input": "And I wanted to add a quick, what should we call this, an addendum? ",
  "translatedText": "そして、これを何と呼ぶべきか、補足を簡単に追加したいと思いました。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.58,
  "end": 9.78
 },
 {
  "input": "A confession? ",
  "translatedText": "告白？ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.08,
  "end": 10.66
 },
 {
  "input": "Basically I just want to explain a place where I made a mistake. ",
  "translatedText": "基本的には、間違った箇所を説明したいだけです。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.02,
  "end": 13.9
 },
 {
  "input": "It turns out there was a very slight bug in the code that I was running to recreate Wordle and then run all of the algorithms to solve it and test their performance. ",
  "translatedText": "Wordle を再作成し、すべてのアルゴリズムを実行して問題を解決し、パフォーマンスをテストする ために実行していたコードに、非常に軽微なバグがあることが判明しました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.46,
  "end": 22.0
 },
 {
  "input": "And it's one of those bugs that affects a very small percentage of cases, so it was easy to miss, and it has only a very slight effect that for the most part doesn't really matter. ",
  "translatedText": "そして、これはごく一 部のケースに影響を与えるバグの 1 つであるため、見落とされやすく、影響はご くわずかであり、ほとんどの場合はあまり問題になりません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 22.6,
  "end": 30.5
 },
 {
  "input": "Basically it had to do with how you assign a color to a guess that has multiple different letters in it. ",
  "translatedText": "基本的には、複数の異 なる文字が含まれる推測に色を割り当てる方法に関係していました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.22,
  "end": 36.36
 },
 {
  "input": "For example, if you guess speed and the true answer is abide, how should you color those two e's from the guess? ",
  "translatedText": "たとえば、速度を推 測し、本当の答えが abide だった場合、推測に基づいてこれら 2 つの e をどのように色付けする必要がありますか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 36.52,
  "end": 42.12
 },
 {
  "input": "Well the way that it works with the Wordle conventions is that the first e would be colored yellow, and the second one would be colored gray. ",
  "translatedText": "Wordle の規約に従った場合、最初の e は黄色、2 番 目の e は灰色になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.06,
  "end": 49.08
 },
 {
  "input": "You might think of that first one as matching up with something from the true answer, and the grayness is telling you there is no second e. ",
  "translatedText": "最初の e は本当の答えの何かと一致して おり、灰色は 2 番目の e がないことを示していると考えるかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 49.6,
  "end": 55.52
 },
 {
  "input": "By contrast, if the answer was something like erase, both of those e's would be colored yellow, telling you that there is a first e in a different location, and there's a second e also in a different location. ",
  "translatedText": "対照的に、答えが消去のようなものであった場合、これらの e は両方とも黄色 になり、最初の e が別の場所にあり、2 番目の e も別の場所にあること がわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.52,
  "end": 66.78
 },
 {
  "input": "Similarly if one of the e's hits and it's green, then that second one would be gray in the case where the true answer has no second e, but it would be yellow in the case where there is a second e and it's just in a different location. ",
  "translatedText": "同様に、e の 1 つがヒットして緑色の場合、真の答えに 2 番目 の e がない場合は 2 番目の e は灰色になりますが、2 番目の e があり 、それが単に異なる場合は黄色になります。位置。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.3,
  "end": 80.04
 },
 {
  "input": "Long story short, somewhere along the way I accidentally introduced behavior that differs from these conventions slightly. ",
  "translatedText": "簡単に言うと、途中のどこかで 、これらの規則とはわずかに異なる動作を誤って導入してしまいました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.7,
  "end": 86.4
 },
 {
  "input": "Honestly it was really dumb. ",
  "translatedText": "正直本当にバカでした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.1,
  "end": 88.54
 },
 {
  "input": "Basically at some point in the middle of the project I wanted to speed up some of the computations, and I was trying a little trick for how it computed the value for this pattern between any given pair of words, and you know I just didn't really think it through and it introduced this slight change. ",
  "translatedText": "基本的に、プロジェクトの途中のある時点で、計算の一部を高速化し たいと思い、特定の単語のペア間のこのパターンの値をどのように計算するかについてちょっと したトリックを試していましたが、ご存知のとおり、私はそれを実行できませんでした。よく考え ずに、このわずかな変更を導入しました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.88,
  "end": 102.72
 },
 {
  "input": "The ironic part is that in the end the actual way to make things fastest is to pre-compute all those patterns so that everything is just a lookup, and so it wouldn't matter how long it takes to do each one, especially if you're writing hard to read buggy code to make it happen. ",
  "translatedText": "皮肉な部分は、最終的に物事を高速化する実際の方法は、す べてが検索だけになるようにすべてのパターンを事前に計算することなので、特に次のような場合には、そ れぞれの実行にどれだけ時間がかかるかは問題ではないということです。それを実現するために、読みにく いバグのあるコードを書いています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.22,
  "end": 115.84
 },
 {
  "input": "You know, you live and you learn. ",
  "translatedText": "ご存知のように、あなたは生きて学びます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 116.4,
  "end": 117.24
 },
 {
  "input": "As far as how this affects the actual video, I mean very little of substance really changes. ",
  "translatedText": "これが実際のビデオにどのような 影響を与えるかというと、実質的にはほとんど変わらないということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.04,
  "end": 122.34
 },
 {
  "input": "Of course the main lessons about what is information, what is entropy, all that stays the same. ",
  "translatedText": "もちろん、情報とは何か、エン トロピーとは何かなどに関する主な教訓は変わりません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 122.66,
  "end": 126.56
 },
 {
  "input": "Every now and then if I'm showing on screen some distribution associated with a given word, that distribution might actually be a little bit off because some of the buckets associated with various patterns should include either more or fewer true answers. ",
  "translatedText": "時々、特定の単語に関連付けられた 分布を画面に表示する場合、その分布は実際には少しずれている可能性があります。これは、さま ざまなパターンに関連付けられたバケットの一部には、より多くの、またはより少ない正解が含 まれているはずであるためです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.86,
  "end": 140.32
 },
 {
  "input": "Even then it doesn't really come up because it was very rare that I would be showing a word that had multiple letters that also hit this edge case. ",
  "translatedText": "それでも、この特殊なケースに該当する複数の文字を含む単 語を表示することは非常にまれだったので、実際には思い浮かびませんでした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.84,
  "end": 146.96
 },
 {
  "input": "But one of the very few things of substance that does change and that arguably does matter a fair bit was the final conclusion around how if we want to find the optimal possible score for the wordle answer list, what opening guess does such an algorithm use? ",
  "translatedText": "しかし、実 際に変化し、間違いなくかなり重要な実質的なものの 1 つは、Wordle の回 答リストの最適なスコアを見つけたい場合、そのようなアルゴリズムはどのような開始推 測を使用するかという最終結論でした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.68,
  "end": 162.46
 },
 {
  "input": "In the video I said the best performance that I could find came from opening with the word crane, which was true only in the sense that the algorithms were playing a very slightly different game. ",
  "translatedText": "ビデオの中で、私が見つけた最高のパフォーマンスはクレ ーンという単語で始まるオープニングから来ていると述べましたが、これはアルゴリズムが非常にわずかに異 なるゲームをプレイしているという意味でのみ真実でした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.08,
  "end": 172.56
 },
 {
  "input": "After fixing it and rerunning it all, there is a different answer for what the theoretically optimal first guess is for this particular list. ",
  "translatedText": "それを修正してすべてを再実行すると、この特 定のリストに対する理論的に最適な最初の推測が何であるかについては、別の答えが得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.16,
  "end": 180.16
 },
 {
  "input": "And look, I know that you know that the point of the video is not to find some technically optimal answer to some random online game. ",
  "translatedText": "ビデオの目的は、ランダムなオンライン ゲームに対する技術的に最適な答えを見つける ことではないことはご存知でしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 181.0,
  "end": 189.1
 },
 {
  "input": "The point of the video is to shamelessly hop on the bandwagon of an internet trend to sneak attack people with an information theory lesson. ",
  "translatedText": "このビデオの要点は、恥知らずにもインターネット のトレンドに便乗して、情報理論のレッスンで人々をこっそり攻撃しようということだ。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.46,
  "end": 195.9
 },
 {
  "input": "And that's all good, I stand by that part. ",
  "translatedText": "それでいいのです、私はその部分を支持します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.32,
  "end": 198.0
 },
 {
  "input": "But I know how the internet works, and for a lot of people the one main takeaway was what is the best opener for the game wordle. ",
  "translatedText": "しかし、私はインターネットがどのように機能するかを知っており、多くの人 にとって重要なポイントは、ゲーム Wordle にとって最適なオープナーは何かということでした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.2,
  "end": 204.6
 },
 {
  "input": "And I get it, I walked into that because I put it in the thumbnail, but presumably you can forgive me if I want to add a little correction here. ",
  "translatedText": "わかりました。サムネ ールに入れたからそのようになりましたが、ここで少し修正を加えたい場合はご容赦い ただけると思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 205.28,
  "end": 211.86
 },
 {
  "input": "And a more meaningful reason to circle back to all this actually is that I never really talked about what went into that final analysis. ",
  "translatedText": "そして、実際にこれらすべてに戻るより意味のある理由は、最終 分析の内容について実際に話したことがないということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.98,
  "end": 218.34
 },
 {
  "input": "And it's interesting as a sublesson in its own right, so it's worth doing here. ",
  "translatedText": "そして、それ自体がサブ レッスンとしても興味深いので、ここで行う価値があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.84,
  "end": 222.42
 },
 {
  "input": "Now if you'll recall, most of our time last video was spent on the challenge of trying to write an algorithm to solve wordle that did not use the official list of all possible answers. ",
  "translatedText": "思い出していただけると思いますが、前回 のビデオでは、考えられるすべての答えの公式リストを使用していない Wordle を解くためのアルゴ リズムを作成するという課題にほとんどの時間が費やされました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.14,
  "end": 232.46
 },
 {
  "input": "To my taste, that feels a bit like overfitting to a test set, and what's more fun is building something that's resilient. ",
  "translatedText": "私の好みでは、それはテストセットに過剰 適合しているように感じられますが、より楽しいのは回復力のあるものを構築することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.98,
  "end": 238.48
 },
 {
  "input": "This is why we went through the whole process of looking at relative word frequencies in the English language to come up with some notion of how likely each one would be to be included as a final answer. ",
  "translatedText": "これが、英 語における相対的な単語の頻度を調べるという全プロセスを経て、それぞれの単語が最終 的な答えとして含まれる可能性がどの程度あるのかという概念を導き出した理由です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.9,
  "end": 248.76
 },
 {
  "input": "However, for what we're doing here, where we're just trying to find an absolute best performance period, I am incorporating that official list and just shamelessly overfitting to the test set, which is to say we know with certainty whether a word is included or not, and we can assign a uniform probability to each one. ",
  "translatedText": "ただし、私たちがここで行っていること、つまり絶対的に最高のパフォーマンスの期間を見つけようとしている だけなので、その公式リストを組み込んで、恥知らずにもテストセットに過剰適合しているだけです。つまり 、単語が正しいかどうかを確実に知っているということです。が含まれるかどうかを判断し、それぞれに均一 の確率を割り当てることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 249.4,
  "end": 265.46
 },
 {
  "input": "If you'll remember, the first step in all of this was to say for a particular opening guess, maybe something like my old favorite, crane, how likely is it that you would see each of the possible patterns? ",
  "translatedText": "覚えていると思いますが、このすべての最初のステップは、特定の冒頭の推 測について、おそらく私の昔からのお気に入りである鶴のようなものについて、考えられる各パターンが表示され る可能性はどのくらいですか? と言うことでした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 266.44,
  "end": 276.18
 },
 {
  "input": "And in this context, where we are shamelessly overfitting to the wordle answer list, all that involves is counting how many of the possible answers give each one of these patterns. ",
  "translatedText": "そして、この文脈では、私たちが恥知らずにも Word le の回答リストに過剰適合しているのですが、必要なのは、これらのパターンのそれぞれに当てはまる回 答がいくつあるかを数えることだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 276.68,
  "end": 285.34
 },
 {
  "input": "And then of course most of our time was spent on this kind of funny looking formula to quantify the amount of information that you would get from this guess that basically involves going through each one of those buckets and saying how much information would you gain that has this log expression that is a fanciful way of saying how many times would you cut your space of possibilities in half if you observed a given pattern. ",
  "translatedText": "そしてもちろん、私たちの時間のほとんどは、この推 測から得られる情報量を定量化するこの種のおかしな見た目の式に費やされました。基本的には、これらのバケットを 1 つずつ調べて、その推測からどのくらいの情報 が得られるかを計算する必要があります。この対数表現は、特定のパターンを観察した 場合、可能性の空間が何回半分になるかを表す空想的な方法です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.98,
  "end": 306.84
 },
 {
  "input": "We take a weighted average of all of those and it gives us a measure of how much we expect to learn from this first guess. ",
  "translatedText": "これらすべての加重 平均を計算すると、この最初の推測からどの程度の学習が期待できるかの目安が得られま す。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.6,
  "end": 313.18
 },
 {
  "input": "In a moment we'll go deeper than this, but if you simply search through all 13,000 different words that you could start with and you ask which one has the highest expected information, it turns out the best possible answer is soar, which doesn't really look like a real word, but I guess it's an obsolete term for a baby hawk. ",
  "translatedText": "この後さらに詳しく説明しますが、手始めに考えられる 13,000 の異なる単 語すべてを単純に検索し、期待される情報が最も高い単語はどれかを尋ねると、考えら れる最良の答えは soar であることがわかります。本物の言葉のようには見えませ んが、赤ちゃんタカを表す死語だと思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.56,
  "end": 333.0
 },
 {
  "input": "The top 15 openers by this metric happen to look like this, but these are not necessarily the best opening guesses because they're only looking one step in with the heuristic of expected information to try to estimate what the true score will be. ",
  "translatedText": "この指標によるトップ 15 のオープニングはたま たま次のようになりますが、これらは、実際のスコアがどのようになるかを推定するために、期待される情報の ヒューリスティックを 1 ステップ調べているだけであるため、必ずしも最良のオープニングの推測ではあり ません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.04,
  "end": 347.54
 },
 {
  "input": "But there's few enough patterns that we can do an exhaustive search two steps in. ",
  "translatedText": "しかし、2 ステップで徹底的な検索を実行できる十分なパターンはほとんどありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 347.92,
  "end": 351.68
 },
 {
  "input": "For example, let's say you opened with soar and the pattern you happen to see was the most likely one, all grays, then you can run identical analysis from that point. ",
  "translatedText": "たとえば、soar で開始し、たまたま見たパターンが最も可能性の高いパターンで、すべ てグレーだった場合、その時点から同じ分析を実行できるとします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.16,
  "end": 360.8
 },
 {
  "input": "For a given proposed second guess, something like kitty, what's the distribution across all patterns in that restricted case where we're restricted only to the words that would produce all grays for soar, and then we measure the flatness of that distribution using this expected information formula, and we do that for all 13,000 possible words that we could use as a second guess. ",
  "translatedText": "与えられた提案された 2 番目の推測 (キティのようなもの) について、急上昇のすべてのグレーを生成する単語のみに 制限される制限されたケースにおけるすべてのパターンにわたる分布はどうなるでしょうか。その後、 この予想値を使用してその分布の平坦性を測定します。情報式を作成し、2 番目の推測として使用で きる 13,000 個の単語すべてに対してそれを行います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.32,
  "end": 381.42
 },
 {
  "input": "Doing this we can find the optimal second guess in that scenario and the amount of information we were expected to get from it, and if we wash rinse and repeat and do this for all of the different possible patterns that you might see, we get a full map of all the best possible second guesses together with the expected information of each. ",
  "translatedText": "これを行うことで、そのシナリオ での最適な 2 番目の推測と、そこから得られると期待される情報の量を見つけることができ ます。洗濯、すすぎを繰り返し、考えられるさまざまなパターンすべてに対してこれを実行する と、次の結果が得られます。考えられるすべての最良の 2 番目の推測の完全なマップと、そ れぞれの予想される情報。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.12,
  "end": 399.2
 },
 {
  "input": "From there, if you take a weighted average of all those second step values, weighted according to how likely you are to fall into that bucket, it gives you a measure of how much information you're likely to gain from the guess soar after the second step. ",
  "translatedText": "そこから、すべての第 2 ステップの値を加重平均し、 そのバケツに該当する可能性に従って重み付けすると、その後の推測の急上昇か らどの程度の情報を得る可能性があるかの尺度が得られます。第二段階。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 416.8
 },
 {
  "input": "When we use this two-step metric as our new means of ranking, the list gets shaken up a bit. ",
  "translatedText": "この 2 段階の指標をランク付けの新しい手段として使用すると、リストは少し変わります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.38,
  "end": 421.78
 },
 {
  "input": "Soar is no longer first place, it falls back to 14th, and instead what rises to the top is slain. ",
  "translatedText": "Soar はもはや 1 位ではなく、14 位に後退し、代わりにトップに上がったものが殺されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.08,
  "end": 427.66
 },
 {
  "input": "Again, doesn't feel very real, and it looks like it is a British term for a spade that's used for cutting turf. ",
  "translatedText": "これもあ まり現実的ではなく、芝を刈るのに使用されるスペードを意味するイギリスの用語のようです 。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.64,
  "end": 436.38
 },
 {
  "input": "Alright, but as you can see it is a really tight race among all of these top contenders for who gains the most information after those two steps. ",
  "translatedText": "わかりましたが、ご覧のとおり、これら 2 つのステップの後に誰が最も多くの情報を取得するかについて 、これらの上位候補全員の間で非常に接戦となっています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.2,
  "end": 445.0
 },
 {
  "input": "And even still, these are not necessarily the best opening guesses, because information is just the heuristic, it's not telling us the actual score if you actually play the game. ",
  "translatedText": "それでも、情報は単なるヒューリスティックで あり、実際にゲームをプレイした場合の実際のスコアを教えてくれるわけではないため、これらは必 ずしも最良の開始予想であるとは限りません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.7,
  "end": 454.0
 },
 {
  "input": "What I did is I ran the simulation of playing all 2315 possible wordle games with all possible answers on the top 250 from this list. ",
  "translatedText": "私がやったことは、このリストの上位 250 にあるすべての 可能な答えを使用して、2,315 の可能な単語ゲームをすべてプレイするシミュレーションを実行したことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 454.58,
  "end": 464.62
 },
 {
  "input": "And by doing this, seeing how they actually perform, the one that ends up very marginally with the best possible score turns out to be Salé, which is an alternate spelling for Salé, which is a light medieval helmet. ",
  "translatedText": "そして、これを行うことで、彼らが実際にどのようにパフォーマンスするかを見て、可能な限り最高の スコアで非常に僅差で終了したのは、中世の軽いヘルメットである Salé の別の綴りである S alé であることがわかります。わかりました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.46,
  "end": 485.98
 },
 {
  "input": "Alright, if that feels a little bit too fake for you, which it does for me, you'll be happy to know that Trace and Crate give almost identical performance. ",
  "translatedText": "もしあなたにとってそれが少し偽物すぎると感じたなら、私にとって はそうでしたが、Trace と Crate がほぼ同じパフォーマンスを発揮することを知って喜んでいただけるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.98,
  "end": 495.78
 },
 {
  "input": "Each of them has the benefit of obviously being a real word, so there is one day when you get it right on the first guess, since both are actual wordle answers. ",
  "translatedText": "それぞれの答えは明らかに実際の単語であるという利点があり、両方とも実際の単語の答えで あるため、最初の推測で正解する日が来ることがあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.3,
  "end": 504.06
 },
 {
  "input": "This move from sorting based on the best two-step entropies to sorting based on the lowest average score also shakes up the list, but not nearly as much. ",
  "translatedText": "最良の 2 段階エントロピーに基づい た並べ替えから、最低の平均スコアに基づいた並べ替えへの移行により、リストも大きく変わりますが、それほ ど大きくはありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 505.02,
  "end": 512.46
 },
 {
  "input": "For example, Salé was previously third place before it bubbles to the top, and Crate and Trace were both fourth and fifth. ",
  "translatedText": "たとえば、サレはトップに浮上する前は 3 位でしたが、クレイトと トレースは両方とも 4 位と 5 位でした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.66,
  "end": 519.08
 },
 {
  "input": "If you're curious, you can get slightly better performance from here by doing a little brute forcing. ",
  "translatedText": "興味がある場合は、少し強引に実行することで 、ここからパフォーマンスをわずかに向上させることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 519.64,
  "end": 523.72
 },
 {
  "input": "There's a very nice blog post by Jonathan Olson, if you're curious about this, where he also lets you explore what the optimal following guesses are for a few of the starting words based on these optimal algorithms. ",
  "translatedText": "これについて興味がある場合は、Jo nathan Olson による非常に優れたブログ投稿があります。そこでは、最適なアルゴリズ ムに基づいて、いくつかの開始単語に対する最適な次の推測が何であるかを調査することもできます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.1,
  "end": 533.66
 },
 {
  "input": "Stepping back from all this though, I'm told by some people that it quote ruins the game to overanalyze it like this and try to find an optimal opening guess. ",
  "translatedText": "しかし、これらすべてから一歩下がって、このように過度に分析し、最適なオープニングの 推測を見つけようとするのはゲームを台無しにする引用だと何人かの人々が言いました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 535.18,
  "end": 543.6
 },
 {
  "input": "You know, it feels kind of dirty if you use that opening guess after learning it, and it feels inefficient if you don't. ",
  "translatedText": "ご存知のとおり、冒頭の推測を覚えた後に使用すると、なんだか汚い感じがしますし、使 用しないと非効率的に感じます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.26,
  "end": 549.66
 },
 {
  "input": "But the thing is, I don't actually think this is the best opener for a human playing the game. ",
  "translatedText": "しかし、問題は、これがゲームをプレイする人間にとって最良の開幕戦で あるとは実際には思わないということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 549.8,
  "end": 554.4
 },
 {
  "input": "For one thing, you would need to know what the optimal second guess is for each one of the patterns that you see. ",
  "translatedText": "まず、表示されるパターンごとに最適な 2 番目 の推測が何であるかを知る必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.88,
  "end": 559.68
 },
 {
  "input": "And more importantly, all of this is in a setting where we are absurdly overfit to the official wordle answer list. ",
  "translatedText": "そしてさらに重要なことは、これらすべてが公式の Wordle 回答リストに不条理なほど過剰適合している状況にあるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 560.26,
  "end": 566.36
 },
 {
  "input": "The moment that, say, the New York Times chooses to change what that list is under the hood, all of this would go out the window. ",
  "translatedText": "たとえば、ニューヨー ク・タイムズがそのリストの内容を水面下で変更することを選択した瞬間、これらすべては窓の外に消えて しまいます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.58,
  "end": 572.88
 },
 {
  "input": "The way that we humans play the game is just very different from what any of these algorithms are doing. ",
  "translatedText": "私たち人間がゲームをプレイする方法は、これらのアルゴリズムが行うものとはま ったく異なります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.58,
  "end": 577.68
 },
 {
  "input": "We don't have the word list memorized, we're not doing exhaustive searches, we get intuition from things like what are the vowels and how are they placed. ",
  "translatedText": "単語リストを暗記したり、徹底的に検索したりするわけではなく 、母音が何なのか、どのように配置されているのかなどから直感をつかみます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.02,
  "end": 585.08
 },
 {
  "input": "I would actually be most happy if those of you watching this video promptly forgot what happens to be the technically best opening guess, and instead came out remembering things like how do you quantify information, or the fact that you should look out for when a greedy algorithm falls short of the globally best performance that you would get from a deeper search. ",
  "translatedText": "このビデオを見ている人が、技術的に最も適切な冒頭の推測をすぐに忘れて、情 報をどのように定量化するか、または貪欲なときに注意する必要があるという 事実などを思い出していただければ、私は実際に最も幸せです。このアルゴリズ ムは、より詳細な検索から得られる世界最高のパフォーマンスには及ばない。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.64,
  "end": 603.1
 },
 {
  "input": "For my taste at least, the joy of writing algorithms to try to play games actually has very little bearing on how I like to play those games as a human. ",
  "translatedText": "少なくとも私の好みでは、ゲームをプレイするためのアルゴリズムを書く喜びは、人間としてそれらのゲーム をプレイするのが好きかどうかにはほとんど関係がありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 603.7,
  "end": 610.76
 },
 {
  "input": "The point of writing algorithms for all this is not to affect the way that we play the game, it's still just a fun word game. ",
  "translatedText": "これらすべてのアルゴリズムを記述すること のポイントは、ゲームのプレイ方法に影響を与えることではなく、それでも単なる楽しい言葉遊びです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 611.3,
  "end": 616.78
 },
 {
  "input": "It's to hone in our muscles for writing algorithms in more meaningful contexts elsewhere. ",
  "translatedText": "それは 、他の場所でより意味のあるコンテキストでアルゴリズムを書くための筋肉を磨くことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.1,
  "end": 620.72
 }
]