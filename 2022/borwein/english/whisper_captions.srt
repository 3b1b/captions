1
00:00:00,000 --> 00:00:02,980
Sometimes it feels like the universe is just messing with you.

2
00:00:03,340 --> 00:00:07,680
I have up on screen here a sequence of computations, and don't worry, in a moment

3
00:00:07,680 --> 00:00:10,380
we're gonna unpack and visualize what each one is really saying.

4
00:00:10,920 --> 00:00:17,260
What I want you to notice is how the sequence follows a very predictable, if random, seeming pattern, and how each computation

5
00:00:17,260 --> 00:00:22,940
happens to equal pi. And if you were just messing around evaluating these on a computer for some reason,

6
00:00:23,160 --> 00:00:25,880
you might think that this was a pattern that would go on forever.

7
00:00:25,880 --> 00:00:32,400
But it doesn't. At some point it stops, and instead of equaling pi, you get a value which is just barely,

8
00:00:33,200 --> 00:00:34,340
barely less than pi.

9
00:00:38,780 --> 00:00:40,940
All right, let's dig into what's going on here.

10
00:00:41,300 --> 00:00:45,080
The main character in the story today is the function sine of x divided by x.

11
00:00:45,460 --> 00:00:48,960
This actually comes up commonly enough in math and engineering that it gets its own name,

12
00:00:49,360 --> 00:00:52,520
sinc, and the way you might think about it is by starting with a normal

13
00:00:52,520 --> 00:00:57,100
oscillating sine curve, and then sort of squishing it down as you get far away from zero by

14
00:00:57,100 --> 00:01:02,500
multiplying it by 1 over x. And the astute among you might ask about what happens at x equals 0,

15
00:01:02,800 --> 00:01:05,740
since when you plug that in it looks like dividing 0 by 0.

16
00:01:06,400 --> 00:01:09,540
And then the even more astute among you, maybe fresh out of a calculus class,

17
00:01:09,900 --> 00:01:14,780
could point out that for values closer and closer to 0, the function gets closer and closer to 1.

18
00:01:15,260 --> 00:01:20,320
So if we simply redefine the sinc function at 0 to equal 1, you get a nice continuous curve.

19
00:01:20,320 --> 00:01:24,880
All of that is a little by the by because the thing we actually care about is the

20
00:01:24,880 --> 00:01:27,920
integral of this curve from negative infinity to infinity,

21
00:01:27,920 --> 00:01:33,880
which you'd think of as meaning the area between the curve and the x-axis, or more precisely the signed area,

22
00:01:34,220 --> 00:01:38,040
meaning you add all the area bound by the positive parts of the graph in the x-axis,

23
00:01:38,520 --> 00:01:42,300
and you subtract all of the parts bound by the negative parts of the graph and the x-axis.

24
00:01:42,740 --> 00:01:46,880
Like we saw at the start, it happens to be the case that this evaluates to be exactly pi,

25
00:01:46,880 --> 00:01:53,380
which is nice and also a little weird, and it's not entirely clear how you would approach this with the usual tools of calculus.

26
00:01:53,980 --> 00:01:56,560
Towards the end of the video, I'll share the trick for how you would do this.

27
00:01:56,840 --> 00:02:03,280
Progressing on with the sequence I opened with, the next step is to take a copy of the sinc function, where you plug in x divided by 3,

28
00:02:03,720 --> 00:02:08,260
which will basically look like the same graph, but stretched out horizontally by a factor of 3.

29
00:02:08,900 --> 00:02:11,180
When we multiply these two functions together,

30
00:02:11,440 --> 00:02:16,040
we get a much more complicated wave whose mass seems to be more concentrated towards the middle,

31
00:02:16,040 --> 00:02:20,000
and with any usual functions you would expect this completely changes the area.

32
00:02:20,380 --> 00:02:23,680
You can't just randomly modify an integral like this and expect nothing to change.

33
00:02:24,260 --> 00:02:28,780
So already it's a little bit weird that this result also equals pi, that nothing has changed.

34
00:02:29,080 --> 00:02:31,180
That's another mystery you should add to your list.

35
00:02:31,660 --> 00:02:36,620
And the next step in the sequence was to take an even more stretched out version of the sinc function by a factor of 5,

36
00:02:37,060 --> 00:02:44,000
multiply that by what we already have, and again look at the signed area underneath the whole curve, which again equals pi.

37
00:02:44,860 --> 00:02:50,940
And it continues on like this. With each iteration, we stretch out by a new odd number and multiply that into what we have.

38
00:02:51,640 --> 00:02:54,660
One thing you might notice is how except at the input x equals 0,

39
00:02:55,260 --> 00:02:59,720
every single part of this function is progressively getting multiplied by something that's smaller than 1.

40
00:03:00,340 --> 00:03:05,000
So you would expect, as the sequence progresses, for things to get squished down more and more, and

41
00:03:05,000 --> 00:03:07,440
if anything you would expect the area to be getting smaller.

42
00:03:08,360 --> 00:03:10,460
Eventually that is exactly what happens,

43
00:03:10,460 --> 00:03:17,780
but what's bizarre is that it stays so stable for so long, and of course more pertinently, that when it does break at the value 15,

44
00:03:18,200 --> 00:03:24,000
it does so by the tiniest tiny amount. And before you go thinking this is the result of some numerical error,

45
00:03:24,060 --> 00:03:28,220
maybe because we're doing something with floating-point arithmetic, if you work this out more precisely,

46
00:03:28,620 --> 00:03:35,840
here is the exact value of that last integral, which is a certain fraction of pi where the numerator and the denominator are absurd.

47
00:03:35,980 --> 00:03:38,700
They're both around 400 billion billion billion.

48
00:03:40,460 --> 00:03:45,980
So this pattern was described in a paper by a father-son pair, Jonathan and David Borwein, which is very fun,

49
00:03:46,220 --> 00:03:50,880
and they mentioned how when a fellow researcher was computing these integrals using a computer algebra system,

50
00:03:51,340 --> 00:03:53,520
he assumed that this had to be some kind of bug.

51
00:03:53,860 --> 00:03:56,260
But it's not a bug, it is a real phenomenon.

52
00:03:56,680 --> 00:04:02,840
And it gets weirder than that actually. If we take all these integrals and include yet another factor, 2 cosine of x,

53
00:04:03,160 --> 00:04:08,580
which again you would think changes their values entirely, you can't just randomly multiply new things into an integral like this,

54
00:04:08,580 --> 00:04:15,040
it continues to equal pi for much much longer, and it's not until you get to the number 113 that it breaks.

55
00:04:15,200 --> 00:04:19,680
And when it breaks, it's by the most puny, absolutely subtle amount that you could imagine.

56
00:04:20,440 --> 00:04:24,080
So the natural question is what on earth is going on here?

57
00:04:24,380 --> 00:04:27,680
And luckily there actually is a really satisfying explanation for all this.

58
00:04:28,180 --> 00:04:32,640
The way I think I'll go about this is to show you a phenomenon that first looks completely unrelated,

59
00:04:32,640 --> 00:04:38,380
but it shows a similar pattern where you have a value that stays really stable until you get to the number 15,

60
00:04:38,380 --> 00:04:40,580
and then it falters by just a tiny amount.

61
00:04:41,300 --> 00:04:48,340
And then after that I'll show why this seemingly unrelated phenomenon is secretly the same as all our integral expressions, but in disguise.

62
00:04:49,120 --> 00:04:51,520
So, turning our attention to what seems completely different,

63
00:04:51,860 --> 00:04:58,540
consider a function that I'm going to be calling rect of x, which is defined to equal 1 if the input is between

64
00:04:58,540 --> 00:05:04,520
negative 1 half and 1 half, and otherwise it's equal to 0. So the function is this boring step, basically.

65
00:05:04,520 --> 00:05:09,840
This will be the first in a sequence of functions that we define, so I'll call it f1 of x, and

66
00:05:09,840 --> 00:05:14,640
each new function in our sequence is going to be a kind of moving average of the previous function.

67
00:05:15,800 --> 00:05:20,960
So for example, the way the second iteration will be defined is to take this sliding window whose width is 1 third,

68
00:05:21,560 --> 00:05:29,180
and for a particular input x, when the window is centered at that input x, the value in my new function drawn below is

69
00:05:29,180 --> 00:05:33,840
defined to be equal to the average value of the first function above inside that window.

70
00:05:33,840 --> 00:05:37,760
So for example, when the window is far enough to the left, every value inside it is 0,

71
00:05:38,100 --> 00:05:43,440
so the graph on the bottom is showing 0. As soon as that window starts to go over the plateau a little bit, the

72
00:05:43,440 --> 00:05:46,860
average value is a little more than 0, and you see that in the graph below.

73
00:05:47,280 --> 00:05:51,900
And notice that when exactly half the window is over that plateau at 1 and half of it is at 0,

74
00:05:52,340 --> 00:05:57,280
the corresponding value in the bottom graph is 1 half, and you get the point. The important thing

75
00:05:57,280 --> 00:06:01,640
I want you to focus on is how when that window is entirely in the plateau above,

76
00:06:01,640 --> 00:06:07,700
where all the values are 1, then the average value is also 1, so we get this plateau on our function at the bottom.

77
00:06:08,300 --> 00:06:12,800
Let's call this bottom function f2 of x, and what I want you to think about is the

78
00:06:12,800 --> 00:06:15,300
length of the plateau for that second function.

79
00:06:15,480 --> 00:06:16,440
How wide should it be?

80
00:06:17,020 --> 00:06:22,840
If you think about it for a moment, the distance between the left edge of the top plateau and the left edge of the bottom

81
00:06:22,840 --> 00:06:27,260
plateau will be exactly half of the width of the window, so half of 1 third.

82
00:06:27,640 --> 00:06:32,820
And similarly on the right side, the distance between the edges of the plateaus is half of the window width.

83
00:06:33,200 --> 00:06:36,660
So overall it's 1 minus that window width, which is 1 minus 1 third.

84
00:06:37,380 --> 00:06:42,300
The value we're going to be computing, the thing that will look stable for a while before it breaks, is

85
00:06:42,300 --> 00:06:48,700
the value of this function at the input 0, which in both of these iterations is equal to 1 because it's inside that plateau.

86
00:06:49,200 --> 00:06:52,820
For the next iteration, we're going to take a moving average of that last function,

87
00:06:52,840 --> 00:06:55,320
but this time with the window whose width is 1 fifth.

88
00:06:55,320 --> 00:07:01,860
It's kind of fun to think about why as you slide around this window you get a smoothed out version of the previous function, and

89
00:07:02,240 --> 00:07:08,480
again, the significant thing I want you to focus on is how when that window is entirely inside the plateau of the previous function,

90
00:07:08,920 --> 00:07:11,460
then by definition the bottom function is going to equal 1.

91
00:07:11,980 --> 00:07:17,180
This time the length of that plateau on the bottom will be the length of the previous one, 1 minus 1 third,

92
00:07:17,600 --> 00:07:19,240
minus the window width, 1 fifth.

93
00:07:19,600 --> 00:07:25,180
The reasoning is the same as before in order to go from the point where the middle of the window is on that top plateau

94
00:07:25,180 --> 00:07:29,700
to where the entirety of the window is inside that plateau is half the window width and

95
00:07:29,700 --> 00:07:35,360
likewise on the right side, and once more the value to record is the output of this function when the input is 0,

96
00:07:35,660 --> 00:07:37,320
which again is exactly 1.

97
00:07:38,580 --> 00:07:44,040
The next iteration is a moving average with a window width of 1 seventh. The plateau gets smaller by that 1 over 7.

98
00:07:44,500 --> 00:07:50,780
Doing one more iteration with 1 over 9, the plateau gets smaller by that amount. And as we keep going the plateau gets thinner and thinner.

99
00:07:51,820 --> 00:07:58,320
And also notice how just outside of the plateau the function is really really close to 1 because it's always been the result of an

100
00:07:58,320 --> 00:08:02,740
average between the plateau at 1 and the neighbors, which themselves are really really close to 1.

101
00:08:03,440 --> 00:08:08,460
The point at which all of this breaks is once we get to the iteration where we're sliding a window with width

102
00:08:08,460 --> 00:08:14,660
1 15th across the whole thing. At that point the previous plateau is actually thinner than the window itself.

103
00:08:14,820 --> 00:08:20,580
So even at the input x equals 0, this moving average will have to be ever so slightly smaller than 1.

104
00:08:20,780 --> 00:08:26,700
And the only thing that's special about the number 15 here is that as we keep adding the reciprocals of these odd fractions,

105
00:08:26,960 --> 00:08:33,220
1 third plus 1 fifth plus 1 seventh on and on, it's once we get to 1 15th that that sum grows to be bigger than 1.

106
00:08:33,580 --> 00:08:37,940
And in the context of our shrinking plateaus, having started with a plateau of width 1,

107
00:08:38,360 --> 00:08:41,140
it's now shrunk down so much that it'll disappear entirely.

108
00:08:41,800 --> 00:08:45,540
The point is with this as a sequence of functions that we've defined by a

109
00:08:45,540 --> 00:08:50,560
seemingly random procedure, if I ask you to compute the values of all of these functions at the input 0,

110
00:08:50,560 --> 00:08:55,300
you get a pattern which initially looks stable. It's 1 1 1 1 1 1 1,

111
00:08:55,300 --> 00:09:00,040
but by the time we get to the eighth iteration it falls short ever so slightly, just barely.

112
00:09:00,680 --> 00:09:05,380
This is analogous, and I claim more than just analogous, to the integrals we saw earlier,

113
00:09:05,400 --> 00:09:11,900
where we have a stable value at pi pi pi pi pi until it falls short just barely. And as it happens, this

114
00:09:11,900 --> 00:09:16,460
constant from our moving average process that's ever so slightly smaller than 1 is

115
00:09:16,460 --> 00:09:22,880
exactly the factor that sits in front of pi in our series of integrals. So the two situations aren't just qualitatively similar,

116
00:09:22,880 --> 00:09:24,840
they're quantitatively the same as well.

117
00:09:25,540 --> 00:09:29,720
And when it comes to the case where we add the 2 cosine of x term inside the integral,

118
00:09:29,940 --> 00:09:32,780
which caused the pattern to last a lot longer before it broke down,

119
00:09:33,040 --> 00:09:36,300
in the analogy what that will correspond to is the same setup,

120
00:09:36,300 --> 00:09:42,900
but where the function we start with has an even longer plateau, stretching from x equals negative 1 up to 1, meaning its length is 2.

121
00:09:42,900 --> 00:09:47,880
So as you do this repeated moving average process, eating into it with these smaller and smaller windows,

122
00:09:48,300 --> 00:09:50,980
it takes a lot longer for them to eat into the whole plateau.

123
00:09:51,700 --> 00:09:57,320
More specifically, the relevant computation is to ask how long do you have to add these reciprocals of odd numbers

124
00:09:57,320 --> 00:10:03,000
until that sum becomes bigger than 2? And it turns out that you have to go until you hit the number 113,

125
00:10:03,420 --> 00:10:08,340
which will correspond to the fact that the integral pattern there continues until you hit 113.

126
00:10:09,100 --> 00:10:15,680
And by the way, I should emphasize that there is nothing special about these reciprocals of odd numbers, 1 3rd, 1 5th, 1 7th.

127
00:10:15,680 --> 00:10:21,920
That just happens to be the sequence of values highlighted by the Borweins in their paper that made the sequence mildly famous in nerd circles.

128
00:10:22,440 --> 00:10:26,880
More generally, we could be inserting any sequence of positive numbers into those sinc functions,

129
00:10:26,880 --> 00:10:31,320
and as long as the sum of those numbers is less than 1, our expression will equal pi.

130
00:10:31,700 --> 00:10:35,180
But as soon as they become bigger than 1, our expression drops a little below pi.

131
00:10:35,180 --> 00:10:40,020
And if you believe me that there's an analogy with these moving averages, you can hopefully see why.

132
00:10:41,840 --> 00:10:47,800
But of course, the burning question is why on earth should these two situations have anything to do with each other?

133
00:10:48,240 --> 00:10:55,240
From here, the argument does bring in two mildly heavy bits of machinery, namely Fourier transforms and convolutions.

134
00:10:55,860 --> 00:11:01,780
And the way I'd like to go about this is to spend the remainder of this video giving you a high-level sense of how the argument will go,

135
00:11:01,780 --> 00:11:05,240
without necessarily assuming you're familiar with either of those two topics,

136
00:11:05,300 --> 00:11:09,640
and then to explain why the details are true in a video that's dedicated to convolutions,

137
00:11:10,200 --> 00:11:18,120
in particular something called the convolution theorem, since it's incredibly beautiful and it's useful well beyond this specific, very esoteric question.

138
00:11:21,080 --> 00:11:24,840
To start, instead of focusing on this function sine of x divided by x,

139
00:11:25,000 --> 00:11:28,760
where we want to show why the signed area underneath its curve is equal to pi,

140
00:11:28,760 --> 00:11:33,600
we'll make a simple substitution where we replace the input x with pi times x,

141
00:11:33,620 --> 00:11:37,220
which has the effect of squishing the graph horizontally by a factor of pi,

142
00:11:37,580 --> 00:11:40,300
and so the area gets scaled down by a factor of pi,

143
00:11:40,660 --> 00:11:44,920
meaning our new goal is to show why this integral on the right is equal to exactly 1.

144
00:11:45,500 --> 00:11:51,520
By the way, in some engineering contexts, people use the name sinc to refer to this function with the pi on the inside,

145
00:11:51,860 --> 00:11:56,160
since it's often very nice to have a normalized function, meaning the area under it is equal to 1.

146
00:11:56,160 --> 00:12:00,520
The point is, showing this integral on the right is exactly the same thing as showing the integral on the left,

147
00:12:00,660 --> 00:12:01,900
it's just a change of variables.

148
00:12:02,580 --> 00:12:07,560
And likewise for all of the other ones in our sequence, go through each of them, replace the x with a pi times x,

149
00:12:08,680 --> 00:12:13,640
and from here the claim is that all these integrals are not just analogous to the moving average examples,

150
00:12:13,800 --> 00:12:17,900
but that both of these are two distinct ways of computing exactly the same thing.

151
00:12:18,500 --> 00:12:23,760
And the connection comes down to the fact that this sinc function, or the engineer sinc function with the pi on the inside,

152
00:12:23,760 --> 00:12:27,620
is related to the rect function using what's known as a Fourier transform.

153
00:12:28,260 --> 00:12:32,560
Now, if you've never heard of a Fourier transform, there are a few other videos on this channel all about it.

154
00:12:32,740 --> 00:12:38,140
The way it's often described is that if you want to break down a function as the sum of a bunch of pure frequencies,

155
00:12:38,580 --> 00:12:42,320
or in the case of an infinite function, a continuous integral of a bunch of pure frequencies,

156
00:12:42,820 --> 00:12:46,740
the Fourier transform will tell you all the strength and phases for all those constituent parts.

157
00:12:47,120 --> 00:12:52,920
But all you really need to know here is that it is something which takes in one function and spits out a new function,

158
00:12:52,920 --> 00:12:58,580
and you often think of it as kind of rephrasing the information of your original function in a different language,

159
00:12:58,940 --> 00:13:00,520
like you're looking at it from a new perspective.

160
00:13:01,320 --> 00:13:06,180
For example, like I said, this sinc function written in this new language where you take a Fourier transform

161
00:13:06,180 --> 00:13:10,200
looks like our top hat rect function. And vice versa, by the way.

162
00:13:10,260 --> 00:13:14,200
This is a nice thing about Fourier transforms for functions that are symmetric about the y-axis,

163
00:13:14,360 --> 00:13:18,680
it is its own inverse, and actually the slightly more general fact that we'll need to show

164
00:13:18,680 --> 00:13:24,300
is how when you transform the stretched out version of our sinc function, where you stretch it horizontally by a factor of k,

165
00:13:24,900 --> 00:13:28,040
what you get is a stretched and squished version of this rect function.

166
00:13:28,600 --> 00:13:31,420
But of course, all of these are just meaningless words and terminology,

167
00:13:31,460 --> 00:13:34,500
unless you can actually do something upon making this translation.

168
00:13:35,100 --> 00:13:39,180
And the real idea behind why Fourier transforms are such a useful thing for math

169
00:13:39,180 --> 00:13:42,520
is that when you take statements and questions about a particular function,

170
00:13:43,160 --> 00:13:47,800
and then you look at what they correspond to with respect to the transformed version of that function,

171
00:13:47,800 --> 00:13:51,700
those statements and questions often look very very different in this new language,

172
00:13:51,700 --> 00:13:54,980
and sometimes it makes the questions a lot easier to answer.

173
00:13:55,660 --> 00:13:59,180
For example, one very nice little fact, another thing on our list of things to show,

174
00:13:59,580 --> 00:14:03,740
is that if you want to compute the integral of some function from negative infinity to infinity,

175
00:14:04,160 --> 00:14:11,860
this signed area under the entirety of its curve, it's the same thing as simply evaluating the Fourier transformed version of that function

176
00:14:11,860 --> 00:14:13,360
at the input zero.

177
00:14:13,820 --> 00:14:16,860
This is a fact that will actually just pop right out of the definition,

178
00:14:16,860 --> 00:14:23,740
and it's representative of a more general vibe that every individual output of the Fourier transform function on the right

179
00:14:23,740 --> 00:14:28,240
corresponds to some kind of global information about the original function on the left.

180
00:14:28,720 --> 00:14:29,720
In our specific case,

181
00:14:29,780 --> 00:14:35,640
it means if you believe me that this sync function and the rect function are related with a Fourier transform like this,

182
00:14:35,700 --> 00:14:41,940
it explains the integral, which is otherwise a very tricky thing to compute, because it's saying all that signed area is the same thing as

183
00:14:41,940 --> 00:14:45,040
evaluating rect at zero, which is just one.

184
00:14:46,140 --> 00:14:51,160
Now, you could complain, surely this just moves the bump under the rug. Surely computing this Fourier transform,

185
00:14:51,260 --> 00:14:54,680
whatever that looks like, would be as hard as computing the original integral.

186
00:14:55,040 --> 00:14:58,640
But the idea is that there's lots of tips and tricks for computing these Fourier transforms,

187
00:14:59,300 --> 00:15:03,720
and moreover, that when you do, it tells you a lot more information than just that integral.

188
00:15:03,880 --> 00:15:06,380
You get a lot of bang for your buck out of doing the computation.

189
00:15:07,200 --> 00:15:10,200
Now, the other key fact that will explain the connection we're hunting for

190
00:15:10,200 --> 00:15:13,660
is that if you have two different functions and you take their product,

191
00:15:13,660 --> 00:15:17,000
and then you take the Fourier transform of that product,

192
00:15:17,140 --> 00:15:21,860
it will be the same thing as if you individually took the Fourier transforms of your original function,

193
00:15:22,340 --> 00:15:27,820
and then combined them using a new kind of operation that we'll talk all about in the next video, known as a convolution.

194
00:15:28,500 --> 00:15:30,660
Now, even though there's a lot to be explained with convolutions,

195
00:15:31,100 --> 00:15:35,180
the upshot will be that in our specific case with these rectangular functions,

196
00:15:35,640 --> 00:15:41,100
taking a convolution looks just like one of the moving averages that we've been talking about this whole time,

197
00:15:41,100 --> 00:15:47,200
combined with our previous fact that integrating in one context looks like evaluating at zero in another context,

198
00:15:47,460 --> 00:15:52,580
if you believe me that multiplying in one context corresponds to this new operation,

199
00:15:52,780 --> 00:15:55,880
convolutions, which for our example you should just think of as moving averages,

200
00:15:56,200 --> 00:15:59,960
that will explain why multiplying more and more of these sinc functions together

201
00:15:59,960 --> 00:16:04,680
can be thought about in terms of these progressive moving averages and always evaluating at zero,

202
00:16:04,980 --> 00:16:11,080
which in turn gives a really lovely intuition for why you would expect such a stable value before eventually something breaks down

203
00:16:11,100 --> 00:16:14,080
as the edges of the plateau inch closer and closer to the center.

204
00:16:15,540 --> 00:16:19,100
This last key fact, by the way, has a special name. It's called the convolution theorem,

205
00:16:19,460 --> 00:16:21,800
and again, it's something that we'll go into much more deeply.

206
00:16:22,960 --> 00:16:30,320
I recognize that it's maybe a little unsatisfying to end things here by laying down three magical facts and saying everything follows from those,

207
00:16:30,520 --> 00:16:37,120
but hopefully this gives you a little glimpse of why powerful tools like Fourier transforms can be so useful for tricky problems.

208
00:16:37,600 --> 00:16:43,580
It's a systematic way to provide a shift in perspective where hard problems can sometimes look easier.

209
00:16:44,040 --> 00:16:48,780
If nothing else, it hopefully provides some motivation to learn about these beautiful things like the convolution theorem.

210
00:16:49,420 --> 00:16:53,520
As one more tiny teaser, another fun consequence of this convolution theorem

211
00:16:53,520 --> 00:16:58,160
will be that it opens the doors for an algorithm that lets you compute the product of two large numbers

212
00:16:58,160 --> 00:17:01,960
very quickly, like way faster than you think should be even possible.

213
00:17:03,000 --> 00:17:04,600
So with that, I'll see you in the next video.

214
00:17:07,120 --> 00:17:20,720
So

