1
00:00:00,000 --> 00:00:02,980
Sometimes it feels like the universe is just messing with you.

2
00:00:03,340 --> 00:00:06,715
I have up on screen here a sequence of computations, and don't worry, 

3
00:00:06,715 --> 00:00:10,380
in a moment we're gonna unpack and visualize what each one is really saying.

4
00:00:10,920 --> 00:00:14,755
What I want you to notice is how the sequence follows a very predictable, 

5
00:00:14,755 --> 00:00:18,540
if random, seeming pattern, and how each computation happens to equal pi.

6
00:00:19,080 --> 00:00:22,913
And if you were just messing around evaluating these on a computer for some reason, 

7
00:00:22,913 --> 00:00:25,880
you might think that this was a pattern that would go on forever.

8
00:00:25,880 --> 00:00:27,100
But it doesn't.

9
00:00:27,400 --> 00:00:30,680
At some point it stops, and instead of equaling pi, 

10
00:00:30,680 --> 00:00:34,340
you get a value which is just barely, barely less than pi.

11
00:00:38,780 --> 00:00:40,940
All right, let's dig into what's going on here.

12
00:00:41,300 --> 00:00:45,080
The main character in the story today is the function sine of x divided by x.

13
00:00:45,460 --> 00:00:49,726
This actually comes up commonly enough in math and engineering that it gets its own name, 

14
00:00:49,726 --> 00:00:53,139
sinc, and the way you might think about it is by starting with a normal 

15
00:00:53,139 --> 00:00:56,457
oscillating sine curve, and then sort of squishing it down as you get 

16
00:00:56,457 --> 00:00:58,780
far away from zero by multiplying it by 1 over x.

17
00:00:59,300 --> 00:01:02,798
And the astute among you might ask about what happens at x equals 0, 

18
00:01:02,798 --> 00:01:05,740
since when you plug that in it looks like dividing 0 by 0.

19
00:01:06,400 --> 00:01:10,135
And then the even more astute among you, maybe fresh out of a calculus class, 

20
00:01:10,135 --> 00:01:12,816
could point out that for values closer and closer to 0, 

21
00:01:12,816 --> 00:01:14,780
the function gets closer and closer to 1.

22
00:01:15,260 --> 00:01:18,560
So if we simply redefine the sinc function at 0 to equal 1, 

23
00:01:18,560 --> 00:01:20,320
you get a nice continuous curve.

24
00:01:20,320 --> 00:01:24,550
All of that is a little by the by because the thing we actually care about is the 

25
00:01:24,550 --> 00:01:27,595
integral of this curve from negative infinity to infinity, 

26
00:01:27,595 --> 00:01:31,464
which you'd think of as meaning the area between the curve and the x-axis, 

27
00:01:31,464 --> 00:01:35,902
or more precisely the signed area, meaning you add all the area bound by the positive 

28
00:01:35,902 --> 00:01:40,081
parts of the graph in the x-axis, and you subtract all of the parts bound by the 

29
00:01:40,081 --> 00:01:42,300
negative parts of the graph and the x-axis.

30
00:01:42,740 --> 00:01:47,132
Like we saw at the start, it happens to be the case that this evaluates to be exactly pi, 

31
00:01:47,132 --> 00:01:50,597
which is nice and also a little weird, and it's not entirely clear how 

32
00:01:50,597 --> 00:01:53,380
you would approach this with the usual tools of calculus.

33
00:01:53,980 --> 00:01:56,560
Towards the end of the video, I'll share the trick for how you would do this.

34
00:01:56,840 --> 00:01:59,212
Progressing on with the sequence I opened with, 

35
00:01:59,212 --> 00:02:03,563
the next step is to take a copy of the sinc function, where you plug in x divided by 3, 

36
00:02:03,563 --> 00:02:05,887
which will basically look like the same graph, 

37
00:02:05,887 --> 00:02:08,259
but stretched out horizontally by a factor of 3.

38
00:02:08,900 --> 00:02:11,239
When we multiply these two functions together, 

39
00:02:11,239 --> 00:02:15,072
we get a much more complicated wave whose mass seems to be more concentrated 

40
00:02:15,072 --> 00:02:18,606
towards the middle, and with any usual functions you would expect this 

41
00:02:18,606 --> 00:02:20,000
completely changes the area.

42
00:02:20,380 --> 00:02:23,680
You can't just randomly modify an integral like this and expect nothing to change.

43
00:02:24,260 --> 00:02:27,564
So already it's a little bit weird that this result also equals pi, 

44
00:02:27,564 --> 00:02:28,780
that nothing has changed.

45
00:02:29,080 --> 00:02:31,180
That's another mystery you should add to your list.

46
00:02:31,660 --> 00:02:35,773
And the next step in the sequence was to take an even more stretched out version 

47
00:02:35,773 --> 00:02:39,734
of the sinc function by a factor of 5, multiply that by what we already have, 

48
00:02:39,734 --> 00:02:44,000
and again look at the signed area underneath the whole curve, which again equals pi.

49
00:02:44,860 --> 00:02:46,480
And it continues on like this.

50
00:02:46,580 --> 00:02:48,902
With each iteration, we stretch out by a new odd 

51
00:02:48,902 --> 00:02:50,940
number and multiply that into what we have.

52
00:02:51,640 --> 00:02:54,740
One thing you might notice is how except at the input x equals 0, 

53
00:02:54,740 --> 00:02:58,686
every single part of this function is progressively getting multiplied by something 

54
00:02:58,686 --> 00:02:59,720
that's smaller than 1.

55
00:03:00,340 --> 00:03:02,514
So you would expect, as the sequence progresses, 

56
00:03:02,514 --> 00:03:04,600
for things to get squished down more and more, 

57
00:03:04,600 --> 00:03:07,440
and if anything you would expect the area to be getting smaller.

58
00:03:08,360 --> 00:03:12,478
Eventually that is exactly what happens, but what's bizarre is that it 

59
00:03:12,478 --> 00:03:16,017
stays so stable for so long, and of course more pertinently, 

60
00:03:16,017 --> 00:03:20,600
that when it does break at the value 15, it does so by the tiniest tiny amount.

61
00:03:21,180 --> 00:03:24,559
And before you go thinking this is the result of some numerical error, 

62
00:03:24,559 --> 00:03:27,796
maybe because we're doing something with floating-point arithmetic, 

63
00:03:27,796 --> 00:03:31,794
if you work this out more precisely, here is the exact value of that last integral, 

64
00:03:31,794 --> 00:03:35,840
which is a certain fraction of pi where the numerator and the denominator are absurd.

65
00:03:35,980 --> 00:03:38,700
They're both around 400 billion billion billion.

66
00:03:40,460 --> 00:03:43,518
So this pattern was described in a paper by a father-son pair, 

67
00:03:43,518 --> 00:03:45,800
Jonathan and David Borwein, which is very fun, 

68
00:03:45,800 --> 00:03:49,975
and they mentioned how when a fellow researcher was computing these integrals using a 

69
00:03:49,975 --> 00:03:53,520
computer algebra system, he assumed that this had to be some kind of bug.

70
00:03:53,860 --> 00:03:56,260
But it's not a bug, it is a real phenomenon.

71
00:03:56,680 --> 00:03:58,120
And it gets weirder than that actually.

72
00:03:58,440 --> 00:04:02,537
If we take all these integrals and include yet another factor, 2 cosine of x, 

73
00:04:02,537 --> 00:04:05,636
which again you would think changes their values entirely, 

74
00:04:05,636 --> 00:04:09,419
you can't just randomly multiply new things into an integral like this, 

75
00:04:09,419 --> 00:04:11,888
it continues to equal pi for much much longer, 

76
00:04:11,888 --> 00:04:15,040
and it's not until you get to the number 113 that it breaks.

77
00:04:15,200 --> 00:04:17,858
And when it breaks, it's by the most puny, absolutely 

78
00:04:17,858 --> 00:04:19,680
subtle amount that you could imagine.

79
00:04:20,440 --> 00:04:24,080
So the natural question is what on earth is going on here?

80
00:04:24,380 --> 00:04:27,680
And luckily there actually is a really satisfying explanation for all this.

81
00:04:28,180 --> 00:04:32,070
The way I think I'll go about this is to show you a phenomenon that first looks 

82
00:04:32,070 --> 00:04:36,300
completely unrelated, but it shows a similar pattern where you have a value that stays 

83
00:04:36,300 --> 00:04:40,580
really stable until you get to the number 15, and then it falters by just a tiny amount.

84
00:04:41,300 --> 00:04:44,820
And then after that I'll show why this seemingly unrelated phenomenon 

85
00:04:44,820 --> 00:04:48,340
is secretly the same as all our integral expressions, but in disguise.

86
00:04:49,120 --> 00:04:52,556
So, turning our attention to what seems completely different, 

87
00:04:52,556 --> 00:04:55,882
consider a function that I'm going to be calling rect of x, 

88
00:04:55,882 --> 00:05:00,317
which is defined to equal 1 if the input is between negative 1 half and 1 half, 

89
00:05:00,317 --> 00:05:01,980
and otherwise it's equal to 0.

90
00:05:02,220 --> 00:05:04,520
So the function is this boring step, basically.

91
00:05:04,520 --> 00:05:07,962
This will be the first in a sequence of functions that we define, 

92
00:05:07,962 --> 00:05:11,249
so I'll call it f1 of x, and each new function in our sequence 

93
00:05:11,249 --> 00:05:14,640
is going to be a kind of moving average of the previous function.

94
00:05:15,800 --> 00:05:20,100
So for example, the way the second iteration will be defined is to take this 

95
00:05:20,100 --> 00:05:23,954
sliding window whose width is 1 third, and for a particular input x, 

96
00:05:23,954 --> 00:05:28,422
when the window is centered at that input x, the value in my new function drawn 

97
00:05:28,422 --> 00:05:32,778
below is defined to be equal to the average value of the first function above 

98
00:05:32,778 --> 00:05:33,840
inside that window.

99
00:05:33,840 --> 00:05:36,618
So for example, when the window is far enough to the left, 

100
00:05:36,618 --> 00:05:39,820
every value inside it is 0, so the graph on the bottom is showing 0.

101
00:05:40,280 --> 00:05:43,299
As soon as that window starts to go over the plateau a little bit, 

102
00:05:43,299 --> 00:05:46,860
the average value is a little more than 0, and you see that in the graph below.

103
00:05:47,280 --> 00:05:51,690
And notice that when exactly half the window is over that plateau at 1 and half of it 

104
00:05:51,690 --> 00:05:56,100
is at 0, the corresponding value in the bottom graph is 1 half, and you get the point.

105
00:05:56,660 --> 00:06:00,237
The important thing I want you to focus on is how when that window is 

106
00:06:00,237 --> 00:06:03,253
entirely in the plateau above, where all the values are 1, 

107
00:06:03,253 --> 00:06:07,700
then the average value is also 1, so we get this plateau on our function at the bottom.

108
00:06:08,300 --> 00:06:11,746
Let's call this bottom function f2 of x, and what I want you to 

109
00:06:11,746 --> 00:06:15,300
think about is the length of the plateau for that second function.

110
00:06:15,480 --> 00:06:16,440
How wide should it be?

111
00:06:17,020 --> 00:06:20,433
If you think about it for a moment, the distance between the left 

112
00:06:20,433 --> 00:06:23,743
edge of the top plateau and the left edge of the bottom plateau 

113
00:06:23,743 --> 00:06:27,260
will be exactly half of the width of the window, so half of 1 third.

114
00:06:27,640 --> 00:06:30,230
And similarly on the right side, the distance between 

115
00:06:30,230 --> 00:06:32,820
the edges of the plateaus is half of the window width.

116
00:06:33,200 --> 00:06:36,660
So overall it's 1 minus that window width, which is 1 minus 1 third.

117
00:06:37,380 --> 00:06:41,103
The value we're going to be computing, the thing that will look stable for 

118
00:06:41,103 --> 00:06:44,678
a while before it breaks, is the value of this function at the input 0, 

119
00:06:44,678 --> 00:06:48,700
which in both of these iterations is equal to 1 because it's inside that plateau.

120
00:06:49,200 --> 00:06:52,952
For the next iteration, we're going to take a moving average of that last function, 

121
00:06:52,952 --> 00:06:55,320
but this time with the window whose width is 1 fifth.

122
00:06:55,320 --> 00:06:59,241
It's kind of fun to think about why as you slide around this window you get a 

123
00:06:59,241 --> 00:07:02,158
smoothed out version of the previous function, and again, 

124
00:07:02,158 --> 00:07:06,230
the significant thing I want you to focus on is how when that window is entirely 

125
00:07:06,230 --> 00:07:10,454
inside the plateau of the previous function, then by definition the bottom function 

126
00:07:10,454 --> 00:07:11,460
is going to equal 1.

127
00:07:11,980 --> 00:07:15,610
This time the length of that plateau on the bottom will be the length 

128
00:07:15,610 --> 00:07:19,240
of the previous one, 1 minus 1 third, minus the window width, 1 fifth.

129
00:07:19,600 --> 00:07:23,991
The reasoning is the same as before in order to go from the point where the middle of 

130
00:07:23,991 --> 00:07:28,332
the window is on that top plateau to where the entirety of the window is inside that 

131
00:07:28,332 --> 00:07:31,651
plateau is half the window width and likewise on the right side, 

132
00:07:31,651 --> 00:07:36,043
and once more the value to record is the output of this function when the input is 0, 

133
00:07:36,043 --> 00:07:37,320
which again is exactly 1.

134
00:07:38,580 --> 00:07:41,880
The next iteration is a moving average with a window width of 1 seventh.

135
00:07:42,100 --> 00:07:44,040
The plateau gets smaller by that 1 over 7.

136
00:07:44,500 --> 00:07:48,060
Doing one more iteration with 1 over 9, the plateau gets smaller by that amount.

137
00:07:48,600 --> 00:07:50,780
And as we keep going the plateau gets thinner and thinner.

138
00:07:51,820 --> 00:07:55,539
And also notice how just outside of the plateau the function is really really 

139
00:07:55,539 --> 00:07:59,020
close to 1 because it's always been the result of an average between the 

140
00:07:59,020 --> 00:08:02,740
plateau at 1 and the neighbors, which themselves are really really close to 1.

141
00:08:03,440 --> 00:08:06,900
The point at which all of this breaks is once we get to the iteration 

142
00:08:06,900 --> 00:08:10,360
where we're sliding a window with width 1 15th across the whole thing.

143
00:08:10,760 --> 00:08:14,660
At that point the previous plateau is actually thinner than the window itself.

144
00:08:14,820 --> 00:08:17,842
So even at the input x equals 0, this moving average 

145
00:08:17,842 --> 00:08:20,580
will have to be ever so slightly smaller than 1.

146
00:08:20,780 --> 00:08:24,839
And the only thing that's special about the number 15 here is that as we keep 

147
00:08:24,839 --> 00:08:27,286
adding the reciprocals of these odd fractions, 

148
00:08:27,286 --> 00:08:29,732
1 third plus 1 fifth plus 1 seventh on and on, 

149
00:08:29,732 --> 00:08:33,220
it's once we get to 1 15th that that sum grows to be bigger than 1.

150
00:08:33,580 --> 00:08:38,105
And in the context of our shrinking plateaus, having started with a plateau of width 1, 

151
00:08:38,105 --> 00:08:41,140
it's now shrunk down so much that it'll disappear entirely.

152
00:08:41,799 --> 00:08:45,454
The point is with this as a sequence of functions that we've defined by a 

153
00:08:45,454 --> 00:08:49,058
seemingly random procedure, if I ask you to compute the values of all of 

154
00:08:49,058 --> 00:08:52,960
these functions at the input 0, you get a pattern which initially looks stable.

155
00:08:53,120 --> 00:08:56,641
It's 1 1 1 1 1 1 1, but by the time we get to the eighth 

156
00:08:56,641 --> 00:09:00,040
iteration it falls short ever so slightly, just barely.

157
00:09:00,680 --> 00:09:05,476
This is analogous, and I claim more than just analogous, to the integrals we saw earlier, 

158
00:09:05,476 --> 00:09:09,740
where we have a stable value at pi pi pi pi pi until it falls short just barely.

159
00:09:10,180 --> 00:09:15,069
And as it happens, this constant from our moving average process that's ever so slightly 

160
00:09:15,069 --> 00:09:19,960
smaller than 1 is exactly the factor that sits in front of pi in our series of integrals.

161
00:09:20,340 --> 00:09:22,984
So the two situations aren't just qualitatively similar, 

162
00:09:22,984 --> 00:09:24,840
they're quantitatively the same as well.

163
00:09:25,540 --> 00:09:29,830
And when it comes to the case where we add the 2 cosine of x term inside the integral, 

164
00:09:29,830 --> 00:09:33,184
which caused the pattern to last a lot longer before it broke down, 

165
00:09:33,184 --> 00:09:36,291
in the analogy what that will correspond to is the same setup, 

166
00:09:36,291 --> 00:09:39,497
but where the function we start with has an even longer plateau, 

167
00:09:39,497 --> 00:09:42,900
stretching from x equals negative 1 up to 1, meaning its length is 2.

168
00:09:42,900 --> 00:09:45,367
So as you do this repeated moving average process, 

169
00:09:45,367 --> 00:09:48,028
eating into it with these smaller and smaller windows, 

170
00:09:48,028 --> 00:09:50,980
it takes a lot longer for them to eat into the whole plateau.

171
00:09:51,700 --> 00:09:55,449
More specifically, the relevant computation is to ask how long do you have 

172
00:09:55,449 --> 00:09:59,300
to add these reciprocals of odd numbers until that sum becomes bigger than 2?

173
00:09:59,720 --> 00:10:03,307
And it turns out that you have to go until you hit the number 113, 

174
00:10:03,307 --> 00:10:07,697
which will correspond to the fact that the integral pattern there continues until 

175
00:10:07,697 --> 00:10:08,340
you hit 113.

176
00:10:09,100 --> 00:10:12,521
And by the way, I should emphasize that there is nothing special 

177
00:10:12,521 --> 00:10:15,680
about these reciprocals of odd numbers, 1 3rd, 1 5th, 1 7th.

178
00:10:15,680 --> 00:10:18,952
That just happens to be the sequence of values highlighted by the Borweins 

179
00:10:18,952 --> 00:10:21,920
in their paper that made the sequence mildly famous in nerd circles.

180
00:10:22,440 --> 00:10:25,497
More generally, we could be inserting any sequence of positive 

181
00:10:25,497 --> 00:10:28,457
numbers into those sinc functions, and as long as the sum of 

182
00:10:28,457 --> 00:10:31,320
those numbers is less than 1, our expression will equal pi.

183
00:10:31,700 --> 00:10:35,180
But as soon as they become bigger than 1, our expression drops a little below pi.

184
00:10:35,180 --> 00:10:38,761
And if you believe me that there's an analogy with these moving averages, 

185
00:10:38,761 --> 00:10:40,020
you can hopefully see why.

186
00:10:41,840 --> 00:10:44,871
But of course, the burning question is why on earth should 

187
00:10:44,871 --> 00:10:47,800
these two situations have anything to do with each other?

188
00:10:48,240 --> 00:10:52,667
From here, the argument does bring in two mildly heavy bits of machinery, 

189
00:10:52,667 --> 00:10:55,240
namely Fourier transforms and convolutions.

190
00:10:55,860 --> 00:10:59,642
And the way I'd like to go about this is to spend the remainder of this video 

191
00:10:59,642 --> 00:11:02,504
giving you a high-level sense of how the argument will go, 

192
00:11:02,504 --> 00:11:06,286
without necessarily assuming you're familiar with either of those two topics, 

193
00:11:06,286 --> 00:11:10,651
and then to explain why the details are true in a video that's dedicated to convolutions, 

194
00:11:10,651 --> 00:11:13,367
in particular something called the convolution theorem, 

195
00:11:13,367 --> 00:11:17,004
since it's incredibly beautiful and it's useful well beyond this specific, 

196
00:11:17,004 --> 00:11:18,120
very esoteric question.

197
00:11:21,080 --> 00:11:24,909
To start, instead of focusing on this function sine of x divided by x, 

198
00:11:24,909 --> 00:11:29,170
where we want to show why the signed area underneath its curve is equal to pi, 

199
00:11:29,170 --> 00:11:33,431
we'll make a simple substitution where we replace the input x with pi times x, 

200
00:11:33,431 --> 00:11:37,530
which has the effect of squishing the graph horizontally by a factor of pi, 

201
00:11:37,530 --> 00:11:40,335
and so the area gets scaled down by a factor of pi, 

202
00:11:40,335 --> 00:11:44,920
meaning our new goal is to show why this integral on the right is equal to exactly 1.

203
00:11:45,500 --> 00:11:48,922
By the way, in some engineering contexts, people use the name sinc to 

204
00:11:48,922 --> 00:11:51,367
refer to this function with the pi on the inside, 

205
00:11:51,367 --> 00:11:54,204
since it's often very nice to have a normalized function, 

206
00:11:54,204 --> 00:11:56,160
meaning the area under it is equal to 1.

207
00:11:56,160 --> 00:11:58,910
The point is, showing this integral on the right is exactly the same 

208
00:11:58,910 --> 00:12:01,900
thing as showing the integral on the left, it's just a change of variables.

209
00:12:02,580 --> 00:12:06,689
And likewise for all of the other ones in our sequence, go through each of them, 

210
00:12:06,689 --> 00:12:10,493
replace the x with a pi times x, and from here the claim is that all these 

211
00:12:10,493 --> 00:12:13,790
integrals are not just analogous to the moving average examples, 

212
00:12:13,790 --> 00:12:17,900
but that both of these are two distinct ways of computing exactly the same thing.

213
00:12:18,500 --> 00:12:21,586
And the connection comes down to the fact that this sinc function, 

214
00:12:21,586 --> 00:12:24,211
or the engineer sinc function with the pi on the inside, 

215
00:12:24,211 --> 00:12:27,620
is related to the rect function using what's known as a Fourier transform.

216
00:12:28,260 --> 00:12:30,271
Now, if you've never heard of a Fourier transform, 

217
00:12:30,271 --> 00:12:32,560
there are a few other videos on this channel all about it.

218
00:12:32,740 --> 00:12:36,251
The way it's often described is that if you want to break down a function as 

219
00:12:36,251 --> 00:12:39,899
the sum of a bunch of pure frequencies, or in the case of an infinite function, 

220
00:12:39,899 --> 00:12:42,362
a continuous integral of a bunch of pure frequencies, 

221
00:12:42,362 --> 00:12:45,919
the Fourier transform will tell you all the strength and phases for all those 

222
00:12:45,919 --> 00:12:46,740
constituent parts.

223
00:12:47,120 --> 00:12:50,292
But all you really need to know here is that it is something which 

224
00:12:50,292 --> 00:12:52,754
takes in one function and spits out a new function, 

225
00:12:52,754 --> 00:12:56,163
and you often think of it as kind of rephrasing the information of your 

226
00:12:56,163 --> 00:12:59,430
original function in a different language, like you're looking at it 

227
00:12:59,430 --> 00:13:00,520
from a new perspective.

228
00:13:01,320 --> 00:13:04,969
For example, like I said, this sinc function written in this new language 

229
00:13:04,969 --> 00:13:08,520
where you take a Fourier transform looks like our top hat rect function.

230
00:13:09,100 --> 00:13:10,200
And vice versa, by the way.

231
00:13:10,260 --> 00:13:13,908
This is a nice thing about Fourier transforms for functions that are symmetric 

232
00:13:13,908 --> 00:13:17,602
about the y-axis, it is its own inverse, and actually the slightly more general 

233
00:13:17,602 --> 00:13:20,974
fact that we'll need to show is how when you transform the stretched out 

234
00:13:20,974 --> 00:13:24,761
version of our sinc function, where you stretch it horizontally by a factor of k, 

235
00:13:24,761 --> 00:13:28,040
what you get is a stretched and squished version of this rect function.

236
00:13:28,600 --> 00:13:31,678
But of course, all of these are just meaningless words and terminology, 

237
00:13:31,678 --> 00:13:34,500
unless you can actually do something upon making this translation.

238
00:13:35,100 --> 00:13:39,085
And the real idea behind why Fourier transforms are such a useful thing for math 

239
00:13:39,085 --> 00:13:42,825
is that when you take statements and questions about a particular function, 

240
00:13:42,825 --> 00:13:46,614
and then you look at what they correspond to with respect to the transformed 

241
00:13:46,614 --> 00:13:50,452
version of that function, those statements and questions often look very very 

242
00:13:50,452 --> 00:13:54,487
different in this new language, and sometimes it makes the questions a lot easier 

243
00:13:54,487 --> 00:13:54,980
to answer.

244
00:13:55,660 --> 00:14:00,085
For example, one very nice little fact, another thing on our list of things to show, 

245
00:14:00,085 --> 00:14:04,457
is that if you want to compute the integral of some function from negative infinity 

246
00:14:04,457 --> 00:14:07,737
to infinity, this signed area under the entirety of its curve, 

247
00:14:07,737 --> 00:14:11,954
it's the same thing as simply evaluating the Fourier transformed version of that 

248
00:14:11,954 --> 00:14:13,360
function at the input zero.

249
00:14:13,820 --> 00:14:17,501
This is a fact that will actually just pop right out of the definition, 

250
00:14:17,501 --> 00:14:21,030
and it's representative of a more general vibe that every individual 

251
00:14:21,030 --> 00:14:24,558
output of the Fourier transform function on the right corresponds to 

252
00:14:24,558 --> 00:14:28,240
some kind of global information about the original function on the left.

253
00:14:28,720 --> 00:14:32,787
In our specific case, it means if you believe me that this sync function and the 

254
00:14:32,787 --> 00:14:37,206
rect function are related with a Fourier transform like this, it explains the integral, 

255
00:14:37,206 --> 00:14:39,767
which is otherwise a very tricky thing to compute, 

256
00:14:39,767 --> 00:14:44,136
because it's saying all that signed area is the same thing as evaluating rect at zero, 

257
00:14:44,136 --> 00:14:45,040
which is just one.

258
00:14:46,140 --> 00:14:49,340
Now, you could complain, surely this just moves the bump under the rug.

259
00:14:49,700 --> 00:14:52,503
Surely computing this Fourier transform, whatever that looks like, 

260
00:14:52,503 --> 00:14:54,680
would be as hard as computing the original integral.

261
00:14:55,040 --> 00:14:57,714
But the idea is that there's lots of tips and tricks for 

262
00:14:57,714 --> 00:15:00,904
computing these Fourier transforms, and moreover, that when you do, 

263
00:15:00,904 --> 00:15:03,720
it tells you a lot more information than just that integral.

264
00:15:03,880 --> 00:15:06,380
You get a lot of bang for your buck out of doing the computation.

265
00:15:07,200 --> 00:15:11,333
Now, the other key fact that will explain the connection we're hunting for is that if 

266
00:15:11,333 --> 00:15:14,265
you have two different functions and you take their product, 

267
00:15:14,265 --> 00:15:17,005
and then you take the Fourier transform of that product, 

268
00:15:17,005 --> 00:15:21,090
it will be the same thing as if you individually took the Fourier transforms of your 

269
00:15:21,090 --> 00:15:25,080
original function, and then combined them using a new kind of operation that we'll 

270
00:15:25,080 --> 00:15:27,820
talk all about in the next video, known as a convolution.

271
00:15:28,500 --> 00:15:31,802
Now, even though there's a lot to be explained with convolutions, 

272
00:15:31,802 --> 00:15:35,754
the upshot will be that in our specific case with these rectangular functions, 

273
00:15:35,754 --> 00:15:39,757
taking a convolution looks just like one of the moving averages that we've been 

274
00:15:39,757 --> 00:15:43,910
talking about this whole time, combined with our previous fact that integrating in 

275
00:15:43,910 --> 00:15:47,012
one context looks like evaluating at zero in another context, 

276
00:15:47,012 --> 00:15:51,264
if you believe me that multiplying in one context corresponds to this new operation, 

277
00:15:51,264 --> 00:15:55,317
convolutions, which for our example you should just think of as moving averages, 

278
00:15:55,317 --> 00:15:59,370
that will explain why multiplying more and more of these sinc functions together 

279
00:15:59,370 --> 00:16:03,272
can be thought about in terms of these progressive moving averages and always 

280
00:16:03,272 --> 00:16:07,475
evaluating at zero, which in turn gives a really lovely intuition for why you would 

281
00:16:07,475 --> 00:16:11,628
expect such a stable value before eventually something breaks down as the edges of 

282
00:16:11,628 --> 00:16:14,080
the plateau inch closer and closer to the center.

283
00:16:15,540 --> 00:16:17,800
This last key fact, by the way, has a special name.

284
00:16:17,900 --> 00:16:19,790
It's called the convolution theorem, and again, 

285
00:16:19,790 --> 00:16:21,800
it's something that we'll go into much more deeply.

286
00:16:22,960 --> 00:16:26,512
I recognize that it's maybe a little unsatisfying to end things here 

287
00:16:26,512 --> 00:16:30,477
by laying down three magical facts and saying everything follows from those, 

288
00:16:30,477 --> 00:16:33,979
but hopefully this gives you a little glimpse of why powerful tools 

289
00:16:33,979 --> 00:16:37,120
like Fourier transforms can be so useful for tricky problems.

290
00:16:37,600 --> 00:16:40,883
It's a systematic way to provide a shift in perspective 

291
00:16:40,883 --> 00:16:43,580
where hard problems can sometimes look easier.

292
00:16:44,040 --> 00:16:46,293
If nothing else, it hopefully provides some motivation to 

293
00:16:46,293 --> 00:16:48,780
learn about these beautiful things like the convolution theorem.

294
00:16:49,420 --> 00:16:53,533
As one more tiny teaser, another fun consequence of this convolution theorem will 

295
00:16:53,533 --> 00:16:57,596
be that it opens the doors for an algorithm that lets you compute the product of 

296
00:16:57,596 --> 00:17:01,960
two large numbers very quickly, like way faster than you think should be even possible.

297
00:17:03,000 --> 00:17:04,599
So with that, I'll see you in the next video.

