1
00:00:00,000 --> 00:00:02,840
Sometimes it feels like the universe is just messing with you.

2
00:00:02,840 --> 00:00:07,720
I have up on screen here a sequence of computations, and don't worry, in a moment

3
00:00:07,720 --> 00:00:10,240
we're gonna unpack and visualize what each one is really saying.

4
00:00:10,480 --> 00:00:16,960
What I want you to notice is how the sequence follows a very predictable, if random, seeming pattern, and how each computation

5
00:00:17,280 --> 00:00:22,880
happens to equal pi. And if you were just messing around evaluating these on a computer for some reason,

6
00:00:22,920 --> 00:00:25,640
you might think that this was a pattern that would go on forever.

7
00:00:25,960 --> 00:00:32,200
But it doesn't. At some point it stops, and instead of equaling pi, you get a value which is just barely,

8
00:00:32,680 --> 00:00:34,680
barely less than pi.

9
00:00:38,880 --> 00:00:40,880
All right, let's dig into what's going on here.

10
00:00:40,880 --> 00:00:44,680
The main character in the story today is the function sine of x divided by x.

11
00:00:45,120 --> 00:00:48,600
This actually comes up commonly enough in math and engineering that it gets its own name,

12
00:00:48,880 --> 00:00:52,280
sinc, and the way you might think about it is by starting with a normal

13
00:00:52,600 --> 00:00:58,920
oscillating sine curve, and then sort of squishing it down as you get far away from zero by multiplying it by 1 over x.

14
00:00:59,040 --> 00:01:05,360
And the astute among you might ask about what happens at x equals 0, since when you plug that in it looks like dividing 0 by 0.

15
00:01:05,360 --> 00:01:09,160
And then the even more astute among you, maybe fresh out of a calculus class,

16
00:01:09,480 --> 00:01:14,640
could point out that for values closer and closer to 0, the function gets closer and closer to 1.

17
00:01:14,880 --> 00:01:20,200
So if we simply redefine the sinc function at 0 to equal 1, you get a nice continuous curve.

18
00:01:20,520 --> 00:01:27,080
All of that is a little by the by because the thing we actually care about is the integral of this curve from negative infinity to

19
00:01:27,320 --> 00:01:33,800
infinity, which you think of as meaning the area between the curve and the x-axis, or more precisely the signed area,

20
00:01:34,000 --> 00:01:37,760
meaning you add all the area bound by the positive parts of the graph in the x-axis,

21
00:01:38,000 --> 00:01:42,000
and you subtract all of the parts bound by the negative parts of the graph and the x-axis.

22
00:01:42,320 --> 00:01:46,760
Like we saw at the start, it happens to be the case that this evaluates to be exactly pi,

23
00:01:47,000 --> 00:01:53,080
which is nice and also a little weird, and it's not entirely clear how you would approach this with the usual tools of calculus.

24
00:01:53,600 --> 00:01:56,320
Towards the end of the video, I'll share the trick for how you would do this.

25
00:01:56,720 --> 00:02:02,960
Progressing on with the sequence I opened with, the next step is to take a copy of the sinc function, where you plug in x divided by 3,

26
00:02:03,400 --> 00:02:08,200
which will basically look like the same graph, but stretched out horizontally by a factor of 3.

27
00:02:08,520 --> 00:02:10,920
When we multiply these two functions together,

28
00:02:11,120 --> 00:02:16,000
we get a much more complicated wave whose mass seems to be more concentrated towards the middle,

29
00:02:16,040 --> 00:02:19,760
and with any usual functions you would expect this completely changes the area.

30
00:02:19,960 --> 00:02:23,360
You can't just randomly modify an integral like this and expect nothing to change.

31
00:02:23,760 --> 00:02:28,360
So already it's a little bit weird that this result also equals pi, that nothing has changed.

32
00:02:28,760 --> 00:02:31,040
That's another mystery you should add to your list.

33
00:02:31,040 --> 00:02:36,480
And the next step in the sequence was to take an even more stretched out version of the sinc function by a factor of 5,

34
00:02:36,960 --> 00:02:44,240
multiply that by what we already have, and again look at the signed area underneath the whole curve, which again equals pi.

35
00:02:45,000 --> 00:02:50,760
And it continues on like this. With each iteration, we stretch out by a new odd number and multiply that into what we have.

36
00:02:51,240 --> 00:02:54,600
One thing you might notice is how except at the input x equals 0,

37
00:02:55,000 --> 00:02:59,600
every single part of this function is progressively getting multiplied by something that's smaller than 1.

38
00:03:00,040 --> 00:03:04,440
So you would expect, as the sequence progresses, for things to get squished down more and more, and

39
00:03:04,720 --> 00:03:07,320
if anything you would expect the area to be getting smaller.

40
00:03:08,400 --> 00:03:13,560
Eventually that is exactly what happens, but what's bizarre is that it stays so stable for so long,

41
00:03:13,560 --> 00:03:20,800
and of course more pertinently, that when it does break at the value 15, it does so by the tiniest tiny amount.

42
00:03:21,080 --> 00:03:23,840
And before you go thinking this is the result of some numerical error,

43
00:03:23,840 --> 00:03:27,960
maybe because we're doing something with floating-point arithmetic, if you work this out more precisely,

44
00:03:28,200 --> 00:03:35,760
here is the exact value of that last integral, which is a certain fraction of pi where the numerator and the denominator are absurd.

45
00:03:35,760 --> 00:03:38,600
They're both around 400 billion billion billion.

46
00:03:39,280 --> 00:03:44,880
So this pattern was described in a paper by a father-son pair, Jonathan and David Borwein, which is very fun,

47
00:03:44,880 --> 00:03:50,120
and they mentioned how when a fellow researcher was computing these integrals using a computer algebra system,

48
00:03:50,120 --> 00:03:52,520
he assumed that this had to be some kind of bug.

49
00:03:52,520 --> 00:03:57,080
But it's not a bug, it is a real phenomenon, and it gets weirder than that actually.

50
00:03:57,080 --> 00:04:01,560
If we take all these integrals and include yet another factor, 2 cosine of x,

51
00:04:01,560 --> 00:04:04,400
which again you would think changes their values entirely,

52
00:04:04,400 --> 00:04:07,520
you can't just randomly multiply new things into an integral like this,

53
00:04:07,520 --> 00:04:13,920
it continues to equal pi for much much longer, and it's not until you get to the number 113 that it breaks.

54
00:04:13,920 --> 00:04:18,680
And when it breaks, it's by the most puny, absolutely subtle amount that you could imagine.

55
00:04:18,680 --> 00:04:22,880
So the natural question is, what on earth is going on here?

56
00:04:22,880 --> 00:04:26,560
And luckily, there actually is a really satisfying explanation for all this.

57
00:04:26,560 --> 00:04:31,440
The way I think I'll go about this is to show you a phenomenon that first looks completely unrelated,

58
00:04:31,440 --> 00:04:37,280
but it shows a similar pattern, where you have a value that stays really stable until you get to the number 113.

59
00:04:37,280 --> 00:04:41,120
You get to the number 15, and then it falters by just a tiny amount.

60
00:04:41,120 --> 00:04:44,960
And then after that, I'll show why this seemingly unrelated phenomenon

61
00:04:44,960 --> 00:04:48,960
is secretly the same as all our integral expressions but in disguise.

62
00:04:48,960 --> 00:04:51,840
So, turning our attention to what seems completely different,

63
00:04:51,840 --> 00:04:55,120
consider a function that I'm going to be calling rect of x,

64
00:04:55,120 --> 00:05:00,560
which is defined to equal 1 if the input is between negative one half and one half,

65
00:05:00,560 --> 00:05:05,040
and otherwise it's equal to 0. So the function is this boring step, basically.

66
00:05:05,040 --> 00:05:09,600
This will be the first in a sequence of functions that we define, so I'll call it f1 of x,

67
00:05:09,600 --> 00:05:15,600
and each new function in our sequence is going to be a kind of moving average of the previous function.

68
00:05:15,600 --> 00:05:21,520
So for example, the way this second iteration will be defined is to take this sliding window whose width is one third,

69
00:05:21,520 --> 00:05:26,160
and for a particular input x, when the window is centered at that input x,

70
00:05:26,160 --> 00:05:28,960
the value in my new function, drawn below,

71
00:05:28,960 --> 00:05:34,080
is defined to be equal to the average value of the first function above inside that window.

72
00:05:34,080 --> 00:05:38,000
So for example, when the window is far enough to the left, every value inside it is 0,

73
00:05:38,000 --> 00:05:40,160
so the graph on the bottom is showing 0.

74
00:05:40,160 --> 00:05:43,120
As soon as that window starts to go over the plateau a little bit,

75
00:05:43,120 --> 00:05:47,120
the average value is a little more than 0, and you see that in the graph below.

76
00:05:47,120 --> 00:05:52,320
And notice that when exactly half the window is over that plateau at 1 and half of it is at 0,

77
00:05:52,320 --> 00:05:56,560
the corresponding value in the bottom graph is one half, and you get the point.

78
00:05:56,560 --> 00:06:01,840
The important thing I want you to focus on is how when that window is entirely in the plateau above,

79
00:06:01,840 --> 00:06:05,360
where all the values are 1, then the average value is also 1,

80
00:06:05,360 --> 00:06:08,160
so we get this plateau on our function at the bottom.

81
00:06:08,160 --> 00:06:10,720
Let's call this bottom function f2 of x,

82
00:06:10,720 --> 00:06:15,360
and what I want you to think about is the length of the plateau for that second function.

83
00:06:15,360 --> 00:06:16,320
How wide should it be?

84
00:06:16,960 --> 00:06:21,520
If you think about it for a moment, the distance between the left edge of the top plateau

85
00:06:21,520 --> 00:06:26,000
and the left edge of the bottom plateau will be exactly half of the width of the window,

86
00:06:26,000 --> 00:06:27,600
so half of one third.

87
00:06:27,600 --> 00:06:31,440
And similarly on the right side, the distance between the edges of the plateaus

88
00:06:31,440 --> 00:06:36,720
is half of the window width, so overall it's 1 minus that window width, which is 1 minus a third.

89
00:06:37,360 --> 00:06:41,520
The value we're going to be computing, the thing that will look stable for a while before it breaks,

90
00:06:42,080 --> 00:06:44,640
is the value of this function at the input 0,

91
00:06:44,640 --> 00:06:49,120
which in both of these iterations is equal to 1 because it's inside that plateau.

92
00:06:49,120 --> 00:06:52,880
For the next iteration, we're going to take a moving average of that last function,

93
00:06:52,880 --> 00:06:55,840
but this time with a window whose width is one fifth.

94
00:06:55,840 --> 00:06:58,880
It's kind of fun to think about why as you slide around this window,

95
00:06:58,880 --> 00:07:01,920
you get a smoothed out version of the previous function.

96
00:07:01,920 --> 00:07:06,400
And again, the significant thing I want you to focus on is how when that window is entirely

97
00:07:06,400 --> 00:07:11,360
inside the plateau of the previous function, then by definition the bottom function is going to equal 1.

98
00:07:11,920 --> 00:07:16,160
This time, the length of that plateau on the bottom will be the length of the previous one,

99
00:07:16,160 --> 00:07:19,520
1 minus a third, minus the window width, one fifth.

100
00:07:19,520 --> 00:07:22,880
The reasoning is the same as before, in order to go from the point where

101
00:07:22,880 --> 00:07:28,000
the middle of the window is on that top plateau to where the entirety of the window is inside that plateau,

102
00:07:28,000 --> 00:07:31,120
is half the window width, and likewise on the right side.

103
00:07:31,120 --> 00:07:35,600
And once more, the value to record is the output of this function when the input is 0,

104
00:07:35,600 --> 00:07:37,200
which again is exactly 1.

105
00:07:38,400 --> 00:07:41,840
The next iteration is a moving average with a window width of one seventh.

106
00:07:41,840 --> 00:07:44,320
The plateau gets smaller by that 1 over 7.

107
00:07:44,320 --> 00:07:48,400
Doing one more iteration with 1 over 9, the plateau gets smaller by that amount.

108
00:07:48,400 --> 00:07:50,720
And as we keep going, the plateau gets thinner and thinner.

109
00:07:51,680 --> 00:07:56,240
And also, notice how just outside of the plateau, the function is really really close to 1,

110
00:07:56,240 --> 00:08:00,480
because it's always been the result of an average between the plateau at 1 and the neighbors,

111
00:08:00,480 --> 00:08:02,160
which themselves are really really close to 1.

112
00:08:02,960 --> 00:08:07,520
The point at which all of this breaks is once we get to the iteration where we're sliding a window

113
00:08:07,520 --> 00:08:09,840
with width one fifteenth across the whole thing.

114
00:08:10,560 --> 00:08:14,400
At that point, the previous plateau is actually thinner than the window itself.

115
00:08:14,400 --> 00:08:20,240
So even at the input x equals 0, this moving average will have to be ever so slightly smaller than 1.

116
00:08:20,960 --> 00:08:24,000
And the only thing that's special about the number 15 here is that

117
00:08:24,000 --> 00:08:26,720
as we keep adding the reciprocals of these odd fractions,

118
00:08:26,720 --> 00:08:29,520
one third plus one fifth plus one seventh, on and on,

119
00:08:29,520 --> 00:08:33,520
it's once we get to one fifteenth that that sum grows to be bigger than 1.

120
00:08:33,520 --> 00:08:38,160
And in the context of our shrinking plateaus, having started with a plateau of width 1,

121
00:08:38,160 --> 00:08:41,120
it's now shrunk down so much that it'll disappear entirely.

122
00:08:41,680 --> 00:08:47,040
The point is, with this as a sequence of functions that we've defined by a seemingly random procedure,

123
00:08:47,040 --> 00:08:50,800
if I ask you to compute the values of all of these functions at the input 0,

124
00:08:50,800 --> 00:08:55,280
you get a pattern which initially looks stable, it's 1 1 1 1 1 1,

125
00:08:55,280 --> 00:09:00,000
but by the time we get to the eighth iteration, it falls short ever so slightly, just barely.

126
00:09:00,800 --> 00:09:05,440
This is analogous, and I claim more than just analogous, to the integrals we saw earlier,

127
00:09:05,440 --> 00:09:10,160
where we have a stable value at pi pi pi pi pi, until it falls short just barely.

128
00:09:10,160 --> 00:09:15,440
And as it happens, this constant from our moving average process that's ever so slightly smaller

129
00:09:15,440 --> 00:09:20,240
than 1 is exactly the factor that sits in front of pi in our series of integrals.

130
00:09:20,240 --> 00:09:24,720
So the two situations aren't just qualitatively similar, they're quantitatively the same as well.

131
00:09:25,360 --> 00:09:29,840
And when it comes to the case where we add the 2 cosine of x term inside the integral,

132
00:09:29,840 --> 00:09:33,040
which caused the pattern to last a lot longer before it broke down,

133
00:09:33,040 --> 00:09:36,240
in the analogy what that will correspond to is the same setup,

134
00:09:36,240 --> 00:09:39,120
but where the function we start with has an even longer plateau,

135
00:09:39,120 --> 00:09:43,200
stretching from x equals negative 1 up to 1, meaning its length is 2.

136
00:09:43,200 --> 00:09:46,960
So as you do this repeated moving average process, eating into it with these smaller

137
00:09:46,960 --> 00:09:50,960
and smaller windows, it takes a lot longer for them to eat into the whole plateau.

138
00:09:51,520 --> 00:09:55,920
More specifically, the relevant computation is to ask how long do you have to add these

139
00:09:55,920 --> 00:09:59,520
reciprocals of odd numbers until that sum becomes bigger than 2?

140
00:09:59,520 --> 00:10:03,360
And it turns out that you have to go until you hit the number 113,

141
00:10:03,360 --> 00:10:08,400
which will correspond to the fact that the integral pattern there continues until you hit 113.

142
00:10:09,280 --> 00:10:12,400
And by the way, I should emphasize that there is nothing special about these

143
00:10:12,400 --> 00:10:15,520
reciprocals of odd numbers, 1 third, 1 fifth, 1 seventh.

144
00:10:15,520 --> 00:10:19,440
That just happens to be the sequence of values highlighted by the Borweins in their paper

145
00:10:19,440 --> 00:10:22,240
that made the sequence mildly famous in nerd circles.

146
00:10:22,240 --> 00:10:26,960
More generally, we could be inserting any sequence of positive numbers into those sinc functions,

147
00:10:26,960 --> 00:10:31,520
and as long as the sum of those numbers is less than 1, our expression will equal pi.

148
00:10:31,520 --> 00:10:35,440
But as soon as they become bigger than 1, our expression drops a little below pi.

149
00:10:35,440 --> 00:10:39,680
And if you believe me that there's an analogy with these moving averages, you can hopefully see why.

150
00:10:40,240 --> 00:10:44,960
But of course, the burning question is why on earth should these two situations have anything

151
00:10:44,960 --> 00:10:46,240
to do with each other?

152
00:10:46,240 --> 00:10:50,560
From here, the argument does bring in two mildly heavy bits of machinery,

153
00:10:50,560 --> 00:10:53,760
namely Fourier transforms and convolutions.

154
00:10:53,760 --> 00:10:57,760
And the way I'd like to go about this is to spend the remainder of this video giving you

155
00:10:57,760 --> 00:11:02,000
a high-level sense of how the argument will go without necessarily assuming you're familiar

156
00:11:02,000 --> 00:11:06,160
with either of those two topics, and then to explain why the details are true in a video

157
00:11:06,160 --> 00:11:08,160
that's dedicated to convolutions.

158
00:11:08,240 --> 00:11:12,320
In particular, something called the convolution theorem, since it's incredibly beautiful

159
00:11:12,320 --> 00:11:16,240
and it's useful well beyond this specific, very esoteric question.

160
00:11:19,600 --> 00:11:24,160
To start, instead of focusing on this function sine of x divided by x, where we want to show

161
00:11:24,160 --> 00:11:28,800
why the signed area underneath its curve is equal to pi, we'll make a simple substitution

162
00:11:28,800 --> 00:11:33,760
where we replace the input x with pi times x, which has the effect of squishing the graph

163
00:11:33,760 --> 00:11:38,560
horizontally by a factor of pi, and so the area gets scaled down by a factor of pi,

164
00:11:38,560 --> 00:11:43,360
meaning our new goal is to show why this integral on the right is equal to exactly one.

165
00:11:43,360 --> 00:11:47,760
By the way, in some engineering contexts, people use the name sinc to refer to this

166
00:11:47,760 --> 00:11:51,840
function with the pi on the inside, since it's often very nice to have a normalized

167
00:11:51,840 --> 00:11:54,640
function, meaning the area under it is equal to one.

168
00:11:54,640 --> 00:11:57,680
The point is, showing this integral on the right is exactly the same thing as showing

169
00:11:57,680 --> 00:12:00,400
the integral on the left, it's just a change of variables.

170
00:12:00,480 --> 00:12:04,080
And likewise for all of the other ones in our sequence, go through each of them, replace

171
00:12:04,080 --> 00:12:09,680
the x with a pi times x, and from here the claim is that all these integrals are not

172
00:12:09,680 --> 00:12:14,240
just analogous to the moving average examples, but that both of these are two distinct ways

173
00:12:14,240 --> 00:12:16,160
of computing exactly the same thing.

174
00:12:16,160 --> 00:12:20,560
And the connection comes down to the fact that this sinc function, or the engineer sinc

175
00:12:20,560 --> 00:12:24,960
function with the pi on the inside, is related to the rect function using what's known as

176
00:12:24,960 --> 00:12:26,320
a Fourier transform.

177
00:12:26,320 --> 00:12:29,440
Now, if you've never heard of a Fourier transform, there are a few things that you

178
00:12:29,440 --> 00:12:32,560
can do about it.

179
00:12:32,560 --> 00:12:36,880
The way it's often described is that if you want to break down a function as the sum of

180
00:12:36,880 --> 00:12:41,040
a bunch of pure frequencies, or in the case of an infinite function, a continuous integral

181
00:12:41,040 --> 00:12:44,880
of a bunch of pure frequencies, the Fourier transform will tell you all the strength and

182
00:12:44,880 --> 00:12:46,960
phases for all those constituent parts.

183
00:12:46,960 --> 00:12:51,280
But all you really need to know here is that it is something which takes in one function

184
00:12:51,280 --> 00:12:56,080
and spits out a new function, and you often think of it as kind of rephrasing the information

185
00:12:56,080 --> 00:12:59,760
of your original function in a different language, like you're looking at it from a

186
00:12:59,760 --> 00:13:00,480
new perspective.

187
00:13:01,040 --> 00:13:05,200
For example, like I said, this sinc function written in this new language where you take

188
00:13:05,200 --> 00:13:08,880
a Fourier transform looks like our top hat rect function.

189
00:13:08,880 --> 00:13:12,480
And vice versa, by the way, this is a nice thing about Fourier transforms for functions

190
00:13:12,480 --> 00:13:15,760
that are symmetric about the y-axis, it is its own inverse.

191
00:13:15,760 --> 00:13:20,080
And actually, the slightly more general fact that we'll need to show is how when you transform

192
00:13:20,080 --> 00:13:23,840
the stretched out version of our sinc function, where you stretch it horizontally by a factor

193
00:13:23,840 --> 00:13:28,400
of k, what you get is a stretched and squished version of this rect function.

194
00:13:28,400 --> 00:13:32,400
But of course, all of these are just meaningless words and terminology unless you can actually

195
00:13:32,400 --> 00:13:34,880
do something upon making this translation.

196
00:13:34,880 --> 00:13:39,760
And the real idea behind why Fourier transforms are such a useful thing for math is that when

197
00:13:39,760 --> 00:13:43,840
you take statements and questions about a particular function, and then you look at

198
00:13:43,840 --> 00:13:48,240
what they correspond to with respect to the transformed version of that function, those

199
00:13:48,240 --> 00:13:51,840
statements and questions often look very, very different in this new language.

200
00:13:51,840 --> 00:13:54,880
And sometimes it makes the questions a lot easier to answer.

201
00:13:55,440 --> 00:13:59,840
For example, one very nice little fact, another thing on our list of things to show, is that

202
00:13:59,840 --> 00:14:04,000
if you want to compute the integral of some function from negative infinity to infinity,

203
00:14:04,000 --> 00:14:09,760
this signed area under the entirety of its curve, it's the same thing as simply evaluating

204
00:14:09,760 --> 00:14:13,360
the Fourier transformed version of that function at the input zero.

205
00:14:13,920 --> 00:14:17,200
This is a fact that will actually just pop right out of the definition.

206
00:14:17,200 --> 00:14:22,160
And it's representative of a more general vibe that every individual output of the Fourier

207
00:14:22,160 --> 00:14:26,640
transformed function on the right corresponds to some kind of global information about the

208
00:14:26,640 --> 00:14:28,560
original function on the left.

209
00:14:28,560 --> 00:14:33,520
In our specific case, it means if you believe me that this sinc function and the rect function

210
00:14:33,520 --> 00:14:37,840
are related with a Fourier transform like this, it explains the integral, which is otherwise

211
00:14:37,840 --> 00:14:41,760
a very tricky thing to compute, because it's saying all that signed area is the same thing

212
00:14:41,760 --> 00:14:45,040
as evaluating rect at zero, which is just one.

213
00:14:45,440 --> 00:14:48,480
Now, you could complain, surely this just moves the bump under the rug.

214
00:14:48,480 --> 00:14:52,960
Surely computing this Fourier transform, whatever that looks like, would be as hard as computing

215
00:14:52,960 --> 00:14:54,160
the original integral.

216
00:14:54,160 --> 00:14:58,480
But the idea is that there's lots of tips and tricks for computing these Fourier transforms.

217
00:14:58,480 --> 00:15:03,040
And moreover, that when you do, it tells you a lot more information than just that integral.

218
00:15:03,040 --> 00:15:05,680
You get a lot of bang for your buck out of doing the computation.

219
00:15:06,320 --> 00:15:10,160
Now, the other key fact that will explain the connection we're hunting for is that if

220
00:15:10,160 --> 00:15:14,880
you have two different functions and you take their product, and then you take the sum of

221
00:15:14,880 --> 00:15:19,440
the Fourier transform of that product, it will be the same thing as if you individually

222
00:15:19,440 --> 00:15:23,600
took the Fourier transforms of your original function and then combined them using a new

223
00:15:23,600 --> 00:15:28,240
kind of operation that we'll talk all about in the next video, known as a convolution.

224
00:15:28,240 --> 00:15:32,000
Now, even though there's a lot to be explained with convolutions, the upshot will be that

225
00:15:32,000 --> 00:15:37,680
in our specific case with these rectangular functions, taking a convolution looks just

226
00:15:37,680 --> 00:15:41,600
like one of the moving averages that we've been talking about this whole time, combined

227
00:15:41,600 --> 00:15:46,080
with our previous fact that integrating in one context looks like evaluating at zero

228
00:15:46,080 --> 00:15:51,360
in another context, if you believe me, that multiplying in one context corresponds to

229
00:15:51,360 --> 00:15:55,280
this new operation, convolutions, which for our example you should just think of as moving

230
00:15:55,280 --> 00:16:00,240
averages, that will explain why multiplying more and more of these sinc functions together

231
00:16:00,240 --> 00:16:04,960
can be thought about in terms of these progressive moving averages and always evaluating at zero,

232
00:16:04,960 --> 00:16:09,280
which in turn gives a really lovely intuition for why you would expect such a stable value

233
00:16:09,280 --> 00:16:13,440
before eventually something breaks down as the edges of the plateau inch closer and closer

234
00:16:13,440 --> 00:16:18,720
to the center. This last key fact, by the way, has a special name, it's called the convolution

235
00:16:18,720 --> 00:16:23,680
theorem, and again it's something that we'll go into much more deeply. I recognize that

236
00:16:23,680 --> 00:16:28,560
it's maybe a little unsatisfying to end things here by laying down three magical facts and

237
00:16:28,560 --> 00:16:33,200
saying everything follows from those, but hopefully this gives you a little glimpse of why powerful

238
00:16:33,200 --> 00:16:39,280
tools like Fourier transforms can be so useful for tricky problems. It's a systematic way to

239
00:16:39,280 --> 00:16:44,560
provide a shift in perspective where hard problems can sometimes look easier. If nothing else,

240
00:16:44,560 --> 00:16:48,400
it hopefully provides some motivation to learn about these beautiful things like the convolution

241
00:16:48,400 --> 00:16:53,920
theorem. As one more tiny teaser, another fun consequence of this convolution theorem will be

242
00:16:53,920 --> 00:16:58,480
that it opens the doors for an algorithm that lets you compute the product of two large numbers

243
00:16:58,480 --> 00:17:01,840
very quickly, like way faster than you think should be even possible.

244
00:17:02,800 --> 00:17:05,840
So with that, I'll see you in the next video.

