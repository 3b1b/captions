[
 {
  "input": "Let's kick things off with a quiz.",
  "translatedText": "퀴즈로 시작하겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 1.62
 },
 {
  "input": "Suppose I take a normal distribution with this familiar bell curve shape, and I have a random variable x that's drawn from that distribution.",
  "translatedText": "이 친숙한 종형 곡선 모양의 정규 분포를 취하고 해당 분포에서 추출된 임의 변수 x가 있다고 가정합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2.36,
  "end": 9.7
 },
 {
  "input": "So what you're looking at right now are repeated samples of that random variable.",
  "translatedText": "따라서 지금 보고 있는 것은 해당 확률 변수의 반복된 샘플입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.52,
  "end": 14.54
 },
 {
  "input": "And as a quick reminder, the way that you interpret this curve, what the function actually means, is that if you want the probability that your sample falls within a given range of values, say the probability that it ends up between negative one and two, well, that would equal the area under this curve in that range of values.",
  "translatedText": "그리고 다시 한번 말씀드리지만, 이 곡선을 해석하는 방식, 즉 함수가 실제로 의미하는 바는 표본이 주어진 값 범위에 속할 확률을 원한다면 음수 1과 2 사이에 있을 확률을 말하라는 것입니다. , 음, 이는 해당 값 범위에서 이 곡선 아래의 면적과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.96,
  "end": 32.8
 },
 {
  "input": "That's what the curve actually means.",
  "translatedText": "이것이 곡선이 실제로 의미하는 바입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 32.84,
  "end": 34.7
 },
 {
  "input": "I'll also pull up a second random variable, also following a normal distribution, but maybe this time a little more spread out, a slightly bigger standard deviation.",
  "translatedText": "또한 정규 분포를 따르는 두 번째 무작위 변수를 가져올 것입니다. 하지만 이번에는 좀 더 분산되어 표준 편차가 약간 더 커질 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 35.26,
  "end": 42.98
 },
 {
  "input": "And here's the quiz for you.",
  "translatedText": "그리고 여기 당신을 위한 퀴즈가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.28,
  "end": 44.44
 },
 {
  "input": "If you repeatedly sample both of these variables, and in each iteration you add up the two results, well, then that sum behaves like its own random variable.",
  "translatedText": "이 두 변수를 반복적으로 샘플링하고 각 반복에서 두 결과를 합산하면 해당 합계는 자체 임의 변수처럼 동작합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 44.6,
  "end": 53.42
 },
 {
  "input": "And the question is what distribution describes that sum that you're looking at?",
  "translatedText": "그리고 문제는 어떤 분포가 여러분이 보고 있는 합계를 설명하는가 하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 53.96,
  "end": 58.88
 },
 {
  "input": "You think about it for a little moment, maybe you have a guess, maybe you think, I don't know, it's another normal distribution, or something with a different shape.",
  "translatedText": "잠시 생각해 보세요. 짐작할 수도 있고, 글쎄요, 또 다른 정규 분포이거나 모양이 다른 것이라고 생각할 수도 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 59.38,
  "end": 66.5
 },
 {
  "input": "Needless to say, guessing is not enough.",
  "translatedText": "말할 필요도 없이 추측만으로는 충분하지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.2,
  "end": 69.12
 },
 {
  "input": "The real quiz is to be able to explain why you get the answer that you do.",
  "translatedText": "진짜 퀴즈는 왜 그런 답을 얻었는지 설명할 수 있는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 69.56,
  "end": 74.26
 },
 {
  "input": "In this case, if you have that deep to your bones visceral level of understanding for why the answer is what it is, you'll be a long way towards understanding why normal distributions serve the special function that they do in probability.",
  "translatedText": "이 경우 답이 왜 그런지에 대한 뼈저린 본능적 수준의 이해가 있다면 정규 분포가 확률에서 수행하는 특별한 기능을 수행하는 이유를 이해하는 데 큰 도움이 될 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.8,
  "end": 87.26
 },
 {
  "input": "Zooming out though, this is actually meant to be a much more general lesson about how you add two different random variables regardless of their distribution, not necessarily just the normally distributed ones.",
  "translatedText": "하지만 축소하면 이는 실제로 정규 분포뿐만 아니라 분포에 관계없이 두 개의 서로 다른 무작위 변수를 추가하는 방법에 대한 훨씬 더 일반적인 교훈을 의미합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.86,
  "end": 98.36
 },
 {
  "input": "This amounts to a special operation that you apply to the distributions underlying those variables.",
  "translatedText": "이는 해당 변수의 기본 분포에 적용하는 특수 작업에 해당합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.1,
  "end": 104.44
 },
 {
  "input": "The operation has a special name, it's called a convolution.",
  "translatedText": "이 작업에는 컨볼루션(convolution)이라는 특별한 이름이 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 104.66,
  "end": 107.52
 },
 {
  "input": "And the primary thing you and I will do today is motivate and build up two distinct ways to visualize what a convolution looks like for continuous functions, and then to talk about how these two different visualizations can each be helpful in different ways, with a special focus on the central limit theorem.",
  "translatedText": "그리고 오늘 여러분과 제가 할 가장 중요한 일은 동기를 부여하고 연속 함수에 대한 컨볼루션이 어떻게 보이는지 시각화하는 두 가지 서로 다른 방법을 구축한 다음 이 두 가지 다른 시각화가 각각 다른 방식으로 도움이 될 수 있는 방법에 대해 이야기하는 것입니다. 중심 극한 정리에 중점을 둡니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.52,
  "end": 124.1
 },
 {
  "input": "After we do the general lesson, I want to return to the opening quiz and offer an unusually satisfying way to answer it.",
  "translatedText": "일반 수업을 마친 후 시작 퀴즈로 돌아가서 매우 만족스러운 답변 방법을 제안하고 싶습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.88,
  "end": 131.66
 },
 {
  "input": "As a quick side note, regular viewers among you might know there's already a video about convolutions on this channel.",
  "translatedText": "참고로 일반 시청자라면 이 채널에 컨볼루션에 대한 동영상이 이미 있다는 것을 알고 계실 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.66,
  "end": 137.68
 },
 {
  "input": "But that one had a pretty different focus, we were only doing the discrete case, and I wanted to show not just probability but the ways that it comes up in a wide variety of contexts.",
  "translatedText": "하지만 그 것은 꽤 다른 초점을 갖고 있었고, 우리는 이산적인 경우만 다루었고 확률뿐만 아니라 그것이 다양한 맥락에서 나타나는 방식을 보여주고 싶었습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 137.68,
  "end": 146.1
 },
 {
  "input": "I'm in a slightly awkward spot because it doesn't really make sense for that to be a prerequisite to this video, but I think the best way to warm up today is to cover essentially one of the same examples used in that video.",
  "translatedText": "그것이 이 비디오의 전제조건이 되는 것이 실제로는 말이 안 되기 때문에 약간 어색한 입장에 있습니다. 하지만 오늘 몸을 풀 수 있는 가장 좋은 방법은 본질적으로 그 비디오에 사용된 것과 동일한 예 중 하나를 다루는 것이라고 생각합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.78,
  "end": 157.54
 },
 {
  "input": "So if you are coming straight from that one, you can probably skip safely ahead.",
  "translatedText": "따라서 그 곳에서 곧장 오는 경우 안전하게 앞으로 건너뛸 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 157.56,
  "end": 161.38
 },
 {
  "input": "Otherwise, let's dive right in.",
  "translatedText": "그렇지 않으면 바로 들어가 보겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.38,
  "end": 163.9
 },
 {
  "input": "For this opening quiz question, each of the random variables can take on a value in a continuous infinite range of values, all possible real numbers.",
  "translatedText": "이 시작 퀴즈 질문의 경우 각 확률 변수는 연속적인 무한 범위의 값, 가능한 모든 실수의 값을 취할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 166.86,
  "end": 174.78
 },
 {
  "input": "It'll be a lot easier if we warm up in a setting that's more discrete and finite, like maybe rolling a pair of weighted dice.",
  "translatedText": "가중치가 부여된 주사위 한 쌍을 굴리는 것처럼 좀 더 명확하고 유한한 환경에서 워밍업을 하면 훨씬 쉬울 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.3,
  "end": 181.78
 },
 {
  "input": "Here, the animation you're looking at is simulating two weighted dice, and you can probably tell what's going on, but just to spell it out explicitly, the blue die is following a distribution that seems to be biased towards lower values, the red die has a distinct distribution, and I'm repeatedly sampling from each one and recording the sum of the two values at each iteration.",
  "translatedText": "여기에서 여러분이 보고 있는 애니메이션은 두 개의 가중치 주사위를 시뮬레이션하고 있으며 무슨 일이 일어나고 있는지 알 수 있을 것입니다. 하지만 명시적으로 설명하자면 파란색 주사위는 더 낮은 값에 편향된 것처럼 보이는 분포를 따르고 있습니다. die에는 뚜렷한 분포가 있으며 각 항목에서 반복적으로 샘플링하고 각 반복에서 두 값의 합을 기록합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 182.56,
  "end": 203.14
 },
 {
  "input": "Repeating samples like this many, many different times can give you a heuristic sense of the final distribution, but our real task today is to compute that distribution precisely.",
  "translatedText": "이와 같은 샘플을 여러 번 반복하면 최종 분포에 대한 경험적 감각을 얻을 수 있지만 오늘날 우리의 실제 임무는 해당 분포를 정확하게 계산하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.74,
  "end": 212.6
 },
 {
  "input": "What is the precise probability of rolling a 2, or a 3, or a 4, or a 5, on and on for all possibilities?",
  "translatedText": "모든 가능성에 대해 2, 3, 4, 5가 계속해서 나올 확률은 얼마나 됩니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 212.6,
  "end": 219.36
 },
 {
  "input": "It's not too hard a question, I'd actually encourage you to pause and try working it out for yourself.",
  "translatedText": "너무 어려운 질문은 아닙니다. 잠시 멈추고 스스로 해결해 보시기 바랍니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.84,
  "end": 224.14
 },
 {
  "input": "The main goal in this warm-up section will be to walk through two distinct ways that you could visualize the underlying computation.",
  "translatedText": "이 준비 섹션의 주요 목표는 기본 계산을 시각화할 수 있는 두 가지 별개의 방법을 살펴보는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.98,
  "end": 231.64
 },
 {
  "input": "For example, one way you could start to think about it is that there are 36 distinct possible outcomes, and we could organize those outcomes in a little 6x6 grid.",
  "translatedText": "예를 들어, 생각해 볼 수 있는 한 가지 방법은 36개의 서로 다른 가능한 결과가 있고 이러한 결과를 작은 6x6 그리드로 구성할 수 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.92,
  "end": 242.36
 },
 {
  "input": "Now if I was to ask you, what is the probability of seeing any one of these specific outcomes, say the probability of seeing a blue 4 and a red 2, what would you say?",
  "translatedText": "이제 제가 여러분에게 이러한 특정 결과 중 하나를 볼 확률, 즉 파란색 4와 빨간색 2를 볼 확률은 얼마인지 묻는다면 어떻게 답하시겠습니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 243.04,
  "end": 252.5
 },
 {
  "input": "We might say it should be the probability of that blue 4 multiplied by the probability of the red 2.",
  "translatedText": "파란색 4의 확률에 빨간색 2의 확률을 곱해야 한다고 말할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.04,
  "end": 258.24
 },
 {
  "input": "And that would be correct assuming that the die rolls are independent from each other.",
  "translatedText": "그리고 주사위 굴림이 서로 독립적이라고 가정하면 이는 정확할 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.78,
  "end": 263.08
 },
 {
  "input": "You might say that's kind of pedantic, of course the die rolls should be independent from each other, but it's a point worth emphasizing because everything that we're going to do from here moving forward, from this simple example all the way up to the central limit theorem, assumes that the random variables are independent.",
  "translatedText": "다소 현학적이라고 말할 수도 있습니다. 물론 주사위 굴림은 서로 독립적이어야 하지만 강조할 가치가 있는 점입니다. 여기서부터 우리가 할 모든 작업은 이 간단한 예부터 다음 단계까지 진행되기 때문입니다. 중심 극한 정리는 확률 변수가 독립적이라고 가정합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 263.54,
  "end": 278.08
 },
 {
  "input": "In the real world, you want to keep a sharp eye out for if this assumption actually holds.",
  "translatedText": "현실 세계에서는 이 가정이 실제로 유효한지 예리하게 관찰하고 싶을 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.66,
  "end": 282.72
 },
 {
  "input": "Now what I'm going to do is take this grid of all possible outcomes, but start filling it in with some numbers.",
  "translatedText": "이제 제가 할 일은 가능한 모든 결과의 표를 선택하고 몇 가지 숫자로 채우는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.64,
  "end": 288.82
 },
 {
  "input": "Maybe we'll put the numbers for all the probabilities of the blue die down on the bottom, all the probabilities for the red die over here on the left, and then we will fill in the grid where the probability for every outcome inside the grid looks like some product between one number from the blue distribution and one number from the red distribution.",
  "translatedText": "아마도 파란색 주사위의 모든 확률은 아래쪽에, 빨간색 주사위의 모든 확률은 여기 왼쪽에 놓은 다음 그리드 안의 모든 결과에 대한 확률이 있는 그리드를 채울 것입니다. 파란색 분포의 한 숫자와 빨간색 분포의 한 숫자 사이의 제품처럼 보입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 289.18,
  "end": 306.18
 },
 {
  "input": "Another way to think about it is we're basically constructing a multiplication table.",
  "translatedText": "그것에 대해 생각하는 또 다른 방법은 기본적으로 구구단을 구성하고 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.68,
  "end": 310.34
 },
 {
  "input": "To be a little more visual about all of this, we could plot each one of these probabilities as the height of a bar above the square in this sort of three-dimensional plot.",
  "translatedText": "이 모든 것에 대해 좀 더 시각적으로 나타내기 위해 이러한 종류의 3차원 플롯에서 이러한 확률 각각을 사각형 위의 막대 높이로 플롯할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 310.7,
  "end": 319.68
 },
 {
  "input": "In some sense, this three-dimensional plot carries all the data that we would need to know about rolling a pair of dice.",
  "translatedText": "어떤 의미에서 이 3차원 플롯은 주사위 굴림에 대해 알아야 할 모든 데이터를 담고 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.12,
  "end": 325.6
 },
 {
  "input": "And so the question is how do we extract the thing that we want to know, the probabilities for various different sums?",
  "translatedText": "그래서 질문은 우리가 알고 싶은 것, 즉 다양한 합계에 대한 확률을 어떻게 추출하는가 하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 325.74,
  "end": 332.16
 },
 {
  "input": "Well, if you highlight all of the outcomes with a certain sum, say a sum of six, notice how all of those end up on a certain diagonal.",
  "translatedText": "글쎄요, 모든 결과를 특정 합계, 예를 들어 6의 합계로 강조 표시하면 그 모든 결과가 어떻게 특정 대각선으로 끝나는지 확인하세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 333.66,
  "end": 341.26
 },
 {
  "input": "Same deal if I highlight all the pairs where the sum is seven.",
  "translatedText": "합계가 7인 모든 쌍을 강조 표시해도 마찬가지입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 341.74,
  "end": 344.72
 },
 {
  "input": "They sit along a different diagonal.",
  "translatedText": "그들은 다른 대각선을 따라 앉아 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 345.1,
  "end": 346.76
 },
 {
  "input": "So to compute the probability of each possible sum, what you do is you add together all of the entries that sit on one of these diagonals.",
  "translatedText": "따라서 가능한 각 합의 확률을 계산하려면 대각선 중 하나에 있는 모든 항목을 더하면 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 347.24,
  "end": 354.8
 },
 {
  "input": "Pulling up the 3D plot, we can better foreshadow where we'll go with this later by saying that the distribution of possible sums looks like combining all of the heights of this plot along one of these diagonal slices.",
  "translatedText": "3D 플롯을 끌어올리면 가능한 합계의 분포가 이 대각선 조각 중 하나를 따라 이 플롯의 모든 높이를 결합하는 것처럼 보인다고 말함으로써 나중에 이것으로 진행할 위치를 더 잘 예고할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.28,
  "end": 370.4
 },
 {
  "input": "It's as if we've taken this full distribution for all possible outcomes and we've kind of collapsed it along one of the directions.",
  "translatedText": "이는 가능한 모든 결과에 대해 이 전체 분포를 취하고 방향 중 하나를 따라 축소한 것과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 372.08,
  "end": 378.98
 },
 {
  "input": "And admittedly, I'm just having a bit of fun with the animations at this point, not like if you were working this out with pencil and paper, you would be drawing some three-dimensional plot.",
  "translatedText": "그리고 솔직히 말해서 지금은 애니메이션을 약간 즐기고 있는 중입니다. 연필과 종이를 사용하여 작업하는 경우 3차원 플롯을 그리는 것과는 다릅니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 380.96,
  "end": 388.9
 },
 {
  "input": "But it's fun!",
  "translatedText": "하지만 재미있어요!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 389.32,
  "end": 390.14
 },
 {
  "input": "When you collapse it on this direction, you actually do get the same distribution, which I knew you should, but it's still fun to see.",
  "translatedText": "이 방향으로 축소하면 실제로 동일한 분포를 얻게 됩니다. 그래야 한다는 것을 알았지만 그래도 보는 것은 재미있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.14,
  "end": 396.38
 },
 {
  "input": "Also, even though all of this might just seem a little bit playful or even unnecessarily complicated, I can promise you this intuition about diagonal slices will come back to us later for a genuinely satisfying proof.",
  "translatedText": "또한 이 모든 것이 약간 재미있거나 심지어 불필요하게 복잡해 보일 수도 있지만, 대각선 조각에 대한 이러한 직관은 나중에 진정으로 만족스러운 증거로 다시 돌아올 것이라고 약속할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.96,
  "end": 408.54
 },
 {
  "input": "But staying focused on the simple dice case a little bit longer, here's the second way that we could think about it.",
  "translatedText": "하지만 간단한 주사위 사건에 좀 더 집중해 보면, 여기에 우리가 생각할 수 있는 두 번째 방법이 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 408.86,
  "end": 414.28
 },
 {
  "input": "Take that bottom distribution and flip it around horizontally, so that the die values increase as you go from right to left.",
  "translatedText": "아래쪽 분포를 수평으로 뒤집어 오른쪽에서 왼쪽으로 갈수록 주사위 값이 증가합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 414.78,
  "end": 421.34
 },
 {
  "input": "Why do this, you might ask?",
  "translatedText": "왜 이런 일을 하는가?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.48,
  "end": 424.04
 },
 {
  "input": "Well, notice now which of the pairs of dice values line up with each other.",
  "translatedText": "자, 이제 어떤 주사위 값 쌍이 서로 일치하는지 확인하세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 424.6,
  "end": 428.48
 },
 {
  "input": "As it's positioned right now, we have 1 and 6, 2 and 5, 3 and 4, and so on.",
  "translatedText": "지금 위치에서는 1과 6, 2와 5, 3과 4 등이 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.86,
  "end": 434.72
 },
 {
  "input": "It is all of the pairs of values that add up to 7.",
  "translatedText": "합이 7이 되는 모든 값 쌍입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.9,
  "end": 438.1
 },
 {
  "input": "So if you want to think about the probability of rolling a 7, a way to hold that computation in your mind is to take all of the pairs of probabilities that line up with each other, multiply together those pairs, and then add up all of the results.",
  "translatedText": "따라서 7이 나올 확률에 대해 생각하고 싶다면, 그 계산을 마음속에 간직하는 방법은 서로 일치하는 모든 확률 쌍을 취하고 그 쌍을 곱한 다음 모든 값을 더하는 것입니다. 결과.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 438.1,
  "end": 452.2
 },
 {
  "input": "Some of you might like to think of this as a kind of dot product.",
  "translatedText": "여러분 중 일부는 이것을 일종의 내적이라고 생각하고 싶을 수도 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 452.94,
  "end": 455.64
 },
 {
  "input": "But the operation as a whole is not just one dot product, but many.",
  "translatedText": "그러나 전체적인 연산은 단지 하나의 내적(dot product)이 아니라 다수입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 456.18,
  "end": 459.92
 },
 {
  "input": "If we were to slide that bottom distribution a little more to the left, so in this case it looks like the die values which line up are 1 and 4, 2 and 3, 3 and 2, 4 and 1, in other words all the ones that add up to a 5, well now if we take the dot product, we multiply the pairs of probabilities that line up and add them together, that would give us the total probability of rolling a 5.",
  "translatedText": "맨 아래 분포를 왼쪽으로 조금 더 슬라이드하면 이 경우 정렬된 주사위 값은 1과 4, 2와 3, 3과 2, 4와 1, 즉 모든 숫자와 같습니다. 5를 더하면 5가 됩니다. 이제 내적을 취하면 정렬된 확률 쌍을 곱하고 더하면 5가 나올 전체 확률이 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 460.36,
  "end": 482.54
 },
 {
  "input": "In general, from this point of view, computing the full distribution for the sum looks like sliding that bottom distribution into various different positions and computing this dot product along the way.",
  "translatedText": "일반적으로 이러한 관점에서 합계에 대한 전체 분포를 계산하는 것은 아래쪽 분포를 다양한 위치로 밀어넣고 그 과정에서 이 내적을 계산하는 것처럼 보입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.2,
  "end": 493.28
 },
 {
  "input": "It is precisely the same operation as the diagonal slices we were looking at earlier.",
  "translatedText": "앞서 살펴본 대각선 조각과 정확히 동일한 작업입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.6,
  "end": 499.82
 },
 {
  "input": "They're just two different ways to visualize the same underlying operation.",
  "translatedText": "이는 동일한 기본 작업을 시각화하는 두 가지 다른 방법일 뿐입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.38,
  "end": 503.8
 },
 {
  "input": "And however you choose to visualize it, this operation that takes in two different distributions and spits out a new one, describing the sum of the relevant random variables, is called a convolution, and we often denote it with this asterisk.",
  "translatedText": "그리고 어떻게 시각화하기로 선택하든 두 개의 서로 다른 분포를 취하고 관련 무작위 변수의 합을 설명하는 새로운 분포를 내놓는 이 작업을 컨볼루션이라고 하며 종종 이 별표로 표시합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.24,
  "end": 520.88
 },
 {
  "input": "Really the way you want to think about it, especially as we set up for the continuous case, is to think of it as combining two different functions and spitting out a new function.",
  "translatedText": "특히 우리가 연속 사례를 설정할 때 실제로 생각하고 싶은 방식은 두 가지 다른 기능을 결합하고 새로운 기능을 뱉어내는 것으로 생각하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 520.88,
  "end": 529.24
 },
 {
  "input": "For example, in this case, maybe I give the function for the first distribution the name px.",
  "translatedText": "예를 들어, 이 경우 첫 번째 분포에 대한 함수에 px라는 이름을 지정할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.32,
  "end": 535.48
 },
 {
  "input": "This would be a function that takes in a possible value for the die, like a 3, and it spits out the corresponding probability.",
  "translatedText": "이것은 3과 같이 주사위에 가능한 값을 가져와 해당 확률을 내놓는 함수입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 535.82,
  "end": 542.98
 },
 {
  "input": "Similarly, let's let py be the function for our second distribution, and px plus y be the function describing the distribution for the sum.",
  "translatedText": "마찬가지로 py를 두 번째 분포에 대한 함수로 설정하고 px + y를 합계에 대한 분포를 설명하는 함수로 설정하겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.44,
  "end": 553.06
 },
 {
  "input": "In the lingo, what you would say is that px plus y is equal to a convolution between px and py.",
  "translatedText": "전문용어에서는 px + y가 px와 py 사이의 컨볼루션과 같다고 말할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.96,
  "end": 561.08
 },
 {
  "input": "And what I want you to think about now is what the formula for this operation should look like.",
  "translatedText": "그리고 제가 지금 여러분이 생각해 보셨으면 하는 것은 이 연산의 공식이 어떤 모습이어야 하는가입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 561.68,
  "end": 566.14
 },
 {
  "input": "You've seen two different ways to visualize it, but how do we actually write it down in symbols?",
  "translatedText": "시각화하는 두 가지 다른 방법을 보았지만 실제로 기호로 어떻게 기록합니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.44,
  "end": 570.46
 },
 {
  "input": "To get your bearings, maybe it's helpful to write down a specific example, like the case of plugging in a 4, where you add up over all the different pairwise products corresponding to pairs of inputs that add up to a 4.",
  "translatedText": "방향을 파악하려면 4를 연결하는 경우와 같이 최대 4를 더하는 입력 쌍에 해당하는 모든 다른 쌍별 곱을 더하는 특정 예를 작성하는 것이 도움이 될 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 570.96,
  "end": 581.66
 },
 {
  "input": "And more generally, here's how it might look.",
  "translatedText": "보다 일반적으로 보면 다음과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.46,
  "end": 584.54
 },
 {
  "input": "This new function takes as an input a possible sum for your random variables, which I'll call s, and what it outputs looks like a sum over a bunch of pairs of values for x and y.",
  "translatedText": "이 새로운 함수는 임의 변수에 대한 가능한 합계를 입력으로 사용하며, 이를 s라고 부르며, 출력되는 내용은 x와 y에 대한 여러 쌍의 값에 대한 합계처럼 보입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 584.98,
  "end": 595.82
 },
 {
  "input": "Except the usual way it's written is not to write with x and y, but instead we just focus on one of those variables, in this case x, letting it range over all of its possible values, which here just means going from 1 to 6.",
  "translatedText": "일반적인 작성 방법은 x와 y로 작성하는 것이 아니라 해당 변수 중 하나(이 경우 x)에만 초점을 맞춰 가능한 모든 값에 걸쳐 범위를 지정하는 것입니다. 여기서는 1에서 6까지 가는 것을 의미합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.82,
  "end": 608.36
 },
 {
  "input": "And instead of writing y, you write s minus x, essentially whatever the number has to be to make sure the sum is s.",
  "translatedText": "그리고 y를 쓰는 대신 s 빼기 x를 씁니다. 기본적으로 합이 s인지 확인하기 위해 필요한 숫자는 무엇이든 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 608.84,
  "end": 615.72
 },
 {
  "input": "Now the astute among you might notice a slightly weird quirk with the formula as it's written.",
  "translatedText": "이제 여러분 중 기민한 분은 작성된 공식에서 약간 이상한 점을 발견할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.3,
  "end": 621.68
 },
 {
  "input": "For example, if you plug in a given value like s equals 4, and you unpack this sum, letting x range over all the possible values going from 1 up to 6, then sometimes that corresponding y value drops below the domain of what we've explicitly defined.",
  "translatedText": "예를 들어, s = 4와 같은 주어진 값을 연결하고 이 합계를 풀고 x가 1에서 6까지의 가능한 모든 값에 걸쳐 있도록 하면 때로는 해당 y 값이 우리가 정의한 영역 아래로 떨어집니다. 명시적으로 정의했습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 622.22,
  "end": 636.96
 },
 {
  "input": "For example, you plug in 0 and negative 1 and negative 2.",
  "translatedText": "예를 들어 0과 -1, -2를 연결합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 637.4,
  "end": 640.54
 },
 {
  "input": "It's not actually that big a deal, essentially you would just say all of these values are 0, so all these later terms don't get counted.",
  "translatedText": "실제로 그렇게 큰 문제는 아닙니다. 기본적으로 모든 값이 0이라고 말하면 이후의 모든 용어는 계산되지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 641.2,
  "end": 648.16
 },
 {
  "input": "And that should kind of make sense.",
  "translatedText": "그리고 그것은 어느 정도 의미가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 648.64,
  "end": 649.74
 },
 {
  "input": "What is the probability that the red die rolls to become a negative 1?",
  "translatedText": "빨간색 주사위가 굴러서 마이너스 1이 될 확률은 얼마입니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 649.9,
  "end": 653.28
 },
 {
  "input": "Well, it's 0.",
  "translatedText": "글쎄요, 0이에요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 653.82,
  "end": 654.82
 },
 {
  "input": "That is an impossible outcome.",
  "translatedText": "그것은 불가능한 결과이다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.86,
  "end": 656.4
 },
 {
  "input": "As a next step, let's turn our attention towards continuous distributions, where your random variable can take on values anywhere in an infinite continuum, like all possible real numbers.",
  "translatedText": "다음 단계로, 임의 변수가 가능한 모든 실수와 같이 무한 연속체의 어느 위치에서나 값을 취할 수 있는 연속 분포에 관심을 돌려보겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 661.04,
  "end": 671.04
 },
 {
  "input": "Maybe you're doing weather modeling and trying to predict the temperature tomorrow at noon, or you're doing some financial projections, or maybe you're modeling the typical wait times before a bus arrives.",
  "translatedText": "어쩌면 날씨 모델링을 하고 내일 정오의 기온을 예측하려고 할 수도 있고, 재정적 예측을 하고 있을 수도 있고, 버스가 도착하기 전의 일반적인 대기 시간을 모델링할 수도 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.52,
  "end": 680.62
 },
 {
  "input": "There are all sorts of things where you need to handle continuity.",
  "translatedText": "연속성을 처리해야 하는 모든 종류의 것들이 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 680.84,
  "end": 683.36
 },
 {
  "input": "In all the graphs that we draw, the x value still represents a possible number that the random variable can take on, but the interpretation of the y-axis is a little bit different, because no longer does this represent probability, instead the thing that we're graphing is what's called probability density.",
  "translatedText": "우리가 그리는 모든 그래프에서 x 값은 여전히 무작위 변수가 취할 수 있는 가능한 숫자를 나타내지만 y축의 해석은 약간 다릅니다. 왜냐하면 이것은 더 이상 확률을 나타내지 않고 대신에 우리가 그래프로 표시하는 것은 확률 밀도라고 불리는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 683.9,
  "end": 699.84
 },
 {
  "input": "This is something we've talked about before, so you know the deal.",
  "translatedText": "이것은 우리가 이전에 이야기한 내용이므로 거래 내용을 알고 계실 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 700.32,
  "end": 703.02
 },
 {
  "input": "Essentially, the probability that a sample of your variable falls within a given range looks like the area under the curve in that range.",
  "translatedText": "기본적으로 변수 샘플이 특정 범위 내에 포함될 확률은 해당 범위의 곡선 아래 영역과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 703.44,
  "end": 711.16
 },
 {
  "input": "The function describing this curve is commonly called a probability density function, a common enough phrase that it's frequently just given the abbreviation PDF.",
  "translatedText": "이 곡선을 설명하는 함수는 일반적으로 확률밀도함수(Probability Density Function)라고 불리며, PDF라는 약어로만 사용되는 경우가 많습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 711.62,
  "end": 719.66
 },
 {
  "input": "And so the proper way to write all of this down would be to say that the probability that your sample falls within a given range looks like the integral of your PDF, the probability density function, in that range.",
  "translatedText": "따라서 이 모든 것을 기록하는 적절한 방법은 샘플이 주어진 범위 내에 포함될 확률이 해당 범위의 확률 밀도 함수인 PDF의 적분과 비슷하다고 말하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 720.38,
  "end": 732.02
 },
 {
  "input": "As a general rule of thumb, any time that you see a sum in the discrete case, you would use an integral in the continuous case.",
  "translatedText": "일반적으로 이산형의 합을 볼 때마다 연속형의 적분을 사용하게 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.88,
  "end": 739.6
 },
 {
  "input": "So let's think about what that means for our main example.",
  "translatedText": "그럼 이것이 우리의 주요 예에서 무엇을 의미하는지 생각해 봅시다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.42,
  "end": 743.3
 },
 {
  "input": "Let's say we have two different random variables, but this time each one will follow a continuous distribution, and we want to understand their sum and the new distribution that describes that sum.",
  "translatedText": "두 개의 서로 다른 확률 변수가 있지만 이번에는 각각이 연속 분포를 따르며 그 합과 그 합을 설명하는 새로운 분포를 이해하고 싶다고 가정해 보겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 743.86,
  "end": 754.1
 },
 {
  "input": "You can probably already guess what the formula will be just by analogy.",
  "translatedText": "비유를 통해 공식이 무엇인지 이미 짐작할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 755.42,
  "end": 758.92
 },
 {
  "input": "Remember, in the formula that we just wrote down, where p sub x is the function for the first variable and p sub y is the function for the second variable, the convolution between them, the thing describing a sum of those variables, itself looks like a sum where we combine a bunch of pairwise products.",
  "translatedText": "기억하세요, 우리가 방금 작성한 공식에서 p sub x는 첫 번째 변수에 대한 함수이고 p sub y는 두 번째 변수에 대한 함수입니다. 두 변수 사이의 컨볼루션은 해당 변수의 합을 설명하는 것 자체가 다음과 같습니다. 여러 쌍의 곱을 결합하는 합계와 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 759.4,
  "end": 775.84
 },
 {
  "input": "The expression in the continuous case really does look 100% analogous, it's just that we swap out that sum for an integral.",
  "translatedText": "연속형의 표현식은 정말 100% 유사해 보입니다. 단지 그 합을 적분으로 바꾸는 것뿐입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.48,
  "end": 782.98
 },
 {
  "input": "Sometimes when students see this definition of a convolution out of context, it can seem a little intimidating.",
  "translatedText": "때때로 학생들이 맥락과 상관없이 이러한 컨볼루션 정의를 볼 때 약간 겁이 날 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.76,
  "end": 788.62
 },
 {
  "input": "Hopefully the analogy is enough to make it clear, but the continuous nature really does give it a different flavor, and it's worth taking a couple minutes to think through what it means on its own terms.",
  "translatedText": "비유가 명확하게 설명하기에 충분하기를 바라지만, 연속적인 성격은 정말 다른 느낌을 주며, 그것이 그 자체로 무엇을 의미하는지 생각해 보는 데 몇 분 정도 시간을 할애할 가치가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 789.1,
  "end": 798.34
 },
 {
  "input": "And so I put together a little interactive demo that helps unpack each part of the expression and what it's really saying.",
  "translatedText": "그래서 저는 표현의 각 부분과 그것이 실제로 말하는 것을 풀어내는 데 도움이 되는 작은 대화형 데모를 만들었습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.34,
  "end": 805.2
 },
 {
  "input": "For example, the first term in this integral is f of x, which represents the density function for the first of the two random variables.",
  "translatedText": "예를 들어, 이 적분의 첫 번째 항은 x의 f입니다. 이는 두 확률 변수 중 첫 번째 변수에 대한 밀도 함수를 나타냅니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 805.8,
  "end": 813.56
 },
 {
  "input": "And in this case I'm choosing this sort of wedge-shaped function for that distribution, but it could be anything.",
  "translatedText": "그리고 이 경우에는 해당 분포에 대해 이런 종류의 쐐기 모양 함수를 선택하지만 무엇이든 될 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 813.94,
  "end": 818.82
 },
 {
  "input": "Similarly, g represents the density function for the second random variable, for which I'm choosing this sort of double lump-shaped distribution.",
  "translatedText": "마찬가지로, g는 두 번째 확률 변수에 대한 밀도 함수를 나타내며, 이에 대해 저는 이런 종류의 이중 덩어리 모양 분포를 선택합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 819.66,
  "end": 826.82
 },
 {
  "input": "And in the same way that earlier we went over all possible pairs of dice values with a given sum, the way you want to think about this integral is that what it wants to do is iterate over all possible pairs of values x and y that are constrained to a given sum, s.",
  "translatedText": "그리고 이전에 주어진 합으로 가능한 모든 주사위 값 쌍을 검토한 것과 같은 방식으로, 이 적분에 대해 생각하고 싶은 방식은 이것이 원하는 모든 가능한 쌍의 x와 y에 대해 반복하는 것입니다. 주어진 합계 s로 제한됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 826.82,
  "end": 842.8
 },
 {
  "input": "We don't really have great notation for doing that symmetrically, so instead the way we commonly write it down gives this artificial emphasis to one of the variables, in this case x, where we let that value x range over all possible real numbers, negative infinity up to infinity, and the thing we plug into the function g is s minus x, essentially whatever it has to be to make sure that this sum is constrained to be s.",
  "translatedText": "우리는 이를 대칭적으로 수행하는 데 대한 훌륭한 표기법을 가지고 있지 않으므로 일반적으로 이를 작성하는 방식은 변수 중 하나(이 경우 x)에 인위적인 강조를 제공합니다. 여기서 x 값은 가능한 모든 실수에 대해 범위를 둡니다. 음의 무한대에서 무한대까지, 그리고 우리가 함수 g에 연결하는 것은 s - x입니다. 본질적으로 이 합이 s로 제한되도록 하기 위해 필요한 모든 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 843.34,
  "end": 867.86
 },
 {
  "input": "So for the demo, instead of graphing g directly, I want to graph g of s minus x.",
  "translatedText": "따라서 데모에서는 g를 직접 그래프로 표시하는 대신 g(s - x)를 그래프로 표시하고 싶습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.38,
  "end": 874.6
 },
 {
  "input": "You might ask yourself, what does that look like?",
  "translatedText": "당신은 스스로에게 물어볼 수 있습니다. 그것은 어떻게 생겼습니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 875.1,
  "end": 877.14
 },
 {
  "input": "Well, if you plug in negative x as the input, that has the effect of flipping around the graph horizontally.",
  "translatedText": "음, 음수 x를 입력으로 연결하면 그래프가 수평으로 뒤집히는 효과가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 877.68,
  "end": 883.9
 },
 {
  "input": "And then if we throw in this parameter s, treated as some kind of constant, that has the effect of shifting the graph either left or right, depending on if s is positive or negative.",
  "translatedText": "그런 다음 일종의 상수로 처리되는 이 매개변수 s를 입력하면 s가 양수인지 음수인지에 따라 그래프가 왼쪽이나 오른쪽으로 이동하는 효과가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 884.76,
  "end": 894.1
 },
 {
  "input": "In the demo, s is a parameter that I'll just grab and shift around a little bit.",
  "translatedText": "데모에서 s는 매개변수를 잡아서 조금씩 이동해 보겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 894.64,
  "end": 898.32
 },
 {
  "input": "The real fun comes from graphing the entire contents of the integral, the product between these two graphs.",
  "translatedText": "진짜 재미는 적분의 전체 내용, 즉 이 두 그래프 사이의 곱을 그래프로 그리는 데서 옵니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 898.7,
  "end": 904.24
 },
 {
  "input": "This is analogous to the list of pairwise products that we saw earlier, but in this case, instead of adding up all of those pairwise products, we want to integrate them together, which you would interpret as the area underneath this product graph.",
  "translatedText": "이는 이전에 본 쌍별 제품 목록과 유사하지만 이 경우 모든 쌍별 제품을 합산하는 대신 이를 통합하여 이 제품 그래프 아래의 영역으로 해석하려고 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 904.78,
  "end": 917.48
 },
 {
  "input": "As I shift around this value of s, the shape of that product graph changes, and so does the corresponding area.",
  "translatedText": "이 s 값을 중심으로 이동하면 해당 제품 그래프의 모양이 바뀌고 해당 영역도 변경됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 918.2,
  "end": 924.26
 },
 {
  "input": "Keep in mind, for all three graphs on the left, the input is x, and the number s is just a parameter.",
  "translatedText": "왼쪽의 세 그래프 모두 입력은 x이고 숫자 s는 매개변수일 뿐이라는 점을 명심하세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 926.92,
  "end": 933.3
 },
 {
  "input": "But for the final graph on the right, for the resulting convolution itself, this number s is the input to that function, and the corresponding output is whatever the area of the lower left graph is, whatever the integral between this combination of f and g turns out to be.",
  "translatedText": "그러나 오른쪽의 마지막 그래프, 즉 결과 컨볼루션 자체의 경우 이 숫자 s는 해당 함수에 대한 입력이고 해당 출력은 왼쪽 하단 그래프의 면적이 무엇이든, f와 g의 이 조합 사이의 적분이 무엇이든 상관없습니다. 로 밝혀지다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 933.3,
  "end": 949.82
 },
 {
  "input": "Here, it might be helpful if we do a simple example, say where each of our two random variables follows a uniform distribution between the values negative one-half and positive one-half.",
  "translatedText": "여기에서 두 개의 무작위 변수 각각이 음의 1/2 값과 양의 1/2 값 사이의 균일한 분포를 따르는 간단한 예를 수행하는 것이 도움이 될 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 953.28,
  "end": 963.76
 },
 {
  "input": "So what that looks like is that our density functions each have this kind of top hat shape, where the graph equals 1 for all inputs between negative one-half and positive one-half, and it equals 0 everywhere else.",
  "translatedText": "따라서 밀도 함수는 각각 이런 종류의 모자 모양을 가지고 있습니다. 여기서 그래프는 음의 1/2과 양의 1/2 사이의 모든 입력에 대해 1이고 다른 모든 곳에서는 0입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 964.46,
  "end": 976.46
 },
 {
  "input": "The question, as always, is what should the distribution for the sum look like?",
  "translatedText": "항상 그렇듯이, 문제는 합계에 대한 분포가 어떤 모습이어야 하느냐는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 977.04,
  "end": 981.44
 },
 {
  "input": "Well, let me show you how it looks inside our demo.",
  "translatedText": "자, 데모 내부에서 어떻게 보이는지 보여드리겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 981.96,
  "end": 984.4
 },
 {
  "input": "In this case, the product between the two graphs has a really easy interpretation.",
  "translatedText": "이 경우 두 그래프 사이의 곱은 매우 쉽게 해석됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.22,
  "end": 989.18
 },
 {
  "input": "It is 1 wherever the graphs overlap with each other, but 0 everywhere else.",
  "translatedText": "그래프가 서로 겹치는 곳에서는 1이고, 그 외의 곳에서는 0입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.18,
  "end": 994.06
 },
 {
  "input": "So if I slide this parameter s far enough to the left that our top graphs don't overlap at all, then the product graph is 0 everywhere, and that's a way of saying this is an impossible sum to achieve.",
  "translatedText": "따라서 이 매개변수를 왼쪽으로 충분히 밀어서 상단 그래프가 전혀 겹치지 않으면 제품 그래프는 모든 곳에서 0이 되며 이는 달성할 수 없는 합계임을 의미합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.56,
  "end": 1006.54
 },
 {
  "input": "That should make sense.",
  "translatedText": "말이 되네요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.22,
  "end": 1008.06
 },
 {
  "input": "Each of the two variables can only get as low as negative one-half, so the sum could never get below negative 1.",
  "translatedText": "두 변수 각각은 음의 1/2만큼만 낮아질 수 있으므로 합계는 절대로 음수 1 아래로 내려갈 수 없습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1008.2,
  "end": 1014.34
 },
 {
  "input": "As I start to slide s to the right and the graphs overlap with each other, the area increases linearly until the graphs overlap entirely and it reaches a maximum.",
  "translatedText": "s를 오른쪽으로 밀기 시작하면 그래프가 서로 겹쳐지고 그래프가 완전히 겹쳐서 최대값에 도달할 때까지 영역이 선형적으로 증가합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1014.34,
  "end": 1025.3
 },
 {
  "input": "And then after that point, it starts to decrease linearly again, which means that the distribution for the sum takes on this kind of wedge shape.",
  "translatedText": "그리고 그 시점 이후에는 다시 선형적으로 감소하기 시작하는데, 이는 합에 대한 분포가 이런 쐐기 모양을 취한다는 것을 의미합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.2,
  "end": 1033.88
 },
 {
  "input": "And I imagine this actually feels somewhat familiar for anyone who's thought about a pair of dice, that is, unweighted dice.",
  "translatedText": "그리고 저는 이것이 실제로 한 쌍의 주사위, 즉 비가중 주사위에 대해 생각하는 사람에게는 다소 친숙하게 느껴질 것이라고 생각합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1035.34,
  "end": 1041.3
 },
 {
  "input": "There, if you add up two different uniformly distributed variables, then the distribution for the sum has a certain wedge shape.",
  "translatedText": "여기서 두 개의 서로 다른 균일 분포 변수를 합산하면 합계에 대한 분포는 특정 쐐기 모양을 갖습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1041.86,
  "end": 1049.72
 },
 {
  "input": "Probabilities increase until they max out at a 7, and then they decrease back down again.",
  "translatedText": "확률은 7이 될 때까지 증가했다가 다시 감소합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.04,
  "end": 1054.54
 },
 {
  "input": "Where this gets a lot more fun is if instead of asking for a sum of two uniformly distributed variables, I ask you what it looks like if we add up three different uniformly distributed variables.",
  "translatedText": "이것이 훨씬 더 재미있어지는 점은 두 개의 균일하게 분포된 변수의 합을 묻는 대신 세 개의 다른 균일하게 분포된 변수를 합산하면 어떤 모습인지 묻는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1056.26,
  "end": 1066.8
 },
 {
  "input": "At first you might say, I don't know, we need some new way to visualize combining three things instead of two.",
  "translatedText": "처음에는 두 가지가 아닌 세 가지를 결합하여 시각화하는 새로운 방법이 필요하다고 말할 수도 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1066.8,
  "end": 1072.58
 },
 {
  "input": "But really what you can do here is think about the sum of the first two as their own variable, which we just figured out follows this wedge shape distribution, and then take a convolution between that and the top hat function.",
  "translatedText": "하지만 실제로 여기서 할 수 있는 것은 처음 두 개의 합을 자체 변수로 생각하는 것입니다. 우리가 방금 알아낸 것은 이 쐐기 모양 분포를 따른 다음 그것과 모자 함수 사이의 컨볼루션을 취하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1073.42,
  "end": 1084.6
 },
 {
  "input": "Pulling up the demo, here's what that would look like.",
  "translatedText": "데모를 실행하면 다음과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.1,
  "end": 1087.36
 },
 {
  "input": "Once again, what makes the top hat function really nice is that multiplying by it sort of has the effect of filtering out values from the top graph.",
  "translatedText": "다시 한 번, 모자 기능을 정말 좋게 만드는 것은 이를 곱하면 상단 그래프에서 값을 필터링하는 효과가 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1087.84,
  "end": 1096.16
 },
 {
  "input": "The product on the bottom looks just like a copy of the top graph, but limited to a certain window.",
  "translatedText": "하단의 제품은 상단 그래프의 복사본과 동일하지만 특정 창으로 제한됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1096.16,
  "end": 1101.76
 },
 {
  "input": "Again, as I slide this around left and right, and the area gets bigger and smaller, the result maxes out in the middle but tapers out to either side, except this time it does so more smoothly.",
  "translatedText": "이번에도 이것을 왼쪽과 오른쪽으로 밀면서 영역이 점점 더 커지고 작아지면서 결과는 중앙에서 최대가 되지만 양쪽으로 점점 가늘어집니다. 단, 이번에는 더 부드럽게 움직입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.62,
  "end": 1112.02
 },
 {
  "input": "It's kind of like we're taking a moving average of that top left graph.",
  "translatedText": "이는 왼쪽 상단 그래프의 이동 평균을 구하는 것과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.6,
  "end": 1116.12
 },
 {
  "input": "Actually, it's more than just kind of, this literally is a moving average of the top left graph.",
  "translatedText": "실제로 이것은 단순한 것 이상입니다. 문자 그대로 왼쪽 상단 그래프의 이동 평균입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1116.94,
  "end": 1121.84
 },
 {
  "input": "One thing you might think to do is take this even further.",
  "translatedText": "당신이 생각할 수 있는 한 가지는 이것을 더욱 발전시키는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1122.4,
  "end": 1125.0
 },
 {
  "input": "The way we started was combining two top hat functions and we got this wedge, then we replaced the first function with that wedge, and then when we took the convolution we got this smoother shape describing a sum of three distinct uniform variables, but we could just repeat.",
  "translatedText": "우리가 시작한 방법은 두 개의 모자 함수를 결합하는 것이었고 이 쐐기를 얻었습니다. 그런 다음 첫 번째 함수를 그 쐐기로 대체한 다음 컨볼루션을 수행했을 때 세 가지 고유한 균일 변수의 합을 설명하는 보다 부드러운 모양을 얻었습니다. 그냥 반복하세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1125.5,
  "end": 1140.5
 },
 {
  "input": "Swap that out for the top function, and then convolve that with the flat rectangular function, and whatever result we see should describe a sum of four uniformly distributed random variables.",
  "translatedText": "이를 top 함수로 바꾼 다음 평평한 직사각형 함수로 컨벌루션합니다. 그러면 우리가 보는 결과는 4개의 균일하게 분포된 확률 변수의 합을 설명해야 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1141.22,
  "end": 1152.38
 },
 {
  "input": "Any of you who watched the video about the central limit theorem should know what to expect.",
  "translatedText": "중심극한정리에 대한 영상을 보신 분들이라면 어떤 내용이 나올지 아실 겁니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1153.66,
  "end": 1157.32
 },
 {
  "input": "As we repeat this process over and over, the shape looks more and more like a bell curve.",
  "translatedText": "이 과정을 계속해서 반복하면 모양이 점점 종형 곡선처럼 보입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1157.82,
  "end": 1162.4
 },
 {
  "input": "Or to be more precise, at each iteration we should rescale the x-axis to make sure that the standard deviation is one, because the dominant effect of this repeated convolution, the kind of repeated moving average process, is to flatten out the function over time.",
  "translatedText": "또는 더 정확하게 말하자면, 반복할 때마다 x축의 크기를 다시 조정하여 표준 편차가 1이 되도록 해야 합니다. 왜냐하면 반복적인 이동 평균 프로세스의 일종인 이 반복된 컨볼루션의 지배적인 효과는 함수를 평면화하는 것이기 때문입니다. 시간.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1162.86,
  "end": 1177.26
 },
 {
  "input": "So in the limit it just flattens out towards zero.",
  "translatedText": "따라서 한계에서는 0을 향해 평평해집니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1177.62,
  "end": 1179.84
 },
 {
  "input": "But rescaling is a way of saying, yeah yeah yeah, I know that it gets flatter, but what's the actual shape underlying it all?",
  "translatedText": "하지만 크기 조정은 다음과 같이 말하는 방식입니다. 예 예 예, 더 평평해진다는 것은 알지만 그 밑에 있는 실제 모양은 무엇입니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.24,
  "end": 1186.04
 },
 {
  "input": "The statement of the central limit theorem, one of the coolest facts from probability, is that you could have started with essentially any distribution and this still would have been true.",
  "translatedText": "확률의 가장 멋진 사실 중 하나인 중심 극한 정리의 진술은 본질적으로 어떤 분포로든 시작할 수 있었고 이는 여전히 사실일 것이라는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1188.06,
  "end": 1197.94
 },
 {
  "input": "That as you take repeated convolutions like this, representing bigger and bigger sums of a given random variable, then the distribution describing that sum, which might start off looking very different from a normal distribution, over time smooths out more and more until it gets arbitrarily close to a normal distribution.",
  "translatedText": "주어진 무작위 변수의 더 크고 더 큰 합을 나타내는 이와 같은 반복적인 컨볼루션을 취하면 정규 분포와 매우 다르게 보일 수 있는 해당 합을 설명하는 분포는 시간이 지남에 따라 임의로 얻을 때까지 점점 더 부드러워집니다. 정규분포에 가깝습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1198.54,
  "end": 1217.42
 },
 {
  "input": "It's as if a bell curve is, in some loose manner of speaking, the smoothest possible distribution, an attractive fixed point in the space of all possible functions, as we apply this process of repeated smoothing through the convolution.",
  "translatedText": "우리가 컨볼루션을 통해 반복적으로 평활화하는 프로세스를 적용할 때 종형 곡선은 느슨하게 말하면 가능한 가장 부드러운 분포이고 가능한 모든 함수의 공간에서 매력적인 고정점인 것과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1218.08,
  "end": 1230.88
 },
 {
  "input": "Naturally you might wonder, why normal distributions?",
  "translatedText": "당연히 왜 정규 분포를 따르는지 궁금할 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1235.4,
  "end": 1238.52
 },
 {
  "input": "Why this function and not some other one?",
  "translatedText": "왜 이 기능은 있고 다른 기능은 안되나요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1238.98,
  "end": 1240.92
 },
 {
  "input": "That's a very good answer, and I think the most fun way to show the answer is in the light of the last visualization that we'll show for convolutions.",
  "translatedText": "매우 좋은 답변입니다. 답변을 표시하는 가장 재미있는 방법은 컨볼루션에 대해 표시할 마지막 시각화에 비추어 보는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.68,
  "end": 1249.16
 },
 {
  "input": "Remember how in the discrete case, the first of our two visualizations involved forming this kind of multiplication table, showing the probabilities for all possible outcomes, and adding up along the diagonals?",
  "translatedText": "이산 사례에서 두 가지 시각화 중 첫 번째가 이러한 종류의 곱셈표를 형성하고 가능한 모든 결과에 대한 확률을 표시하고 대각선을 따라 합산하는 방법을 기억하십니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.28,
  "end": 1261.42
 },
 {
  "input": "You've probably guessed it by now, but our last step is to generalize this to the continuous case.",
  "translatedText": "지금쯤 짐작하셨겠지만, 마지막 단계는 이것을 연속형 사례로 일반화하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.96,
  "end": 1267.62
 },
 {
  "input": "And it is beautiful, but you have to be a little bit careful.",
  "translatedText": "그리고 그것은 아름답지만 조금 조심해야 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1268.56,
  "end": 1270.86
 },
 {
  "input": "Pulling up the same two functions we had before, f of x and g of y, what in this case would be analogous to the grid of possible pairs that we were looking at earlier?",
  "translatedText": "이전에 가졌던 동일한 두 함수인 x의 f와 y의 g를 끌어내면, 이 경우 이전에 살펴본 가능한 쌍의 그리드와 유사한 것은 무엇입니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.98,
  "end": 1281.46
 },
 {
  "input": "Well in this case, each of the variables can take on any real number, so we want to think about all possible pairs of real numbers, and the xy-plane comes to mind.",
  "translatedText": "이 경우 각 변수는 어떤 실수든 취할 수 있으므로 가능한 모든 실수 쌍에 대해 생각하고 싶고 xy 평면이 떠오릅니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1282.48,
  "end": 1291.5
 },
 {
  "input": "Every point corresponds to a possible outcome when we sample from both distributions.",
  "translatedText": "모든 점은 두 분포에서 샘플링할 때 가능한 결과에 해당합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1292.64,
  "end": 1297.04
 },
 {
  "input": "Now the probability of any one of these outcomes, xy, or rather the probability density around that point, will look like f of x times g of y, again, assuming that the two are independent.",
  "translatedText": "이제 이러한 결과 중 하나의 확률 xy, 또는 오히려 해당 지점 주변의 확률 밀도는 두 개가 독립적이라고 가정할 때 f(x) 곱하기 g(y)처럼 보일 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.14,
  "end": 1309.58
 },
 {
  "input": "So a natural thing to do is to graph this function, f of x times g of y, as a two-variable function, which would give something that looks like a surface above the xy-plane.",
  "translatedText": "따라서 자연스러운 일은 이 함수 f(x x x g x y)를 두 변수 함수로 그래프화하는 것입니다. 그러면 xy 평면 위의 표면처럼 보이는 것을 얻을 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1309.58,
  "end": 1319.92
 },
 {
  "input": "Notice in this example how if we look at it from one angle, where we see the x values changing, it has the shape of our first graph, but if we look at it from another angle, emphasizing the change in the y direction, it takes on the shape of our second graph.",
  "translatedText": "이 예에서 x 값이 변하는 것을 한 각도에서 보면 첫 번째 그래프의 모양을 가지지만, y 방향의 변화를 강조하여 다른 각도에서 보면 그래프는 첫 번째 그래프의 모양을 갖게 됩니다. 두 번째 그래프의 모양을 취합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1320.56,
  "end": 1333.84
 },
 {
  "input": "This three-dimensional graph encodes all of the information we need.",
  "translatedText": "이 3차원 그래프에는 우리에게 필요한 모든 정보가 인코딩되어 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1334.22,
  "end": 1337.8
 },
 {
  "input": "It shows all the probability densities for every possible outcome.",
  "translatedText": "가능한 모든 결과에 대한 모든 확률 밀도를 보여줍니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1337.8,
  "end": 1341.12
 },
 {
  "input": "And if you want to limit your view just to those outcomes where x plus y is constrained to be a given sum, what that looks like is limiting our view to a diagonal slice, specifically a slice over the line x plus y equals some constant.",
  "translatedText": "그리고 x + y가 주어진 합계로 제한되는 결과로만 뷰를 제한하려는 경우 이는 우리의 뷰를 대각선 슬라이스로 제한하는 것입니다. 특히 x + y 선 위의 슬라이스는 일부 상수와 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1341.9,
  "end": 1355.4
 },
 {
  "input": "All of the possible probability densities for the outcome subject to this constraint look sort of like a slice under this graph, and as we change around what specific sum we're constraining to, it shifts around which specific diagonal slice we're looking at.",
  "translatedText": "이 제약 조건이 적용되는 결과에 대해 가능한 모든 확률 밀도는 이 그래프 아래의 일종의 조각처럼 보입니다. 우리가 제한하는 특정 합계를 변경하면 우리가 보고 있는 특정 대각선 조각 주위로 이동합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1355.98,
  "end": 1370.48
 },
 {
  "input": "Now what you might predict is that the way to combine all of the probability densities along one of these slices, the way to integrate them together, can be interpreted as the area under this curve, which is a slice of the surface.",
  "translatedText": "이제 여러분이 예측할 수 있는 것은 이러한 조각 중 하나를 따라 모든 확률 밀도를 결합하는 방법, 즉 이들을 함께 통합하는 방법이 표면의 조각인 이 곡선 아래의 영역으로 해석될 수 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1373.94,
  "end": 1387.14
 },
 {
  "input": "And that is almost correct.",
  "translatedText": "그리고 그것은 거의 정확합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1387.94,
  "end": 1389.42
 },
 {
  "input": "There's a subtle detail regarding a factor of the square root of two that we need to talk about, but up to a constant factor, the areas of these slices give us the values of the convolution.",
  "translatedText": "우리가 이야기해야 할 2의 제곱근 요소에 관한 미묘한 세부 사항이 있지만 상수 요소까지 이러한 조각의 영역은 우리에게 컨볼루션 값을 제공합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.74,
  "end": 1400.68
 },
 {
  "input": "In fact, all of these slices that we're looking at are precisely the same as the product graph that we were looking at earlier.",
  "translatedText": "실제로 우리가 보고 있는 이러한 모든 조각은 이전에 보았던 제품 그래프와 정확히 동일합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1401.5,
  "end": 1408.24
 },
 {
  "input": "Here, to emphasize this point, let me pull up both visualizations side by side, and I'm going to slowly decrease the value of s, which on the left means we're looking at different slices, and on the right means we're shifting around the modified graph of g.",
  "translatedText": "여기서는 이 점을 강조하기 위해 두 시각화를 나란히 놓고 s의 값을 천천히 낮추겠습니다. 왼쪽은 서로 다른 조각을 보고 있음을 의미하고 오른쪽은 다음을 의미합니다. g의 수정된 그래프 주위로 다시 이동합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1409.44,
  "end": 1424.3
 },
 {
  "input": "Notice how at all points the shape of the graph on the bottom right, the product between the functions, looks exactly the same as the shape of the diagonal slice.",
  "translatedText": "오른쪽 하단에 있는 그래프의 모양, 즉 함수 간의 곱이 모든 지점에서 대각선 조각의 모양과 정확히 동일해 보이는지 확인하세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1425.52,
  "end": 1434.76
 },
 {
  "input": "And this should make sense.",
  "translatedText": "그리고 이것은 의미가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1438.44,
  "end": 1439.7
 },
 {
  "input": "They are two distinct ways to visualize the same thing.",
  "translatedText": "동일한 것을 시각화하는 두 가지 서로 다른 방법입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1439.84,
  "end": 1442.6
 },
 {
  "input": "It sounds like a lot when we put it into words, but what we're looking at are all the possible products between outputs of the functions corresponding to pairs of inputs that have a given sum.",
  "translatedText": "말로 표현하면 너무 많은 것처럼 들리지만, 우리가 보고 있는 것은 주어진 합을 갖는 입력 쌍에 해당하는 함수의 출력 사이에서 가능한 모든 곱입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1443.04,
  "end": 1453.94
 },
 {
  "input": "Again, it's kind of a mouthful, but I think you see what I'm saying, and we now have two different ways to see it.",
  "translatedText": "다시 한 번 말하지만, 제가 말하는 내용을 아실 것이라고 생각합니다. 이제 이를 보는 두 가지 방법이 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1454.76,
  "end": 1460.45
 },
 {
  "input": "The nice thing about the diagonal slice visualization is that it makes it much more clear that it's a symmetric operation.",
  "translatedText": "대각선 슬라이스 시각화의 좋은 점은 대칭 작업임을 훨씬 더 명확하게 한다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.0,
  "end": 1477.1
 },
 {
  "input": "It's much more obvious that f convolved with g is the same thing as g convolved with f.",
  "translatedText": "f와 g의 컨볼루션은 g와 f의 컨볼루션과 동일하다는 것이 훨씬 더 분명합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.1,
  "end": 1483.02
 },
 {
  "input": "Technically, the diagonal slices are not exactly the same shape.",
  "translatedText": "기술적으로 대각선 조각은 정확히 같은 모양이 아닙니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1484.08,
  "end": 1487.58
 },
 {
  "input": "They've actually been stretched out by a factor of the square root of 2.",
  "translatedText": "실제로는 2의 제곱근만큼 늘어났습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1487.9,
  "end": 1491.16
 },
 {
  "input": "The basic reason is that if you imagine taking some small step along one of these lines where x plus y equals a constant, then the change in your x value, the delta x here, is not the same thing as the length of that step.",
  "translatedText": "기본적인 이유는 x + y가 상수인 선 중 하나를 따라 작은 단계를 밟는다고 상상한다면 x 값의 변화, 즉 델타 x는 해당 단계의 길이와 동일하지 않기 때문입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1491.88,
  "end": 1505.2
 },
 {
  "input": "That step is actually longer by a factor of the square root of 2.",
  "translatedText": "이 단계는 실제로 2의 제곱근만큼 더 깁니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1505.2,
  "end": 1508.88
 },
 {
  "input": "I will leave a note up on the screen for the calculus enthusiasts among you who want to pause and ponder, but the upshot is very simply that the outputs of our convolution are technically not quite the areas of these diagonal slices.",
  "translatedText": "나는 잠시 멈추고 숙고하고 싶은 미적분학 애호가들을 위해 화면에 메모를 남길 것입니다. 그러나 결론은 매우 간단하게 우리 컨볼루션의 출력이 기술적으로 이러한 대각선 조각의 영역이 아니라는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1509.66,
  "end": 1521.1
 },
 {
  "input": "We have to divide those areas by a square root of 2.",
  "translatedText": "이 면적을 2의 제곱근으로 나누어야 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1521.6,
  "end": 1524.34
 },
 {
  "input": "Stepping back from all of this for a moment, I just think this is so beautiful.",
  "translatedText": "이 모든 것에서 잠시 물러나서 생각해보면 정말 아름답다고 생각합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1526.14,
  "end": 1529.54
 },
 {
  "input": "We started with such a simple question, or at least such a seemingly simple question, how do you add up two random variables?",
  "translatedText": "우리는 이러한 간단한 질문, 또는 적어도 겉으로는 단순해 보이는 질문, 두 개의 무작위 변수를 어떻게 합산합니까? 부터 시작했습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1530.04,
  "end": 1536.68
 },
 {
  "input": "And what we end up with is this very intricate operation for combining two different functions.",
  "translatedText": "그리고 결국 우리는 두 가지 다른 기능을 결합하는 매우 복잡한 작업을 수행하게 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1537.3,
  "end": 1541.84
 },
 {
  "input": "We have at least two very pretty ways to understand it, but still, some of you might be raising your hands and saying, pretty pictures are all well and good, but do they actually help you calculate something?",
  "translatedText": "우리는 그것을 이해할 수 있는 최소한 두 가지 아주 예쁜 방법을 갖고 있습니다. 하지만 여러분 중 일부는 손을 들고 이렇게 말할 수도 있습니다. 예쁜 그림은 모두 좋고 좋은데, 실제로 뭔가를 계산하는 데 도움이 되나요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1542.68,
  "end": 1552.56
 },
 {
  "input": "For example, I still have not answered the opening quiz question about adding two normally distributed random variables.",
  "translatedText": "예를 들어, 나는 두 개의 정규 분포 확률 변수를 추가하는 것에 대한 시작 퀴즈 질문에 아직 답하지 않았습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1553.04,
  "end": 1559.28
 },
 {
  "input": "Well, the ordinary way that you would approach this kind of question, if it showed up on a homework or something like that, is that you would plug in the formula for a normal distribution into the definition of a convolution, the integral that we've been describing here.",
  "translatedText": "글쎄요, 이런 종류의 질문에 접근하는 일반적인 방법은 숙제나 그와 유사한 것에 나타나는 경우 정규 분포 공식을 컨볼루션의 정의에 연결하는 것입니다. 여기에 설명했습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1559.88,
  "end": 1573.96
 },
 {
  "input": "And in that case, the visualizations would really just be there to clarify what the expression is saying, but they sit in the back seat.",
  "translatedText": "그런 경우 시각화는 실제로 표현이 말하는 내용을 명확히 하기 위해 존재하지만 뒷좌석에 앉아 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1575.08,
  "end": 1581.42
 },
 {
  "input": "In this case, the integral is not prohibitively difficult, there are analytical methods, but for this example, I want to show you a more fun method where the visualizations, specifically the diagonal slices, will play a much more front and center role in the proof itself.",
  "translatedText": "이 경우 적분은 엄청나게 어렵지 않으며 분석적인 방법이 있지만 이 예에서는 시각화, 특히 대각선 조각이 훨씬 더 앞쪽과 중앙 역할을 하는 더 재미있는 방법을 보여주고 싶습니다. 증명 그 자체.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1581.92,
  "end": 1597.04
 },
 {
  "input": "I think many of you may actually enjoy taking a moment to predict how this will look for yourself.",
  "translatedText": "나는 여러분 중 많은 사람들이 실제로 이것이 어떻게 보일지 예측하는 시간을 갖는 것을 좋아할 것이라고 생각합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1597.9,
  "end": 1602.16
 },
 {
  "input": "Think about what this 3D graph would look like in the case of two normal distributions, and what properties that it has that you might be able to take advantage of.",
  "translatedText": "두 정규 분포의 경우 이 3D 그래프가 어떤 모습일지, 그리고 활용할 수 있는 속성은 무엇인지 생각해 보세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1602.68,
  "end": 1611.58
 },
 {
  "input": "And it is for sure easiest if you start with a case where both distributions have the same standard deviation.",
  "translatedText": "그리고 두 분포의 표준 편차가 동일한 경우로 시작하는 것이 확실히 가장 쉽습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1612.48,
  "end": 1617.78
 },
 {
  "input": "Whenever you want the details, and to see how the answer fits into the central limit theorem, come join me in the next video.",
  "translatedText": "자세한 내용을 원하고 그 답이 중심 극한 정리에 어떻게 부합하는지 확인하려면 언제든지 다음 동영상을 시청하세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1619.08,
  "end": 1624.98
 }
]