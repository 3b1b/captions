[
 {
  "input": "This is a Galton board. ",
  "translatedText": "Это доска Гальтона. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 1.26
 },
 {
  "input": "Maybe you've seen one before, it's a popular demonstration of how, even when a single event is chaotic and random, with an effectively unknowable outcome, it's still possible to make precise statements about a large number of events, namely how the relative proportions for many different outcomes are distributed. ",
  "translatedText": "Возможно, вы видели такое раньше: это популярная демонстрация того, что даже когда одно событие хаотично и случайно, с практически неизвестным результатом, все еще возможно делать точные утверждения о большом количестве событий, а именно, как относительные пропорции для многих различных результатов распределены. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2.52,
  "end": 18.3
 },
 {
  "input": "More specifically, the Galton board illustrates one of the most prominent distributions in all of probability, known as the normal distribution, more colloquially known as a bell curve, and also called a Gaussian distribution. ",
  "translatedText": "Более конкретно, доска Гальтона иллюстрирует одно из наиболее ярких распределений во всей вероятности, известное как нормальное распределение, в просторечии называемое колоколообразной кривой, а также называемое распределением Гаусса. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.38,
  "end": 31.9
 },
 {
  "input": "There's a very specific function to describe this distribution, it's very pretty, we'll get into it later, but right now I just want to emphasize how the normal distribution is, as the name suggests, very common, it shows up in a lot of seemingly unrelated contexts. ",
  "translatedText": "Для описания этого распределения есть очень специфическая функция, она очень красивая, мы займемся этим позже, но сейчас я просто хочу подчеркнуть, что нормальное распределение, как следует из названия, очень распространено, оно встречается во многих случаях. из, казалось бы, несвязанных контекстов. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 32.5,
  "end": 45.04
 },
 {
  "input": "If you were to take a large number of people who sit in a similar demographic and plot their heights, those heights tend to follow a normal distribution. ",
  "translatedText": "Если вы возьмете большое количество людей, относящихся к одной и той же демографической группе, и нанесете на график их рост, то этот рост, как правило, будет соответствовать нормальному распределению. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.02,
  "end": 53.0
 },
 {
  "input": "If you look at a large swath of very big natural numbers and you ask how many distinct prime factors does each one of those numbers have, the answers will very closely track with a certain normal distribution. ",
  "translatedText": "Если вы посмотрите на большой набор очень больших натуральных чисел и спросите, сколько различных простых делителей имеет каждое из этих чисел, ответы будут очень точно соответствовать определенному нормальному распределению. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 53.66,
  "end": 64.96
 },
 {
  "input": "Now our topic for today is one of the crown jewels in all of probability theory, it's one of the key facts that explains why this distribution is as common as it is, known as the central limit theorem. ",
  "translatedText": "Наша сегодняшняя тема — одна из жемчужин всей теории вероятностей, это один из ключевых фактов, объясняющих, почему это распределение настолько распространено, что оно известно как центральная предельная теорема. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.58,
  "end": 76.02
 },
 {
  "input": "This lesson is meant to go back to the basics, giving you the fundamentals on what the central limit theorem is saying, what normal distributions are, and I want to assume minimal background. ",
  "translatedText": "Этот урок предназначен для того, чтобы вернуться к основам, дать вам основные сведения о том, что говорит центральная предельная теорема, что такое нормальные распределения, и я хочу предположить минимальную предысторию. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.64,
  "end": 85.26
 },
 {
  "input": "We're going to go decently deep into it, but after this I'd still like to go deeper and explain why the theorem is true, why the function underlying the normal distribution has the very specific form that it does, why that formula has a pi in it, and, most fun, why those last two facts are actually more related than a lot of traditional explanations would suggest. ",
  "translatedText": "Мы собираемся углубиться в это, но после этого мне все же хотелось бы пойти глубже и объяснить, почему теорема верна, почему функция, лежащая в основе нормального распределения, имеет очень специфическую форму, почему эта формула имеет в нем есть число «пи», и, что самое интересное, почему эти два последних факта на самом деле связаны между собой больше, чем предполагают многие традиционные объяснения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.26,
  "end": 105.56
 },
 {
  "input": "That second lesson is also meant to be the follow-on to the convolutions video that I promised, so there's a lot of interrelated topics here. ",
  "translatedText": "Этот второй урок также должен стать продолжением обещанного мной видео по извилистым волосам, так что здесь много взаимосвязанных тем. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.48,
  "end": 113.37
 },
 {
  "input": "But right now, back to the fundamentals, I'd like to kick things off with a overly simplified model of the Galton board. ",
  "translatedText": "Но сейчас, возвращаясь к основам, я хотел бы начать с чрезмерно упрощенной модели доски Гальтона. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.57,
  "end": 119.17
 },
 {
  "input": "In this model we will assume that each ball falls directly onto a certain central peg and that it has a 50-50 probability of bouncing to the left or to the right, and we'll think of each of those outcomes as either adding one or subtracting one from its position. ",
  "translatedText": "В этой модели мы будем предполагать, что каждый шар падает прямо на определенный центральный колышек и что вероятность его отскока влево или вправо составляет 50 на 50, и мы будем думать о каждом из этих результатов как о добавлении одного или вычитание единицы из своей позиции. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.89,
  "end": 134.11
 },
 {
  "input": "Once one of those is chosen, we make the highly unrealistic assumption that it happens to land dead on in the middle of the peg adjacent below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the right. ",
  "translatedText": "Как только один из них выбран, мы делаем крайне нереалистичное предположение, что он случайно приземлится в середине колышка, примыкающего под ним, где он снова столкнется с тем же выбором 50 на 50: отскочить влево или Направо. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.67,
  "end": 147.07
 },
 {
  "input": "For the one I'm showing on screen, there are five different rows of pegs, so our little hopping ball makes five different random choices between plus one and minus one, and we can think of its final position as basically being the sum of all of those different numbers, which in this case happens to be one, and we might label all of the different buckets with the sum that they represent. ",
  "translatedText": "Для того, что я показываю на экране, есть пять разных рядов колышков, поэтому наш маленький прыгающий шарик делает пять разных случайных выборов между плюс один и минус один, и мы можем думать о его конечном положении как о сумме всех из этих разных чисел, которое в данном случае оказывается одним, и мы могли бы пометить все разные сегменты суммой, которую они представляют. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.43,
  "end": 166.35
 },
 {
  "input": "As we repeat this, we're looking at different possible sums for those five random numbers. ",
  "translatedText": "Повторяя это, мы рассматриваем различные возможные суммы этих пяти случайных чисел. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 166.35,
  "end": 171.29
 },
 {
  "input": "And for those of you who are inclined to complain that this is a highly unrealistic model for the true Galton board, let me emphasize the goal right now is not to accurately model physics. ",
  "translatedText": "А для тех из вас, кто склонен жаловаться на то, что это крайне нереалистичная модель настоящей платы Гальтона, позвольте мне подчеркнуть, что цель сейчас не в точном моделировании физики. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.05,
  "end": 181.67
 },
 {
  "input": "The goal is to give a simple example to illustrate the central limit theorem, and for that, idealized though this might be, it actually gives us a really good example. ",
  "translatedText": "Цель состоит в том, чтобы дать простой пример, иллюстрирующий центральную предельную теорему, и, каким бы идеализированным он ни был, на самом деле он дает нам действительно хороший пример. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 181.83,
  "end": 190.03
 },
 {
  "input": "If we let many different balls fall, making yet another unrealistic assumption that they don't influence each other as if they're all ghosts, then the number of balls that fall into each different bucket gives us some loose sense for how likely each one of those buckets is. ",
  "translatedText": "Если мы позволим упасть множеству разных шаров, сделав еще одно нереалистичное предположение, что они не влияют друг на друга, как если бы они все были призраками, тогда количество шаров, попадающих в каждое ведро, даст нам некоторое неопределенное представление о том, насколько вероятен каждый из них. из этих ведер есть. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.57,
  "end": 203.39
 },
 {
  "input": "In this example, the numbers are simple enough that it's not too hard to explicitly calculate what the probability is for falling into each bucket. ",
  "translatedText": "В этом примере числа достаточно просты, поэтому нетрудно явно вычислить вероятность попадания в каждое ведро. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.83,
  "end": 210.01
 },
 {
  "input": "If you do want to think that through, you'll find it very reminiscent of Pascal's triangle. ",
  "translatedText": "Если вы захотите обдумать это, вы обнаружите, что это очень напоминает треугольник Паскаля. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.27,
  "end": 213.83
 },
 {
  "input": "But the neat thing about our theorem is how far it goes beyond the simple examples. ",
  "translatedText": "Но самое интересное в нашей теореме то, что она выходит далеко за рамки простых примеров. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.95,
  "end": 218.27
 },
 {
  "input": "So to start off at least, rather than making explicit calculations, let's just simulate things by running a large number of samples and letting the total number of results in each different outcome give us some sense for what that distribution looks like. ",
  "translatedText": "Итак, по крайней мере, для начала, вместо того, чтобы проводить явные вычисления, давайте просто смоделируем ситуацию, запустив большое количество выборок и позволив общему количеству результатов в каждом отдельном исходе дать нам некоторое представление о том, как выглядит это распределение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.67,
  "end": 229.97
 },
 {
  "input": "As I said, the one on screen has five rows, so each sum that we're considering includes only five numbers. ",
  "translatedText": "Как я уже сказал, на экране пять строк, поэтому каждая рассматриваемая сумма включает только пять чисел. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 230.45,
  "end": 236.21
 },
 {
  "input": "The basic idea of the central limit theorem is that if you increase the size of that sum, for example here that would mean increasing the number of rows of pegs for each ball to bounce off, then the distribution that describes where that sum is going to fall looks more and more like a bell curve. ",
  "translatedText": "Основная идея центральной предельной теоремы заключается в том, что если вы увеличите размер этой суммы, например здесь, это будет означать увеличение количества рядов колышков для каждого шара, от которого отскочит, тогда распределение, которое описывает, куда пойдет эта сумма. падение все больше похоже на колоколообразную кривую. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.81,
  "end": 253.33
 },
 {
  "input": "Here, it's actually worth taking a moment to write down that general idea. ",
  "translatedText": "Здесь действительно стоит потратить время на то, чтобы записать эту общую идею. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.47,
  "end": 258.35
 },
 {
  "input": "The setup is that we have a random variable, and that's basically shorthand for a random process where each outcome of that process is associated with some number. ",
  "translatedText": "Установка такова, что у нас есть случайная переменная, и это, по сути, сокращение для случайного процесса, где каждый результат этого процесса связан с некоторым числом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 259.27,
  "end": 268.19
 },
 {
  "input": "We'll call that random number x. ",
  "translatedText": "Мы назовем это случайное число x. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 268.49,
  "end": 269.97
 },
 {
  "input": "For example, each bounce off the peg is a random process modeled with two outcomes. ",
  "translatedText": "Например, каждый отскок от прищепки — это случайный процесс, моделируемый с двумя исходами. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.97,
  "end": 274.39
 },
 {
  "input": "Those outcomes are associated with the numbers negative one and positive one. ",
  "translatedText": "Эти результаты связаны с числами «отрицательный» и «положительный». ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.85,
  "end": 277.89
 },
 {
  "input": "Another example of a random variable would be rolling a die, where you have six different outcomes, each one associated with a number. ",
  "translatedText": "Другим примером случайной величины может быть бросок игральной кости, в результате которого у вас есть шесть разных результатов, каждый из которых связан с числом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.53,
  "end": 284.83
 },
 {
  "input": "What we're doing is taking multiple different samples of that variable and adding them all together. ",
  "translatedText": "Мы берем несколько разных образцов этой переменной и складываем их все вместе. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.47,
  "end": 290.41
 },
 {
  "input": "On our Galton board, that looks like letting the ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice and adding up the results. ",
  "translatedText": "На нашей доске Гальтона это выглядит так, будто мяч отскакивает от нескольких разных колышков на пути вниз, а в случае с кубиком вы можете представить, что бросаете много разных кубиков и суммируете результаты. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.77,
  "end": 300.97
 },
 {
  "input": "The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into different possible values, will look more and more like a bell curve. ",
  "translatedText": "Утверждение центральной предельной теоремы состоит в том, что по мере того, как вы позволяете размеру этой суммы становиться все больше и больше, распределение этой суммы, вероятность того, что она попадет в различные возможные значения, будет все больше и больше напоминать колоколообразную кривую. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.43,
  "end": 314.11
 },
 {
  "input": "That's it, that is the general idea. ",
  "translatedText": "Вот и все, это общая идея. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.43,
  "end": 317.13
 },
 {
  "input": "Over the course of this lesson, our job is to make that statement more quantitative. ",
  "translatedText": "В ходе этого урока наша задача — сделать это утверждение более количественным. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 317.55,
  "end": 321.53
 },
 {
  "input": "We're going to put some numbers to it, put some formulas to it, show how you can use it to make predictions. ",
  "translatedText": "Мы собираемся добавить к нему некоторые цифры, формулы и показать, как вы можете использовать его для прогнозирования. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.07,
  "end": 326.35
 },
 {
  "input": "For example, here's the kind of question I want you to be able to answer by the end of this video. ",
  "translatedText": "Например, вот вопрос, на который я хочу, чтобы вы смогли ответить к концу этого видео. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.21,
  "end": 331.57
 },
 {
  "input": "Suppose you rolled the die 100 times and you added together the results. ",
  "translatedText": "Предположим, вы бросили игральную кость 100 раз и сложили результаты. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.19,
  "end": 335.89
 },
 {
  "input": "Could you find a range of values such that you're 95% sure that the sum will fall within that range? ",
  "translatedText": "Можете ли вы найти такой диапазон значений, в котором вы на 95 % уверены, что сумма попадет в этот диапазон? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.63,
  "end": 342.17
 },
 {
  "input": "Or maybe I should say find the smallest possible range of values such that this is true. ",
  "translatedText": "Или, может быть, мне следует сказать: найдите наименьший возможный диапазон значений, в котором это будет верно. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.83,
  "end": 346.55
 },
 {
  "input": "The neat thing is you'll be able to answer this question whether it's a fair die or if it's a weighted die. ",
  "translatedText": "Самое интересное, что вы сможете ответить на этот вопрос, является ли это честным кубиком или взвешенным. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 347.39,
  "end": 352.13
 },
 {
  "input": "Now let me say at the top that this theorem has three different assumptions that go into it, three things that have to be true before the theorem follows. ",
  "translatedText": "Теперь позвольте мне сказать вверху, что эта теорема включает в себя три различных предположения, три вещи, которые должны быть верными, прежде чем из нее вытекает теорема. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.45,
  "end": 360.13
 },
 {
  "input": "And I'm actually not going to tell you what they are until the very end of the video. ",
  "translatedText": "И я на самом деле не собираюсь рассказывать вам, что это такое, до самого конца видео. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 360.43,
  "end": 363.79
 },
 {
  "input": "Instead I want you to keep your eye out and see if you can notice and maybe predict what those three assumptions are going to be. ",
  "translatedText": "Вместо этого я хочу, чтобы вы внимательно наблюдали и посмотрели, сможете ли вы заметить и, возможно, предсказать, какими будут эти три предположения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 364.27,
  "end": 369.67
 },
 {
  "input": "As a next step, to better illustrate just how general this theorem is, I want to run a couple more simulations for you focused on the dice example. ",
  "translatedText": "В качестве следующего шага, чтобы лучше проиллюстрировать, насколько общей является эта теорема, я хочу провести для вас еще пару симуляций, сосредоточенных на примере игры в кости. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.71,
  "end": 377.39
 },
 {
  "input": "Usually if you think of rolling a die you think of the six outcomes as being equally probable, but the theorem actually doesn't care about that. ",
  "translatedText": "Обычно, когда вы думаете о броске игральной кости, вы думаете, что шесть исходов равновероятны, но на самом деле теорема не заботится об этом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 380.91,
  "end": 387.63
 },
 {
  "input": "We could start with a weighted die, something with a non-trivial distribution across the outcomes, and the core idea still holds. ",
  "translatedText": "Мы могли бы начать с взвешенной кости, чего-то с нетривиальным распределением результатов, и основная идея останется в силе. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 387.83,
  "end": 394.55
 },
 {
  "input": "For the simulation what I'll do is take some distribution like this one that is skewed towards lower values. ",
  "translatedText": "Для моделирования я возьму некоторое распределение, подобное этому, со сдвигом в сторону меньших значений. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.03,
  "end": 399.93
 },
 {
  "input": "I'm going to take 10 distinct samples from that distribution and then I'll record the sum of that sample on the plot on the bottom. ",
  "translatedText": "Я возьму 10 различных выборок из этого распределения, а затем запишу сумму этой выборки на графике внизу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 400.25,
  "end": 407.55
 },
 {
  "input": "Then I'm going to do this many many different times, always with a sum of size 10, but keep track of where those sums ended up to give us a sense of the distribution. ",
  "translatedText": "Затем я собираюсь сделать это много-много раз, всегда с суммой размером 10, но отслеживайте, где в конечном итоге оказались эти суммы, чтобы дать нам представление о распределении. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 408.63,
  "end": 416.59
 },
 {
  "input": "And in fact let me rescale the y direction to give us room to run an even larger number of samples. ",
  "translatedText": "И на самом деле, позвольте мне изменить масштаб по направлению y, чтобы дать нам возможность выполнить еще большее количество образцов. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.97,
  "end": 424.73
 },
 {
  "input": "And I'll let it go all the way up to a couple thousand, and as it does you'll notice that the shape that starts to emerge looks like a bell curve. ",
  "translatedText": "И я позволю этому увеличиться до пары тысяч, и при этом вы заметите, что фигура, которая начинает проявляться, выглядит как колоколообразная кривая. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.03,
  "end": 432.49
 },
 {
  "input": "Maybe if you squint your eyes you can see it skews a tiny bit to the left, but it's neat that something so symmetric emerged from a starting point that was so asymmetric. ",
  "translatedText": "Возможно, если вы прищуритесь, вы увидите, что он немного смещается влево, но это здорово, что что-то столь симметричное возникло из такой асимметричной отправной точки. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.87,
  "end": 441.01
 },
 {
  "input": "To better illustrate what the central limit theorem is all about, let me run four of these simulations in parallel, where on the upper left I'm doing it where we're only adding two dice at a time, on the upper right we're doing it where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at a time, and then we'll do another one with a bigger sum, 15 at a time. ",
  "translatedText": "Чтобы лучше проиллюстрировать суть центральной предельной теоремы, позвольте мне запустить четыре таких моделирования параллельно: в левом верхнем углу я делаю это, когда мы добавляем только две игральные кости за раз, а в верхнем правом мы: Мы делаем это, когда мы добавляем пять кубиков за раз, нижний левый — это тот, который мы только что видели, добавляя по 10 кубиков за раз, а затем мы сделаем еще один с большей суммой, 15 за раз. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 441.47,
  "end": 461.37
 },
 {
  "input": "Notice how on the upper left when we're just adding two dice, the resulting distribution doesn't really look like a bell curve, it looks a lot more reminiscent of the one we started with skewed towards the left. ",
  "translatedText": "Обратите внимание, что в левом верхнем углу, когда мы просто добавляем два кубика, полученное распределение на самом деле не выглядит как колоколообразная кривая, оно намного больше напоминает то, с которым мы начали, с перекосом влево. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.25,
  "end": 472.03
 },
 {
  "input": "But as we allow for more and more dice in each sum, the resulting shape that comes up in these distributions looks more and more symmetric. ",
  "translatedText": "Но поскольку мы учитываем все больше и больше кубиков в каждой сумме, результирующая форма, возникающая в этих распределениях, выглядит все более и более симметричной. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.81,
  "end": 479.81
 },
 {
  "input": "It has the lump in the middle and fade towards the tail's shape of a bell curve. ",
  "translatedText": "У него есть выступ посередине, а хвост плавно переходит в колоколообразную форму. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.95,
  "end": 483.89
 },
 {
  "input": "And let me emphasize again, you can start with any different distribution. ",
  "translatedText": "И еще раз подчеркну: начать можно с любого другого дистрибутива. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 487.05,
  "end": 490.49
 },
 {
  "input": "Here I'll run it again, but where most of the probability is tied up in the numbers 1 and 6, with very low probability for the mid values. ",
  "translatedText": "Здесь я запущу его еще раз, но большая часть вероятности связана с числами 1 и 6, с очень низкой вероятностью для средних значений. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.49,
  "end": 497.49
 },
 {
  "input": "Despite completely changing the distribution for an individual roll of the die, it's still the case that a bell curve shape will emerge as we consider the different sums. ",
  "translatedText": "Несмотря на полное изменение распределения для отдельного броска игральной кости, форма колоколообразной кривой по-прежнему возникает, когда мы рассматриваем различные суммы. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.19,
  "end": 506.55
 },
 {
  "input": "Illustrating things with a simulation like this is very fun, and it's kind of neat to see order emerge from chaos, but it also feels a little imprecise. ",
  "translatedText": "Иллюстрировать вещи с помощью такой симуляции очень весело, и приятно видеть, как порядок возникает из хаоса, но это также кажется немного неточным. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.27,
  "end": 515.03
 },
 {
  "input": "Like in this case, when I cut off the simulation at 3000 samples, even though it kind of looks like a bell curve, the different buckets seem pretty spiky. ",
  "translatedText": "Как в этом случае, когда я отсек симуляцию на 3000 выборках, хотя она выглядит как колоколообразная кривая, разные сегменты кажутся довольно резкими. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 515.39,
  "end": 522.99
 },
 {
  "input": "And you might wonder, is it supposed to look that way, or is that just an artifact of the randomness in the simulation? ",
  "translatedText": "И вы можете задаться вопросом, так ли это должно выглядеть или это просто артефакт случайности в симуляции? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 522.99,
  "end": 528.55
 },
 {
  "input": "And if it is, how many samples do we need before we can be sure that what we're looking at is representative of the true distribution? ",
  "translatedText": "И если да, то сколько образцов нам нужно, прежде чем мы сможем быть уверены, что то, на что мы смотрим, является репрезентативным для истинного распределения? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 529.01,
  "end": 535.11
 },
 {
  "input": "Instead moving forward, let's get a little more theoretical and show the precise shape that these distributions will take on in the long run. ",
  "translatedText": "Вместо того, чтобы двигаться вперед, давайте немного больше теоретического и покажем точную форму, которую эти распределения примут в долгосрочной перспективе. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.19,
  "end": 545.47
 },
 {
  "input": "The easiest case to make this calculation is if we have a uniform distribution, where each possible face of the die has an equal probability, 1 6th. ",
  "translatedText": "Проще всего выполнить этот расчет, если у нас есть равномерное распределение, где каждая возможная грань игральной кости имеет равную вероятность, 1/6. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.13,
  "end": 553.97
 },
 {
  "input": "For example, if you then want to know how likely different sums are for a pair of dice, it's essentially a counting game, where you count up how many distinct pairs take on the same sum, which in the diagram I've drawn, you can conveniently think about by going through all of the different diagonals. ",
  "translatedText": "Например, если вы затем хотите узнать, насколько вероятны разные суммы для пары игральных костей, это, по сути, игра подсчета, в которой вы подсчитываете, сколько различных пар составляют одну и ту же сумму, что на диаграмме, которую я нарисовал, вы можно удобно обдумать, пройдя через все различные диагонали. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.99,
  "end": 568.49
 },
 {
  "input": "Since each such pair has an equal chance of showing up, 1 in 36, all you have to do is count the sizes of these buckets. ",
  "translatedText": "Поскольку вероятность появления каждой такой пары равна 1 из 36, вам остается только посчитать размеры этих ведер. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.41,
  "end": 577.53
 },
 {
  "input": "That gives us a definitive shape for the distribution describing a sum of two dice, and if we were to play the same game with all possible triplets, the resulting distribution would look like this. ",
  "translatedText": "Это дает нам окончательную форму распределения, описывающего сумму двух игральных костей, и если бы мы играли в ту же игру со всеми возможными тройками, полученное распределение выглядело бы следующим образом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.19,
  "end": 588.13
 },
 {
  "input": "Now what's more challenging, but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that single die. ",
  "translatedText": "Теперь, что более сложно, но гораздо интереснее, это спросить, что произойдет, если у нас будет неравномерное распределение для этого единственного кубика. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 588.69,
  "end": 594.99
 },
 {
  "input": "We actually talked all about this in the last video. ",
  "translatedText": "Обо всём этом мы, собственно, и говорили в прошлом видео. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.55,
  "end": 597.97
 },
 {
  "input": "You do essentially the same thing, you go through all the distinct pairs of dice which add up to the same value. ",
  "translatedText": "По сути, вы делаете то же самое: перебираете все различные пары игральных костей, сумма которых дает одно и то же значение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.45,
  "end": 603.67
 },
 {
  "input": "It's just that instead of counting those pairs, for each pair you multiply the two probabilities of each particular face coming up, and then you add all those together. ",
  "translatedText": "Просто вместо того, чтобы считать эти пары, вы для каждой пары умножаете две вероятности появления каждого конкретного лица, а затем складываете все это вместе. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 603.97,
  "end": 612.75
 },
 {
  "input": "The computation that does this for all possible sums has a fancy name, it's called a convolution, but it's essentially just the weighted version of the counting game that anyone who's played with a pair of dice already finds familiar. ",
  "translatedText": "Вычисление, которое делает это для всех возможных сумм, имеет причудливое название, оно называется сверткой, но по сути это просто взвешенная версия игры подсчета, которая уже знакома любому, кто играл с парой игральных костей. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 613.29,
  "end": 624.47
 },
 {
  "input": "For our purposes in this lesson, I'll have the computer calculate all that, simply display the results for you, and invite you to observe certain patterns, but under the hood, this is what's going on. ",
  "translatedText": "Для целей этого урока я попрошу компьютер все это рассчитать, просто отобразить вам результаты и предложить вам наблюдать определенные закономерности, но под капотом происходит вот что. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.03,
  "end": 635.33
 },
 {
  "input": "So just to be crystal clear on what's being represented here, if you imagine sampling two different values from that top distribution, the one describing a single die, and adding them together, then the second distribution I'm drawing represents how likely you are to see various different sums. ",
  "translatedText": "Итак, чтобы внести ясность в то, что здесь представлено, если вы представите, что выбираете два разных значения из этого верхнего распределения, описывающего одну игральную кость, и суммируете их вместе, то второе распределение, которое я рисую, показывает, насколько вероятно, что вы увидеть различные разные суммы. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 636.65,
  "end": 652.23
 },
 {
  "input": "Likewise, if you imagine sampling three distinct values from that top distribution, and adding them together, the next plot represents the probabilities for various different sums in that case. ",
  "translatedText": "Аналогично, если вы представите себе выборку трех различных значений из этого верхнего распределения и сложение их вместе, следующий график представляет вероятности для различных разных сумм в этом случае. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 652.89,
  "end": 662.49
 },
 {
  "input": "So if I compute what the distributions for these sums look like for larger and larger sums, well you know what I'm going to say, it looks more and more like a bell curve. ",
  "translatedText": "Итак, если я вычислю, как будут выглядеть распределения этих сумм для все больших и больших сумм, ну, вы знаете, что я собираюсь сказать, это все больше и больше будет похоже на колоколообразную кривую. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 663.51,
  "end": 672.39
 },
 {
  "input": "But before we get to that, I want you to make a couple more simple observations. ",
  "translatedText": "Но прежде чем мы перейдем к этому, я хочу, чтобы вы сделали еще пару простых наблюдений. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.35,
  "end": 676.45
 },
 {
  "input": "For example, these distributions seem to be wandering to the right, and also they seem to be getting more spread out, and a little bit more flat. ",
  "translatedText": "Например, кажется, что эти распределения смещаются вправо, а также становятся более разбросанными и немного более плоскими. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.45,
  "end": 684.79
 },
 {
  "input": "You cannot describe the central limit theorem quantitatively without taking into account both of those effects, which in turn requires describing the mean and the standard deviation. ",
  "translatedText": "Вы не можете описать центральную предельную теорему количественно, не принимая во внимание оба этих эффекта, что, в свою очередь, требует описания среднего значения и стандартного отклонения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.25,
  "end": 693.19
 },
 {
  "input": "Maybe you're already familiar with those, but I want to make minimal assumptions here, and it never hurts to review, so let's quickly go over both of those. ",
  "translatedText": "Возможно, вы уже знакомы с ними, но я хочу сделать здесь минимальные предположения, и обзор никогда не помешает, поэтому давайте быстро рассмотрим оба из них. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 693.95,
  "end": 700.61
 },
 {
  "input": "The mean of a distribution, often denoted with the Greek letter mu, is a way of capturing the center of mass for that distribution. ",
  "translatedText": "Среднее значение распределения, часто обозначаемое греческой буквой мю, представляет собой способ определения центра масс этого распределения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 703.41,
  "end": 710.71
 },
 {
  "input": "It's calculated as the expected value of our random variable, which is a way of saying you go through all of the different possible outcomes, and you multiply the probability of that outcome times the value of the variable. ",
  "translatedText": "Оно рассчитывается как ожидаемое значение нашей случайной величины, что означает, что вы перебираете все возможные результаты и умножаете вероятность этого результата на значение переменной. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 711.19,
  "end": 722.85
 },
 {
  "input": "If higher values are more probable, that weighted sum is going to be bigger. ",
  "translatedText": "Если более высокие значения более вероятны, эта взвешенная сумма будет больше. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 723.19,
  "end": 726.41
 },
 {
  "input": "If lower values are more probable, that weighted sum is going to be smaller. ",
  "translatedText": "Если более низкие значения более вероятны, эта взвешенная сумма будет меньше. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.75,
  "end": 729.95
 },
 {
  "input": "A little more interesting is if you want to measure how spread out this distribution is, because there's multiple different ways you might do it. ",
  "translatedText": "Немного интереснее, если вы хотите измерить, насколько распространено это распределение, потому что есть несколько разных способов сделать это. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.79,
  "end": 737.13
 },
 {
  "input": "One of them is called the variance. ",
  "translatedText": "Один из них называется дисперсией. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.53,
  "end": 740.29
 },
 {
  "input": "The idea there is to look at the difference between each possible value and the mean, square that difference, and ask for its expected value. ",
  "translatedText": "Идея состоит в том, чтобы посмотреть разницу между каждым возможным значением и средним значением, возвести эту разницу в квадрат и запросить ее ожидаемое значение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.83,
  "end": 748.27
 },
 {
  "input": "The idea is that whether your value is below or above the mean, when you square that difference, you get a positive number, and the larger the difference, the bigger that number. ",
  "translatedText": "Идея состоит в том, что независимо от того, находится ли ваше значение ниже или выше среднего, когда вы возводите эту разницу в квадрат, вы получаете положительное число, и чем больше разница, тем больше это число. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 748.73,
  "end": 756.65
 },
 {
  "input": "Squaring it like this turns out to make the math much much nicer than if we did something like an absolute value, but the downside is that it's hard to think about this as a distance in our diagram because the units are off. ",
  "translatedText": "Возведение в квадрат таким образом делает математические вычисления намного более точными, чем если бы мы делали что-то вроде абсолютного значения, но недостатком является то, что на нашей диаграмме трудно думать об этом как о расстоянии, потому что единицы измерения отключены. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.37,
  "end": 768.13
 },
 {
  "input": "Kind of like the units here are square units, whereas a distance in our diagram would be a kind of linear unit. ",
  "translatedText": "Вроде как единицы здесь — это квадратные единицы, тогда как расстояние на нашей диаграмме будет своего рода линейной единицей. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.33,
  "end": 773.31
 },
 {
  "input": "So another way to measure spread is what's called the standard deviation, which is the square root of this value. ",
  "translatedText": "Итак, еще один способ измерения разброса — это так называемое стандартное отклонение, которое представляет собой квадратный корень из этого значения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 773.71,
  "end": 779.19
 },
 {
  "input": "That can be interpreted much more reasonably as a distance on our diagram, and it's commonly denoted with the Greek letter sigma, so you know m for mean as for standard deviation, but both in Greek. ",
  "translatedText": "Гораздо более разумно это можно интерпретировать как расстояние на нашей диаграмме, и оно обычно обозначается греческой буквой сигма, поэтому вы знаете, что m означает среднее значение и стандартное отклонение, но оба на греческом языке. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 779.47,
  "end": 789.65
 },
 {
  "input": "Looking back at our sequence of distributions, let's talk about the mean and standard deviation. ",
  "translatedText": "Оглядываясь назад на нашу последовательность распределений, давайте поговорим о среднем и стандартном отклонении. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.87,
  "end": 796.15
 },
 {
  "input": "If we call the mean of the initial distribution mu, which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if I tell you that the mean of the next one is 2 times mu. ",
  "translatedText": "Если мы назовем среднее значение начального распределения mu, которое для иллюстрированного случая равно 2.24, надеюсь, вы не удивитесь, если я скажу вам, что среднее значение следующего числа в 2 раза больше му. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.63,
  "end": 806.73
 },
 {
  "input": "That is, you roll a pair of dice, you want to know the expected value of the sum, it's two times the expected value for a single die. ",
  "translatedText": "То есть вы бросаете пару игральных костей и хотите узнать ожидаемое значение суммы, оно в два раза превышает ожидаемое значение для одного кубика. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 807.13,
  "end": 812.81
 },
 {
  "input": "Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth. ",
  "translatedText": "Аналогично, ожидаемое значение нашей суммы размера 3 в 3 раза больше мю, и так далее, и тому подобное. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 813.85,
  "end": 819.41
 },
 {
  "input": "The mean just marches steadily on to the right, which is why our distributions seem to be drifting off in that direction. ",
  "translatedText": "Среднее значение неуклонно движется вправо, поэтому наши распределения, похоже, смещаются в этом направлении. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 819.63,
  "end": 824.87
 },
 {
  "input": "A little more challenging, but very important, is to describe how the standard deviation changes. ",
  "translatedText": "Немного сложнее, но очень важно описать, как изменяется стандартное отклонение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.35,
  "end": 829.91
 },
 {
  "input": "The key fact here is that if you have two different random variables, then the variance for the sum of those variables is the same as just adding together the original two variances. ",
  "translatedText": "Ключевым фактом здесь является то, что если у вас есть две разные случайные величины, то дисперсия суммы этих переменных такая же, как если бы вы просто сложили две исходные дисперсии. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 830.49,
  "end": 839.37
 },
 {
  "input": "This is one of those facts that you can just compute when you unpack all the definitions. ",
  "translatedText": "Это один из тех фактов, которые можно просто вычислить, распаковав все определения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 839.93,
  "end": 843.63
 },
 {
  "input": "There are a couple nice intuitions for why it's true. ",
  "translatedText": "Есть пара хороших предположений, почему это правда. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 843.63,
  "end": 846.21
 },
 {
  "input": "My tentative plan is to just actually make a series about probability and talk about things like intuitions underlying variance and its cousins there. ",
  "translatedText": "Мой предварительный план состоит в том, чтобы просто сделать серию о вероятности и рассказать о таких вещах, как интуиция, лежащая в основе дисперсии, и ее родственники. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 846.63,
  "end": 853.53
 },
 {
  "input": "But right now, the main thing I want you to highlight is how it's the variance that adds, it's not the standard deviation that adds. ",
  "translatedText": "Но сейчас главное, что я хочу, чтобы вы подчеркнули, это то, что добавляется дисперсия, а не стандартное отклонение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 854.01,
  "end": 860.15
 },
 {
  "input": "So, critically, if you were to take n different realizations of the same random variable and ask what the sum looks like, the variance of that sum is n times the variance of your original variable, meaning the standard deviation, the square root of all this, is the square root of n times the original standard deviation. ",
  "translatedText": "Итак, что особенно важно, если вы возьмете n различных реализаций одной и той же случайной величины и спросите, как выглядит сумма, дисперсия этой суммы в n раз превышает дисперсию вашей исходной переменной, то есть стандартное отклонение, квадратный корень из всех это квадратный корень из n, умноженного на исходное стандартное отклонение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 860.41,
  "end": 878.25
 },
 {
  "input": "For example, back in our sequence of distributions, if we label the standard deviation of our initial one with sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on and so forth. ",
  "translatedText": "Например, вернемся к нашей последовательности распределений, если мы обозначим стандартное отклонение нашего начального распределения сигмой, то следующее стандартное отклонение будет квадратным корнем из 2 сигм, и после этого оно будет выглядеть как квадратный корень из 3 раза сигма и так далее и тому подобное. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 879.29,
  "end": 893.09
 },
 {
  "input": "This, like I said, is very important. ",
  "translatedText": "Это, как я уже сказал, очень важно. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.75,
  "end": 895.65
 },
 {
  "input": "It means that even though our distributions are getting spread out, they're not spreading out all that quickly, they only do so in proportion to the square root of the size of the sum. ",
  "translatedText": "Это означает, что хотя наши распределения и распределяются, они распространяются не так быстро, а только пропорционально квадратному корню из размера суммы. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.07,
  "end": 904.13
 },
 {
  "input": "As we prepare to make a more quantitative description of the central limit theorem, the core intuition I want you to keep in your head is that we'll basically realign all of these distributions so that their means line up together, and then rescale them so that all of the standard deviations are just going to be equal to 1. ",
  "translatedText": "Пока мы готовимся к более количественному описанию центральной предельной теоремы, основная интуиция, которую я хочу, чтобы вы держали в голове, заключается в том, что мы, по сути, перестроим все эти распределения так, чтобы их средние значения выровнялись вместе, а затем изменим их масштаб таким образом. что все стандартные отклонения будут равны 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 904.71,
  "end": 920.61
 },
 {
  "input": "And when we do that, the shape that results gets closer and closer to a certain universal shape, described with an elegant little function that we'll unpack in just a moment. ",
  "translatedText": "И когда мы это делаем, получаемая в результате форма становится все ближе и ближе к определенной универсальной форме, описываемой с помощью элегантной маленькой функции, которую мы распакуем буквально через мгновение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 921.29,
  "end": 929.87
 },
 {
  "input": "And let me say one more time, the real magic here is how we could have started with any distribution, describing a single roll of the die, and if we play the same game, considering what the distributions for the many different sums look like, and we realign them so that the means line up, and we rescale them so that the standard deviations are all 1, we still approach that same universal shape, which is kind of mind-boggling. ",
  "translatedText": "И позвольте мне сказать еще раз: настоящее волшебство заключается в том, что мы могли бы начать с любого распределения, описывая один бросок игральной кости, и если мы будем играть в ту же игру, учитывая, как выглядят распределения для множества разных сумм: и мы перестраиваем их так, чтобы средние значения совпадали, и масштабируем их так, чтобы все стандартные отклонения были равны 1, мы все равно приближаемся к той же универсальной форме, которая просто ошеломляет. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.47,
  "end": 952.95
 },
 {
  "input": "And now, my friends, is probably as good a time as any to finally get into the formula for a normal distribution. ",
  "translatedText": "И сейчас, друзья мои, возможно, самое подходящее время наконец разобраться с формулой нормального распределения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 954.81,
  "end": 960.85
 },
 {
  "input": "And the way I'd like to do this is to basically peel back all the layers and build it up one piece at a time. ",
  "translatedText": "И то, как я хотел бы это сделать, состоит в том, чтобы по сути снять все слои и построить их по одному кусочку за раз. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 961.49,
  "end": 965.93
 },
 {
  "input": "The function e to the x, or anything to the x, describes exponential growth, and if you make that exponent negative, which flips around the graph horizontally, you might think of it as describing exponential decay. ",
  "translatedText": "Функция e для x или что-то еще для x описывает экспоненциальный рост, и если вы сделаете этот показатель отрицательным, который переворачивает график по горизонтали, вы можете думать о нем как об описании экспоненциального затухания. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.53,
  "end": 977.87
 },
 {
  "input": "To make this decay in both directions, you could do something to make sure the exponent is always negative and growing, like taking the negative absolute value. ",
  "translatedText": "Чтобы сделать это затухание в обоих направлениях, вы можете сделать что-нибудь, чтобы показатель степени всегда был отрицательным и рос, например, приняв отрицательное абсолютное значение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.51,
  "end": 985.43
 },
 {
  "input": "That would give us this kind of awkward sharp point in the middle, but if instead you make that exponent the negative square of x, you get a smoother version of the same thing, which decays in both directions. ",
  "translatedText": "Это дало бы нам неуклюжую острую точку посередине, но если вместо этого вы сделаете этот показатель отрицательным квадратом x, вы получите более гладкую версию того же самого объекта, который затухает в обоих направлениях. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.93,
  "end": 995.81
 },
 {
  "input": "This gives us the basic bell curve shape. ",
  "translatedText": "Это дает нам базовую форму колоколообразной кривой. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 996.33,
  "end": 998.19
 },
 {
  "input": "Now if you throw a constant in front of that x, and you scale that constant up and down, it lets you stretch and squish the graph horizontally, allowing you to describe narrow and wider bell curves. ",
  "translatedText": "Теперь, если вы поместите константу перед этим x и масштабируете эту константу вверх и вниз, это позволит вам растягивать и сжимать график по горизонтали, позволяя вам описывать узкие и более широкие кривые нормального распределения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.65,
  "end": 1008.37
 },
 {
  "input": "And a quick thing I'd like to point out here is that based on the rules of exponentiation, as we tweak around that constant c, you could also think about it as simply changing the base of the exponentiation. ",
  "translatedText": "И я хотел бы сразу отметить, что, основываясь на правилах возведения в степень, когда мы настраиваем константу c, вы также можете думать об этом как о простом изменении основания возведения в степень. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1009.01,
  "end": 1019.75
 },
 {
  "input": "And in that sense, the number e is not really all that special for our formula. ",
  "translatedText": "И в этом смысле число e не является чем-то особенным для нашей формулы. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1020.15,
  "end": 1023.63
 },
 {
  "input": "We could replace it with any other positive constant, and you'll get the same family of curves as we tweak that constant. ",
  "translatedText": "Мы могли бы заменить ее любой другой положительной константой, и вы получите то же самое семейство кривых, когда мы настроим эту константу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.05,
  "end": 1030.49
 },
 {
  "input": "Make it a 2, same family of curves. ",
  "translatedText": "Пусть это будет 2, одно и то же семейство кривых. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.51,
  "end": 1033.11
 },
 {
  "input": "Make it a 3, same family of curves. ",
  "translatedText": "Пусть это будет цифра 3, одно и то же семейство кривых. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.33,
  "end": 1035.07
 },
 {
  "input": "The reason we use e is that it gives that constant a very readable meaning. ",
  "translatedText": "Причина, по которой мы используем e, заключается в том, что это придает константе очень понятный смысл. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1035.75,
  "end": 1039.49
 },
 {
  "input": "Or rather, if we reconfigure things a little bit so that the exponent looks like negative one half times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn this into a probability distribution, that constant sigma will be the standard deviation of that distribution. ",
  "translatedText": "Или, скорее, если мы немного изменим ситуацию так, чтобы показатель степени выглядел как отрицательный, равный половине умноженного на х, деленного на определенную константу, которую мы назовем квадратом сигмы, то как только мы превратим это в распределение вероятностей, эта постоянная сигма будет быть стандартным отклонением этого распределения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.11,
  "end": 1057.21
 },
 {
  "input": "And that's very nice. ",
  "translatedText": "И это очень приятно. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.81,
  "end": 1058.57
 },
 {
  "input": "But before we can interpret this as a probability distribution, we need the area under the curve to be 1. ",
  "translatedText": "Но прежде чем мы сможем интерпретировать это как распределение вероятностей, нам нужно, чтобы площадь под кривой была равна 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.91,
  "end": 1064.31
 },
 {
  "input": "And the reason for that is how the curve is interpreted. ",
  "translatedText": "Причина этого в том, как интерпретируется кривая. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1064.83,
  "end": 1066.91
 },
 {
  "input": "Unlike discrete distributions, when it comes to something continuous, you don't ask about the probability of a particular point. ",
  "translatedText": "В отличие от дискретных распределений, когда речь идет о чем-то непрерывном, вы не спрашиваете о вероятности конкретной точки. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1067.37,
  "end": 1073.37
 },
 {
  "input": "Instead, you ask for the probability that a value falls between two different values. ",
  "translatedText": "Вместо этого вы запрашиваете вероятность того, что значение попадает между двумя разными значениями. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1073.79,
  "end": 1078.23
 },
 {
  "input": "And what the curve is telling you is that that probability equals the area under the curve between those two values. ",
  "translatedText": "И кривая говорит вам, что эта вероятность равна площади под кривой между этими двумя значениями. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.75,
  "end": 1085.43
 },
 {
  "input": "There's a whole other video about this, they're called probability density functions. ",
  "translatedText": "Об этом есть совершенно другое видео, они называются функциями плотности вероятности. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1086.03,
  "end": 1089.43
 },
 {
  "input": "The main point right now is that the area under the entire curve represents the probability that something happens, that some number comes up. ",
  "translatedText": "Главное сейчас то, что площадь под всей кривой представляет вероятность того, что что-то произойдет, что выпадет какое-то число. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1089.83,
  "end": 1097.15
 },
 {
  "input": "That should be 1, which is why we want the area under this to be 1. ",
  "translatedText": "Это должно быть 1, поэтому мы хотим, чтобы площадь под ним была равна 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1097.41,
  "end": 1100.63
 },
 {
  "input": "As it stands with the basic bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root of pi. ",
  "translatedText": "В соответствии с базовой формой колоколообразной кривой от e до отрицательного квадрата x, площадь не равна 1, на самом деле это квадратный корень из числа Пи. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1101.05,
  "end": 1107.79
 },
 {
  "input": "I know, right? ",
  "translatedText": "Я точно знаю? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.41,
  "end": 1109.15
 },
 {
  "input": "What is pi doing here? ",
  "translatedText": "Что здесь делает Пи? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1109.27,
  "end": 1110.19
 },
 {
  "input": "What does this have to do with circles? ",
  "translatedText": "Какое это имеет отношение к кругам? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1110.29,
  "end": 1111.47
 },
 {
  "input": "Like I said at the start, I'd love to talk all about that in the next video. ",
  "translatedText": "Как я уже сказал в начале, мне бы хотелось поговорить обо всем этом в следующем видео. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.01,
  "end": 1115.05
 },
 {
  "input": "But if you can spare your excitement for our purposes right now, all it means is that we should divide this function by the square root of pi, and it gives us the area we want. ",
  "translatedText": "Но если вы можете прямо сейчас потратить свое волнение на наши цели, все это означает, что нам нужно разделить эту функцию на квадратный корень из числа пи, и это даст нам нужную нам площадь. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.33,
  "end": 1123.17
 },
 {
  "input": "Throwing back in the constants we had earlier, the 1 half and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square root of 2. ",
  "translatedText": "Возвращаясь к константам, которые мы использовали ранее, 1 половине и сигме, мы получаем эффект растягивания графика в сигму, умноженную на квадратный корень из 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.61,
  "end": 1131.79
 },
 {
  "input": "So we also need to divide out by that in order to make sure it has an area of 1. ",
  "translatedText": "Поэтому нам также нужно разделить на это значение, чтобы убедиться, что его площадь равна 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.41,
  "end": 1136.47
 },
 {
  "input": "And combining those fractions, the factor out front looks like 1 divided by sigma times the square root of 2 pi. ",
  "translatedText": "И, объединив эти дроби, множитель выглядит как 1, разделенная на сигму, умноженную на квадратный корень из 2 пи. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1136.47,
  "end": 1142.11
 },
 {
  "input": "This, finally, is a valid probability distribution. ",
  "translatedText": "Наконец, это действительное распределение вероятностей. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1142.91,
  "end": 1145.85
 },
 {
  "input": "As we tweak that value sigma, resulting in narrower and wider curves, that constant in the front always guarantees that the area equals 1. ",
  "translatedText": "Когда мы настраиваем это значение сигмы, что приводит к более узким и широким кривым, эта константа спереди всегда гарантирует, что площадь равна 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.45,
  "end": 1154.31
 },
 {
  "input": "The special case where sigma equals 1 has a specific name, we call it the standard normal distribution, which plays an especially important role for you and me in this lesson. ",
  "translatedText": "Особый случай, когда сигма равна 1, имеет особое название, мы называем его стандартным нормальным распределением, которое играет для нас с вами в этом уроке особенно важную роль. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.91,
  "end": 1164.51
 },
 {
  "input": "And all possible normal distributions are not only parameterized with this value sigma, but we also subtract off another constant mu from the variable x, and this essentially just lets you slide the graph left and right so that you can prescribe the mean of this distribution. ",
  "translatedText": "И все возможные нормальные распределения не только параметризуются этим значением сигма, но мы также вычитаем еще одну константу мю из переменной x, и это, по сути, просто позволяет вам сдвигать график влево и вправо, чтобы вы могли прописать среднее значение этого распределения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.13,
  "end": 1180.21
 },
 {
  "input": "So in short, we have two parameters, one describing the mean, one describing the standard deviation, and they're all tied together in this big formula involving an e and a pi. ",
  "translatedText": "Короче говоря, у нас есть два параметра: один описывает среднее значение, другой — стандартное отклонение, и все они связаны в одну большую формулу, включающую е и пи. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.99,
  "end": 1189.19
 },
 {
  "input": "Now that all of that is on the table, let's look back again at the idea of starting with some random variable and asking what the distributions for sums of that variable look like. ",
  "translatedText": "Теперь, когда все это на столе, давайте еще раз вернемся к идее начать с некоторой случайной величины и задаться вопросом, как выглядят распределения сумм этой переменной. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.19,
  "end": 1199.81
 },
 {
  "input": "As we've already gone over, when you increase the size of that sum, the resulting distribution will shift according to a growing mean, and it slowly spreads out according to a growing standard deviation. ",
  "translatedText": "Как мы уже говорили, когда вы увеличиваете размер этой суммы, результирующее распределение будет смещаться в соответствии с растущим средним значением и медленно распространяться в соответствии с растущим стандартным отклонением. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1200.13,
  "end": 1209.81
 },
 {
  "input": "And putting some actual formulas to it, if we know the mean of our underlying random variable, we call it mu, and we also know its standard deviation, and we call it sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the standard deviation will be sigma times the square root of that size. ",
  "translatedText": "И, применив к этому некоторые реальные формулы, если мы знаем среднее значение нашей базовой случайной величины, мы называем ее мю, и мы также знаем ее стандартное отклонение и называем ее сигмой, тогда среднее значение суммы внизу будет мю. умножить на размер суммы, а стандартное отклонение будет равно сигме, умноженной на квадратный корень из этого размера. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1210.33,
  "end": 1227.73
 },
 {
  "input": "So now, if we want to claim that this looks more and more like a bell curve, and a bell curve is only described by two different parameters, the mean and the standard deviation, you know what to do. ",
  "translatedText": "Итак, теперь, если мы хотим заявить, что это все больше и больше похоже на колоколообразную кривую, а колоколообразная кривая описывается только двумя разными параметрами: средним значением и стандартным отклонением, вы знаете, что делать. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1228.19,
  "end": 1237.71
 },
 {
  "input": "You could plug those two values into the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve that should closely fit our distribution. ",
  "translatedText": "Вы можете подставить эти два значения в формулу, и это даст вам весьма явную, хотя и довольно сложную формулу для кривой, которая должна точно соответствовать нашему распределению. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1237.93,
  "end": 1246.99
 },
 {
  "input": "But there's another way we can describe it that's a little more elegant and lends itself to a very fun visual that we can build up to. ",
  "translatedText": "Но есть и другой способ описать это, более элегантный и создающий очень забавный визуальный эффект, который мы можем создать. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1248.39,
  "end": 1254.81
 },
 {
  "input": "Instead of focusing on the sum of all of these random variables, let's modify this expression a little bit, where what we'll do is we'll look at the mean that we expect that sum to take, and we subtract it off so that our new expression has a mean of 0, and then we're going to look at the standard deviation we expect of our sum, and divide out by that, which basically just rescales the units so that the standard deviation of our expression will equal 1. ",
  "translatedText": "Вместо того, чтобы сосредотачиваться на сумме всех этих случайных величин, давайте немного изменим это выражение: мы посмотрим на среднее значение, которое, как мы ожидаем, примет эта сумма, и вычтем его так, чтобы наше новое выражение имеет среднее значение 0, а затем мы посмотрим на стандартное отклонение, которое мы ожидаем от нашей суммы, и разделим на него, что по сути просто меняет масштаб единиц, так что стандартное отклонение нашего выражения будет равно 1. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1255.27,
  "end": 1278.77
 },
 {
  "input": "This might seem like a more complicated expression, but it actually has a highly readable meaning. ",
  "translatedText": "Это выражение может показаться более сложным, но на самом деле оно имеет очень читаемый смысл. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1279.35,
  "end": 1284.09
 },
 {
  "input": "It's essentially saying how many standard deviations away from the mean is this sum? ",
  "translatedText": "По сути, это вопрос о том, на сколько стандартных отклонений от среднего значения находится эта сумма? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1284.45,
  "end": 1289.67
 },
 {
  "input": "For example, this bar here corresponds to a certain value that you might find when you roll 10 dice and you add them all up, and its position a little above negative 1 is telling you that that value is a little bit less than one standard deviation lower than the mean. ",
  "translatedText": "Например, эта полоса здесь соответствует определенному значению, которое вы можете найти, когда бросаете 10 игральных костей и суммируете их все, и ее положение немного выше отрицательной 1 говорит вам, что это значение немного меньше одного стандартного отклонения. ниже среднего. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.75,
  "end": 1303.87
 },
 {
  "input": "Also, by the way, in anticipation for the animation I'm trying to build to here, the way I'm representing things on that lower plot is that the area of each one of these bars is telling us the probability of the corresponding value rather than the height. ",
  "translatedText": "Кроме того, кстати, в предвкушении анимации, которую я пытаюсь здесь построить, я представляю вещи на нижнем графике так: площадь каждой из этих полос говорит нам о вероятности соответствующего значения. а не высота. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1305.13,
  "end": 1316.99
 },
 {
  "input": "You might think of the y-axis as representing not probability but a kind of probability density. ",
  "translatedText": "Вы можете подумать, что ось Y представляет собой не вероятность, а своего рода плотность вероятности. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.23,
  "end": 1321.93
 },
 {
  "input": "The reason for this is to set the stage so that it aligns with the way we interpret continuous distributions, where the probability of falling between a range of values is equal to an area under a curve between those values. ",
  "translatedText": "Причина этого в том, чтобы подготовить почву так, чтобы она соответствовала тому, как мы интерпретируем непрерывные распределения, где вероятность попадания в диапазон значений равна площади под кривой между этими значениями. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1322.27,
  "end": 1333.55
 },
 {
  "input": "In particular, the area of all the bars together is going to be 1. ",
  "translatedText": "В частности, площадь всех столбцов вместе будет равна 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1333.91,
  "end": 1336.73
 },
 {
  "input": "Now, with all of that in place, let's have a little fun. ",
  "translatedText": "Теперь, когда все это готово, давайте немного повеселимся. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1338.23,
  "end": 1340.95
 },
 {
  "input": "Let me start by rolling things back so that the distribution on the bottom represents a relatively small sum, like adding together only three such random variables. ",
  "translatedText": "Позвольте мне начать с отката назад, чтобы распределение внизу представляло собой относительно небольшую сумму, как если бы вы сложили вместе только три таких случайных величины. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1341.33,
  "end": 1349.01
 },
 {
  "input": "Notice what happens as I change the distribution we start with. ",
  "translatedText": "Обратите внимание, что происходит, когда я меняю дистрибутив, с которого мы начали. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.45,
  "end": 1352.43
 },
 {
  "input": "As it changes, the distribution on the bottom completely changes its shape. ",
  "translatedText": "При его изменении распределение внизу полностью меняет свою форму. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1352.73,
  "end": 1356.29
 },
 {
  "input": "It's very dependent on what we started with. ",
  "translatedText": "Это очень зависит от того, с чего мы начали. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1356.51,
  "end": 1358.77
 },
 {
  "input": "If we let the size of our sum get a little bit bigger, say going up to 10, and as I change the distribution for x, it largely stays looking like a bell curve, but I can find some distributions that get it to change shape. ",
  "translatedText": "Если мы позволим размеру нашей суммы стать немного больше, скажем, до 10, и когда я изменю распределение для x, она в основном останется похожей на колоколообразную кривую, но я могу найти некоторые распределения, которые заставят ее изменить форму. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.35,
  "end": 1371.63
 },
 {
  "input": "For example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results in this kind of spiky bell curve, and if you'll recall, earlier on I actually showed this in the form of a simulation. ",
  "translatedText": "Например, действительно однобокий вариант, где почти вся вероятность заключена в числах 1 или 6, приводит к такой остроконечной колоколообразной кривой, и, если вы помните, ранее я фактически показал это в форме моделирования. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1372.23,
  "end": 1383.51
 },
 {
  "input": "So if you were wondering whether that spikiness was an artifact of the randomness or reflected the true distribution, turns out it reflects the true distribution. ",
  "translatedText": "Итак, если вам интересно, была ли эта остроконечность артефактом случайности или отражала истинное распределение, оказывается, она отражает истинное распределение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1384.13,
  "end": 1391.85
 },
 {
  "input": "In this case, 10 is not a large enough sum for the central limit theorem to kick in. ",
  "translatedText": "В этом случае 10 — недостаточно большая сумма для того, чтобы сработала центральная предельная теорема. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.29,
  "end": 1396.47
 },
 {
  "input": "But if instead I let that sum grow and I consider adding 50 different values, which is actually not that big, then no matter how I change the distribution for our underlying random variable, it has essentially no effect on the shape of the plot on the bottom. ",
  "translatedText": "Но если вместо этого я позволю этой сумме расти и рассмотрю возможность добавления 50 различных значений, что на самом деле не так уж и много, то независимо от того, как я изменю распределение нашей базовой случайной величины, это по существу не повлияет на форму графика на нижний. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1396.47,
  "end": 1410.69
 },
 {
  "input": "No matter where we start, all of the information and nuance for the distribution of x gets washed away, and we tend towards this single universal shape described by a very elegant function for the standard normal distribution, 1 over square root of 2 pi times e to the negative x squared over 2. ",
  "translatedText": "Независимо от того, с чего мы начинаем, вся информация и нюансы распределения x смываются, и мы склоняемся к этой единственной универсальной форме, описываемой очень элегантной функцией для стандартного нормального распределения: 1 вместо квадратного корня из 2 пи, умноженных на e. к отрицательному х в квадрате более 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1411.17,
  "end": 1427.07
 },
 {
  "input": "This, this right here is what the central limit theorem is all about. ",
  "translatedText": "Вот в этом-то и состоит суть центральной предельной теоремы. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1427.81,
  "end": 1430.81
 },
 {
  "input": "Almost nothing you can do to this initial distribution changes the shape we tend towards. ",
  "translatedText": "Почти ничего, что вы можете сделать с этим начальным распределением, не меняет форму, к которой мы стремимся. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1431.13,
  "end": 1435.31
 },
 {
  "input": "Now, the more theoretically minded among you might still be wondering, what is the actual theorem? ",
  "translatedText": "Те из вас, кто более склонен к теории, возможно, все еще задаются вопросом, какова на самом деле теорема? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1439.03,
  "end": 1444.51
 },
 {
  "input": "Like, what's the mathematical statement that could be proved or disproved that we're claiming here? ",
  "translatedText": "Например, какое математическое утверждение мы здесь утверждаем, которое можно доказать или опровергнуть? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1444.81,
  "end": 1448.91
 },
 {
  "input": "If you want a nice formal statement, here's how it might go. ",
  "translatedText": "Если вам нужно красивое официальное заявление, вот как это может быть. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1449.03,
  "end": 1451.67
 },
 {
  "input": "Consider this value, where we're summing up n different instantiations of our random variable, but tweaked and tuned so that its mean and standard deviation are 1. ",
  "translatedText": "Рассмотрим это значение, где мы суммируем n различных экземпляров нашей случайной величины, но изменены и настроены так, что ее среднее значение и стандартное отклонение равны 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1452.13,
  "end": 1459.89
 },
 {
  "input": "Again, meaning you can read it as asking how many standard deviations away from the mean is the sum. ",
  "translatedText": "Опять же, это означает, что вы можете прочитать это как вопрос, на сколько стандартных отклонений от среднего составляет сумма. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1460.23,
  "end": 1465.35
 },
 {
  "input": "Then the actual rigorous no-jokes-this-time statement of the central limit theorem is that if you consider the probability that this value falls between two given real numbers, a and b, and you consider the limit of that probability as the size of your sum goes to infinity, then that limit is equal to a certain integral, which basically describes the area under a standard normal distribution between those two values. ",
  "translatedText": "Тогда фактическая строгая формулировка центральной предельной теоремы (на этот раз без шуток): если вы рассматриваете вероятность того, что это значение попадает между двумя заданными действительными числами, a и b, и вы рассматриваете предел этой вероятности как размер ваша сумма стремится к бесконечности, тогда этот предел равен определенному интегралу, который в основном описывает площадь при стандартном нормальном распределении между этими двумя значениями. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1465.77,
  "end": 1489.65
 },
 {
  "input": "Again, there are three underlying assumptions that I have yet to tell you, but other than those, in all of its gory detail, this right here is the central limit theorem. ",
  "translatedText": "Опять же, есть три основных предположения, о которых мне еще предстоит вам рассказать, но кроме них, во всех кровавых подробностях, вот это и есть центральная предельная теорема. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1491.25,
  "end": 1500.03
 },
 {
  "input": "All of that is a bit theoretical, so it might be helpful to bring things back down to Earth and turn back to the concrete example that I mentioned at the start, where you imagine rolling a die 100 times, and let's assume it's a fair die for this example, and you add together the results. ",
  "translatedText": "Все это немного теоретически, поэтому было бы полезно вернуться на Землю и вернуться к конкретному примеру, который я упомянул в начале, где вы представляете, что бросаете игральную кость 100 раз, и давайте предположим, что это справедливый результат. для этого примера, и вы суммируете результаты. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1504.55,
  "end": 1518.13
 },
 {
  "input": "The challenge for you is to find a range of values such that you're 95% sure that the sum will fall within this range. ",
  "translatedText": "Ваша задача — найти такой диапазон значений, в котором вы на 95 % уверены, что сумма будет находиться в этом диапазоне. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1518.87,
  "end": 1525.83
 },
 {
  "input": "For questions like this, there's a handy rule of thumb about normal distributions, which is that about 68% of your values are going to fall within one standard deviation of the mean, 95% of your values, the thing we care about, fall within two standard deviations of the mean, and a whopping 99.7% of your values will fall within three standard deviations of the mean. ",
  "translatedText": "Для подобных вопросов существует удобное эмпирическое правило нормального распределения, которое гласит, что около 68% ваших значений будут находиться в пределах одного стандартного отклонения от среднего значения, а 95% ваших значений, то, что нас волнует, попадают в этот диапазон. два стандартных отклонения среднего значения и колоссальные 99.7% ваших значений будут находиться в пределах трех стандартных отклонений от среднего значения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1527.13,
  "end": 1546.97
 },
 {
  "input": "It's a rule of thumb that's commonly memorized by people who do a lot of probability and stats. ",
  "translatedText": "Это практическое правило, которое обычно запоминают люди, которые много занимаются вероятностями и статистикой. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1547.45,
  "end": 1551.45
 },
 {
  "input": "Naturally, this gives us what we need for our example, and let me go ahead and draw out what this would look like, where I'll show the distribution for a fair die up at the top, and the distribution for a sum of 100 such dice on the bottom, which by now as you know looks like a certain normal distribution. ",
  "translatedText": "Естественно, это дает нам то, что нам нужно для нашего примера, и позвольте мне нарисовать, как это будет выглядеть, где я покажу распределение для справедливого кубика вверху и распределение для суммы 100. такие игральные кости внизу, которые, как вы знаете, теперь выглядят как некое нормальное распределение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.49,
  "end": 1567.29
 },
 {
  "input": "Step one with a problem like this is to find the mean of your initial distribution, which in this case will look like 1 6th times 1 plus 1 6th times 2 on and on and on, and works out to be 3.5. ",
  "translatedText": "Первый шаг в решении подобной проблемы — найти среднее значение вашего начального распределения, которое в данном случае будет выглядеть как 16-е, умноженное на 1, плюс 16-е, умноженное на 2, и так далее, и в итоге будет равно 3.5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1567.95,
  "end": 1578.91
 },
 {
  "input": "We also need the standard deviation, which requires calculating the variance, which as you know involves adding all the squares of the differences between the values and the means, and it works out to be 2.92, square root of that comes out to be 1.71. ",
  "translatedText": "Нам также нужно стандартное отклонение, для которого требуется вычислить дисперсию, которая, как вы знаете, включает в себя сложение всех квадратов разностей между значениями и средними значениями, и в результате получается 2.92, квадратный корень из которого равен 1.71. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1579.41,
  "end": 1592.43
 },
 {
  "input": "Those are the only two numbers we need, and I will invite you again to reflect on how magical it is that those are the only two numbers that you need to completely understand the bottom distribution. ",
  "translatedText": "Это единственные два числа, которые нам нужны, и я снова приглашаю вас задуматься о том, насколько волшебно то, что это единственные два числа, которые вам нужны для полного понимания нижнего распределения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1592.95,
  "end": 1601.69
 },
 {
  "input": "Its mean will be 100 times mu, which is 350, and its standard deviation will be the square root of 100 times sigma, so 10 times sigma 17.1. ",
  "translatedText": "Его среднее значение будет в 100 раз мю, что равно 350, а его стандартное отклонение будет равно квадратному корню из 100-кратного сигмы, то есть 10-кратного сигмы 17. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1602.43,
  "end": 1612.61
 },
 {
  "input": "Remembering our handy rule of thumb, we're looking for values two standard deviations away from the mean, and when you subtract 2 sigma from the mean you end up with about 316, and when you add 2 sigma you end up with 384. ",
  "translatedText": "1. Помня наше удобное эмпирическое правило, мы ищем значения, отстоящие на два стандартных отклонения от среднего значения, и когда вы вычитаете 2 сигмы из среднего значения, вы получаете примерно 316, а когда вы добавляете 2 сигмы, вы получаете 384. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1613.03,
  "end": 1626.33
 },
 {
  "input": "And there you go, that gives us the answer. ",
  "translatedText": "И вот, это дает нам ответ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1627.35,
  "end": 1628.95
 },
 {
  "input": "Okay, I promised to wrap things up shortly, but while we're on this example there's one more question that's worth your time to ponder. ",
  "translatedText": "Хорошо, я обещал подвести итоги в ближайшее время, но пока мы рассматриваем этот пример, есть еще один вопрос, на который стоит потратить время. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.47,
  "end": 1637.45
 },
 {
  "input": "Instead of just asking about the sum of 100 die rolls, let's say I had you divide that number by 100, which basically means all the numbers in our diagram in the bottom get divided by 100. ",
  "translatedText": "Вместо того, чтобы просто спрашивать о сумме 100 бросков кубика, предположим, я попросил вас разделить это число на 100, что по сути означает, что все числа на нашей диаграмме внизу делятся на 100. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1638.25,
  "end": 1648.09
 },
 {
  "input": "Take a moment to interpret what this all would be saying then. ",
  "translatedText": "Найдите минутку, чтобы интерпретировать, о чем все это говорит тогда. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1648.57,
  "end": 1651.57
 },
 {
  "input": "The expression essentially tells you the empirical average for 100 different die rolls, and that interval we found is now telling you what range you are expecting to see for that empirical average. ",
  "translatedText": "По сути, это выражение сообщает вам эмпирическое среднее значение для 100 различных бросков кубика, и найденный нами интервал теперь говорит вам, какой диапазон вы ожидаете увидеть для этого эмпирического среднего значения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1652.07,
  "end": 1663.49
 },
 {
  "input": "In other words, you might expect it to be around 3.5, that's the expected value for a die roll, but what's much less obvious and what the central limit theorem lets you compute is how close to that expected value you'll reasonably find yourself. ",
  "translatedText": "Другими словами, вы можете ожидать, что оно будет около 3.5, это ожидаемое значение для броска кубика, но что гораздо менее очевидно и что позволяет вычислить центральная предельная теорема, так это то, насколько близко к этому ожидаемому значению вы окажетесь. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1664.35,
  "end": 1676.57
 },
 {
  "input": "In particular, it's worth your time to take a moment mulling over what the standard deviation for this empirical average is, and what happens to it as you look at a bigger and bigger sample of die rolls. ",
  "translatedText": "В частности, стоит уделить время размышлениям о том, каково стандартное отклонение для этого эмпирического среднего значения и что с ним происходит, когда вы смотрите на все большую и большую выборку бросков кубика. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1677.59,
  "end": 1687.13
 },
 {
  "input": "Lastly, but probably most importantly, let's talk about the assumptions that go into this theorem. ",
  "translatedText": "И наконец, но, вероятно, самое главное: давайте поговорим о предположениях, лежащих в основе этой теоремы. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.95,
  "end": 1697.41
 },
 {
  "input": "The first one is that all of these variables that we're adding up are independent from each other. ",
  "translatedText": "Во-первых, все эти переменные, которые мы суммируем, независимы друг от друга. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1698.01,
  "end": 1702.53
 },
 {
  "input": "The outcome of one process doesn't influence the outcome of any other process. ",
  "translatedText": "Результат одного процесса не влияет на результат любого другого процесса. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.85,
  "end": 1706.31
 },
 {
  "input": "The second is that all of these variables are drawn from the same distribution. ",
  "translatedText": "Во-вторых, все эти переменные взяты из одного и того же распределения. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1707.25,
  "end": 1710.95
 },
 {
  "input": "Both of these have been implicitly assumed with our dice example. ",
  "translatedText": "Оба этих условия неявно предполагались в нашем примере с игральными костями. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1711.31,
  "end": 1714.39
 },
 {
  "input": "We've been treating the outcome of each die roll as independent from the outcome of all the others, and we're assuming that each die follows the same distribution. ",
  "translatedText": "Мы рассматривали результат каждого броска кубика как независимый от результата всех остальных и предполагаем, что каждый кубик имеет одинаковое распределение. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1714.79,
  "end": 1722.03
 },
 {
  "input": "Sometimes in the literature you'll see these two assumptions lumped together under the initials IID for independent and identically distributed. ",
  "translatedText": "Иногда в литературе вы встретите эти два предположения, объединенные инициалами IID, обозначающими независимые и одинаково распределенные. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1722.85,
  "end": 1729.91
 },
 {
  "input": "One situation where these assumptions are decidedly not true would be the Galton board. ",
  "translatedText": "Одной из ситуаций, когда эти предположения явно неверны, является совет директоров Гальтона. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1730.53,
  "end": 1735.11
 },
 {
  "input": "I mean, think about it. ",
  "translatedText": "Я имею в виду, подумай об этом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.71,
  "end": 1736.83
 },
 {
  "input": "Is it the case that the way a ball bounces off of one of the pegs is independent from how it's going to bounce off the next peg? ",
  "translatedText": "Действительно ли то, как мяч отскочит от одного из колышков, не зависит от того, как он отскочит от следующего колышка? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1736.97,
  "end": 1743.19
 },
 {
  "input": "Absolutely not. ",
  "translatedText": "Точно нет. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1743.83,
  "end": 1744.61
 },
 {
  "input": "Depending on the last bounce, it's coming in with a completely different trajectory. ",
  "translatedText": "В зависимости от последнего отскока он движется по совершенно разной траектории. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1744.77,
  "end": 1747.87
 },
 {
  "input": "And is it the case that the distribution of possible outcomes off of each peg are the same for each peg that it hits? ",
  "translatedText": "И действительно ли распределение возможных результатов для каждой привязки одинаково для каждой привязки, к которой она попадает? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1748.21,
  "end": 1754.67
 },
 {
  "input": "Again, almost certainly not. ",
  "translatedText": "Опять же, почти наверняка нет. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1755.19,
  "end": 1756.71
 },
 {
  "input": "Maybe it hits one peg glancing to the left, meaning the outcomes are hugely skewed in that direction, and then hits the next one glancing to the right. ",
  "translatedText": "Может быть, он попадает в одну точку, глядя влево, а это означает, что результаты сильно искажаются в этом направлении, а затем попадает в следующую точку, глядя вправо. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1756.71,
  "end": 1763.71
 },
 {
  "input": "When I made all those simplifying assumptions in the opening example, it wasn't just to make this easier to think about. ",
  "translatedText": "Когда я сделал все эти упрощающие предположения в первом примере, я делал это не только для того, чтобы об этом было легче думать. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1765.73,
  "end": 1771.63
 },
 {
  "input": "It's also that those assumptions were necessary for this to actually be an example of the central limit theorem. ",
  "translatedText": "Кроме того, эти предположения были необходимы для того, чтобы это действительно было примером центральной предельной теоремы. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1771.97,
  "end": 1777.07
 },
 {
  "input": "Nevertheless, it seems to be true that for the real Galton board, despite violating both of these, a normal distribution does kind of come about? ",
  "translatedText": "Тем не менее, похоже, что для реальной доски Гальтона, несмотря на нарушение обоих условий, нормальное распределение все-таки получается? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1778.13,
  "end": 1785.47
 },
 {
  "input": "Part of the reason might be that there are generalizations of the theorem beyond the scope of this video that relax these assumptions, especially the second one. ",
  "translatedText": "Частично причина может заключаться в том, что существуют обобщения теоремы, выходящие за рамки этого видео, которые ослабляют эти предположения, особенно второе. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1786.05,
  "end": 1793.89
 },
 {
  "input": "But I do want to caution you against the fact that many times people seem to assume that a variable is normally distributed, even when there's no actual justification to do so. ",
  "translatedText": "Но я хочу предостеречь вас от того факта, что часто люди предполагают, что переменная распределяется нормально, даже если для этого нет реального обоснования. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1794.49,
  "end": 1803.07
 },
 {
  "input": "The third assumption is actually fairly subtle. ",
  "translatedText": "Третье предположение на самом деле довольно тонкое. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1804.29,
  "end": 1806.21
 },
 {
  "input": "It's that the variance we've been computing for these variables is finite. ",
  "translatedText": "Дело в том, что дисперсия, которую мы вычисляли для этих переменных, конечна. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1806.21,
  "end": 1810.27
 },
 {
  "input": "This was never an issue for the dice example, because there were only six possible outcomes. ",
  "translatedText": "В примере с игральными костями это никогда не было проблемой, поскольку возможных исходов было только шесть. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1810.81,
  "end": 1814.85
 },
 {
  "input": "But in certain situations where you have an infinite set of outcomes, when you go to compute the variance, the sum ends up diverging off to infinity. ",
  "translatedText": "Но в некоторых ситуациях, когда у вас есть бесконечный набор результатов, когда вы начинаете вычислять дисперсию, сумма в конечном итоге расходится до бесконечности. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1815.03,
  "end": 1822.51
 },
 {
  "input": "These can be perfectly valid probability distributions, and they do come up in practice. ",
  "translatedText": "Это могут быть совершенно правильные распределения вероятностей, и они действительно встречаются на практике. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1823.45,
  "end": 1827.25
 },
 {
  "input": "But in those situations, as you consider adding many different instantiations of that variable and letting that sum approach infinity, even if the first two assumptions hold, it is very much a possibility that the thing you tend towards is not actually a normal distribution. ",
  "translatedText": "Но в таких ситуациях, когда вы рассматриваете возможность добавления множества различных экземпляров этой переменной и позволяете этой сумме приближаться к бесконечности, даже если первые два предположения верны, весьма вероятно, что то, к чему вы склонны, на самом деле не является нормальным распределением. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1827.55,
  "end": 1841.19
 },
 {
  "input": "If you've understood everything up to this point, you now have a very strong foundation in what the central limit theorem is all about. ",
  "translatedText": "Если вы все поняли до этого момента, то теперь у вас есть очень прочная основа в том, что такое центральная предельная теорема. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1842.15,
  "end": 1847.65
 },
 {
  "input": "And next up, I'd like to explain why it is that this particular function is the thing that we tend towards, and why it has a pi in it, what it has to do with circles. ",
  "translatedText": "И далее я хотел бы объяснить, почему именно эта функция является тем, к чему мы склонны, и почему в ней есть число «пи», какое отношение она имеет к кругам. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1848.29,
  "end": 1874.17
 }
]