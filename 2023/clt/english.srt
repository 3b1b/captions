1
00:00:00,000 --> 00:00:05,280
This is a Galton board. Maybe you've seen one before, it's a popular demonstration of how,

2
00:00:05,280 --> 00:00:10,400
even when a single event is chaotic and random, with an effectively unknowable outcome,

3
00:00:10,400 --> 00:00:14,480
it's still possible to make precise statements about a large number of events,

4
00:00:14,480 --> 00:00:18,320
namely how the relative proportions for many different outcomes are distributed.

5
00:00:20,560 --> 00:00:24,560
More specifically, the Galton board illustrates one of the most prominent distributions in all

6
00:00:24,560 --> 00:00:29,840
of probability, known as the normal distribution, more colloquially known as a bell curve,

7
00:00:29,840 --> 00:00:34,480
and also called a Gaussian distribution. There's a very specific function to describe this

8
00:00:34,480 --> 00:00:38,800
distribution, it's very pretty, we'll get into it later, but right now I just want to emphasize

9
00:00:38,800 --> 00:00:43,200
how the normal distribution is, as the name suggests, very common, it shows up in a lot

10
00:00:43,200 --> 00:00:48,480
of seemingly unrelated contexts. If you were to take a large number of people who sit in a similar

11
00:00:48,480 --> 00:00:54,240
demographic and plot their heights, those heights tend to follow a normal distribution. If you look

12
00:00:54,240 --> 00:00:59,600
at a large swath of very big natural numbers and you ask how many distinct prime factors does each

13
00:00:59,600 --> 00:01:05,280
one of those numbers have, the answers will very closely track with a certain normal distribution.

14
00:01:05,280 --> 00:01:10,240
Now our topic for today is one of the crown jewels in all of probability theory, it's one of the key

15
00:01:10,240 --> 00:01:15,600
facts that explains why this distribution is as common as it is, known as the central limit

16
00:01:15,600 --> 00:01:20,240
theorem. This lesson is meant to go back to the basics, giving you the fundamentals on what the

17
00:01:20,240 --> 00:01:24,160
central limit theorem is saying, what normal distributions are, and I want to assume minimal

18
00:01:24,160 --> 00:01:29,280
background. We're going to go decently deep into it, but after this I'd still like to go deeper and

19
00:01:29,280 --> 00:01:34,800
explain why the theorem is true, why the function underlying the normal distribution has the very

20
00:01:34,800 --> 00:01:41,920
specific form that it does, why that formula has a pi in it, and, most fun, why those last two facts

21
00:01:41,920 --> 00:01:47,600
are actually more related than a lot of traditional explanations would suggest. That second lesson is

22
00:01:47,600 --> 00:01:52,000
also meant to be the follow-on to the convolutions video that I promised, so there's a lot of

23
00:01:52,000 --> 00:01:56,400
interrelated topics here. But right now, back to the fundamentals, I'd like to kick things off with

24
00:01:56,400 --> 00:02:03,440
a overly simplified model of the Galton board. In this model we will assume that each ball falls

25
00:02:03,440 --> 00:02:08,720
directly onto a certain central peg and that it has a 50-50 probability of bouncing to the left

26
00:02:08,720 --> 00:02:13,200
or to the right, and we'll think of each of those outcomes as either adding one or subtracting one

27
00:02:13,200 --> 00:02:18,800
from its position. Once one of those is chosen, we make the highly unrealistic assumption that

28
00:02:18,800 --> 00:02:23,760
it happens to land dead on in the middle of the peg adjacent below it, where again it'll be faced

29
00:02:23,760 --> 00:02:28,640
with the same 50-50 choice of bouncing to the left or to the right. For the one I'm showing on screen,

30
00:02:28,640 --> 00:02:33,360
there are five different rows of pegs, so our little hopping ball makes five different random

31
00:02:33,360 --> 00:02:38,480
choices between plus one and minus one, and we can think of its final position as basically being the

32
00:02:38,480 --> 00:02:43,760
sum of all of those different numbers, which in this case happens to be one, and we might label

33
00:02:43,760 --> 00:02:48,000
all of the different buckets with the sum that they represent. As we repeat this, we're looking

34
00:02:48,000 --> 00:02:54,400
at different possible sums for those five random numbers. And for those of you who are inclined to

35
00:02:54,400 --> 00:02:59,120
complain that this is a highly unrealistic model for the true Galton board, let me emphasize the

36
00:02:59,120 --> 00:03:04,480
goal right now is not to accurately model physics. The goal is to give a simple example to illustrate

37
00:03:04,480 --> 00:03:09,200
the central limit theorem, and for that, idealized though this might be, it actually gives us a

38
00:03:09,200 --> 00:03:14,480
really good example. If we let many different balls fall, making yet another unrealistic assumption

39
00:03:14,480 --> 00:03:18,960
that they don't influence each other as if they're all ghosts, then the number of balls that fall into

40
00:03:18,960 --> 00:03:23,680
each different bucket gives us some loose sense for how likely each one of those buckets is.

41
00:03:23,680 --> 00:03:27,840
In this example, the numbers are simple enough that it's not too hard to explicitly calculate

42
00:03:27,840 --> 00:03:31,280
what the probability is for falling into each bucket. If you do want to think that through,

43
00:03:31,280 --> 00:03:35,840
you'll find it very reminiscent of Pascal's triangle. But the neat thing about our theorem

44
00:03:35,840 --> 00:03:40,800
is how far it goes beyond the simple examples. So to start off at least, rather than making explicit

45
00:03:40,800 --> 00:03:45,360
calculations, let's just simulate things by running a large number of samples and letting the total

46
00:03:45,360 --> 00:03:49,600
number of results in each different outcome give us some sense for what that distribution looks

47
00:03:49,600 --> 00:03:55,440
like. As I said, the one on screen has five rows, so each sum that we're considering includes only

48
00:03:55,440 --> 00:04:01,040
five numbers. The basic idea of the central limit theorem is that if you increase the size of that

49
00:04:01,040 --> 00:04:06,480
sum, for example here that would mean increasing the number of rows of pegs for each ball to bounce

50
00:04:06,480 --> 00:04:12,640
off, then the distribution that describes where that sum is going to fall looks more and more like

51
00:04:12,640 --> 00:04:19,760
a bell curve. Here, it's actually worth taking a moment to write down that general idea. The setup

52
00:04:19,760 --> 00:04:25,120
is that we have a random variable, and that's basically shorthand for a random process where

53
00:04:25,120 --> 00:04:30,240
each outcome of that process is associated with some number. We'll call that random number x. For

54
00:04:30,240 --> 00:04:35,200
example, each bounce off the peg is a random process modeled with two outcomes. Those outcomes

55
00:04:35,200 --> 00:04:39,920
are associated with the numbers negative one and positive one. Another example of a random variable

56
00:04:39,920 --> 00:04:44,400
would be rolling a die, where you have six different outcomes, each one associated with a

57
00:04:44,400 --> 00:04:49,760
number. What we're doing is taking multiple different samples of that variable and adding them

58
00:04:49,760 --> 00:04:54,640
all together. On our Galton board, that looks like letting the ball bounce off multiple different pegs

59
00:04:54,640 --> 00:04:59,760
on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice

60
00:04:59,840 --> 00:05:04,880
and adding up the results. The claim of the central limit theorem is that as you let the size of that

61
00:05:04,880 --> 00:05:10,400
sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into

62
00:05:10,400 --> 00:05:16,720
different possible values, will look more and more like a bell curve. That's it, that is the general

63
00:05:16,720 --> 00:05:22,080
idea. Over the course of this lesson, our job is to make that statement more quantitative. We're

64
00:05:22,080 --> 00:05:26,400
going to put some numbers to it, put some formulas to it, show how you can use it to make predictions.

65
00:05:27,360 --> 00:05:31,440
For example, here's the kind of question I want you to be able to answer by the end of this video.

66
00:05:32,080 --> 00:05:37,840
Suppose you rolled the die 100 times and you added together the results. Could you find a range of

67
00:05:37,840 --> 00:05:43,840
values such that you're 95% sure that the sum will fall within that range? Or maybe I should say find

68
00:05:43,840 --> 00:05:48,560
the smallest possible range of values such that this is true. The neat thing is you'll be able to

69
00:05:48,560 --> 00:05:54,480
answer this question whether it's a fair die or if it's a weighted die. Now let me say at the top

70
00:05:54,480 --> 00:05:58,880
that this theorem has three different assumptions that go into it, three things that have to be true

71
00:05:58,880 --> 00:06:03,040
before the theorem follows. And I'm actually not going to tell you what they are until the very

72
00:06:03,040 --> 00:06:07,200
end of the video. Instead I want you to keep your eye out and see if you can notice and maybe

73
00:06:07,200 --> 00:06:12,480
predict what those three assumptions are going to be. As a next step, to better illustrate just how

74
00:06:12,480 --> 00:06:16,880
general this theorem is, I want to run a couple more simulations for you focused on the dice

75
00:06:16,880 --> 00:06:24,880
example. Usually if you think of rolling a die you think of the six outcomes as being equally

76
00:06:24,880 --> 00:06:29,600
probable, but the theorem actually doesn't care about that. We could start with a weighted die,

77
00:06:29,600 --> 00:06:34,960
something with a non-trivial distribution across the outcomes, and the core idea still holds. For

78
00:06:34,960 --> 00:06:39,120
the simulation what I'll do is take some distribution like this one that is skewed towards

79
00:06:39,120 --> 00:06:45,040
lower values. I'm going to take 10 distinct samples from that distribution and then I'll record the

80
00:06:45,040 --> 00:06:50,320
sum of that sample on the plot on the bottom. Then I'm going to do this many many different

81
00:06:50,320 --> 00:06:55,360
times, always with a sum of size 10, but keep track of where those sums ended up to give us

82
00:06:55,360 --> 00:07:03,200
a sense of the distribution. And in fact let me rescale the y direction to give us room to run

83
00:07:03,200 --> 00:07:08,000
an even larger number of samples. And I'll let it go all the way up to a couple thousand, and as it

84
00:07:08,000 --> 00:07:13,680
does you'll notice that the shape that starts to emerge looks like a bell curve. Maybe if you squint

85
00:07:13,680 --> 00:07:18,560
your eyes you can see it skews a tiny bit to the left, but it's neat that something so symmetric

86
00:07:18,560 --> 00:07:22,720
emerged from a starting point that was so asymmetric. To better illustrate what the central

87
00:07:22,720 --> 00:07:27,840
limit theorem is all about, let me run four of these simulations in parallel, where on the upper

88
00:07:27,840 --> 00:07:32,480
left I'm doing it where we're only adding two dice at a time, on the upper right we're doing it

89
00:07:32,480 --> 00:07:37,200
where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at

90
00:07:37,200 --> 00:07:43,440
a time, and then we'll do another one with a bigger sum, 15 at a time. Notice how on the upper left

91
00:07:43,440 --> 00:07:48,160
when we're just adding two dice, the resulting distribution doesn't really look like a bell curve,

92
00:07:48,160 --> 00:07:51,920
it looks a lot more reminiscent of the one we started with skewed towards the left.

93
00:07:52,640 --> 00:07:57,600
But as we allow for more and more dice in each sum, the resulting shape that comes up in these

94
00:07:57,600 --> 00:08:02,160
distributions looks more and more symmetric. It has the lump in the middle and fade towards the

95
00:08:02,160 --> 00:08:10,640
tail's shape of a bell curve. And let me emphasize again, you can start with any different distribution.

96
00:08:10,640 --> 00:08:15,200
Here I'll run it again, but where most of the probability is tied up in the numbers 1 and 6,

97
00:08:15,200 --> 00:08:20,160
with very low probability for the mid values. Despite completely changing the distribution

98
00:08:20,160 --> 00:08:25,200
for an individual roll of the die, it's still the case that a bell curve shape will emerge as we

99
00:08:25,200 --> 00:08:30,400
consider the different sums. Illustrating things with a simulation like this is very fun, and it's

100
00:08:30,400 --> 00:08:35,840
kind of neat to see order emerge from chaos, but it also feels a little imprecise. Like in this

101
00:08:35,840 --> 00:08:40,560
case, when I cut off the simulation at 3000 samples, even though it kind of looks like a bell

102
00:08:40,560 --> 00:08:45,520
curve, the different buckets seem pretty spiky. And you might wonder, is it supposed to look that

103
00:08:45,520 --> 00:08:50,560
way, or is that just an artifact of the randomness in the simulation? And if it is, how many samples

104
00:08:50,560 --> 00:08:55,040
do we need before we can be sure that what we're looking at is representative of the true distribution?

105
00:08:59,120 --> 00:09:03,280
Instead moving forward, let's get a little more theoretical and show the precise shape that these

106
00:09:03,360 --> 00:09:08,480
distributions will take on in the long run. The easiest case to make this calculation is if we

107
00:09:08,480 --> 00:09:13,760
have a uniform distribution, where each possible face of the die has an equal probability, 1 6th.

108
00:09:13,760 --> 00:09:17,840
For example, if you then want to know how likely different sums are for a pair of dice,

109
00:09:17,840 --> 00:09:23,520
it's essentially a counting game, where you count up how many distinct pairs take on the same sum,

110
00:09:23,520 --> 00:09:27,120
which in the diagram I've drawn, you can conveniently think about by going through

111
00:09:27,120 --> 00:09:34,080
all of the different diagonals. Since each such pair has an equal chance of showing up,

112
00:09:34,080 --> 00:09:39,600
1 in 36, all you have to do is count the sizes of these buckets. That gives us a definitive shape

113
00:09:39,600 --> 00:09:44,640
for the distribution describing a sum of two dice, and if we were to play the same game with all

114
00:09:44,640 --> 00:09:49,680
possible triplets, the resulting distribution would look like this. Now what's more challenging,

115
00:09:49,680 --> 00:09:54,320
but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that

116
00:09:54,320 --> 00:09:59,680
single die. We actually talked all about this in the last video. You do essentially the same

117
00:09:59,680 --> 00:10:04,480
thing, you go through all the distinct pairs of dice which add up to the same value. It's just

118
00:10:04,480 --> 00:10:09,760
that instead of counting those pairs, for each pair you multiply the two probabilities of each

119
00:10:09,760 --> 00:10:14,560
particular face coming up, and then you add all those together. The computation that does this

120
00:10:14,560 --> 00:10:19,280
for all possible sums has a fancy name, it's called a convolution, but it's essentially just

121
00:10:19,280 --> 00:10:23,680
the weighted version of the counting game that anyone who's played with a pair of dice already

122
00:10:23,680 --> 00:10:28,800
finds familiar. For our purposes in this lesson, I'll have the computer calculate all that,

123
00:10:28,800 --> 00:10:33,760
simply display the results for you, and invite you to observe certain patterns, but under the hood,

124
00:10:33,760 --> 00:10:39,760
this is what's going on. So just to be crystal clear on what's being represented here,

125
00:10:39,760 --> 00:10:44,400
if you imagine sampling two different values from that top distribution, the one describing

126
00:10:44,400 --> 00:10:49,920
a single die, and adding them together, then the second distribution I'm drawing represents how

127
00:10:49,920 --> 00:10:55,120
likely you are to see various different sums. Likewise, if you imagine sampling three distinct

128
00:10:55,120 --> 00:10:59,440
values from that top distribution, and adding them together, the next plot represents the

129
00:10:59,440 --> 00:11:06,080
probabilities for various different sums in that case. So if I compute what the distributions for

130
00:11:06,080 --> 00:11:11,360
these sums look like for larger and larger sums, well you know what I'm going to say, it looks more

131
00:11:11,360 --> 00:11:15,840
and more like a bell curve. But before we get to that, I want you to make a couple more simple

132
00:11:15,840 --> 00:11:21,680
observations. For example, these distributions seem to be wandering to the right, and also they

133
00:11:21,680 --> 00:11:26,480
seem to be getting more spread out, and a little bit more flat. You cannot describe the central

134
00:11:26,480 --> 00:11:30,640
limit theorem quantitatively without taking into account both of those effects, which in turn

135
00:11:30,640 --> 00:11:35,040
requires describing the mean and the standard deviation. Maybe you're already familiar with

136
00:11:35,040 --> 00:11:39,200
those, but I want to make minimal assumptions here, and it never hurts to review, so let's

137
00:11:39,200 --> 00:11:46,320
quickly go over both of those. The mean of a distribution, often denoted with the Greek

138
00:11:46,320 --> 00:11:51,840
letter mu, is a way of capturing the center of mass for that distribution. It's calculated as

139
00:11:51,840 --> 00:11:56,880
the expected value of our random variable, which is a way of saying you go through all of the

140
00:11:56,880 --> 00:12:02,320
different possible outcomes, and you multiply the probability of that outcome times the value of the

141
00:12:02,320 --> 00:12:07,200
variable. If higher values are more probable, that weighted sum is going to be bigger. If lower

142
00:12:07,200 --> 00:12:11,680
values are more probable, that weighted sum is going to be smaller. A little more interesting

143
00:12:11,680 --> 00:12:15,520
is if you want to measure how spread out this distribution is, because there's multiple

144
00:12:15,520 --> 00:12:21,920
different ways you might do it. One of them is called the variance. The idea there is to look

145
00:12:21,920 --> 00:12:27,360
at the difference between each possible value and the mean, square that difference, and ask for its

146
00:12:27,360 --> 00:12:32,480
expected value. The idea is that whether your value is below or above the mean, when you square

147
00:12:32,480 --> 00:12:36,560
that difference, you get a positive number, and the larger the difference, the bigger that number.

148
00:12:37,360 --> 00:12:41,200
Squaring it like this turns out to make the math much much nicer than if we did something like an

149
00:12:41,200 --> 00:12:46,560
absolute value, but the downside is that it's hard to think about this as a distance in our diagram

150
00:12:46,560 --> 00:12:51,280
because the units are off. Kind of like the units here are square units, whereas a distance in our

151
00:12:51,280 --> 00:12:56,480
diagram would be a kind of linear unit. So another way to measure spread is what's called the standard

152
00:12:56,480 --> 00:13:01,840
deviation, which is the square root of this value. That can be interpreted much more reasonably as a

153
00:13:01,840 --> 00:13:07,040
distance on our diagram, and it's commonly denoted with the Greek letter sigma, so you know m for

154
00:13:07,040 --> 00:13:14,240
mean as for standard deviation, but both in Greek. Looking back at our sequence of distributions,

155
00:13:14,240 --> 00:13:19,280
let's talk about the mean and standard deviation. If we call the mean of the initial distribution mu,

156
00:13:19,280 --> 00:13:24,240
which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if I tell you

157
00:13:24,240 --> 00:13:28,880
that the mean of the next one is 2 times mu. That is, you roll a pair of dice, you want to know the

158
00:13:28,880 --> 00:13:34,160
expected value of the sum, it's two times the expected value for a single die. Similarly,

159
00:13:34,160 --> 00:13:39,840
the expected value for our sum of size 3 is 3 times mu, and so on and so forth. The mean

160
00:13:39,840 --> 00:13:44,160
just marches steadily on to the right, which is why our distributions seem to be drifting off in

161
00:13:44,160 --> 00:13:48,640
that direction. A little more challenging, but very important, is to describe how the standard

162
00:13:48,640 --> 00:13:53,920
deviation changes. The key fact here is that if you have two different random variables, then the

163
00:13:53,920 --> 00:13:58,800
variance for the sum of those variables is the same as just adding together the original two

164
00:13:58,800 --> 00:14:03,920
variances. This is one of those facts that you can just compute when you unpack all the definitions.

165
00:14:03,920 --> 00:14:08,400
There are a couple nice intuitions for why it's true. My tentative plan is to just actually make

166
00:14:08,400 --> 00:14:12,720
a series about probability and talk about things like intuitions underlying variance and its

167
00:14:12,720 --> 00:14:17,280
cousins there. But right now, the main thing I want you to highlight is how it's the variance

168
00:14:17,280 --> 00:14:22,080
that adds, it's not the standard deviation that adds. So, critically, if you were to take n

169
00:14:22,080 --> 00:14:27,840
different realizations of the same random variable and ask what the sum looks like, the variance of

170
00:14:27,840 --> 00:14:34,000
that sum is n times the variance of your original variable, meaning the standard deviation, the

171
00:14:34,000 --> 00:14:39,760
square root of all this, is the square root of n times the original standard deviation. For example,

172
00:14:39,760 --> 00:14:44,160
back in our sequence of distributions, if we label the standard deviation of our initial one with

173
00:14:44,160 --> 00:14:49,680
sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after

174
00:14:49,680 --> 00:14:54,800
that it looks like the square root of 3 times sigma, and so on and so forth. This, like I said,

175
00:14:54,800 --> 00:14:59,440
is very important. It means that even though our distributions are getting spread out, they're not

176
00:14:59,440 --> 00:15:03,760
spreading out all that quickly, they only do so in proportion to the square root of the size of the

177
00:15:03,760 --> 00:15:08,640
sum. As we prepare to make a more quantitative description of the central limit theorem,

178
00:15:08,640 --> 00:15:13,200
the core intuition I want you to keep in your head is that we'll basically realign all of these

179
00:15:13,200 --> 00:15:18,480
distributions so that their means line up together, and then rescale them so that all of the standard

180
00:15:18,480 --> 00:15:23,840
deviations are just going to be equal to 1. And when we do that, the shape that results gets closer

181
00:15:23,840 --> 00:15:29,040
and closer to a certain universal shape, described with an elegant little function that we'll unpack

182
00:15:29,040 --> 00:15:34,240
in just a moment. And let me say one more time, the real magic here is how we could have started

183
00:15:34,240 --> 00:15:39,200
with any distribution, describing a single roll of the die, and if we play the same game,

184
00:15:39,200 --> 00:15:43,760
considering what the distributions for the many different sums look like, and we realign them so

185
00:15:43,760 --> 00:15:47,920
that the means line up, and we rescale them so that the standard deviations are all 1,

186
00:15:48,480 --> 00:15:52,800
we still approach that same universal shape, which is kind of mind-boggling.

187
00:15:55,040 --> 00:16:00,160
And now, my friends, is probably as good a time as any to finally get into the formula for a normal

188
00:16:00,160 --> 00:16:04,800
distribution. And the way I'd like to do this is to basically peel back all the layers and build it

189
00:16:04,800 --> 00:16:11,280
up one piece at a time. The function e to the x, or anything to the x, describes exponential growth,

190
00:16:11,280 --> 00:16:15,520
and if you make that exponent negative, which flips around the graph horizontally,

191
00:16:15,520 --> 00:16:20,000
you might think of it as describing exponential decay. To make this decay in both directions,

192
00:16:20,000 --> 00:16:23,440
you could do something to make sure the exponent is always negative and growing,

193
00:16:23,440 --> 00:16:28,160
like taking the negative absolute value. That would give us this kind of awkward sharp point

194
00:16:28,160 --> 00:16:32,240
in the middle, but if instead you make that exponent the negative square of x,

195
00:16:32,240 --> 00:16:36,240
you get a smoother version of the same thing, which decays in both directions.

196
00:16:36,240 --> 00:16:40,640
This gives us the basic bell curve shape. Now if you throw a constant in front of that x,

197
00:16:40,640 --> 00:16:45,440
and you scale that constant up and down, it lets you stretch and squish the graph horizontally,

198
00:16:45,440 --> 00:16:50,160
allowing you to describe narrow and wider bell curves. And a quick thing I'd like to point out

199
00:16:50,160 --> 00:16:56,080
here is that based on the rules of exponentiation, as we tweak around that constant c, you could also

200
00:16:56,080 --> 00:17:01,680
think about it as simply changing the base of the exponentiation. And in that sense, the number e is

201
00:17:01,680 --> 00:17:06,960
not really all that special for our formula. We could replace it with any other positive constant,

202
00:17:06,960 --> 00:17:12,640
and you'll get the same family of curves as we tweak that constant. Make it a 2, same family

203
00:17:12,640 --> 00:17:18,240
of curves. Make it a 3, same family of curves. The reason we use e is that it gives that constant

204
00:17:18,240 --> 00:17:23,280
a very readable meaning. Or rather, if we reconfigure things a little bit so that the

205
00:17:23,280 --> 00:17:28,160
exponent looks like negative one half times x divided by a certain constant, which we'll

206
00:17:28,160 --> 00:17:33,520
suggestively call sigma squared, then once we turn this into a probability distribution,

207
00:17:33,520 --> 00:17:38,880
that constant sigma will be the standard deviation of that distribution. And that's very nice. But

208
00:17:38,880 --> 00:17:43,600
before we can interpret this as a probability distribution, we need the area under the curve

209
00:17:43,600 --> 00:17:48,960
to be 1. And the reason for that is how the curve is interpreted. Unlike discrete distributions,

210
00:17:48,960 --> 00:17:53,680
when it comes to something continuous, you don't ask about the probability of a particular point.

211
00:17:53,680 --> 00:17:59,040
Instead, you ask for the probability that a value falls between two different values. And what the

212
00:17:59,040 --> 00:18:04,880
curve is telling you is that that probability equals the area under the curve between those two

213
00:18:04,880 --> 00:18:09,680
values. There's a whole other video about this, they're called probability density functions.

214
00:18:09,680 --> 00:18:15,040
The main point right now is that the area under the entire curve represents the probability that

215
00:18:15,040 --> 00:18:19,920
something happens, that some number comes up. That should be 1, which is why we want the area under

216
00:18:19,920 --> 00:18:24,560
this to be 1. As it stands with the basic bell curve shape of e to the negative x squared,

217
00:18:24,560 --> 00:18:30,240
the area is not 1, it's actually the square root of pi. I know, right? What is pi doing here? What

218
00:18:30,240 --> 00:18:34,320
does this have to do with circles? Like I said at the start, I'd love to talk all about that in the

219
00:18:34,320 --> 00:18:38,640
next video. But if you can spare your excitement for our purposes right now, all it means is that

220
00:18:38,640 --> 00:18:43,680
we should divide this function by the square root of pi, and it gives us the area we want. Throwing

221
00:18:43,680 --> 00:18:48,960
back in the constants we had earlier, the 1 half and the sigma, the effect there is to stretch out

222
00:18:48,960 --> 00:18:54,480
the graph by a factor of sigma times the square root of 2. So we also need to divide out by that

223
00:18:54,480 --> 00:18:58,960
in order to make sure it has an area of 1. And combining those fractions, the factor out front

224
00:18:58,960 --> 00:19:05,200
looks like 1 divided by sigma times the square root of 2 pi. This, finally, is a valid probability

225
00:19:05,200 --> 00:19:10,880
distribution. As we tweak that value sigma, resulting in narrower and wider curves, that

226
00:19:10,880 --> 00:19:17,360
constant in the front always guarantees that the area equals 1. The special case where sigma equals

227
00:19:17,360 --> 00:19:22,400
1 has a specific name, we call it the standard normal distribution, which plays an especially

228
00:19:22,400 --> 00:19:27,600
important role for you and me in this lesson. And all possible normal distributions are not

229
00:19:27,600 --> 00:19:33,280
only parameterized with this value sigma, but we also subtract off another constant mu from the

230
00:19:33,280 --> 00:19:38,080
variable x, and this essentially just lets you slide the graph left and right so that you can

231
00:19:38,080 --> 00:19:43,280
prescribe the mean of this distribution. So in short, we have two parameters, one describing the

232
00:19:43,280 --> 00:19:47,520
mean, one describing the standard deviation, and they're all tied together in this big formula

233
00:19:47,520 --> 00:19:54,160
involving an e and a pi. Now that all of that is on the table, let's look back again at the idea of

234
00:19:54,160 --> 00:19:59,200
starting with some random variable and asking what the distributions for sums of that variable

235
00:19:59,200 --> 00:20:03,440
look like. As we've already gone over, when you increase the size of that sum, the resulting

236
00:20:03,440 --> 00:20:08,480
distribution will shift according to a growing mean, and it slowly spreads out according to a

237
00:20:08,480 --> 00:20:13,360
growing standard deviation. And putting some actual formulas to it, if we know the mean of our

238
00:20:13,360 --> 00:20:18,240
underlying random variable, we call it mu, and we also know its standard deviation, and we call it

239
00:20:18,240 --> 00:20:24,000
sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the

240
00:20:24,000 --> 00:20:29,600
standard deviation will be sigma times the square root of that size. So now, if we want to claim

241
00:20:29,600 --> 00:20:34,240
that this looks more and more like a bell curve, and a bell curve is only described by two different

242
00:20:34,240 --> 00:20:38,800
parameters, the mean and the standard deviation, you know what to do. You could plug those two

243
00:20:38,800 --> 00:20:43,840
values into the formula, and it gives you a highly explicit, albeit kind of complicated,

244
00:20:43,840 --> 00:20:46,960
formula for a curve that should closely fit our distribution.

245
00:20:48,480 --> 00:20:52,960
But there's another way we can describe it that's a little more elegant and lends itself to a very

246
00:20:52,960 --> 00:20:57,920
fun visual that we can build up to. Instead of focusing on the sum of all of these random

247
00:20:57,920 --> 00:21:02,560
variables, let's modify this expression a little bit, where what we'll do is we'll look at the mean

248
00:21:02,560 --> 00:21:07,600
that we expect that sum to take, and we subtract it off so that our new expression has a mean of 0,

249
00:21:08,160 --> 00:21:12,560
and then we're going to look at the standard deviation we expect of our sum, and divide out

250
00:21:12,560 --> 00:21:17,920
by that, which basically just rescales the units so that the standard deviation of our expression

251
00:21:17,920 --> 00:21:22,960
will equal 1. This might seem like a more complicated expression, but it actually has a

252
00:21:22,960 --> 00:21:28,800
highly readable meaning. It's essentially saying how many standard deviations away from the mean

253
00:21:28,800 --> 00:21:34,800
is this sum? For example, this bar here corresponds to a certain value that you might find when you

254
00:21:34,800 --> 00:21:39,520
roll 10 dice and you add them all up, and its position a little above negative 1 is telling

255
00:21:39,520 --> 00:21:43,760
you that that value is a little bit less than one standard deviation lower than the mean.

256
00:21:44,800 --> 00:21:49,680
Also, by the way, in anticipation for the animation I'm trying to build to here, the way I'm representing

257
00:21:49,680 --> 00:21:54,880
things on that lower plot is that the area of each one of these bars is telling us the probability of

258
00:21:54,880 --> 00:21:59,440
the corresponding value rather than the height. You might think of the y-axis as representing not

259
00:21:59,440 --> 00:22:04,400
probability but a kind of probability density. The reason for this is to set the stage so that it

260
00:22:04,400 --> 00:22:08,800
aligns with the way we interpret continuous distributions, where the probability of falling

261
00:22:08,800 --> 00:22:14,400
between a range of values is equal to an area under a curve between those values. In particular,

262
00:22:14,400 --> 00:22:20,400
the area of all the bars together is going to be 1. Now, with all of that in place, let's have a

263
00:22:20,400 --> 00:22:24,960
little fun. Let me start by rolling things back so that the distribution on the bottom represents

264
00:22:24,960 --> 00:22:30,160
a relatively small sum, like adding together only three such random variables. Notice what happens

265
00:22:30,160 --> 00:22:35,440
as I change the distribution we start with. As it changes, the distribution on the bottom completely

266
00:22:35,440 --> 00:22:42,240
changes its shape. It's very dependent on what we started with. If we let the size of our sum get a

267
00:22:42,240 --> 00:22:47,760
little bit bigger, say going up to 10, and as I change the distribution for x, it largely stays

268
00:22:47,760 --> 00:22:52,240
looking like a bell curve, but I can find some distributions that get it to change shape. For

269
00:22:52,240 --> 00:22:57,600
example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results

270
00:22:57,600 --> 00:23:02,400
in this kind of spiky bell curve, and if you'll recall, earlier on I actually showed this in the

271
00:23:02,400 --> 00:23:07,120
form of a simulation. So if you were wondering whether that spikiness was an artifact of the

272
00:23:07,120 --> 00:23:12,160
randomness or reflected the true distribution, turns out it reflects the true distribution.

273
00:23:12,160 --> 00:23:17,040
In this case, 10 is not a large enough sum for the central limit theorem to kick in. But if instead

274
00:23:17,040 --> 00:23:22,400
I let that sum grow and I consider adding 50 different values, which is actually not that big,

275
00:23:22,960 --> 00:23:28,480
then no matter how I change the distribution for our underlying random variable, it has essentially

276
00:23:28,480 --> 00:23:33,280
no effect on the shape of the plot on the bottom. No matter where we start, all of the information

277
00:23:33,280 --> 00:23:38,640
and nuance for the distribution of x gets washed away, and we tend towards this single universal

278
00:23:38,640 --> 00:23:44,160
shape described by a very elegant function for the standard normal distribution, 1 over square root of

279
00:23:44,160 --> 00:23:49,920
2 pi times e to the negative x squared over 2. This, this right here is what the central limit

280
00:23:49,920 --> 00:23:54,480
theorem is all about. Almost nothing you can do to this initial distribution changes the shape

281
00:23:54,480 --> 00:24:02,960
we tend towards. Now, the more theoretically minded among you might still be wondering,

282
00:24:02,960 --> 00:24:07,280
what is the actual theorem? Like, what's the mathematical statement that could be proved or

283
00:24:07,280 --> 00:24:11,440
disproved that we're claiming here? If you want a nice formal statement, here's how it might go.

284
00:24:12,080 --> 00:24:16,720
Consider this value, where we're summing up n different instantiations of our random variable,

285
00:24:16,720 --> 00:24:21,040
but tweaked and tuned so that its mean and standard deviation are 1. Again, meaning you can

286
00:24:21,040 --> 00:24:26,640
read it as asking how many standard deviations away from the mean is the sum. Then the actual

287
00:24:26,640 --> 00:24:30,880
rigorous no-jokes-this-time statement of the central limit theorem is that if you consider

288
00:24:30,880 --> 00:24:37,040
the probability that this value falls between two given real numbers, a and b, and you consider the

289
00:24:37,040 --> 00:24:43,520
limit of that probability as the size of your sum goes to infinity, then that limit is equal to a

290
00:24:43,520 --> 00:24:48,560
certain integral, which basically describes the area under a standard normal distribution between

291
00:24:48,560 --> 00:24:55,040
those two values. Again, there are three underlying assumptions that I have yet to tell you,

292
00:24:55,040 --> 00:24:59,760
but other than those, in all of its gory detail, this right here is the central limit theorem.

293
00:25:04,720 --> 00:25:08,480
All of that is a bit theoretical, so it might be helpful to bring things back down to Earth

294
00:25:08,480 --> 00:25:12,560
and turn back to the concrete example that I mentioned at the start, where you imagine

295
00:25:12,560 --> 00:25:17,520
rolling a die 100 times, and let's assume it's a fair die for this example, and you add together

296
00:25:17,520 --> 00:25:23,680
the results. The challenge for you is to find a range of values such that you're 95% sure

297
00:25:23,680 --> 00:25:29,040
that the sum will fall within this range. For questions like this, there's a handy rule of

298
00:25:29,040 --> 00:25:34,080
thumb about normal distributions, which is that about 68% of your values are going to fall within

299
00:25:34,080 --> 00:25:39,520
one standard deviation of the mean, 95% of your values, the thing we care about, fall within two

300
00:25:39,520 --> 00:25:45,440
standard deviations of the mean, and a whopping 99.7% of your values will fall within three

301
00:25:45,440 --> 00:25:49,920
standard deviations of the mean. It's a rule of thumb that's commonly memorized by people who do

302
00:25:49,920 --> 00:25:55,040
a lot of probability and stats. Naturally, this gives us what we need for our example,

303
00:25:55,040 --> 00:25:59,200
and let me go ahead and draw out what this would look like, where I'll show the distribution for

304
00:25:59,200 --> 00:26:05,040
a fair die up at the top, and the distribution for a sum of 100 such dice on the bottom, which by now

305
00:26:05,040 --> 00:26:09,600
as you know looks like a certain normal distribution. Step one with a problem like this

306
00:26:09,600 --> 00:26:14,720
is to find the mean of your initial distribution, which in this case will look like 1 6th times 1

307
00:26:14,720 --> 00:26:20,880
plus 1 6th times 2 on and on and on, and works out to be 3.5. We also need the standard deviation,

308
00:26:20,880 --> 00:26:25,440
which requires calculating the variance, which as you know involves adding all the squares of

309
00:26:25,440 --> 00:26:30,800
the differences between the values and the means, and it works out to be 2.92, square root of that

310
00:26:30,800 --> 00:26:36,000
comes out to be 1.71. Those are the only two numbers we need, and I will invite you again to

311
00:26:36,000 --> 00:26:40,160
reflect on how magical it is that those are the only two numbers that you need to completely

312
00:26:40,160 --> 00:26:46,560
understand the bottom distribution. Its mean will be 100 times mu, which is 350, and its standard

313
00:26:46,560 --> 00:26:53,600
deviation will be the square root of 100 times sigma, so 10 times sigma 17.1. Remembering our

314
00:26:53,600 --> 00:26:58,320
handy rule of thumb, we're looking for values two standard deviations away from the mean,

315
00:26:58,320 --> 00:27:04,800
and when you subtract 2 sigma from the mean you end up with about 316, and when you add 2 sigma

316
00:27:04,960 --> 00:27:08,880
you end up with 384. And there you go, that gives us the answer.

317
00:27:11,600 --> 00:27:15,840
Okay, I promised to wrap things up shortly, but while we're on this example there's one more

318
00:27:15,840 --> 00:27:21,440
question that's worth your time to ponder. Instead of just asking about the sum of 100 die rolls,

319
00:27:21,440 --> 00:27:25,840
let's say I had you divide that number by 100, which basically means all the numbers in our

320
00:27:25,840 --> 00:27:31,120
diagram in the bottom get divided by 100. Take a moment to interpret what this all would be saying

321
00:27:31,120 --> 00:27:36,560
then. The expression essentially tells you the empirical average for 100 different die rolls,

322
00:27:37,200 --> 00:27:43,040
and that interval we found is now telling you what range you are expecting to see for that empirical

323
00:27:43,040 --> 00:27:48,480
average. In other words, you might expect it to be around 3.5, that's the expected value for a die

324
00:27:48,480 --> 00:27:53,200
roll, but what's much less obvious and what the central limit theorem lets you compute is how

325
00:27:53,200 --> 00:27:58,640
close to that expected value you'll reasonably find yourself. In particular, it's worth your

326
00:27:58,640 --> 00:28:03,680
time to take a moment mulling over what the standard deviation for this empirical average is,

327
00:28:03,680 --> 00:28:07,040
and what happens to it as you look at a bigger and bigger sample of die rolls.

328
00:28:12,960 --> 00:28:17,920
Lastly, but probably most importantly, let's talk about the assumptions that go into this theorem.

329
00:28:17,920 --> 00:28:22,720
The first one is that all of these variables that we're adding up are independent from each other.

330
00:28:22,720 --> 00:28:26,320
The outcome of one process doesn't influence the outcome of any other process.

331
00:28:27,040 --> 00:28:31,440
The second is that all of these variables are drawn from the same distribution.

332
00:28:31,440 --> 00:28:35,920
Both of these have been implicitly assumed with our dice example. We've been treating the outcome

333
00:28:35,920 --> 00:28:40,400
of each die roll as independent from the outcome of all the others, and we're assuming that each

334
00:28:40,400 --> 00:28:45,040
die follows the same distribution. Sometimes in the literature you'll see these two assumptions

335
00:28:45,040 --> 00:28:50,320
lumped together under the initials IID for independent and identically distributed.

336
00:28:50,320 --> 00:28:55,440
One situation where these assumptions are decidedly not true would be the Galton board.

337
00:28:55,440 --> 00:29:00,320
I mean, think about it. Is it the case that the way a ball bounces off of one of the pegs

338
00:29:00,320 --> 00:29:05,200
is independent from how it's going to bounce off the next peg? Absolutely not. Depending on the

339
00:29:05,200 --> 00:29:09,200
last bounce, it's coming in with a completely different trajectory. And is it the case that

340
00:29:09,200 --> 00:29:15,600
the distribution of possible outcomes off of each peg are the same for each peg that it hits? Again,

341
00:29:15,600 --> 00:29:19,680
almost certainly not. Maybe it hits one peg glancing to the left, meaning the outcomes

342
00:29:19,680 --> 00:29:23,680
are hugely skewed in that direction, and then hits the next one glancing to the right.

343
00:29:25,680 --> 00:29:30,320
When I made all those simplifying assumptions in the opening example, it wasn't just to make

344
00:29:30,320 --> 00:29:35,120
this easier to think about. It's also that those assumptions were necessary for this to actually

345
00:29:35,120 --> 00:29:39,920
be an example of the central limit theorem. Nevertheless, it seems to be true that for

346
00:29:39,920 --> 00:29:45,040
the real Galton board, despite violating both of these, a normal distribution does kind of come

347
00:29:45,040 --> 00:29:49,840
about? Part of the reason might be that there are generalizations of the theorem beyond the scope

348
00:29:49,920 --> 00:29:55,120
of this video that relax these assumptions, especially the second one. But I do want to

349
00:29:55,120 --> 00:29:59,520
caution you against the fact that many times people seem to assume that a variable is normally

350
00:29:59,520 --> 00:30:05,200
distributed, even when there's no actual justification to do so. The third assumption is

351
00:30:05,200 --> 00:30:10,800
actually fairly subtle. It's that the variance we've been computing for these variables is finite.

352
00:30:10,800 --> 00:30:15,120
This was never an issue for the dice example, because there were only six possible outcomes.

353
00:30:15,120 --> 00:30:19,280
But in certain situations where you have an infinite set of outcomes, when you go to compute

354
00:30:19,280 --> 00:30:25,200
the variance, the sum ends up diverging off to infinity. These can be perfectly valid probability

355
00:30:25,200 --> 00:30:30,160
distributions, and they do come up in practice. But in those situations, as you consider adding

356
00:30:30,160 --> 00:30:34,720
many different instantiations of that variable and letting that sum approach infinity, even if the

357
00:30:34,720 --> 00:30:39,520
first two assumptions hold, it is very much a possibility that the thing you tend towards is

358
00:30:39,520 --> 00:30:44,320
not actually a normal distribution. If you've understood everything up to this point, you now

359
00:30:44,320 --> 00:30:48,960
have a very strong foundation in what the central limit theorem is all about. And next up, I'd like

360
00:30:48,960 --> 00:30:53,200
to explain why it is that this particular function is the thing that we tend towards,

361
00:30:53,200 --> 00:30:56,640
and why it has a pi in it, what it has to do with circles.

