[
 {
  "input": "This is a Galton board. ",
  "translatedText": "Bu bir Galton tahtası. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 1.26
 },
 {
  "input": "Maybe you've seen one before, it's a popular demonstration of how, even when a single event is chaotic and random, with an effectively unknowable outcome, it's still possible to make precise statements about a large number of events, namely how the relative proportions for many different outcomes are distributed. ",
  "translatedText": "Belki daha önce bir tane görmüşsünüzdür; bu, tek bir olay kaotik ve rastgele olduğunda ve sonucu fiilen bilinemeyen olsa bile, çok sayıda olay hakkında kesin ifadelerde bulunmanın hâlâ mümkün olduğunu, yani göreceli oranların nasıl olduğunu gösteren popüler bir gösteri. Çünkü birçok farklı sonuç dağıtılıyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2.52,
  "end": 18.3
 },
 {
  "input": "More specifically, the Galton board illustrates one of the most prominent distributions in all of probability, known as the normal distribution, more colloquially known as a bell curve, and also called a Gaussian distribution. ",
  "translatedText": "Daha spesifik olarak, Galton panosu, normal dağılım olarak bilinen, halk dilinde çan eğrisi olarak da bilinen ve aynı zamanda Gauss dağılımı olarak da adlandırılan, tüm olasılıklar arasında en belirgin dağılımlardan birini gösterir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.38,
  "end": 31.9
 },
 {
  "input": "There's a very specific function to describe this distribution, it's very pretty, we'll get into it later, but right now I just want to emphasize how the normal distribution is, as the name suggests, very common, it shows up in a lot of seemingly unrelated contexts. ",
  "translatedText": "Bu dağılımı tanımlayan çok spesifik bir fonksiyon var, çok güzel, ona daha sonra değineceğiz, ama şimdi sadece normal dağılımın adından da anlaşılacağı gibi çok yaygın olduğunu vurgulamak istiyorum, birçok durumda ortaya çıkıyor görünüşte alakasız bağlamlar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 32.5,
  "end": 45.04
 },
 {
  "input": "If you were to take a large number of people who sit in a similar demographic and plot their heights, those heights tend to follow a normal distribution. ",
  "translatedText": "Benzer demografik grupta yer alan çok sayıda insanı alıp boylarını çizerseniz, bu boyların normal bir dağılım izlediğini görürsünüz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.02,
  "end": 53.0
 },
 {
  "input": "If you look at a large swath of very big natural numbers and you ask how many distinct prime factors does each one of those numbers have, the answers will very closely track with a certain normal distribution. ",
  "translatedText": "Çok büyük doğal sayılardan oluşan geniş bir alana bakarsanız ve bu sayıların her birinin kaç farklı asal çarpanı olduğunu sorarsanız, cevaplar belirli bir normal dağılıma çok yakın bir şekilde takip edilecektir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 53.66,
  "end": 64.96
 },
 {
  "input": "Now our topic for today is one of the crown jewels in all of probability theory, it's one of the key facts that explains why this distribution is as common as it is, known as the central limit theorem. ",
  "translatedText": "Şimdi bugünkü konumuz olasılık teorisinin en önemli mücevherlerinden biri, bu dağılımın neden bu kadar yaygın olduğunu açıklayan temel gerçeklerden biri, merkezi limit teoremi olarak biliniyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.58,
  "end": 76.02
 },
 {
  "input": "This lesson is meant to go back to the basics, giving you the fundamentals on what the central limit theorem is saying, what normal distributions are, and I want to assume minimal background. ",
  "translatedText": "Bu ders temellere geri dönmeyi, size merkezi limit teoreminin ne söylediğine, normal dağılımların ne olduğuna dair temel bilgileri vermeyi amaçlamaktadır ve ben de minimum arka planı varsaymak istiyorum. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.64,
  "end": 85.26
 },
 {
  "input": "We're going to go decently deep into it, but after this I'd still like to go deeper and explain why the theorem is true, why the function underlying the normal distribution has the very specific form that it does, why that formula has a pi in it, and, most fun, why those last two facts are actually more related than a lot of traditional explanations would suggest. ",
  "translatedText": "Yeterince derinlere ineceğiz, ancak bundan sonra yine de daha derine inip teoremin neden doğru olduğunu, normal dağılımın altında yatan fonksiyonun neden bu kadar spesifik bir forma sahip olduğunu, bu formülün neden böyle olduğunu açıklamak istiyorum. Bunda bir pi var ve en eğlencelisi de bu son iki olgunun aslında neden pek çok geleneksel açıklamanın önerdiğinden daha fazla ilişkili olduğu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.26,
  "end": 105.56
 },
 {
  "input": "That second lesson is also meant to be the follow-on to the convolutions video that I promised, so there's a lot of interrelated topics here. ",
  "translatedText": "Bu ikinci ders aynı zamanda söz verdiğim evrişimler videosunun devamı niteliğinde olduğundan burada birbiriyle bağlantılı pek çok konu var. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.48,
  "end": 113.37
 },
 {
  "input": "But right now, back to the fundamentals, I'd like to kick things off with a overly simplified model of the Galton board. ",
  "translatedText": "Ancak şu anda, temellere dönersek, işleri Galton tahtasının aşırı basitleştirilmiş bir modeliyle başlatmak istiyorum. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.57,
  "end": 119.17
 },
 {
  "input": "In this model we will assume that each ball falls directly onto a certain central peg and that it has a 50-50 probability of bouncing to the left or to the right, and we'll think of each of those outcomes as either adding one or subtracting one from its position. ",
  "translatedText": "Bu modelde, her topun doğrudan belirli bir merkezi çiviye düştüğünü ve 50-50 sola veya sağa sıçrama olasılığının olduğunu varsayacağız ve bu sonuçların her birini ya bir ya da bir ek olarak düşüneceğiz. birini konumundan çıkarmak. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.89,
  "end": 134.11
 },
 {
  "input": "Once one of those is chosen, we make the highly unrealistic assumption that it happens to land dead on in the middle of the peg adjacent below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the right. ",
  "translatedText": "Bunlardan biri seçildiğinde, onun hemen altındaki bitişik çivinin ortasına düşeceği ve yine aynı 50-50 sola sıçrama seçeneğiyle karşı karşıya kalacağı yönünde son derece gerçekçi olmayan bir varsayımda bulunuruz. Sağa. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.67,
  "end": 147.07
 },
 {
  "input": "For the one I'm showing on screen, there are five different rows of pegs, so our little hopping ball makes five different random choices between plus one and minus one, and we can think of its final position as basically being the sum of all of those different numbers, which in this case happens to be one, and we might label all of the different buckets with the sum that they represent. ",
  "translatedText": "Ekranda gösterdiğim örnekte beş farklı mandal sırası var, yani küçük zıplayan topumuz artı bir ile eksi bir arasında beş farklı rastgele seçim yapıyor ve onun son konumunu temel olarak hepsinin toplamı olarak düşünebiliriz. Bu durumda bir olan bu farklı sayıların tümünü temsil ettikleri toplamla etiketleyebiliriz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.43,
  "end": 166.35
 },
 {
  "input": "As we repeat this, we're looking at different possible sums for those five random numbers. ",
  "translatedText": "Bunu tekrarlarken, bu beş rastgele sayının farklı olası toplamlarına bakıyoruz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 166.35,
  "end": 171.29
 },
 {
  "input": "And for those of you who are inclined to complain that this is a highly unrealistic model for the true Galton board, let me emphasize the goal right now is not to accurately model physics. ",
  "translatedText": "Ve bunun gerçek Galton tahtası için son derece gerçekçi olmayan bir model olduğundan şikayet etme eğiliminde olanlarınız için, şu anda amacın fiziği doğru bir şekilde modellemek olmadığını vurgulamama izin verin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.05,
  "end": 181.67
 },
 {
  "input": "The goal is to give a simple example to illustrate the central limit theorem, and for that, idealized though this might be, it actually gives us a really good example. ",
  "translatedText": "Amaç, merkezi limit teoremini açıklamak için basit bir örnek vermektir ve bunun için idealleştirilmiş olsa da, aslında bize gerçekten iyi bir örnek verir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 181.83,
  "end": 190.03
 },
 {
  "input": "If we let many different balls fall, making yet another unrealistic assumption that they don't influence each other as if they're all ghosts, then the number of balls that fall into each different bucket gives us some loose sense for how likely each one of those buckets is. ",
  "translatedText": "Pek çok farklı topun düşmesine izin verirsek ve sanki hepsi hayaletmiş gibi birbirlerini etkilemediklerine dair gerçekçi olmayan bir varsayım daha yaparsak, o zaman her farklı kovaya düşen topların sayısı bize her birinin ne kadar muhtemel olduğuna dair gevşek bir fikir verir. bu kovalardan biri. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.57,
  "end": 203.39
 },
 {
  "input": "In this example, the numbers are simple enough that it's not too hard to explicitly calculate what the probability is for falling into each bucket. ",
  "translatedText": "Bu örnekte sayılar yeterince basittir ve her bir kutuya düşme olasılığının ne olduğunu açıkça hesaplamak çok da zor değildir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.83,
  "end": 210.01
 },
 {
  "input": "If you do want to think that through, you'll find it very reminiscent of Pascal's triangle. ",
  "translatedText": "Eğer bunu derinlemesine düşünmek isterseniz, bunun Pascal üçgenini çok anımsattığını göreceksiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.27,
  "end": 213.83
 },
 {
  "input": "But the neat thing about our theorem is how far it goes beyond the simple examples. ",
  "translatedText": "Ancak teoremimizin güzel yanı, basit örneklerin ne kadar ötesine geçtiğidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.95,
  "end": 218.27
 },
 {
  "input": "So to start off at least, rather than making explicit calculations, let's just simulate things by running a large number of samples and letting the total number of results in each different outcome give us some sense for what that distribution looks like. ",
  "translatedText": "En azından başlamak için, açık hesaplamalar yapmak yerine, çok sayıda örnek çalıştırarak ve her farklı sonuçtaki toplam sonuç sayısının bize bu dağılımın nasıl göründüğüne dair bir fikir vermesine izin vererek bazı şeyleri simüle edelim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.67,
  "end": 229.97
 },
 {
  "input": "As I said, the one on screen has five rows, so each sum that we're considering includes only five numbers. ",
  "translatedText": "Söylediğim gibi, ekrandaki beş satırdan oluşuyor, dolayısıyla ele aldığımız her toplam yalnızca beş sayı içeriyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 230.45,
  "end": 236.21
 },
 {
  "input": "The basic idea of the central limit theorem is that if you increase the size of that sum, for example here that would mean increasing the number of rows of pegs for each ball to bounce off, then the distribution that describes where that sum is going to fall looks more and more like a bell curve. ",
  "translatedText": "Merkezi limit teoreminin temel fikri şudur; eğer bu toplamın boyutunu artırırsanız, örneğin burada bu, her topun sıçrayacağı mandal sıralarının sayısını artırmak anlamına gelir, o zaman bu toplamın nereye gideceğini açıklayan dağılımdır. sonbahar giderek daha çok bir çan eğrisine benziyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.81,
  "end": 253.33
 },
 {
  "input": "Here, it's actually worth taking a moment to write down that general idea. ",
  "translatedText": "Burada, aslında bu genel fikri yazmak için biraz zaman ayırmaya değer. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.47,
  "end": 258.35
 },
 {
  "input": "The setup is that we have a random variable, and that's basically shorthand for a random process where each outcome of that process is associated with some number. ",
  "translatedText": "Kurulum, bir rastgele değişkene sahip olmamızdır ve bu, temelde, bu sürecin her sonucunun bir sayıyla ilişkilendirildiği rastgele bir sürecin kısaltmasıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 259.27,
  "end": 268.19
 },
 {
  "input": "We'll call that random number x. ",
  "translatedText": "Bu rastgele sayıya x diyeceğiz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 268.49,
  "end": 269.97
 },
 {
  "input": "For example, each bounce off the peg is a random process modeled with two outcomes. ",
  "translatedText": "Örneğin, sabitten her bir sıçrama, iki sonuçla modellenen rastgele bir süreçtir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.97,
  "end": 274.39
 },
 {
  "input": "Those outcomes are associated with the numbers negative one and positive one. ",
  "translatedText": "Bu sonuçlar negatif bir ve pozitif bir sayılarıyla ilişkilidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.85,
  "end": 277.89
 },
 {
  "input": "Another example of a random variable would be rolling a die, where you have six different outcomes, each one associated with a number. ",
  "translatedText": "Rastgele değişkene başka bir örnek, her biri bir sayıyla ilişkilendirilen altı farklı sonucun olduğu bir zarın atılmasıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.53,
  "end": 284.83
 },
 {
  "input": "What we're doing is taking multiple different samples of that variable and adding them all together. ",
  "translatedText": "Yaptığımız şey bu değişkenin birden fazla farklı örneğini alıp hepsini bir araya toplamak. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.47,
  "end": 290.41
 },
 {
  "input": "On our Galton board, that looks like letting the ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice and adding up the results. ",
  "translatedText": "Galton tahtamızda bu, topun dibe doğru giderken birden fazla farklı çividen sekmesine izin vermek gibi görünüyor ve bir zar durumunda, birçok farklı zarın atıldığını ve sonuçların toplandığını hayal edebilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.77,
  "end": 300.97
 },
 {
  "input": "The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into different possible values, will look more and more like a bell curve. ",
  "translatedText": "Merkezi limit teoreminin iddiası şudur: Bu toplamın boyutunun giderek büyümesine izin verdikçe, o zaman bu toplamın dağılımı, yani farklı olası değerlere düşme olasılığı, giderek daha çok bir çan eğrisine benzeyecektir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.43,
  "end": 314.11
 },
 {
  "input": "That's it, that is the general idea. ",
  "translatedText": "İşte bu, genel fikir bu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.43,
  "end": 317.13
 },
 {
  "input": "Over the course of this lesson, our job is to make that statement more quantitative. ",
  "translatedText": "Bu ders boyunca bizim görevimiz bu ifadeyi daha niceliksel hale getirmektir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 317.55,
  "end": 321.53
 },
 {
  "input": "We're going to put some numbers to it, put some formulas to it, show how you can use it to make predictions. ",
  "translatedText": "Ona bazı sayılar koyacağız, bazı formüller koyacağız, tahminlerde bulunmak için onu nasıl kullanabileceğinizi göstereceğiz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.07,
  "end": 326.35
 },
 {
  "input": "For example, here's the kind of question I want you to be able to answer by the end of this video. ",
  "translatedText": "Örneğin, bu videonun sonunda cevaplayabilmenizi istediğim türden bir soru. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.21,
  "end": 331.57
 },
 {
  "input": "Suppose you rolled the die 100 times and you added together the results. ",
  "translatedText": "Diyelim ki zarı 100 kez attınız ve sonuçları topladınız. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.19,
  "end": 335.89
 },
 {
  "input": "Could you find a range of values such that you're 95% sure that the sum will fall within that range? ",
  "translatedText": "Toplamın bu aralığa gireceğinden %95 emin olduğunuz bir değer aralığı bulabilir misiniz? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.63,
  "end": 342.17
 },
 {
  "input": "Or maybe I should say find the smallest possible range of values such that this is true. ",
  "translatedText": "Veya belki de bunun doğru olmasını sağlayacak mümkün olan en küçük değer aralığını bulmalıyım demeliyim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.83,
  "end": 346.55
 },
 {
  "input": "The neat thing is you'll be able to answer this question whether it's a fair die or if it's a weighted die. ",
  "translatedText": "Güzel olan şu ki, bu soruya adil bir zar mı yoksa ağırlıklı bir zar mı olduğu sorusunu cevaplayabileceksiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 347.39,
  "end": 352.13
 },
 {
  "input": "Now let me say at the top that this theorem has three different assumptions that go into it, three things that have to be true before the theorem follows. ",
  "translatedText": "Şimdi en üstte bu teoremin üç farklı varsayımı olduğunu söyleyeyim; teoremi takip etmeden önce bunların doğru olması gereken üç şey var. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.45,
  "end": 360.13
 },
 {
  "input": "And I'm actually not going to tell you what they are until the very end of the video. ",
  "translatedText": "Ve aslında videonun sonuna kadar size bunların ne olduğunu söylemeyeceğim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 360.43,
  "end": 363.79
 },
 {
  "input": "Instead I want you to keep your eye out and see if you can notice and maybe predict what those three assumptions are going to be. ",
  "translatedText": "Bunun yerine gözünüzü açık tutmanızı ve bu üç varsayımın ne olacağını fark edip edemeyeceğinizi ve belki de tahmin edip edemeyeceğinizi görmenizi istiyorum. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 364.27,
  "end": 369.67
 },
 {
  "input": "As a next step, to better illustrate just how general this theorem is, I want to run a couple more simulations for you focused on the dice example. ",
  "translatedText": "Bir sonraki adım olarak, bu teoremin ne kadar genel olduğunu daha iyi göstermek için, sizin için zar örneğine odaklanan birkaç simülasyon daha yapmak istiyorum. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.71,
  "end": 377.39
 },
 {
  "input": "Usually if you think of rolling a die you think of the six outcomes as being equally probable, but the theorem actually doesn't care about that. ",
  "translatedText": "Genellikle bir zarı atmayı düşündüğünüzde, altı sonucun eşit derecede olası olduğunu düşünürsünüz, ancak aslında teorem bunu umursamıyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 380.91,
  "end": 387.63
 },
 {
  "input": "We could start with a weighted die, something with a non-trivial distribution across the outcomes, and the core idea still holds. ",
  "translatedText": "Sonuçlar arasında önemsiz olmayan bir dağılıma sahip olan ağırlıklı bir kalıpla başlayabiliriz ve temel fikir hala geçerli. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 387.83,
  "end": 394.55
 },
 {
  "input": "For the simulation what I'll do is take some distribution like this one that is skewed towards lower values. ",
  "translatedText": "Simülasyon için yapacağım şey bunun gibi daha düşük değerlere doğru çarpık bir dağılım almak. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.03,
  "end": 399.93
 },
 {
  "input": "I'm going to take 10 distinct samples from that distribution and then I'll record the sum of that sample on the plot on the bottom. ",
  "translatedText": "Bu dağılımdan 10 farklı örnek alacağım ve sonra bu örneğin toplamını alttaki çizime kaydedeceğim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 400.25,
  "end": 407.55
 },
 {
  "input": "Then I'm going to do this many many different times, always with a sum of size 10, but keep track of where those sums ended up to give us a sense of the distribution. ",
  "translatedText": "Daha sonra bunu pek çok farklı kez yapacağım, her zaman 10 büyüklüğünde bir toplamla, ama bize dağılıma dair bir fikir vermek için bu toplamların nerede bittiğini takip edeceğim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 408.63,
  "end": 416.59
 },
 {
  "input": "And in fact let me rescale the y direction to give us room to run an even larger number of samples. ",
  "translatedText": "Ve aslında bize daha fazla sayıda numuneyi çalıştırmamız için yer açmak üzere y yönünü yeniden ölçeklendirmeme izin verin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.97,
  "end": 424.73
 },
 {
  "input": "And I'll let it go all the way up to a couple thousand, and as it does you'll notice that the shape that starts to emerge looks like a bell curve. ",
  "translatedText": "Ve birkaç bine kadar çıkmasına izin vereceğim ve bu şekilde ortaya çıkmaya başlayan şeklin bir çan eğrisine benzediğini fark edeceksiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.03,
  "end": 432.49
 },
 {
  "input": "Maybe if you squint your eyes you can see it skews a tiny bit to the left, but it's neat that something so symmetric emerged from a starting point that was so asymmetric. ",
  "translatedText": "Belki gözlerinizi kısarsanız biraz sola doğru eğildiğini görebilirsiniz, ancak bu kadar asimetrik bir başlangıç noktasından bu kadar simetrik bir şeyin ortaya çıkması çok güzel. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.87,
  "end": 441.01
 },
 {
  "input": "To better illustrate what the central limit theorem is all about, let me run four of these simulations in parallel, where on the upper left I'm doing it where we're only adding two dice at a time, on the upper right we're doing it where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at a time, and then we'll do another one with a bigger sum, 15 at a time. ",
  "translatedText": "Merkezi limit teoreminin neyle ilgili olduğunu daha iyi göstermek için, bu simülasyonlardan dördünü paralel olarak çalıştırayım; sol üstte bunu yapıyorum, bir seferde yalnızca iki zar ekliyoruz, sağ üstte ise bunu yapıyoruz. bunu bir kerede beş zar ekleyerek yapıyoruz, sol altta az önce gördüğümüz bir kerede 10 zar ekliyoruz ve sonra daha büyük toplamlı bir tane daha yapacağız, her seferinde 15 zar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 441.47,
  "end": 461.37
 },
 {
  "input": "Notice how on the upper left when we're just adding two dice, the resulting distribution doesn't really look like a bell curve, it looks a lot more reminiscent of the one we started with skewed towards the left. ",
  "translatedText": "Sol üstte sadece iki zarı topladığımızda ortaya çıkan dağılımın aslında bir çan eğrisine benzemediğine dikkat edin, daha çok sola doğru çarpık olarak başladığımız dağılımı anımsatıyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.25,
  "end": 472.03
 },
 {
  "input": "But as we allow for more and more dice in each sum, the resulting shape that comes up in these distributions looks more and more symmetric. ",
  "translatedText": "Ancak her toplamda daha fazla zara izin verdikçe, bu dağılımlarda ortaya çıkan şekil giderek daha simetrik görünüyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.81,
  "end": 479.81
 },
 {
  "input": "It has the lump in the middle and fade towards the tail's shape of a bell curve. ",
  "translatedText": "Ortasında bir yumru vardır ve kuyruk şekline doğru çan eğrisine doğru kaybolur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.95,
  "end": 483.89
 },
 {
  "input": "And let me emphasize again, you can start with any different distribution. ",
  "translatedText": "Tekrar vurgulamak isterim ki, herhangi bir farklı dağıtımla başlayabilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 487.05,
  "end": 490.49
 },
 {
  "input": "Here I'll run it again, but where most of the probability is tied up in the numbers 1 and 6, with very low probability for the mid values. ",
  "translatedText": "Burada tekrar çalıştıracağım, ancak olasılığın çoğu 1 ve 6 sayılarına bağlı, orta değerler için ise çok düşük olasılık. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.49,
  "end": 497.49
 },
 {
  "input": "Despite completely changing the distribution for an individual roll of the die, it's still the case that a bell curve shape will emerge as we consider the different sums. ",
  "translatedText": "Zarın tek bir atışındaki dağılım tamamen değişmesine rağmen, farklı toplamları dikkate aldığımızda hala bir çan eğrisi şekli ortaya çıkacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.19,
  "end": 506.55
 },
 {
  "input": "Illustrating things with a simulation like this is very fun, and it's kind of neat to see order emerge from chaos, but it also feels a little imprecise. ",
  "translatedText": "Bir şeyleri bunun gibi bir simülasyonla göstermek çok eğlenceli ve kaostan düzenin çıktığını görmek güzel ama aynı zamanda biraz belirsiz de geliyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.27,
  "end": 515.03
 },
 {
  "input": "Like in this case, when I cut off the simulation at 3000 samples, even though it kind of looks like a bell curve, the different buckets seem pretty spiky. ",
  "translatedText": "Bu durumda olduğu gibi, simülasyonu 3000 örnekte kestiğimde, her ne kadar çan eğrisi gibi görünse de, farklı gruplar oldukça dikenli görünüyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 515.39,
  "end": 522.99
 },
 {
  "input": "And you might wonder, is it supposed to look that way, or is that just an artifact of the randomness in the simulation? ",
  "translatedText": "Ve şunu merak edebilirsiniz: Bu şekilde mi görünmesi gerekiyor, yoksa bu sadece simülasyondaki rastgeleliğin bir eseri mi? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 522.99,
  "end": 528.55
 },
 {
  "input": "And if it is, how many samples do we need before we can be sure that what we're looking at is representative of the true distribution? ",
  "translatedText": "Ve eğer öyleyse, baktığımız şeyin gerçek dağılımı temsil ettiğinden emin olabilmek için kaç örneğe ihtiyacımız var? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 529.01,
  "end": 535.11
 },
 {
  "input": "Instead moving forward, let's get a little more theoretical and show the precise shape that these distributions will take on in the long run. ",
  "translatedText": "İleriye gitmek yerine biraz daha teorik olalım ve bu dağılımların uzun vadede alacağı kesin şekli gösterelim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.19,
  "end": 545.47
 },
 {
  "input": "The easiest case to make this calculation is if we have a uniform distribution, where each possible face of the die has an equal probability, 1 6th. ",
  "translatedText": "Bu hesaplamayı yapmanın en kolay durumu, zarın her olası yüzünün eşit olasılığa (16'ncı) sahip olduğu tekdüze bir dağılıma sahip olmamızdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.13,
  "end": 553.97
 },
 {
  "input": "For example, if you then want to know how likely different sums are for a pair of dice, it's essentially a counting game, where you count up how many distinct pairs take on the same sum, which in the diagram I've drawn, you can conveniently think about by going through all of the different diagonals. ",
  "translatedText": "Örneğin, bir çift zar için farklı toplamların ne kadar muhtemel olduğunu bilmek isterseniz, bu aslında bir sayma oyunudur; burada, aynı toplam için kaç farklı çiftin alındığını sayarsınız; benim çizdiğim diyagramda bunu görürsünüz. tüm farklı köşegenlerden geçerek rahatlıkla düşünebiliriz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.99,
  "end": 568.49
 },
 {
  "input": "Since each such pair has an equal chance of showing up, 1 in 36, all you have to do is count the sizes of these buckets. ",
  "translatedText": "Bu çiftlerin her birinin ortaya çıkma şansı eşit olduğundan (36'da 1), tek yapmanız gereken bu kovaların boyutlarını saymaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.41,
  "end": 577.53
 },
 {
  "input": "That gives us a definitive shape for the distribution describing a sum of two dice, and if we were to play the same game with all possible triplets, the resulting distribution would look like this. ",
  "translatedText": "Bu bize iki zarın toplamını tanımlayan dağılım için kesin bir şekil verir ve aynı oyunu mümkün olan tüm üçlülerle oynasaydık, ortaya çıkan dağılım şu şekilde görünürdü. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.19,
  "end": 588.13
 },
 {
  "input": "Now what's more challenging, but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that single die. ",
  "translatedText": "Şimdi daha zor ama çok daha ilginç olan şey, o tek zar için düzgün olmayan bir dağılıma sahipsek ne olacağını sormaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 588.69,
  "end": 594.99
 },
 {
  "input": "We actually talked all about this in the last video. ",
  "translatedText": "Aslında bunların hepsini son videoda konuşmuştuk. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.55,
  "end": 597.97
 },
 {
  "input": "You do essentially the same thing, you go through all the distinct pairs of dice which add up to the same value. ",
  "translatedText": "Aslında aynı şeyi yaparsınız, toplamı aynı değere ulaşan tüm farklı zar çiftlerinden geçersiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.45,
  "end": 603.67
 },
 {
  "input": "It's just that instead of counting those pairs, for each pair you multiply the two probabilities of each particular face coming up, and then you add all those together. ",
  "translatedText": "Sadece bu çiftleri saymak yerine, her bir çift için, her belirli yüzün iki olasılığını çarparsınız ve sonra hepsini bir araya getirirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 603.97,
  "end": 612.75
 },
 {
  "input": "The computation that does this for all possible sums has a fancy name, it's called a convolution, but it's essentially just the weighted version of the counting game that anyone who's played with a pair of dice already finds familiar. ",
  "translatedText": "Bunu mümkün olan tüm toplamlar için yapan hesaplamanın süslü bir adı var, buna evrişim denir, ancak aslında bu, bir çift zarla oynayan herkesin zaten tanıdık bulduğu sayma oyununun ağırlıklı versiyonudur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 613.29,
  "end": 624.47
 },
 {
  "input": "For our purposes in this lesson, I'll have the computer calculate all that, simply display the results for you, and invite you to observe certain patterns, but under the hood, this is what's going on. ",
  "translatedText": "Bu dersteki amaçlarımız doğrultusunda, bilgisayara tüm bunları hesaplatacağım, sonuçları sizin için göstermesini sağlayacağım ve sizi belirli kalıpları gözlemlemeye davet edeceğim, ancak işin özünde olup biten budur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.03,
  "end": 635.33
 },
 {
  "input": "So just to be crystal clear on what's being represented here, if you imagine sampling two different values from that top distribution, the one describing a single die, and adding them together, then the second distribution I'm drawing represents how likely you are to see various different sums. ",
  "translatedText": "Burada neyin temsil edildiği konusunda net olmak gerekirse, üstteki dağılımdan iki farklı değeri örneklediğinizi, tek bir zarı tanımlayan ve bunları bir araya toplayan değeri hayal ederseniz, o zaman çizdiğim ikinci dağılım ne kadar muhtemel olduğunuzu temsil eder. çeşitli farklı toplamlara bakın. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 636.65,
  "end": 652.23
 },
 {
  "input": "Likewise, if you imagine sampling three distinct values from that top distribution, and adding them together, the next plot represents the probabilities for various different sums in that case. ",
  "translatedText": "Benzer şekilde, eğer bu üst dağılımdan üç farklı değer örnekleyip bunları bir araya toplamayı hayal ederseniz, bir sonraki çizim bu durumda çeşitli farklı toplamların olasılıklarını temsil eder. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 652.89,
  "end": 662.49
 },
 {
  "input": "So if I compute what the distributions for these sums look like for larger and larger sums, well you know what I'm going to say, it looks more and more like a bell curve. ",
  "translatedText": "Yani, bu toplamların dağılımlarının giderek daha büyük toplamlar için nasıl göründüğünü hesaplarsam, ne diyeceğimi bilirsiniz, giderek daha çok bir çan eğrisine benzemeye başlar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 663.51,
  "end": 672.39
 },
 {
  "input": "But before we get to that, I want you to make a couple more simple observations. ",
  "translatedText": "Ancak buna geçmeden önce sizden birkaç basit gözlem daha yapmanızı istiyorum. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.35,
  "end": 676.45
 },
 {
  "input": "For example, these distributions seem to be wandering to the right, and also they seem to be getting more spread out, and a little bit more flat. ",
  "translatedText": "Örneğin, bu dağılımlar sağa doğru gidiyor gibi görünüyor ve aynı zamanda daha da yayılıyor ve biraz daha düzleşiyor gibi görünüyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.45,
  "end": 684.79
 },
 {
  "input": "You cannot describe the central limit theorem quantitatively without taking into account both of those effects, which in turn requires describing the mean and the standard deviation. ",
  "translatedText": "Bu etkilerin her ikisini de hesaba katmadan merkezi limit teoremini niceliksel olarak tanımlayamazsınız; bu da ortalamanın ve standart sapmanın tanımlanmasını gerektirir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.25,
  "end": 693.19
 },
 {
  "input": "Maybe you're already familiar with those, but I want to make minimal assumptions here, and it never hurts to review, so let's quickly go over both of those. ",
  "translatedText": "Belki bunlara zaten aşinasınızdır, ancak burada minimum düzeyde varsayımlarda bulunmak istiyorum ve gözden geçirmekten zarar gelmez, o yüzden hızlıca her ikisinin üzerinden geçelim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 693.95,
  "end": 700.61
 },
 {
  "input": "The mean of a distribution, often denoted with the Greek letter mu, is a way of capturing the center of mass for that distribution. ",
  "translatedText": "Çoğunlukla Yunanca mu harfiyle gösterilen dağılımın ortalaması, söz konusu dağılımın kütle merkezini yakalamanın bir yoludur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 703.41,
  "end": 710.71
 },
 {
  "input": "It's calculated as the expected value of our random variable, which is a way of saying you go through all of the different possible outcomes, and you multiply the probability of that outcome times the value of the variable. ",
  "translatedText": "Rastgele değişkenimizin beklenen değeri olarak hesaplanır; bu, tüm farklı olası sonuçları gözden geçirdiğinizi ve bu sonucun olasılığını değişkenin değeriyle çarptığınızı söylemenin bir yoludur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 711.19,
  "end": 722.85
 },
 {
  "input": "If higher values are more probable, that weighted sum is going to be bigger. ",
  "translatedText": "Daha yüksek değerler daha muhtemelse, bu ağırlıklı toplam daha büyük olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 723.19,
  "end": 726.41
 },
 {
  "input": "If lower values are more probable, that weighted sum is going to be smaller. ",
  "translatedText": "Daha düşük değerler daha muhtemelse, bu ağırlıklı toplam daha küçük olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.75,
  "end": 729.95
 },
 {
  "input": "A little more interesting is if you want to measure how spread out this distribution is, because there's multiple different ways you might do it. ",
  "translatedText": "Bu dağıtımın ne kadar yayıldığını ölçmek istiyorsanız biraz daha ilginç, çünkü bunu yapmanın birçok farklı yolu var. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.79,
  "end": 737.13
 },
 {
  "input": "One of them is called the variance. ",
  "translatedText": "Bunlardan birine varyans denir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.53,
  "end": 740.29
 },
 {
  "input": "The idea there is to look at the difference between each possible value and the mean, square that difference, and ask for its expected value. ",
  "translatedText": "Buradaki fikir, her olası değer ile ortalama arasındaki farka bakmak, bu farkın karesini almak ve beklenen değerini sormaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.83,
  "end": 748.27
 },
 {
  "input": "The idea is that whether your value is below or above the mean, when you square that difference, you get a positive number, and the larger the difference, the bigger that number. ",
  "translatedText": "Buradaki fikir şudur: Değeriniz ortalamanın altında ya da üstünde olsun, bu farkın karesini aldığınızda pozitif bir sayı elde edersiniz ve fark ne kadar büyük olursa sayı da o kadar büyük olur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 748.73,
  "end": 756.65
 },
 {
  "input": "Squaring it like this turns out to make the math much much nicer than if we did something like an absolute value, but the downside is that it's hard to think about this as a distance in our diagram because the units are off. ",
  "translatedText": "Bu şekilde karesini almak, mutlak değer gibi bir şey yapmamıza kıyasla matematiği çok daha güzel hale getiriyor, ancak olumsuz tarafı, birimler kapalı olduğundan diyagramımızda bunu bir mesafe olarak düşünmenin zor olmasıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.37,
  "end": 768.13
 },
 {
  "input": "Kind of like the units here are square units, whereas a distance in our diagram would be a kind of linear unit. ",
  "translatedText": "Buradaki birimler bir nevi kare birimlerdir, halbuki diyagramımızdaki mesafe bir tür doğrusal birim olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.33,
  "end": 773.31
 },
 {
  "input": "So another way to measure spread is what's called the standard deviation, which is the square root of this value. ",
  "translatedText": "Yayılımı ölçmenin başka bir yolu da bu değerin karekökü olan standart sapmadır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 773.71,
  "end": 779.19
 },
 {
  "input": "That can be interpreted much more reasonably as a distance on our diagram, and it's commonly denoted with the Greek letter sigma, so you know m for mean as for standard deviation, but both in Greek. ",
  "translatedText": "Bu, diyagramımızda bir mesafe olarak çok daha makul bir şekilde yorumlanabilir ve genellikle Yunanca sigma harfiyle gösterilir, yani ortalama için m'yi standart sapma olarak bilirsiniz, ancak her ikisi de Yunanca'dadır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 779.47,
  "end": 789.65
 },
 {
  "input": "Looking back at our sequence of distributions, let's talk about the mean and standard deviation. ",
  "translatedText": "Dağılım sıramıza dönüp baktığımızda ortalama ve standart sapmadan bahsedelim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.87,
  "end": 796.15
 },
 {
  "input": "If we call the mean of the initial distribution mu, which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if I tell you that the mean of the next one is 2 times mu. ",
  "translatedText": "Başlangıç dağılımının ortalamasını mu olarak adlandırırsak, gösterilen için bu 2 olur. 24, umarım bir sonrakinin ortalamasının 2 çarpı mu olduğunu söylersem çok da şaşırtıcı olmaz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.63,
  "end": 806.73
 },
 {
  "input": "That is, you roll a pair of dice, you want to know the expected value of the sum, it's two times the expected value for a single die. ",
  "translatedText": "Yani, bir çift zar atarsınız, toplamın beklenen değerini bilmek istersiniz; bu, tek bir zar için beklenen değerin iki katıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 807.13,
  "end": 812.81
 },
 {
  "input": "Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth. ",
  "translatedText": "Benzer şekilde, 3 boyutumuzun toplamı için beklenen değer 3 çarpı mu'dur ve bu böyle devam eder. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 813.85,
  "end": 819.41
 },
 {
  "input": "The mean just marches steadily on to the right, which is why our distributions seem to be drifting off in that direction. ",
  "translatedText": "Ortalama sürekli olarak sağa doğru ilerliyor, bu yüzden dağılımlarımız o yöne doğru sürükleniyor gibi görünüyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 819.63,
  "end": 824.87
 },
 {
  "input": "A little more challenging, but very important, is to describe how the standard deviation changes. ",
  "translatedText": "Biraz daha zorlu ama çok önemli olan ise standart sapmanın nasıl değiştiğini açıklamaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.35,
  "end": 829.91
 },
 {
  "input": "The key fact here is that if you have two different random variables, then the variance for the sum of those variables is the same as just adding together the original two variances. ",
  "translatedText": "Buradaki temel gerçek şu ki, eğer iki farklı rastgele değişkeniniz varsa, o zaman bu değişkenlerin toplamının varyansı, orijinal iki varyansın toplanmasıyla aynı olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 830.49,
  "end": 839.37
 },
 {
  "input": "This is one of those facts that you can just compute when you unpack all the definitions. ",
  "translatedText": "Bu, tüm tanımları açtığınızda hesaplayabileceğiniz gerçeklerden biridir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 839.93,
  "end": 843.63
 },
 {
  "input": "There are a couple nice intuitions for why it's true. ",
  "translatedText": "Bunun neden doğru olduğuna dair birkaç güzel sezgi var. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 843.63,
  "end": 846.21
 },
 {
  "input": "My tentative plan is to just actually make a series about probability and talk about things like intuitions underlying variance and its cousins there. ",
  "translatedText": "Geçici planım aslında olasılık hakkında bir dizi yapmak ve varyansın altında yatan sezgiler ve onun kuzenleri gibi şeyler hakkında konuşmak. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 846.63,
  "end": 853.53
 },
 {
  "input": "But right now, the main thing I want you to highlight is how it's the variance that adds, it's not the standard deviation that adds. ",
  "translatedText": "Ancak şu anda vurgulamanızı istediğim asıl şey, eklenen şeyin standart sapma değil, varyans olduğudur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 854.01,
  "end": 860.15
 },
 {
  "input": "So, critically, if you were to take n different realizations of the same random variable and ask what the sum looks like, the variance of that sum is n times the variance of your original variable, meaning the standard deviation, the square root of all this, is the square root of n times the original standard deviation. ",
  "translatedText": "Yani, kritik olarak, aynı rastgele değişkenin n farklı gerçekleşmesini alıp toplamın neye benzediğini sorarsanız, bu toplamın varyansı orijinal değişkeninizin varyansının n katıdır, yani standart sapma, hepsinin karekökü olur. bu, orijinal standart sapmanın n katının kareköküdür. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 860.41,
  "end": 878.25
 },
 {
  "input": "For example, back in our sequence of distributions, if we label the standard deviation of our initial one with sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on and so forth. ",
  "translatedText": "Örneğin, dağılım sıramıza geri dönersek, başlangıçtaki standart sapmamızı sigma ile etiketlersek, sonraki standart sapma 2 çarpı sigmanın karekökü olacak ve bundan sonra da şunun karekökü gibi görünecek: 3 çarpı sigma, vb. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 879.29,
  "end": 893.09
 },
 {
  "input": "This, like I said, is very important. ",
  "translatedText": "Bu dediğim gibi çok önemli. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.75,
  "end": 895.65
 },
 {
  "input": "It means that even though our distributions are getting spread out, they're not spreading out all that quickly, they only do so in proportion to the square root of the size of the sum. ",
  "translatedText": "Bu, dağılımlarımızın yayılmasına rağmen o kadar hızlı yayılmadıkları, yalnızca toplamın boyutunun kareköküyle orantılı olduğu anlamına geliyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.07,
  "end": 904.13
 },
 {
  "input": "As we prepare to make a more quantitative description of the central limit theorem, the core intuition I want you to keep in your head is that we'll basically realign all of these distributions so that their means line up together, and then rescale them so that all of the standard deviations are just going to be equal to 1. ",
  "translatedText": "Merkezi limit teoreminin daha niceliksel bir tanımını yapmaya hazırlanırken, aklınızda tutmanızı istediğim temel sezgi, temel olarak tüm bu dağılımları, ortalamaları birlikte sıralanacak şekilde yeniden hizalayacağız ve sonra bunları yeniden ölçeklendireceğiz. tüm standart sapmaların 1'e eşit olacağını. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 904.71,
  "end": 920.61
 },
 {
  "input": "And when we do that, the shape that results gets closer and closer to a certain universal shape, described with an elegant little function that we'll unpack in just a moment. ",
  "translatedText": "Ve bunu yaptığımızda, ortaya çıkan şekil, birazdan açacağımız zarif, küçük bir işlevle tanımlanan belirli bir evrensel şekle giderek daha da yaklaşıyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 921.29,
  "end": 929.87
 },
 {
  "input": "And let me say one more time, the real magic here is how we could have started with any distribution, describing a single roll of the die, and if we play the same game, considering what the distributions for the many different sums look like, and we realign them so that the means line up, and we rescale them so that the standard deviations are all 1, we still approach that same universal shape, which is kind of mind-boggling. ",
  "translatedText": "Ve bir kez daha söylememe izin verin, buradaki gerçek sihir, tek bir zar atışını tanımlayarak herhangi bir dağılımla nasıl başlayabileceğimizdir ve eğer aynı oyunu oynarsak, birçok farklı toplamın dağılımlarının neye benzediğini düşünürsek, ve onları ortalamalar aynı hizaya gelecek şekilde yeniden ayarlıyoruz ve standart sapmaların tümü 1 olacak şekilde yeniden ölçeklendiriyoruz, yine de aynı evrensel şekle yaklaşıyoruz ki bu biraz kafa karıştırıcı. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.47,
  "end": 952.95
 },
 {
  "input": "And now, my friends, is probably as good a time as any to finally get into the formula for a normal distribution. ",
  "translatedText": "Ve şimdi dostlarım, normal dağılım formülüne nihayet girmenin tam zamanı. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 954.81,
  "end": 960.85
 },
 {
  "input": "And the way I'd like to do this is to basically peel back all the layers and build it up one piece at a time. ",
  "translatedText": "Ve bunu yapmak istediğim yol temelde tüm katmanları soymak ve her seferinde tek parça oluşturmak. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 961.49,
  "end": 965.93
 },
 {
  "input": "The function e to the x, or anything to the x, describes exponential growth, and if you make that exponent negative, which flips around the graph horizontally, you might think of it as describing exponential decay. ",
  "translatedText": "e'den x'e fonksiyonu veya x'e herhangi bir fonksiyon üstel büyümeyi tanımlar ve eğer grafiğin etrafında yatay olarak dönen bu üssü negatif yaparsanız, bunun üstel azalmayı tanımladığını düşünebilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.53,
  "end": 977.87
 },
 {
  "input": "To make this decay in both directions, you could do something to make sure the exponent is always negative and growing, like taking the negative absolute value. ",
  "translatedText": "Bu bozunumun her iki yönde olmasını sağlamak için, üssün her zaman negatif ve büyüyen olmasını sağlayacak bir şeyler yapabilirsiniz, örneğin negatif mutlak değeri almak gibi. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.51,
  "end": 985.43
 },
 {
  "input": "That would give us this kind of awkward sharp point in the middle, but if instead you make that exponent the negative square of x, you get a smoother version of the same thing, which decays in both directions. ",
  "translatedText": "Bu bize ortada tuhaf bir keskin nokta verirdi, ancak bunun yerine bu üssü x'in negatif karesi yaparsanız, aynı şeyin her iki yönde de azalan daha düzgün bir versiyonunu elde edersiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.93,
  "end": 995.81
 },
 {
  "input": "This gives us the basic bell curve shape. ",
  "translatedText": "Bu bize temel çan eğrisi şeklini verir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 996.33,
  "end": 998.19
 },
 {
  "input": "Now if you throw a constant in front of that x, and you scale that constant up and down, it lets you stretch and squish the graph horizontally, allowing you to describe narrow and wider bell curves. ",
  "translatedText": "Şimdi, bu x'in önüne bir sabit atarsanız ve bu sabiti yukarı ve aşağı ölçeklendirirseniz, grafiği yatay olarak uzatmanıza ve sıkıştırmanıza olanak tanır, böylece daha dar ve daha geniş çan eğrileri tanımlamanıza olanak tanır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.65,
  "end": 1008.37
 },
 {
  "input": "And a quick thing I'd like to point out here is that based on the rules of exponentiation, as we tweak around that constant c, you could also think about it as simply changing the base of the exponentiation. ",
  "translatedText": "Ve burada belirtmek istediğim kısa bir şey şu ki, üstel alma kurallarına dayanarak, c sabiti etrafında küçük değişiklikler yaptığımızda, bunu basitçe üstelin tabanını değiştirmek olarak da düşünebilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1009.01,
  "end": 1019.75
 },
 {
  "input": "And in that sense, the number e is not really all that special for our formula. ",
  "translatedText": "Ve bu anlamda e sayısı bizim formülümüz için o kadar da özel değil. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1020.15,
  "end": 1023.63
 },
 {
  "input": "We could replace it with any other positive constant, and you'll get the same family of curves as we tweak that constant. ",
  "translatedText": "Bunu başka herhangi bir pozitif sabitle değiştirebiliriz ve bu sabiti değiştirdiğimizde aynı eğri ailesini elde edersiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.05,
  "end": 1030.49
 },
 {
  "input": "Make it a 2, same family of curves. ",
  "translatedText": "Bunu 2 tane aynı eğri ailesi yapın. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.51,
  "end": 1033.11
 },
 {
  "input": "Make it a 3, same family of curves. ",
  "translatedText": "Bunu 3'lü, aynı eğri ailesi yapın. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.33,
  "end": 1035.07
 },
 {
  "input": "The reason we use e is that it gives that constant a very readable meaning. ",
  "translatedText": "E'yi kullanmamızın nedeni, bu sabite çok okunabilir bir anlam vermesidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1035.75,
  "end": 1039.49
 },
 {
  "input": "Or rather, if we reconfigure things a little bit so that the exponent looks like negative one half times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn this into a probability distribution, that constant sigma will be the standard deviation of that distribution. ",
  "translatedText": "Ya da daha doğrusu, eğer bazı şeyleri üs negatif yarım çarpı x bölü belirli bir sabite benzeyecek şekilde yeniden düzenlersek, buna anlamlı bir şekilde sigma kare diyeceğiz, o zaman bunu bir olasılık dağılımına dönüştürdüğümüzde, o sabit sigma şöyle olacaktır: bu dağılımın standart sapması olsun. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.11,
  "end": 1057.21
 },
 {
  "input": "And that's very nice. ",
  "translatedText": "Ve bu çok hoş. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.81,
  "end": 1058.57
 },
 {
  "input": "But before we can interpret this as a probability distribution, we need the area under the curve to be 1. ",
  "translatedText": "Ancak bunu bir olasılık dağılımı olarak yorumlayabilmemiz için önce eğrinin altındaki alanın 1 olmasına ihtiyacımız var. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.91,
  "end": 1064.31
 },
 {
  "input": "And the reason for that is how the curve is interpreted. ",
  "translatedText": "Bunun nedeni de eğrinin nasıl yorumlandığıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1064.83,
  "end": 1066.91
 },
 {
  "input": "Unlike discrete distributions, when it comes to something continuous, you don't ask about the probability of a particular point. ",
  "translatedText": "Kesikli dağılımlardan farklı olarak sürekli bir şey söz konusu olduğunda belirli bir noktanın olasılığını sormazsınız. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1067.37,
  "end": 1073.37
 },
 {
  "input": "Instead, you ask for the probability that a value falls between two different values. ",
  "translatedText": "Bunun yerine bir değerin iki farklı değer arasında olma olasılığını sorarsınız. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1073.79,
  "end": 1078.23
 },
 {
  "input": "And what the curve is telling you is that that probability equals the area under the curve between those two values. ",
  "translatedText": "Eğrinin size söylediği şey, olasılığın bu iki değer arasındaki eğrinin altındaki alana eşit olduğudur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.75,
  "end": 1085.43
 },
 {
  "input": "There's a whole other video about this, they're called probability density functions. ",
  "translatedText": "Bununla ilgili başka bir video daha var, bunlara olasılık yoğunluk fonksiyonları deniyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1086.03,
  "end": 1089.43
 },
 {
  "input": "The main point right now is that the area under the entire curve represents the probability that something happens, that some number comes up. ",
  "translatedText": "Şu andaki ana nokta, tüm eğrinin altındaki alanın bir şeyin olma, bir sayının ortaya çıkma olasılığını temsil etmesidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1089.83,
  "end": 1097.15
 },
 {
  "input": "That should be 1, which is why we want the area under this to be 1. ",
  "translatedText": "Bu 1 olmalı, bu yüzden bunun altındaki alanın 1 olmasını istiyoruz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1097.41,
  "end": 1100.63
 },
 {
  "input": "As it stands with the basic bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root of pi. ",
  "translatedText": "Temel çan eğrisi şekli olan e üzeri negatif x kare ile aynı olduğundan, alan 1 değil, aslında pi'nin kareköküdür. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1101.05,
  "end": 1107.79
 },
 {
  "input": "I know, right? ",
  "translatedText": "Doğruyu biliyorum? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.41,
  "end": 1109.15
 },
 {
  "input": "What is pi doing here? ",
  "translatedText": "Pi'nin burada ne işi var? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1109.27,
  "end": 1110.19
 },
 {
  "input": "What does this have to do with circles? ",
  "translatedText": "Bunun çevrelerle ne alakası var? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1110.29,
  "end": 1111.47
 },
 {
  "input": "Like I said at the start, I'd love to talk all about that in the next video. ",
  "translatedText": "Başlangıçta söylediğim gibi, bir sonraki videoda bunun hakkında konuşmayı çok isterim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.01,
  "end": 1115.05
 },
 {
  "input": "But if you can spare your excitement for our purposes right now, all it means is that we should divide this function by the square root of pi, and it gives us the area we want. ",
  "translatedText": "Ama şimdi heyecanınızı bizim amacımıza ayırabilirseniz, bu fonksiyonu pi'nin kareköküne bölmemiz gerektiği anlamına gelir ve bu bize istediğimiz alanı verir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.33,
  "end": 1123.17
 },
 {
  "input": "Throwing back in the constants we had earlier, the 1 half and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square root of 2. ",
  "translatedText": "Daha önce sahip olduğumuz sabitleri, 1 yarıyı ve sigmayı geri alırsak, buradaki etki, grafiği sigma çarpı 2'nin karekökü kadar uzatmak olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.61,
  "end": 1131.79
 },
 {
  "input": "So we also need to divide out by that in order to make sure it has an area of 1. ",
  "translatedText": "Alanının 1 olduğundan emin olmak için buna da bölmemiz gerekiyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.41,
  "end": 1136.47
 },
 {
  "input": "And combining those fractions, the factor out front looks like 1 divided by sigma times the square root of 2 pi. ",
  "translatedText": "Ve bu kesirleri birleştirdiğimizde, ön faktör 1 bölü sigma çarpı 2 pi'nin karekökü gibi görünüyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1136.47,
  "end": 1142.11
 },
 {
  "input": "This, finally, is a valid probability distribution. ",
  "translatedText": "Bu, son olarak, geçerli bir olasılık dağılımıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1142.91,
  "end": 1145.85
 },
 {
  "input": "As we tweak that value sigma, resulting in narrower and wider curves, that constant in the front always guarantees that the area equals 1. ",
  "translatedText": "Bu sigma değerini değiştirdiğimizde, daha dar ve daha geniş eğriler elde edilirken, ön taraftaki bu sabit her zaman alanın 1'e eşit olmasını garanti eder. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.45,
  "end": 1154.31
 },
 {
  "input": "The special case where sigma equals 1 has a specific name, we call it the standard normal distribution, which plays an especially important role for you and me in this lesson. ",
  "translatedText": "Sigma'nın 1'e eşit olduğu özel durumun özel bir adı vardır, biz buna standart normal dağılım diyoruz ve bu derste sizin ve benim için özellikle önemli bir rol oynuyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.91,
  "end": 1164.51
 },
 {
  "input": "And all possible normal distributions are not only parameterized with this value sigma, but we also subtract off another constant mu from the variable x, and this essentially just lets you slide the graph left and right so that you can prescribe the mean of this distribution. ",
  "translatedText": "Ve mümkün olan tüm normal dağılımlar sadece bu sigma değeriyle parametreleştirilmez, aynı zamanda x değişkeninden başka bir sabit mu çıkarırız ve bu aslında grafiği sola ve sağa kaydırmanıza olanak tanır, böylece bu dağılımın ortalamasını belirleyebilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.13,
  "end": 1180.21
 },
 {
  "input": "So in short, we have two parameters, one describing the mean, one describing the standard deviation, and they're all tied together in this big formula involving an e and a pi. ",
  "translatedText": "Kısaca, iki parametremiz var, biri ortalamayı, diğeri standart sapmayı tanımlıyor ve bunların hepsi bir e ve bir pi içeren bu büyük formülde birbirine bağlı. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.99,
  "end": 1189.19
 },
 {
  "input": "Now that all of that is on the table, let's look back again at the idea of starting with some random variable and asking what the distributions for sums of that variable look like. ",
  "translatedText": "Artık bunların hepsi masada olduğuna göre, bazı rastgele değişkenlerle başlama ve bu değişkenin toplamlarının dağılımlarının nasıl göründüğünü sorma fikrine tekrar bakalım. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.19,
  "end": 1199.81
 },
 {
  "input": "As we've already gone over, when you increase the size of that sum, the resulting distribution will shift according to a growing mean, and it slowly spreads out according to a growing standard deviation. ",
  "translatedText": "Daha önce de belirttiğimiz gibi, bu toplamın boyutunu artırdığınızda, ortaya çıkan dağılım büyüyen ortalamaya göre değişecek ve yavaş yavaş büyüyen standart sapmaya göre yayılacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1200.13,
  "end": 1209.81
 },
 {
  "input": "And putting some actual formulas to it, if we know the mean of our underlying random variable, we call it mu, and we also know its standard deviation, and we call it sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the standard deviation will be sigma times the square root of that size. ",
  "translatedText": "Ve buna bazı gerçek formüller koyarsak, eğer temel rastgele değişkenimizin ortalamasını biliyorsak, buna mu diyoruz ve standart sapmasını da biliyorsak ve buna sigma diyoruz, o zaman alttaki toplamın ortalaması mu olacaktır. çarpı toplamın boyutu ve standart sapma, bu boyutun karekökünün sigma katı olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1210.33,
  "end": 1227.73
 },
 {
  "input": "So now, if we want to claim that this looks more and more like a bell curve, and a bell curve is only described by two different parameters, the mean and the standard deviation, you know what to do. ",
  "translatedText": "Şimdi, bunun giderek daha çok bir çan eğrisine benzediğini ve bir çan eğrisinin yalnızca ortalama ve standart sapma olmak üzere iki farklı parametreyle tanımlandığını iddia etmek istersek ne yapacağınızı biliyorsunuz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1228.19,
  "end": 1237.71
 },
 {
  "input": "You could plug those two values into the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve that should closely fit our distribution. ",
  "translatedText": "Bu iki değeri formüle yerleştirebilirsiniz ve bu size, dağılımımıza çok iyi uyması gereken bir eğri için oldukça açık, ancak biraz karmaşık bir formül verir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1237.93,
  "end": 1246.99
 },
 {
  "input": "But there's another way we can describe it that's a little more elegant and lends itself to a very fun visual that we can build up to. ",
  "translatedText": "Ancak onu tanımlamanın biraz daha şık ve çok eğlenceli bir görselliğe sahip olan başka bir yolu daha var. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1248.39,
  "end": 1254.81
 },
 {
  "input": "Instead of focusing on the sum of all of these random variables, let's modify this expression a little bit, where what we'll do is we'll look at the mean that we expect that sum to take, and we subtract it off so that our new expression has a mean of 0, and then we're going to look at the standard deviation we expect of our sum, and divide out by that, which basically just rescales the units so that the standard deviation of our expression will equal 1. ",
  "translatedText": "Tüm bu rastgele değişkenlerin toplamına odaklanmak yerine, hadi bu ifadeyi biraz değiştirelim; burada yapacağımız şey, bu toplamın almasını beklediğimiz ortalamaya bakmak ve bunu çıkarmaktır, böylece yeni ifademizin ortalaması 0'dır ve sonra toplamımız için beklediğimiz standart sapmaya bakacağız ve buna böleceğiz, bu temelde birimleri ifademizin standart sapması 1 olacak şekilde yeniden ölçeklendirir. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1255.27,
  "end": 1278.77
 },
 {
  "input": "This might seem like a more complicated expression, but it actually has a highly readable meaning. ",
  "translatedText": "Bu daha karmaşık bir ifade gibi görünebilir ancak aslında oldukça okunabilir bir anlama sahiptir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1279.35,
  "end": 1284.09
 },
 {
  "input": "It's essentially saying how many standard deviations away from the mean is this sum? ",
  "translatedText": "Esasen bu toplamın ortalamadan kaç standart sapma uzakta olduğunu söylüyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1284.45,
  "end": 1289.67
 },
 {
  "input": "For example, this bar here corresponds to a certain value that you might find when you roll 10 dice and you add them all up, and its position a little above negative 1 is telling you that that value is a little bit less than one standard deviation lower than the mean. ",
  "translatedText": "Örneğin, buradaki bu çubuk, 10 zar attığınızda ve hepsini topladığınızda bulabileceğiniz belirli bir değere karşılık gelir ve eksi 1'in biraz üzerinde konumu, bu değerin bir standart sapmadan biraz daha küçük olduğunu gösterir. ortalamadan daha düşüktür. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.75,
  "end": 1303.87
 },
 {
  "input": "Also, by the way, in anticipation for the animation I'm trying to build to here, the way I'm representing things on that lower plot is that the area of each one of these bars is telling us the probability of the corresponding value rather than the height. ",
  "translatedText": "Ayrıca, bu arada, burada oluşturmaya çalıştığım animasyonun beklentisiyle, alt grafikteki şeyleri temsil etme şeklim, bu çubukların her birinin alanının bize karşılık gelen değerin olasılığını söylemesidir. yükseklikten ziyade. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1305.13,
  "end": 1316.99
 },
 {
  "input": "You might think of the y-axis as representing not probability but a kind of probability density. ",
  "translatedText": "Y ekseninin olasılığı değil, bir tür olasılık yoğunluğunu temsil ettiğini düşünebilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.23,
  "end": 1321.93
 },
 {
  "input": "The reason for this is to set the stage so that it aligns with the way we interpret continuous distributions, where the probability of falling between a range of values is equal to an area under a curve between those values. ",
  "translatedText": "Bunun nedeni, bir değer aralığı arasına düşme olasılığının bu değerler arasındaki bir eğrinin altındaki alana eşit olduğu, sürekli dağılımları yorumlama şeklimizle aynı hizada olacak şekilde sahneyi ayarlamaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1322.27,
  "end": 1333.55
 },
 {
  "input": "In particular, the area of all the bars together is going to be 1. ",
  "translatedText": "Özellikle tüm çubukların alanı 1 olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1333.91,
  "end": 1336.73
 },
 {
  "input": "Now, with all of that in place, let's have a little fun. ",
  "translatedText": "Şimdi tüm bunları yerine getirdiğimize göre, biraz eğlenelim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1338.23,
  "end": 1340.95
 },
 {
  "input": "Let me start by rolling things back so that the distribution on the bottom represents a relatively small sum, like adding together only three such random variables. ",
  "translatedText": "Alttaki dağılım nispeten küçük bir toplamı temsil edecek şekilde işleri geriye doğru yuvarlayarak başlayayım; tıpkı bu türden yalnızca üç rastgele değişkenin bir araya getirilmesi gibi. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1341.33,
  "end": 1349.01
 },
 {
  "input": "Notice what happens as I change the distribution we start with. ",
  "translatedText": "Başladığımız dağıtımı değiştirdiğimde ne olduğuna dikkat edin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.45,
  "end": 1352.43
 },
 {
  "input": "As it changes, the distribution on the bottom completely changes its shape. ",
  "translatedText": "Değiştikçe alttaki dağılım tamamen şekil değiştiriyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1352.73,
  "end": 1356.29
 },
 {
  "input": "It's very dependent on what we started with. ",
  "translatedText": "Neyle başladığımıza çok bağlı. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1356.51,
  "end": 1358.77
 },
 {
  "input": "If we let the size of our sum get a little bit bigger, say going up to 10, and as I change the distribution for x, it largely stays looking like a bell curve, but I can find some distributions that get it to change shape. ",
  "translatedText": "Toplamımızın boyutunun biraz daha büyümesine izin verirsek, örneğin 10'a kadar çıkarsak ve x'in dağılımını değiştirdiğimde, büyük ölçüde çan eğrisi gibi görünmeye devam eder, ancak şeklini değiştirmesini sağlayacak bazı dağılımlar bulabilirim . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.35,
  "end": 1371.63
 },
 {
  "input": "For example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results in this kind of spiky bell curve, and if you'll recall, earlier on I actually showed this in the form of a simulation. ",
  "translatedText": "Örneğin, neredeyse tüm olasılığın 1 veya 6 sayılarında olduğu gerçekten orantısız olan, bu tür dikenli çan eğrisiyle sonuçlanır ve hatırlarsanız, daha önce bunu aslında bir simülasyon şeklinde göstermiştim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1372.23,
  "end": 1383.51
 },
 {
  "input": "So if you were wondering whether that spikiness was an artifact of the randomness or reflected the true distribution, turns out it reflects the true distribution. ",
  "translatedText": "Yani bu dikenliliğin rastgeleliğin bir sonucu mu olduğunu yoksa gerçek dağılımı mı yansıttığını merak ediyorsanız, bunun gerçek dağılımı yansıttığı ortaya çıkar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1384.13,
  "end": 1391.85
 },
 {
  "input": "In this case, 10 is not a large enough sum for the central limit theorem to kick in. ",
  "translatedText": "Bu durumda 10, merkezi limit teoreminin devreye girmesi için yeterince büyük bir toplam değildir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.29,
  "end": 1396.47
 },
 {
  "input": "But if instead I let that sum grow and I consider adding 50 different values, which is actually not that big, then no matter how I change the distribution for our underlying random variable, it has essentially no effect on the shape of the plot on the bottom. ",
  "translatedText": "Ama bunun yerine bu toplamın büyümesine izin verirsem ve 50 farklı değer eklemeyi düşünürsem, ki bu aslında o kadar da büyük değil, o zaman temeldeki rastgele değişkenimizin dağılımını ne kadar değiştirirsem değiştireyim, bunun aslında grafikteki grafiğin şekli üzerinde hiçbir etkisi olmaz. alt. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1396.47,
  "end": 1410.69
 },
 {
  "input": "No matter where we start, all of the information and nuance for the distribution of x gets washed away, and we tend towards this single universal shape described by a very elegant function for the standard normal distribution, 1 over square root of 2 pi times e to the negative x squared over 2. ",
  "translatedText": "Nereden başlarsak başlayalım, x'in dağılımına ilişkin tüm bilgi ve nüanslar silinip gider ve standart normal dağılım için çok zarif bir fonksiyonla tanımlanan bu tek evrensel şekle doğru yöneliriz, 1 bölü karekök 2 pi çarpı e. negatif x kare bölü 2'ye. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1411.17,
  "end": 1427.07
 },
 {
  "input": "This, this right here is what the central limit theorem is all about. ",
  "translatedText": "Merkezi limit teoreminin konusu tam da burası. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1427.81,
  "end": 1430.81
 },
 {
  "input": "Almost nothing you can do to this initial distribution changes the shape we tend towards. ",
  "translatedText": "Bu ilk dağılıma yapabileceğiniz neredeyse hiçbir şey yöneldiğimiz şekli değiştirmez. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1431.13,
  "end": 1435.31
 },
 {
  "input": "Now, the more theoretically minded among you might still be wondering, what is the actual theorem? ",
  "translatedText": "Şimdi, aranızda teorik olarak daha fazla düşünenler hala gerçek teoremin ne olduğunu merak ediyor olabilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1439.03,
  "end": 1444.51
 },
 {
  "input": "Like, what's the mathematical statement that could be proved or disproved that we're claiming here? ",
  "translatedText": "Mesela burada iddia ettiğimiz kanıtlanabilecek veya çürütülebilecek matematiksel ifade nedir? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1444.81,
  "end": 1448.91
 },
 {
  "input": "If you want a nice formal statement, here's how it might go. ",
  "translatedText": "Güzel ve resmi bir açıklama istiyorsanız, işte böyle olabilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1449.03,
  "end": 1451.67
 },
 {
  "input": "Consider this value, where we're summing up n different instantiations of our random variable, but tweaked and tuned so that its mean and standard deviation are 1. ",
  "translatedText": "Rastgele değişkenimizin n farklı örneğini özetlediğimiz, ancak ortalaması ve standart sapması 1 olacak şekilde ayarladığımız ve ayarladığımız bu değeri düşünün. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1452.13,
  "end": 1459.89
 },
 {
  "input": "Again, meaning you can read it as asking how many standard deviations away from the mean is the sum. ",
  "translatedText": "Yine, bunu ortalamadan kaç standart sapmanın toplam olduğunu sormak olarak okuyabilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1460.23,
  "end": 1465.35
 },
 {
  "input": "Then the actual rigorous no-jokes-this-time statement of the central limit theorem is that if you consider the probability that this value falls between two given real numbers, a and b, and you consider the limit of that probability as the size of your sum goes to infinity, then that limit is equal to a certain integral, which basically describes the area under a standard normal distribution between those two values. ",
  "translatedText": "O zaman merkezi limit teoreminin bu seferki şakasız gerçek ifadesi şudur: Bu değerin verilen iki gerçek sayı, a ve b arasında olma olasılığını dikkate alırsanız ve bu olasılığın sınırını, Toplamınız sonsuza giderse, o zaman bu limit belirli bir integrale eşit olur ve bu da temel olarak bu iki değer arasındaki standart normal dağılım altındaki alanı tanımlar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1465.77,
  "end": 1489.65
 },
 {
  "input": "Again, there are three underlying assumptions that I have yet to tell you, but other than those, in all of its gory detail, this right here is the central limit theorem. ",
  "translatedText": "Yine, size henüz söylemediğim üç temel varsayım var, ancak bunların dışında, tüm kanlı ayrıntılarıyla, buradaki merkezi limit teoremidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1491.25,
  "end": 1500.03
 },
 {
  "input": "All of that is a bit theoretical, so it might be helpful to bring things back down to Earth and turn back to the concrete example that I mentioned at the start, where you imagine rolling a die 100 times, and let's assume it's a fair die for this example, and you add together the results. ",
  "translatedText": "Bunların hepsi biraz teorik, bu yüzden işleri Dünya'ya geri getirmek ve başlangıçta bahsettiğim somut örneğe geri dönmek faydalı olabilir; burada bir zarı 100 kez attığınızı hayal edin ve bunun adil bir zar olduğunu varsayalım. bu örnek için, sonuçları bir araya getiriyorsunuz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1504.55,
  "end": 1518.13
 },
 {
  "input": "The challenge for you is to find a range of values such that you're 95% sure that the sum will fall within this range. ",
  "translatedText": "Sizin için zorluk, toplamın bu aralığa gireceğinden %95 emin olacağınız bir değer aralığı bulmaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1518.87,
  "end": 1525.83
 },
 {
  "input": "For questions like this, there's a handy rule of thumb about normal distributions, which is that about 68% of your values are going to fall within one standard deviation of the mean, 95% of your values, the thing we care about, fall within two standard deviations of the mean, and a whopping 99.7% of your values will fall within three standard deviations of the mean. ",
  "translatedText": "Bunun gibi sorular için, normal dağılımlarla ilgili kullanışlı bir genel kural vardır; bu, değerlerinizin yaklaşık %68'inin ortalamanın bir standart sapması dahilinde olacağı, değerlerinizin %95'inin, yani bizim önemsediğimiz şeyin, ortalamanın bir standart sapması dahilinde olacağıdır. ortalamanın iki standart sapması ve muazzam bir 99. Değerlerinizin %7'si ortalamanın üç standart sapması dahilinde olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1527.13,
  "end": 1546.97
 },
 {
  "input": "It's a rule of thumb that's commonly memorized by people who do a lot of probability and stats. ",
  "translatedText": "Bu, çok fazla olasılık ve istatistikle ilgilenen kişilerin yaygın olarak ezberlediği bir temel kuraldır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1547.45,
  "end": 1551.45
 },
 {
  "input": "Naturally, this gives us what we need for our example, and let me go ahead and draw out what this would look like, where I'll show the distribution for a fair die up at the top, and the distribution for a sum of 100 such dice on the bottom, which by now as you know looks like a certain normal distribution. ",
  "translatedText": "Doğal olarak, bu bize örneğimiz için ihtiyacımız olan şeyi veriyor ve devam edip bunun neye benzeyeceğini çizeyim, burada adil bir zar için dağılımı en üstte göstereceğim ve toplam 100 dağılımını göstereceğim. altta böyle bir zar var ki, bildiğiniz gibi bu, belli bir normal dağılıma benziyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.49,
  "end": 1567.29
 },
 {
  "input": "Step one with a problem like this is to find the mean of your initial distribution, which in this case will look like 1 6th times 1 plus 1 6th times 2 on and on and on, and works out to be 3.5. ",
  "translatedText": "Bunun gibi bir problemde birinci adım, başlangıç dağılımınızın ortalamasını bulmaktır; bu durumda bu, 1 6'ncı çarpı 1 artı 1 6'ncı çarpı 2'nin devamı ve devamı gibi görünecek ve 3 olarak sonuçlanacaktır. 5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1567.95,
  "end": 1578.91
 },
 {
  "input": "We also need the standard deviation, which requires calculating the variance, which as you know involves adding all the squares of the differences between the values and the means, and it works out to be 2.92, square root of that comes out to be 1.71. ",
  "translatedText": "Ayrıca, varyansın hesaplanmasını gerektiren standart sapmaya da ihtiyacımız var; bu, bildiğiniz gibi, değerler ve ortalamalar arasındaki farkların tüm karelerinin eklenmesini içerir ve 2 olarak sonuçlanır. 92, bunun karekökü 1 çıkıyor. 71. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1579.41,
  "end": 1592.43
 },
 {
  "input": "Those are the only two numbers we need, and I will invite you again to reflect on how magical it is that those are the only two numbers that you need to completely understand the bottom distribution. ",
  "translatedText": "İhtiyacımız olan yegane iki sayı bunlar ve sizi alt dağılımı tamamen anlamak için ihtiyaç duyduğunuz yegâne iki sayı olmasının ne kadar sihirli olduğunu düşünmeye tekrar davet edeceğim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1592.95,
  "end": 1601.69
 },
 {
  "input": "Its mean will be 100 times mu, which is 350, and its standard deviation will be the square root of 100 times sigma, so 10 times sigma 17.1. ",
  "translatedText": "Ortalaması 100 çarpı mu, yani 350 olacaktır ve standart sapması 100 çarpı sigma'nın karekökü, yani 10 çarpı sigma 17 olacaktır. 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1602.43,
  "end": 1612.61
 },
 {
  "input": "Remembering our handy rule of thumb, we're looking for values two standard deviations away from the mean, and when you subtract 2 sigma from the mean you end up with about 316, and when you add 2 sigma you end up with 384. ",
  "translatedText": "Kullanışlı temel kuralımızı hatırlayarak, ortalamadan iki standart sapma uzaktaki değerleri arıyoruz ve ortalamadan 2 sigma çıkardığınızda yaklaşık 316 elde edersiniz ve 2 sigma eklediğinizde 384 elde edersiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1613.03,
  "end": 1626.33
 },
 {
  "input": "And there you go, that gives us the answer. ",
  "translatedText": "Ve işte bu bize cevabı veriyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1627.35,
  "end": 1628.95
 },
 {
  "input": "Okay, I promised to wrap things up shortly, but while we're on this example there's one more question that's worth your time to ponder. ",
  "translatedText": "Tamam, konuyu kısa sürede toparlayacağıma söz verdim, ancak bu örnek üzerindeyken düşünmeye zaman ayırmaya değer bir soru daha var. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.47,
  "end": 1637.45
 },
 {
  "input": "Instead of just asking about the sum of 100 die rolls, let's say I had you divide that number by 100, which basically means all the numbers in our diagram in the bottom get divided by 100. ",
  "translatedText": "Sadece 100 zar atışının toplamını sormak yerine, diyelim ki bu sayıyı 100'e bölmenizi istedim, bu da temel olarak alttaki şemamızdaki tüm sayıların 100'e bölünmesi anlamına geliyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1638.25,
  "end": 1648.09
 },
 {
  "input": "Take a moment to interpret what this all would be saying then. ",
  "translatedText": "O zaman tüm bunların ne anlama geldiğini yorumlamak için bir dakikanızı ayırın. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1648.57,
  "end": 1651.57
 },
 {
  "input": "The expression essentially tells you the empirical average for 100 different die rolls, and that interval we found is now telling you what range you are expecting to see for that empirical average. ",
  "translatedText": "İfade aslında size 100 farklı kalıp atışının ampirik ortalamasını söyler ve bulduğumuz bu aralık artık size o ampirik ortalama için hangi aralığı görmeyi beklediğinizi söyler. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1652.07,
  "end": 1663.49
 },
 {
  "input": "In other words, you might expect it to be around 3.5, that's the expected value for a die roll, but what's much less obvious and what the central limit theorem lets you compute is how close to that expected value you'll reasonably find yourself. ",
  "translatedText": "Başka bir deyişle, 3 civarında olmasını bekleyebilirsiniz. 5, bu bir zar atışının beklenen değeridir, ancak çok daha az belirgin olan ve merkezi limit teoreminin hesaplamanıza izin verdiği şey, kendinizi makul bir şekilde bu beklenen değere ne kadar yakın bulacağınızdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1664.35,
  "end": 1676.57
 },
 {
  "input": "In particular, it's worth your time to take a moment mulling over what the standard deviation for this empirical average is, and what happens to it as you look at a bigger and bigger sample of die rolls. ",
  "translatedText": "Özellikle, bu ampirik ortalama için standart sapmanın ne olduğu ve giderek daha büyük kalıp ruloları örneğine baktığınızda buna ne olacağı üzerinde düşünmek için biraz zaman ayırmaya değer. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1677.59,
  "end": 1687.13
 },
 {
  "input": "Lastly, but probably most importantly, let's talk about the assumptions that go into this theorem. ",
  "translatedText": "Son olarak ama muhtemelen en önemlisi, bu teoremin içerdiği varsayımlardan bahsedelim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.95,
  "end": 1697.41
 },
 {
  "input": "The first one is that all of these variables that we're adding up are independent from each other. ",
  "translatedText": "Birincisi, topladığımız tüm bu değişkenlerin birbirinden bağımsız olmasıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1698.01,
  "end": 1702.53
 },
 {
  "input": "The outcome of one process doesn't influence the outcome of any other process. ",
  "translatedText": "Bir sürecin sonucu başka bir sürecin sonucunu etkilemez. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.85,
  "end": 1706.31
 },
 {
  "input": "The second is that all of these variables are drawn from the same distribution. ",
  "translatedText": "İkincisi ise bu değişkenlerin hepsinin aynı dağılımdan çekilmesidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1707.25,
  "end": 1710.95
 },
 {
  "input": "Both of these have been implicitly assumed with our dice example. ",
  "translatedText": "Zar örneğimizde bunların her ikisi de dolaylı olarak varsayılmıştır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1711.31,
  "end": 1714.39
 },
 {
  "input": "We've been treating the outcome of each die roll as independent from the outcome of all the others, and we're assuming that each die follows the same distribution. ",
  "translatedText": "Her zar atışının sonucunu diğerlerinin sonuçlarından bağımsız olarak ele alıyoruz ve her zarın aynı dağılımı takip ettiğini varsayıyoruz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1714.79,
  "end": 1722.03
 },
 {
  "input": "Sometimes in the literature you'll see these two assumptions lumped together under the initials IID for independent and identically distributed. ",
  "translatedText": "Bazen literatürde bu iki varsayımın bağımsız ve aynı şekilde dağıtılmış anlamına gelen IID baş harfleri altında bir araya toplandığını göreceksiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1722.85,
  "end": 1729.91
 },
 {
  "input": "One situation where these assumptions are decidedly not true would be the Galton board. ",
  "translatedText": "Bu varsayımların kesinlikle doğru olmadığı durumlardan biri de Galton kuruludur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1730.53,
  "end": 1735.11
 },
 {
  "input": "I mean, think about it. ",
  "translatedText": "Yani, bir düşün. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.71,
  "end": 1736.83
 },
 {
  "input": "Is it the case that the way a ball bounces off of one of the pegs is independent from how it's going to bounce off the next peg? ",
  "translatedText": "Bir topun çivilerden birinden sekme şeklinin bir sonraki çividen nasıl sekeceğinden bağımsız olduğu bir durum mu var? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1736.97,
  "end": 1743.19
 },
 {
  "input": "Absolutely not. ",
  "translatedText": "Kesinlikle hayır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1743.83,
  "end": 1744.61
 },
 {
  "input": "Depending on the last bounce, it's coming in with a completely different trajectory. ",
  "translatedText": "Son sıçramaya bağlı olarak tamamen farklı bir yörüngeyle geliyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1744.77,
  "end": 1747.87
 },
 {
  "input": "And is it the case that the distribution of possible outcomes off of each peg are the same for each peg that it hits? ",
  "translatedText": "Ve her bir çivinin olası sonuçlarının dağılımı, çarptığı her çivi için aynı mıdır? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1748.21,
  "end": 1754.67
 },
 {
  "input": "Again, almost certainly not. ",
  "translatedText": "Yine, neredeyse kesinlikle hayır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1755.19,
  "end": 1756.71
 },
 {
  "input": "Maybe it hits one peg glancing to the left, meaning the outcomes are hugely skewed in that direction, and then hits the next one glancing to the right. ",
  "translatedText": "Belki sola bakarken bir çiviye çarpıyor, bu da sonuçların o yönde büyük ölçüde çarpık olduğu anlamına geliyor ve ardından sağa bakarken bir sonraki çiviye çarpıyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1756.71,
  "end": 1763.71
 },
 {
  "input": "When I made all those simplifying assumptions in the opening example, it wasn't just to make this easier to think about. ",
  "translatedText": "Açılış örneğinde tüm bu basitleştirici varsayımları yapmamın amacı sadece bunun hakkında düşünmeyi kolaylaştırmak değildi. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1765.73,
  "end": 1771.63
 },
 {
  "input": "It's also that those assumptions were necessary for this to actually be an example of the central limit theorem. ",
  "translatedText": "Aynı zamanda bu varsayımların aslında merkezi limit teoreminin bir örneği olması için gerekli olduğu da söylenebilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1771.97,
  "end": 1777.07
 },
 {
  "input": "Nevertheless, it seems to be true that for the real Galton board, despite violating both of these, a normal distribution does kind of come about? ",
  "translatedText": "Bununla birlikte, gerçek Galton kurulu için her ikisini de ihlal etmesine rağmen normal bir dağılımın ortaya çıktığı doğru gibi görünüyor? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1778.13,
  "end": 1785.47
 },
 {
  "input": "Part of the reason might be that there are generalizations of the theorem beyond the scope of this video that relax these assumptions, especially the second one. ",
  "translatedText": "Bunun bir nedeni, teoremin bu videonun kapsamı dışında bu varsayımları, özellikle de ikinci varsayımları gevşeten genellemelerinin bulunması olabilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1786.05,
  "end": 1793.89
 },
 {
  "input": "But I do want to caution you against the fact that many times people seem to assume that a variable is normally distributed, even when there's no actual justification to do so. ",
  "translatedText": "Ancak sizi, çoğu zaman, gerçek bir gerekçe olmasa bile, insanların bir değişkenin normal şekilde dağıldığını varsaydığı gerçeğine karşı uyarmak isterim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1794.49,
  "end": 1803.07
 },
 {
  "input": "The third assumption is actually fairly subtle. ",
  "translatedText": "Üçüncü varsayım aslında oldukça incelikli. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1804.29,
  "end": 1806.21
 },
 {
  "input": "It's that the variance we've been computing for these variables is finite. ",
  "translatedText": "Bu değişkenler için hesapladığımız varyansın sonlu olması. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1806.21,
  "end": 1810.27
 },
 {
  "input": "This was never an issue for the dice example, because there were only six possible outcomes. ",
  "translatedText": "Zar örneğinde bu hiçbir zaman sorun olmadı çünkü yalnızca altı olası sonuç vardı. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1810.81,
  "end": 1814.85
 },
 {
  "input": "But in certain situations where you have an infinite set of outcomes, when you go to compute the variance, the sum ends up diverging off to infinity. ",
  "translatedText": "Ancak sonsuz sayıda sonucun olduğu belirli durumlarda, varyansı hesaplamaya gittiğinizde, toplam sonsuza doğru sapar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1815.03,
  "end": 1822.51
 },
 {
  "input": "These can be perfectly valid probability distributions, and they do come up in practice. ",
  "translatedText": "Bunlar tamamen geçerli olasılık dağılımları olabilir ve pratikte ortaya çıkarlar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1823.45,
  "end": 1827.25
 },
 {
  "input": "But in those situations, as you consider adding many different instantiations of that variable and letting that sum approach infinity, even if the first two assumptions hold, it is very much a possibility that the thing you tend towards is not actually a normal distribution. ",
  "translatedText": "Ancak bu durumlarda, bu değişkenin birçok farklı örneğini eklemeyi düşündüğünüzde ve bu toplamın sonsuza yaklaşmasına izin verdiğinizde, ilk iki varsayım geçerli olsa bile, yöneldiğiniz şeyin aslında normal bir dağılım olmaması büyük bir olasılıktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1827.55,
  "end": 1841.19
 },
 {
  "input": "If you've understood everything up to this point, you now have a very strong foundation in what the central limit theorem is all about. ",
  "translatedText": "Bu noktaya kadar her şeyi anladıysanız, artık merkezi limit teoreminin neyle ilgili olduğu konusunda çok güçlü bir temele sahipsiniz demektir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1842.15,
  "end": 1847.65
 },
 {
  "input": "And next up, I'd like to explain why it is that this particular function is the thing that we tend towards, and why it has a pi in it, what it has to do with circles. ",
  "translatedText": "Daha sonra, neden bu özel fonksiyonun yöneldiğimiz bir şey olduğunu, neden içinde bir pi bulunduğunu, bunun çemberlerle ne ilgisi olduğunu açıklamak istiyorum. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1848.29,
  "end": 1874.17
 }
]