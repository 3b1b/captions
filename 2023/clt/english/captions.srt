1
00:00:00,000 --> 00:00:01,260
This is a Galton board.

2
00:00:02,520 --> 00:00:05,887
Maybe you've seen one before, it's a popular demonstration of how, 

3
00:00:05,887 --> 00:00:10,309
even when a single event is chaotic and random, with an effectively unknowable outcome, 

4
00:00:10,309 --> 00:00:14,279
it's still possible to make precise statements about a large number of events, 

5
00:00:14,279 --> 00:00:18,300
namely how the relative proportions for many different outcomes are distributed.

6
00:00:20,380 --> 00:00:24,202
More specifically, the Galton board illustrates one of the most prominent 

7
00:00:24,202 --> 00:00:27,715
distributions in all probability, known as the normal distribution, 

8
00:00:27,715 --> 00:00:31,900
more colloquially known as a bell curve, and also called a Gaussian distribution.

9
00:00:32,500 --> 00:00:36,365
There's a very specific function to describe this distribution, it's very pretty, 

10
00:00:36,365 --> 00:00:40,089
we'll get into it later, but right now I just want to emphasize how the normal 

11
00:00:40,089 --> 00:00:42,541
distribution is, as the name suggests, very common, 

12
00:00:42,541 --> 00:00:45,040
it shows up in a lot of seemingly unrelated contexts.

13
00:00:46,020 --> 00:00:49,578
If you were to take a large number of people who sit in a similar demographic 

14
00:00:49,578 --> 00:00:53,000
and plot their heights, those heights tend to follow a normal distribution.

15
00:00:53,660 --> 00:00:56,780
If you look at a large swath of very big natural numbers, 

16
00:00:56,780 --> 00:01:01,139
and you ask how many distinct prime factors does each one of those numbers have, 

17
00:01:01,139 --> 00:01:04,959
the answers will very closely track with a certain normal distribution.

18
00:01:05,580 --> 00:01:09,787
Now our topic for today is one of the crown jewels in all of probability theory, 

19
00:01:09,787 --> 00:01:14,202
it's one of the key facts that explains why this distribution is as common as it is, 

20
00:01:14,202 --> 00:01:16,020
known as the central limit theorem.

21
00:01:16,640 --> 00:01:20,651
This lesson is meant to go back to the basics, 

22
00:01:20,651 --> 00:01:25,260
giving you the fundamentals on what the background is.

23
00:01:25,260 --> 00:01:29,364
We're going to go decently deep into it, but after this I'd still like to 

24
00:01:29,364 --> 00:01:31,971
go deeper and explain why the theorem is true, 

25
00:01:31,971 --> 00:01:36,075
why the function underlying the normal distribution has the very specific 

26
00:01:36,075 --> 00:01:39,791
form that it does, why that formula has a pi in it, and, most fun, 

27
00:01:39,791 --> 00:01:44,062
why those last two facts are actually more related than a lot of traditional 

28
00:01:44,062 --> 00:01:45,560
explanations would suggest.

29
00:01:46,480 --> 00:01:50,047
That second lesson is also meant to be the follow-on to the convolutions 

30
00:01:50,047 --> 00:01:53,370
video that I promised, so there's a lot of interrelated topics here.

31
00:01:53,570 --> 00:01:56,254
But right now, back to the fundamentals, I'd like to kick 

32
00:01:56,254 --> 00:01:59,170
things off with an overly simplified model of the Galton board.

33
00:02:00,890 --> 00:02:05,246
In this model we will assume that each ball falls directly onto a certain central peg, 

34
00:02:05,246 --> 00:02:09,102
and that it has a 50-50 probability of bouncing to the left or to the right, 

35
00:02:09,102 --> 00:02:13,459
and we'll think of each of those outcomes as either adding one or subtracting one from 

36
00:02:13,459 --> 00:02:14,110
its position.

37
00:02:14,670 --> 00:02:18,734
Once one of those is chosen, we make the highly unrealistic assumption that it 

38
00:02:18,734 --> 00:02:22,233
happens to land dead on in the middle of the peg adjacent below it, 

39
00:02:22,233 --> 00:02:26,401
where again it'll be faced with the same 50-50 choice of bouncing to the left or 

40
00:02:26,401 --> 00:02:27,070
to the right.

41
00:02:27,430 --> 00:02:31,116
For the one I'm showing on screen, there are five different rows of pegs, 

42
00:02:31,116 --> 00:02:35,101
so our little hopping ball makes five different random choices between plus one 

43
00:02:35,101 --> 00:02:38,936
and minus one, and we can think of its final position as basically being the 

44
00:02:38,936 --> 00:02:42,772
sum of all of those different numbers, which in this case happens to be one, 

45
00:02:42,772 --> 00:02:46,856
and we might label all of the different buckets with the sum that they represent, 

46
00:02:46,856 --> 00:02:51,290
as we repeat this we're looking at different possible sums for those five random numbers.

47
00:02:53,050 --> 00:02:57,342
And for those of you who are inclined to complain that this is a highly unrealistic model 

48
00:02:57,342 --> 00:03:01,635
for the true Galton board, let me emphasize the goal right now is not to accurately model 

49
00:03:01,635 --> 00:03:05,785
physics, the goal is to give a simple example to illustrate the central limit theorem, 

50
00:03:05,785 --> 00:03:10,030
and for that, idealized though this might be, it actually gives us a really good example.

51
00:03:10,570 --> 00:03:13,740
If we let many different balls fall, making yet another unrealistic 

52
00:03:13,740 --> 00:03:17,236
assumption that they don't influence each other, as if they're all ghosts, 

53
00:03:17,236 --> 00:03:20,406
then the number of balls that fall into each different bucket gives 

54
00:03:20,406 --> 00:03:23,390
us some loose sense for how likely each one of those buckets is.

55
00:03:23,830 --> 00:03:26,898
In this example, the numbers are simple enough that it's not too hard to 

56
00:03:26,898 --> 00:03:30,010
explicitly calculate what the probability is for falling into each bucket.

57
00:03:30,270 --> 00:03:34,018
If you do want to think that through, you'll find it very reminiscent of Pascal's 

58
00:03:34,018 --> 00:03:37,858
triangle, but the neat thing about our theorem is how far it goes beyond the simple 

59
00:03:37,858 --> 00:03:38,270
examples.

60
00:03:38,670 --> 00:03:41,683
So to start off at least, rather than making explicit calculations, 

61
00:03:41,683 --> 00:03:45,494
let's just simulate things by running a large number of samples and letting the total 

62
00:03:45,494 --> 00:03:49,482
number of results in each different outcome give us some sense for what that distribution 

63
00:03:49,482 --> 00:03:49,970
looks like.

64
00:03:50,450 --> 00:03:53,275
As I said, the one on screen has five rows, so each 

65
00:03:53,275 --> 00:03:56,210
sum that we're considering includes only five numbers.

66
00:03:56,810 --> 00:04:01,901
The basic idea of the central limit theorem is that if you increase the size of that sum, 

67
00:04:01,901 --> 00:04:06,144
for example here would mean increasing the number of rows of pegs for each 

68
00:04:06,144 --> 00:04:10,218
ball to bounce off, then the distribution that describes where that sum 

69
00:04:10,218 --> 00:04:13,330
is going to fall looks more and more like a bell curve.

70
00:04:15,470 --> 00:04:18,350
Here, it's actually worth taking a moment to write down that general idea.

71
00:04:19,269 --> 00:04:23,647
The setup is that we have a random variable, and that's basically shorthand for 

72
00:04:23,647 --> 00:04:28,190
a random process where each outcome of that process is associated with some number.

73
00:04:28,490 --> 00:04:29,970
We'll call that random number x.

74
00:04:29,970 --> 00:04:34,390
For example, each bounce off the peg is a random process modeled with two outcomes.

75
00:04:34,850 --> 00:04:37,890
Those outcomes are associated with the numbers negative one and positive one.

76
00:04:38,530 --> 00:04:41,397
Another example of a random variable would be rolling a die, 

77
00:04:41,397 --> 00:04:44,830
where you have six different outcomes, each one associated with a number.

78
00:04:45,470 --> 00:04:47,742
What we're doing is taking multiple different 

79
00:04:47,742 --> 00:04:50,410
samples of that variable and adding them all together.

80
00:04:50,770 --> 00:04:54,232
On our Galton board, that looks like letting the ball bounce off multiple 

81
00:04:54,232 --> 00:04:57,601
different pegs on its way down to the bottom, and in the case of a die, 

82
00:04:57,601 --> 00:05:00,970
you might imagine rolling many different dice and adding up the results.

83
00:05:01,430 --> 00:05:05,674
The claim of the central limit theorem is that as you let the size of that sum 

84
00:05:05,674 --> 00:05:08,790
get bigger and bigger, then the distribution of that sum, 

85
00:05:08,790 --> 00:05:11,853
how likely it is to fall into different possible values, 

86
00:05:11,853 --> 00:05:14,110
will look more and more like a bell curve.

87
00:05:15,430 --> 00:05:17,130
That's it, that is the general idea.

88
00:05:17,550 --> 00:05:21,530
Over the course of this lesson, our job is to make that statement more quantitative.

89
00:05:22,070 --> 00:05:24,606
We're going to put some numbers to it, put some formulas to it, 

90
00:05:24,606 --> 00:05:26,350
show how you can use it to make predictions.

91
00:05:27,210 --> 00:05:29,345
For example, here's the kind of question I want 

92
00:05:29,345 --> 00:05:31,570
you to be able to answer by the end of this video.

93
00:05:32,190 --> 00:05:35,890
Suppose you rolled a die 100 times and you added together the results.

94
00:05:36,630 --> 00:05:39,400
Could you find a range of values such that you're 

95
00:05:39,400 --> 00:05:42,170
95% sure that the sum will fall within that range?

96
00:05:42,830 --> 00:05:46,550
Or maybe I should say find the smallest possible range of values such that this is true.

97
00:05:47,390 --> 00:05:49,915
The neat thing is you'll be able to answer this question 

98
00:05:49,915 --> 00:05:52,130
whether it's a fair die or if it's a weighted die.

99
00:05:53,450 --> 00:05:56,746
Now let me say at the top that this theorem has three different assumptions 

100
00:05:56,746 --> 00:06:00,130
that go into it, three things that have to be true before the theorem follows.

101
00:06:00,430 --> 00:06:03,790
And I'm actually not going to tell you what they are until the very end of the video.

102
00:06:04,270 --> 00:06:07,032
Instead I want you to keep your eye out and see if you can notice 

103
00:06:07,032 --> 00:06:09,670
and maybe predict what those three assumptions are going to be.

104
00:06:10,710 --> 00:06:13,936
As a next step, to better illustrate just how general this theorem is, 

105
00:06:13,936 --> 00:06:17,390
I want to run a couple more simulations for you focused on the dice example.

106
00:06:20,910 --> 00:06:24,223
Usually if you think of rolling a die you think of the six outcomes as 

107
00:06:24,223 --> 00:06:27,630
being equally probable, but the theorem actually doesn't care about that.

108
00:06:27,830 --> 00:06:31,216
We could start with a weighted die, something with a non-trivial 

109
00:06:31,216 --> 00:06:34,550
distribution across the outcomes, and the core idea still holds.

110
00:06:35,030 --> 00:06:37,661
For the simulation what I'll do is take some distribution 

111
00:06:37,661 --> 00:06:39,930
like this one that is skewed towards lower values.

112
00:06:40,250 --> 00:06:43,872
I'm going to take 10 distinct samples from that distribution and 

113
00:06:43,872 --> 00:06:47,550
then I'll record the sum of that sample on the plot on the bottom.

114
00:06:48,630 --> 00:06:52,610
Then I'm going to do this many many different times, always with a sum of size 10, 

115
00:06:52,610 --> 00:06:56,590
but keep track of where those sums ended up to give us a sense of the distribution.

116
00:06:59,970 --> 00:07:02,422
And in fact let me rescale the y direction to give 

117
00:07:02,422 --> 00:07:04,730
us room to run an even larger number of samples.

118
00:07:05,030 --> 00:07:07,891
And I'll let it go all the way up to a couple thousand, 

119
00:07:07,891 --> 00:07:12,490
and as it does you'll notice that the shape that starts to emerge looks like a bell curve.

120
00:07:12,870 --> 00:07:16,461
Maybe if you squint your eyes you can see it skews a tiny bit to the left, 

121
00:07:16,461 --> 00:07:20,483
but it's neat that something so symmetric emerged from a starting point that was so 

122
00:07:20,483 --> 00:07:21,010
asymmetric.

123
00:07:21,470 --> 00:07:24,737
To better illustrate what the central limit theorem is all about, 

124
00:07:24,737 --> 00:07:27,212
let me run four of these simulations in parallel, 

125
00:07:27,212 --> 00:07:31,221
where on the upper left I'm doing it where we're only adding two dice at a time, 

126
00:07:31,221 --> 00:07:34,885
on the upper right we're doing it where we're adding five dice at a time, 

127
00:07:34,885 --> 00:07:38,300
the lower left is the one that we just saw adding 10 dice at a time, 

128
00:07:38,300 --> 00:07:41,370
and then we'll do another one with a bigger sum, 15 at a time.

129
00:07:42,250 --> 00:07:45,110
Notice how on the upper left when we're just adding two dice, 

130
00:07:45,110 --> 00:07:48,154
the resulting distribution doesn't really look like a bell curve, 

131
00:07:48,154 --> 00:07:52,030
it looks a lot more reminiscent of the one we started with, skewed towards the left.

132
00:07:52,810 --> 00:07:55,428
But as we allow for more and more dice in each sum, 

133
00:07:55,428 --> 00:07:59,810
the resulting shape that comes up in these distributions looks more and more symmetric.

134
00:07:59,950 --> 00:08:03,890
It has the lump in the middle and fade towards the tail's shape of a bell curve.

135
00:08:07,050 --> 00:08:10,490
And let me emphasize again, you can start with any different distribution.

136
00:08:10,490 --> 00:08:13,990
Here I'll run it again, but where most of the probability is tied up 

137
00:08:13,990 --> 00:08:17,490
in the numbers 1 and 6, with very low probability for the mid values.

138
00:08:18,190 --> 00:08:22,124
Despite completely changing the distribution for an individual roll of the die, 

139
00:08:22,124 --> 00:08:26,550
it's still the case that a bell curve shape will emerge as we consider the different sums.

140
00:08:27,270 --> 00:08:30,384
Illustrating things with a simulation like this is very fun, 

141
00:08:30,384 --> 00:08:33,141
and it's kind of neat to see order emerge from chaos, 

142
00:08:33,141 --> 00:08:35,030
but it also feels a little imprecise.

143
00:08:35,390 --> 00:08:38,559
Like in this case, when I cut off the simulation at 3000 samples, 

144
00:08:38,559 --> 00:08:40,865
even though it kind of looks like a bell curve, 

145
00:08:40,865 --> 00:08:43,891
the different buckets seem pretty spiky, and you might wonder, 

146
00:08:43,891 --> 00:08:47,157
is it supposed to look that way, or is that just an artifact of the 

147
00:08:47,157 --> 00:08:48,550
randomness in the simulation?

148
00:08:49,010 --> 00:08:52,151
And if it is, how many samples do we need before we can be sure that 

149
00:08:52,151 --> 00:08:55,110
what we're looking at is representative of the true distribution?

150
00:08:59,190 --> 00:09:02,376
Instead moving forward, let's get a little more theoretical and show 

151
00:09:02,376 --> 00:09:05,470
the precise shape these distributions will take on in the long run.

152
00:09:06,130 --> 00:09:10,367
The easiest case to make this calculation is if we have a uniform distribution, 

153
00:09:10,367 --> 00:09:13,970
where each possible face of the die has an equal probability, 1 6th.

154
00:09:13,990 --> 00:09:18,271
For example, if you then want to know how likely different sums are for a pair of dice, 

155
00:09:18,271 --> 00:09:21,726
it's essentially a counting game, where you count up how many distinct 

156
00:09:21,726 --> 00:09:24,694
pairs take on the same sum, which in the diagram I've drawn, 

157
00:09:24,694 --> 00:09:28,490
you can conveniently think about by going through all the different diagonals.

158
00:09:31,410 --> 00:09:34,266
Since each such pair has an equal chance of showing up, 

159
00:09:34,266 --> 00:09:37,530
1 in 36, all you have to do is count the sizes of these buckets.

160
00:09:38,190 --> 00:09:42,428
That gives us a definitive shape for the distribution describing a sum of two dice, 

161
00:09:42,428 --> 00:09:45,708
and if we were to play the same game with all possible triplets, 

162
00:09:45,708 --> 00:09:48,130
the resulting distribution would look like this.

163
00:09:48,690 --> 00:09:51,292
Now what's more challenging, but a lot more interesting, 

164
00:09:51,292 --> 00:09:54,990
is to ask what happens if we have a non-uniform distribution for that single die.

165
00:09:55,550 --> 00:09:57,970
We actually talked all about this in the last video.

166
00:09:58,450 --> 00:10:00,966
You do essentially the same thing, you go through all 

167
00:10:00,966 --> 00:10:03,670
the distinct pairs of dice which add up to the same value.

168
00:10:03,970 --> 00:10:06,478
It's just that instead of counting those pairs, 

169
00:10:06,478 --> 00:10:10,868
for each pair you multiply the two probabilities of each particular face coming up, 

170
00:10:10,868 --> 00:10:12,750
and then you add all those together.

171
00:10:13,290 --> 00:10:16,682
The computation that does this for all possible sums has a fancy name, 

172
00:10:16,682 --> 00:10:20,361
it's called a convolution, but it's essentially just the weighted version of 

173
00:10:20,361 --> 00:10:24,470
the counting game that anyone who's played with a pair of dice already finds familiar.

174
00:10:25,030 --> 00:10:28,944
For our purposes in this lesson, I'll have the computer calculate all that, 

175
00:10:28,944 --> 00:10:33,064
simply display the results for you, and invite you to observe certain patterns, 

176
00:10:33,064 --> 00:10:35,330
but under the hood, this is what's going on.

177
00:10:36,650 --> 00:10:39,924
So just to be crystal clear on what's being represented here, 

178
00:10:39,924 --> 00:10:43,779
if you imagine sampling two different values from that top distribution, 

179
00:10:43,779 --> 00:10:46,895
the one describing a single die, and adding them together, 

180
00:10:46,895 --> 00:10:50,804
then the second distribution I'm drawing represents how likely you are to 

181
00:10:50,804 --> 00:10:52,230
see various different sums.

182
00:10:52,890 --> 00:10:57,068
Likewise, if you imagine sampling three distinct values from that top distribution, 

183
00:10:57,068 --> 00:11:00,500
and adding them together, the next plot represents the probabilities 

184
00:11:00,500 --> 00:11:02,490
for various different sums in that case.

185
00:11:03,510 --> 00:11:08,002
So if I compute what the distributions for these sums look like for larger and larger 

186
00:11:08,002 --> 00:11:12,390
sums, well you know what I'm going to say, it looks more and more like a bell curve.

187
00:11:13,350 --> 00:11:16,450
But before we get to that, I want you to make a couple more simple observations.

188
00:11:17,450 --> 00:11:20,892
For example, these distributions seem to be wandering to the right, 

189
00:11:20,892 --> 00:11:24,790
and also they seem to be getting more spread out, and a little bit more flat.

190
00:11:25,250 --> 00:11:27,911
You cannot describe the central limit theorem quantitatively 

191
00:11:27,911 --> 00:11:30,136
without taking into account both of those effects, 

192
00:11:30,136 --> 00:11:33,190
which in turn requires describing the mean and the standard deviation.

193
00:11:33,950 --> 00:11:37,664
Maybe you're already familiar with those, but I want to make minimal assumptions here, 

194
00:11:37,664 --> 00:11:40,610
and it never hurts to review, so let's quickly go over both of those.

195
00:11:43,410 --> 00:11:47,199
The mean of a distribution, often denoted with the Greek letter mu, 

196
00:11:47,199 --> 00:11:50,710
is a way of capturing the center of mass for that distribution.

197
00:11:51,190 --> 00:11:54,431
It's calculated as the expected value of our random variable, 

198
00:11:54,431 --> 00:11:58,614
which is a way of saying you go through all of the different possible outcomes, 

199
00:11:58,614 --> 00:12:02,850
and you multiply the probability of that outcome times the value of the variable.

200
00:12:03,190 --> 00:12:06,410
If higher values are more probable, that weighted sum is going to be bigger.

201
00:12:06,750 --> 00:12:09,950
If lower values are more probable, that weighted sum is going to be smaller.

202
00:12:10,790 --> 00:12:14,681
A little more interesting is if you want to measure how spread out this distribution is, 

203
00:12:14,681 --> 00:12:17,130
because there's multiple different ways you might do it.

204
00:12:18,530 --> 00:12:20,290
One of them is called the variance.

205
00:12:20,830 --> 00:12:25,367
The idea there is to look at the difference between each possible value and the mean, 

206
00:12:25,367 --> 00:12:28,270
square that difference, and ask for its expected value.

207
00:12:28,730 --> 00:12:31,577
The idea is that whether your value is below or above the mean, 

208
00:12:31,577 --> 00:12:34,247
when you square that difference, you get a positive number, 

209
00:12:34,247 --> 00:12:36,650
and the larger the difference, the bigger that number.

210
00:12:37,370 --> 00:12:41,165
Squaring it like this turns out to make the math much much nicer than if we did 

211
00:12:41,165 --> 00:12:45,150
something like an absolute value, but the downside is that it's hard to think about 

212
00:12:45,150 --> 00:12:48,044
this as a distance in our diagram because the units are off, 

213
00:12:48,044 --> 00:12:52,123
kind of like the units here are square units, whereas a distance in our diagram would 

214
00:12:52,123 --> 00:12:53,310
be a kind of linear unit.

215
00:12:53,710 --> 00:12:57,298
So another way to measure spread is what's called the standard deviation, 

216
00:12:57,298 --> 00:12:59,190
which is the square root of this value.

217
00:12:59,470 --> 00:13:03,574
That can be interpreted much more reasonably as a distance on our diagram, 

218
00:13:03,574 --> 00:13:06,585
and it's commonly denoted with the Greek letter sigma, 

219
00:13:06,585 --> 00:13:09,650
so you know m for standard deviation, but both in Greek.

220
00:13:11,870 --> 00:13:13,965
Looking back at our sequence of distributions, 

221
00:13:13,965 --> 00:13:16,150
let's talk about the mean and standard deviation.

222
00:13:16,630 --> 00:13:19,295
If we call the mean of the initial distribution mu, 

223
00:13:19,295 --> 00:13:21,859
which for the one illustrated happens to be 2.24, 

224
00:13:21,859 --> 00:13:25,191
hopefully it won't be too surprising if I tell you that the mean 

225
00:13:25,191 --> 00:13:26,730
of the next one is 2 times mu.

226
00:13:27,130 --> 00:13:30,631
That is, you roll a pair of dice, you want to know the expected value of the sum, 

227
00:13:30,631 --> 00:13:32,810
it's two times the expected value for a single die.

228
00:13:33,850 --> 00:13:39,410
Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth.

229
00:13:39,630 --> 00:13:41,708
The mean just marches steadily on to the right, 

230
00:13:41,708 --> 00:13:44,870
which is why our distributions seem to be drifting off in that direction.

231
00:13:45,350 --> 00:13:47,559
A little more challenging, but very important, 

232
00:13:47,559 --> 00:13:49,910
is to describe how the standard deviation changes.

233
00:13:50,490 --> 00:13:53,905
The key fact here is that if you have two different random variables, 

234
00:13:53,905 --> 00:13:56,881
then the variance for the sum of those variables is the same 

235
00:13:56,881 --> 00:13:59,370
as just adding together the original two variances.

236
00:13:59,930 --> 00:14:03,630
This is one of those facts that you can just compute when you unpack all the definitions.

237
00:14:03,630 --> 00:14:06,210
There are a couple nice intuitions for why it's true.

238
00:14:06,630 --> 00:14:10,033
My tentative plan is to just actually make a series about probability and 

239
00:14:10,033 --> 00:14:13,530
talk about things like intuitions underlying variance and its cousins there.

240
00:14:14,010 --> 00:14:18,196
But right now, the main thing I want you to highlight is how it's the variance that adds, 

241
00:14:18,196 --> 00:14:20,150
it's not the standard deviation that adds.

242
00:14:20,410 --> 00:14:24,926
So, critically, if you were to take n different realizations of the same random 

243
00:14:24,926 --> 00:14:29,273
variable and ask what the sum looks like, the variance of sum is n times the 

244
00:14:29,273 --> 00:14:33,112
variance of your original variable, meaning the standard deviation, 

245
00:14:33,112 --> 00:14:37,685
the square root of all this, is the square root of n times the original standard 

246
00:14:37,685 --> 00:14:38,250
deviation.

247
00:14:39,290 --> 00:14:42,034
For example, back in our sequence of distributions, 

248
00:14:42,034 --> 00:14:45,517
if we label the standard deviation of our initial one with sigma, 

249
00:14:45,517 --> 00:14:49,844
then the next standard deviation is going to be the square root of 2 times sigma, 

250
00:14:49,844 --> 00:14:54,014
and after that it looks like the square root of 3 times sigma, and so on This, 

251
00:14:54,014 --> 00:14:55,650
like I said, is very important.

252
00:14:56,070 --> 00:14:59,048
It means that even though our distributions are getting spread out, 

253
00:14:59,048 --> 00:15:01,676
they're not spreading out all that quickly, they only do so 

254
00:15:01,676 --> 00:15:04,130
in proportion to the square root of the size of the sum.

255
00:15:04,710 --> 00:15:08,794
As we prepare to make a more quantitative description of the central limit theorem, 

256
00:15:08,794 --> 00:15:12,830
the core intuition I want you to keep in your head is that we'll basically realign 

257
00:15:12,830 --> 00:15:15,990
all of these distributions so that their means line up together, 

258
00:15:15,990 --> 00:15:19,977
and then rescale them so that all of the standard deviations are just going to be 

259
00:15:19,977 --> 00:15:20,610
equal to one.

260
00:15:21,290 --> 00:15:25,727
And when we do that, the shape that results gets closer and closer to a certain universal 

261
00:15:25,727 --> 00:15:29,870
shape, described with an elegant little function that we'll unpack in just a moment.

262
00:15:30,470 --> 00:15:34,851
And let me say one more time, the real magic here is how we could have started with 

263
00:15:34,851 --> 00:15:39,284
any distribution, describing a single roll of the die, and if we play the same game, 

264
00:15:39,284 --> 00:15:43,144
considering what the distributions for the many different sums look like, 

265
00:15:43,144 --> 00:15:45,595
and we realign them so that the means line up, 

266
00:15:45,595 --> 00:15:48,986
and we rescale them so that the standard deviations are all one, 

267
00:15:48,986 --> 00:15:52,950
we still approach that same universal shape, which is kind of mind-boggling.

268
00:15:54,810 --> 00:15:57,749
And now, my friends, is probably as good a time as any 

269
00:15:57,749 --> 00:16:00,850
to finally get into the formula for a normal distribution.

270
00:16:01,490 --> 00:16:03,648
And the way I'd like to do this is to basically peel 

271
00:16:03,648 --> 00:16:05,930
back all the layers and build it up one piece at a time.

272
00:16:06,530 --> 00:16:10,610
The function e to the x, or anything to the x, describes exponential growth, 

273
00:16:10,610 --> 00:16:15,008
and if you make that exponent negative, which flips around the graph horizontally, 

274
00:16:15,008 --> 00:16:17,870
you might think of it as describing exponential decay.

275
00:16:18,510 --> 00:16:21,926
To make this decay in both directions, you could do something to make sure the 

276
00:16:21,926 --> 00:16:25,430
exponent is always negative and growing, like taking the negative absolute value.

277
00:16:25,930 --> 00:16:29,097
That would give us this kind of awkward sharp point in the middle, 

278
00:16:29,097 --> 00:16:32,122
but if instead you make that exponent the negative square of x, 

279
00:16:32,122 --> 00:16:35,810
you get a smoother version of the same thing, which decays in both directions.

280
00:16:36,330 --> 00:16:38,190
This gives us the basic bell curve shape.

281
00:16:38,650 --> 00:16:41,006
Now if you throw a constant in front of that x, 

282
00:16:41,006 --> 00:16:44,197
and you scale that constant up and down, it lets you stretch and 

283
00:16:44,197 --> 00:16:48,370
squish the graph horizontally, allowing you to describe narrow and wider bell curves.

284
00:16:49,010 --> 00:16:52,434
And a quick thing I'd like to point out here is that based on the 

285
00:16:52,434 --> 00:16:55,599
rules of exponentiation, as we tweak around that constant c, 

286
00:16:55,599 --> 00:16:59,750
you could also think about it as simply changing the base of the exponentiation.

287
00:17:00,150 --> 00:17:03,630
And in that sense, the number e is not really all that special for our formula.

288
00:17:04,050 --> 00:17:06,924
We could replace it with any other positive constant, 

289
00:17:06,924 --> 00:17:10,490
and you'll get the same family of curves as we tweak that constant.

290
00:17:11,510 --> 00:17:13,109
Make it a 2, same family of curves.

291
00:17:13,329 --> 00:17:15,069
Make it a 3, same family of curves.

292
00:17:15,750 --> 00:17:19,490
The reason we use e is that it gives that constant a very readable meaning.

293
00:17:20,109 --> 00:17:24,315
Or rather, if we reconfigure things a little bit so that the exponent looks 

294
00:17:24,315 --> 00:17:27,636
like negative 1 half times x divided by a certain constant, 

295
00:17:27,636 --> 00:17:31,786
which we'll suggestively call sigma squared, then once we turn this into a 

296
00:17:31,786 --> 00:17:36,047
probability distribution, that constant sigma will be the standard deviation 

297
00:17:36,047 --> 00:17:37,210
of that distribution.

298
00:17:37,810 --> 00:17:38,570
And that's very nice.

299
00:17:38,910 --> 00:17:42,201
But before we can interpret this as a probability distribution, 

300
00:17:42,201 --> 00:17:44,310
we need the area under the curve to be 1.

301
00:17:44,830 --> 00:17:46,910
And the reason for that is how the curve is interpreted.

302
00:17:47,370 --> 00:17:50,651
Unlike discrete distributions, when it comes to something continuous, 

303
00:17:50,651 --> 00:17:53,370
you don't ask about the probability of a particular point.

304
00:17:53,790 --> 00:17:58,230
Instead, you ask for the probability that a value falls between two different values.

305
00:17:58,750 --> 00:18:02,147
And what the curve is telling you is that that probability 

306
00:18:02,147 --> 00:18:05,430
equals the area under the curve between those two values.

307
00:18:06,030 --> 00:18:09,430
There's a whole other video about this, they're called probability density functions.

308
00:18:09,830 --> 00:18:13,747
The main point right now is that the area under the entire curve represents 

309
00:18:13,747 --> 00:18:17,150
the probability that something happens, that some number comes up.

310
00:18:17,410 --> 00:18:20,630
That should be 1, which is why we want the area under this to be 1.

311
00:18:21,050 --> 00:18:24,981
As it stands with the basic bell curve shape of e to the negative x squared, 

312
00:18:24,981 --> 00:18:27,790
the area is not 1, it's actually the square root of pi.

313
00:18:28,410 --> 00:18:29,150
I know, right?

314
00:18:29,270 --> 00:18:30,190
What is pi doing here?

315
00:18:30,290 --> 00:18:31,470
What does this have to do with circles?

316
00:18:32,010 --> 00:18:35,050
Like I said at the start, I'd love to talk all about that in the next video.

317
00:18:35,330 --> 00:18:38,253
But if you can spare your excitement, for our purposes right now, 

318
00:18:38,253 --> 00:18:41,708
all it means is that we should divide this function by the square root of pi, 

319
00:18:41,708 --> 00:18:43,170
and it gives us the area we want.

320
00:18:43,610 --> 00:18:47,261
Throwing back in the constants we had earlier, the one half and the sigma, 

321
00:18:47,261 --> 00:18:51,303
the effect there is to stretch out the graph by a factor of sigma times the square 

322
00:18:51,303 --> 00:18:51,790
root of 2.

323
00:18:52,410 --> 00:18:56,480
So we also need to divide out by that in order to make sure it has an area of 1, 

324
00:18:56,480 --> 00:18:59,647
and combining those fractions, the factor out front looks like 

325
00:18:59,647 --> 00:19:02,110
1 divided by sigma times the square root of 2 pi.

326
00:19:02,910 --> 00:19:05,850
This, finally, is a valid probability distribution.

327
00:19:06,450 --> 00:19:10,436
As we tweak that value sigma, resulting in narrower and wider curves, 

328
00:19:10,436 --> 00:19:14,310
that constant in the front always guarantees that the area equals 1.

329
00:19:15,910 --> 00:19:18,809
The special case where sigma equals 1 has a specific name, 

330
00:19:18,809 --> 00:19:23,035
we call it the standard normal distribution, which plays an especially important role 

331
00:19:23,035 --> 00:19:24,510
for you and me in this lesson.

332
00:19:25,130 --> 00:19:29,938
And all possible normal distributions are not only parameterized with this value sigma, 

333
00:19:29,938 --> 00:19:33,544
but we also subtract off another constant mu from the variable x, 

334
00:19:33,544 --> 00:19:37,314
and this essentially just lets you slide the graph left and right so 

335
00:19:37,314 --> 00:19:40,210
that you can prescribe the mean of this distribution.

336
00:19:40,990 --> 00:19:43,895
So in short, we have two parameters, one describing the mean, 

337
00:19:43,895 --> 00:19:48,065
one describing the standard deviation, and they're all tied together in this big formula 

338
00:19:48,065 --> 00:19:49,190
involving an e and a pi.

339
00:19:49,190 --> 00:19:54,441
Now that all of that is on the table, let's look back again at the idea of starting with 

340
00:19:54,441 --> 00:19:59,515
some random variable and asking what the distributions for sums of that variable look 

341
00:19:59,515 --> 00:19:59,810
like.

342
00:20:00,130 --> 00:20:03,372
As we've already gone over, when you increase the size of that sum, 

343
00:20:03,372 --> 00:20:06,567
the resulting distribution will shift according to a growing mean, 

344
00:20:06,567 --> 00:20:09,810
and it slowly spreads out according to a growing standard deviation.

345
00:20:10,330 --> 00:20:14,480
And putting some actual formulas to it, if we know the mean of our underlying 

346
00:20:14,480 --> 00:20:18,364
random variable, we call it mu, and we also know its standard deviation, 

347
00:20:18,364 --> 00:20:22,568
and we call it sigma, then the mean for the sum on the bottom will be mu times 

348
00:20:22,568 --> 00:20:26,772
the size of the sum, and the standard deviation will be sigma times the square 

349
00:20:26,772 --> 00:20:27,730
root of that size.

350
00:20:28,190 --> 00:20:31,892
So now, if we want to claim that this looks more and more like a bell curve, 

351
00:20:31,892 --> 00:20:34,969
and a bell curve is only described by two different parameters, 

352
00:20:34,969 --> 00:20:37,710
the mean and the standard deviation, you know what to do.

353
00:20:37,930 --> 00:20:42,382
You could plug those two values into the formula, and it gives you a highly explicit, 

354
00:20:42,382 --> 00:20:46,990
albeit kind of complicated, formula for a curve that should closely fit our distribution.

355
00:20:48,390 --> 00:20:51,456
But there's another way we can describe it that's a little more 

356
00:20:51,456 --> 00:20:54,810
elegant and lends itself to a very fun visual that we can build up to.

357
00:20:55,270 --> 00:20:58,576
Instead of focusing on the sum of all of these random variables, 

358
00:20:58,576 --> 00:21:02,492
let's modify this expression a little bit, where what we'll do is we'll look 

359
00:21:02,492 --> 00:21:06,358
at the mean that we expect that sum to take, and we subtract it off so that 

360
00:21:06,358 --> 00:21:10,173
our new expression has a mean of zero, and then we're going to look at the 

361
00:21:10,173 --> 00:21:13,479
standard deviation we expect of our sum, and divide out by that, 

362
00:21:13,479 --> 00:21:17,447
which basically just rescales the units so that the standard deviation of our 

363
00:21:17,447 --> 00:21:18,770
expression will equal one.

364
00:21:19,350 --> 00:21:21,865
This might seem like a more complicated expression, 

365
00:21:21,865 --> 00:21:24,090
but it actually has a highly readable meaning.

366
00:21:24,450 --> 00:21:29,670
It's essentially saying how many standard deviations away from the mean is this sum?

367
00:21:30,750 --> 00:21:34,977
For example, this bar here corresponds to a certain value that you might find when you 

368
00:21:34,977 --> 00:21:39,156
roll 10 dice and you add them all up, and its position a little above negative one is 

369
00:21:39,156 --> 00:21:43,432
telling you that that value is a little bit less than one standard deviation lower than 

370
00:21:43,432 --> 00:21:43,870
the mean.

371
00:21:45,130 --> 00:21:48,897
Also, by the way, in anticipation for the animation I'm trying to build to here, 

372
00:21:48,897 --> 00:21:52,757
the way I'm representing things on that lower plot is that the area of each one of 

373
00:21:52,757 --> 00:21:56,664
these bars is telling us the probability of the corresponding value rather than the 

374
00:21:56,664 --> 00:21:56,990
height.

375
00:21:57,230 --> 00:21:59,482
You might think of the y-axis as representing 

376
00:21:59,482 --> 00:22:01,930
not probability but a kind of probability density.

377
00:22:02,270 --> 00:22:05,996
The reason for this is to set the stage so that it aligns with the way we 

378
00:22:05,996 --> 00:22:09,873
interpret continuous distributions, where the probability of falling between 

379
00:22:09,873 --> 00:22:13,550
a range of values is equal to an area under a curve between those values.

380
00:22:13,910 --> 00:22:16,730
In particular, the area of all the bars together is going to be one.

381
00:22:18,230 --> 00:22:20,950
Now, with all of that in place, let's have a little fun.

382
00:22:21,330 --> 00:22:25,357
Let me start by rolling things back so that the distribution on the bottom represents 

383
00:22:25,357 --> 00:22:29,010
a relatively small sum, like adding together only three such random variables.

384
00:22:29,450 --> 00:22:32,430
Notice what happens as I change the distribution we start with.

385
00:22:32,730 --> 00:22:36,290
As it changes, the distribution on the bottom completely changes its shape.

386
00:22:36,510 --> 00:22:38,770
It's very dependent on what we started with.

387
00:22:40,350 --> 00:22:44,178
If we let the size of our sum get a little bit bigger, say going up to 10, 

388
00:22:44,178 --> 00:22:48,465
and as I change the distribution for x, it largely stays looking like a bell curve, 

389
00:22:48,465 --> 00:22:51,630
but I can find some distributions that get it to change shape.

390
00:22:52,230 --> 00:22:55,874
For example, the really lopsided one where almost all the probability 

391
00:22:55,874 --> 00:22:59,310
is in the numbers 1 or 6 results in this kind of spiky bell curve.

392
00:22:59,770 --> 00:23:03,510
And if you'll recall, earlier on I actually showed this in the form of a simulation.

393
00:23:04,130 --> 00:23:08,129
Though if you were wondering whether that spikiness was an artifact of the randomness 

394
00:23:08,129 --> 00:23:11,850
or reflected the true distribution, turns out it reflects the true distribution.

395
00:23:12,290 --> 00:23:16,470
In this case, 10 is not a large enough sum for the central limit theorem to kick in.

396
00:23:16,470 --> 00:23:20,752
But if instead I let that sum grow and I consider adding 50 different values, 

397
00:23:20,752 --> 00:23:25,419
which is actually not that big, then no matter how I change the distribution for our 

398
00:23:25,419 --> 00:23:30,086
underlying random variable, it has essentially no effect on the shape of the plot on 

399
00:23:30,086 --> 00:23:30,690
the bottom.

400
00:23:31,170 --> 00:23:34,835
No matter where we start, all of the information and nuance for the 

401
00:23:34,835 --> 00:23:38,500
distribution of x gets washed away, and we tend towards this single 

402
00:23:38,500 --> 00:23:42,273
universal shape described by a very elegant function for the standard 

403
00:23:42,273 --> 00:23:47,070
normal distribution, 1 over square root of 2 pi times e to the negative x squared over 2.

404
00:23:47,810 --> 00:23:50,810
This, this right here is what the central limit theorem is all about.

405
00:23:51,130 --> 00:23:55,310
Almost nothing you can do to this initial distribution changes the shape we tend towards.

406
00:23:59,030 --> 00:24:02,054
Now, the more theoretically minded among you might still be 

407
00:24:02,054 --> 00:24:05,431
wondering what is the actual theorem, like what's the mathematical 

408
00:24:05,431 --> 00:24:08,910
statement that could be proved or disproved that we're claiming here.

409
00:24:09,030 --> 00:24:11,670
If you want a nice formal statement, here's how it might go.

410
00:24:12,130 --> 00:24:16,640
Consider this value where we're summing up n different instantiations of our variable, 

411
00:24:16,640 --> 00:24:20,217
but tweaked and tuned so that its mean and standard deviation are 1, 

412
00:24:20,217 --> 00:24:24,520
again meaning you can read it as asking how many standard deviations away from the 

413
00:24:24,520 --> 00:24:25,350
mean is the sum.

414
00:24:25,770 --> 00:24:30,489
Then the actual rigorous no-jokes-this-time statement of the central limit theorem 

415
00:24:30,489 --> 00:24:35,322
is that if you consider the probability that this value falls between two given real 

416
00:24:35,322 --> 00:24:40,154
numbers, a and b, and you consider the limit of that probability as the size of your 

417
00:24:40,154 --> 00:24:44,134
sum goes to infinity, then that limit is equal to a certain integral, 

418
00:24:44,134 --> 00:24:49,024
which basically describes the area under a standard normal distribution between those 

419
00:24:49,024 --> 00:24:49,650
two values.

420
00:24:51,250 --> 00:24:55,146
Again, there are three underlying assumptions that I have yet to tell you, 

421
00:24:55,146 --> 00:24:57,692
but other than those, in all of its gory detail, 

422
00:24:57,692 --> 00:25:00,030
this right here is the central limit theorem.

423
00:25:04,550 --> 00:25:07,992
All of that is a bit theoretical, so it might be helpful to bring things 

424
00:25:07,992 --> 00:25:12,141
back down to earth and turn back to the concrete example that I mentioned at the start, 

425
00:25:12,141 --> 00:25:15,536
where you imagine rolling a die 100 times, and let's assume it's a fair 

426
00:25:15,536 --> 00:25:18,130
die for this example, and you add together the results.

427
00:25:18,870 --> 00:25:22,467
The challenge for you is to find a range of values such that 

428
00:25:22,467 --> 00:25:25,830
you're 95% sure that the sum will fall within this range.

429
00:25:27,130 --> 00:25:33,118
For questions like this, there's a handy rule of thumb about normal distributions, 

430
00:25:33,118 --> 00:25:38,023
which is that about 68% of your values are going to fall within two 

431
00:25:38,023 --> 00:25:43,002
standard deviations of the mean, and a whopping 99.7% of your values 

432
00:25:43,002 --> 00:25:46,970
will fall within three standard deviations of the mean.

433
00:25:47,450 --> 00:25:49,428
It's a rule of thumb that's commonly memorized 

434
00:25:49,428 --> 00:25:51,450
by people who do a lot of probability and stats.

435
00:25:52,490 --> 00:25:55,366
Naturally, this gives us what we need for our example, 

436
00:25:55,366 --> 00:25:58,504
and let me go ahead and draw out what this would look like, 

437
00:25:58,504 --> 00:26:01,798
where I'll show the distribution for a fair die up at the top, 

438
00:26:01,798 --> 00:26:04,884
and the distribution for a sum of 100 such dice on bottom, 

439
00:26:04,884 --> 00:26:07,290
which by now looks like a normal distribution.

440
00:26:07,950 --> 00:26:12,655
Step 1 with a problem like this is to find the mean of your initial distribution, 

441
00:26:12,655 --> 00:26:17,532
which in this case will look like 1 6th times 1 plus 1 6th times 2 on and on and on, 

442
00:26:17,532 --> 00:26:18,910
and works out to be 3.5.

443
00:26:19,410 --> 00:26:23,750
We also need the standard deviation, which requires calculating the variance, 

444
00:26:23,750 --> 00:26:28,034
which as you know involves adding all the squares of the differences between 

445
00:26:28,034 --> 00:26:32,430
the values and the means, and it works out to be 2.92, the square root of 1.71.

446
00:26:32,950 --> 00:26:35,743
Those are the only two numbers we need, and I will invite you 

447
00:26:35,743 --> 00:26:38,536
again to reflect on how magical it is that those are the only 

448
00:26:38,536 --> 00:26:41,690
two numbers you need to completely understand the bottom distribution.

449
00:26:42,430 --> 00:26:47,665
Its mean will be 100 times mu, which is 350, and its standard deviation 

450
00:26:47,665 --> 00:26:52,610
will be the square root of 100 times sigma, so 10 times sigma, 17.1.

451
00:26:53,030 --> 00:26:57,584
Remembering our handy rule of thumb, we're looking for values two standard 

452
00:26:57,584 --> 00:27:01,957
deviations away from the mean, and when you subtract 2 sigma from mean, 

453
00:27:01,957 --> 00:27:06,330
you end up with about 316, and when you add 2 sigma you end up with 384.

454
00:27:07,350 --> 00:27:08,950
There you go, that gives us the answer.

455
00:27:11,470 --> 00:27:14,855
Okay, I promised to wrap things up shortly, but while we're on this example, 

456
00:27:14,855 --> 00:27:17,450
there's one more question that's worth your time to ponder.

457
00:27:18,250 --> 00:27:21,128
Instead of just asking about the sum of 100 die rolls, 

458
00:27:21,128 --> 00:27:23,588
let's say I had you divide that number by 100, 

459
00:27:23,588 --> 00:27:28,090
which basically means all the numbers in our diagram in the bottom get divided by 100.

460
00:27:28,570 --> 00:27:31,570
Take a moment to interpret what this all would be saying then.

461
00:27:32,070 --> 00:27:37,171
The expression essentially tells you the empirical average for 100 different die rolls, 

462
00:27:37,171 --> 00:27:40,939
and that interval we found is now telling you what range you are 

463
00:27:40,939 --> 00:27:43,490
expecting to see for that empirical average.

464
00:27:44,350 --> 00:27:47,043
In other words, you might expect it to be around 3.5, 

465
00:27:47,043 --> 00:27:51,033
that's the expected value for a die roll, but what's much less obvious and what 

466
00:27:51,033 --> 00:27:54,973
the central limit theorem lets you compute is how close to that expected value 

467
00:27:54,973 --> 00:27:56,570
you'll reasonably find yourself.

468
00:27:57,590 --> 00:28:00,691
In particular, it's worth your time to take a moment mulling over 

469
00:28:00,691 --> 00:28:03,464
what the standard deviation for this empirical average is, 

470
00:28:03,464 --> 00:28:07,130
and what happens to it as you look at a bigger and bigger sample of die rolls.

471
00:28:12,950 --> 00:28:15,225
Lastly, but probably most importantly, let's talk 

472
00:28:15,225 --> 00:28:17,410
about the assumptions that go into this theorem.

473
00:28:18,010 --> 00:28:20,316
The first one is that all of these variables that 

474
00:28:20,316 --> 00:28:22,530
we're adding up are independent from each other.

475
00:28:22,850 --> 00:28:26,310
The outcome of one process doesn't influence the outcome of any other process.

476
00:28:27,250 --> 00:28:30,950
The second is that all of these variables are drawn from the same distribution.

477
00:28:31,310 --> 00:28:34,390
Both of these have been implicitly assumed with our dice example.

478
00:28:34,790 --> 00:28:38,387
We've been treating the outcome of each die roll as independent from the outcome 

479
00:28:38,387 --> 00:28:42,030
of all the others, and we're assuming that each die follows the same distribution.

480
00:28:42,850 --> 00:28:46,183
Sometimes in the literature you'll see these two assumptions lumped 

481
00:28:46,183 --> 00:28:49,910
together under the initials IID for independent and identically distributed.

482
00:28:50,530 --> 00:28:55,110
One situation where these assumptions are decidedly not true would be the Galton board.

483
00:28:55,710 --> 00:28:56,830
I mean, think about it.

484
00:28:56,970 --> 00:29:00,177
Is it the case that the way a ball bounces off of one of the pegs 

485
00:29:00,177 --> 00:29:03,190
is independent from how it's going to bounce off the next peg?

486
00:29:03,830 --> 00:29:04,610
Absolutely not.

487
00:29:04,770 --> 00:29:07,870
Depending on the last bounce, it's coming in with a completely different trajectory.

488
00:29:08,210 --> 00:29:11,633
And is it the case that the distribution of possible outcomes 

489
00:29:11,633 --> 00:29:14,670
off of each peg are the same for each peg that it hits?

490
00:29:15,190 --> 00:29:16,710
Again, almost certainly not.

491
00:29:16,710 --> 00:29:20,233
Maybe it hits one peg glancing to the left, meaning the outcomes are hugely 

492
00:29:20,233 --> 00:29:23,710
skewed in that direction, and then hits the next one glancing to the right.

493
00:29:25,730 --> 00:29:29,171
When I made all those simplifying assumptions in the opening example, 

494
00:29:29,171 --> 00:29:31,630
it wasn't just to make this easier to think about.

495
00:29:31,970 --> 00:29:34,565
It's also that those assumptions were necessary for this 

496
00:29:34,565 --> 00:29:37,070
to actually be an example of the central limit theorem.

497
00:29:38,130 --> 00:29:41,470
Nevertheless, it seems to be true that for the real Galton board, 

498
00:29:41,470 --> 00:29:45,470
despite violating both of these, a normal distribution does kind of come about?

499
00:29:46,050 --> 00:29:49,994
Part of the reason might be that there are generalizations of the theorem beyond 

500
00:29:49,994 --> 00:29:53,890
the scope of this video that relax these assumptions, especially the second one.

501
00:29:54,490 --> 00:29:58,828
But I do want to caution you against the fact that many times people seem to assume that 

502
00:29:58,828 --> 00:30:03,070
a variable is normally distributed, even when there's no actual justification to do so.

503
00:30:04,290 --> 00:30:06,210
The third assumption is actually fairly subtle.

504
00:30:06,210 --> 00:30:10,270
It's that the variance we've been computing for these variables is finite.

505
00:30:10,810 --> 00:30:13,162
This was never an issue for the dice example because 

506
00:30:13,162 --> 00:30:14,850
there were only six possible outcomes.

507
00:30:15,030 --> 00:30:18,544
But in certain situations where you have an infinite set of outcomes, 

508
00:30:18,544 --> 00:30:22,510
when you go to compute the variance, the sum ends up diverging off to infinity.

509
00:30:23,450 --> 00:30:27,250
These can be perfectly valid probability distributions, and they do come up in practice.

510
00:30:27,550 --> 00:30:30,674
But in those situations, as you consider adding many different 

511
00:30:30,674 --> 00:30:34,245
instantiations of that variable and letting that sum approach infinity, 

512
00:30:34,245 --> 00:30:37,717
even if the first two assumptions hold, it is very much a possibility 

513
00:30:37,717 --> 00:30:41,190
that the thing you tend towards is not actually a normal distribution.

514
00:30:42,150 --> 00:30:44,187
If you've understood everything up to this point, 

515
00:30:44,187 --> 00:30:47,650
you now have a very strong foundation in what the central limit theorem is all about.

516
00:30:48,290 --> 00:30:52,000
And next up, I'd like to explain why it is that this particular function is the 

517
00:30:52,000 --> 00:30:55,990
thing that we tend towards, and why it has a pi in it, what it has to do with circles.

518
00:31:11,950 --> 00:31:14,170
Thank you.

