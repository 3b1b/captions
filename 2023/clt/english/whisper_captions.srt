1
00:00:00,000 --> 00:00:05,340
This is a Galton board. Maybe you've seen one before, it's a popular demonstration of how,

2
00:00:05,560 --> 00:00:10,040
even when a single event is chaotic and random, with an effectively unknowable outcome,

3
00:00:10,500 --> 00:00:14,220
it's still possible to make precise statements about a large number of events,

4
00:00:14,600 --> 00:00:18,300
namely how the relative proportions for many different outcomes are distributed.

5
00:00:20,380 --> 00:00:24,680
More specifically, the Galton board illustrates one of the most prominent distributions in all

6
00:00:24,680 --> 00:00:29,620
probability, known as the normal distribution, more colloquially known as a bell curve,

7
00:00:29,980 --> 00:00:34,620
and also called a Gaussian distribution. There's a very specific function to describe this

8
00:00:34,620 --> 00:00:38,800
distribution, it's very pretty, we'll get into it later, but right now I just want to emphasize

9
00:00:38,800 --> 00:00:43,320
how the normal distribution is, as the name suggests, very common, it shows up in a lot

10
00:00:43,320 --> 00:00:48,540
of seemingly unrelated contexts. If you were to take a large number of people who sit in a similar

11
00:00:48,540 --> 00:00:54,360
demographic and plot their heights, those heights tend to follow a normal distribution. If you look

12
00:00:54,360 --> 00:00:59,780
at a large swath of very big natural numbers, and you ask how many distinct prime factors does each

13
00:00:59,780 --> 00:01:04,960
one of those numbers have, the answers will very closely track with a certain normal distribution.

14
00:01:05,580 --> 00:01:10,320
Now our topic for today is one of the crown jewels in all of probability theory, it's one of the key

15
00:01:10,320 --> 00:01:15,700
facts that explains why this distribution is as common as it is, known as the central limit

16
00:01:15,700 --> 00:01:24,340
theorem. This lesson is meant to go back to the basics, giving you the fundamentals on what the

17
00:01:24,340 --> 00:01:29,140
background is. We're going to go decently deep into it, but after this I'd still like to go deeper

18
00:01:29,140 --> 00:01:34,480
and explain why the theorem is true, why the function underlying the normal distribution has

19
00:01:34,480 --> 00:01:41,240
the very specific form that it does, why that formula has a pi in it, and, most fun, why those

20
00:01:41,240 --> 00:01:45,560
last two facts are actually more related than a lot of traditional explanations would suggest.

21
00:01:46,480 --> 00:01:50,690
That second lesson is also meant to be the follow-on to the convolutions video that I

22
00:01:50,690 --> 00:01:55,350
promised, so there's a lot of interrelated topics here. But right now, back to the fundamentals, I'd

23
00:01:55,350 --> 00:02:02,210
like to kick things off with an overly simplified model of the Galton board. In this model we will

24
00:02:02,210 --> 00:02:07,230
assume that each ball falls directly onto a certain central peg, and that it has a 50-50

25
00:02:07,230 --> 00:02:11,390
probability of bouncing to the left or to the right, and we'll think of each of those outcomes

26
00:02:11,390 --> 00:02:16,850
as either adding one or subtracting one from its position. Once one of those is chosen, we make the

27
00:02:16,850 --> 00:02:21,690
highly unrealistic assumption that it happens to land dead on in the middle of the peg adjacent

28
00:02:21,690 --> 00:02:26,850
below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the

29
00:02:26,850 --> 00:02:31,670
right. For the one I'm showing on screen, there are five different rows of pegs, so our little hopping

30
00:02:31,670 --> 00:02:36,550
ball makes five different random choices between plus one and minus one, and we can think of its

31
00:02:36,550 --> 00:02:41,550
final position as basically being the sum of all of those different numbers, which in this case

32
00:02:41,550 --> 00:02:46,350
happens to be one, and we might label all of the different buckets with the sum that they represent,

33
00:02:46,350 --> 00:02:51,290
as we repeat this we're looking at different possible sums for those five random numbers.

34
00:02:53,050 --> 00:02:57,030
And for those of you who are inclined to complain that this is a highly unrealistic model for the

35
00:02:57,030 --> 00:03:01,670
true Galton board, let me emphasize the goal right now is not to accurately model physics,

36
00:03:01,830 --> 00:03:07,330
the goal is to give a simple example to illustrate the central limit theorem, and for that, idealized

37
00:03:07,330 --> 00:03:12,310
though this might be, it actually gives us a really good example. If we let many different balls fall,

38
00:03:12,310 --> 00:03:15,950
making yet another unrealistic assumption that they don't influence each other,

39
00:03:16,090 --> 00:03:19,830
as if they're all ghosts, then the number of balls that fall into each different bucket

40
00:03:19,830 --> 00:03:25,090
gives us some loose sense for how likely each one of those buckets is. In this example, the numbers

41
00:03:25,090 --> 00:03:28,910
are simple enough that it's not too hard to explicitly calculate what the probability is

42
00:03:28,910 --> 00:03:32,730
for falling into each bucket. If you do want to think that through, you'll find it very reminiscent

43
00:03:32,730 --> 00:03:37,790
of Pascal's triangle, but the neat thing about our theorem is how far it goes beyond the simple

44
00:03:37,790 --> 00:03:42,650
examples. So to start off at least, rather than making explicit calculations, let's just simulate

45
00:03:42,650 --> 00:03:46,670
things by running a large number of samples and letting the total number of results in each

46
00:03:46,670 --> 00:03:51,370
different outcome give us some sense for what that distribution looks like. As I said, the one on

47
00:03:51,370 --> 00:03:57,290
screen has five rows, so each sum that we're considering includes only five numbers. The basic

48
00:03:57,290 --> 00:04:02,610
idea of the central limit theorem is that if you increase the size of that sum, for example here

49
00:04:02,610 --> 00:04:08,310
would mean increasing the number of rows of pegs for each ball to bounce off, then the distribution

50
00:04:08,310 --> 00:04:13,330
that describes where that sum is going to fall looks more and more like a bell curve.

51
00:04:15,470 --> 00:04:18,350
Here, it's actually worth taking a moment to write down that general idea.

52
00:04:19,270 --> 00:04:24,730
The setup is that we have a random variable, and that's basically shorthand for a random process

53
00:04:24,730 --> 00:04:29,970
where each outcome of that process is associated with some number. We'll call that random number x.

54
00:04:29,970 --> 00:04:34,390
For example, each bounce off the peg is a random process modeled with two outcomes.

55
00:04:34,850 --> 00:04:37,890
Those outcomes are associated with the numbers negative one and positive one.

56
00:04:38,530 --> 00:04:42,570
Another example of a random variable would be rolling a die, where you have six different

57
00:04:42,570 --> 00:04:47,810
outcomes, each one associated with a number. What we're doing is taking multiple different samples

58
00:04:47,810 --> 00:04:52,650
of that variable and adding them all together. On our Galton board, that looks like letting the

59
00:04:52,650 --> 00:04:57,570
ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die,

60
00:04:57,570 --> 00:05:00,970
you might imagine rolling many different dice and adding up the results.

61
00:05:01,430 --> 00:05:06,450
The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger,

62
00:05:06,650 --> 00:05:11,570
then the distribution of that sum, how likely it is to fall into different possible values,

63
00:05:11,990 --> 00:05:17,890
will look more and more like a bell curve. That's it, that is the general idea. Over the

64
00:05:17,890 --> 00:05:22,490
course of this lesson, our job is to make that statement more quantitative. We're going to put

65
00:05:22,490 --> 00:05:26,350
some numbers to it, put some formulas to it, show how you can use it to make predictions.

66
00:05:27,210 --> 00:05:31,570
For example, here's the kind of question I want you to be able to answer by the end of this video.

67
00:05:32,190 --> 00:05:37,990
Suppose you rolled a die 100 times and you added together the results. Could you find a range of

68
00:05:37,990 --> 00:05:43,910
values such that you're 95% sure that the sum will fall within that range? Or maybe I should say find

69
00:05:43,910 --> 00:05:48,570
the smallest possible range of values such that this is true. The neat thing is you'll be able

70
00:05:48,570 --> 00:05:54,310
to answer this question whether it's a fair die or if it's a weighted die. Now let me say at the

71
00:05:54,310 --> 00:05:58,350
top that this theorem has three different assumptions that go into it, three things that

72
00:05:58,350 --> 00:06:02,450
have to be true before the theorem follows. And I'm actually not going to tell you what they are

73
00:06:02,450 --> 00:06:06,790
until the very end of the video. Instead I want you to keep your eye out and see if you can notice

74
00:06:06,790 --> 00:06:12,070
and maybe predict what those three assumptions are going to be. As a next step, to better illustrate

75
00:06:12,070 --> 00:06:16,850
just how general this theorem is, I want to run a couple more simulations for you focused on the dice

76
00:06:16,850 --> 00:06:24,870
example. Usually if you think of rolling a die you think of the six outcomes as being equally

77
00:06:24,870 --> 00:06:29,550
probable, but the theorem actually doesn't care about that. We could start with a weighted die,

78
00:06:29,810 --> 00:06:35,130
something with a non-trivial distribution across the outcomes, and the core idea still holds. For

79
00:06:35,130 --> 00:06:39,190
the simulation what I'll do is take some distribution like this one that is skewed towards

80
00:06:39,190 --> 00:06:45,130
lower values. I'm going to take 10 distinct samples from that distribution and then I'll record the

81
00:06:45,130 --> 00:06:50,350
sum of that sample on the plot on the bottom. Then I'm going to do this many many different

82
00:06:50,350 --> 00:06:55,490
times, always with a sum of size 10, but keep track of where those sums ended up to give us

83
00:06:55,490 --> 00:07:03,250
a sense of the distribution. And in fact let me rescale the y direction to give us room to run

84
00:07:03,250 --> 00:07:08,070
an even larger number of samples. And I'll let it go all the way up to a couple thousand, and as it

85
00:07:08,070 --> 00:07:13,770
does you'll notice that the shape that starts to emerge looks like a bell curve. Maybe if you squint

86
00:07:13,770 --> 00:07:18,530
your eyes you can see it skews a tiny bit to the left, but it's neat that something so symmetric

87
00:07:18,530 --> 00:07:22,830
emerged from a starting point that was so asymmetric. To better illustrate what the central

88
00:07:22,830 --> 00:07:27,910
limit theorem is all about, let me run four of these simulations in parallel, where on the upper

89
00:07:27,910 --> 00:07:32,610
left I'm doing it where we're only adding two dice at a time, on the upper right we're doing it

90
00:07:32,610 --> 00:07:37,290
where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at

91
00:07:37,290 --> 00:07:43,530
a time, and then we'll do another one with a bigger sum, 15 at a time. Notice how on the upper left

92
00:07:43,530 --> 00:07:48,090
when we're just adding two dice, the resulting distribution doesn't really look like a bell curve,

93
00:07:48,410 --> 00:07:52,030
it looks a lot more reminiscent of the one we started with, skewed towards the left.

94
00:07:52,810 --> 00:07:57,750
But as we allow for more and more dice in each sum, the resulting shape that comes up in these

95
00:07:57,750 --> 00:08:02,290
distributions looks more and more symmetric. It has the lump in the middle and fade towards the

96
00:08:02,290 --> 00:08:10,490
tail's shape of a bell curve. And let me emphasize again, you can start with any different distribution.

97
00:08:10,490 --> 00:08:15,070
Here I'll run it again, but where most of the probability is tied up in the numbers 1 and 6,

98
00:08:15,110 --> 00:08:20,090
with very low probability for the mid values. Despite completely changing the distribution

99
00:08:20,090 --> 00:08:25,310
for an individual roll of the die, it's still the case that a bell curve shape will emerge as we

100
00:08:25,310 --> 00:08:30,510
consider the different sums. Illustrating things with a simulation like this is very fun, and it's

101
00:08:30,510 --> 00:08:36,010
kind of neat to see order emerge from chaos, but it also feels a little imprecise. Like in this

102
00:08:36,010 --> 00:08:40,510
case, when I cut off the simulation at 3000 samples, even though it kind of looks like a

103
00:08:40,510 --> 00:08:45,290
bell curve, the different buckets seem pretty spiky, and you might wonder, is it supposed to

104
00:08:45,290 --> 00:08:50,210
look that way, or is that just an artifact of the randomness in the simulation? And if it is, how

105
00:08:50,210 --> 00:08:54,450
many samples do we need before we can be sure that what we're looking at is representative of the true

106
00:08:54,450 --> 00:09:03,130
distribution? Instead moving forward, let's get a little more theoretical and show the precise shape

107
00:09:03,130 --> 00:09:08,590
these distributions will take on in the long run. The easiest case to make this calculation is if we

108
00:09:08,590 --> 00:09:13,970
have a uniform distribution, where each possible face of the die has an equal probability, 1 6th.

109
00:09:13,990 --> 00:09:17,510
For example, if you then want to know how likely different sums are for a pair of dice,

110
00:09:17,950 --> 00:09:23,330
it's essentially a counting game, where you count up how many distinct pairs take on the same sum,

111
00:09:23,690 --> 00:09:27,370
which in the diagram I've drawn, you can conveniently think about by going through all

112
00:09:27,370 --> 00:09:34,950
the different diagonals. Since each such pair has an equal chance of showing up, 1 in 36,

113
00:09:35,390 --> 00:09:40,150
all you have to do is count the sizes of these buckets. That gives us a definitive shape for the

114
00:09:40,150 --> 00:09:45,110
distribution describing a sum of two dice, and if we were to play the same game with all possible

115
00:09:45,110 --> 00:09:50,250
triplets, the resulting distribution would look like this. Now what's more challenging, but a lot

116
00:09:50,250 --> 00:09:54,990
more interesting, is to ask what happens if we have a non-uniform distribution for that single die.

117
00:09:55,550 --> 00:10:00,030
We actually talked all about this in the last video. You do essentially the same thing,

118
00:10:00,090 --> 00:10:03,670
you go through all the distinct pairs of dice which add up to the same value.

119
00:10:03,970 --> 00:10:09,150
It's just that instead of counting those pairs, for each pair you multiply the two probabilities

120
00:10:09,150 --> 00:10:14,470
of each particular face coming up, and then you add all those together. The computation that does

121
00:10:14,470 --> 00:10:19,290
this for all possible sums has a fancy name, it's called a convolution, but it's essentially just

122
00:10:19,290 --> 00:10:23,710
the weighted version of the counting game that anyone who's played with a pair of dice already

123
00:10:23,710 --> 00:10:28,650
finds familiar. For our purposes in this lesson, I'll have the computer calculate all that,

124
00:10:28,950 --> 00:10:33,750
simply display the results for you, and invite you to observe certain patterns, but under the hood,

125
00:10:34,010 --> 00:10:39,710
this is what's going on. So just to be crystal clear on what's being represented here,

126
00:10:39,890 --> 00:10:44,370
if you imagine sampling two different values from that top distribution, the one describing

127
00:10:44,370 --> 00:10:49,950
a single die, and adding them together, then the second distribution I'm drawing represents how

128
00:10:49,950 --> 00:10:55,130
likely you are to see various different sums. Likewise, if you imagine sampling three distinct

129
00:10:55,130 --> 00:10:59,610
values from that top distribution, and adding them together, the next plot represents the

130
00:10:59,610 --> 00:11:06,210
probabilities for various different sums in that case. So if I compute what the distributions for

131
00:11:06,210 --> 00:11:11,490
these sums look like for larger and larger sums, well you know what I'm going to say, it looks more

132
00:11:11,490 --> 00:11:15,870
and more like a bell curve. But before we get to that, I want you to make a couple more simple

133
00:11:15,870 --> 00:11:21,850
observations. For example, these distributions seem to be wandering to the right, and also they

134
00:11:21,850 --> 00:11:26,650
seem to be getting more spread out, and a little bit more flat. You cannot describe the central

135
00:11:26,650 --> 00:11:30,670
limit theorem quantitatively without taking into account both of those effects, which in turn

136
00:11:30,670 --> 00:11:35,170
requires describing the mean and the standard deviation. Maybe you're already familiar with

137
00:11:35,170 --> 00:11:39,270
those, but I want to make minimal assumptions here, and it never hurts to review, so let's

138
00:11:39,270 --> 00:11:46,430
quickly go over both of those. The mean of a distribution, often denoted with the Greek

139
00:11:46,430 --> 00:11:51,970
letter mu, is a way of capturing the center of mass for that distribution. It's calculated as

140
00:11:51,970 --> 00:11:57,010
the expected value of our random variable, which is a way of saying you go through all of the

141
00:11:57,010 --> 00:12:02,430
different possible outcomes, and you multiply the probability of that outcome times the value of the

142
00:12:02,430 --> 00:12:07,230
variable. If higher values are more probable, that weighted sum is going to be bigger. If lower

143
00:12:07,230 --> 00:12:11,630
values are more probable, that weighted sum is going to be smaller. A little more interesting

144
00:12:11,630 --> 00:12:15,550
is if you want to measure how spread out this distribution is, because there's multiple

145
00:12:15,550 --> 00:12:22,010
different ways you might do it. One of them is called the variance. The idea there is to look

146
00:12:22,010 --> 00:12:27,450
at the difference between each possible value and the mean, square that difference, and ask for its

147
00:12:27,450 --> 00:12:32,510
expected value. The idea is that whether your value is below or above the mean, when you square

148
00:12:32,510 --> 00:12:36,650
that difference, you get a positive number, and the larger the difference, the bigger that number.

149
00:12:37,370 --> 00:12:41,270
Squaring it like this turns out to make the math much much nicer than if we did something like an

150
00:12:41,270 --> 00:12:46,170
absolute value, but the downside is that it's hard to think about this as a distance in our diagram

151
00:12:46,170 --> 00:12:51,430
because the units are off, kind of like the units here are square units, whereas a distance in our

152
00:12:51,430 --> 00:12:56,590
diagram would be a kind of linear unit. So another way to measure spread is what's called the standard

153
00:12:56,590 --> 00:13:01,990
deviation, which is the square root of this value. That can be interpreted much more reasonably as a

154
00:13:01,990 --> 00:13:07,170
distance on our diagram, and it's commonly denoted with the Greek letter sigma, so you know m for

155
00:13:07,730 --> 00:13:14,010
standard deviation, but both in Greek. Looking back at our sequence of distributions,

156
00:13:14,350 --> 00:13:18,930
let's talk about the mean and standard deviation. If we call the mean of the initial distribution mu,

157
00:13:19,470 --> 00:13:24,350
which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if I tell you

158
00:13:24,350 --> 00:13:28,990
that the mean of the next one is 2 times mu. That is, you roll a pair of dice, you want to know the

159
00:13:28,990 --> 00:13:34,170
expected value of the sum, it's two times the expected value for a single die. Similarly,

160
00:13:34,170 --> 00:13:39,950
the expected value for our sum of size 3 is 3 times mu, and so on and so forth. The mean

161
00:13:39,950 --> 00:13:44,270
just marches steadily on to the right, which is why our distributions seem to be drifting off in

162
00:13:44,270 --> 00:13:48,770
that direction. A little more challenging, but very important, is to describe how the standard

163
00:13:48,770 --> 00:13:54,050
deviation changes. The key fact here is that if you have two different random variables, then the

164
00:13:54,050 --> 00:13:58,850
variance for the sum of those variables is the same as just adding together the original two

165
00:13:58,850 --> 00:14:03,630
variances. This is one of those facts that you can just compute when you unpack all the definitions.

166
00:14:03,630 --> 00:14:08,490
There are a couple nice intuitions for why it's true. My tentative plan is to just actually make

167
00:14:08,490 --> 00:14:12,810
a series about probability and talk about things like intuitions underlying variance and its

168
00:14:12,810 --> 00:14:17,270
cousins there. But right now, the main thing I want you to highlight is how it's the variance

169
00:14:17,270 --> 00:14:22,130
that adds, it's not the standard deviation that adds. So, critically, if you were to take n

170
00:14:22,130 --> 00:14:27,990
different realizations of the same random variable and ask what the sum looks like, the variance of

171
00:14:27,990 --> 00:14:33,670
sum is n times the variance of your original variable, meaning the standard deviation,

172
00:14:34,050 --> 00:14:38,250
the square root of all this, is the square root of n times the original standard deviation.

173
00:14:39,290 --> 00:14:42,550
For example, back in our sequence of distributions, if we label the standard

174
00:14:42,550 --> 00:14:47,490
deviation of our initial one with sigma, then the next standard deviation is going to be the square

175
00:14:47,490 --> 00:14:53,090
root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on

176
00:14:53,750 --> 00:14:58,230
This, like I said, is very important. It means that even though our distributions are getting

177
00:14:58,230 --> 00:15:02,570
spread out, they're not spreading out all that quickly, they only do so in proportion to the

178
00:15:02,570 --> 00:15:07,510
square root of the size of the sum. As we prepare to make a more quantitative description of the

179
00:15:07,510 --> 00:15:11,970
central limit theorem, the core intuition I want you to keep in your head is that we'll basically

180
00:15:12,390 --> 00:15:17,730
realign all of these distributions so that their means line up together, and then rescale them so

181
00:15:17,730 --> 00:15:22,210
that all of the standard deviations are just going to be equal to one. And when we do that,

182
00:15:22,210 --> 00:15:26,330
the shape that results gets closer and closer to a certain universal shape,

183
00:15:26,550 --> 00:15:29,870
described with an elegant little function that we'll unpack in just a moment.

184
00:15:30,470 --> 00:15:34,830
And let me say one more time, the real magic here is how we could have started with any

185
00:15:34,830 --> 00:15:39,010
distribution, describing a single roll of the die, and if we play the same game,

186
00:15:39,370 --> 00:15:42,430
considering what the distributions for the many different sums look like,

187
00:15:42,710 --> 00:15:46,690
and we realign them so that the means line up, and we rescale them so that the standard

188
00:15:46,690 --> 00:15:52,950
deviations are all one, we still approach that same universal shape, which is kind of mind-boggling.

189
00:15:54,810 --> 00:16:00,250
And now, my friends, is probably as good a time as any to finally get into the formula for a normal

190
00:16:00,250 --> 00:16:04,850
distribution. And the way I'd like to do this is to basically peel back all the layers and build it

191
00:16:04,850 --> 00:16:10,950
up one piece at a time. The function e to the x, or anything to the x, describes exponential growth,

192
00:16:11,350 --> 00:16:15,190
and if you make that exponent negative, which flips around the graph horizontally,

193
00:16:15,190 --> 00:16:20,030
you might think of it as describing exponential decay. To make this decay in both directions,

194
00:16:20,030 --> 00:16:23,290
you could do something to make sure the exponent is always negative and growing,

195
00:16:23,530 --> 00:16:28,170
like taking the negative absolute value. That would give us this kind of awkward sharp point

196
00:16:28,170 --> 00:16:31,970
in the middle, but if instead you make that exponent the negative square of x,

197
00:16:32,350 --> 00:16:35,810
you get a smoother version of the same thing, which decays in both directions.

198
00:16:36,330 --> 00:16:40,630
This gives us the basic bell curve shape. Now if you throw a constant in front of that x,

199
00:16:40,630 --> 00:16:45,130
and you scale that constant up and down, it lets you stretch and squish the graph horizontally,

200
00:16:45,510 --> 00:16:50,270
allowing you to describe narrow and wider bell curves. And a quick thing I'd like to point out

201
00:16:50,270 --> 00:16:56,090
here is that based on the rules of exponentiation, as we tweak around that constant c, you could also

202
00:16:56,090 --> 00:17:01,810
think about it as simply changing the base of the exponentiation. And in that sense, the number e is

203
00:17:01,810 --> 00:17:06,730
not really all that special for our formula. We could replace it with any other positive constant,

204
00:17:06,730 --> 00:17:10,490
and you'll get the same family of curves as we tweak that constant.

205
00:17:11,510 --> 00:17:15,070
Make it a 2, same family of curves. Make it a 3, same family of curves.

206
00:17:15,750 --> 00:17:19,490
The reason we use e is that it gives that constant a very readable meaning.

207
00:17:20,110 --> 00:17:25,470
Or rather, if we reconfigure things a little bit so that the exponent looks like negative 1 half

208
00:17:25,470 --> 00:17:31,610
times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn

209
00:17:31,610 --> 00:17:36,030
this into a probability distribution, that constant sigma will be the standard deviation

210
00:17:36,030 --> 00:17:41,050
of that distribution. And that's very nice. But before we can interpret this as a probability

211
00:17:41,050 --> 00:17:46,410
distribution, we need the area under the curve to be 1. And the reason for that is how the curve is

212
00:17:46,410 --> 00:17:51,410
interpreted. Unlike discrete distributions, when it comes to something continuous, you don't ask

213
00:17:51,410 --> 00:17:55,890
about the probability of a particular point. Instead, you ask for the probability that a

214
00:17:55,890 --> 00:18:01,570
value falls between two different values. And what the curve is telling you is that that probability

215
00:18:01,570 --> 00:18:07,490
equals the area under the curve between those two values. There's a whole other video about this,

216
00:18:07,610 --> 00:18:12,050
they're called probability density functions. The main point right now is that the area under

217
00:18:12,050 --> 00:18:17,150
the entire curve represents the probability that something happens, that some number comes up.

218
00:18:17,410 --> 00:18:22,350
That should be 1, which is why we want the area under this to be 1. As it stands with the basic

219
00:18:22,350 --> 00:18:27,330
bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root

220
00:18:27,330 --> 00:18:32,590
of pi. I know, right? What is pi doing here? What does this have to do with circles? Like I said at

221
00:18:32,590 --> 00:18:36,550
the start, I'd love to talk all about that in the next video. But if you can spare your excitement,

222
00:18:36,790 --> 00:18:40,910
for our purposes right now, all it means is that we should divide this function by the square root

223
00:18:40,910 --> 00:18:46,090
of pi, and it gives us the area we want. Throwing back in the constants we had earlier, the one half

224
00:18:46,090 --> 00:18:51,270
and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square

225
00:18:51,270 --> 00:18:56,470
root of 2. So we also need to divide out by that in order to make sure it has an area of 1,

226
00:18:56,470 --> 00:19:01,190
and combining those fractions, the factor out front looks like 1 divided by sigma times the

227
00:19:01,190 --> 00:19:07,550
square root of 2 pi. This, finally, is a valid probability distribution. As we tweak that value

228
00:19:07,550 --> 00:19:12,810
sigma, resulting in narrower and wider curves, that constant in the front always guarantees

229
00:19:12,810 --> 00:19:19,310
that the area equals 1. The special case where sigma equals 1 has a specific name, we call it

230
00:19:19,310 --> 00:19:24,090
the standard normal distribution, which plays an especially important role for you and me in this

231
00:19:24,090 --> 00:19:29,750
lesson. And all possible normal distributions are not only parameterized with this value sigma,

232
00:19:30,190 --> 00:19:35,830
but we also subtract off another constant mu from the variable x, and this essentially just lets you

233
00:19:35,830 --> 00:19:40,210
slide the graph left and right so that you can prescribe the mean of this distribution.

234
00:19:40,990 --> 00:19:44,770
So in short, we have two parameters, one describing the mean, one describing the standard

235
00:19:44,770 --> 00:19:49,190
deviation, and they're all tied together in this big formula involving an e and a pi.

236
00:19:49,190 --> 00:19:54,950
Now that all of that is on the table, let's look back again at the idea of starting with some

237
00:19:54,950 --> 00:20:00,390
random variable and asking what the distributions for sums of that variable look like. As we've

238
00:20:00,390 --> 00:20:04,870
already gone over, when you increase the size of that sum, the resulting distribution will shift

239
00:20:04,870 --> 00:20:09,810
according to a growing mean, and it slowly spreads out according to a growing standard deviation.

240
00:20:10,330 --> 00:20:14,670
And putting some actual formulas to it, if we know the mean of our underlying random variable,

241
00:20:14,670 --> 00:20:19,510
we call it mu, and we also know its standard deviation, and we call it sigma, then the mean

242
00:20:19,510 --> 00:20:25,170
for the sum on the bottom will be mu times the size of the sum, and the standard deviation will

243
00:20:25,170 --> 00:20:30,390
be sigma times the square root of that size. So now, if we want to claim that this looks more

244
00:20:30,390 --> 00:20:34,730
and more like a bell curve, and a bell curve is only described by two different parameters,

245
00:20:35,110 --> 00:20:39,470
the mean and the standard deviation, you know what to do. You could plug those two values into

246
00:20:39,470 --> 00:20:45,030
the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve

247
00:20:45,030 --> 00:20:50,550
that should closely fit our distribution. But there's another way we can describe it that's

248
00:20:50,550 --> 00:20:54,810
a little more elegant and lends itself to a very fun visual that we can build up to.

249
00:20:55,270 --> 00:21:00,090
Instead of focusing on the sum of all of these random variables, let's modify this expression

250
00:21:00,090 --> 00:21:04,090
a little bit, where what we'll do is we'll look at the mean that we expect that sum to take,

251
00:21:04,290 --> 00:21:09,090
and we subtract it off so that our new expression has a mean of zero, and then we're going to look

252
00:21:09,090 --> 00:21:14,630
at the standard deviation we expect of our sum, and divide out by that, which basically just

253
00:21:14,630 --> 00:21:20,150
rescales the units so that the standard deviation of our expression will equal one. This might seem

254
00:21:20,150 --> 00:21:24,710
like a more complicated expression, but it actually has a highly readable meaning. It's

255
00:21:24,710 --> 00:21:31,170
essentially saying how many standard deviations away from the mean is this sum? For example,

256
00:21:31,330 --> 00:21:36,090
this bar here corresponds to a certain value that you might find when you roll 10 dice and you add

257
00:21:36,090 --> 00:21:40,450
them all up, and its position a little above negative one is telling you that that value

258
00:21:40,450 --> 00:21:46,090
is a little bit less than one standard deviation lower than the mean. Also, by the way, in

259
00:21:46,090 --> 00:21:50,290
anticipation for the animation I'm trying to build to here, the way I'm representing things on that

260
00:21:50,290 --> 00:21:55,170
lower plot is that the area of each one of these bars is telling us the probability of the

261
00:21:55,170 --> 00:21:59,510
corresponding value rather than the height. You might think of the y-axis as representing not

262
00:21:59,510 --> 00:22:04,330
probability but a kind of probability density. The reason for this is to set the stage so that

263
00:22:04,330 --> 00:22:08,830
it aligns with the way we interpret continuous distributions, where the probability of falling

264
00:22:08,830 --> 00:22:14,430
between a range of values is equal to an area under a curve between those values. In particular,

265
00:22:14,610 --> 00:22:20,510
the area of all the bars together is going to be one. Now, with all of that in place, let's have a

266
00:22:20,510 --> 00:22:24,990
little fun. Let me start by rolling things back so that the distribution on the bottom represents

267
00:22:24,990 --> 00:22:30,190
a relatively small sum, like adding together only three such random variables. Notice what happens

268
00:22:30,190 --> 00:22:35,470
as I change the distribution we start with. As it changes, the distribution on the bottom completely

269
00:22:35,470 --> 00:22:42,390
changes its shape. It's very dependent on what we started with. If we let the size of our sum get a

270
00:22:42,390 --> 00:22:47,850
little bit bigger, say going up to 10, and as I change the distribution for x, it largely stays

271
00:22:47,850 --> 00:22:52,410
looking like a bell curve, but I can find some distributions that get it to change shape. For

272
00:22:52,410 --> 00:22:57,590
example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results

273
00:22:57,590 --> 00:23:02,550
in this kind of spiky bell curve. And if you'll recall, earlier on I actually showed this in the

274
00:23:02,550 --> 00:23:07,270
form of a simulation. Though if you were wondering whether that spikiness was an artifact of the

275
00:23:07,270 --> 00:23:11,850
randomness or reflected the true distribution, turns out it reflects the true distribution.

276
00:23:12,290 --> 00:23:17,090
In this case, 10 is not a large enough sum for the central limit theorem to kick in. But if instead

277
00:23:17,090 --> 00:23:22,570
I let that sum grow and I consider adding 50 different values, which is actually not that big,

278
00:23:22,570 --> 00:23:27,130
then no matter how I change the distribution for our underlying random variable,

279
00:23:27,630 --> 00:23:32,190
it has essentially no effect on the shape of the plot on the bottom. No matter where we start,

280
00:23:32,430 --> 00:23:36,950
all of the information and nuance for the distribution of x gets washed away, and we tend

281
00:23:36,950 --> 00:23:42,190
towards this single universal shape described by a very elegant function for the standard normal

282
00:23:42,190 --> 00:23:48,850
distribution, 1 over square root of 2 pi times e to the negative x squared over 2. This, this right

283
00:23:48,850 --> 00:23:52,870
here is what the central limit theorem is all about. Almost nothing you can do to this initial

284
00:23:52,870 --> 00:24:01,610
distribution changes the shape we tend towards. Now, the more theoretically minded among you

285
00:24:01,610 --> 00:24:06,390
might still be wondering what is the actual theorem, like what's the mathematical statement

286
00:24:06,390 --> 00:24:10,430
that could be proved or disproved that we're claiming here. If you want a nice formal statement,

287
00:24:10,730 --> 00:24:15,910
here's how it might go. Consider this value where we're summing up n different instantiations of our

288
00:24:15,910 --> 00:24:19,890
variable, but tweaked and tuned so that its mean and standard deviation are 1,

289
00:24:20,230 --> 00:24:25,350
again meaning you can read it as asking how many standard deviations away from the mean is the sum.

290
00:24:25,770 --> 00:24:29,530
Then the actual rigorous no-jokes-this-time statement of the central limit theorem

291
00:24:29,530 --> 00:24:34,910
is that if you consider the probability that this value falls between two given real numbers,

292
00:24:35,170 --> 00:24:40,630
a and b, and you consider the limit of that probability as the size of your sum goes to

293
00:24:40,630 --> 00:24:45,610
infinity, then that limit is equal to a certain integral, which basically describes

294
00:24:45,610 --> 00:24:49,650
the area under a standard normal distribution between those two values.

295
00:24:51,250 --> 00:24:55,010
Again, there are three underlying assumptions that I have yet to tell you,

296
00:24:55,150 --> 00:25:00,030
but other than those, in all of its gory detail, this right here is the central limit theorem.

297
00:25:04,550 --> 00:25:08,270
All of that is a bit theoretical, so it might be helpful to bring things back down to earth

298
00:25:08,270 --> 00:25:11,770
and turn back to the concrete example that I mentioned at the start,

299
00:25:12,110 --> 00:25:16,450
where you imagine rolling a die 100 times, and let's assume it's a fair die for this example,

300
00:25:16,710 --> 00:25:22,270
and you add together the results. The challenge for you is to find a range of values such that

301
00:25:22,270 --> 00:25:28,710
you're 95% sure that the sum will fall within this range. For questions like this, there's a handy

302
00:25:28,710 --> 00:25:38,250
rule of thumb about normal distributions, which is that about 68% of your values are going to

303
00:25:38,250 --> 00:25:44,950
fall within two standard deviations of the mean, and a whopping 99.7% of your values will fall

304
00:25:44,950 --> 00:25:49,250
within three standard deviations of the mean. It's a rule of thumb that's commonly memorized

305
00:25:49,250 --> 00:25:54,250
by people who do a lot of probability and stats. Naturally, this gives us what we need for our

306
00:25:54,250 --> 00:25:58,570
example, and let me go ahead and draw out what this would look like, where I'll show the

307
00:25:58,570 --> 00:26:03,910
distribution for a fair die up at the top, and the distribution for a sum of 100 such dice on

308
00:26:03,910 --> 00:26:09,630
bottom, which by now looks like a normal distribution. Step 1 with a problem like this

309
00:26:09,630 --> 00:26:14,870
is to find the mean of your initial distribution, which in this case will look like 1 6th times 1

310
00:26:14,870 --> 00:26:20,870
plus 1 6th times 2 on and on and on, and works out to be 3.5. We also need the standard deviation,

311
00:26:21,130 --> 00:26:25,610
which requires calculating the variance, which as you know involves adding all the squares of

312
00:26:25,610 --> 00:26:30,830
the differences between the values and the means, and it works out to be 2.92, the square root of

313
00:26:30,830 --> 00:26:37,210
1.71. Those are the only two numbers we need, and I will invite you again to reflect on how magical

314
00:26:37,210 --> 00:26:41,690
it is that those are the only two numbers you need to completely understand the bottom distribution.

315
00:26:42,430 --> 00:26:48,690
Its mean will be 100 times mu, which is 350, and its standard deviation will be the square root of

316
00:26:48,690 --> 00:26:54,910
100 times sigma, so 10 times sigma, 17.1. Remembering our handy rule of thumb, we're

317
00:26:54,910 --> 00:27:00,290
looking for values two standard deviations away from the mean, and when you subtract 2 sigma from

318
00:27:00,290 --> 00:27:07,850
mean, you end up with about 316, and when you add 2 sigma you end up with 384. There you go,

319
00:27:08,030 --> 00:27:13,450
that gives us the answer. Okay, I promised to wrap things up shortly,

320
00:27:13,690 --> 00:27:17,450
but while we're on this example, there's one more question that's worth your time to ponder.

321
00:27:18,250 --> 00:27:23,190
Instead of just asking about the sum of 100 die rolls, let's say I had you divide that number by

322
00:27:23,190 --> 00:27:28,090
100, which basically means all the numbers in our diagram in the bottom get divided by 100.

323
00:27:28,570 --> 00:27:33,430
Take a moment to interpret what this all would be saying then. The expression essentially tells you

324
00:27:33,430 --> 00:27:40,110
the empirical average for 100 different die rolls, and that interval we found is now telling you what

325
00:27:40,110 --> 00:27:46,150
range you are expecting to see for that empirical average. In other words, you might expect it to be

326
00:27:46,150 --> 00:27:51,110
around 3.5, that's the expected value for a die roll, but what's much less obvious and what the

327
00:27:51,110 --> 00:27:55,770
central limit theorem lets you compute is how close to that expected value you'll reasonably

328
00:27:55,770 --> 00:28:01,010
find yourself. In particular, it's worth your time to take a moment mulling over what the standard

329
00:28:01,010 --> 00:28:06,070
deviation for this empirical average is, and what happens to it as you look at a bigger and bigger

330
00:28:06,070 --> 00:28:16,390
sample of die rolls. Lastly, but probably most importantly, let's talk about the assumptions

331
00:28:16,390 --> 00:28:20,830
that go into this theorem. The first one is that all of these variables that we're adding up

332
00:28:20,830 --> 00:28:25,350
are independent from each other. The outcome of one process doesn't influence the outcome

333
00:28:25,350 --> 00:28:30,950
of any other process. The second is that all of these variables are drawn from the same distribution.

334
00:28:31,310 --> 00:28:35,930
Both of these have been implicitly assumed with our dice example. We've been treating the outcome

335
00:28:35,930 --> 00:28:40,470
of each die roll as independent from the outcome of all the others, and we're assuming that each

336
00:28:40,470 --> 00:28:44,990
die follows the same distribution. Sometimes in the literature you'll see these two assumptions

337
00:28:44,990 --> 00:28:50,690
lumped together under the initials IID for independent and identically distributed. One

338
00:28:50,690 --> 00:28:56,210
situation where these assumptions are decidedly not true would be the Galton board. I mean, think

339
00:28:56,210 --> 00:29:01,270
about it. Is it the case that the way a ball bounces off of one of the pegs is independent

340
00:29:01,270 --> 00:29:05,990
from how it's going to bounce off the next peg? Absolutely not. Depending on the last bounce, it's

341
00:29:05,990 --> 00:29:10,490
coming in with a completely different trajectory. And is it the case that the distribution of

342
00:29:10,490 --> 00:29:16,710
possible outcomes off of each peg are the same for each peg that it hits? Again, almost certainly not.

343
00:29:16,710 --> 00:29:20,010
Maybe it hits one peg glancing to the left, meaning the outcomes are

344
00:29:20,010 --> 00:29:23,710
hugely skewed in that direction, and then hits the next one glancing to the right.

345
00:29:25,730 --> 00:29:30,590
When I made all those simplifying assumptions in the opening example, it wasn't just to make this

346
00:29:30,590 --> 00:29:35,230
easier to think about. It's also that those assumptions were necessary for this to actually

347
00:29:35,230 --> 00:29:40,150
be an example of the central limit theorem. Nevertheless, it seems to be true that for the

348
00:29:40,150 --> 00:29:45,470
real Galton board, despite violating both of these, a normal distribution does kind of come about?

349
00:29:46,050 --> 00:29:49,910
Part of the reason might be that there are generalizations of the theorem beyond the scope

350
00:29:49,910 --> 00:29:55,430
of this video that relax these assumptions, especially the second one. But I do want to caution

351
00:29:55,430 --> 00:30:00,130
you against the fact that many times people seem to assume that a variable is normally distributed,

352
00:30:00,330 --> 00:30:06,210
even when there's no actual justification to do so. The third assumption is actually fairly subtle.

353
00:30:06,210 --> 00:30:11,950
It's that the variance we've been computing for these variables is finite. This was never an issue

354
00:30:11,950 --> 00:30:16,610
for the dice example because there were only six possible outcomes. But in certain situations where

355
00:30:16,610 --> 00:30:21,710
you have an infinite set of outcomes, when you go to compute the variance, the sum ends up diverging

356
00:30:21,710 --> 00:30:26,830
off to infinity. These can be perfectly valid probability distributions, and they do come up in

357
00:30:26,830 --> 00:30:31,810
practice. But in those situations, as you consider adding many different instantiations of that

358
00:30:31,810 --> 00:30:37,010
variable and letting that sum approach infinity, even if the first two assumptions hold, it is very

359
00:30:37,010 --> 00:30:41,190
much a possibility that the thing you tend towards is not actually a normal distribution.

360
00:30:42,150 --> 00:30:46,350
If you've understood everything up to this point, you now have a very strong foundation in what the

361
00:30:46,350 --> 00:30:51,150
central limit theorem is all about. And next up, I'd like to explain why it is that this particular

362
00:30:51,150 --> 00:30:55,990
function is the thing that we tend towards, and why it has a pi in it, what it has to do with circles.

363
00:31:11,950 --> 00:31:14,170
Thank you.

