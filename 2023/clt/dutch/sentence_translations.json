[
 {
  "input": "This is a Galton board.",
  "translatedText": "Dit is een Galton bord.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 1.26
 },
 {
  "input": "Maybe you've seen one before, it's a popular demonstration of how, even when a single event is chaotic and random, with an effectively unknowable outcome, it's still possible to make precise statements about a large number of events, namely how the relative proportions for many different outcomes are distributed.",
  "translatedText": "Misschien heb je er al een gezien, het is een populaire demonstratie van hoe het, zelfs als een enkele gebeurtenis chaotisch en willekeurig is, met een feitelijk onkenbare uitkomst, nog steeds mogelijk is om precieze uitspraken te doen over een groot aantal gebeurtenissen, namelijk hoe de relatieve verhoudingen voor veel verschillende uitkomsten verdeeld zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 2.52,
  "end": 18.3
 },
 {
  "input": "More specifically, the Galton board illustrates one of the most prominent distributions in all probability, known as the normal distribution, more colloquially known as a bell curve, and also called a Gaussian distribution.",
  "translatedText": "Meer in het bijzonder illustreert het Galton-bord een van de meest prominente verdelingen in alle waarschijnlijkheid, bekend als de normale verdeling, in de volksmond beter bekend als een klokcurve, en ook wel een Gaussische verdeling genoemd.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.38,
  "end": 31.9
 },
 {
  "input": "There's a very specific function to describe this distribution, it's very pretty, we'll get into it later, but right now I just want to emphasize how the normal distribution is, as the name suggests, very common, it shows up in a lot of seemingly unrelated contexts.",
  "translatedText": "Er is een heel specifieke functie om deze verdeling te beschrijven, hij is heel mooi, daar komen we later op terug, maar nu wil ik alleen maar benadrukken hoe de normale verdeling, zoals de naam al zegt, heel gewoon is, hij komt in veel schijnbaar ongerelateerde contexten voor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 32.5,
  "end": 45.04
 },
 {
  "input": "If you were to take a large number of people who sit in a similar demographic and plot their heights, those heights tend to follow a normal distribution.",
  "translatedText": "Als je een groot aantal mensen neemt die in een vergelijkbare bevolkingsgroep zitten en hun lengte uitzet, dan volgt die lengte meestal een normale verdeling.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 46.02,
  "end": 53.0
 },
 {
  "input": "If you look at a large swath of very big natural numbers, and you ask how many distinct prime factors does each one of those numbers have, the answers will very closely track with a certain normal distribution.",
  "translatedText": "Als je kijkt naar een groot aantal zeer grote natuurlijke getallen en je vraagt hoeveel verschillende priemfactoren elk van die getallen heeft, dan zullen de antwoorden zeer nauwkeurig overeenkomen met een bepaalde normale verdeling.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 53.66,
  "end": 64.96
 },
 {
  "input": "Now our topic for today is one of the crown jewels in all of probability theory, it's one of the key facts that explains why this distribution is as common as it is, known as the central limit theorem.",
  "translatedText": "Ons onderwerp voor vandaag is een van de kroonjuwelen in de hele waarschijnlijkheidstheorie, het is een van de belangrijkste feiten die verklaart waarom deze verdeling zo gewoon is, bekend als de centrale limietstelling.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 65.58,
  "end": 76.02
 },
 {
  "input": "This lesson is meant to go back to the basics, giving you the fundamentals on what the background is.",
  "translatedText": "Deze les is bedoeld om terug te gaan naar de basis en je de basis te geven van wat de achtergrond is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.64,
  "end": 85.26
 },
 {
  "input": "We're going to go decently deep into it, but after this I'd still like to go deeper and explain why the theorem is true, why the function underlying the normal distribution has the very specific form that it does, why that formula has a pi in it, and, most fun, why those last two facts are actually more related than a lot of traditional explanations would suggest.",
  "translatedText": "We gaan er redelijk diep op in, maar hierna wil ik nog dieper gaan en uitleggen waarom de stelling waar is, waarom de functie die ten grondslag ligt aan de normale verdeling de zeer specifieke vorm heeft die hij heeft, waarom er een pi in die formule zit, en, het leukste, waarom die laatste twee feiten eigenlijk meer met elkaar te maken hebben dan veel traditionele verklaringen suggereren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.26,
  "end": 105.56
 },
 {
  "input": "That second lesson is also meant to be the follow-on to the convolutions video that I promised, so there's a lot of interrelated topics here.",
  "translatedText": "Die tweede les is ook bedoeld als vervolg op de convoluties video die ik beloofd had, dus er zijn veel onderwerpen die met elkaar te maken hebben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 106.48,
  "end": 113.37
 },
 {
  "input": "But right now, back to the fundamentals, I'd like to kick things off with an overly simplified model of the Galton board.",
  "translatedText": "Maar nu, terug naar de basis, wil ik beginnen met een overdreven vereenvoudigd model van het Galton-bord.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.57,
  "end": 119.17
 },
 {
  "input": "In this model we will assume that each ball falls directly onto a certain central peg, and that it has a 50-50 probability of bouncing to the left or to the right, and we'll think of each of those outcomes as either adding one or subtracting one from its position.",
  "translatedText": "In dit model gaan we ervan uit dat elke bal direct op een bepaalde centrale pin valt, en dat de kans dat hij naar links of naar rechts stuit 50-50 is, en we beschouwen elk van deze uitkomsten als het optellen van één of het aftrekken van één van de positie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.89,
  "end": 134.11
 },
 {
  "input": "Once one of those is chosen, we make the highly unrealistic assumption that it happens to land dead on in the middle of the peg adjacent below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the right.",
  "translatedText": "Zodra een van deze is gekozen, nemen we de zeer onrealistische aanname dat deze toevallig precies in het midden van de pin eronder terechtkomt, waar hij weer voor dezelfde 50-50 keuze komt te staan om naar links of naar rechts te stuiteren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 134.67,
  "end": 147.07
 },
 {
  "input": "For the one I'm showing on screen, there are five different rows of pegs, so our little hopping ball makes five different random choices between plus one and minus one, and we can think of its final position as basically being the sum of all of those different numbers, which in this case happens to be one, and we might label all of the different buckets with the sum that they represent, as we repeat this we're looking at different possible sums for those five random numbers.",
  "translatedText": "Voor degene die ik op het scherm laat zien, zijn er vijf verschillende rijen met pinnen, dus ons kleine huppelende balletje maakt vijf verschillende willekeurige keuzes tussen plus één en min één, en we kunnen de uiteindelijke positie zien als de som van al die verschillende getallen, die in dit geval toevallig één is, en we kunnen alle verschillende emmers labelen met de som die ze vertegenwoordigen, als we dit herhalen kijken we naar verschillende mogelijke sommen voor die vijf willekeurige getallen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 147.43,
  "end": 171.29
 },
 {
  "input": "And for those of you who are inclined to complain that this is a highly unrealistic model for the true Galton board, let me emphasize the goal right now is not to accurately model physics, the goal is to give a simple example to illustrate the central limit theorem, and for that, idealized though this might be, it actually gives us a really good example.",
  "translatedText": "En voor degenen onder jullie die geneigd zijn te klagen dat dit een zeer onrealistisch model is voor het echte Galton-bord, wil ik benadrukken dat het doel nu niet is om de natuurkunde nauwkeurig te modelleren, het doel is om een eenvoudig voorbeeld te geven om de stelling van de centrale limiet te illustreren, en daarvoor, hoe geïdealiseerd dit ook is, geeft het ons eigenlijk een heel goed voorbeeld.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.05,
  "end": 190.03
 },
 {
  "input": "If we let many different balls fall, making yet another unrealistic assumption that they don't influence each other, as if they're all ghosts, then the number of balls that fall into each different bucket gives us some loose sense for how likely each one of those buckets is.",
  "translatedText": "Als we veel verschillende ballen laten vallen, met weer een onrealistische aanname dat ze elkaar niet beïnvloeden, alsof het allemaal geesten zijn, dan geeft het aantal ballen dat in elke emmer valt ons een idee van hoe waarschijnlijk elk van die emmers is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 190.57,
  "end": 203.39
 },
 {
  "input": "In this example, the numbers are simple enough that it's not too hard to explicitly calculate what the probability is for falling into each bucket.",
  "translatedText": "In dit voorbeeld zijn de getallen eenvoudig genoeg om expliciet te berekenen wat de kans is dat je in elke emmer valt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 203.83,
  "end": 210.01
 },
 {
  "input": "If you do want to think that through, you'll find it very reminiscent of Pascal's triangle, but the neat thing about our theorem is how far it goes beyond the simple examples.",
  "translatedText": "Als je dat wilt doordenken, zul je merken dat het erg doet denken aan de driehoek van Pascal, maar het leuke van onze stelling is hoe ver deze verder gaat dan de eenvoudige voorbeelden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.27,
  "end": 218.27
 },
 {
  "input": "So to start off at least, rather than making explicit calculations, let's just simulate things by running a large number of samples and letting the total number of results in each different outcome give us some sense for what that distribution looks like.",
  "translatedText": "Dus laten we om te beginnen, in plaats van expliciete berekeningen te maken, de dingen simuleren door een groot aantal steekproeven te doen en het totale aantal resultaten in elke verschillende uitkomst ons een idee te laten geven van hoe die verdeling eruit ziet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 218.67,
  "end": 229.97
 },
 {
  "input": "As I said, the one on screen has five rows, so each sum that we're considering includes only five numbers.",
  "translatedText": "Zoals ik al zei, heeft degene op het scherm vijf rijen, dus elke som die we bekijken bevat slechts vijf getallen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 230.45,
  "end": 236.21
 },
 {
  "input": "The basic idea of the central limit theorem is that if you increase the size of that sum, for example here would mean increasing the number of rows of pegs for each ball to bounce off, then the distribution that describes where that sum is going to fall looks more and more like a bell curve.",
  "translatedText": "Het basisidee van het centrale limiettheorema is dat als je de grootte van die som vergroot, wat hier bijvoorbeeld zou betekenen dat je het aantal rijen pinnen voor elke bal om tegenaan te stuiteren vergroot, de verdeling die beschrijft waar die som gaat vallen steeds meer op een klokcurve gaat lijken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.81,
  "end": 253.33
 },
 {
  "input": "Here, it's actually worth taking a moment to write down that general idea.",
  "translatedText": "Hier is het eigenlijk de moeite waard om even de tijd te nemen om dat algemene idee op te schrijven.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.47,
  "end": 258.35
 },
 {
  "input": "The setup is that we have a random variable, and that's basically shorthand for a random process where each outcome of that process is associated with some number.",
  "translatedText": "De opzet is dat we een willekeurige variabele hebben, en dat is eigenlijk steno voor een willekeurig proces waarbij elke uitkomst van dat proces geassocieerd is met een bepaald getal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 259.27,
  "end": 268.19
 },
 {
  "input": "We'll call that random number x.",
  "translatedText": "We noemen dat willekeurig getal x.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 268.49,
  "end": 269.97
 },
 {
  "input": "For example, each bounce off the peg is a random process modeled with two outcomes.",
  "translatedText": "Bijvoorbeeld, elke stuiter van de wasknijper is een willekeurig proces gemodelleerd met twee uitkomsten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.97,
  "end": 274.39
 },
 {
  "input": "Those outcomes are associated with the numbers negative one and positive one.",
  "translatedText": "Die uitkomsten worden geassocieerd met de getallen negatief één en positief één.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 274.85,
  "end": 277.89
 },
 {
  "input": "Another example of a random variable would be rolling a die, where you have six different outcomes, each one associated with a number.",
  "translatedText": "Een ander voorbeeld van een willekeurige variabele is het gooien van een dobbelsteen, waarbij je zes verschillende uitkomsten hebt, elk geassocieerd met een getal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.53,
  "end": 284.83
 },
 {
  "input": "What we're doing is taking multiple different samples of that variable and adding them all together.",
  "translatedText": "Wat we doen is meerdere verschillende steekproeven van die variabele nemen en ze allemaal bij elkaar optellen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 285.47,
  "end": 290.41
 },
 {
  "input": "On our Galton board, that looks like letting the ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice and adding up the results.",
  "translatedText": "Op ons Galton-bord lijkt dat op het laten stuiteren van de bal tegen verschillende pinnen op weg naar de bodem, en in het geval van een dobbelsteen zou je je kunnen voorstellen dat je met veel verschillende dobbelstenen gooit en de resultaten optelt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 290.77,
  "end": 300.97
 },
 {
  "input": "The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into different possible values, will look more and more like a bell curve.",
  "translatedText": "De stelling van het centrale limiettheorema is dat als je de grootte van die som steeds groter laat worden, de verdeling van die som, hoe waarschijnlijk het is om in verschillende mogelijke waarden te vallen, steeds meer op een klokvormige curve gaat lijken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 301.43,
  "end": 314.11
 },
 {
  "input": "That's it, that is the general idea.",
  "translatedText": "Dat is het, dat is het algemene idee.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 315.43,
  "end": 317.13
 },
 {
  "input": "Over the course of this lesson, our job is to make that statement more quantitative.",
  "translatedText": "In de loop van deze les is het onze taak om die verklaring meer kwantitatief te maken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 317.55,
  "end": 321.53
 },
 {
  "input": "We're going to put some numbers to it, put some formulas to it, show how you can use it to make predictions.",
  "translatedText": "We gaan er wat cijfers en formules op loslaten en laten zien hoe je er voorspellingen mee kunt doen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 322.07,
  "end": 326.35
 },
 {
  "input": "For example, here's the kind of question I want you to be able to answer by the end of this video.",
  "translatedText": "Dit is bijvoorbeeld een vraag waarvan ik wil dat je die aan het einde van deze video kunt beantwoorden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 327.21,
  "end": 331.57
 },
 {
  "input": "Suppose you rolled a die 100 times and you added together the results.",
  "translatedText": "Stel dat je 100 keer met een dobbelsteen zou gooien en de resultaten bij elkaar zou optellen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 332.19,
  "end": 335.89
 },
 {
  "input": "Could you find a range of values such that you're 95% sure that the sum will fall within that range?",
  "translatedText": "Kun je een reeks waarden vinden waarvan je 95% zeker weet dat de som binnen die reeks valt?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.63,
  "end": 342.17
 },
 {
  "input": "Or maybe I should say find the smallest possible range of values such that this is true.",
  "translatedText": "Of misschien moet ik zeggen: vind het kleinst mogelijke bereik van waarden zodat dit waar is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 342.83,
  "end": 346.55
 },
 {
  "input": "The neat thing is you'll be able to answer this question whether it's a fair die or if it's a weighted die.",
  "translatedText": "Het leuke is dat je deze vraag kunt beantwoorden als het een eerlijke dobbelsteen is of als het een gewogen dobbelsteen is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 347.39,
  "end": 352.13
 },
 {
  "input": "Now let me say at the top that this theorem has three different assumptions that go into it, three things that have to be true before the theorem follows.",
  "translatedText": "Laat ik bovenaan zeggen dat deze stelling drie verschillende aannames heeft, drie dingen die waar moeten zijn voordat de stelling volgt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.45,
  "end": 360.13
 },
 {
  "input": "And I'm actually not going to tell you what they are until the very end of the video.",
  "translatedText": "En ik ga je eigenlijk pas helemaal aan het einde van de video vertellen wat ze zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 360.43,
  "end": 363.79
 },
 {
  "input": "Instead I want you to keep your eye out and see if you can notice and maybe predict what those three assumptions are going to be.",
  "translatedText": "In plaats daarvan wil ik dat je je ogen open houdt en kijkt of je kunt opmerken en misschien kunt voorspellen wat die drie aannames zullen zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 364.27,
  "end": 369.67
 },
 {
  "input": "As a next step, to better illustrate just how general this theorem is, I want to run a couple more simulations for you focused on the dice example.",
  "translatedText": "Als volgende stap, om beter te illustreren hoe algemeen deze stelling is, wil ik nog een paar simulaties voor je uitvoeren gericht op het dobbelsteenvoorbeeld.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 370.71,
  "end": 377.39
 },
 {
  "input": "Usually if you think of rolling a die you think of the six outcomes as being equally probable, but the theorem actually doesn't care about that.",
  "translatedText": "Meestal denk je bij het gooien van een dobbelsteen dat de zes uitkomsten even waarschijnlijk zijn, maar de stelling maakt zich daar eigenlijk niet druk om.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 380.91,
  "end": 387.63
 },
 {
  "input": "We could start with a weighted die, something with a non-trivial distribution across the outcomes, and the core idea still holds.",
  "translatedText": "We zouden kunnen beginnen met een gewogen dobbelsteen, iets met een niet-triviale verdeling over de uitkomsten, en dan geldt het kernidee nog steeds.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.83,
  "end": 394.55
 },
 {
  "input": "For the simulation what I'll do is take some distribution like this one that is skewed towards lower values.",
  "translatedText": "Voor de simulatie neem ik een verdeling zoals deze die scheef is naar lagere waarden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.03,
  "end": 399.93
 },
 {
  "input": "I'm going to take 10 distinct samples from that distribution and then I'll record the sum of that sample on the plot on the bottom.",
  "translatedText": "Ik ga 10 verschillende steekproeven nemen uit die verdeling en dan noteer ik de som van die steekproeven op de plot aan de onderkant.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 400.25,
  "end": 407.55
 },
 {
  "input": "Then I'm going to do this many many different times, always with a sum of size 10, but keep track of where those sums ended up to give us a sense of the distribution.",
  "translatedText": "Vervolgens ga ik dit veel verschillende keren doen, steeds met een som van grootte 10, maar ik houd bij waar die sommen zijn uitgekomen om ons een idee te geven van de verdeling.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 408.63,
  "end": 416.59
 },
 {
  "input": "And in fact let me rescale the y direction to give us room to run an even larger number of samples.",
  "translatedText": "En laat ik de y-richting herschalen om ons de ruimte te geven om een nog groter aantal monsters te nemen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 419.97,
  "end": 424.73
 },
 {
  "input": "And I'll let it go all the way up to a couple thousand, and as it does you'll notice that the shape that starts to emerge looks like a bell curve.",
  "translatedText": "En ik laat het helemaal doorlopen tot een paar duizend, en als dat gebeurt zul je zien dat de vorm die begint te ontstaan lijkt op een klokvormige curve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.03,
  "end": 432.49
 },
 {
  "input": "Maybe if you squint your eyes you can see it skews a tiny bit to the left, but it's neat that something so symmetric emerged from a starting point that was so asymmetric.",
  "translatedText": "Misschien als je je ogen dichtknijpt kun je zien dat het een klein beetje naar links helt, maar het is mooi dat er iets zo symmetrisch is ontstaan uit een startpunt dat zo asymmetrisch was.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.87,
  "end": 441.01
 },
 {
  "input": "To better illustrate what the central limit theorem is all about, let me run four of these simulations in parallel, where on the upper left I'm doing it where we're only adding two dice at a time, on the upper right we're doing it where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at a time, and then we'll do another one with a bigger sum, 15 at a time.",
  "translatedText": "Om beter te illustreren waar het bij de stelling van de centrale limiet om gaat, laat ik vier van deze simulaties parallel lopen, waarbij ik linksboven doe waar we slechts twee dobbelstenen per keer toevoegen, rechtsboven doen we het waar we vijf dobbelstenen per keer toevoegen, linksonder is degene die we net zagen met 10 dobbelstenen per keer, en dan doen we er nog een met een grotere som, 15 per keer.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 441.47,
  "end": 461.37
 },
 {
  "input": "Notice how on the upper left when we're just adding two dice, the resulting distribution doesn't really look like a bell curve, it looks a lot more reminiscent of the one we started with, skewed towards the left.",
  "translatedText": "Merk op hoe linksboven, wanneer we slechts twee dobbelstenen toevoegen, de resulterende verdeling er niet echt uitziet als een klokvormige curve, maar veel meer lijkt op de verdeling waarmee we begonnen, scheef naar links.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 462.25,
  "end": 472.03
 },
 {
  "input": "But as we allow for more and more dice in each sum, the resulting shape that comes up in these distributions looks more and more symmetric.",
  "translatedText": "Maar als we steeds meer dobbelstenen in elke som opnemen, ziet de resulterende vorm in deze verdelingen er steeds symmetrischer uit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 472.81,
  "end": 479.81
 },
 {
  "input": "It has the lump in the middle and fade towards the tail's shape of a bell curve.",
  "translatedText": "Het heeft de bobbel in het midden en vervaagt naar de staart toe in de vorm van een bell curve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.95,
  "end": 483.89
 },
 {
  "input": "And let me emphasize again, you can start with any different distribution.",
  "translatedText": "En ik wil nogmaals benadrukken dat je met elke andere verdeling kunt beginnen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 487.05,
  "end": 490.49
 },
 {
  "input": "Here I'll run it again, but where most of the probability is tied up in the numbers 1 and 6, with very low probability for the mid values.",
  "translatedText": "Hier voer ik het nog een keer uit, maar dan met de meeste waarschijnlijkheid in de getallen 1 en 6, met een zeer lage waarschijnlijkheid voor de middelste waarden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 490.49,
  "end": 497.49
 },
 {
  "input": "Despite completely changing the distribution for an individual roll of the die, it's still the case that a bell curve shape will emerge as we consider the different sums.",
  "translatedText": "Ondanks het feit dat de verdeling voor een individuele dobbelsteenworp volledig verandert, is het nog steeds zo dat er een klokvorm ontstaat als we de verschillende sommen bekijken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.19,
  "end": 506.55
 },
 {
  "input": "Illustrating things with a simulation like this is very fun, and it's kind of neat to see order emerge from chaos, but it also feels a little imprecise.",
  "translatedText": "Dingen illustreren met een simulatie als deze is erg leuk, en het is best leuk om orde uit chaos te zien ontstaan, maar het voelt ook een beetje onnauwkeurig.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.27,
  "end": 515.03
 },
 {
  "input": "Like in this case, when I cut off the simulation at 3000 samples, even though it kind of looks like a bell curve, the different buckets seem pretty spiky, and you might wonder, is it supposed to look that way, or is that just an artifact of the randomness in the simulation?",
  "translatedText": "Zoals in dit geval, als ik de simulatie afsnijd bij 3000 samples, ook al lijkt het een beetje op een bell curve, de verschillende emmers lijken behoorlijk piekerig, en je kunt je afvragen, hoort het er zo uit te zien, of is dat gewoon een artefact van de willekeurigheid in de simulatie?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 515.39,
  "end": 528.55
 },
 {
  "input": "And if it is, how many samples do we need before we can be sure that what we're looking at is representative of the true distribution?",
  "translatedText": "En als dat zo is, hoeveel monsters hebben we dan nodig om er zeker van te zijn dat waar we naar kijken representatief is voor de echte verdeling?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 529.01,
  "end": 535.11
 },
 {
  "input": "Instead moving forward, let's get a little more theoretical and show the precise shape these distributions will take on in the long run.",
  "translatedText": "Laten we in plaats daarvan iets theoretischer te werk gaan en de precieze vorm laten zien die deze verdelingen op de lange termijn zullen aannemen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.19,
  "end": 545.47
 },
 {
  "input": "The easiest case to make this calculation is if we have a uniform distribution, where each possible face of the die has an equal probability, 1 6th.",
  "translatedText": "Het eenvoudigste geval om deze berekening te maken is als we een uniforme verdeling hebben, waarbij elk mogelijk vlak van de dobbelsteen een gelijke waarschijnlijkheid heeft, 1 6e.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 546.13,
  "end": 553.97
 },
 {
  "input": "For example, if you then want to know how likely different sums are for a pair of dice, it's essentially a counting game, where you count up how many distinct pairs take on the same sum, which in the diagram I've drawn, you can conveniently think about by going through all the different diagonals.",
  "translatedText": "Als je dan bijvoorbeeld wilt weten hoe waarschijnlijk verschillende sommen zijn voor een dobbelsteenpaar, is het in wezen een telspel, waarbij je telt hoeveel verschillende paren dezelfde som aannemen, wat je in het diagram dat ik heb getekend handig kunt bedenken door alle verschillende diagonalen te doorlopen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.99,
  "end": 568.49
 },
 {
  "input": "Since each such pair has an equal chance of showing up, 1 in 36, all you have to do is count the sizes of these buckets.",
  "translatedText": "Omdat elk paar een gelijke kans heeft om op te duiken, 1 op 36, hoef je alleen maar de grootte van deze emmers te tellen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.41,
  "end": 577.53
 },
 {
  "input": "That gives us a definitive shape for the distribution describing a sum of two dice, and if we were to play the same game with all possible triplets, the resulting distribution would look like this.",
  "translatedText": "Dat geeft ons een definitieve vorm voor de verdeling die een som van twee dobbelstenen beschrijft, en als we hetzelfde spel zouden spelen met alle mogelijke tripletten, zou de resulterende verdeling er als volgt uitzien.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 578.19,
  "end": 588.13
 },
 {
  "input": "Now what's more challenging, but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that single die.",
  "translatedText": "Wat nu uitdagender is, maar veel interessanter, is de vraag wat er gebeurt als we een niet-uniforme verdeling hebben voor die enkele dobbelsteen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 588.69,
  "end": 594.99
 },
 {
  "input": "We actually talked all about this in the last video.",
  "translatedText": "We hebben het hier eigenlijk allemaal over gehad in de vorige video.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 595.55,
  "end": 597.97
 },
 {
  "input": "You do essentially the same thing, you go through all the distinct pairs of dice which add up to the same value.",
  "translatedText": "Je doet in wezen hetzelfde, je gaat door alle verschillende paren dobbelstenen die opgeteld dezelfde waarde hebben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 598.45,
  "end": 603.67
 },
 {
  "input": "It's just that instead of counting those pairs, for each pair you multiply the two probabilities of each particular face coming up, and then you add all those together.",
  "translatedText": "In plaats van die paren te tellen, vermenigvuldig je voor elk paar de twee waarschijnlijkheden dat elk bepaald gezicht verschijnt en tel je die bij elkaar op.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 603.97,
  "end": 612.75
 },
 {
  "input": "The computation that does this for all possible sums has a fancy name, it's called a convolution, but it's essentially just the weighted version of the counting game that anyone who's played with a pair of dice already finds familiar.",
  "translatedText": "De berekening die dit doet voor alle mogelijke sommen heeft een mooie naam, het heet een convolutie, maar het is in wezen gewoon de gewogen versie van het telspel dat iedereen die wel eens met een paar dobbelstenen heeft gespeeld al bekend voorkomt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.29,
  "end": 624.47
 },
 {
  "input": "For our purposes in this lesson, I'll have the computer calculate all that, simply display the results for you, and invite you to observe certain patterns, but under the hood, this is what's going on.",
  "translatedText": "Voor ons doel in deze les laat ik de computer dat allemaal berekenen, de resultaten eenvoudigweg voor je weergeven en je uitnodigen om bepaalde patronen te observeren, maar onder de motorkap gebeurt er het volgende.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 625.03,
  "end": 635.33
 },
 {
  "input": "So just to be crystal clear on what's being represented here, if you imagine sampling two different values from that top distribution, the one describing a single die, and adding them together, then the second distribution I'm drawing represents how likely you are to see various different sums.",
  "translatedText": "Dus om glashelder te zijn over wat hier wordt weergegeven, als je je voorstelt dat je twee verschillende waarden uit die bovenste verdeling neemt, die ene die een enkele dobbelsteen beschrijft, en die bij elkaar optelt, dan geeft de tweede verdeling die ik teken aan hoe groot de kans is dat je verschillende bedragen ziet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 636.65,
  "end": 652.23
 },
 {
  "input": "Likewise, if you imagine sampling three distinct values from that top distribution, and adding them together, the next plot represents the probabilities for various different sums in that case.",
  "translatedText": "Op dezelfde manier, als je je voorstelt dat je drie verschillende waarden uit die bovenste verdeling neemt en die bij elkaar optelt, geeft de volgende grafiek de waarschijnlijkheid weer voor verschillende sommen in dat geval.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 652.89,
  "end": 662.49
 },
 {
  "input": "So if I compute what the distributions for these sums look like for larger and larger sums, well you know what I'm going to say, it looks more and more like a bell curve.",
  "translatedText": "Dus als ik bereken hoe de verdelingen voor deze sommen eruit zien voor grotere en grotere sommen, nou je weet wat ik ga zeggen, dan lijkt het steeds meer op een bell curve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 663.51,
  "end": 672.39
 },
 {
  "input": "But before we get to that, I want you to make a couple more simple observations.",
  "translatedText": "Maar voordat we daaraan beginnen, wil ik dat je nog een paar eenvoudige opmerkingen maakt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 673.35,
  "end": 676.45
 },
 {
  "input": "For example, these distributions seem to be wandering to the right, and also they seem to be getting more spread out, and a little bit more flat.",
  "translatedText": "Deze verdelingen lijken bijvoorbeeld naar rechts af te dwalen, en ze lijken ook meer gespreid en een beetje vlakker te worden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 677.45,
  "end": 684.79
 },
 {
  "input": "You cannot describe the central limit theorem quantitatively without taking into account both of those effects, which in turn requires describing the mean and the standard deviation.",
  "translatedText": "Je kunt de stelling van de centrale limiet niet kwantitatief beschrijven zonder rekening te houden met beide effecten, wat weer vereist dat je het gemiddelde en de standaardafwijking beschrijft.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.25,
  "end": 693.19
 },
 {
  "input": "Maybe you're already familiar with those, but I want to make minimal assumptions here, and it never hurts to review, so let's quickly go over both of those.",
  "translatedText": "Misschien ben je daar al bekend mee, maar ik wil hier zo min mogelijk aannames doen en het kan nooit kwaad om ze nog eens te bekijken, dus laten we ze allebei even snel doornemen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 693.95,
  "end": 700.61
 },
 {
  "input": "The mean of a distribution, often denoted with the Greek letter mu, is a way of capturing the center of mass for that distribution.",
  "translatedText": "Het gemiddelde van een verdeling, vaak aangeduid met de Griekse letter mu, is een manier om het massamiddelpunt van die verdeling vast te leggen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 703.41,
  "end": 710.71
 },
 {
  "input": "It's calculated as the expected value of our random variable, which is a way of saying you go through all of the different possible outcomes, and you multiply the probability of that outcome times the value of the variable.",
  "translatedText": "Het wordt berekend als de verwachte waarde van onze willekeurige variabele, wat een manier is om te zeggen dat je alle verschillende mogelijke uitkomsten doorloopt en de waarschijnlijkheid van die uitkomst vermenigvuldigt met de waarde van de variabele.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 711.19,
  "end": 722.85
 },
 {
  "input": "If higher values are more probable, that weighted sum is going to be bigger.",
  "translatedText": "Als hogere waarden waarschijnlijker zijn, zal die gewogen som groter zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 723.19,
  "end": 726.41
 },
 {
  "input": "If lower values are more probable, that weighted sum is going to be smaller.",
  "translatedText": "Als lagere waarden waarschijnlijker zijn, zal die gewogen som kleiner zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 726.75,
  "end": 729.95
 },
 {
  "input": "A little more interesting is if you want to measure how spread out this distribution is, because there's multiple different ways you might do it.",
  "translatedText": "Iets interessanter is het als je wilt meten hoe gespreid deze verdeling is, omdat je dat op verschillende manieren kunt doen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.79,
  "end": 737.13
 },
 {
  "input": "One of them is called the variance.",
  "translatedText": "Eén daarvan heet de variantie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.53,
  "end": 740.29
 },
 {
  "input": "The idea there is to look at the difference between each possible value and the mean, square that difference, and ask for its expected value.",
  "translatedText": "Het idee is om te kijken naar het verschil tussen elke mogelijke waarde en het gemiddelde, dat verschil te kwadrateren en te vragen naar de verwachte waarde.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 740.83,
  "end": 748.27
 },
 {
  "input": "The idea is that whether your value is below or above the mean, when you square that difference, you get a positive number, and the larger the difference, the bigger that number.",
  "translatedText": "Het idee is dat of je waarde nu onder of boven het gemiddelde ligt, als je dat verschil kwadrateert, je een positief getal krijgt, en hoe groter het verschil, hoe groter dat getal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 748.73,
  "end": 756.65
 },
 {
  "input": "Squaring it like this turns out to make the math much much nicer than if we did something like an absolute value, but the downside is that it's hard to think about this as a distance in our diagram because the units are off, kind of like the units here are square units, whereas a distance in our diagram would be a kind of linear unit.",
  "translatedText": "Door het op deze manier te kwadrateren wordt de wiskunde veel mooier dan wanneer we zoiets als een absolute waarde zouden doen, maar het nadeel is dat het moeilijk is om dit als een afstand in ons diagram te zien omdat de eenheden niet gelijk zijn, alsof de eenheden hier vierkante eenheden zijn, terwijl een afstand in ons diagram een soort lineaire eenheid zou zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.37,
  "end": 773.31
 },
 {
  "input": "So another way to measure spread is what's called the standard deviation, which is the square root of this value.",
  "translatedText": "Een andere manier om spreiding te meten is de zogenaamde standaardafwijking, die de vierkantswortel van deze waarde is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 773.71,
  "end": 779.19
 },
 {
  "input": "That can be interpreted much more reasonably as a distance on our diagram, and it's commonly denoted with the Greek letter sigma, so you know m for standard deviation, but both in Greek.",
  "translatedText": "Dat kan veel redelijker worden geïnterpreteerd als een afstand op ons diagram, en het wordt gewoonlijk aangeduid met de Griekse letter sigma, dus je kent m voor standaardafwijking, maar allebei in het Grieks.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 779.47,
  "end": 789.65
 },
 {
  "input": "Looking back at our sequence of distributions, let's talk about the mean and standard deviation.",
  "translatedText": "Als we terugkijken naar onze reeks verdelingen, laten we het dan hebben over het gemiddelde en de standaardafwijking.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.87,
  "end": 796.15
 },
 {
  "input": "If we call the mean of the initial distribution mu, which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if I tell you that the mean of the next one is 2 times mu.",
  "translatedText": "Als we het gemiddelde van de eerste verdeling mu noemen, wat voor de geïllustreerde toevallig 2,24 is, is het hopelijk niet al te verrassend als ik je vertel dat het gemiddelde van de volgende verdeling 2 keer mu is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.63,
  "end": 806.73
 },
 {
  "input": "That is, you roll a pair of dice, you want to know the expected value of the sum, it's two times the expected value for a single die.",
  "translatedText": "Dat wil zeggen, je gooit met een paar dobbelstenen, je wilt de verwachte waarde van de som weten, die is twee keer de verwachte waarde voor een enkele dobbelsteen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 807.13,
  "end": 812.81
 },
 {
  "input": "Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth.",
  "translatedText": "Op dezelfde manier is de verwachte waarde voor onze som van grootte 3 3 keer mu, enzovoort.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 813.85,
  "end": 819.41
 },
 {
  "input": "The mean just marches steadily on to the right, which is why our distributions seem to be drifting off in that direction.",
  "translatedText": "Het gemiddelde marcheert gewoon gestaag naar rechts en daarom lijken onze verdelingen in die richting af te drijven.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 819.63,
  "end": 824.87
 },
 {
  "input": "A little more challenging, but very important, is to describe how the standard deviation changes.",
  "translatedText": "Iets lastiger, maar erg belangrijk, is om te beschrijven hoe de standaardafwijking verandert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.35,
  "end": 829.91
 },
 {
  "input": "The key fact here is that if you have two different random variables, then the variance for the sum of those variables is the same as just adding together the original two variances.",
  "translatedText": "Het belangrijkste feit hier is dat als je twee verschillende willekeurige variabelen hebt, de variantie voor de som van die variabelen hetzelfde is als wanneer je de oorspronkelijke twee varianties bij elkaar optelt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 830.49,
  "end": 839.37
 },
 {
  "input": "This is one of those facts that you can just compute when you unpack all the definitions.",
  "translatedText": "Dit is een van die feiten die je gewoon kunt berekenen als je alle definities uitpakt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 839.93,
  "end": 843.63
 },
 {
  "input": "There are a couple nice intuitions for why it's true.",
  "translatedText": "Er zijn een paar aardige intuïties voor waarom het waar is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 843.63,
  "end": 846.21
 },
 {
  "input": "My tentative plan is to just actually make a series about probability and talk about things like intuitions underlying variance and its cousins there.",
  "translatedText": "Mijn voorlopige plan is om eigenlijk gewoon een serie te maken over waarschijnlijkheid en het te hebben over dingen als intuïties die ten grondslag liggen aan variantie en zijn neven en nichten daar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 846.63,
  "end": 853.53
 },
 {
  "input": "But right now, the main thing I want you to highlight is how it's the variance that adds, it's not the standard deviation that adds.",
  "translatedText": "Maar nu wil ik dat je vooral benadrukt dat het de variantie is die toevoegt, niet de standaardafwijking.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 854.01,
  "end": 860.15
 },
 {
  "input": "So, critically, if you were to take n different realizations of the same random variable and ask what the sum looks like, the variance of sum is n times the variance of your original variable, meaning the standard deviation, the square root of all this, is the square root of n times the original standard deviation.",
  "translatedText": "Dus, kritisch gezien, als je n verschillende realisaties van dezelfde willekeurige variabele neemt en vraagt hoe de som eruit ziet, is de variantie van de som n keer de variantie van je oorspronkelijke variabele, wat betekent dat de standaardafwijking, de vierkantswortel van dit alles, de vierkantswortel is van n keer de oorspronkelijke standaardafwijking.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 860.41,
  "end": 878.25
 },
 {
  "input": "For example, back in our sequence of distributions, if we label the standard deviation of our initial one with sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on This, like I said, is very important.",
  "translatedText": "Als we bijvoorbeeld in onze reeks verdelingen de standaardafwijking van onze eerste verdelingen labelen met sigma, dan wordt de volgende standaardafwijking de vierkantswortel van 2 keer sigma, en daarna lijkt het op de vierkantswortel van 3 keer sigma, enzovoort Dit is, zoals ik al zei, heel belangrijk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 879.29,
  "end": 895.65
 },
 {
  "input": "It means that even though our distributions are getting spread out, they're not spreading out all that quickly, they only do so in proportion to the square root of the size of the sum.",
  "translatedText": "Het betekent dat onze verdelingen zich weliswaar verspreiden, maar niet zo snel, ze verspreiden zich alleen in verhouding tot de vierkantswortel van de grootte van de som.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.07,
  "end": 904.13
 },
 {
  "input": "As we prepare to make a more quantitative description of the central limit theorem, the core intuition I want you to keep in your head is that we'll basically realign all of these distributions so that their means line up together, and then rescale them so that all of the standard deviations are just going to be equal to one.",
  "translatedText": "Terwijl we ons voorbereiden op een meer kwantitatieve beschrijving van de stelling van de centrale limiet, is de belangrijkste intuïtie die je in je hoofd moet houden dat we in principe al deze verdelingen opnieuw uitlijnen zodat hun gemiddelden op één lijn liggen, en ze dan herschalen zodat alle standaarddeviaties gelijk zijn aan één.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 904.71,
  "end": 920.61
 },
 {
  "input": "And when we do that, the shape that results gets closer and closer to a certain universal shape, described with an elegant little function that we'll unpack in just a moment.",
  "translatedText": "En als we dat doen, komt de vorm die ontstaat steeds dichter bij een bepaalde universele vorm, beschreven met een elegante kleine functie die we zo meteen zullen uitpakken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 921.29,
  "end": 929.87
 },
 {
  "input": "And let me say one more time, the real magic here is how we could have started with any distribution, describing a single roll of the die, and if we play the same game, considering what the distributions for the many different sums look like, and we realign them so that the means line up, and we rescale them so that the standard deviations are all one, we still approach that same universal shape, which is kind of mind-boggling.",
  "translatedText": "En laat ik nog één keer zeggen dat de echte magie hier is dat we met elke verdeling hadden kunnen beginnen, die een enkele worp van de dobbelsteen beschrijft, en als we hetzelfde spel spelen en kijken hoe de verdelingen voor de vele verschillende sommen eruit zien, en we richten ze opnieuw in zodat de gemiddelden op één lijn liggen, en we herschalen ze zodat de standaarddeviaties allemaal één zijn, dan benaderen we nog steeds diezelfde universele vorm, wat nogal verbijsterend is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 930.47,
  "end": 952.95
 },
 {
  "input": "And now, my friends, is probably as good a time as any to finally get into the formula for a normal distribution.",
  "translatedText": "En nu, mijn vrienden, is het waarschijnlijk het beste moment om eindelijk de formule voor een normale verdeling te bespreken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 954.81,
  "end": 960.85
 },
 {
  "input": "And the way I'd like to do this is to basically peel back all the layers and build it up one piece at a time.",
  "translatedText": "En de manier waarop ik dit wil doen is door in principe alle lagen af te pellen en het stuk voor stuk op te bouwen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 961.49,
  "end": 965.93
 },
 {
  "input": "The function e to the x, or anything to the x, describes exponential growth, and if you make that exponent negative, which flips around the graph horizontally, you might think of it as describing exponential decay.",
  "translatedText": "De functie e naar de x, of wat dan ook naar de x, beschrijft exponentiële groei, en als je die exponent negatief maakt, waardoor de grafiek horizontaal omdraait, kun je denken dat het exponentieel verval beschrijft.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 966.53,
  "end": 977.87
 },
 {
  "input": "To make this decay in both directions, you could do something to make sure the exponent is always negative and growing, like taking the negative absolute value.",
  "translatedText": "Om dit in beide richtingen te laten vervallen, zou je iets kunnen doen om ervoor te zorgen dat de exponent altijd negatief is en groeit, zoals het nemen van de negatieve absolute waarde.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.51,
  "end": 985.43
 },
 {
  "input": "That would give us this kind of awkward sharp point in the middle, but if instead you make that exponent the negative square of x, you get a smoother version of the same thing, which decays in both directions.",
  "translatedText": "Dat zou ons een soort onhandige scherpe punt in het midden geven, maar als je in plaats daarvan die exponent het negatieve kwadraat van x maakt, krijg je een vloeiendere versie van hetzelfde, die in beide richtingen afneemt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 985.93,
  "end": 995.81
 },
 {
  "input": "This gives us the basic bell curve shape.",
  "translatedText": "Dit geeft ons de basisvorm van de bell curve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 996.33,
  "end": 998.19
 },
 {
  "input": "Now if you throw a constant in front of that x, and you scale that constant up and down, it lets you stretch and squish the graph horizontally, allowing you to describe narrow and wider bell curves.",
  "translatedText": "Als je nu een constante voor die x zet en je schaalt die constante op en neer, dan kun je de grafiek horizontaal uitrekken en in elkaar drukken, waardoor je smalle en bredere klokbochten kunt beschrijven.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.65,
  "end": 1008.37
 },
 {
  "input": "And a quick thing I'd like to point out here is that based on the rules of exponentiation, as we tweak around that constant c, you could also think about it as simply changing the base of the exponentiation.",
  "translatedText": "En ik wil er even op wijzen dat je, gebaseerd op de regels van exponentiatie, als we de constante c aanpassen, dit ook kunt zien als het veranderen van de basis van de exponentiatie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1009.01,
  "end": 1019.75
 },
 {
  "input": "And in that sense, the number e is not really all that special for our formula.",
  "translatedText": "En in die zin is het getal e niet echt bijzonder voor onze formule.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1020.15,
  "end": 1023.63
 },
 {
  "input": "We could replace it with any other positive constant, and you'll get the same family of curves as we tweak that constant.",
  "translatedText": "We kunnen die vervangen door elke andere positieve constante en dan krijg je dezelfde reeks krommen als we die constante aanpassen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1024.05,
  "end": 1030.49
 },
 {
  "input": "Make it a 2, same family of curves.",
  "translatedText": "Maak er een 2 van, dezelfde familie van krommen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.51,
  "end": 1033.11
 },
 {
  "input": "Make it a 3, same family of curves.",
  "translatedText": "Maak er een 3 van, dezelfde familie van krommen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1033.33,
  "end": 1035.07
 },
 {
  "input": "The reason we use e is that it gives that constant a very readable meaning.",
  "translatedText": "De reden dat we e gebruiken is dat het die constante een zeer leesbare betekenis geeft.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1035.75,
  "end": 1039.49
 },
 {
  "input": "Or rather, if we reconfigure things a little bit so that the exponent looks like negative 1 half times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn this into a probability distribution, that constant sigma will be the standard deviation of that distribution.",
  "translatedText": "Of liever, als we de dingen een beetje herconfigureren zodat de exponent eruitziet als negatief anderhalf keer x gedeeld door een bepaalde constante, die we suggestief sigma kwadraat zullen noemen, dan zal die constante sigma, als we er een kansverdeling van maken, de standaardafwijking van die verdeling zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1040.11,
  "end": 1057.21
 },
 {
  "input": "And that's very nice.",
  "translatedText": "En dat is erg leuk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.81,
  "end": 1058.57
 },
 {
  "input": "But before we can interpret this as a probability distribution, we need the area under the curve to be 1.",
  "translatedText": "Maar voordat we dit kunnen interpreteren als een kansverdeling, moet de oppervlakte onder de curve 1 zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1058.91,
  "end": 1064.31
 },
 {
  "input": "And the reason for that is how the curve is interpreted.",
  "translatedText": "En de reden daarvoor is hoe de curve wordt geïnterpreteerd.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1064.83,
  "end": 1066.91
 },
 {
  "input": "Unlike discrete distributions, when it comes to something continuous, you don't ask about the probability of a particular point.",
  "translatedText": "In tegenstelling tot discrete verdelingen, vraag je bij iets continu niet naar de waarschijnlijkheid van een bepaald punt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1067.37,
  "end": 1073.37
 },
 {
  "input": "Instead, you ask for the probability that a value falls between two different values.",
  "translatedText": "In plaats daarvan vraag je naar de waarschijnlijkheid dat een waarde tussen twee verschillende waarden valt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1073.79,
  "end": 1078.23
 },
 {
  "input": "And what the curve is telling you is that that probability equals the area under the curve between those two values.",
  "translatedText": "En de curve vertelt je dat die waarschijnlijkheid gelijk is aan het gebied onder de curve tussen die twee waarden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1078.75,
  "end": 1085.43
 },
 {
  "input": "There's a whole other video about this, they're called probability density functions.",
  "translatedText": "Hier is een hele andere video over, ze worden kansdichtheidsfuncties genoemd.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1086.03,
  "end": 1089.43
 },
 {
  "input": "The main point right now is that the area under the entire curve represents the probability that something happens, that some number comes up.",
  "translatedText": "Het belangrijkste punt op dit moment is dat het gebied onder de hele curve de waarschijnlijkheid vertegenwoordigt dat er iets gebeurt, dat er een getal naar boven komt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1089.83,
  "end": 1097.15
 },
 {
  "input": "That should be 1, which is why we want the area under this to be 1.",
  "translatedText": "Dat moet 1 zijn, daarom willen we dat het gebied eronder 1 is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1097.41,
  "end": 1100.63
 },
 {
  "input": "As it stands with the basic bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root of pi.",
  "translatedText": "Zoals het er nu uitziet met de basisvorm van de klokcurve van e tot de negatieve x in het kwadraat, is de oppervlakte niet 1, maar de vierkantswortel van pi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1101.05,
  "end": 1107.79
 },
 {
  "input": "I know, right?",
  "translatedText": "Ik weet het, toch?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.41,
  "end": 1109.15
 },
 {
  "input": "What is pi doing here?",
  "translatedText": "Wat doet pi hier?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1109.27,
  "end": 1110.19
 },
 {
  "input": "What does this have to do with circles?",
  "translatedText": "Wat heeft dit met cirkels te maken?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1110.29,
  "end": 1111.47
 },
 {
  "input": "Like I said at the start, I'd love to talk all about that in the next video.",
  "translatedText": "Zoals ik in het begin al zei, wil ik het daar graag over hebben in de volgende video.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.01,
  "end": 1115.05
 },
 {
  "input": "But if you can spare your excitement, for our purposes right now, all it means is that we should divide this function by the square root of pi, and it gives us the area we want.",
  "translatedText": "Maar als je je opwinding kunt sparen, voor ons doel op dit moment betekent het alleen dat we deze functie moeten delen door de vierkantswortel van pi, en dat geeft ons de oppervlakte die we willen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1115.33,
  "end": 1123.17
 },
 {
  "input": "Throwing back in the constants we had earlier, the one half and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square root of 2.",
  "translatedText": "Als we de constanten die we eerder hadden, de ene helft en de sigma, weer in de grafiek gooien, is het effect dat de grafiek wordt uitgerekt met een factor sigma maal de vierkantswortel van 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.61,
  "end": 1131.79
 },
 {
  "input": "So we also need to divide out by that in order to make sure it has an area of 1, and combining those fractions, the factor out front looks like 1 divided by sigma times the square root of 2 pi.",
  "translatedText": "We moeten dus ook delen door dat om er zeker van te zijn dat het een oppervlakte van 1 heeft, en door die breuken te combineren ziet de factor vooraan eruit als 1 gedeeld door sigma maal de vierkantswortel van 2 pi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1132.41,
  "end": 1142.11
 },
 {
  "input": "This, finally, is a valid probability distribution.",
  "translatedText": "Dit is uiteindelijk een geldige kansverdeling.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1142.91,
  "end": 1145.85
 },
 {
  "input": "As we tweak that value sigma, resulting in narrower and wider curves, that constant in the front always guarantees that the area equals 1.",
  "translatedText": "Als we die waarde sigma aanpassen, wat resulteert in smallere en bredere krommen, garandeert die constante vooraan altijd dat de oppervlakte gelijk is aan 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1146.45,
  "end": 1154.31
 },
 {
  "input": "The special case where sigma equals 1 has a specific name, we call it the standard normal distribution, which plays an especially important role for you and me in this lesson.",
  "translatedText": "Het speciale geval waarin sigma gelijk is aan 1 heeft een specifieke naam, we noemen het de standaard normale verdeling, die voor jou en mij een belangrijke rol speelt in deze les.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.91,
  "end": 1164.51
 },
 {
  "input": "And all possible normal distributions are not only parameterized with this value sigma, but we also subtract off another constant mu from the variable x, and this essentially just lets you slide the graph left and right so that you can prescribe the mean of this distribution.",
  "translatedText": "En alle mogelijke normale verdelingen worden niet alleen geparametriseerd met deze waarde sigma, maar we trekken ook een andere constante mu af van de variabele x, en hiermee kun je in wezen de grafiek naar links en rechts schuiven zodat je het gemiddelde van deze verdeling kunt voorschrijven.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1165.13,
  "end": 1180.21
 },
 {
  "input": "So in short, we have two parameters, one describing the mean, one describing the standard deviation, and they're all tied together in this big formula involving an e and a pi.",
  "translatedText": "Dus in het kort hebben we twee parameters, één die het gemiddelde beschrijft en één die de standaardafwijking beschrijft, en ze zijn allemaal samengevoegd in deze grote formule met een e en een pi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1180.99,
  "end": 1189.19
 },
 {
  "input": "Now that all of that is on the table, let's look back again at the idea of starting with some random variable and asking what the distributions for sums of that variable look like.",
  "translatedText": "Laten we, nu dat allemaal op tafel ligt, nog eens terugkijken naar het idee om te beginnen met een willekeurige variabele en ons af te vragen hoe de verdelingen voor sommen van die variabele eruit zien.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.19,
  "end": 1199.81
 },
 {
  "input": "As we've already gone over, when you increase the size of that sum, the resulting distribution will shift according to a growing mean, and it slowly spreads out according to a growing standard deviation.",
  "translatedText": "Zoals we al hebben besproken, zal de resulterende verdeling verschuiven naarmate het gemiddelde toeneemt en zich langzaam verspreiden naarmate de standaardafwijking toeneemt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1200.13,
  "end": 1209.81
 },
 {
  "input": "And putting some actual formulas to it, if we know the mean of our underlying random variable, we call it mu, and we also know its standard deviation, and we call it sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the standard deviation will be sigma times the square root of that size.",
  "translatedText": "Als we het gemiddelde van onze onderliggende willekeurige variabele kennen, we noemen het mu, en we kennen ook de standaardafwijking, we noemen het sigma, dan zal het gemiddelde voor de som aan de onderkant mu maal de grootte van de som zijn, en de standaardafwijking zal sigma maal de vierkantswortel van die grootte zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1210.33,
  "end": 1227.73
 },
 {
  "input": "So now, if we want to claim that this looks more and more like a bell curve, and a bell curve is only described by two different parameters, the mean and the standard deviation, you know what to do.",
  "translatedText": "Dus als we nu willen beweren dat dit steeds meer op een belcurve gaat lijken, en een belcurve wordt slechts beschreven door twee verschillende parameters, het gemiddelde en de standaardafwijking, dan weet je wat je moet doen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1228.19,
  "end": 1237.71
 },
 {
  "input": "You could plug those two values into the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve that should closely fit our distribution.",
  "translatedText": "Je kunt die twee waarden in de formule stoppen en dan krijg je een zeer expliciete, zij het wat ingewikkelde formule voor een kromme die goed zou moeten passen bij onze verdeling.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1237.93,
  "end": 1246.99
 },
 {
  "input": "But there's another way we can describe it that's a little more elegant and lends itself to a very fun visual that we can build up to.",
  "translatedText": "Maar er is een andere manier om het te beschrijven die iets eleganter is en die zich leent voor een heel leuk beeld waar we naar toe kunnen werken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1248.39,
  "end": 1254.81
 },
 {
  "input": "Instead of focusing on the sum of all of these random variables, let's modify this expression a little bit, where what we'll do is we'll look at the mean that we expect that sum to take, and we subtract it off so that our new expression has a mean of zero, and then we're going to look at the standard deviation we expect of our sum, and divide out by that, which basically just rescales the units so that the standard deviation of our expression will equal one.",
  "translatedText": "In plaats van ons te richten op de som van al deze willekeurige variabelen, passen we deze uitdrukking een beetje aan, waarbij we kijken naar het gemiddelde dat we verwachten dat de som zal hebben, en dat trekken we er van af zodat onze nieuwe uitdrukking een gemiddelde van nul heeft, en dan gaan we kijken naar de standaardafwijking die we verwachten van onze som, en delen door dat, wat in feite gewoon de eenheden herschaalt zodat de standaardafwijking van onze uitdrukking gelijk is aan één.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1255.27,
  "end": 1278.77
 },
 {
  "input": "This might seem like a more complicated expression, but it actually has a highly readable meaning.",
  "translatedText": "Dit lijkt misschien een ingewikkelde uitdrukking, maar het heeft eigenlijk een zeer leesbare betekenis.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1279.35,
  "end": 1284.09
 },
 {
  "input": "It's essentially saying how many standard deviations away from the mean is this sum?",
  "translatedText": "Het zegt in wezen: hoeveel standaardafwijkingen van het gemiddelde is deze som?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1284.45,
  "end": 1289.67
 },
 {
  "input": "For example, this bar here corresponds to a certain value that you might find when you roll 10 dice and you add them all up, and its position a little above negative one is telling you that that value is a little bit less than one standard deviation lower than the mean.",
  "translatedText": "Deze balk hier komt bijvoorbeeld overeen met een bepaalde waarde die je zou kunnen vinden als je 10 dobbelstenen gooit en ze allemaal optelt, en de positie iets boven negatief één vertelt je dat die waarde iets minder dan één standaardafwijking lager is dan het gemiddelde.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1290.75,
  "end": 1303.87
 },
 {
  "input": "Also, by the way, in anticipation for the animation I'm trying to build to here, the way I'm representing things on that lower plot is that the area of each one of these bars is telling us the probability of the corresponding value rather than the height.",
  "translatedText": "Overigens, vooruitlopend op de animatie die ik hier probeer te maken, is de manier waarop ik de dingen in die onderste plot weergeef dat de oppervlakte van elk van deze balken ons de waarschijnlijkheid van de corresponderende waarde vertelt in plaats van de hoogte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.13,
  "end": 1316.99
 },
 {
  "input": "You might think of the y-axis as representing not probability but a kind of probability density.",
  "translatedText": "Je kunt de y-as niet zien als een weergave van waarschijnlijkheid, maar als een soort waarschijnlijkheidsdichtheid.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.23,
  "end": 1321.93
 },
 {
  "input": "The reason for this is to set the stage so that it aligns with the way we interpret continuous distributions, where the probability of falling between a range of values is equal to an area under a curve between those values.",
  "translatedText": "De reden hiervoor is om de stap zo te zetten dat deze overeenkomt met de manier waarop we continue verdelingen interpreteren, waarbij de waarschijnlijkheid dat je tussen een reeks waarden valt gelijk is aan de oppervlakte onder een kromme tussen die waarden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1322.27,
  "end": 1333.55
 },
 {
  "input": "In particular, the area of all the bars together is going to be one.",
  "translatedText": "In het bijzonder wordt de oppervlakte van alle balken samen één.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1333.91,
  "end": 1336.73
 },
 {
  "input": "Now, with all of that in place, let's have a little fun.",
  "translatedText": "Laten we nu, met dat alles op zijn plaats, een beetje plezier maken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1338.23,
  "end": 1340.95
 },
 {
  "input": "Let me start by rolling things back so that the distribution on the bottom represents a relatively small sum, like adding together only three such random variables.",
  "translatedText": "Laat ik beginnen met de dingen terug te draaien zodat de verdeling aan de onderkant een relatief kleine som vertegenwoordigt, alsof je slechts drie van zulke willekeurige variabelen bij elkaar optelt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1341.33,
  "end": 1349.01
 },
 {
  "input": "Notice what happens as I change the distribution we start with.",
  "translatedText": "Kijk wat er gebeurt als ik de verdeling verander waarmee we beginnen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1349.45,
  "end": 1352.43
 },
 {
  "input": "As it changes, the distribution on the bottom completely changes its shape.",
  "translatedText": "Als het verandert, verandert de verdeling op de bodem volledig van vorm.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1352.73,
  "end": 1356.29
 },
 {
  "input": "It's very dependent on what we started with.",
  "translatedText": "Het is erg afhankelijk van waar we mee begonnen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1356.51,
  "end": 1358.77
 },
 {
  "input": "If we let the size of our sum get a little bit bigger, say going up to 10, and as I change the distribution for x, it largely stays looking like a bell curve, but I can find some distributions that get it to change shape.",
  "translatedText": "Als we de grootte van onze som een beetje groter laten worden, bijvoorbeeld tot 10, en ik verander de verdeling voor x, dan blijft het er grotendeels uitzien als een bell curve, maar ik kan enkele verdelingen vinden die de vorm laten veranderen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.35,
  "end": 1371.63
 },
 {
  "input": "For example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results in this kind of spiky bell curve.",
  "translatedText": "Bijvoorbeeld, de echt scheve waar bijna alle waarschijnlijkheid in de getallen 1 of 6 zit, resulteert in een soort spiky bell curve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1372.23,
  "end": 1379.31
 },
 {
  "input": "And if you'll recall, earlier on I actually showed this in the form of a simulation.",
  "translatedText": "En als je het je herinnert, heb ik dit eerder laten zien in de vorm van een simulatie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1379.77,
  "end": 1383.51
 },
 {
  "input": "Though if you were wondering whether that spikiness was an artifact of the randomness or reflected the true distribution, turns out it reflects the true distribution.",
  "translatedText": "Maar als je je afvroeg of die stekeligheid een artefact van de willekeur was of de ware verdeling weerspiegelde, dan blijkt dat het de ware verdeling weerspiegelt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1384.13,
  "end": 1391.85
 },
 {
  "input": "In this case, 10 is not a large enough sum for the central limit theorem to kick in.",
  "translatedText": "In dit geval is 10 geen som die groot genoeg is om de stelling van de centrale limiet te laten werken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1392.29,
  "end": 1396.47
 },
 {
  "input": "But if instead I let that sum grow and I consider adding 50 different values, which is actually not that big, then no matter how I change the distribution for our underlying random variable, it has essentially no effect on the shape of the plot on the bottom.",
  "translatedText": "Maar als ik in plaats daarvan die som laat groeien en overweeg om 50 verschillende waarden toe te voegen, wat eigenlijk niet zo groot is, dan maakt het niet uit hoe ik de verdeling voor onze onderliggende willekeurige variabele verander, het heeft in wezen geen effect op de vorm van de plot aan de onderkant.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1396.47,
  "end": 1410.69
 },
 {
  "input": "No matter where we start, all of the information and nuance for the distribution of x gets washed away, and we tend towards this single universal shape described by a very elegant function for the standard normal distribution, 1 over square root of 2 pi times e to the negative x squared over 2.",
  "translatedText": "Waar we ook beginnen, alle informatie en nuance voor de verdeling van x wordt weggespoeld en we neigen naar deze ene universele vorm die wordt beschreven door een zeer elegante functie voor de standaard normale verdeling, 1 over vierkantswortel van 2 pi maal e tot de negatieve x kwadraat over 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1411.17,
  "end": 1427.07
 },
 {
  "input": "This, this right here is what the central limit theorem is all about.",
  "translatedText": "Dit, dit hier is waar de stelling van de centrale limiet over gaat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1427.81,
  "end": 1430.81
 },
 {
  "input": "Almost nothing you can do to this initial distribution changes the shape we tend towards.",
  "translatedText": "Bijna niets wat je aan deze initiële verdeling kunt doen, verandert de vorm waar we naar neigen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1431.13,
  "end": 1435.31
 },
 {
  "input": "Now, the more theoretically minded among you might still be wondering what is the actual theorem, like what's the mathematical statement that could be proved or disproved that we're claiming here.",
  "translatedText": "De meer theoretisch denkenden onder jullie vragen zich misschien nog steeds af wat de werkelijke stelling is, zoals de wiskundige verklaring die bewezen of weerlegd kan worden die we hier beweren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1439.03,
  "end": 1448.91
 },
 {
  "input": "If you want a nice formal statement, here's how it might go.",
  "translatedText": "Als je een mooie formele verklaring wilt, kun je het volgende doen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.03,
  "end": 1451.67
 },
 {
  "input": "Consider this value where we're summing up n different instantiations of our variable, but tweaked and tuned so that its mean and standard deviation are 1, again meaning you can read it as asking how many standard deviations away from the mean is the sum.",
  "translatedText": "Neem deze waarde waar we n verschillende instantiaties van onze variabele bij elkaar optellen, maar dan zo getweakt en getuned dat het gemiddelde en de standaardafwijking 1 zijn, wat weer betekent dat je het kunt lezen als de vraag hoeveel standaardafwijkingen van het gemiddelde de som is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1452.13,
  "end": 1465.35
 },
 {
  "input": "Then the actual rigorous no-jokes-this-time statement of the central limit theorem is that if you consider the probability that this value falls between two given real numbers, a and b, and you consider the limit of that probability as the size of your sum goes to infinity, then that limit is equal to a certain integral, which basically describes the area under a standard normal distribution between those two values.",
  "translatedText": "Dan is de feitelijke rigoureuze geen-grappen-dit-tijd verklaring van de stelling van de centrale limiet dat als je de waarschijnlijkheid beschouwt dat deze waarde tussen twee gegeven reële getallen valt, a en b, en je beschouwt de limiet van die waarschijnlijkheid als de grootte van je som naar oneindig gaat, dan is die limiet gelijk aan een bepaalde integraal, die in feite het gebied onder een standaard normale verdeling tussen die twee waarden beschrijft.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1465.77,
  "end": 1489.65
 },
 {
  "input": "Again, there are three underlying assumptions that I have yet to tell you, but other than those, in all of its gory detail, this right here is the central limit theorem.",
  "translatedText": "Nogmaals, er zijn drie onderliggende aannames die ik je nog moet vertellen, maar afgezien daarvan, in al zijn bloederige details, is dit hier de stelling van de centrale limiet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1491.25,
  "end": 1500.03
 },
 {
  "input": "All of that is a bit theoretical, so it might be helpful to bring things back down to earth and turn back to the concrete example that I mentioned at the start, where you imagine rolling a die 100 times, and let's assume it's a fair die for this example, and you add together the results.",
  "translatedText": "Dat is allemaal een beetje theoretisch, dus het is misschien handig om de zaken terug te brengen naar de aarde en terug te keren naar het concrete voorbeeld dat ik in het begin noemde, waarbij je je voorstelt dat je 100 keer met een dobbelsteen gooit, en laten we aannemen dat het voor dit voorbeeld een eerlijke dobbelsteen is, en je telt de resultaten bij elkaar op.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1504.55,
  "end": 1518.13
 },
 {
  "input": "The challenge for you is to find a range of values such that you're 95% sure that the sum will fall within this range.",
  "translatedText": "De uitdaging voor jou is om een reeks waarden te vinden zodat je 95% zeker weet dat de som binnen deze reeks zal vallen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1518.87,
  "end": 1525.83
 },
 {
  "input": "For questions like this, there's a handy rule of thumb about normal distributions, which is that about 68% of your values are going to fall within two standard deviations of the mean, and a whopping 99.7% of your values will fall within three standard deviations of the mean.",
  "translatedText": "Voor vragen als deze is er een handige vuistregel over normale verdelingen, namelijk dat ongeveer 68% van je waarden binnen twee standaardafwijkingen van het gemiddelde vallen, en maar liefst 99,7% van je waarden binnen drie standaardafwijkingen van het gemiddelde.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1527.13,
  "end": 1546.97
 },
 {
  "input": "It's a rule of thumb that's commonly memorized by people who do a lot of probability and stats.",
  "translatedText": "Het is een vuistregel die vaak wordt onthouden door mensen die veel met waarschijnlijkheid en statistieken bezig zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1547.45,
  "end": 1551.45
 },
 {
  "input": "Naturally, this gives us what we need for our example, and let me go ahead and draw out what this would look like, where I'll show the distribution for a fair die up at the top, and the distribution for a sum of 100 such dice on bottom, which by now looks like a normal distribution.",
  "translatedText": "Dit geeft ons natuurlijk wat we nodig hebben voor ons voorbeeld, en laat ik eens uittekenen hoe dit eruit zou zien, waarbij ik de verdeling voor een eerlijke dobbelsteen bovenaan laat zien, en de verdeling voor een som van 100 van zulke dobbelstenen onderaan, wat er nu uitziet als een normale verdeling.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1552.49,
  "end": 1567.29
 },
 {
  "input": "Step 1 with a problem like this is to find the mean of your initial distribution, which in this case will look like 1 6th times 1 plus 1 6th times 2 on and on and on, and works out to be 3.5.",
  "translatedText": "Stap 1 bij een probleem als dit is het vinden van het gemiddelde van je initiële verdeling, die er in dit geval uitziet als 1 6e maal 1 plus 1 6e maal 2 enzovoort, en die 3,5 is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1567.95,
  "end": 1578.91
 },
 {
  "input": "We also need the standard deviation, which requires calculating the variance, which as you know involves adding all the squares of the differences between the values and the means, and it works out to be 2.92, the square root of 1.71.",
  "translatedText": "We hebben ook de standaardafwijking nodig, waarvoor we de variantie moeten berekenen, wat zoals je weet inhoudt dat we alle kwadraten van de verschillen tussen de waarden en de gemiddelden bij elkaar moeten optellen, en dat wordt 2,92, de vierkantswortel van 1,71.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1579.41,
  "end": 1592.43
 },
 {
  "input": "Those are the only two numbers we need, and I will invite you again to reflect on how magical it is that those are the only two numbers you need to completely understand the bottom distribution.",
  "translatedText": "Dat zijn de enige twee getallen die we nodig hebben, en ik nodig je nogmaals uit om na te denken over hoe magisch het is dat dit de enige twee getallen zijn die je nodig hebt om de onderste verdeling volledig te begrijpen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.95,
  "end": 1601.69
 },
 {
  "input": "Its mean will be 100 times mu, which is 350, and its standard deviation will be the square root of 100 times sigma, so 10 times sigma, 17.1.",
  "translatedText": "Het gemiddelde zal 100 keer mu zijn, wat 350 is, en de standaardafwijking zal de vierkantswortel van 100 keer sigma zijn, dus 10 keer sigma, 17,1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1602.43,
  "end": 1612.61
 },
 {
  "input": "Remembering our handy rule of thumb, we're looking for values two standard deviations away from the mean, and when you subtract 2 sigma from mean, you end up with about 316, and when you add 2 sigma you end up with 384.",
  "translatedText": "Onze handige vuistregel indachtig, zoeken we naar waarden die twee standaarddeviaties van het gemiddelde af liggen, en als je 2 sigma van het gemiddelde aftrekt, kom je uit op ongeveer 316, en als je 2 sigma optelt kom je uit op 384.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1613.03,
  "end": 1626.33
 },
 {
  "input": "There you go, that gives us the answer.",
  "translatedText": "Ziezo, dat geeft ons het antwoord.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1627.35,
  "end": 1628.95
 },
 {
  "input": "Okay, I promised to wrap things up shortly, but while we're on this example, there's one more question that's worth your time to ponder.",
  "translatedText": "Oké, ik heb beloofd om binnenkort af te ronden, maar nu we toch bezig zijn met dit voorbeeld, is er nog één vraag die de moeite waard is om over na te denken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1631.47,
  "end": 1637.45
 },
 {
  "input": "Instead of just asking about the sum of 100 die rolls, let's say I had you divide that number by 100, which basically means all the numbers in our diagram in the bottom get divided by 100.",
  "translatedText": "In plaats van alleen te vragen naar de som van 100 worpen met dobbelstenen, zou ik je vragen om dat getal door 100 te delen, wat in feite betekent dat alle getallen in ons diagram onderaan door 100 worden gedeeld.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1638.25,
  "end": 1648.09
 },
 {
  "input": "Take a moment to interpret what this all would be saying then.",
  "translatedText": "Neem even de tijd om te interpreteren wat dit allemaal zou betekenen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1648.57,
  "end": 1651.57
 },
 {
  "input": "The expression essentially tells you the empirical average for 100 different die rolls, and that interval we found is now telling you what range you are expecting to see for that empirical average.",
  "translatedText": "De uitdrukking vertelt je in wezen het empirische gemiddelde voor 100 verschillende dobbelsteenworpen, en het interval dat we vonden vertelt je nu welk bereik je verwacht te zien voor dat empirische gemiddelde.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1652.07,
  "end": 1663.49
 },
 {
  "input": "In other words, you might expect it to be around 3.5, that's the expected value for a die roll, but what's much less obvious and what the central limit theorem lets you compute is how close to that expected value you'll reasonably find yourself.",
  "translatedText": "Met andere woorden, je zou kunnen verwachten dat het rond de 3,5 ligt, dat is de verwachte waarde voor een dobbelsteenworp, maar wat veel minder voor de hand ligt en wat je met het centrale limiettheorema kunt berekenen, is hoe dicht je redelijkerwijs bij die verwachte waarde zult zitten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1664.35,
  "end": 1676.57
 },
 {
  "input": "In particular, it's worth your time to take a moment mulling over what the standard deviation for this empirical average is, and what happens to it as you look at a bigger and bigger sample of die rolls.",
  "translatedText": "Het is vooral de moeite waard om even na te denken over wat de standaardafwijking voor dit empirische gemiddelde is en wat ermee gebeurt als je naar een steeds grotere steekproef van dobbelsteenworpen kijkt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1677.59,
  "end": 1687.13
 },
 {
  "input": "Lastly, but probably most importantly, let's talk about the assumptions that go into this theorem.",
  "translatedText": "Laten we het tot slot, maar waarschijnlijk het belangrijkste, hebben over de aannames die bij deze stelling horen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1692.95,
  "end": 1697.41
 },
 {
  "input": "The first one is that all of these variables that we're adding up are independent from each other.",
  "translatedText": "De eerste is dat al deze variabelen die we optellen onafhankelijk van elkaar zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1698.01,
  "end": 1702.53
 },
 {
  "input": "The outcome of one process doesn't influence the outcome of any other process.",
  "translatedText": "De uitkomst van het ene proces heeft geen invloed op de uitkomst van een ander proces.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1702.85,
  "end": 1706.31
 },
 {
  "input": "The second is that all of these variables are drawn from the same distribution.",
  "translatedText": "De tweede is dat al deze variabelen uit dezelfde verdeling komen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1707.25,
  "end": 1710.95
 },
 {
  "input": "Both of these have been implicitly assumed with our dice example.",
  "translatedText": "Beide zijn impliciet aangenomen met ons dobbelsteenvoorbeeld.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1711.31,
  "end": 1714.39
 },
 {
  "input": "We've been treating the outcome of each die roll as independent from the outcome of all the others, and we're assuming that each die follows the same distribution.",
  "translatedText": "We hebben de uitkomst van elke dobbelsteenworp behandeld als onafhankelijk van de uitkomst van alle andere en we nemen aan dat elke dobbelsteen dezelfde verdeling heeft.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1714.79,
  "end": 1722.03
 },
 {
  "input": "Sometimes in the literature you'll see these two assumptions lumped together under the initials IID for independent and identically distributed.",
  "translatedText": "Soms zie je in de literatuur deze twee aannames samengevoegd onder de initialen IID voor onafhankelijk en identiek verdeeld.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1722.85,
  "end": 1729.91
 },
 {
  "input": "One situation where these assumptions are decidedly not true would be the Galton board.",
  "translatedText": "Een situatie waarin deze aannames beslist niet waar zijn, is het Galton-bestuur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1730.53,
  "end": 1735.11
 },
 {
  "input": "I mean, think about it.",
  "translatedText": "Ik bedoel, denk er eens over na.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1735.71,
  "end": 1736.83
 },
 {
  "input": "Is it the case that the way a ball bounces off of one of the pegs is independent from how it's going to bounce off the next peg?",
  "translatedText": "Is het zo dat de manier waarop een bal stuitert van een van de haringen onafhankelijk is van hoe hij zal stuiteren van de volgende pin?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1736.97,
  "end": 1743.19
 },
 {
  "input": "Absolutely not.",
  "translatedText": "Absoluut niet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1743.83,
  "end": 1744.61
 },
 {
  "input": "Depending on the last bounce, it's coming in with a completely different trajectory.",
  "translatedText": "Afhankelijk van de laatste stuiter, komt hij binnen met een heel ander traject.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1744.77,
  "end": 1747.87
 },
 {
  "input": "And is it the case that the distribution of possible outcomes off of each peg are the same for each peg that it hits?",
  "translatedText": "En is het zo dat de verdeling van mogelijke uitkomsten van elke pin hetzelfde is voor elke pin die hij raakt?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1748.21,
  "end": 1754.67
 },
 {
  "input": "Again, almost certainly not.",
  "translatedText": "Nogmaals, bijna zeker van niet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1755.19,
  "end": 1756.71
 },
 {
  "input": "Maybe it hits one peg glancing to the left, meaning the outcomes are hugely skewed in that direction, and then hits the next one glancing to the right.",
  "translatedText": "Misschien raakt het een pin die naar links glijdt, wat betekent dat de uitkomsten enorm scheef zijn in die richting, en raakt het dan de volgende die naar rechts glijdt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1756.71,
  "end": 1763.71
 },
 {
  "input": "When I made all those simplifying assumptions in the opening example, it wasn't just to make this easier to think about.",
  "translatedText": "Toen ik al die vereenvoudigende aannames deed in het openingsvoorbeeld, was dat niet alleen om het makkelijker te maken om over na te denken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1765.73,
  "end": 1771.63
 },
 {
  "input": "It's also that those assumptions were necessary for this to actually be an example of the central limit theorem.",
  "translatedText": "Het is ook dat die aannames nodig waren om dit echt een voorbeeld van de stelling van de centrale limiet te laten zijn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1771.97,
  "end": 1777.07
 },
 {
  "input": "Nevertheless, it seems to be true that for the real Galton board, despite violating both of these, a normal distribution does kind of come about?",
  "translatedText": "Toch lijkt het waar te zijn dat voor het echte Galton-bord, ondanks het schenden van beide, toch een soort normale verdeling ontstaat?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1778.13,
  "end": 1785.47
 },
 {
  "input": "Part of the reason might be that there are generalizations of the theorem beyond the scope of this video that relax these assumptions, especially the second one.",
  "translatedText": "Een deel van de reden kan zijn dat er generalisaties van de stelling zijn die buiten het bereik van deze video vallen en die deze aannames versoepelen, vooral de tweede.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1786.05,
  "end": 1793.89
 },
 {
  "input": "But I do want to caution you against the fact that many times people seem to assume that a variable is normally distributed, even when there's no actual justification to do so.",
  "translatedText": "Maar ik wil je wel waarschuwen voor het feit dat mensen vaak lijken aan te nemen dat een variabele normaal verdeeld is, zelfs als daar geen echte rechtvaardiging voor is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1794.49,
  "end": 1803.07
 },
 {
  "input": "The third assumption is actually fairly subtle.",
  "translatedText": "De derde aanname is eigenlijk vrij subtiel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1804.29,
  "end": 1806.21
 },
 {
  "input": "It's that the variance we've been computing for these variables is finite.",
  "translatedText": "Het is dat de variantie die we voor deze variabelen hebben berekend eindig is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1806.21,
  "end": 1810.27
 },
 {
  "input": "This was never an issue for the dice example because there were only six possible outcomes.",
  "translatedText": "Dit was nooit een probleem voor het dobbelsteenvoorbeeld omdat er maar zes mogelijke uitkomsten waren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1810.81,
  "end": 1814.85
 },
 {
  "input": "But in certain situations where you have an infinite set of outcomes, when you go to compute the variance, the sum ends up diverging off to infinity.",
  "translatedText": "Maar in bepaalde situaties waarin je een oneindige reeks uitkomsten hebt, als je de variantie gaat berekenen, divergeert de som naar oneindig.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1815.03,
  "end": 1822.51
 },
 {
  "input": "These can be perfectly valid probability distributions, and they do come up in practice.",
  "translatedText": "Dit kunnen perfect geldige kansverdelingen zijn en ze komen in de praktijk ook voor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1823.45,
  "end": 1827.25
 },
 {
  "input": "But in those situations, as you consider adding many different instantiations of that variable and letting that sum approach infinity, even if the first two assumptions hold, it is very much a possibility that the thing you tend towards is not actually a normal distribution.",
  "translatedText": "Maar in die situaties, als je overweegt om veel verschillende instantiaties van die variabele bij elkaar op te tellen en die som tot in het oneindige te laten naderen, is het, zelfs als de eerste twee aannames gelden, heel goed mogelijk dat datgene waar je naar toe neigt niet echt een normale verdeling is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1827.55,
  "end": 1841.19
 },
 {
  "input": "If you've understood everything up to this point, you now have a very strong foundation in what the central limit theorem is all about.",
  "translatedText": "Als je alles tot nu toe hebt begrepen, dan heb je nu een zeer sterke basis in wat de stelling van de centrale limiet inhoudt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1842.15,
  "end": 1847.65
 },
 {
  "input": "And next up, I'd like to explain why it is that this particular function is the thing that we tend towards, and why it has a pi in it, what it has to do with circles.",
  "translatedText": "En nu wil ik graag uitleggen waarom we juist naar deze functie neigen en waarom er een pi in zit, wat het met cirkels te maken heeft.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1848.29,
  "end": 1855.99
 },
 {
  "input": "Thank you.",
  "translatedText": "Dank je wel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1871.95,
  "end": 1874.17
 }
]