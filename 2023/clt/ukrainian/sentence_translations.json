[
 {
  "input": "This is a Galton board. ",
  "translatedText": "Це дошка Гальтона. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 1.26
 },
 {
  "input": "Maybe you've seen one before, it's a popular demonstration of how, even when a single event is chaotic and random, with an effectively unknowable outcome, it's still possible to make precise statements about a large number of events, namely how the relative proportions for many different outcomes are distributed. ",
  "translatedText": "Можливо, ви бачили таку раніше, це популярна демонстрація того, як, навіть коли одна подія є хаотичною та випадковою, із фактично невідомим результатом, все одно можна зробити точні твердження щодо великої кількості подій, а саме, як відносні пропорції для багатьох різних результатів розподіляються. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2.52,
  "end": 18.3
 },
 {
  "input": "More specifically, the Galton board illustrates one of the most prominent distributions in all of probability, known as the normal distribution, more colloquially known as a bell curve, and also called a Gaussian distribution. ",
  "translatedText": "Точніше, дошка Гальтона ілюструє один із найвидатніших розподілів за всією ймовірністю, відомий як нормальний розподіл, більш розмовно відомий як дзвоноподібна крива, а також називається розподілом Гаусса. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.38,
  "end": 31.9
 },
 {
  "input": "There's a very specific function to describe this distribution, it's very pretty, we'll get into it later, but right now I just want to emphasize how the normal distribution is, as the name suggests, very common, it shows up in a lot of seemingly unrelated contexts. ",
  "translatedText": "Існує дуже специфічна функція для опису цього розподілу, вона дуже гарна, ми поговоримо про це пізніше, але зараз я просто хочу підкреслити, що нормальний розподіл, як випливає з назви, дуже поширений, він проявляється у багатьох здавалося б непов’язаних контекстів. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 32.5,
  "end": 45.04
 },
 {
  "input": "If you were to take a large number of people who sit in a similar demographic and plot their heights, those heights tend to follow a normal distribution. ",
  "translatedText": "Якщо ви візьмете велику кількість людей зі схожою демографічною групою та побудуєте їхній зріст, цей зріст буде мати нормальний розподіл. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.02,
  "end": 53.0
 },
 {
  "input": "If you look at a large swath of very big natural numbers and you ask how many distinct prime factors does each one of those numbers have, the answers will very closely track with a certain normal distribution. ",
  "translatedText": "Якщо ви подивіться на велику групу дуже великих натуральних чисел і запитаєте, скільки різних простих множників має кожне з цих чисел, відповіді будуть дуже точно відповідати певному нормальному розподілу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 53.66,
  "end": 64.96
 },
 {
  "input": "Now our topic for today is one of the crown jewels in all of probability theory, it's one of the key facts that explains why this distribution is as common as it is, known as the central limit theorem. ",
  "translatedText": "Наша сьогоднішня тема — одна з перлин у короні всієї теорії ймовірностей, це один із ключових фактів, який пояснює, чому цей розподіл настільки поширений, як він є, відомий як центральна гранична теорема. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.58,
  "end": 76.02
 },
 {
  "input": "This lesson is meant to go back to the basics, giving you the fundamentals on what the central limit theorem is saying, what normal distributions are, and I want to assume minimal background. ",
  "translatedText": "Цей урок призначений для того, щоб повернутися до основ, дати вам основи того, що говорить центральна гранична теорема, що таке нормальні розподіли, і я хочу припустити мінімальний фон. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.64,
  "end": 85.26
 },
 {
  "input": "We're going to go decently deep into it, but after this I'd still like to go deeper and explain why the theorem is true, why the function underlying the normal distribution has the very specific form that it does, why that formula has a pi in it, and, most fun, why those last two facts are actually more related than a lot of traditional explanations would suggest. ",
  "translatedText": "Ми збираємося заглибитися в це досить глибоко, але після цього я все одно хотів би заглибитися і пояснити, чому теорема вірна, чому функція, що лежить в основі нормального розподілу, має дуже специфічну форму, чому ця формула має Пі в цьому, і, що найцікавіше, чому ці два останніх факти насправді пов’язані більше, ніж багато традиційних пояснень. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.26,
  "end": 105.56
 },
 {
  "input": "That second lesson is also meant to be the follow-on to the convolutions video that I promised, so there's a lot of interrelated topics here. ",
  "translatedText": "Цей другий урок також має бути продовженням відео про згортки, яке я обіцяв, тому тут є багато взаємопов’язаних тем. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.48,
  "end": 113.37
 },
 {
  "input": "But right now, back to the fundamentals, I'd like to kick things off with a overly simplified model of the Galton board. ",
  "translatedText": "Але прямо зараз, повертаючись до основ, я хотів би почати з надто спрощеної моделі дошки Гальтона. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.57,
  "end": 119.17
 },
 {
  "input": "In this model we will assume that each ball falls directly onto a certain central peg and that it has a 50-50 probability of bouncing to the left or to the right, and we'll think of each of those outcomes as either adding one or subtracting one from its position. ",
  "translatedText": "У цій моделі ми припустимо, що кожна кулька падає прямо на певний центральний кілочок і має ймовірність 50-50 відскочити ліворуч або праворуч, і ми будемо розглядати кожен із цих результатів як додавання одного або віднявши одиницю від її положення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.89,
  "end": 134.11
 },
 {
  "input": "Once one of those is chosen, we make the highly unrealistic assumption that it happens to land dead on in the middle of the peg adjacent below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the right. ",
  "translatedText": "Як тільки один із них вибрано, ми робимо вкрай нереалістичне припущення, що він приземляється мертвим на середину кілочка під ним, де він знову зіткнеться з тим самим вибором 50-50: відскочити ліворуч або направо. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.67,
  "end": 147.07
 },
 {
  "input": "For the one I'm showing on screen, there are five different rows of pegs, so our little hopping ball makes five different random choices between plus one and minus one, and we can think of its final position as basically being the sum of all of those different numbers, which in this case happens to be one, and we might label all of the different buckets with the sum that they represent. ",
  "translatedText": "Для тієї, яку я показую на екрані, є п’ять різних рядів кілочків, тож наш маленький м’яч, що стрибає, робить п’ять різних випадкових виборів між плюс один і мінус один, і ми можемо вважати його кінцеве положення фактично сумою всіх цих різних чисел, які в даному випадку дорівнюють одному, і ми можемо позначити всі різні сегменти сумою, яку вони представляють. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.43,
  "end": 166.35
 },
 {
  "input": "As we repeat this, we're looking at different possible sums for those five random numbers. ",
  "translatedText": "Коли ми повторюємо це, ми розглядаємо різні можливі суми для цих п’яти випадкових чисел. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 166.35,
  "end": 171.29
 },
 {
  "input": "And for those of you who are inclined to complain that this is a highly unrealistic model for the true Galton board, let me emphasize the goal right now is not to accurately model physics. ",
  "translatedText": "І для тих із вас, хто схильний скаржитися, що це вкрай нереалістична модель для справжньої дошки Гальтона, дозвольте мені підкреслити, що мета зараз не полягає в точному моделюванні фізики. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.05,
  "end": 181.67
 },
 {
  "input": "The goal is to give a simple example to illustrate the central limit theorem, and for that, idealized though this might be, it actually gives us a really good example. ",
  "translatedText": "Мета полягає в тому, щоб навести простий приклад, щоб проілюструвати центральну граничну теорему, і для цього, хоча це ідеалізовано, це насправді дає нам дійсно хороший приклад. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 181.83,
  "end": 190.03
 },
 {
  "input": "If we let many different balls fall, making yet another unrealistic assumption that they don't influence each other as if they're all ghosts, then the number of balls that fall into each different bucket gives us some loose sense for how likely each one of those buckets is. ",
  "translatedText": "Якщо ми дозволимо впасти багатьом різним кулькам, зробивши ще одне нереалістичне припущення, що вони не впливають одна на одну, ніби всі вони привиди, тоді кількість кульок, які потрапляють у кожне відро, дає нам певне уявлення про ймовірність кожного з них тих відер є. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.57,
  "end": 203.39
 },
 {
  "input": "In this example, the numbers are simple enough that it's not too hard to explicitly calculate what the probability is for falling into each bucket. ",
  "translatedText": "У цьому прикладі цифри досить прості, тому не надто важко чітко обчислити ймовірність потрапляння в кожне відро. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.83,
  "end": 210.01
 },
 {
  "input": "If you do want to think that through, you'll find it very reminiscent of Pascal's triangle. ",
  "translatedText": "Якщо ви все-таки захочете продумати це, ви побачите, що це дуже нагадує трикутник Паскаля. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.27,
  "end": 213.83
 },
 {
  "input": "But the neat thing about our theorem is how far it goes beyond the simple examples. ",
  "translatedText": "Але найцікавіше в нашій теоремі те, наскільки вона виходить за межі простих прикладів. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.95,
  "end": 218.27
 },
 {
  "input": "So to start off at least, rather than making explicit calculations, let's just simulate things by running a large number of samples and letting the total number of results in each different outcome give us some sense for what that distribution looks like. ",
  "translatedText": "Тому, принаймні, для початку, замість того, щоб робити явні обчислення, давайте просто змоделюємо речі, запустивши велику кількість зразків і дозволивши загальній кількості результатів у кожному окремому результаті дати нам певне розуміння того, як виглядає цей розподіл. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.67,
  "end": 229.97
 },
 {
  "input": "As I said, the one on screen has five rows, so each sum that we're considering includes only five numbers. ",
  "translatedText": "Як я вже сказав, на екрані є п’ять рядків, тому кожна сума, яку ми розглядаємо, містить лише п’ять чисел. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 230.45,
  "end": 236.21
 },
 {
  "input": "The basic idea of the central limit theorem is that if you increase the size of that sum, for example here that would mean increasing the number of rows of pegs for each ball to bounce off, then the distribution that describes where that sum is going to fall looks more and more like a bell curve. ",
  "translatedText": "Основна ідея центральної граничної теореми полягає в тому, що якщо ви збільшуєте розмір цієї суми, наприклад, тут це означатиме збільшення кількості кілочків для кожного м’яча, який відскакує, тоді розподіл, який описує, куди ця сума спрямовується осінь все більше нагадує криву дзвоника. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.81,
  "end": 253.33
 },
 {
  "input": "Here, it's actually worth taking a moment to write down that general idea. ",
  "translatedText": "Тут насправді варто витратити час, щоб записати цю загальну ідею. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.47,
  "end": 258.35
 },
 {
  "input": "The setup is that we have a random variable, and that's basically shorthand for a random process where each outcome of that process is associated with some number. ",
  "translatedText": "Налаштування полягає в тому, що у нас є випадкова змінна, і це, по суті, скорочення випадкового процесу, де кожен результат цього процесу пов’язаний з деяким числом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 259.27,
  "end": 268.19
 },
 {
  "input": "We'll call that random number x. ",
  "translatedText": "Ми назвемо це випадкове число x. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 268.49,
  "end": 269.97
 },
 {
  "input": "For example, each bounce off the peg is a random process modeled with two outcomes. ",
  "translatedText": "Наприклад, кожен відскок від кілочка є випадковим процесом, змодельованим із двома результатами. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.97,
  "end": 274.39
 },
 {
  "input": "Those outcomes are associated with the numbers negative one and positive one. ",
  "translatedText": "Ці результати пов’язані з числами від’ємний і додатний. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.85,
  "end": 277.89
 },
 {
  "input": "Another example of a random variable would be rolling a die, where you have six different outcomes, each one associated with a number. ",
  "translatedText": "Іншим прикладом випадкової змінної може бути кидання кубика, де ви маєте шість різних результатів, кожен з яких пов’язаний з числом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.53,
  "end": 284.83
 },
 {
  "input": "What we're doing is taking multiple different samples of that variable and adding them all together. ",
  "translatedText": "Ми беремо кілька різних зразків цієї змінної та додаємо їх усі разом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.47,
  "end": 290.41
 },
 {
  "input": "On our Galton board, that looks like letting the ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice and adding up the results. ",
  "translatedText": "На нашій дошці Гальтона це виглядає так, ніби м’яч відскакує від кількох кілочків на своєму шляху вниз, а у випадку з кубиком ви можете уявити, що кидаєте багато різних кубиків і додаєте результати. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.77,
  "end": 300.97
 },
 {
  "input": "The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into different possible values, will look more and more like a bell curve. ",
  "translatedText": "Твердження центральної граничної теореми полягає в тому, що в міру того, як ви дозволяєте розміру цієї суми ставати все більшою і більшою, розподіл цієї суми, наскільки ймовірно, що вона впаде в різні можливі значення, буде виглядати все більше і більше схожим на дзвоноподібну криву. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.43,
  "end": 314.11
 },
 {
  "input": "That's it, that is the general idea. ",
  "translatedText": "Ось і все, така загальна ідея. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.43,
  "end": 317.13
 },
 {
  "input": "Over the course of this lesson, our job is to make that statement more quantitative. ",
  "translatedText": "Протягом цього уроку наша робота полягає в тому, щоб зробити це твердження більш кількісним. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 317.55,
  "end": 321.53
 },
 {
  "input": "We're going to put some numbers to it, put some formulas to it, show how you can use it to make predictions. ",
  "translatedText": "Ми наведемо кілька цифр, формул, покажемо, як це можна використовувати для прогнозування. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.07,
  "end": 326.35
 },
 {
  "input": "For example, here's the kind of question I want you to be able to answer by the end of this video. ",
  "translatedText": "Ось, наприклад, запитання, на яке я хочу, щоб ви змогли відповісти до кінця цього відео. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.21,
  "end": 331.57
 },
 {
  "input": "Suppose you rolled the die 100 times and you added together the results. ",
  "translatedText": "Припустімо, ви кинули кубик 100 разів і підсумували результати. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.19,
  "end": 335.89
 },
 {
  "input": "Could you find a range of values such that you're 95% sure that the sum will fall within that range? ",
  "translatedText": "Чи можете ви знайти такий діапазон значень, щоб ви були на 95% впевнені, що сума потраплятиме в цей діапазон? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.63,
  "end": 342.17
 },
 {
  "input": "Or maybe I should say find the smallest possible range of values such that this is true. ",
  "translatedText": "Або, можливо, я повинен сказати знайти найменший можливий діапазон значень, щоб це було правдою. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.83,
  "end": 346.55
 },
 {
  "input": "The neat thing is you'll be able to answer this question whether it's a fair die or if it's a weighted die. ",
  "translatedText": "Цікаво те, що ви зможете відповісти на це запитання, незалежно від того, чи це справедливий кубик, чи це зважений кубик. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 347.39,
  "end": 352.13
 },
 {
  "input": "Now let me say at the top that this theorem has three different assumptions that go into it, three things that have to be true before the theorem follows. ",
  "translatedText": "Тепер дозвольте мені сказати на початку, що ця теорема містить три різні припущення, три речі, які мають бути істинними, перш ніж теорема випливає з неї. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.45,
  "end": 360.13
 },
 {
  "input": "And I'm actually not going to tell you what they are until the very end of the video. ",
  "translatedText": "І я не збираюся розповідати вам, що це таке, до самого кінця відео. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 360.43,
  "end": 363.79
 },
 {
  "input": "Instead I want you to keep your eye out and see if you can notice and maybe predict what those three assumptions are going to be. ",
  "translatedText": "Натомість я хочу, щоб ви уважно стежили за тим, чи можете ви помітити та, можливо, передбачити, якими будуть ці три припущення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 364.27,
  "end": 369.67
 },
 {
  "input": "As a next step, to better illustrate just how general this theorem is, I want to run a couple more simulations for you focused on the dice example. ",
  "translatedText": "Як наступний крок, щоб краще проілюструвати, наскільки загальною є ця теорема, я хочу запустити для вас ще пару симуляцій, зосереджених на прикладі кубиків. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.71,
  "end": 377.39
 },
 {
  "input": "Usually if you think of rolling a die you think of the six outcomes as being equally probable, but the theorem actually doesn't care about that. ",
  "translatedText": "Зазвичай, якщо ви думаєте про кидання кубика, ви вважаєте, що шість результатів є однаково вірогідними, але теорему насправді це не хвилює. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 380.91,
  "end": 387.63
 },
 {
  "input": "We could start with a weighted die, something with a non-trivial distribution across the outcomes, and the core idea still holds. ",
  "translatedText": "Ми могли б почати зі зваженого кубика, чогось із нетривіальним розподілом результатів, і основна ідея все ще актуальна. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 387.83,
  "end": 394.55
 },
 {
  "input": "For the simulation what I'll do is take some distribution like this one that is skewed towards lower values. ",
  "translatedText": "Для симуляції я візьму якийсь розподіл, як цей, який має перекіс у бік нижчих значень. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.03,
  "end": 399.93
 },
 {
  "input": "I'm going to take 10 distinct samples from that distribution and then I'll record the sum of that sample on the plot on the bottom. ",
  "translatedText": "Я візьму 10 окремих зразків із цього розподілу, а потім запишу суму цього зразка на графіку внизу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 400.25,
  "end": 407.55
 },
 {
  "input": "Then I'm going to do this many many different times, always with a sum of size 10, but keep track of where those sums ended up to give us a sense of the distribution. ",
  "translatedText": "Потім я збираюся зробити це багато разів, завжди з сумою розміром 10, але слідкувати, де ці суми закінчилися, щоб дати нам відчуття розподілу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 408.63,
  "end": 416.59
 },
 {
  "input": "And in fact let me rescale the y direction to give us room to run an even larger number of samples. ",
  "translatedText": "І фактично дозвольте мені змінити масштаб напрямку y, щоб дати нам місце для запуску ще більшої кількості зразків. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.97,
  "end": 424.73
 },
 {
  "input": "And I'll let it go all the way up to a couple thousand, and as it does you'll notice that the shape that starts to emerge looks like a bell curve. ",
  "translatedText": "І я дозволю йому піднятися до пари тисяч, і коли це станеться, ви помітите, що форма, яка починає з’являтися, виглядає як дзвоноподібна крива. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.03,
  "end": 432.49
 },
 {
  "input": "Maybe if you squint your eyes you can see it skews a tiny bit to the left, but it's neat that something so symmetric emerged from a starting point that was so asymmetric. ",
  "translatedText": "Можливо, якщо ви примружите очі, ви побачите, що він трохи зміщується вліво, але це добре, що щось настільки симетричне виникло з початкової точки, яка була такою асиметричною. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.87,
  "end": 441.01
 },
 {
  "input": "To better illustrate what the central limit theorem is all about, let me run four of these simulations in parallel, where on the upper left I'm doing it where we're only adding two dice at a time, on the upper right we're doing it where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at a time, and then we'll do another one with a bigger sum, 15 at a time. ",
  "translatedText": "Щоб краще проілюструвати суть центральної граничної теореми, дозвольте мені запустити чотири з цих симуляцій паралельно, де у верхньому лівому куті я роблю це, коли ми додаємо лише два кубики за раз, у верхньому правому куті ми Коли ми робимо це, коли ми додаємо п’ять кубиків за раз, нижній ліворуч це той, який ми щойно бачили, як додаємо 10 кубиків за раз, а потім ми зробимо ще один із більшою сумою, 15 за раз. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 441.47,
  "end": 461.37
 },
 {
  "input": "Notice how on the upper left when we're just adding two dice, the resulting distribution doesn't really look like a bell curve, it looks a lot more reminiscent of the one we started with skewed towards the left. ",
  "translatedText": "Зверніть увагу, як у верхньому лівому куті, коли ми просто додаємо два кубики, отриманий розподіл насправді не виглядає як дзвоноподібна крива, вона набагато більше нагадує криву, яку ми почали зі зміщенням вліво. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.25,
  "end": 472.03
 },
 {
  "input": "But as we allow for more and more dice in each sum, the resulting shape that comes up in these distributions looks more and more symmetric. ",
  "translatedText": "Але оскільки ми враховуємо все більше кубиків у кожній сумі, результуюча форма, яка виникає в цих розподілах, виглядає все більш і більш симетричною. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.81,
  "end": 479.81
 },
 {
  "input": "It has the lump in the middle and fade towards the tail's shape of a bell curve. ",
  "translatedText": "Він має грудку в середині та зникає до форми хвоста у формі дзвона. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.95,
  "end": 483.89
 },
 {
  "input": "And let me emphasize again, you can start with any different distribution. ",
  "translatedText": "І дозвольте мені ще раз підкреслити, ви можете почати з будь-якого іншого розподілу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 487.05,
  "end": 490.49
 },
 {
  "input": "Here I'll run it again, but where most of the probability is tied up in the numbers 1 and 6, with very low probability for the mid values. ",
  "translatedText": "Тут я проведу це ще раз, але де більша частина ймовірності пов’язана з числами 1 і 6, з дуже низькою ймовірністю для середніх значень. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.49,
  "end": 497.49
 },
 {
  "input": "Despite completely changing the distribution for an individual roll of the die, it's still the case that a bell curve shape will emerge as we consider the different sums. ",
  "translatedText": "Незважаючи на повну зміну розподілу для окремого кидка кубика, під час розгляду різних сум все одно з’являється дзвоноподібна крива. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.19,
  "end": 506.55
 },
 {
  "input": "Illustrating things with a simulation like this is very fun, and it's kind of neat to see order emerge from chaos, but it also feels a little imprecise. ",
  "translatedText": "Ілюструвати речі за допомогою симуляції, як це, дуже весело, і це начебто охайно спостерігати, як порядок виникає з хаосу, але це також виглядає трохи неточно. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.27,
  "end": 515.03
 },
 {
  "input": "Like in this case, when I cut off the simulation at 3000 samples, even though it kind of looks like a bell curve, the different buckets seem pretty spiky. ",
  "translatedText": "Як у цьому випадку, коли я обриваю симуляцію на 3000 зразках, навіть якщо це виглядає як дзвоноподібна крива, різні відра здаються досить гострими. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 515.39,
  "end": 522.99
 },
 {
  "input": "And you might wonder, is it supposed to look that way, or is that just an artifact of the randomness in the simulation? ",
  "translatedText": "І ви можете запитати, чи це має виглядати саме так, чи це просто артефакт випадковості в симуляції? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 522.99,
  "end": 528.55
 },
 {
  "input": "And if it is, how many samples do we need before we can be sure that what we're looking at is representative of the true distribution? ",
  "translatedText": "І якщо так, то скільки зразків нам потрібно, щоб ми могли бути впевнені, що те, що ми дивимося, є репрезентативним для справжнього розподілу? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 529.01,
  "end": 535.11
 },
 {
  "input": "Instead moving forward, let's get a little more theoretical and show the precise shape that these distributions will take on in the long run. ",
  "translatedText": "Замість того, щоб рухатися вперед, давайте трохи більше теоретично і покажемо точну форму, яку ці розподіли набудуть у довгостроковій перспективі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.19,
  "end": 545.47
 },
 {
  "input": "The easiest case to make this calculation is if we have a uniform distribution, where each possible face of the die has an equal probability, 1 6th. ",
  "translatedText": "Найпростіше зробити цей розрахунок, якщо ми маємо рівномірний розподіл, де кожна можлива грань кубика має однакову ймовірність, 1 6-ту. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.13,
  "end": 553.97
 },
 {
  "input": "For example, if you then want to know how likely different sums are for a pair of dice, it's essentially a counting game, where you count up how many distinct pairs take on the same sum, which in the diagram I've drawn, you can conveniently think about by going through all of the different diagonals. ",
  "translatedText": "Наприклад, якщо ви потім хочете знати, наскільки ймовірні різні суми для пари кубиків, це, по суті, гра в підрахунок, де ви підраховуєте, скільки різних пар отримують ту саму суму, що на діаграмі, яку я намалював, ви можна зручно подумати, перебираючи всі різні діагоналі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.99,
  "end": 568.49
 },
 {
  "input": "Since each such pair has an equal chance of showing up, 1 in 36, all you have to do is count the sizes of these buckets. ",
  "translatedText": "Оскільки кожна така пара має рівні шанси з’явитися, 1 з 36, все, що вам потрібно зробити, це порахувати розміри цих відер. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.41,
  "end": 577.53
 },
 {
  "input": "That gives us a definitive shape for the distribution describing a sum of two dice, and if we were to play the same game with all possible triplets, the resulting distribution would look like this. ",
  "translatedText": "Це дає нам остаточну форму для розподілу, що описує суму двох кубиків, і якби ми грали в одну гру з усіма можливими трійками, кінцевий розподіл виглядав би так. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.19,
  "end": 588.13
 },
 {
  "input": "Now what's more challenging, but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that single die. ",
  "translatedText": "Тепер що складніше, але набагато цікавіше, це запитати, що станеться, якщо ми матимемо нерівномірний розподіл для цього одного кубика. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 588.69,
  "end": 594.99
 },
 {
  "input": "We actually talked all about this in the last video. ",
  "translatedText": "Власне про все це ми говорили в минулому відео. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.55,
  "end": 597.97
 },
 {
  "input": "You do essentially the same thing, you go through all the distinct pairs of dice which add up to the same value. ",
  "translatedText": "Ви робите, по суті, те саме, ви проходите через усі різні пари кубиків, які в сумі мають однакову вартість. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.45,
  "end": 603.67
 },
 {
  "input": "It's just that instead of counting those pairs, for each pair you multiply the two probabilities of each particular face coming up, and then you add all those together. ",
  "translatedText": "Просто замість того, щоб підраховувати ці пари, для кожної пари ви множите дві ймовірності появи кожного конкретного обличчя, а потім додаєте їх разом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 603.97,
  "end": 612.75
 },
 {
  "input": "The computation that does this for all possible sums has a fancy name, it's called a convolution, but it's essentially just the weighted version of the counting game that anyone who's played with a pair of dice already finds familiar. ",
  "translatedText": "Обчислення, яке робить це для всіх можливих сум, має дивовижну назву, воно називається згорткою, але, по суті, це лише зважена версія гри в підрахунок, яку вже знає кожен, хто грав з парою кубиків. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 613.29,
  "end": 624.47
 },
 {
  "input": "For our purposes in this lesson, I'll have the computer calculate all that, simply display the results for you, and invite you to observe certain patterns, but under the hood, this is what's going on. ",
  "translatedText": "Для наших цілей у цьому уроці я попрошу комп’ютер обчислити все це, просто відобразити вам результати та запропонувати вам спостерігати за певними закономірностями, але під капотом відбувається ось що. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.03,
  "end": 635.33
 },
 {
  "input": "So just to be crystal clear on what's being represented here, if you imagine sampling two different values from that top distribution, the one describing a single die, and adding them together, then the second distribution I'm drawing represents how likely you are to see various different sums. ",
  "translatedText": "Щоб чітко зрозуміти, що тут представлено, якщо ви уявите собі вибірку двох різних значень із цього верхнього розподілу, одного, що описує один кубик, і додавання їх разом, тоді другий розподіл, який я малюю, представляє, наскільки ймовірно ви побачити різні суми. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 636.65,
  "end": 652.23
 },
 {
  "input": "Likewise, if you imagine sampling three distinct values from that top distribution, and adding them together, the next plot represents the probabilities for various different sums in that case. ",
  "translatedText": "Подібним чином, якщо ви уявите вибірку трьох різних значень із цього найвищого розподілу та додасте їх разом, наступний графік представлятиме ймовірності для різних сум у цьому випадку. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 652.89,
  "end": 662.49
 },
 {
  "input": "So if I compute what the distributions for these sums look like for larger and larger sums, well you know what I'm going to say, it looks more and more like a bell curve. ",
  "translatedText": "Отже, якщо я обчислюю, як виглядають розподіли цих сум для все більших і більших сум, ви знаєте, що я збираюся сказати, це все більше і більше виглядає як дзвоноподібна крива. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 663.51,
  "end": 672.39
 },
 {
  "input": "But before we get to that, I want you to make a couple more simple observations. ",
  "translatedText": "Але перш ніж ми перейдемо до цього, я хочу, щоб ви зробили ще пару простих зауважень. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.35,
  "end": 676.45
 },
 {
  "input": "For example, these distributions seem to be wandering to the right, and also they seem to be getting more spread out, and a little bit more flat. ",
  "translatedText": "Наприклад, здається, що ці розподіли блукають праворуч, а також вони стають більш розкиданими та трохи більш плоскими. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.45,
  "end": 684.79
 },
 {
  "input": "You cannot describe the central limit theorem quantitatively without taking into account both of those effects, which in turn requires describing the mean and the standard deviation. ",
  "translatedText": "Ви не можете описати центральну граничну теорему кількісно без урахування обох цих ефектів, що, у свою чергу, вимагає опису середнього значення та стандартного відхилення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.25,
  "end": 693.19
 },
 {
  "input": "Maybe you're already familiar with those, but I want to make minimal assumptions here, and it never hurts to review, so let's quickly go over both of those. ",
  "translatedText": "Можливо, ви вже з ними знайомі, але я хочу зробити тут мінімальні припущення, і це ніколи не завадить переглянути, тож давайте швидко переглянемо обидва. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 693.95,
  "end": 700.61
 },
 {
  "input": "The mean of a distribution, often denoted with the Greek letter mu, is a way of capturing the center of mass for that distribution. ",
  "translatedText": "Середнє значення розподілу, яке часто позначається грецькою літерою mu, є способом захоплення центру мас для цього розподілу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 703.41,
  "end": 710.71
 },
 {
  "input": "It's calculated as the expected value of our random variable, which is a way of saying you go through all of the different possible outcomes, and you multiply the probability of that outcome times the value of the variable. ",
  "translatedText": "Він розраховується як очікуване значення нашої випадкової змінної, що є способом сказати, що ви проходите через усі різні можливі результати, і ви множите ймовірність цього результату на значення змінної. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 711.19,
  "end": 722.85
 },
 {
  "input": "If higher values are more probable, that weighted sum is going to be bigger. ",
  "translatedText": "Якщо вищі значення більш імовірні, ця зважена сума буде більшою. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 723.19,
  "end": 726.41
 },
 {
  "input": "If lower values are more probable, that weighted sum is going to be smaller. ",
  "translatedText": "Якщо нижчі значення більш імовірні, ця зважена сума буде меншою. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.75,
  "end": 729.95
 },
 {
  "input": "A little more interesting is if you want to measure how spread out this distribution is, because there's multiple different ways you might do it. ",
  "translatedText": "Трохи цікавіше, якщо ви хочете виміряти, наскільки розповсюджений цей розподіл, тому що це можна зробити кількома різними способами. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.79,
  "end": 737.13
 },
 {
  "input": "One of them is called the variance. ",
  "translatedText": "Одна з них називається дисперсією. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.53,
  "end": 740.29
 },
 {
  "input": "The idea there is to look at the difference between each possible value and the mean, square that difference, and ask for its expected value. ",
  "translatedText": "Ідея полягає в тому, щоб подивитися на різницю між кожним можливим значенням і середнім, звести цю різницю в квадрат і запитати її очікуване значення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.83,
  "end": 748.27
 },
 {
  "input": "The idea is that whether your value is below or above the mean, when you square that difference, you get a positive number, and the larger the difference, the bigger that number. ",
  "translatedText": "Ідея полягає в тому, що незалежно від того, чи є ваше значення нижче або вище середнього, коли ви зводите цю різницю в квадрат, ви отримуєте позитивне число, і чим більша різниця, тим більше це число. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 748.73,
  "end": 756.65
 },
 {
  "input": "Squaring it like this turns out to make the math much much nicer than if we did something like an absolute value, but the downside is that it's hard to think about this as a distance in our diagram because the units are off. ",
  "translatedText": "Виявляється, що таке зведення в квадрат робить математику набагато кращою, ніж якби ми робили щось на кшталт абсолютного значення, але недоліком є те, що на нашій діаграмі важко подумати про це як про відстань, оскільки одиниці вимірюються. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.37,
  "end": 768.13
 },
 {
  "input": "Kind of like the units here are square units, whereas a distance in our diagram would be a kind of linear unit. ",
  "translatedText": "Схоже на те, що одиницями тут є квадрати, тоді як відстань на нашій діаграмі буде свого роду лінійною одиницею. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.33,
  "end": 773.31
 },
 {
  "input": "So another way to measure spread is what's called the standard deviation, which is the square root of this value. ",
  "translatedText": "Отже, інший спосіб вимірювання розповсюдження – це те, що називається стандартним відхиленням, яке є квадратним коренем із цього значення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 773.71,
  "end": 779.19
 },
 {
  "input": "That can be interpreted much more reasonably as a distance on our diagram, and it's commonly denoted with the Greek letter sigma, so you know m for mean as for standard deviation, but both in Greek. ",
  "translatedText": "Це можна більш розумно інтерпретувати як відстань на нашій діаграмі, і її зазвичай позначають грецькою літерою сигма, тож ви знаєте m як середнє, так і стандартне відхилення, але обидва грецькими. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 779.47,
  "end": 789.65
 },
 {
  "input": "Looking back at our sequence of distributions, let's talk about the mean and standard deviation. ",
  "translatedText": "Озираючись на нашу послідовність розподілів, давайте поговоримо про середнє значення та стандартне відхилення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.87,
  "end": 796.15
 },
 {
  "input": "If we call the mean of the initial distribution mu, which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if I tell you that the mean of the next one is 2 times mu. ",
  "translatedText": "Якщо ми назвемо середнє початкового розподілу mu, яке для зображеного дорівнює 2.24, сподіваюся, це не буде надто дивним, якщо я скажу вам, що середнє значення наступного дорівнює 2 мю. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.63,
  "end": 806.73
 },
 {
  "input": "That is, you roll a pair of dice, you want to know the expected value of the sum, it's two times the expected value for a single die. ",
  "translatedText": "Тобто ви кидаєте пару гральних кубиків і хочете знати очікуване значення суми, воно вдвічі перевищує очікуване значення для одного кубика. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 807.13,
  "end": 812.81
 },
 {
  "input": "Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth. ",
  "translatedText": "Подібним чином очікуване значення для нашої суми розміру 3 дорівнює 3 мю, і так далі, і так далі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 813.85,
  "end": 819.41
 },
 {
  "input": "The mean just marches steadily on to the right, which is why our distributions seem to be drifting off in that direction. ",
  "translatedText": "Середнє просто неухильно рухається вправо, тому наші розподіли, здається, дрейфують у цьому напрямку. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 819.63,
  "end": 824.87
 },
 {
  "input": "A little more challenging, but very important, is to describe how the standard deviation changes. ",
  "translatedText": "Трохи складніше, але дуже важливо, описати, як змінюється стандартне відхилення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.35,
  "end": 829.91
 },
 {
  "input": "The key fact here is that if you have two different random variables, then the variance for the sum of those variables is the same as just adding together the original two variances. ",
  "translatedText": "Ключовим фактом тут є те, що якщо у вас є дві різні випадкові змінні, то дисперсія для суми цих змінних є такою самою, як просто додавання двох вихідних дисперсій. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 830.49,
  "end": 839.37
 },
 {
  "input": "This is one of those facts that you can just compute when you unpack all the definitions. ",
  "translatedText": "Це один із тих фактів, які ви можете просто обчислити, коли розпаковуєте всі визначення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 839.93,
  "end": 843.63
 },
 {
  "input": "There are a couple nice intuitions for why it's true. ",
  "translatedText": "Є кілька приємних інтуїцій, які пояснюють, чому це правда. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 843.63,
  "end": 846.21
 },
 {
  "input": "My tentative plan is to just actually make a series about probability and talk about things like intuitions underlying variance and its cousins there. ",
  "translatedText": "Мій попередній план полягає в тому, щоб просто створити серію про ймовірність і поговорити про такі речі, як інтуїція, що лежить в основі дисперсії, і її двоюрідних братів. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 846.63,
  "end": 853.53
 },
 {
  "input": "But right now, the main thing I want you to highlight is how it's the variance that adds, it's not the standard deviation that adds. ",
  "translatedText": "Але зараз головне, що я хочу, щоб ви висвітлили, це те, що додає дисперсія, а не стандартне відхилення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 854.01,
  "end": 860.15
 },
 {
  "input": "So, critically, if you were to take n different realizations of the same random variable and ask what the sum looks like, the variance of that sum is n times the variance of your original variable, meaning the standard deviation, the square root of all this, is the square root of n times the original standard deviation. ",
  "translatedText": "Отже, критично, якщо ви візьмете n різних реалізацій тієї самої випадкової змінної та запитаєте, як виглядає сума, дисперсія цієї суми в n разів перевищує дисперсію вашої вихідної змінної, тобто стандартне відхилення, квадратний корінь із усіх це квадратний корінь з початкового стандартного відхилення, помноженого на n. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 860.41,
  "end": 878.25
 },
 {
  "input": "For example, back in our sequence of distributions, if we label the standard deviation of our initial one with sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on and so forth. ",
  "translatedText": "Наприклад, у нашій послідовності розподілів, якщо ми позначимо стандартне відхилення нашого початкового розподілу сигмою, тоді наступне стандартне відхилення буде квадратним коренем із 2 помножених на сигму, а після цього воно виглядатиме як квадратний корінь із 3 помножити на сигму і так далі і так далі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 879.29,
  "end": 893.09
 },
 {
  "input": "This, like I said, is very important. ",
  "translatedText": "Це, як я вже сказав, дуже важливо. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.75,
  "end": 895.65
 },
 {
  "input": "It means that even though our distributions are getting spread out, they're not spreading out all that quickly, they only do so in proportion to the square root of the size of the sum. ",
  "translatedText": "Це означає, що навіть незважаючи на те, що наші розподіли розподіляються, вони не розподіляються настільки швидко, вони лише пропорційно квадратному кореню з розміру суми. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.07,
  "end": 904.13
 },
 {
  "input": "As we prepare to make a more quantitative description of the central limit theorem, the core intuition I want you to keep in your head is that we'll basically realign all of these distributions so that their means line up together, and then rescale them so that all of the standard deviations are just going to be equal to 1. ",
  "translatedText": "Готуючись до більш кількісного опису центральної граничної теореми, основна інтуїція, яку я хочу, щоб ви запам’ятали, полягає в тому, що ми фактично перебудуємо всі ці розподіли так, щоб їхні середні вишикувались, а потім перемасштабуємо їх так що всі стандартні відхилення дорівнюватимуть 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 904.71,
  "end": 920.61
 },
 {
  "input": "And when we do that, the shape that results gets closer and closer to a certain universal shape, described with an elegant little function that we'll unpack in just a moment. ",
  "translatedText": "І коли ми це робимо, отримана форма стає все ближчою до певної універсальної форми, описаної маленькою елегантною функцією, яку ми розгорнемо за мить. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 921.29,
  "end": 929.87
 },
 {
  "input": "And let me say one more time, the real magic here is how we could have started with any distribution, describing a single roll of the die, and if we play the same game, considering what the distributions for the many different sums look like, and we realign them so that the means line up, and we rescale them so that the standard deviations are all 1, we still approach that same universal shape, which is kind of mind-boggling. ",
  "translatedText": "І дозвольте мені ще раз сказати, справжня магія полягає в тому, як ми могли почати з будь-якого розподілу, описуючи один кидок кубика, і якщо ми граємо в ту саму гру, розглядаючи, як виглядають розподіли для багатьох різних сум, і ми переставляємо їх так, щоб середні вирівнювалися, і ми масштабуємо їх так, щоб усі стандартні відхилення дорівнювали 1, ми все ще наближаємося до тієї самої універсальної форми, що вражає. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.47,
  "end": 952.95
 },
 {
  "input": "And now, my friends, is probably as good a time as any to finally get into the formula for a normal distribution. ",
  "translatedText": "А тепер, мої друзі, напевно такий самий зручний час, як будь-який інший, щоб нарешті розібратися у формулі нормального розподілу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 954.81,
  "end": 960.85
 },
 {
  "input": "And the way I'd like to do this is to basically peel back all the layers and build it up one piece at a time. ",
  "translatedText": "І те, як я хотів би це зробити, полягає в тому, щоб очистити всі шари і побудувати це по частині. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 961.49,
  "end": 965.93
 },
 {
  "input": "The function e to the x, or anything to the x, describes exponential growth, and if you make that exponent negative, which flips around the graph horizontally, you might think of it as describing exponential decay. ",
  "translatedText": "Функція e до x або будь-яка інша до x описує експоненціальне зростання, і якщо ви зробите цей показник від’ємним, який перевертає графік по горизонталі, ви можете подумати про це як про експоненціальний спад. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.53,
  "end": 977.87
 },
 {
  "input": "To make this decay in both directions, you could do something to make sure the exponent is always negative and growing, like taking the negative absolute value. ",
  "translatedText": "Щоб зробити це розпад в обох напрямках, ви можете зробити щось, щоб переконатися, що експонента завжди буде від’ємною та зростатиме, наприклад взяти від’ємне абсолютне значення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.51,
  "end": 985.43
 },
 {
  "input": "That would give us this kind of awkward sharp point in the middle, but if instead you make that exponent the negative square of x, you get a smoother version of the same thing, which decays in both directions. ",
  "translatedText": "Це дало б нам таку незграбну гостру точку в середині, але якщо натомість ви зробите цей показник від’ємним квадратом х, ви отримаєте більш гладку версію того самого, що спадає в обох напрямках. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.93,
  "end": 995.81
 },
 {
  "input": "This gives us the basic bell curve shape. ",
  "translatedText": "Це дає нам базову дзвоноподібну форму. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 996.33,
  "end": 998.19
 },
 {
  "input": "Now if you throw a constant in front of that x, and you scale that constant up and down, it lets you stretch and squish the graph horizontally, allowing you to describe narrow and wider bell curves. ",
  "translatedText": "Тепер, якщо ви поставите константу перед цим х і масштабуєте цю константу вгору та вниз, це дозволить вам розтягнути та стиснути графік горизонтально, дозволяючи вам описати вузькі та ширші дзвонові криві. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.65,
  "end": 1008.37
 },
 {
  "input": "And a quick thing I'd like to point out here is that based on the rules of exponentiation, as we tweak around that constant c, you could also think about it as simply changing the base of the exponentiation. ",
  "translatedText": "І коротко, що я хотів би зазначити тут, це те, що, виходячи з правил піднесення до степеня, коли ми налаштовуємо цю константу c, ви також можете подумати про це як про звичайну зміну основи піднесення до степеня. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1009.01,
  "end": 1019.75
 },
 {
  "input": "And in that sense, the number e is not really all that special for our formula. ",
  "translatedText": "І в цьому сенсі число e не є чимось особливим для нашої формули. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1020.15,
  "end": 1023.63
 },
 {
  "input": "We could replace it with any other positive constant, and you'll get the same family of curves as we tweak that constant. ",
  "translatedText": "Ми можемо замінити її будь-якою іншою позитивною константою, і ви отримаєте ту саму групу кривих, коли ми налаштуємо цю константу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.05,
  "end": 1030.49
 },
 {
  "input": "Make it a 2, same family of curves. ",
  "translatedText": "Зробіть це 2 однакові групи кривих. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.51,
  "end": 1033.11
 },
 {
  "input": "Make it a 3, same family of curves. ",
  "translatedText": "Зробіть це тією самою сім’єю кривих. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.33,
  "end": 1035.07
 },
 {
  "input": "The reason we use e is that it gives that constant a very readable meaning. ",
  "translatedText": "Причина, чому ми використовуємо e, полягає в тому, що це надає цій константі дуже зрозуміле значення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1035.75,
  "end": 1039.49
 },
 {
  "input": "Or rather, if we reconfigure things a little bit so that the exponent looks like negative one half times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn this into a probability distribution, that constant sigma will be the standard deviation of that distribution. ",
  "translatedText": "Або, точніше, якщо ми трохи переналаштуємо речі так, щоб експонента виглядала як від’ємна половина, помножена на х, поділена на певну константу, яку ми будемо називати сигма-квадратом, тоді як тільки ми перетворимо це на розподіл ймовірностей, ця константа сигма буде бути стандартним відхиленням цього розподілу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.11,
  "end": 1057.21
 },
 {
  "input": "And that's very nice. ",
  "translatedText": "І це дуже приємно. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.81,
  "end": 1058.57
 },
 {
  "input": "But before we can interpret this as a probability distribution, we need the area under the curve to be 1. ",
  "translatedText": "Але перш ніж ми зможемо інтерпретувати це як розподіл ймовірностей, нам потрібно, щоб площа під кривою дорівнювала 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.91,
  "end": 1064.31
 },
 {
  "input": "And the reason for that is how the curve is interpreted. ",
  "translatedText": "І причина цього полягає в тому, як інтерпретується крива. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1064.83,
  "end": 1066.91
 },
 {
  "input": "Unlike discrete distributions, when it comes to something continuous, you don't ask about the probability of a particular point. ",
  "translatedText": "На відміну від дискретних розподілів, коли йдеться про щось безперервне, ви не запитуєте про ймовірність певної точки. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1067.37,
  "end": 1073.37
 },
 {
  "input": "Instead, you ask for the probability that a value falls between two different values. ",
  "translatedText": "Замість цього ви запитуєте ймовірність того, що значення потрапляє між двома різними значеннями. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1073.79,
  "end": 1078.23
 },
 {
  "input": "And what the curve is telling you is that that probability equals the area under the curve between those two values. ",
  "translatedText": "Крива говорить вам, що ймовірність дорівнює площі під кривою між цими двома значеннями. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.75,
  "end": 1085.43
 },
 {
  "input": "There's a whole other video about this, they're called probability density functions. ",
  "translatedText": "Є ще одне відео про це, вони називаються функціями щільності ймовірності. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1086.03,
  "end": 1089.43
 },
 {
  "input": "The main point right now is that the area under the entire curve represents the probability that something happens, that some number comes up. ",
  "translatedText": "Зараз головне, що площа під усією кривою представляє ймовірність того, що щось станеться, що з’явиться якесь число. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1089.83,
  "end": 1097.15
 },
 {
  "input": "That should be 1, which is why we want the area under this to be 1. ",
  "translatedText": "Це має бути 1, тому ми хочемо, щоб площа під цим була 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1097.41,
  "end": 1100.63
 },
 {
  "input": "As it stands with the basic bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root of pi. ",
  "translatedText": "У базовій формі дзвоноподібної кривої e до від’ємного х у квадраті площа дорівнює не 1, це фактично квадратний корінь з пі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1101.05,
  "end": 1107.79
 },
 {
  "input": "I know, right? ",
  "translatedText": "я знаю, правда? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.41,
  "end": 1109.15
 },
 {
  "input": "What is pi doing here? ",
  "translatedText": "Що тут робить пі? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1109.27,
  "end": 1110.19
 },
 {
  "input": "What does this have to do with circles? ",
  "translatedText": "Яке це має відношення до кіл? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1110.29,
  "end": 1111.47
 },
 {
  "input": "Like I said at the start, I'd love to talk all about that in the next video. ",
  "translatedText": "Як я сказав на початку, я хотів би поговорити про це в наступному відео. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.01,
  "end": 1115.05
 },
 {
  "input": "But if you can spare your excitement for our purposes right now, all it means is that we should divide this function by the square root of pi, and it gives us the area we want. ",
  "translatedText": "Але якщо ви можете не хвилюватися для наших цілей прямо зараз, все це означає, що ми повинні розділити цю функцію на квадратний корінь з пі, і це дасть нам потрібну площу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.33,
  "end": 1123.17
 },
 {
  "input": "Throwing back in the constants we had earlier, the 1 half and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square root of 2. ",
  "translatedText": "Повертаючись до констант, які ми мали раніше, 1 половина та сигма, ефект розтягування графіка на коефіцієнт сигма, помножений на квадратний корінь з 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.61,
  "end": 1131.79
 },
 {
  "input": "So we also need to divide out by that in order to make sure it has an area of 1. ",
  "translatedText": "Тож нам також потрібно розділити на це, щоб переконатися, що його площа дорівнює 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.41,
  "end": 1136.47
 },
 {
  "input": "And combining those fractions, the factor out front looks like 1 divided by sigma times the square root of 2 pi. ",
  "translatedText": "І об’єднавши ці дроби, множник виглядає як 1 поділене на сигму, помножене на квадратний корінь з 2 пі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1136.47,
  "end": 1142.11
 },
 {
  "input": "This, finally, is a valid probability distribution. ",
  "translatedText": "Це, нарешті, дійсний розподіл ймовірностей. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1142.91,
  "end": 1145.85
 },
 {
  "input": "As we tweak that value sigma, resulting in narrower and wider curves, that constant in the front always guarantees that the area equals 1. ",
  "translatedText": "Коли ми змінюємо це значення сигми, що призводить до вужчих і ширших кривих, ця константа в передній частині завжди гарантує, що площа дорівнює 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.45,
  "end": 1154.31
 },
 {
  "input": "The special case where sigma equals 1 has a specific name, we call it the standard normal distribution, which plays an especially important role for you and me in this lesson. ",
  "translatedText": "Спеціальний випадок, коли сигма дорівнює 1, має конкретну назву, ми називаємо його стандартним нормальним розподілом, який відіграє особливо важливу роль для нас з вами в цьому уроці. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.91,
  "end": 1164.51
 },
 {
  "input": "And all possible normal distributions are not only parameterized with this value sigma, but we also subtract off another constant mu from the variable x, and this essentially just lets you slide the graph left and right so that you can prescribe the mean of this distribution. ",
  "translatedText": "І всі можливі нормальні розподіли не тільки параметризовані цим значенням сигма, але ми також віднімаємо іншу константу mu від змінної x, і це, по суті, просто дозволяє вам ковзати графіком вліво і вправо, щоб ви могли вказати середнє значення цього розподілу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.13,
  "end": 1180.21
 },
 {
  "input": "So in short, we have two parameters, one describing the mean, one describing the standard deviation, and they're all tied together in this big formula involving an e and a pi. ",
  "translatedText": "Коротше кажучи, ми маємо два параметри, один описує середнє значення, другий описує стандартне відхилення, і всі вони пов’язані разом у цій великій формулі, що включає е та пі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.99,
  "end": 1189.19
 },
 {
  "input": "Now that all of that is on the table, let's look back again at the idea of starting with some random variable and asking what the distributions for sums of that variable look like. ",
  "translatedText": "Тепер, коли все це на столі, давайте знову поглянемо на ідею почати з деякої випадкової змінної та запитати, як виглядають розподіли сум цієї змінної. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.19,
  "end": 1199.81
 },
 {
  "input": "As we've already gone over, when you increase the size of that sum, the resulting distribution will shift according to a growing mean, and it slowly spreads out according to a growing standard deviation. ",
  "translatedText": "Як ми вже згадували, коли ви збільшуєте розмір цієї суми, результуючий розподіл змінюватиметься відповідно до зростаючого середнього, і він повільно поширюється відповідно до зростаючого стандартного відхилення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1200.13,
  "end": 1209.81
 },
 {
  "input": "And putting some actual formulas to it, if we know the mean of our underlying random variable, we call it mu, and we also know its standard deviation, and we call it sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the standard deviation will be sigma times the square root of that size. ",
  "translatedText": "І якщо ми знаємо середнє значення нашої основної випадкової змінної, ми називаємо це mu, і ми також знаємо її стандартне відхилення, і ми називаємо це сигмою, тоді середнє для суми внизу буде mu помножити на розмір суми, а стандартне відхилення буде сигма, помножена на квадратний корінь із цього розміру. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1210.33,
  "end": 1227.73
 },
 {
  "input": "So now, if we want to claim that this looks more and more like a bell curve, and a bell curve is only described by two different parameters, the mean and the standard deviation, you know what to do. ",
  "translatedText": "Отже, якщо ми хочемо стверджувати, що це все більше і більше виглядає як дзвоноподібна крива, а дзвоноподібна крива описується лише двома різними параметрами, середнім і стандартним відхиленням, ви знаєте, що робити. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1228.19,
  "end": 1237.71
 },
 {
  "input": "You could plug those two values into the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve that should closely fit our distribution. ",
  "translatedText": "Ви можете підключити ці два значення до формули, і це дасть вам чітку, хоча й дещо складну, формулу для кривої, яка має точно відповідати нашому розподілу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1237.93,
  "end": 1246.99
 },
 {
  "input": "But there's another way we can describe it that's a little more elegant and lends itself to a very fun visual that we can build up to. ",
  "translatedText": "Але є інший спосіб, яким ми можемо це описати, який є трохи елегантнішим і піддається дуже веселому візуальному ефекту, який ми можемо створити. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1248.39,
  "end": 1254.81
 },
 {
  "input": "Instead of focusing on the sum of all of these random variables, let's modify this expression a little bit, where what we'll do is we'll look at the mean that we expect that sum to take, and we subtract it off so that our new expression has a mean of 0, and then we're going to look at the standard deviation we expect of our sum, and divide out by that, which basically just rescales the units so that the standard deviation of our expression will equal 1. ",
  "translatedText": "Замість того, щоб зосереджуватися на сумі всіх цих випадкових змінних, давайте трохи модифікуємо цей вираз, де ми будемо дивитися на середнє, яке ми очікуємо, що ця сума матиме, і ми віднімаємо його так, що наш новий вираз має середнє значення 0, а потім ми подивимося на стандартне відхилення, яке ми очікуємо від нашої суми, і розділимо на нього, що, по суті, просто перемасштабує одиниці так, щоб стандартне відхилення нашого виразу дорівнювало 1 . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1255.27,
  "end": 1278.77
 },
 {
  "input": "This might seem like a more complicated expression, but it actually has a highly readable meaning. ",
  "translatedText": "Це може здатися більш складним виразом, але насправді воно має добре читабельне значення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1279.35,
  "end": 1284.09
 },
 {
  "input": "It's essentially saying how many standard deviations away from the mean is this sum? ",
  "translatedText": "По суті, це означає, на скільки стандартних відхилень від середнього значення становить ця сума? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1284.45,
  "end": 1289.67
 },
 {
  "input": "For example, this bar here corresponds to a certain value that you might find when you roll 10 dice and you add them all up, and its position a little above negative 1 is telling you that that value is a little bit less than one standard deviation lower than the mean. ",
  "translatedText": "Наприклад, ця смужка тут відповідає певному значенню, яке ви можете знайти, коли ви кидаєте 10 кубиків і додаєте їх усі, а її положення трохи вище мінус 1 говорить вам, що це значення трохи менше, ніж одне стандартне відхилення нижче середнього. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.75,
  "end": 1303.87
 },
 {
  "input": "Also, by the way, in anticipation for the animation I'm trying to build to here, the way I'm representing things on that lower plot is that the area of each one of these bars is telling us the probability of the corresponding value rather than the height. ",
  "translatedText": "Крім того, до речі, в очікуванні анімації, яку я намагаюся створити тут, спосіб представлення речей на цьому нижньому графіку полягає в тому, що площа кожного з цих стовпчиків повідомляє нам про ймовірність відповідного значення а не висота. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1305.13,
  "end": 1316.99
 },
 {
  "input": "You might think of the y-axis as representing not probability but a kind of probability density. ",
  "translatedText": "Можна подумати, що вісь ординат представляє не ймовірність, а свого роду щільність ймовірності. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.23,
  "end": 1321.93
 },
 {
  "input": "The reason for this is to set the stage so that it aligns with the way we interpret continuous distributions, where the probability of falling between a range of values is equal to an area under a curve between those values. ",
  "translatedText": "Причина цього полягає в тому, щоб налаштувати сцену так, щоб вона узгоджувалася з тим, як ми інтерпретуємо неперервні розподіли, де ймовірність потрапляння між діапазоном значень дорівнює площі під кривою між цими значеннями. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1322.27,
  "end": 1333.55
 },
 {
  "input": "In particular, the area of all the bars together is going to be 1. ",
  "translatedText": "Зокрема, площа всіх брусків разом дорівнюватиме 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1333.91,
  "end": 1336.73
 },
 {
  "input": "Now, with all of that in place, let's have a little fun. ",
  "translatedText": "Тепер, маючи все це на місці, давайте трохи повеселимося. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1338.23,
  "end": 1340.95
 },
 {
  "input": "Let me start by rolling things back so that the distribution on the bottom represents a relatively small sum, like adding together only three such random variables. ",
  "translatedText": "Дозвольте мені почати з того, що повернути все назад так, щоб розподіл у нижній частині являв собою відносно невелику суму, як додавання лише трьох таких випадкових змінних. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1341.33,
  "end": 1349.01
 },
 {
  "input": "Notice what happens as I change the distribution we start with. ",
  "translatedText": "Зверніть увагу, що відбувається, коли я змінюю розподіл, з якого ми починаємо. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.45,
  "end": 1352.43
 },
 {
  "input": "As it changes, the distribution on the bottom completely changes its shape. ",
  "translatedText": "Змінюючись, розподіл на дні повністю змінює свою форму. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1352.73,
  "end": 1356.29
 },
 {
  "input": "It's very dependent on what we started with. ",
  "translatedText": "Це дуже залежить від того, з чого ми почали. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1356.51,
  "end": 1358.77
 },
 {
  "input": "If we let the size of our sum get a little bit bigger, say going up to 10, and as I change the distribution for x, it largely stays looking like a bell curve, but I can find some distributions that get it to change shape. ",
  "translatedText": "Якщо ми дозволимо розміру нашої суми стати трохи більшим, скажімо, до 10, і коли я змінюю розподіл для x, вона здебільшого залишиться схожою на дзвонову криву, але я можу знайти деякі розподіли, які змушують її змінювати форму . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.35,
  "end": 1371.63
 },
 {
  "input": "For example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results in this kind of spiky bell curve, and if you'll recall, earlier on I actually showed this in the form of a simulation. ",
  "translatedText": "Наприклад, дійсно однобока, де майже вся ймовірність полягає в числах 1 або 6, призводить до такого роду гостроподібної кривої, і якщо ви пам’ятаєте, раніше я фактично показав це у формі симуляції. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1372.23,
  "end": 1383.51
 },
 {
  "input": "So if you were wondering whether that spikiness was an artifact of the randomness or reflected the true distribution, turns out it reflects the true distribution. ",
  "translatedText": "Отже, якщо вам цікаво, чи була ця різкість артефактом випадковості чи відображає справжній розподіл, виявляється, що вона відображає справжній розподіл. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1384.13,
  "end": 1391.85
 },
 {
  "input": "In this case, 10 is not a large enough sum for the central limit theorem to kick in. ",
  "translatedText": "У цьому випадку 10 не є достатньо великою сумою для центральної граничної теореми. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.29,
  "end": 1396.47
 },
 {
  "input": "But if instead I let that sum grow and I consider adding 50 different values, which is actually not that big, then no matter how I change the distribution for our underlying random variable, it has essentially no effect on the shape of the plot on the bottom. ",
  "translatedText": "Але якщо натомість я дозволю цій сумі зростати і подумаю про додавання 50 різних значень, що насправді не так вже й велике, тоді незалежно від того, як я зміню розподіл для нашої базової випадкової змінної, це практично не вплине на форму графіка на дно. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1396.47,
  "end": 1410.69
 },
 {
  "input": "No matter where we start, all of the information and nuance for the distribution of x gets washed away, and we tend towards this single universal shape described by a very elegant function for the standard normal distribution, 1 over square root of 2 pi times e to the negative x squared over 2. ",
  "translatedText": "Незалежно від того, з чого ми починаємо, уся інформація та нюанси щодо розподілу x змиваються, і ми прагнемо до цієї єдиної універсальної форми, описаної дуже елегантною функцією для стандартного нормального розподілу, 1 із кореня квадратного з 2 pi, помноженого на e до мінус х у квадраті на 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1411.17,
  "end": 1427.07
 },
 {
  "input": "This, this right here is what the central limit theorem is all about. ",
  "translatedText": "Це, ось тут і є суть центральної граничної теореми. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1427.81,
  "end": 1430.81
 },
 {
  "input": "Almost nothing you can do to this initial distribution changes the shape we tend towards. ",
  "translatedText": "Майже нічого, що ви можете зробити з цим початковим розподілом, не змінить форму, до якої ми прагнемо. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1431.13,
  "end": 1435.31
 },
 {
  "input": "Now, the more theoretically minded among you might still be wondering, what is the actual theorem? ",
  "translatedText": "Тепер, більш теоретично налаштовані серед вас, можливо, все ще цікавляться, що таке фактична теорема? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1439.03,
  "end": 1444.51
 },
 {
  "input": "Like, what's the mathematical statement that could be proved or disproved that we're claiming here? ",
  "translatedText": "Наприклад, яке математичне твердження, яке можна було б довести чи спростувати, ми тут стверджуємо? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1444.81,
  "end": 1448.91
 },
 {
  "input": "If you want a nice formal statement, here's how it might go. ",
  "translatedText": "Якщо ви хочете гарну офіційну заяву, ось як це може бути. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1449.03,
  "end": 1451.67
 },
 {
  "input": "Consider this value, where we're summing up n different instantiations of our random variable, but tweaked and tuned so that its mean and standard deviation are 1. ",
  "translatedText": "Розглянемо це значення, де ми підсумовуємо n різних екземплярів нашої випадкової змінної, але налаштовані так, щоб її середнє значення та стандартне відхилення дорівнювали 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1452.13,
  "end": 1459.89
 },
 {
  "input": "Again, meaning you can read it as asking how many standard deviations away from the mean is the sum. ",
  "translatedText": "Знову ж таки, це означає, що ви можете прочитати це як запитання, на скільки стандартних відхилень від середнього є сума. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1460.23,
  "end": 1465.35
 },
 {
  "input": "Then the actual rigorous no-jokes-this-time statement of the central limit theorem is that if you consider the probability that this value falls between two given real numbers, a and b, and you consider the limit of that probability as the size of your sum goes to infinity, then that limit is equal to a certain integral, which basically describes the area under a standard normal distribution between those two values. ",
  "translatedText": "Тоді фактичне суворе твердження центральної граничної теореми, яке не є жартом, полягає в тому, що якщо ви розглядаєте ймовірність того, що це значення потрапляє між двома заданими дійсними числами, a і b, і ви розглядаєте межу цієї ймовірності як розмір ваша сума прямує до нескінченності, тоді ця межа дорівнює певному інтегралу, який в основному описує площу під стандартним нормальним розподілом між цими двома значеннями. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1465.77,
  "end": 1489.65
 },
 {
  "input": "Again, there are three underlying assumptions that I have yet to tell you, but other than those, in all of its gory detail, this right here is the central limit theorem. ",
  "translatedText": "Знову ж таки, є три основні припущення, які я вам ще не сказав, але крім них, у всіх своїх кривавих деталях, це центральна гранична теорема. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1491.25,
  "end": 1500.03
 },
 {
  "input": "All of that is a bit theoretical, so it might be helpful to bring things back down to Earth and turn back to the concrete example that I mentioned at the start, where you imagine rolling a die 100 times, and let's assume it's a fair die for this example, and you add together the results. ",
  "translatedText": "Усе це трохи теоретично, тож може бути корисно повернути речі на Землю та повернутися до конкретного прикладу, який я згадав на початку, де ви уявляєте, як кидаєте кубик 100 разів, і припустімо, що це справедливий кубик. для цього прикладу, і ви додаєте результати. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1504.55,
  "end": 1518.13
 },
 {
  "input": "The challenge for you is to find a range of values such that you're 95% sure that the sum will fall within this range. ",
  "translatedText": "Завдання для вас полягає в тому, щоб знайти такий діапазон значень, щоб ви були на 95% впевнені, що сума потраплятиме в цей діапазон. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1518.87,
  "end": 1525.83
 },
 {
  "input": "For questions like this, there's a handy rule of thumb about normal distributions, which is that about 68% of your values are going to fall within one standard deviation of the mean, 95% of your values, the thing we care about, fall within two standard deviations of the mean, and a whopping 99.7% of your values will fall within three standard deviations of the mean. ",
  "translatedText": "Для подібних запитань є зручне емпіричне правило щодо нормального розподілу, яке полягає в тому, що приблизно 68% ваших значень будуть потрапляти в межах одного стандартного відхилення від середнього, 95% ваших значень, те, що нас хвилює, потрапляють в межах два стандартних відхилення від середнього значення та колосальні 99.7% ваших значень потраплятимуть у межах трьох стандартних відхилень від середнього. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1527.13,
  "end": 1546.97
 },
 {
  "input": "It's a rule of thumb that's commonly memorized by people who do a lot of probability and stats. ",
  "translatedText": "Це емпіричне правило, яке зазвичай запам’ятовують люди, які багато займаються ймовірністю та статистикою. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1547.45,
  "end": 1551.45
 },
 {
  "input": "Naturally, this gives us what we need for our example, and let me go ahead and draw out what this would look like, where I'll show the distribution for a fair die up at the top, and the distribution for a sum of 100 such dice on the bottom, which by now as you know looks like a certain normal distribution. ",
  "translatedText": "Природно, це дає нам те, що нам потрібно для нашого прикладу, і дозвольте мені намалювати, як це виглядатиме, де я покажу розподіл для справедливого кубика вгорі та розподіл для суми 100 такі кубики внизу, які на даний момент, як ви знаєте, виглядають як певний нормальний розподіл. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.49,
  "end": 1567.29
 },
 {
  "input": "Step one with a problem like this is to find the mean of your initial distribution, which in this case will look like 1 6th times 1 plus 1 6th times 2 on and on and on, and works out to be 3.5. ",
  "translatedText": "Перший крок із подібною проблемою полягає в тому, щоб знайти середнє значення вашого початкового розподілу, яке в цьому випадку виглядатиме як 1 6, помножене на 1 плюс 1 6, помножене на 2 і далі, і далі, і вийде 3.5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1567.95,
  "end": 1578.91
 },
 {
  "input": "We also need the standard deviation, which requires calculating the variance, which as you know involves adding all the squares of the differences between the values and the means, and it works out to be 2.92, square root of that comes out to be 1.71. ",
  "translatedText": "Нам також потрібне стандартне відхилення, яке потребує обчислення дисперсії, яка, як ви знаєте, передбачає додавання всіх квадратів різниць між значеннями та середніми, і в результаті виходить 2.92, квадратний корінь з цього виходить 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1579.41,
  "end": 1592.43
 },
 {
  "input": "Those are the only two numbers we need, and I will invite you again to reflect on how magical it is that those are the only two numbers that you need to completely understand the bottom distribution. ",
  "translatedText": "71. Це єдині два числа, які нам потрібні, і я знову запрошую вас поміркувати про те, наскільки це чарівно, що це єдині два числа, які вам потрібні, щоб повністю зрозуміти нижній розподіл. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1592.95,
  "end": 1601.69
 },
 {
  "input": "Its mean will be 100 times mu, which is 350, and its standard deviation will be the square root of 100 times sigma, so 10 times sigma 17.1. ",
  "translatedText": "Його середнє значення становитиме 100 мю, що дорівнює 350, а його стандартне відхилення буде квадратним коренем із 100 сигма, тобто 10 сигма 17.1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1602.43,
  "end": 1612.61
 },
 {
  "input": "Remembering our handy rule of thumb, we're looking for values two standard deviations away from the mean, and when you subtract 2 sigma from the mean you end up with about 316, and when you add 2 sigma you end up with 384. ",
  "translatedText": "Пам’ятаючи наше зручне емпіричне правило, ми шукаємо значення на два стандартні відхилення від середнього, і коли ви віднімете 2 сигма від середнього, ви отримаєте приблизно 316, а коли ви додасте 2 сигма, ви отримаєте 384. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1613.03,
  "end": 1626.33
 },
 {
  "input": "And there you go, that gives us the answer. ",
  "translatedText": "І ось, це дає нам відповідь. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1627.35,
  "end": 1628.95
 },
 {
  "input": "Okay, I promised to wrap things up shortly, but while we're on this example there's one more question that's worth your time to ponder. ",
  "translatedText": "Гаразд, я пообіцяв незабаром завершити все, але поки ми розглядаємо цей приклад, є ще одне запитання, над яким варто поміркувати. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.47,
  "end": 1637.45
 },
 {
  "input": "Instead of just asking about the sum of 100 die rolls, let's say I had you divide that number by 100, which basically means all the numbers in our diagram in the bottom get divided by 100. ",
  "translatedText": "Замість того, щоб просто запитувати про суму 100 кидків кубика, скажімо, я попросив вас розділити це число на 100, що фактично означає, що всі числа на нашій діаграмі внизу діляться на 100. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1638.25,
  "end": 1648.09
 },
 {
  "input": "Take a moment to interpret what this all would be saying then. ",
  "translatedText": "Знайдіть хвилинку, щоб витлумачити, що тоді все це означає. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1648.57,
  "end": 1651.57
 },
 {
  "input": "The expression essentially tells you the empirical average for 100 different die rolls, and that interval we found is now telling you what range you are expecting to see for that empirical average. ",
  "translatedText": "Вираз, по суті, повідомляє вам емпіричне середнє значення для 100 різних кидків кубика, і цей інтервал, який ми знайшли, тепер говорить вам, який діапазон ви очікуєте побачити для цього емпіричного середнього. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1652.07,
  "end": 1663.49
 },
 {
  "input": "In other words, you might expect it to be around 3.5, that's the expected value for a die roll, but what's much less obvious and what the central limit theorem lets you compute is how close to that expected value you'll reasonably find yourself. ",
  "translatedText": "Іншими словами, ви можете очікувати, що це буде близько 3.5, це очікуване значення для кидка кубика, але те, що є набагато менш очевидним і теорема про центральну межу дозволяє вам обчислити, це те, наскільки близько до цього очікуваного значення ви обґрунтовано опинитесь. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1664.35,
  "end": 1676.57
 },
 {
  "input": "In particular, it's worth your time to take a moment mulling over what the standard deviation for this empirical average is, and what happens to it as you look at a bigger and bigger sample of die rolls. ",
  "translatedText": "Зокрема, варто поміркувати над тим, яким є стандартне відхилення для цього емпіричного середнього значення, і що з ним відбувається, коли ви дивитеся на все більшу і більшу вибірку кидків кубиків. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1677.59,
  "end": 1687.13
 },
 {
  "input": "Lastly, but probably most importantly, let's talk about the assumptions that go into this theorem. ",
  "translatedText": "Нарешті, але, мабуть, найважливіше, давайте поговоримо про припущення, які входять до цієї теореми. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.95,
  "end": 1697.41
 },
 {
  "input": "The first one is that all of these variables that we're adding up are independent from each other. ",
  "translatedText": "Перший полягає в тому, що всі ці змінні, які ми складаємо, не залежать одна від одної. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1698.01,
  "end": 1702.53
 },
 {
  "input": "The outcome of one process doesn't influence the outcome of any other process. ",
  "translatedText": "Результат одного процесу не впливає на результат будь-якого іншого процесу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.85,
  "end": 1706.31
 },
 {
  "input": "The second is that all of these variables are drawn from the same distribution. ",
  "translatedText": "По-друге, всі ці змінні беруться з одного розподілу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1707.25,
  "end": 1710.95
 },
 {
  "input": "Both of these have been implicitly assumed with our dice example. ",
  "translatedText": "Обидва вони були неявно припущені в нашому прикладі з кубиками. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1711.31,
  "end": 1714.39
 },
 {
  "input": "We've been treating the outcome of each die roll as independent from the outcome of all the others, and we're assuming that each die follows the same distribution. ",
  "translatedText": "Ми розглядаємо результат кожного кидка кубика як незалежний від результату всіх інших, і ми припускаємо, що кожен кубик має однаковий розподіл. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1714.79,
  "end": 1722.03
 },
 {
  "input": "Sometimes in the literature you'll see these two assumptions lumped together under the initials IID for independent and identically distributed. ",
  "translatedText": "Іноді в літературі ви побачите ці два припущення разом під ініціалами IID для незалежних і однаково розподілених. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1722.85,
  "end": 1729.91
 },
 {
  "input": "One situation where these assumptions are decidedly not true would be the Galton board. ",
  "translatedText": "Однією ситуацією, коли ці припущення явно не відповідають дійсності, була б дошка Гальтона. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1730.53,
  "end": 1735.11
 },
 {
  "input": "I mean, think about it. ",
  "translatedText": "Я маю на увазі, подумайте про це. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.71,
  "end": 1736.83
 },
 {
  "input": "Is it the case that the way a ball bounces off of one of the pegs is independent from how it's going to bounce off the next peg? ",
  "translatedText": "Чи так те, як м’яч відскакує від одного кілочка, не залежить від того, як він відскочить від наступного? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1736.97,
  "end": 1743.19
 },
 {
  "input": "Absolutely not. ",
  "translatedText": "Абсолютно не. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1743.83,
  "end": 1744.61
 },
 {
  "input": "Depending on the last bounce, it's coming in with a completely different trajectory. ",
  "translatedText": "Залежно від останнього відскоку, він надходить із зовсім іншою траєкторією. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1744.77,
  "end": 1747.87
 },
 {
  "input": "And is it the case that the distribution of possible outcomes off of each peg are the same for each peg that it hits? ",
  "translatedText": "І чи справді розподіл можливих результатів для кожного кілочка однаковий для кожного кілочка, який він потрапляє? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1748.21,
  "end": 1754.67
 },
 {
  "input": "Again, almost certainly not. ",
  "translatedText": "Знову ж таки, майже напевно ні. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1755.19,
  "end": 1756.71
 },
 {
  "input": "Maybe it hits one peg glancing to the left, meaning the outcomes are hugely skewed in that direction, and then hits the next one glancing to the right. ",
  "translatedText": "Можливо, він потрапляє на один кілочок, дивлячись ліворуч, тобто результати сильно спотворені в цьому напрямку, а потім потрапляє на наступний кілочок, дивлячись праворуч. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1756.71,
  "end": 1763.71
 },
 {
  "input": "When I made all those simplifying assumptions in the opening example, it wasn't just to make this easier to think about. ",
  "translatedText": "Коли я зробив усі ці спрощуючі припущення у початковому прикладі, це було не лише для того, щоб полегшити обдумування. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1765.73,
  "end": 1771.63
 },
 {
  "input": "It's also that those assumptions were necessary for this to actually be an example of the central limit theorem. ",
  "translatedText": "Крім того, ці припущення були необхідні для того, щоб це фактично було прикладом центральної граничної теореми. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1771.97,
  "end": 1777.07
 },
 {
  "input": "Nevertheless, it seems to be true that for the real Galton board, despite violating both of these, a normal distribution does kind of come about? ",
  "translatedText": "Тим не менш, здається правдою, що для справжньої дошки Гальтона, незважаючи на порушення обох цих вимог, нормальний розподіл таки виникає? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1778.13,
  "end": 1785.47
 },
 {
  "input": "Part of the reason might be that there are generalizations of the theorem beyond the scope of this video that relax these assumptions, especially the second one. ",
  "translatedText": "Частково причина може полягати в тому, що існують узагальнення теореми, які виходять за межі цього відео, які послаблюють ці припущення, особливо друге. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1786.05,
  "end": 1793.89
 },
 {
  "input": "But I do want to caution you against the fact that many times people seem to assume that a variable is normally distributed, even when there's no actual justification to do so. ",
  "translatedText": "Але я хочу застерегти вас від того факту, що часто люди припускають, що змінна розподілена нормально, навіть якщо для цього немає реальних підстав. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1794.49,
  "end": 1803.07
 },
 {
  "input": "The third assumption is actually fairly subtle. ",
  "translatedText": "Третє припущення насправді досить тонке. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1804.29,
  "end": 1806.21
 },
 {
  "input": "It's that the variance we've been computing for these variables is finite. ",
  "translatedText": "Справа в тому, що дисперсія, яку ми обчислили для цих змінних, є кінцевою. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1806.21,
  "end": 1810.27
 },
 {
  "input": "This was never an issue for the dice example, because there were only six possible outcomes. ",
  "translatedText": "Це ніколи не було проблемою для прикладу з кубиками, оскільки було лише шість можливих результатів. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1810.81,
  "end": 1814.85
 },
 {
  "input": "But in certain situations where you have an infinite set of outcomes, when you go to compute the variance, the sum ends up diverging off to infinity. ",
  "translatedText": "Але в певних ситуаціях, коли у вас є нескінченний набір результатів, коли ви починаєте обчислювати дисперсію, сума в кінцевому підсумку розходиться до нескінченності. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1815.03,
  "end": 1822.51
 },
 {
  "input": "These can be perfectly valid probability distributions, and they do come up in practice. ",
  "translatedText": "Це можуть бути абсолютно дійсні розподіли ймовірностей, і вони трапляються на практиці. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1823.45,
  "end": 1827.25
 },
 {
  "input": "But in those situations, as you consider adding many different instantiations of that variable and letting that sum approach infinity, even if the first two assumptions hold, it is very much a possibility that the thing you tend towards is not actually a normal distribution. ",
  "translatedText": "Але в таких ситуаціях, коли ви розглядаєте додавання багатьох різних екземплярів цієї змінної та дозволяєте цій сумі наближатися до нескінченності, навіть якщо перші два припущення виконуються, дуже велика ймовірність того, що те, до чого ви прагнете, насправді не є нормальним розподілом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1827.55,
  "end": 1841.19
 },
 {
  "input": "If you've understood everything up to this point, you now have a very strong foundation in what the central limit theorem is all about. ",
  "translatedText": "Якщо ви зрозуміли все до цього моменту, тепер у вас є дуже міцна основа для центральної граничної теореми. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1842.15,
  "end": 1847.65
 },
 {
  "input": "And next up, I'd like to explain why it is that this particular function is the thing that we tend towards, and why it has a pi in it, what it has to do with circles. ",
  "translatedText": "І далі я хотів би пояснити, чому саме ця функція є тією річчю, до якої ми схильні, і чому в ній є пі, що це має спільного з колами. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1848.29,
  "end": 1874.17
 }
]