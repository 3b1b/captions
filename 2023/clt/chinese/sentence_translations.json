[
 {
  "input": "This is a Galton board.",
  "translatedText": "这是一个高尔顿钉板",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 1.26
 },
 {
  "input": "Maybe you've seen one before, it's a popular demonstration of how, even when a single event is chaotic and random, with an effectively unknowable outcome, it's still possible to make precise statements about a large number of events, namely how the relative proportions for many different outcomes are distributed.",
  "translatedText": "也许你以前见过 它通常用来说明 即使某一单独的事件的结果混乱、随机 大量这种事件不同结果的相对比例 仍能够被准确描述",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 2.52,
  "end": 18.3
 },
 {
  "input": "More specifically, the Galton board illustrates one of the most prominent distributions in all probability, known as the normal distribution, more colloquially known as a bell curve, and also called a Gaussian distribution.",
  "translatedText": "更具体地说，\"高尔顿棋盘 \"展示了所有概率中最突出的分布之一，即正态分布，更通俗地说，就是钟形曲线，也叫高斯分布。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.38,
  "end": 31.9
 },
 {
  "input": "There's a very specific function to describe this distribution, it's very pretty, we'll get into it later, but right now I just want to emphasize how the normal distribution is, as the name suggests, very common, it shows up in a lot of seemingly unrelated contexts.",
  "translatedText": "有一个非常明确的函数来描述这个分布 我们稍后会深入讨论这个美妙的式子 但现在我想强调的是 为何“正态分布”正是一种常态 出现在许多看似无关的场景中",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 32.5,
  "end": 45.04
 },
 {
  "input": "If you were to take a large number of people who sit in a similar demographic and plot their heights, those heights tend to follow a normal distribution.",
  "translatedText": "假如你调查特征相似的一大批人 画出他们的身高分布图 你会发现这往往遵循正态分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 46.02,
  "end": 53.0
 },
 {
  "input": "If you look at a large swath of very big natural numbers, and you ask how many distinct prime factors does each one of those numbers have, the answers will very closely track with a certain normal distribution.",
  "translatedText": "如果你观察一大片非常大的自然数，并询问其中每个数有多少个不同的质因数，答案将非常接近某种正态分布。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 53.66,
  "end": 64.96
 },
 {
  "input": "Now our topic for today is one of the crown jewels in all of probability theory, it's one of the key facts that explains why this distribution is as common as it is, known as the central limit theorem.",
  "translatedText": "我们今天的主题 是概率论的璀璨明珠 也是正态分布之所以如此普遍的一大原因 它就是中心极限定理",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 65.58,
  "end": 76.02
 },
 {
  "input": "This lesson is meant to go back to the basics, giving you the fundamentals on what the central limit theorem is saying, what normal distributions are, and I want to assume minimal background.",
  "translatedText": "这节课的目的是回归基础 用最少的背景知识 介绍中心极限定理以及正态分布的基本内容",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.64,
  "end": 85.26
 },
 {
  "input": "We're going to go decently deep into it, but after this I'd still like to go deeper and explain why the theorem is true, why the function underlying the normal distribution has the very specific form that it does, why that formula has a pi in it, and, most fun, why those last two facts are actually more related than a lot of traditional explanations would suggest.",
  "translatedText": "我们会讲得挺深 但在此之后 我仍然想更进一步解释中心极限定理为何成立 为什么描述正态分布的函数 要长成这样子 为什么式子里还有个 π 而最有趣的是 正态分布的函数与π之间的联系 比很多传统的解释 要来得更紧密",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.26,
  "end": 105.56
 },
 {
  "input": "That second lesson is also meant to be the follow-on to the convolutions video that I promised, so there's a lot of interrelated topics here.",
  "translatedText": "而这一部分 也是我之前承诺的卷积视频的后续 这里的许多内容都相互关联",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 106.48,
  "end": 113.37
 },
 {
  "input": "But right now, back to the fundamentals, I'd like to kick things off with an overly simplified model of the Galton board.",
  "translatedText": "但现在，回到基本原理上来，我想以一个过于简化的高尔顿棋盘模型作为开场白。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.57,
  "end": 119.17
 },
 {
  "input": "In this model we will assume that each ball falls directly onto a certain central peg, and that it has a 50-50 probability of bouncing to the left or to the right, and we'll think of each of those outcomes as either adding one or subtracting one from its position.",
  "translatedText": "在这个模型中，我们假设每个球都直接落在某个中心钉上，它向左或向右弹跳的概率各占一半，我们把每种结果都看作是在其位置上加一或减一。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.89,
  "end": 134.11
 },
 {
  "input": "Once one of those is chosen, we make the highly unrealistic assumption that it happens to land dead on in the middle of the peg adjacent below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the right.",
  "translatedText": "选择了方向之后 我们还要做一个非常不切实际的假设 那就是它会恰好落在下一层某个钉子的正中间 然后再次面临向左或向右弹跳的选择",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 134.67,
  "end": 147.07
 },
 {
  "input": "For the one I'm showing on screen, there are five different rows of pegs, so our little hopping ball makes five different random choices between plus one and minus one, and we can think of its final position as basically being the sum of all of those different numbers, which in this case happens to be one, and we might label all of the different buckets with the sum that they represent, as we repeat this we're looking at different possible sums for those five random numbers.",
  "translatedText": "对于我在屏幕上展示的这个游戏，有五行不同的图钉，所以我们的小跳球会在加 1 和减 1 之间做出五种不同的随机选择，我们可以认为它的最终位置基本上就是所有这些不同数字的总和，在本例中恰好是 1，我们可以给所有不同的桶标上它们所代表的总和，当我们重复这个过程时，我们就会看到这五个随机数字的不同可能总和。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 147.43,
  "end": 171.29
 },
 {
  "input": "And for those of you who are inclined to complain that this is a highly unrealistic model for the true Galton board, let me emphasize the goal right now is not to accurately model physics, the goal is to give a simple example to illustrate the central limit theorem, and for that, idealized though this might be, it actually gives us a really good example.",
  "translatedText": "对于那些倾向于抱怨这是一个非常不现实的真正高尔顿棋盘模型的人，请允许我强调一下，我们现在的目标并不是要精确地模拟物理，我们的目标是举一个简单的例子来说明中心极限定理，为此，尽管这可能是理想化的，但它实际上为我们提供了一个非常好的例子。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.05,
  "end": 190.03
 },
 {
  "input": "If we let many different balls fall, making yet another unrealistic assumption that they don't influence each other, as if they're all ghosts, then the number of balls that fall into each different bucket gives us some loose sense for how likely each one of those buckets is.",
  "translatedText": "如果我们让许多不同的球掉下来，再做一个不切实际的假设，让它们互不影响，就好像它们都是幽灵一样，那么掉进每个不同桶里的球的数量就能让我们对每个桶的可能性有一定的了解。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 190.57,
  "end": 203.39
 },
 {
  "input": "In this example, the numbers are simple enough that it's not too hard to explicitly calculate what the probability is for falling into each bucket.",
  "translatedText": "这个例子中的数都不复杂 所以我们可以轻松计算出球落入各个桶的概率",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 203.83,
  "end": 210.01
 },
 {
  "input": "If you do want to think that through, you'll find it very reminiscent of Pascal's triangle, but the neat thing about our theorem is how far it goes beyond the simple examples.",
  "translatedText": "如果你真的想把这个问题想清楚，你会发现它很容易让人联想到帕斯卡三角形，但我们这个定理的精妙之处在于它远远超出了简单的例子。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.27,
  "end": 218.27
 },
 {
  "input": "So to start off at least, rather than making explicit calculations, let's just simulate things by running a large number of samples and letting the total number of results in each different outcome give us some sense for what that distribution looks like.",
  "translatedText": "所以 一上来我们先不去仔细算 而是大量重复地模拟这个过程 让这个模拟中每个结果出现的次数 为我们展示这个分布的大致形状",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 218.67,
  "end": 229.97
 },
 {
  "input": "As I said, the one on screen has five rows, so each sum that we're considering includes only five numbers.",
  "translatedText": "就像我之前说的 这个屏幕上有五行钉子 也就意味着我们考虑的和只由五个数得出",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 230.45,
  "end": 236.21
 },
 {
  "input": "The basic idea of the central limit theorem is that if you increase the size of that sum, for example here would mean increasing the number of rows of pegs for each ball to bounce off, then the distribution that describes where that sum is going to fall looks more and more like a bell curve.",
  "translatedText": "中心极限定理的基本思想是，如果你增加总和的大小，比如这里指的是增加每个小球反弹的球钉的行数，那么描述总和落点的分布看起来就越来越像一条钟形曲线。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.81,
  "end": 253.33
 },
 {
  "input": "Here, it's actually worth taking a moment to write down that general idea.",
  "translatedText": "我们还是花点时间来写下大致的想法：",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.47,
  "end": 258.35
 },
 {
  "input": "The setup is that we have a random variable, and that's basically shorthand for a random process where each outcome of that process is associated with some number.",
  "translatedText": "首先我们有一个随机变量 其实就是把一个随机的过程中 得到的每个结果都写成一个数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 259.27,
  "end": 268.19
 },
 {
  "input": "We'll call that random number x.",
  "translatedText": "我们将这个随机变量称为 X",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 268.49,
  "end": 269.97
 },
 {
  "input": "For example, each bounce off the peg is a random process modeled with two outcomes.",
  "translatedText": "例如每次小球的弹跳 都是一个随机的过程 只有两种结果",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.97,
  "end": 274.39
 },
 {
  "input": "Those outcomes are associated with the numbers negative one and positive one.",
  "translatedText": "左和右分别记为 -1 和 +1",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 274.85,
  "end": 277.89
 },
 {
  "input": "Another example of a random variable would be rolling a die, where you have six different outcomes, each one associated with a number.",
  "translatedText": "另一个随机变量的例子是掷骰子 总共有六种结果 分别写成 1~6",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.53,
  "end": 284.83
 },
 {
  "input": "What we're doing is taking multiple different samples of that variable and adding them all together.",
  "translatedText": "而我们要做的就是对这个随机变量多次取样 最后全部相加",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 285.47,
  "end": 290.41
 },
 {
  "input": "On our Galton board, that looks like letting the ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice and adding up the results.",
  "translatedText": "用高尔顿板的例子来说就是 让球在一路下落到底的过程中多次被钉子反弹 而在骰子的例子中 则是掷很多不同的骰子并将点数相加",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 290.77,
  "end": 300.97
 },
 {
  "input": "The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into different possible values, will look more and more like a bell curve.",
  "translatedText": "中心极限定理说明了 随着相加的随机变量越来越多 这个总和的分布 也就是取各种值的概率 将越来越接近一个钟形曲线",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 301.43,
  "end": 314.11
 },
 {
  "input": "That's it, that is the general idea.",
  "translatedText": "而这也是中心极限定理的大致思想",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 315.43,
  "end": 317.13
 },
 {
  "input": "Over the course of this lesson, our job is to make that statement more quantitative.",
  "translatedText": "我们这节课的目标 是去量化这个笼统的描述",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 317.55,
  "end": 321.53
 },
 {
  "input": "We're going to put some numbers to it, put some formulas to it, show how you can use it to make predictions.",
  "translatedText": "为这个描述添上一些数和式子 并展示如何用它进行预测",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 322.07,
  "end": 326.35
 },
 {
  "input": "For example, here's the kind of question I want you to be able to answer by the end of this video.",
  "translatedText": "例如 看完这个视频以后 我希望你能回答像下面这种问题",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 327.21,
  "end": 331.57
 },
 {
  "input": "Suppose you rolled a die 100 times and you added together the results.",
  "translatedText": "假设你掷了 100 次骰子，然后将结果相加。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 332.19,
  "end": 335.89
 },
 {
  "input": "Could you find a range of values such that you're 95% sure that the sum will fall within that range?",
  "translatedText": "请报出一个取值范围 能够保证有 95% 的把握 总和会落在这个范围内",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.63,
  "end": 342.17
 },
 {
  "input": "Or maybe I should say find the smallest possible range of values such that this is true.",
  "translatedText": "我还应该加上一句 这个要寻找的范围越窄越好",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 342.83,
  "end": 346.55
 },
 {
  "input": "The neat thing is you'll be able to answer this question whether it's a fair die or if it's a weighted die.",
  "translatedText": "神奇的是 无论你掷的骰子是否均匀 你总能回答这个问题",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 347.39,
  "end": 352.13
 },
 {
  "input": "Now let me say at the top that this theorem has three different assumptions that go into it, three things that have to be true before the theorem follows.",
  "translatedText": "让我把话放在前面 中心极限定理有三条假设 它们是定理成立的前提条件",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.45,
  "end": 360.13
 },
 {
  "input": "And I'm actually not going to tell you what they are until the very end of the video.",
  "translatedText": "不过具体要等到视频的结尾我才会告诉你",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 360.43,
  "end": 363.79
 },
 {
  "input": "Instead I want you to keep your eye out and see if you can notice and maybe predict what those three assumptions are going to be.",
  "translatedText": "但我希望你看仔细点 看看是否能注意到并猜出 这三个假设究竟是什么",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 364.27,
  "end": 369.67
 },
 {
  "input": "As a next step, to better illustrate just how general this theorem is, I want to run a couple more simulations for you focused on the dice example.",
  "translatedText": "为了更好地说明这个定理有多通用 我想先多模拟几个掷骰子的例子",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 370.71,
  "end": 377.39
 },
 {
  "input": "Usually if you think of rolling a die you think of the six outcomes as being equally probable, but the theorem actually doesn't care about that.",
  "translatedText": "通常 如果你掷一个骰子 你会认为六个结果是等概率的 但是定理并不在意这个条件",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 380.91,
  "end": 387.63
 },
 {
  "input": "We could start with a weighted die, something with a non-trivial distribution across the outcomes, and the core idea still holds.",
  "translatedText": "我们可以从一个偏心骰子开始 它的结果分布并不均匀 但定理的核心观点却仍然成立",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 387.83,
  "end": 394.55
 },
 {
  "input": "For the simulation what I'll do is take some distribution like this one that is skewed towards lower values.",
  "translatedText": "我们可以取一个分布来进行模拟 例如这个更容易丢出小点数的骰子",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.03,
  "end": 399.93
 },
 {
  "input": "I'm going to take 10 distinct samples from that distribution and then I'll record the sum of that sample on the plot on the bottom.",
  "translatedText": "然后我从这个分布中抽十个样本 在下面的频率图上记录这些样本的和",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 400.25,
  "end": 407.55
 },
 {
  "input": "Then I'm going to do this many many different times, always with a sum of size 10, but keep track of where those sums ended up to give us a sense of the distribution.",
  "translatedText": "反复进行这个过程 每次抽十个样本 把和标记在频率图上 让我们对这个和的分布有个直观理解",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 408.63,
  "end": 416.59
 },
 {
  "input": "And in fact let me rescale the y direction to give us room to run an even larger number of samples.",
  "translatedText": "事实上 我可以把这个图压扁一点 给后面模拟的结果预留空间",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 419.97,
  "end": 424.73
 },
 {
  "input": "And I'll let it go all the way up to a couple thousand, and as it does you'll notice that the shape that starts to emerge looks like a bell curve.",
  "translatedText": "然后跑个几千轮模拟 在这个过程中 你会注意到这个分布的形状开始变化 变得越来越像一个钟形曲线",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.03,
  "end": 432.49
 },
 {
  "input": "Maybe if you squint your eyes you can see it skews a tiny bit to the left, but it's neat that something so symmetric emerged from a starting point that was so asymmetric.",
  "translatedText": "虽然可能你眯起眼睛会发现它有点往左偏 可神奇的是如此对称的图像 居然是用偏心骰子的分布模拟出来的",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.87,
  "end": 441.01
 },
 {
  "input": "To better illustrate what the central limit theorem is all about, let me run four of these simulations in parallel, where on the upper left I'm doing it where we're only adding two dice at a time, on the upper right we're doing it where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at a time, and then we'll do another one with a bigger sum, 15 at a time.",
  "translatedText": "为了更好地解释中心极限定理到底是什么 让我同时模拟这四个东西 左上角是两个偏心骰子的点数和 右上角是一次五个骰子的点数和 左下角是我们刚刚模拟的十个骰子的点数和 最后右下角我们搞个大的 模拟 15 个骰子的点数和",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 441.47,
  "end": 461.37
 },
 {
  "input": "Notice how on the upper left when we're just adding two dice, the resulting distribution doesn't really look like a bell curve, it looks a lot more reminiscent of the one we started with, skewed towards the left.",
  "translatedText": "注意左上角，当我们只增加两颗骰子时，得到的分布看起来并不像钟形曲线，它看起来更像我们开始时的分布，向左倾斜。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 462.25,
  "end": 472.03
 },
 {
  "input": "But as we allow for more and more dice in each sum, the resulting shape that comes up in these distributions looks more and more symmetric.",
  "translatedText": "但是随着我们用的骰子越来越多 得到的这个点数和分布的形状 就会越来越对称",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 472.81,
  "end": 479.81
 },
 {
  "input": "It has the lump in the middle and fade towards the tail's shape of a bell curve.",
  "translatedText": "它的形状中间凸起两端逐渐平缓 趋近钟形曲线",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.95,
  "end": 483.89
 },
 {
  "input": "And let me emphasize again, you can start with any different distribution.",
  "translatedText": "要强调的是 这个规律用任意点数分布的骰子都适用",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 487.05,
  "end": 490.49
 },
 {
  "input": "Here I'll run it again, but where most of the probability is tied up in the numbers 1 and 6, with very low probability for the mid values.",
  "translatedText": "然后我再模拟一次 但这次骰子十分可能掷出 1 和 6 几乎不可能掷出中间点数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 490.49,
  "end": 497.49
 },
 {
  "input": "Despite completely changing the distribution for an individual roll of the die, it's still the case that a bell curve shape will emerge as we consider the different sums.",
  "translatedText": "尽管完全改变了骰子点数的概率分布 当我们记录这些不同总和数值出现的次数时 我们仍然观察到钟形曲线形状逐渐形成",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.19,
  "end": 506.55
 },
 {
  "input": "Illustrating things with a simulation like this is very fun, and it's kind of neat to see order emerge from chaos, but it also feels a little imprecise.",
  "translatedText": "用这样的模拟来解释这个定理非常有趣 并且神奇的是 无论一开始多么混乱 最终都归于钟形曲线 但它也如现在展示的这样 没有那么精确",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.27,
  "end": 515.03
 },
 {
  "input": "Like in this case, when I cut off the simulation at 3000 samples, even though it kind of looks like a bell curve, the different buckets seem pretty spiky, and you might wonder, is it supposed to look that way, or is that just an artifact of the randomness in the simulation?",
  "translatedText": "在这种情况下，当我切断 3000 个样本的模拟时，尽管它看起来有点像钟形曲线，但不同的桶看起来非常尖锐，你可能会想，它应该是这样的吗，或者这只是模拟中随机性的一个伪影？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 515.39,
  "end": 528.55
 },
 {
  "input": "And if it is, how many samples do we need before we can be sure that what we're looking at is representative of the true distribution?",
  "translatedText": "如果是的话 那我们到底需要多少样本 才能确信我们现在看到的图像 能够代表真实分布情况呢",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 529.01,
  "end": 535.11
 },
 {
  "input": "Instead moving forward, let's get a little more theoretical and show the precise shape these distributions will take on in the long run.",
  "translatedText": "接下来，让我们从理论角度来说明这些分布在长期内的精确形态。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.19,
  "end": 545.47
 },
 {
  "input": "The easiest case to make this calculation is if we have a uniform distribution, where each possible face of the die has an equal probability, 1 6th.",
  "translatedText": "最简单的情况是我们有一个公平的骰子 它掷出骰子的每一面点数的概率都是 1/6",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 546.13,
  "end": 553.97
 },
 {
  "input": "For example, if you then want to know how likely different sums are for a pair of dice, it's essentially a counting game, where you count up how many distinct pairs take on the same sum, which in the diagram I've drawn, you can conveniently think about by going through all the different diagonals.",
  "translatedText": "例如，如果你想知道一对骰子的不同和值的可能性有多大，这本质上是一个计数游戏，你可以数出有多少对骰子的和值是相同的，在我画的图中，你可以通过所有不同的对角线方便地进行思考。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.99,
  "end": 568.49
 },
 {
  "input": "Since each such pair has an equal chance of showing up, 1 in 36, all you have to do is count the sizes of these buckets.",
  "translatedText": "并且掷出各对骰子的概率都是 1/36 你只要数有多少种组合可以得到某个点数和",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.41,
  "end": 577.53
 },
 {
  "input": "That gives us a definitive shape for the distribution describing a sum of two dice, and if we were to play the same game with all possible triplets, the resulting distribution would look like this.",
  "translatedText": "就能够得到准确的点数和概率分布 如果我们把骰子增加到三个 那么我们得到的分布长这样",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 578.19,
  "end": 588.13
 },
 {
  "input": "Now what's more challenging, but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that single die.",
  "translatedText": "现在更有挑战也更有趣的问题是 换成偏心骰子以后会发生什么",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 588.69,
  "end": 594.99
 },
 {
  "input": "We actually talked all about this in the last video.",
  "translatedText": "其实我们在上一个视频中已经讨论过这个问题",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 595.55,
  "end": 597.97
 },
 {
  "input": "You do essentially the same thing, you go through all the distinct pairs of dice which add up to the same value.",
  "translatedText": "你只需要做和刚刚相同的事情 找出相同点数和的不同骰子组合",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 598.45,
  "end": 603.67
 },
 {
  "input": "It's just that instead of counting those pairs, for each pair you multiply the two probabilities of each particular face coming up, and then you add all those together.",
  "translatedText": "只是你不能光数个数 而是要把每对里两个点数的概率相乘 然后把所有点数和相同的概率加在一起",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 603.97,
  "end": 612.75
 },
 {
  "input": "The computation that does this for all possible sums has a fancy name, it's called a convolution, but it's essentially just the weighted version of the counting game that anyone who's played with a pair of dice already finds familiar.",
  "translatedText": "这个计算各个点数和概率的过程 其实有个高大上的名字 叫 卷积 但本质上它只是带权重的计数游戏 刚刚认真听课的同学肯定不会陌生",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.29,
  "end": 624.47
 },
 {
  "input": "For our purposes in this lesson, I'll have the computer calculate all that, simply display the results for you, and invite you to observe certain patterns, but under the hood, this is what's going on.",
  "translatedText": "为了减少不必要的麻烦 这些运算全部由计算机进行 然后把结果展示给你 指引你发现其中的规律 而刚刚所介绍的这些 则是幕后的运算过程",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 625.03,
  "end": 635.33
 },
 {
  "input": "So just to be crystal clear on what's being represented here, if you imagine sampling two different values from that top distribution, the one describing a single die, and adding them together, then the second distribution I'm drawing represents how likely you are to see various different sums.",
  "translatedText": "为了让你完全理解这都在干啥 假设从最上面的那个单个骰子点数概率的分布图中 取两个点数加在一起 那么我下面画的那个分布 表示相加得到各个点数和的概率",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 636.65,
  "end": 652.23
 },
 {
  "input": "Likewise, if you imagine sampling three distinct values from that top distribution, and adding them together, the next plot represents the probabilities for various different sums in that case.",
  "translatedText": "同理 假设从最上面的分布中取三个不同的值 然后把它们加在一起 在这种情况下各个点数和的概率 则是像最下面这张图展现的一样",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 652.89,
  "end": 662.49
 },
 {
  "input": "So if I compute what the distributions for these sums look like for larger and larger sums, well you know what I'm going to say, it looks more and more like a bell curve.",
  "translatedText": "依此类推 如果我像这样计算不同点数和的概率分布 当我用的骰子数越来越多时 我猜你已经知道我要说什么了 它会长得越来越像钟形曲线",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 663.51,
  "end": 672.39
 },
 {
  "input": "But before we get to that, I want you to make a couple more simple observations.",
  "translatedText": "但在我们讨论这个现象之前 我希望你再简单观察一些规律",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 673.35,
  "end": 676.45
 },
 {
  "input": "For example, these distributions seem to be wandering to the right, and also they seem to be getting more spread out, and a little bit more flat.",
  "translatedText": "例如 这些分布的图像在不断向右偏移 而且分布得越来越宽 图像也慢慢变扁平",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 677.45,
  "end": 684.79
 },
 {
  "input": "You cannot describe the central limit theorem quantitatively without taking into account both of those effects, which in turn requires describing the mean and the standard deviation.",
  "translatedText": "如果不考虑上面这两种现象 你将很难定量描述中心极限定理 而这就要求我们能够描述它们的均值和标准差",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.25,
  "end": 693.19
 },
 {
  "input": "Maybe you're already familiar with those, but I want to make minimal assumptions here, and it never hurts to review, so let's quickly go over both of those.",
  "translatedText": "或许你已经对这两个概念很熟悉了 不过我不会假设看这个视频的都了解 而且复习一下也无妨 那么让我们快速回顾一下这两个概念",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 693.95,
  "end": 700.61
 },
 {
  "input": "The mean of a distribution, often denoted with the Greek letter mu, is a way of capturing the center of mass for that distribution.",
  "translatedText": "一个分布的均值 通常用希腊字母 μ 表示 描述了这个分布的重心",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 703.41,
  "end": 710.71
 },
 {
  "input": "It's calculated as the expected value of our random variable, which is a way of saying you go through all of the different possible outcomes, and you multiply the probability of that outcome times the value of the variable.",
  "translatedText": "而它的值恰好是随机变量的期望值 在这里 期望意味着 把这个随机变量各个取值乘以对应的概率的结果加起来",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 711.19,
  "end": 722.85
 },
 {
  "input": "If higher values are more probable, that weighted sum is going to be bigger.",
  "translatedText": "这样一来 如果更可能出现较大的值 这个加权和会更大",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 723.19,
  "end": 726.41
 },
 {
  "input": "If lower values are more probable, that weighted sum is going to be smaller.",
  "translatedText": "反之 如果更可能出现较小的数值 这个加权和也会更小",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 726.75,
  "end": 729.95
 },
 {
  "input": "A little more interesting is if you want to measure how spread out this distribution is, because there's multiple different ways you might do it.",
  "translatedText": "而测量这个分布的范围有多广 由于许多不同的概念都可以做到 就显得更有意思了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.79,
  "end": 737.13
 },
 {
  "input": "One of them is called the variance.",
  "translatedText": "其中一种方法叫做方差",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 738.53,
  "end": 740.29
 },
 {
  "input": "The idea there is to look at the difference between each possible value and the mean, square that difference, and ask for its expected value.",
  "translatedText": "它是通过先计算 随机变量各个取值与均值之间的差 并取这个差值的平方作为新的随机变量求期望",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 740.83,
  "end": 748.27
 },
 {
  "input": "The idea is that whether your value is below or above the mean, when you square that difference, you get a positive number, and the larger the difference, the bigger that number.",
  "translatedText": "这样一来 无论你的随机变量取值是低于还是高于均值 当你取这个差值的平方时 你总能得到一个正数 而且差值越大 这个数值也越大",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 748.73,
  "end": 756.65
 },
 {
  "input": "Squaring it like this turns out to make the math much much nicer than if we did something like an absolute value, but the downside is that it's hard to think about this as a distance in our diagram because the units are off, kind of like the units here are square units, whereas a distance in our diagram would be a kind of linear unit.",
  "translatedText": "这样的平方计算比绝对值计算要好得多，但缺点是很难在图中将其视为距离，因为单位不同，就像这里的单位是平方单位，而图中的距离是一种线性单位。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 757.37,
  "end": 773.31
 },
 {
  "input": "So another way to measure spread is what's called the standard deviation, which is the square root of this value.",
  "translatedText": "因此 另一种测量分布范围宽度的方法就是用标准差 其实就是方差的平方根",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 773.71,
  "end": 779.19
 },
 {
  "input": "That can be interpreted much more reasonably as a distance on our diagram, and it's commonly denoted with the Greek letter sigma, so you know m for mean as for standard deviation, but both in Greek.",
  "translatedText": "这样一来 我们可以更合理地把它说成是在图上的“距离” 通常用希腊字母 σ 表示 也就是 m 表示均值 s 表示标准差 只不过用希腊字母表示",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 779.47,
  "end": 789.65
 },
 {
  "input": "Looking back at our sequence of distributions, let's talk about the mean and standard deviation.",
  "translatedText": "回顾一下刚刚图中的一系列分布 我们可以谈谈均值和标准差",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.87,
  "end": 796.15
 },
 {
  "input": "If we call the mean of the initial distribution mu, which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if I tell you that the mean of the next one is 2 times mu.",
  "translatedText": "如果我们把最早提到的数值分布的均值称为 μ 对于图上所示的这个例子 μ 恰好是 2.24 而要是我告诉你下一个的分布均值是 2μ 你应该不会被惊到吧",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.63,
  "end": 806.73
 },
 {
  "input": "That is, you roll a pair of dice, you want to know the expected value of the sum, it's two times the expected value for a single die.",
  "translatedText": "换句话说 当你求一对骰子点数和的期望时 它是单个骰子点数期望值的两倍",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 807.13,
  "end": 812.81
 },
 {
  "input": "Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth.",
  "translatedText": "而类似的 三个骰子点数和的期望是 3μ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 813.85,
  "end": 819.41
 },
 {
  "input": "The mean just marches steadily on to the right, which is why our distributions seem to be drifting off in that direction.",
  "translatedText": "依此类推 每加一个骰子 均值就会向右移动一些 这就是为什么我们发现分布整体在朝右移动",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 819.63,
  "end": 824.87
 },
 {
  "input": "A little more challenging, but very important, is to describe how the standard deviation changes.",
  "translatedText": "而更有挑战但非常重要的一点是 描述标准差是怎么变化的",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.35,
  "end": 829.91
 },
 {
  "input": "The key fact here is that if you have two different random variables, then the variance for the sum of those variables is the same as just adding together the original two variances.",
  "translatedText": "关键点在于 如果你有两个随机变量 那么这两个变量之和的方差 和将原始的两个变量方差相加所得到的值完全相同",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 830.49,
  "end": 839.37
 },
 {
  "input": "This is one of those facts that you can just compute when you unpack all the definitions.",
  "translatedText": "在用上了一堆的定义以后 你可以通过计算来验证这一点",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 839.93,
  "end": 843.63
 },
 {
  "input": "There are a couple nice intuitions for why it's true.",
  "translatedText": "关于这一点有一些很妙的直观解释",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 843.63,
  "end": 846.21
 },
 {
  "input": "My tentative plan is to just actually make a series about probability and talk about things like intuitions underlying variance and its cousins there.",
  "translatedText": "我暂时的计划是做一个关于概率的系列视频 来直观解释方差及其相关的概念",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 846.63,
  "end": 853.53
 },
 {
  "input": "But right now, the main thing I want you to highlight is how it's the variance that adds, it's not the standard deviation that adds.",
  "translatedText": "但现在我希望你重点关注的 是为什么加的是方差而不是标准差",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 854.01,
  "end": 860.15
 },
 {
  "input": "So, critically, if you were to take n different realizations of the same random variable and ask what the sum looks like, the variance of sum is n times the variance of your original variable, meaning the standard deviation, the square root of all this, is the square root of n times the original standard deviation.",
  "translatedText": "因此，关键的是，如果你对同一个随机变量进行 n 次不同的变现，并询问其总和是什么样子的，总和的方差就是原始变量方差的 n 倍，这意味着标准偏差，即所有这一切的平方根，就是原始标准偏差 n 倍的平方根。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 860.41,
  "end": 878.25
 },
 {
  "input": "For example, back in our sequence of distributions, if we label the standard deviation of our initial one with sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on and so forth. This, like I said, is very important.",
  "translatedText": "例如，回到我们的分布序列中，如果我们用西格玛标出初始分布的标准差，那么下一个标准差就是 2 倍西格玛的平方根，然后是 3 倍西格玛的平方根，以此类推。正如我所说，这一点非常重要。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 879.29,
  "end": 895.65
 },
 {
  "input": "It means that even though our distributions are getting spread out, they're not spreading out all that quickly, they only do so in proportion to the square root of the size of the sum.",
  "translatedText": "因为这意味着尽管我们总和分布得越来越宽 但是它们并没有扩散得那么快 这个速度正比于参与求和的变量个数的平方根",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.07,
  "end": 904.13
 },
 {
  "input": "As we prepare to make a more quantitative description of the central limit theorem, the core intuition I want you to keep in your head is that we'll basically realign all of these distributions so that their means line up together, and then rescale them so that all of the standard deviations are just going to be equal to one.",
  "translatedText": "当我们准备对中心极限定理进行更量化的描述时，我希望你们记住的核心直觉是，我们基本上要重新调整所有这些分布，使它们的均值一致，然后重新调整它们的尺度，使所有的标准差都等于 1。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 904.71,
  "end": 920.61
 },
 {
  "input": "And when we do that, the shape that results gets closer and closer to a certain universal shape, described with an elegant little function that we'll unpack in just a moment.",
  "translatedText": "这样做所得到的分布形状 随着变量数增多趋近于同一个形状 这个形状背后有一个优美的函数 一会就来说",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 921.29,
  "end": 929.87
 },
 {
  "input": "And let me say one more time, the real magic here is how we could have started with any distribution, describing a single roll of the die, and if we play the same game, considering what the distributions for the many different sums look like, and we realign them so that the means line up, and we rescale them so that the standard deviations are all one, we still approach that same universal shape, which is kind of mind-boggling.",
  "translatedText": "让我再说一遍，这里真正的奥妙在于，我们可以从描述一次掷骰子的任何分布开始，如果我们玩同样的游戏，考虑许多不同总和的分布是什么样子，我们重新调整它们，使均值一致，我们重新缩放它们，使标准偏差都是一个，我们仍然接近相同的普遍形状，这有点令人难以置信。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 930.47,
  "end": 952.95
 },
 {
  "input": "And now, my friends, is probably as good a time as any to finally get into the formula for a normal distribution.",
  "translatedText": "说了这么多 终于可以正式聊正态分布的式子了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 954.81,
  "end": 960.85
 },
 {
  "input": "And the way I'd like to do this is to basically peel back all the layers and build it up one piece at a time.",
  "translatedText": "我想要做的就是 把它全部拆开再一点一点组装起来",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 961.49,
  "end": 965.93
 },
 {
  "input": "The function e to the x, or anything to the x, describes exponential growth, and if you make that exponent negative, which flips around the graph horizontally, you might think of it as describing exponential decay.",
  "translatedText": "函数 eˣ 或者随便什么的 x 次方 表示指数增长 如果给指数前面加个负号 就会沿 y 轴函数图像 可以当成是指数衰减",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 966.53,
  "end": 977.87
 },
 {
  "input": "To make this decay in both directions, you could do something to make sure the exponent is always negative and growing, like taking the negative absolute value.",
  "translatedText": "为了让它在正负方向上都衰减 我们要确保指数非正 而且越往两边越小 例如取绝对值的相反数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.51,
  "end": 985.43
 },
 {
  "input": "That would give us this kind of awkward sharp point in the middle, but if instead you make that exponent the negative square of x, you get a smoother version of the same thing, which decays in both directions.",
  "translatedText": "不过这会使中间有个奇怪的刺突 但如果指数是 -x² 不仅同样往两边衰减 还能得到更光滑的函数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 985.93,
  "end": 995.81
 },
 {
  "input": "This gives us the basic bell curve shape.",
  "translatedText": "这样就得到了一个钟形曲线",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 996.33,
  "end": 998.19
 },
 {
  "input": "Now if you throw a constant in front of that x, and you scale that constant up and down, it lets you stretch and squish the graph horizontally, allowing you to describe narrow and wider bell curves.",
  "translatedText": "接着 如果你在 x 前面乘一个常数 然后修改这个常数的大小 就可以在水平方向上伸缩图形 随你的意得到窄的或者宽的曲线",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.65,
  "end": 1008.37
 },
 {
  "input": "And a quick thing I'd like to point out here is that based on the rules of exponentiation, as we tweak around that constant c, you could also think about it as simply changing the base of the exponentiation.",
  "translatedText": "稍微插一句 根据指数的运算法则 当我们调整常数 c 时 可以看作是改变了这个函数的底数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1009.01,
  "end": 1019.75
 },
 {
  "input": "And in that sense, the number e is not really all that special for our formula.",
  "translatedText": "照这样说 e 对于这个式子来说并没有什么特殊",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1020.15,
  "end": 1023.63
 },
 {
  "input": "We could replace it with any other positive constant, and you'll get the same family of curves as we tweak that constant.",
  "translatedText": "可以随便拿什么大于 1 的数来做底 当我们调整系数 c 时 得到的也是这一族曲线",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1024.05,
  "end": 1030.49
 },
 {
  "input": "Make it a 2, same family of curves.",
  "translatedText": "底为 2 时 底为 3 时 都是这一族",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.51,
  "end": 1033.11
 },
 {
  "input": "Make it a 3, same family of curves.",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1033.33,
  "end": 1035.07
 },
 {
  "input": "The reason we use e is that it gives that constant a very readable meaning.",
  "translatedText": "之所以这里用 e 是因为它给了常数 c 一个明确的意义",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1035.75,
  "end": 1039.49
 },
 {
  "input": "Or rather, if we reconfigure things a little bit so that the exponent looks like negative 1 half times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn this into a probability distribution, that constant sigma will be the standard deviation of that distribution.",
  "translatedText": "或者说，如果我们稍微调整一下，让指数看起来像 x 的负 1.5 倍除以某个常数（我们姑且称之为西格玛平方），那么一旦我们将其转化为概率分布，这个常数西格玛就是该分布的标准偏差。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1040.11,
  "end": 1057.21
 },
 {
  "input": "And that's very nice.",
  "translatedText": "真棒",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.81,
  "end": 1058.57
 },
 {
  "input": "But before we can interpret this as a probability distribution, we need the area under the curve to be 1.",
  "translatedText": "但要是想说它是一个概率分布 我们还需要让曲线和概率分布的含义相匹配",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1058.91,
  "end": 1064.31
 },
 {
  "input": "And the reason for that is how the curve is interpreted.",
  "translatedText": "它和 x 轴夹的面积必须为 1",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1064.83,
  "end": 1066.91
 },
 {
  "input": "Unlike discrete distributions, when it comes to something continuous, you don't ask about the probability of a particular point.",
  "translatedText": "与离散分布不同 你不会去管连续分布某一个点的概率",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1067.37,
  "end": 1073.37
 },
 {
  "input": "Instead, you ask for the probability that a value falls between two different values.",
  "translatedText": "而是在意随机值落在两个值之间的概率",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1073.79,
  "end": 1078.23
 },
 {
  "input": "And what the curve is telling you is that that probability equals the area under the curve between those two values.",
  "translatedText": "曲线告诉你的则是 这个概率等于两个值中间曲线下的面积",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1078.75,
  "end": 1085.43
 },
 {
  "input": "There's a whole other video about this, they're called probability density functions.",
  "translatedText": "这类函数叫做概率密度函数 我有一期视频专门介绍它",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1086.03,
  "end": 1089.43
 },
 {
  "input": "The main point right now is that the area under the entire curve represents the probability that something happens, that some number comes up.",
  "translatedText": "这里的关键是 曲线下的总面积 代表了所有可能结果的概率总和 也就是 1",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1089.83,
  "end": 1097.15
 },
 {
  "input": "That should be 1, which is why we want the area under this to be 1.",
  "translatedText": "因此我们才希望曲线下的总面积是 1",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1097.41,
  "end": 1100.63
 },
 {
  "input": "As it stands with the basic bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root of pi.",
  "translatedText": "回到基本的钟形曲线 e 的 -x² 次方 它下面的面积并不是 1 而是根号 π",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1101.05,
  "end": 1107.79
 },
 {
  "input": "I know, right?",
  "translatedText": "我知道你想说啥",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.41,
  "end": 1109.15
 },
 {
  "input": "What is pi doing here?",
  "translatedText": "哪来的 π？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1109.27,
  "end": 1110.19
 },
 {
  "input": "What does this have to do with circles?",
  "translatedText": "哪来的圆？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1110.29,
  "end": 1111.47
 },
 {
  "input": "Like I said at the start, I'd love to talk all about that in the next video.",
  "translatedText": "前面说了 要到下一个视频才能说清楚",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.01,
  "end": 1115.05
 },
 {
  "input": "But if you can spare your excitement, for our purposes right now, all it means is that we should divide this function by the square root of pi, and it gives us the area we want.",
  "translatedText": "不过，如果你能不那么激动的话，对于我们现在的目的来说，这只意味着我们应该用这个函数除以 pi 的平方根，就能得到我们想要的面积。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1115.33,
  "end": 1123.17
 },
 {
  "input": "Throwing back in the constants we had earlier, the one half and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square root of 2.",
  "translatedText": "将我们之前使用过的常量，即二分之一和西格玛重新抛回，其效果是将图形拉长了西格玛乘以 2 的平方根。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.61,
  "end": 1131.79
 },
 {
  "input": "So we also need to divide out by that in order to make sure it has an area of 1, and combining those fractions, the factor out front looks like 1 divided by sigma times the square root of 2 pi.",
  "translatedText": "因此，我们还需要除以这个系数，以确保它的面积为 1，将这些分数合并，前面的因数看起来就像 1 除以 sigma 乘以 2 pi 的平方根。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1132.41,
  "end": 1142.11
 },
 {
  "input": "This, finally, is a valid probability distribution.",
  "translatedText": "终于能用了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1142.91,
  "end": 1145.85
 },
 {
  "input": "As we tweak that value sigma, resulting in narrower and wider curves, that constant in the front always guarantees that the area equals 1.",
  "translatedText": "当我们改变 σ 时 曲线会变窄变宽 而前面的常数确保总面积始终为 1",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1146.45,
  "end": 1154.31
 },
 {
  "input": "The special case where sigma equals 1 has a specific name, we call it the standard normal distribution, which plays an especially important role for you and me in this lesson.",
  "translatedText": "特殊情况是 σ 等于 1 的情况 我们称之为标准正态分布 它是我们这节课的主角",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.91,
  "end": 1164.51
 },
 {
  "input": "And all possible normal distributions are not only parameterized with this value sigma, but we also subtract off another constant mu from the variable x, and this essentially just lets you slide the graph left and right so that you can prescribe the mean of this distribution.",
  "translatedText": "所有可能的正态分布 参数里不仅有这个标准差 σ 还有一个 x 要减去的参数 μ 调整 μ 可以左右滑动图像 也描述了这个分布的均值",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1165.13,
  "end": 1180.21
 },
 {
  "input": "So in short, we have two parameters, one describing the mean, one describing the standard deviation, and they're all tied together in this big formula involving an e and a pi.",
  "translatedText": "总结一下 我们有两个参数 μ 描述均值 σ 描述标准差 它们都通过这个包含 e 和 π 的大公式联系在一起",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1180.99,
  "end": 1189.19
 },
 {
  "input": "Now that all of that is on the table, let's look back again at the idea of starting with some random variable and asking what the distributions for sums of that variable look like.",
  "translatedText": "现在所有这些都理清楚了 让我们再从随机变量开始 问自己这个变量的总和的分布长什么样",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.19,
  "end": 1199.81
 },
 {
  "input": "As we've already gone over, when you increase the size of that sum, the resulting distribution will shift according to a growing mean, and it slowly spreads out according to a growing standard deviation.",
  "translatedText": "就像刚刚说的一样 随着这个随机变量不断重复 总和分布会不断往均值方向移动 并且随着标准差的逐渐增大慢慢变宽",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1200.13,
  "end": 1209.81
 },
 {
  "input": "And putting some actual formulas to it, if we know the mean of our underlying random variable, we call it mu, and we also know its standard deviation, and we call it sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the standard deviation will be sigma times the square root of that size.",
  "translatedText": "代入式子来说的话就是 如果假设最开始随机变量的均值是 μ 标准差是 σ 的话 那么下面这个总和分布的的均值 就会是 μ 乘以随机变量的个数 而标准差也就是 σ 乘以个数的平方根",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1210.33,
  "end": 1227.73
 },
 {
  "input": "So now, if we want to claim that this looks more and more like a bell curve, and a bell curve is only described by two different parameters, the mean and the standard deviation, you know what to do.",
  "translatedText": "现在我们知道钟形曲线只有均值和标准差 如果我们想说分布逐渐趋近钟形曲线的话 你应该已经知道该干啥了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1228.19,
  "end": 1237.71
 },
 {
  "input": "You could plug those two values into the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve that should closely fit our distribution.",
  "translatedText": "只要把参数代入式子 就能得到一个非常精确 但是有点复杂的式子 而这个式子描述的曲线则应该和我们的分布相拟合",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1237.93,
  "end": 1246.99
 },
 {
  "input": "But there's another way we can describe it that's a little more elegant and lends itself to a very fun visual that we can build up to.",
  "translatedText": "此外还有一种方式 不仅描述起来更加优雅 观赏性也很强",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1248.39,
  "end": 1254.81
 },
 {
  "input": "Instead of focusing on the sum of all of these random variables, let's modify this expression a little bit, where what we'll do is we'll look at the mean that we expect that sum to take, and we subtract it off so that our new expression has a mean of zero, and then we're going to look at the standard deviation we expect of our sum, and divide out by that, which basically just rescales the units so that the standard deviation of our expression will equal one.",
  "translatedText": "我们不用关注所有这些随机变量的总和，而是稍微修改一下这个表达式，我们要做的是，先看一下我们预期总和的平均值，然后把它减去，这样我们的新表达式的平均值就是 0，然后我们再看一下我们预期总和的标准偏差，然后除以这个标准偏差，基本上就是重新调整单位，这样我们表达式的标准偏差就等于 1。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1255.27,
  "end": 1278.77
 },
 {
  "input": "This might seem like a more complicated expression, but it actually has a highly readable meaning.",
  "translatedText": "虽然看上去变复杂了 但它的含义更明确了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1279.35,
  "end": 1284.09
 },
 {
  "input": "It's essentially saying how many standard deviations away from the mean is this sum?",
  "translatedText": "它实际上是在说 这个总和离均值有多少个标准差",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1284.45,
  "end": 1289.67
 },
 {
  "input": "For example, this bar here corresponds to a certain value that you might find when you roll 10 dice and you add them all up, and its position a little above negative one is telling you that that value is a little bit less than one standard deviation lower than the mean.",
  "translatedText": "例如，这个条形对应的是掷出 10 个骰子并将其相加后得出的某个值，它的位置略高于负 1，表示该值比平均值低一个标准差。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1290.75,
  "end": 1303.87
 },
 {
  "input": "Also, by the way, in anticipation for the animation I'm trying to build to here, the way I'm representing things on that lower plot is that the area of each one of these bars is telling us the probability of the corresponding value rather than the height.",
  "translatedText": "提前说一句 我等下要在这里放的动画 下面的那个图里 表示概率的是柱子的面积而不是高度",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1305.13,
  "end": 1316.99
 },
 {
  "input": "You might think of the y-axis as representing not probability but a kind of probability density.",
  "translatedText": "也可以说成是 y 轴表示的不是概率而是概率密度",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.23,
  "end": 1321.93
 },
 {
  "input": "The reason for this is to set the stage so that it aligns with the way we interpret continuous distributions, where the probability of falling between a range of values is equal to an area under a curve between those values.",
  "translatedText": "这样可以给后面铺垫 和连续概率分布的图像表示相统一 也就是在随机变量取值落在一个区间内的概率 等于这个区间曲线下的面积",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1322.27,
  "end": 1333.55
 },
 {
  "input": "In particular, the area of all the bars together is going to be one.",
  "translatedText": "特别是，所有条形的面积加起来将是 1。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1333.91,
  "end": 1336.73
 },
 {
  "input": "Now, with all of that in place, let's have a little fun.",
  "translatedText": "讲了这么多 是时候来点好玩的了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1338.23,
  "end": 1340.95
 },
 {
  "input": "Let me start by rolling things back so that the distribution on the bottom represents a relatively small sum, like adding together only three such random variables.",
  "translatedText": "让我们往前倒一点 这样一来下面的分布只是少数几次加和 例如只是三个随机变量的和",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1341.33,
  "end": 1349.01
 },
 {
  "input": "Notice what happens as I change the distribution we start with.",
  "translatedText": "现在如果我换一个初始概率分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1349.45,
  "end": 1352.43
 },
 {
  "input": "As it changes, the distribution on the bottom completely changes its shape.",
  "translatedText": "注意下面的分布已经完全变了个样",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1352.73,
  "end": 1356.29
 },
 {
  "input": "It's very dependent on what we started with.",
  "translatedText": "说明它的形状受初始分布影响很大",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1356.51,
  "end": 1358.77
 },
 {
  "input": "If we let the size of our sum get a little bit bigger, say going up to 10, and as I change the distribution for x, it largely stays looking like a bell curve, but I can find some distributions that get it to change shape.",
  "translatedText": "如果我们增加随机变量的个数 加到十个 当我改变 X 的分布时 它基本上还像一个钟形曲线 但我可以找到些初始分布 使它改变形状",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.35,
  "end": 1371.63
 },
 {
  "input": "For example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results in this kind of spiky bell curve.",
  "translatedText": "例如，几乎所有的概率都在 1 或 6 中，这就形成了这种尖尖的钟形曲线。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1372.23,
  "end": 1379.31
 },
 {
  "input": "And if you'll recall, earlier on I actually showed this in the form of a simulation.",
  "translatedText": "如果你还记得，早些时候我实际上以模拟的形式展示了这一点。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1379.77,
  "end": 1383.51
 },
 {
  "input": "Though if you were wondering whether that spikiness was an artifact of the randomness or reflected the true distribution, turns out it reflects the true distribution.",
  "translatedText": "不过，如果你想知道这种 \"尖刺 \"是随机性的产物，还是反映了真实的分布情况，事实证明它反映了真实的分布情况。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1384.13,
  "end": 1391.85
 },
 {
  "input": "In this case, 10 is not a large enough sum for the central limit theorem to kick in.",
  "translatedText": "对这个例子来说 十个随机变量不足以满足中心极限定理",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1392.29,
  "end": 1396.47
 },
 {
  "input": "But if instead I let that sum grow and I consider adding 50 different values, which is actually not that big, then no matter how I change the distribution for our underlying random variable, it has essentially no effect on the shape of the plot on the bottom.",
  "translatedText": "但如果我让这个和增长到比如 50 个随机变量 实际上也不是很多 但这时候无论我如何改变随机变量的概率分布 它基本上对下面图里的形状没有影响",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1396.47,
  "end": 1410.69
 },
 {
  "input": "No matter where we start, all of the information and nuance for the distribution of x gets washed away, and we tend towards this single universal shape described by a very elegant function for the standard normal distribution, 1 over square root of 2 pi times e to the negative x squared over 2.",
  "translatedText": "无论初始分布如何 随机变量 X 的一切分布细节都会被抹去 趋近于这个统一的形状 即背后有着优雅函数的标准正态分布 *优雅的吟诵*",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1411.17,
  "end": 1427.07
 },
 {
  "input": "This, this right here is what the central limit theorem is all about.",
  "translatedText": "这就是中心极限定理的全部内容",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1427.81,
  "end": 1430.81
 },
 {
  "input": "Almost nothing you can do to this initial distribution changes the shape we tend towards.",
  "translatedText": "你无论对初始分布做什么 都没法改变它所趋近的形状",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1431.13,
  "end": 1435.31
 },
 {
  "input": "Now, the more theoretically minded among you might still be wondering what is the actual theorem, like what's the mathematical statement that could be proved or disproved that we're claiming here.",
  "translatedText": "现在，有理论头脑的人可能还在想，究竟什么是真正的定理，比如我们在这里声称的可以证明或反驳的数学语句是什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1439.03,
  "end": 1448.91
 },
 {
  "input": "If you want a nice formal statement, here's how it might go.",
  "translatedText": "那我就来给一个形式化的表述",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1449.03,
  "end": 1451.67
 },
 {
  "input": "Consider this value where we're summing up n different instantiations of our variable, but tweaked and tuned so that its mean and standard deviation are 1, again meaning you can read it as asking how many standard deviations away from the mean is the sum.",
  "translatedText": "考虑一下这个值，我们正在对变量的 n 个不同实例求和，但经过调整后，其平均值和标准差都是 1，也就是说，你可以把它理解为问平均值的标准差是多少。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1452.13,
  "end": 1465.35
 },
 {
  "input": "Then the actual rigorous no-jokes-this-time statement of the central limit theorem is that if you consider the probability that this value falls between two given real numbers, a and b, and you consider the limit of that probability as the size of your sum goes to infinity, then that limit is equal to a certain integral, which basically describes the area under a standard normal distribution between those two values.",
  "translatedText": "那么中心极限定理不开玩笑十分严谨的表述是这样的 如果你考虑一个值 落在两个给定的实数 a 和 b 之间的概率 从它极限的角度看 当 N 趋向于无穷大时 这个极限的值等于某个特定的积分 这个积分基本上就是 标准正态分布曲线下这两个值之间的面积",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1465.77,
  "end": 1489.65
 },
 {
  "input": "Again, there are three underlying assumptions that I have yet to tell you, but other than those, in all of its gory detail, this right here is the central limit theorem.",
  "translatedText": "再重复一次 有三个基本假设我还没有告诉你们 除了这些 就基本上是中心极限定理的所有细节了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1491.25,
  "end": 1500.03
 },
 {
  "input": "All of that is a bit theoretical, so it might be helpful to bring things back down to earth and turn back to the concrete example that I mentioned at the start, where you imagine rolling a die 100 times, and let's assume it's a fair die for this example, and you add together the results.",
  "translatedText": "所有这些都有点理论化了，所以我们不妨回到现实中来，回到我一开始提到的那个具体例子，你可以想象掷一个骰子 100 次，在这个例子中我们假设它是一个公平的骰子，然后把结果相加。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1504.55,
  "end": 1518.13
 },
 {
  "input": "The challenge for you is to find a range of values such that you're 95% sure that the sum will fall within this range.",
  "translatedText": "你现在要找到一个越小越好的范围 使得你有 95% 的把握保证点数和会落在这个范围内",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1518.87,
  "end": 1525.83
 },
 {
  "input": "For questions like this, there's a handy rule of thumb about normal distributions, which is that about 68% of your values are going to fall within one standard deviation of the mean, 95% of your values, the thing we care about, fall within two standard deviations of the mean, and a whopping 99.7% of your values will fall within three standard deviations of the mean.",
  "translatedText": "对于这类问题 正态分布有一个方便的经验法则 即约 68% 的值 会落在离均值一个标准差之内 95% 的值 刚好我们这里要用到 落在离均值两个标准差之内 而高达 99.7% 的值 会落在离均值三个标准差之内",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1527.13,
  "end": 1546.97
 },
 {
  "input": "It's a rule of thumb that's commonly memorized by people who do a lot of probability and stats.",
  "translatedText": "这是经常接触统计概率的人 耳熟能详的经验法则",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1547.45,
  "end": 1551.45
 },
 {
  "input": "Naturally, this gives us what we need for our example, and let me go ahead and draw out what this would look like, where I'll show the distribution for a fair die up at the top, and the distribution for a sum of 100 such dice on the bottom, which by now as you know looks like a certain normal distribution.",
  "translatedText": "而这自然也就给了我们的例子所需的东西 现在让我现在把它实际画出来 上面的图展示的是公平骰子的概率分布 底部展示 100 个这样的骰子点数和的分布 现在你知道 这看起来像某个正态分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1552.49,
  "end": 1567.29
 },
 {
  "input": "Step 1 with a problem like this is to find the mean of your initial distribution, which in this case will look like 1 6th times 1 plus 1 6th times 2 on and on and on, and works out to be 3.5.",
  "translatedText": "解决此类问题的第一步是找出初始分布的平均值，在本例中，平均值是 1 的 6 次方乘以 1 再加上 1 的 6 次方乘以 2，以此类推，结果是 3.5。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1567.95,
  "end": 1578.91
 },
 {
  "input": "We also need the standard deviation, which requires calculating the variance, which as you know involves adding all the squares of the differences between the values and the means, and it works out to be 2.92, square root of that comes out to be 1.71.",
  "translatedText": "我们还需要找到标准差 也就是首先需要计算方差 正如你所知 方差是(各个值与均值之差)的平方和 这里结果是 2.92 开平方后得到 1.71",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1579.41,
  "end": 1592.43
 },
 {
  "input": "Those are the only two numbers we need, and I will invite you again to reflect on how magical it is that those are the only two numbers you need to completely understand the bottom distribution.",
  "translatedText": "我们只需要这两个数字，我想请大家再思考一下，要完全理解底层分布，只需要这两个数字是多么神奇。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1592.95,
  "end": 1601.69
 },
 {
  "input": "Its mean will be 100 times mu, which is 350, and its standard deviation will be the square root of 100 times sigma, so 10 times sigma, 17.1.",
  "translatedText": "它的平均值是μ的 100 倍，即 350，标准差是σ的 100 倍的平方根，即σ的 10 倍，即 17.1。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1602.43,
  "end": 1612.61
 },
 {
  "input": "Remembering our handy rule of thumb, we're looking for values two standard deviations away from the mean, and when you subtract 2 sigma from mean, you end up with about 316, and when you add 2 sigma you end up with 384.",
  "translatedText": "记住我们方便的经验法则，我们要寻找的是与平均值相差两个标准差的值，当你从平均值减去 2 个西格玛时，最终得到的是约 316，而当你加上 2 个西格玛时，最终得到的是 384。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1613.03,
  "end": 1626.33
 },
 {
  "input": "There you go, that gives us the answer.",
  "translatedText": "这就是答案。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1627.35,
  "end": 1628.95
 },
 {
  "input": "Okay, I promised to wrap things up shortly, but while we're on this example, there's one more question that's worth your time to ponder.",
  "translatedText": "好了，我答应很快就结束，但在我们讨论这个例子的时候，还有一个问题值得你花时间思考。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1631.47,
  "end": 1637.45
 },
 {
  "input": "Instead of just asking about the sum of 100 die rolls, let's say I had you divide that number by 100, which basically means all the numbers in our diagram in the bottom get divided by 100.",
  "translatedText": "现在我不问投 100 次骰子的点数和了 而是让你将这个数字除以 100 这也就意味着画面下半部分中的所有数字都要除以 100",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1638.25,
  "end": 1648.09
 },
 {
  "input": "Take a moment to interpret what this all would be saying then.",
  "translatedText": "花点时间考虑一下这意味着什么",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1648.57,
  "end": 1651.57
 },
 {
  "input": "The expression essentially tells you the empirical average for 100 different die rolls, and that interval we found is now telling you what range you are expecting to see for that empirical average.",
  "translatedText": "这个表达式其实是在计算 掷 100 次骰子实际的平均点数 并且我们找的那个区间其实是 你预期觉得平均点数会落在的范围",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1652.07,
  "end": 1663.49
 },
 {
  "input": "In other words, you might expect it to be around 3.5, that's the expected value for a die roll, but what's much less obvious and what the central limit theorem lets you compute is how close to that expected value you'll reasonably find yourself.",
  "translatedText": "换句话说 你可能期望它在 3.5 左右 这是一个骰子掷出的期望值 但不那么明显的是 中心极限定理告诉你的是 合理范围内 你投的点数和期望值之间会差多少",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1664.35,
  "end": 1676.57
 },
 {
  "input": "In particular, it's worth your time to take a moment mulling over what the standard deviation for this empirical average is, and what happens to it as you look at a bigger and bigger sample of die rolls.",
  "translatedText": "这时就尤其值得你花点时间去弄清楚 这个实际平均点数的标准差有多大 以及当你掷越来越多的骰子时 标准差会怎么变",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1677.59,
  "end": 1687.13
 },
 {
  "input": "Lastly, but probably most importantly, let's talk about the assumptions that go into this theorem.",
  "translatedText": "最后 但可能最重要的是 让我们谈谈这个定理的假设",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1692.95,
  "end": 1697.41
 },
 {
  "input": "The first one is that all of these variables that we're adding up are independent from each other.",
  "translatedText": "第一个是我们相加的所有这些变量 是相互独立的",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1698.01,
  "end": 1702.53
 },
 {
  "input": "The outcome of one process doesn't influence the outcome of any other process.",
  "translatedText": "也就是一个过程的结果不会影响其他任何过程的结果",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1702.85,
  "end": 1706.31
 },
 {
  "input": "The second is that all of these variables are drawn from the same distribution.",
  "translatedText": "第二个是所有这些变量都遵循相同的分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1707.25,
  "end": 1710.95
 },
 {
  "input": "Both of these have been implicitly assumed with our dice example.",
  "translatedText": "在我们前面掷骰子的例子中 这两个假设都是隐含的",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1711.31,
  "end": 1714.39
 },
 {
  "input": "We've been treating the outcome of each die roll as independent from the outcome of all the others, and we're assuming that each die follows the same distribution.",
  "translatedText": "我们一直将每次掷骰子的结果 视为独立于剩下每次掷骰子的结果 并假设每个骰子遵循相同的分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1714.79,
  "end": 1722.03
 },
 {
  "input": "Sometimes in the literature you'll see these two assumptions lumped together under the initials IID for independent and identically distributed.",
  "translatedText": "有时在文献中 你会看到这两个假设合在一起 用首字母缩写 i.i.d 表示 独立同分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1722.85,
  "end": 1729.91
 },
 {
  "input": "One situation where these assumptions are decidedly not true would be the Galton board.",
  "translatedText": "一个这些假设明显不成立的情况 正是高尔顿板",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1730.53,
  "end": 1735.11
 },
 {
  "input": "I mean, think about it.",
  "translatedText": "我意思是 你想想看",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1735.71,
  "end": 1736.83
 },
 {
  "input": "Is it the case that the way a ball bounces off of one of the pegs is independent from how it's going to bounce off the next peg?",
  "translatedText": "一个球在一个钉子上弹开的方式 是否独立于它在下一个钉子上弹开的方式",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1736.97,
  "end": 1743.19
 },
 {
  "input": "Absolutely not.",
  "translatedText": "绝对不是",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1743.83,
  "end": 1744.61
 },
 {
  "input": "Depending on the last bounce, it's coming in with a completely different trajectory.",
  "translatedText": "取决于前一次的弹跳 之后它的轨迹也会完全不同",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1744.77,
  "end": 1747.87
 },
 {
  "input": "And is it the case that the distribution of possible outcomes off of each peg are the same for each peg that it hits?",
  "translatedText": "并且 小球在每一个钉子上的弹向分布 是否跟之前钉子的都一样？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1748.21,
  "end": 1754.67
 },
 {
  "input": "Again, almost certainly not.",
  "translatedText": "同样 几乎肯定不是",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1755.19,
  "end": 1756.71
 },
 {
  "input": "Maybe it hits one peg glancing to the left, meaning the outcomes are hugely skewed in that direction, and then hits the next one glancing to the right.",
  "translatedText": "也许它在一个钉子上向左斜着击中 这意味着弹跳的方向向左严重偏斜 然后在下一个钉子上向右斜着击中",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1756.71,
  "end": 1763.71
 },
 {
  "input": "When I made all those simplifying assumptions in the opening example, it wasn't just to make this easier to think about.",
  "translatedText": "在开篇的示例中 我做了所有假设来简化 并不仅仅是为了让它更容易思考",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1765.73,
  "end": 1771.63
 },
 {
  "input": "It's also that those assumptions were necessary for this to actually be an example of the central limit theorem.",
  "translatedText": "这些假设也是必要的 因为这实际上是中心极限定理的一个要求",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1771.97,
  "end": 1777.07
 },
 {
  "input": "Nevertheless, it seems to be true that for the real Galton board, despite violating both of these, a normal distribution does kind of come about?",
  "translatedText": "尽管如此 在真实的高尔顿板上 即使违反了这两个假设 正态分布似乎还是会出现……",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1778.13,
  "end": 1785.47
 },
 {
  "input": "Part of the reason might be that there are generalizations of the theorem beyond the scope of this video that relax these assumptions, especially the second one.",
  "translatedText": "部分原因是中心极限定理有更通用的形式 可以放宽这些假设 尤其是第二个假设",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1786.05,
  "end": 1793.89
 },
 {
  "input": "But I do want to caution you against the fact that many times people seem to assume that a variable is normally distributed, even when there's no actual justification to do so.",
  "translatedText": "但我确实想提醒你 很多时候 即使没有验证是否满足这些假设 人们似乎也会默认这个变量服从正态分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1794.49,
  "end": 1803.07
 },
 {
  "input": "The third assumption is actually fairly subtle.",
  "translatedText": "第三个假设实际上相当微妙",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1804.29,
  "end": 1806.21
 },
 {
  "input": "It's that the variance we've been computing for these variables is finite.",
  "translatedText": "也就是 这些变量的方差要求是有限的",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1806.21,
  "end": 1810.27
 },
 {
  "input": "This was never an issue for the dice example because there were only six possible outcomes.",
  "translatedText": "在掷骰子的例子中，这从来都不是问题，因为只有六种可能的结果。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1810.81,
  "end": 1814.85
 },
 {
  "input": "But in certain situations where you have an infinite set of outcomes, when you go to compute the variance, the sum ends up diverging off to infinity.",
  "translatedText": "但比如当结果的范围是无限的时候 你再去计算方差 总和有可能会发散至无穷",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1815.03,
  "end": 1822.51
 },
 {
  "input": "These can be perfectly valid probability distributions, and they do come up in practice.",
  "translatedText": "这类概率分布完全可能存在 在应用中也确实会出现",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1823.45,
  "end": 1827.25
 },
 {
  "input": "But in those situations, as you consider adding many different instantiations of that variable and letting that sum approach infinity, even if the first two assumptions hold, it is very much a possibility that the thing you tend towards is not actually a normal distribution.",
  "translatedText": "但是在这些情况下 当你把许多这样的随机变量相加 并且总和趋近于无穷大时 即使前两个假设成立 很有可能你最终得到的东西 实际上并不是正态分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1827.55,
  "end": 1841.19
 },
 {
  "input": "If you've understood everything up to this point, you now have a very strong foundation in what the central limit theorem is all about.",
  "translatedText": "如果你理解了到目前为止的所有内容 那么你现在对中心极限定理的本质 已经有了非常扎实的基础",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1842.15,
  "end": 1847.65
 },
 {
  "input": "And next up, I'd like to explain why it is that this particular function is the thing that we tend towards, and why it has a pi in it, what it has to do with circles.",
  "translatedText": "接下来，我想解释一下，为什么我们倾向于使用这个特殊函数，为什么它里面有一个π，它和圆有什么关系。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1848.29,
  "end": 1855.99
 },
 {
  "input": "Thank you.",
  "translatedText": "谢谢。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1871.95,
  "end": 1874.17
 }
]