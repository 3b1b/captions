[
 {
  "input": "This is a Galton board. ",
  "translatedText": "갈튼보드 입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 1.26
 },
 {
  "input": "Maybe you've seen one before, it's a popular demonstration of how, even when a single event is chaotic and random, with an effectively unknowable outcome, it's still possible to make precise statements about a large number of events, namely how the relative proportions for many different outcomes are distributed. ",
  "translatedText": "아마도 이전에 본 적이 있을 것입니다. 이는 단일 사건이 혼란스럽고 무작위적일 때에도 결과를 효과적으로 알 수 없는 경우에도 수많은 사건에 대해 정확한 진술을 하는 것이 어떻게 여전히 가능한지, 즉 상대적인 비율이 어떻게 되는지를 보여주는 대중적인 시연입니다. 다양한 결과가 분배되기 때문입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2.52,
  "end": 18.3
 },
 {
  "input": "More specifically, the Galton board illustrates one of the most prominent distributions in all of probability, known as the normal distribution, more colloquially known as a bell curve, and also called a Gaussian distribution. ",
  "translatedText": "보다 구체적으로 Galton 보드는 모든 확률에서 가장 눈에 띄는 분포 중 하나를 보여줍니다. 이는 정규 분포로 알려져 있으며 구어체로는 종형 곡선으로 알려져 있으며 가우시안 분포라고도 합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.38,
  "end": 31.9
 },
 {
  "input": "There's a very specific function to describe this distribution, it's very pretty, we'll get into it later, but right now I just want to emphasize how the normal distribution is, as the name suggests, very common, it shows up in a lot of seemingly unrelated contexts. ",
  "translatedText": "이 분포를 설명하는 매우 구체적인 함수가 있습니다. 매우 아름답습니다. 나중에 자세히 설명하겠습니다. 하지만 지금은 이름에서 알 수 있듯이 정규 분포가 얼마나 일반적이며 많이 나타나는지 강조하고 싶습니다. 전혀 관련없어 보이는 맥락. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 32.5,
  "end": 45.04
 },
 {
  "input": "If you were to take a large number of people who sit in a similar demographic and plot their heights, those heights tend to follow a normal distribution. ",
  "translatedText": "비슷한 인구통계에 속한 많은 사람들을 대상으로 키를 표시하면 해당 키는 정규 분포를 따르는 경향이 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.02,
  "end": 53.0
 },
 {
  "input": "If you look at a large swath of very big natural numbers and you ask how many distinct prime factors does each one of those numbers have, the answers will very closely track with a certain normal distribution. ",
  "translatedText": "매우 큰 자연수의 넓은 범위를 보고 해당 숫자 각각이 얼마나 많은 고유한 소인수를 가지고 있는지 묻는다면 대답은 특정 정규 분포를 매우 밀접하게 추적할 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 53.66,
  "end": 64.96
 },
 {
  "input": "Now our topic for today is one of the crown jewels in all of probability theory, it's one of the key facts that explains why this distribution is as common as it is, known as the central limit theorem. ",
  "translatedText": "이제 오늘 우리의 주제는 모든 확률 이론의 가장 중요한 보석 중 하나이며, 이 분포가 왜 그렇게 흔한지 설명하는 핵심 사실 중 하나인 중심 극한 정리입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.58,
  "end": 76.02
 },
 {
  "input": "This lesson is meant to go back to the basics, giving you the fundamentals on what the central limit theorem is saying, what normal distributions are, and I want to assume minimal background. ",
  "translatedText": "이 강의는 기본으로 돌아가서 중심 극한 정리가 말하는 내용, 정규 분포가 무엇인지에 대한 기본 사항을 제공하며 최소한의 배경 지식을 가정하고 싶습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.64,
  "end": 85.26
 },
 {
  "input": "We're going to go decently deep into it, but after this I'd still like to go deeper and explain why the theorem is true, why the function underlying the normal distribution has the very specific form that it does, why that formula has a pi in it, and, most fun, why those last two facts are actually more related than a lot of traditional explanations would suggest. ",
  "translatedText": "우리는 그것에 대해 상당히 깊이 들어갈 것이지만, 이 후에도 나는 여전히 더 깊이 들어가서 정리가 참인 이유, 정규 분포의 기초가 되는 함수가 왜 매우 특정한 형태를 가지고 있는지, 왜 그 공식이 그리고 가장 재미있는 점은 마지막 두 사실이 실제로 많은 전통적인 설명이 제안하는 것보다 더 관련이 있다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.26,
  "end": 105.56
 },
 {
  "input": "That second lesson is also meant to be the follow-on to the convolutions video that I promised, so there's a lot of interrelated topics here. ",
  "translatedText": "두 번째 강의는 제가 약속한 컨볼루션 비디오의 후속 강의이기도 하므로 여기에는 상호 연관된 주제가 많이 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.48,
  "end": 113.37
 },
 {
  "input": "But right now, back to the fundamentals, I'd like to kick things off with a overly simplified model of the Galton board. ",
  "translatedText": "하지만 지금은 기본으로 돌아가서 Galton 보드의 지나치게 단순화된 모델로 작업을 시작하고 싶습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.57,
  "end": 119.17
 },
 {
  "input": "In this model we will assume that each ball falls directly onto a certain central peg and that it has a 50-50 probability of bouncing to the left or to the right, and we'll think of each of those outcomes as either adding one or subtracting one from its position. ",
  "translatedText": "이 모델에서 우리는 각 공이 특정 중앙 말뚝에 직접 떨어지고 왼쪽이나 오른쪽으로 튕길 확률이 50-50이라고 가정합니다. 그 위치에서 하나를 뺍니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.89,
  "end": 134.11
 },
 {
  "input": "Once one of those is chosen, we make the highly unrealistic assumption that it happens to land dead on in the middle of the peg adjacent below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the right. ",
  "translatedText": "그 중 하나가 선택되면, 우리는 그것이 바로 아래에 있는 말뚝 중앙에 떨어져 착지할 것이라는 매우 비현실적인 가정을 합니다. 여기서 다시 왼쪽으로 튕겨 나갈 것인지, 아니면 왼쪽으로 튕겨 나갈 것인지에 대한 동일한 50-50 선택에 직면하게 될 것입니다. 오른쪽으로. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.67,
  "end": 147.07
 },
 {
  "input": "For the one I'm showing on screen, there are five different rows of pegs, so our little hopping ball makes five different random choices between plus one and minus one, and we can think of its final position as basically being the sum of all of those different numbers, which in this case happens to be one, and we might label all of the different buckets with the sum that they represent. ",
  "translatedText": "제가 화면에 보여주고 있는 것에는 5개의 서로 다른 행의 못이 있습니다. 따라서 우리의 작은 뛰어다니는 공은 +1과 -1 사이에서 다섯 가지 무작위 선택을 하며, 우리는 그것의 최종 위치를 기본적으로 모든 것의 합이라고 생각할 수 있습니다. 이 경우에는 1이 되는 서로 다른 숫자 중 하나이며, 우리는 그것이 나타내는 합계로 서로 다른 모든 버킷에 라벨을 붙일 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.43,
  "end": 166.35
 },
 {
  "input": "As we repeat this, we're looking at different possible sums for those five random numbers. ",
  "translatedText": "이것을 반복하면서 우리는 다섯 개의 난수에 대해 가능한 다양한 합계를 살펴봅니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 166.35,
  "end": 171.29
 },
 {
  "input": "And for those of you who are inclined to complain that this is a highly unrealistic model for the true Galton board, let me emphasize the goal right now is not to accurately model physics. ",
  "translatedText": "그리고 이것이 진정한 Galton 보드에 대한 매우 비현실적인 모델이라고 불평하는 경향이 있는 분들을 위해 지금의 목표는 물리학을 정확하게 모델링하는 것이 아니라는 점을 강조하고 싶습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.05,
  "end": 181.67
 },
 {
  "input": "The goal is to give a simple example to illustrate the central limit theorem, and for that, idealized though this might be, it actually gives us a really good example. ",
  "translatedText": "목표는 중심 극한 정리를 설명하기 위해 간단한 예를 제공하는 것이며, 이상화될 수도 있지만 실제로는 정말 좋은 예를 제공합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 181.83,
  "end": 190.03
 },
 {
  "input": "If we let many different balls fall, making yet another unrealistic assumption that they don't influence each other as if they're all ghosts, then the number of balls that fall into each different bucket gives us some loose sense for how likely each one of those buckets is. ",
  "translatedText": "만약 우리가 많은 다른 공들이 떨어지게 하고, 그것들이 모두 유령인 것처럼 서로 영향을 미치지 않는다는 또 다른 비현실적인 가정을 한다면, 각각의 서로 다른 양동이에 떨어지는 공의 수는 우리에게 각각의 공이 얼마나 가능성이 있는지에 대한 대략적인 감각을 제공합니다. 그 버킷 중 하나는 다음과 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.57,
  "end": 203.39
 },
 {
  "input": "In this example, the numbers are simple enough that it's not too hard to explicitly calculate what the probability is for falling into each bucket. ",
  "translatedText": "이 예에서는 숫자가 충분히 간단하므로 각 버킷에 포함될 확률을 명시적으로 계산하는 것이 그리 어렵지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.83,
  "end": 210.01
 },
 {
  "input": "If you do want to think that through, you'll find it very reminiscent of Pascal's triangle. ",
  "translatedText": "그것을 곰곰이 생각해 보면 파스칼의 삼각형이 매우 연상된다는 것을 알게 될 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.27,
  "end": 213.83
 },
 {
  "input": "But the neat thing about our theorem is how far it goes beyond the simple examples. ",
  "translatedText": "그러나 우리 정리의 멋진 점은 그것이 단순한 예를 얼마나 넘어서는가입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.95,
  "end": 218.27
 },
 {
  "input": "So to start off at least, rather than making explicit calculations, let's just simulate things by running a large number of samples and letting the total number of results in each different outcome give us some sense for what that distribution looks like. ",
  "translatedText": "따라서 최소한 명시적인 계산을 하기보다는 시작하기 위해 많은 수의 샘플을 실행하고 각각의 다른 결과의 총 결과 수를 통해 해당 분포가 어떤 모양인지 알 수 있도록 시뮬레이션해 보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.67,
  "end": 229.97
 },
 {
  "input": "As I said, the one on screen has five rows, so each sum that we're considering includes only five numbers. ",
  "translatedText": "앞서 말했듯이 화면에 있는 항목에는 5개의 행이 있으므로 우리가 고려하는 각 합계에는 5개의 숫자만 포함됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 230.45,
  "end": 236.21
 },
 {
  "input": "The basic idea of the central limit theorem is that if you increase the size of that sum, for example here that would mean increasing the number of rows of pegs for each ball to bounce off, then the distribution that describes where that sum is going to fall looks more and more like a bell curve. ",
  "translatedText": "중심 극한 정리의 기본 아이디어는 합의 크기를 늘리면(예를 들어 여기서는 각 공이 튕겨 나가는 못의 행 수를 늘리는 것을 의미함), 그러면 해당 합이 어디로 가는지 설명하는 분포가 된다는 것입니다. 가을은 점점 더 종형 곡선처럼 보입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.81,
  "end": 253.33
 },
 {
  "input": "Here, it's actually worth taking a moment to write down that general idea. ",
  "translatedText": "여기서는 실제로 잠시 시간을 내어 일반적인 아이디어를 적어 볼 가치가 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.47,
  "end": 258.35
 },
 {
  "input": "The setup is that we have a random variable, and that's basically shorthand for a random process where each outcome of that process is associated with some number. ",
  "translatedText": "설정은 무작위 변수를 가지고 있다는 것입니다. 이는 기본적으로 해당 프로세스의 각 결과가 일부 숫자와 연관되는 무작위 프로세스의 약어입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 259.27,
  "end": 268.19
 },
 {
  "input": "We'll call that random number x. ",
  "translatedText": "우리는 그 난수를 x라고 부르겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 268.49,
  "end": 269.97
 },
 {
  "input": "For example, each bounce off the peg is a random process modeled with two outcomes. ",
  "translatedText": "예를 들어, 페그에서 각각의 바운스는 두 가지 결과로 모델링된 무작위 프로세스입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.97,
  "end": 274.39
 },
 {
  "input": "Those outcomes are associated with the numbers negative one and positive one. ",
  "translatedText": "이러한 결과는 음수 1과 양수 1의 숫자와 연관되어 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.85,
  "end": 277.89
 },
 {
  "input": "Another example of a random variable would be rolling a die, where you have six different outcomes, each one associated with a number. ",
  "translatedText": "무작위 변수의 또 다른 예는 주사위를 굴리는 것입니다. 여기서는 각각 숫자와 연관된 6개의 서로 다른 결과가 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.53,
  "end": 284.83
 },
 {
  "input": "What we're doing is taking multiple different samples of that variable and adding them all together. ",
  "translatedText": "우리가 하고 있는 일은 해당 변수의 여러 다른 샘플을 채취하여 모두 합치는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.47,
  "end": 290.41
 },
 {
  "input": "On our Galton board, that looks like letting the ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice and adding up the results. ",
  "translatedText": "Galton 보드에서는 공이 바닥으로 내려가는 동안 여러 개의 다른 말뚝에서 튕겨 나가는 것처럼 보이며, 주사위의 경우 다양한 주사위를 굴려 결과를 합산하는 것을 상상할 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.77,
  "end": 300.97
 },
 {
  "input": "The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into different possible values, will look more and more like a bell curve. ",
  "translatedText": "중심 극한 정리의 주장은 합의 크기를 점점 더 크게 할수록 해당 합의 분포, 즉 다른 가능한 값에 속할 가능성이 점점 더 종형 곡선처럼 보일 것이라는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.43,
  "end": 314.11
 },
 {
  "input": "That's it, that is the general idea. ",
  "translatedText": "그것이 바로 일반적인 생각입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.43,
  "end": 317.13
 },
 {
  "input": "Over the course of this lesson, our job is to make that statement more quantitative. ",
  "translatedText": "이 수업이 진행되는 동안 우리의 임무는 해당 진술을 보다 정량적으로 만드는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 317.55,
  "end": 321.53
 },
 {
  "input": "We're going to put some numbers to it, put some formulas to it, show how you can use it to make predictions. ",
  "translatedText": "우리는 여기에 몇 가지 숫자를 입력하고, 몇 가지 공식을 입력하고, 이를 사용하여 예측하는 방법을 보여줄 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.07,
  "end": 326.35
 },
 {
  "input": "For example, here's the kind of question I want you to be able to answer by the end of this video. ",
  "translatedText": "예를 들어, 이 비디오가 끝날 때까지 여러분이 대답할 수 있기를 바라는 질문은 다음과 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.21,
  "end": 331.57
 },
 {
  "input": "Suppose you rolled the die 100 times and you added together the results. ",
  "translatedText": "주사위를 100번 굴리고 그 결과를 합산했다고 가정해 보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.19,
  "end": 335.89
 },
 {
  "input": "Could you find a range of values such that you're 95% sure that the sum will fall within that range? ",
  "translatedText": "합계가 해당 범위 내에 포함될 것이라고 95% 확신하는 값의 범위를 찾을 수 있습니까? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.63,
  "end": 342.17
 },
 {
  "input": "Or maybe I should say find the smallest possible range of values such that this is true. ",
  "translatedText": "아니면 이것이 참이 되도록 가능한 가장 작은 범위의 값을 찾아내야 할 수도 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.83,
  "end": 346.55
 },
 {
  "input": "The neat thing is you'll be able to answer this question whether it's a fair die or if it's a weighted die. ",
  "translatedText": "좋은 점은 그것이 공정한 주사위인지 가중 주사위인지 이 질문에 답할 수 있다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 347.39,
  "end": 352.13
 },
 {
  "input": "Now let me say at the top that this theorem has three different assumptions that go into it, three things that have to be true before the theorem follows. ",
  "translatedText": "이제 맨 위에서 이 정리에는 세 가지 다른 가정이 들어가고, 정리가 따르기 전에 세 가지가 참이어야 한다고 말씀드리겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.45,
  "end": 360.13
 },
 {
  "input": "And I'm actually not going to tell you what they are until the very end of the video. ",
  "translatedText": "그리고 저는 사실 영상이 끝날 때까지 그것이 무엇인지 말하지 않을 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 360.43,
  "end": 363.79
 },
 {
  "input": "Instead I want you to keep your eye out and see if you can notice and maybe predict what those three assumptions are going to be. ",
  "translatedText": "대신에 저는 여러분이 주의를 기울여 이 세 가지 가정이 무엇인지 알아차리고 예측할 수 있는지 확인하기를 바랍니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 364.27,
  "end": 369.67
 },
 {
  "input": "As a next step, to better illustrate just how general this theorem is, I want to run a couple more simulations for you focused on the dice example. ",
  "translatedText": "다음 단계로, 이 정리가 얼마나 일반적인지 더 잘 설명하기 위해 주사위 예제에 초점을 맞춘 몇 가지 시뮬레이션을 더 실행하고 싶습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.71,
  "end": 377.39
 },
 {
  "input": "Usually if you think of rolling a die you think of the six outcomes as being equally probable, but the theorem actually doesn't care about that. ",
  "translatedText": "일반적으로 주사위를 굴린다고 생각하면 여섯 가지 결과가 동일할 가능성이 있다고 생각하지만 정리에서는 실제로 그런 점에 관심을 두지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 380.91,
  "end": 387.63
 },
 {
  "input": "We could start with a weighted die, something with a non-trivial distribution across the outcomes, and the core idea still holds. ",
  "translatedText": "결과에 걸쳐 사소하지 않은 분포를 갖는 가중치 주사위로 시작할 수 있으며 핵심 아이디어는 여전히 유지됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 387.83,
  "end": 394.55
 },
 {
  "input": "For the simulation what I'll do is take some distribution like this one that is skewed towards lower values. ",
  "translatedText": "시뮬레이션을 위해 제가 할 일은 더 낮은 값으로 치우쳐 있는 이와 같은 분포를 취하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.03,
  "end": 399.93
 },
 {
  "input": "I'm going to take 10 distinct samples from that distribution and then I'll record the sum of that sample on the plot on the bottom. ",
  "translatedText": "해당 분포에서 10개의 서로 다른 샘플을 추출한 다음 해당 샘플의 합계를 하단 플롯에 기록하겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 400.25,
  "end": 407.55
 },
 {
  "input": "Then I'm going to do this many many different times, always with a sum of size 10, but keep track of where those sums ended up to give us a sense of the distribution. ",
  "translatedText": "그런 다음 항상 합계 크기가 10인 이 작업을 여러 번 수행할 것입니다. 그러나 분포에 대한 감각을 제공하기 위해 해당 합계가 어디에서 끝나는지 추적합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 408.63,
  "end": 416.59
 },
 {
  "input": "And in fact let me rescale the y direction to give us room to run an even larger number of samples. ",
  "translatedText": "실제로 더 많은 수의 샘플을 실행할 수 있는 공간을 제공하기 위해 y 방향의 크기를 다시 조정하겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.97,
  "end": 424.73
 },
 {
  "input": "And I'll let it go all the way up to a couple thousand, and as it does you'll notice that the shape that starts to emerge looks like a bell curve. ",
  "translatedText": "그리고 저는 그것을 최대 2,000까지 놔둘 것입니다. 그러면 나타나기 시작하는 모양이 종형 곡선처럼 보인다는 것을 알게 될 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.03,
  "end": 432.49
 },
 {
  "input": "Maybe if you squint your eyes you can see it skews a tiny bit to the left, but it's neat that something so symmetric emerged from a starting point that was so asymmetric. ",
  "translatedText": "눈을 가늘게 뜨면 왼쪽으로 약간 기울어져 있는 것을 볼 수 있지만, 너무 비대칭이었던 출발점에서 이렇게 대칭적인 것이 튀어나온 것은 정말 멋진 일입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.87,
  "end": 441.01
 },
 {
  "input": "To better illustrate what the central limit theorem is all about, let me run four of these simulations in parallel, where on the upper left I'm doing it where we're only adding two dice at a time, on the upper right we're doing it where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at a time, and then we'll do another one with a bigger sum, 15 at a time. ",
  "translatedText": "중심 극한 정리가 무엇인지 더 잘 설명하기 위해 이 시뮬레이션 중 4개를 병렬로 실행하겠습니다. 여기서 왼쪽 상단에서는 한 번에 두 개의 주사위만 추가하고 있고 오른쪽 상단에서는 한 번에 5개의 주사위를 추가하는 방식을 다시 사용하겠습니다. 왼쪽 아래는 한 번에 10개의 주사위를 추가하는 것을 본 것입니다. 그런 다음 더 큰 합을 사용하여 한 번에 15개의 주사위를 더 추가할 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 441.47,
  "end": 461.37
 },
 {
  "input": "Notice how on the upper left when we're just adding two dice, the resulting distribution doesn't really look like a bell curve, it looks a lot more reminiscent of the one we started with skewed towards the left. ",
  "translatedText": "왼쪽 상단에서 두 개의 주사위를 추가할 때 결과 분포가 실제로 종형 곡선처럼 보이지 않고 왼쪽으로 치우쳐 시작한 분포를 훨씬 더 연상시키는지 확인하세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.25,
  "end": 472.03
 },
 {
  "input": "But as we allow for more and more dice in each sum, the resulting shape that comes up in these distributions looks more and more symmetric. ",
  "translatedText": "그러나 각 합에 더 많은 주사위를 허용할수록 이러한 분포에서 나타나는 결과 모양은 점점 더 대칭적으로 보입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.81,
  "end": 479.81
 },
 {
  "input": "It has the lump in the middle and fade towards the tail's shape of a bell curve. ",
  "translatedText": "중앙에 덩어리가 있고 꼬리의 종형 곡선 모양으로 희미해집니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.95,
  "end": 483.89
 },
 {
  "input": "And let me emphasize again, you can start with any different distribution. ",
  "translatedText": "다시 한 번 강조하겠습니다. 다양한 배포판으로 시작할 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 487.05,
  "end": 490.49
 },
 {
  "input": "Here I'll run it again, but where most of the probability is tied up in the numbers 1 and 6, with very low probability for the mid values. ",
  "translatedText": "여기서 다시 실행해 보겠습니다. 하지만 대부분의 확률은 숫자 1과 6에 묶여 있고 중간 값의 확률은 매우 낮습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.49,
  "end": 497.49
 },
 {
  "input": "Despite completely changing the distribution for an individual roll of the die, it's still the case that a bell curve shape will emerge as we consider the different sums. ",
  "translatedText": "주사위의 개별 롤에 대한 분포가 완전히 변경되었음에도 불구하고 다른 합을 고려하면 여전히 종형 곡선 모양이 나타나는 경우가 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.19,
  "end": 506.55
 },
 {
  "input": "Illustrating things with a simulation like this is very fun, and it's kind of neat to see order emerge from chaos, but it also feels a little imprecise. ",
  "translatedText": "이런 시뮬레이션으로 사물을 묘사하는 것은 매우 재미있고, 혼돈에서 질서가 나타나는 것을 보는 것이 깔끔하지만 약간 부정확한 느낌도 듭니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.27,
  "end": 515.03
 },
 {
  "input": "Like in this case, when I cut off the simulation at 3000 samples, even though it kind of looks like a bell curve, the different buckets seem pretty spiky. ",
  "translatedText": "이 경우처럼 3000개 샘플에서 시뮬레이션을 끊으면 종형 곡선처럼 보이지만 서로 다른 버킷이 꽤 뾰족해 보입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 515.39,
  "end": 522.99
 },
 {
  "input": "And you might wonder, is it supposed to look that way, or is that just an artifact of the randomness in the simulation? ",
  "translatedText": "그리고 당신은 그것이 그렇게 보이도록 되어 있는 것인지, 아니면 시뮬레이션의 임의성에 따른 인공물인지 궁금할 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 522.99,
  "end": 528.55
 },
 {
  "input": "And if it is, how many samples do we need before we can be sure that what we're looking at is representative of the true distribution? ",
  "translatedText": "만약 그렇다면, 우리가 보고 있는 것이 실제 분포를 대표하는지 확인하려면 얼마나 많은 샘플이 필요합니까? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 529.01,
  "end": 535.11
 },
 {
  "input": "Instead moving forward, let's get a little more theoretical and show the precise shape that these distributions will take on in the long run. ",
  "translatedText": "앞으로 나아가는 대신 좀 더 이론적으로 접근하여 이러한 분포가 장기적으로 취할 정확한 형태를 보여드리겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.19,
  "end": 545.47
 },
 {
  "input": "The easiest case to make this calculation is if we have a uniform distribution, where each possible face of the die has an equal probability, 1 6th. ",
  "translatedText": "이 계산을 하는 가장 쉬운 경우는 주사위의 가능한 각 면이 1/6이라는 동일한 확률을 갖는 균일 분포를 갖는 경우입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.13,
  "end": 553.97
 },
 {
  "input": "For example, if you then want to know how likely different sums are for a pair of dice, it's essentially a counting game, where you count up how many distinct pairs take on the same sum, which in the diagram I've drawn, you can conveniently think about by going through all of the different diagonals. ",
  "translatedText": "예를 들어, 한 쌍의 주사위에 대해 합이 얼마나 다를 가능성이 있는지 알고 싶다면 이는 본질적으로 계산 게임입니다. 여기서는 동일한 합에 대해 몇 개의 서로 다른 쌍이 있는지 계산합니다. 제가 그린 다이어그램에서는 다양한 대각선을 모두 살펴보며 편리하게 생각할 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.99,
  "end": 568.49
 },
 {
  "input": "Since each such pair has an equal chance of showing up, 1 in 36, all you have to do is count the sizes of these buckets. ",
  "translatedText": "각 쌍이 나타날 확률은 36개 중 1개로 동일하므로, 여러분이 해야 할 일은 버킷의 크기를 세는 것뿐입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.41,
  "end": 577.53
 },
 {
  "input": "That gives us a definitive shape for the distribution describing a sum of two dice, and if we were to play the same game with all possible triplets, the resulting distribution would look like this. ",
  "translatedText": "이는 두 개의 주사위의 합을 설명하는 분포에 대한 최종 형태를 제공하며, 가능한 모든 트리플렛을 사용하여 동일한 게임을 한다면 결과 분포는 다음과 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.19,
  "end": 588.13
 },
 {
  "input": "Now what's more challenging, but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that single die. ",
  "translatedText": "이제 더 어렵지만 훨씬 더 흥미로운 것은 단일 다이에 대해 균일하지 않은 분포가 있으면 어떻게 되는지 묻는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 588.69,
  "end": 594.99
 },
 {
  "input": "We actually talked all about this in the last video. ",
  "translatedText": "사실 우리는 지난 영상에서 이 모든 것에 대해 이야기했습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.55,
  "end": 597.97
 },
 {
  "input": "You do essentially the same thing, you go through all the distinct pairs of dice which add up to the same value. ",
  "translatedText": "당신은 본질적으로 같은 일을 합니다. 당신은 같은 가치를 더하는 모든 별개의 주사위 쌍을 통과합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.45,
  "end": 603.67
 },
 {
  "input": "It's just that instead of counting those pairs, for each pair you multiply the two probabilities of each particular face coming up, and then you add all those together. ",
  "translatedText": "단지 그 쌍을 세는 대신, 각 쌍에 대해 각 특정 얼굴이 나타날 확률 두 개를 곱한 다음 그 모든 것을 더하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 603.97,
  "end": 612.75
 },
 {
  "input": "The computation that does this for all possible sums has a fancy name, it's called a convolution, but it's essentially just the weighted version of the counting game that anyone who's played with a pair of dice already finds familiar. ",
  "translatedText": "가능한 모든 합에 대해 이 작업을 수행하는 계산은 컨볼루션이라고 불리는 화려한 이름을 가지고 있지만 본질적으로 주사위 한 쌍을 가지고 플레이한 사람이라면 누구나 이미 익숙하다고 생각하는 계산 게임의 가중치 버전일 뿐입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 613.29,
  "end": 624.47
 },
 {
  "input": "For our purposes in this lesson, I'll have the computer calculate all that, simply display the results for you, and invite you to observe certain patterns, but under the hood, this is what's going on. ",
  "translatedText": "이 강의의 목적을 위해 컴퓨터가 모든 것을 계산하도록 하고, 단순히 결과를 표시하고, 특정 패턴을 관찰하도록 초대할 것입니다. 그러나 내부적으로는 이것이 진행되고 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.03,
  "end": 635.33
 },
 {
  "input": "So just to be crystal clear on what's being represented here, if you imagine sampling two different values from that top distribution, the one describing a single die, and adding them together, then the second distribution I'm drawing represents how likely you are to see various different sums. ",
  "translatedText": "따라서 여기에 표시되는 내용을 명확하게 설명하자면, 단일 다이를 설명하는 상위 분포에서 두 개의 서로 다른 값을 샘플링하고 이를 더하는 것을 상상한다면 제가 그리는 두 번째 분포는 다음과 같은 가능성을 나타냅니다. 다양한 금액을 확인하세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 636.65,
  "end": 652.23
 },
 {
  "input": "Likewise, if you imagine sampling three distinct values from that top distribution, and adding them together, the next plot represents the probabilities for various different sums in that case. ",
  "translatedText": "마찬가지로, 상위 분포에서 서로 다른 세 가지 값을 샘플링하여 이를 합산한다고 가정하면 다음 플롯은 해당 경우의 다양한 합계에 대한 확률을 나타냅니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 652.89,
  "end": 662.49
 },
 {
  "input": "So if I compute what the distributions for these sums look like for larger and larger sums, well you know what I'm going to say, it looks more and more like a bell curve. ",
  "translatedText": "따라서 이 합계에 대한 분포가 더 크고 더 큰 합계에 대해 어떻게 보이는지 계산하면 내가 무슨 말을 하려는지 알겠지만 점점 더 종형 곡선처럼 보입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 663.51,
  "end": 672.39
 },
 {
  "input": "But before we get to that, I want you to make a couple more simple observations. ",
  "translatedText": "하지만 그에 도달하기 전에 몇 가지 간단한 관찰을 더 해주시기 바랍니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.35,
  "end": 676.45
 },
 {
  "input": "For example, these distributions seem to be wandering to the right, and also they seem to be getting more spread out, and a little bit more flat. ",
  "translatedText": "예를 들어, 이러한 분포는 오른쪽으로 이동하는 것처럼 보이고, 또한 점점 더 분산되고 조금 더 평평해지는 것처럼 보입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.45,
  "end": 684.79
 },
 {
  "input": "You cannot describe the central limit theorem quantitatively without taking into account both of those effects, which in turn requires describing the mean and the standard deviation. ",
  "translatedText": "이러한 효과를 모두 고려하지 않고는 중심 극한 정리를 정량적으로 설명할 수 없으며, 이를 위해서는 평균과 표준 편차를 설명해야 합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.25,
  "end": 693.19
 },
 {
  "input": "Maybe you're already familiar with those, but I want to make minimal assumptions here, and it never hurts to review, so let's quickly go over both of those. ",
  "translatedText": "어쩌면 여러분은 이미 그것들에 익숙할 수도 있지만, 여기서는 최소한의 가정을 하고 싶고 검토하는 것도 나쁘지 않으므로 두 가지 모두 빠르게 살펴보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 693.95,
  "end": 700.61
 },
 {
  "input": "The mean of a distribution, often denoted with the Greek letter mu, is a way of capturing the center of mass for that distribution. ",
  "translatedText": "종종 그리스 문자 mu로 표시되는 분포의 평균은 해당 분포의 질량 중심을 포착하는 방법입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 703.41,
  "end": 710.71
 },
 {
  "input": "It's calculated as the expected value of our random variable, which is a way of saying you go through all of the different possible outcomes, and you multiply the probability of that outcome times the value of the variable. ",
  "translatedText": "이는 무작위 변수의 기대값으로 계산됩니다. 즉, 가능한 모든 결과를 거치고 해당 결과의 확률에 변수 값을 곱한다는 의미입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 711.19,
  "end": 722.85
 },
 {
  "input": "If higher values are more probable, that weighted sum is going to be bigger. ",
  "translatedText": "더 높은 값이 더 가능성이 높으면 해당 가중치 합계는 더 커질 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 723.19,
  "end": 726.41
 },
 {
  "input": "If lower values are more probable, that weighted sum is going to be smaller. ",
  "translatedText": "더 낮은 값이 더 가능성이 높으면 해당 가중치 합계는 더 작아집니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.75,
  "end": 729.95
 },
 {
  "input": "A little more interesting is if you want to measure how spread out this distribution is, because there's multiple different ways you might do it. ",
  "translatedText": "좀 더 흥미로운 점은 이 분포가 얼마나 퍼져 있는지 측정하려는 경우입니다. 이를 수행할 수 있는 방법은 여러 가지가 있기 때문입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.79,
  "end": 737.13
 },
 {
  "input": "One of them is called the variance. ",
  "translatedText": "그 중 하나를 분산이라고 합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.53,
  "end": 740.29
 },
 {
  "input": "The idea there is to look at the difference between each possible value and the mean, square that difference, and ask for its expected value. ",
  "translatedText": "가능한 각 값과 평균 사이의 차이를 살펴보고 그 차이를 제곱한 다음 예상 값을 묻는 것이 아이디어입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.83,
  "end": 748.27
 },
 {
  "input": "The idea is that whether your value is below or above the mean, when you square that difference, you get a positive number, and the larger the difference, the bigger that number. ",
  "translatedText": "값이 평균보다 낮든 높든 그 차이를 제곱하면 양수가 되고, 차이가 클수록 그 숫자도 커진다는 아이디어입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 748.73,
  "end": 756.65
 },
 {
  "input": "Squaring it like this turns out to make the math much much nicer than if we did something like an absolute value, but the downside is that it's hard to think about this as a distance in our diagram because the units are off. ",
  "translatedText": "이렇게 제곱하면 절대값과 같은 작업을 수행하는 것보다 수학이 훨씬 더 좋아지지만, 단위가 꺼져 있기 때문에 다이어그램에서 이를 거리로 생각하기 어렵다는 단점이 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.37,
  "end": 768.13
 },
 {
  "input": "Kind of like the units here are square units, whereas a distance in our diagram would be a kind of linear unit. ",
  "translatedText": "여기의 단위는 제곱 단위인 반면, 다이어그램의 거리는 일종의 선형 단위입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.33,
  "end": 773.31
 },
 {
  "input": "So another way to measure spread is what's called the standard deviation, which is the square root of this value. ",
  "translatedText": "따라서 스프레드를 측정하는 또 다른 방법은 이 값의 제곱근인 표준 편차입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 773.71,
  "end": 779.19
 },
 {
  "input": "That can be interpreted much more reasonably as a distance on our diagram, and it's commonly denoted with the Greek letter sigma, so you know m for mean as for standard deviation, but both in Greek. ",
  "translatedText": "이는 다이어그램에서 거리로 훨씬 더 합리적으로 해석될 수 있으며 일반적으로 그리스 문자 시그마로 표시됩니다. 따라서 m은 표준 편차와 평균으로 알지만 둘 다 그리스어로 알 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 779.47,
  "end": 789.65
 },
 {
  "input": "Looking back at our sequence of distributions, let's talk about the mean and standard deviation. ",
  "translatedText": "분포 순서를 다시 살펴보면서 평균과 표준편차에 대해 이야기해 보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.87,
  "end": 796.15
 },
 {
  "input": "If we call the mean of the initial distribution mu, which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if I tell you that the mean of the next one is 2 times mu. ",
  "translatedText": "초기 분포의 평균을 mu라고 부르면, 그림에서 2가 됩니다. 24, 다음의 평균이 2배 mu라고 말해도 별로 놀랄 일이 아니길 바랍니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.63,
  "end": 806.73
 },
 {
  "input": "That is, you roll a pair of dice, you want to know the expected value of the sum, it's two times the expected value for a single die. ",
  "translatedText": "즉, 주사위 두 개를 굴렸을 때 그 합계의 기대값을 알고 싶은 경우, 그 합계는 단일 주사위의 기대값의 2배입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 807.13,
  "end": 812.81
 },
 {
  "input": "Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth. ",
  "translatedText": "마찬가지로, 크기 3의 합에 대한 기대값은 3배 mu 등입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 813.85,
  "end": 819.41
 },
 {
  "input": "The mean just marches steadily on to the right, which is why our distributions seem to be drifting off in that direction. ",
  "translatedText": "평균은 꾸준히 오른쪽으로 이동하기 때문에 분포가 그 방향으로 표류하는 것처럼 보입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 819.63,
  "end": 824.87
 },
 {
  "input": "A little more challenging, but very important, is to describe how the standard deviation changes. ",
  "translatedText": "조금 더 어렵지만 매우 중요한 것은 표준 편차가 어떻게 변하는지 설명하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.35,
  "end": 829.91
 },
 {
  "input": "The key fact here is that if you have two different random variables, then the variance for the sum of those variables is the same as just adding together the original two variances. ",
  "translatedText": "여기서 중요한 사실은 두 개의 서로 다른 확률 변수가 있는 경우 해당 변수의 합계에 대한 분산은 원래 두 개의 분산을 더한 것과 동일하다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 830.49,
  "end": 839.37
 },
 {
  "input": "This is one of those facts that you can just compute when you unpack all the definitions. ",
  "translatedText": "이것은 모든 정의를 풀어보면 바로 계산할 수 있는 사실 중 하나입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 839.93,
  "end": 843.63
 },
 {
  "input": "There are a couple nice intuitions for why it's true. ",
  "translatedText": "왜 그것이 사실인지에 대한 몇 가지 좋은 직관이 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 843.63,
  "end": 846.21
 },
 {
  "input": "My tentative plan is to just actually make a series about probability and talk about things like intuitions underlying variance and its cousins there. ",
  "translatedText": "나의 임시 계획은 실제로 확률에 관한 시리즈를 만들고 분산과 그 사촌의 기초가 되는 직관과 같은 것에 대해 이야기하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 846.63,
  "end": 853.53
 },
 {
  "input": "But right now, the main thing I want you to highlight is how it's the variance that adds, it's not the standard deviation that adds. ",
  "translatedText": "하지만 지금 제가 강조하고 싶은 가장 중요한 점은 추가되는 것이 표준 편차가 아니라 분산이라는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 854.01,
  "end": 860.15
 },
 {
  "input": "So, critically, if you were to take n different realizations of the same random variable and ask what the sum looks like, the variance of that sum is n times the variance of your original variable, meaning the standard deviation, the square root of all this, is the square root of n times the original standard deviation. ",
  "translatedText": "따라서 비판적으로, 동일한 무작위 변수에 대해 n개의 서로 다른 구현을 취하고 합계가 어떻게 보이는지 묻는 경우 해당 합계의 분산은 원래 변수의 분산의 n배입니다. 즉, 표준 편차, 모든 것의 제곱근을 의미합니다. 이는 원래 표준편차의 n배의 제곱근입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 860.41,
  "end": 878.25
 },
 {
  "input": "For example, back in our sequence of distributions, if we label the standard deviation of our initial one with sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on and so forth. ",
  "translatedText": "예를 들어, 분포 순서로 돌아가서 초기 표준 편차를 시그마로 표시하면 다음 표준 편차는 2 곱하기 시그마의 제곱근이 되고 그 이후에는 다음의 제곱근처럼 보입니다. 3번 시그마 등등. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 879.29,
  "end": 893.09
 },
 {
  "input": "This, like I said, is very important. ",
  "translatedText": "내가 말했듯이 이것은 매우 중요합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.75,
  "end": 895.65
 },
 {
  "input": "It means that even though our distributions are getting spread out, they're not spreading out all that quickly, they only do so in proportion to the square root of the size of the sum. ",
  "translatedText": "이는 분포가 퍼져 나가더라도 그렇게 빨리 퍼지지는 않고 합계 크기의 제곱근에 비례해서만 퍼지는 것을 의미합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.07,
  "end": 904.13
 },
 {
  "input": "As we prepare to make a more quantitative description of the central limit theorem, the core intuition I want you to keep in your head is that we'll basically realign all of these distributions so that their means line up together, and then rescale them so that all of the standard deviations are just going to be equal to 1. ",
  "translatedText": "중심 극한 정리에 대한 보다 정량적인 설명을 준비하면서 여러분이 염두에 두셨으면 하는 핵심 직관은 기본적으로 모든 분포를 재정렬하여 평균이 함께 정렬되도록 한 다음 크기를 다시 조정한다는 것입니다. 모든 표준편차는 1이 될 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 904.71,
  "end": 920.61
 },
 {
  "input": "And when we do that, the shape that results gets closer and closer to a certain universal shape, described with an elegant little function that we'll unpack in just a moment. ",
  "translatedText": "그리고 우리가 그렇게 할 때, 그 결과로 나타나는 모양은 우리가 잠시 후에 풀 수 있는 우아하고 작은 기능으로 설명되는 어떤 보편적인 모양에 점점 더 가까워집니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 921.29,
  "end": 929.87
 },
 {
  "input": "And let me say one more time, the real magic here is how we could have started with any distribution, describing a single roll of the die, and if we play the same game, considering what the distributions for the many different sums look like, and we realign them so that the means line up, and we rescale them so that the standard deviations are all 1, we still approach that same universal shape, which is kind of mind-boggling. ",
  "translatedText": "다시 한 번 말씀드리지만, 여기서 진짜 마법은 주사위 하나를 굴리는 것을 설명하는 임의의 분포로 시작할 수 있다는 것입니다. 그리고 동일한 게임을 한다면 다양한 합계에 대한 분포가 어떻게 보이는지 고려하면, 그리고 우리는 평균이 정렬되도록 그것들을 재정렬하고 표준편차가 모두 1이 되도록 크기를 조정합니다. 우리는 여전히 동일한 보편적인 형태에 접근하는데, 이는 일종의 놀라운 일입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.47,
  "end": 952.95
 },
 {
  "input": "And now, my friends, is probably as good a time as any to finally get into the formula for a normal distribution. ",
  "translatedText": "그리고 이제 친구들이여, 아마도 마침내 정규 분포 공식에 들어갈 수 있는 가장 좋은 시기일 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 954.81,
  "end": 960.85
 },
 {
  "input": "And the way I'd like to do this is to basically peel back all the layers and build it up one piece at a time. ",
  "translatedText": "제가 하고 싶은 방식은 기본적으로 모든 레이어를 벗겨내고 한 번에 한 조각씩 쌓아가는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 961.49,
  "end": 965.93
 },
 {
  "input": "The function e to the x, or anything to the x, describes exponential growth, and if you make that exponent negative, which flips around the graph horizontally, you might think of it as describing exponential decay. ",
  "translatedText": "x에 대한 e 함수 또는 x에 대한 함수는 지수 증가를 설명하며 그래프를 수평으로 뒤집는 지수를 음수로 만들면 지수 붕괴를 설명하는 것으로 생각할 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.53,
  "end": 977.87
 },
 {
  "input": "To make this decay in both directions, you could do something to make sure the exponent is always negative and growing, like taking the negative absolute value. ",
  "translatedText": "이 붕괴를 양방향으로 수행하려면 음의 절대값을 취하는 것과 같이 지수가 항상 음수이고 증가하는지 확인하는 작업을 수행할 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.51,
  "end": 985.43
 },
 {
  "input": "That would give us this kind of awkward sharp point in the middle, but if instead you make that exponent the negative square of x, you get a smoother version of the same thing, which decays in both directions. ",
  "translatedText": "이렇게 하면 중간에 이런 종류의 어색한 뾰족한 점이 생기겠지만 대신 해당 지수를 x의 음의 제곱으로 만들면 양방향으로 붕괴되는 동일한 것의 더 부드러운 버전을 얻게 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.93,
  "end": 995.81
 },
 {
  "input": "This gives us the basic bell curve shape. ",
  "translatedText": "이것은 우리에게 기본적인 종형 곡선 모양을 제공합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 996.33,
  "end": 998.19
 },
 {
  "input": "Now if you throw a constant in front of that x, and you scale that constant up and down, it lets you stretch and squish the graph horizontally, allowing you to describe narrow and wider bell curves. ",
  "translatedText": "이제 x 앞에 상수를 놓고 해당 상수의 크기를 위아래로 조정하면 그래프를 수평으로 늘리거나 뭉개버릴 수 있어 좁고 넓은 종형 곡선을 설명할 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.65,
  "end": 1008.37
 },
 {
  "input": "And a quick thing I'd like to point out here is that based on the rules of exponentiation, as we tweak around that constant c, you could also think about it as simply changing the base of the exponentiation. ",
  "translatedText": "그리고 제가 여기서 지적하고 싶은 빠른 점은 지수화의 규칙에 기초하여 상수 c를 조정할 때 단순히 지수화의 밑을 변경하는 것으로 생각할 수도 있다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1009.01,
  "end": 1019.75
 },
 {
  "input": "And in that sense, the number e is not really all that special for our formula. ",
  "translatedText": "그리고 그런 의미에서 숫자 e는 실제로 우리 공식에서 그렇게 특별하지는 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1020.15,
  "end": 1023.63
 },
 {
  "input": "We could replace it with any other positive constant, and you'll get the same family of curves as we tweak that constant. ",
  "translatedText": "이를 다른 양의 상수로 대체할 수 있으며 해당 상수를 조정하면 동일한 곡선군을 얻게 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.05,
  "end": 1030.49
 },
 {
  "input": "Make it a 2, same family of curves. ",
  "translatedText": "동일한 곡선군 2개로 만듭니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.51,
  "end": 1033.11
 },
 {
  "input": "Make it a 3, same family of curves. ",
  "translatedText": "3개의 동일한 곡선군으로 만듭니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.33,
  "end": 1035.07
 },
 {
  "input": "The reason we use e is that it gives that constant a very readable meaning. ",
  "translatedText": "e를 사용하는 이유는 해당 상수에 매우 읽기 쉬운 의미를 부여하기 때문입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1035.75,
  "end": 1039.49
 },
 {
  "input": "Or rather, if we reconfigure things a little bit so that the exponent looks like negative one half times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn this into a probability distribution, that constant sigma will be the standard deviation of that distribution. ",
  "translatedText": "또는 오히려 지수가 음의 1/2 곱하기 x를 특정 상수로 나눈 것처럼 보이도록 조금 재구성하면(시그마 제곱이라고 부르겠습니다.) 일단 이것을 확률 분포로 바꾸면 상수 시그마는 해당 분포의 표준편차가 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.11,
  "end": 1057.21
 },
 {
  "input": "And that's very nice. ",
  "translatedText": "그리고 그것은 매우 좋은 일입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.81,
  "end": 1058.57
 },
 {
  "input": "But before we can interpret this as a probability distribution, we need the area under the curve to be 1. ",
  "translatedText": "그러나 이것을 확률 분포로 해석하려면 곡선 아래 면적이 1이 되어야 합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.91,
  "end": 1064.31
 },
 {
  "input": "And the reason for that is how the curve is interpreted. ",
  "translatedText": "그 이유는 곡선이 해석되는 방식에 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1064.83,
  "end": 1066.91
 },
 {
  "input": "Unlike discrete distributions, when it comes to something continuous, you don't ask about the probability of a particular point. ",
  "translatedText": "이산형 분포와 달리 연속적인 것에 대해서는 특정 지점의 확률을 묻지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1067.37,
  "end": 1073.37
 },
 {
  "input": "Instead, you ask for the probability that a value falls between two different values. ",
  "translatedText": "대신, 값이 서로 다른 두 값 사이에 포함될 확률을 묻습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1073.79,
  "end": 1078.23
 },
 {
  "input": "And what the curve is telling you is that that probability equals the area under the curve between those two values. ",
  "translatedText": "그리고 곡선이 말해주는 것은 그 확률이 두 값 사이의 곡선 아래 면적과 동일하다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.75,
  "end": 1085.43
 },
 {
  "input": "There's a whole other video about this, they're called probability density functions. ",
  "translatedText": "이에 대한 완전히 다른 비디오가 있습니다. 확률 밀도 함수라고 합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1086.03,
  "end": 1089.43
 },
 {
  "input": "The main point right now is that the area under the entire curve represents the probability that something happens, that some number comes up. ",
  "translatedText": "지금 중요한 점은 전체 곡선 아래의 영역이 어떤 일이 일어날 확률, 어떤 숫자가 나올 확률을 나타낸다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1089.83,
  "end": 1097.15
 },
 {
  "input": "That should be 1, which is why we want the area under this to be 1. ",
  "translatedText": "이는 1이어야 하며, 이것이 바로 우리가 이 아래의 면적을 1로 하려는 이유입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1097.41,
  "end": 1100.63
 },
 {
  "input": "As it stands with the basic bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root of pi. ",
  "translatedText": "e의 기본 종형 곡선 모양을 음의 x 제곱으로 나타내기 때문에 면적은 1이 아니라 실제로 파이의 제곱근입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1101.05,
  "end": 1107.79
 },
 {
  "input": "I know, right? ",
  "translatedText": "나도 알아, 그렇지? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.41,
  "end": 1109.15
 },
 {
  "input": "What is pi doing here? ",
  "translatedText": "파이는 여기서 무엇을 하고 있나요? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1109.27,
  "end": 1110.19
 },
 {
  "input": "What does this have to do with circles? ",
  "translatedText": "이것이 서클과 어떤 관련이 있나요? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1110.29,
  "end": 1111.47
 },
 {
  "input": "Like I said at the start, I'd love to talk all about that in the next video. ",
  "translatedText": "처음에 말했듯이, 다음 영상에서 그 모든 것에 대해 이야기하고 싶습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.01,
  "end": 1115.05
 },
 {
  "input": "But if you can spare your excitement for our purposes right now, all it means is that we should divide this function by the square root of pi, and it gives us the area we want. ",
  "translatedText": "하지만 지금 당장 우리의 목적을 위해 흥분을 금할 수 있다면, 그것은 우리가 이 함수를 파이의 제곱근으로 나누어야 하고 그것이 우리가 원하는 면적을 제공한다는 것을 의미합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.33,
  "end": 1123.17
 },
 {
  "input": "Throwing back in the constants we had earlier, the 1 half and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square root of 2. ",
  "translatedText": "이전에 사용했던 상수인 1/2와 시그마를 다시 적용하면 그래프가 시그마 곱하기 2의 제곱근만큼 늘어나는 효과가 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.61,
  "end": 1131.79
 },
 {
  "input": "So we also need to divide out by that in order to make sure it has an area of 1. ",
  "translatedText": "따라서 면적이 1이 되도록 이를 나누어야 합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.41,
  "end": 1136.47
 },
 {
  "input": "And combining those fractions, the factor out front looks like 1 divided by sigma times the square root of 2 pi. ",
  "translatedText": "그리고 그 분수들을 합치면, 앞의 인수는 1을 시그마 곱하기 2파이의 제곱근으로 나눈 것과 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1136.47,
  "end": 1142.11
 },
 {
  "input": "This, finally, is a valid probability distribution. ",
  "translatedText": "이것은 마지막으로 유효한 확률 분포입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1142.91,
  "end": 1145.85
 },
 {
  "input": "As we tweak that value sigma, resulting in narrower and wider curves, that constant in the front always guarantees that the area equals 1. ",
  "translatedText": "값 시그마를 조정하여 더 좁고 넓은 곡선을 만들 때 앞에 있는 상수는 항상 면적이 1과 같다는 것을 보장합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.45,
  "end": 1154.31
 },
 {
  "input": "The special case where sigma equals 1 has a specific name, we call it the standard normal distribution, which plays an especially important role for you and me in this lesson. ",
  "translatedText": "시그마가 1인 특별한 경우에는 특정한 이름이 있는데, 이를 표준 정규 분포라고 부르는데, 이는 이 강의에서 여러분과 저에게 특히 중요한 역할을 합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.91,
  "end": 1164.51
 },
 {
  "input": "And all possible normal distributions are not only parameterized with this value sigma, but we also subtract off another constant mu from the variable x, and this essentially just lets you slide the graph left and right so that you can prescribe the mean of this distribution. ",
  "translatedText": "그리고 가능한 모든 정규 분포는 이 값 시그마로 매개변수화될 뿐만 아니라 변수 x에서 또 다른 상수 mu를 뺍니다. 이는 기본적으로 그래프를 왼쪽과 오른쪽으로 슬라이드하여 이 분포의 평균을 규정할 수 있게 해줍니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.13,
  "end": 1180.21
 },
 {
  "input": "So in short, we have two parameters, one describing the mean, one describing the standard deviation, and they're all tied together in this big formula involving an e and a pi. ",
  "translatedText": "간단히 말해서, 우리는 평균을 설명하는 매개변수와 표준 편차를 설명하는 매개변수 두 개를 갖고 있으며, 이 매개변수는 모두 e와 pi를 포함하는 이 큰 공식으로 함께 묶여 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.99,
  "end": 1189.19
 },
 {
  "input": "Now that all of that is on the table, let's look back again at the idea of starting with some random variable and asking what the distributions for sums of that variable look like. ",
  "translatedText": "이제 모든 것이 테이블 위에 있으므로 임의의 변수로 시작하여 해당 변수의 합계에 대한 분포가 어떻게 생겼는지 묻는 아이디어를 다시 살펴보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.19,
  "end": 1199.81
 },
 {
  "input": "As we've already gone over, when you increase the size of that sum, the resulting distribution will shift according to a growing mean, and it slowly spreads out according to a growing standard deviation. ",
  "translatedText": "이미 설명한 것처럼 해당 합계의 크기를 늘리면 결과 분포는 평균 증가에 따라 이동하고 표준 편차 증가에 따라 천천히 퍼집니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1200.13,
  "end": 1209.81
 },
 {
  "input": "And putting some actual formulas to it, if we know the mean of our underlying random variable, we call it mu, and we also know its standard deviation, and we call it sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the standard deviation will be sigma times the square root of that size. ",
  "translatedText": "그리고 여기에 실제 공식을 대입하면 기본 확률 변수의 평균을 알고 이를 mu라고 부르며, 표준 편차도 알고 시그마라고 부르면 아래쪽 합계의 평균은 mu가 됩니다. 합의 크기를 곱한 값이고, 표준편차는 해당 크기의 제곱근에 시그마를 곱한 값이 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1210.33,
  "end": 1227.73
 },
 {
  "input": "So now, if we want to claim that this looks more and more like a bell curve, and a bell curve is only described by two different parameters, the mean and the standard deviation, you know what to do. ",
  "translatedText": "이제 이것이 점점 더 종형 곡선처럼 보이고 종형 곡선이 평균과 표준 편차라는 두 가지 다른 매개변수로만 설명된다고 주장하고 싶다면 무엇을 해야 할지 알 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1228.19,
  "end": 1237.71
 },
 {
  "input": "You could plug those two values into the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve that should closely fit our distribution. ",
  "translatedText": "이 두 값을 공식에 연결하면 분포에 꼭 맞는 곡선에 대한 매우 명확하고 복잡하기는 공식이 제공됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1237.93,
  "end": 1246.99
 },
 {
  "input": "But there's another way we can describe it that's a little more elegant and lends itself to a very fun visual that we can build up to. ",
  "translatedText": "하지만 좀 더 우아하고 우리가 구축할 수 있는 매우 재미있는 시각적 효과를 제공하는 또 다른 방법이 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1248.39,
  "end": 1254.81
 },
 {
  "input": "Instead of focusing on the sum of all of these random variables, let's modify this expression a little bit, where what we'll do is we'll look at the mean that we expect that sum to take, and we subtract it off so that our new expression has a mean of 0, and then we're going to look at the standard deviation we expect of our sum, and divide out by that, which basically just rescales the units so that the standard deviation of our expression will equal 1. ",
  "translatedText": "이러한 모든 확률 변수의 합에 초점을 맞추는 대신 이 표현식을 약간 수정해 보겠습니다. 여기서 우리가 할 일은 해당 합이 취할 것으로 예상되는 평균을 살펴보고 이를 빼는 것입니다. 새 표현식의 평균은 0입니다. 그런 다음 합계에 대해 기대하는 표준 편차를 살펴보고 이를 나누어 나누겠습니다. 기본적으로 표현식의 표준 편차가 1이 되도록 단위를 다시 조정합니다. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1255.27,
  "end": 1278.77
 },
 {
  "input": "This might seem like a more complicated expression, but it actually has a highly readable meaning. ",
  "translatedText": "좀 더 복잡한 표현처럼 보일 수도 있지만 실제로는 매우 읽기 쉬운 의미를 갖고 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1279.35,
  "end": 1284.09
 },
 {
  "input": "It's essentially saying how many standard deviations away from the mean is this sum? ",
  "translatedText": "이는 본질적으로 이 합계가 평균에서 몇 표준편차만큼 떨어져 있는지를 말하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1284.45,
  "end": 1289.67
 },
 {
  "input": "For example, this bar here corresponds to a certain value that you might find when you roll 10 dice and you add them all up, and its position a little above negative 1 is telling you that that value is a little bit less than one standard deviation lower than the mean. ",
  "translatedText": "예를 들어, 여기 있는 이 막대는 주사위 10개를 굴려 모두 더했을 때 찾을 수 있는 특정 값에 해당하며, 이 막대의 위치가 -1보다 약간 높다는 것은 해당 값이 1표준편차보다 약간 작다는 것을 의미합니다. 평균보다 낮습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.75,
  "end": 1303.87
 },
 {
  "input": "Also, by the way, in anticipation for the animation I'm trying to build to here, the way I'm representing things on that lower plot is that the area of each one of these bars is telling us the probability of the corresponding value rather than the height. ",
  "translatedText": "또한, 그런데, 제가 여기서 만들고자 하는 애니메이션에 대한 예상으로, 제가 아래쪽 플롯에서 사물을 표현하는 방식은 이 막대들 각각의 면적이 해당 값의 확률을 알려준다는 것입니다. 키보다는요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1305.13,
  "end": 1316.99
 },
 {
  "input": "You might think of the y-axis as representing not probability but a kind of probability density. ",
  "translatedText": "y축은 확률이 아니라 일종의 확률 밀도를 나타내는 것으로 생각할 수도 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.23,
  "end": 1321.93
 },
 {
  "input": "The reason for this is to set the stage so that it aligns with the way we interpret continuous distributions, where the probability of falling between a range of values is equal to an area under a curve between those values. ",
  "translatedText": "그 이유는 값 범위 사이에 속할 확률이 해당 값 사이의 곡선 아래 면적과 동일한 연속 분포를 해석하는 방식과 일치하도록 단계를 설정하기 위한 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1322.27,
  "end": 1333.55
 },
 {
  "input": "In particular, the area of all the bars together is going to be 1. ",
  "translatedText": "특히 모든 막대의 면적은 1이 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1333.91,
  "end": 1336.73
 },
 {
  "input": "Now, with all of that in place, let's have a little fun. ",
  "translatedText": "이제 모든 것이 준비되었으므로 조금 재미있게 놀아보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1338.23,
  "end": 1340.95
 },
 {
  "input": "Let me start by rolling things back so that the distribution on the bottom represents a relatively small sum, like adding together only three such random variables. ",
  "translatedText": "맨 아래의 분포가 상대적으로 작은 합계를 나타내도록 롤백하는 것부터 시작하겠습니다. 예를 들어 이러한 무작위 변수 3개만 더하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1341.33,
  "end": 1349.01
 },
 {
  "input": "Notice what happens as I change the distribution we start with. ",
  "translatedText": "시작하는 배포판을 변경하면 어떤 일이 발생하는지 확인하세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.45,
  "end": 1352.43
 },
 {
  "input": "As it changes, the distribution on the bottom completely changes its shape. ",
  "translatedText": "변화함에 따라 바닥의 분포도 모양이 완전히 변합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1352.73,
  "end": 1356.29
 },
 {
  "input": "It's very dependent on what we started with. ",
  "translatedText": "그것은 우리가 무엇을 시작했느냐에 따라 크게 달라집니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1356.51,
  "end": 1358.77
 },
 {
  "input": "If we let the size of our sum get a little bit bigger, say going up to 10, and as I change the distribution for x, it largely stays looking like a bell curve, but I can find some distributions that get it to change shape. ",
  "translatedText": "합계의 크기를 10까지 조금 더 크게 하고 x에 대한 분포를 변경하면 대체로 종형 곡선처럼 보이지만 모양이 바뀌는 분포를 찾을 수 있습니다. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.35,
  "end": 1371.63
 },
 {
  "input": "For example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results in this kind of spiky bell curve, and if you'll recall, earlier on I actually showed this in the form of a simulation. ",
  "translatedText": "예를 들어, 거의 모든 확률이 숫자 1 또는 6에 있는 매우 편향된 결과는 이런 종류의 뾰족한 종형 곡선을 생성하며, 기억하실지 모르겠지만 앞서 실제로 이것을 시뮬레이션의 형태로 보여주었습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1372.23,
  "end": 1383.51
 },
 {
  "input": "So if you were wondering whether that spikiness was an artifact of the randomness or reflected the true distribution, turns out it reflects the true distribution. ",
  "translatedText": "따라서 그 뾰족함이 무작위성의 인공물인지 아니면 실제 분포를 반영했는지 궁금하다면 그것이 실제 분포를 반영하는 것으로 밝혀졌습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1384.13,
  "end": 1391.85
 },
 {
  "input": "In this case, 10 is not a large enough sum for the central limit theorem to kick in. ",
  "translatedText": "이 경우, 10은 중심극한정리를 적용할 만큼 큰 합이 아닙니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.29,
  "end": 1396.47
 },
 {
  "input": "But if instead I let that sum grow and I consider adding 50 different values, which is actually not that big, then no matter how I change the distribution for our underlying random variable, it has essentially no effect on the shape of the plot on the bottom. ",
  "translatedText": "그러나 그 합계를 늘리고 실제로는 그다지 크지 않은 50개의 다른 값을 추가하는 것을 고려한다면 기본 무작위 변수의 분포를 어떻게 변경하더라도 기본적으로 플롯의 모양에는 아무런 영향을 미치지 않습니다. 맨 아래. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1396.47,
  "end": 1410.69
 },
 {
  "input": "No matter where we start, all of the information and nuance for the distribution of x gets washed away, and we tend towards this single universal shape described by a very elegant function for the standard normal distribution, 1 over square root of 2 pi times e to the negative x squared over 2. ",
  "translatedText": "어디에서 시작하든 x 분포에 대한 모든 정보와 미묘한 차이는 씻겨 나가고 표준 정규 분포에 대한 매우 우아한 함수인 1/2pi 곱하기 e로 설명되는 이 단일한 보편적인 모양을 지향하는 경향이 있습니다. 2에 대한 마이너스 x 제곱입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1411.17,
  "end": 1427.07
 },
 {
  "input": "This, this right here is what the central limit theorem is all about. ",
  "translatedText": "바로 이것이 중심 극한 정리의 전부입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1427.81,
  "end": 1430.81
 },
 {
  "input": "Almost nothing you can do to this initial distribution changes the shape we tend towards. ",
  "translatedText": "이 초기 분포에 대해 할 수 있는 일은 거의 아무것도 우리가 지향하는 모양을 변경하지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1431.13,
  "end": 1435.31
 },
 {
  "input": "Now, the more theoretically minded among you might still be wondering, what is the actual theorem? ",
  "translatedText": "자, 이론적으로 생각하는 분들은 여전히 실제 정리가 무엇인지 궁금해하실 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1439.03,
  "end": 1444.51
 },
 {
  "input": "Like, what's the mathematical statement that could be proved or disproved that we're claiming here? ",
  "translatedText": "예를 들어, 우리가 여기서 주장하는 것을 증명하거나 반증할 수 있는 수학적 진술은 무엇입니까? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1444.81,
  "end": 1448.91
 },
 {
  "input": "If you want a nice formal statement, here's how it might go. ",
  "translatedText": "멋진 공식적인 진술을 원한다면 다음과 같이 하세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1449.03,
  "end": 1451.67
 },
 {
  "input": "Consider this value, where we're summing up n different instantiations of our random variable, but tweaked and tuned so that its mean and standard deviation are 1. ",
  "translatedText": "이 값을 고려하면 무작위 변수의 n개의 서로 다른 인스턴스화를 합산하지만 평균과 표준 편차가 1이 되도록 조정하고 조정합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1452.13,
  "end": 1459.89
 },
 {
  "input": "Again, meaning you can read it as asking how many standard deviations away from the mean is the sum. ",
  "translatedText": "다시 말하지만, 평균에서 얼마나 많은 표준 편차가 합인지 묻는 것으로 읽을 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1460.23,
  "end": 1465.35
 },
 {
  "input": "Then the actual rigorous no-jokes-this-time statement of the central limit theorem is that if you consider the probability that this value falls between two given real numbers, a and b, and you consider the limit of that probability as the size of your sum goes to infinity, then that limit is equal to a certain integral, which basically describes the area under a standard normal distribution between those two values. ",
  "translatedText": "그러면 중심 극한 정리에 대한 실제 엄밀한 농담이 아닌 진술은 이 값이 주어진 두 실수 a와 b 사이에 속할 확률을 고려하고 그 확률의 한계를 다음의 크기로 간주한다는 것입니다. 합계가 무한대가 되면 그 한계는 기본적으로 두 값 사이의 표준 정규 분포 아래 영역을 설명하는 특정 적분과 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1465.77,
  "end": 1489.65
 },
 {
  "input": "Again, there are three underlying assumptions that I have yet to tell you, but other than those, in all of its gory detail, this right here is the central limit theorem. ",
  "translatedText": "다시 말하지만, 제가 아직 여러분에게 말하지 않은 세 가지 기본 가정이 있습니다. 그러나 그 외에도 모든 피투성이의 세부 사항은 바로 여기에 중심 극한 정리가 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1491.25,
  "end": 1500.03
 },
 {
  "input": "All of that is a bit theoretical, so it might be helpful to bring things back down to Earth and turn back to the concrete example that I mentioned at the start, where you imagine rolling a die 100 times, and let's assume it's a fair die for this example, and you add together the results. ",
  "translatedText": "그 모든 것은 다소 이론적인 것이므로, 다시 지구로 돌아가서 제가 처음에 언급한 구체적인 예로 돌아가서 주사위를 100번 굴리는 것을 상상하고 그것이 공정한 주사위라고 가정하는 것이 도움이 될 수 있습니다. 이 예에서는 결과를 합산합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1504.55,
  "end": 1518.13
 },
 {
  "input": "The challenge for you is to find a range of values such that you're 95% sure that the sum will fall within this range. ",
  "translatedText": "당신이 직면한 과제는 합계가 이 범위 내에 포함될 것이라고 95% 확신하는 값의 범위를 찾는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1518.87,
  "end": 1525.83
 },
 {
  "input": "For questions like this, there's a handy rule of thumb about normal distributions, which is that about 68% of your values are going to fall within one standard deviation of the mean, 95% of your values, the thing we care about, fall within two standard deviations of the mean, and a whopping 99.7% of your values will fall within three standard deviations of the mean. ",
  "translatedText": "이와 같은 질문에는 정규 분포에 대한 편리한 경험 법칙이 있습니다. 즉, 값의 약 68%가 평균의 1표준편차 내에 속하고, 우리가 관심을 갖는 값의 95%가 평균의 1표준편차 내에 속한다는 것입니다. 평균의 표준편차는 2개이고 무려 99입니다. 값의 7%는 평균의 3표준편차 내에 속합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1527.13,
  "end": 1546.97
 },
 {
  "input": "It's a rule of thumb that's commonly memorized by people who do a lot of probability and stats. ",
  "translatedText": "확률과 통계를 많이 다루는 사람들이 흔히 외우는 경험 법칙입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1547.45,
  "end": 1551.45
 },
 {
  "input": "Naturally, this gives us what we need for our example, and let me go ahead and draw out what this would look like, where I'll show the distribution for a fair die up at the top, and the distribution for a sum of 100 such dice on the bottom, which by now as you know looks like a certain normal distribution. ",
  "translatedText": "당연히 이것은 우리의 예에 필요한 것을 제공합니다. 계속해서 이것이 어떻게 생겼는지 그려 보겠습니다. 공정한 주사위의 분포는 상단에 표시되고 합계 100에 대한 분포는 표시됩니다. 바닥에 주사위가 있는데, 지금쯤이면 어떤 정규 분포처럼 보입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.49,
  "end": 1567.29
 },
 {
  "input": "Step one with a problem like this is to find the mean of your initial distribution, which in this case will look like 1 6th times 1 plus 1 6th times 2 on and on and on, and works out to be 3.5. ",
  "translatedText": "이와 같은 문제의 첫 번째 단계는 초기 분포의 평균을 찾는 것입니다. 이 경우 평균은 1 6 곱하기 1 더하기 1 6 곱하기 2 처럼 보이고 계속해서 3이 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1567.95,
  "end": 1578.91
 },
 {
  "input": "We also need the standard deviation, which requires calculating the variance, which as you know involves adding all the squares of the differences between the values and the means, and it works out to be 2.92, square root of that comes out to be 1.71. ",
  "translatedText": "5. 우리는 또한 분산을 계산해야 하는 표준편차가 필요합니다. 아시다시피 값과 평균 사이의 차이의 모든 제곱을 더하면 2가 됩니다. 92, 그 제곱근은 1이 됩니다. 71. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1579.41,
  "end": 1592.43
 },
 {
  "input": "Those are the only two numbers we need, and I will invite you again to reflect on how magical it is that those are the only two numbers that you need to completely understand the bottom distribution. ",
  "translatedText": "이것들은 우리에게 필요한 유일한 두 숫자입니다. 그리고 이 두 숫자가 하위 분포를 완전히 이해하는 데 필요한 유일한 숫자라는 것이 얼마나 마법적인지 다시 한 번 생각해 보시기 바랍니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1592.95,
  "end": 1601.69
 },
 {
  "input": "Its mean will be 100 times mu, which is 350, and its standard deviation will be the square root of 100 times sigma, so 10 times sigma 17.1. ",
  "translatedText": "평균은 100 곱하기 mu, 즉 350이고, 표준 편차는 100 곱하기 시그마의 제곱근이므로 10 곱하기 시그마 17이 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1602.43,
  "end": 1612.61
 },
 {
  "input": "Remembering our handy rule of thumb, we're looking for values two standard deviations away from the mean, and when you subtract 2 sigma from the mean you end up with about 316, and when you add 2 sigma you end up with 384. ",
  "translatedText": "1. 편리한 경험 법칙을 기억하면, 평균에서 2 표준편차 떨어진 값을 찾고, 평균에서 2 시그마를 빼면 약 316이 되고, 2 시그마를 더하면 384가 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1613.03,
  "end": 1626.33
 },
 {
  "input": "And there you go, that gives us the answer. ",
  "translatedText": "그리고 거기에 답이 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1627.35,
  "end": 1628.95
 },
 {
  "input": "Okay, I promised to wrap things up shortly, but while we're on this example there's one more question that's worth your time to ponder. ",
  "translatedText": "좋습니다. 곧 마무리하겠다고 약속했습니다. 하지만 이 예를 진행하는 동안 잠시 생각해 볼 가치가 있는 질문이 하나 더 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.47,
  "end": 1637.45
 },
 {
  "input": "Instead of just asking about the sum of 100 die rolls, let's say I had you divide that number by 100, which basically means all the numbers in our diagram in the bottom get divided by 100. ",
  "translatedText": "단지 100개의 주사위 굴림의 합을 묻는 대신, 그 숫자를 100으로 나누라고 했다고 가정해 보겠습니다. 이는 기본적으로 하단에 있는 다이어그램의 모든 숫자가 100으로 나누어진다는 의미입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1638.25,
  "end": 1648.09
 },
 {
  "input": "Take a moment to interpret what this all would be saying then. ",
  "translatedText": "그러면 이 모든 것이 무엇을 말하는지 잠시 시간을 내어 해석해 보십시오. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1648.57,
  "end": 1651.57
 },
 {
  "input": "The expression essentially tells you the empirical average for 100 different die rolls, and that interval we found is now telling you what range you are expecting to see for that empirical average. ",
  "translatedText": "이 표현식은 본질적으로 100개의 서로 다른 주사위 굴림에 대한 경험적 평균을 알려주며, 우리가 찾은 간격은 이제 해당 경험적 평균에 대해 어떤 범위를 볼 것으로 예상하는지 알려줍니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1652.07,
  "end": 1663.49
 },
 {
  "input": "In other words, you might expect it to be around 3.5, that's the expected value for a die roll, but what's much less obvious and what the central limit theorem lets you compute is how close to that expected value you'll reasonably find yourself. ",
  "translatedText": "즉, 대략 3 정도가 될 것으로 예상할 수 있습니다. 5, 그것은 주사위 굴림에 대한 기대값이지만 훨씬 덜 명확하고 중심 극한 정리를 통해 계산할 수 있는 것은 합리적으로 찾을 수 있는 기대값에 얼마나 가까운지입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1664.35,
  "end": 1676.57
 },
 {
  "input": "In particular, it's worth your time to take a moment mulling over what the standard deviation for this empirical average is, and what happens to it as you look at a bigger and bigger sample of die rolls. ",
  "translatedText": "특히, 이 경험적 평균의 표준 편차가 무엇인지, 그리고 점점 더 큰 주사위 굴림 샘플을 볼 때 표준 편차가 어떻게 되는지 잠시 생각해 보는 것은 시간을 할애할 가치가 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1677.59,
  "end": 1687.13
 },
 {
  "input": "Lastly, but probably most importantly, let's talk about the assumptions that go into this theorem. ",
  "translatedText": "마지막으로 아마도 가장 중요한 것은 이 정리에 적용되는 가정에 대해 이야기해 보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.95,
  "end": 1697.41
 },
 {
  "input": "The first one is that all of these variables that we're adding up are independent from each other. ",
  "translatedText": "첫 번째는 우리가 합산하는 모든 변수가 서로 독립적이라는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1698.01,
  "end": 1702.53
 },
 {
  "input": "The outcome of one process doesn't influence the outcome of any other process. ",
  "translatedText": "한 프로세스의 결과는 다른 프로세스의 결과에 영향을 미치지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.85,
  "end": 1706.31
 },
 {
  "input": "The second is that all of these variables are drawn from the same distribution. ",
  "translatedText": "두 번째는 이러한 변수가 모두 동일한 분포에서 추출된다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1707.25,
  "end": 1710.95
 },
 {
  "input": "Both of these have been implicitly assumed with our dice example. ",
  "translatedText": "이 두 가지 모두 우리의 주사위 예에서 암시적으로 가정되었습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1711.31,
  "end": 1714.39
 },
 {
  "input": "We've been treating the outcome of each die roll as independent from the outcome of all the others, and we're assuming that each die follows the same distribution. ",
  "translatedText": "우리는 각 주사위 굴림의 결과를 다른 모든 주사위의 결과와 독립적인 것으로 취급해 왔으며 각 주사위가 동일한 분포를 따른다고 가정합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1714.79,
  "end": 1722.03
 },
 {
  "input": "Sometimes in the literature you'll see these two assumptions lumped together under the initials IID for independent and identically distributed. ",
  "translatedText": "때로는 문헌에서 이 두 가지 가정이 독립적이고 동일하게 분포된다는 이니셜 IID 아래에 함께 묶인 것을 볼 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1722.85,
  "end": 1729.91
 },
 {
  "input": "One situation where these assumptions are decidedly not true would be the Galton board. ",
  "translatedText": "이러한 가정이 확실히 사실이 아닌 상황 중 하나는 Galton 보드입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1730.53,
  "end": 1735.11
 },
 {
  "input": "I mean, think about it. ",
  "translatedText": "내 말은, 생각해 보세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.71,
  "end": 1736.83
 },
 {
  "input": "Is it the case that the way a ball bounces off of one of the pegs is independent from how it's going to bounce off the next peg? ",
  "translatedText": "공이 말뚝 중 하나에서 튕겨 나가는 방식이 다음 말뚝에서 튕겨 나가는 방식과 독립적인 경우인가요? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1736.97,
  "end": 1743.19
 },
 {
  "input": "Absolutely not. ",
  "translatedText": "절대적으로하지. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1743.83,
  "end": 1744.61
 },
 {
  "input": "Depending on the last bounce, it's coming in with a completely different trajectory. ",
  "translatedText": "마지막 바운스에 따라 완전히 다른 궤적으로 들어옵니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1744.77,
  "end": 1747.87
 },
 {
  "input": "And is it the case that the distribution of possible outcomes off of each peg are the same for each peg that it hits? ",
  "translatedText": "그리고 각 페그에서 가능한 결과의 분포가 각 페그에 대해 동일한 경우입니까? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1748.21,
  "end": 1754.67
 },
 {
  "input": "Again, almost certainly not. ",
  "translatedText": "다시 말하지만, 거의 확실하지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1755.19,
  "end": 1756.71
 },
 {
  "input": "Maybe it hits one peg glancing to the left, meaning the outcomes are hugely skewed in that direction, and then hits the next one glancing to the right. ",
  "translatedText": "어쩌면 왼쪽으로 기울어진 하나의 페그에 부딪힐 수도 있습니다. 즉, 결과가 해당 방향으로 크게 치우쳐 있고, 오른쪽으로 기울어진 다음 페그에 부딪힐 수도 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1756.71,
  "end": 1763.71
 },
 {
  "input": "When I made all those simplifying assumptions in the opening example, it wasn't just to make this easier to think about. ",
  "translatedText": "첫 번째 예에서 내가 모든 단순화된 가정을 만들었을 때, 단지 이것을 생각하기 쉽게 만들기 위한 것이 아니었습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1765.73,
  "end": 1771.63
 },
 {
  "input": "It's also that those assumptions were necessary for this to actually be an example of the central limit theorem. ",
  "translatedText": "또한 이것이 실제로 중심 극한 정리의 예가 되기 위해서는 이러한 가정이 필요했습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1771.97,
  "end": 1777.07
 },
 {
  "input": "Nevertheless, it seems to be true that for the real Galton board, despite violating both of these, a normal distribution does kind of come about? ",
  "translatedText": "그럼에도 불구하고 실제 Galton 보드에서는 이 두 가지를 모두 위반함에도 불구하고 정규 분포가 나타나는 것이 사실인 것 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1778.13,
  "end": 1785.47
 },
 {
  "input": "Part of the reason might be that there are generalizations of the theorem beyond the scope of this video that relax these assumptions, especially the second one. ",
  "translatedText": "그 이유 중 하나는 이러한 가정, 특히 두 번째 가정을 완화하는 이 비디오의 범위를 넘어서는 정리의 일반화가 있기 때문일 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1786.05,
  "end": 1793.89
 },
 {
  "input": "But I do want to caution you against the fact that many times people seem to assume that a variable is normally distributed, even when there's no actual justification to do so. ",
  "translatedText": "그러나 실제로 그럴 만한 정당성이 없음에도 불구하고 사람들이 변수가 정규 분포를 따른다고 가정하는 경우가 많다는 사실에 대해 주의를 드리고 싶습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1794.49,
  "end": 1803.07
 },
 {
  "input": "The third assumption is actually fairly subtle. ",
  "translatedText": "세 번째 가정은 실제로 상당히 미묘합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1804.29,
  "end": 1806.21
 },
 {
  "input": "It's that the variance we've been computing for these variables is finite. ",
  "translatedText": "우리가 이러한 변수에 대해 계산한 분산은 유한합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1806.21,
  "end": 1810.27
 },
 {
  "input": "This was never an issue for the dice example, because there were only six possible outcomes. ",
  "translatedText": "주사위 예시에서는 가능한 결과가 6개뿐이었기 때문에 이는 결코 문제가 되지 않았습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1810.81,
  "end": 1814.85
 },
 {
  "input": "But in certain situations where you have an infinite set of outcomes, when you go to compute the variance, the sum ends up diverging off to infinity. ",
  "translatedText": "그러나 결과 집합이 무한한 특정 상황에서 분산을 계산하면 합계가 무한대로 발산됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1815.03,
  "end": 1822.51
 },
 {
  "input": "These can be perfectly valid probability distributions, and they do come up in practice. ",
  "translatedText": "이는 완벽하게 유효한 확률 분포일 수 있으며 실제로 나타납니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1823.45,
  "end": 1827.25
 },
 {
  "input": "But in those situations, as you consider adding many different instantiations of that variable and letting that sum approach infinity, even if the first two assumptions hold, it is very much a possibility that the thing you tend towards is not actually a normal distribution. ",
  "translatedText": "그러나 이러한 상황에서는 해당 변수의 다양한 인스턴스화를 추가하고 그 합계가 무한대에 접근하도록 고려하면 처음 두 가정이 유지되더라도 경향이 있는 것이 실제로 정규 분포가 아닐 가능성이 매우 높습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1827.55,
  "end": 1841.19
 },
 {
  "input": "If you've understood everything up to this point, you now have a very strong foundation in what the central limit theorem is all about. ",
  "translatedText": "지금까지 모든 내용을 이해했다면 이제 중심 극한 정리에 대한 매우 강력한 기초를 다진 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1842.15,
  "end": 1847.65
 },
 {
  "input": "And next up, I'd like to explain why it is that this particular function is the thing that we tend towards, and why it has a pi in it, what it has to do with circles. ",
  "translatedText": "다음으로, 왜 이 특정 함수가 우리가 지향하는 것인지, 왜 파이가 포함되어 있는지, 원과 어떤 관련이 있는지 설명하고 싶습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1848.29,
  "end": 1874.17
 }
]