[
 {
  "input": "This is a Galton board. ",
  "translatedText": "זה לוח של גלטון. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 1.26
 },
 {
  "input": "Maybe you've seen one before, it's a popular demonstration of how, even when a single event is chaotic and random, with an effectively unknowable outcome, it's still possible to make precise statements about a large number of events, namely how the relative proportions for many different outcomes are distributed. ",
  "translatedText": "אולי ראיתם אחד בעבר, זו הדגמה פופולרית של איך, אפילו כאשר אירוע בודד הוא כאוטי ואקראי, עם תוצאה בלתי ידועה למעשה, עדיין אפשר להצהיר הצהרות מדויקות על מספר רב של אירועים, כלומר איך הפרופורציות היחסיות עבור תוצאות רבות ושונות מתפלגות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 2.52,
  "end": 18.3
 },
 {
  "input": "More specifically, the Galton board illustrates one of the most prominent distributions in all of probability, known as the normal distribution, more colloquially known as a bell curve, and also called a Gaussian distribution. ",
  "translatedText": "ליתר דיוק, לוח גלטון ממחיש את אחת ההתפלגויות הבולטות בכל תורת ההסתברות, המכונה התפלגות נורמלית, הידועה יותר כעקומת פעמון, ונקראת גם התפלגות גאוסית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 20.38,
  "end": 31.9
 },
 {
  "input": "There's a very specific function to describe this distribution, it's very pretty, we'll get into it later, but right now I just want to emphasize how the normal distribution is, as the name suggests, very common, it shows up in a lot of seemingly unrelated contexts. ",
  "translatedText": "יש פונקציה מאוד ספציפית לתאר את ההתפלגות הזו, היא מאוד יפה, ניכנס אליה מאוחר יותר, אבל כרגע אני רק רוצה להדגיש איך ההתפלגות הנורמלית, כפי שהשם מרמז, מאוד נפוצה, היא מופיעה בהרבה הקשרים שלכאורה לא קשורים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 32.5,
  "end": 45.04
 },
 {
  "input": "If you were to take a large number of people who sit in a similar demographic and plot their heights, those heights tend to follow a normal distribution. ",
  "translatedText": "אם הייתם לוקחים מספר רב של אנשים מאוכלוסיה דמוגרפית דומה ומשרטטים את הגבהים שלהם, הגבהים האלה נוטים להתפלגות נורמלית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 46.02,
  "end": 53.0
 },
 {
  "input": "If you look at a large swath of very big natural numbers and you ask how many distinct prime factors does each one of those numbers have, the answers will very closely track with a certain normal distribution. ",
  "translatedText": "אם תסתכלו על אוסף גדול של מספרים טבעיים גדולים מאוד ותשאלו כמה גורמים ראשוניים נפרדים יש לכל אחד מאותם מספרים, התשובות יהיו קרובות מאד להתפלגות נורמלית מסוימת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 53.66,
  "end": 64.96
 },
 {
  "input": "Now our topic for today is one of the crown jewels in all of probability theory, it's one of the key facts that explains why this distribution is as common as it is, known as the central limit theorem. ",
  "translatedText": "עכשיו הנושא שלנו להיום הוא אחד מיהלומי הכתר בכל תורת ההסתברות, זו אחת העובדות המרכזיות שמסבירות מדוע ההתפלגות הזו נפוצה כמו שהיא, ומכונה משפט הגבול המרכזי. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 65.58,
  "end": 76.02
 },
 {
  "input": "This lesson is meant to go back to the basics, giving you the fundamentals on what the central limit theorem is saying, what normal distributions are, and I want to assume minimal background. ",
  "translatedText": "השיעור הזה נועד לחזור ליסודות, לתת לכם את היסודות על מה שמשפט הגבול המרכזי אומר, מהן התפלגויות נורמליות, ואני רוצה להניח רקע מינימלי. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 76.64,
  "end": 85.26
 },
 {
  "input": "We're going to go decently deep into it, but after this I'd still like to go deeper and explain why the theorem is true, why the function underlying the normal distribution has the very specific form that it does, why that formula has a pi in it, and, most fun, why those last two facts are actually more related than a lot of traditional explanations would suggest. ",
  "translatedText": "אנחנו הולכים להעמיק בזה, אבל אחרי כן אני עדיין רוצה להעמיק ולהסביר למה המשפט נכון, למה לפונקציה שבבסיס ההתפלגות הנורמלית יש את הצורה הספציפית מאוד שלה, למה מופיע פאי בנוסחה הזו, והכי מהנה, מדוע שתי העובדות האחרונות הללו קשורות למעשה יותר ממה שהרבה הסברים מסורתיים מרמזים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 85.26,
  "end": 105.56
 },
 {
  "input": "That second lesson is also meant to be the follow-on to the convolutions video that I promised, so there's a lot of interrelated topics here. ",
  "translatedText": "השיעור השני אמור להיות גם ההמשך לסרטון הקונבולוציות שהבטחתי, אז יש כאן הרבה נושאים הקשורים זה בזה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 106.48,
  "end": 113.37
 },
 {
  "input": "But right now, back to the fundamentals, I'd like to kick things off with a overly simplified model of the Galton board. ",
  "translatedText": "אבל כרגע, בחזרה ליסודות, אני רוצה להתחיל עם דגם פשוט מדי של לוח גלטון. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 113.57,
  "end": 119.17
 },
 {
  "input": "In this model we will assume that each ball falls directly onto a certain central peg and that it has a 50-50 probability of bouncing to the left or to the right, and we'll think of each of those outcomes as either adding one or subtracting one from its position. ",
  "translatedText": "במודל זה נניח שכל כדור נופל ישירות על יתד מרכזי מסוים ושיש לו סבירות של 50-50 לקפוץ שמאלה או ימינה, ונחשוב על כל אחת מהתוצאות האלה כעל הוספת אחד או הפחתת אחד מהמיקום שלו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 120.89,
  "end": 134.11
 },
 {
  "input": "Once one of those is chosen, we make the highly unrealistic assumption that it happens to land dead on in the middle of the peg adjacent below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the right. ",
  "translatedText": "ברגע שאחת מהאפשרויות נבחרה, אנו מניחים את ההנחה המאוד לא מציאותית שהוא במקרה נוחת בדיוק באמצע היתד הסמוך מתחתיו, שם שוב הוא יעמוד בפני אותה בחירה של 50-50 של הקפצה שמאלה או לימין. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 134.67,
  "end": 147.07
 },
 {
  "input": "For the one I'm showing on screen, there are five different rows of pegs, so our little hopping ball makes five different random choices between plus one and minus one, and we can think of its final position as basically being the sum of all of those different numbers, which in this case happens to be one, and we might label all of the different buckets with the sum that they represent. ",
  "translatedText": "עבור זה שאני מראה על המסך, יש חמש שורות שונות של יתדות, כך שכדור הקפיצה הקטן שלנו עושה חמש בחירות אקראיות שונות בין פלוס אחד למינוס אחד, ואנחנו יכולים לחשוב על מיקומו הסופי שהוא בעצם הסכום של כל המספרים השונים האלה, שבמקרה זה הוא אחד, ואנחנו יכולים לתייג את כל המיכלים השונים עם הסכום שהם מייצגים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 147.43,
  "end": 166.35
 },
 {
  "input": "As we repeat this, we're looking at different possible sums for those five random numbers. ",
  "translatedText": "ככל שאנחנו חוזרים על זה, אנחנו מסתכלים על סכומים אפשריים שונים עבור חמשת המספרים האקראיים האלה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 166.35,
  "end": 171.29
 },
 {
  "input": "And for those of you who are inclined to complain that this is a highly unrealistic model for the true Galton board, let me emphasize the goal right now is not to accurately model physics. ",
  "translatedText": "ולמי מכם שנוטה להתלונן שזהו מודל מאוד לא מציאותי של לוח גלטון האמיתי, הרשו לי להדגיש שהמטרה כרגע היא לא להציג דגם פיזיקלי מדויק. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 173.05,
  "end": 181.67
 },
 {
  "input": "The goal is to give a simple example to illustrate the central limit theorem, and for that, idealized though this might be, it actually gives us a really good example. ",
  "translatedText": "המטרה היא לתת דוגמה פשוטה כדי להמחיש את משפט הגבול המרכזי, ולשם כך, למרות שזו יכולה להיות אידיאליזציה, היא למעשה נותנת לנו דוגמה ממש טובה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 181.83,
  "end": 190.03
 },
 {
  "input": "If we let many different balls fall, making yet another unrealistic assumption that they don't influence each other as if they're all ghosts, then the number of balls that fall into each different bucket gives us some loose sense for how likely each one of those buckets is. ",
  "translatedText": "אם אנו נותנים לכדורים רבים ליפול, תוך הנחה לא מציאותית נוספת שהם לא משפיעים זה על זה כאילו כולם רוחות רפאים, אז מספר הכדורים שנופלים לכל מיכל נותן לנו תחושה כלשהי לגבי הסבירות של מהו כל אחד מהמיכלים האלה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 190.57,
  "end": 203.39
 },
 {
  "input": "In this example, the numbers are simple enough that it's not too hard to explicitly calculate what the probability is for falling into each bucket. ",
  "translatedText": "בדוגמה זו, המספרים פשוטים מספיק כך שלא קשה מדי לחשב במפורש מה ההסתברות ליפול לכל מיכל. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 203.83,
  "end": 210.01
 },
 {
  "input": "If you do want to think that through, you'll find it very reminiscent of Pascal's triangle. ",
  "translatedText": "אם אתם רוצים לחשוב על זה, תמצאו שזה מאוד מזכיר את המשולש של פסקל. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 210.27,
  "end": 213.83
 },
 {
  "input": "But the neat thing about our theorem is how far it goes beyond the simple examples. ",
  "translatedText": "אבל הדבר היפה במשפט שלנו הוא עד כמה הוא הולך מעבר לדוגמאות הפשוטות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 213.95,
  "end": 218.27
 },
 {
  "input": "So to start off at least, rather than making explicit calculations, let's just simulate things by running a large number of samples and letting the total number of results in each different outcome give us some sense for what that distribution looks like. ",
  "translatedText": "אז כדי להתחיל לפחות, במקום לבצע חישובים מפורשים, בואו פשוט נדמה דברים על ידי הפעלת מספר רב של דגימות וניתן למספר הכולל של התוצאות לכל ערך שונה לתת לנו תחושה של איך נראית ההתפלגות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 218.67,
  "end": 229.97
 },
 {
  "input": "As I said, the one on screen has five rows, so each sum that we're considering includes only five numbers. ",
  "translatedText": "כפי שאמרתי, לזה שעל המסך יש חמש שורות, כך שכל סכום שאנו שוקלים כולל רק חמישה מספרים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 230.45,
  "end": 236.21
 },
 {
  "input": "The basic idea of the central limit theorem is that if you increase the size of that sum, for example here that would mean increasing the number of rows of pegs for each ball to bounce off, then the distribution that describes where that sum is going to fall looks more and more like a bell curve. ",
  "translatedText": "הרעיון הבסיסי של משפט הגבול המרכזי הוא שאם תגדילו את גודל הסכום, למשל כאן זה אומר הגדלת מספר שורות היתדות עבור כל כדור שיקפוץ, אז ההתפלגות המתארת לאן הסכום הולך נראה יותר ויותר כמו עקומת פעמון. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 236.81,
  "end": 253.33
 },
 {
  "input": "Here, it's actually worth taking a moment to write down that general idea. ",
  "translatedText": "כאן, בעצם שווה להקדיש רגע כדי לרשום את הרעיון הכללי. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 255.47,
  "end": 258.35
 },
 {
  "input": "The setup is that we have a random variable, and that's basically shorthand for a random process where each outcome of that process is associated with some number. ",
  "translatedText": "ההגדרה היא שיש לנו משתנה אקראי, וזה בעצם קיצור של תהליך אקראי שבו כל תוצאה של תהליך זה קשורה למספר כלשהו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 259.27,
  "end": 268.19
 },
 {
  "input": "We'll call that random number x. ",
  "translatedText": "נקרא למספר האקראי הזה x. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 268.49,
  "end": 269.97
 },
 {
  "input": "For example, each bounce off the peg is a random process modeled with two outcomes. ",
  "translatedText": "לדוגמה, כל הקפצה מהיתד היא תהליך אקראי המבוסס על שתי תוצאות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.97,
  "end": 274.39
 },
 {
  "input": "Those outcomes are associated with the numbers negative one and positive one. ",
  "translatedText": "תוצאות אלו קשורות למספרים מונוס 1 ופלוס 1. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 274.85,
  "end": 277.89
 },
 {
  "input": "Another example of a random variable would be rolling a die, where you have six different outcomes, each one associated with a number. ",
  "translatedText": "דוגמה נוספת למשתנה אקראי תהיה הטלת קובייה, שבה יש שש תוצאות שונות, שכל אחת מהן קשורה למספר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 278.53,
  "end": 284.83
 },
 {
  "input": "What we're doing is taking multiple different samples of that variable and adding them all together. ",
  "translatedText": "מה שאנחנו עושים זה לקחת מספר דוגמאות שונות של המשתנה הזה ולצרף את כולן ביחד. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 285.47,
  "end": 290.41
 },
 {
  "input": "On our Galton board, that looks like letting the ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice and adding up the results. ",
  "translatedText": "על לוח הגלטון שלנו, זה נראה כמו לתת לכדור לקפוץ ממספר יתדות שונות בדרכו למטה, ובמקרה של קובייה, אתם עשויים לדמיין זריקה של קוביות רבות ושונות וחיבור התוצאות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 290.77,
  "end": 300.97
 },
 {
  "input": "The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into different possible values, will look more and more like a bell curve. ",
  "translatedText": "הטענה של משפט הגבול המרכזי היא שככל שאתם נותנים לגודל הסכום להיות גדול יותר ויותר, אז ההתפלגות של הסכום, מה הסיכוי שהוא ייפול לערכים אפשריים שונים, תיראה יותר ויותר כמו עקומת פעמון. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 301.43,
  "end": 314.11
 },
 {
  "input": "That's it, that is the general idea. ",
  "translatedText": "זהו, זה הרעיון הכללי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.43,
  "end": 317.13
 },
 {
  "input": "Over the course of this lesson, our job is to make that statement more quantitative. ",
  "translatedText": "במהלך השיעור הזה, התפקיד שלנו הוא להפוך את ההצהרה הזו לכמותית יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 317.55,
  "end": 321.53
 },
 {
  "input": "We're going to put some numbers to it, put some formulas to it, show how you can use it to make predictions. ",
  "translatedText": "אנחנו הולכים לשים בה כמה מספרים, לשים בה כמה נוסחאות, להראות איך אתם יכולים להשתמש בזה כדי ליצור תחזיות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 322.07,
  "end": 326.35
 },
 {
  "input": "For example, here's the kind of question I want you to be able to answer by the end of this video. ",
  "translatedText": "לדוגמה, הנה סוג השאלות שאני רוצה שתצליחו לענות עליהן עד סוף הסרטון הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.21,
  "end": 331.57
 },
 {
  "input": "Suppose you rolled the die 100 times and you added together the results. ",
  "translatedText": "נניח שהטלתם את הקוביה 100 פעמים וצירפתם את התוצאות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 332.19,
  "end": 335.89
 },
 {
  "input": "Could you find a range of values such that you're 95% sure that the sum will fall within that range? ",
  "translatedText": "האם תוכלו למצוא טווח של ערכים כך שאתם בטוחים ב-95% שהסכום ייפול בטווח הזה? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 336.63,
  "end": 342.17
 },
 {
  "input": "Or maybe I should say find the smallest possible range of values such that this is true. ",
  "translatedText": "או אולי אני צריך לומר למצוא את הטווח הקטן ביותר האפשרי של ערכים כך שזה נכון. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.83,
  "end": 346.55
 },
 {
  "input": "The neat thing is you'll be able to answer this question whether it's a fair die or if it's a weighted die. ",
  "translatedText": "הדבר היפה הוא שתוכלו לענות על השאלה הזו בין אם מדובר בקובייה הוגנת ובין אם מדובר בקובייה משוקללת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 347.39,
  "end": 352.13
 },
 {
  "input": "Now let me say at the top that this theorem has three different assumptions that go into it, three things that have to be true before the theorem follows. ",
  "translatedText": "עכשיו הרשו לי להתחיל בכך שלמשפט הזה יש שלוש הנחות שונות שמובלעות בתוכו, שלושה דברים שצריכים להיות נכונים כדי שהמשפט יהיה נכון. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 353.45,
  "end": 360.13
 },
 {
  "input": "And I'm actually not going to tell you what they are until the very end of the video. ",
  "translatedText": "ואני בעצם לא הולך לספר לכם מה הם עד סוף הסרטון. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 360.43,
  "end": 363.79
 },
 {
  "input": "Instead I want you to keep your eye out and see if you can notice and maybe predict what those three assumptions are going to be. ",
  "translatedText": "במקום זאת, אני רוצה שתפקחו עין ותראו אם אתם יכולים לשים לב ואולי לחזות מה יהיו שלוש ההנחות האלה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 364.27,
  "end": 369.67
 },
 {
  "input": "As a next step, to better illustrate just how general this theorem is, I want to run a couple more simulations for you focused on the dice example. ",
  "translatedText": "כשלב הבא, כדי להמחיש טוב יותר עד כמה המשפט הזה כללי, אני רוצה להריץ עבורכם עוד כמה סימולציות המתמקדות בדוגמה של הקוביות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 370.71,
  "end": 377.39
 },
 {
  "input": "Usually if you think of rolling a die you think of the six outcomes as being equally probable, but the theorem actually doesn't care about that. ",
  "translatedText": "בדרך כלל אם אתם חושבים על הטלת קובייה אתם חושבים על שש התוצאות כסבירות באותה מידה, אבל למשפט למעשה לא אכפת מזה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 380.91,
  "end": 387.63
 },
 {
  "input": "We could start with a weighted die, something with a non-trivial distribution across the outcomes, and the core idea still holds. ",
  "translatedText": "אנחנו יכולים להתחיל עם קובייה משוקללת, משהו עם התפלגות לא טריוויאלית על פני התוצאות, והרעיון המרכזי עדיין מתקיים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 387.83,
  "end": 394.55
 },
 {
  "input": "For the simulation what I'll do is take some distribution like this one that is skewed towards lower values. ",
  "translatedText": "עבור הסימולציה מה שאני אעשה זה לקחת התפלגות כמו זו שמוטה לערכים נמוכים יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.03,
  "end": 399.93
 },
 {
  "input": "I'm going to take 10 distinct samples from that distribution and then I'll record the sum of that sample on the plot on the bottom. ",
  "translatedText": "אני הולך לקחת 10 דגימות נפרדות מההתפלגות הזו ואז ארשום את הסכום של הדגימה הזו בתחתית התרשים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 400.25,
  "end": 407.55
 },
 {
  "input": "Then I'm going to do this many many different times, always with a sum of size 10, but keep track of where those sums ended up to give us a sense of the distribution. ",
  "translatedText": "אז אני הולך לעשות את זה הרבה פעמים שונות, תמיד עם סכום בגודל 10, אבל עם מעקב היכן הסכומים האלה הגיעו כדי לתת לנו תחושה של התפלגות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 408.63,
  "end": 416.59
 },
 {
  "input": "And in fact let me rescale the y direction to give us room to run an even larger number of samples. ",
  "translatedText": "ולמעשה הרשו לי לשנות את קנה המידה של כיוון ה-y כדי לתת לנו מקום להריץ מספר גדול עוד יותר של דגימות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.97,
  "end": 424.73
 },
 {
  "input": "And I'll let it go all the way up to a couple thousand, and as it does you'll notice that the shape that starts to emerge looks like a bell curve. ",
  "translatedText": "ואני אתן לזה ללכת עד כמה אלפים, ובזמן שזה יקרה אתם תשימו לב שהצורה שמתחילה להופיע נראית כמו עקומה של פעמון. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 425.03,
  "end": 432.49
 },
 {
  "input": "Maybe if you squint your eyes you can see it skews a tiny bit to the left, but it's neat that something so symmetric emerged from a starting point that was so asymmetric. ",
  "translatedText": "אולי אם תצמצמו את עיניכם תוכלו לראות הטיה קטנטנה שמאלה, אבל זה יפה שמשהו כל כך סימטרי צץ מנקודת התחלה שהייתה כל כך אסימטרית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 432.87,
  "end": 441.01
 },
 {
  "input": "To better illustrate what the central limit theorem is all about, let me run four of these simulations in parallel, where on the upper left I'm doing it where we're only adding two dice at a time, on the upper right we're doing it where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at a time, and then we'll do another one with a bigger sum, 15 at a time. ",
  "translatedText": "כדי להמחיש טוב יותר במה עוסק משפט הגבול המרכזי, הרשו לי להריץ ארבע מהסימולציות הללו במקביל, כאשר בפינה השמאלית העליונה אני עושה זאת כאשר אנו מוסיפים רק שתי קוביות בכל פעם, בצד ימין למעלה אנו אנחנו עושים את זה כאשר אנחנו מוסיפים חמש קוביות בכל פעם, השמאלית התחתונה היא זו שראינו עכשיו ושבה מוסיפים 10 קוביות בכל פעם, ואז נעשה עוד אחת עם סכום גדול יותר, 15 בכל פעם. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 441.47,
  "end": 461.37
 },
 {
  "input": "Notice how on the upper left when we're just adding two dice, the resulting distribution doesn't really look like a bell curve, it looks a lot more reminiscent of the one we started with skewed towards the left. ",
  "translatedText": "שימו לב איך בצד שמאל למעלה כשאנחנו מוסיפים רק שתי קוביות, ההתפלגות המתקבלת לא ממש נראית כמו עקומת פעמון, היא מזכירה הרבה יותר את זו שהתחלנו איתה עם הטיה לכיוון שמאל. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 462.25,
  "end": 472.03
 },
 {
  "input": "But as we allow for more and more dice in each sum, the resulting shape that comes up in these distributions looks more and more symmetric. ",
  "translatedText": "אבל ככל שאנו מאפשרים יותר ויותר קוביות בכל סכום, הצורה המתקבלת שעולה מהתפלגויות אלו נראית יותר ויותר סימטרית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 472.81,
  "end": 479.81
 },
 {
  "input": "It has the lump in the middle and fade towards the tail's shape of a bell curve. ",
  "translatedText": "יש לה את התפיחה באמצע והיא יורדת לקראת צורת הזנב של עקומת פעמון. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 479.95,
  "end": 483.89
 },
 {
  "input": "And let me emphasize again, you can start with any different distribution. ",
  "translatedText": "ותנו לי להדגיש שוב, אתם יכולים להתחיל עם כל התפלגות אחרת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 487.05,
  "end": 490.49
 },
 {
  "input": "Here I'll run it again, but where most of the probability is tied up in the numbers 1 and 6, with very low probability for the mid values. ",
  "translatedText": "כאן אני אריץ את זה שוב, אבל כאשר רוב ההסתברות קשורה למספרים 1 ו-6, עם הסתברות נמוכה מאוד לערכי האמצע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.49,
  "end": 497.49
 },
 {
  "input": "Despite completely changing the distribution for an individual roll of the die, it's still the case that a bell curve shape will emerge as we consider the different sums. ",
  "translatedText": "למרות שינוי מוחלט של ההתפלגות עבור זריקת קובייה בודדת, צורת עקומת פעמון עדיין תופיע כשאנחנו בוחנים את הסכומים השונים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 498.19,
  "end": 506.55
 },
 {
  "input": "Illustrating things with a simulation like this is very fun, and it's kind of neat to see order emerge from chaos, but it also feels a little imprecise. ",
  "translatedText": "להמחיש דברים בסימולציה כזו זה מאוד כיף, וזה די יפה לראות סדר עולה מתוך הכאוס, אבל זה גם מרגיש קצת לא מדויק. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 507.27,
  "end": 515.03
 },
 {
  "input": "Like in this case, when I cut off the simulation at 3000 samples, even though it kind of looks like a bell curve, the different buckets seem pretty spiky. ",
  "translatedText": "כמו במקרה הזה, כשניתקתי את הסימולציה ב-3000 דגימות, למרות שזה נראה כמו עקומת פעמון, המיכלים השונים נראים די חדים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 515.39,
  "end": 522.99
 },
 {
  "input": "And you might wonder, is it supposed to look that way, or is that just an artifact of the randomness in the simulation? ",
  "translatedText": "ואתם עשויים לתהות, האם זה אמור להיראות כך, או שזה רק תוצאה של האקראיות בסימולציה? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 522.99,
  "end": 528.55
 },
 {
  "input": "And if it is, how many samples do we need before we can be sure that what we're looking at is representative of the true distribution? ",
  "translatedText": "ואם כן, כמה דגימות אנחנו צריכים לפני שנוכל להיות בטוחים שמה שאנחנו מסתכלים עליו מייצג את ההתפלגות האמיתית? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 529.01,
  "end": 535.11
 },
 {
  "input": "Instead moving forward, let's get a little more theoretical and show the precise shape that these distributions will take on in the long run. ",
  "translatedText": "במקום להתקדם, בואו נהיה קצת יותר תיאורטיים ונראה את הצורה המדויקת שההתפלגויות הללו יקבלו בטווח הארוך. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 539.19,
  "end": 545.47
 },
 {
  "input": "The easiest case to make this calculation is if we have a uniform distribution, where each possible face of the die has an equal probability, 1 6th. ",
  "translatedText": "המקרה הקל ביותר לחישוב הזה הוא כשיש לנו התפלגות אחידה, כאשר לכל צד אפשרי של הקוביה יש הסתברות שווה, 1 ל-6. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 546.13,
  "end": 553.97
 },
 {
  "input": "For example, if you then want to know how likely different sums are for a pair of dice, it's essentially a counting game, where you count up how many distinct pairs take on the same sum, which in the diagram I've drawn, you can conveniently think about by going through all of the different diagonals. ",
  "translatedText": "לדוגמה, אם אתם רוצים לדעת מה הסיכוי לסכומים שונים עבור זוג קוביות, זה בעצם משחק ספירה, שבו אתם סופרים לכמה זוגות נפרדים יש אותו סכום, כשבדיאגרמה שציירתי, אתם יכולים בקלות לחשוב עליהם על ידי מעבר על כל האלכסונים השונים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 553.99,
  "end": 568.49
 },
 {
  "input": "Since each such pair has an equal chance of showing up, 1 in 36, all you have to do is count the sizes of these buckets. ",
  "translatedText": "מכיוון שלכל זוג כזה יש סיכוי שווה להופיע, 1 ל-36, כל שעליכם לעשות הוא לספור את הגדלים של המיכלים הללו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 571.41,
  "end": 577.53
 },
 {
  "input": "That gives us a definitive shape for the distribution describing a sum of two dice, and if we were to play the same game with all possible triplets, the resulting distribution would look like this. ",
  "translatedText": "זה נותן לנו צורה סופית להתפלגות המתארת סכום של שתי קוביות, ואם היינו משחקים באותו משחק עם כל השלשות האפשריות, ההתפלגות המתקבלת הייתה נראית כך. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.19,
  "end": 588.13
 },
 {
  "input": "Now what's more challenging, but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that single die. ",
  "translatedText": "עכשיו מה שיותר מאתגר, אבל הרבה יותר מעניין, הוא לשאול מה יקרה אם תהיה לנו חלוקה לא אחידה לאותה קובייה בודדת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 588.69,
  "end": 594.99
 },
 {
  "input": "We actually talked all about this in the last video. ",
  "translatedText": "למעשה דיברנו על זה בסרטון האחרון. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.55,
  "end": 597.97
 },
 {
  "input": "You do essentially the same thing, you go through all the distinct pairs of dice which add up to the same value. ",
  "translatedText": "אתם עושים את אותו הדבר בעצם, אתם עוברים על כל זוגות הקוביות הנבדלים שמצטברים לאותו ערך. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 598.45,
  "end": 603.67
 },
 {
  "input": "It's just that instead of counting those pairs, for each pair you multiply the two probabilities of each particular face coming up, and then you add all those together. ",
  "translatedText": "רק שבמקום לספור את הזוגות האלה, עבור כל זוג אתם מכפילים את שתי ההסתברויות של כל צד מסוים שמגיע, ואז אתם מוסיפים את כל אלה יחד. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 603.97,
  "end": 612.75
 },
 {
  "input": "The computation that does this for all possible sums has a fancy name, it's called a convolution, but it's essentially just the weighted version of the counting game that anyone who's played with a pair of dice already finds familiar. ",
  "translatedText": "לחישוב שעושה זאת עבור כל הסכומים האפשריים יש שם מפואר, זה נקרא קונבולציה, אבל זוהי בעצם רק הגרסה המשוקללת של משחק הספירה שכל מי ששיחק עם זוג קוביות כבר מוצא כמוכר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 613.29,
  "end": 624.47
 },
 {
  "input": "For our purposes in this lesson, I'll have the computer calculate all that, simply display the results for you, and invite you to observe certain patterns, but under the hood, this is what's going on. ",
  "translatedText": "למטרותינו בשיעור זה, אני אגרום למחשב לחשב את כל זה, פשוט להציג עבורכם את התוצאות, ולהזמין אתכם לצפות בתבניות מסוימות, אבל מתחת למכסה המנוע, זה מה שקורה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.03,
  "end": 635.33
 },
 {
  "input": "So just to be crystal clear on what's being represented here, if you imagine sampling two different values from that top distribution, the one describing a single die, and adding them together, then the second distribution I'm drawing represents how likely you are to see various different sums. ",
  "translatedText": "אז רק כדי להיות ברור לגמרי לגבי מה שמיוצג כאן, אם אתם מדמיינים דגימה של שני ערכים שונים מההתפלגות העליונה ההיא, זו שמתארת קובייה בודדת, ומחברים אותם יחד, אז ההתפלגות השנייה שאני מצייר מייצגת את הסבירות שלכם לראות סכומים שונים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 636.65,
  "end": 652.23
 },
 {
  "input": "Likewise, if you imagine sampling three distinct values from that top distribution, and adding them together, the next plot represents the probabilities for various different sums in that case. ",
  "translatedText": "באופן דומה, אם אתם מדמיינים דגימה של שלושה ערכים נפרדים מאותה התפלגות עליונה, ולחבר אותם יחד, התרשים הבא מייצג את ההסתברויות עבור סכומים שונים במקרה זה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 652.89,
  "end": 662.49
 },
 {
  "input": "So if I compute what the distributions for these sums look like for larger and larger sums, well you know what I'm going to say, it looks more and more like a bell curve. ",
  "translatedText": "אז אם אני מחשב איך נראות ההתפלגויות של הסכומים האלה עבור סכומים גדולים יותר ויותר, ובכן, אתם יודעים מה אני הולך להגיד, זה נראה יותר ויותר כמו עקומת פעמון. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 663.51,
  "end": 672.39
 },
 {
  "input": "But before we get to that, I want you to make a couple more simple observations. ",
  "translatedText": "אבל לפני שנגיע לזה, אני רוצה שתשימו לב לעוד כמה דברים פשוטים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 673.35,
  "end": 676.45
 },
 {
  "input": "For example, these distributions seem to be wandering to the right, and also they seem to be getting more spread out, and a little bit more flat. ",
  "translatedText": "לדוגמה, נראה שההתפלגויות הללו נודדות ימינה, וגם נראה שהן מתפרסות יותר וקצת יותר שטוחות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 677.45,
  "end": 684.79
 },
 {
  "input": "You cannot describe the central limit theorem quantitatively without taking into account both of those effects, which in turn requires describing the mean and the standard deviation. ",
  "translatedText": "אינכם יכולים לתאר את משפט הגבול המרכזי בצורה כמותית מבלי לקחת בחשבון את שתי ההשפעות הללו, אשר מחייבות לתאר את הממוצע ואת סטיית התקן. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 685.25,
  "end": 693.19
 },
 {
  "input": "Maybe you're already familiar with those, but I want to make minimal assumptions here, and it never hurts to review, so let's quickly go over both of those. ",
  "translatedText": "אולי אתם כבר מכירים את המושגים האלו, אבל אני רוצה להניח הנחות מינימליות כאן, וזה אף פעם לא מזיק לסקור מחדש, אז בואו נעבור במהירות על שניהם. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 693.95,
  "end": 700.61
 },
 {
  "input": "The mean of a distribution, often denoted with the Greek letter mu, is a way of capturing the center of mass for that distribution. ",
  "translatedText": "הממוצע של התפלגות, המסומן לעתים קרובות באות היוונית mu, הוא דרך למצוא את מרכז המסה של התפלגות זו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 703.41,
  "end": 710.71
 },
 {
  "input": "It's calculated as the expected value of our random variable, which is a way of saying you go through all of the different possible outcomes, and you multiply the probability of that outcome times the value of the variable. ",
  "translatedText": "הוא מחושב כערך הצפוי של המשתנה האקראי שלנו, שזו דרך לומר שאתם עוברים על כל התוצאות האפשריות השונות, ואתם מכפילים את ההסתברות של התוצאה הזו כפול ערכו של המשתנה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 711.19,
  "end": 722.85
 },
 {
  "input": "If higher values are more probable, that weighted sum is going to be bigger. ",
  "translatedText": "אם ערכים גבוהים יותר הם יותר סבירים, הסכום המשוקלל הזה יהיה גדול יותר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 723.19,
  "end": 726.41
 },
 {
  "input": "If lower values are more probable, that weighted sum is going to be smaller. ",
  "translatedText": "אם ערכים נמוכים יותר הם יותר סבירים, הסכום המשוקלל הזה יהיה קטן יותר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 726.75,
  "end": 729.95
 },
 {
  "input": "A little more interesting is if you want to measure how spread out this distribution is, because there's multiple different ways you might do it. ",
  "translatedText": "קצת יותר מעניין אם אתם רוצים למדוד עד כמה התפלגות זו מפוזרת, כי ישנן מספר דרכים שונות שבהן תוכלו לעשות זאת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 730.79,
  "end": 737.13
 },
 {
  "input": "One of them is called the variance. ",
  "translatedText": "אחד מהם נקרא השונות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.53,
  "end": 740.29
 },
 {
  "input": "The idea there is to look at the difference between each possible value and the mean, square that difference, and ask for its expected value. ",
  "translatedText": "הרעיון הוא להסתכל על ההבדל בין כל ערך אפשרי לבין הממוצע, להעלות בריבוע את ההפרש ולמצוא את הערך הצפוי שלו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 740.83,
  "end": 748.27
 },
 {
  "input": "The idea is that whether your value is below or above the mean, when you square that difference, you get a positive number, and the larger the difference, the bigger that number. ",
  "translatedText": "הרעיון הוא שבין אם הערך מתחת לממוצע או מעליו, כאשר מעלים בריבוע את ההפרש, אתם מקבלים מספר חיובי, וככל שההפרש גדול יותר, כך המספר גדול יותר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 748.73,
  "end": 756.65
 },
 {
  "input": "Squaring it like this turns out to make the math much much nicer than if we did something like an absolute value, but the downside is that it's hard to think about this as a distance in our diagram because the units are off. ",
  "translatedText": "העלאה זו בריבוע הופכת את המתמטיקה להרבה הרבה יותר יפה מאשר אם היינו משתמשים במשהו כמו ערך מוחלט, אבל החיסרון הוא שקשה לחשוב על זה כעל מרחק בתרשים שלנו כי היחידות לא מתואמות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 757.37,
  "end": 768.13
 },
 {
  "input": "Kind of like the units here are square units, whereas a distance in our diagram would be a kind of linear unit. ",
  "translatedText": "בערך כמו שהיחידות כאן הן יחידות בריבוע, בעוד שמרחק בתרשים שלנו יהיה סוג של יחידה לינארית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 768.33,
  "end": 773.31
 },
 {
  "input": "So another way to measure spread is what's called the standard deviation, which is the square root of this value. ",
  "translatedText": "אז דרך נוספת למדוד התפשטות היא מה שנקרא סטיית התקן, שהיא השורש הריבועי של הערך הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 773.71,
  "end": 779.19
 },
 {
  "input": "That can be interpreted much more reasonably as a distance on our diagram, and it's commonly denoted with the Greek letter sigma, so you know m for mean as for standard deviation, but both in Greek. ",
  "translatedText": "היא יכול להתפרש בצורה הרבה יותר סבירה כמרחק בתרשים שלנו, והוא מסומן בדרך כלל באות היוונית סיגמה, אז אתם מכירים m עבור ממוצע ו-s  עבור סטיית תקן, אבל שניהם ביוונית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 779.47,
  "end": 789.65
 },
 {
  "input": "Looking back at our sequence of distributions, let's talk about the mean and standard deviation. ",
  "translatedText": "במבט נוסף על רצף ההתפלגויות שלנו, בואו נדבר על הממוצע ועל סטיית התקן. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 791.87,
  "end": 796.15
 },
 {
  "input": "If we call the mean of the initial distribution mu, which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if I tell you that the mean of the next one is 2 times mu. ",
  "translatedText": "אם נקרא לממוצע של ההתפלגות הראשונית mu, אשר עבור זו המוצגת הוא במקרה 2.24, אני מקווה שזה לא יהיה מפתיע מדי אם אני אגיד לכם שהממוצע של הבא הוא פי 2. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 796.63,
  "end": 806.73
 },
 {
  "input": "That is, you roll a pair of dice, you want to know the expected value of the sum, it's two times the expected value for a single die. ",
  "translatedText": "כלומר, אתם מטילים זוג קוביות, אתם רוצים לדעת את הערך הצפוי של הסכום, זה פי שניים מהערך הצפוי לקובייה בודדת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 807.13,
  "end": 812.81
 },
 {
  "input": "Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth. ",
  "translatedText": "באופן דומה, הערך הצפוי עבור הסכום שלנו בגודל 3 הוא פי 3 mu, וכן הלאה וכן הלאה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 813.85,
  "end": 819.41
 },
 {
  "input": "The mean just marches steadily on to the right, which is why our distributions seem to be drifting off in that direction. ",
  "translatedText": "הממוצע פשוט זז ימינה בהתמדה, ולכן נראה שההתפלגות שלנו נסחפת לכיוון הזה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 819.63,
  "end": 824.87
 },
 {
  "input": "A little more challenging, but very important, is to describe how the standard deviation changes. ",
  "translatedText": "קצת יותר מאתגר, אבל חשוב מאוד, הוא לתאר כיצד סטיית התקן משתנה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.35,
  "end": 829.91
 },
 {
  "input": "The key fact here is that if you have two different random variables, then the variance for the sum of those variables is the same as just adding together the original two variances. ",
  "translatedText": "עובדת המפתח כאן היא שאם יש לכם שני משתנים אקראיים שונים, השונות עבור סכום המשתנים האלה זהה לחיבור של שתי השונויות המקוריות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 830.49,
  "end": 839.37
 },
 {
  "input": "This is one of those facts that you can just compute when you unpack all the definitions. ",
  "translatedText": "זו אחת העובדות שאתם יכולים פשוט לחשב כשאתם מנתחים את כל ההגדרות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 839.93,
  "end": 843.63
 },
 {
  "input": "There are a couple nice intuitions for why it's true. ",
  "translatedText": "יש כמה אינטואיציות נחמדות למה זה נכון. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 843.63,
  "end": 846.21
 },
 {
  "input": "My tentative plan is to just actually make a series about probability and talk about things like intuitions underlying variance and its cousins there. ",
  "translatedText": "התוכנית הטנטטיבית שלי היא פשוט לעשות סדרה על הסתברות ולדבר על דברים כמו אינטואיציות שבבסיס השונות ובני הדודים שלה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 846.63,
  "end": 853.53
 },
 {
  "input": "But right now, the main thing I want you to highlight is how it's the variance that adds, it's not the standard deviation that adds. ",
  "translatedText": "אבל כרגע, הדבר העיקרי שאני רוצה שתדגישו הוא איך השונות היא שמצטברת, זו לא סטיית התקן שמצטברת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 854.01,
  "end": 860.15
 },
 {
  "input": "So, critically, if you were to take n different realizations of the same random variable and ask what the sum looks like, the variance of that sum is n times the variance of your original variable, meaning the standard deviation, the square root of all this, is the square root of n times the original standard deviation. ",
  "translatedText": "אז אם הייתם לוקחים n מימושים שונים של אותו משתנה אקראי ושואלים איך נראה הסכום, השונות של הסכום הזה היא פי n מהשונות של המשתנה המקורי שלכם, כלומר סטיית התקן, השורש הריבועי של כולם. זהו השורש הריבועי של n כפול סטיית התקן המקורית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 860.41,
  "end": 878.25
 },
 {
  "input": "For example, back in our sequence of distributions, if we label the standard deviation of our initial one with sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on and so forth. ",
  "translatedText": "לדוגמה, ברצף ההתפלגויות שלנו, אם נסמן את סטיית התקן של ההתחלה שלנו בסיגמא, סטיית התקן הבאה תהיה השורש הריבועי של פי 2 סיגמא, ובהמשך זה נראה כמו השורש הריבועי של 3 פעמים סיגמא, וכן הלאה וכן הלאה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 879.29,
  "end": 893.09
 },
 {
  "input": "This, like I said, is very important. ",
  "translatedText": "זה, כמו שאמרתי, חשוב מאוד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.75,
  "end": 895.65
 },
 {
  "input": "It means that even though our distributions are getting spread out, they're not spreading out all that quickly, they only do so in proportion to the square root of the size of the sum. ",
  "translatedText": "זה אומר שלמרות שההתפלגויות שלנו מתפזרות, הן לא מתפשטות כל כך מהר, הן עושות זאת רק ביחס לשורש הריבועי של גודל הסכום. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 896.07,
  "end": 904.13
 },
 {
  "input": "As we prepare to make a more quantitative description of the central limit theorem, the core intuition I want you to keep in your head is that we'll basically realign all of these distributions so that their means line up together, and then rescale them so that all of the standard deviations are just going to be equal to 1. ",
  "translatedText": "כשאנחנו מתכוננים לתיאור כמותי יותר של משפט הגבול המרכזי, האינטואיציה המרכזית שאני רוצה שתשמרו בראשכם היא שאנחנו בעצם נערוך מחדש את כל ההתפלגויות האלה כך שהמרכזים שלהן יסתדרו יחד, ואז נשנה את קנה המידה כך שכל סטיות התקן פשוט יהיו שוות ל-1. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 904.71,
  "end": 920.61
 },
 {
  "input": "And when we do that, the shape that results gets closer and closer to a certain universal shape, described with an elegant little function that we'll unpack in just a moment. ",
  "translatedText": "וכשאנחנו עושים את זה, הצורה שמתקבלת מתקרבת יותר ויותר לצורה אוניברסלית מסוימת, המתוארת בפונקציה קטנה ואלגנטית שנסביר מיד. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 921.29,
  "end": 929.87
 },
 {
  "input": "And let me say one more time, the real magic here is how we could have started with any distribution, describing a single roll of the die, and if we play the same game, considering what the distributions for the many different sums look like, and we realign them so that the means line up, and we rescale them so that the standard deviations are all 1, we still approach that same universal shape, which is kind of mind-boggling. ",
  "translatedText": "ותנו לי לומר עוד פעם, הקסם האמיתי כאן הוא איך היינו יכולים להתחיל עם כל התפלגות שמתארת זריקת קובייה בודדת, ואם נשחק באותו משחק, בהתחשב באיך נראות ההתפלגויות עבור הסכומים הרבים השונים, ואנחנו מארגנים אותם מחדש כך שהמרכזים יסתדרו, ואנחנו משנים את קנה המידה שלהם כך שסטיות התקן יהיו כולן 1, אנחנו עדיין מתקרבים לאותה צורה אוניברסלית, שהיא סוג של תוצאה מפתיעה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 930.47,
  "end": 952.95
 },
 {
  "input": "And now, my friends, is probably as good a time as any to finally get into the formula for a normal distribution. ",
  "translatedText": "ועכשיו ידידי, זה כנראה זמן טוב להיכנס סוף סוף לנוסחה של ההתפלגות הנורמלית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 954.81,
  "end": 960.85
 },
 {
  "input": "And the way I'd like to do this is to basically peel back all the layers and build it up one piece at a time. ",
  "translatedText": "והדרך שבה הייתי רוצה לעשות זאת היא בעצם לקלף את כל השכבות ולבנות אותה שיכבה אחת בכל פעם. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 961.49,
  "end": 965.93
 },
 {
  "input": "The function e to the x, or anything to the x, describes exponential growth, and if you make that exponent negative, which flips around the graph horizontally, you might think of it as describing exponential decay. ",
  "translatedText": "הפונקציה e בחזקת-x, או כל דבר בחזקת-x, מתארת צמיחה מעריכית, ואם תהפכו את המעריך הזה לשלילי, שינוי שהופך את הגרף אופקית, אולי תחשבו על זה כתאור של דעיכה מעריכית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 966.53,
  "end": 977.87
 },
 {
  "input": "To make this decay in both directions, you could do something to make sure the exponent is always negative and growing, like taking the negative absolute value. ",
  "translatedText": "כדי לגרום לדעיכה הזו בשני הכיוונים, אתם יכולים לעשות משהו כדי לוודא שהמעריך תמיד שלילי וגדל, כמו לקחת את הערך המוחלט השלילי. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 978.51,
  "end": 985.43
 },
 {
  "input": "That would give us this kind of awkward sharp point in the middle, but if instead you make that exponent the negative square of x, you get a smoother version of the same thing, which decays in both directions. ",
  "translatedText": "זה ייתן לנו סוג כזה של נקודה חדה מוזרה באמצע, אבל אם במקום זאת תהפכו את האקספוננט הזה לריבוע השלילי של x, תקבלו גרסה חלקה יותר של אותו דבר, שדועך בשני הכיוונים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 985.93,
  "end": 995.81
 },
 {
  "input": "This gives us the basic bell curve shape. ",
  "translatedText": "זה נותן לנו את צורת עקומת הפעמון הבסיסית. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 996.33,
  "end": 998.19
 },
 {
  "input": "Now if you throw a constant in front of that x, and you scale that constant up and down, it lets you stretch and squish the graph horizontally, allowing you to describe narrow and wider bell curves. ",
  "translatedText": "עכשיו, אם אתם שמים קבוע לפני ה-X, ומשנים את קנה המידה של הקבוע הזה למעלה ולמטה, זה מאפשר לכם למתוח ולמעוך את הגרף אופקית, דבר שמאפשר לכם לתאר עקומות פעמון צרות ורחבות יותר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 998.65,
  "end": 1008.37
 },
 {
  "input": "And a quick thing I'd like to point out here is that based on the rules of exponentiation, as we tweak around that constant c, you could also think about it as simply changing the base of the exponentiation. ",
  "translatedText": "ודבר מהיר שהייתי רוצה לציין כאן הוא שבהתבסס על כללי החזקה, כשאנחנו משנים אותו קבוע c, אתם יכולים גם לחשוב על זה פשוט כשינוי בסיס החזקה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1009.01,
  "end": 1019.75
 },
 {
  "input": "And in that sense, the number e is not really all that special for our formula. ",
  "translatedText": "ובמובן הזה, המספר e לא ממש מיוחד עבור הנוסחה שלנו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1020.15,
  "end": 1023.63
 },
 {
  "input": "We could replace it with any other positive constant, and you'll get the same family of curves as we tweak that constant. ",
  "translatedText": "נוכל להחליף אותו בכל קבוע חיובי אחר, ונקבל את אותה משפחה של עקומות כאשר אנו משנים את הקבוע הזה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1024.05,
  "end": 1030.49
 },
 {
  "input": "Make it a 2, same family of curves. ",
  "translatedText": "הפוך אותו ל-2, אותה משפחה של עקומות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.51,
  "end": 1033.11
 },
 {
  "input": "Make it a 3, same family of curves. ",
  "translatedText": "הפוך אותו ל-3, אותה משפחה של עקומות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.33,
  "end": 1035.07
 },
 {
  "input": "The reason we use e is that it gives that constant a very readable meaning. ",
  "translatedText": "הסיבה שאנו משתמשים ב-e היא שהוא נותן לאותו הקבוע משמעות קריאה מאוד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1035.75,
  "end": 1039.49
 },
 {
  "input": "Or rather, if we reconfigure things a little bit so that the exponent looks like negative one half times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn this into a probability distribution, that constant sigma will be the standard deviation of that distribution. ",
  "translatedText": "או ליתר דיוק, אם נגדיר את הדברים קצת מחדש כך שהמעריך ייראה כמו מינוס חצי כפול x חלקי קבוע מסוים, שנקרא לו סיגמא בריבוע, אז ברגע שנהפוך את זה להתפלגות הסתברותית, הקבוע סיגמא יהיה סטיית התקן של התפלגות זו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1040.11,
  "end": 1057.21
 },
 {
  "input": "And that's very nice. ",
  "translatedText": "וזה נחמד מאוד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.81,
  "end": 1058.57
 },
 {
  "input": "But before we can interpret this as a probability distribution, we need the area under the curve to be 1. ",
  "translatedText": "אבל לפני שנוכל לפרש זאת כהתפלגות הסתברותית, אנו צריכים שהשטח מתחת לעקומה יהיה 1. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1058.91,
  "end": 1064.31
 },
 {
  "input": "And the reason for that is how the curve is interpreted. ",
  "translatedText": "והסיבה לכך היא איך מתפרשת העקומה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1064.83,
  "end": 1066.91
 },
 {
  "input": "Unlike discrete distributions, when it comes to something continuous, you don't ask about the probability of a particular point. ",
  "translatedText": "בניגוד להתפלגות בדידות, כשמדובר במשהו רציף, לא שואלים על ההסתברות של נקודה מסוימת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1067.37,
  "end": 1073.37
 },
 {
  "input": "Instead, you ask for the probability that a value falls between two different values. ",
  "translatedText": "במקום זאת, שואלים על ההסתברות שערך נופל בין שני ערכים שונים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1073.79,
  "end": 1078.23
 },
 {
  "input": "And what the curve is telling you is that that probability equals the area under the curve between those two values. ",
  "translatedText": "ומה שהעקומה אומרת לך זה שההסתברות הזו שווה לשטח מתחת לעקומה בין שני הערכים האלה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.75,
  "end": 1085.43
 },
 {
  "input": "There's a whole other video about this, they're called probability density functions. ",
  "translatedText": "יש סרטון אחר לגמרי על זה, הן נקראות פונקציות צפיפות הסתברותיות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1086.03,
  "end": 1089.43
 },
 {
  "input": "The main point right now is that the area under the entire curve represents the probability that something happens, that some number comes up. ",
  "translatedText": "הנקודה העיקרית כרגע היא שהשטח מתחת לעקומה כולה מייצג את ההסתברות שמשהו יקרה, שמספר כלשהו עולה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1089.83,
  "end": 1097.15
 },
 {
  "input": "That should be 1, which is why we want the area under this to be 1. ",
  "translatedText": "זה צריך להיות 1, וזו הסיבה שאנחנו רוצים שהשטח מתחת לעקומה יהיה 1. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1097.41,
  "end": 1100.63
 },
 {
  "input": "As it stands with the basic bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root of pi. ",
  "translatedText": "כפי שהוא, עם צורת עקומת הפעמון הבסיסית של e בחזקה השלילית של-x בריבוע, השטח אינו 1, הוא למעשה השורש הריבועי של pi. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1101.05,
  "end": 1107.79
 },
 {
  "input": "I know, right? ",
  "translatedText": "אני יודע, נכון? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.41,
  "end": 1109.15
 },
 {
  "input": "What is pi doing here? ",
  "translatedText": "מה פאי עושה כאן? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1109.27,
  "end": 1110.19
 },
 {
  "input": "What does this have to do with circles? ",
  "translatedText": "מה זה קשור למעגלים? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1110.29,
  "end": 1111.47
 },
 {
  "input": "Like I said at the start, I'd love to talk all about that in the next video. ",
  "translatedText": "כמו שאמרתי בהתחלה, אשמח לדבר על הכל בסרטון הבא. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.01,
  "end": 1115.05
 },
 {
  "input": "But if you can spare your excitement for our purposes right now, all it means is that we should divide this function by the square root of pi, and it gives us the area we want. ",
  "translatedText": "אבל אם אתם יכולים לוותר בינתיים על הציפייה שלכם למטרות שלנו, כל מה שזה אומר הוא שעלינו לחלק את הפונקציה הזו בשורש הריבועי של pi, וזה נותן לנו את השטח שאנחנו רוצים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1115.33,
  "end": 1123.17
 },
 {
  "input": "Throwing back in the constants we had earlier, the 1 half and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square root of 2. ",
  "translatedText": "אם מחזירים את הקבועים שהיו לנו קודם, החצי והסיגמה, ההשפעה שם היא למתוח את הגרף בגורם של סיגמה כפול השורש הריבועי של 2. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1123.61,
  "end": 1131.79
 },
 {
  "input": "So we also need to divide out by that in order to make sure it has an area of 1. ",
  "translatedText": "אז אנחנו צריכים גם לחלק לפי זה כדי לוודא שיש לו שטח של 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.41,
  "end": 1136.47
 },
 {
  "input": "And combining those fractions, the factor out front looks like 1 divided by sigma times the square root of 2 pi. ",
  "translatedText": "ובשילוב השברים האלה, הפקטור בחזית נראה כמו 1 חלקי סיגמה כפול השורש הריבועי של 2 פאי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1136.47,
  "end": 1142.11
 },
 {
  "input": "This, finally, is a valid probability distribution. ",
  "translatedText": "זו, לבסוף, התפלגות הסתברות תקפה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1142.91,
  "end": 1145.85
 },
 {
  "input": "As we tweak that value sigma, resulting in narrower and wider curves, that constant in the front always guarantees that the area equals 1. ",
  "translatedText": "כאשר אנו משנים את הערך של הסיגמה, וכתוצאה מכך מקבלים עקומות צרות ורחבות יותר, הקבוע הקדמי תמיד מבטיח שהשטח שווה ל-1. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1146.45,
  "end": 1154.31
 },
 {
  "input": "The special case where sigma equals 1 has a specific name, we call it the standard normal distribution, which plays an especially important role for you and me in this lesson. ",
  "translatedText": "למקרה המיוחד שבו סיגמא שווה ל-1 יש שם ספציפי, אנו קוראים לו התפלגות נורמלית סטנדרטית, אשר משחקת תפקיד חשוב במיוחד עבורם ובשבילי בשיעור זה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1155.91,
  "end": 1164.51
 },
 {
  "input": "And all possible normal distributions are not only parameterized with this value sigma, but we also subtract off another constant mu from the variable x, and this essentially just lets you slide the graph left and right so that you can prescribe the mean of this distribution. ",
  "translatedText": "וכל ההתפלגויות הנורמליות האפשריות לא רק עוברות פרמטריזציה עם הערך הזה סיגמה, אלא אנחנו גם מפחיתים עוד קבוע mu מהמשתנה x, וזה בעצם רק מאפשר לכם להחליק את הגרף ימינה ושמאלה כדי שתוכלו לקבוע את הממוצע של ההתפלגות הזו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1165.13,
  "end": 1180.21
 },
 {
  "input": "So in short, we have two parameters, one describing the mean, one describing the standard deviation, and they're all tied together in this big formula involving an e and a pi. ",
  "translatedText": "אז בקיצור, יש לנו שני פרמטרים, אחד מתאר את הממוצע, אחד מתאר את סטיית התקן, וכולם קשורים יחד בנוסחה הגדולה הזו הכוללת e ו-pi. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.99,
  "end": 1189.19
 },
 {
  "input": "Now that all of that is on the table, let's look back again at the idea of starting with some random variable and asking what the distributions for sums of that variable look like. ",
  "translatedText": "עכשיו, כשכל זה על הפרק, הבה נסתכל שוב על הרעיון להתחיל באיזה משתנה אקראי ולשאול איך נראות ההתפלגויות לסכומים של המשתנה הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.19,
  "end": 1199.81
 },
 {
  "input": "As we've already gone over, when you increase the size of that sum, the resulting distribution will shift according to a growing mean, and it slowly spreads out according to a growing standard deviation. ",
  "translatedText": "כפי שכבר הסברנו, כאשר אתם מגדילים את גודל הסכום הזה, ההתפלגות המתקבלת תשתנה לפי ממוצע גדל, והיא תתפשט לאט לפי סטיית תקן הולכת וגדלה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1200.13,
  "end": 1209.81
 },
 {
  "input": "And putting some actual formulas to it, if we know the mean of our underlying random variable, we call it mu, and we also know its standard deviation, and we call it sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the standard deviation will be sigma times the square root of that size. ",
  "translatedText": "ואם נבחן כמה נוסחאות ראליות, אם אנחנו יודעים את הממוצע של המשתנה האקראי הבסיסי שלנו, נקרא לו mu, ואנחנו גם יודעים את סטיית התקן שלו, ואנחנו קוראים לזה סיגמה, אז הממוצע של הסכום בתחתית יהיה mu כפול גודל הסכום, וסטיית התקן תהיה סיגמה כפול השורש הריבועי של גודל זה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1210.33,
  "end": 1227.73
 },
 {
  "input": "So now, if we want to claim that this looks more and more like a bell curve, and a bell curve is only described by two different parameters, the mean and the standard deviation, you know what to do. ",
  "translatedText": "אז עכשיו, אם אנחנו רוצים לטעון שזה נראה יותר ויותר כמו עקומת פעמון, ועקומת פעמון מתוארת רק על ידי שני פרמטרים שונים, הממוצע וסטיית התקן, אתם יודעים מה לעשות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1228.19,
  "end": 1237.71
 },
 {
  "input": "You could plug those two values into the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve that should closely fit our distribution. ",
  "translatedText": "אתם יכולים לחבר את שני הערכים האלה לנוסחה, והיא נותנת לכם נוסחה מאוד מפורשת, גם אם סוג של מסובכת, לעקומה שאמורה להתאים היטב להתפלגות שלנו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1237.93,
  "end": 1246.99
 },
 {
  "input": "But there's another way we can describe it that's a little more elegant and lends itself to a very fun visual that we can build up to. ",
  "translatedText": "אבל יש דרך אחרת שבה אנחנו יכולים לתאר את זה שהיא קצת יותר אלגנטית ומביאה לתצוגה מאוד מהנה שאנחנו יכולים לפתח. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1248.39,
  "end": 1254.81
 },
 {
  "input": "Instead of focusing on the sum of all of these random variables, let's modify this expression a little bit, where what we'll do is we'll look at the mean that we expect that sum to take, and we subtract it off so that our new expression has a mean of 0, and then we're going to look at the standard deviation we expect of our sum, and divide out by that, which basically just rescales the units so that the standard deviation of our expression will equal 1. ",
  "translatedText": "במקום להתמקד בסכום של כל המשתנים האקראיים האלה, הבה נשנה מעט את הביטוי הזה, ומה שנעשה הוא להסתכל על הממוצע שאנו מצפים שהסכום הזה יקבל, ונחסר אותו כך שלביטוי החדש שלנו יש ממוצע של 0, ואז נסתכל על סטיית התקן שאנו מצפים מהסכום שלנו, ונחלק בזה, חלוקה שבעצם רק משנה את קנה המידה של היחידות כך שסטיית התקן של הביטוי שלנו תהיה שווה ל-1 . ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1255.27,
  "end": 1278.77
 },
 {
  "input": "This might seem like a more complicated expression, but it actually has a highly readable meaning. ",
  "translatedText": "זה אולי נראה כמו ביטוי מסובך יותר, אבל למעשה יש לו משמעות קריאה מאוד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1279.35,
  "end": 1284.09
 },
 {
  "input": "It's essentially saying how many standard deviations away from the mean is this sum? ",
  "translatedText": "זה בעצם אומר בכמה סטיות תקן הסכום הזה רחוק מהממוצע? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1284.45,
  "end": 1289.67
 },
 {
  "input": "For example, this bar here corresponds to a certain value that you might find when you roll 10 dice and you add them all up, and its position a little above negative 1 is telling you that that value is a little bit less than one standard deviation lower than the mean. ",
  "translatedText": "לדוגמה, סרגל זה כאן מתאים לערך מסוים שאתם עשויים למצוא כאשר אתם מטילים 10 קוביות ומחברים את כולן, והמיקום שלו קצת מעל 1 השלילי אומר לכם שהערך הזה הוא קצת פחות מסטיית תקן אחת נמוך מהממוצע. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1290.75,
  "end": 1303.87
 },
 {
  "input": "Also, by the way, in anticipation for the animation I'm trying to build to here, the way I'm representing things on that lower plot is that the area of each one of these bars is telling us the probability of the corresponding value rather than the height. ",
  "translatedText": "כמו כן, אגב, בציפייה לאנימציה שאני מנסה לבנות לה כאן, הדרך שבה אני מייצג דברים על התרשים התחתון הזה הוא שהשטח של כל אחת מהעמודות האלה מציין את ההסתברות של הערך המתאים ולא את ההגובה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1305.13,
  "end": 1316.99
 },
 {
  "input": "You might think of the y-axis as representing not probability but a kind of probability density. ",
  "translatedText": "אפשר לחשוב על ציר ה-y כמייצג לא הסתברות אלא סוג של צפיפות הסתברות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.23,
  "end": 1321.93
 },
 {
  "input": "The reason for this is to set the stage so that it aligns with the way we interpret continuous distributions, where the probability of falling between a range of values is equal to an area under a curve between those values. ",
  "translatedText": "הסיבה לכך היא להגדיר את השלב כך שיתאים לדרך בה אנו מפרשים התפלגויות רציפות, כאשר ההסתברות ליפול בטווח ערכים שווה לשטח מתחת לעקומה בין הערכים האלו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1322.27,
  "end": 1333.55
 },
 {
  "input": "In particular, the area of all the bars together is going to be 1. ",
  "translatedText": "במיוחד, השטח של כל העמודות יחד יהיה 1. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1333.91,
  "end": 1336.73
 },
 {
  "input": "Now, with all of that in place, let's have a little fun. ",
  "translatedText": "עכשיו, כשכל זה קיים, בואו נהנה קצת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1338.23,
  "end": 1340.95
 },
 {
  "input": "Let me start by rolling things back so that the distribution on the bottom represents a relatively small sum, like adding together only three such random variables. ",
  "translatedText": "תנו לי להתחיל בחזרה לאחור כך שההתפלגות בתחתית מייצגת סכום קטן יחסית, כמו חיבור של רק שלושה משתנים אקראיים כאלה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1341.33,
  "end": 1349.01
 },
 {
  "input": "Notice what happens as I change the distribution we start with. ",
  "translatedText": "שימו לב מה קורה כשאני משנה את ההתפלגות שבה אנחנו מתחילים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.45,
  "end": 1352.43
 },
 {
  "input": "As it changes, the distribution on the bottom completely changes its shape. ",
  "translatedText": "כשהיא משתנה, ההתפלגות בתחתית משנה לחלוטין את צורתה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1352.73,
  "end": 1356.29
 },
 {
  "input": "It's very dependent on what we started with. ",
  "translatedText": "זה מאוד תלוי במה התחלנו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1356.51,
  "end": 1358.77
 },
 {
  "input": "If we let the size of our sum get a little bit bigger, say going up to 10, and as I change the distribution for x, it largely stays looking like a bell curve, but I can find some distributions that get it to change shape. ",
  "translatedText": "אם נניח לגודל הסכום שלנו לגדול קצת יותר, נניח ל-10, וכשאני משנה את ההתפלגות עבור x, הוא נשאר ברובו כמו עקומת פעמון, אבל אני יכול למצוא כמה התפלגויות שגורמות לו לשנות צורה . ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1360.35,
  "end": 1371.63
 },
 {
  "input": "For example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results in this kind of spiky bell curve, and if you'll recall, earlier on I actually showed this in the form of a simulation. ",
  "translatedText": "לדוגמה, המצב המוטה שבו כמעט כל ההסתברות היא במספרים 1 או 6 מביאה לסוג כזה של עקומת פעמון מחודדת, ואם תזכרו, קודם לכן למעשה הראיתי זאת בצורה של סימולציה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1372.23,
  "end": 1383.51
 },
 {
  "input": "So if you were wondering whether that spikiness was an artifact of the randomness or reflected the true distribution, turns out it reflects the true distribution. ",
  "translatedText": "אז אם תהיתם אם הדוקרנות הזו היא תוצאה של האקראיות או שיקפה את ההתפלגות האמיתית, מסתבר שהיא משקפת את ההתפלגות האמיתית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1384.13,
  "end": 1391.85
 },
 {
  "input": "In this case, 10 is not a large enough sum for the central limit theorem to kick in. ",
  "translatedText": "במקרה זה, 10 אינו סכום מספיק גדול כדי שמשפט הגבול המרכזי יתחיל לפעול. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.29,
  "end": 1396.47
 },
 {
  "input": "But if instead I let that sum grow and I consider adding 50 different values, which is actually not that big, then no matter how I change the distribution for our underlying random variable, it has essentially no effect on the shape of the plot on the bottom. ",
  "translatedText": "אבל אם במקום זאת אתן לסכום הזה לגדול ואני שוקל להוסיף 50 ערכים שונים, שהם למעשה לא כל כך גדולים, אז לא משנה איך אני משנה את ההתפלגות עבור המשתנה האקראי הבסיסי שלנו, אין לזה השפעה בעצם על צורת התרשים ב- תַחתִית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1396.47,
  "end": 1410.69
 },
 {
  "input": "No matter where we start, all of the information and nuance for the distribution of x gets washed away, and we tend towards this single universal shape described by a very elegant function for the standard normal distribution, 1 over square root of 2 pi times e to the negative x squared over 2. ",
  "translatedText": "לא משנה מאיפה נתחיל, כל המידע והניואנסים להתפלגות x נעלמים, ואנו נוטים לצורה אוניברסלית יחידה זו המתוארת על ידי פונקציה אלגנטית מאוד עבור ההתפלגות הנורמלית הסטנדרטית, 1 חלקי שורש ריבועי של 2 pi כפול e בחזקת מינוס-X בריבוע חלקי 2. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1411.17,
  "end": 1427.07
 },
 {
  "input": "This, this right here is what the central limit theorem is all about. ",
  "translatedText": "זהו, זה בדיוק מה שמשפט הגבול המרכזי עוסק בו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1427.81,
  "end": 1430.81
 },
 {
  "input": "Almost nothing you can do to this initial distribution changes the shape we tend towards. ",
  "translatedText": "כמעט שום דבר שאתם יכולים לעשות להתפלגות הראשונית הזו משנה את הצורה שאנו נוטים לכיוונה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1431.13,
  "end": 1435.31
 },
 {
  "input": "Now, the more theoretically minded among you might still be wondering, what is the actual theorem? ",
  "translatedText": "כעת, מי שביניכם בעלי גישה יותר תיאורטית אולי עדיין תוהה, מהו המשפט בפועל? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1439.03,
  "end": 1444.51
 },
 {
  "input": "Like, what's the mathematical statement that could be proved or disproved that we're claiming here? ",
  "translatedText": "זא, מהי האמירה המתמטית שניתן להוכיח או להפריך שאנו טוענים כאן? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1444.81,
  "end": 1448.91
 },
 {
  "input": "If you want a nice formal statement, here's how it might go. ",
  "translatedText": "אם אתם רוצים הצהרה רשמית חביבה, הנה איך זה יכול ללכת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1449.03,
  "end": 1451.67
 },
 {
  "input": "Consider this value, where we're summing up n different instantiations of our random variable, but tweaked and tuned so that its mean and standard deviation are 1. ",
  "translatedText": "קחו בחשבון את הערך הזה, שבו אנו מסכמים n מופעים שונים של המשתנה האקראי שלנו, אך מכוונים ומכווננים כך שהממוצע וסטיית התקן שלו הם 1. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1452.13,
  "end": 1459.89
 },
 {
  "input": "Again, meaning you can read it as asking how many standard deviations away from the mean is the sum. ",
  "translatedText": "שוב, כלומר אתם יכולים לקרוא את זה כשאלה בכמה סטיות תקן הסכום רחוק מהממוצע. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1460.23,
  "end": 1465.35
 },
 {
  "input": "Then the actual rigorous no-jokes-this-time statement of the central limit theorem is that if you consider the probability that this value falls between two given real numbers, a and b, and you consider the limit of that probability as the size of your sum goes to infinity, then that limit is equal to a certain integral, which basically describes the area under a standard normal distribution between those two values. ",
  "translatedText": "אז ההצהרה הקפדנית האמיתית ללא בדיחות-הפעם של משפט הגבול המרכזי היא שאם אתם מחשיבים את ההסתברות שהערך הזה נופל בין שני מספרים ממשיים נתונים, a ו-b, ואתה מחשיב את הגבול של ההסתברות הזו כגודל של הסכום שלכם באינסוף, ואז הגבול הזה שווה לאינטגרל מסוים, שמתאר בעצם את השטח תחת התפלגות נורמלית סטנדרטית בין שני הערכים הללו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1465.77,
  "end": 1489.65
 },
 {
  "input": "Again, there are three underlying assumptions that I have yet to tell you, but other than those, in all of its gory detail, this right here is the central limit theorem. ",
  "translatedText": "שוב, ישנן שלוש הנחות יסוד שעדיין לא סיפרתי לכם עליהן, אבל חוץ מאלה, על כל הפרטים המרשימים שלו, ממש כאן זהו משפט הגבול המרכזי. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1491.25,
  "end": 1500.03
 },
 {
  "input": "All of that is a bit theoretical, so it might be helpful to bring things back down to Earth and turn back to the concrete example that I mentioned at the start, where you imagine rolling a die 100 times, and let's assume it's a fair die for this example, and you add together the results. ",
  "translatedText": "כל זה קצת תיאורטי, אז זה עשוי להיות מועיל להחזיר את הדברים לכדור הארץ ולחזור לדוגמה הקונקרטית שהזכרתי בהתחלה, שבה אתם מדמיינים הטלת קובייה 100 פעמים, ובואו נניח שזו קובייה הוגנת עבור דוגמה זו, ואתם מחברים את התוצאות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1504.55,
  "end": 1518.13
 },
 {
  "input": "The challenge for you is to find a range of values such that you're 95% sure that the sum will fall within this range. ",
  "translatedText": "האתגר עבורכם הוא למצוא טווח של ערכים כך שאתם בטוחים ב-95% שהסכום יהיה בטווח הזה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1518.87,
  "end": 1525.83
 },
 {
  "input": "For questions like this, there's a handy rule of thumb about normal distributions, which is that about 68% of your values are going to fall within one standard deviation of the mean, 95% of your values, the thing we care about, fall within two standard deviations of the mean, and a whopping 99.7% of your values will fall within three standard deviations of the mean. ",
  "translatedText": "לשאלות כאלה, יש כלל אצבע שימושי לגבי התפלגויות נורמליות, שהוא שכ-68% מהערכים עומדים ליפול בתוך סטיית תקן אחת מהממוצע, 95% מהערכים, הדבר שאכפת לנו ממנו, נופלים בתוך שתי סטיות תקן של הממוצע, ו-99.7%  מהערכים שלך יפלו בתוך שלוש סטיות תקן מהממוצע. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1527.13,
  "end": 1546.97
 },
 {
  "input": "It's a rule of thumb that's commonly memorized by people who do a lot of probability and stats. ",
  "translatedText": "זה כלל אצבע שמשונן בדרך כלל על ידי אנשים שעוסקים הרבה בהסתברות וסטטיסטיקות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1547.45,
  "end": 1551.45
 },
 {
  "input": "Naturally, this gives us what we need for our example, and let me go ahead and draw out what this would look like, where I'll show the distribution for a fair die up at the top, and the distribution for a sum of 100 such dice on the bottom, which by now as you know looks like a certain normal distribution. ",
  "translatedText": "כמובן, זה נותן לנו את מה שאנחנו צריכים עבור הדוגמה שלנו, ותנו לי להמשיך ולשרטט איך זה ייראה, כאשר אני אראה את ההתפלגות עבור קובייה הוגנת בחלק העליון, ואת ההתפלגות עבור סכום של 100 קוביות כאלה בתחתית, שעד עכשיו, כפי שאתם יודעים, נראית כמו התפלגות נורמלית מסוימת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1552.49,
  "end": 1567.29
 },
 {
  "input": "Step one with a problem like this is to find the mean of your initial distribution, which in this case will look like 1 6th times 1 plus 1 6th times 2 on and on and on, and works out to be 3.5. ",
  "translatedText": "שלב ראשון עם בעיה כזו הוא למצוא את הממוצע של ההתפלגות הראשונית שלכם, שבמקרה זה תראה כמו שישית כפול 1 ועוד שישית כפול 2 והלאה והלאה, והתוצאה המתקבלת היא 3. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1567.95,
  "end": 1578.91
 },
 {
  "input": "We also need the standard deviation, which requires calculating the variance, which as you know involves adding all the squares of the differences between the values and the means, and it works out to be 2.92, square root of that comes out to be 1.71. ",
  "translatedText": "5. צריך גם את סטיית התקן שדורשת את חישוב השונות, שכידוע כרוכה בהוספת כל הריבועים של ההפרשים בין הערכים והאמצעים, ומסתבר שהיא 2.92, השורש הריבועי של זה יוצא כ-1.71. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1579.41,
  "end": 1592.43
 },
 {
  "input": "Those are the only two numbers we need, and I will invite you again to reflect on how magical it is that those are the only two numbers that you need to completely understand the bottom distribution. ",
  "translatedText": "אלה שני המספרים היחידים שאנחנו צריכים, ואני אזמין אותכם לחשוב שוב על כמה זה קסום שאלו שני המספרים היחידים שאתם צריכים כדי להבין לחלוטין את ההתפלגות התחתונה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1592.95,
  "end": 1601.69
 },
 {
  "input": "Its mean will be 100 times mu, which is 350, and its standard deviation will be the square root of 100 times sigma, so 10 times sigma 17.1. ",
  "translatedText": "הממוצע שלו יהיה פי 100 mu , שהו 350, וסטיית התקן שלו תהיה השורש הריבועי של פי 100 סיגמא, אז פי 10 סיגמא 17.1. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1602.43,
  "end": 1612.61
 },
 {
  "input": "Remembering our handy rule of thumb, we're looking for values two standard deviations away from the mean, and when you subtract 2 sigma from the mean you end up with about 316, and when you add 2 sigma you end up with 384. ",
  "translatedText": "נזכור את כלל האצבע השימושי שלנו, אנו מחפשים ערכים במרחק שתי סטיות תקן מהממוצע, וכשאתם מפחיתים 2 סיגמא מהממוצע אתם מגיעים ל-316 בערך, וכשאתם מוסיפים 2 סיגמא אתם מגיעים ל-384. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1613.03,
  "end": 1626.33
 },
 {
  "input": "And there you go, that gives us the answer. ",
  "translatedText": "והנה, זה נותן לנו את התשובה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1627.35,
  "end": 1628.95
 },
 {
  "input": "Okay, I promised to wrap things up shortly, but while we're on this example there's one more question that's worth your time to ponder. ",
  "translatedText": "אוקיי, הבטחתי לסכם את הדברים בקרוב, אבל בזמן שאנחנו בדוגמה הזו יש עוד שאלה אחת ששווה את הזמן שלכם להרהר בה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1631.47,
  "end": 1637.45
 },
 {
  "input": "Instead of just asking about the sum of 100 die rolls, let's say I had you divide that number by 100, which basically means all the numbers in our diagram in the bottom get divided by 100. ",
  "translatedText": "במקום פשוט לשאול על הסכום של 100 הטלות, נניח שהייתי מחלק את המספר הזה ב-100, כלומר בעצם כל המספרים בתרשים שלנו בתחתית מחולקים ב-100. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1638.25,
  "end": 1648.09
 },
 {
  "input": "Take a moment to interpret what this all would be saying then. ",
  "translatedText": "קחו רגע לפרש מה כל מה שזה אומר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1648.57,
  "end": 1651.57
 },
 {
  "input": "The expression essentially tells you the empirical average for 100 different die rolls, and that interval we found is now telling you what range you are expecting to see for that empirical average. ",
  "translatedText": "הביטוי בעצם נותן את הממוצע האמפירי עבור 100 הטלות שונות של קוביות, והמרווח הזה שמצאנו אומר לכם איזה טווח אתם מצפים לראות עבור הממוצע האמפירי הזה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1652.07,
  "end": 1663.49
 },
 {
  "input": "In other words, you might expect it to be around 3.5, that's the expected value for a die roll, but what's much less obvious and what the central limit theorem lets you compute is how close to that expected value you'll reasonably find yourself. ",
  "translatedText": "במילים אחרות, אתם עשויים לצפות שזה יהיה בסביבות 3.5, שזהו הערך הצפוי להטלת קובייה, אבל מה שהרבה פחות ברור ומה שמשפט הגבול המרכזי מאפשר לכם לחשב זה כמה קרוב לערך הצפוי הזה תמצאו את עצמכם באופן סביר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1664.35,
  "end": 1676.57
 },
 {
  "input": "In particular, it's worth your time to take a moment mulling over what the standard deviation for this empirical average is, and what happens to it as you look at a bigger and bigger sample of die rolls. ",
  "translatedText": "בפרט, שווה את הזמן שלכם להרהר רגע מה סטיית התקן של הממוצע האמפירי הזה, ומה קורה לו כשאתם מסתכלים על מדגם גדול יותר ויותר של הטלות של קוביות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1677.59,
  "end": 1687.13
 },
 {
  "input": "Lastly, but probably most importantly, let's talk about the assumptions that go into this theorem. ",
  "translatedText": "לבסוף, אבל כנראה הכי חשוב, בואו נדבר על ההנחות שנכנסות למשפט הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.95,
  "end": 1697.41
 },
 {
  "input": "The first one is that all of these variables that we're adding up are independent from each other. ",
  "translatedText": "הראשונה היא שכל המשתנים האלה שאנחנו מוסיפים הם בלתי תלויים זה בזה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1698.01,
  "end": 1702.53
 },
 {
  "input": "The outcome of one process doesn't influence the outcome of any other process. ",
  "translatedText": "התוצאה של תהליך אחד לא משפיעה על התוצאה של שום תהליך אחר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.85,
  "end": 1706.31
 },
 {
  "input": "The second is that all of these variables are drawn from the same distribution. ",
  "translatedText": "השניה היא שכל המשתנים הללו נשלפים מאותה התפלגות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1707.25,
  "end": 1710.95
 },
 {
  "input": "Both of these have been implicitly assumed with our dice example. ",
  "translatedText": "שתי אלו הונחו באופן מרומז עם דוגמה לקוביות שלנו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1711.31,
  "end": 1714.39
 },
 {
  "input": "We've been treating the outcome of each die roll as independent from the outcome of all the others, and we're assuming that each die follows the same distribution. ",
  "translatedText": "התייחסנו לתוצאה של כל קובייה כבלתי תלויה בתוצאה של כל האחרות, ואנחנו מניחים שכל קוביה עוקבת אחר אותה התפלגות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1714.79,
  "end": 1722.03
 },
 {
  "input": "Sometimes in the literature you'll see these two assumptions lumped together under the initials IID for independent and identically distributed. ",
  "translatedText": "לפעמים בספרות תראו את שתי ההנחות הללו מחוברות יחד תחת ראשי התיבות IID (independent and identically distributed). ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1722.85,
  "end": 1729.91
 },
 {
  "input": "One situation where these assumptions are decidedly not true would be the Galton board. ",
  "translatedText": "מצב אחד שבו הנחות אלו אינן נכונות בהחלט יהיה לוח גלטון. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1730.53,
  "end": 1735.11
 },
 {
  "input": "I mean, think about it. ",
  "translatedText": "כלומר, תחשבו על זה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1735.71,
  "end": 1736.83
 },
 {
  "input": "Is it the case that the way a ball bounces off of one of the pegs is independent from how it's going to bounce off the next peg? ",
  "translatedText": "האם זה המקרה שהאופן שבו כדור קופץ מאחת מהיתדות אינו תלוי באיך הוא עומד לקפוץ מהיתד הבא? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1736.97,
  "end": 1743.19
 },
 {
  "input": "Absolutely not. ",
  "translatedText": "בהחלט לא. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1743.83,
  "end": 1744.61
 },
 {
  "input": "Depending on the last bounce, it's coming in with a completely different trajectory. ",
  "translatedText": "בהתאם לקפיצה האחרונה, מגיע מסלול שונה לחלוטין. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1744.77,
  "end": 1747.87
 },
 {
  "input": "And is it the case that the distribution of possible outcomes off of each peg are the same for each peg that it hits? ",
  "translatedText": "והאם זהו המצב שהתפלגות התוצאות האפשריות מכל יתד זהה לכל יתד שבו הוא פוגע? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1748.21,
  "end": 1754.67
 },
 {
  "input": "Again, almost certainly not. ",
  "translatedText": "שוב, כמעט בטוח שלא. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1755.19,
  "end": 1756.71
 },
 {
  "input": "Maybe it hits one peg glancing to the left, meaning the outcomes are hugely skewed in that direction, and then hits the next one glancing to the right. ",
  "translatedText": "אולי הוא פוגע ביתד אחד ונזרק שמאלה, כלומר התוצאות מוטות מאוד בכיוון הזה, ואז פוגע ביתד הבא ומועף ימינה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1756.71,
  "end": 1763.71
 },
 {
  "input": "When I made all those simplifying assumptions in the opening example, it wasn't just to make this easier to think about. ",
  "translatedText": "כשהנחתי את כל ההנחות המפשטות בדוגמה הפותחת, זה לא היה רק כדי להקל על המחשבה על זה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1765.73,
  "end": 1771.63
 },
 {
  "input": "It's also that those assumptions were necessary for this to actually be an example of the central limit theorem. ",
  "translatedText": "זה היה גם כי ההנחות הללו היו הכרחיות כדי שזו תהיה למעשה דוגמה למשפט הגבול המרכזי. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1771.97,
  "end": 1777.07
 },
 {
  "input": "Nevertheless, it seems to be true that for the real Galton board, despite violating both of these, a normal distribution does kind of come about? ",
  "translatedText": "עם זאת, נראה שזה נכון שעבור הלוח האמיתי של גלטון, למרות הפרה של שתי אלו, אכן נוצרת התפלגות נורמלית? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1778.13,
  "end": 1785.47
 },
 {
  "input": "Part of the reason might be that there are generalizations of the theorem beyond the scope of this video that relax these assumptions, especially the second one. ",
  "translatedText": "חלק מהסיבה עשויה להיות שיש הכללות של המשפט מעבר להיקף הסרטון הזה שמחלישות את ההנחות הללו, במיוחד את השנייה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1786.05,
  "end": 1793.89
 },
 {
  "input": "But I do want to caution you against the fact that many times people seem to assume that a variable is normally distributed, even when there's no actual justification to do so. ",
  "translatedText": "אבל אני כן רוצה להזהיר אותכם מהעובדה שפעמים רבות נראה שאנשים מניחים שמשתנה מתפלג באופן נורמאלי, גם כשאין הצדקה ממשית לעשות זאת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1794.49,
  "end": 1803.07
 },
 {
  "input": "The third assumption is actually fairly subtle. ",
  "translatedText": "ההנחה השלישית היא למעשה עדינה למדי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1804.29,
  "end": 1806.21
 },
 {
  "input": "It's that the variance we've been computing for these variables is finite. ",
  "translatedText": "היא שהשונות שחישבנו עבור המשתנים האלה היא סופית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1806.21,
  "end": 1810.27
 },
 {
  "input": "This was never an issue for the dice example, because there were only six possible outcomes. ",
  "translatedText": "זה אף פעם לא היה בעיה עבור דוגמת הקוביות, כי היו רק שש תוצאות אפשריות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1810.81,
  "end": 1814.85
 },
 {
  "input": "But in certain situations where you have an infinite set of outcomes, when you go to compute the variance, the sum ends up diverging off to infinity. ",
  "translatedText": "אבל במצבים מסוימים שבהם יש לכם קבוצה אינסופית של תוצאות, כאשר אתם הולכים לחשב את השונות, הסכום מסתיים בסופו של דבר באינסוף. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1815.03,
  "end": 1822.51
 },
 {
  "input": "These can be perfectly valid probability distributions, and they do come up in practice. ",
  "translatedText": "אלו יכולות להיות התפלגויות הסתברות תקפות לחלוטין, והן מופיעות בפועל. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1823.45,
  "end": 1827.25
 },
 {
  "input": "But in those situations, as you consider adding many different instantiations of that variable and letting that sum approach infinity, even if the first two assumptions hold, it is very much a possibility that the thing you tend towards is not actually a normal distribution. ",
  "translatedText": "אבל במצבים האלה, כאשר אתם שוקלים להוסיף מופעים רבים ושונים של אותו משתנה ולתת לסכום הזה להתקרב לאינסוף, גם אם שתי ההנחות הראשונות מתקיימות, קיימת אפשרות מאוד מעשית שהדבר שאתם נוטים לכיוונו אינו למעשה התפלגות נורמלית. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1827.55,
  "end": 1841.19
 },
 {
  "input": "If you've understood everything up to this point, you now have a very strong foundation in what the central limit theorem is all about. ",
  "translatedText": "אם הבנתם הכל עד לנקודה זו, כעת יש לכם בסיס חזק מאוד במה עוסק משפט הגבול המרכזי. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1842.15,
  "end": 1847.65
 },
 {
  "input": "And next up, I'd like to explain why it is that this particular function is the thing that we tend towards, and why it has a pi in it, what it has to do with circles. ",
  "translatedText": "ובהמשך, אני רוצה להסביר מדוע הפונקציה הספציפית הזו היא הדבר שאליו אנו נוטים לכיוונו, למה יש בה פאי, ומה זה קשור למעגלים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1848.29,
  "end": 1874.17
 }
]