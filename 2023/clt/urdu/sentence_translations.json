[
 {
  "input": "This is a Galton board. ",
  "translatedText": "یہ گالٹن بورڈ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 1.26
 },
 {
  "input": "Maybe you've seen one before, it's a popular demonstration of how, even when a single event is chaotic and random, with an effectively unknowable outcome, it's still possible to make precise statements about a large number of events, namely how the relative proportions for many different outcomes are distributed. ",
  "translatedText": "ہوسکتا ہے کہ آپ نے پہلے دیکھا ہو، یہ اس بات کا ایک مقبول مظاہرہ ہے کہ جب کوئی ایک واقعہ افراتفری اور بے ترتیب ہو، مؤثر طریقے سے نا معلوم نتیجہ کے ساتھ، تب بھی یہ ممکن ہے کہ بڑی تعداد میں واقعات کے بارے میں قطعی بیانات دیے جائیں، یعنی کس طرح متعلقہ تناسب بہت سے مختلف نتائج کے لئے تقسیم کر رہے ہیں. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2.52,
  "end": 18.3
 },
 {
  "input": "More specifically, the Galton board illustrates one of the most prominent distributions in all of probability, known as the normal distribution, more colloquially known as a bell curve, and also called a Gaussian distribution. ",
  "translatedText": "مزید خاص طور پر، گالٹن بورڈ تمام امکانات میں سب سے نمایاں تقسیم کی وضاحت کرتا ہے، جسے عام تقسیم کے نام سے جانا جاتا ہے، جسے بول چال میں گھنٹی کے وکر کے نام سے جانا جاتا ہے، اور اسے گاوسی تقسیم بھی کہا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.38,
  "end": 31.9
 },
 {
  "input": "There's a very specific function to describe this distribution, it's very pretty, we'll get into it later, but right now I just want to emphasize how the normal distribution is, as the name suggests, very common, it shows up in a lot of seemingly unrelated contexts. ",
  "translatedText": "اس تقسیم کو بیان کرنے کے لیے ایک خاص فنکشن ہے، یہ بہت خوبصورت ہے، ہم بعد میں اس پر غور کریں گے، لیکن ابھی میں صرف اس بات پر زور دینا چاہتا ہوں کہ نارمل ڈسٹری بیوشن کیسی ہے، جیسا کہ نام سے پتہ چلتا ہے، بہت عام، یہ بہت زیادہ ظاہر ہوتا ہے۔بظاہر غیر متعلقہ سیاق و سباق کا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 32.5,
  "end": 45.04
 },
 {
  "input": "If you were to take a large number of people who sit in a similar demographic and plot their heights, those heights tend to follow a normal distribution. ",
  "translatedText": "اگر آپ ایسے لوگوں کی ایک بڑی تعداد کو لینا چاہتے ہیں جو اسی طرح کی آبادی میں بیٹھتے ہیں اور اپنی اونچائیوں کی منصوبہ بندی کرتے ہیں، تو وہ بلندیاں ایک عام تقسیم کی پیروی کرتی ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.02,
  "end": 53.0
 },
 {
  "input": "If you look at a large swath of very big natural numbers and you ask how many distinct prime factors does each one of those numbers have, the answers will very closely track with a certain normal distribution. ",
  "translatedText": "اگر آپ بہت بڑے قدرتی اعداد کی ایک بڑی تعداد کو دیکھتے ہیں اور آپ پوچھتے ہیں کہ ان نمبروں میں سے ہر ایک کے کتنے الگ الگ بنیادی عوامل ہیں، تو جوابات ایک خاص عام تقسیم کے ساتھ بہت قریب سے ٹریک کریں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 53.66,
  "end": 64.96
 },
 {
  "input": "Now our topic for today is one of the crown jewels in all of probability theory, it's one of the key facts that explains why this distribution is as common as it is, known as the central limit theorem. ",
  "translatedText": "اب ہمارا آج کا موضوع تمام امکانی تھیوری میں تاج کے زیورات میں سے ایک ہے، یہ ان اہم حقائق میں سے ایک ہے جو اس بات کی وضاحت کرتا ہے کہ یہ تقسیم اتنی ہی عام کیوں ہے، جسے مرکزی حد نظریہ کے نام سے جانا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.58,
  "end": 76.02
 },
 {
  "input": "This lesson is meant to go back to the basics, giving you the fundamentals on what the central limit theorem is saying, what normal distributions are, and I want to assume minimal background. ",
  "translatedText": "اس سبق کا مقصد بنیادی باتوں پر واپس جانا ہے، آپ کو بنیادی باتیں بتانا ہے کہ مرکزی حد نظریہ کیا کہہ رہا ہے، عام تقسیم کیا ہیں، اور میں کم سے کم پس منظر کو فرض کرنا چاہتا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.64,
  "end": 85.26
 },
 {
  "input": "We're going to go decently deep into it, but after this I'd still like to go deeper and explain why the theorem is true, why the function underlying the normal distribution has the very specific form that it does, why that formula has a pi in it, and, most fun, why those last two facts are actually more related than a lot of traditional explanations would suggest. ",
  "translatedText": "ہم اس میں اچھی طرح سے گہرائی میں جانے والے ہیں، لیکن اس کے بعد میں مزید گہرائی میں جانا چاہوں گا اور یہ بتانا چاہوں گا کہ تھیوریم کیوں درست ہے، عام تقسیم کے تحت کام کرنے والے فنکشن کی مخصوص شکل کیوں ہوتی ہے، کیوں کہ اس فارمولے میں اس میں ایک pi، اور، سب سے زیادہ مزے کے، کیوں کہ وہ آخری دو حقائق درحقیقت بہت سی روایتی وضاحتوں سے زیادہ متعلق ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.26,
  "end": 105.56
 },
 {
  "input": "That second lesson is also meant to be the follow-on to the convolutions video that I promised, so there's a lot of interrelated topics here. ",
  "translatedText": "اس دوسرے سبق کا مطلب بھی convolutions ویڈیو کی پیروی کرنا ہے جس کا میں نے وعدہ کیا تھا، لہذا یہاں بہت سارے باہم منسلک موضوعات ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.48,
  "end": 113.37
 },
 {
  "input": "But right now, back to the fundamentals, I'd like to kick things off with a overly simplified model of the Galton board. ",
  "translatedText": "لیکن ابھی، بنیادی باتوں کی طرف، میں گالٹن بورڈ کے حد سے زیادہ آسان ماڈل کے ساتھ چیزوں کو شروع کرنا چاہوں گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.57,
  "end": 119.17
 },
 {
  "input": "In this model we will assume that each ball falls directly onto a certain central peg and that it has a 50-50 probability of bouncing to the left or to the right, and we'll think of each of those outcomes as either adding one or subtracting one from its position. ",
  "translatedText": "اس ماڈل میں ہم یہ فرض کریں گے کہ ہر گیند براہ راست ایک خاص مرکزی کھونٹی پر گرتی ہے اور اس کے بائیں یا دائیں طرف اچھالنے کا 50-50 امکان ہوتا ہے، اور ہم ان میں سے ہر ایک کے نتائج کے بارے میں سوچیں گے کہ یا تو ایک کو شامل کرنا یا ایک کو اس کی پوزیشن سے گھٹانا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.89,
  "end": 134.11
 },
 {
  "input": "Once one of those is chosen, we make the highly unrealistic assumption that it happens to land dead on in the middle of the peg adjacent below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the right. ",
  "translatedText": "ان میں سے کسی ایک کو منتخب کرنے کے بعد، ہم انتہائی غیر حقیقت پسندانہ مفروضہ بناتے ہیں کہ یہ اس کے نیچے سے ملحقہ کھونٹی کے وسط میں مردہ ہو جائے گا، جہاں اسے دوبارہ بائیں طرف اچھالنے کے 50-50 انتخاب کا سامنا کرنا پڑے گا۔دائیں طرف. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.67,
  "end": 147.07
 },
 {
  "input": "For the one I'm showing on screen, there are five different rows of pegs, so our little hopping ball makes five different random choices between plus one and minus one, and we can think of its final position as basically being the sum of all of those different numbers, which in this case happens to be one, and we might label all of the different buckets with the sum that they represent. ",
  "translatedText": "جس کو میں اسکرین پر دکھا رہا ہوں، اس کے لیے کھونٹوں کی پانچ مختلف قطاریں ہیں، اس لیے ہماری چھوٹی ہاپنگ گیند جمع ایک اور مائنس ون کے درمیان پانچ مختلف بے ترتیب انتخاب کرتی ہے، اور ہم اس کی آخری پوزیشن کے بارے میں سوچ سکتے ہیں کہ بنیادی طور پر سب کا مجموعہ ہے۔ان مختلف نمبروں میں سے، جو اس معاملے میں ایک ہوتا ہے، اور ہم تمام مختلف بالٹیوں کو اس رقم کے ساتھ لیبل لگا سکتے ہیں جس کی وہ نمائندگی کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.43,
  "end": 166.35
 },
 {
  "input": "As we repeat this, we're looking at different possible sums for those five random numbers. ",
  "translatedText": "جیسا کہ ہم اسے دہراتے ہیں، ہم ان پانچ بے ترتیب نمبروں کے لیے مختلف ممکنہ رقمیں دیکھ رہے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 166.35,
  "end": 171.29
 },
 {
  "input": "And for those of you who are inclined to complain that this is a highly unrealistic model for the true Galton board, let me emphasize the goal right now is not to accurately model physics. ",
  "translatedText": "اور آپ میں سے ان لوگوں کے لیے جو یہ شکایت کرنے پر مائل ہیں کہ یہ حقیقی گالٹن بورڈ کے لیے ایک انتہائی غیر حقیقی ماڈل ہے، میں ابھی اس بات پر زور دیتا ہوں کہ فزکس کو درست طریقے سے ماڈل بنانا نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.05,
  "end": 181.67
 },
 {
  "input": "The goal is to give a simple example to illustrate the central limit theorem, and for that, idealized though this might be, it actually gives us a really good example. ",
  "translatedText": "مقصد مرکزی حد نظریہ کو واضح کرنے کے لیے ایک سادہ سی مثال دینا ہے، اور اس کے لیے، اگرچہ یہ ہو سکتا ہے، مثالی بنایا جائے، یہ ہمیں واقعی ایک اچھی مثال دیتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 181.83,
  "end": 190.03
 },
 {
  "input": "If we let many different balls fall, making yet another unrealistic assumption that they don't influence each other as if they're all ghosts, then the number of balls that fall into each different bucket gives us some loose sense for how likely each one of those buckets is. ",
  "translatedText": "اگر ہم بہت سی مختلف گیندوں کو گرنے دیتے ہیں، اور ایک اور غیر حقیقی مفروضہ بناتے ہیں کہ وہ ایک دوسرے پر اثر انداز نہیں ہوتے ہیں جیسے کہ وہ سب بھوت ہیں، تو پھر ہر ایک بالٹی میں گرنے والی گیندوں کی تعداد سے ہمیں کچھ ڈھیلا احساس ملتا ہے کہ ہر ایک کا کتنا امکان ہے۔ان بالٹیوں میں سے ہے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.57,
  "end": 203.39
 },
 {
  "input": "In this example, the numbers are simple enough that it's not too hard to explicitly calculate what the probability is for falling into each bucket. ",
  "translatedText": "اس مثال میں، اعداد اتنے سادہ ہیں کہ ہر بالٹی میں گرنے کا امکان کیا ہے اس کا واضح طور پر حساب لگانا زیادہ مشکل نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.83,
  "end": 210.01
 },
 {
  "input": "If you do want to think that through, you'll find it very reminiscent of Pascal's triangle. ",
  "translatedText": "اگر آپ اس کے ذریعے سوچنا چاہتے ہیں، تو آپ کو یہ پاسکل کے مثلث کی بہت یاد دلاتا نظر آئے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.27,
  "end": 213.83
 },
 {
  "input": "But the neat thing about our theorem is how far it goes beyond the simple examples. ",
  "translatedText": "لیکن ہمارے نظریہ کے بارے میں صاف بات یہ ہے کہ یہ سادہ مثالوں سے کتنا آگے ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.95,
  "end": 218.27
 },
 {
  "input": "So to start off at least, rather than making explicit calculations, let's just simulate things by running a large number of samples and letting the total number of results in each different outcome give us some sense for what that distribution looks like. ",
  "translatedText": "لہٰذا کم از کم شروع کرنے کے لیے، واضح حساب کتاب کرنے کے بجائے، آئیے نمونے کی ایک بڑی تعداد کو چلا کر اور ہر ایک مختلف نتائج میں نتائج کی کل تعداد کو دے کر ہمیں کچھ سمجھ دیں کہ تقسیم کیسی نظر آتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.67,
  "end": 229.97
 },
 {
  "input": "As I said, the one on screen has five rows, so each sum that we're considering includes only five numbers. ",
  "translatedText": "جیسا کہ میں نے کہا، اسکرین پر موجود ایک میں پانچ قطاریں ہیں، لہذا ہر رقم جس پر ہم غور کر رہے ہیں اس میں صرف پانچ نمبر شامل ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 230.45,
  "end": 236.21
 },
 {
  "input": "The basic idea of the central limit theorem is that if you increase the size of that sum, for example here that would mean increasing the number of rows of pegs for each ball to bounce off, then the distribution that describes where that sum is going to fall looks more and more like a bell curve. ",
  "translatedText": "مرکزی حد تھیوریم کا بنیادی خیال یہ ہے کہ اگر آپ اس رقم کے سائز میں اضافہ کرتے ہیں، مثال کے طور پر یہاں اس کا مطلب یہ ہوگا کہ ہر گیند کو اچھالنے کے لیے پیگز کی قطاروں کی تعداد میں اضافہ، پھر وہ تقسیم جو بیان کرتی ہے کہ وہ رقم کہاں جا رہی ہے۔زوال زیادہ سے زیادہ گھنٹی کے وکر کی طرح لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.81,
  "end": 253.33
 },
 {
  "input": "Here, it's actually worth taking a moment to write down that general idea. ",
  "translatedText": "یہاں، اس عمومی خیال کو لکھنے کے لیے ایک لمحہ نکالنا درحقیقت قابل قدر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.47,
  "end": 258.35
 },
 {
  "input": "The setup is that we have a random variable, and that's basically shorthand for a random process where each outcome of that process is associated with some number. ",
  "translatedText": "سیٹ اپ یہ ہے کہ ہمارے پاس ایک بے ترتیب متغیر ہے، اور یہ بنیادی طور پر ایک بے ترتیب عمل کے لیے شارٹ ہینڈ ہے جہاں اس عمل کا ہر نتیجہ کسی نہ کسی تعداد سے منسلک ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 259.27,
  "end": 268.19
 },
 {
  "input": "We'll call that random number x. ",
  "translatedText": "ہم اس بے ترتیب نمبر x کو کال کریں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 268.49,
  "end": 269.97
 },
 {
  "input": "For example, each bounce off the peg is a random process modeled with two outcomes. ",
  "translatedText": "مثال کے طور پر، پیگ سے ہر ایک اچھال ایک بے ترتیب عمل ہے جس کا نمونہ دو نتائج کے ساتھ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.97,
  "end": 274.39
 },
 {
  "input": "Those outcomes are associated with the numbers negative one and positive one. ",
  "translatedText": "وہ نتائج منفی ایک اور مثبت نمبروں سے وابستہ ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.85,
  "end": 277.89
 },
 {
  "input": "Another example of a random variable would be rolling a die, where you have six different outcomes, each one associated with a number. ",
  "translatedText": "بے ترتیب متغیر کی ایک اور مثال ڈائی کو رول کرنا ہوگی، جہاں آپ کے چھ مختلف نتائج ہوں گے، ہر ایک نمبر سے وابستہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.53,
  "end": 284.83
 },
 {
  "input": "What we're doing is taking multiple different samples of that variable and adding them all together. ",
  "translatedText": "ہم کیا کر رہے ہیں اس متغیر کے متعدد مختلف نمونے لینا اور ان سب کو ایک ساتھ شامل کرنا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.47,
  "end": 290.41
 },
 {
  "input": "On our Galton board, that looks like letting the ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice and adding up the results. ",
  "translatedText": "ہمارے گیلٹن بورڈ پر، ایسا لگتا ہے کہ گیند کو نیچے کی طرف جاتے ہوئے متعدد مختلف پیگز کو اچھالنے دیا جائے، اور مرنے کی صورت میں، آپ بہت سے مختلف ڈائس کو رول کرنے اور نتائج کو شامل کرنے کا تصور کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.77,
  "end": 300.97
 },
 {
  "input": "The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into different possible values, will look more and more like a bell curve. ",
  "translatedText": "مرکزی حد نظریہ کا دعویٰ یہ ہے کہ جیسے جیسے آپ اس رقم کے سائز کو بڑا اور بڑا ہونے دیں گے، تو اس رقم کی تقسیم، مختلف ممکنہ قدروں میں پڑنے کا کتنا امکان ہے، گھنٹی کے وکر کی طرح زیادہ سے زیادہ نظر آئے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.43,
  "end": 314.11
 },
 {
  "input": "That's it, that is the general idea. ",
  "translatedText": "یہ ہے، یہ عام خیال ہے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.43,
  "end": 317.13
 },
 {
  "input": "Over the course of this lesson, our job is to make that statement more quantitative. ",
  "translatedText": "اس سبق کے دوران، ہمارا کام اس بیان کو زیادہ مقداری بنانا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 317.55,
  "end": 321.53
 },
 {
  "input": "We're going to put some numbers to it, put some formulas to it, show how you can use it to make predictions. ",
  "translatedText": "ہم اس پر کچھ نمبر لگانے جا رہے ہیں، اس میں کچھ فارمولے ڈالیں گے، یہ دکھائیں گے کہ آپ اسے پیشن گوئی کرنے کے لیے کیسے استعمال کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.07,
  "end": 326.35
 },
 {
  "input": "For example, here's the kind of question I want you to be able to answer by the end of this video. ",
  "translatedText": "مثال کے طور پر، یہاں اس قسم کا سوال ہے جس کا میں آپ کو اس ویڈیو کے آخر تک جواب دینا چاہتا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.21,
  "end": 331.57
 },
 {
  "input": "Suppose you rolled the die 100 times and you added together the results. ",
  "translatedText": "فرض کریں کہ آپ نے ڈائی کو 100 بار رول کیا اور آپ نے نتائج کو ایک ساتھ شامل کیا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.19,
  "end": 335.89
 },
 {
  "input": "Could you find a range of values such that you're 95% sure that the sum will fall within that range? ",
  "translatedText": "کیا آپ قدروں کی ایک رینج تلاش کر سکتے ہیں کہ آپ کو 95% یقین ہو کہ رقم اس حد میں آئے گی؟ یا شاید مجھے یہ کہنا چاہئے کہ اقدار کی سب سے چھوٹی ممکنہ حد تلاش کریں جیسے کہ یہ سچ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.63,
  "end": 342.17
 },
 {
  "input": "Or maybe I should say find the smallest possible range of values such that this is true. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.83,
  "end": 346.55
 },
 {
  "input": "The neat thing is you'll be able to answer this question whether it's a fair die or if it's a weighted die. ",
  "translatedText": "صاف بات یہ ہے کہ آپ اس سوال کا جواب دینے کے قابل ہو جائیں گے کہ آیا یہ منصفانہ ڈائی ہے یا یہ ایک وزنی ڈائی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 347.39,
  "end": 352.13
 },
 {
  "input": "Now let me say at the top that this theorem has three different assumptions that go into it, three things that have to be true before the theorem follows. ",
  "translatedText": "اب میں اوپر کہتا ہوں کہ اس تھیوریم کے تین مختلف مفروضے ہیں جو اس میں جاتے ہیں، تین چیزیں جو تھیوریم کی پیروی کرنے سے پہلے درست ہونی چاہئیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.45,
  "end": 360.13
 },
 {
  "input": "And I'm actually not going to tell you what they are until the very end of the video. ",
  "translatedText": "اور میں اصل میں آپ کو یہ نہیں بتانے جا رہا ہوں کہ ویڈیو کے بالکل آخر تک وہ کیا ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 360.43,
  "end": 363.79
 },
 {
  "input": "Instead I want you to keep your eye out and see if you can notice and maybe predict what those three assumptions are going to be. ",
  "translatedText": "اس کے بجائے میں چاہتا ہوں کہ آپ اپنی نظریں باہر رکھیں اور دیکھیں کہ کیا آپ نوٹس کر سکتے ہیں اور شاید پیش گوئی کر سکتے ہیں کہ وہ تین مفروضے کیا ہونے جا رہے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 364.27,
  "end": 369.67
 },
 {
  "input": "As a next step, to better illustrate just how general this theorem is, I want to run a couple more simulations for you focused on the dice example. ",
  "translatedText": "اگلے مرحلے کے طور پر، یہ واضح کرنے کے لیے کہ یہ نظریہ کتنا عام ہے، میں آپ کے لیے ڈائس کی مثال پر توجہ مرکوز کرنے کے لیے چند مزید نقلیں چلانا چاہتا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.71,
  "end": 377.39
 },
 {
  "input": "Usually if you think of rolling a die you think of the six outcomes as being equally probable, but the theorem actually doesn't care about that. ",
  "translatedText": "عام طور پر اگر آپ ڈائی رول کرنے کے بارے میں سوچتے ہیں تو آپ ان چھ نتائج کے بارے میں سوچتے ہیں جو اتنے ہی ممکنہ ہیں، لیکن نظریہ دراصل اس کی پرواہ نہیں کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 380.91,
  "end": 387.63
 },
 {
  "input": "We could start with a weighted die, something with a non-trivial distribution across the outcomes, and the core idea still holds. ",
  "translatedText": "ہم ایک وزنی ڈائی کے ساتھ شروع کر سکتے ہیں، نتائج میں غیر معمولی تقسیم کے ساتھ، اور بنیادی خیال اب بھی برقرار ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 387.83,
  "end": 394.55
 },
 {
  "input": "For the simulation what I'll do is take some distribution like this one that is skewed towards lower values. ",
  "translatedText": "تخروپن کے لئے میں کیا کروں گا اس طرح کی کچھ تقسیم لوں گا جو کم اقدار کی طرف متوجہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.03,
  "end": 399.93
 },
 {
  "input": "I'm going to take 10 distinct samples from that distribution and then I'll record the sum of that sample on the plot on the bottom. ",
  "translatedText": "میں اس تقسیم سے 10 الگ الگ نمونے لینے جا رہا ہوں اور پھر میں نیچے والے پلاٹ پر اس نمونے کا مجموعہ ریکارڈ کروں گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 400.25,
  "end": 407.55
 },
 {
  "input": "Then I'm going to do this many many different times, always with a sum of size 10, but keep track of where those sums ended up to give us a sense of the distribution. ",
  "translatedText": "پھر میں یہ بہت سے مختلف بار کرنے جا رہا ہوں، ہمیشہ سائز 10 کی رقم کے ساتھ، لیکن اس بات پر نظر رکھیں کہ وہ رقمیں کہاں ختم ہوئیں تاکہ ہمیں تقسیم کا اندازہ ہو سکے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 408.63,
  "end": 416.59
 },
 {
  "input": "And in fact let me rescale the y direction to give us room to run an even larger number of samples. ",
  "translatedText": "اور درحقیقت مجھے y سمت کو دوبارہ ترتیب دینے دیں تاکہ ہمیں اس سے بھی زیادہ تعداد میں نمونے چلانے کے لیے جگہ دی جائے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.97,
  "end": 424.73
 },
 {
  "input": "And I'll let it go all the way up to a couple thousand, and as it does you'll notice that the shape that starts to emerge looks like a bell curve. ",
  "translatedText": "اور میں اسے ایک دو ہزار تک جانے دوں گا، اور جیسا کہ آپ دیکھیں گے کہ جو شکل ابھرنا شروع ہوتی ہے وہ گھنٹی کے وکر کی طرح دکھائی دیتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.03,
  "end": 432.49
 },
 {
  "input": "Maybe if you squint your eyes you can see it skews a tiny bit to the left, but it's neat that something so symmetric emerged from a starting point that was so asymmetric. ",
  "translatedText": "ہو سکتا ہے کہ اگر آپ اپنی آنکھیں گھماتے ہیں تو آپ دیکھ سکتے ہیں کہ یہ بائیں طرف تھوڑا سا جھکا ہوا ہے، لیکن یہ صاف ہے کہ اس قدر متوازی نقطہ آغاز سے ابھرا جو بہت غیر متناسب تھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.87,
  "end": 441.01
 },
 {
  "input": "To better illustrate what the central limit theorem is all about, let me run four of these simulations in parallel, where on the upper left I'm doing it where we're only adding two dice at a time, on the upper right we're doing it where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at a time, and then we'll do another one with a bigger sum, 15 at a time. ",
  "translatedText": "مرکزی حد تھیوریم کے بارے میں بہتر طور پر واضح کرنے کے لیے، مجھے ان میں سے چار نقلیں متوازی طور پر چلانے دیں، جہاں اوپری بائیں جانب میں یہ کر رہا ہوں جہاں ہم ایک وقت میں صرف دو نرد جوڑ رہے ہیں، اوپری دائیں جانب ہم' اسے دوبارہ کر رہے ہیں جہاں ہم ایک وقت میں پانچ نرد جوڑ رہے ہیں، نیچے بائیں وہ ہے جسے ہم نے ابھی ایک وقت میں 10 نرد جوڑتے ہوئے دیکھا ہے، اور پھر ہم ایک بڑی رقم کے ساتھ ایک اور کریں گے، ایک وقت میں 15۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 441.47,
  "end": 461.37
 },
 {
  "input": "Notice how on the upper left when we're just adding two dice, the resulting distribution doesn't really look like a bell curve, it looks a lot more reminiscent of the one we started with skewed towards the left. ",
  "translatedText": "غور کریں کہ اوپری بائیں طرف جب ہم صرف دو ڈائس جوڑ رہے ہوتے ہیں، نتیجے میں تقسیم درحقیقت گھنٹی کے منحنی خطوط کی طرح نظر نہیں آتی، یہ اس سے کہیں زیادہ یاد دلاتا ہے جسے ہم نے بائیں جانب ترچھے کے ساتھ شروع کیا تھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.25,
  "end": 472.03
 },
 {
  "input": "But as we allow for more and more dice in each sum, the resulting shape that comes up in these distributions looks more and more symmetric. ",
  "translatedText": "لیکن جیسا کہ ہم ہر رقم میں زیادہ سے زیادہ نرد کی اجازت دیتے ہیں، نتیجے میں جو شکل ان تقسیموں میں آتی ہے وہ زیادہ سے زیادہ ہم آہنگ نظر آتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.81,
  "end": 479.81
 },
 {
  "input": "It has the lump in the middle and fade towards the tail's shape of a bell curve. ",
  "translatedText": "اس کے درمیان میں گانٹھ ہوتی ہے اور دم کی طرف گھنٹی کے منحنی شکل کی طرف دھندلا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.95,
  "end": 483.89
 },
 {
  "input": "And let me emphasize again, you can start with any different distribution. ",
  "translatedText": "اور مجھے دوبارہ زور دینے دو، آپ کسی بھی مختلف تقسیم کے ساتھ شروع کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 487.05,
  "end": 490.49
 },
 {
  "input": "Here I'll run it again, but where most of the probability is tied up in the numbers 1 and 6, with very low probability for the mid values. ",
  "translatedText": "یہاں میں اسے دوبارہ چلاؤں گا، لیکن جہاں زیادہ تر امکان نمبر 1 اور 6 میں بندھا ہوا ہے، درمیانی اقدار کے لیے بہت کم امکان کے ساتھ۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.49,
  "end": 497.49
 },
 {
  "input": "Despite completely changing the distribution for an individual roll of the die, it's still the case that a bell curve shape will emerge as we consider the different sums. ",
  "translatedText": "ڈائی کے انفرادی رول کی تقسیم کو مکمل طور پر تبدیل کرنے کے باوجود، یہ اب بھی ایسا ہی ہے کہ جب ہم مختلف رقموں پر غور کریں گے تو گھنٹی کے منحنی شکل ابھرے گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.19,
  "end": 506.55
 },
 {
  "input": "Illustrating things with a simulation like this is very fun, and it's kind of neat to see order emerge from chaos, but it also feels a little imprecise. ",
  "translatedText": "اس طرح کے نقالی کے ساتھ چیزوں کی تصویر کشی کرنا بہت مزہ آتا ہے، اور یہ دیکھ کر صاف ستھرا لگتا ہے کہ ترتیب کو افراتفری سے ابھرتا ہے، لیکن یہ تھوڑا سا غلط بھی محسوس ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.27,
  "end": 515.03
 },
 {
  "input": "Like in this case, when I cut off the simulation at 3000 samples, even though it kind of looks like a bell curve, the different buckets seem pretty spiky. ",
  "translatedText": "جیسا کہ اس معاملے میں، جب میں نے 3000 نمونوں میں نقلی کو کاٹ دیا، اگرچہ یہ گھنٹی کے منحنی شکل کی طرح لگتا ہے، مختلف بالٹیاں کافی تیز لگتی ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 515.39,
  "end": 522.99
 },
 {
  "input": "And you might wonder, is it supposed to look that way, or is that just an artifact of the randomness in the simulation? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 522.99,
  "end": 528.55
 },
 {
  "input": "And if it is, how many samples do we need before we can be sure that what we're looking at is representative of the true distribution? ",
  "translatedText": "اور آپ حیران ہوسکتے ہیں، کیا یہ اس طرح نظر آنا چاہئے، یا یہ نقلی میں بے ترتیب پن کا صرف ایک نمونہ ہے؟ اور اگر یہ ہے، تو ہمیں کتنے نمونوں کی ضرورت ہے اس سے پہلے کہ ہم اس بات کا یقین کر لیں کہ ہم جو دیکھ رہے ہیں وہ حقیقی تقسیم کا نمائندہ ہے؟ آگے بڑھنے کے بجائے، آئیے تھوڑا اور نظریاتی حاصل کریں اور وہ قطعی شکل دکھائیں جو یہ تقسیم طویل مدت میں لے گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 529.01,
  "end": 535.11
 },
 {
  "input": "Instead moving forward, let's get a little more theoretical and show the precise shape that these distributions will take on in the long run. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.19,
  "end": 545.47
 },
 {
  "input": "The easiest case to make this calculation is if we have a uniform distribution, where each possible face of the die has an equal probability, 1 6th. ",
  "translatedText": "اس حساب کو کرنے کا سب سے آسان معاملہ یہ ہے کہ اگر ہمارے پاس یکساں تقسیم ہے، جہاں مرنے کے ہر ممکنہ چہرے کا امکان برابر ہے، 1 6۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.13,
  "end": 553.97
 },
 {
  "input": "For example, if you then want to know how likely different sums are for a pair of dice, it's essentially a counting game, where you count up how many distinct pairs take on the same sum, which in the diagram I've drawn, you can conveniently think about by going through all of the different diagonals. ",
  "translatedText": "مثال کے طور پر، اگر آپ پھر یہ جاننا چاہتے ہیں کہ نرد کے جوڑے کے لیے مختلف رقمیں ہونے کا کتنا امکان ہے، یہ بنیادی طور پر گنتی کا ایک کھیل ہے، جہاں آپ گنتے ہیں کہ ایک ہی رقم پر کتنے الگ الگ جوڑے ہوتے ہیں، جو میں نے خاکہ میں کھینچا ہے، آپ تمام مختلف اخترن سے گزر کر آسانی سے سوچ سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.99,
  "end": 568.49
 },
 {
  "input": "Since each such pair has an equal chance of showing up, 1 in 36, all you have to do is count the sizes of these buckets. ",
  "translatedText": "چونکہ اس طرح کے ہر جوڑے کے ظاہر ہونے کا مساوی موقع ہے، 36 میں سے 1، آپ کو بس ان بالٹیوں کے سائز کو گننا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.41,
  "end": 577.53
 },
 {
  "input": "That gives us a definitive shape for the distribution describing a sum of two dice, and if we were to play the same game with all possible triplets, the resulting distribution would look like this. ",
  "translatedText": "یہ ہمیں تقسیم کے لیے ایک قطعی شکل دیتا ہے جس میں دو نرد کا مجموعہ بیان کیا جاتا ہے، اور اگر ہم ایک ہی کھیل کو تمام ممکنہ ٹرپلٹس کے ساتھ کھیلتے ہیں، تو نتیجے میں تقسیم اس طرح نظر آئے گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.19,
  "end": 588.13
 },
 {
  "input": "Now what's more challenging, but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that single die. ",
  "translatedText": "اب اس سے زیادہ چیلنج کیا ہے، لیکن بہت زیادہ دلچسپ، یہ پوچھنا ہے کہ اگر ہمارے پاس اس سنگل ڈائی کے لیے غیر یکساں تقسیم ہو تو کیا ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 588.69,
  "end": 594.99
 },
 {
  "input": "We actually talked all about this in the last video. ",
  "translatedText": "ہم نے اصل میں اس کے بارے میں پچھلی ویڈیو میں بات کی تھی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.55,
  "end": 597.97
 },
 {
  "input": "You do essentially the same thing, you go through all the distinct pairs of dice which add up to the same value. ",
  "translatedText": "آپ بنیادی طور پر ایک ہی کام کرتے ہیں، آپ نرد کے تمام الگ الگ جوڑوں سے گزرتے ہیں جو ایک ہی قدر میں اضافہ کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.45,
  "end": 603.67
 },
 {
  "input": "It's just that instead of counting those pairs, for each pair you multiply the two probabilities of each particular face coming up, and then you add all those together. ",
  "translatedText": "یہ صرف اتنا ہے کہ ان جوڑوں کو گننے کے بجائے، ہر جوڑے کے لیے آپ ہر ایک مخصوص چہرے کے سامنے آنے والے دو احتمالات کو ضرب دیتے ہیں، اور پھر آپ ان سب کو ایک ساتھ شامل کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 603.97,
  "end": 612.75
 },
 {
  "input": "The computation that does this for all possible sums has a fancy name, it's called a convolution, but it's essentially just the weighted version of the counting game that anyone who's played with a pair of dice already finds familiar. ",
  "translatedText": "جو حساب کتاب ہر ممکنہ رقم کے لیے ایسا کرتا ہے اس کا ایک فینسی نام ہے، اسے کنوولوشن کہا جاتا ہے، لیکن یہ بنیادی طور پر گنتی کے کھیل کا صرف وزنی ورژن ہے جسے ڈائس کے جوڑے کے ساتھ کھیلنے والے کو پہلے سے ہی واقف معلوم ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 613.29,
  "end": 624.47
 },
 {
  "input": "For our purposes in this lesson, I'll have the computer calculate all that, simply display the results for you, and invite you to observe certain patterns, but under the hood, this is what's going on. ",
  "translatedText": "اس سبق میں ہمارے مقاصد کے لیے، میں کمپیوٹر سے ان سب کا حساب لگاؤں گا، بس آپ کے لیے نتائج دکھاؤں گا، اور آپ کو کچھ نمونوں کا مشاہدہ کرنے کی دعوت دوں گا، لیکن ہڈ کے نیچے، یہی ہو رہا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.03,
  "end": 635.33
 },
 {
  "input": "So just to be crystal clear on what's being represented here, if you imagine sampling two different values from that top distribution, the one describing a single die, and adding them together, then the second distribution I'm drawing represents how likely you are to see various different sums. ",
  "translatedText": "تو صرف اس بات پر واضح ہونے کے لیے کہ یہاں کس چیز کی نمائندگی کی جا رہی ہے، اگر آپ تصور کرتے ہیں کہ اس ٹاپ ڈسٹری بیوشن سے دو مختلف قدروں کا نمونہ لینا، ایک واحد ڈائی کو بیان کرنا، اور ان کو ایک ساتھ شامل کرنا، تو دوسری تقسیم جو میں ڈرائنگ کر رہا ہوں، اس بات کی نمائندگی کرتا ہے کہ آپ کے کتنے امکانات ہیں۔مختلف مختلف رقمیں دیکھیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 636.65,
  "end": 652.23
 },
 {
  "input": "Likewise, if you imagine sampling three distinct values from that top distribution, and adding them together, the next plot represents the probabilities for various different sums in that case. ",
  "translatedText": "اسی طرح، اگر آپ تصور کرتے ہیں کہ اس ٹاپ ڈسٹری بیوشن سے تین الگ الگ قدروں کا نمونہ لیں، اور انہیں ایک ساتھ شامل کریں، تو اگلا پلاٹ اس معاملے میں مختلف مختلف رقوم کے امکانات کی نمائندگی کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 652.89,
  "end": 662.49
 },
 {
  "input": "So if I compute what the distributions for these sums look like for larger and larger sums, well you know what I'm going to say, it looks more and more like a bell curve. ",
  "translatedText": "لہذا اگر میں شمار کرتا ہوں کہ ان رقوم کی تقسیم بڑی اور بڑی رقوم کے لیے کیسی دکھتی ہے، تو آپ جانتے ہیں کہ میں کیا کہنے جا رہا ہوں، یہ زیادہ سے زیادہ گھنٹی کے وکر کی طرح لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 663.51,
  "end": 672.39
 },
 {
  "input": "But before we get to that, I want you to make a couple more simple observations. ",
  "translatedText": "لیکن اس سے پہلے کہ ہم اس تک پہنچیں، میں چاہتا ہوں کہ آپ کچھ اور آسان مشاہدات کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.35,
  "end": 676.45
 },
 {
  "input": "For example, these distributions seem to be wandering to the right, and also they seem to be getting more spread out, and a little bit more flat. ",
  "translatedText": "مثال کے طور پر، ایسا لگتا ہے کہ یہ تقسیم دائیں طرف گھوم رہی ہیں، اور یہ بھی لگتا ہے کہ وہ مزید پھیلتی جا رہی ہیں، اور تھوڑا سا زیادہ فلیٹ۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.45,
  "end": 684.79
 },
 {
  "input": "You cannot describe the central limit theorem quantitatively without taking into account both of those effects, which in turn requires describing the mean and the standard deviation. ",
  "translatedText": "آپ ان دونوں اثرات کو مدنظر رکھے بغیر مرکزی حد نظریہ کو مقداری طور پر بیان نہیں کر سکتے، جس کے نتیجے میں وسط اور معیاری انحراف کو بیان کرنے کی ضرورت ہوتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.25,
  "end": 693.19
 },
 {
  "input": "Maybe you're already familiar with those, but I want to make minimal assumptions here, and it never hurts to review, so let's quickly go over both of those. ",
  "translatedText": "ہوسکتا ہے کہ آپ ان سے پہلے ہی واقف ہوں، لیکن میں یہاں کم سے کم مفروضے کرنا چاہتا ہوں، اور اس کا جائزہ لینے میں کبھی تکلیف نہیں ہوتی، تو آئیے ان دونوں پر جلدی سے غور کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 693.95,
  "end": 700.61
 },
 {
  "input": "The mean of a distribution, often denoted with the Greek letter mu, is a way of capturing the center of mass for that distribution. ",
  "translatedText": "تقسیم کا مطلب، اکثر یونانی حرف mu سے ظاہر ہوتا ہے، اس تقسیم کے لیے مرکزِ ماس کو حاصل کرنے کا ایک طریقہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 703.41,
  "end": 710.71
 },
 {
  "input": "It's calculated as the expected value of our random variable, which is a way of saying you go through all of the different possible outcomes, and you multiply the probability of that outcome times the value of the variable. ",
  "translatedText": "اس کا حساب ہمارے بے ترتیب متغیر کی متوقع قدر کے طور پر کیا جاتا ہے، جو یہ کہنے کا ایک طریقہ ہے کہ آپ تمام مختلف ممکنہ نتائج سے گزرتے ہیں، اور آپ اس نتیجے کے امکان کو متغیر کی قدر سے ضرب دیتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 711.19,
  "end": 722.85
 },
 {
  "input": "If higher values are more probable, that weighted sum is going to be bigger. ",
  "translatedText": "اگر اعلی قدریں زیادہ ممکنہ ہیں، تو وہ وزنی رقم بڑی ہونے والی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 723.19,
  "end": 726.41
 },
 {
  "input": "If lower values are more probable, that weighted sum is going to be smaller. ",
  "translatedText": "اگر کم قدریں زیادہ ممکنہ ہیں، تو وہ وزنی رقم چھوٹی ہونے والی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.75,
  "end": 729.95
 },
 {
  "input": "A little more interesting is if you want to measure how spread out this distribution is, because there's multiple different ways you might do it. ",
  "translatedText": "تھوڑی اور دلچسپ بات یہ ہے کہ اگر آپ یہ پیمائش کرنا چاہتے ہیں کہ یہ تقسیم کس حد تک پھیلی ہوئی ہے، کیونکہ اس کے متعدد مختلف طریقے ہیں جو آپ کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.79,
  "end": 737.13
 },
 {
  "input": "One of them is called the variance. ",
  "translatedText": "ان میں سے ایک کو تغیر کہا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 738.53,
  "end": 740.29
 },
 {
  "input": "The idea there is to look at the difference between each possible value and the mean, square that difference, and ask for its expected value. ",
  "translatedText": "وہاں کا خیال یہ ہے کہ ہر ممکنہ قدر اور اوسط کے درمیان فرق کو دیکھیں، اس فرق کو مربع کریں، اور اس کی متوقع قدر پوچھیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.83,
  "end": 748.27
 },
 {
  "input": "The idea is that whether your value is below or above the mean, when you square that difference, you get a positive number, and the larger the difference, the bigger that number. ",
  "translatedText": "خیال یہ ہے کہ چاہے آپ کی قیمت اوسط سے نیچے ہو یا اس سے اوپر، جب آپ اس فرق کو مربع کرتے ہیں، تو آپ کو ایک مثبت نمبر ملتا ہے، اور فرق جتنا بڑا ہوگا، اتنا ہی بڑا نمبر ملتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 748.73,
  "end": 756.65
 },
 {
  "input": "Squaring it like this turns out to make the math much much nicer than if we did something like an absolute value, but the downside is that it's hard to think about this as a distance in our diagram because the units are off. ",
  "translatedText": "اس کو اس طرح مربع کرنا ریاضی کو اس سے کہیں زیادہ بہتر بناتا ہے کہ اگر ہم نے مطلق قدر کی طرح کچھ کیا ہے، لیکن منفی پہلو یہ ہے کہ اس کے بارے میں ہمارے خاکے میں فاصلے کے طور پر سوچنا مشکل ہے کیونکہ یونٹ بند ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 757.37,
  "end": 768.13
 },
 {
  "input": "Kind of like the units here are square units, whereas a distance in our diagram would be a kind of linear unit. ",
  "translatedText": "اس طرح کی اکائیاں یہاں مربع اکائیاں ہیں، جب کہ ہمارے خاکے میں فاصلہ ایک قسم کی لکیری اکائی ہوگی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 768.33,
  "end": 773.31
 },
 {
  "input": "So another way to measure spread is what's called the standard deviation, which is the square root of this value. ",
  "translatedText": "تو پھیلاؤ کی پیمائش کرنے کا ایک اور طریقہ وہ ہے جسے معیاری انحراف کہا جاتا ہے، جو اس قدر کا مربع جڑ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 773.71,
  "end": 779.19
 },
 {
  "input": "That can be interpreted much more reasonably as a distance on our diagram, and it's commonly denoted with the Greek letter sigma, so you know m for mean as for standard deviation, but both in Greek. ",
  "translatedText": "اس کی تشریح ہمارے خاکے پر ایک فاصلے کے طور پر بہت زیادہ معقول طور پر کی جا سکتی ہے، اور اسے عام طور پر یونانی حرف سگما سے ظاہر کیا جاتا ہے، اس لیے آپ m کو معیاری انحراف کے لیے جانتے ہیں، لیکن دونوں یونانی میں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 779.47,
  "end": 789.65
 },
 {
  "input": "Looking back at our sequence of distributions, let's talk about the mean and standard deviation. ",
  "translatedText": "ہماری تقسیم کی ترتیب کو دیکھتے ہوئے، آئیے اوسط اور معیاری انحراف کے بارے میں بات کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.87,
  "end": 796.15
 },
 {
  "input": "If we call the mean of the initial distribution mu, which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if I tell you that the mean of the next one is 2 times mu. ",
  "translatedText": "اگر ہم ابتدائی تقسیم کے اوسط کو mu کہتے ہیں، جو کہ ایک کے لیے 2 ہوتا ہے۔24، امید ہے کہ یہ زیادہ حیرت کی بات نہیں ہوگی اگر میں آپ کو بتاؤں کہ اگلے کا مطلب 2 گنا mu ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.63,
  "end": 806.73
 },
 {
  "input": "That is, you roll a pair of dice, you want to know the expected value of the sum, it's two times the expected value for a single die. ",
  "translatedText": "یعنی، آپ ڈائس کا ایک جوڑا رول کرتے ہیں، آپ رقم کی متوقع قیمت جاننا چاہتے ہیں، یہ سنگل ڈائی کی متوقع قیمت سے دو گنا زیادہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 807.13,
  "end": 812.81
 },
 {
  "input": "Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth. ",
  "translatedText": "اسی طرح، ہمارے سائز 3 کے مجموعے کی متوقع قدر 3 گنا mu ہے، اور اسی طرح آگے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 813.85,
  "end": 819.41
 },
 {
  "input": "The mean just marches steadily on to the right, which is why our distributions seem to be drifting off in that direction. ",
  "translatedText": "وسط صرف دائیں طرف مستقل طور پر مارچ کرتا ہے، یہی وجہ ہے کہ ہماری تقسیم اس سمت میں بہتی ہوئی نظر آتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 819.63,
  "end": 824.87
 },
 {
  "input": "A little more challenging, but very important, is to describe how the standard deviation changes. ",
  "translatedText": "تھوڑا زیادہ مشکل، لیکن بہت اہم، یہ بیان کرنا ہے کہ معیاری انحراف کیسے بدلتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.35,
  "end": 829.91
 },
 {
  "input": "The key fact here is that if you have two different random variables, then the variance for the sum of those variables is the same as just adding together the original two variances. ",
  "translatedText": "یہاں اہم حقیقت یہ ہے کہ اگر آپ کے پاس دو مختلف بے ترتیب متغیرات ہیں، تو ان متغیرات کے مجموعہ کا تغیر وہی ہے جیسا کہ اصل دو تغیرات کو ایک ساتھ شامل کرنا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 830.49,
  "end": 839.37
 },
 {
  "input": "This is one of those facts that you can just compute when you unpack all the definitions. ",
  "translatedText": "یہ ان حقائق میں سے ایک ہے جس کا شمار آپ اس وقت کر سکتے ہیں جب آپ تمام تعریفیں کھول دیتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 839.93,
  "end": 843.63
 },
 {
  "input": "There are a couple nice intuitions for why it's true. ",
  "translatedText": "یہ سچ کیوں ہے اس کے لیے کچھ اچھے وجدان ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 843.63,
  "end": 846.21
 },
 {
  "input": "My tentative plan is to just actually make a series about probability and talk about things like intuitions underlying variance and its cousins there. ",
  "translatedText": "میرا عارضی منصوبہ صرف اصل میں امکان کے بارے میں ایک سیریز بنانا ہے اور ان چیزوں کے بارے میں بات کرنا ہے جیسے بنیادی تغیرات اور اس کے کزن۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 846.63,
  "end": 853.53
 },
 {
  "input": "But right now, the main thing I want you to highlight is how it's the variance that adds, it's not the standard deviation that adds. ",
  "translatedText": "لیکن اس وقت، میں آپ کو جس چیز پر روشنی ڈالنا چاہتا ہوں وہ یہ ہے کہ یہ وہ تغیر ہے جو شامل کرتا ہے، یہ وہ معیاری انحراف نہیں ہے جو اضافہ کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 854.01,
  "end": 860.15
 },
 {
  "input": "So, critically, if you were to take n different realizations of the same random variable and ask what the sum looks like, the variance of that sum is n times the variance of your original variable, meaning the standard deviation, the square root of all this, is the square root of n times the original standard deviation. ",
  "translatedText": "لہٰذا، تنقیدی طور پر، اگر آپ ایک ہی بے ترتیب متغیر کے مختلف ادراکوں کو لیتے ہیں اور پوچھتے ہیں کہ رقم کیسی نظر آتی ہے، تو اس رقم کا تغیر آپ کے اصل متغیر کے n گنا ہوگا، یعنی معیاری انحراف، سب کا مربع جڑ یہ، اصل معیاری انحراف کے n گنا کا مربع جڑ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 860.41,
  "end": 878.25
 },
 {
  "input": "For example, back in our sequence of distributions, if we label the standard deviation of our initial one with sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on and so forth. ",
  "translatedText": "مثال کے طور پر، ہماری تقسیم کی ترتیب میں، اگر ہم اپنے ابتدائی کے معیاری انحراف کو سگما کے ساتھ لیبل کرتے ہیں، تو اگلا معیاری انحراف 2 گنا سگما کا مربع جڑ ہوگا، اور اس کے بعد یہ اس کے مربع جڑ کی طرح لگتا ہے۔3 بار سگما، اور اسی طرح اور آگے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 879.29,
  "end": 893.09
 },
 {
  "input": "This, like I said, is very important. ",
  "translatedText": "یہ، جیسا کہ میں نے کہا، بہت اہم ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.75,
  "end": 895.65
 },
 {
  "input": "It means that even though our distributions are getting spread out, they're not spreading out all that quickly, they only do so in proportion to the square root of the size of the sum. ",
  "translatedText": "اس کا مطلب یہ ہے کہ اگرچہ ہماری تقسیمیں پھیل رہی ہیں، وہ اتنی تیزی سے نہیں پھیل رہی ہیں، وہ صرف رقم کے سائز کے مربع جڑ کے تناسب سے ایسا کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.07,
  "end": 904.13
 },
 {
  "input": "As we prepare to make a more quantitative description of the central limit theorem, the core intuition I want you to keep in your head is that we'll basically realign all of these distributions so that their means line up together, and then rescale them so that all of the standard deviations are just going to be equal to 1. ",
  "translatedText": "جب ہم مرکزی حد نظریہ کی مزید مقداری وضاحت کرنے کی تیاری کرتے ہیں، تو میں آپ کو اپنے ذہن میں رکھنے کی بنیادی وجدان یہ ہے کہ ہم بنیادی طور پر ان تمام تقسیموں کو دوبارہ ترتیب دیں گے تاکہ ان کے ذرائع آپس میں مل جائیں، اور پھر ان کو دوبارہ ترتیب دیں کہ تمام معیاری انحراف صرف 1 کے برابر ہونے جا رہے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 904.71,
  "end": 920.61
 },
 {
  "input": "And when we do that, the shape that results gets closer and closer to a certain universal shape, described with an elegant little function that we'll unpack in just a moment. ",
  "translatedText": "اور جب ہم ایسا کرتے ہیں، تو وہ شکل جس کے نتیجے میں ایک خاص عالمگیر شکل کے قریب تر ہوتا جاتا ہے، جسے ایک خوبصورت چھوٹے فنکشن کے ساتھ بیان کیا جاتا ہے جسے ہم صرف ایک لمحے میں کھول دیں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 921.29,
  "end": 929.87
 },
 {
  "input": "And let me say one more time, the real magic here is how we could have started with any distribution, describing a single roll of the die, and if we play the same game, considering what the distributions for the many different sums look like, and we realign them so that the means line up, and we rescale them so that the standard deviations are all 1, we still approach that same universal shape, which is kind of mind-boggling. ",
  "translatedText": "اور میں ایک بار اور کہوں، یہاں اصل جادو یہ ہے کہ ہم کسی بھی تقسیم کے ساتھ کیسے شروعات کر سکتے تھے، ڈائی کے ایک رول کو بیان کرتے ہوئے، اور اگر ہم ایک ہی کھیل کو کھیلتے ہیں، اس بات پر غور کرتے ہوئے کہ بہت سی مختلف رقموں کی تقسیم کیسی نظر آتی ہے، اور ہم ان کو دوبارہ ترتیب دیتے ہیں تاکہ ذرائع ایک قطار میں لگ جائیں، اور ہم ان کو دوبارہ اسکیل کرتے ہیں تاکہ معیاری انحرافات تمام 1 ہوں، ہم پھر بھی اسی عالمگیر شکل تک پہنچتے ہیں، جو کہ ایک طرح کی ذہن ساز ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.47,
  "end": 952.95
 },
 {
  "input": "And now, my friends, is probably as good a time as any to finally get into the formula for a normal distribution. ",
  "translatedText": "اور اب، میرے دوستو، شاید اتنا ہی اچھا وقت ہے جتنا کہ آخر کار عام تقسیم کے فارمولے میں داخل ہونے کا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 954.81,
  "end": 960.85
 },
 {
  "input": "And the way I'd like to do this is to basically peel back all the layers and build it up one piece at a time. ",
  "translatedText": "اور جس طرح سے میں یہ کرنا چاہوں گا وہ یہ ہے کہ بنیادی طور پر تمام تہوں کو چھیلنا اور اسے ایک وقت میں ایک ٹکڑا بنانا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 961.49,
  "end": 965.93
 },
 {
  "input": "The function e to the x, or anything to the x, describes exponential growth, and if you make that exponent negative, which flips around the graph horizontally, you might think of it as describing exponential decay. ",
  "translatedText": "ایکس کا فنکشن ای، یا ایکس کے لیے کوئی بھی چیز، ایکسپوینیشنل نمو کو بیان کرتی ہے، اور اگر آپ اس ایکسپویننٹ کو منفی بناتے ہیں، جو گراف کے گرد افقی طور پر پلٹتا ہے، تو آپ اسے ایکسپونیشنل ڈیک کو بیان کرنے کے طور پر سوچ سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.53,
  "end": 977.87
 },
 {
  "input": "To make this decay in both directions, you could do something to make sure the exponent is always negative and growing, like taking the negative absolute value. ",
  "translatedText": "اس کشی کو دونوں سمتوں میں بنانے کے لیے، آپ یہ یقینی بنانے کے لیے کچھ کر سکتے ہیں کہ ایکسپوننٹ ہمیشہ منفی اور بڑھ رہا ہے، جیسے منفی مطلق قدر لینا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.51,
  "end": 985.43
 },
 {
  "input": "That would give us this kind of awkward sharp point in the middle, but if instead you make that exponent the negative square of x, you get a smoother version of the same thing, which decays in both directions. ",
  "translatedText": "اس سے ہمیں درمیان میں اس قسم کا عجیب تیز نقطہ ملے گا، لیکن اگر اس کے بجائے آپ اس ایکسپونٹنٹ کو x کا منفی مربع بناتے ہیں، تو آپ کو اسی چیز کا ایک ہموار ورژن ملتا ہے، جو دونوں سمتوں میں زوال پذیر ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.93,
  "end": 995.81
 },
 {
  "input": "This gives us the basic bell curve shape. ",
  "translatedText": "یہ ہمیں بنیادی گھنٹی وکر کی شکل دیتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 996.33,
  "end": 998.19
 },
 {
  "input": "Now if you throw a constant in front of that x, and you scale that constant up and down, it lets you stretch and squish the graph horizontally, allowing you to describe narrow and wider bell curves. ",
  "translatedText": "اب اگر آپ اس ایکس کے سامنے ایک مستقل پھینکتے ہیں، اور آپ اس مستقل کو اوپر اور نیچے پیمانہ کرتے ہیں، تو یہ آپ کو گراف کو افقی طور پر پھیلانے اور اسکویش کرنے دیتا ہے، جس سے آپ تنگ اور وسیع گھنٹی کے منحنی خطوط کو بیان کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.65,
  "end": 1008.37
 },
 {
  "input": "And a quick thing I'd like to point out here is that based on the rules of exponentiation, as we tweak around that constant c, you could also think about it as simply changing the base of the exponentiation. ",
  "translatedText": "اور ایک فوری چیز جو میں یہاں بتانا چاہوں گا وہ یہ ہے کہ کفایت شعاری کے اصولوں کی بنیاد پر، جیسا کہ ہم اس مستقل c کے ارد گرد موافقت کرتے ہیں، آپ اس کے بارے میں یہ بھی سوچ سکتے ہیں کہ صرف کفایت کی بنیاد کو تبدیل کرنا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1009.01,
  "end": 1019.75
 },
 {
  "input": "And in that sense, the number e is not really all that special for our formula. ",
  "translatedText": "اور اس لحاظ سے، ای نمبر واقعی ہمارے فارمولے کے لیے اتنا خاص نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1020.15,
  "end": 1023.63
 },
 {
  "input": "We could replace it with any other positive constant, and you'll get the same family of curves as we tweak that constant. ",
  "translatedText": "ہم اسے کسی دوسرے مثبت مستقل سے بدل سکتے ہیں، اور آپ کو منحنی خطوط کا وہی خاندان ملے گا جب ہم اس مستقل کو موافقت کریں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.05,
  "end": 1030.49
 },
 {
  "input": "Make it a 2, same family of curves. ",
  "translatedText": "اسے 2، منحنی خطوط کا ایک ہی خاندان بنائیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.51,
  "end": 1033.11
 },
 {
  "input": "Make it a 3, same family of curves. ",
  "translatedText": "اسے 3، منحنی خطوط کا ایک ہی خاندان بنائیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.33,
  "end": 1035.07
 },
 {
  "input": "The reason we use e is that it gives that constant a very readable meaning. ",
  "translatedText": "ہم e استعمال کرنے کی وجہ یہ ہے کہ یہ اس مستقل کو پڑھنے کے قابل معنی دیتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1035.75,
  "end": 1039.49
 },
 {
  "input": "Or rather, if we reconfigure things a little bit so that the exponent looks like negative one half times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn this into a probability distribution, that constant sigma will be the standard deviation of that distribution. ",
  "translatedText": "یا اس کے بجائے، اگر ہم چیزوں کو تھوڑا سا دوبارہ ترتیب دیں تاکہ ایکسپوننٹ منفی آدھا گنا x کو ایک مخصوص مستقل سے تقسیم کیا جائے، جسے ہم تجویزی طور پر سگما اسکوائر کہیں گے، پھر ایک بار جب ہم اسے امکانی تقسیم میں بدل دیتے ہیں، تو وہ مستقل سگما بدل جائے گا۔اس تقسیم کا معیاری انحراف ہو۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.11,
  "end": 1057.21
 },
 {
  "input": "And that's very nice. ",
  "translatedText": "اور یہ بہت اچھا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.81,
  "end": 1058.57
 },
 {
  "input": "But before we can interpret this as a probability distribution, we need the area under the curve to be 1. ",
  "translatedText": "لیکن اس سے پہلے کہ ہم اسے امکانی تقسیم سے تعبیر کر سکیں، ہمیں چاہیے کہ وکر کے نیچے کا رقبہ 1 ہو۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.91,
  "end": 1064.31
 },
 {
  "input": "And the reason for that is how the curve is interpreted. ",
  "translatedText": "اور اس کی وجہ یہ ہے کہ وکر کی تشریح کیسے کی جاتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1064.83,
  "end": 1066.91
 },
 {
  "input": "Unlike discrete distributions, when it comes to something continuous, you don't ask about the probability of a particular point. ",
  "translatedText": "مجرد تقسیم کے برعکس، جب بات مسلسل کسی چیز کی ہو، تو آپ کسی خاص نقطہ کے امکان کے بارے میں نہیں پوچھتے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1067.37,
  "end": 1073.37
 },
 {
  "input": "Instead, you ask for the probability that a value falls between two different values. ",
  "translatedText": "اس کے بجائے، آپ اس امکان کے بارے میں پوچھتے ہیں کہ ایک قدر دو مختلف اقدار کے درمیان آتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1073.79,
  "end": 1078.23
 },
 {
  "input": "And what the curve is telling you is that that probability equals the area under the curve between those two values. ",
  "translatedText": "اور جو وکر آپ کو بتا رہا ہے وہ یہ ہے کہ امکان ان دو قدروں کے درمیان وکر کے نیچے کے رقبے کے برابر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.75,
  "end": 1085.43
 },
 {
  "input": "There's a whole other video about this, they're called probability density functions. ",
  "translatedText": "اس کے بارے میں ایک پوری دوسری ویڈیو ہے، انہیں امکانی کثافت کے افعال کہتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1086.03,
  "end": 1089.43
 },
 {
  "input": "The main point right now is that the area under the entire curve represents the probability that something happens, that some number comes up. ",
  "translatedText": "ابھی اہم نکتہ یہ ہے کہ پورے وکر کے نیچے کا رقبہ اس امکان کی نمائندگی کرتا ہے کہ کچھ ہوتا ہے، کچھ نمبر آتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1089.83,
  "end": 1097.15
 },
 {
  "input": "That should be 1, which is why we want the area under this to be 1. ",
  "translatedText": "یہ 1 ہونا چاہیے، اسی لیے ہم چاہتے ہیں کہ اس کے تحت رقبہ 1 ہو۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1097.41,
  "end": 1100.63
 },
 {
  "input": "As it stands with the basic bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root of pi. ",
  "translatedText": "جیسا کہ یہ e سے لے کر منفی x مربع کی بنیادی گھنٹی کی شکل کے ساتھ کھڑا ہے، یہ رقبہ 1 نہیں ہے، یہ دراصل pi کا مربع جڑ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1101.05,
  "end": 1107.79
 },
 {
  "input": "I know, right? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.41,
  "end": 1109.15
 },
 {
  "input": "What is pi doing here? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1109.27,
  "end": 1110.19
 },
 {
  "input": "What does this have to do with circles? ",
  "translatedText": "مجھے صحیح معلوم؟ پی آئی یہاں کیا کر رہی ہے؟ اس کا حلقوں سے کیا تعلق؟ جیسا کہ میں نے شروع میں کہا تھا، میں اگلی ویڈیو میں اس کے بارے میں بات کرنا پسند کروں گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1110.29,
  "end": 1111.47
 },
 {
  "input": "Like I said at the start, I'd love to talk all about that in the next video. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.01,
  "end": 1115.05
 },
 {
  "input": "But if you can spare your excitement for our purposes right now, all it means is that we should divide this function by the square root of pi, and it gives us the area we want. ",
  "translatedText": "لیکن اگر آپ ابھی ہمارے مقاصد کے لیے اپنے جوش و خروش کو چھوڑ سکتے ہیں، تو اس کا مطلب یہ ہے کہ ہمیں اس فنکشن کو pi کے مربع جڑ سے تقسیم کرنا چاہیے، اور یہ ہمیں وہ علاقہ دیتا ہے جو ہم چاہتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.33,
  "end": 1123.17
 },
 {
  "input": "Throwing back in the constants we had earlier, the 1 half and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square root of 2. ",
  "translatedText": "ہمارے پاس پہلے والے مستقل میں، 1 نصف اور سگما کو واپس پھینکنے کا اثر یہ ہوتا ہے کہ گراف کو سگما کے ایک عنصر سے 2 کے مربع جڑ سے بڑھایا جائے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.61,
  "end": 1131.79
 },
 {
  "input": "So we also need to divide out by that in order to make sure it has an area of 1. ",
  "translatedText": "لہٰذا ہمیں اس سے بھی تقسیم کرنے کی ضرورت ہے تاکہ یہ یقینی بنایا جا سکے کہ اس کا رقبہ 1 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.41,
  "end": 1136.47
 },
 {
  "input": "And combining those fractions, the factor out front looks like 1 divided by sigma times the square root of 2 pi. ",
  "translatedText": "اور ان حصوں کو ملانے سے، سامنے کا عنصر 1 کی طرح دکھائی دیتا ہے جو 2 pi کے مربع جڑ کو سگما سے تقسیم کیا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1136.47,
  "end": 1142.11
 },
 {
  "input": "This, finally, is a valid probability distribution. ",
  "translatedText": "یہ، آخر میں، ایک درست امکانی تقسیم ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1142.91,
  "end": 1145.85
 },
 {
  "input": "As we tweak that value sigma, resulting in narrower and wider curves, that constant in the front always guarantees that the area equals 1. ",
  "translatedText": "جیسا کہ ہم اس قدر سگما کو موافقت کرتے ہیں، جس کے نتیجے میں تنگ اور وسیع منحنی خطوط ہوتے ہیں، جو کہ سامنے کا مستقل ہمیشہ اس بات کی ضمانت دیتا ہے کہ رقبہ 1 کے برابر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.45,
  "end": 1154.31
 },
 {
  "input": "The special case where sigma equals 1 has a specific name, we call it the standard normal distribution, which plays an especially important role for you and me in this lesson. ",
  "translatedText": "خاص کیس جہاں سگما برابر 1 کا ایک مخصوص نام ہوتا ہے، ہم اسے معیاری عام تقسیم کہتے ہیں، جو اس سبق میں آپ اور میرے لیے خاص طور پر اہم کردار ادا کرتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.91,
  "end": 1164.51
 },
 {
  "input": "And all possible normal distributions are not only parameterized with this value sigma, but we also subtract off another constant mu from the variable x, and this essentially just lets you slide the graph left and right so that you can prescribe the mean of this distribution. ",
  "translatedText": "اور تمام ممکنہ عام تقسیم کو نہ صرف اس قدر سگما کے ساتھ پیرامیٹرائز کیا جاتا ہے، بلکہ ہم متغیر x سے ایک اور مستقل mu کو بھی گھٹا دیتے ہیں، اور یہ بنیادی طور پر آپ کو گراف کو بائیں اور دائیں سلائیڈ کرنے دیتا ہے تاکہ آپ اس تقسیم کا اوسط لکھ سکیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.13,
  "end": 1180.21
 },
 {
  "input": "So in short, we have two parameters, one describing the mean, one describing the standard deviation, and they're all tied together in this big formula involving an e and a pi. ",
  "translatedText": "تو مختصراً، ہمارے پاس دو پیرامیٹرز ہیں، ایک وسط کو بیان کرتا ہے، ایک معیاری انحراف کو بیان کرتا ہے، اور وہ سب اس بڑے فارمولے میں ایک ساتھ بندھے ہوئے ہیں جس میں ایک e اور a pi شامل ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.99,
  "end": 1189.19
 },
 {
  "input": "Now that all of that is on the table, let's look back again at the idea of starting with some random variable and asking what the distributions for sums of that variable look like. ",
  "translatedText": "اب جب کہ یہ سب کچھ میز پر ہے، آئیے کچھ بے ترتیب متغیر کے ساتھ شروع کرنے کے خیال پر دوبارہ نظر ڈالیں اور پوچھیں کہ اس متغیر کی رقم کی تقسیم کیسی نظر آتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.19,
  "end": 1199.81
 },
 {
  "input": "As we've already gone over, when you increase the size of that sum, the resulting distribution will shift according to a growing mean, and it slowly spreads out according to a growing standard deviation. ",
  "translatedText": "جیسا کہ ہم پہلے ہی گزر چکے ہیں، جب آپ اس رقم کے سائز میں اضافہ کرتے ہیں، تو نتیجے میں تقسیم بڑھتے ہوئے اوسط کے مطابق بدل جائے گی، اور یہ ایک بڑھتے ہوئے معیاری انحراف کے مطابق آہستہ آہستہ پھیل جاتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1200.13,
  "end": 1209.81
 },
 {
  "input": "And putting some actual formulas to it, if we know the mean of our underlying random variable, we call it mu, and we also know its standard deviation, and we call it sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the standard deviation will be sigma times the square root of that size. ",
  "translatedText": "اور اس پر کچھ اصل فارمولے ڈالتے ہوئے، اگر ہم اپنے بنیادی بے ترتیب متغیر کا مطلب جانتے ہیں، تو ہم اسے mu کہتے ہیں، اور ہم اس کے معیاری انحراف کو بھی جانتے ہیں، اور ہم اسے سگما کہتے ہیں، تو نچلے حصے کی رقم کا اوسط mu ہوگا۔رقم کے سائز کا گنا، اور معیاری انحراف اس سائز کے مربع جڑ کا سگما گنا ہوگا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1210.33,
  "end": 1227.73
 },
 {
  "input": "So now, if we want to claim that this looks more and more like a bell curve, and a bell curve is only described by two different parameters, the mean and the standard deviation, you know what to do. ",
  "translatedText": "تو اب، اگر ہم یہ دعوی کرنا چاہتے ہیں کہ یہ زیادہ سے زیادہ گھنٹی کے منحنی خطوط کی طرح لگتا ہے، اور گھنٹی کا منحنی صرف دو مختلف پیرامیٹرز، اوسط اور معیاری انحراف کے ذریعہ بیان کیا جاتا ہے، تو آپ جانتے ہیں کہ کیا کرنا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1228.19,
  "end": 1237.71
 },
 {
  "input": "You could plug those two values into the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve that should closely fit our distribution. ",
  "translatedText": "آپ ان دو قدروں کو فارمولے میں شامل کر سکتے ہیں، اور یہ آپ کو ایک انتہائی واضح، پیچیدہ قسم کے ہوتے ہوئے بھی، ایک منحنی خطوط کے لیے فارمولہ فراہم کرتا ہے جو ہماری تقسیم کے قریب سے فٹ ہونا چاہیے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1237.93,
  "end": 1246.99
 },
 {
  "input": "But there's another way we can describe it that's a little more elegant and lends itself to a very fun visual that we can build up to. ",
  "translatedText": "لیکن ایک اور طریقہ ہے جس سے ہم اسے بیان کر سکتے ہیں جو کچھ زیادہ ہی خوبصورت ہے اور خود کو ایک بہت ہی پرلطف بصری کے لیے قرض دیتا ہے جسے ہم تیار کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1248.39,
  "end": 1254.81
 },
 {
  "input": "Instead of focusing on the sum of all of these random variables, let's modify this expression a little bit, where what we'll do is we'll look at the mean that we expect that sum to take, and we subtract it off so that our new expression has a mean of 0, and then we're going to look at the standard deviation we expect of our sum, and divide out by that, which basically just rescales the units so that the standard deviation of our expression will equal 1. ",
  "translatedText": "ان تمام بے ترتیب متغیرات کے مجموعے پر توجہ مرکوز کرنے کے بجائے، آئیے اس اظہار کو تھوڑا سا تبدیل کریں، جہاں ہم کیا کریں گے ہم اس کا مطلب دیکھیں گے کہ ہم اس رقم کو لینے کی توقع رکھتے ہیں، اور ہم اسے گھٹا دیتے ہیں تاکہ ہمارے نئے اظہار کا مطلب 0 ہے، اور پھر ہم اس معیاری انحراف کو دیکھیں گے جس کی ہم اپنی رقم سے توقع کرتے ہیں، اور اس سے تقسیم کریں گے، جو بنیادی طور پر صرف اکائیوں کو دوبارہ اسکیل کرتا ہے تاکہ ہمارے اظہار کا معیاری انحراف 1 کے برابر ہو جائے۔. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1255.27,
  "end": 1278.77
 },
 {
  "input": "This might seem like a more complicated expression, but it actually has a highly readable meaning. ",
  "translatedText": "یہ ایک زیادہ پیچیدہ اظہار کی طرح لگتا ہے، لیکن یہ اصل میں ایک انتہائی پڑھنے کے قابل معنی ہے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1279.35,
  "end": 1284.09
 },
 {
  "input": "It's essentially saying how many standard deviations away from the mean is this sum? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1284.45,
  "end": 1289.67
 },
 {
  "input": "For example, this bar here corresponds to a certain value that you might find when you roll 10 dice and you add them all up, and its position a little above negative 1 is telling you that that value is a little bit less than one standard deviation lower than the mean. ",
  "translatedText": "یہ بنیادی طور پر یہ کہہ رہا ہے کہ یہ رقم اوسط سے کتنے معیاری انحرافات دور ہے؟ مثال کے طور پر، یہاں یہ بار ایک خاص قدر سے مطابقت رکھتا ہے جو آپ کو اس وقت مل سکتی ہے جب آپ 10 ڈائس رول کرتے ہیں اور آپ ان سب کو جوڑتے ہیں، اور اس کی پوزیشن منفی 1 سے تھوڑا اوپر آپ کو بتا رہی ہے کہ وہ قدر ایک معیاری انحراف سے تھوڑی کم ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.75,
  "end": 1303.87
 },
 {
  "input": "Also, by the way, in anticipation for the animation I'm trying to build to here, the way I'm representing things on that lower plot is that the area of each one of these bars is telling us the probability of the corresponding value rather than the height. ",
  "translatedText": "اوسط سے کم اس کے علاوہ، ویسے بھی، اینیمیشن کی توقع میں جو میں یہاں بنانے کی کوشش کر رہا ہوں، جس طرح میں اس نچلے پلاٹ پر چیزوں کی نمائندگی کر رہا ہوں وہ یہ ہے کہ ان سلاخوں میں سے ہر ایک کا رقبہ ہمیں متعلقہ قدر کا امکان بتا رہا ہے۔اونچائی کے بجائے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1305.13,
  "end": 1316.99
 },
 {
  "input": "You might think of the y-axis as representing not probability but a kind of probability density. ",
  "translatedText": "آپ y-axis کے بارے میں سوچ سکتے ہیں کہ یہ امکان نہیں بلکہ ایک قسم کی امکانی کثافت ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.23,
  "end": 1321.93
 },
 {
  "input": "The reason for this is to set the stage so that it aligns with the way we interpret continuous distributions, where the probability of falling between a range of values is equal to an area under a curve between those values. ",
  "translatedText": "اس کی وجہ یہ ہے کہ اس مرحلے کو ترتیب دیا جائے تاکہ یہ اس طرح سے ہم آہنگ ہو جس طرح ہم مسلسل تقسیم کی تشریح کرتے ہیں، جہاں قدروں کی ایک رینج کے درمیان گرنے کا امکان ان اقدار کے درمیان ایک وکر کے نیچے والے علاقے کے برابر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1322.27,
  "end": 1333.55
 },
 {
  "input": "In particular, the area of all the bars together is going to be 1. ",
  "translatedText": "خاص طور پر، تمام سلاخوں کا ایک ساتھ رقبہ 1 ہونے والا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1333.91,
  "end": 1336.73
 },
 {
  "input": "Now, with all of that in place, let's have a little fun. ",
  "translatedText": "اب، ان سب کے ساتھ، آئیے تھوڑا مزہ کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1338.23,
  "end": 1340.95
 },
 {
  "input": "Let me start by rolling things back so that the distribution on the bottom represents a relatively small sum, like adding together only three such random variables. ",
  "translatedText": "آئیے میں چیزوں کو واپس موڑ کر شروع کرتا ہوں تاکہ نیچے کی تقسیم نسبتاً چھوٹی رقم کی نمائندگی کرے، جیسے صرف تین ایسے بے ترتیب متغیرات کو اکٹھا کرنا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1341.33,
  "end": 1349.01
 },
 {
  "input": "Notice what happens as I change the distribution we start with. ",
  "translatedText": "نوٹ کریں کہ کیا ہوتا ہے جب میں اس تقسیم کو تبدیل کرتا ہوں جس سے ہم شروع کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.45,
  "end": 1352.43
 },
 {
  "input": "As it changes, the distribution on the bottom completely changes its shape. ",
  "translatedText": "جیسے جیسے یہ تبدیل ہوتا ہے، نیچے کی تقسیم اپنی شکل کو مکمل طور پر بدل دیتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1352.73,
  "end": 1356.29
 },
 {
  "input": "It's very dependent on what we started with. ",
  "translatedText": "یہ اس بات پر بہت منحصر ہے کہ ہم نے کیا شروع کیا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1356.51,
  "end": 1358.77
 },
 {
  "input": "If we let the size of our sum get a little bit bigger, say going up to 10, and as I change the distribution for x, it largely stays looking like a bell curve, but I can find some distributions that get it to change shape. ",
  "translatedText": "اگر ہم اپنی رقم کے سائز کو تھوڑا بڑا ہونے دیں، کہتے ہیں کہ 10 تک جا رہا ہے، اور جب میں x کے لیے تقسیم کو تبدیل کرتا ہوں، تو یہ بڑی حد تک گھنٹی کے وکر کی طرح نظر آتا ہے، لیکن میں کچھ ایسی تقسیم تلاش کر سکتا ہوں جو اسے شکل بدلنے کا باعث بنیں۔. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.35,
  "end": 1371.63
 },
 {
  "input": "For example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results in this kind of spiky bell curve, and if you'll recall, earlier on I actually showed this in the form of a simulation. ",
  "translatedText": "مثال کے طور پر، واقعی ایک طرف جہاں تقریباً تمام امکانات نمبر 1 یا 6 میں ہوتے ہیں اس کے نتیجے میں اس قسم کی گھنٹی کی گھنٹی ہوتی ہے، اور اگر آپ کو یاد ہو تو، اس سے پہلے میں نے حقیقت میں اسے نقلی شکل میں دکھایا تھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1372.23,
  "end": 1383.51
 },
 {
  "input": "So if you were wondering whether that spikiness was an artifact of the randomness or reflected the true distribution, turns out it reflects the true distribution. ",
  "translatedText": "لہذا اگر آپ سوچ رہے تھے کہ آیا یہ تیز پن بے ترتیب پن کا نمونہ تھا یا حقیقی تقسیم کی عکاسی کرتا ہے، تو پتہ چلتا ہے کہ یہ حقیقی تقسیم کی عکاسی کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1384.13,
  "end": 1391.85
 },
 {
  "input": "In this case, 10 is not a large enough sum for the central limit theorem to kick in. ",
  "translatedText": "اس صورت میں، 10 اتنی بڑی رقم نہیں ہے کہ مرکزی حد تھیوریم کو داخل کیا جا سکے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.29,
  "end": 1396.47
 },
 {
  "input": "But if instead I let that sum grow and I consider adding 50 different values, which is actually not that big, then no matter how I change the distribution for our underlying random variable, it has essentially no effect on the shape of the plot on the bottom. ",
  "translatedText": "لیکن اگر اس کے بجائے میں اس رقم کو بڑھنے دیتا ہوں اور میں 50 مختلف اقدار کو شامل کرنے پر غور کرتا ہوں، جو کہ حقیقت میں اتنی بڑی نہیں ہے، تو اس سے کوئی فرق نہیں پڑتا ہے کہ میں اپنے بنیادی بے ترتیب متغیر کی تقسیم کو کس طرح تبدیل کرتا ہوں، اس کا بنیادی طور پر پلاٹ کی شکل پر کوئی اثر نہیں پڑتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1396.47,
  "end": 1410.69
 },
 {
  "input": "No matter where we start, all of the information and nuance for the distribution of x gets washed away, and we tend towards this single universal shape described by a very elegant function for the standard normal distribution, 1 over square root of 2 pi times e to the negative x squared over 2. ",
  "translatedText": "نیچے اس بات سے کوئی فرق نہیں پڑتا ہے کہ ہم کہاں سے شروع کرتے ہیں، x کی تقسیم کے لیے تمام معلومات اور باریکیاں ختم ہو جاتی ہیں، اور ہم اس واحد عالمگیر شکل کی طرف مائل ہوتے ہیں جو معیاری نارمل تقسیم کے لیے ایک بہت ہی خوبصورت فنکشن کے ذریعے بیان کیے گئے ہیں، 2 pi گنا e کے مربع جڑ سے زیادہ منفی x مربع 2 پر۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1411.17,
  "end": 1427.07
 },
 {
  "input": "This, this right here is what the central limit theorem is all about. ",
  "translatedText": "یہ، یہ یہاں وہی ہے جس کے بارے میں مرکزی حد نظریہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1427.81,
  "end": 1430.81
 },
 {
  "input": "Almost nothing you can do to this initial distribution changes the shape we tend towards. ",
  "translatedText": "اس ابتدائی تقسیم کے لیے آپ تقریباً کچھ نہیں کر سکتے جس شکل کی طرف ہمارا رجحان بدلتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1431.13,
  "end": 1435.31
 },
 {
  "input": "Now, the more theoretically minded among you might still be wondering, what is the actual theorem? ",
  "translatedText": "اب، آپ میں سے زیادہ نظریاتی ذہن اب بھی سوچ رہے ہوں گے، اصل نظریہ کیا ہے؟ جیسے، وہ کون سا ریاضیاتی بیان ہے جو ثابت یا غلط ثابت ہو سکتا ہے کہ ہم یہاں دعویٰ کر رہے ہیں؟ اگر آپ ایک اچھا رسمی بیان چاہتے ہیں، تو یہ کیسے ہو سکتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1439.03,
  "end": 1444.51
 },
 {
  "input": "Like, what's the mathematical statement that could be proved or disproved that we're claiming here? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1444.81,
  "end": 1448.91
 },
 {
  "input": "If you want a nice formal statement, here's how it might go. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1449.03,
  "end": 1451.67
 },
 {
  "input": "Consider this value, where we're summing up n different instantiations of our random variable, but tweaked and tuned so that its mean and standard deviation are 1. ",
  "translatedText": "اس قدر پر غور کریں، جہاں ہم اپنے بے ترتیب متغیر کی مختلف صورتوں کا خلاصہ کر رہے ہیں، لیکن اس کے درمیانی اور معیاری انحراف 1 کو ٹویک اور ٹیون کیا گیا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1452.13,
  "end": 1459.89
 },
 {
  "input": "Again, meaning you can read it as asking how many standard deviations away from the mean is the sum. ",
  "translatedText": "ایک بار پھر، مطلب یہ ہے کہ آپ اسے یہ پوچھ کر پڑھ سکتے ہیں کہ اوسط سے کتنے معیاری انحرافات کا مجموعہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1460.23,
  "end": 1465.35
 },
 {
  "input": "Then the actual rigorous no-jokes-this-time statement of the central limit theorem is that if you consider the probability that this value falls between two given real numbers, a and b, and you consider the limit of that probability as the size of your sum goes to infinity, then that limit is equal to a certain integral, which basically describes the area under a standard normal distribution between those two values. ",
  "translatedText": "پھر مرکزی حد تھیوریم کا اس بار کا اصل سخت نو-مزاق بیان یہ ہے کہ اگر آپ اس امکان پر غور کرتے ہیں کہ یہ قدر دو دیے گئے حقیقی نمبروں، a اور b کے درمیان آتی ہے، اور آپ اس امکان کی حد کو اس کے سائز کے طور پر سمجھتے ہیں۔آپ کی رقم لامحدودیت تک جاتی ہے، پھر وہ حد ایک خاص انٹیگرل کے برابر ہوتی ہے، جو بنیادی طور پر ان دو قدروں کے درمیان ایک معیاری نارمل تقسیم کے تحت علاقے کو بیان کرتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1465.77,
  "end": 1489.65
 },
 {
  "input": "Again, there are three underlying assumptions that I have yet to tell you, but other than those, in all of its gory detail, this right here is the central limit theorem. ",
  "translatedText": "ایک بار پھر، تین بنیادی مفروضے ہیں جن کے بارے میں میں نے ابھی آپ کو بتانا ہے، لیکن ان کے علاوہ، اس کی تمام خستہ حال تفصیل میں، یہ یہاں مرکزی حد نظریہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1491.25,
  "end": 1500.03
 },
 {
  "input": "All of that is a bit theoretical, so it might be helpful to bring things back down to Earth and turn back to the concrete example that I mentioned at the start, where you imagine rolling a die 100 times, and let's assume it's a fair die for this example, and you add together the results. ",
  "translatedText": "یہ سب کچھ تھوڑا سا نظریاتی ہے، لہذا چیزوں کو زمین پر واپس لانا اور اس ٹھوس مثال کی طرف واپس جانا مددگار ثابت ہو سکتا ہے جس کا میں نے شروع میں ذکر کیا تھا، جہاں آپ 100 بار ڈائی کو رول کرنے کا تصور کرتے ہیں، اور آئیے فرض کریں کہ یہ ایک منصفانہ موت ہے۔اس مثال کے لیے، اور آپ نتائج کو ایک ساتھ شامل کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1504.55,
  "end": 1518.13
 },
 {
  "input": "The challenge for you is to find a range of values such that you're 95% sure that the sum will fall within this range. ",
  "translatedText": "آپ کے لیے چیلنج قدروں کی ایک حد تلاش کرنا ہے جس سے آپ کو 95% یقین ہو کہ رقم اس حد میں آئے گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1518.87,
  "end": 1525.83
 },
 {
  "input": "For questions like this, there's a handy rule of thumb about normal distributions, which is that about 68% of your values are going to fall within one standard deviation of the mean, 95% of your values, the thing we care about, fall within two standard deviations of the mean, and a whopping 99.7% of your values will fall within three standard deviations of the mean. ",
  "translatedText": "اس طرح کے سوالات کے لیے، عام تقسیم کے بارے میں ایک آسان اصول ہے، جو یہ ہے کہ آپ کی تقریباً 68% اقدار اوسط کے ایک معیاری انحراف کے اندر گرنے والی ہیں، آپ کی 95% اقدار، جس چیز کی ہمیں پرواہ ہے، اس کے اندر آتی ہے۔وسط کے دو معیاری انحراف، اور ایک مکمل 99۔آپ کی 7% اقدار اوسط کے تین معیاری انحراف کے اندر آئیں گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1527.13,
  "end": 1546.97
 },
 {
  "input": "It's a rule of thumb that's commonly memorized by people who do a lot of probability and stats. ",
  "translatedText": "یہ انگوٹھے کا ایک اصول ہے جو عام طور پر ان لوگوں کے ذریعہ حفظ کیا جاتا ہے جو بہت زیادہ امکانات اور اعدادوشمار کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1547.45,
  "end": 1551.45
 },
 {
  "input": "Naturally, this gives us what we need for our example, and let me go ahead and draw out what this would look like, where I'll show the distribution for a fair die up at the top, and the distribution for a sum of 100 such dice on the bottom, which by now as you know looks like a certain normal distribution. ",
  "translatedText": "فطری طور پر، اس سے ہمیں اپنی مثال کے لیے ضرورت کی چیز ملتی ہے، اور مجھے آگے بڑھنے دیں اور یہ بتانے دیں کہ یہ کیسا نظر آئے گا، جہاں میں سب سے اوپر ایک میلے کے لیے تقسیم، اور 100 کی رقم کی تقسیم دکھاؤں گا۔نچلے حصے پر اس طرح کے نرد، جو اب تک آپ جانتے ہیں کہ ایک مخصوص عام تقسیم کی طرح لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.49,
  "end": 1567.29
 },
 {
  "input": "Step one with a problem like this is to find the mean of your initial distribution, which in this case will look like 1 6th times 1 plus 1 6th times 2 on and on and on, and works out to be 3.5. ",
  "translatedText": "اس طرح کے مسئلے کے ساتھ پہلا مرحلہ آپ کی ابتدائی تقسیم کا مطلب تلاش کرنا ہے، جو اس صورت میں 1 6 ویں دفعہ 1 جمع 1 6 ویں بار 2 آن اور آن اور آن کی طرح نظر آئے گا، اور 3 ہونے پر کام کرتا ہے۔5۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1567.95,
  "end": 1578.91
 },
 {
  "input": "We also need the standard deviation, which requires calculating the variance, which as you know involves adding all the squares of the differences between the values and the means, and it works out to be 2.92, square root of that comes out to be 1.71. ",
  "translatedText": "ہمیں معیاری انحراف کی بھی ضرورت ہے، جس میں تغیر کا حساب لگانا ضروری ہے، جس میں آپ جانتے ہیں کہ اقدار اور ذرائع کے درمیان فرق کے تمام مربعوں کو شامل کرنا شامل ہے، اور یہ 2 ہونے پر کام کرتا ہے۔92، اس کا مربع جڑ 1 نکلتا ہے۔71. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1579.41,
  "end": 1592.43
 },
 {
  "input": "Those are the only two numbers we need, and I will invite you again to reflect on how magical it is that those are the only two numbers that you need to completely understand the bottom distribution. ",
  "translatedText": "یہ صرف دو نمبر ہیں جن کی ہمیں ضرورت ہے، اور میں آپ کو دوبارہ اس بات پر غور کرنے کے لیے مدعو کروں گا کہ یہ کتنا جادوئی ہے کہ یہ صرف دو نمبر ہیں جنہیں آپ کو نیچے کی تقسیم کو مکمل طور پر سمجھنے کی ضرورت ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1592.95,
  "end": 1601.69
 },
 {
  "input": "Its mean will be 100 times mu, which is 350, and its standard deviation will be the square root of 100 times sigma, so 10 times sigma 17.1. ",
  "translatedText": "اس کا اوسط 100 گنا mu ہوگا، جو کہ 350 ہے، اور اس کا معیاری انحراف 100 گنا سگما کا مربع جڑ ہوگا، تو 10 گنا سگما 17۔1۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1602.43,
  "end": 1612.61
 },
 {
  "input": "Remembering our handy rule of thumb, we're looking for values two standard deviations away from the mean, and when you subtract 2 sigma from the mean you end up with about 316, and when you add 2 sigma you end up with 384. ",
  "translatedText": "انگوٹھے کے اپنے آسان اصول کو یاد رکھتے ہوئے، ہم اوسط سے دو معیاری انحراف کی قدریں تلاش کر رہے ہیں، اور جب آپ اوسط سے 2 سگما کو گھٹاتے ہیں تو آپ کا اختتام تقریباً 316 ہوتا ہے، اور جب آپ 2 سگما شامل کرتے ہیں تو آپ کا اختتام 384 ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1613.03,
  "end": 1626.33
 },
 {
  "input": "And there you go, that gives us the answer. ",
  "translatedText": "اور آپ وہاں جائیں، جو ہمیں جواب دیتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1627.35,
  "end": 1628.95
 },
 {
  "input": "Okay, I promised to wrap things up shortly, but while we're on this example there's one more question that's worth your time to ponder. ",
  "translatedText": "ٹھیک ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.47,
  "end": 1637.45
 },
 {
  "input": "Instead of just asking about the sum of 100 die rolls, let's say I had you divide that number by 100, which basically means all the numbers in our diagram in the bottom get divided by 100. ",
  "translatedText": "صرف 100 ڈائی رولز کے مجموعے کے بارے میں پوچھنے کے بجائے، آئیے یہ کہتے ہیں کہ میں نے آپ کو اس نمبر کو 100 سے تقسیم کیا تھا، جس کا بنیادی مطلب یہ ہے کہ نیچے والے ہمارے خاکے میں موجود تمام نمبروں کو 100 سے تقسیم کیا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1638.25,
  "end": 1648.09
 },
 {
  "input": "Take a moment to interpret what this all would be saying then. ",
  "translatedText": "اس کی تشریح کرنے کے لئے ایک لمحہ نکالیں کہ یہ سب کیا کہہ رہے ہوں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1648.57,
  "end": 1651.57
 },
 {
  "input": "The expression essentially tells you the empirical average for 100 different die rolls, and that interval we found is now telling you what range you are expecting to see for that empirical average. ",
  "translatedText": "اظہار بنیادی طور پر آپ کو 100 مختلف ڈائی رولز کے لیے تجرباتی اوسط بتاتا ہے، اور جو وقفہ ہمیں ملا ہے وہ اب آپ کو بتا رہا ہے کہ آپ اس تجرباتی اوسط کے لیے کس حد تک دیکھنے کی توقع کر رہے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1652.07,
  "end": 1663.49
 },
 {
  "input": "In other words, you might expect it to be around 3.5, that's the expected value for a die roll, but what's much less obvious and what the central limit theorem lets you compute is how close to that expected value you'll reasonably find yourself. ",
  "translatedText": "دوسرے لفظوں میں، آپ توقع کر سکتے ہیں کہ یہ 3 کے لگ بھگ ہے۔5، یہ ایک ڈائی رول کے لیے متوقع قدر ہے، لیکن جو بہت کم واضح ہے اور جو مرکزی حد نظریہ آپ کو شمار کرنے دیتا ہے وہ یہ ہے کہ اس متوقع قدر کے کتنے قریب آپ خود کو معقول طور پر تلاش کریں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1664.35,
  "end": 1676.57
 },
 {
  "input": "In particular, it's worth your time to take a moment mulling over what the standard deviation for this empirical average is, and what happens to it as you look at a bigger and bigger sample of die rolls. ",
  "translatedText": "خاص طور پر، اس تجرباتی اوسط کے لیے معیاری انحراف کیا ہے، اور جب آپ ڈائی رولز کے بڑے اور بڑے نمونے کو دیکھتے ہیں تو اس کے ساتھ کیا ہوتا ہے، اس پر غور کرنے کے لیے ایک لمحہ نکالنا آپ کے وقت کے قابل ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1677.59,
  "end": 1687.13
 },
 {
  "input": "Lastly, but probably most importantly, let's talk about the assumptions that go into this theorem. ",
  "translatedText": "آخر میں، لیکن شاید سب سے اہم بات، آئیے ان مفروضوں کے بارے میں بات کرتے ہیں جو اس نظریہ میں جاتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.95,
  "end": 1697.41
 },
 {
  "input": "The first one is that all of these variables that we're adding up are independent from each other. ",
  "translatedText": "پہلا یہ ہے کہ یہ تمام متغیرات جو ہم شامل کر رہے ہیں ایک دوسرے سے آزاد ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1698.01,
  "end": 1702.53
 },
 {
  "input": "The outcome of one process doesn't influence the outcome of any other process. ",
  "translatedText": "ایک عمل کا نتیجہ کسی دوسرے عمل کے نتائج کو متاثر نہیں کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.85,
  "end": 1706.31
 },
 {
  "input": "The second is that all of these variables are drawn from the same distribution. ",
  "translatedText": "دوسرا یہ کہ یہ تمام متغیرات ایک ہی تقسیم سے اخذ کیے گئے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1707.25,
  "end": 1710.95
 },
 {
  "input": "Both of these have been implicitly assumed with our dice example. ",
  "translatedText": "ان دونوں کو ہماری ڈائس مثال کے ساتھ واضح طور پر فرض کیا گیا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1711.31,
  "end": 1714.39
 },
 {
  "input": "We've been treating the outcome of each die roll as independent from the outcome of all the others, and we're assuming that each die follows the same distribution. ",
  "translatedText": "ہم ہر ڈائی رول کے نتائج کو باقی تمام کے نتائج سے آزاد سمجھ رہے ہیں، اور ہم یہ فرض کر رہے ہیں کہ ہر ڈائی ایک ہی تقسیم کی پیروی کرتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1714.79,
  "end": 1722.03
 },
 {
  "input": "Sometimes in the literature you'll see these two assumptions lumped together under the initials IID for independent and identically distributed. ",
  "translatedText": "کبھی کبھی ادب میں آپ دیکھیں گے کہ یہ دونوں مفروضے آزاد اور یکساں طور پر تقسیم کیے جانے والے ابتدائی IID کے تحت ایک ساتھ ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1722.85,
  "end": 1729.91
 },
 {
  "input": "One situation where these assumptions are decidedly not true would be the Galton board. ",
  "translatedText": "ایک صورت حال جہاں یہ مفروضے قطعی طور پر درست نہیں ہیں وہ گیلٹن بورڈ ہوگی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1730.53,
  "end": 1735.11
 },
 {
  "input": "I mean, think about it. ",
  "translatedText": "میرا مطلب ہے، اس کے بارے میں سوچو۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.71,
  "end": 1736.83
 },
 {
  "input": "Is it the case that the way a ball bounces off of one of the pegs is independent from how it's going to bounce off the next peg? ",
  "translatedText": "کیا یہ معاملہ ہے کہ جس طرح سے گیند کسی ایک پیگ سے اچھالتی ہے وہ اس بات سے آزاد ہے کہ وہ اگلے پیگ کو کیسے اچھال رہی ہے؟ بالکل نہیں. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1736.97,
  "end": 1743.19
 },
 {
  "input": "Absolutely not. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1743.83,
  "end": 1744.61
 },
 {
  "input": "Depending on the last bounce, it's coming in with a completely different trajectory. ",
  "translatedText": "آخری اچھال پر منحصر ہے، یہ بالکل مختلف رفتار کے ساتھ آ رہا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1744.77,
  "end": 1747.87
 },
 {
  "input": "And is it the case that the distribution of possible outcomes off of each peg are the same for each peg that it hits? ",
  "translatedText": "اور کیا یہ معاملہ ہے کہ ہر پیگ کے ممکنہ نتائج کی تقسیم ہر پیگ کے لیے یکساں ہے جس سے وہ ٹکراتی ہے؟ ایک بار پھر، تقریبا یقینی طور پر نہیں. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1748.21,
  "end": 1754.67
 },
 {
  "input": "Again, almost certainly not. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1755.19,
  "end": 1756.71
 },
 {
  "input": "Maybe it hits one peg glancing to the left, meaning the outcomes are hugely skewed in that direction, and then hits the next one glancing to the right. ",
  "translatedText": "ہو سکتا ہے کہ یہ بائیں طرف جھانکتے ہوئے ایک کھونٹی سے ٹکرا جائے، یعنی نتائج اس سمت میں بہت زیادہ ترچھے ہوئے ہیں، اور پھر دائیں طرف جھانکتے ہوئے اگلے کو مارتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1756.71,
  "end": 1763.71
 },
 {
  "input": "When I made all those simplifying assumptions in the opening example, it wasn't just to make this easier to think about. ",
  "translatedText": "جب میں نے ابتدائی مثال میں وہ تمام آسان بنانے والے مفروضے بنائے، تو یہ صرف اس کے بارے میں سوچنا آسان بنانے کے لیے نہیں تھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1765.73,
  "end": 1771.63
 },
 {
  "input": "It's also that those assumptions were necessary for this to actually be an example of the central limit theorem. ",
  "translatedText": "یہ بھی ہے کہ وہ مفروضے اس کے لیے ضروری تھے کہ اصل میں مرکزی حد نظریہ کی مثال بنیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1771.97,
  "end": 1777.07
 },
 {
  "input": "Nevertheless, it seems to be true that for the real Galton board, despite violating both of these, a normal distribution does kind of come about? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1778.13,
  "end": 1785.47
 },
 {
  "input": "Part of the reason might be that there are generalizations of the theorem beyond the scope of this video that relax these assumptions, especially the second one. ",
  "translatedText": "بہر حال، یہ سچ معلوم ہوتا ہے کہ اصلی گالٹن بورڈ کے لیے، ان دونوں کی خلاف ورزی کے باوجود، کیا ایک عام تقسیم کسی قسم کی ہوتی ہے؟ اس کی ایک وجہ یہ ہو سکتی ہے کہ اس ویڈیو کے دائرہ کار سے باہر تھیوریم کی عمومیتیں ہیں جو ان مفروضوں کو آرام دیتی ہیں، خاص طور پر دوسرا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1786.05,
  "end": 1793.89
 },
 {
  "input": "But I do want to caution you against the fact that many times people seem to assume that a variable is normally distributed, even when there's no actual justification to do so. ",
  "translatedText": "لیکن میں آپ کو اس حقیقت سے خبردار کرنا چاہتا ہوں کہ کئی بار لوگ یہ سمجھتے ہیں کہ متغیر کو عام طور پر تقسیم کیا جاتا ہے، یہاں تک کہ جب ایسا کرنے کا کوئی حقیقی جواز نہ ہو۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1794.49,
  "end": 1803.07
 },
 {
  "input": "The third assumption is actually fairly subtle. ",
  "translatedText": "تیسرا مفروضہ دراصل کافی لطیف ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1804.29,
  "end": 1806.21
 },
 {
  "input": "It's that the variance we've been computing for these variables is finite. ",
  "translatedText": "یہ ہے کہ ہم ان متغیرات کے لیے جو تغیرات مرتب کر رہے ہیں وہ محدود ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1806.21,
  "end": 1810.27
 },
 {
  "input": "This was never an issue for the dice example, because there were only six possible outcomes. ",
  "translatedText": "ڈائس مثال کے لیے یہ کبھی بھی کوئی مسئلہ نہیں تھا، کیونکہ صرف چھ ممکنہ نتائج تھے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1810.81,
  "end": 1814.85
 },
 {
  "input": "But in certain situations where you have an infinite set of outcomes, when you go to compute the variance, the sum ends up diverging off to infinity. ",
  "translatedText": "لیکن بعض حالات میں جہاں آپ کے پاس نتائج کا لامحدود سیٹ ہوتا ہے، جب آپ تغیرات کی گنتی کرنے جاتے ہیں، تو رقم لامحدودیت کی طرف موڑ کر ختم ہو جاتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1815.03,
  "end": 1822.51
 },
 {
  "input": "These can be perfectly valid probability distributions, and they do come up in practice. ",
  "translatedText": "یہ بالکل درست امکانی تقسیم ہو سکتے ہیں، اور یہ عملی طور پر سامنے آتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1823.45,
  "end": 1827.25
 },
 {
  "input": "But in those situations, as you consider adding many different instantiations of that variable and letting that sum approach infinity, even if the first two assumptions hold, it is very much a possibility that the thing you tend towards is not actually a normal distribution. ",
  "translatedText": "لیکن ان حالات میں، جیسا کہ آپ اس متغیر کی بہت سی مختلف صورتیں شامل کرنے پر غور کرتے ہیں اور اس مجموعے کو لامحدود ہونے دیتے ہیں، یہاں تک کہ اگر پہلے دو مفروضوں کو برقرار رکھا جائے، تو اس بات کا بہت زیادہ امکان ہے کہ جس چیز کی طرف آپ کا رجحان ہے وہ دراصل ایک عام تقسیم نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1827.55,
  "end": 1841.19
 },
 {
  "input": "If you've understood everything up to this point, you now have a very strong foundation in what the central limit theorem is all about. ",
  "translatedText": "اگر آپ اس وقت تک سب کچھ سمجھ چکے ہیں، تو اب آپ کے پاس اس بات کی بہت مضبوط بنیاد ہے کہ مرکزی حد کا نظریہ کیا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1842.15,
  "end": 1847.65
 },
 {
  "input": "And next up, I'd like to explain why it is that this particular function is the thing that we tend towards, and why it has a pi in it, what it has to do with circles. ",
  "translatedText": "اور اس کے بعد، میں یہ بتانا چاہوں گا کہ ایسا کیوں ہے کہ یہ خاص فنکشن وہ چیز ہے جس کی طرف ہمارا رجحان ہے، اور اس میں ایک pi کیوں ہے، اس کا حلقوں سے کیا تعلق ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1848.29,
  "end": 1874.17
 }
]