1
00:00:00,000 --> 00:00:06,120
The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.

2
00:00:06,640 --> 00:00:08,340
But you might wonder, why this function?

3
00:00:08,720 --> 00:00:14,740
Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle,

4
00:00:15,080 --> 00:00:20,440
why is it that the theory of probability seems to have a special place in its heart for this particular expression?

5
00:00:21,380 --> 00:00:24,300
For the last many videos I've been hinting at an answer to this question,

6
00:00:24,600 --> 00:00:27,680
and here we'll finally arrive at something like a satisfying answer.

7
00:00:27,680 --> 00:00:32,320
As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem,

8
00:00:32,740 --> 00:00:36,320
which describes how as you add multiple copies of a random variable,

9
00:00:36,680 --> 00:00:41,700
for example rolling a weighted die many different times, or letting a ball bounce off of a peg repeatedly,

10
00:00:42,180 --> 00:00:47,720
then the distribution describing that sum tends to look approximately like a normal distribution.

11
00:00:48,440 --> 00:00:53,260
What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions,

12
00:00:53,620 --> 00:00:56,220
that approximation to a normal becomes better and better.

13
00:00:56,940 --> 00:01:01,980
But I never explained why this theorem is actually true. We only talked about what it's claiming.

14
00:01:03,080 --> 00:01:07,880
In the last video we started talking about the math involved in adding two random variables.

15
00:01:08,260 --> 00:01:11,240
If you have two random variables, each following some distribution,

16
00:01:11,680 --> 00:01:15,120
then to find the distribution describing the sum of those variables,

17
00:01:15,540 --> 00:01:19,700
you compute something known as a convolution between the two original functions.

18
00:01:19,880 --> 00:01:25,940
And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.

19
00:01:25,940 --> 00:01:29,560
Today our basic job is to work through a particular example,

20
00:01:29,560 --> 00:01:34,340
which is to ask what happens when you add two normally distributed random variables,

21
00:01:35,040 --> 00:01:41,780
which, as you know by now, is the same as asking what do you get if you compute a convolution between two Gaussian functions.

22
00:01:42,520 --> 00:01:46,660
I'd like to share an especially pleasing visual way that you can think about this calculation,

23
00:01:47,020 --> 00:01:52,360
which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.

24
00:01:52,360 --> 00:01:58,240
After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.

25
00:01:58,320 --> 00:02:03,560
It's the step that answers the question of why a Gaussian and not something else is the central limit.

26
00:02:04,200 --> 00:02:05,840
But first, let's dive in.

27
00:02:09,780 --> 00:02:14,440
The full formula for a Gaussian is more complicated than just e to the negative x squared.

28
00:02:14,820 --> 00:02:19,420
The exponent is typically written as negative one half times x divided by sigma squared,

29
00:02:19,420 --> 00:02:24,200
where sigma describes the spread of the distribution, specifically the standard deviation.

30
00:02:24,680 --> 00:02:27,740
All of this needs to be multiplied by a fraction on the front,

31
00:02:28,020 --> 00:02:33,420
which is there to make sure that the area under the curve is one, making it a valid probability distribution.

32
00:02:34,020 --> 00:02:37,480
And if you want to consider distributions that aren't necessarily centered at zero,

33
00:02:37,800 --> 00:02:41,180
you would also throw another parameter, mu, into the exponent like this.

34
00:02:41,540 --> 00:02:45,120
Although for everything we'll be doing here, we just consider centered distributions.

35
00:02:45,800 --> 00:02:52,020
Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions,

36
00:02:52,800 --> 00:02:56,360
the direct way to do this would be to take the definition of a convolution,

37
00:02:56,580 --> 00:02:58,940
this integral expression we built up last video,

38
00:02:59,240 --> 00:03:03,760
and then to plug in for each one of the functions involved the formula for a Gaussian.

39
00:03:04,220 --> 00:03:06,340
It's kind of a lot of symbols when you throw it all together,

40
00:03:06,480 --> 00:03:10,080
but more than anything, working this out is an exercise in completing the square.

41
00:03:10,560 --> 00:03:13,220
And there's nothing wrong with that. That will get you the answer that you want.

42
00:03:13,760 --> 00:03:16,420
But of course, you know me, I'm a sucker for visual intuition,

43
00:03:16,640 --> 00:03:20,660
and in this case, there's another way to think about it that I haven't seen written about before

44
00:03:20,660 --> 00:03:24,180
that offers a very nice connection to other aspects of this distribution,

45
00:03:24,420 --> 00:03:27,860
like the presence of pi and certain ways to derive where it comes from.

46
00:03:28,200 --> 00:03:33,140
And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution,

47
00:03:33,460 --> 00:03:37,960
and just showing the computation for the simplified form, e to the negative x squared.

48
00:03:37,960 --> 00:03:44,080
The essence of what we want to compute is what the convolution between two copies of this function looks like.

49
00:03:44,460 --> 00:03:48,600
If you'll remember, in the last video we had two different ways to visualize convolutions,

50
00:03:49,140 --> 00:03:52,920
and the one we'll be using here is the second one involving diagonal slices.

51
00:03:53,280 --> 00:03:55,340
And as a quick reminder of the way that worked,

52
00:03:55,780 --> 00:04:00,900
if you have two different distributions that are described by two different functions, f and g,

53
00:04:01,180 --> 00:04:06,000
then every possible pair of values that you might get when you sample from these two distributions

54
00:04:06,000 --> 00:04:09,560
can be thought of as individual points on the xy-plane.

55
00:04:10,360 --> 00:04:17,520
And the probability density of landing on one such point, assuming independence, looks like f of x times g of y.

56
00:04:18,000 --> 00:04:23,200
So what we do is we look at a graph of that expression as a two-variable function of x and y,

57
00:04:23,560 --> 00:04:29,620
which is a way of showing the distribution of all possible outcomes when we sample from the two different variables.

58
00:04:30,560 --> 00:04:34,920
To interpret the convolution of f and g evaluated on some input s,

59
00:04:34,920 --> 00:04:40,360
which is a way of saying how likely are you to get a pair of samples that adds up to this sum s,

60
00:04:40,980 --> 00:04:46,240
what you do is you look at a slice of this graph over the line x plus y equals s,

61
00:04:46,740 --> 00:04:49,300
and you consider the area under that slice.

62
00:04:51,100 --> 00:04:56,320
This area is almost, but not quite, the value of the convolution at s.

63
00:04:56,800 --> 00:05:00,160
For a mildly technical reason, you need to divide by the square root of 2.

64
00:05:00,840 --> 00:05:03,440
Still, this area is the key feature to focus on.

65
00:05:03,440 --> 00:05:11,040
You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum.

66
00:05:13,300 --> 00:05:19,480
In the specific case where these two functions look like e to the negative x squared and e to the negative y squared,

67
00:05:19,940 --> 00:05:23,500
the resulting 3D graph has a really nice property that you can exploit.

68
00:05:23,720 --> 00:05:25,680
It's rotationally symmetric.

69
00:05:26,880 --> 00:05:32,720
You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared,

70
00:05:32,720 --> 00:05:38,460
and this term describes the square of the distance between any point on the xy plane and the origin.

71
00:05:39,200 --> 00:05:43,160
So in other words, the expression is purely a function of the distance from the origin.

72
00:05:44,560 --> 00:05:47,920
And by the way, this would not be true for any other distribution.

73
00:05:48,100 --> 00:05:51,280
It's a property that uniquely characterizes bell curves.

74
00:05:53,160 --> 00:05:59,040
So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about,

75
00:05:59,040 --> 00:06:05,540
and honestly, calculating the area would just amount to computing the original integral that defines a convolution in the first place.

76
00:06:05,940 --> 00:06:09,360
So in most cases, the visual intuition doesn't really buy you anything.

77
00:06:10,360 --> 00:06:13,920
But in the case of bell curves, you can leverage that rotational symmetry.

78
00:06:14,800 --> 00:06:20,480
Here, focus on one of these slices over the line x plus y equals s for some value of s.

79
00:06:21,300 --> 00:06:25,840
And remember, the convolution that we're trying to compute is a function of s.

80
00:06:25,840 --> 00:06:31,100
The thing that you want is an expression of s that tells you the area under this slice.

81
00:06:31,700 --> 00:06:37,900
Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s,

82
00:06:38,320 --> 00:06:45,320
and a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two.

83
00:06:45,860 --> 00:06:51,420
Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees

84
00:06:51,420 --> 00:06:56,360
where you'd find something parallel to the y-axis the same distance away from the origin.

85
00:06:57,640 --> 00:07:04,540
The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions

86
00:07:04,540 --> 00:07:08,260
because it only involves taking an integral with respect to y.

87
00:07:08,740 --> 00:07:14,760
The value of x on this slice is a constant. Specifically, it would be the constant s divided by the square root of two.

88
00:07:14,760 --> 00:07:23,380
So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out.

89
00:07:23,880 --> 00:07:30,200
This is the important point. All of the stuff that's involving s is now entirely separate from the integrated variable.

90
00:07:30,820 --> 00:07:35,200
This remaining integral is a little bit tricky. I did a whole video on it, it's actually quite famous.

91
00:07:35,500 --> 00:07:39,000
But you almost don't really care. The point is that it's just some number.

92
00:07:39,000 --> 00:07:45,480
That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s.

93
00:07:46,880 --> 00:07:54,280
And essentially this is our answer. We were looking for an expression for the area of these slices as a function of s, and now we have it.

94
00:07:54,380 --> 00:07:58,840
It looks like e to the negative s squared divided by two, scaled by some constant.

95
00:07:59,300 --> 00:08:05,620
In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent.

96
00:08:05,620 --> 00:08:14,160
As I said earlier, the convolution evaluated at s is not quite this area. Technically it's this area divided by the square root of two.

97
00:08:14,800 --> 00:08:19,240
We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant.

98
00:08:19,680 --> 00:08:25,680
What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian.

99
00:08:27,560 --> 00:08:35,380
If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma,

100
00:08:35,380 --> 00:08:41,300
essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front,

101
00:08:41,560 --> 00:08:50,380
and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma.

102
00:08:50,980 --> 00:08:56,060
If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result.

103
00:08:56,380 --> 00:09:02,500
Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process.

104
00:09:03,260 --> 00:09:09,440
Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations.

105
00:09:10,420 --> 00:09:13,940
Still, some of you might be raising your hands and saying, what's the big deal?

106
00:09:14,480 --> 00:09:19,420
I mean, when you first heard the question, what do you get when you add two normally distributed random variables,

107
00:09:19,700 --> 00:09:24,320
you probably even guessed that the answer should be another normally distributed variable.

108
00:09:24,760 --> 00:09:26,360
After all, what else is it going to be?

109
00:09:26,860 --> 00:09:30,240
Normal distributions are supposedly quite common, so why not?

110
00:09:30,240 --> 00:09:33,340
You could even say that this should follow from the central limit theorem.

111
00:09:33,860 --> 00:09:35,480
But that would have it all backwards.

112
00:09:36,180 --> 00:09:40,540
First of all, the supposed ubiquity of normal distributions is often a little exaggerated,

113
00:09:40,700 --> 00:09:44,700
but to the extent that they do come up, it is because of the central limit theorem,

114
00:09:45,140 --> 00:09:48,380
but it would be cheating to say the central limit theorem implies this result

115
00:09:48,380 --> 00:09:53,760
because this computation we just did is the reason that the function at the heart of the central limit theorem

116
00:09:53,760 --> 00:09:57,060
is a Gaussian in the first place, and not some other function.

117
00:09:57,060 --> 00:10:00,320
We've talked all about the central limit theorem before,

118
00:10:00,440 --> 00:10:04,900
but essentially it says if you repeatedly add copies of a random variable to itself,

119
00:10:05,060 --> 00:10:10,020
which mathematically looks like repeatedly computing convolutions against a given distribution,

120
00:10:10,420 --> 00:10:16,500
then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution.

121
00:10:16,980 --> 00:10:21,360
Technically, there's a small assumption the distribution you start with can't have infinite variance,

122
00:10:21,420 --> 00:10:23,220
but it's a relatively soft assumption.

123
00:10:23,220 --> 00:10:26,960
The magic is that for a huge category of initial distributions,

124
00:10:27,360 --> 00:10:31,360
this process of adding a whole bunch of random variables drawn from that distribution

125
00:10:31,360 --> 00:10:35,100
always tends towards this one universal shape, a Gaussian.

126
00:10:35,820 --> 00:10:39,300
One common approach to proving this theorem involves two separate steps.

127
00:10:39,600 --> 00:10:44,080
The first step is to show that for all the different finite variance distributions you might start with,

128
00:10:44,260 --> 00:10:50,000
there exists a single universal shape that this process of repeated convolutions tends towards.

129
00:10:50,000 --> 00:10:54,240
This step is actually pretty technical, it goes a little beyond what I want to talk about here.

130
00:10:54,520 --> 00:10:57,400
You often use these objects called moment generating functions

131
00:10:57,400 --> 00:11:01,300
that gives you a very abstract argument that there must be some universal shape,

132
00:11:01,620 --> 00:11:04,800
but it doesn't make any claim about what that particular shape is,

133
00:11:05,060 --> 00:11:09,980
just that everything in this big family is tending towards a single point in the space of distributions.

134
00:11:10,620 --> 00:11:13,320
So then step number two is what we just showed in this video,

135
00:11:13,580 --> 00:11:17,400
prove that the convolution of two Gaussians gives another Gaussian.

136
00:11:17,400 --> 00:11:21,440
What that means is that as you apply this process of repeated convolutions,

137
00:11:21,860 --> 00:11:26,660
a Gaussian doesn't change, it's a fixed point, so the only thing it can approach is itself,

138
00:11:27,140 --> 00:11:29,820
and since it's one member in this big family of distributions,

139
00:11:30,060 --> 00:11:32,800
all of which must be tending towards a single universal shape,

140
00:11:33,080 --> 00:11:35,060
it must be that universal shape.

141
00:11:35,580 --> 00:11:37,920
I mentioned at the start how this calculation, step two,

142
00:11:38,240 --> 00:11:41,820
is something that you can do directly, just symbolically with the definitions,

143
00:11:41,980 --> 00:11:45,020
but one of the reasons I'm so charmed by a geometric argument

144
00:11:45,020 --> 00:11:48,020
that leverages the rotational symmetry of this graph

145
00:11:48,020 --> 00:11:52,300
is that it directly connects to a few things that we've talked about on this channel before,

146
00:11:52,400 --> 00:11:56,140
for example, the Herschel-Maxwell derivation of a Gaussian,

147
00:11:56,140 --> 00:11:59,260
which essentially says that you can view this rotational symmetry

148
00:11:59,260 --> 00:12:01,840
as the defining feature of the distribution,

149
00:12:02,100 --> 00:12:05,380
that it locks you into this e to the negative x squared form,

150
00:12:05,620 --> 00:12:10,760
and also as an added bonus, it connects to the classic proof for why pi shows up in the formula,

151
00:12:10,960 --> 00:12:14,740
meaning we now have a direct line between the presence and mystery of that pi

152
00:12:14,740 --> 00:12:16,500
and the central limit theorem.

153
00:12:17,060 --> 00:12:20,620
Also, on a recent Patreon post, the channel supporter Daksha Vaid-Quinter

154
00:12:20,620 --> 00:12:23,880
brought my attention to a completely different approach I hadn't seen before,

155
00:12:24,060 --> 00:12:25,760
which leverages the use of entropy,

156
00:12:26,220 --> 00:12:29,580
and again, for the theoretically curious among you, I'll leave some links in the description.

157
00:12:30,960 --> 00:12:33,220
By the way, if you want to stay up to date with new videos,

158
00:12:33,320 --> 00:12:37,000
and also any other projects that I put out there, like the Summer of Math Exposition,

159
00:12:37,280 --> 00:12:38,400
there is a mailing list.

160
00:12:38,720 --> 00:12:42,780
It's relatively new, and I'm pretty sparing about only posting what I think people will enjoy.

161
00:12:43,220 --> 00:12:46,360
Usually I try not to be too promotional at the end of videos these days,

162
00:12:46,480 --> 00:12:48,900
but if you are interested in following the work that I do,

163
00:12:49,140 --> 00:12:51,700
this is probably one of the most enduring ways to do so.

164
00:13:12,780 --> 00:13:15,260
Thanks for watching!

