1
00:00:00,000 --> 00:00:03,243
The basic function underlying a normal distribution, 

2
00:00:03,243 --> 00:00:06,120
aka a Gaussian, is e to the negative x squared.

3
00:00:06,640 --> 00:00:08,340
But you might wonder, why this function?

4
00:00:08,720 --> 00:00:12,464
Of all the expressions we could dream up that give you some symmetric smooth 

5
00:00:12,464 --> 00:00:14,847
graph with mass concentrated towards the middle, 

6
00:00:14,847 --> 00:00:18,640
why is it that the theory of probability seems to have a special place in its 

7
00:00:18,640 --> 00:00:20,440
heart for this particular expression?

8
00:00:21,380 --> 00:00:24,663
For the last many videos I've been hinting at an answer to this question, 

9
00:00:24,663 --> 00:00:27,680
and here we'll finally arrive at something like a satisfying answer.

10
00:00:27,680 --> 00:00:31,645
As a quick refresher on where we are, a couple videos ago we talked about 

11
00:00:31,645 --> 00:00:35,610
the central limit theorem, which describes how as you add multiple copies 

12
00:00:35,610 --> 00:00:39,843
of a random variable, for example rolling a weighted die many different times, 

13
00:00:39,843 --> 00:00:42,522
or letting a ball bounce off of a peg repeatedly, 

14
00:00:42,522 --> 00:00:46,541
then the distribution describing that sum tends to look approximately like 

15
00:00:46,541 --> 00:00:47,720
a normal distribution.

16
00:00:48,440 --> 00:00:52,142
What the central limit theorem says is as you make that sum bigger and bigger, 

17
00:00:52,142 --> 00:00:56,220
under appropriate conditions, that approximation to a normal becomes better and better.

18
00:00:56,940 --> 00:01:00,180
But I never explained why this theorem is actually true.

19
00:01:00,220 --> 00:01:01,980
We only talked about what it's claiming.

20
00:01:03,080 --> 00:01:05,532
In the last video we started talking about the 

21
00:01:05,532 --> 00:01:07,880
math involved in adding two random variables.

22
00:01:08,260 --> 00:01:11,844
If you have two random variables, each following some distribution, 

23
00:01:11,844 --> 00:01:15,482
then to find the distribution describing the sum of those variables, 

24
00:01:15,482 --> 00:01:19,700
you compute something known as a convolution between the two original functions.

25
00:01:19,880 --> 00:01:22,964
And we spent a lot of time building up two distinct ways 

26
00:01:22,964 --> 00:01:25,940
to visualize what this convolution operation really is.

27
00:01:25,940 --> 00:01:29,505
Today our basic job is to work through a particular example, 

28
00:01:29,505 --> 00:01:34,473
which is to ask what happens when you add two normally distributed random variables, 

29
00:01:34,473 --> 00:01:38,448
which, as you know by now, is the same as asking what do you get if 

30
00:01:38,448 --> 00:01:41,780
you compute a convolution between two Gaussian functions.

31
00:01:42,520 --> 00:01:45,831
I'd like to share an especially pleasing visual way that you can think 

32
00:01:45,831 --> 00:01:49,188
about this calculation, which hopefully offers some sense of what makes 

33
00:01:49,188 --> 00:01:52,360
the e to the negative x squared function special in the first place.

34
00:01:52,360 --> 00:01:55,254
After we walk through it, we'll talk about how this calculation 

35
00:01:55,254 --> 00:01:58,240
is one of the steps involved in proving the central limit theorem.

36
00:01:58,320 --> 00:02:00,837
It's the step that answers the question of why a 

37
00:02:00,837 --> 00:02:03,560
Gaussian and not something else is the central limit.

38
00:02:04,200 --> 00:02:05,840
But first, let's dive in.

39
00:02:09,780 --> 00:02:14,440
The full formula for a Gaussian is more complicated than just e to the negative x squared.

40
00:02:14,820 --> 00:02:19,483
The exponent is typically written as negative one half times x divided by sigma squared, 

41
00:02:19,483 --> 00:02:24,200
where sigma describes the spread of the distribution, specifically the standard deviation.

42
00:02:24,680 --> 00:02:27,881
All of this needs to be multiplied by a fraction on the front, 

43
00:02:27,881 --> 00:02:31,235
which is there to make sure that the area under the curve is one, 

44
00:02:31,235 --> 00:02:33,420
making it a valid probability distribution.

45
00:02:34,020 --> 00:02:37,875
And if you want to consider distributions that aren't necessarily centered at zero, 

46
00:02:37,875 --> 00:02:41,180
you would also throw another parameter, mu, into the exponent like this.

47
00:02:41,540 --> 00:02:45,120
Although for everything we'll be doing here, we just consider centered distributions.

48
00:02:45,800 --> 00:02:48,405
Now if you look at our central goal for today, 

49
00:02:48,405 --> 00:02:52,063
which is to compute a convolution between two Gaussian functions, 

50
00:02:52,063 --> 00:02:56,276
the direct way to do this would be to take the definition of a convolution, 

51
00:02:56,276 --> 00:02:58,992
this integral expression we built up last video, 

52
00:02:58,992 --> 00:03:03,760
and then to plug in for each one of the functions involved the formula for a Gaussian.

53
00:03:04,220 --> 00:03:06,760
It's kind of a lot of symbols when you throw it all together, 

54
00:03:06,760 --> 00:03:10,080
but more than anything, working this out is an exercise in completing the square.

55
00:03:10,560 --> 00:03:11,580
And there's nothing wrong with that.

56
00:03:11,720 --> 00:03:13,220
That will get you the answer that you want.

57
00:03:13,760 --> 00:03:17,504
But of course, you know me, I'm a sucker for visual intuition, and in this case, 

58
00:03:17,504 --> 00:03:21,156
there's another way to think about it that I haven't seen written about before 

59
00:03:21,156 --> 00:03:24,577
that offers a very nice connection to other aspects of this distribution, 

60
00:03:24,577 --> 00:03:27,860
like the presence of pi and certain ways to derive where it comes from.

61
00:03:28,200 --> 00:03:31,437
And the way I'd like to do this is by first peeling away all of the 

62
00:03:31,437 --> 00:03:33,865
constants associated with the actual distribution, 

63
00:03:33,865 --> 00:03:37,960
and just showing the computation for the simplified form, e to the negative x squared.

64
00:03:37,960 --> 00:03:40,797
The essence of what we want to compute is what the 

65
00:03:40,797 --> 00:03:44,080
convolution between two copies of this function looks like.

66
00:03:44,460 --> 00:03:48,360
If you'll remember, in the last video we had two different ways to visualize 

67
00:03:48,360 --> 00:03:52,920
convolutions, and the one we'll be using here is the second one involving diagonal slices.

68
00:03:53,280 --> 00:03:55,902
And as a quick reminder of the way that worked, 

69
00:03:55,902 --> 00:04:00,655
if you have two different distributions that are described by two different functions, 

70
00:04:00,655 --> 00:04:04,588
f and g, then every possible pair of values that you might get when you 

71
00:04:04,588 --> 00:04:08,685
sample from these two distributions can be thought of as individual points 

72
00:04:08,685 --> 00:04:09,560
on the xy-plane.

73
00:04:10,360 --> 00:04:14,067
And the probability density of landing on one such point, 

74
00:04:14,067 --> 00:04:17,519
assuming independence, looks like f of x times g of y.

75
00:04:18,000 --> 00:04:22,039
So what we do is we look at a graph of that expression as a two-variable 

76
00:04:22,039 --> 00:04:25,968
function of x and y, which is a way of showing the distribution of all 

77
00:04:25,968 --> 00:04:29,620
possible outcomes when we sample from the two different variables.

78
00:04:30,560 --> 00:04:34,904
To interpret the convolution of f and g evaluated on some input s, 

79
00:04:34,904 --> 00:04:39,703
which is a way of saying how likely are you to get a pair of samples that 

80
00:04:39,703 --> 00:04:44,371
adds up to this sum s, what you do is you look at a slice of this graph 

81
00:04:44,371 --> 00:04:49,300
over the line x plus y equals s, and you consider the area under that slice.

82
00:04:51,100 --> 00:04:56,320
This area is almost, but not quite, the value of the convolution at s.

83
00:04:56,800 --> 00:05:00,160
For a mildly technical reason, you need to divide by the square root of 2.

84
00:05:00,840 --> 00:05:03,440
Still, this area is the key feature to focus on.

85
00:05:03,440 --> 00:05:07,412
You can think of it as a way to combine together all the probability 

86
00:05:07,412 --> 00:05:11,040
densities for all of the outcomes corresponding to a given sum.

87
00:05:13,300 --> 00:05:16,646
In the specific case where these two functions look like e to 

88
00:05:16,646 --> 00:05:19,668
the negative x squared and e to the negative y squared, 

89
00:05:19,668 --> 00:05:23,500
the resulting 3D graph has a really nice property that you can exploit.

90
00:05:23,720 --> 00:05:25,680
It's rotationally symmetric.

91
00:05:26,880 --> 00:05:30,812
You can see this by combining the terms and noticing that it's entirely 

92
00:05:30,812 --> 00:05:34,527
a function of x squared plus y squared, and this term describes the 

93
00:05:34,527 --> 00:05:38,460
square of the distance between any point on the xy plane and the origin.

94
00:05:39,200 --> 00:05:43,160
So in other words, the expression is purely a function of the distance from the origin.

95
00:05:44,560 --> 00:05:47,920
And by the way, this would not be true for any other distribution.

96
00:05:48,100 --> 00:05:51,280
It's a property that uniquely characterizes bell curves.

97
00:05:53,160 --> 00:05:57,319
So for most other pairs of functions, these diagonal slices will be some complicated 

98
00:05:57,319 --> 00:05:59,668
shape that's hard to think about, and honestly, 

99
00:05:59,668 --> 00:06:03,925
calculating the area would just amount to computing the original integral that defines 

100
00:06:03,925 --> 00:06:05,540
a convolution in the first place.

101
00:06:05,940 --> 00:06:09,360
So in most cases, the visual intuition doesn't really buy you anything.

102
00:06:10,360 --> 00:06:13,920
But in the case of bell curves, you can leverage that rotational symmetry.

103
00:06:14,800 --> 00:06:20,480
Here, focus on one of these slices over the line x plus y equals s for some value of s.

104
00:06:21,300 --> 00:06:25,840
And remember, the convolution that we're trying to compute is a function of s.

105
00:06:25,840 --> 00:06:31,100
The thing that you want is an expression of s that tells you the area under this slice.

106
00:06:31,700 --> 00:06:36,392
Well, if you look at that line, it intersects the x-axis at s zero and the y-axis 

107
00:06:36,392 --> 00:06:40,913
at zero s, and a little bit of Pythagoras will show you that the straight line 

108
00:06:40,913 --> 00:06:45,320
distance from the origin to this line is s divided by the square root of two.

109
00:06:45,860 --> 00:06:49,360
Now, because of the symmetry, this slice is identical to one 

110
00:06:49,360 --> 00:06:52,802
that you get rotating 45 degrees where you'd find something 

111
00:06:52,802 --> 00:06:56,360
parallel to the y-axis the same distance away from the origin.

112
00:06:57,640 --> 00:07:02,366
The key is that computing this other area of a slice parallel to the y-axis is much, 

113
00:07:02,366 --> 00:07:05,702
much easier than slices in other directions because it only 

114
00:07:05,702 --> 00:07:08,260
involves taking an integral with respect to y.

115
00:07:08,740 --> 00:07:11,440
The value of x on this slice is a constant.

116
00:07:11,620 --> 00:07:14,760
Specifically, it would be the constant s divided by the square root of two.

117
00:07:14,760 --> 00:07:18,231
So when you're computing the integral, finding this area, 

118
00:07:18,231 --> 00:07:23,380
all of this term here behaves like it was just some number, and you can factor it out.

119
00:07:23,880 --> 00:07:24,940
This is the important point.

120
00:07:25,280 --> 00:07:30,200
All of the stuff that's involving s is now entirely separate from the integrated variable.

121
00:07:30,820 --> 00:07:33,000
This remaining integral is a little bit tricky.

122
00:07:33,080 --> 00:07:35,200
I did a whole video on it, it's actually quite famous.

123
00:07:35,500 --> 00:07:36,900
But you almost don't really care.

124
00:07:37,240 --> 00:07:39,000
The point is that it's just some number.

125
00:07:39,000 --> 00:07:41,645
That number happens to be the square root of pi, 

126
00:07:41,645 --> 00:07:45,480
but what really matters is that it's something with no dependence on s.

127
00:07:46,880 --> 00:07:48,480
And essentially this is our answer.

128
00:07:48,780 --> 00:07:53,255
We were looking for an expression for the area of these slices as a function of s, 

129
00:07:53,255 --> 00:07:54,280
and now we have it.

130
00:07:54,380 --> 00:07:58,840
It looks like e to the negative s squared divided by two, scaled by some constant.

131
00:07:59,300 --> 00:08:02,209
In other words, it's also a bell curve, another Gaussian, 

132
00:08:02,209 --> 00:08:05,620
just stretched out a little bit because of this two in the exponent.

133
00:08:05,620 --> 00:08:10,860
As I said earlier, the convolution evaluated at s is not quite this area.

134
00:08:11,340 --> 00:08:14,160
Technically it's this area divided by the square root of two.

135
00:08:14,800 --> 00:08:16,901
We talked about it in the last video, but it doesn't 

136
00:08:16,901 --> 00:08:19,240
really matter because it just gets baked into the constant.

137
00:08:19,680 --> 00:08:22,906
What really matters is the conclusion that a convolution 

138
00:08:22,906 --> 00:08:25,680
between two Gaussians is itself another Gaussian.

139
00:08:27,560 --> 00:08:31,930
If you were to go back and reintroduce all of the constants for a normal distribution 

140
00:08:31,930 --> 00:08:34,980
with a mean zero and an arbitrary standard deviation sigma, 

141
00:08:34,980 --> 00:08:39,300
essentially identical reasoning will lead to the same square root of two factor that 

142
00:08:39,300 --> 00:08:43,366
shows up in the exponent and out front, and it leads to the conclusion that the 

143
00:08:43,366 --> 00:08:47,838
convolution between two such normal distributions is another normal distribution with a 

144
00:08:47,838 --> 00:08:50,380
standard deviation square root of two times sigma.

145
00:08:50,980 --> 00:08:53,543
If you haven't computed a lot of convolutions before, 

146
00:08:53,543 --> 00:08:56,060
it's worth emphasizing this is a very special result.

147
00:08:56,380 --> 00:08:59,912
Almost always you end up with a completely different kind of function, 

148
00:08:59,912 --> 00:09:02,500
but here there's a sort of stability to the process.

149
00:09:03,260 --> 00:09:06,455
Also, for those of you who enjoy exercises, I'll leave one up on the screen 

150
00:09:06,455 --> 00:09:09,440
for how you would handle the case of two different standard deviations.

151
00:09:10,420 --> 00:09:13,940
Still, some of you might be raising your hands and saying, what's the big deal?

152
00:09:14,480 --> 00:09:17,744
I mean, when you first heard the question, what do you get when you 

153
00:09:17,744 --> 00:09:20,000
add two normally distributed random variables, 

154
00:09:20,000 --> 00:09:24,320
you probably even guessed that the answer should be another normally distributed variable.

155
00:09:24,760 --> 00:09:26,360
After all, what else is it going to be?

156
00:09:26,860 --> 00:09:30,240
Normal distributions are supposedly quite common, so why not?

157
00:09:30,240 --> 00:09:33,340
You could even say that this should follow from the central limit theorem.

158
00:09:33,860 --> 00:09:35,480
But that would have it all backwards.

159
00:09:36,180 --> 00:09:40,003
First of all, the supposed ubiquity of normal distributions is often a little 

160
00:09:40,003 --> 00:09:42,600
exaggerated, but to the extent that they do come up, 

161
00:09:42,600 --> 00:09:46,914
it is because of the central limit theorem, but it would be cheating to say the central 

162
00:09:46,914 --> 00:09:51,080
limit theorem implies this result because this computation we just did is the reason 

163
00:09:51,080 --> 00:09:55,344
that the function at the heart of the central limit theorem is a Gaussian in the first 

164
00:09:55,344 --> 00:09:57,060
place, and not some other function.

165
00:09:57,060 --> 00:10:00,300
We've talked all about the central limit theorem before, 

166
00:10:00,300 --> 00:10:05,131
but essentially it says if you repeatedly add copies of a random variable to itself, 

167
00:10:05,131 --> 00:10:09,792
which mathematically looks like repeatedly computing convolutions against a given 

168
00:10:09,792 --> 00:10:13,260
distribution, then after appropriate shifting and rescaling, 

169
00:10:13,260 --> 00:10:16,500
the tendency is always to approach a normal distribution.

170
00:10:16,980 --> 00:10:20,189
Technically, there's a small assumption the distribution you start with 

171
00:10:20,189 --> 00:10:23,220
can't have infinite variance, but it's a relatively soft assumption.

172
00:10:23,220 --> 00:10:26,875
The magic is that for a huge category of initial distributions, 

173
00:10:26,875 --> 00:10:30,759
this process of adding a whole bunch of random variables drawn from 

174
00:10:30,759 --> 00:10:35,100
that distribution always tends towards this one universal shape, a Gaussian.

175
00:10:35,820 --> 00:10:39,300
One common approach to proving this theorem involves two separate steps.

176
00:10:39,600 --> 00:10:43,188
The first step is to show that for all the different finite variance 

177
00:10:43,188 --> 00:10:46,724
distributions you might start with, there exists a single universal 

178
00:10:46,724 --> 00:10:50,000
shape that this process of repeated convolutions tends towards.

179
00:10:50,000 --> 00:10:52,142
This step is actually pretty technical, it goes 

180
00:10:52,142 --> 00:10:54,240
a little beyond what I want to talk about here.

181
00:10:54,520 --> 00:10:58,348
You often use these objects called moment generating functions that gives you 

182
00:10:58,348 --> 00:11:01,587
a very abstract argument that there must be some universal shape, 

183
00:11:01,587 --> 00:11:04,875
but it doesn't make any claim about what that particular shape is, 

184
00:11:04,875 --> 00:11:08,654
just that everything in this big family is tending towards a single point in 

185
00:11:08,654 --> 00:11:09,980
the space of distributions.

186
00:11:10,620 --> 00:11:13,878
So then step number two is what we just showed in this video, 

187
00:11:13,878 --> 00:11:17,400
prove that the convolution of two Gaussians gives another Gaussian.

188
00:11:17,400 --> 00:11:21,517
What that means is that as you apply this process of repeated convolutions, 

189
00:11:21,517 --> 00:11:24,063
a Gaussian doesn't change, it's a fixed point, 

190
00:11:24,063 --> 00:11:28,342
so the only thing it can approach is itself, and since it's one member in this 

191
00:11:28,342 --> 00:11:32,947
big family of distributions, all of which must be tending towards a single universal 

192
00:11:32,947 --> 00:11:35,060
shape, it must be that universal shape.

193
00:11:35,580 --> 00:11:38,562
I mentioned at the start how this calculation, step two, 

194
00:11:38,562 --> 00:11:42,696
is something that you can do directly, just symbolically with the definitions, 

195
00:11:42,696 --> 00:11:46,935
but one of the reasons I'm so charmed by a geometric argument that leverages the 

196
00:11:46,935 --> 00:11:51,330
rotational symmetry of this graph is that it directly connects to a few things that 

197
00:11:51,330 --> 00:11:54,260
we've talked about on this channel before, for example, 

198
00:11:54,260 --> 00:11:56,720
the Herschel-Maxwell derivation of a Gaussian, 

199
00:11:56,720 --> 00:12:01,011
which essentially says that you can view this rotational symmetry as the defining 

200
00:12:01,011 --> 00:12:05,458
feature of the distribution, that it locks you into this e to the negative x squared 

201
00:12:05,458 --> 00:12:10,011
form, and also as an added bonus, it connects to the classic proof for why pi shows up 

202
00:12:10,011 --> 00:12:14,511
in the formula, meaning we now have a direct line between the presence and mystery of 

203
00:12:14,511 --> 00:12:16,500
that pi and the central limit theorem.

204
00:12:17,060 --> 00:12:20,357
Also, on a recent Patreon post, the channel supporter Daksha Vaid-Quinter 

205
00:12:20,357 --> 00:12:23,832
brought my attention to a completely different approach I hadn't seen before, 

206
00:12:23,832 --> 00:12:27,753
which leverages the use of entropy, and again, for the theoretically curious among you, 

207
00:12:27,753 --> 00:12:29,580
I'll leave some links in the description.

208
00:12:30,960 --> 00:12:33,585
By the way, if you want to stay up to date with new videos, 

209
00:12:33,585 --> 00:12:37,349
and also any other projects that I put out there, like the Summer of Math Exposition, 

210
00:12:37,349 --> 00:12:38,400
there is a mailing list.

211
00:12:38,720 --> 00:12:40,879
It's relatively new, and I'm pretty sparing about 

212
00:12:40,879 --> 00:12:42,780
only posting what I think people will enjoy.

213
00:12:43,220 --> 00:12:55,661
Usually I try not to be too promotional at the end of videos these days, 

214
00:12:55,661 --> 00:13:05,716
but if you are interested in following the work that I do, 

215
00:13:05,716 --> 00:13:15,260
this is probably one of the most enduring ways to do so.

