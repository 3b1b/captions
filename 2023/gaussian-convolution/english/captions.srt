1
00:00:00,000 --> 00:00:06,560
The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.

2
00:00:06,560 --> 00:00:08,660
But you might wonder, why this function?

3
00:00:08,660 --> 00:00:14,960
Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle,

4
00:00:14,960 --> 00:00:21,320
why is it that the theory of probability seems to have a special place in its heart for this particular expression?

5
00:00:21,320 --> 00:00:24,560
For the last many videos I've been hinting at an answer to this question,

6
00:00:24,560 --> 00:00:28,160
and here we'll finally arrive at something like a satisfying answer.

7
00:00:28,160 --> 00:00:32,600
As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem,

8
00:00:32,600 --> 00:00:36,640
which describes how as you add multiple copies of a random variable,

9
00:00:36,640 --> 00:00:42,120
for example rolling a weighted die many different times or letting a ball bounce off of a peg repeatedly,

10
00:00:42,120 --> 00:00:48,360
then the distribution describing that sum tends to look approximately like a normal distribution.

11
00:00:48,360 --> 00:00:53,520
What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions,

12
00:00:53,520 --> 00:00:57,280
that approximation to a normal becomes better and better.

13
00:00:57,280 --> 00:01:03,360
But I never explained why this theorem is actually true, we only talked about what it's claiming.

14
00:01:03,360 --> 00:01:08,240
In the last video we started talking about the math involved in adding two random variables.

15
00:01:08,240 --> 00:01:11,680
If you have two random variables, each following some distribution,

16
00:01:11,680 --> 00:01:15,600
then to find the distribution describing the sum of those variables,

17
00:01:15,600 --> 00:01:20,200
you compute something known as a convolution between the two original functions.

18
00:01:20,200 --> 00:01:26,440
And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.

19
00:01:26,440 --> 00:01:29,760
Today our basic job is to work through a particular example,

20
00:01:29,760 --> 00:01:35,040
which is to ask what happens when you add two normally distributed random variables,

21
00:01:35,040 --> 00:01:42,520
which as you know by now is the same as asking what do you get if you compute a convolution between two Gaussian functions.

22
00:01:42,520 --> 00:01:47,040
I'd like to share an especially pleasing visual way that you can think about this calculation,

23
00:01:47,040 --> 00:01:52,880
which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.

24
00:01:52,880 --> 00:01:58,480
After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.

25
00:01:58,480 --> 00:02:04,160
It's the step that answers the question of why a Gaussian and not something else is the central limit.

26
00:02:04,160 --> 00:02:05,680
But first, let's dive in.

27
00:02:10,160 --> 00:02:14,800
The full formula for a Gaussian is more complicated than just e to the negative x squared.

28
00:02:14,800 --> 00:02:19,600
The exponent is typically written as negative one half times x divided by sigma squared,

29
00:02:19,600 --> 00:02:22,520
where sigma describes the spread of the distribution,

30
00:02:22,520 --> 00:02:24,600
specifically the standard deviation.

31
00:02:24,600 --> 00:02:28,000
All of this needs to be multiplied by a fraction on the front,

32
00:02:28,000 --> 00:02:33,960
which is there to make sure that the area under the curve is one, making it a valid probability distribution.

33
00:02:33,960 --> 00:02:37,720
And if you want to consider distributions that aren't necessarily centered at zero,

34
00:02:37,720 --> 00:02:41,480
you would also throw another parameter, mu, into the exponent like this.

35
00:02:41,480 --> 00:02:46,200
Although for everything we'll be doing here, we just consider centered distributions.

36
00:02:46,200 --> 00:02:48,480
Now if you look at our central goal for today,

37
00:02:48,480 --> 00:02:52,640
which is to compute a convolution between two Gaussian functions,

38
00:02:52,640 --> 00:02:56,480
the direct way to do this would be to take the definition of a convolution,

39
00:02:56,480 --> 00:02:59,200
this integral expression we built up last video,

40
00:02:59,200 --> 00:03:04,080
and then to plug in for each one of the functions involved the formula for a Gaussian.

41
00:03:04,080 --> 00:03:06,480
It's kind of a lot of symbols when you throw it all together,

42
00:03:06,480 --> 00:03:10,480
but more than anything, working this out is an exercise in completing the square.

43
00:03:10,480 --> 00:03:13,760
And there's nothing wrong with that. That will get you the answer that you want.

44
00:03:13,760 --> 00:03:16,640
But of course, you know me, I'm a sucker for visual intuition,

45
00:03:16,640 --> 00:03:20,800
and in this case, there's another way to think about it that I haven't seen written about before,

46
00:03:20,800 --> 00:03:24,320
that offers a very nice connection to other aspects of this distribution,

47
00:03:24,320 --> 00:03:28,000
like the presence of pi and certain ways to derive where it comes from.

48
00:03:28,000 --> 00:03:31,920
And the way I'd like to do this is by first peeling away all of the constants associated

49
00:03:31,920 --> 00:03:36,640
with the actual distribution, and just showing the computation for the simplified form,

50
00:03:36,640 --> 00:03:38,240
e to the negative x squared.

51
00:03:38,240 --> 00:03:44,640
The essence of what we want to compute is what the convolution between two copies of this function looks like.

52
00:03:44,640 --> 00:03:49,040
If you'll remember, in the last video we had two different ways to visualize convolutions,

53
00:03:49,040 --> 00:03:53,120
and the one we'll be using here is the second one, involving diagonal slices.

54
00:03:53,120 --> 00:03:55,680
And as a quick reminder of the way that worked,

55
00:03:55,680 --> 00:04:00,160
if you have two different distributions that are described by two different functions,

56
00:04:00,160 --> 00:04:05,280
f and g, then every possible pair of values that you might get when you sample from these two

57
00:04:05,280 --> 00:04:09,440
distributions can be thought of as individual points on the xy-plane.

58
00:04:10,160 --> 00:04:15,120
And the probability density of landing on one such point, assuming independence,

59
00:04:15,120 --> 00:04:17,760
looks like f of x times g of y.

60
00:04:17,760 --> 00:04:23,520
So what we do is we look at a graph of that expression as a two-variable function of x and y,

61
00:04:23,520 --> 00:04:27,200
which is a way of showing the distribution of all possible outcomes

62
00:04:27,200 --> 00:04:29,600
when we sample from the two different variables.

63
00:04:30,480 --> 00:04:35,120
To interpret the convolution of f and g evaluated on some input s,

64
00:04:35,120 --> 00:04:40,160
which is a way of saying how likely are you to get a pair of samples that adds up to this sum s,

65
00:04:40,880 --> 00:04:46,000
what you do is you look at a slice of this graph over the line x plus y equals s,

66
00:04:46,560 --> 00:04:49,120
and you consider the area under that slice.

67
00:04:51,200 --> 00:04:56,080
This area is almost, but not quite, the value of the convolution at s.

68
00:04:56,640 --> 00:05:00,000
For a mildly technical reason, you need to divide by the square root of two.

69
00:05:00,720 --> 00:05:03,520
Still, this area is the key feature to focus on.

70
00:05:03,520 --> 00:05:07,680
You can think of it as a way to combine together all the probability densities

71
00:05:07,680 --> 00:05:10,800
for all of the outcomes corresponding to a given sum.

72
00:05:13,520 --> 00:05:17,840
In the specific case where these two functions look like e to the negative x squared

73
00:05:17,840 --> 00:05:19,840
and e to the negative y squared,

74
00:05:19,840 --> 00:05:23,840
the resulting 3D graph has a really nice property that you can exploit.

75
00:05:23,840 --> 00:05:25,360
It's rotationally symmetric.

76
00:05:27,120 --> 00:05:31,520
You can see this by combining the terms and noticing that it's entirely a function of x

77
00:05:31,520 --> 00:05:36,640
squared plus y squared, and this term describes the square of the distance between any point on

78
00:05:36,640 --> 00:05:42,160
the xy plane and the origin. So in other words, the expression is purely a function of the distance

79
00:05:42,160 --> 00:05:48,960
from the origin. And by the way, this would not be true for any other distribution. It's a property

80
00:05:48,960 --> 00:05:54,960
that uniquely characterizes bell curves. So for most other pairs of functions,

81
00:05:54,960 --> 00:05:59,840
these diagonal slices will be some complicated shape that's hard to think about, and honestly

82
00:05:59,840 --> 00:06:03,840
calculating the area would just amount to computing the original integral that defines

83
00:06:03,840 --> 00:06:09,280
a convolution in the first place. So in most cases, the visual intuition doesn't really buy you anything.

84
00:06:10,160 --> 00:06:13,920
But in the case of bell curves, you can leverage that rotational symmetry.

85
00:06:14,560 --> 00:06:20,320
Here, focus on one of these slices over the line x plus y equals s for some value of s.

86
00:06:21,120 --> 00:06:27,200
And remember, the convolution that we're trying to compute is a function of s. The thing that you want

87
00:06:27,200 --> 00:06:33,120
is an expression of s that tells you the area under this slice. Well, if you look at that line,

88
00:06:33,120 --> 00:06:39,840
it intersects the x-axis at s zero and the y-axis at zero s. And a little bit of Pythagoras will show

89
00:06:39,840 --> 00:06:45,680
you that the straight line distance from the origin to this line is s divided by the square root of two.

90
00:06:45,680 --> 00:06:51,600
Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees,

91
00:06:51,600 --> 00:06:56,320
where you'd find something parallel to the y-axis the same distance away from the origin.

92
00:06:57,520 --> 00:07:02,480
The key is that computing this other area of a slice parallel to the y-axis is much,

93
00:07:02,480 --> 00:07:06,880
much easier than slices in other directions, because it only involves taking an integral

94
00:07:06,880 --> 00:07:12,960
with respect to y. The value of x on this slice is a constant. Specifically, it would be the constant

95
00:07:12,960 --> 00:07:17,920
s divided by the square root of two. So when you're computing the integral, finding this area,

96
00:07:18,480 --> 00:07:24,080
all of this term here behaves like it was just some number, and you can factor it out. This is

97
00:07:24,080 --> 00:07:29,120
the important point. All of the stuff that's involving s is now entirely separate from the

98
00:07:29,120 --> 00:07:34,080
integrated variable. This remaining integral is a little bit tricky. I did a whole video on it,

99
00:07:34,080 --> 00:07:38,560
it's actually quite famous. But you almost don't really care. The point is that it's just some

100
00:07:38,560 --> 00:07:43,360
number. That number happens to be the square root of pi, but what really matters is that it's

101
00:07:43,360 --> 00:07:49,600
something with no dependence on s. And essentially, this is our answer. We were looking for an

102
00:07:49,600 --> 00:07:54,800
expression for the area of these slices as a function of s, and now we have it. It looks like

103
00:07:54,800 --> 00:08:01,120
e to the negative s squared divided by two, scaled by some constant. In other words, it's also a bell

104
00:08:01,120 --> 00:08:05,680
curve, another Gaussian, just stretched out a little bit because of this two in the exponent.

105
00:08:06,320 --> 00:08:12,080
As I said earlier, the convolution evaluated at s is not quite this area. Technically, it's this

106
00:08:12,080 --> 00:08:16,960
area divided by the square root of two. We talked about it in the last video, but it doesn't really

107
00:08:16,960 --> 00:08:21,440
matter because it just gets baked into the constant. What really matters is the conclusion

108
00:08:21,440 --> 00:08:28,800
that a convolution between two Gaussians is itself another Gaussian. If you were to go back and

109
00:08:28,800 --> 00:08:33,600
reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary

110
00:08:33,600 --> 00:08:38,720
standard deviation sigma, essentially identical reasoning will lead to the same square root of

111
00:08:38,720 --> 00:08:42,960
two factor that shows up in the exponent and out front, and it leads to the conclusion that the

112
00:08:42,960 --> 00:08:47,920
convolution between two such normal distributions is another normal distribution with a standard

113
00:08:47,920 --> 00:08:53,200
deviation square root of two times sigma. If you haven't computed a lot of convolutions before,

114
00:08:53,200 --> 00:08:58,240
it's worth emphasizing this is a very special result. Almost always you end up with a completely

115
00:08:58,240 --> 00:09:04,000
different kind of function, but here there's a sort of stability to the process. Also, for those

116
00:09:04,000 --> 00:09:07,920
of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of

117
00:09:07,920 --> 00:09:13,040
two different standard deviations. Still, some of you might be raising your hands and saying,

118
00:09:13,040 --> 00:09:17,520
what's the big deal? I mean, when you first heard the question, what do you get when you add two

119
00:09:17,520 --> 00:09:22,720
normally distributed random variables, you probably even guessed that the answer should be another

120
00:09:22,720 --> 00:09:27,840
normally distributed variable. After all, what else is it going to be? Normal distributions are

121
00:09:27,840 --> 00:09:32,720
supposedly quite common, so why not? You could even say that this should follow from the central

122
00:09:32,720 --> 00:09:37,920
limit theorem, but that would have it all backwards. First of all, the supposed ubiquity

123
00:09:37,920 --> 00:09:42,320
of normal distributions is often a little exaggerated, but to the extent that they do

124
00:09:42,320 --> 00:09:46,320
come up, it is because of the central limit theorem, but it would be cheating to say the

125
00:09:46,320 --> 00:09:51,520
central limit theorem implies this result because this computation we just did is the reason that

126
00:09:51,520 --> 00:09:56,160
the function at the heart of the central limit theorem is a Gaussian in the first place and not

127
00:09:56,160 --> 00:10:01,120
some other function. We've talked all about the central limit theorem before, but essentially it

128
00:10:01,120 --> 00:10:06,480
says if you repeatedly add copies of a random variable to itself, which mathematically looks

129
00:10:06,480 --> 00:10:11,920
like repeatedly computing convolutions against a given distribution, then after appropriate shifting

130
00:10:11,920 --> 00:10:18,000
and rescaling, the tendency is always to approach a normal distribution. Technically there's a small

131
00:10:18,000 --> 00:10:22,720
assumption the distribution you start with can't have infinite variance, but it's a relatively soft

132
00:10:22,720 --> 00:10:28,560
assumption. The magic is that for a huge category of initial distributions, this process of adding a

133
00:10:28,560 --> 00:10:33,360
whole bunch of random variables drawn from that distribution always tends towards this one

134
00:10:33,360 --> 00:10:39,360
universal shape, a Gaussian. One common approach to proving this theorem involves two separate steps.

135
00:10:39,360 --> 00:10:43,200
The first step is to show that for all the different finite variance distributions you

136
00:10:43,200 --> 00:10:49,040
might start with, there exists a single universal shape that this process of repeated convolutions

137
00:10:49,040 --> 00:10:53,280
tends towards. This step is actually pretty technical, it goes a little beyond what I want

138
00:10:53,280 --> 00:10:57,920
to talk about here. You often use these objects called moment generating functions that gives you

139
00:10:58,000 --> 00:11:02,720
a very abstract argument that there must be some universal shape, but it doesn't make any claim

140
00:11:02,720 --> 00:11:07,680
about what that particular shape is, just that everything in this big family is tending towards

141
00:11:07,680 --> 00:11:12,640
a single point in the space of distributions. So then step number two is what we just showed in

142
00:11:12,640 --> 00:11:18,480
this video, prove that the convolution of two Gaussians gives another Gaussian. What that means

143
00:11:18,480 --> 00:11:23,360
is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a

144
00:11:23,360 --> 00:11:28,720
fixed point. So the only thing it can approach is itself, and since it's one member in this big

145
00:11:28,720 --> 00:11:33,520
family of distributions, all of which must be tending towards a single universal shape, it must

146
00:11:33,520 --> 00:11:38,720
be that universal shape. I mentioned at the start how this calculation, step two, is something that

147
00:11:38,720 --> 00:11:43,760
you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed

148
00:11:43,760 --> 00:11:48,880
by a geometric argument that leverages the rotational symmetry of this graph is that it

149
00:11:48,880 --> 00:11:53,280
directly connects to a few things that we've talked about on this channel before. For example,

150
00:11:53,280 --> 00:11:58,240
the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this

151
00:11:58,240 --> 00:12:03,600
rotational symmetry as the defining feature of the distribution, that it locks you into this

152
00:12:03,600 --> 00:12:08,880
e to the negative x squared form, and also as an added bonus it connects to the classic proof for

153
00:12:08,880 --> 00:12:13,920
why pi shows up in the formula, meaning we now have a direct line between the presence and mystery

154
00:12:13,920 --> 00:12:19,520
of that pi and the central limit theorem. Also on a recent Patreon post, the channel supporter

155
00:12:19,520 --> 00:12:23,840
Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before,

156
00:12:23,840 --> 00:12:28,480
which leverages the use of entropy, and again for the theoretically curious among you I'll leave

157
00:12:28,480 --> 00:12:33,920
some links in the description. By the way, if you want to stay up to date with new videos and also

158
00:12:33,920 --> 00:12:38,560
any other projects that I put out there like the Summer of Math Exposition, there is a mailing list.

159
00:12:38,560 --> 00:12:43,120
It's relatively new and I'm pretty sparing about only posting what I think people will enjoy.

160
00:12:43,120 --> 00:12:47,680
Usually I try not to be too promotional at the end of videos these days, but if you are interested in

161
00:12:47,680 --> 00:12:52,400
following the work that I do, this is probably one of the most enduring ways to do so.

