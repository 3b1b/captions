[
 {
  "input": "The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.",
  "translatedText": "Normal dağılımın altında yatan temel fonksiyon, diğer adıyla Gauss, e üzeri negatif x karedir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.12
 },
 {
  "input": "But you might wonder, why this function?",
  "translatedText": "Ama merak edebilirsiniz, neden bu fonksiyon?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.64,
  "end": 8.34
 },
 {
  "input": "Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle, why is it that the theory of probability seems to have a special place in its heart for this particular expression?",
  "translatedText": "Kütlenin ortaya doğru yoğunlaştığı simetrik düzgün bir grafik veren hayal edebildiğimiz tüm ifadeler arasında, olasılık teorisinin neden bu özel ifade için kalbinde özel bir yeri var gibi görünüyor?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.72,
  "end": 20.44
 },
 {
  "input": "For the last many videos I've been hinting at an answer to this question, and here we'll finally arrive at something like a satisfying answer.",
  "translatedText": "Son birçok videoda bu sorunun cevabını ima ediyordum ve burada nihayet tatmin edici bir cevaba ulaşacağız.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.38,
  "end": 27.68
 },
 {
  "input": "As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem, which describes how as you add multiple copies of a random variable, for example rolling a weighted die many different times or letting a ball bounce off of a peg repeatedly, then the distribution describing that sum tends to look approximately like a normal distribution.",
  "translatedText": "Nerede olduğumuza dair kısa bir hatırlatma olarak, birkaç video önce merkezi limit teoremi hakkında konuşmuştuk; bu teorem, bir rastgele değişkenin birden fazla kopyasını nasıl ekleyeceğinizi açıklar; örneğin ağırlıklı bir zarı birçok farklı kez atmak veya bir topun zıplamasına izin vermek gibi. Tekrar tekrar sabitlenirse, bu toplamı tanımlayan dağılım yaklaşık olarak normal bir dağılıma benzeme eğilimi gösterir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.68,
  "end": 47.72
 },
 {
  "input": "What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions, that approximation to a normal becomes better and better.",
  "translatedText": "Merkezi limit teoreminin söylediği şey, uygun koşullar altında bu toplamı büyüttükçe normale yaklaşmanın giderek daha iyi hale gelmesidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 48.44,
  "end": 56.22
 },
 {
  "input": "But I never explained why this theorem is actually true, we only talked about what it's claiming.",
  "translatedText": "Ama bu teoremin gerçekte neden doğru olduğunu hiçbir zaman açıklamadım, sadece iddia ettiği şey hakkında konuştuk.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.94,
  "end": 61.98
 },
 {
  "input": "In the last video we started talking about the math involved in adding two random variables.",
  "translatedText": "Son videoda iki rastgele değişkenin toplanmasıyla ilgili matematik hakkında konuşmaya başladık.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.08,
  "end": 67.88
 },
 {
  "input": "If you have two random variables, each following some distribution, then to find the distribution describing the sum of those variables, you compute something known as a convolution between the two original functions.",
  "translatedText": "Her biri bir dağılımı takip eden iki rastgele değişkeniniz varsa, bu değişkenlerin toplamını açıklayan dağılımı bulmak için, iki orijinal fonksiyon arasında evrişim olarak bilinen bir şeyi hesaplarsınız.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 68.26,
  "end": 79.7
 },
 {
  "input": "And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.",
  "translatedText": "Ve bu evrişim işleminin gerçekte ne olduğunu görselleştirmek için iki farklı yol oluşturmak için çok zaman harcadık.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.88,
  "end": 85.94
 },
 {
  "input": "Today our basic job is to work through a particular example, which is to ask what happens when you add two normally distributed random variables, which as you know by now is the same as asking what do you get if you compute a convolution between two Gaussian functions.",
  "translatedText": "Bugün temel işimiz belirli bir örnek üzerinde çalışmaktır; bu, normal dağılıma sahip iki rastgele değişkeni topladığınızda ne olacağını sormaktır; bu, şu ana kadar bildiğiniz gibi, iki Gaussian arasında bir evrişimi hesaplarsanız ne elde edeceğinizi sormakla aynıdır. işlevler.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.94,
  "end": 101.78
 },
 {
  "input": "I'd like to share an especially pleasing visual way that you can think about this calculation, which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.",
  "translatedText": "Bu hesaplama hakkında düşünebileceğiniz özellikle hoş bir görsel yolu paylaşmak istiyorum; bu, ilk etapta e üzeri negatif x kare fonksiyonunu özel kılan şeyin ne olduğuna dair bir fikir verebilir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.52,
  "end": 112.36
 },
 {
  "input": "After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.",
  "translatedText": "Üzerinden geçtikten sonra, bu hesaplamanın merkezi limit teoremini kanıtlamanın adımlarından biri olduğundan bahsedeceğiz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.36,
  "end": 118.24
 },
 {
  "input": "It's the step that answers the question of why a Gaussian and not something else is the central limit.",
  "translatedText": "Bu, neden merkezi sınırın başka bir şey değil de Gauss olduğu sorusunu yanıtlayan adımdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.32,
  "end": 123.56
 },
 {
  "input": "But first, let's dive in.",
  "translatedText": "Ama önce konuya girelim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.2,
  "end": 125.84
 },
 {
  "input": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
  "translatedText": "Gaussian'ın tam formülü e üzeri eksi x kareden daha karmaşıktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.78,
  "end": 134.44
 },
 {
  "input": "The exponent is typically written as negative one half times x divided by sigma squared, where sigma describes the spread of the distribution, specifically the standard deviation.",
  "translatedText": "Üs genellikle negatif yarım çarpı x bölü sigma kare olarak yazılır; burada sigma, dağılımın yayılmasını, özellikle standart sapmayı tanımlar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.82,
  "end": 144.2
 },
 {
  "input": "All of this needs to be multiplied by a fraction on the front, which is there to make sure that the area under the curve is one, making it a valid probability distribution.",
  "translatedText": "Tüm bunların, eğrinin altındaki alanın bir olduğundan emin olmak için ön taraftaki bir kesirle çarpılması gerekir, bu da onu geçerli bir olasılık dağılımı yapar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.68,
  "end": 153.42
 },
 {
  "input": "And if you want to consider distributions that aren't necessarily centered at zero, you would also throw another parameter, mu, into the exponent like this.",
  "translatedText": "Ve eğer sıfır merkezli olmayan dağılımları dikkate almak istiyorsanız, bunun gibi üsse başka bir parametre olan mu'yu da atarsınız.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 154.02,
  "end": 161.18
 },
 {
  "input": "Although for everything we'll be doing here, we just consider centered distributions.",
  "translatedText": "Burada yapacağımız her şey için sadece merkezli dağıtımları düşünüyoruz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.54,
  "end": 165.12
 },
 {
  "input": "Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions, the direct way to do this would be to take the definition of a convolution, this integral expression we built up last video, and then to plug in for each one of the functions involved the formula for a Gaussian.",
  "translatedText": "Şimdi, bugünkü ana hedefimize bakarsanız, yani iki Gauss fonksiyonu arasındaki evrişimi hesaplamak, bunu yapmanın doğrudan yolu evrişimin tanımını, yani geçen videoda oluşturduğumuz bu integral ifadesini almak ve sonra bunu yapmak olacaktır. Gauss formülünü içeren işlevlerin her biri için eklentiyi kullanın.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.8,
  "end": 183.76
 },
 {
  "input": "It's kind of a lot of symbols when you throw it all together, but more than anything, working this out is an exercise in completing the square.",
  "translatedText": "Hepsini bir araya getirdiğinizde çok fazla sembol ortaya çıkıyor, ama her şeyden önemlisi bunu çözmek bir kareyi tamamlama egzersizidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.22,
  "end": 190.08
 },
 {
  "input": "And there's nothing wrong with that.",
  "translatedText": "Ve bunda yanlış bir şey yok.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.56,
  "end": 191.58
 },
 {
  "input": "That will get you the answer that you want.",
  "translatedText": "Bu size istediğiniz cevabı verecektir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.72,
  "end": 193.22
 },
 {
  "input": "But of course, you know me, I'm a sucker for visual intuition, and in this case, there's another way to think about it that I haven't seen written about before, that offers a very nice connection to other aspects of this distribution, like the presence of pi and certain ways to derive where it comes from.",
  "translatedText": "Ama tabii ki, beni bilirsiniz, görsel sezgi konusunda enayiyim ve bu durumda, bunun hakkında daha önce yazılı olarak görmediğim, konunun diğer yönleriyle çok güzel bir bağlantı sunan başka bir düşünme yolu daha var. dağıtım, pi'nin varlığı ve onun nereden geldiğini belirlemenin belirli yolları gibi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.76,
  "end": 207.86
 },
 {
  "input": "And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution, and just showing the computation for the simplified form, e to the negative x squared.",
  "translatedText": "Ve bunu yapmak istediğim yol, öncelikle gerçek dağılımla ilgili tüm sabitleri çıkarıp, basitleştirilmiş form için hesaplamayı göstermek: e üzeri negatif x kare.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 208.2,
  "end": 217.96
 },
 {
  "input": "The essence of what we want to compute is what the convolution between two copies of this function looks like.",
  "translatedText": "Hesaplamak istediğimiz şeyin özü, bu fonksiyonun iki kopyası arasındaki evrişimin nasıl göründüğüdür.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.96,
  "end": 224.08
 },
 {
  "input": "If you'll remember, in the last video we had two different ways to visualize convolutions, and the one we'll be using here is the second one, involving diagonal slices.",
  "translatedText": "Hatırlarsanız, son videoda evrişimleri görselleştirmenin iki farklı yolu vardı ve burada kullanacağımız yöntem çapraz dilimler içeren ikinci yöntem olacak.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.46,
  "end": 232.92
 },
 {
  "input": "And as a quick reminder of the way that worked, if you have two different distributions that are described by two different functions, f and g, then every possible pair of values that you might get when you sample from these two distributions can be thought of as individual points on the xy-plane.",
  "translatedText": "İşleyiş şeklini kısaca hatırlatmak gerekirse, f ve g gibi iki farklı fonksiyonla tanımlanan iki farklı dağılımınız varsa, bu iki dağılımdan örnek aldığınızda elde edebileceğiniz her olası değer çifti düşünülebilir. xy düzleminde bireysel noktalar olarak.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 233.28,
  "end": 249.56
 },
 {
  "input": "And the probability density of landing on one such point, assuming independence, looks like f of x times g of y.",
  "translatedText": "Ve böyle bir noktaya iniş yapmanın olasılık yoğunluğu, bağımsız olduğu varsayıldığında, f x çarpı g y şeklinde görünür.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.36,
  "end": 257.52
 },
 {
  "input": "So what we do is we look at a graph of that expression as a two-variable function of x and y, which is a way of showing the distribution of all possible outcomes when we sample from the two different variables.",
  "translatedText": "Yani yaptığımız şey, bu ifadenin grafiğine x ve y'nin iki değişkenli fonksiyonu olarak bakmaktır; bu, iki farklı değişkenden örnek aldığımızda olası tüm sonuçların dağılımını göstermenin bir yoludur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.0,
  "end": 269.62
 },
 {
  "input": "To interpret the convolution of f and g evaluated on some input s, which is a way of saying how likely are you to get a pair of samples that adds up to this sum s, what you do is you look at a slice of this graph over the line x plus y equals s, and you consider the area under that slice.",
  "translatedText": "Bazı girdiler üzerinde değerlendirilen f ve g'nin evrişimini yorumlamak için, ki bu, bu toplamı s'ye ekleyen bir örnek çifti alma olasılığınızın ne kadar olduğunu söylemenin bir yoludur, yaptığınız şey, bu grafiğin bir dilimine bakmaktır. x artı y çizgisinin üzerinde s eşittir ve bu dilimin altındaki alanı dikkate alırsınız.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.56,
  "end": 289.3
 },
 {
  "input": "This area is almost, but not quite, the value of the convolution at s.",
  "translatedText": "Bu alan tam olmasa da hemen hemen s'deki evrişimin değeridir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.1,
  "end": 296.32
 },
 {
  "input": "For a mildly technical reason, you need to divide by the square root of two.",
  "translatedText": "Biraz teknik bir nedenden ötürü, ikinin kareköküne bölmeniz gerekir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.8,
  "end": 300.16
 },
 {
  "input": "Still, this area is the key feature to focus on.",
  "translatedText": "Yine de bu alan odaklanılması gereken temel özelliktir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.84,
  "end": 303.44
 },
 {
  "input": "You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum.",
  "translatedText": "Bunu, belirli bir toplama karşılık gelen tüm sonuçlar için tüm olasılık yoğunluklarını bir araya getirmenin bir yolu olarak düşünebilirsiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.44,
  "end": 311.04
 },
 {
  "input": "In the specific case where these two functions look like e to the negative x squared and e to the negative y squared, the resulting 3D graph has a really nice property that you can exploit.",
  "translatedText": "Bu iki fonksiyonun e üzeri negatif x kare ve e üzeri negatif y kare gibi göründüğü özel durumda, ortaya çıkan 3 boyutlu grafiğin yararlanabileceğiniz gerçekten güzel bir özelliği vardır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.3,
  "end": 323.5
 },
 {
  "input": "It's rotationally symmetric.",
  "translatedText": "Dönme açısından simetriktir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.72,
  "end": 325.68
 },
 {
  "input": "You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared, and this term describes the square of the distance between any point on the xy plane and the origin.",
  "translatedText": "Bunu terimleri birleştirerek ve bunun tamamen x kare artı y karenin bir fonksiyonu olduğunu fark ederek görebilirsiniz ve bu terim, xy düzlemindeki herhangi bir nokta ile başlangıç noktası arasındaki mesafenin karesini tanımlar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.88,
  "end": 338.46
 },
 {
  "input": "So in other words, the expression is purely a function of the distance from the origin.",
  "translatedText": "Yani başka bir deyişle ifade tamamen orijine olan uzaklığın bir fonksiyonudur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 339.2,
  "end": 343.16
 },
 {
  "input": "And by the way, this would not be true for any other distribution.",
  "translatedText": "Bu arada, bu başka hiçbir dağıtım için geçerli olmayacaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.56,
  "end": 347.92
 },
 {
  "input": "It's a property that uniquely characterizes bell curves.",
  "translatedText": "Bu, çan eğrilerini benzersiz şekilde karakterize eden bir özelliktir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 351.28
 },
 {
  "input": "So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about, and honestly calculating the area would just amount to computing the original integral that defines a convolution in the first place.",
  "translatedText": "Yani diğer birçok fonksiyon çifti için bu köşegen dilimler, düşünülmesi zor olan karmaşık bir şekil olacaktır ve alanı dürüstçe hesaplamak, ilk etapta bir evrişimi tanımlayan orijinal integrali hesaplamak anlamına gelecektir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.16,
  "end": 365.54
 },
 {
  "input": "So in most cases, the visual intuition doesn't really buy you anything.",
  "translatedText": "Yani çoğu durumda görsel sezgi size aslında hiçbir şey satın almaz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.94,
  "end": 369.36
 },
 {
  "input": "But in the case of bell curves, you can leverage that rotational symmetry.",
  "translatedText": "Ancak çan eğrileri durumunda bu dönme simetrisinden yararlanabilirsiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.36,
  "end": 373.92
 },
 {
  "input": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
  "translatedText": "Burada, s'nin bir değeri için x artı y eşittir s çizgisi üzerindeki bu dilimlerden birine odaklanın.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.8,
  "end": 380.48
 },
 {
  "input": "And remember, the convolution that we're trying to compute is a function of s.",
  "translatedText": "Ve unutmayın, hesaplamaya çalıştığımız evrişim s'nin bir fonksiyonudur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.3,
  "end": 385.84
 },
 {
  "input": "The thing that you want is an expression of s that tells you the area under this slice.",
  "translatedText": "İstediğiniz şey, bu dilimin altındaki alanı söyleyen bir s ifadesidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.84,
  "end": 391.1
 },
 {
  "input": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s.",
  "translatedText": "Peki, eğer bu doğruya bakarsanız, x eksenini s sıfırda ve y eksenini sıfır s'de kesiyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.7,
  "end": 397.9
 },
 {
  "input": "And a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two.",
  "translatedText": "Ve biraz Pisagor size orijinden bu doğruya olan düz çizginin mesafesinin s'nin ikinin kareköküne bölümü olduğunu gösterecek.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.32,
  "end": 405.32
 },
 {
  "input": "Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees, where you'd find something parallel to the y-axis the same distance away from the origin.",
  "translatedText": "Şimdi, simetri nedeniyle bu dilim, 45 derece dönen dilimle aynı; burada y eksenine paralel, başlangıç noktasından aynı uzaklıkta bir şey bulacaksınız.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 416.36
 },
 {
  "input": "The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions, because it only involves taking an integral with respect to y.",
  "translatedText": "Önemli olan, y eksenine paralel bir dilimin diğer alanını hesaplamanın diğer yönlerdeki dilimlerden çok çok daha kolay olmasıdır, çünkü bu yalnızca y'ye göre bir integral almayı içerir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.64,
  "end": 428.26
 },
 {
  "input": "The value of x on this slice is a constant.",
  "translatedText": "Bu dilimdeki x'in değeri bir sabittir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.74,
  "end": 431.44
 },
 {
  "input": "Specifically, it would be the constant s divided by the square root of two.",
  "translatedText": "Spesifik olarak, s sabitinin ikinin kareköküne bölümü olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 434.76
 },
 {
  "input": "So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out.",
  "translatedText": "İntegrali hesaplarken, bu alanı bulurken, buradaki terimin tamamı sanki sadece bir sayıymış gibi davranır ve bunu çarpanlara ayırabilirsiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.76,
  "end": 443.38
 },
 {
  "input": "This is the important point.",
  "translatedText": "Önemli olan nokta burası.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.88,
  "end": 444.94
 },
 {
  "input": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
  "translatedText": "S'yi içeren her şey artık entegre değişkenden tamamen ayrı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.28,
  "end": 450.2
 },
 {
  "input": "This remaining integral is a little bit tricky.",
  "translatedText": "Geriye kalan bu integral biraz çetrefilli.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.82,
  "end": 453.0
 },
 {
  "input": "I did a whole video on it, it's actually quite famous.",
  "translatedText": "Bununla ilgili bir video hazırladım, aslında oldukça ünlü.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.08,
  "end": 455.2
 },
 {
  "input": "But you almost don't really care.",
  "translatedText": "Ama neredeyse hiç umursamıyorsun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.5,
  "end": 456.9
 },
 {
  "input": "The point is that it's just some number.",
  "translatedText": "Mesele şu ki, bu sadece bir sayı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.24,
  "end": 459.0
 },
 {
  "input": "That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s.",
  "translatedText": "Bu sayı pi'nin kareköküdür, ama asıl önemli olan onun s'ye bağımlı olmayan bir şey olmasıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.0,
  "end": 465.48
 },
 {
  "input": "And essentially, this is our answer.",
  "translatedText": "Ve aslında cevabımız bu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.88,
  "end": 468.48
 },
 {
  "input": "We were looking for an expression for the area of these slices as a function of s, and now we have it.",
  "translatedText": "Bu dilimlerin alanı için s'nin bir fonksiyonu olarak bir ifade arıyorduk ve şimdi bulduk.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.78,
  "end": 474.28
 },
 {
  "input": "It looks like e to the negative s squared divided by two, scaled by some constant.",
  "translatedText": "Bir sabitle ölçeklendirilmiş, e üzeri negatif s kare bölü ikiye benziyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.38,
  "end": 478.84
 },
 {
  "input": "In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent.",
  "translatedText": "Başka bir deyişle, bu aynı zamanda bir çan eğrisi, başka bir Gaussian, üstelik bu ikiden dolayı biraz uzamış.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.3,
  "end": 485.62
 },
 {
  "input": "As I said earlier, the convolution evaluated at s is not quite this area.",
  "translatedText": "Daha önce de söylediğim gibi s'de değerlendirilen evrişim tam olarak bu alan değildir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.62,
  "end": 490.86
 },
 {
  "input": "Technically, it's this area divided by the square root of two.",
  "translatedText": "Teknik olarak bu alanın ikinin kareköküne bölümüdür.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.34,
  "end": 494.16
 },
 {
  "input": "We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant.",
  "translatedText": "Geçen videoda bundan bahsetmiştik ama bunun pek önemi yok çünkü bu sadece sabit hale geliyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.8,
  "end": 499.24
 },
 {
  "input": "What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian.",
  "translatedText": "Gerçekten önemli olan, iki Gausslu arasındaki evrişimin kendisinin başka bir Gausslu olduğu sonucuna varılmasıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.68,
  "end": 505.68
 },
 {
  "input": "If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma, essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front, and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma.",
  "translatedText": "Geriye dönüp ortalama sıfır ve keyfi bir standart sapma sigması ile normal dağılım için tüm sabitleri yeniden girerseniz, esasen aynı akıl yürütme, üstte ve dışarıda görünen iki faktörün aynı kareköküne yol açacaktır, ve bu tür iki normal dağılım arasındaki evrişimin, standart sapması karekökü iki çarpı sigma olan başka bir normal dağılım olduğu sonucuna varır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.56,
  "end": 530.38
 },
 {
  "input": "If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result.",
  "translatedText": "Daha önce çok fazla evrişim hesaplamadıysanız bunun çok özel bir sonuç olduğunu vurgulamakta fayda var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.98,
  "end": 536.06
 },
 {
  "input": "Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process.",
  "translatedText": "Neredeyse her zaman tamamen farklı türde bir işlevle karşılaşırsınız, ancak burada süreçte bir tür istikrar vardır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.38,
  "end": 542.5
 },
 {
  "input": "Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations.",
  "translatedText": "Ayrıca egzersiz yapmaktan hoşlananlar için, iki farklı standart sapma durumunu nasıl halledebileceğinizi göstermek için ekrana bir tane bırakacağım.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.26,
  "end": 549.44
 },
 {
  "input": "Still, some of you might be raising your hands and saying, what's the big deal?",
  "translatedText": "Yine de bazılarınız ellerini kaldırıp şöyle diyebilir: Bu kadar önemli olan ne?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.42,
  "end": 553.94
 },
 {
  "input": "I mean, when you first heard the question, what do you get when you add two normally distributed random variables, you probably even guessed that the answer should be another normally distributed variable.",
  "translatedText": "Demek istediğim, normal dağılıma sahip iki rastgele değişkeni topladığınızda ne elde edersiniz sorusunu ilk duyduğunuzda, muhtemelen cevabın normal dağılım gösteren başka bir değişken olması gerektiğini bile tahmin etmişsinizdir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.48,
  "end": 564.32
 },
 {
  "input": "After all, what else is it going to be?",
  "translatedText": "Sonuçta başka ne olacak?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.76,
  "end": 566.36
 },
 {
  "input": "Normal distributions are supposedly quite common, so why not?",
  "translatedText": "Normal dağılımların oldukça yaygın olduğu varsayılıyor, öyleyse neden olmasın?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.86,
  "end": 570.24
 },
 {
  "input": "You could even say that this should follow from the central limit theorem, but that would have it all backwards.",
  "translatedText": "Hatta bunun merkezi limit teoreminden çıkması gerektiğini bile söyleyebilirsiniz, ancak bu her şeyi tersten ifade eder.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 570.24,
  "end": 575.48
 },
 {
  "input": "First of all, the supposed ubiquity of normal distributions is often a little exaggerated, but to the extent that they do come up, it is because of the central limit theorem, but it would be cheating to say the central limit theorem implies this result because this computation we just did is the reason that the function at the heart of the central limit theorem is a Gaussian in the first place and not some other function.",
  "translatedText": "Her şeyden önce, normal dağılımların varsayılan her yerde bulunması çoğu zaman biraz abartılır, ancak ortaya çıktıkları ölçüde bunun nedeni merkezi limit teoremidir, ancak merkezi limit teoreminin bu sonucu ima ettiğini söylemek hile olur çünkü Az önce yaptığımız bu hesaplama, merkezi limit teoreminin merkezindeki fonksiyonun her şeyden önce bir Gaussian olmasının ve başka bir fonksiyon olmamasının nedenidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.18,
  "end": 597.06
 },
 {
  "input": "We've talked all about the central limit theorem before, but essentially it says if you repeatedly add copies of a random variable to itself, which mathematically looks like repeatedly computing convolutions against a given distribution, then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution.",
  "translatedText": "Merkezi limit teoremi hakkında daha önce çok konuştuk, ancak aslında bu, bir rastgele değişkenin kopyalarını kendisine tekrar tekrar eklerseniz, ki bu matematiksel olarak belirli bir dağılıma karşı tekrar tekrar evrişimleri hesaplamaya benziyor, o zaman uygun kaydırma ve yeniden ölçeklendirmeden sonra eğilimin şöyle olduğunu söylüyor: her zaman normal dağılıma yaklaşmak.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.06,
  "end": 616.5
 },
 {
  "input": "Technically there's a small assumption the distribution you start with can't have infinite variance, but it's a relatively soft assumption.",
  "translatedText": "Teknik olarak, başladığınız dağıtımın sonsuz varyansa sahip olamayacağı küçük bir varsayım var, ancak bu nispeten yumuşak bir varsayımdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 616.98,
  "end": 623.22
 },
 {
  "input": "The magic is that for a huge category of initial distributions, this process of adding a whole bunch of random variables drawn from that distribution always tends towards this one universal shape, a Gaussian.",
  "translatedText": "İşin büyüsü, çok büyük bir başlangıç dağılım kategorisi için, bu dağılımdan alınan bir grup rastgele değişkenin eklenmesi sürecinin her zaman bu tek evrensel şekle, bir Gaussian'a doğru yönelmesidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.22,
  "end": 635.1
 },
 {
  "input": "One common approach to proving this theorem involves two separate steps.",
  "translatedText": "Bu teoremi kanıtlamaya yönelik yaygın bir yaklaşım iki ayrı adımı içerir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.82,
  "end": 639.3
 },
 {
  "input": "The first step is to show that for all the different finite variance distributions you might start with, there exists a single universal shape that this process of repeated convolutions tends towards.",
  "translatedText": "İlk adım, başlayabileceğiniz tüm farklı sonlu varyans dağılımları için, bu tekrarlanan evrişim sürecinin yöneldiği tek bir evrensel şeklin var olduğunu göstermektir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.6,
  "end": 650.0
 },
 {
  "input": "This step is actually pretty technical, it goes a little beyond what I want to talk about here.",
  "translatedText": "Bu adım aslında oldukça teknik, burada konuşmak istediklerimin biraz ötesine geçiyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.0,
  "end": 654.24
 },
 {
  "input": "You often use these objects called moment generating functions that gives you a very abstract argument that there must be some universal shape, but it doesn't make any claim about what that particular shape is, just that everything in this big family is tending towards a single point in the space of distributions.",
  "translatedText": "Moment üreten işlevler olarak adlandırılan bu nesneleri sıklıkla kullanırsınız ve bu size evrensel bir şeklin olması gerektiği konusunda çok soyut bir argüman sunar, ancak bu belirli şeklin ne olduğu hakkında herhangi bir iddiada bulunmaz, sadece bu büyük ailedeki her şey bir şeye doğru yönelmektedir. dağılım uzayında tek nokta.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.52,
  "end": 669.98
 },
 {
  "input": "So then step number two is what we just showed in this video, prove that the convolution of two Gaussians gives another Gaussian.",
  "translatedText": "O halde ikinci adım, bu videoda gösterdiğimiz şeydir; iki Gauss'un evrişiminin başka bir Gauss'u verdiğini kanıtlayın.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 670.62,
  "end": 677.4
 },
 {
  "input": "What that means is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a fixed point.",
  "translatedText": "Bunun anlamı şu; bu tekrarlanan evrişim sürecini uyguladığınızda Gaussian değişmez, sabit bir noktadır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.4,
  "end": 684.06
 },
 {
  "input": "So the only thing it can approach is itself, and since it's one member in this big family of distributions, all of which must be tending towards a single universal shape, it must be that universal shape.",
  "translatedText": "Dolayısıyla yaklaşabileceği tek şey kendisidir ve bu büyük dağılım ailesinin bir üyesi olduğundan, bunların hepsi tek bir evrensel şekle doğru yönelmelidir, o da bu evrensel şekil olmalıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 684.2,
  "end": 695.06
 },
 {
  "input": "I mentioned at the start how this calculation, step two, is something that you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed by a geometric argument that leverages the rotational symmetry of this graph is that it directly connects to a few things that we've talked about on this channel before.",
  "translatedText": "Başlangıçta bu hesaplamanın, ikinci adımın, tanımlarla sadece sembolik olarak doğrudan yapabileceğiniz bir şey olduğundan bahsetmiştim, ancak bu grafiğin dönme simetrisinden yararlanan geometrik bir argümandan bu kadar etkilenmemin nedenlerinden biri de şu: daha önce bu kanalda konuştuğumuz birkaç şeyle doğrudan bağlantılıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.58,
  "end": 712.3
 },
 {
  "input": "For example, the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this rotational symmetry as the defining feature of the distribution, that it locks you into this e to the negative x squared form, and also as an added bonus it connects to the classic proof for why pi shows up in the formula, meaning we now have a direct line between the presence and mystery of that pi and the central limit theorem.",
  "translatedText": "Örneğin, bir Gaussian'ın Herschel-Maxwell türevi, bu dönme simetrisini dağılımın tanımlayıcı özelliği olarak görebileceğinizi, sizi bu e üzeri negatif x kare formuna kilitlediğini ve aynı zamanda ek bir bonus olduğunu söylüyor. Pi'nin formülde neden göründüğüne dair klasik kanıtla bağlantılıdır; bu da artık Pi'nin varlığı ve gizemi ile merkezi limit teoremi arasında doğrudan bir çizgiye sahip olduğumuz anlamına gelir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 712.4,
  "end": 736.5
 },
 {
  "input": "Also on a recent Patreon post, the channel supporter Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before, which leverages the use of entropy, and again for the theoretically curious among you I'll leave some links in the description.",
  "translatedText": "Ayrıca yakın tarihli bir Patreon gönderisinde kanal destekçisi Daksha Vaid-Quinter, daha önce görmediğim, entropi kullanımından yararlanan tamamen farklı bir yaklaşıma dikkatimi çekti ve yine aranızdaki teorik meraklılar için bazı bağlantılar bırakacağım. açıklamada.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 737.06,
  "end": 749.58
 },
 {
  "input": "By the way, if you want to stay up to date with new videos and also any other projects that I put out there like the Summer of Math Exposition, there is a mailing list.",
  "translatedText": "Bu arada, yeni videolardan ve ayrıca Matematik Yazı Sergisi gibi orada ortaya koyduğum diğer projelerden haberdar olmak istiyorsanız bir e-posta listesi var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 750.96,
  "end": 758.4
 },
 {
  "input": "It's relatively new and I'm pretty sparing about only posting what I think people will enjoy.",
  "translatedText": "Nispeten yeni ve yalnızca insanların keyif alacağını düşündüğüm şeyleri yayınlama konusunda oldukça tutumlu davranıyorum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 758.72,
  "end": 762.78
 },
 {
  "input": "Usually I try not to be too promotional at the end of videos these days, but if you are interested in following the work that I do, this is probably one of the most enduring ways to do so.",
  "translatedText": "Bu günlerde genellikle videoların sonunda fazla tanıtım yapmamaya çalışıyorum ancak yaptığım işi takip etmekle ilgileniyorsanız, bu muhtemelen bunu yapmanın en kalıcı yollarından biridir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 763.22,
  "end": 795.26
 }
]