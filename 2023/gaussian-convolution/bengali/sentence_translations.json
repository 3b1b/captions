[
 {
  "input": "The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.",
  "translatedText": "একটি সাধারণ বন্টনের অন্তর্নিহিত মৌলিক ফাংশন, ওরফে একটি গাউসিয়ান, হল e থেকে ঋণাত্মক x বর্গক্ষেত্র।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.12
 },
 {
  "input": "But you might wonder, why this function?",
  "translatedText": "কিন্তু আপনি ভাবতে পারেন, কেন এই ফাংশন?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.64,
  "end": 8.34
 },
 {
  "input": "Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle, why is it that the theory of probability seems to have a special place in its heart for this particular expression?",
  "translatedText": "সমস্ত অভিব্যক্তির মধ্যে আমরা স্বপ্ন দেখতে পারি যা আপনাকে কিছু প্রতিসম মসৃণ গ্রাফ দেয় যার ভর মাঝখানে কেন্দ্রীভূত হয়, কেন এই বিশেষ অভিব্যক্তিটির জন্য সম্ভাব্যতার তত্ত্বটি তার হৃদয়ে একটি বিশেষ স্থান রয়েছে বলে মনে হয়?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.72,
  "end": 20.44
 },
 {
  "input": "For the last many videos I've been hinting at an answer to this question, and here we'll finally arrive at something like a satisfying answer.",
  "translatedText": "গত অনেক ভিডিওর জন্য আমি এই প্রশ্নের উত্তরের ইঙ্গিত দিয়েছি, এবং এখানে আমরা অবশেষে একটি সন্তোষজনক উত্তরের মতো কিছুতে পৌঁছাব।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.38,
  "end": 27.68
 },
 {
  "input": "As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem, which describes how as you add multiple copies of a random variable, for example rolling a weighted die many different times or letting a ball bounce off of a peg repeatedly, then the distribution describing that sum tends to look approximately like a normal distribution.",
  "translatedText": "আমরা কোথায় আছি তার দ্রুত রিফ্রেসার হিসাবে, কয়েকটা ভিডিও আগে আমরা কেন্দ্রীয় সীমা উপপাদ্য সম্পর্কে কথা বলেছিলাম, যা বর্ণনা করে যে আপনি কীভাবে একটি র্যান্ডম ভেরিয়েবলের একাধিক অনুলিপি যুক্ত করেন, উদাহরণস্বরূপ একটি ওজনযুক্ত ডাইকে বিভিন্ন বার ঘুরিয়ে দেওয়া বা একটি বলকে বাউন্স করতে দেওয়া। একটি পেগ বারবার, তারপর সেই যোগফল বর্ণনাকারী বিতরণটি প্রায় একটি সাধারণ বিতরণের মতো দেখায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.68,
  "end": 47.72
 },
 {
  "input": "What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions, that approximation to a normal becomes better and better.",
  "translatedText": "কেন্দ্রীয় সীমা উপপাদ্য যা বলে তা হল আপনি যখন সেই যোগফলকে বড় এবং বড় করবেন, উপযুক্ত পরিস্থিতিতে, একটি স্বাভাবিকের অনুমান আরও ভাল এবং আরও ভাল হয়ে উঠবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 48.44,
  "end": 56.22
 },
 {
  "input": "But I never explained why this theorem is actually true, we only talked about what it's claiming.",
  "translatedText": "কিন্তু আমি কখনই ব্যাখ্যা করিনি কেন এই উপপাদ্যটি প্রকৃতপক্ষে সত্য, আমরা কেবল এটি কী দাবি করছে তা নিয়ে কথা বলেছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.94,
  "end": 61.98
 },
 {
  "input": "In the last video we started talking about the math involved in adding two random variables.",
  "translatedText": "শেষ ভিডিওতে আমরা দুটি র্যান্ডম ভেরিয়েবল যোগ করার সাথে জড়িত গণিত সম্পর্কে কথা বলা শুরু করেছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.08,
  "end": 67.88
 },
 {
  "input": "If you have two random variables, each following some distribution, then to find the distribution describing the sum of those variables, you compute something known as a convolution between the two original functions.",
  "translatedText": "আপনার যদি দুটি র্যান্ডম ভেরিয়েবল থাকে, প্রত্যেকটি কিছু ডিস্ট্রিবিউশন অনুসরণ করে, তাহলে সেই ভেরিয়েবলের যোগফল বর্ণনা করে ডিস্ট্রিবিউশন খুঁজে বের করতে, আপনি দুটি মূল ফাংশনের মধ্যে কনভল্যুশন হিসাবে পরিচিত কিছু গণনা করুন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 68.26,
  "end": 79.7
 },
 {
  "input": "And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.",
  "translatedText": "এবং এই কনভোল্যুশন অপারেশনটি আসলে কী তা কল্পনা করার জন্য আমরা দুটি স্বতন্ত্র উপায় তৈরি করতে অনেক সময় ব্যয় করেছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.88,
  "end": 85.94
 },
 {
  "input": "Today our basic job is to work through a particular example, which is to ask what happens when you add two normally distributed random variables, which as you know by now is the same as asking what do you get if you compute a convolution between two Gaussian functions.",
  "translatedText": "আজকে আমাদের মৌলিক কাজ হল একটি নির্দিষ্ট উদাহরণের মাধ্যমে কাজ করা, যা হল জিজ্ঞাসা করা যখন আপনি দুটি সাধারনভাবে বিতরণ করা র্যান্ডম ভেরিয়েবল যোগ করেন, যা আপনি এতক্ষণে জানেন যে আপনি দুটি গাউসিয়ানের মধ্যে একটি কনভোলেশন গণনা করলে আপনি কী পাবেন তা জিজ্ঞাসা করার মতই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.94,
  "end": 101.78
 },
 {
  "input": "I'd like to share an especially pleasing visual way that you can think about this calculation, which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.",
  "translatedText": "ফাংশন আমি একটি বিশেষভাবে আনন্দদায়ক ভিজ্যুয়াল উপায় শেয়ার করতে চাই যা আপনি এই গণনাটি সম্পর্কে চিন্তা করতে পারেন, যা আশাকরি প্রথম স্থানে ই-তে নেতিবাচক x স্কোয়ার ফাংশনটিকে বিশেষ করে তোলে তার কিছুটা ধারণা দেয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.52,
  "end": 112.36
 },
 {
  "input": "After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.",
  "translatedText": "আমরা এটির মধ্য দিয়ে হেঁটে যাওয়ার পরে, আমরা আলোচনা করব কীভাবে এই গণনাটি কেন্দ্রীয় সীমা উপপাদ্য প্রমাণ করার জন্য জড়িত পদক্ষেপগুলির মধ্যে একটি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.36,
  "end": 118.24
 },
 {
  "input": "It's the step that answers the question of why a Gaussian and not something else is the central limit.",
  "translatedText": "এটি এমন একটি পদক্ষেপ যা কেন একটি গাউসিয়ান এবং অন্য কিছু নয় এই প্রশ্নের উত্তর দেয় কেন্দ্রীয় সীমা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.32,
  "end": 123.56
 },
 {
  "input": "But first, let's dive in.",
  "translatedText": "তবে প্রথমে, এর মধ্যে ডুব দেওয়া যাক।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.2,
  "end": 125.84
 },
 {
  "input": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
  "translatedText": "একটি গাউসিয়ানের জন্য সম্পূর্ণ সূত্রটি নেতিবাচক x বর্গক্ষেত্রের তুলনায় e থেকে আরও জটিল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.78,
  "end": 134.44
 },
 {
  "input": "The exponent is typically written as negative one half times x divided by sigma squared, where sigma describes the spread of the distribution, specifically the standard deviation.",
  "translatedText": "সূচকটিকে সাধারণত ঋণাত্মক হিসাবে লেখা হয় x কে সিগমা বর্গ দ্বারা ভাগ করে, যেখানে সিগমা বন্টনের বিস্তারকে বর্ণনা করে, বিশেষত আদর্শ বিচ্যুতি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.82,
  "end": 144.2
 },
 {
  "input": "All of this needs to be multiplied by a fraction on the front, which is there to make sure that the area under the curve is one, making it a valid probability distribution.",
  "translatedText": "এই সমস্তগুলিকে সামনের অংশে একটি ভগ্নাংশ দ্বারা গুণ করা দরকার, যেটি নিশ্চিত করার জন্য বক্ররেখার নীচের ক্ষেত্রটি এক, এটি একটি বৈধ সম্ভাব্যতা বন্টন করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.68,
  "end": 153.42
 },
 {
  "input": "And if you want to consider distributions that aren't necessarily centered at zero, you would also throw another parameter, mu, into the exponent like this.",
  "translatedText": "এবং যদি আপনি বন্টনগুলি বিবেচনা করতে চান যেগুলি অগত্যা শূন্য কেন্দ্রিক নয়, আপনি অন্য একটি প্যারামিটার, mu, এইভাবে সূচকে নিক্ষেপ করবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 154.02,
  "end": 161.18
 },
 {
  "input": "Although for everything we'll be doing here, we just consider centered distributions.",
  "translatedText": "যদিও সবকিছুর জন্য আমরা এখানে করব, আমরা শুধু কেন্দ্রীভূত বিতরণ বিবেচনা করি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.54,
  "end": 165.12
 },
 {
  "input": "Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions, the direct way to do this would be to take the definition of a convolution, this integral expression we built up last video, and then to plug in for each one of the functions involved the formula for a Gaussian.",
  "translatedText": "এখন আপনি যদি আজকের জন্য আমাদের কেন্দ্রীয় লক্ষ্যটি দেখেন, যা হল দুটি গাউসিয়ান ফাংশনের মধ্যে একটি কনভোল্যুশন গণনা করা, এটি করার সরাসরি উপায় হল একটি কনভোল্যুশনের সংজ্ঞা নেওয়া, এই অবিচ্ছেদ্য অভিব্যক্তিটি আমরা শেষ ভিডিও তৈরি করেছি, এবং তারপরে প্রতিটি ফাংশনের জন্য প্লাগ ইন একটি গাউসিয়ান জন্য সূত্র জড়িত.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.8,
  "end": 183.76
 },
 {
  "input": "It's kind of a lot of symbols when you throw it all together, but more than anything, working this out is an exercise in completing the square.",
  "translatedText": "আপনি যখন এটিকে একসাথে নিক্ষেপ করেন তখন এটি অনেকগুলি প্রতীকের মতো, তবে যে কোনও কিছুর চেয়েও বেশি, এটি কাজ করে বর্গটি সম্পূর্ণ করার একটি অনুশীলন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.22,
  "end": 190.08
 },
 {
  "input": "And there's nothing wrong with that.",
  "translatedText": "আর এতে দোষের কিছু নেই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.56,
  "end": 191.58
 },
 {
  "input": "That will get you the answer that you want.",
  "translatedText": "এটি আপনি যে উত্তর চান তা পাবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.72,
  "end": 193.22
 },
 {
  "input": "But of course, you know me, I'm a sucker for visual intuition, and in this case, there's another way to think about it that I haven't seen written about before, that offers a very nice connection to other aspects of this distribution, like the presence of pi and certain ways to derive where it comes from.",
  "translatedText": "তবে অবশ্যই, আপনি আমাকে জানেন, আমি চাক্ষুষ অন্তর্দৃষ্টির জন্য একজন চোষা, এবং এই ক্ষেত্রে, এটি সম্পর্কে চিন্তা করার আরেকটি উপায় আছে যা আমি আগে লিখতে দেখিনি, যা এর অন্যান্য দিকগুলির সাথে খুব সুন্দর সংযোগ প্রদান করে ডিস্ট্রিবিউশন, যেমন পাই এর উপস্থিতি এবং এটি কোথা থেকে আসে তা বের করার নির্দিষ্ট উপায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.76,
  "end": 207.86
 },
 {
  "input": "And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution, and just showing the computation for the simplified form, e to the negative x squared.",
  "translatedText": "এবং আমি যেভাবে এটি করতে চাই তা হল প্রকৃত বন্টনের সাথে যুক্ত সমস্ত ধ্রুবকগুলিকে প্রথমে খোসা ছাড়ানো এবং শুধু সরলীকৃত ফর্মের জন্য গণনা দেখানো, ই ঋণাত্মক x বর্গক্ষেত্রে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 208.2,
  "end": 217.96
 },
 {
  "input": "The essence of what we want to compute is what the convolution between two copies of this function looks like.",
  "translatedText": "আমরা যা গণনা করতে চাই তার সারমর্ম হল এই ফাংশনের দুটি কপির মধ্যে কনভল্যুশন কেমন দেখায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.96,
  "end": 224.08
 },
 {
  "input": "If you'll remember, in the last video we had two different ways to visualize convolutions, and the one we'll be using here is the second one, involving diagonal slices.",
  "translatedText": "আপনার যদি মনে থাকে, শেষ ভিডিওতে আমাদের কনভল্যুশন কল্পনা করার দুটি ভিন্ন উপায় ছিল, এবং আমরা এখানে যেটি ব্যবহার করব তা হল দ্বিতীয়টি, তির্যক স্লাইস যুক্ত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.46,
  "end": 232.92
 },
 {
  "input": "And as a quick reminder of the way that worked, if you have two different distributions that are described by two different functions, f and g, then every possible pair of values that you might get when you sample from these two distributions can be thought of as individual points on the xy-plane.",
  "translatedText": "এবং যেভাবে কাজ করেছে তার একটি দ্রুত অনুস্মারক হিসাবে, যদি আপনার কাছে দুটি ভিন্ন ডিস্ট্রিবিউশন থাকে যা দুটি ভিন্ন ফাংশন দ্বারা বর্ণনা করা হয়, f এবং g, তাহলে প্রতিটি সম্ভাব্য জোড়া মান যা আপনি পেতে পারেন যখন আপনি এই দুটি বিতরণ থেকে নমুনা পেতে পারেন। xy-প্লেনে পৃথক পয়েন্ট হিসাবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 233.28,
  "end": 249.56
 },
 {
  "input": "And the probability density of landing on one such point, assuming independence, looks like f of x times g of y.",
  "translatedText": "এবং এমন একটি বিন্দুতে অবতরণ করার সম্ভাবনার ঘনত্ব, স্বাধীনতা ধরে নিলে, y এর x বার g এর f এর মতো দেখায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.36,
  "end": 257.52
 },
 {
  "input": "So what we do is we look at a graph of that expression as a two-variable function of x and y, which is a way of showing the distribution of all possible outcomes when we sample from the two different variables.",
  "translatedText": "তাই আমরা যা করি তা হল আমরা x এবং y এর দুটি-ভেরিয়েবল ফাংশন হিসাবে সেই এক্সপ্রেশনের একটি গ্রাফ দেখি, যা দুটি ভিন্ন ভেরিয়েবল থেকে নমুনা নেওয়ার সময় সম্ভাব্য সমস্ত ফলাফলের বিতরণ দেখানোর একটি উপায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.0,
  "end": 269.62
 },
 {
  "input": "To interpret the convolution of f and g evaluated on some input s, which is a way of saying how likely are you to get a pair of samples that adds up to this sum s, what you do is you look at a slice of this graph over the line x plus y equals s, and you consider the area under that slice.",
  "translatedText": "কিছু ইনপুট s-এ মূল্যায়ন করা f এবং g-এর আবর্তন ব্যাখ্যা করার জন্য, যা বলার একটি উপায় যে আপনি এই যোগফল s যোগ করে এমন এক জোড়া নমুনা পাওয়ার সম্ভাবনা কতটা, আপনি এই গ্রাফের একটি স্লাইস দেখেন লাইনের উপরে x প্লাস y সমান s, এবং আপনি সেই স্লাইসের নীচে ক্ষেত্রফল বিবেচনা করুন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.56,
  "end": 289.3
 },
 {
  "input": "This area is almost, but not quite, the value of the convolution at s.",
  "translatedText": "এই ক্ষেত্রটি প্রায়, কিন্তু পুরোপুরি নয়, s এ কনভ্যুলেশনের মান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.1,
  "end": 296.32
 },
 {
  "input": "For a mildly technical reason, you need to divide by the square root of two.",
  "translatedText": "একটি হালকা প্রযুক্তিগত কারণে, আপনাকে দুইটির বর্গমূল দ্বারা ভাগ করতে হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.8,
  "end": 300.16
 },
 {
  "input": "Still, this area is the key feature to focus on.",
  "translatedText": "তবুও, এই ক্ষেত্রটিতে ফোকাস করার মূল বৈশিষ্ট্য।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.84,
  "end": 303.44
 },
 {
  "input": "You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum.",
  "translatedText": "আপনি এটিকে একটি প্রদত্ত যোগফলের সাথে সম্পর্কিত সমস্ত ফলাফলের জন্য সমস্ত সম্ভাব্যতা ঘনত্বকে একত্রিত করার উপায় হিসাবে ভাবতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.44,
  "end": 311.04
 },
 {
  "input": "In the specific case where these two functions look like e to the negative x squared and e to the negative y squared, the resulting 3D graph has a really nice property that you can exploit.",
  "translatedText": "নির্দিষ্ট ক্ষেত্রে যেখানে এই দুটি ফাংশন নেতিবাচক x বর্গক্ষেত্র এবং e থেকে ঋণাত্মক y বর্গক্ষেত্রের মতো দেখায়, ফলে 3D গ্রাফে একটি সত্যিই চমৎকার সম্পত্তি রয়েছে যা আপনি কাজে লাগাতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.3,
  "end": 323.5
 },
 {
  "input": "It's rotationally symmetric.",
  "translatedText": "এটি ঘূর্ণায়মানভাবে প্রতিসম।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.72,
  "end": 325.68
 },
 {
  "input": "You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared, and this term describes the square of the distance between any point on the xy plane and the origin.",
  "translatedText": "আপনি পদগুলিকে একত্রিত করে দেখতে পারেন যে এটি সম্পূর্ণভাবে x বর্গ প্লাস y বর্গক্ষেত্রের একটি ফাংশন, এবং এই শব্দটি xy সমতলে যে কোনো বিন্দু এবং উৎপত্তির মধ্যে দূরত্বের বর্গকে বর্ণনা করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.88,
  "end": 338.46
 },
 {
  "input": "So in other words, the expression is purely a function of the distance from the origin.",
  "translatedText": "সুতরাং অন্য কথায়, অভিব্যক্তিটি সম্পূর্ণরূপে উৎপত্তি থেকে দূরত্বের একটি ফাংশন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 339.2,
  "end": 343.16
 },
 {
  "input": "And by the way, this would not be true for any other distribution.",
  "translatedText": "এবং যাইহোক, এটি অন্য কোনো বিতরণের জন্য সত্য হবে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.56,
  "end": 347.92
 },
 {
  "input": "It's a property that uniquely characterizes bell curves.",
  "translatedText": "এটি এমন একটি সম্পত্তি যা বেল কার্ভকে অনন্যভাবে চিহ্নিত করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 351.28
 },
 {
  "input": "So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about, and honestly calculating the area would just amount to computing the original integral that defines a convolution in the first place.",
  "translatedText": "সুতরাং বেশিরভাগ অন্যান্য জোড়া ফাংশনের জন্য, এই তির্যক স্লাইসগুলি এমন কিছু জটিল আকৃতি হবে যা সম্পর্কে চিন্তা করা কঠিন, এবং সততার সাথে ক্ষেত্রফল গণনা করা কেবলমাত্র মূল অবিচ্ছেদ্য কম্পিউটিংয়ের পরিমাণ হবে যা প্রথম স্থানে একটি কনভল্যুশনকে সংজ্ঞায়িত করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.16,
  "end": 365.54
 },
 {
  "input": "So in most cases, the visual intuition doesn't really buy you anything.",
  "translatedText": "তাই বেশিরভাগ ক্ষেত্রে, চাক্ষুষ অন্তর্দৃষ্টি সত্যিই আপনাকে কিছু কিনতে পারে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.94,
  "end": 369.36
 },
 {
  "input": "But in the case of bell curves, you can leverage that rotational symmetry.",
  "translatedText": "কিন্তু বেল বক্ররেখার ক্ষেত্রে, আপনি সেই ঘূর্ণনশীল প্রতিসাম্যটি লাভ করতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.36,
  "end": 373.92
 },
 {
  "input": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
  "translatedText": "এখানে, s এর কিছু মানের জন্য x প্লাস y সমান s লাইনের উপর এই স্লাইসগুলির একটিতে ফোকাস করুন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.8,
  "end": 380.48
 },
 {
  "input": "And remember, the convolution that we're trying to compute is a function of s.",
  "translatedText": "এবং মনে রাখবেন, আমরা কম্পিউট করার চেষ্টা করছি যে convolution s একটি ফাংশন.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.3,
  "end": 385.84
 },
 {
  "input": "The thing that you want is an expression of s that tells you the area under this slice.",
  "translatedText": "আপনি যে জিনিসটি চান তা হল s-এর একটি অভিব্যক্তি যা আপনাকে এই স্লাইসের অধীনে এলাকাটি বলে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.84,
  "end": 391.1
 },
 {
  "input": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s.",
  "translatedText": "ঠিক আছে, যদি আপনি সেই রেখাটি দেখেন, এটি x-অক্ষকে s শূন্যে এবং y-অক্ষকে শূন্য s-এ ছেদ করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.7,
  "end": 397.9
 },
 {
  "input": "And a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two.",
  "translatedText": "এবং পিথাগোরাসের সামান্য কিছু আপনাকে দেখাবে যে উৎপত্তি থেকে এই রেখার সরলরেখার দূরত্ব s দুইটির বর্গমূল দ্বারা ভাগ করা হয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.32,
  "end": 405.32
 },
 {
  "input": "Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees, where you'd find something parallel to the y-axis the same distance away from the origin.",
  "translatedText": "এখন, প্রতিসাম্যের কারণে, এই স্লাইসটি একটির সাথে অভিন্ন যা আপনি 45 ডিগ্রি ঘোরান, যেখানে আপনি উৎপত্তি থেকে একই দূরত্বে y-অক্ষের সমান্তরাল কিছু খুঁজে পাবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 416.36
 },
 {
  "input": "The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions, because it only involves taking an integral with respect to y.",
  "translatedText": "মূল বিষয় হল যে y-অক্ষের সমান্তরাল একটি স্লাইসের এই অন্য ক্ষেত্রটিকে গণনা করা অন্যান্য দিকের স্লাইসের তুলনায় অনেক বেশি, অনেক সহজ, কারণ এতে শুধুমাত্র y-এর ক্ষেত্রে একটি অখণ্ডতা নেওয়া জড়িত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.64,
  "end": 428.26
 },
 {
  "input": "The value of x on this slice is a constant.",
  "translatedText": "এই স্লাইসে x এর মান একটি ধ্রুবক।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.74,
  "end": 431.44
 },
 {
  "input": "Specifically, it would be the constant s divided by the square root of two.",
  "translatedText": "বিশেষত, এটা হবে ধ্রুবক s কে দুইটির বর্গমূল দ্বারা ভাগ করা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 434.76
 },
 {
  "input": "So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out.",
  "translatedText": "তাই যখন আপনি ইন্টিগ্রাল কম্পিউট করছেন, এই ক্ষেত্রটি খুঁজে বের করছেন, এই শব্দটি এখানে এমন আচরণ করে যেন এটি কিছু সংখ্যা ছিল এবং আপনি এটিকে ফ্যাক্টর করতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.76,
  "end": 443.38
 },
 {
  "input": "This is the important point.",
  "translatedText": "এই গুরুত্বপূর্ণ পয়েন্ট.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.88,
  "end": 444.94
 },
 {
  "input": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
  "translatedText": "s জড়িত স্টাফ সব এখন সম্পূর্ণরূপে ইন্টিগ্রেটেড পরিবর্তনশীল থেকে পৃথক.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.28,
  "end": 450.2
 },
 {
  "input": "This remaining integral is a little bit tricky.",
  "translatedText": "এই অবশিষ্ট অবিচ্ছেদ্য একটু বিট চতুর.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.82,
  "end": 453.0
 },
 {
  "input": "I did a whole video on it, it's actually quite famous.",
  "translatedText": "আমি এটিতে একটি সম্পূর্ণ ভিডিও করেছি, এটি আসলে বেশ বিখ্যাত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.08,
  "end": 455.2
 },
 {
  "input": "But you almost don't really care.",
  "translatedText": "কিন্তু আপনি প্রায় সত্যিই যত্ন না.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.5,
  "end": 456.9
 },
 {
  "input": "The point is that it's just some number.",
  "translatedText": "বিন্দু হল যে এটা শুধু কিছু সংখ্যা.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.24,
  "end": 459.0
 },
 {
  "input": "That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s.",
  "translatedText": "এই সংখ্যাটি পাই এর বর্গমূল হতে পারে, কিন্তু আসলেই যেটা গুরুত্বপূর্ণ তা হল এটি এমন কিছু যা s-এর উপর নির্ভর করে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.0,
  "end": 465.48
 },
 {
  "input": "And essentially, this is our answer.",
  "translatedText": "এবং মূলত, এই আমাদের উত্তর.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.88,
  "end": 468.48
 },
 {
  "input": "We were looking for an expression for the area of these slices as a function of s, and now we have it.",
  "translatedText": "আমরা s এর ফাংশন হিসাবে এই স্লাইসগুলির ক্ষেত্রফলের জন্য একটি অভিব্যক্তি খুঁজছিলাম এবং এখন আমাদের কাছে এটি রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.78,
  "end": 474.28
 },
 {
  "input": "It looks like e to the negative s squared divided by two, scaled by some constant.",
  "translatedText": "মনে হচ্ছে e থেকে ঋণাত্মক s বর্গকে দুই দ্বারা বিভক্ত, কিছু ধ্রুবক দ্বারা স্কেল করা হয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.38,
  "end": 478.84
 },
 {
  "input": "In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent.",
  "translatedText": "অন্য কথায়, এটিও একটি বেল কার্ভ, আরেকটি গাউসিয়ান, সূচকে এই দুটির কারণে কিছুটা প্রসারিত হয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.3,
  "end": 485.62
 },
 {
  "input": "As I said earlier, the convolution evaluated at s is not quite this area.",
  "translatedText": "আমি আগেই বলেছি, s-এ মূল্যায়ন করা কনভোলিউশনটি পুরোপুরি এই এলাকা নয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.62,
  "end": 490.86
 },
 {
  "input": "Technically, it's this area divided by the square root of two.",
  "translatedText": "টেকনিক্যালি, এই এলাকাটি দুইটির বর্গমূল দ্বারা বিভক্ত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.34,
  "end": 494.16
 },
 {
  "input": "We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant.",
  "translatedText": "আমরা শেষ ভিডিওতে এটি সম্পর্কে কথা বলেছি, তবে এটি আসলেই কোন ব্যাপার না কারণ এটি কেবল ধ্রুবকটিতে বেক হয়ে যায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.8,
  "end": 499.24
 },
 {
  "input": "What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian.",
  "translatedText": "যেটা আসলেই গুরুত্বপূর্ণ তা হল এই উপসংহার যে দুই গাউসিয়ানদের মধ্যে একটা দ্বন্দ্ব নিজেই আরেকটা গাউসিয়ান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.68,
  "end": 505.68
 },
 {
  "input": "If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma, essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front, and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma.",
  "translatedText": "আপনি যদি ফিরে যান এবং একটি গড় শূন্য এবং একটি নির্বিচারে স্ট্যান্ডার্ড বিচ্যুতি সিগমা সহ একটি স্বাভাবিক বন্টনের জন্য সমস্ত ধ্রুবকগুলিকে পুনরায় প্রবর্তন করতে চান, মূলত অভিন্ন যুক্তি দুটি ফ্যাক্টরের একই বর্গমূলের দিকে নিয়ে যাবে যা সূচকে এবং সামনের দিকে দেখা যায়, এবং এটি এই উপসংহারে নিয়ে যায় যে এই ধরনের দুটি স্বাভাবিক বন্টনের মধ্যে আবর্তন হল আরেকটি স্বাভাবিক বন্টন যার একটি আদর্শ বিচ্যুতি বর্গমূল দুই গুণ সিগমা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.56,
  "end": 530.38
 },
 {
  "input": "If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result.",
  "translatedText": "আপনি যদি আগে অনেক কনভোলিউশন গণনা না করে থাকেন তবে এটি একটি বিশেষ ফলাফলের উপর জোর দেওয়া মূল্যবান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.98,
  "end": 536.06
 },
 {
  "input": "Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process.",
  "translatedText": "প্রায় সবসময় আপনি একটি সম্পূর্ণ ভিন্ন ধরনের ফাংশন দিয়ে শেষ করেন, কিন্তু এখানে প্রক্রিয়াটির এক ধরণের স্থিতিশীলতা রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.38,
  "end": 542.5
 },
 {
  "input": "Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations.",
  "translatedText": "এছাড়াও, আপনারা যারা ব্যায়াম উপভোগ করেন, তাদের জন্য আমি পর্দায় একটি রেখে দেব যে আপনি কীভাবে দুটি ভিন্ন স্ট্যান্ডার্ড বিচ্যুতির ক্ষেত্রে পরিচালনা করবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.26,
  "end": 549.44
 },
 {
  "input": "Still, some of you might be raising your hands and saying, what's the big deal?",
  "translatedText": "তারপরও, আপনারা কেউ কেউ হয়তো হাত তুলে বলছেন, এতে বড় কথা কী?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.42,
  "end": 553.94
 },
 {
  "input": "I mean, when you first heard the question, what do you get when you add two normally distributed random variables, you probably even guessed that the answer should be another normally distributed variable.",
  "translatedText": "আমি বলতে চাচ্ছি, আপনি যখন প্রথম প্রশ্নটি শুনেছিলেন, আপনি যখন দুটি সাধারণভাবে বিতরণ করা র্যান্ডম ভেরিয়েবল যোগ করেন তখন আপনি কী পান, আপনি সম্ভবত অনুমান করেছিলেন যে উত্তরটি অন্য একটি সাধারণভাবে বিতরণ করা পরিবর্তনশীল হওয়া উচিত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.48,
  "end": 564.32
 },
 {
  "input": "After all, what else is it going to be?",
  "translatedText": "সব পরে, এটা আর কি হতে যাচ্ছে?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.76,
  "end": 566.36
 },
 {
  "input": "Normal distributions are supposedly quite common, so why not?",
  "translatedText": "সাধারণ বিতরণ অনুমিতভাবে বেশ সাধারণ, তাই কেন নয়?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.86,
  "end": 570.24
 },
 {
  "input": "You could even say that this should follow from the central limit theorem, but that would have it all backwards.",
  "translatedText": "আপনি এমনকি বলতে পারেন যে এটি কেন্দ্রীয় সীমা উপপাদ্য থেকে অনুসরণ করা উচিত, তবে এটি সব পিছনের দিকে থাকবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 570.24,
  "end": 575.48
 },
 {
  "input": "First of all, the supposed ubiquity of normal distributions is often a little exaggerated, but to the extent that they do come up, it is because of the central limit theorem, but it would be cheating to say the central limit theorem implies this result because this computation we just did is the reason that the function at the heart of the central limit theorem is a Gaussian in the first place and not some other function.",
  "translatedText": "প্রথমত, স্বাভাবিক বন্টনের অনুমিত সর্বব্যাপীতা প্রায়শই একটু অতিরঞ্জিত হয়, কিন্তু সেগুলি যে পরিমাণে আসে, এটি কেন্দ্রীয় সীমা উপপাদ্যের কারণে, তবে কেন্দ্রীয় সীমা উপপাদ্যটি এই ফলাফলকে বোঝায় বলা প্রতারণা করা হবে কারণ এই গণনাটি আমরা এইমাত্র করেছি এই কারণে যে কেন্দ্রীয় সীমা উপপাদ্যের কেন্দ্রস্থলে ফাংশনটি প্রথমে একটি গাউসিয়ান এবং অন্য কোন ফাংশন নয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.18,
  "end": 597.06
 },
 {
  "input": "We've talked all about the central limit theorem before, but essentially it says if you repeatedly add copies of a random variable to itself, which mathematically looks like repeatedly computing convolutions against a given distribution, then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution.",
  "translatedText": "আমরা এর আগে কেন্দ্রীয় সীমা উপপাদ্য সম্পর্কে সমস্ত কথা বলেছি, তবে মূলত এটি বলে যে আপনি যদি বারবার নিজের সাথে একটি র্যান্ডম ভেরিয়েবলের অনুলিপি যুক্ত করেন, যা গাণিতিকভাবে একটি প্রদত্ত বন্টনের বিরুদ্ধে বারবার কম্পিউটিংয়ের মত দেখায়, তারপর উপযুক্ত স্থানান্তর এবং পুনঃস্কেল করার পরে, প্রবণতা হয় সর্বদা একটি স্বাভাবিক বিতরণের কাছে যেতে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.06,
  "end": 616.5
 },
 {
  "input": "Technically there's a small assumption the distribution you start with can't have infinite variance, but it's a relatively soft assumption.",
  "translatedText": "টেকনিক্যালি একটি ছোট অনুমান আছে যে ডিস্ট্রিবিউশন দিয়ে আপনি শুরু করেন তার অসীম বৈচিত্র্য থাকতে পারে না, তবে এটি তুলনামূলকভাবে নরম অনুমান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 616.98,
  "end": 623.22
 },
 {
  "input": "The magic is that for a huge category of initial distributions, this process of adding a whole bunch of random variables drawn from that distribution always tends towards this one universal shape, a Gaussian.",
  "translatedText": "ম্যাজিক হল যে প্রাথমিক বন্টনের একটি বিশাল শ্রেনীর জন্য, সেই বন্টন থেকে অঙ্কিত এলোমেলো ভেরিয়েবলের পুরো গুচ্ছ যোগ করার এই প্রক্রিয়াটি সর্বদা এই একটি সার্বজনীন আকৃতির দিকে ঝোঁক, একটি গাউসিয়ান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.22,
  "end": 635.1
 },
 {
  "input": "One common approach to proving this theorem involves two separate steps.",
  "translatedText": "এই উপপাদ্য প্রমাণ করার জন্য একটি সাধারণ পদ্ধতির দুটি পৃথক ধাপ জড়িত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.82,
  "end": 639.3
 },
 {
  "input": "The first step is to show that for all the different finite variance distributions you might start with, there exists a single universal shape that this process of repeated convolutions tends towards.",
  "translatedText": "প্রথম ধাপ হল দেখানো যে সমস্ত বিভিন্ন সীমিত বৈচিত্র্য বন্টনগুলির সাথে আপনি শুরু করতে পারেন, একটি একক সর্বজনীন আকৃতি রয়েছে যেটির দিকে বারবার কনভোলিউশনের এই প্রক্রিয়াটি ঝোঁক।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.6,
  "end": 650.0
 },
 {
  "input": "This step is actually pretty technical, it goes a little beyond what I want to talk about here.",
  "translatedText": "এই পদক্ষেপটি আসলে বেশ প্রযুক্তিগত, এটি আমি এখানে যা বলতে চাই তার একটু বাইরে চলে যায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.0,
  "end": 654.24
 },
 {
  "input": "You often use these objects called moment generating functions that gives you a very abstract argument that there must be some universal shape, but it doesn't make any claim about what that particular shape is, just that everything in this big family is tending towards a single point in the space of distributions.",
  "translatedText": "আপনি প্রায়শই এই বস্তুগুলি ব্যবহার করেন যার নাম মোমেন্ট জেনারেটিং ফাংশন যা আপনাকে একটি খুব বিমূর্ত যুক্তি দেয় যে কিছু সার্বজনীন আকৃতি থাকতে হবে, তবে এটি সেই নির্দিষ্ট আকৃতিটি কী তা নিয়ে কোনও দাবি করে না, শুধু এই যে এই বড় পরিবারের সবকিছুই একটির দিকে ঝুঁকছে। ডিস্ট্রিবিউশনের জায়গায় একক পয়েন্ট।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.52,
  "end": 669.98
 },
 {
  "input": "So then step number two is what we just showed in this video, prove that the convolution of two Gaussians gives another Gaussian.",
  "translatedText": "তাহলে দ্বিতীয় ধাপ হল যা আমরা এই ভিডিওতে দেখিয়েছি, প্রমাণ করুন যে দুটি গাউসিয়ানের কনভল্যুশন আরেকটি গাউসিয়ান দেয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 670.62,
  "end": 677.4
 },
 {
  "input": "What that means is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a fixed point.",
  "translatedText": "এর মানে হল যে আপনি বারবার কনভোলিউশনের এই প্রক্রিয়াটি প্রয়োগ করার সাথে সাথে একজন গাউসিয়ান পরিবর্তন হয় না, এটি একটি নির্দিষ্ট বিন্দু।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.4,
  "end": 684.06
 },
 {
  "input": "So the only thing it can approach is itself, and since it's one member in this big family of distributions, all of which must be tending towards a single universal shape, it must be that universal shape.",
  "translatedText": "সুতরাং এটির কাছে যাওয়ার একমাত্র জিনিসটি নিজেই, এবং যেহেতু এটি বিতরণের এই বৃহৎ পরিবারের একজন সদস্য, যার সবকটিই অবশ্যই একটি একক সর্বজনীন আকৃতির দিকে ঝুঁকতে হবে, এটি অবশ্যই সেই সর্বজনীন আকৃতি হতে হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 684.2,
  "end": 695.06
 },
 {
  "input": "I mentioned at the start how this calculation, step two, is something that you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed by a geometric argument that leverages the rotational symmetry of this graph is that it directly connects to a few things that we've talked about on this channel before.",
  "translatedText": "আমি শুরুতে উল্লেখ করেছি কিভাবে এই গণনাটি, দ্বিতীয় ধাপ, এমন কিছু যা আপনি সরাসরি করতে পারেন, শুধু প্রতীকীভাবে সংজ্ঞা দিয়ে, কিন্তু একটি জ্যামিতিক যুক্তি দ্বারা আমি এত মুগ্ধ হয়েছি যা এই গ্রাফের ঘূর্ণন প্রতিসাম্যকে লাভ করে তা হল এটি সরাসরি কিছু জিনিসের সাথে সংযোগ করে যা আমরা আগে এই চ্যানেলে কথা বলেছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.58,
  "end": 712.3
 },
 {
  "input": "For example, the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this rotational symmetry as the defining feature of the distribution, that it locks you into this e to the negative x squared form, and also as an added bonus it connects to the classic proof for why pi shows up in the formula, meaning we now have a direct line between the presence and mystery of that pi and the central limit theorem.",
  "translatedText": "উদাহরণস্বরূপ, একটি গাউসিয়ানের হার্শেল-ম্যাক্সওয়েল ডেরিভেশন, যা মূলত বলে যে আপনি এই ঘূর্ণন প্রতিসাম্যটিকে বিতরণের সংজ্ঞায়িত বৈশিষ্ট্য হিসাবে দেখতে পারেন, এটি আপনাকে এই ই-তে ঋণাত্মক x বর্গ আকারে আটকে রাখে এবং একটি অতিরিক্ত বোনাস হিসাবেও এটি সূত্রে পাই কেন প্রদর্শিত হয় তার ক্লাসিক প্রমাণের সাথে সংযোগ করে, যার অর্থ এখন আমাদের কাছে সেই পাই এবং কেন্দ্রীয় সীমা উপপাদ্যের উপস্থিতি এবং রহস্যের মধ্যে একটি সরাসরি লাইন রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 712.4,
  "end": 736.5
 },
 {
  "input": "Also on a recent Patreon post, the channel supporter Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before, which leverages the use of entropy, and again for the theoretically curious among you I'll leave some links in the description.",
  "translatedText": "এছাড়াও একটি সাম্প্রতিক প্যাট্রিয়ন পোস্টে, চ্যানেল সমর্থক দক্ষিণ বৈদ-কুইন্টার একটি সম্পূর্ণ ভিন্ন পদ্ধতির প্রতি আমার দৃষ্টি আকর্ষণ করেছেন যা আমি আগে দেখিনি, যা এনট্রপির ব্যবহারকে কাজে লাগায়, এবং আবার আপনাদের মধ্যে তাত্ত্বিকভাবে কৌতূহলীদের জন্য আমি কিছু লিঙ্ক রেখে যাচ্ছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 737.06,
  "end": 749.58
 },
 {
  "input": "By the way, if you want to stay up to date with new videos and also any other projects that I put out there like the Summer of Math Exposition, there is a mailing list.",
  "translatedText": "বর্ণনায় যাইহোক, আপনি যদি নতুন ভিডিওগুলির সাথে আপ টু ডেট থাকতে চান এবং অন্য কোন প্রোজেক্ট যা আমি সেখানে রেখেছি যেমন সামার অফ ম্যাথ এক্সপোজিশন, সেখানে একটি মেইলিং তালিকা রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 750.96,
  "end": 758.4
 },
 {
  "input": "It's relatively new and I'm pretty sparing about only posting what I think people will enjoy.",
  "translatedText": "এটি তুলনামূলকভাবে নতুন এবং আমি শুধুমাত্র পোস্ট করার বিষয়ে খুব কমই আছি যা আমি মনে করি লোকেরা উপভোগ করবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 758.72,
  "end": 762.78
 },
 {
  "input": "Usually I try not to be too promotional at the end of videos these days, but if you are interested in following the work that I do, this is probably one of the most enduring ways to do so.",
  "translatedText": "সাধারণত আমি আজকাল ভিডিওর শেষে খুব বেশি প্রচারণামূলক না হওয়ার চেষ্টা করি, তবে আপনি যদি আমার কাজটি অনুসরণ করতে আগ্রহী হন তবে এটি সম্ভবত এটি করার সবচেয়ে স্থায়ী উপায়গুলির মধ্যে একটি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 763.22,
  "end": 795.26
 }
]