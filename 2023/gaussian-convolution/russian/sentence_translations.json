[
 {
  "input": "The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.",
  "translatedText": "Основная функция, лежащая в основе нормального распределения, также известного как гауссово, равна e в отрицательном квадрате x.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.12
 },
 {
  "input": "But you might wonder, why this function?",
  "translatedText": "Но вы можете задаться вопросом: зачем эта функция?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.64,
  "end": 8.34
 },
 {
  "input": "Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle, why is it that the theory of probability seems to have a special place in its heart for this particular expression?",
  "translatedText": "Из всех выражений, которые мы могли придумать и которые дают некий симметричный гладкий график с массой, сконцентрированной ближе к середине, почему в теории вероятностей, кажется, особое место отведено этому конкретному выражению?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.72,
  "end": 20.44
 },
 {
  "input": "For the last many videos I've been hinting at an answer to this question, and here we'll finally arrive at something like a satisfying answer.",
  "translatedText": "В последних видео я намекал на ответ на этот вопрос, и здесь мы, наконец, пришли к чему-то вроде удовлетворительного ответа.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.38,
  "end": 27.68
 },
 {
  "input": "As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem, which describes how as you add multiple copies of a random variable, for example rolling a weighted die many different times or letting a ball bounce off of a peg repeatedly, then the distribution describing that sum tends to look approximately like a normal distribution.",
  "translatedText": "В качестве краткого напоминания о том, где мы находимся, пару видеороликов назад мы говорили о центральной предельной теореме, которая описывает, как вы добавляете несколько копий случайной величины, например, бросая взвешенную игральную кость много раз или позволяя мячу отскочить от привязку несколько раз, то распределение, описывающее эту сумму, имеет тенденцию выглядеть примерно как нормальное распределение.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.68,
  "end": 47.72
 },
 {
  "input": "What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions, that approximation to a normal becomes better and better.",
  "translatedText": "Центральная предельная теорема гласит, что по мере того, как вы увеличиваете эту сумму, при соответствующих условиях, приближение к нормальному становится все лучше и лучше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 48.44,
  "end": 56.22
 },
 {
  "input": "But I never explained why this theorem is actually true, we only talked about what it's claiming.",
  "translatedText": "Но я никогда не объяснял, почему эта теорема на самом деле верна, мы только говорили о том, что она утверждает.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.94,
  "end": 61.98
 },
 {
  "input": "In the last video we started talking about the math involved in adding two random variables.",
  "translatedText": "В последнем видео мы начали говорить о математике, связанной с сложением двух случайных величин.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.08,
  "end": 67.88
 },
 {
  "input": "If you have two random variables, each following some distribution, then to find the distribution describing the sum of those variables, you compute something known as a convolution between the two original functions.",
  "translatedText": "Если у вас есть две случайные величины, каждая из которых соответствует некоторому распределению, то, чтобы найти распределение, описывающее сумму этих переменных, вы вычисляете нечто, известное как свертка между двумя исходными функциями.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 68.26,
  "end": 79.7
 },
 {
  "input": "And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.",
  "translatedText": "И мы потратили много времени на создание двух различных способов визуализации того, что на самом деле представляет собой эта операция свертки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.88,
  "end": 85.94
 },
 {
  "input": "Today our basic job is to work through a particular example, which is to ask what happens when you add two normally distributed random variables, which as you know by now is the same as asking what do you get if you compute a convolution between two Gaussian functions.",
  "translatedText": "Сегодня наша основная задача — проработать конкретный пример, а именно задаться вопросом, что произойдет, если вы добавите две нормально распределенные случайные величины, что, как вы уже знаете, то же самое, что спросить, что вы получите, если вычислите свертку между двумя гауссовскими величинами. функции.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.94,
  "end": 101.78
 },
 {
  "input": "I'd like to share an especially pleasing visual way that you can think about this calculation, which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.",
  "translatedText": "Я хотел бы поделиться особенно приятным визуальным способом размышления об этом вычислении, который, надеюсь, дает некоторое представление о том, что делает функцию e в отрицательном квадрате x особенной.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.52,
  "end": 112.36
 },
 {
  "input": "After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.",
  "translatedText": "После того, как мы разберемся с этим, мы поговорим о том, что этот расчет является одним из шагов доказательства центральной предельной теоремы.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.36,
  "end": 118.24
 },
 {
  "input": "It's the step that answers the question of why a Gaussian and not something else is the central limit.",
  "translatedText": "Это шаг, который отвечает на вопрос, почему центральным пределом является гауссиан, а не что-то другое.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.32,
  "end": 123.56
 },
 {
  "input": "But first, let's dive in.",
  "translatedText": "Но сначала давайте углубимся.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.2,
  "end": 125.84
 },
 {
  "input": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
  "translatedText": "Полная формула гауссианы сложнее, чем просто е в квадрате отрицательного х.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.78,
  "end": 134.44
 },
 {
  "input": "The exponent is typically written as negative one half times x divided by sigma squared, where sigma describes the spread of the distribution, specifically the standard deviation.",
  "translatedText": "Показатель степени обычно записывается как отрицательное значение, равное половине x, деленной на квадрат сигмы, где сигма описывает разброс распределения, в частности стандартное отклонение.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.82,
  "end": 144.2
 },
 {
  "input": "All of this needs to be multiplied by a fraction on the front, which is there to make sure that the area under the curve is one, making it a valid probability distribution.",
  "translatedText": "Все это необходимо умножить на дробь спереди, чтобы убедиться, что площадь под кривой равна единице, что делает ее действительным распределением вероятностей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.68,
  "end": 153.42
 },
 {
  "input": "And if you want to consider distributions that aren't necessarily centered at zero, you would also throw another parameter, mu, into the exponent like this.",
  "translatedText": "И если вы хотите рассмотреть распределения, которые не обязательно центрированы в нуле, вы также должны добавить в экспоненту еще один параметр, mu, вот так.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 154.02,
  "end": 161.18
 },
 {
  "input": "Although for everything we'll be doing here, we just consider centered distributions.",
  "translatedText": "Хотя во всем, что мы будем здесь делать, мы рассматриваем только центрированные распределения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.54,
  "end": 165.12
 },
 {
  "input": "Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions, the direct way to do this would be to take the definition of a convolution, this integral expression we built up last video, and then to plug in for each one of the functions involved the formula for a Gaussian.",
  "translatedText": "Теперь, если вы посмотрите на нашу главную цель на сегодня, а именно вычислить свертку между двумя гауссовыми функциями, прямой способ сделать это — взять определение свертки, это интегральное выражение, которое мы построили в прошлом видео, а затем подключите для каждой из функций формулу гаусса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.8,
  "end": 183.76
 },
 {
  "input": "It's kind of a lot of symbols when you throw it all together, but more than anything, working this out is an exercise in completing the square.",
  "translatedText": "Когда вы складываете все это вместе, это своего рода множество символов, но, прежде всего, работа над этим — это упражнение по заполнению квадрата.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.22,
  "end": 190.08
 },
 {
  "input": "And there's nothing wrong with that.",
  "translatedText": "И в этом нет ничего плохого.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.56,
  "end": 191.58
 },
 {
  "input": "That will get you the answer that you want.",
  "translatedText": "Это даст вам ответ, который вы хотите.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.72,
  "end": 193.22
 },
 {
  "input": "But of course, you know me, I'm a sucker for visual intuition, and in this case, there's another way to think about it that I haven't seen written about before, that offers a very nice connection to other aspects of this distribution, like the presence of pi and certain ways to derive where it comes from.",
  "translatedText": "Но, конечно, вы меня знаете, я любитель визуальной интуиции, и в данном случае есть другой способ думать об этом, о котором я раньше не писал, который предлагает очень хорошую связь с другими аспектами этого процесса. распределение, такое как наличие числа Пи и определенные способы определения его происхождения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.76,
  "end": 207.86
 },
 {
  "input": "And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution, and just showing the computation for the simplified form, e to the negative x squared.",
  "translatedText": "И я хотел бы сделать это, сначала отбросив все константы, связанные с фактическим распределением, и просто показав вычисления для упрощенной формы, е в отрицательном квадрате х.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 208.2,
  "end": 217.96
 },
 {
  "input": "The essence of what we want to compute is what the convolution between two copies of this function looks like.",
  "translatedText": "Суть того, что мы хотим вычислить, заключается в том, как выглядит свертка между двумя копиями этой функции.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.96,
  "end": 224.08
 },
 {
  "input": "If you'll remember, in the last video we had two different ways to visualize convolutions, and the one we'll be using here is the second one, involving diagonal slices.",
  "translatedText": "Если вы помните, в прошлом видео у нас было два разных способа визуализации сверток, и здесь мы будем использовать второй, включающий диагональные срезы.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.46,
  "end": 232.92
 },
 {
  "input": "And as a quick reminder of the way that worked, if you have two different distributions that are described by two different functions, f and g, then every possible pair of values that you might get when you sample from these two distributions can be thought of as individual points on the xy-plane.",
  "translatedText": "И в качестве быстрого напоминания о том, как это работает: если у вас есть два разных распределения, которые описываются двумя разными функциями, f и g, то можно рассмотреть любую возможную пару значений, которые вы можете получить при выборке из этих двух распределений. как отдельные точки на плоскости xy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 233.28,
  "end": 249.56
 },
 {
  "input": "And the probability density of landing on one such point, assuming independence, looks like f of x times g of y.",
  "translatedText": "А плотность вероятности приземления в одной такой точке, при условии независимости, выглядит как f от x, умноженная на g от y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.36,
  "end": 257.52
 },
 {
  "input": "So what we do is we look at a graph of that expression as a two-variable function of x and y, which is a way of showing the distribution of all possible outcomes when we sample from the two different variables.",
  "translatedText": "Итак, что мы делаем, так это смотрим на график этого выражения как на функцию двух переменных от x и y, что является способом показать распределение всех возможных результатов при выборке из двух разных переменных.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.0,
  "end": 269.62
 },
 {
  "input": "To interpret the convolution of f and g evaluated on some input s, which is a way of saying how likely are you to get a pair of samples that adds up to this sum s, what you do is you look at a slice of this graph over the line x plus y equals s, and you consider the area under that slice.",
  "translatedText": "Чтобы интерпретировать свертку f и g, оцененную на некоторых входных s, что является способом сказать, насколько вероятно, что вы получите пару выборок, которая в сумме составит эту сумму s, что вам нужно сделать, это посмотреть на фрагмент этого графика над линией x плюс y равно s, и вы считаете площадь под этим срезом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.56,
  "end": 289.3
 },
 {
  "input": "This area is almost, but not quite, the value of the convolution at s.",
  "translatedText": "Эта площадь почти, но не совсем, равна значению свертки в точке s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.1,
  "end": 296.32
 },
 {
  "input": "For a mildly technical reason, you need to divide by the square root of two.",
  "translatedText": "По слегка технической причине вам нужно разделить на квадратный корень из двух.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.8,
  "end": 300.16
 },
 {
  "input": "Still, this area is the key feature to focus on.",
  "translatedText": "Тем не менее, эта область является ключевой особенностью, на которой следует сосредоточиться.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.84,
  "end": 303.44
 },
 {
  "input": "You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum.",
  "translatedText": "Вы можете думать об этом как о способе объединить все плотности вероятности для всех результатов, соответствующих данной сумме.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.44,
  "end": 311.04
 },
 {
  "input": "In the specific case where these two functions look like e to the negative x squared and e to the negative y squared, the resulting 3D graph has a really nice property that you can exploit.",
  "translatedText": "В конкретном случае, когда эти две функции выглядят как e при отрицательном квадрате x и e при отрицательном квадрате y, полученный трехмерный график обладает действительно хорошим свойством, которое можно использовать.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.3,
  "end": 323.5
 },
 {
  "input": "It's rotationally symmetric.",
  "translatedText": "Он вращательно-симметричный.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.72,
  "end": 325.68
 },
 {
  "input": "You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared, and this term describes the square of the distance between any point on the xy plane and the origin.",
  "translatedText": "Вы можете увидеть это, объединив термины и заметив, что это полностью функция x в квадрате плюс y в квадрате, и этот термин описывает квадрат расстояния между любой точкой на плоскости xy и началом координат.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.88,
  "end": 338.46
 },
 {
  "input": "So in other words, the expression is purely a function of the distance from the origin.",
  "translatedText": "Другими словами, выражение является чисто функцией расстояния от начала координат.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 339.2,
  "end": 343.16
 },
 {
  "input": "And by the way, this would not be true for any other distribution.",
  "translatedText": "И, кстати, это не относится ни к одному другому дистрибутиву.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.56,
  "end": 347.92
 },
 {
  "input": "It's a property that uniquely characterizes bell curves.",
  "translatedText": "Это свойство однозначно характеризует гауссовые кривые.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 351.28
 },
 {
  "input": "So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about, and honestly calculating the area would just amount to computing the original integral that defines a convolution in the first place.",
  "translatedText": "Таким образом, для большинства других пар функций эти диагональные срезы будут иметь сложную форму, о которой трудно подумать, и честное вычисление площади будет просто равнозначно вычислению исходного интеграла, который в первую очередь определяет свертку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.16,
  "end": 365.54
 },
 {
  "input": "So in most cases, the visual intuition doesn't really buy you anything.",
  "translatedText": "Так что в большинстве случаев зрительная интуиция вам ничего не даст.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.94,
  "end": 369.36
 },
 {
  "input": "But in the case of bell curves, you can leverage that rotational symmetry.",
  "translatedText": "Но в случае колоколообразных кривых вы можете использовать эту вращательную симметрию.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.36,
  "end": 373.92
 },
 {
  "input": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
  "translatedText": "Здесь сосредоточьтесь на одном из этих срезов на линии x плюс y, равной s для некоторого значения s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.8,
  "end": 380.48
 },
 {
  "input": "And remember, the convolution that we're trying to compute is a function of s.",
  "translatedText": "И помните, свертка, которую мы пытаемся вычислить, является функцией s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.3,
  "end": 385.84
 },
 {
  "input": "The thing that you want is an expression of s that tells you the area under this slice.",
  "translatedText": "Вам нужно выражение s, которое сообщает вам площадь под этим срезом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.84,
  "end": 391.1
 },
 {
  "input": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s.",
  "translatedText": "Что ж, если вы посмотрите на эту линию, она пересекает ось X в нулевой точке s и ось Y в нулевой точке s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.7,
  "end": 397.9
 },
 {
  "input": "And a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two.",
  "translatedText": "И немного Пифагора покажет вам, что расстояние по прямой от начала координат до этой линии делится на квадратный корень из двух.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.32,
  "end": 405.32
 },
 {
  "input": "Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees, where you'd find something parallel to the y-axis the same distance away from the origin.",
  "translatedText": "Теперь, из-за симметрии, этот срез идентичен тому, который вы получаете, повернув на 45 градусов, где вы обнаружите что-то параллельное оси Y на том же расстоянии от начала координат.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 416.36
 },
 {
  "input": "The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions, because it only involves taking an integral with respect to y.",
  "translatedText": "Ключ в том, что вычисление этой другой области среза, параллельного оси Y, намного проще, чем срезов в других направлениях, поскольку оно включает в себя только взятие интеграла по y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.64,
  "end": 428.26
 },
 {
  "input": "The value of x on this slice is a constant.",
  "translatedText": "Значение x в этом срезе является константой.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.74,
  "end": 431.44
 },
 {
  "input": "Specifically, it would be the constant s divided by the square root of two.",
  "translatedText": "В частности, это будет константа s, деленная на квадратный корень из двух.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 434.76
 },
 {
  "input": "So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out.",
  "translatedText": "Итак, когда вы вычисляете интеграл, находите эту площадь, все эти члены ведут себя так, как будто это просто какое-то число, и вы можете его вычислить.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.76,
  "end": 443.38
 },
 {
  "input": "This is the important point.",
  "translatedText": "Это важный момент.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.88,
  "end": 444.94
 },
 {
  "input": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
  "translatedText": "Все, что связано с s, теперь полностью отделено от интегрированной переменной.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.28,
  "end": 450.2
 },
 {
  "input": "This remaining integral is a little bit tricky.",
  "translatedText": "Этот оставшийся интеграл немного сложнее.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.82,
  "end": 453.0
 },
 {
  "input": "I did a whole video on it, it's actually quite famous.",
  "translatedText": "Я снял на эту тему целое видео, оно на самом деле довольно известное.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.08,
  "end": 455.2
 },
 {
  "input": "But you almost don't really care.",
  "translatedText": "Но тебя это почти не волнует.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.5,
  "end": 456.9
 },
 {
  "input": "The point is that it's just some number.",
  "translatedText": "Дело в том, что это всего лишь какое-то число.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.24,
  "end": 459.0
 },
 {
  "input": "That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s.",
  "translatedText": "Это число оказывается квадратным корнем из числа Пи, но что действительно важно, так это то, что оно не зависит от s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.0,
  "end": 465.48
 },
 {
  "input": "And essentially, this is our answer.",
  "translatedText": "И, по сути, это наш ответ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.88,
  "end": 468.48
 },
 {
  "input": "We were looking for an expression for the area of these slices as a function of s, and now we have it.",
  "translatedText": "Мы искали выражение для площади этих срезов как функцию s, и теперь оно у нас есть.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.78,
  "end": 474.28
 },
 {
  "input": "It looks like e to the negative s squared divided by two, scaled by some constant.",
  "translatedText": "Это выглядит как e в отрицательном квадрате, разделенное на два, масштабированное некоторой константой.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.38,
  "end": 478.84
 },
 {
  "input": "In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent.",
  "translatedText": "Другими словами, это тоже колоколообразная кривая, еще одна гауссова кривая, просто немного растянутая из-за этих двух в показателе степени.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.3,
  "end": 485.62
 },
 {
  "input": "As I said earlier, the convolution evaluated at s is not quite this area.",
  "translatedText": "Как я уже говорил ранее, свертка, оцененная в s, не совсем в этой области.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.62,
  "end": 490.86
 },
 {
  "input": "Technically, it's this area divided by the square root of two.",
  "translatedText": "Технически, это площадь, разделенная на квадратный корень из двух.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.34,
  "end": 494.16
 },
 {
  "input": "We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant.",
  "translatedText": "Мы говорили об этом в прошлом видео, но это не имеет особого значения, потому что это просто встроено в константу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.8,
  "end": 499.24
 },
 {
  "input": "What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian.",
  "translatedText": "Что действительно важно, так это вывод о том, что свертка между двумя гауссианами сама по себе является еще одним гауссианом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.68,
  "end": 505.68
 },
 {
  "input": "If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma, essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front, and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma.",
  "translatedText": "Если бы вы вернулись назад и заново ввели все константы для нормального распределения со средним нулем и произвольной сигмой стандартного отклонения, по существу идентичные рассуждения приведут к тому же квадратному корню из двух множителей, который проявляется в показателе степени и впереди, и это приводит к выводу, что свертка между двумя такими нормальными распределениями является еще одним нормальным распределением со стандартным квадратным корнем, равным двум сигмам.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.56,
  "end": 530.38
 },
 {
  "input": "If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result.",
  "translatedText": "Если вы раньше не вычисляли много сверток, стоит подчеркнуть, что это очень особенный результат.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.98,
  "end": 536.06
 },
 {
  "input": "Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process.",
  "translatedText": "Почти всегда вы получаете совершенно другую функцию, но здесь есть своего рода стабильность процесса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.38,
  "end": 542.5
 },
 {
  "input": "Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations.",
  "translatedText": "Кроме того, для тех из вас, кто любит упражнения, я оставлю на экране одно, как бы вы поступили в случае двух разных стандартных отклонений.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.26,
  "end": 549.44
 },
 {
  "input": "Still, some of you might be raising your hands and saying, what's the big deal?",
  "translatedText": "Тем не менее, некоторые из вас, возможно, поднимут руки и спросят: в чем дело?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.42,
  "end": 553.94
 },
 {
  "input": "I mean, when you first heard the question, what do you get when you add two normally distributed random variables, you probably even guessed that the answer should be another normally distributed variable.",
  "translatedText": "То есть, когда вы впервые услышали вопрос, что получится, если сложить две нормально распределенные случайные величины, вы, наверное, даже догадались, что ответом должна быть еще одна нормально распределенная переменная.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.48,
  "end": 564.32
 },
 {
  "input": "After all, what else is it going to be?",
  "translatedText": "В конце концов, что еще будет?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.76,
  "end": 566.36
 },
 {
  "input": "Normal distributions are supposedly quite common, so why not?",
  "translatedText": "Нормальные распределения предположительно довольно распространены, так почему бы и нет?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.86,
  "end": 570.24
 },
 {
  "input": "You could even say that this should follow from the central limit theorem, but that would have it all backwards.",
  "translatedText": "Можно даже сказать, что это должно следовать из центральной предельной теоремы, но тогда все будет наоборот.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 570.24,
  "end": 575.48
 },
 {
  "input": "First of all, the supposed ubiquity of normal distributions is often a little exaggerated, but to the extent that they do come up, it is because of the central limit theorem, but it would be cheating to say the central limit theorem implies this result because this computation we just did is the reason that the function at the heart of the central limit theorem is a Gaussian in the first place and not some other function.",
  "translatedText": "Прежде всего, предполагаемая повсеместность нормальных распределений часто немного преувеличена, но в той степени, в которой они действительно возникают, это происходит из-за центральной предельной теоремы, но было бы обманом сказать, что центральная предельная теорема подразумевает этот результат, потому что Вычисление, которое мы только что провели, является причиной того, что функция, лежащая в основе центральной предельной теоремы, в первую очередь является гауссовой, а не какой-либо другой функцией.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.18,
  "end": 597.06
 },
 {
  "input": "We've talked all about the central limit theorem before, but essentially it says if you repeatedly add copies of a random variable to itself, which mathematically looks like repeatedly computing convolutions against a given distribution, then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution.",
  "translatedText": "Мы уже говорили о центральной предельной теореме ранее, но по сути она говорит, что если вы неоднократно добавляете копии случайной величины к самой себе, что математически выглядит как многократное вычисление сверток для заданного распределения, то после соответствующего сдвига и изменения масштаба тенденция будет такой: всегда приближаться к нормальному распределению.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.06,
  "end": 616.5
 },
 {
  "input": "Technically there's a small assumption the distribution you start with can't have infinite variance, but it's a relatively soft assumption.",
  "translatedText": "Технически есть небольшое предположение, что распределение, с которого вы начинаете, не может иметь бесконечную дисперсию, но это относительно мягкое предположение.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 616.98,
  "end": 623.22
 },
 {
  "input": "The magic is that for a huge category of initial distributions, this process of adding a whole bunch of random variables drawn from that distribution always tends towards this one universal shape, a Gaussian.",
  "translatedText": "Волшебство заключается в том, что для огромной категории начальных распределений этот процесс добавления целого ряда случайных величин, взятых из этого распределения, всегда стремится к одной универсальной форме — гауссовой.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.22,
  "end": 635.1
 },
 {
  "input": "One common approach to proving this theorem involves two separate steps.",
  "translatedText": "Один из распространенных подходов к доказательству этой теоремы включает два отдельных шага.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.82,
  "end": 639.3
 },
 {
  "input": "The first step is to show that for all the different finite variance distributions you might start with, there exists a single universal shape that this process of repeated convolutions tends towards.",
  "translatedText": "Первый шаг — показать, что для всех различных распределений конечной дисперсии, с которых вы можете начать, существует единая универсальная форма, к которой стремится этот процесс повторяющихся сверток.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.6,
  "end": 650.0
 },
 {
  "input": "This step is actually pretty technical, it goes a little beyond what I want to talk about here.",
  "translatedText": "Этот шаг на самом деле довольно технический, он выходит за рамки того, о чем я хочу здесь поговорить.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.0,
  "end": 654.24
 },
 {
  "input": "You often use these objects called moment generating functions that gives you a very abstract argument that there must be some universal shape, but it doesn't make any claim about what that particular shape is, just that everything in this big family is tending towards a single point in the space of distributions.",
  "translatedText": "Вы часто используете эти объекты, называемые функциями, генерирующими моменты, которые дают вам очень абстрактный аргумент в пользу того, что должна быть какая-то универсальная форма, но при этом не делается никаких заявлений о том, что представляет собой эта конкретная форма, просто все в этом большом семействе стремится к одна точка в пространстве распределений.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.52,
  "end": 669.98
 },
 {
  "input": "So then step number two is what we just showed in this video, prove that the convolution of two Gaussians gives another Gaussian.",
  "translatedText": "Итак, шаг номер два — это то, что мы только что показали в этом видео: доказать, что свертка двух гауссиан дает еще один гауссиан.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 670.62,
  "end": 677.4
 },
 {
  "input": "What that means is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a fixed point.",
  "translatedText": "Это означает, что когда вы применяете этот процесс повторяющихся сверток, гауссиан не меняется, это фиксированная точка.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.4,
  "end": 684.06
 },
 {
  "input": "So the only thing it can approach is itself, and since it's one member in this big family of distributions, all of which must be tending towards a single universal shape, it must be that universal shape.",
  "translatedText": "Таким образом, единственное, к чему он может приблизиться, — это он сам, и поскольку он является одним из членов этого большого семейства распределений, все из которых должны стремиться к одной универсальной форме, оно должно быть этой универсальной формой.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 684.2,
  "end": 695.06
 },
 {
  "input": "I mentioned at the start how this calculation, step two, is something that you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed by a geometric argument that leverages the rotational symmetry of this graph is that it directly connects to a few things that we've talked about on this channel before.",
  "translatedText": "В начале я упомянул, что это вычисление, второй шаг, можно выполнить напрямую, просто символически, с помощью определений, но одна из причин, по которой я так очарован геометрическим аргументом, использующим вращательную симметрию этого графика, заключается в том, что это напрямую связано с несколькими вещами, о которых мы говорили на этом канале раньше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.58,
  "end": 712.3
 },
 {
  "input": "For example, the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this rotational symmetry as the defining feature of the distribution, that it locks you into this e to the negative x squared form, and also as an added bonus it connects to the classic proof for why pi shows up in the formula, meaning we now have a direct line between the presence and mystery of that pi and the central limit theorem.",
  "translatedText": "Например, вывод гауссианы Гершелем-Максвеллом, который, по сути, говорит, что вы можете рассматривать эту вращательную симметрию как определяющую особенность распределения, что она привязывает вас к этой форме e с отрицательным квадратом x, а также в качестве дополнительного бонуса. оно соединяется с классическим доказательством того, почему число «пи» появляется в формуле, а это означает, что теперь у нас есть прямая связь между присутствием и тайной этого числа «пи» и центральной предельной теоремой.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 712.4,
  "end": 736.5
 },
 {
  "input": "Also on a recent Patreon post, the channel supporter Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before, which leverages the use of entropy, and again for the theoretically curious among you I'll leave some links in the description.",
  "translatedText": "Также в недавнем посте на Patreon сторонник канала Дакша Вайд-Квинтер обратил мое внимание на совершенно другой подход, которого я раньше не видел, который использует использование энтропии, и снова для теоретически любопытных из вас я оставлю несколько ссылок. в описании.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 737.06,
  "end": 749.58
 },
 {
  "input": "By the way, if you want to stay up to date with new videos and also any other projects that I put out there like the Summer of Math Exposition, there is a mailing list.",
  "translatedText": "Кстати, если вы хотите быть в курсе новых видео, а также любых других проектов, которые я публикую, например, «Лето математической выставки», есть список рассылки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 750.96,
  "end": 758.4
 },
 {
  "input": "It's relatively new and I'm pretty sparing about only posting what I think people will enjoy.",
  "translatedText": "Он относительно новый, и я довольно экономно публикую только то, что, по моему мнению, понравится людям.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 758.72,
  "end": 762.78
 },
 {
  "input": "Usually I try not to be too promotional at the end of videos these days, but if you are interested in following the work that I do, this is probably one of the most enduring ways to do so.",
  "translatedText": "Обычно в наши дни я стараюсь не слишком рекламировать видео в конце, но если вам интересно следить за тем, что я делаю, это, вероятно, один из самых надежных способов сделать это.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 763.22,
  "end": 795.26
 }
]