[
 {
  "input": "The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.",
  "translatedText": "正規分布の基礎となる基本関数、別名ガウス関数は、e の負の x の 2 乗です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.12
 },
 {
  "input": "But you might wonder, why this function?",
  "translatedText": "しかし、なぜこの機能があるのか疑問に思われるかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.64,
  "end": 8.34
 },
 {
  "input": "Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle, why is it that the theory of probability seems to have a special place in its heart for this particular expression?",
  "translatedText": "質量が中央に向かって集中している対称的な滑らかなグラフを与える、私たちが思いつくすべての式の中で 、この特定の式について確率論がその中心に特別な位置を占めているように見えるのはなぜでしょうか?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.72,
  "end": 20.44
 },
 {
  "input": "For the last many videos I've been hinting at an answer to this question, and here we'll finally arrive at something like a satisfying answer.",
  "translatedText": "これまでの多くのビデオで、私はこの質問に対する答えをほのめかしてき ましたが、ここでようやく満足のいく答えのようなものに到達します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.38,
  "end": 27.68
 },
 {
  "input": "As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem, which describes how as you add multiple copies of a random variable, for example rolling a weighted die many different times or letting a ball bounce off of a peg repeatedly, then the distribution describing that sum tends to look approximately like a normal distribution.",
  "translatedText": "私たちの現在の状況を簡単におさらいとして、いくつかのビデオで中心極限定理について 説明しました。 これは、たとえば重み付きサイコロを何度も転がしたり、ボールを反射 させたりするなど、確率変数の複数のコピーを追加する方法を説明します。 ペグを繰り返 し実行すると、その合計を表す分布は正規分布にほぼ似たものになる傾向があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.68,
  "end": 47.72
 },
 {
  "input": "What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions, that approximation to a normal becomes better and better.",
  "translatedText": "中心極限定理が示しているのは、適切な条件下でその和をどんどん大 きくしていくと、法線への近似がますます良くなるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 48.44,
  "end": 56.22
 },
 {
  "input": "But I never explained why this theorem is actually true, we only talked about what it's claiming.",
  "translatedText": "しかし、私はこの定理が実際に真実である理由を一度も説明せず、それが主張していることについてのみ話しました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.94,
  "end": 61.98
 },
 {
  "input": "In the last video we started talking about the math involved in adding two random variables.",
  "translatedText": "前回のビデオでは、2 つの確率変数を追加する際の数学について話し始めました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.08,
  "end": 67.88
 },
 {
  "input": "If you have two random variables, each following some distribution, then to find the distribution describing the sum of those variables, you compute something known as a convolution between the two original functions.",
  "translatedText": "それぞれが何らかの分布に従う 2 つの確率変数がある場 合、それらの変数の合計を表す分布を見つけるには、2 つの元の関数間の畳み込みと呼ばれるものを計算します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 68.26,
  "end": 79.7
 },
 {
  "input": "And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.",
  "translatedText": "そして、私たちはこの畳み込み演算が実際に何であるかを視覚化する 2 つの異なる方法を構築するのに多くの時間を費やしました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.88,
  "end": 85.94
 },
 {
  "input": "Today our basic job is to work through a particular example, which is to ask what happens when you add two normally distributed random variables, which as you know by now is the same as asking what do you get if you compute a convolution between two Gaussian functions.",
  "translatedText": "今日の私たちの基本的な仕事は、特定の例に取り組むことです。 これは、2 つの正規分布確 率変数を追加すると何が起こるかを尋ねることです。 これは、もうおわかりのように、2 つ のガウス分布の間の畳み込みを計算すると何が得られるかを尋ねることと同じです。 機能。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.94,
  "end": 101.78
 },
 {
  "input": "I'd like to share an especially pleasing visual way that you can think about this calculation, which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.",
  "translatedText": "この計算について考えることができる、特に楽しい視覚的な方法を共有したいと思います。 これにより、そ もそも、負の x の 2 乗関数に対する e が特別である理由がある程度理解できると思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.52,
  "end": 112.36
 },
 {
  "input": "After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.",
  "translatedText": "説明を進めた後、この計算が中心極限定理の証明に含まれるステップの 1 つであることについて説明します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.36,
  "end": 118.24
 },
 {
  "input": "It's the step that answers the question of why a Gaussian and not something else is the central limit.",
  "translatedText": "これは、なぜ他のものではなくガウスが中心限界であるのかという疑問に答えるステップです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.32,
  "end": 123.56
 },
 {
  "input": "But first, let's dive in.",
  "translatedText": "しかし、まず始めてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.2,
  "end": 125.84
 },
 {
  "input": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
  "translatedText": "ガウスの完全な公式は、単に e を負の x の 2 乗するよりも複雑です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.78,
  "end": 134.44
 },
 {
  "input": "The exponent is typically written as negative one half times x divided by sigma squared, where sigma describes the spread of the distribution, specifically the standard deviation.",
  "translatedText": "指数は通常、x の負の 2 分の 1 をシグマ の 2 乗で割ったものとして記述されます。 シ グマは分布の広がり、特に標準偏差を表します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.82,
  "end": 144.2
 },
 {
  "input": "All of this needs to be multiplied by a fraction on the front, which is there to make sure that the area under the curve is one, making it a valid probability distribution.",
  "translatedText": "これらすべてに前部の分数を掛ける必要があります。 これは、曲線の下の領域 が確実に 1 になるようにするためのもので、有効な確率分布になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.68,
  "end": 153.42
 },
 {
  "input": "And if you want to consider distributions that aren't necessarily centered at zero, you would also throw another parameter, mu, into the exponent like this.",
  "translatedText": "また、必ずしもゼロを中心としない分布を考慮したい場合は、 次のように指数に別のパラメーター mu を投入します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 154.02,
  "end": 161.18
 },
 {
  "input": "Although for everything we'll be doing here, we just consider centered distributions.",
  "translatedText": "ただし、ここで行うすべてのことについては、中心に配置された分布のみを考慮します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.54,
  "end": 165.12
 },
 {
  "input": "Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions, the direct way to do this would be to take the definition of a convolution, this integral expression we built up last video, and then to plug in for each one of the functions involved the formula for a Gaussian.",
  "translatedText": "さて、今日の中心的な目標である 2 つのガウス関 数間の畳み込みを計算することを見ると、これを行う 直接的な方法は、畳み込みの定義、つまり前回のビデ オで作成したこの積分式を取得し、次のようにするこ とです。 ガウスの公式を含む各関数のプラグイン。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.8,
  "end": 183.76
 },
 {
  "input": "It's kind of a lot of symbols when you throw it all together, but more than anything, working this out is an exercise in completing the square.",
  "translatedText": "すべて一緒にするとかなりの記号になりますが、何よりも、これ を解決することは正方形を完成させるための練習になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.22,
  "end": 190.08
 },
 {
  "input": "And there's nothing wrong with that.",
  "translatedText": "そしてそれは何も悪いことではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.56,
  "end": 191.58
 },
 {
  "input": "That will get you the answer that you want.",
  "translatedText": "そうすればあなたが望む答えが得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.72,
  "end": 193.22
 },
 {
  "input": "But of course, you know me, I'm a sucker for visual intuition, and in this case, there's another way to think about it that I haven't seen written about before, that offers a very nice connection to other aspects of this distribution, like the presence of pi and certain ways to derive where it comes from.",
  "translatedText": "しかし、もちろん、ご存知のとおり、私は視覚的直観が苦手です。 そしてこの場合、これまで書かれたことのない別の考え方があり、 この問題の他の側面と非常にうまく関連しています。 円周率の存在 や、円周率がどこから来たのかを導き出す特定の方法などの分布。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.76,
  "end": 207.86
 },
 {
  "input": "And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution, and just showing the computation for the simplified form, e to the negative x squared.",
  "translatedText": "そして、私がこれを行う方法は、最初に実際の分布に関連付けら れたすべての定数を取り除き、単純化された形式、つまり負の x の 2 乗に対する e の計算だけを示すことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 208.2,
  "end": 217.96
 },
 {
  "input": "The essence of what we want to compute is what the convolution between two copies of this function looks like.",
  "translatedText": "私たちが計算したいことの本質は、この関数の 2 つのコピー間の畳み込みがどのようになるかということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.96,
  "end": 224.08
 },
 {
  "input": "If you'll remember, in the last video we had two different ways to visualize convolutions, and the one we'll be using here is the second one, involving diagonal slices.",
  "translatedText": "覚えていると思いますが、前回のビデオでは畳み込みを視覚化する 2 つの異なる方 法がありました。 ここで使用する方法は、対角スライスを含む 2 番目の方法です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.46,
  "end": 232.92
 },
 {
  "input": "And as a quick reminder of the way that worked, if you have two different distributions that are described by two different functions, f and g, then every possible pair of values that you might get when you sample from these two distributions can be thought of as individual points on the xy-plane.",
  "translatedText": "動作方法を簡単に思い出してください。 2 つの異なる関数 f と g で記述される 2 つの異なる分布がある場合、これら 2 つ の分布からサンプリングするときに取得される可能性のある値のすべて のペアを考えることができます。 xy 平面上の個々の点として。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 233.28,
  "end": 249.56
 },
 {
  "input": "And the probability density of landing on one such point, assuming independence, looks like f of x times g of y.",
  "translatedText": "そして、そのような点に着地する確率密度は、独立性を仮定す ると、x の f と y の g の積のようになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.36,
  "end": 257.52
 },
 {
  "input": "So what we do is we look at a graph of that expression as a two-variable function of x and y, which is a way of showing the distribution of all possible outcomes when we sample from the two different variables.",
  "translatedText": "そこで私たちが行うことは、その式のグラフを x と y の 2 変数関数として見ることです。 これは、2 つの異なる変数からサン プリングしたときに考えられるすべての結果の分布を示す方法です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.0,
  "end": 269.62
 },
 {
  "input": "To interpret the convolution of f and g evaluated on some input s, which is a way of saying how likely are you to get a pair of samples that adds up to this sum s, what you do is you look at a slice of this graph over the line x plus y equals s, and you consider the area under that slice.",
  "translatedText": "いくつかの入力 s で評価された f と g の畳み込みを解釈するには、つま り、合計がこの合計 s になるサンプルのペアを取得する可能性がどのくらいあ るかを知る方法として、このグラフのスライスを見ることになります。 線 x に y を加えたものが s に等しいので、そのスライスの下の領域を考慮します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.56,
  "end": 289.3
 },
 {
  "input": "This area is almost, but not quite, the value of the convolution at s.",
  "translatedText": "この領域は、完全ではありませんが、s での畳み込みの値とほぼ同じです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.1,
  "end": 296.32
 },
 {
  "input": "For a mildly technical reason, you need to divide by the square root of two.",
  "translatedText": "やや技術的な理由から、2 の平方根で割る必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.8,
  "end": 300.16
 },
 {
  "input": "Still, this area is the key feature to focus on.",
  "translatedText": "それでも、この領域は注目すべき重要な機能です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.84,
  "end": 303.44
 },
 {
  "input": "You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum.",
  "translatedText": "これは、特定の合計に対応するすべての結果のすべての 確率密度を組み合わせる方法と考えることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.44,
  "end": 311.04
 },
 {
  "input": "In the specific case where these two functions look like e to the negative x squared and e to the negative y squared, the resulting 3D graph has a really nice property that you can exploit.",
  "translatedText": "これら 2 つの関数が、負の x の 2 乗に対して e と、負の y の 2 乗に対して e のように見える特定のケースでは、結果とし て得られる 3D グラフには、活用できる非常に優れた特性があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.3,
  "end": 323.5
 },
 {
  "input": "It's rotationally symmetric.",
  "translatedText": "回転対称ですね。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.72,
  "end": 325.68
 },
 {
  "input": "You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared, and this term describes the square of the distance between any point on the xy plane and the origin.",
  "translatedText": "これは、これらの用語を組み合わせることでわかり、完全に x の 2 乗と y の 2 乗の関数であることがわかります。 この用語は、xy 平面上の任意の点と原点の間の距離 の 2 乗を表します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.88,
  "end": 338.46
 },
 {
  "input": "So in other words, the expression is purely a function of the distance from the origin.",
  "translatedText": "つまり、この式は純粋に原点からの距離の関 数です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 339.2,
  "end": 343.16
 },
 {
  "input": "And by the way, this would not be true for any other distribution.",
  "translatedText": "ちなみに、これは他のディストリビューションには当てはまりません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.56,
  "end": 347.92
 },
 {
  "input": "It's a property that uniquely characterizes bell curves.",
  "translatedText": "これは、ベル カーブを独特に特徴づけるプロパティです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 351.28
 },
 {
  "input": "So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about, and honestly calculating the area would just amount to computing the original integral that defines a convolution in the first place.",
  "translatedText": "したがって、他のほとんどの関数のペア では、これらの対角スライスは考えるのが難しい複雑な形状になるため、正直 に言って面積を計算することは、そもそも畳み込みを定義する元の積分を計算 することになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.16,
  "end": 365.54
 },
 {
  "input": "So in most cases, the visual intuition doesn't really buy you anything.",
  "translatedText": "したがって、ほとんどの場合、視覚的な直感では実際には何も購入できません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.94,
  "end": 369.36
 },
 {
  "input": "But in the case of bell curves, you can leverage that rotational symmetry.",
  "translatedText": "しかし、ベルカーブの場合は、その回転対称性を利用できます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.36,
  "end": 373.92
 },
 {
  "input": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
  "translatedText": "ここで、ある値の s について、x と y が s に等しいという直線上のこれらのスライスの 1 つに焦点を当てます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.8,
  "end": 380.48
 },
 {
  "input": "And remember, the convolution that we're trying to compute is a function of s.",
  "translatedText": "そして、計算しようとしている畳み込みは s の関数であることを思い出してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.3,
  "end": 385.84
 },
 {
  "input": "The thing that you want is an expression of s that tells you the area under this slice.",
  "translatedText": "必要なのは、この スライスの下の領域を示す s の式です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.84,
  "end": 391.1
 },
 {
  "input": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s.",
  "translatedText": "この線を見ると、その線は 0 で x 軸と交差し、0 で y 軸と交差しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.7,
  "end": 397.9
 },
 {
  "input": "And a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two.",
  "translatedText": "そして、ピタゴラスを少し学ぶと、 原点からこの線までの直線距離が 2 の平方根で割られることがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.32,
  "end": 405.32
 },
 {
  "input": "Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees, where you'd find something parallel to the y-axis the same distance away from the origin.",
  "translatedText": "さて、対称性のため、このスライスは 45 度回転して得られるスライスと同一 であり、原点から同じ距離離れたところに y 軸に平行なものが見つかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 416.36
 },
 {
  "input": "The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions, because it only involves taking an integral with respect to y.",
  "translatedText": "重要なのは、y 軸に平行なスライスの他の領域を計算することは、y に関す る積分を取るだけなので、他の方向のスライスよりもはるかに簡単であるとい うことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.64,
  "end": 428.26
 },
 {
  "input": "The value of x on this slice is a constant.",
  "translatedText": "このスライスの x の値は定数です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.74,
  "end": 431.44
 },
 {
  "input": "Specifically, it would be the constant s divided by the square root of two.",
  "translatedText": "具体的には、定数 s を 2 の 平方根で割ったものになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 434.76
 },
 {
  "input": "So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out.",
  "translatedText": "したがって、積分を計算してこの領域を見つけるとき、こ の項のすべてが単なる数値であるかのように動作し、因数分解することができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.76,
  "end": 443.38
 },
 {
  "input": "This is the important point.",
  "translatedText": "これが 重要な点です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.88,
  "end": 444.94
 },
 {
  "input": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
  "translatedText": "s に関係するものはすべて、統合された変数から完全に分 離されました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.28,
  "end": 450.2
 },
 {
  "input": "This remaining integral is a little bit tricky.",
  "translatedText": "この残りの積分は少し注意が必要です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.82,
  "end": 453.0
 },
 {
  "input": "I did a whole video on it, it's actually quite famous.",
  "translatedText": "私はそれについてビデオを作りました 、それは実際には非常に有名です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.08,
  "end": 455.2
 },
 {
  "input": "But you almost don't really care.",
  "translatedText": "しかし、あなたはほとんど気にしません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.5,
  "end": 456.9
 },
 {
  "input": "The point is that it's just some number.",
  "translatedText": "重要なのは、それは単なる数字だということ です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.24,
  "end": 459.0
 },
 {
  "input": "That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s.",
  "translatedText": "その数値はたまたま pi の平方根ですが、本当に重要なのは、それが s に依存し ないものであるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.0,
  "end": 465.48
 },
 {
  "input": "And essentially, this is our answer.",
  "translatedText": "そして本質的に、これが私たちの答えです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.88,
  "end": 468.48
 },
 {
  "input": "We were looking for an expression for the area of these slices as a function of s, and now we have it.",
  "translatedText": "s の関数としてこれら のスライスの面積を表す式を探していましたが、今ではそれが見つかりました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.78,
  "end": 474.28
 },
 {
  "input": "It looks like e to the negative s squared divided by two, scaled by some constant.",
  "translatedText": "これは、e を負の s の 2 乗で 2 で割ったものを、ある定数でスケールしたもののように見えます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.38,
  "end": 478.84
 },
 {
  "input": "In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent.",
  "translatedText": "言い換えれば、これは鐘曲線で もあり、別のガウス曲線であり、指数部のこの 2 つのために少しだけ伸びたものです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.3,
  "end": 485.62
 },
 {
  "input": "As I said earlier, the convolution evaluated at s is not quite this area.",
  "translatedText": "前に述べたように、s で評価される畳み込みはこの領域には達していません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.62,
  "end": 490.86
 },
 {
  "input": "Technically, it's this area divided by the square root of two.",
  "translatedText": "技術的には、この面積を 2 の平方根で割った値になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.34,
  "end": 494.16
 },
 {
  "input": "We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant.",
  "translatedText": "前回のビデオで説明しましたが、定数に組み込まれて いるだけなので、あまり重要ではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.8,
  "end": 499.24
 },
 {
  "input": "What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian.",
  "translatedText": "本当に重要なのは、2 つのガウス 間の畳み込み自体が別のガウスであるという結論です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.68,
  "end": 505.68
 },
 {
  "input": "If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma, essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front, and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma.",
  "translatedText": "戻って、平均ゼロと任 意の標準偏差シグマを持つ正規分布のすべての定数を再導入した場合、 本質的に同じ推論により、指数と前に現れる 2 つの因子の同じ平方 根が得られます。 そして、このような 2 つの正規分布の間の畳み込 みは、標準偏差の平方根がシグマの 2 倍である別の正規分布である という結論につながります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.56,
  "end": 530.38
 },
 {
  "input": "If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result.",
  "translatedText": "これまでに多くの畳み込みを計算したことがない場合は、これが 非常に特殊な結果であることを強調する価値があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.98,
  "end": 536.06
 },
 {
  "input": "Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process.",
  "translatedText": "ほとんどの場合、まったく異なる種類の 関数が完成しますが、ここではプロセスに一種の安定性があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.38,
  "end": 542.5
 },
 {
  "input": "Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations.",
  "translatedText": "また、演習が 好きな方のために、2 つの異なる標準偏差の場合にどのように対処するかを 画面上に残しておきます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.26,
  "end": 549.44
 },
 {
  "input": "Still, some of you might be raising your hands and saying, what's the big deal?",
  "translatedText": "それでも、手を挙げて、何が大したことなのかと言う人もいる かもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.42,
  "end": 553.94
 },
 {
  "input": "I mean, when you first heard the question, what do you get when you add two normally distributed random variables, you probably even guessed that the answer should be another normally distributed variable.",
  "translatedText": "つまり、2 つの正規分布する確率変数を加算すると何が得られるか という質問を最初に聞いたとき、おそらく答えは別の正規分布する変数であるはずだと さえ推測したでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.48,
  "end": 564.32
 },
 {
  "input": "After all, what else is it going to be?",
  "translatedText": "結局のところ、他には何があるのでしょうか？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.76,
  "end": 566.36
 },
 {
  "input": "Normal distributions are supposedly quite common, so why not?",
  "translatedText": "正規分布はかなり一般的だと思わ れますが、なぜそうではないのでしょうか?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.86,
  "end": 570.24
 },
 {
  "input": "You could even say that this should follow from the central limit theorem, but that would have it all backwards.",
  "translatedText": "これは中心極限定理に従うべきだとさえ言えま すが、それではすべてが逆になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 570.24,
  "end": 575.48
 },
 {
  "input": "First of all, the supposed ubiquity of normal distributions is often a little exaggerated, but to the extent that they do come up, it is because of the central limit theorem, but it would be cheating to say the central limit theorem implies this result because this computation we just did is the reason that the function at the heart of the central limit theorem is a Gaussian in the first place and not some other function.",
  "translatedText": "まず第一に、正規分布の偏在性 は少し誇張されていることがよくありますが、実際にそのようなことが 起こるのは中心極限定理によるものですが、中心極限定理がこの結果を 暗示していると言うのは不正です。 私たちが行ったこの計算が、中心極 限定理の中心となる関数がそもそもガウス関数であり、他の関数ではな い理由です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.18,
  "end": 597.06
 },
 {
  "input": "We've talked all about the central limit theorem before, but essentially it says if you repeatedly add copies of a random variable to itself, which mathematically looks like repeatedly computing convolutions against a given distribution, then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution.",
  "translatedText": "中心極限定理についてはこれまでにすべて説明してきましたが、本質的 には、確率変数のコピーをそれ自体に繰り返し追加すると、数学的には与えられた分布 に対して畳み込みを繰り返し計算するように見え、適切なシフトとリスケーリングの後 、傾向は次のようになります。 常に正規分布に近づきます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.06,
  "end": 616.5
 },
 {
  "input": "Technically there's a small assumption the distribution you start with can't have infinite variance, but it's a relatively soft assumption.",
  "translatedText": "技術的には、開始する分布 が無限の分散を持つことはできないという小さな仮定がありますが、それは比較的緩やかな仮 定です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 616.98,
  "end": 623.22
 },
 {
  "input": "The magic is that for a huge category of initial distributions, this process of adding a whole bunch of random variables drawn from that distribution always tends towards this one universal shape, a Gaussian.",
  "translatedText": "魔法のようなものは、初期分布の巨大なカテゴリに対して、その分布から抽出された大量 の確率変数を追加するこのプロセスが常に、この 1 つの普遍的な形状であるガウスに向かう傾 向があるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.22,
  "end": 635.1
 },
 {
  "input": "One common approach to proving this theorem involves two separate steps.",
  "translatedText": "この定理を証明するための一般的なアプローチの 1 つは、2 つの別々のステップを必要とします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.82,
  "end": 639.3
 },
 {
  "input": "The first step is to show that for all the different finite variance distributions you might start with, there exists a single universal shape that this process of repeated convolutions tends towards.",
  "translatedText": "最初のステップは、最初に始めるすべての異なる有限分散分布に対して、この繰り返し たたみ込みのプロセスが向かう傾向にある単一の普遍的な形状が存在することを示す ことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.6,
  "end": 650.0
 },
 {
  "input": "This step is actually pretty technical, it goes a little beyond what I want to talk about here.",
  "translatedText": "このステップは実際にはかなり技術的であり、ここで説明したい内容を少し 超えています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.0,
  "end": 654.24
 },
 {
  "input": "You often use these objects called moment generating functions that gives you a very abstract argument that there must be some universal shape, but it doesn't make any claim about what that particular shape is, just that everything in this big family is tending towards a single point in the space of distributions.",
  "translatedText": "モーメント生成関数と呼ばれるこれらのオブジェクトをよく使用しますが、これは 、何らかの普遍的な形状が存在するに違いないという非常に抽象的な議論を提供しますが、その特定 の形状が何であるかについては何も主張せず、この大きなファミリー内のすべてのものはある傾向に あるというだけです。 分布空間内の単一点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.52,
  "end": 669.98
 },
 {
  "input": "So then step number two is what we just showed in this video, prove that the convolution of two Gaussians gives another Gaussian.",
  "translatedText": "ステップ 2 は、このビデオで示したもので、2 つのガウス分布の畳み込みによって別のガウス分布が得られることを証明します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 670.62,
  "end": 677.4
 },
 {
  "input": "What that means is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a fixed point.",
  "translatedText": "これが意味するの は、この繰り返し畳み込みのプロセスを適用しても、ガウス分布は変化せず、固定点であるという ことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.4,
  "end": 684.06
 },
 {
  "input": "So the only thing it can approach is itself, and since it's one member in this big family of distributions, all of which must be tending towards a single universal shape, it must be that universal shape.",
  "translatedText": "したがって、それがアプローチできる唯一のものはそれ自体であり、それはこの大きな分布ファミリーのメ ンバーの 1 つであるため、そのすべてが単一の普遍的な形状に向かう傾向があるはずであり、それはその普遍的な形 状でなければなりません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 684.2,
  "end": 695.06
 },
 {
  "input": "I mentioned at the start how this calculation, step two, is something that you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed by a geometric argument that leverages the rotational symmetry of this graph is that it directly connects to a few things that we've talked about on this channel before.",
  "translatedText": "冒頭で、この計算 (ステップ 2) が、定義を使って記号的 に直接実行できることを述べましたが、このグラフの回転対称性を利用した幾何学 的な議論に私が非常に魅了されている理由の 1 つは次のとおりです。 これは、 このチャンネルで以前に話したいくつかのことに直接つながります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.58,
  "end": 712.3
 },
 {
  "input": "For example, the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this rotational symmetry as the defining feature of the distribution, that it locks you into this e to the negative x squared form, and also as an added bonus it connects to the classic proof for why pi shows up in the formula, meaning we now have a direct line between the presence and mystery of that pi and the central limit theorem.",
  "translatedText": "たとえば、ガ ウスのハーシェル・マクスウェル導出では、本質的に、この回転対称性を分布の定義的な 特徴として見ることができ、この e を負の x の 2 乗形式に固定し、さらに追加 のボーナスとしても使用できると述べています。 それは、なぜ円周率が式に現れるのかに ついての古典的な証明につながります。 つまり、円周率の存在と謎と中心極限定理との間 に直接の線があることを意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 712.4,
  "end": 736.5
 },
 {
  "input": "Also on a recent Patreon post, the channel supporter Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before, which leverages the use of entropy, and again for the theoretically curious among you I'll leave some links in the description.",
  "translatedText": "また、最近の Patreon の投稿で、チャンネル サポータ ーの Daksha Vaid-Quinter が、エントロピーの使用を活用する、これまで見たことのない まったく異なるアプローチに私の注意を促しました。 理論的に興味のある皆さんのために、いくつかのリンクを残 しておきます。 説明にあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 737.06,
  "end": 749.58
 },
 {
  "input": "By the way, if you want to stay up to date with new videos and also any other projects that I put out there like the Summer of Math Exposition, there is a mailing list.",
  "translatedText": "ちなみに、新しいビデオや、Summer of Math Expo のよ うな私が公開したその他のプロジェクトの最新情報を入手したい場合は、メーリング リストがあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 750.96,
  "end": 758.4
 },
 {
  "input": "It's relatively new and I'm pretty sparing about only posting what I think people will enjoy.",
  "translatedText": "比較的新しいものなので、人々が楽しめると思うものだけを投稿することにかなり気を配っています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 758.72,
  "end": 762.78
 },
 {
  "input": "Usually I try not to be too promotional at the end of videos these days, but if you are interested in following the work that I do, this is probably one of the most enduring ways to do so.",
  "translatedText": "最近では、ビデオの最後で宣伝しすぎないようにしていますが、私の作品をフォローす ることに興味があるのであれば、これはおそらく最も永続的な方法の 1 つです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 763.22,
  "end": 795.26
 }
]