[
 {
  "translatedText": "הפונקציה הבסיסית העומדת בבסיס התפלגות נורמלית, הלא היא גאוסית, היא e ל-x השלילי בריבוע.",
  "input": "The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.12
 },
 {
  "translatedText": "אבל אתה עשוי לתהות, למה הפונקציה הזו?",
  "input": "But you might wonder, why this function?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.64,
  "end": 8.34
 },
 {
  "translatedText": "מבין כל הביטויים שיכולנו לחלום על הנותנים גרף חלק סימטרי עם מסה מרוכזת לכיוון האמצע, מדוע נראה שלתורת ההסתברות יש מקום מיוחד בלבה לביטוי המסוים הזה?",
  "input": "Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle, why is it that the theory of probability seems to have a special place in its heart for this particular expression?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.72,
  "end": 20.44
 },
 {
  "translatedText": "בסרטונים הרבים האחרונים רמזתי לתשובה לשאלה הזו, והנה סוף סוף נגיע למשהו כמו תשובה מספקת.",
  "input": "For the last many videos I've been hinting at an answer to this question, and here we'll finally arrive at something like a satisfying answer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.38,
  "end": 27.68
 },
 {
  "translatedText": "בתור רענון מהיר על המקום שבו אנחנו נמצאים, לפני כמה סרטונים דיברנו על משפט הגבול המרכזי, שמתאר כיצד כאשר מוסיפים מספר עותקים של משתנה אקראי, למשל זריקת קובייה משוקללת פעמים רבות ושונות, או נותנת לכדור לקפוץ. של יתד שוב ושוב, אז ההתפלגות המתארת את הסכום הזה נוטה להיראות בערך כמו התפלגות נורמלית.",
  "input": "As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem, which describes how as you add multiple copies of a random variable, for example rolling a weighted die many different times, or letting a ball bounce off of a peg repeatedly, then the distribution describing that sum tends to look approximately like a normal distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.68,
  "end": 47.72
 },
 {
  "translatedText": "מה שמשפט הגבול המרכזי אומר הוא שכאשר אתה מגדיל את הסכום הזה יותר ויותר, בתנאים מתאימים, הקירוב לנורמלית הופך טוב יותר ויותר.",
  "input": "What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions, that approximation to a normal becomes better and better.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 48.44,
  "end": 56.22
 },
 {
  "translatedText": "אבל מעולם לא הסברתי מדוע המשפט הזה באמת נכון.",
  "input": "But I never explained why this theorem is actually true.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.94,
  "end": 60.18
 },
 {
  "translatedText": "דיברנו רק על מה שהוא טוען.",
  "input": "We only talked about what it's claiming.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.22,
  "end": 61.98
 },
 {
  "translatedText": "בסרטון האחרון התחלנו לדבר על המתמטיקה הכרוכה בהוספת שני משתנים אקראיים.",
  "input": "In the last video we started talking about the math involved in adding two random variables.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.08,
  "end": 67.88
 },
 {
  "translatedText": "אם יש לך שני משתנים אקראיים, כל אחד אחרי התפלגות כלשהי, אז כדי למצוא את ההתפלגות המתארת את סכום המשתנים האלה, אתה מחשב משהו המכונה קונבולציה בין שתי הפונקציות המקוריות.",
  "input": "If you have two random variables, each following some distribution, then to find the distribution describing the sum of those variables, you compute something known as a convolution between the two original functions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 68.26,
  "end": 79.7
 },
 {
  "translatedText": "והשקענו זמן רב בבניית שתי דרכים שונות להמחיש מהי באמת פעולת הקונבולציה הזו.",
  "input": "And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.88,
  "end": 85.94
 },
 {
  "translatedText": "היום העבודה הבסיסית שלנו היא לעבוד על דוגמה מסוימת, שהיא לשאול מה קורה כשמוסיפים שני משתנים אקראיים מפוזרים נורמליים, שכפי שאתם יודעים עכשיו, זהה לשאלה מה מקבלים אם מחשבים קונבולציה בין שתי פונקציות גאוסיות.",
  "input": "Today our basic job is to work through a particular example, which is to ask what happens when you add two normally distributed random variables, which, as you know by now, is the same as asking what do you get if you compute a convolution between two Gaussian functions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.94,
  "end": 101.78
 },
 {
  "translatedText": "ברצוני לחלוק דרך ויזואלית נעימה במיוחד שתוכלו לחשוב על החישוב הזה, אשר בתקווה מציעה תחושה מסוימת של מה שהופך את הפונקציה e ל-x שלילי בריבוע למיוחד מלכתחילה.",
  "input": "I'd like to share an especially pleasing visual way that you can think about this calculation, which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.52,
  "end": 112.36
 },
 {
  "translatedText": "אחרי שנעבור עליו, נדבר על איך החישוב הזה הוא אחד השלבים הכרוכים בהוכחת משפט הגבול המרכזי.",
  "input": "After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.36,
  "end": 118.24
 },
 {
  "translatedText": "זה הצעד שעונה על השאלה למה גאוס ולא משהו אחר הוא הגבול המרכזי.",
  "input": "It's the step that answers the question of why a Gaussian and not something else is the central limit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.32,
  "end": 123.56
 },
 {
  "translatedText": "אבל קודם כל, בואו נצלול פנימה.",
  "input": "But first, let's dive in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.2,
  "end": 125.84
 },
 {
  "translatedText": "הנוסחה המלאה של גאוס מסובכת יותר מסתם e ל-x השלילי בריבוע.",
  "input": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.78,
  "end": 134.44
 },
 {
  "translatedText": "המעריך כתוב בדרך כלל כשלילי חצי כפול x חלקי סיגמא בריבוע, כאשר סיגמא מתארת את התפשטות ההתפלגות, במיוחד את סטיית התקן.",
  "input": "The exponent is typically written as negative one half times x divided by sigma squared, where sigma describes the spread of the distribution, specifically the standard deviation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.82,
  "end": 144.2
 },
 {
  "translatedText": "כל זה צריך להיות מוכפל בשבר בחזית, שהוא שם כדי לוודא שהשטח מתחת לעקומה הוא אחד, מה שהופך אותו להתפלגות הסתברות תקפה.",
  "input": "All of this needs to be multiplied by a fraction on the front, which is there to make sure that the area under the curve is one, making it a valid probability distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.68,
  "end": 153.42
 },
 {
  "translatedText": "ואם אתה רוצה לשקול התפלגויות שאינן בהכרח מרוכזות באפס, תזרוק גם פרמטר נוסף, mu, לתוך המעריך כך.",
  "input": "And if you want to consider distributions that aren't necessarily centered at zero, you would also throw another parameter, mu, into the exponent like this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 154.02,
  "end": 161.18
 },
 {
  "translatedText": "למרות שלכל מה שנעשה כאן, אנחנו רק שוקלים הפצות ממוקדות.",
  "input": "Although for everything we'll be doing here, we just consider centered distributions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.54,
  "end": 165.12
 },
 {
  "translatedText": "עכשיו אם תסתכל על המטרה המרכזית שלנו להיום, שהיא לחשב קונבולציה בין שתי פונקציות גאוסיות, הדרך הישירה לעשות זאת תהיה לקחת את ההגדרה של קונבולציה, את הביטוי האינטגרלי הזה שבנינו בסרטון האחרון, ואז חבר עבור כל אחת מהפונקציות הכרוכות בנוסחה של גאוס.",
  "input": "Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions, the direct way to do this would be to take the definition of a convolution, this integral expression we built up last video, and then to plug in for each one of the functions involved the formula for a Gaussian.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.8,
  "end": 183.76
 },
 {
  "translatedText": "זה סוג של הרבה סמלים כשאתה זורק הכל ביחד, אבל יותר מהכל, עיבוד זה הוא תרגיל בהשלמת הריבוע.",
  "input": "It's kind of a lot of symbols when you throw it all together, but more than anything, working this out is an exercise in completing the square.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.22,
  "end": 190.08
 },
 {
  "translatedText": "ואין בזה שום דבר רע.",
  "input": "And there's nothing wrong with that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.56,
  "end": 191.58
 },
 {
  "translatedText": "זה יביא לך את התשובה שאתה רוצה.",
  "input": "That will get you the answer that you want.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.72,
  "end": 193.22
 },
 {
  "translatedText": "אבל כמובן, אתם מכירים אותי, אני פראייר לאינטואיציה חזותית, ובמקרה הזה, יש דרך אחרת לחשוב על זה שלא ראיתי שכתבתי עליה בעבר שמציעה חיבור נחמד מאוד להיבטים אחרים של ההפצה הזו. , כמו נוכחות של pi ודרכים מסוימות להסיק מהיכן הוא מגיע.",
  "input": "But of course, you know me, I'm a sucker for visual intuition, and in this case, there's another way to think about it that I haven't seen written about before that offers a very nice connection to other aspects of this distribution, like the presence of pi and certain ways to derive where it comes from.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.76,
  "end": 207.86
 },
 {
  "translatedText": "והדרך שבה הייתי רוצה לעשות זאת היא קודם כל לקלף את כל הקבועים הקשורים להתפלגות בפועל, ורק הצגת החישוב של הצורה הפשוטה, e ל-x השלילי בריבוע.",
  "input": "And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution, and just showing the computation for the simplified form, e to the negative x squared.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 208.2,
  "end": 217.96
 },
 {
  "translatedText": "המהות של מה שאנחנו רוצים לחשב היא איך נראית הקונבולולוציה בין שני עותקים של פונקציה זו.",
  "input": "The essence of what we want to compute is what the convolution between two copies of this function looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.96,
  "end": 224.08
 },
 {
  "translatedText": "אם תזכרו, בסרטון האחרון היו לנו שתי דרכים שונות לדמיין פיתולים, וזו שבה נשתמש כאן היא השנייה הכוללת פרוסות אלכסוניות.",
  "input": "If you'll remember, in the last video we had two different ways to visualize convolutions, and the one we'll be using here is the second one involving diagonal slices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.46,
  "end": 232.92
 },
 {
  "translatedText": "וכתזכורת מהירה לדרך שעבדה, אם יש לך שתי התפלגויות שונות שמתוארות על ידי שתי פונקציות שונות, f ו-g, אז אפשר לחשוב על כל זוג ערכים אפשרי שתקבל כשאתה מדגימה משתי ההתפלגויות האלה כנקודות בודדות במישור ה-xy.",
  "input": "And as a quick reminder of the way that worked, if you have two different distributions that are described by two different functions, f and g, then every possible pair of values that you might get when you sample from these two distributions can be thought of as individual points on the xy-plane.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 233.28,
  "end": 249.56
 },
 {
  "translatedText": "וצפיפות ההסתברות לנחיתה בנקודה אחת כזו, בהנחה של עצמאות, נראית כמו f של x כפול g של y.",
  "input": "And the probability density of landing on one such point, assuming independence, looks like f of x times g of y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.36,
  "end": 257.52
 },
 {
  "translatedText": "אז מה שאנחנו עושים זה שאנחנו מסתכלים על גרף של הביטוי הזה כפונקציה של שני משתנים של x ו-y, שהיא דרך להראות את ההתפלגות של כל התוצאות האפשריות כשאנחנו דוגמים משני המשתנים השונים.",
  "input": "So what we do is we look at a graph of that expression as a two-variable function of x and y, which is a way of showing the distribution of all possible outcomes when we sample from the two different variables.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.0,
  "end": 269.62
 },
 {
  "translatedText": "כדי לפרש את הקונבולולוציה של f ו-g המוערכת בכמה קלט s, שהיא דרך לומר מה הסיכוי שתקבל זוג דגימות שמצטבר לסכום זה s, מה שאתה עושה זה להסתכל על פרוסה מהגרף הזה מעל הקו x פלוס y שווה s, ואתה מחשיב את השטח מתחת לפרוסה הזו.",
  "input": "To interpret the convolution of f and g evaluated on some input s, which is a way of saying how likely are you to get a pair of samples that adds up to this sum s, what you do is you look at a slice of this graph over the line x plus y equals s, and you consider the area under that slice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.56,
  "end": 289.3
 },
 {
  "translatedText": "אזור זה הוא כמעט, אבל לא לגמרי, הערך של הפיתול ב-s.",
  "input": "This area is almost, but not quite, the value of the convolution at s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.1,
  "end": 296.32
 },
 {
  "translatedText": "מסיבה טכנית קלה, עליך לחלק בשורש הריבועי של 2.",
  "input": "For a mildly technical reason, you need to divide by the square root of 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.8,
  "end": 300.16
 },
 {
  "translatedText": "ובכל זאת, אזור זה הוא התכונה העיקרית להתמקד בה.",
  "input": "Still, this area is the key feature to focus on.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.84,
  "end": 303.44
 },
 {
  "translatedText": "אתה יכול לחשוב על זה כדרך לשלב יחד את כל צפיפות ההסתברות עבור כל התוצאות המתאימות לסכום נתון.",
  "input": "You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.44,
  "end": 311.04
 },
 {
  "translatedText": "במקרה הספציפי שבו שתי הפונקציות הללו נראות כמו e ל-x שלילי בריבוע ו-e ל-y שלילי בריבוע, לגרף התלת-ממד המתקבל יש תכונה ממש נחמדה שתוכלו לנצל.",
  "input": "In the specific case where these two functions look like e to the negative x squared and e to the negative y squared, the resulting 3D graph has a really nice property that you can exploit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.3,
  "end": 323.5
 },
 {
  "translatedText": "זה סימטרי סיבובית.",
  "input": "It's rotationally symmetric.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.72,
  "end": 325.68
 },
 {
  "translatedText": "אתה יכול לראות זאת על ידי שילוב המונחים ולשים לב שזה לגמרי פונקציה של x בריבוע פלוס y בריבוע, והמונח הזה מתאר את ריבוע המרחק בין כל נקודה במישור xy לבין המקור.",
  "input": "You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared, and this term describes the square of the distance between any point on the xy plane and the origin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.88,
  "end": 338.46
 },
 {
  "translatedText": "אז במילים אחרות, הביטוי הוא אך ורק פונקציה של המרחק מהמקור.",
  "input": "So in other words, the expression is purely a function of the distance from the origin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 339.2,
  "end": 343.16
 },
 {
  "translatedText": "ודרך אגב, זה לא יהיה נכון לגבי שום הפצה אחרת.",
  "input": "And by the way, this would not be true for any other distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.56,
  "end": 347.92
 },
 {
  "translatedText": "זהו נכס המאפיין באופן ייחודי את עקומות הפעמון.",
  "input": "It's a property that uniquely characterizes bell curves.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 351.28
 },
 {
  "translatedText": "אז עבור רוב זוגות הפונקציות האחרות, הפרוסות האלכסוניות הללו יהיו איזו צורה מסובכת שקשה לחשוב עליה, ולמען האמת, חישוב השטח פשוט יסתכם בחישוב האינטגרל המקורי שמגדיר קונבולציה מלכתחילה.",
  "input": "So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about, and honestly, calculating the area would just amount to computing the original integral that defines a convolution in the first place.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.16,
  "end": 365.54
 },
 {
  "translatedText": "אז ברוב המקרים, האינטואיציה החזותית לא באמת קונה לך כלום.",
  "input": "So in most cases, the visual intuition doesn't really buy you anything.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.94,
  "end": 369.36
 },
 {
  "translatedText": "אבל במקרה של עקומות פעמון, אתה יכול למנף את הסימטריה הסיבובית הזו.",
  "input": "But in the case of bell curves, you can leverage that rotational symmetry.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.36,
  "end": 373.92
 },
 {
  "translatedText": "כאן, התמקד באחת הפרוסות הללו מעל הקו x פלוס y שווה s עבור ערך כלשהו של s.",
  "input": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.8,
  "end": 380.48
 },
 {
  "translatedText": "וזכור, הקונבולולוציה שאנו מנסים לחשב היא פונקציה של s.",
  "input": "And remember, the convolution that we're trying to compute is a function of s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.3,
  "end": 385.84
 },
 {
  "translatedText": "הדבר שאתה רוצה הוא ביטוי של s שאומר לך את האזור מתחת לפרוסה הזו.",
  "input": "The thing that you want is an expression of s that tells you the area under this slice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.84,
  "end": 391.1
 },
 {
  "translatedText": "ובכן, אם אתה מסתכל על הישר הזה, הוא חוצה את ציר ה-x ב-s אפס ואת ציר ה-y באפס s, וקצת של פיתגורס יראה לך שמרחק הישר מהמקור לישר זה מחולק ב-s. בשורש של שניים.",
  "input": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s, and a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.7,
  "end": 405.32
 },
 {
  "translatedText": "עכשיו, בגלל הסימטריה, הפרוסה הזו זהה לפרוסה שמסתובבת ב-45 מעלות, שם תמצא משהו מקביל לציר ה-y באותו מרחק מהמקור.",
  "input": "Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees where you'd find something parallel to the y-axis the same distance away from the origin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 416.36
 },
 {
  "translatedText": "המפתח הוא שחישוב השטח האחר הזה של פרוסה המקבילה לציר ה-y הוא הרבה הרבה יותר קל מאשר פרוסות בכיוונים אחרים, כי זה כרוך רק בלקיחת אינטגרל ביחס ל-y.",
  "input": "The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions because it only involves taking an integral with respect to y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.64,
  "end": 428.26
 },
 {
  "translatedText": "הערך של x בפרוסה זו הוא קבוע.",
  "input": "The value of x on this slice is a constant.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.74,
  "end": 431.44
 },
 {
  "translatedText": "באופן ספציפי, זה יהיה הקבוע s חלקי השורש הריבועי של שניים.",
  "input": "Specifically, it would be the constant s divided by the square root of two.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 434.76
 },
 {
  "translatedText": "אז כשאתה מחשב את האינטגרל, מוצא את השטח הזה, כל המונח הזה כאן מתנהג כאילו זה היה רק מספר, ואתה יכול לפרט אותו.",
  "input": "So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.76,
  "end": 443.38
 },
 {
  "translatedText": "זו הנקודה החשובה.",
  "input": "This is the important point.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.88,
  "end": 444.94
 },
 {
  "translatedText": "כל הדברים שמעורבים ב-s נפרדים כעת לחלוטין מהמשתנה המשולב.",
  "input": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.28,
  "end": 450.2
 },
 {
  "translatedText": "האינטגרל הנותר הזה קצת מסובך.",
  "input": "This remaining integral is a little bit tricky.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.82,
  "end": 453.0
 },
 {
  "translatedText": "עשיתי על זה סרטון שלם, הוא למעשה די מפורסם.",
  "input": "I did a whole video on it, it's actually quite famous.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.08,
  "end": 455.2
 },
 {
  "translatedText": "אבל כמעט לא ממש אכפת לך.",
  "input": "But you almost don't really care.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.5,
  "end": 456.9
 },
 {
  "translatedText": "הנקודה היא שזה רק מספר כלשהו.",
  "input": "The point is that it's just some number.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.24,
  "end": 459.0
 },
 {
  "translatedText": "המספר הזה הוא במקרה השורש הריבועי של pi, אבל מה שבאמת חשוב הוא שזה משהו ללא תלות ב-s.",
  "input": "That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.0,
  "end": 465.48
 },
 {
  "translatedText": "ובעצם זו התשובה שלנו.",
  "input": "And essentially this is our answer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.88,
  "end": 468.48
 },
 {
  "translatedText": "חיפשנו ביטוי לשטח של הפרוסות האלה כפונקציה של s, ועכשיו יש לנו את זה.",
  "input": "We were looking for an expression for the area of these slices as a function of s, and now we have it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.78,
  "end": 474.28
 },
 {
  "translatedText": "זה נראה כמו e ל-s השלילי בריבוע חלקי שניים, בקנה מידה בקבוע כלשהו.",
  "input": "It looks like e to the negative s squared divided by two, scaled by some constant.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.38,
  "end": 478.84
 },
 {
  "translatedText": "במילים אחרות, זו גם עקומת פעמון, עוד גאוסית, פשוט נמתחה מעט בגלל השניים האלה במעריך.",
  "input": "In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.3,
  "end": 485.62
 },
 {
  "translatedText": "כפי שאמרתי קודם, הקונבולציה המוערכת ב-s היא לא בדיוק התחום הזה.",
  "input": "As I said earlier, the convolution evaluated at s is not quite this area.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.62,
  "end": 490.86
 },
 {
  "translatedText": "טכנית זה השטח הזה חלקי השורש הריבועי של שניים.",
  "input": "Technically it's this area divided by the square root of two.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.34,
  "end": 494.16
 },
 {
  "translatedText": "דיברנו על זה בסרטון האחרון, אבל זה לא ממש משנה כי זה פשוט נהיה אפוי לתוך הקבוע.",
  "input": "We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.8,
  "end": 499.24
 },
 {
  "translatedText": "מה שחשוב באמת היא המסקנה שפיתול בין שני גאוסים הוא בעצמו גאוס אחר.",
  "input": "What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.68,
  "end": 505.68
 },
 {
  "translatedText": "אם היית חוזר אחורה ומציג מחדש את כל הקבועים להתפלגות נורמלית עם אפס ממוצע וסטיית תקן שרירותית סיגמא, נימוק זהה בעצם יוביל לאותו שורש ריבועי של שני גורמים שמופיע במעריך ובחזית, וזה מוביל למסקנה שהקונבולולוציה בין שתי התפלגויות נורמליות כאלה היא התפלגות נורמלית נוספת עם שורש ריבועי סטיית תקן של פעמיים סיגמה.",
  "input": "If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma, essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front, and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.56,
  "end": 530.38
 },
 {
  "translatedText": "אם לא חישבתם הרבה פיתולים בעבר, כדאי להדגיש שזו תוצאה מאוד מיוחדת.",
  "input": "If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.98,
  "end": 536.06
 },
 {
  "translatedText": "כמעט תמיד אתה מסיים עם סוג אחר לגמרי של פונקציה, אבל כאן יש סוג של יציבות לתהליך.",
  "input": "Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.38,
  "end": 542.5
 },
 {
  "translatedText": "כמו כן, לאלו מכם שנהנים מתרגילים, אשאיר אחד על המסך כיצד תתמודדו במקרה של שתי סטיות תקן שונות.",
  "input": "Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.26,
  "end": 549.44
 },
 {
  "translatedText": "ובכל זאת, אולי חלקכם מרימים ידיים ואומרים, מה העניין הגדול?",
  "input": "Still, some of you might be raising your hands and saying, what's the big deal?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.42,
  "end": 553.94
 },
 {
  "translatedText": "כלומר, כששמעת לראשונה את השאלה, מה אתה מקבל כשאתה מוסיף שני משתנים אקראיים מחולקים נורמלית, כנראה אפילו ניחשת שהתשובה צריכה להיות משתנה אחר בחלוקה נורמלית.",
  "input": "I mean, when you first heard the question, what do you get when you add two normally distributed random variables, you probably even guessed that the answer should be another normally distributed variable.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.48,
  "end": 564.32
 },
 {
  "translatedText": "אחרי הכל, מה זה עוד הולך להיות?",
  "input": "After all, what else is it going to be?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.76,
  "end": 566.36
 },
 {
  "translatedText": "התפלגויות נורמליות כביכול נפוצות למדי, אז למה לא?",
  "input": "Normal distributions are supposedly quite common, so why not?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.86,
  "end": 570.24
 },
 {
  "translatedText": "אפשר אפילו לומר שזה אמור לנבוע ממשפט הגבול המרכזי.",
  "input": "You could even say that this should follow from the central limit theorem.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 570.24,
  "end": 573.34
 },
 {
  "translatedText": "אבל זה יהפוך את הכל לאחור.",
  "input": "But that would have it all backwards.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.86,
  "end": 575.48
 },
 {
  "translatedText": "קודם כל, הנוכחות המשוערת של התפלגויות נורמליות לעתים קרובות מעט מוגזמת, אבל במידה שהן עולות, זה בגלל משפט הגבול המרכזי, אבל זה יהיה רמאות לומר שמשפט הגבול המרכזי מרמז על תוצאה זו מכיוון החישוב הזה שעשינו זה עתה הוא הסיבה שהפונקציה שבלב משפט הגבול המרכזי היא גאוס מלכתחילה, ולא פונקציה אחרת כלשהי.",
  "input": "First of all, the supposed ubiquity of normal distributions is often a little exaggerated, but to the extent that they do come up, it is because of the central limit theorem, but it would be cheating to say the central limit theorem implies this result because this computation we just did is the reason that the function at the heart of the central limit theorem is a Gaussian in the first place, and not some other function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.18,
  "end": 597.06
 },
 {
  "translatedText": "דיברנו הכל על משפט הגבול המרכזי בעבר, אבל בעצם הוא אומר שאם אתה מוסיף שוב ושוב עותקים של משתנה אקראי לעצמו, שנראה מתמטית כמו חישוב חוזר של פיתולים מול התפלגות נתונה, אז לאחר הסטה ושינוי קנה מידה מתאימים, הנטייה היא תמיד להתקרב להתפלגות נורמלית.",
  "input": "We've talked all about the central limit theorem before, but essentially it says if you repeatedly add copies of a random variable to itself, which mathematically looks like repeatedly computing convolutions against a given distribution, then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.06,
  "end": 616.5
 },
 {
  "translatedText": "מבחינה טכנית, יש הנחה קטנה שהתפלגות שאתה מתחיל איתה לא יכולה להיות בעלת שונות אינסופית, אבל זו הנחה יחסית רכה.",
  "input": "Technically, there's a small assumption the distribution you start with can't have infinite variance, but it's a relatively soft assumption.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 616.98,
  "end": 623.22
 },
 {
  "translatedText": "הקסם הוא שעבור קטגוריה ענקית של התפלגויות ראשוניות, התהליך הזה של הוספת חבורה שלמה של משתנים אקראיים הנלקחים מאותה התפלגות נוטה תמיד לצורה אוניברסלית אחת זו, גאוס.",
  "input": "The magic is that for a huge category of initial distributions, this process of adding a whole bunch of random variables drawn from that distribution always tends towards this one universal shape, a Gaussian.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.22,
  "end": 635.1
 },
 {
  "translatedText": "גישה נפוצה אחת להוכחת המשפט הזה כוללת שני שלבים נפרדים.",
  "input": "One common approach to proving this theorem involves two separate steps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.82,
  "end": 639.3
 },
 {
  "translatedText": "הצעד הראשון הוא להראות שלכל התפלגויות השונות הסופיות השונות שאתה עשוי להתחיל איתן, קיימת צורה אוניברסלית אחת שאליה נוטה תהליך זה של פיתולים חוזרים ונשנים.",
  "input": "The first step is to show that for all the different finite variance distributions you might start with, there exists a single universal shape that this process of repeated convolutions tends towards.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.6,
  "end": 650.0
 },
 {
  "translatedText": "השלב הזה הוא למעשה די טכני, הוא קצת מעבר למה שאני רוצה לדבר עליו כאן.",
  "input": "This step is actually pretty technical, it goes a little beyond what I want to talk about here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.0,
  "end": 654.24
 },
 {
  "translatedText": "לעתים קרובות אתה משתמש באובייקטים האלה הנקראים פונקציות מחוללות רגעים שנותנים לך טיעון מופשט מאוד שחייבת להיות איזו צורה אוניברסלית, אבל זה לא טוען שום טענה לגבי מהי הצורה המסוימת הזו, רק שהכל במשפחה הגדולה הזו נוטה לכיוון של נקודה אחת במרחב ההפצות.",
  "input": "You often use these objects called moment generating functions that gives you a very abstract argument that there must be some universal shape, but it doesn't make any claim about what that particular shape is, just that everything in this big family is tending towards a single point in the space of distributions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.52,
  "end": 669.98
 },
 {
  "translatedText": "אז שלב מספר שני הוא מה שהראינו זה עתה בסרטון הזה, הוכיחו שהקונבולולוציה של שני גאוסים נותנת גאוס נוסף.",
  "input": "So then step number two is what we just showed in this video, prove that the convolution of two Gaussians gives another Gaussian.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 670.62,
  "end": 677.4
 },
 {
  "translatedText": "מה שזה אומר הוא שכאשר אתה מיישם את התהליך הזה של פיתולים חוזרים ונשנים, גאוס לא משתנה, זו נקודה קבועה, כך שהדבר היחיד שהוא יכול לגשת אליו הוא עצמו, ומכיוון שהוא חבר אחד במשפחה הגדולה הזו של הפצות, כל שחייב להיות נוטה לצורה אוניברסלית אחת, היא חייבת להיות אותה צורה אוניברסלית.",
  "input": "What that means is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a fixed point, so the only thing it can approach is itself, and since it's one member in this big family of distributions, all of which must be tending towards a single universal shape, it must be that universal shape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.4,
  "end": 695.06
 },
 {
  "translatedText": "ציינתי בהתחלה איך החישוב הזה, שלב שני, הוא משהו שאתה יכול לעשות ישירות, רק באופן סמלי עם ההגדרות, אבל אחת הסיבות שאני כל כך מוקסם מטיעון גיאומטרי שממנף את הסימטריה הסיבובית של הגרף הזה היא ש זה מתחבר ישירות לכמה דברים שדיברנו עליהם בערוץ הזה בעבר, למשל, הגזירה של הרשל-מקסוול של גאוס, שבעצם אומרת שאתה יכול לראות את הסימטריה הסיבובית הזו כמאפיין המגדיר של ההתפלגות, שהיא נועלת אותך ב-e הזה לצורה השלילית בריבוע x, וגם כבונוס נוסף, זה מתחבר להוכחה הקלאסית מדוע pi מופיע בנוסחה, כלומר יש לנו כעת קו ישיר בין הנוכחות והמסתורין של אותו pi ו משפט הגבול המרכזי.",
  "input": "I mentioned at the start how this calculation, step two, is something that you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed by a geometric argument that leverages the rotational symmetry of this graph is that it directly connects to a few things that we've talked about on this channel before, for example, the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this rotational symmetry as the defining feature of the distribution, that it locks you into this e to the negative x squared form, and also as an added bonus, it connects to the classic proof for why pi shows up in the formula, meaning we now have a direct line between the presence and mystery of that pi and the central limit theorem.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.58,
  "end": 736.5
 },
 {
  "translatedText": "כמו כן, בפוסט האחרון של Patreon, תומך הערוץ דקשה וייד-קווינטר הפנה את תשומת לבי לגישה אחרת לגמרי שלא ראיתי קודם לכן, שממנפת את השימוש באנטרופיה, ושוב, לסקרנים תיאורטית שביניכם, אעשה זאת. השאר כמה קישורים בתיאור.",
  "input": "Also, on a recent Patreon post, the channel supporter Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before, which leverages the use of entropy, and again, for the theoretically curious among you, I'll leave some links in the description.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 737.06,
  "end": 749.58
 },
 {
  "translatedText": "אגב, אם אתם רוצים להישאר מעודכנים בסרטונים חדשים, וגם בכל פרויקט אחר שהוצאתי שם, כמו תערוכת קיץ של מתמטיקה, יש רשימת תפוצה.",
  "input": "By the way, if you want to stay up to date with new videos, and also any other projects that I put out there, like the Summer of Math Exposition, there is a mailing list.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 750.96,
  "end": 758.4
 },
 {
  "translatedText": "זה חדש יחסית, ואני די חוסך בלפרסם רק מה שאני חושב שאנשים ייהנו.",
  "input": "It's relatively new, and I'm pretty sparing about only posting what I think people will enjoy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 758.72,
  "end": 762.78
 },
 {
  "translatedText": "בדרך כלל אני משתדל לא להיות יותר מדי קידום מכירות בסוף הסרטונים בימים אלה, אבל אם אתה מעוניין לעקוב אחר העבודה שאני עושה, זו כנראה אחת הדרכים המתמשכות לעשות זאת.",
  "input": "Usually I try not to be too promotional at the end of videos these days, but if you are interested in following the work that I do, this is probably one of the most enduring ways to do so.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 763.22,
  "end": 795.26
 }
]