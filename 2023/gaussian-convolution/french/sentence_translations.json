[
 {
  "input": "The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.",
  "translatedText": "La fonction de base sous-jacente à une distribution normale, également appelée gaussienne, est e au carré de x négatif.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.12
 },
 {
  "input": "But you might wonder, why this function?",
  "translatedText": "Mais vous vous demandez peut-être pourquoi cette fonction?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.64,
  "end": 8.34
 },
 {
  "input": "Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle, why is it that the theory of probability seems to have a special place in its heart for this particular expression?",
  "translatedText": "De toutes les expressions que nous pourrions imaginer et qui donnent un graphique lisse symétrique avec une masse concentrée vers le milieu, pourquoi la théorie des probabilités semble-t-elle avoir une place particulière en son cœur pour cette expression particulière?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.72,
  "end": 20.44
 },
 {
  "input": "For the last many videos I've been hinting at an answer to this question, and here we'll finally arrive at something like a satisfying answer.",
  "translatedText": "Au cours des dernières vidéos, j'ai fait allusion à une réponse à cette question, et ici nous arriverons enfin à quelque chose qui ressemble à une réponse satisfaisante.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.38,
  "end": 27.68
 },
 {
  "input": "As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem, which describes how as you add multiple copies of a random variable, for example rolling a weighted die many different times or letting a ball bounce off of a peg repeatedly, then the distribution describing that sum tends to look approximately like a normal distribution.",
  "translatedText": "Pour vous rappeler où nous en sommes, il y a quelques vidéos, nous avons parlé du théorème central limite, qui décrit comment, lorsque vous ajoutez plusieurs copies d'une variable aléatoire, par exemple lancer un dé pondéré plusieurs fois ou laisser une balle rebondir sur un ancrage à plusieurs reprises, alors la distribution décrivant cette somme a tendance à ressembler approximativement à une distribution normale.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.68,
  "end": 47.72
 },
 {
  "input": "What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions, that approximation to a normal becomes better and better.",
  "translatedText": "Ce que dit le théorème central limite, c'est qu'à mesure que vous augmentez cette somme, dans des conditions appropriées, cette approximation d'une normale devient de mieux en mieux.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 48.44,
  "end": 56.22
 },
 {
  "input": "But I never explained why this theorem is actually true, we only talked about what it's claiming.",
  "translatedText": "Mais je n’ai jamais expliqué pourquoi ce théorème est réellement vrai, nous avons seulement parlé de ce qu’il prétend.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.94,
  "end": 61.98
 },
 {
  "input": "In the last video we started talking about the math involved in adding two random variables.",
  "translatedText": "Dans la dernière vidéo, nous avons commencé à parler des mathématiques impliquées dans l'addition de deux variables aléatoires.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.08,
  "end": 67.88
 },
 {
  "input": "If you have two random variables, each following some distribution, then to find the distribution describing the sum of those variables, you compute something known as a convolution between the two original functions.",
  "translatedText": "Si vous avez deux variables aléatoires, chacune suivant une certaine distribution, alors pour trouver la distribution décrivant la somme de ces variables, vous calculez ce qu'on appelle une convolution entre les deux fonctions d'origine.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 68.26,
  "end": 79.7
 },
 {
  "input": "And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.",
  "translatedText": "Et nous avons passé beaucoup de temps à élaborer deux manières distinctes de visualiser ce qu’est réellement cette opération de convolution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.88,
  "end": 85.94
 },
 {
  "input": "Today our basic job is to work through a particular example, which is to ask what happens when you add two normally distributed random variables, which as you know by now is the same as asking what do you get if you compute a convolution between two Gaussian functions.",
  "translatedText": "Aujourd'hui, notre travail de base consiste à travailler sur un exemple particulier, qui consiste à demander ce qui se passe lorsque vous ajoutez deux variables aléatoires normalement distribuées, ce qui, comme vous le savez maintenant, revient à demander ce que vous obtenez si vous calculez une convolution entre deux variables gaussiennes. les fonctions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.94,
  "end": 101.78
 },
 {
  "input": "I'd like to share an especially pleasing visual way that you can think about this calculation, which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.",
  "translatedText": "J'aimerais partager une manière visuelle particulièrement agréable de réfléchir à ce calcul, qui, espérons-le, donne une idée de ce qui rend le e de la fonction x au carré négatif spécial en premier lieu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.52,
  "end": 112.36
 },
 {
  "input": "After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.",
  "translatedText": "Après l'avoir parcouru, nous expliquerons en quoi ce calcul est l'une des étapes impliquées dans la preuve du théorème central limite.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.36,
  "end": 118.24
 },
 {
  "input": "It's the step that answers the question of why a Gaussian and not something else is the central limit.",
  "translatedText": "C'est l'étape qui répond à la question de savoir pourquoi une gaussienne et non autre chose est la limite centrale.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.32,
  "end": 123.56
 },
 {
  "input": "But first, let's dive in.",
  "translatedText": "Mais d’abord, plongeons-y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.2,
  "end": 125.84
 },
 {
  "input": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
  "translatedText": "La formule complète d’une gaussienne est plus compliquée que simplement e au carré négatif de x.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.78,
  "end": 134.44
 },
 {
  "input": "The exponent is typically written as negative one half times x divided by sigma squared, where sigma describes the spread of the distribution, specifically the standard deviation.",
  "translatedText": "L'exposant est généralement écrit sous la forme moins une moitié de x divisé par sigma au carré, où sigma décrit l'étendue de la distribution, en particulier l'écart type.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.82,
  "end": 144.2
 },
 {
  "input": "All of this needs to be multiplied by a fraction on the front, which is there to make sure that the area under the curve is one, making it a valid probability distribution.",
  "translatedText": "Tout cela doit être multiplié par une fraction sur le devant, qui est là pour garantir que l'aire sous la courbe est une, ce qui en fait une distribution de probabilité valide.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.68,
  "end": 153.42
 },
 {
  "input": "And if you want to consider distributions that aren't necessarily centered at zero, you would also throw another parameter, mu, into the exponent like this.",
  "translatedText": "Et si vous souhaitez considérer des distributions qui ne sont pas nécessairement centrées sur zéro, vous devez également ajouter un autre paramètre, mu, dans l'exposant comme celui-ci.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 154.02,
  "end": 161.18
 },
 {
  "input": "Although for everything we'll be doing here, we just consider centered distributions.",
  "translatedText": "Bien que pour tout ce que nous allons faire ici, nous considérons uniquement les distributions centrées.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.54,
  "end": 165.12
 },
 {
  "input": "Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions, the direct way to do this would be to take the definition of a convolution, this integral expression we built up last video, and then to plug in for each one of the functions involved the formula for a Gaussian.",
  "translatedText": "Maintenant, si vous regardez notre objectif principal d'aujourd'hui, qui est de calculer une convolution entre deux fonctions gaussiennes, la manière directe d'y parvenir serait de prendre la définition d'une convolution, cette expression intégrale que nous avons construite dans la dernière vidéo, puis de branchez pour chacune des fonctions impliquées la formule d'une gaussienne.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.8,
  "end": 183.76
 },
 {
  "input": "It's kind of a lot of symbols when you throw it all together, but more than anything, working this out is an exercise in completing the square.",
  "translatedText": "Cela fait en quelque sorte beaucoup de symboles lorsque vous mélangez tout cela, mais plus que tout, travailler sur cela est un exercice pour compléter le carré.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.22,
  "end": 190.08
 },
 {
  "input": "And there's nothing wrong with that.",
  "translatedText": "Et il n’y a rien de mal à cela.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.56,
  "end": 191.58
 },
 {
  "input": "That will get you the answer that you want.",
  "translatedText": "Cela vous donnera la réponse que vous souhaitez.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.72,
  "end": 193.22
 },
 {
  "input": "But of course, you know me, I'm a sucker for visual intuition, and in this case, there's another way to think about it that I haven't seen written about before, that offers a very nice connection to other aspects of this distribution, like the presence of pi and certain ways to derive where it comes from.",
  "translatedText": "Mais bien sûr, vous me connaissez, je suis un adepte de l'intuition visuelle, et dans ce cas, il y a une autre façon d'y penser que je n'ai jamais vu écrit auparavant, qui offre une très belle connexion avec d'autres aspects de cela. distribution, comme la présence de pi et certaines façons de déterminer d'où il vient.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.76,
  "end": 207.86
 },
 {
  "input": "And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution, and just showing the computation for the simplified form, e to the negative x squared.",
  "translatedText": "Et la façon dont j'aimerais procéder est d'abord de supprimer toutes les constantes associées à la distribution réelle et de simplement montrer le calcul pour la forme simplifiée, e au carré négatif de x.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 208.2,
  "end": 217.96
 },
 {
  "input": "The essence of what we want to compute is what the convolution between two copies of this function looks like.",
  "translatedText": "L’essence de ce que nous voulons calculer est à quoi ressemble la convolution entre deux copies de cette fonction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.96,
  "end": 224.08
 },
 {
  "input": "If you'll remember, in the last video we had two different ways to visualize convolutions, and the one we'll be using here is the second one, involving diagonal slices.",
  "translatedText": "Si vous vous en souvenez, dans la dernière vidéo, nous avions deux manières différentes de visualiser les convolutions, et celle que nous utiliserons ici est la seconde, impliquant des tranches diagonales.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.46,
  "end": 232.92
 },
 {
  "input": "And as a quick reminder of the way that worked, if you have two different distributions that are described by two different functions, f and g, then every possible pair of values that you might get when you sample from these two distributions can be thought of as individual points on the xy-plane.",
  "translatedText": "Et pour rappel rapide de la façon dont cela a fonctionné, si vous avez deux distributions différentes décrites par deux fonctions différentes, f et g, alors toutes les paires possibles de valeurs que vous pourriez obtenir lorsque vous échantillonnez à partir de ces deux distributions peuvent être considérées. comme points individuels sur le plan xy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 233.28,
  "end": 249.56
 },
 {
  "input": "And the probability density of landing on one such point, assuming independence, looks like f of x times g of y.",
  "translatedText": "Et la densité de probabilité d’atterrir sur un de ces points, en supposant l’indépendance, ressemble à f de x fois g de y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.36,
  "end": 257.52
 },
 {
  "input": "So what we do is we look at a graph of that expression as a two-variable function of x and y, which is a way of showing the distribution of all possible outcomes when we sample from the two different variables.",
  "translatedText": "Nous examinons donc un graphique de cette expression comme une fonction à deux variables de x et y, ce qui est une façon de montrer la distribution de tous les résultats possibles lorsque nous échantillonnons à partir de deux variables différentes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.0,
  "end": 269.62
 },
 {
  "input": "To interpret the convolution of f and g evaluated on some input s, which is a way of saying how likely are you to get a pair of samples that adds up to this sum s, what you do is you look at a slice of this graph over the line x plus y equals s, and you consider the area under that slice.",
  "translatedText": "Pour interpréter la convolution de f et g évaluées sur certaines entrées s, ce qui est une façon de dire quelle est la probabilité que vous obteniez une paire d'échantillons qui totalisent cette somme s, vous regardez une tranche de ce graphique sur la ligne x plus y est égal à s, et vous considérez la zone sous cette tranche.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.56,
  "end": 289.3
 },
 {
  "input": "This area is almost, but not quite, the value of the convolution at s.",
  "translatedText": "Cette zone correspond presque, mais pas tout à fait, à la valeur de la convolution à s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.1,
  "end": 296.32
 },
 {
  "input": "For a mildly technical reason, you need to divide by the square root of two.",
  "translatedText": "Pour une raison légèrement technique, vous devez diviser par la racine carrée de deux.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.8,
  "end": 300.16
 },
 {
  "input": "Still, this area is the key feature to focus on.",
  "translatedText": "Pourtant, ce domaine est l’élément clé sur lequel se concentrer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.84,
  "end": 303.44
 },
 {
  "input": "You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum.",
  "translatedText": "Vous pouvez y voir un moyen de combiner toutes les densités de probabilité pour tous les résultats correspondant à une somme donnée.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.44,
  "end": 311.04
 },
 {
  "input": "In the specific case where these two functions look like e to the negative x squared and e to the negative y squared, the resulting 3D graph has a really nice property that you can exploit.",
  "translatedText": "Dans le cas spécifique où ces deux fonctions ressemblent à e au carré de x négatif et à e au carré de y négatif, le graphe 3D résultant possède une propriété très intéressante que vous pouvez exploiter.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.3,
  "end": 323.5
 },
 {
  "input": "It's rotationally symmetric.",
  "translatedText": "C'est symétrique en rotation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.72,
  "end": 325.68
 },
 {
  "input": "You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared, and this term describes the square of the distance between any point on the xy plane and the origin.",
  "translatedText": "Vous pouvez le voir en combinant les termes et en remarquant que c'est entièrement une fonction de x au carré plus y au carré, et ce terme décrit le carré de la distance entre n'importe quel point du plan xy et l'origine.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.88,
  "end": 338.46
 },
 {
  "input": "So in other words, the expression is purely a function of the distance from the origin.",
  "translatedText": "En d’autres termes, l’expression est purement fonction de la distance par rapport à l’origine.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 339.2,
  "end": 343.16
 },
 {
  "input": "And by the way, this would not be true for any other distribution.",
  "translatedText": "Et d’ailleurs, cela ne serait vrai pour aucune autre distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.56,
  "end": 347.92
 },
 {
  "input": "It's a property that uniquely characterizes bell curves.",
  "translatedText": "C'est une propriété qui caractérise de manière unique les courbes en cloche.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 351.28
 },
 {
  "input": "So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about, and honestly calculating the area would just amount to computing the original integral that defines a convolution in the first place.",
  "translatedText": "Ainsi, pour la plupart des autres paires de fonctions, ces tranches diagonales auront une forme compliquée à laquelle il est difficile de penser, et honnêtement, calculer l'aire reviendrait simplement à calculer l'intégrale d'origine qui définit une convolution en premier lieu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.16,
  "end": 365.54
 },
 {
  "input": "So in most cases, the visual intuition doesn't really buy you anything.",
  "translatedText": "Ainsi, dans la plupart des cas, l’intuition visuelle ne vous rapporte rien.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.94,
  "end": 369.36
 },
 {
  "input": "But in the case of bell curves, you can leverage that rotational symmetry.",
  "translatedText": "Mais dans le cas des courbes en cloche, vous pouvez tirer parti de cette symétrie de rotation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.36,
  "end": 373.92
 },
 {
  "input": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
  "translatedText": "Ici, concentrez-vous sur l'une de ces tranches sur la ligne x plus y est égal à s pour une certaine valeur de s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.8,
  "end": 380.48
 },
 {
  "input": "And remember, the convolution that we're trying to compute is a function of s.",
  "translatedText": "Et rappelez-vous, la convolution que nous essayons de calculer est fonction de s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.3,
  "end": 385.84
 },
 {
  "input": "The thing that you want is an expression of s that tells you the area under this slice.",
  "translatedText": "Ce que vous voulez, c'est une expression de s qui vous indique la zone située sous cette tranche.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.84,
  "end": 391.1
 },
 {
  "input": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s.",
  "translatedText": "Eh bien, si vous regardez cette ligne, elle coupe l’axe des x à s zéro et l’axe des y à zéro s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.7,
  "end": 397.9
 },
 {
  "input": "And a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two.",
  "translatedText": "Et un peu de Pythagore vous montrera que la distance en ligne droite entre l'origine et cette ligne est s divisée par la racine carrée de deux.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.32,
  "end": 405.32
 },
 {
  "input": "Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees, where you'd find something parallel to the y-axis the same distance away from the origin.",
  "translatedText": "Maintenant, en raison de la symétrie, cette tranche est identique à celle que vous obtenez en rotation de 45 degrés, où vous trouveriez quelque chose de parallèle à l'axe y à la même distance de l'origine.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 416.36
 },
 {
  "input": "The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions, because it only involves taking an integral with respect to y.",
  "translatedText": "La clé est que calculer cette autre zone d’une tranche parallèle à l’axe y est beaucoup, beaucoup plus facile que les tranches dans d’autres directions, car cela implique uniquement de prendre une intégrale par rapport à y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.64,
  "end": 428.26
 },
 {
  "input": "The value of x on this slice is a constant.",
  "translatedText": "La valeur de x sur cette tranche est une constante.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.74,
  "end": 431.44
 },
 {
  "input": "Specifically, it would be the constant s divided by the square root of two.",
  "translatedText": "Plus précisément, ce serait la constante s divisée par la racine carrée de deux.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 434.76
 },
 {
  "input": "So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out.",
  "translatedText": "Ainsi, lorsque vous calculez l'intégrale et trouvez cette aire, tout ce terme se comporte ici comme s'il s'agissait simplement d'un nombre, et vous pouvez le factoriser.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.76,
  "end": 443.38
 },
 {
  "input": "This is the important point.",
  "translatedText": "C'est le point important.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.88,
  "end": 444.94
 },
 {
  "input": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
  "translatedText": "Tout ce qui implique s est désormais entièrement séparé de la variable intégrée.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.28,
  "end": 450.2
 },
 {
  "input": "This remaining integral is a little bit tricky.",
  "translatedText": "Cette intégrale restante est un peu délicate.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.82,
  "end": 453.0
 },
 {
  "input": "I did a whole video on it, it's actually quite famous.",
  "translatedText": "J'ai fait une vidéo entière dessus, c'est en fait assez célèbre.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.08,
  "end": 455.2
 },
 {
  "input": "But you almost don't really care.",
  "translatedText": "Mais vous ne vous en souciez presque pas vraiment.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.5,
  "end": 456.9
 },
 {
  "input": "The point is that it's just some number.",
  "translatedText": "Le fait est que ce n'est qu'un chiffre.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.24,
  "end": 459.0
 },
 {
  "input": "That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s.",
  "translatedText": "Ce nombre se trouve être la racine carrée de pi, mais ce qui compte vraiment, c'est qu'il ne dépend pas de s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.0,
  "end": 465.48
 },
 {
  "input": "And essentially, this is our answer.",
  "translatedText": "Et essentiellement, voici notre réponse.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.88,
  "end": 468.48
 },
 {
  "input": "We were looking for an expression for the area of these slices as a function of s, and now we have it.",
  "translatedText": "Nous cherchions une expression pour l’aire de ces tranches en fonction de s, et maintenant nous l’avons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.78,
  "end": 474.28
 },
 {
  "input": "It looks like e to the negative s squared divided by two, scaled by some constant.",
  "translatedText": "Cela ressemble à e au carré négatif s divisé par deux, mis à l'échelle par une constante.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.38,
  "end": 478.84
 },
 {
  "input": "In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent.",
  "translatedText": "En d’autres termes, c’est aussi une courbe en cloche, une autre gaussienne, juste un peu allongée à cause de ces deux dans l’exposant.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.3,
  "end": 485.62
 },
 {
  "input": "As I said earlier, the convolution evaluated at s is not quite this area.",
  "translatedText": "Comme je l'ai dit plus tôt, la convolution évaluée à s n'est pas tout à fait cette zone.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.62,
  "end": 490.86
 },
 {
  "input": "Technically, it's this area divided by the square root of two.",
  "translatedText": "Techniquement, c'est cette surface divisée par la racine carrée de deux.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.34,
  "end": 494.16
 },
 {
  "input": "We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant.",
  "translatedText": "Nous en avons parlé dans la dernière vidéo, mais cela n'a pas vraiment d'importance car cela s'intègre simplement dans la constante.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.8,
  "end": 499.24
 },
 {
  "input": "What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian.",
  "translatedText": "Ce qui compte vraiment, c'est la conclusion selon laquelle une convolution entre deux gaussiennes est elle-même une autre gaussienne.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.68,
  "end": 505.68
 },
 {
  "input": "If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma, essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front, and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma.",
  "translatedText": "Si vous deviez revenir en arrière et réintroduire toutes les constantes d'une distribution normale avec un zéro moyen et un sigma d'écart type arbitraire, un raisonnement essentiellement identique conduirait à la même racine carrée de deux facteurs qui apparaît dans l'exposant et à l'avant, et cela conduit à la conclusion que la convolution entre deux de ces distributions normales est une autre distribution normale avec un écart type racine carrée de deux fois sigma.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.56,
  "end": 530.38
 },
 {
  "input": "If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result.",
  "translatedText": "Si vous n'avez pas calculé beaucoup de convolutions auparavant, il convient de souligner qu'il s'agit d'un résultat très spécial.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.98,
  "end": 536.06
 },
 {
  "input": "Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process.",
  "translatedText": "On se retrouve presque toujours avec un type de fonction complètement différent, mais ici, il y a une sorte de stabilité dans le processus.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.38,
  "end": 542.5
 },
 {
  "input": "Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations.",
  "translatedText": "De plus, pour ceux d'entre vous qui aiment les exercices, j'en laisserai un à l'écran expliquant comment vous géreriez le cas de deux écarts types différents.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.26,
  "end": 549.44
 },
 {
  "input": "Still, some of you might be raising your hands and saying, what's the big deal?",
  "translatedText": "Pourtant, certains d’entre vous lèvent peut-être la main et demandent : quel est le problème?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.42,
  "end": 553.94
 },
 {
  "input": "I mean, when you first heard the question, what do you get when you add two normally distributed random variables, you probably even guessed that the answer should be another normally distributed variable.",
  "translatedText": "Je veux dire, lorsque vous avez entendu la question pour la première fois, qu'obtenez-vous lorsque vous ajoutez deux variables aléatoires normalement distribuées, vous avez probablement même deviné que la réponse devrait être une autre variable normalement distribuée.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.48,
  "end": 564.32
 },
 {
  "input": "After all, what else is it going to be?",
  "translatedText": "Après tout, qu’est-ce que ça va être d’autre?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.76,
  "end": 566.36
 },
 {
  "input": "Normal distributions are supposedly quite common, so why not?",
  "translatedText": "Les distributions normales sont censées être assez courantes, alors pourquoi pas?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.86,
  "end": 570.24
 },
 {
  "input": "You could even say that this should follow from the central limit theorem, but that would have it all backwards.",
  "translatedText": "On pourrait même dire que cela devrait découler du théorème central limite, mais ce serait tout inverser.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 570.24,
  "end": 575.48
 },
 {
  "input": "First of all, the supposed ubiquity of normal distributions is often a little exaggerated, but to the extent that they do come up, it is because of the central limit theorem, but it would be cheating to say the central limit theorem implies this result because this computation we just did is the reason that the function at the heart of the central limit theorem is a Gaussian in the first place and not some other function.",
  "translatedText": "Tout d’abord, l’omniprésence supposée des distributions normales est souvent un peu exagérée, mais dans la mesure où elles apparaissent, c’est à cause du théorème central limite, mais il serait tricheur de dire que le théorème central limite implique ce résultat parce que ce calcul que nous venons de faire est la raison pour laquelle la fonction au cœur du théorème central limite est en premier lieu une gaussienne et non une autre fonction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.18,
  "end": 597.06
 },
 {
  "input": "We've talked all about the central limit theorem before, but essentially it says if you repeatedly add copies of a random variable to itself, which mathematically looks like repeatedly computing convolutions against a given distribution, then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution.",
  "translatedText": "Nous avons déjà parlé du théorème central limite, mais il dit essentiellement que si vous ajoutez à plusieurs reprises des copies d'une variable aléatoire à elle-même, ce qui ressemble mathématiquement à un calcul répété de convolutions par rapport à une distribution donnée, alors après un décalage et une mise à l'échelle appropriés, la tendance est toujours pour se rapprocher d'une distribution normale.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.06,
  "end": 616.5
 },
 {
  "input": "Technically there's a small assumption the distribution you start with can't have infinite variance, but it's a relatively soft assumption.",
  "translatedText": "Techniquement, il existe une petite hypothèse selon laquelle la distribution avec laquelle vous commencez ne peut pas avoir une variance infinie, mais c'est une hypothèse relativement souple.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 616.98,
  "end": 623.22
 },
 {
  "input": "The magic is that for a huge category of initial distributions, this process of adding a whole bunch of random variables drawn from that distribution always tends towards this one universal shape, a Gaussian.",
  "translatedText": "La magie est que pour une énorme catégorie de distributions initiales, ce processus d'ajout de tout un tas de variables aléatoires tirées de cette distribution tend toujours vers cette forme universelle unique, une gaussienne.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.22,
  "end": 635.1
 },
 {
  "input": "One common approach to proving this theorem involves two separate steps.",
  "translatedText": "Une approche courante pour prouver ce théorème implique deux étapes distinctes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.82,
  "end": 639.3
 },
 {
  "input": "The first step is to show that for all the different finite variance distributions you might start with, there exists a single universal shape that this process of repeated convolutions tends towards.",
  "translatedText": "La première étape consiste à montrer que pour toutes les différentes distributions de variance finie avec lesquelles vous pourriez commencer, il existe une seule forme universelle vers laquelle tend ce processus de convolutions répétées.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.6,
  "end": 650.0
 },
 {
  "input": "This step is actually pretty technical, it goes a little beyond what I want to talk about here.",
  "translatedText": "Cette étape est en fait assez technique, elle va un peu au-delà de ce dont je souhaite parler ici.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.0,
  "end": 654.24
 },
 {
  "input": "You often use these objects called moment generating functions that gives you a very abstract argument that there must be some universal shape, but it doesn't make any claim about what that particular shape is, just that everything in this big family is tending towards a single point in the space of distributions.",
  "translatedText": "Vous utilisez souvent ces objets appelés fonctions génératrices de moments qui vous donnent un argument très abstrait selon lequel il doit y avoir une forme universelle, mais cela ne prétend pas quelle est cette forme particulière, juste que tout dans cette grande famille tend vers une forme universelle. point unique dans l’espace des distributions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.52,
  "end": 669.98
 },
 {
  "input": "So then step number two is what we just showed in this video, prove that the convolution of two Gaussians gives another Gaussian.",
  "translatedText": "Alors la deuxième étape est ce que nous venons de montrer dans cette vidéo, prouver que la convolution de deux gaussiennes donne une autre gaussienne.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 670.62,
  "end": 677.4
 },
 {
  "input": "What that means is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a fixed point.",
  "translatedText": "Cela signifie que lorsque vous appliquez ce processus de convolutions répétées, une gaussienne ne change pas, c'est un point fixe.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.4,
  "end": 684.06
 },
 {
  "input": "So the only thing it can approach is itself, and since it's one member in this big family of distributions, all of which must be tending towards a single universal shape, it must be that universal shape.",
  "translatedText": "Donc la seule chose qu'il peut approcher, c'est lui-même, et comme il s'agit d'un membre de cette grande famille de distributions, qui doivent toutes tendre vers une seule forme universelle, ce doit être cette forme universelle.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 684.2,
  "end": 695.06
 },
 {
  "input": "I mentioned at the start how this calculation, step two, is something that you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed by a geometric argument that leverages the rotational symmetry of this graph is that it directly connects to a few things that we've talked about on this channel before.",
  "translatedText": "J'ai mentionné au début que ce calcul, la deuxième étape, est quelque chose que vous pouvez faire directement, juste symboliquement avec les définitions, mais l'une des raisons pour lesquelles je suis si charmé par un argument géométrique qui exploite la symétrie de rotation de ce graphique est que cela se connecte directement à quelques éléments dont nous avons déjà parlé sur cette chaîne.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.58,
  "end": 712.3
 },
 {
  "input": "For example, the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this rotational symmetry as the defining feature of the distribution, that it locks you into this e to the negative x squared form, and also as an added bonus it connects to the classic proof for why pi shows up in the formula, meaning we now have a direct line between the presence and mystery of that pi and the central limit theorem.",
  "translatedText": "Par exemple, la dérivation Herschel-Maxwell d'une gaussienne, qui dit essentiellement que vous pouvez considérer cette symétrie de rotation comme la caractéristique déterminante de la distribution, qu'elle vous enferme dans ce e à la forme x carrée négative, et aussi comme un bonus supplémentaire cela se connecte à la preuve classique expliquant pourquoi pi apparaît dans la formule, ce qui signifie que nous avons maintenant une ligne directe entre la présence et le mystère de ce pi et le théorème central limite.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 712.4,
  "end": 736.5
 },
 {
  "input": "Also on a recent Patreon post, the channel supporter Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before, which leverages the use of entropy, and again for the theoretically curious among you I'll leave some links in the description.",
  "translatedText": "Également dans un article récent sur Patreon, Daksha Vaid-Quinter, partisan de la chaîne, a attiré mon attention sur une approche complètement différente que je n'avais jamais vue auparavant, qui exploite l'utilisation de l'entropie, et encore une fois, pour les curieux d'entre vous en théorie, je laisserai quelques liens. dans le descriptif.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 737.06,
  "end": 749.58
 },
 {
  "input": "By the way, if you want to stay up to date with new videos and also any other projects that I put out there like the Summer of Math Exposition, there is a mailing list.",
  "translatedText": "À propos, si vous souhaitez rester au courant des nouvelles vidéos et de tout autre projet que je propose comme l'exposition Summer of Math, il existe une liste de diffusion.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 750.96,
  "end": 758.4
 },
 {
  "input": "It's relatively new and I'm pretty sparing about only posting what I think people will enjoy.",
  "translatedText": "C'est relativement nouveau et j'hésite à publier uniquement ce que je pense que les gens apprécieront.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 758.72,
  "end": 762.78
 },
 {
  "input": "Usually I try not to be too promotional at the end of videos these days, but if you are interested in following the work that I do, this is probably one of the most enduring ways to do so.",
  "translatedText": "Habituellement, j'essaie de ne pas être trop promotionnel à la fin des vidéos ces jours-ci, mais si vous êtes intéressé à suivre le travail que je fais, c'est probablement l'une des façons les plus durables de le faire.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 763.22,
  "end": 795.26
 }
]