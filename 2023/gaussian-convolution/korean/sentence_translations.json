[
 {
  "input": "The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.",
  "translatedText": "정규 분포(가우스라고도 함)의 기본 함수는 e의 음수 x 제곱입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.12
 },
 {
  "input": "But you might wonder, why this function?",
  "translatedText": "하지만 왜 이 기능이 사용되는지 궁금할 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.64,
  "end": 8.34
 },
 {
  "input": "Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle, why is it that the theory of probability seems to have a special place in its heart for this particular expression?",
  "translatedText": "질량이 중앙에 집중된 대칭형 매끄러운 그래프를 제공하는 우리가 꿈꿀 수 있는 모든 표현 중에서 확률 이론이 이 특정 표현의 중심에 특별한 위치를 차지하는 것처럼 보이는 이유는 무엇입니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.72,
  "end": 20.44
 },
 {
  "input": "For the last many videos I've been hinting at an answer to this question, and here we'll finally arrive at something like a satisfying answer.",
  "translatedText": "지난 많은 영상에서 저는 이 질문에 대한 답을 암시해왔고, 여기서는 마침내 만족스러운 답에 도달하게 될 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.38,
  "end": 27.68
 },
 {
  "input": "As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem, which describes how as you add multiple copies of a random variable, for example rolling a weighted die many different times or letting a ball bounce off of a peg repeatedly, then the distribution describing that sum tends to look approximately like a normal distribution.",
  "translatedText": "우리가 현재 어디에 있는지 다시 한번 상기시켜 드리기 위해 몇 편의 비디오에서 우리는 무작위 변수의 여러 복사본을 추가하는 방법을 설명하는 중심 극한 정리에 대해 이야기했습니다. 예를 들어 가중치가 있는 주사위를 여러 번 굴리거나 공이 튕겨 나가게 하는 등의 방법을 설명합니다. 반복적으로 페그를 사용하면 해당 합계를 설명하는 분포가 대략 정규 분포와 비슷하게 보이는 경향이 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.68,
  "end": 47.72
 },
 {
  "input": "What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions, that approximation to a normal becomes better and better.",
  "translatedText": "중심 극한 정리가 말하는 것은 적절한 조건 하에서 그 합을 점점 더 크게 만들면 법선에 대한 근사가 점점 더 좋아진다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 48.44,
  "end": 56.22
 },
 {
  "input": "But I never explained why this theorem is actually true, we only talked about what it's claiming.",
  "translatedText": "하지만 나는 이 정리가 실제로 왜 사실인지 설명하지 않았고, 우리는 그것이 주장하는 것에 대해서만 이야기했습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.94,
  "end": 61.98
 },
 {
  "input": "In the last video we started talking about the math involved in adding two random variables.",
  "translatedText": "지난 비디오에서 우리는 두 개의 무작위 변수를 추가하는 것과 관련된 수학에 대해 이야기하기 시작했습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.08,
  "end": 67.88
 },
 {
  "input": "If you have two random variables, each following some distribution, then to find the distribution describing the sum of those variables, you compute something known as a convolution between the two original functions.",
  "translatedText": "각각 어떤 분포를 따르는 두 개의 확률 변수가 있는 경우 해당 변수의 합을 설명하는 분포를 찾으려면 두 원래 함수 사이의 컨볼루션이라고 알려진 것을 계산합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 68.26,
  "end": 79.7
 },
 {
  "input": "And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.",
  "translatedText": "그리고 우리는 이 컨볼루션 작업이 실제로 무엇인지 시각화하기 위해 두 가지 서로 다른 방법을 구축하는 데 많은 시간을 보냈습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.88,
  "end": 85.94
 },
 {
  "input": "Today our basic job is to work through a particular example, which is to ask what happens when you add two normally distributed random variables, which as you know by now is the same as asking what do you get if you compute a convolution between two Gaussian functions.",
  "translatedText": "오늘 우리의 기본 임무는 특정 예를 통해 작업하는 것입니다. 이는 두 개의 정규 분포 확률 변수를 추가하면 어떤 일이 발생하는지 묻는 것입니다. 지금까지 알고 있듯이 이는 두 가우스 사이의 컨볼루션을 계산하면 무엇을 얻는지 묻는 것과 같습니다. 기능.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.94,
  "end": 101.78
 },
 {
  "input": "I'd like to share an especially pleasing visual way that you can think about this calculation, which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.",
  "translatedText": "저는 여러분이 이 계산에 대해 생각할 수 있는 특히 기분 좋은 시각적 방법을 공유하고 싶습니다. 이는 처음에 음수 x 제곱 함수에 대한 e를 특별하게 만드는 이유에 대한 이해를 제공하기를 바랍니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.52,
  "end": 112.36
 },
 {
  "input": "After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.",
  "translatedText": "이를 살펴본 후 이 계산이 중심 극한 정리를 증명하는 데 관련된 단계 중 하나인 방법에 대해 이야기하겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.36,
  "end": 118.24
 },
 {
  "input": "It's the step that answers the question of why a Gaussian and not something else is the central limit.",
  "translatedText": "다른 것이 아닌 가우스가 중심 극한인 이유에 대한 질문에 답하는 단계입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.32,
  "end": 123.56
 },
 {
  "input": "But first, let's dive in.",
  "translatedText": "하지만 먼저 들어가 보겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.2,
  "end": 125.84
 },
 {
  "input": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
  "translatedText": "가우시안의 전체 공식은 e의 음수 x 제곱보다 더 복잡합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.78,
  "end": 134.44
 },
 {
  "input": "The exponent is typically written as negative one half times x divided by sigma squared, where sigma describes the spread of the distribution, specifically the standard deviation.",
  "translatedText": "지수는 일반적으로 음의 1/2 곱하기 x를 시그마 제곱으로 나눈 값으로 표시됩니다. 여기서 시그마는 분포의 확산, 특히 표준 편차를 나타냅니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.82,
  "end": 144.2
 },
 {
  "input": "All of this needs to be multiplied by a fraction on the front, which is there to make sure that the area under the curve is one, making it a valid probability distribution.",
  "translatedText": "이 모든 것에 앞면의 분수를 곱해야 합니다. 이는 곡선 아래 면적이 1이 되도록 하여 유효한 확률 분포가 되도록 하기 위한 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.68,
  "end": 153.42
 },
 {
  "input": "And if you want to consider distributions that aren't necessarily centered at zero, you would also throw another parameter, mu, into the exponent like this.",
  "translatedText": "그리고 반드시 0에 중심을 두지 않는 분포를 고려하려면 이와 같이 지수에 또 다른 매개변수인 mu를 입력해야 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 154.02,
  "end": 161.18
 },
 {
  "input": "Although for everything we'll be doing here, we just consider centered distributions.",
  "translatedText": "여기서 수행할 모든 작업에서는 중심 분포만 고려합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.54,
  "end": 165.12
 },
 {
  "input": "Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions, the direct way to do this would be to take the definition of a convolution, this integral expression we built up last video, and then to plug in for each one of the functions involved the formula for a Gaussian.",
  "translatedText": "이제 두 가우스 함수 사이의 컨볼루션을 계산하는 것인 오늘의 주요 목표를 살펴보면 이를 수행하는 직접적인 방법은 지난 비디오에서 구성한 적분 표현인 컨볼루션의 정의를 취하는 것입니다. 가우스 공식과 관련된 각 함수에 연결하세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.8,
  "end": 183.76
 },
 {
  "input": "It's kind of a lot of symbols when you throw it all together, but more than anything, working this out is an exercise in completing the square.",
  "translatedText": "모두 함께 던져 보면 많은 상징이 있지만 무엇보다도 이것을 해결하는 것은 사각형을 완성하는 연습입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.22,
  "end": 190.08
 },
 {
  "input": "And there's nothing wrong with that.",
  "translatedText": "그리고 그것은 아무런 문제가 없습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.56,
  "end": 191.58
 },
 {
  "input": "That will get you the answer that you want.",
  "translatedText": "그러면 원하는 답을 얻을 수 있을 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.72,
  "end": 193.22
 },
 {
  "input": "But of course, you know me, I'm a sucker for visual intuition, and in this case, there's another way to think about it that I haven't seen written about before, that offers a very nice connection to other aspects of this distribution, like the presence of pi and certain ways to derive where it comes from.",
  "translatedText": "하지만 물론 아시다시피 저는 시각적 직관에 약합니다. 이 경우에는 이전에 글에서 본 적이 없는 또 다른 사고 방식이 있습니다. 이는 이 문제의 다른 측면과 아주 좋은 연결을 제공합니다. 파이의 존재와 파이의 출처를 도출하는 특정 방법과 같은 분포.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.76,
  "end": 207.86
 },
 {
  "input": "And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution, and just showing the computation for the simplified form, e to the negative x squared.",
  "translatedText": "제가 하고 싶은 방법은 먼저 실제 분포와 관련된 모든 상수를 벗겨내고 단순화된 형태의 e 대 음수 x 제곱에 대한 계산을 표시하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 208.2,
  "end": 217.96
 },
 {
  "input": "The essence of what we want to compute is what the convolution between two copies of this function looks like.",
  "translatedText": "우리가 계산하려는 것의 본질은 이 함수의 두 복사본 사이의 컨볼루션이 어떻게 생겼는지입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.96,
  "end": 224.08
 },
 {
  "input": "If you'll remember, in the last video we had two different ways to visualize convolutions, and the one we'll be using here is the second one, involving diagonal slices.",
  "translatedText": "기억하실지 모르겠지만, 지난 비디오에서는 컨볼루션을 시각화하는 두 가지 다른 방법이 있었고 여기서 사용할 방법은 대각선 슬라이스와 관련된 두 번째 방법입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.46,
  "end": 232.92
 },
 {
  "input": "And as a quick reminder of the way that worked, if you have two different distributions that are described by two different functions, f and g, then every possible pair of values that you might get when you sample from these two distributions can be thought of as individual points on the xy-plane.",
  "translatedText": "작동 방식을 빠르게 상기시켜 드리자면, 두 개의 서로 다른 함수 f와 g로 설명되는 두 개의 서로 다른 분포가 있는 경우 이 두 분포에서 샘플링할 때 얻을 수 있는 모든 가능한 값 쌍을 생각해 볼 수 있습니다. xy 평면의 개별 점으로.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 233.28,
  "end": 249.56
 },
 {
  "input": "And the probability density of landing on one such point, assuming independence, looks like f of x times g of y.",
  "translatedText": "그리고 독립성을 가정할 때 그러한 지점 중 하나에 착륙할 확률 밀도는 f(x) 곱하기 g(y)와 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.36,
  "end": 257.52
 },
 {
  "input": "So what we do is we look at a graph of that expression as a two-variable function of x and y, which is a way of showing the distribution of all possible outcomes when we sample from the two different variables.",
  "translatedText": "그래서 우리가 하는 일은 해당 표현식의 그래프를 x와 y의 두 변수 함수로 보는 것입니다. 이는 두 개의 서로 다른 변수에서 샘플링할 때 가능한 모든 결과의 분포를 보여주는 방법입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.0,
  "end": 269.62
 },
 {
  "input": "To interpret the convolution of f and g evaluated on some input s, which is a way of saying how likely are you to get a pair of samples that adds up to this sum s, what you do is you look at a slice of this graph over the line x plus y equals s, and you consider the area under that slice.",
  "translatedText": "일부 입력 s에 대해 평가된 f와 g의 컨볼루션을 해석하려면(이 합계 s에 합산되는 한 쌍의 샘플을 얻을 가능성이 얼마나 되는지 말하는 방법), 해야 할 일은 이 그래프의 한 조각을 보는 것입니다. 선 위의 x + y는 s와 같고 해당 조각 아래의 면적을 고려합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.56,
  "end": 289.3
 },
 {
  "input": "This area is almost, but not quite, the value of the convolution at s.",
  "translatedText": "이 영역은 s에서의 컨볼루션 값과 거의 비슷하지만 완전히 일치하지는 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.1,
  "end": 296.32
 },
 {
  "input": "For a mildly technical reason, you need to divide by the square root of two.",
  "translatedText": "약간 기술적인 이유로 인해 2의 제곱근으로 나누어야 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.8,
  "end": 300.16
 },
 {
  "input": "Still, this area is the key feature to focus on.",
  "translatedText": "그럼에도 불구하고 이 부분은 집중해야 할 핵심 기능입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.84,
  "end": 303.44
 },
 {
  "input": "You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum.",
  "translatedText": "주어진 합계에 해당하는 모든 결과에 대한 모든 확률 밀도를 함께 결합하는 방법으로 생각할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.44,
  "end": 311.04
 },
 {
  "input": "In the specific case where these two functions look like e to the negative x squared and e to the negative y squared, the resulting 3D graph has a really nice property that you can exploit.",
  "translatedText": "이 두 함수가 음수 x 제곱에 대해 e, 음수 y 제곱에 대해 e처럼 보이는 특정 경우에 결과 3D 그래프는 활용할 수 있는 정말 좋은 속성을 갖습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.3,
  "end": 323.5
 },
 {
  "input": "It's rotationally symmetric.",
  "translatedText": "회전대칭입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.72,
  "end": 325.68
 },
 {
  "input": "You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared, and this term describes the square of the distance between any point on the xy plane and the origin.",
  "translatedText": "용어를 결합하고 이것이 전적으로 x 제곱 + y 제곱의 함수임을 알 수 있으며, 이 용어는 xy 평면의 임의 지점과 원점 사이의 거리의 제곱을 나타냅니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.88,
  "end": 338.46
 },
 {
  "input": "So in other words, the expression is purely a function of the distance from the origin.",
  "translatedText": "즉, 표현은 순전히 원점으로부터의 거리의 함수입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 339.2,
  "end": 343.16
 },
 {
  "input": "And by the way, this would not be true for any other distribution.",
  "translatedText": "그건 그렇고, 이것은 다른 배포판에서는 사실이 아닙니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.56,
  "end": 347.92
 },
 {
  "input": "It's a property that uniquely characterizes bell curves.",
  "translatedText": "이는 종형 곡선을 고유하게 특징짓는 특성입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 351.28
 },
 {
  "input": "So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about, and honestly calculating the area would just amount to computing the original integral that defines a convolution in the first place.",
  "translatedText": "따라서 대부분의 다른 함수 쌍의 경우 이러한 대각선 슬라이스는 생각하기 어려운 복잡한 모양이 되며, 면적을 정직하게 계산하는 것은 처음에 컨볼루션을 정의하는 원래 적분을 계산하는 것과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.16,
  "end": 365.54
 },
 {
  "input": "So in most cases, the visual intuition doesn't really buy you anything.",
  "translatedText": "따라서 대부분의 경우 시각적 직관은 실제로 아무것도 사지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.94,
  "end": 369.36
 },
 {
  "input": "But in the case of bell curves, you can leverage that rotational symmetry.",
  "translatedText": "그러나 종형 곡선의 경우 회전 대칭을 활용할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.36,
  "end": 373.92
 },
 {
  "input": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
  "translatedText": "여기에서는 x + y가 s의 일부 값에 대해 s와 같은 선 위의 조각 중 하나에 초점을 맞춥니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.8,
  "end": 380.48
 },
 {
  "input": "And remember, the convolution that we're trying to compute is a function of s.",
  "translatedText": "그리고 우리가 계산하려는 컨볼루션은 s의 함수라는 것을 기억하세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.3,
  "end": 385.84
 },
 {
  "input": "The thing that you want is an expression of s that tells you the area under this slice.",
  "translatedText": "당신이 원하는 것은 이 조각 아래의 영역을 알려주는 s 표현식입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.84,
  "end": 391.1
 },
 {
  "input": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s.",
  "translatedText": "글쎄, 그 선을 보면 s 0에서 x축과 0 s에서 y축과 교차합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.7,
  "end": 397.9
 },
 {
  "input": "And a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two.",
  "translatedText": "그리고 약간의 피타고라스는 원점에서 이 선까지의 직선 거리가 s를 2의 제곱근으로 나눈다는 것을 보여줄 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.32,
  "end": 405.32
 },
 {
  "input": "Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees, where you'd find something parallel to the y-axis the same distance away from the origin.",
  "translatedText": "이제 대칭으로 인해 이 조각은 45도 회전하는 조각과 동일합니다. 여기서 원점에서 같은 거리만큼 떨어져 있는 y축에 평행한 것을 찾을 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 416.36
 },
 {
  "input": "The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions, because it only involves taking an integral with respect to y.",
  "translatedText": "핵심은 y축에 평행한 슬라이스의 다른 영역을 계산하는 것이 다른 방향의 슬라이스보다 훨씬 더 쉽다는 것입니다. 왜냐하면 y에 대한 적분만 취하기 때문입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.64,
  "end": 428.26
 },
 {
  "input": "The value of x on this slice is a constant.",
  "translatedText": "이 슬라이스의 x 값은 상수입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.74,
  "end": 431.44
 },
 {
  "input": "Specifically, it would be the constant s divided by the square root of two.",
  "translatedText": "구체적으로 말하면 상수 s를 2의 제곱근으로 나눈 값입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 434.76
 },
 {
  "input": "So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out.",
  "translatedText": "따라서 적분을 계산하여 이 면적을 찾을 때 여기 있는 모든 항은 단지 숫자인 것처럼 동작하므로 인수분해할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.76,
  "end": 443.38
 },
 {
  "input": "This is the important point.",
  "translatedText": "이것이 중요한 포인트입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.88,
  "end": 444.94
 },
 {
  "input": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
  "translatedText": "s와 관련된 모든 항목은 이제 통합 변수와 완전히 분리됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.28,
  "end": 450.2
 },
 {
  "input": "This remaining integral is a little bit tricky.",
  "translatedText": "이 나머지 적분은 약간 까다롭습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.82,
  "end": 453.0
 },
 {
  "input": "I did a whole video on it, it's actually quite famous.",
  "translatedText": "제가 전체 영상을 찍어봤는데 사실 꽤 유명해요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.08,
  "end": 455.2
 },
 {
  "input": "But you almost don't really care.",
  "translatedText": "그러나 당신은 거의 신경 쓰지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.5,
  "end": 456.9
 },
 {
  "input": "The point is that it's just some number.",
  "translatedText": "요점은 그것이 단지 숫자일 뿐이라는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.24,
  "end": 459.0
 },
 {
  "input": "That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s.",
  "translatedText": "그 숫자는 우연히 파이의 제곱근이 되지만, 정말 중요한 것은 그것이 s에 의존하지 않는다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.0,
  "end": 465.48
 },
 {
  "input": "And essentially, this is our answer.",
  "translatedText": "그리고 본질적으로 이것이 우리의 대답입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.88,
  "end": 468.48
 },
 {
  "input": "We were looking for an expression for the area of these slices as a function of s, and now we have it.",
  "translatedText": "우리는 s의 함수로서 이러한 조각의 면적에 대한 표현을 찾고 있었고 이제 그것을 얻었습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.78,
  "end": 474.28
 },
 {
  "input": "It looks like e to the negative s squared divided by two, scaled by some constant.",
  "translatedText": "이는 e를 음수 s의 제곱으로 나눈 값을 2로 나누고 상수로 스케일링한 것과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.38,
  "end": 478.84
 },
 {
  "input": "In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent.",
  "translatedText": "즉, 이것은 지수의 이 두 가지 때문에 약간 늘어난 또 다른 가우스인 종형 곡선이기도 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.3,
  "end": 485.62
 },
 {
  "input": "As I said earlier, the convolution evaluated at s is not quite this area.",
  "translatedText": "앞서 말했듯이 s에서 평가된 컨볼루션은 이 영역이 아닙니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.62,
  "end": 490.86
 },
 {
  "input": "Technically, it's this area divided by the square root of two.",
  "translatedText": "기술적으로는 이 영역을 2의 제곱근으로 나눈 값입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.34,
  "end": 494.16
 },
 {
  "input": "We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant.",
  "translatedText": "지난 영상에서 이에 대해 이야기했지만, 상수에 구워지기 때문에 별로 중요하지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.8,
  "end": 499.24
 },
 {
  "input": "What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian.",
  "translatedText": "정말 중요한 것은 두 가우스 사이의 컨볼루션 자체가 또 다른 가우스라는 결론입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.68,
  "end": 505.68
 },
 {
  "input": "If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma, essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front, and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma.",
  "translatedText": "평균이 0이고 임의의 표준 편차 시그마가 있는 정규 분포에 대한 모든 상수를 다시 도입한다면 본질적으로 동일한 추론으로 인해 지수와 앞에 나타나는 두 요소의 동일한 제곱근이 생성됩니다. 그리고 두 정규 분포 사이의 컨볼루션은 표준 편차 제곱근이 2배 시그마인 또 다른 정규 분포라는 결론에 도달합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.56,
  "end": 530.38
 },
 {
  "input": "If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result.",
  "translatedText": "이전에 많은 컨볼루션을 계산해 본 적이 없다면 이것이 매우 특별한 결과라는 점을 강조할 가치가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.98,
  "end": 536.06
 },
 {
  "input": "Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process.",
  "translatedText": "거의 항상 완전히 다른 종류의 기능으로 끝나지만 여기에는 프로세스에 대한 일종의 안정성이 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.38,
  "end": 542.5
 },
 {
  "input": "Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations.",
  "translatedText": "또한 연습을 즐기는 분들을 위해 두 가지 표준 편차가 있는 경우를 어떻게 처리할지 화면에 하나 남겨 두겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.26,
  "end": 549.44
 },
 {
  "input": "Still, some of you might be raising your hands and saying, what's the big deal?",
  "translatedText": "그런데도 손을 들고 무슨 일이냐라고 말하는 분들도 계실 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.42,
  "end": 553.94
 },
 {
  "input": "I mean, when you first heard the question, what do you get when you add two normally distributed random variables, you probably even guessed that the answer should be another normally distributed variable.",
  "translatedText": "내 말은, 여러분이 두 개의 정규 분포 확률 변수를 더하면 무엇을 얻게 되는지라는 질문을 처음 들었을 때 아마도 답이 또 다른 정규 분포 변수여야 한다고 추측했을 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.48,
  "end": 564.32
 },
 {
  "input": "After all, what else is it going to be?",
  "translatedText": "결국, 또 무엇이 될까요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.76,
  "end": 566.36
 },
 {
  "input": "Normal distributions are supposedly quite common, so why not?",
  "translatedText": "정규분포는 꽤 흔하다고 알려져 있는데, 왜 안 될까요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.86,
  "end": 570.24
 },
 {
  "input": "You could even say that this should follow from the central limit theorem, but that would have it all backwards.",
  "translatedText": "이것이 중심 극한 정리를 따라야 한다고 말할 수도 있지만, 그렇게 하면 모든 것이 거꾸로 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 570.24,
  "end": 575.48
 },
 {
  "input": "First of all, the supposed ubiquity of normal distributions is often a little exaggerated, but to the extent that they do come up, it is because of the central limit theorem, but it would be cheating to say the central limit theorem implies this result because this computation we just did is the reason that the function at the heart of the central limit theorem is a Gaussian in the first place and not some other function.",
  "translatedText": "우선, 정규분포의 편재성에 대한 가정은 약간 과장된 경우가 많지만 실제로 나타나는 정도는 중심극한정리 때문이지만 중심극한정리가 이러한 결과를 암시한다고 말하는 것은 속이는 것입니다. 우리가 방금 수행한 이 계산은 중심 극한 정리의 중심에 있는 함수가 우선 가우스이고 다른 함수가 아닌 이유입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.18,
  "end": 597.06
 },
 {
  "input": "We've talked all about the central limit theorem before, but essentially it says if you repeatedly add copies of a random variable to itself, which mathematically looks like repeatedly computing convolutions against a given distribution, then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution.",
  "translatedText": "우리는 이전에 중심 극한 정리에 대해 모두 이야기했지만 본질적으로 주어진 분포에 대해 반복적으로 컨볼루션을 계산하는 것처럼 수학적으로 보이는 임의 변수의 복사본을 자신에게 반복적으로 추가하면 적절한 이동 및 크기 조정 후에 경향은 다음과 같습니다. 항상 정규분포에 접근합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.06,
  "end": 616.5
 },
 {
  "input": "Technically there's a small assumption the distribution you start with can't have infinite variance, but it's a relatively soft assumption.",
  "translatedText": "기술적으로는 시작하는 분포가 무한한 분산을 가질 수 없다는 작은 가정이 있지만 상대적으로 부드러운 가정입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 616.98,
  "end": 623.22
 },
 {
  "input": "The magic is that for a huge category of initial distributions, this process of adding a whole bunch of random variables drawn from that distribution always tends towards this one universal shape, a Gaussian.",
  "translatedText": "마법은 초기 분포의 거대한 범주에 대해 해당 분포에서 추출된 일련의 무작위 변수를 추가하는 이 프로세스가 항상 하나의 보편적인 모양인 가우스를 향하는 경향이 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.22,
  "end": 635.1
 },
 {
  "input": "One common approach to proving this theorem involves two separate steps.",
  "translatedText": "이 정리를 증명하는 일반적인 접근 방식 중 하나는 두 가지 개별 단계를 포함합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.82,
  "end": 639.3
 },
 {
  "input": "The first step is to show that for all the different finite variance distributions you might start with, there exists a single universal shape that this process of repeated convolutions tends towards.",
  "translatedText": "첫 번째 단계는 시작할 수 있는 모든 다양한 유한 분산 분포에 대해 반복되는 컨볼루션 프로세스가 지향하는 단일 보편적인 모양이 존재한다는 것을 보여주는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.6,
  "end": 650.0
 },
 {
  "input": "This step is actually pretty technical, it goes a little beyond what I want to talk about here.",
  "translatedText": "이 단계는 실제로 꽤 기술적이며 여기서 이야기하고 싶은 것보다 조금 더 나아갑니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.0,
  "end": 654.24
 },
 {
  "input": "You often use these objects called moment generating functions that gives you a very abstract argument that there must be some universal shape, but it doesn't make any claim about what that particular shape is, just that everything in this big family is tending towards a single point in the space of distributions.",
  "translatedText": "당신은 종종 보편적인 모양이 있어야 한다는 매우 추상적인 주장을 제공하는 모멘트 생성 함수라는 개체를 사용하지만, 그 특정 모양이 무엇인지에 대해서는 주장하지 않고 단지 이 대가족의 모든 것이 다음을 향하는 경향이 있다는 것뿐입니다. 분포 공간의 단일 지점.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.52,
  "end": 669.98
 },
 {
  "input": "So then step number two is what we just showed in this video, prove that the convolution of two Gaussians gives another Gaussian.",
  "translatedText": "그러면 두 번째 단계는 우리가 이 비디오에서 방금 보여준 것입니다. 두 가우스의 컨볼루션이 또 다른 가우스를 제공한다는 것을 증명하십시오.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 670.62,
  "end": 677.4
 },
 {
  "input": "What that means is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a fixed point.",
  "translatedText": "이것이 의미하는 바는 반복되는 컨볼루션 프로세스를 적용할 때 가우스가 변하지 않고 고정된 점이라는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.4,
  "end": 684.06
 },
 {
  "input": "So the only thing it can approach is itself, and since it's one member in this big family of distributions, all of which must be tending towards a single universal shape, it must be that universal shape.",
  "translatedText": "따라서 접근할 수 있는 유일한 것은 그 자체이며, 이 큰 분포 계열의 한 구성원이기 때문에 모두 단일한 보편적인 형태를 향해야 하며, 그것은 그 보편적인 형태임에 틀림없습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 684.2,
  "end": 695.06
 },
 {
  "input": "I mentioned at the start how this calculation, step two, is something that you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed by a geometric argument that leverages the rotational symmetry of this graph is that it directly connects to a few things that we've talked about on this channel before.",
  "translatedText": "처음에 이 계산인 2단계가 정의를 사용하여 기호적으로만 직접적으로 수행할 수 있는 작업이라고 언급했습니다. 하지만 제가 이 그래프의 회전 대칭을 활용하는 기하학적 논증에 매료된 이유 중 하나는 다음과 같습니다. 이는 이전에 이 채널에서 이야기한 몇 가지 내용과 직접적으로 연결됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.58,
  "end": 712.3
 },
 {
  "input": "For example, the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this rotational symmetry as the defining feature of the distribution, that it locks you into this e to the negative x squared form, and also as an added bonus it connects to the classic proof for why pi shows up in the formula, meaning we now have a direct line between the presence and mystery of that pi and the central limit theorem.",
  "translatedText": "예를 들어 Herschel-Maxwell의 Gaussian 파생은 본질적으로 이 회전 대칭을 분포의 정의 특징으로 볼 수 있고 이 e를 음의 x 제곱 형식으로 고정하고 추가 보너스로 볼 수 있다고 말합니다. 이는 파이가 공식에 나타나는 이유에 대한 고전적인 증명과 연결됩니다. 즉, 이제 파이의 존재와 미스터리와 중심 극한 정리 사이에 직접적인 선이 있다는 의미입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 712.4,
  "end": 736.5
 },
 {
  "input": "Also on a recent Patreon post, the channel supporter Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before, which leverages the use of entropy, and again for the theoretically curious among you I'll leave some links in the description.",
  "translatedText": "또한 최근 Patreon 게시물에서 채널 후원자 Daksha Vaid-Quinter는 엔트로피 사용을 활용하는 이전에 본 적이 없는 완전히 다른 접근 방식에 관심을 가져왔습니다. 이론적으로 궁금하신 분들을 위해 몇 가지 링크를 남겨드리겠습니다. 설명에서.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 737.06,
  "end": 749.58
 },
 {
  "input": "By the way, if you want to stay up to date with new videos and also any other projects that I put out there like the Summer of Math Exposition, there is a mailing list.",
  "translatedText": "그건 그렇고, 새로운 비디오와 여름 수학 박람회처럼 제가 발표한 다른 프로젝트에 대한 최신 소식을 받고 싶다면 메일링 리스트가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 750.96,
  "end": 758.4
 },
 {
  "input": "It's relatively new and I'm pretty sparing about only posting what I think people will enjoy.",
  "translatedText": "비교적 새로운 내용이고 사람들이 좋아할 것이라고 생각되는 내용만 게시하는 데에는 좀 아깝습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 758.72,
  "end": 762.78
 },
 {
  "input": "Usually I try not to be too promotional at the end of videos these days, but if you are interested in following the work that I do, this is probably one of the most enduring ways to do so.",
  "translatedText": "요즘은 보통 영상 말미에 너무 홍보하지 않으려고 노력하지만, 제가 하는 작업을 팔로우하는 데 관심이 있으시다면 이것이 아마도 가장 오래 지속되는 방법 중 하나일 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 763.22,
  "end": 795.26
 }
]