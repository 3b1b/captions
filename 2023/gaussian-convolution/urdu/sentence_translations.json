[
 {
  "input": "The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared. ",
  "translatedText": "ایک عام تقسیم کے تحت بنیادی فعل، عرف گاوسی، e سے منفی x مربع ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.12
 },
 {
  "input": "But you might wonder, why this function? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.64,
  "end": 8.34
 },
 {
  "input": "Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle, why is it that the theory of probability seems to have a special place in its heart for this particular expression? ",
  "translatedText": "لیکن آپ حیران ہوسکتے ہیں، یہ فنکشن کیوں؟ ان تمام اظہارات میں سے جو ہم خواب میں دیکھ سکتے ہیں کہ آپ کو وسط کی طرف کمیت کے ساتھ کچھ ہموار گراف فراہم کرتے ہیں، ایسا کیوں لگتا ہے کہ نظریہ امکان اس مخصوص اظہار کے لیے اپنے دل میں ایک خاص جگہ رکھتا ہے؟ پچھلی کئی ویڈیوز سے میں اس سوال کے جواب کی طرف اشارہ کر رہا ہوں، اور یہاں ہم آخر کار ایک تسلی بخش جواب کی طرح کچھ حاصل کریں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.72,
  "end": 20.44
 },
 {
  "input": "For the last many videos I've been hinting at an answer to this question, and here we'll finally arrive at something like a satisfying answer. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.38,
  "end": 27.68
 },
 {
  "input": "As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem, which describes how as you add multiple copies of a random variable, for example rolling a weighted die many different times or letting a ball bounce off of a peg repeatedly, then the distribution describing that sum tends to look approximately like a normal distribution. ",
  "translatedText": "ہم کہاں ہیں اس بارے میں فوری ریفریشر کے طور پر، کچھ ویڈیوز پہلے ہم نے مرکزی حد نظریہ کے بارے میں بات کی تھی، جو یہ بتاتا ہے کہ جب آپ ایک بے ترتیب متغیر کی متعدد کاپیاں شامل کرتے ہیں، مثال کے طور پر ایک وزنی ڈائی کو کئی مختلف بار رول کرنا یا گیند کو اچھالنے دینا۔ایک پیگ بار بار، پھر اس رقم کو بیان کرنے والی تقسیم تقریباً ایک عام تقسیم کی طرح نظر آتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.68,
  "end": 47.72
 },
 {
  "input": "What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions, that approximation to a normal becomes better and better. ",
  "translatedText": "مرکزی حد نظریہ جو کہتا ہے وہ یہ ہے کہ جب آپ مناسب حالات میں اس رقم کو بڑا اور بڑا بناتے ہیں، تو ایک نارمل کا قربت بہتر سے بہتر ہوتا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 48.44,
  "end": 56.22
 },
 {
  "input": "But I never explained why this theorem is actually true, we only talked about what it's claiming. ",
  "translatedText": "لیکن میں نے کبھی وضاحت نہیں کی کہ یہ نظریہ حقیقت میں کیوں درست ہے، ہم نے صرف اس کے بارے میں بات کی جس کا یہ دعویٰ کر رہا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.94,
  "end": 61.98
 },
 {
  "input": "In the last video we started talking about the math involved in adding two random variables. ",
  "translatedText": "پچھلی ویڈیو میں ہم نے دو بے ترتیب متغیرات کو شامل کرنے میں شامل ریاضی کے بارے میں بات کرنا شروع کی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.08,
  "end": 67.88
 },
 {
  "input": "If you have two random variables, each following some distribution, then to find the distribution describing the sum of those variables, you compute something known as a convolution between the two original functions. ",
  "translatedText": "اگر آپ کے پاس دو بے ترتیب متغیرات ہیں، جن میں سے ہر ایک کچھ تقسیم کے بعد ہے، پھر ان متغیرات کے مجموعے کو بیان کرنے والی تقسیم کو تلاش کرنے کے لیے، آپ دو اصل افعال کے درمیان کنولیشن کے نام سے جانا جانے والی چیز کا حساب لگاتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 68.26,
  "end": 79.7
 },
 {
  "input": "And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is. ",
  "translatedText": "اور ہم نے یہ تصور کرنے کے لیے دو الگ الگ طریقے بنانے میں کافی وقت صرف کیا کہ یہ کنولوشن آپریشن واقعی کیا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.88,
  "end": 85.94
 },
 {
  "input": "Today our basic job is to work through a particular example, which is to ask what happens when you add two normally distributed random variables, which as you know by now is the same as asking what do you get if you compute a convolution between two Gaussian functions. ",
  "translatedText": "آج ہمارا بنیادی کام ایک خاص مثال کے ذریعے کام کرنا ہے، جو یہ پوچھنا ہے کہ جب آپ دو عام طور پر تقسیم شدہ بے ترتیب متغیرات کو شامل کرتے ہیں تو کیا ہوتا ہے، جیسا کہ آپ اب تک جانتے ہیں کہ یہ پوچھنے کے مترادف ہے کہ اگر آپ دو Gaussian کے درمیان کنولیشن کی گنتی کرتے ہیں تو آپ کو کیا حاصل ہوتا ہے۔افعال. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.94,
  "end": 101.78
 },
 {
  "input": "I'd like to share an especially pleasing visual way that you can think about this calculation, which hopefully offers some sense of what makes the e to the negative x squared function special in the first place. ",
  "translatedText": "میں ایک خاص طور پر خوش کن بصری طریقہ کا اشتراک کرنا چاہتا ہوں جس سے آپ اس حساب کے بارے میں سوچ سکتے ہیں، جو امید ہے کہ اس بات کا کچھ احساس پیش کرتا ہے کہ e کو منفی x مربع فنکشن کو پہلی جگہ خاص بناتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.52,
  "end": 112.36
 },
 {
  "input": "After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem. ",
  "translatedText": "اس کے ذریعے چلنے کے بعد، ہم اس بارے میں بات کریں گے کہ یہ حساب کتاب مرکزی حد نظریہ کو ثابت کرنے میں شامل اقدامات میں سے ایک ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.36,
  "end": 118.24
 },
 {
  "input": "It's the step that answers the question of why a Gaussian and not something else is the central limit. ",
  "translatedText": "یہ وہ قدم ہے جو اس سوال کا جواب دیتا ہے کہ کیوں ایک گاوسی اور کچھ اور نہیں مرکزی حد ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.32,
  "end": 123.56
 },
 {
  "input": "But first, let's dive in. ",
  "translatedText": "لیکن پہلے، آئیے اندر کودیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.2,
  "end": 125.84
 },
 {
  "input": "The full formula for a Gaussian is more complicated than just e to the negative x squared. ",
  "translatedText": "Gaussian کے لیے مکمل فارمولہ صرف e سے منفی x مربع سے زیادہ پیچیدہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.78,
  "end": 134.44
 },
 {
  "input": "The exponent is typically written as negative one half times x divided by sigma squared, where sigma describes the spread of the distribution, specifically the standard deviation. ",
  "translatedText": "ایکسپوننٹ کو عام طور پر منفی آدھا گنا x کو سگما مربع سے تقسیم کرکے لکھا جاتا ہے، جہاں سگما تقسیم کے پھیلاؤ کو بیان کرتا ہے، خاص طور پر معیاری انحراف۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.82,
  "end": 144.2
 },
 {
  "input": "All of this needs to be multiplied by a fraction on the front, which is there to make sure that the area under the curve is one, making it a valid probability distribution. ",
  "translatedText": "ان سب کو سامنے والے حصے سے ضرب کرنے کی ضرورت ہے، جو اس بات کو یقینی بنانے کے لیے موجود ہے کہ وکر کے نیچے کا رقبہ ایک ہے، جس سے یہ ایک درست امکانی تقسیم ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.68,
  "end": 153.42
 },
 {
  "input": "And if you want to consider distributions that aren't necessarily centered at zero, you would also throw another parameter, mu, into the exponent like this. ",
  "translatedText": "اور اگر آپ ان تقسیموں پر غور کرنا چاہتے ہیں جو ضروری طور پر صفر پر مرکوز نہیں ہیں، تو آپ ایک اور پیرامیٹر، mu، کو بھی اس طرح ایکسپوننٹ میں پھینک دیں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 154.02,
  "end": 161.18
 },
 {
  "input": "Although for everything we'll be doing here, we just consider centered distributions. ",
  "translatedText": "اگرچہ ہر چیز کے لیے ہم یہاں کر رہے ہیں، ہم صرف مرکزی تقسیم پر غور کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.54,
  "end": 165.12
 },
 {
  "input": "Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions, the direct way to do this would be to take the definition of a convolution, this integral expression we built up last video, and then to plug in for each one of the functions involved the formula for a Gaussian. ",
  "translatedText": "اب اگر آپ آج کے لیے ہمارے مرکزی مقصد کو دیکھیں، جو کہ دو گاوسی فنکشنز کے درمیان ایک کنولیشن کا حساب لگانا ہے، تو ایسا کرنے کا سیدھا طریقہ یہ ہوگا کہ کنوولوشن کی تعریف کو لیا جائے، یہ لازمی اظہار ہم نے پچھلی ویڈیو بنایا تھا، اور پھر ہر ایک فنکشن کے لیے پلگ ان کریں جس میں گاوسی کا فارمولہ شامل ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.8,
  "end": 183.76
 },
 {
  "input": "It's kind of a lot of symbols when you throw it all together, but more than anything, working this out is an exercise in completing the square. ",
  "translatedText": "جب آپ ان سب کو ایک ساتھ پھینک دیتے ہیں تو یہ بہت ساری علامتیں ہیں، لیکن کسی بھی چیز سے بڑھ کر، اس پر کام کرنا مربع کو مکمل کرنے کی مشق ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.22,
  "end": 190.08
 },
 {
  "input": "And there's nothing wrong with that. ",
  "translatedText": "اور اس میں کوئی حرج نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.56,
  "end": 191.58
 },
 {
  "input": "That will get you the answer that you want. ",
  "translatedText": "اس سے آپ کو وہ جواب مل جائے گا جو آپ چاہتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.72,
  "end": 193.22
 },
 {
  "input": "But of course, you know me, I'm a sucker for visual intuition, and in this case, there's another way to think about it that I haven't seen written about before, that offers a very nice connection to other aspects of this distribution, like the presence of pi and certain ways to derive where it comes from. ",
  "translatedText": "لیکن یقیناً، آپ مجھے جانتے ہیں، میں بصری وجدان کا شکار ہوں، اور اس معاملے میں، اس کے بارے میں سوچنے کا ایک اور طریقہ ہے جس کے بارے میں میں نے پہلے لکھا نہیں دیکھا، جو اس کے دوسرے پہلوؤں سے بہت اچھا تعلق پیش کرتا ہے۔تقسیم، جیسے pi کی موجودگی اور اخذ کرنے کے کچھ طریقے جہاں سے آتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.76,
  "end": 207.86
 },
 {
  "input": "And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution, and just showing the computation for the simplified form, e to the negative x squared. ",
  "translatedText": "اور جس طرح سے میں یہ کرنا چاہوں گا وہ ہے پہلے اصل تقسیم سے وابستہ تمام مستقلات کو چھیل کر، اور صرف سادہ شکل کے لیے حساب کو دکھانا، e کو منفی x مربع میں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 208.2,
  "end": 217.96
 },
 {
  "input": "The essence of what we want to compute is what the convolution between two copies of this function looks like. ",
  "translatedText": "ہم جس چیز کی گنتی کرنا چاہتے ہیں اس کا خلاصہ یہ ہے کہ اس فنکشن کی دو کاپیوں کے درمیان کنولیشن کیسا لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.96,
  "end": 224.08
 },
 {
  "input": "If you'll remember, in the last video we had two different ways to visualize convolutions, and the one we'll be using here is the second one, involving diagonal slices. ",
  "translatedText": "اگر آپ کو یاد ہو تو، پچھلی ویڈیو میں ہمارے پاس کنوولوشنز کو دیکھنے کے دو مختلف طریقے تھے، اور جو ہم یہاں استعمال کریں گے وہ دوسرا ہے، جس میں ترچھی سلائسیں شامل ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.46,
  "end": 232.92
 },
 {
  "input": "And as a quick reminder of the way that worked, if you have two different distributions that are described by two different functions, f and g, then every possible pair of values that you might get when you sample from these two distributions can be thought of as individual points on the xy-plane. ",
  "translatedText": "اور کام کرنے کے طریقے کی فوری یاد دہانی کے طور پر، اگر آپ کے پاس دو مختلف تقسیمیں ہیں جن کو دو مختلف فنکشنز، f اور g کے ذریعے بیان کیا گیا ہے، تو اقدار کا ہر ممکنہ جوڑا جو آپ کو ان دو تقسیموں سے نمونہ لینے پر حاصل ہو سکتا ہے، کے بارے میں سوچا جا سکتا ہے۔xy-plane پر انفرادی پوائنٹس کے طور پر۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 233.28,
  "end": 249.56
 },
 {
  "input": "And the probability density of landing on one such point, assuming independence, looks like f of x times g of y. ",
  "translatedText": "اور ایسے ہی ایک نقطے پر اترنے کے امکان کی کثافت، آزادی فرض کرتے ہوئے، y کے x گنا g کے f کی طرح نظر آتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.36,
  "end": 257.52
 },
 {
  "input": "So what we do is we look at a graph of that expression as a two-variable function of x and y, which is a way of showing the distribution of all possible outcomes when we sample from the two different variables. ",
  "translatedText": "تو ہم کیا کرتے ہیں ہم اس اظہار کے گراف کو x اور y کے دو متغیر فعل کے طور پر دیکھتے ہیں، جو کہ تمام ممکنہ نتائج کی تقسیم کو ظاہر کرنے کا ایک طریقہ ہے جب ہم دو مختلف متغیرات سے نمونہ لیتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.0,
  "end": 269.62
 },
 {
  "input": "To interpret the convolution of f and g evaluated on some input s, which is a way of saying how likely are you to get a pair of samples that adds up to this sum s, what you do is you look at a slice of this graph over the line x plus y equals s, and you consider the area under that slice. ",
  "translatedText": "کچھ ان پٹ s پر جانچے گئے f اور g کے اختلاط کی تشریح کرنے کے لیے، جو یہ بتانے کا ایک طریقہ ہے کہ آپ کو نمونوں کا ایک جوڑا ملنے کا کتنا امکان ہے جو اس رقم میں اضافہ کرتا ہے، آپ کیا کرتے ہیں آپ اس گراف کے ایک ٹکڑے کو دیکھتے ہیں۔لائن کے اوپر x جمع y برابر s، اور آپ اس سلائس کے نیچے والے حصے پر غور کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.56,
  "end": 289.3
 },
 {
  "input": "This area is almost, but not quite, the value of the convolution at s. ",
  "translatedText": "یہ رقبہ تقریباً ہے، لیکن کافی نہیں، s پر کنولیشن کی قدر۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.1,
  "end": 296.32
 },
 {
  "input": "For a mildly technical reason, you need to divide by the square root of two. ",
  "translatedText": "معمولی تکنیکی وجہ سے، آپ کو دو کے مربع جڑ سے تقسیم کرنے کی ضرورت ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.8,
  "end": 300.16
 },
 {
  "input": "Still, this area is the key feature to focus on. ",
  "translatedText": "پھر بھی، اس علاقے پر توجہ مرکوز کرنے کی کلیدی خصوصیت ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.84,
  "end": 303.44
 },
 {
  "input": "You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum. ",
  "translatedText": "آپ اس کے بارے میں سوچ سکتے ہیں کہ ایک دی گئی رقم کے مطابق تمام نتائج کے لیے تمام امکانات کی کثافتوں کو ایک ساتھ جوڑ دیا جائے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.44,
  "end": 311.04
 },
 {
  "input": "In the specific case where these two functions look like e to the negative x squared and e to the negative y squared, the resulting 3D graph has a really nice property that you can exploit. ",
  "translatedText": "مخصوص صورت میں جہاں یہ دو فنکشنز e سے منفی x مربع اور e سے منفی y مربع میں نظر آتے ہیں، نتیجے میں 3D گراف میں واقعی ایک اچھی خاصیت ہے جس سے آپ فائدہ اٹھا سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.3,
  "end": 323.5
 },
 {
  "input": "It's rotationally symmetric. ",
  "translatedText": "یہ گردشی طور پر ہموار ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.72,
  "end": 325.68
 },
 {
  "input": "You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared, and this term describes the square of the distance between any point on the xy plane and the origin. ",
  "translatedText": "آپ اسے اصطلاحات کو ملا کر دیکھ سکتے ہیں اور یہ دیکھ سکتے ہیں کہ یہ مکمل طور پر x مربع جمع y مربع کا ایک فنکشن ہے، اور یہ اصطلاح xy جہاز اور اصل کے کسی بھی نقطہ کے درمیان فاصلے کے مربع کو بیان کرتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.88,
  "end": 338.46
 },
 {
  "input": "So in other words, the expression is purely a function of the distance from the origin. ",
  "translatedText": "تو دوسرے الفاظ میں، اظہار خالصتاً اصل سے دوری کا ایک فعل ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 339.2,
  "end": 343.16
 },
 {
  "input": "And by the way, this would not be true for any other distribution. ",
  "translatedText": "اور ویسے، یہ کسی دوسری تقسیم کے لیے درست نہیں ہوگا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.56,
  "end": 347.92
 },
 {
  "input": "It's a property that uniquely characterizes bell curves. ",
  "translatedText": "یہ ایک ایسی پراپرٹی ہے جو گھنٹی کے منحنی خطوط کو منفرد طور پر نمایاں کرتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 351.28
 },
 {
  "input": "So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about, and honestly calculating the area would just amount to computing the original integral that defines a convolution in the first place. ",
  "translatedText": "لہذا فنکشنز کے زیادہ تر دوسرے جوڑوں کے لیے، یہ ترچھی سلائسیں کچھ پیچیدہ شکل ہوں گی جن کے بارے میں سوچنا مشکل ہے، اور ایمانداری سے رقبہ کا حساب لگانا اصل انٹیگرل کی کمپیوٹنگ کے مترادف ہوگا جو پہلی جگہ ایک کنولیشن کی وضاحت کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.16,
  "end": 365.54
 },
 {
  "input": "So in most cases, the visual intuition doesn't really buy you anything. ",
  "translatedText": "لہذا زیادہ تر معاملات میں، بصری وجدان واقعی آپ کو کچھ نہیں خریدتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.94,
  "end": 369.36
 },
 {
  "input": "But in the case of bell curves, you can leverage that rotational symmetry. ",
  "translatedText": "لیکن گھنٹی کے منحنی خطوط کی صورت میں، آپ اس گردشی توازن کا فائدہ اٹھا سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.36,
  "end": 373.92
 },
 {
  "input": "Here, focus on one of these slices over the line x plus y equals s for some value of s. ",
  "translatedText": "یہاں، s کی کچھ قدر کے لیے x plus y برابر s لائن پر ان سلائسوں میں سے ایک پر فوکس کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.8,
  "end": 380.48
 },
 {
  "input": "And remember, the convolution that we're trying to compute is a function of s. ",
  "translatedText": "اور یاد رکھیں، ہم جس کنوولوشن کو شمار کرنے کی کوشش کر رہے ہیں وہ s کا ایک فنکشن ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.3,
  "end": 385.84
 },
 {
  "input": "The thing that you want is an expression of s that tells you the area under this slice. ",
  "translatedText": "آپ جو چیز چاہتے ہیں وہ s کا اظہار ہے جو آپ کو اس سلائس کے نیچے کا علاقہ بتاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.84,
  "end": 391.1
 },
 {
  "input": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s. ",
  "translatedText": "ٹھیک ہے، اگر آپ اس لائن کو دیکھیں تو یہ x-axis کو s صفر پر اور y-axis کو صفر s پر کاٹتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.7,
  "end": 397.9
 },
 {
  "input": "And a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two. ",
  "translatedText": "اور Pythagoras کا تھوڑا سا آپ کو دکھائے گا کہ اصل سے اس لائن تک سیدھی لائن کا فاصلہ s کو دو کے مربع جڑ سے تقسیم کیا گیا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.32,
  "end": 405.32
 },
 {
  "input": "Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees, where you'd find something parallel to the y-axis the same distance away from the origin. ",
  "translatedText": "اب، ہم آہنگی کی وجہ سے، یہ ٹکڑا اس سے مماثل ہے جسے آپ 45 ڈگری پر گھومتے ہیں، جہاں آپ کو y-axis کے متوازی کچھ ملے گا جو اصل سے دور ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 416.36
 },
 {
  "input": "The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions, because it only involves taking an integral with respect to y. ",
  "translatedText": "کلیدی بات یہ ہے کہ y-axis کے متوازی سلائس کے اس دوسرے حصے کو کمپیوٹنگ کرنا دوسری سمتوں میں سلائسز سے کہیں زیادہ آسان ہے، کیونکہ اس میں صرف y کے حوالے سے ایک انٹیگرل لینا شامل ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.64,
  "end": 428.26
 },
 {
  "input": "The value of x on this slice is a constant. ",
  "translatedText": "اس سلائس پر x کی قدر ایک مستقل ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.74,
  "end": 431.44
 },
 {
  "input": "Specifically, it would be the constant s divided by the square root of two. ",
  "translatedText": "خاص طور پر، یہ مستقل s کو دو کے مربع جڑ سے تقسیم کیا جائے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 434.76
 },
 {
  "input": "So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out. ",
  "translatedText": "لہذا جب آپ انٹیگرل کو کمپیوٹنگ کر رہے ہیں، اس علاقے کو تلاش کر رہے ہیں، یہاں یہ تمام اصطلاح اس طرح برتاؤ کرتی ہے جیسے یہ صرف کچھ تعداد تھی، اور آپ اس کو نکال سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.76,
  "end": 443.38
 },
 {
  "input": "This is the important point. ",
  "translatedText": "یہ اہم نکتہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.88,
  "end": 444.94
 },
 {
  "input": "All of the stuff that's involving s is now entirely separate from the integrated variable. ",
  "translatedText": "وہ تمام چیزیں جن میں s شامل ہے اب مکمل طور پر مربوط متغیر سے الگ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.28,
  "end": 450.2
 },
 {
  "input": "This remaining integral is a little bit tricky. ",
  "translatedText": "یہ بقیہ لازمی تھوڑا سا مشکل ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.82,
  "end": 453.0
 },
 {
  "input": "I did a whole video on it, it's actually quite famous. ",
  "translatedText": "میں نے اس پر ایک پوری ویڈیو بنائی، یہ دراصل کافی مشہور ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.08,
  "end": 455.2
 },
 {
  "input": "But you almost don't really care. ",
  "translatedText": "لیکن آپ کو تقریبا واقعی پرواہ نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.5,
  "end": 456.9
 },
 {
  "input": "The point is that it's just some number. ",
  "translatedText": "نقطہ یہ ہے کہ یہ صرف کچھ نمبر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.24,
  "end": 459.0
 },
 {
  "input": "That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s. ",
  "translatedText": "یہ نمبر pi کا مربع جڑ ہوتا ہے، لیکن جو چیز واقعی اہم ہے وہ یہ ہے کہ یہ ایسی چیز ہے جس کا s پر کوئی انحصار نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.0,
  "end": 465.48
 },
 {
  "input": "And essentially, this is our answer. ",
  "translatedText": "اور بنیادی طور پر، یہ ہمارا جواب ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.88,
  "end": 468.48
 },
 {
  "input": "We were looking for an expression for the area of these slices as a function of s, and now we have it. ",
  "translatedText": "ہم s کے فنکشن کے طور پر ان سلائسوں کے رقبے کے لیے ایک اظہار تلاش کر رہے تھے، اور اب ہمارے پاس ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.78,
  "end": 474.28
 },
 {
  "input": "It looks like e to the negative s squared divided by two, scaled by some constant. ",
  "translatedText": "ایسا لگتا ہے کہ e سے منفی s کے مربع کو دو سے تقسیم کیا گیا ہے، کچھ مستقل سے چھوٹا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.38,
  "end": 478.84
 },
 {
  "input": "In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent. ",
  "translatedText": "دوسرے لفظوں میں، یہ ایک گھنٹی کا منحنی خطوط بھی ہے، ایک اور گاوسی، صرف اس دو کی وجہ سے تھوڑا سا پھیلا ہوا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.3,
  "end": 485.62
 },
 {
  "input": "As I said earlier, the convolution evaluated at s is not quite this area. ",
  "translatedText": "جیسا کہ میں نے پہلے کہا تھا، s پر اندازہ لگایا گیا کنوولوشن اس علاقے میں بالکل نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.62,
  "end": 490.86
 },
 {
  "input": "Technically, it's this area divided by the square root of two. ",
  "translatedText": "تکنیکی طور پر، یہ اس علاقے کو دو کے مربع جڑ سے تقسیم کیا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.34,
  "end": 494.16
 },
 {
  "input": "We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant. ",
  "translatedText": "ہم نے پچھلی ویڈیو میں اس کے بارے میں بات کی تھی، لیکن اس سے کوئی فرق نہیں پڑتا کیونکہ یہ صرف مسلسل میں پکایا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.8,
  "end": 499.24
 },
 {
  "input": "What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian. ",
  "translatedText": "جو چیز واقعی اہمیت رکھتی ہے وہ یہ ہے کہ دو گاوسیوں کے درمیان ایک کنولیشن بذات خود ایک اور گاوسی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.68,
  "end": 505.68
 },
 {
  "input": "If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma, essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front, and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma. ",
  "translatedText": "اگر آپ واپس جائیں اور ایک اوسط صفر اور ایک صوابدیدی معیاری انحراف سگما کے ساتھ معمول کی تقسیم کے لیے تمام مستقلات کو دوبارہ پیش کریں، تو بنیادی طور پر یکساں استدلال دو عنصر کے ایک ہی مربع جڑ کی طرف لے جائے گا جو ظاہری شکل میں ظاہر ہوتا ہے اور سامنے، اور یہ اس نتیجے پر پہنچتا ہے کہ ایسی دو نارمل تقسیموں کے درمیان کنولیشن ایک اور عام تقسیم ہے جس کا معیاری انحراف مربع جڑ دو گنا سگما ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.56,
  "end": 530.38
 },
 {
  "input": "If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result. ",
  "translatedText": "اگر آپ نے پہلے بہت سارے کنوولوشنز کی گنتی نہیں کی ہے، تو اس بات پر زور دینے کے قابل ہے کہ یہ ایک بہت ہی خاص نتیجہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.98,
  "end": 536.06
 },
 {
  "input": "Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process. ",
  "translatedText": "تقریباً ہمیشہ آپ بالکل مختلف قسم کے فنکشن کے ساتھ ختم ہوتے ہیں، لیکن یہاں اس عمل میں ایک طرح کا استحکام ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.38,
  "end": 542.5
 },
 {
  "input": "Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations. ",
  "translatedText": "اس کے علاوہ، آپ میں سے ان لوگوں کے لیے جو مشقوں سے لطف اندوز ہوتے ہیں، میں اسکرین پر ایک چھوڑ دوں گا کہ آپ دو مختلف معیاری انحراف کے معاملے کو کس طرح سنبھالیں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.26,
  "end": 549.44
 },
 {
  "input": "Still, some of you might be raising your hands and saying, what's the big deal? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.42,
  "end": 553.94
 },
 {
  "input": "I mean, when you first heard the question, what do you get when you add two normally distributed random variables, you probably even guessed that the answer should be another normally distributed variable. ",
  "translatedText": "پھر بھی، آپ میں سے کچھ ہاتھ اٹھا کر کہہ رہے ہوں گے، یہ کیا بڑی بات ہے؟ میرا مطلب ہے، جب آپ نے پہلی بار یہ سوال سنا تھا کہ جب آپ دو عام طور پر تقسیم شدہ بے ترتیب متغیرات کو شامل کرتے ہیں تو آپ کو کیا حاصل ہوتا ہے، آپ نے شاید اندازہ بھی لگایا ہو کہ جواب ایک اور عام طور پر تقسیم شدہ متغیر ہونا چاہیے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.48,
  "end": 564.32
 },
 {
  "input": "After all, what else is it going to be? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.76,
  "end": 566.36
 },
 {
  "input": "Normal distributions are supposedly quite common, so why not? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.86,
  "end": 570.24
 },
 {
  "input": "You could even say that this should follow from the central limit theorem, but that would have it all backwards. ",
  "translatedText": "آخر یہ اور کیا ہونے والا ہے؟ عام تقسیم قیاس بہت عام ہے، تو کیوں نہیں؟ آپ یہاں تک کہہ سکتے ہیں کہ یہ مرکزی حد نظریہ سے عمل کرنا چاہئے، لیکن اس سے یہ سب پیچھے ہوگا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 570.24,
  "end": 575.48
 },
 {
  "input": "First of all, the supposed ubiquity of normal distributions is often a little exaggerated, but to the extent that they do come up, it is because of the central limit theorem, but it would be cheating to say the central limit theorem implies this result because this computation we just did is the reason that the function at the heart of the central limit theorem is a Gaussian in the first place and not some other function. ",
  "translatedText": "سب سے پہلے، عام تقسیم کی قیاس ہر جگہ اکثر تھوڑی بہت مبالغہ آرائی کی جاتی ہے، لیکن جس حد تک وہ سامنے آتے ہیں، اس کی وجہ مرکزی حد تھیوریم ہے، لیکن یہ کہنا دھوکہ دہی ہو گا کہ مرکزی حد نظریہ اس نتیجے پر دلالت کرتا ہے کیونکہ یہ گنتی ہم نے ابھی کی ہے اس کی وجہ یہ ہے کہ مرکزی حد نظریہ کے مرکز میں فعل پہلی جگہ گاوسی ہے نہ کہ کوئی اور فعل۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.18,
  "end": 597.06
 },
 {
  "input": "We've talked all about the central limit theorem before, but essentially it says if you repeatedly add copies of a random variable to itself, which mathematically looks like repeatedly computing convolutions against a given distribution, then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution. ",
  "translatedText": "ہم نے مرکزی حد نظریہ کے بارے میں پہلے بھی بات کی ہے، لیکن بنیادی طور پر یہ کہتا ہے کہ اگر آپ اپنے آپ میں ایک بے ترتیب متغیر کی کاپیاں بار بار شامل کرتے ہیں، جو کہ ریاضی کے لحاظ سے کسی دی گئی تقسیم کے خلاف بار بار کمپیوٹنگ کنولیشنز کی طرح لگتا ہے، تو مناسب شفٹنگ اور ری اسکیلنگ کے بعد، رجحان ہوتا ہے۔ہمیشہ ایک عام تقسیم سے رجوع کرنا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.06,
  "end": 616.5
 },
 {
  "input": "Technically there's a small assumption the distribution you start with can't have infinite variance, but it's a relatively soft assumption. ",
  "translatedText": "تکنیکی طور پر ایک چھوٹا سا مفروضہ ہے جس تقسیم کے ساتھ آپ شروع کرتے ہیں اس میں لامحدود تغیر نہیں ہو سکتا، لیکن یہ نسبتاً نرم مفروضہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 616.98,
  "end": 623.22
 },
 {
  "input": "The magic is that for a huge category of initial distributions, this process of adding a whole bunch of random variables drawn from that distribution always tends towards this one universal shape, a Gaussian. ",
  "translatedText": "جادو یہ ہے کہ ابتدائی تقسیم کے ایک بڑے زمرے کے لیے، اس تقسیم سے اخذ کیے گئے بے ترتیب متغیرات کے ایک پورے گروپ کو شامل کرنے کا یہ عمل ہمیشہ اس ایک عالمگیر شکل، ایک گاوسی کی طرف ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.22,
  "end": 635.1
 },
 {
  "input": "One common approach to proving this theorem involves two separate steps. ",
  "translatedText": "اس نظریہ کو ثابت کرنے کے لیے ایک عام نقطہ نظر میں دو الگ الگ مراحل شامل ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.82,
  "end": 639.3
 },
 {
  "input": "The first step is to show that for all the different finite variance distributions you might start with, there exists a single universal shape that this process of repeated convolutions tends towards. ",
  "translatedText": "پہلا قدم یہ ظاہر کرنا ہے کہ ان تمام مختلف محدود تغیرات کی تقسیم کے لیے جن کے ساتھ آپ شروع کر سکتے ہیں، ایک واحد عالمگیر شکل موجود ہے جس کی طرف بار بار کنولیشنز کا یہ عمل ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.6,
  "end": 650.0
 },
 {
  "input": "This step is actually pretty technical, it goes a little beyond what I want to talk about here. ",
  "translatedText": "یہ مرحلہ درحقیقت کافی تکنیکی ہے، یہ اس سے تھوڑا آگے ہے جس کے بارے میں میں یہاں بات کرنا چاہتا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.0,
  "end": 654.24
 },
 {
  "input": "You often use these objects called moment generating functions that gives you a very abstract argument that there must be some universal shape, but it doesn't make any claim about what that particular shape is, just that everything in this big family is tending towards a single point in the space of distributions. ",
  "translatedText": "آپ اکثر ان چیزوں کو استعمال کرتے ہیں جنہیں لمحہ پیدا کرنے والے فنکشن کہتے ہیں جو آپ کو ایک بہت ہی تجریدی دلیل دیتے ہیں کہ کوئی نہ کوئی آفاقی شکل ضرور ہونی چاہیے، لیکن یہ اس بات کا کوئی دعویٰ نہیں کرتا کہ وہ خاص شکل کیا ہے، بس یہ کہ اس بڑے خاندان میں ہر چیز ایک کی طرف مائل ہے۔تقسیم کی جگہ میں واحد نقطہ۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.52,
  "end": 669.98
 },
 {
  "input": "So then step number two is what we just showed in this video, prove that the convolution of two Gaussians gives another Gaussian. ",
  "translatedText": "تو پھر مرحلہ نمبر دو وہ ہے جو ہم نے ابھی اس ویڈیو میں دکھایا ہے، ثابت کریں کہ دو گاوسیوں کا کنولوشن ایک اور گاوسی دیتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 670.62,
  "end": 677.4
 },
 {
  "input": "What that means is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a fixed point. ",
  "translatedText": "اس کا مطلب یہ ہے کہ جب آپ بار بار کنوولوشنز کے اس عمل کو لاگو کرتے ہیں، تو گاوسی تبدیل نہیں ہوتا، یہ ایک مقررہ نقطہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.4,
  "end": 684.06
 },
 {
  "input": "So the only thing it can approach is itself, and since it's one member in this big family of distributions, all of which must be tending towards a single universal shape, it must be that universal shape. ",
  "translatedText": "لہٰذا صرف ایک ہی چیز جس سے یہ رابطہ کر سکتا ہے وہ خود ہے، اور چونکہ تقسیم کے اس بڑے خاندان میں یہ ایک رکن ہے، جن میں سے سبھی کو ایک ہی عالمگیر شکل کی طرف مائل ہونا چاہیے، اس لیے یہ عالمگیر شکل ہونی چاہیے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 684.2,
  "end": 695.06
 },
 {
  "input": "I mentioned at the start how this calculation, step two, is something that you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed by a geometric argument that leverages the rotational symmetry of this graph is that it directly connects to a few things that we've talked about on this channel before. ",
  "translatedText": "میں نے شروع میں ذکر کیا تھا کہ یہ حساب، دوسرا مرحلہ، وہ چیز ہے جو آپ براہ راست، صرف علامتی طور پر تعریفوں کے ساتھ کر سکتے ہیں، لیکن ایک وجہ یہ ہے کہ میں ایک ہندسی دلیل سے بہت متاثر ہوں جو اس گراف کی گردشی توازن کا فائدہ اٹھاتا ہے۔یہ کچھ چیزوں سے براہ راست جڑتا ہے جن کے بارے میں ہم پہلے اس چینل پر بات کر چکے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.58,
  "end": 712.3
 },
 {
  "input": "For example, the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this rotational symmetry as the defining feature of the distribution, that it locks you into this e to the negative x squared form, and also as an added bonus it connects to the classic proof for why pi shows up in the formula, meaning we now have a direct line between the presence and mystery of that pi and the central limit theorem. ",
  "translatedText": "مثال کے طور پر، Gaussian کا ہرشل میکسویل اخذ، جو بنیادی طور پر یہ کہتا ہے کہ آپ اس گردشی توازن کو تقسیم کی وضاحتی خصوصیت کے طور پر دیکھ سکتے ہیں، کہ یہ آپ کو اس e میں منفی x مربع شکل میں بند کر دیتا ہے، اور ایک اضافی بونس کے طور پر بھی۔یہ اس کلاسک ثبوت سے جڑتا ہے کہ pi فارمولے میں کیوں ظاہر ہوتا ہے، یعنی اب ہمارے پاس اس pi کی موجودگی اور اسرار اور مرکزی حد نظریہ کے درمیان ایک سیدھی لکیر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 712.4,
  "end": 736.5
 },
 {
  "input": "Also on a recent Patreon post, the channel supporter Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before, which leverages the use of entropy, and again for the theoretically curious among you I'll leave some links in the description. ",
  "translatedText": "پیٹریون کی ایک حالیہ پوسٹ پر بھی، چینل کے حامی دکشا وید-کوئنٹر نے میری توجہ ایک بالکل مختلف انداز کی طرف دلائی جو میں نے پہلے نہیں دیکھی تھی، جو اینٹروپی کے استعمال سے فائدہ اٹھاتا ہے، اور آپ کے درمیان نظریاتی طور پر تجسس کے لیے میں کچھ لنکس چھوڑ دوں گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 737.06,
  "end": 749.58
 },
 {
  "input": "By the way, if you want to stay up to date with new videos and also any other projects that I put out there like the Summer of Math Exposition, there is a mailing list. ",
  "translatedText": "تفصیل میں ویسے، اگر آپ نئی ویڈیوز کے ساتھ اپ ٹو ڈیٹ رہنا چاہتے ہیں اور کسی دوسرے پروجیکٹ کے ساتھ جو میں نے وہاں پیش کیا ہے جیسے سمر آف میتھ ایکسپوزیشن، تو ایک میلنگ لسٹ موجود ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 750.96,
  "end": 758.4
 },
 {
  "input": "It's relatively new and I'm pretty sparing about only posting what I think people will enjoy. ",
  "translatedText": "یہ نسبتاً نیا ہے اور میں صرف وہی پوسٹ کرنے کے بارے میں کافی بچت کر رہا ہوں جو میرے خیال میں لوگ لطف اندوز ہوں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 758.72,
  "end": 762.78
 },
 {
  "input": "Usually I try not to be too promotional at the end of videos these days, but if you are interested in following the work that I do, this is probably one of the most enduring ways to do so. ",
  "translatedText": "عام طور پر میں ان دنوں ویڈیوز کے اختتام پر زیادہ پروموشنل نہ ہونے کی کوشش کرتا ہوں، لیکن اگر آپ میرے کام کی پیروی کرنے میں دلچسپی رکھتے ہیں، تو یہ شاید ایسا کرنے کے سب سے زیادہ پائیدار طریقوں میں سے ایک ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 763.22,
  "end": 795.26
 }
]