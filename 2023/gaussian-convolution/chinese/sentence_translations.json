[
 {
  "input": "The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.",
  "translatedText": "正态分布（又称高斯分布）的基本函数是 e 的负 x 平方。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.12
 },
 {
  "input": "But you might wonder, why this function?",
  "translatedText": "但你可能会想，为什么要使用这个函数呢？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 6.64,
  "end": 8.34
 },
 {
  "input": "Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle, why is it that the theory of probability seems to have a special place in its heart for this particular expression?",
  "translatedText": "在我们能想到的所有表达式中，它们都会给你一些质量集中在中间的对称平滑图 ，为什么概率论似乎在它的核心中对于这个特定的表达式有一个特殊的位置？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.72,
  "end": 20.44
 },
 {
  "input": "For the last many videos I've been hinting at an answer to this question, and here we'll finally arrive at something like a satisfying answer.",
  "translatedText": "在过去的许多视频中，我一直在暗示这个问题的答 案，在这里我们最终会得到一个令人满意的答案。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.38,
  "end": 27.68
 },
 {
  "input": "As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem, which describes how as you add multiple copies of a random variable, for example rolling a weighted die many different times or letting a ball bounce off of a peg repeatedly, then the distribution describing that sum tends to look approximately like a normal distribution.",
  "translatedText": "作为对我们所处位置的快速回顾，几个视频前我们讨 论了中心极限定理，它描述了如何添加随机变量的多 个副本，例如多次滚动加权骰子或让球从重复钉住， 那么描述该总和的分布往往看起来近似于正态分布。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.68,
  "end": 47.72
 },
 {
  "input": "What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions, that approximation to a normal becomes better and better.",
  "translatedText": "中心极限定理所说的是，在适当的条件下，当你让这个 总和越来越大时，对正常值的近似就会变得越来越好。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 48.44,
  "end": 56.22
 },
 {
  "input": "But I never explained why this theorem is actually true, we only talked about what it's claiming.",
  "translatedText": "但我从未解释过为什么这个定理实际上是正确的，我们只是讨论了它的主张。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.94,
  "end": 61.98
 },
 {
  "input": "In the last video we started talking about the math involved in adding two random variables.",
  "translatedText": "在上一个视频中，我们开始讨论添加两个随机变量所涉及的数学。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.08,
  "end": 67.88
 },
 {
  "input": "If you have two random variables, each following some distribution, then to find the distribution describing the sum of those variables, you compute something known as a convolution between the two original functions.",
  "translatedText": "如果您有两个随机变量，每个变量都遵循某种 分布，那么为了找到描述这些变量之和的分 布，您需要计算两个原始函数之间的卷积。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 68.26,
  "end": 79.7
 },
 {
  "input": "And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.",
  "translatedText": "我们花了很多时间建立两种不同的方法来可视化这个卷积运算到底是什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.88,
  "end": 85.94
 },
 {
  "input": "Today our basic job is to work through a particular example, which is to ask what happens when you add two normally distributed random variables, which as you know by now is the same as asking what do you get if you compute a convolution between two Gaussian functions.",
  "translatedText": "今天我们的基本工作是完成一个特定的示例，即询问当您添加两 个正态分布的随机变量时会发生什么，正如您现在所知，这与询 问如果计算两个高斯函数之间的卷积会得到什么结果相同功能。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.94,
  "end": 101.78
 },
 {
  "input": "I'd like to share an especially pleasing visual way that you can think about this calculation, which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.",
  "translatedText": "我想分享一种特别令人愉悦的视觉方式，让您可以思考这个计算 ，它希望能够让您了解负 x 平方函数的 e 的特殊之处。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.52,
  "end": 112.36
 },
 {
  "input": "After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.",
  "translatedText": "在我们完成它之后，我们将讨论这个计算如何成为证明中心极限定理所涉及的步骤之一。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.36,
  "end": 118.24
 },
 {
  "input": "It's the step that answers the question of why a Gaussian and not something else is the central limit.",
  "translatedText": "这一步回答了为什么高斯函数而不是其他东西是中心极限的问题。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.32,
  "end": 123.56
 },
 {
  "input": "But first, let's dive in.",
  "translatedText": "但首先，让我们深入了解一下。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.2,
  "end": 125.84
 },
 {
  "input": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
  "translatedText": "高斯的完整公式比 e 到负 x 平方更复杂。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.78,
  "end": 134.44
 },
 {
  "input": "The exponent is typically written as negative one half times x divided by sigma squared, where sigma describes the spread of the distribution, specifically the standard deviation.",
  "translatedText": "指数通常写为负二分之一 x 除以 sigma 平方，其中 sigma 描述分布的展度，特别是标准差。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.82,
  "end": 144.2
 },
 {
  "input": "All of this needs to be multiplied by a fraction on the front, which is there to make sure that the area under the curve is one, making it a valid probability distribution.",
  "translatedText": "所有这些都需要乘以前面的一个分数，这是为了确 保曲线下的面积为一，使其成为有效的概率分布。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.68,
  "end": 153.42
 },
 {
  "input": "And if you want to consider distributions that aren't necessarily centered at zero, you would also throw another parameter, mu, into the exponent like this.",
  "translatedText": "如果您想考虑不一定以零为中心的分布，您还可以 将另一个参数 mu 放入指数中，如下所示。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 154.02,
  "end": 161.18
 },
 {
  "input": "Although for everything we'll be doing here, we just consider centered distributions.",
  "translatedText": "尽管对于我们将在这里做的所有事情，我们只考虑中心分布。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.54,
  "end": 165.12
 },
 {
  "input": "Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions, the direct way to do this would be to take the definition of a convolution, this integral expression we built up last video, and then to plug in for each one of the functions involved the formula for a Gaussian.",
  "translatedText": "现在，如果您看看我们今天的中心目标， 即计算两个高斯函数之间的卷积，那么执 行此操作的直接方法是采用卷积的定义， 即我们在上一个视频中构建的积分表达式 ，然后为涉及高斯公式的每个函数代入。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.8,
  "end": 183.76
 },
 {
  "input": "It's kind of a lot of symbols when you throw it all together, but more than anything, working this out is an exercise in completing the square.",
  "translatedText": "当你把它们放在一起时，它就像是很多符号，但最 重要的是，解决这个问题是完成正方形的练习。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.22,
  "end": 190.08
 },
 {
  "input": "And there's nothing wrong with that.",
  "translatedText": "这并没有什么问题。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.56,
  "end": 191.58
 },
 {
  "input": "That will get you the answer that you want.",
  "translatedText": "这会给你你想要的答案。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.72,
  "end": 193.22
 },
 {
  "input": "But of course, you know me, I'm a sucker for visual intuition, and in this case, there's another way to think about it that I haven't seen written about before, that offers a very nice connection to other aspects of this distribution, like the presence of pi and certain ways to derive where it comes from.",
  "translatedText": "但是当然，你知道我，我是一个视觉直觉的傻瓜，在这 种情况下，还有另一种方式来思考它，我以前没有见 过写过，这提供了与此的其他方面的非常好的联系分布 ，例如 pi 的存在以及确定其来源的某些方法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.76,
  "end": 207.86
 },
 {
  "input": "And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution, and just showing the computation for the simplified form, e to the negative x squared.",
  "translatedText": "我想要做到这一点的方法是首先剥离与实 际分布相关的所有常数，并仅显示简化形 式的计算，即 e 到负 x 平方。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 208.2,
  "end": 217.96
 },
 {
  "input": "The essence of what we want to compute is what the convolution between two copies of this function looks like.",
  "translatedText": "我们想要计算的本质是这个函数的两个副本之间的卷积是什么样子的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.96,
  "end": 224.08
 },
 {
  "input": "If you'll remember, in the last video we had two different ways to visualize convolutions, and the one we'll be using here is the second one, involving diagonal slices.",
  "translatedText": "如果您还记得，在上一个视频中，我们有两种不同的方法来可视 化卷积，我们在这里使用的是第二种方法，涉及对角线切片。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.46,
  "end": 232.92
 },
 {
  "input": "And as a quick reminder of the way that worked, if you have two different distributions that are described by two different functions, f and g, then every possible pair of values that you might get when you sample from these two distributions can be thought of as individual points on the xy-plane.",
  "translatedText": "作为工作方式的快速提醒，如果您有两个不同的 分布，由两个不同的函数 f 和 g 描述， 那么您可以考虑从这两个分布中采样时可能获得 的每对可能的值作为 xy 平面上的各个点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 233.28,
  "end": 249.56
 },
 {
  "input": "And the probability density of landing on one such point, assuming independence, looks like f of x times g of y.",
  "translatedText": "假设独立性，落在这样一个点上的概率密度看 起来就像 x 的 f 乘以 y 的 g。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.36,
  "end": 257.52
 },
 {
  "input": "So what we do is we look at a graph of that expression as a two-variable function of x and y, which is a way of showing the distribution of all possible outcomes when we sample from the two different variables.",
  "translatedText": "因此，我们所做的就是将该表达式视为 x 和 y 的二变量函数，这是一种显示当我们从两 个不同变量中采样时所有可能结果分布的方法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.0,
  "end": 269.62
 },
 {
  "input": "To interpret the convolution of f and g evaluated on some input s, which is a way of saying how likely are you to get a pair of samples that adds up to this sum s, what you do is you look at a slice of this graph over the line x plus y equals s, and you consider the area under that slice.",
  "translatedText": "为了解释在某些输入 s 上评估的 f 和 g 的卷积 ，这是一种表示您获得加起来达到此总和 s 的一对样 本的可能性的方式，您要做的就是查看该图的一部分在线 x 加 y 等于 s，然后考虑该切片下方的面积。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.56,
  "end": 289.3
 },
 {
  "input": "This area is almost, but not quite, the value of the convolution at s.",
  "translatedText": "该区域几乎（但不完全）是 s 处卷积的值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.1,
  "end": 296.32
 },
 {
  "input": "For a mildly technical reason, you need to divide by the square root of two.",
  "translatedText": "出于轻微的技术原因，您需要除以二的平方根。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.8,
  "end": 300.16
 },
 {
  "input": "Still, this area is the key feature to focus on.",
  "translatedText": "尽管如此，这个领域仍然是值得关注的关键特征。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.84,
  "end": 303.44
 },
 {
  "input": "You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum.",
  "translatedText": "您可以将其视为将与给定总和相对应的所有 结果的所有概率密度组合在一起的方法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.44,
  "end": 311.04
 },
 {
  "input": "In the specific case where these two functions look like e to the negative x squared and e to the negative y squared, the resulting 3D graph has a really nice property that you can exploit.",
  "translatedText": "在这两个函数看起来像 e 代表 x 负平方和 e 代表负 y 平方的特定情况下，生成的 3D 图形具有一个非常好的属性，您可以利用。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.3,
  "end": 323.5
 },
 {
  "input": "It's rotationally symmetric.",
  "translatedText": "它是旋转对称的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.72,
  "end": 325.68
 },
 {
  "input": "You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared, and this term describes the square of the distance between any point on the xy plane and the origin.",
  "translatedText": "您可以通过组合这些术语来看到这一点，并注意到它完全是 x 平方 加 y 平方的函数，并且该术语描述了 xy 平面上任何点与原点 之间距离的平方。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.88,
  "end": 338.46
 },
 {
  "input": "So in other words, the expression is purely a function of the distance from the origin.",
  "translatedText": "换句话说，该表达式纯粹是距原点距离的 函数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 339.2,
  "end": 343.16
 },
 {
  "input": "And by the way, this would not be true for any other distribution.",
  "translatedText": "顺便说一句，对于任何其他发行版来说，情况并非如此。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.56,
  "end": 347.92
 },
 {
  "input": "It's a property that uniquely characterizes bell curves.",
  "translatedText": "这是钟 形曲线的独特特征。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 351.28
 },
 {
  "input": "So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about, and honestly calculating the area would just amount to computing the original integral that defines a convolution in the first place.",
  "translatedText": "因此，对于大多数其他函数 对来说，这些对角线切片将是一些难以想象的复杂形 状，并且诚实地计算面积仅相当于计算首先定义卷 积的原始积分。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.16,
  "end": 365.54
 },
 {
  "input": "So in most cases, the visual intuition doesn't really buy you anything.",
  "translatedText": "所以在大多数情况下，视觉直觉并不能真正给你带来任何东西。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.94,
  "end": 369.36
 },
 {
  "input": "But in the case of bell curves, you can leverage that rotational symmetry.",
  "translatedText": "但对于钟形曲线，您可以利用旋转对称性。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.36,
  "end": 373.92
 },
 {
  "input": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
  "translatedText": "在这里，重点关注线 x 加 y 等于 s（对于 s 的某个值）上的这些切片之一。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.8,
  "end": 380.48
 },
 {
  "input": "And remember, the convolution that we're trying to compute is a function of s.",
  "translatedText": "请记住，我们尝试计算的卷积是 s 的函数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.3,
  "end": 385.84
 },
 {
  "input": "The thing that you want is an expression of s that tells you the area under this slice.",
  "translatedText": "您想要的是 s 的表达式，它告诉您该切片下的面积。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.84,
  "end": 391.1
 },
 {
  "input": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s.",
  "translatedText": "好吧，如果你看一下那条线，它会在 s 零处与 x 轴相交，在 s 零处与 y 轴相交。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.7,
  "end": 397.9
 },
 {
  "input": "And a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two.",
  "translatedText": "毕达哥拉斯的一点知识会告 诉你，从原点到这条线的直线距离是 s 除以二的平方根。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.32,
  "end": 405.32
 },
 {
  "input": "Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees, where you'd find something parallel to the y-axis the same distance away from the origin.",
  "translatedText": "现在，由于对称性，该切片与旋转 45 度的切片相同，在旋转 45 度时，您会发现平行于 y 轴且距原点距离相同的东西。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 416.36
 },
 {
  "input": "The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions, because it only involves taking an integral with respect to y.",
  "translatedText": "关键是计算平行于 y 轴的切片的其他面积比其他 方向的切片容易得多，因为它只涉及对 y 进行 积分。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.64,
  "end": 428.26
 },
 {
  "input": "The value of x on this slice is a constant.",
  "translatedText": "该切片上的 x 值是一个常数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.74,
  "end": 431.44
 },
 {
  "input": "Specifically, it would be the constant s divided by the square root of two.",
  "translatedText": "具体来说，它是常数 s 除以 2 的平方根。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 434.76
 },
 {
  "input": "So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out.",
  "translatedText": "因此，当您计算积分、找到该面积时，这里 所有的项都表现得就像只是某个数字，您可以将其分解出来。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.76,
  "end": 443.38
 },
 {
  "input": "This is the important point.",
  "translatedText": "这是重 要的一点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.88,
  "end": 444.94
 },
 {
  "input": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
  "translatedText": "所有涉及 s 的内容现在完全与集成变 量分开。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.28,
  "end": 450.2
 },
 {
  "input": "This remaining integral is a little bit tricky.",
  "translatedText": "这个剩余积分有点棘手。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.82,
  "end": 453.0
 },
 {
  "input": "I did a whole video on it, it's actually quite famous.",
  "translatedText": "我为此制作了一个完整的视 频，它实际上非常有名。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.08,
  "end": 455.2
 },
 {
  "input": "But you almost don't really care.",
  "translatedText": "但你几乎并不真正关心。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.5,
  "end": 456.9
 },
 {
  "input": "The point is that it's just some number.",
  "translatedText": "关键是这只是一些数字。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.24,
  "end": 459.0
 },
 {
  "input": "That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s.",
  "translatedText": "这个数字恰好是 pi 的平方根，但真正重要的是 它与 s 无关。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.0,
  "end": 465.48
 },
 {
  "input": "And essentially, this is our answer.",
  "translatedText": "本质上，这就是我们的答案。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.88,
  "end": 468.48
 },
 {
  "input": "We were looking for an expression for the area of these slices as a function of s, and now we have it.",
  "translatedText": "我们正在寻找这些 切片面积作为 s 函数的表达式，现在我们有了。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.78,
  "end": 474.28
 },
 {
  "input": "It looks like e to the negative s squared divided by two, scaled by some constant.",
  "translatedText": "它看起来就像 e 等于负 s 的平方除以二，并按某个常数缩放。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.38,
  "end": 478.84
 },
 {
  "input": "In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent.",
  "translatedText": "换句话说，它也是一条钟 形曲线，另一个高斯曲线，只是因为指数中的这两个而被拉伸了一点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.3,
  "end": 485.62
 },
 {
  "input": "As I said earlier, the convolution evaluated at s is not quite this area.",
  "translatedText": "正如我之前所说，在 s 处评估的卷积并不完全是这个区域。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.62,
  "end": 490.86
 },
 {
  "input": "Technically, it's this area divided by the square root of two.",
  "translatedText": "从技术上讲，它是 这个面积除以二的平方根。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.34,
  "end": 494.16
 },
 {
  "input": "We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant.",
  "translatedText": "我们在上一个视频中讨论过它，但这并不 重要，因为它只是被融入到常数中。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.8,
  "end": 499.24
 },
 {
  "input": "What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian.",
  "translatedText": "真正重要的是这样的结论：两个 高斯函数之间的卷积本身就是另一个高斯函数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.68,
  "end": 505.68
 },
 {
  "input": "If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma, essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front, and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma.",
  "translatedText": "如果您要返回并 重新引入均值为零和任意标准差西格玛的正态分布的所 有常数，本质上相同的推理将导致出现在指数和前面 的两个因子的相同平方根，由此得出的结论是，两个这 样的正态分布之间的卷积是另一个标准差平方根为两 倍西格玛的正态分布。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.56,
  "end": 530.38
 },
 {
  "input": "If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result.",
  "translatedText": "如果您之前没有计算过大量卷积，那么值得 强调的是，这是一个非常特殊的结果。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.98,
  "end": 536.06
 },
 {
  "input": "Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process.",
  "translatedText": "几乎总是你会得到一种完全 不同的函数，但这里的过程有一定的稳定性。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.38,
  "end": 542.5
 },
 {
  "input": "Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations.",
  "translatedText": "另外，对于 那些喜欢练习的人，我将在屏幕上留下一个内容，告诉您如何处理两 个不同标准差的情况。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.26,
  "end": 549.44
 },
 {
  "input": "Still, some of you might be raising your hands and saying, what's the big deal?",
  "translatedText": "尽管如此，你们中的一些人可能会举手说，有什 么大不了的？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 550.42,
  "end": 553.94
 },
 {
  "input": "I mean, when you first heard the question, what do you get when you add two normally distributed random variables, you probably even guessed that the answer should be another normally distributed variable.",
  "translatedText": "我的意思是，当您第一次听到这个问题时，当您添加两 个正态分布随机变量时会得到什么，您甚至可能猜测答案应该是另 一个正态分布变量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.48,
  "end": 564.32
 },
 {
  "input": "After all, what else is it going to be?",
  "translatedText": "毕竟，它还会是什么？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.76,
  "end": 566.36
 },
 {
  "input": "Normal distributions are supposedly quite common, so why not?",
  "translatedText": "正态分布据说很常 见，那为什么不呢？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 566.86,
  "end": 570.24
 },
 {
  "input": "You could even say that this should follow from the central limit theorem, but that would have it all backwards.",
  "translatedText": "你甚至可以说这应该是从中心极限定 理得出的，但那将是一切倒退。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 570.24,
  "end": 575.48
 },
 {
  "input": "First of all, the supposed ubiquity of normal distributions is often a little exaggerated, but to the extent that they do come up, it is because of the central limit theorem, but it would be cheating to say the central limit theorem implies this result because this computation we just did is the reason that the function at the heart of the central limit theorem is a Gaussian in the first place and not some other function.",
  "translatedText": "首先，所谓正态分布的 普遍性往往有点夸张，但就其确实出现而言，这是因 为中心极限定理，但如果说中心极限定理暗示了这 个结果，那是在作弊，因为我们刚刚所做的计算是因 为中心极限定理的核心函数首先是高斯函数而不是其 他函数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.18,
  "end": 597.06
 },
 {
  "input": "We've talked all about the central limit theorem before, but essentially it says if you repeatedly add copies of a random variable to itself, which mathematically looks like repeatedly computing convolutions against a given distribution, then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution.",
  "translatedText": "我们之前已经讨论过中心极限定理，但本质上它说 的是，如果你重复地将随机变量的副本添加到自身，这在数学上 看起来就像针对给定分布重复计算卷积，那么在适当的移位和 重新缩放之后，趋势是总是接近正态分布。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.06,
  "end": 616.5
 },
 {
  "input": "Technically there's a small assumption the distribution you start with can't have infinite variance, but it's a relatively soft assumption.",
  "translatedText": "从技术上讲，有一个 小的假设，您开始的分布不能有无限的方差，但这是一个相对较软的 假设。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 616.98,
  "end": 623.22
 },
 {
  "input": "The magic is that for a huge category of initial distributions, this process of adding a whole bunch of random variables drawn from that distribution always tends towards this one universal shape, a Gaussian.",
  "translatedText": "神奇的是，对于一大类初始分布，添加从该分布中 提取的一大堆随机变量的过程总是倾向于这种通用形状， 即高斯分布。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.22,
  "end": 635.1
 },
 {
  "input": "One common approach to proving this theorem involves two separate steps.",
  "translatedText": "证明该定理的一种常见方法涉及两个单独的步骤。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.82,
  "end": 639.3
 },
 {
  "input": "The first step is to show that for all the different finite variance distributions you might start with, there exists a single universal shape that this process of repeated convolutions tends towards.",
  "translatedText": "第一步是表明，对于您可能开始使用的所有不同的有限 方差分布，存在这种重复卷积过程所趋向的单一通用 形状。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.6,
  "end": 650.0
 },
 {
  "input": "This step is actually pretty technical, it goes a little beyond what I want to talk about here.",
  "translatedText": "这一步实际上是相当技术性的，它有点超出了我想在这里讨 论的范围。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.0,
  "end": 654.24
 },
 {
  "input": "You often use these objects called moment generating functions that gives you a very abstract argument that there must be some universal shape, but it doesn't make any claim about what that particular shape is, just that everything in this big family is tending towards a single point in the space of distributions.",
  "translatedText": "您经常使用这些称为矩生成函数的对象，它们为您提 供了一个非常抽象的论点，即必须存在某种通用形状，但它没有 对特定形状是什么做出任何声明，只是这个大家族中的所有事物 都趋向于分布空间中的单点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.52,
  "end": 669.98
 },
 {
  "input": "So then step number two is what we just showed in this video, prove that the convolution of two Gaussians gives another Gaussian.",
  "translatedText": "那么第二步就是我们刚刚在视频中展 示的，证明两个高斯函数的卷积给出另一个高斯函数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 670.62,
  "end": 677.4
 },
 {
  "input": "What that means is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a fixed point.",
  "translatedText": "这意味着当 您应用这个重复卷积过程时，高斯不会改变，它是一个固 定点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.4,
  "end": 684.06
 },
 {
  "input": "So the only thing it can approach is itself, and since it's one member in this big family of distributions, all of which must be tending towards a single universal shape, it must be that universal shape.",
  "translatedText": "因此，它唯一可以接近的就是它自己，并且由于它是这个分布大家 族中的一员，所有这些分布都必须趋向于单一的通用形状，因此它必须是 那个通用形状。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 684.2,
  "end": 695.06
 },
 {
  "input": "I mentioned at the start how this calculation, step two, is something that you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed by a geometric argument that leverages the rotational symmetry of this graph is that it directly connects to a few things that we've talked about on this channel before.",
  "translatedText": "我在开始时提到了第二步的计算是如何直接 进行的，只是象征性地使用定义，但我对利用该图的旋 转对称性的几何论证如此着迷的原因之一是它直接连接 到我们之前在这个频道上讨论过的一些事情。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.58,
  "end": 712.3
 },
 {
  "input": "For example, the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this rotational symmetry as the defining feature of the distribution, that it locks you into this e to the negative x squared form, and also as an added bonus it connects to the classic proof for why pi shows up in the formula, meaning we now have a direct line between the presence and mystery of that pi and the central limit theorem.",
  "translatedText": "例如，高斯 的 Herschel-Maxwell 导数，本质上是说你可以将 这种旋转对称性视为分布的定义特征，它将你锁定在这个 e 的负 x 平方形式中，同时也是一个额外的好处它与为什么 pi 出现在公 式中的经典证明有关，这意味着我们现在在 pi 的存在和神秘与中心 极限定理之间有直接的联系。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 712.4,
  "end": 736.5
 },
 {
  "input": "Also on a recent Patreon post, the channel supporter Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before, which leverages the use of entropy, and again for the theoretically curious among you I'll leave some links in the description.",
  "translatedText": "另外，在最近的 Patreon 帖子中，频 道支持者 Daksha Vaid-Quinter 让我注意到了一种我以前从 未见过的完全不同的方法，该方法利用了熵的使用，对于理论上好奇的你们，我将留 下一些链接在描述中。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 737.06,
  "end": 749.58
 },
 {
  "input": "By the way, if you want to stay up to date with new videos and also any other projects that I put out there like the Summer of Math Exposition, there is a mailing list.",
  "translatedText": "顺便说一句，如果您想及时了解新视频以及我在那 里发布的任何其他项目（例如夏季数学博览会），可以使用邮件列表。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 750.96,
  "end": 758.4
 },
 {
  "input": "It's relatively new and I'm pretty sparing about only posting what I think people will enjoy.",
  "translatedText": "这是相对较新的内容，我很少只发布我认为人们会喜欢的内容。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 758.72,
  "end": 762.78
 },
 {
  "input": "Usually I try not to be too promotional at the end of videos these days, but if you are interested in following the work that I do, this is probably one of the most enduring ways to do so.",
  "translatedText": "通常，我现在尽量不在视频结尾进行过多宣传，但如果您 有兴趣关注我所做的工作，这可能是最持久的方式之一。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 763.22,
  "end": 795.26
 }
]