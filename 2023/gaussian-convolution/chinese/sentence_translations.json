[
 {
  "input": "The basic function underlying a normal distribution, aka a Gaussian, is e to the negative x squared.",
  "translatedText": "正态分布 又名高斯分布 其函数本质上 是 e 的 -x² 次方",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.12
 },
 {
  "input": "But you might wonder, why this function?",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 6.64,
  "end": 8.34
 },
 {
  "input": "Of all the expressions we could dream up that give you some symmetric smooth graph with mass concentrated towards the middle, why is it that the theory of probability seems to have a special place in its heart for this particular expression?",
  "translatedText": "我们能想到很多个 图像对称光滑 质量集中的函数 但为什么 能在概率论的核心占据一席之地的 偏偏是它呢",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 8.72,
  "end": 20.44
 },
 {
  "input": "For the last many videos I've been hinting at an answer to this question, and here we'll finally arrive at something like a satisfying answer.",
  "translatedText": "在前面好几期视频中 我提示了这一问题的答案 现在 我们终于可以得到较为满意的结论了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 21.38,
  "end": 27.68
 },
 {
  "input": "As a quick refresher on where we are, a couple videos ago we talked about the central limit theorem, which describes how as you add multiple copies of a random variable, for example rolling a weighted die many different times, or letting a ball bounce off of a peg repeatedly, then the distribution describing that sum tends to look approximately like a normal distribution.",
  "translatedText": "为了让大家快速了解我们现在的情况，我们在几个视频前谈到了中心极限定理，它描述了当你增加一个随机变量的多个副本时，例如掷一个加权骰子很多次，或者让一个球从一个钉子上反复弹起，那么描述这个总和的分布就会趋于近似于正态分布。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 27.68,
  "end": 47.72
 },
 {
  "input": "What the central limit theorem says is as you make that sum bigger and bigger, under appropriate conditions, that approximation to a normal becomes better and better.",
  "translatedText": "中心极限定理说明了 在适当条件下 随着总和越来越大 总和 会越来越近似于正态",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 48.44,
  "end": 56.22
 },
 {
  "input": "But I never explained why this theorem is actually true.",
  "translatedText": "但我从来没有解释过为什么这个定理实际上是正确的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.94,
  "end": 60.18
 },
 {
  "input": "We only talked about what it's claiming.",
  "translatedText": "我们只谈了它的主张。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 60.22,
  "end": 61.98
 },
 {
  "input": "In the last video we started talking about the math involved in adding two random variables.",
  "translatedText": "上期视频 我们先讨论了随机变量相加所涉及的数学知识",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.08,
  "end": 67.88
 },
 {
  "input": "If you have two random variables, each following some distribution, then to find the distribution describing the sum of those variables, you compute something known as a convolution between the two original functions.",
  "translatedText": "如果两个随机变量分别符合某些分布 你想求出两变量之和的分布 就需要计算两函数的“卷积”",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 68.26,
  "end": 79.7
 },
 {
  "input": "And we spent a lot of time building up two distinct ways to visualize what this convolution operation really is.",
  "translatedText": "我们花时间构建了两种不同的思路 来形象地展示 卷积操作究竟是什么",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 79.88,
  "end": 85.94
 },
 {
  "input": "Today our basic job is to work through a particular example, which is to ask what happens when you add two normally distributed random variables, which, as you know by now, is the same as asking what do you get if you compute a convolution between two Gaussian functions.",
  "translatedText": "今天，我们的基本工作是通过一个特殊的例子来解决这个问题，即当我们将两个正态分布的随机变量相加时会发生什么，正如大家现在所知道的，这等同于我们在计算两个高斯函数之间的卷积时会得到什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.94,
  "end": 101.78
 },
 {
  "input": "I'd like to share an especially pleasing visual way that you can think about this calculation, which hopefully offers some sense of what makes the e to the negative x squared function special in the first place.",
  "translatedText": "我想分享一种很棒的直观的方式 来考虑这个计算 希望能让你从中了解到 函数 e 的 -x² 次方的独特之处",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 102.52,
  "end": 112.36
 },
 {
  "input": "After we walk through it, we'll talk about how this calculation is one of the steps involved in proving the central limit theorem.",
  "translatedText": "之后 我们将讨论 该计算是证明中心极限定理的其中一步",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 112.36,
  "end": 118.24
 },
 {
  "input": "It's the step that answers the question of why a Gaussian and not something else is the central limit.",
  "translatedText": "这一步回答的关键问题是 为何只有高斯分布 才是中心的极限",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.32,
  "end": 123.56
 },
 {
  "input": "But first, let's dive in.",
  "translatedText": "好 我们进入正题",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.2,
  "end": 125.84
 },
 {
  "input": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
  "translatedText": "高斯函数的完整公式比 e 的 -x² 次方复杂很多",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 129.78,
  "end": 134.44
 },
 {
  "input": "The exponent is typically written as negative one half times x divided by sigma squared, where sigma describes the spread of the distribution, specifically the standard deviation.",
  "translatedText": "指数通常写为 - 1/2 乘以 (x/σ)² 即标准差 其中 σ 描述了分布的分散程度",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 134.82,
  "end": 144.2
 },
 {
  "input": "All of this needs to be multiplied by a fraction on the front, which is there to make sure that the area under the curve is one, making it a valid probability distribution.",
  "translatedText": "所有这些 需要在前面乘以一个分数 以确保曲线下的面积为 1 使其成为有意义的概率分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 144.68,
  "end": 153.42
 },
 {
  "input": "And if you want to consider distributions that aren't necessarily centered at zero, you would also throw another parameter, mu, into the exponent like this.",
  "translatedText": "如果你想考虑那些 中心不一定为 0 的分布 你还需要在指数中加入另一个参数 µ 如上所示",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 154.02,
  "end": 161.18
 },
 {
  "input": "Although for everything we'll be doing here, we just consider centered distributions.",
  "translatedText": "不过接下来 我们只考虑以 0 为中心的分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.54,
  "end": 165.12
 },
 {
  "input": "Now if you look at our central goal for today, which is to compute a convolution between two Gaussian functions, the direct way to do this would be to take the definition of a convolution, this integral expression we built up last video, and then to plug in for each one of the functions involved the formula for a Gaussian.",
  "translatedText": "不妨想想我们今天的核心目标 要计算两个高斯函数之间的卷积 最直接的方法就是根据卷积的定义 即上个视频中构建的积分表达式 然后把高斯函数 分别带入积分中的两个函数项",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 165.8,
  "end": 183.76
 },
 {
  "input": "It's kind of a lot of symbols when you throw it all together, but more than anything, working this out is an exercise in completing the square.",
  "translatedText": "把一大堆符号摞在一起属实有点复杂 但说白了 这一团计算只是配方法的一道练习题",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 184.22,
  "end": 190.08
 },
 {
  "input": "And there's nothing wrong with that.",
  "translatedText": "直接计算也没什么毛病",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 190.56,
  "end": 191.58
 },
 {
  "input": "That will get you the answer that you want.",
  "translatedText": "你一样能得到你所期望的答案",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 191.72,
  "end": 193.22
 },
 {
  "input": "But of course, you know me, I'm a sucker for visual intuition, and in this case, there's another way to think about it that I haven't seen written about before that offers a very nice connection to other aspects of this distribution, like the presence of pi and certain ways to derive where it comes from.",
  "translatedText": "当然，你了解我，我很喜欢视觉直觉，而且在这种情况下，还有另一种我以前从未见过的思考方式，它与这种分布的其他方面有着很好的联系，比如π的存在以及推导π来源的某些方法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 193.76,
  "end": 207.86
 },
 {
  "input": "And the way I'd like to do this is by first peeling away all of the constants associated with the actual distribution, and just showing the computation for the simplified form, e to the negative x squared.",
  "translatedText": "我打算用一种新颖的方法来解释 首先我们将实际分布中的常数 通通丢掉 计算的过程中只用简化的 e 的 -x² 次方",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 208.2,
  "end": 217.96
 },
 {
  "input": "The essence of what we want to compute is what the convolution between two copies of this function looks like.",
  "translatedText": "我们实际想要得到的 是这函数的两个副本的卷积结果",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.96,
  "end": 224.08
 },
 {
  "input": "If you'll remember, in the last video we had two different ways to visualize convolutions, and the one we'll be using here is the second one involving diagonal slices.",
  "translatedText": "如果你还记得，在上一个视频中，我们有两种不同的方法来可视化卷积，这里我们要使用的是第二种涉及对角切片的方法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 224.46,
  "end": 232.92
 },
 {
  "input": "And as a quick reminder of the way that worked, if you have two different distributions that are described by two different functions, f and g, then every possible pair of values that you might get when you sample from these two distributions can be thought of as individual points on the xy-plane.",
  "translatedText": "快速复习一下这种方法的流程 如果你有两个分布 概率密度函数分别为 f 和 g 那么从这两个分布进行采样时 可能得到的每一对的值 都可以被视为 xy 平面上的一个点",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 233.28,
  "end": 249.56
 },
 {
  "input": "And the probability density of landing on one such point, assuming independence, looks like f of x times g of y.",
  "translatedText": "假设两个分布相互独立 落到这一个点附近的概率密度 像是 f(x) 乘以 g(y)",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 250.36,
  "end": 257.52
 },
 {
  "input": "So what we do is we look at a graph of that expression as a two-variable function of x and y, which is a way of showing the distribution of all possible outcomes when we sample from the two different variables.",
  "translatedText": "让我们来看一下 这个关于 x 和 y 的二元函数的图像 当我们从两个不同的变量进行采样时 它能显示所有可能结果的分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 258.0,
  "end": 269.62
 },
 {
  "input": "To interpret the convolution of f and g evaluated on some input s, which is a way of saying how likely are you to get a pair of samples that adds up to this sum s, what you do is you look at a slice of this graph over the line x plus y equals s, and you consider the area under that slice.",
  "translatedText": "要看出 f 和 g 的卷积在某个输入 s 上的几何意义 即 有多大可能得到这样一对样本 使得总和为 s 你应该查看该图像在直线 x + y = s 上的切片 并考虑该切片下的面积",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.56,
  "end": 289.3
 },
 {
  "input": "This area is almost, but not quite, the value of the convolution at s.",
  "translatedText": "这个面积差不多但不完全等于在 s 处的卷积值",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 291.1,
  "end": 296.32
 },
 {
  "input": "For a mildly technical reason, you need to divide by the square root of 2.",
  "translatedText": "出于轻微的技术原因，您需要除以 2 的平方根。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.8,
  "end": 300.16
 },
 {
  "input": "Still, this area is the key feature to focus on.",
  "translatedText": "总之这个面积是需要留意的关键特征",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 300.84,
  "end": 303.44
 },
 {
  "input": "You can think of it as a way to combine together all the probability densities for all of the outcomes corresponding to a given sum.",
  "translatedText": "你可以把它当作 把「一定总和下对应的所有结果」的概率密度组合起来",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 303.44,
  "end": 311.04
 },
 {
  "input": "In the specific case where these two functions look like e to the negative x squared and e to the negative y squared, the resulting 3D graph has a really nice property that you can exploit.",
  "translatedText": "在这个实例中 两函数分别是 e 的 -x² 次方和 -y² 次方 得到的三维图像具有非常好的特性 值得加以利用",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.3,
  "end": 323.5
 },
 {
  "input": "It's rotationally symmetric.",
  "translatedText": "即 旋转对称性",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.72,
  "end": 325.68
 },
 {
  "input": "You can see this by combining the terms and noticing that it's entirely a function of x squared plus y squared, and this term describes the square of the distance between any point on the xy plane and the origin.",
  "translatedText": "合并指数后你能观察到 指数项完全是关于 x² + y² 的函数 这项描述了「xy 平面上任意一点到原点的距离」的平方",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 326.88,
  "end": 338.46
 },
 {
  "input": "So in other words, the expression is purely a function of the distance from the origin.",
  "translatedText": "换句话说 这个表达式纯粹是与原点距离有关的函数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 339.2,
  "end": 343.16
 },
 {
  "input": "And by the way, this would not be true for any other distribution.",
  "translatedText": "顺便提一下 这个特性不适用于任何其他分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.56,
  "end": 347.92
 },
 {
  "input": "It's a property that uniquely characterizes bell curves.",
  "translatedText": "这个特性 还可以唯一刻画出钟形曲线",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 348.1,
  "end": 351.28
 },
 {
  "input": "So for most other pairs of functions, these diagonal slices will be some complicated shape that's hard to think about, and honestly, calculating the area would just amount to computing the original integral that defines a convolution in the first place.",
  "translatedText": "因此，对于大多数其他函数对来说，这些对角线切片将是一些难以想象的复杂形状，而且老实说，计算面积只相当于计算定义卷积的原始积分。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.16,
  "end": 365.54
 },
 {
  "input": "So in most cases, the visual intuition doesn't really buy you anything.",
  "translatedText": "因此通常来说 这种直观可视化并没有太大的好处",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.94,
  "end": 369.36
 },
 {
  "input": "But in the case of bell curves, you can leverage that rotational symmetry.",
  "translatedText": "但是对于钟形曲线 你可以巧用这种旋转对称性",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 370.36,
  "end": 373.92
 },
 {
  "input": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
  "translatedText": "这里 固定 s 值 注意直线 x + y = s 的切片",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.8,
  "end": 380.48
 },
 {
  "input": "And remember, the convolution that we're trying to compute is a function of s.",
  "translatedText": "记住 我们要计算的卷积是关于 s 的函数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 381.3,
  "end": 385.84
 },
 {
  "input": "The thing that you want is an expression of s that tells you the area under this slice.",
  "translatedText": "你想得到一个关于 s 的表达式 来告诉你该切片的面积",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.84,
  "end": 391.1
 },
 {
  "input": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis at zero s, and a little bit of Pythagoras will show you that the straight line distance from the origin to this line is s divided by the square root of two.",
  "translatedText": "那么，如果你观察这条直线，它与 x 轴相交于 s 零点，与 y 轴相交于 s 零点，稍微学一下勾股定理就会知道，从原点到这条直线的直线距离是 s 除以 2 的平方根。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 391.7,
  "end": 405.32
 },
 {
  "input": "Now, because of the symmetry, this slice is identical to one that you get rotating 45 degrees where you'd find something parallel to the y-axis the same distance away from the origin.",
  "translatedText": "现在，由于对称性，这个切片与旋转 45 度后得到的切片完全相同，在旋转 45 度后，你会发现与 Y 轴平行的东西与原点的距离相同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 405.86,
  "end": 416.36
 },
 {
  "input": "The key is that computing this other area of a slice parallel to the y-axis is much, much easier than slices in other directions because it only involves taking an integral with respect to y.",
  "translatedText": "关键在于，计算平行于 y 轴的切片的其他面积要比计算其他方向的切片容易得多，因为它只需要计算与 y 有关的积分。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 417.64,
  "end": 428.26
 },
 {
  "input": "The value of x on this slice is a constant.",
  "translatedText": "这个切片上的 x 值是一个常数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 428.74,
  "end": 431.44
 },
 {
  "input": "Specifically, it would be the constant s divided by the square root of two.",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 431.62,
  "end": 434.76
 },
 {
  "input": "So when you're computing the integral, finding this area, all of this term here behaves like it was just some number, and you can factor it out.",
  "translatedText": "因此当你在通过积分 去求面积时 这一项都只是一个数值 你可以将其提取出来",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 434.76,
  "end": 443.38
 },
 {
  "input": "This is the important point.",
  "translatedText": "这一点非常重要",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 443.88,
  "end": 444.94
 },
 {
  "input": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
  "translatedText": "所有和 s 有关的项 现在都与积分变量完全分开了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.28,
  "end": 450.2
 },
 {
  "input": "This remaining integral is a little bit tricky.",
  "translatedText": "剩余的这个积分有点棘手",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.82,
  "end": 453.0
 },
 {
  "input": "I did a whole video on it, it's actually quite famous.",
  "translatedText": "它其实很出名 我之前有一整期来讲它",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.08,
  "end": 455.2
 },
 {
  "input": "But you almost don't really care.",
  "translatedText": "但你真的不用太在意这个",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 455.5,
  "end": 456.9
 },
 {
  "input": "The point is that it's just some number.",
  "translatedText": "毕竟答案只是一个常数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 457.24,
  "end": 459.0
 },
 {
  "input": "That number happens to be the square root of pi, but what really matters is that it's something with no dependence on s.",
  "translatedText": "真正重要的是 它与 s 的取值无关",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.0,
  "end": 465.48
 },
 {
  "input": "And essentially this is our answer.",
  "translatedText": "这就是我们的答案。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 466.88,
  "end": 468.48
 },
 {
  "input": "We were looking for an expression for the area of these slices as a function of s, and now we have it.",
  "translatedText": "我们之前在找 以 s 表示切片面积的函数表达式 现在已经找到了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 468.78,
  "end": 474.28
 },
 {
  "input": "It looks like e to the negative s squared divided by two, scaled by some constant.",
  "translatedText": "看起来像是 e 的 -s²/2 次方乘以某常数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.38,
  "end": 478.84
 },
 {
  "input": "In other words, it's also a bell curve, another Gaussian, just stretched out a little bit because of this two in the exponent.",
  "translatedText": "换句话说 这仍是一个钟形曲线 只是由于指数中的 2 它被拉伸了一点",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.3,
  "end": 485.62
 },
 {
  "input": "As I said earlier, the convolution evaluated at s is not quite this area.",
  "translatedText": "就像我之前说过的 卷积在 s 处的值并不完全等于这个面积",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 485.62,
  "end": 490.86
 },
 {
  "input": "Technically it's this area divided by the square root of two.",
  "translatedText": "严格来说，就是这个面积除以 2 的平方根。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 491.34,
  "end": 494.16
 },
 {
  "input": "We talked about it in the last video, but it doesn't really matter because it just gets baked into the constant.",
  "translatedText": "我们在上一个视频中提到过 但倒也不重要 毕竟归给后面的常数就好",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 494.8,
  "end": 499.24
 },
 {
  "input": "What really matters is the conclusion that a convolution between two Gaussians is itself another Gaussian.",
  "translatedText": "真正重要的结论是 两个高斯函数之间的卷积 本身仍然是一个高斯函数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 499.68,
  "end": 505.68
 },
 {
  "input": "If you were to go back and reintroduce all of the constants for a normal distribution with a mean zero and an arbitrary standard deviation sigma, essentially identical reasoning will lead to the same square root of two factor that shows up in the exponent and out front, and it leads to the conclusion that the convolution between two such normal distributions is another normal distribution with a standard deviation square root of two times sigma.",
  "translatedText": "如果你回去重新加上正态分布中的常数 平均值依然是 0 标准差 σ 可以是任意值 那么用同样的方法推导 也会得到这个 √2 因子 它出现在指数 以及前面的倍数中 我们由此可以得出结论 两个标准差为 σ 的正态分布的卷积 是另一个正态分布 标准差为 √2σ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.56,
  "end": 530.38
 },
 {
  "input": "If you haven't computed a lot of convolutions before, it's worth emphasizing this is a very special result.",
  "translatedText": "如果你之前没有算过很多卷积的话 那我要强调 这个结论其实是非常特殊的",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 530.98,
  "end": 536.06
 },
 {
  "input": "Almost always you end up with a completely different kind of function, but here there's a sort of stability to the process.",
  "translatedText": "你几乎总是会得到另一个不同的函数 但正态分布的卷积具有某种稳定性",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.38,
  "end": 542.5
 },
 {
  "input": "Also, for those of you who enjoy exercises, I'll leave one up on the screen for how you would handle the case of two different standard deviations.",
  "translatedText": "我还给想练手的观众朋友们 留了一道题目 看看你如何应对两个标准差不同的情形",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.26,
  "end": 549.44
 },
 {
  "input": "Still, some of you might be raising your hands and saying, what's the big deal?",
  "translatedText": "话说回来 有人就要举手说了 “这有什么大不了的”",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 550.42,
  "end": 553.94
 },
 {
  "input": "I mean, when you first heard the question, what do you get when you add two normally distributed random variables, you probably even guessed that the answer should be another normally distributed variable.",
  "translatedText": "你看 当你一开始听到这个问题 “两个正态随机变量相加 是什么呢” 你甚至可以猜到 答案应该是另一个正态分布的变量",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 554.48,
  "end": 564.32
 },
 {
  "input": "After all, what else is it going to be?",
  "translatedText": "毕竟 还能有什么其他答案呢",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 564.76,
  "end": 566.36
 },
 {
  "input": "Normal distributions are supposedly quite common, so why not?",
  "translatedText": "正态分布都这么普遍了 还能是啥",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 566.86,
  "end": 570.24
 },
 {
  "input": "You could even say that this should follow from the central limit theorem.",
  "translatedText": "甚至可以说，这应该是中心极限定理的结论。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 570.24,
  "end": 573.34
 },
 {
  "input": "But that would have it all backwards.",
  "translatedText": "但这样做就全错了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.86,
  "end": 575.48
 },
 {
  "input": "First of all, the supposed ubiquity of normal distributions is often a little exaggerated, but to the extent that they do come up, it is because of the central limit theorem, but it would be cheating to say the central limit theorem implies this result because this computation we just did is the reason that the function at the heart of the central limit theorem is a Gaussian in the first place, and not some other function.",
  "translatedText": "首先，所谓正态分布无处不在的说法往往有些夸大其词，但就正态分布确实出现过的程度而言，这是因为中心极限定理的存在，但如果说中心极限定理暗示了这一结果，那就是欺骗了，因为我们刚才所做的计算就是中心极限定理的核心函数首先是高斯函数而不是其他函数的原因。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.18,
  "end": 597.06
 },
 {
  "input": "We've talked all about the central limit theorem before, but essentially it says if you repeatedly add copies of a random variable to itself, which mathematically looks like repeatedly computing convolutions against a given distribution, then after appropriate shifting and rescaling, the tendency is always to approach a normal distribution.",
  "translatedText": "我们前面讨论过中心极限定理 简而言之 如果你重复相加同一个随机变量 数学上像是对同一个概率密度函数多次重复求卷积 那么经过合适的平移和缩放 结果总会趋近于正态分布",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 597.06,
  "end": 616.5
 },
 {
  "input": "Technically, there's a small assumption the distribution you start with can't have infinite variance, but it's a relatively soft assumption.",
  "translatedText": "从技术上讲，有一个小假设，即你开始使用的分布不可能有无限方差，但这是一个相对较软的假设。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 616.98,
  "end": 623.22
 },
 {
  "input": "The magic is that for a huge category of initial distributions, this process of adding a whole bunch of random variables drawn from that distribution always tends towards this one universal shape, a Gaussian.",
  "translatedText": "神奇的是 对于很大一类的初始分布 从该分布中取很多随机变量并相加的这个过程 总会趋近于这个万有的形状 高斯函数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 623.22,
  "end": 635.1
 },
 {
  "input": "One common approach to proving this theorem involves two separate steps.",
  "translatedText": "证明该定理的一个常用手法是 分两步走",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.82,
  "end": 639.3
 },
 {
  "input": "The first step is to show that for all the different finite variance distributions you might start with, there exists a single universal shape that this process of repeated convolutions tends towards.",
  "translatedText": "第一步是要证明 对于任意有限方差的初始分布 存在某个万有的形状 其为重复卷积的过程的极限",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 639.6,
  "end": 650.0
 },
 {
  "input": "This step is actually pretty technical, it goes a little beyond what I want to talk about here.",
  "translatedText": "说实话 这一步需要一些技巧 有点超出了我想讨论的范围",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 650.0,
  "end": 654.24
 },
 {
  "input": "You often use these objects called moment generating functions that gives you a very abstract argument that there must be some universal shape, but it doesn't make any claim about what that particular shape is, just that everything in this big family is tending towards a single point in the space of distributions.",
  "translatedText": "这通常要用到一个工具 叫做“矩母函数” 然后很抽象地去论证 必然有这样一个万有形状 不过这并不能说明 这个具体的形状是什么 只能说明 一大类函数中的每一个 都会收敛到分布空间中唯一的一点",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.52,
  "end": 669.98
 },
 {
  "input": "So then step number two is what we just showed in this video, prove that the convolution of two Gaussians gives another Gaussian.",
  "translatedText": "因此 接下来的第二步 就是前面视频的部分 证明两个高斯函数的卷积 依然是高斯函数",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 670.62,
  "end": 677.4
 },
 {
  "input": "What that means is that as you apply this process of repeated convolutions, a Gaussian doesn't change, it's a fixed point, so the only thing it can approach is itself, and since it's one member in this big family of distributions, all of which must be tending towards a single universal shape, it must be that universal shape.",
  "translatedText": "这意味着，当你应用这个重复卷积的过程时，高斯不会改变，它是一个定点，所以它唯一能接近的就是它自己，而由于它是这个分布大家族中的一员，所有的分布都必须趋向于一个单一的普遍形状，所以它必须是那个普遍形状。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 677.4,
  "end": 695.06
 },
 {
  "input": "I mentioned at the start how this calculation, step two, is something that you can do directly, just symbolically with the definitions, but one of the reasons I'm so charmed by a geometric argument that leverages the rotational symmetry of this graph is that it directly connects to a few things that we've talked about on this channel before, for example, the Herschel-Maxwell derivation of a Gaussian, which essentially says that you can view this rotational symmetry as the defining feature of the distribution, that it locks you into this e to the negative x squared form, and also as an added bonus, it connects to the classic proof for why pi shows up in the formula, meaning we now have a direct line between the presence and mystery of that pi and the central limit theorem.",
  "translatedText": "我在一开始就提到了第二步的计算，你可以直接用定义符号来计算，但我之所以对利用这个图形的旋转对称性的几何论证如此着迷，原因之一是它直接与我们之前在这个频道讨论过的一些东西联系在一起，例如高斯的赫歇尔-麦克斯韦推导、从本质上讲，你可以将这种旋转对称性视为分布的定义特征，它将你锁定在负 x 平方形式的 e 上，而且作为额外的奖励，它还与 pi 为什么会出现在公式中的经典证明相联系，这意味着我们现在可以直接将 pi 的存在和神秘性与中心极限定理联系起来。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 695.58,
  "end": 736.5
 },
 {
  "input": "Also, on a recent Patreon post, the channel supporter Daksha Vaid-Quinter brought my attention to a completely different approach I hadn't seen before, which leverages the use of entropy, and again, for the theoretically curious among you, I'll leave some links in the description.",
  "translatedText": "此外，在最近的 Patreon 帖子中，频道支持者达克沙-瓦伊德-昆特（Daksha Vaid-Quinter）让我注意到了一种我之前从未见过的完全不同的方法，它利用了熵的作用，同样，对于你们中对理论好奇的人，我会在描述中留下一些链接。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 737.06,
  "end": 749.58
 },
 {
  "input": "By the way, if you want to stay up to date with new videos, and also any other projects that I put out there, like the Summer of Math Exposition, there is a mailing list.",
  "translatedText": "顺便说一下，如果你想了解新视频的最新情况，以及我推出的任何其他项目，比如数学之夏博览会，我们有一个邮件列表。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 750.96,
  "end": 758.4
 },
 {
  "input": "It's relatively new, and I'm pretty sparing about only posting what I think people will enjoy.",
  "translatedText": "这是一个相对较新的网站，我很谨慎，只发布我认为人们会喜欢的内容。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 758.72,
  "end": 762.78
 },
 {
  "input": "Usually I try not to be too promotional at the end of videos these days, but if you are interested in following the work that I do, this is probably one of the most enduring ways to do so.",
  "translatedText": "近来我在视频结尾尽量不太会打广告 但如果你希望关注我在做的工作 订阅这个邮件列表也许是最持久的方式了",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 763.22,
  "end": 795.26
 }
]