The months ahead of you hold within them a lot of hard work, some neat examples, some not-so-neat examples, beautiful connections to physics, not-so-beautiful piles of formulas to memorize, plenty of moments of getting stuck and banging your head into a wall, a few nice aha moments sprinkled in as well, and some genuinely lovely graphical intuition to help guide you through it all. 
But if the course ahead of you is anything like my first introduction to calculus, or any of the first courses I've seen in the years since, there's one topic you will not see, but which I believe stands to greatly accelerate your learning. 
You see, almost all of the visual intuitions from that first year are based on graphs. 
The derivative is the slope of a graph, the integral is a certain area under that graph. 
But as you generalize calculus beyond functions whose inputs and outputs are simply numbers, it's not always possible to graph the function you're analyzing. 
So if all your intuitions for the fundamental ideas, like derivatives, are rooted too rigidly in graphs, it can make for a very tall and largely unnecessary conceptual hurdle between you and the more quote-unquote advanced topics like multivariable calculus and complex analysis, differential geometry. 
What I want to share with you is a way to think about derivatives, which I'll refer to as the transformational view, that generalizes more seamlessly into some of those more general contexts where calculus comes up. 
And then we'll use this alternate view to analyze a fun puzzle about repeated fractions. 
But first off, I just want to make sure we're all on the same page about what the standard visual is. 
If you were to graph a function, which simply takes real numbers as inputs and outputs, one of the first things you learn in a calculus course is that the derivative gives you the slope of this graph, where what we mean by that is that the derivative of the function is a new function which for every input x returns that slope. 
Now I'd encourage you not to think of this derivative as slope idea as being the definition of a derivative. 
Instead think of it as being more fundamentally about how sensitive the function is to tiny little nudges around the input. 
And the slope is just one way to think about that sensitivity relevant only to this particular way of viewing functions. 
I have not just another video, but a full series on this topic if it's something you want to learn more about. 
The basic idea behind the alternate visual for the derivative is to think of this function as mapping all of the input points on the number line to their corresponding outputs on a different number line. 
In this context, what the derivative gives you is a measure of how much the input space gets stretched or squished in various regions. 
That is, if you were to zoom in around a specific input and take a look at some evenly spaced points around it, the derivative of the function of that input is going to tell you how spread out or contracted those points become after the mapping. 
Here, a specific example helps. 
Take the function x2, it maps 1 to 1, 2 to 4, 3 to 9, and so on. 
You can also see how it acts on all of the points in between. 
If you were to zoom in on a little cluster of points around the input 1, and see where they land around the relevant output, which for this function also happens to be 1, you'd notice that they tend to get stretched out. 
In fact, it roughly looks like stretching out by a factor of 2. 
The closer you zoom in, the more this local behavior looks just like multiplying by a This is what it means for the derivative of x2 at the input x equals 1 to be 2. 
It's what that fact looks like in the context of transformations. 
If you looked at a neighborhood of points around the input 3, they would get stretched out by a factor of 6. 
This is what it means for the derivative of this function at the input 3 to equal 6. 
Around the input 1 fourth, a small region tends to get contracted specifically by a factor of 1 half, and that's what it looks like for a derivative to be smaller than 1. 
The input 0 is interesting. 
Zooming in by a factor of 10, it doesn't really look like a constant stretching or squishing. 
For one thing, all of the outputs end up on the right positive side of things. 
As you zoom in closer and closer, by 100x or by 1000x, it looks more and more like a And this is what it looks like for the derivative to be 0. 
The local behavior looks more and more like multiplying the whole number line by 0. 
It doesn't have to completely collapse everything to a point at a particular zoom level, instead it's a matter of what the limiting behavior is as you zoom in closer and closer. 
It's also instructive to take a look at the negative inputs here. 
Things start to feel a little cramped since they collide with where all the positive input values go, and this is one of the downsides of thinking of functions as transformations. 
But for derivatives, we only really care about the local behavior anyway, what happens in a small range around a given input. 
Here, notice that the inputs in a little neighborhood around, say, negative 2, don't just get stretched out, they also get flipped around. 
Specifically, the action on such a neighborhood looks more and more like multiplying by negative 4 the closer you zoom in. 
This is what it looks like for the derivative of a function to be negative. 
And I think you get the point, this is all well and good, but let's see how this is actually useful in solving a problem. 
A friend of mine recently asked me a pretty fun question about the infinite fraction 1 plus 1 divided by 1 plus 1 divided by 1 plus 1 divided by 1, and clearly you watch math videos online, so maybe you've seen this before, but my friend's question actually cuts to something you might not have thought about before, relevant to the view of derivatives we're looking at here. 
The typical way you might evaluate an expression like this is to set it equal to x, and then notice that there is a copy of the full fraction inside itself. 
So you can replace that copy with another x, and then just solve for x. 
That is, what you want is to find a fixed point of the function 1 plus 1 divided by x. 
But here's the thing, there are actually two solutions for x, two special numbers where 1 plus 1 divided by that number gives you back the same thing. 
One is the golden ratio, phi, around 1.618, and the other is negative 0.618, which happens to be negative 1 divided by phi. 
I like to call this other number phi's little brother, since just about any property that phi has, this number also has. 
And this raises the question, would it be valid to say that the infinite fraction we saw is somehow also equal to phi's little brother, negative 0.618? 
Maybe you initially say, obviously not, everything on the left hand side is positive, so how could it possibly equal a negative number? 
Well, first we should be clear about what we actually mean by an expression like this. 
One way you could think about it, and it's not the only way, there's freedom for choice here, is to imagine starting with some constant, like 1, and then repeatedly applying the function 1 plus 1 divided by x, and then asking, what is this approach as you keep going? 
I mean, certainly symbolically what you get looks more and more like our infinite fraction, so maybe if you wanted to equal a number, you should ask what this series of numbers approaches. 
And if that's your view of things, maybe you start off with a negative number, so it's not so crazy for the whole expression to end up negative. 
After all, if you start with negative 1 divided by phi, then applying this function 1 plus 1 over x, you get back the same number, negative 1 divided by phi, so no matter how many times you apply it, you're staying fixed at this value. 
But even then, there is one reason you should view phi as the favorite brother in this pair. 
Here, try this, pull up a calculator of some kind, then start with any random number, and plug it into this function, 1 plus 1 divided by x, and plug that number into 1 plus 1 over x, and again, and again, and again, and again. 
No matter what constant you start with, you eventually end up at 1.618. 
Even if you start with a negative number, even one that's really close to phi's little brother, eventually it shies away from that value and jumps back over to phi. 
So, what's going on here? 
Why is one of these fixed points favored above the other one? 
Maybe you can already see how the transformational understanding of derivatives is helpful for understanding this setup, but for the sake of having a point of contrast, I want to show you how a problem like this is often taught using graphs. 
If you were to plug in some random input to this function, the y value tells you the corresponding output, right? 
So to think about plugging that output back into the function, you might first move horizontally until you hit the line y equals x, and that's going to give you a position where the x value corresponds to your previous y value, right? 
So then from there, you can move vertically to see what output this new x value has, and then you repeat. 
You move horizontally to the line y equals x to find a point whose x value is the same as the output you just got, and then you move vertically to apply the function again. 
Now personally, I think this is kind of an awkward way to think about repeatedly applying a function, don't you? 
I mean, it makes sense, but you kind of have to pause and think about it to remember which way to draw the lines. 
And you can, if you want, think through what conditions make this spiderweb process narrow in on a fixed point, versus propagating away from it. 
In fact, go ahead, pause right now, and try to think it through as an exercise. 
It has to do with slopes. 
Or if you want to skip the exercise for something that I think gives a much more satisfying understanding, think about how this function acts as a transformation. 
So I'm going to go ahead and start here by drawing a bunch of arrows to indicate where the various sampled input points will go. 
And side note, don't you think this gives a neat emergent pattern? 
I wasn't expecting this, but it was cool to see it pop up when animating. 
I guess the action of 1 divided by x gives this nice emergent circle, and then we're just shifting things over by 1. 
Anyway, I want you to think about what it means to repeatedly apply some function, like 1 plus 1 over x, in this context. 
Well after letting it map all of the inputs to the outputs, you could consider those as the new inputs, and then just apply the same process again, and then again, and do it however many times you want. 
Notice, in animating this with a few dots representing the sample points, it doesn't take many iterations at all before all of those dots kind of clump in around 1. 
618. 
Now remember, we know that 1.618 and its little brother, negative 0.618 on and on, stay fixed in place during each iteration of this process. 
But zoom in on a neighborhood around phi. 
During the map, points in that region get contracted around phi, meaning that the function 1 plus 1 over x has a derivative with a magnitude less than 1 at this input. 
In fact, this derivative works out to be around negative 0.38. 
So what that means is that each repeated application scrunches the neighborhood around this number smaller and smaller, like a gravitational pull towards phi. 
So now tell me what you think happens in the neighborhood of phi's little brother. 
Over there, the derivative actually has a magnitude larger than 1, so points near the fixed point are repelled away from it. 
And when you work it out, you can see that they get stretched by more than a factor of 2 in each iteration. 
They also get flipped around, because the derivative is negative here, but the salient fact for the sake of stability is just the magnitude. 
Mathematicians would call this right value a stable fixed point, and the left one is an unstable fixed point. 
Something is considered stable if when you perturb it just a little bit, it tends to come back towards where it started, rather than going away from it. 
So what we're seeing is a very useful little fact, that the stability of a fixed point is determined by whether or not the magnitude of its derivative is bigger or smaller than 1. 
This explains why phi always shows up in the numerical play, where you're just hitting enter on your calculator over and over, but phi's little brother never does. 
As to whether or not you want to consider phi's little brother a valid value of the infinite fraction, well that's really up to you. 
Everything we just showed suggests that if you think of this expression as representing a limiting process, then because every possible seed value other than phi's little brother gives you a series converging to phi, it does feel silly to put them on equal footing with each other. 
But maybe you don't think of it as a limit, maybe the kind of math you're doing lends itself to treating this as a purely algebraic object, like the solutions of a polynomial, which simply has multiple values. 
Anyway, that's beside the point, and my point here is not that viewing derivatives as this change in density is somehow better than the graphical intuition on the whole. 
In fact, picturing an entire function this way can be kind of clunky and impractical as compared to graphs. 
My point is that it deserves more of a mention in most of the introductory calculus courses, because it can help make a student's understanding of the derivative a little more flexible. 
Like I mentioned, the real reason I'd recommend you carry this perspective with you as you learn new topics is not so much for what it does with your understanding of single variable calculus, it's for what comes after.