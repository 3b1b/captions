[
 {
  "input": "Hello, hello again.",
  "translatedText": "Привіт, привіт ще раз.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.98,
  "end": 13.0
 },
 {
  "input": "So moving forward, I'll be assuming that you have a visual understanding of linear transformations and how they're represented with matrices, the way that I've been talking about in the last few videos.",
  "translatedText": "Отже, рухаючись вперед, я буду припускати, що ви маєте візуальне розуміння лінійних перетворень і того, як вони представлені за допомогою матриць, про що я говорив у кількох останніх відео.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.52,
  "end": 21.84
 },
 {
  "input": "If you think about a couple of these linear transformations, you might notice how some of them seem to stretch space out, while others squish it on in.",
  "translatedText": "Якщо ви подумаєте про пару цих лінійних перетворень, ви можете помітити, як деякі з них, здається, розтягують простір, а інші здавлюють його.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 22.66,
  "end": 30.42
 },
 {
  "input": "One thing that turns out to be pretty useful for understanding one of these transformations is to measure exactly how much it stretches or squishes things.",
  "translatedText": "Одна річ, яка виявляється досить корисною для розуміння одного з цих перетворень, це точно виміряти, наскільки воно розтягує або хлюпає речі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.14,
  "end": 38.92
 },
 {
  "input": "More specifically, to measure the factor by which the area of a given region increases or decreases.",
  "translatedText": "Точніше, щоб виміряти коефіцієнт, на який збільшується або зменшується площа даного регіону.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.52,
  "end": 45.82
 },
 {
  "input": "For example, look at the matrix with columns 3, 0 and 0, 2.",
  "translatedText": "Наприклад, подивіться на матрицю зі стовпцями 3, 0 і 0, 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 47.18,
  "end": 50.88
 },
 {
  "input": "It scales i-hat by a factor of 3 and scales j-hat by a factor of 2.",
  "translatedText": "Він масштабує i-hat у 3 рази, а j-hat — у 2 рази.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.32,
  "end": 56.18
 },
 {
  "input": "Now, if we focus our attention on the 1 by 1 square whose bottom sits on i-hat and whose left side sits on j-hat, after the transformation, this turns into a 2 by 3 rectangle.",
  "translatedText": "Тепер, якщо ми зосередимо нашу увагу на квадраті 1 на 1, нижня частина якого розташована на i-hat, а ліва сторона — на j-hat, після перетворення він перетвориться на прямокутник 2 на 3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.7,
  "end": 67.52
 },
 {
  "input": "Since this region started out with area 1 and ended up with area 6, we can say the linear transformation has scaled its area by a factor of 6.",
  "translatedText": "Оскільки ця область починалася з області 1 і закінчувалася областю 6, ми можемо сказати, що лінійна трансформація збільшила її площу в 6 разів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 68.38,
  "end": 77.28
 },
 {
  "input": "Compare that to a shear whose matrix has columns 1, 0 and 1, 1, meaning i-hat stays in place and j-hat moves over to 1, 1.",
  "translatedText": "Порівняйте це зі зсувом, матриця якого має стовпці 1, 0 і 1, 1, тобто i-hat залишається на місці, а j-hat переходить до 1, 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.18,
  "end": 86.1
 },
 {
  "input": "That same unit square determined by i-hat and j-hat gets slanted and turned into a parallelogram, but the area of that parallelogram is still 1, since its base and height each continue to have length 1.",
  "translatedText": "Той самий одиничний квадрат, визначений i-hat і j-hat, нахиляється і перетворюється на паралелограм, але площа цього паралелограма все ще дорівнює 1, оскільки його основа і висота продовжують мати довжину 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.0,
  "end": 98.38
 },
 {
  "input": "So, even though this transformation smushes things about, it seems to leave areas unchanged, at least in the case of that 1 unit square.",
  "translatedText": "Таким чином, навіть якщо це перетворення руйнує речі, воно, здається, залишає площі незмінними, принаймні у випадку цієї 1 одиниці квадрата.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.18,
  "end": 105.62
 },
 {
  "input": "Actually though, if you know how much the area of that one single unit square changes, it can tell you how the area of any possible region in space changes.",
  "translatedText": "Насправді, якщо ви знаєте, наскільки змінюється площа цього єдиного квадрата, це може сказати вам, як змінюється площа будь-якої можливої області в просторі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.82,
  "end": 115.52
 },
 {
  "input": "For starters, notice that whatever happens to one square in the grid has to happen to any other square in the grid, no matter the size.",
  "translatedText": "Для початку зауважте, що все, що станеться з одним квадратом у сітці, має статися з будь-яким іншим квадратом у сітці, незалежно від розміру.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 116.3,
  "end": 123.58
 },
 {
  "input": "This follows from the fact that grid lines remain parallel and evenly spaced.",
  "translatedText": "Це випливає з того факту, що лінії сітки залишаються паралельними і рівномірними.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.34,
  "end": 128.04
 },
 {
  "input": "Then, any shape that's not a grid square can be approximated by grid squares pretty well, with arbitrarily good approximations if you use small enough grid squares.",
  "translatedText": "Тоді будь-яка форма, яка не є квадратом сітки, може бути досить добре апроксимована квадратами сітки, з як завгодно хорошими наближеннями, якщо ви використовуєте досить маленькі квадрати сітки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 128.76,
  "end": 137.52
 },
 {
  "input": "So, since the areas of all those tiny grid squares are being scaled by some single amount, the area of the blob as a whole will also be scaled by that same single amount.",
  "translatedText": "Отже, оскільки площі всіх цих крихітних квадратів сітки масштабуються на певну одну величину, площа краплі в цілому також буде масштабована на ту саму одну величину.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 137.52,
  "end": 147.82
 },
 {
  "input": "This very special scaling factor, the factor by which a linear transformation changes any area, is called the determinant of that transformation.",
  "translatedText": "Цей особливий коефіцієнт масштабування, коефіцієнт, за допомогою якого лінійне перетворення змінює будь-яку область, називається детермінантом цього перетворення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 148.9,
  "end": 157.12
 },
 {
  "input": "I'll show how to compute the determinant of a transformation using its matrix later on in this video, but understanding what it represents is, trust me, much more important than the computation.",
  "translatedText": "Пізніше в цьому відео я покажу, як обчислити детермінант перетворення за допомогою його матриці, але розуміння того, що воно представляє, повірте мені, набагато важливіше, ніж обчислення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.12,
  "end": 168.42
 },
 {
  "input": "For example, the determinant of a transformation would be 3 if that transformation increases the area of a region by a factor of 3.",
  "translatedText": "Наприклад, визначник трансформації дорівнюватиме 3, якщо ця трансформація збільшує площу регіону в 3 рази.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.58,
  "end": 177.04
 },
 {
  "input": "The determinant of a transformation would be 1 half if it squishes down all areas by a factor of 1 half.",
  "translatedText": "Детермінант трансформації дорівнює 1 половині, якщо вона зменшує всі області в 1 раз.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 178.18,
  "end": 184.34
 },
 {
  "input": "And the determinant of a 2D transformation is 0 if it squishes all of space onto a line, or even onto a single point.",
  "translatedText": "І детермінант двовимірного перетворення дорівнює 0, якщо він розміщує весь простір на лінії або навіть на одній точці.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 186.0,
  "end": 193.5
 },
 {
  "input": "Since then, the area of any region would become 0.",
  "translatedText": "Відтоді площа будь-якої області стала б 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 194.0,
  "end": 196.76
 },
 {
  "input": "That last example will prove to be pretty important.",
  "translatedText": "Цей останній приклад виявиться дуже важливим.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.62,
  "end": 199.6
 },
 {
  "input": "It means that checking if the determinant of a given matrix is 0 will give a way of computing whether or not the transformation associated with that matrix squishes everything into a smaller dimension.",
  "translatedText": "Це означає, що перевірка того, чи визначник даної матриці дорівнює 0, дасть спосіб обчислення, чи перетворення, пов’язане з цією матрицею, зміщує все в менший вимір.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 200.02,
  "end": 209.74
 },
 {
  "input": "You'll see in the next few videos why this is even a useful thing to think about, but for now, I just want to lay down all of the visual intuition, which, in and of itself, is a beautiful thing to think about.",
  "translatedText": "У кількох наступних відео ви побачите, чому це навіть корисно подумати, але зараз я просто хочу викласти всю візуальну інтуїцію, яка, сама по собі, є прекрасною річчю для роздумів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.52,
  "end": 220.1
 },
 {
  "input": "Okay, I need to confess that what I've said so far is not quite right.",
  "translatedText": "Гаразд, мушу зізнатися, що те, що я сказав досі, не зовсім правильно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 222.12,
  "end": 225.56
 },
 {
  "input": "The full concept of the determinant allows for negative values.",
  "translatedText": "Повна концепція визначника допускає від’ємні значення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.88,
  "end": 229.28
 },
 {
  "input": "But what would the idea of scaling an area by a negative amount even mean?",
  "translatedText": "Але що взагалі означатиме ідея масштабування області на від’ємну суму?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 229.72,
  "end": 233.48
 },
 {
  "input": "This has to do with the idea of orientation.",
  "translatedText": "Це пов’язано з ідеєю орієнтації.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 234.94,
  "end": 236.96
 },
 {
  "input": "For example, notice how this transformation gives the sensation of flipping space over.",
  "translatedText": "Наприклад, зверніть увагу, як ця трансформація створює відчуття перевертання простору.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 237.8,
  "end": 242.68
 },
 {
  "input": "If you were thinking of 2D space as a sheet of paper, a transformation like that one seems to turn over that sheet onto the other side.",
  "translatedText": "Якщо ви думали про двовимірний простір як про аркуш паперу, подібне перетворення ніби перевертає аркуш на іншу сторону.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 243.24,
  "end": 249.86
 },
 {
  "input": "Many transformations that do this are said to invert the orientation of space.",
  "translatedText": "Кажуть, що багато перетворень, які роблять це, інвертують орієнтацію простору.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.64,
  "end": 255.04
 },
 {
  "input": "Another way to think about it is in terms of i-hat and j-hat.",
  "translatedText": "Інший спосіб подумати про це в термінах i-hat і j-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.84,
  "end": 258.6
 },
 {
  "input": "Notice that in their starting positions, j-hat is to the left of i-hat.",
  "translatedText": "Зверніть увагу, що в початкових позиціях j-hat знаходиться ліворуч від i-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 259.16,
  "end": 263.06
 },
 {
  "input": "If after a transformation, j-hat is now on the right of i-hat, the orientation of space has been inverted.",
  "translatedText": "Якщо після трансформації j-hat тепер знаходиться праворуч від i-hat, орієнтація простору була інвертована.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 263.62,
  "end": 270.2
 },
 {
  "input": "Whenever this happens, whenever the orientation of space is inverted, the determinant will be negative.",
  "translatedText": "Щоразу, коли це відбувається, коли орієнтація простору інвертується, визначник буде негативним.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.12,
  "end": 276.58
 },
 {
  "input": "The absolute value of the determinant, though, still tells you the factor by which areas have been scaled.",
  "translatedText": "Однак абсолютне значення визначника все ще говорить вам про коефіцієнт, за яким масштабовано площі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.46,
  "end": 282.4
 },
 {
  "input": "For example, the matrix with columns 1, 1 and 2, negative 1 encodes a transformation that has determinant, I'll just tell you, negative 3.",
  "translatedText": "Наприклад, матриця зі стовпцями 1, 1 і 2, мінус 1 кодує перетворення, яке має детермінант, я просто скажу вам, мінус 3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.02,
  "end": 290.68
 },
 {
  "input": "And what this means is that space gets flipped over and areas are scaled by a factor of 3.",
  "translatedText": "І це означає, що простір перевертається, а області масштабуються з коефіцієнтом 3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.46,
  "end": 296.28
 },
 {
  "input": "So why would this idea of a negative area scaling factor be a natural way to describe orientation flipping?",
  "translatedText": "Тож чому ця ідея негативного коефіцієнта масштабування площі є природним способом опису перевертання орієнтації?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 297.78,
  "end": 303.7
 },
 {
  "input": "Think about the series of transformations you get by slowly letting i-hat get closer and closer to j-hat.",
  "translatedText": "Подумайте про ряд трансформацій, які ви отримуєте, повільно дозволяючи i-hat наближатися до j-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.26,
  "end": 310.14
 },
 {
  "input": "As i-hat gets closer, all of the areas in space are getting squished more and more, meaning the determinant approaches 0.",
  "translatedText": "У міру наближення i-hat всі області в просторі дедалі більше здавлюються, тобто визначник наближається до 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 310.72,
  "end": 317.1
 },
 {
  "input": "Once i-hat lines up perfectly with j-hat, the determinant is 0.",
  "translatedText": "Якщо i-hat ідеально збігається з j-hat, визначник дорівнює 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 317.82,
  "end": 321.64
 },
 {
  "input": "Then, if i-hat continues the way that it was going, doesn't it kind of feel natural for the determinant to keep decreasing into the negative numbers?",
  "translatedText": "Тоді, якщо i-hat продовжує шлях, яким він йшов, чи не здається природним, що визначник продовжує зменшуватися до від’ємних чисел?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.44,
  "end": 329.28
 },
 {
  "input": "So that's the understanding of determinants in two dimensions.",
  "translatedText": "Отже, це розуміння визначників у двох вимірах.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 330.68,
  "end": 333.56
 },
 {
  "input": "What do you think it should mean for three dimensions?",
  "translatedText": "Як ви думаєте, що це має означати для трьох вимірів?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 333.56,
  "end": 335.94
 },
 {
  "input": "It also tells you how much a transformation scales things, but this time it tells you how much volumes get scaled.",
  "translatedText": "Він також повідомляє вам, наскільки трансформація масштабує речі, але цього разу вона повідомляє вам, наскільки обсяги масштабуються.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.92,
  "end": 343.24
 },
 {
  "input": "Just as in two dimensions, where this is easiest to think about by focusing on one particular square with an area 1 and watching only what happens to it, in three dimensions it helps to focus your attention on the specific 1 by 1 by 1 cube whose edges are resting on the basis vectors i-hat, j-hat, and k-hat.",
  "translatedText": "Так само, як у двох вимірах, де це найпростіше подумати, зосередившись на одному конкретному квадраті з площею 1 і спостерігаючи лише за тим, що з ним відбувається, у тривимірах це допомагає зосередити вашу увагу на конкретному кубі 1 на 1 на 1, чий ребра спираються на базисні вектори i-hat, j-hat і k-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 345.34,
  "end": 363.44
 },
 {
  "input": "After the transformation, that cube might get warped into some kind of slanty slanty cube.",
  "translatedText": "Після перетворення цей куб може деформуватися в якийсь похилий похилий куб.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 364.32,
  "end": 369.3
 },
 {
  "input": "This shape, by the way, has the best name ever, parallel a pipette, a name that's made even more delightful when your professor has a nice thick Russian accent.",
  "translatedText": "Ця форма, до речі, має найкращу назву, паралельна піпетці, назва, яка стає ще приємнішою, коли ваш професор має симпатичний російський акцент.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 370.34,
  "end": 377.44
 },
 {
  "input": "Since this cube starts out with a volume of 1 and the determinant gives the factor by which any volume is scaled, you can think of the determinant simply as being the volume of that parallel a pipette that the cube turns into.",
  "translatedText": "Оскільки цей куб починається з об’єму 1, а визначник дає коефіцієнт, за яким масштабується будь-який об’єм, ви можете думати про визначник просто як про об’єм тієї паралельної піпетки, на яку перетворюється куб.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.52,
  "end": 390.64
 },
 {
  "input": "A determinant of 0 would mean that all of space is squished onto something with 0 volume, meaning either a flat plane, a line, or, in the most extreme case, onto a single point.",
  "translatedText": "Визначник 0 означав би, що весь простір стиснутий на щось із нульовим об’ємом, що означає або плоску площину, лінію, або, у крайньому випадку, на одну точку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 392.38,
  "end": 402.5
 },
 {
  "input": "Those of you who watched chapter 2 will recognize this as meaning that the columns of the matrix are linearly dependent.",
  "translatedText": "Ті з вас, хто дивився розділ 2, зрозуміють, що стовпці матриці є лінійно залежними.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.76,
  "end": 409.24
 },
 {
  "input": "Can you see why?",
  "translatedText": "Ви бачите чому?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.76,
  "end": 410.42
 },
 {
  "input": "What about negative determinants?",
  "translatedText": "А як щодо негативних детермінант?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 414.92,
  "end": 416.64
 },
 {
  "input": "What should that mean for three dimensions?",
  "translatedText": "Що це має означати для трьох вимірів?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.78,
  "end": 418.1
 },
 {
  "input": "One way to describe orientation in 3D is with the right hand rule.",
  "translatedText": "Один із способів описати орієнтацію в 3D – це правило правої руки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.78,
  "end": 422.68
 },
 {
  "input": "Point the forefinger of your right hand in the direction of i-hat, stick out your middle finger in the direction of j-hat, and notice how when you point your thumb up, it's in the direction of k-hat.",
  "translatedText": "Направте вказівний палець правої руки в напрямку i-hat, витягніть середній палець у напрямку j-hat і зверніть увагу, що, коли ви вказуєте великим пальцем вгору, він знаходиться в напрямку k-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.3,
  "end": 432.76
 },
 {
  "input": "If you can still do that after the transformation, orientation has not changed, and the determinant is positive.",
  "translatedText": "Якщо ви все ще можете це зробити після перетворення, орієнтація не змінилася, а визначник позитивний.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.88,
  "end": 440.9
 },
 {
  "input": "Otherwise, if after the transformation it only makes sense to do that with your left hand, orientation has been flipped, and the determinant is negative.",
  "translatedText": "Інакше, якщо після перетворення має сенс робити це лише лівою рукою, орієнтацію було перевернуто, а визначник є від’ємним.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 441.54,
  "end": 449.38
 },
 {
  "input": "So if you haven't seen it before, you're probably wondering by now, how do you actually compute the determinant?",
  "translatedText": "Отже, якщо ви не бачили цього раніше, ви, мабуть, зараз дивуєтеся, як насправді обчислити визначник?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 451.9,
  "end": 457.04
 },
 {
  "input": "For a 2x2 matrix with entries a, b, c, d, the formula is a times d minus b times c.",
  "translatedText": "Для матриці 2x2 із записами a, b, c, d формула така: a, помножене на d, мінус b, помножене на c.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.56,
  "end": 464.42
 },
 {
  "input": "Here's part of an intuition for where this formula comes from.",
  "translatedText": "Ось частина інтуїції, звідки походить ця формула.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.74,
  "end": 468.5
 },
 {
  "input": "Let's say that the terms b and c both happened to be 0.",
  "translatedText": "Скажімо, обидва доданки b і c дорівнюють 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.88,
  "end": 471.78
 },
 {
  "input": "Then the term a tells you how much i-hat is stretched in the x direction, and the term d tells you how much j-hat is stretched in the y direction.",
  "translatedText": "Тоді член a говорить вам, наскільки i-hat розтягнуто в напрямку x, а член d говорить вам, наскільки j-hat розтягнуто в напрямку y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 471.78,
  "end": 481.16
 },
 {
  "input": "So since those other terms are 0, it should make sense that a times d gives the area of the rectangle that our favorite unit square turns into, kind of like the 3, 0, 0, 2 example from earlier.",
  "translatedText": "Отже, оскільки ці інші доданки дорівнюють 0, має бути зрозуміло, що a помножене на d дає площу прямокутника, на який перетворюється наш улюблений одиничний квадрат, як у попередньому прикладі 3, 0, 0, 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 482.76,
  "end": 493.36
 },
 {
  "input": "Even if only one of b or c are 0, you'll have a parallelogram with a base a and a height d, so the area should still be a times d.",
  "translatedText": "Навіть якщо лише одне з b або c дорівнює 0, у вас буде паралелограм з основою a і висотою d, тому площа все одно має бути помноженою на d.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.36,
  "end": 504.5
 },
 {
  "input": "Loosely speaking, if both b and c are non-zero, then that b times c term tells you how much this parallelogram is stretched or squished in the diagonal direction.",
  "translatedText": "Грубо кажучи, якщо b і c не дорівнюють нулю, тоді цей доданок b помножено на c говорить вам, наскільки цей паралелограм розтягнутий або стиснутий у діагональному напрямку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 505.46,
  "end": 515.46
 },
 {
  "input": "For those of you hungry for a more precise description of this b times c term, here's a helpful diagram if you'd like to pause and ponder.",
  "translatedText": "Для тих із вас, хто прагне отримати більш точний опис цього члена b на c, ось корисна діаграма, якщо ви хочете зупинитися та поміркувати.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 516.66,
  "end": 522.88
 },
 {
  "input": "Now if you feel like computing determinants by hand is something that you need to know, the only way to get it down is to just practice it with a few.",
  "translatedText": "Тепер, якщо ви відчуваєте, що обчислення визначників вручну — це те, що вам потрібно знати, єдиний спосіб зрозуміти це — просто попрактикуватися в цьому з кількома.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 523.98,
  "end": 531.2
 },
 {
  "input": "There's really not that much I can say or animate that's going to drill in the computation.",
  "translatedText": "Насправді я не так багато можу сказати чи оживити, що може вплинути на обчислення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 531.2,
  "end": 535.18
 },
 {
  "input": "This is all triply true for three-dimensional determinants.",
  "translatedText": "Все це втричі вірно для тривимірних визначників.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.12,
  "end": 538.64
 },
 {
  "input": "There is a formula, and if you feel like that's something you need to know, you should practice with a few matrices, or, you know, go watch Sal Khan work through a few.",
  "translatedText": "Є формула, і якщо ви відчуваєте, що це те, що вам потрібно знати, вам слід потренуватися з кількома матрицями, або, знаєте, піти подивитися, як Сал Хан працює над кількома.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.04,
  "end": 546.34
 },
 {
  "input": "Honestly, though, I don't think that those computations fall within the essence of linear algebra, but I definitely think that understanding what the determinant represents falls within that essence.",
  "translatedText": "Хоча, чесно кажучи, я не думаю, що ці обчислення належать до суті лінійної алгебри, але я точно вважаю, що розуміння того, що представляє визначник, належить до цієї суті.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 547.24,
  "end": 556.46
 },
 {
  "input": "Here's kind of a fun question to think about before the next video.",
  "translatedText": "Ось веселе запитання, над яким варто подумати перед наступним відео.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 558.06,
  "end": 560.64
 },
 {
  "input": "If you multiply two matrices together, the determinant of the resulting matrix is the same as the product of the determinants of the original two matrices.",
  "translatedText": "Якщо ви помножите дві матриці разом, визначник отриманої матриці буде таким самим, як добуток визначників двох початкових матриць.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 560.64,
  "end": 570.08
 },
 {
  "input": "If you tried to justify this with numbers, it would take a really long time, but see if you can explain why this makes sense in just one sentence.",
  "translatedText": "Якби ви спробували обґрунтувати це цифрами, це займе дуже багато часу, але подивіться, чи можете ви пояснити, чому це має сенс, лише одним реченням.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.1,
  "end": 577.88
 },
 {
  "input": "Next up, I'll be relating the idea of linear transformations covered so far to one of the areas where linear algebra is most useful, linear systems of equations.",
  "translatedText": "Далі я пов’язу ідею лінійних перетворень, розглянуту досі, з однією з областей, де лінійна алгебра є найбільш корисною, лінійними системами рівнянь.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.0,
  "end": 590.96
 },
 {
  "input": "See you then!",
  "translatedText": "Побачимось!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 591.48,
  "end": 591.6
 }
]