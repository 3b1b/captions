[
 {
  "input": "Hello, hello again. ",
  "translatedText": "你好，你好。",
  "model": "google_nmt",
  "from_community_srt": "嘿 又见面了！",
  "n_reviews": 0,
  "start": 11.98,
  "end": 13.0
 },
 {
  "input": "So moving forward, I'll be assuming that you have a visual understanding of linear transformations and how they're represented with matrices, the way that I've been talking about in the last few videos. ",
  "translatedText": "因此，接下来，我将假设您对线性变换以 及它们如何用矩阵表示有直观的理解，这 就是我在过去几个视频中讨论的方式。",
  "model": "google_nmt",
  "from_community_srt": "继续上次的内容 通过前几期视频的学习，",
  "n_reviews": 0,
  "start": 13.52,
  "end": 21.84
 },
 {
  "input": "If you think about a couple of these linear transformations, you might notice how some of them seem to stretch space out, while others squish it on in. ",
  "translatedText": "如果您考虑其中的几个线性变换， 您可能会注意到其中一些似乎拉伸 了空间，而另一些则将空间压缩。",
  "model": "google_nmt",
  "from_community_srt": "我假定你已经对线性变换有一个形象的理解 并且你也知道如何用矩阵表示它们 现在想象一些线性变换 你可能注意到其中有的将空间向外拉伸，",
  "n_reviews": 0,
  "start": 22.66,
  "end": 30.42
 },
 {
  "input": "One thing that turns out to be pretty useful for understanding one of these transformations is to measure exactly how much it stretches or squishes things. ",
  "translatedText": "事实证明，对于理解这些变换之一非常有用的 一件事是准确测量它拉伸或压缩物体的程度。",
  "model": "google_nmt",
  "from_community_srt": "有的则将空间向内挤压 有件事对理解这些线性变换很有用 那就是测量变换究竟对空间有多少拉伸或挤压 更具体一点，",
  "n_reviews": 0,
  "start": 31.14,
  "end": 38.92
 },
 {
  "input": "More specifically, to measure the factor by which the area of a given region increases or decreases. ",
  "translatedText": "更具体地说，测量给定区域的面积增加或减少的因素。",
  "model": "google_nmt",
  "from_community_srt": "就是测量一个给定区域面积增大或减小的比例 比如说这样一个以(3,",
  "n_reviews": 0,
  "start": 39.52,
  "end": 45.82
 },
 {
  "input": "For example, look at the matrix with columns 3, 0 and 0, 2. ",
  "translatedText": "例如，查看包含第 3, 0 和 0, 2 列的矩阵。",
  "model": "google_nmt",
  "from_community_srt": "0)和(0,",
  "n_reviews": 0,
  "start": 47.18,
  "end": 50.88
 },
 {
  "input": "It scales i-hat by a factor of 3 and scales j-hat by a factor of 2. ",
  "translatedText": "它将 i-hat 缩放为 3 倍，将 j-hat 缩放为 2 倍。",
  "model": "google_nmt",
  "from_community_srt": "2)为列的矩阵 它将i帽伸长为原来的3倍，",
  "n_reviews": 0,
  "start": 51.32,
  "end": 56.18
 },
 {
  "input": "Now, if we focus our attention on the 1 by 1 square whose bottom sits on i-hat and whose left side sits on j-hat, after the transformation, this turns into a 2 by 3 rectangle. ",
  "translatedText": "现在，如果我们将注意力集中在底部位于 i-hat 上、左侧位于 j-hat 上的 1 x 1 正方 形上，则变换后，它会变成一个 2 x 3 矩形。",
  "model": "google_nmt",
  "from_community_srt": "将j帽伸长为原来的2倍 现在如果我们关注以i帽为底边， 以j帽为左边的1×1方形 在变换之后，",
  "n_reviews": 0,
  "start": 56.7,
  "end": 67.52
 },
 {
  "input": "Since this region started out with area 1 and ended up with area 6, we can say the linear transformation has scaled its area by a factor of 6. ",
  "translatedText": "由于该区域从区域 1 开始并以区域 6 结束，因 此我们可以说线性变换已将其面积缩放了 6 倍。",
  "model": "google_nmt",
  "from_community_srt": "它会变成一个2×3的矩形 因为这个区域初始面积为1， 最终面积为6 所以我们说这个线性变换将它的面积变为6倍 剪切矩阵的列为(1,",
  "n_reviews": 0,
  "start": 68.38,
  "end": 77.28
 },
 {
  "input": "Compare that to a shear whose matrix has columns 1, 0 and 1, 1, meaning i-hat stays in place and j-hat moves over to 1, 1. ",
  "translatedText": "将其与矩阵具有列 1, 0 和 1, 1 的剪切机进行比较，这意 味着 i-hat 保持在原位，而 j-hat 移动到 1, 1。",
  "model": "google_nmt",
  "from_community_srt": "0)和(1, 1) 也就是说i帽保持不变， 而j帽移动至(1,",
  "n_reviews": 0,
  "start": 78.18,
  "end": 86.1
 },
 {
  "input": "That same unit square determined by i-hat and j-hat gets slanted and turned into a parallelogram, but the area of that parallelogram is still 1, since its base and height each continue to have length 1. ",
  "translatedText": "由 i-hat 和 j-hat 确定的同一单位正 方形被倾斜并变成平行四边形，但该平行四边形的面积 仍然为 1，因为其底边和高度的长度仍然为 1。",
  "model": "google_nmt",
  "from_community_srt": "1) 由i帽和j帽决定的单位正方形在变换后倾斜为一个平行四边形 但这个平行四边形的面积仍旧为1 因为它的底和高的长度还是1 所以说，",
  "n_reviews": 0,
  "start": 87.0,
  "end": 98.38
 },
 {
  "input": "So, even though this transformation smushes things about, it seems to leave areas unchanged, at least in the case of that 1 unit square. ",
  "translatedText": "因此，尽管这种变换弄乱了一切，但它似乎使面积保 持不变，至少在 1 个单位平方的情况下是这样。",
  "model": "google_nmt",
  "from_community_srt": "即便这个变换将空间向右挤压 至少对于这个单位正方形来说，",
  "n_reviews": 0,
  "start": 99.18,
  "end": 105.62
 },
 {
  "input": "Actually though, if you know how much the area of that one single unit square changes, it can tell you how the area of any possible region in space changes. ",
  "translatedText": "但实际上，如果你知道一个单位正方形的面积变化了多少 ，它就可以告诉你空间中任何可能区域的面积如何变化。",
  "model": "google_nmt",
  "from_community_srt": "它似乎并不改变面积 实际上， 你只要知道这个单位正方形面积变化的比例 它就能告诉你其他任意区域的面积变化比例 首先需要注意一点，",
  "n_reviews": 0,
  "start": 106.82,
  "end": 115.52
 },
 {
  "input": "For starters, notice that whatever happens to one square in the grid has to happen to any other square in the grid, no matter the size. ",
  "translatedText": "首先，请注意，无论网格中的一个方格发生什么，无论 大小如何，都必须发生在网格中的任何其他方格上。",
  "model": "google_nmt",
  "from_community_srt": "无论一个方格如何变化 对其他大小的方格来说，",
  "n_reviews": 0,
  "start": 116.3,
  "end": 123.58
 },
 {
  "input": "This follows from the fact that grid lines remain parallel and evenly spaced. ",
  "translatedText": "这是因为网格线保持平行且间隔均匀。",
  "model": "google_nmt",
  "from_community_srt": "都会有相同变化 这是由“网格线保持平行且等距分布”这一事实推断得出的 对于不是方格的形状，",
  "n_reviews": 0,
  "start": 124.34,
  "end": 128.04
 },
 {
  "input": "Then, any shape that's not a grid square can be approximated by grid squares pretty well, with arbitrarily good approximations if you use small enough grid squares. ",
  "translatedText": "然后，任何不是网格正方形的形状都可以很好地用网格正方形来近似 ，如果您使用足够小的网格正方形，则可以得到任意好的近似值。",
  "model": "google_nmt",
  "from_community_srt": "它们可以用许多方格良好近似 只要使用的方格足够小，",
  "n_reviews": 0,
  "start": 128.76,
  "end": 137.52
 },
 {
  "input": "So, since the areas of all those tiny grid squares are being scaled by some single amount, the area of the blob as a whole will also be scaled by that same single amount. ",
  "translatedText": "因此，由于所有这些微小网格方块的面积都按某个单一量 缩放，因此整个斑点的面积也将按相同的单一量缩放。",
  "model": "google_nmt",
  "from_community_srt": "近似就能足够好 由于所有小方格都进行了一个比例的缩放 所以整个形状也进行了同样比例的缩放 这个特殊的缩放比例，",
  "n_reviews": 0,
  "start": 137.52,
  "end": 147.82
 },
 {
  "input": "This very special scaling factor, the factor by which a linear transformation changes any area, is called the determinant of that transformation. ",
  "translatedText": "这个非常特殊的缩放因子，即线性变换改变 任何区域的因子，称为该变换的行列式。",
  "model": "google_nmt",
  "from_community_srt": "即线性变换改变面积的比例 被称为这个变换的行列式 在这个视频后半段，",
  "n_reviews": 0,
  "start": 148.9,
  "end": 157.12
 },
 {
  "input": "I'll show how to compute the determinant of a transformation using its matrix later on in this video, but understanding what it represents is, trust me, much more important than the computation. ",
  "translatedText": "稍后我将在本视频中展示如何使用其矩 阵来计算变换的行列式，但相信我，理 解它所代表的内容比计算重要得多。",
  "model": "google_nmt",
  "from_community_srt": "我会展示如何用矩阵计算一个线性变换的行列式 但是理解它的意义比计算本身重要得多 比如说，",
  "n_reviews": 0,
  "start": 159.12,
  "end": 168.42
 },
 {
  "input": "For example, the determinant of a transformation would be 3 if that transformation increases the area of a region by a factor of 3. ",
  "translatedText": "例如，如果某个变换将某个区域的面积增加 了 3 倍，则该变换的行列式将为 3。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.58,
  "end": 177.04
 },
 {
  "input": "The determinant of a transformation would be 1 half if it squishes down all areas by a factor of 1 half. ",
  "translatedText": "如果变换将所有区域压缩为二分之一 ，则变换的行列式将为二分之一。",
  "model": "google_nmt",
  "from_community_srt": "一个线性变换的行列式是3 就是说它将一个区域的面积增加为原来的3倍 一个线性变换的行列式是1/2 就是说它将一个区域的面积缩小一半 而一个二维线性变换的行列式为0 说明它将整个平面压缩到一条线，",
  "n_reviews": 0,
  "start": 178.18,
  "end": 184.34
 },
 {
  "input": "And the determinant of a 2D transformation is 0 if it squishes all of space onto a line, or even onto a single point. ",
  "translatedText": "如果 2D 变换将所有空间压缩到一条线上，甚至 压缩到一个点上，则 2D 变换的行列式为 0。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 186.0,
  "end": 193.5
 },
 {
  "input": "Since then, the area of any region would become 0. ",
  "translatedText": "从此以后，任何区域的面积都将变为0。",
  "model": "google_nmt",
  "from_community_srt": "甚至是一个点上 因为此时任何区域的面积都变成了0 最后这个例子相当重要 这是说，",
  "n_reviews": 0,
  "start": 194.0,
  "end": 196.76
 },
 {
  "input": "That last example will prove to be pretty important. ",
  "translatedText": "最后一个例子将被证明非常重要。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.62,
  "end": 199.6
 },
 {
  "input": "It means that checking if the determinant of a given matrix is 0 will give a way of computing whether or not the transformation associated with that matrix squishes everything into a smaller dimension. ",
  "translatedText": "这意味着检查给定矩阵的行列式是否为 0 将提供一种计算与该矩阵相关的变换是 否将所有内容压缩到更小的维度的方法。",
  "model": "google_nmt",
  "from_community_srt": "只需要检验一个矩阵的行列式是否为0 我们就能了解这个矩阵所代表的变换是否将空间压缩到更小的维度上 接下来的几期视频中，",
  "n_reviews": 0,
  "start": 200.02,
  "end": 209.74
 },
 {
  "input": "You'll see in the next few videos why this is even a useful thing to think about, but for now, I just want to lay down all of the visual intuition, which, in and of itself, is a beautiful thing to think about. ",
  "translatedText": "您将在接下来的几个视频中看到为什么这甚至是一件 值得思考的有用的事情，但现在，我只想放下所有的 视觉直觉，这本身就是一件值得思考的美丽事情。",
  "model": "google_nmt",
  "from_community_srt": "你会明白它为什么有用 但是就现在而言， 我只想让你留下直观的印象 这种直觉本身也是非常美妙的 我得承认一点，",
  "n_reviews": 0,
  "start": 210.52,
  "end": 220.1
 },
 {
  "input": "Okay, I need to confess that what I've said so far is not quite right. ",
  "translatedText": "好吧，我必须承认到目前为止我所说的并不完全正确。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 222.12,
  "end": 225.56
 },
 {
  "input": "The full concept of the determinant allows for negative values. ",
  "translatedText": "行列式的完整概念允许负值。",
  "model": "google_nmt",
  "from_community_srt": "到目前为止我所说的并不完全正确 完整概念下的行列式是允许出现负值的 那将一个区域缩放负数倍到底是什么意思？",
  "n_reviews": 0,
  "start": 225.88,
  "end": 229.28
 },
 {
  "input": "But what would the idea of scaling an area by a negative amount even mean? ",
  "translatedText": "但是按负数缩放区域的想法到底意味着什么呢？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 229.72,
  "end": 233.48
 },
 {
  "input": "This has to do with the idea of orientation. ",
  "translatedText": "这与方向的观念有关。",
  "model": "google_nmt",
  "from_community_srt": "这和定向的概念有关 举个例子，",
  "n_reviews": 0,
  "start": 234.94,
  "end": 236.96
 },
 {
  "input": "For example, notice how this transformation gives the sensation of flipping space over. ",
  "translatedText": "例如，请注意这种变换如何给人翻转空间的感觉。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 237.8,
  "end": 242.68
 },
 {
  "input": "If you were thinking of 2D space as a sheet of paper, a transformation like that one seems to turn over that sheet onto the other side. ",
  "translatedText": "如果您将二维空间视为一张纸，那么像这 样的变换似乎会将这张纸翻转到另一面。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 243.24,
  "end": 249.86
 },
 {
  "input": "Many transformations that do this are said to invert the orientation of space. ",
  "translatedText": "许多这样做的变换据说会反转空间的方向。",
  "model": "google_nmt",
  "from_community_srt": "注意这个变换在感觉上将整个平面翻转了 如果你将二维空间想象为一张纸 这个变换像是将纸翻转到了另一面 我们称类似这样的变换改变了空间的定向 另一种方式是根据i帽和j帽来考虑 注意在初始状态时，",
  "n_reviews": 0,
  "start": 250.64,
  "end": 255.04
 },
 {
  "input": "Another way to think about it is in terms of i-hat and j-hat. ",
  "translatedText": "另一种思考方式是根据 i-hat 和 j-hat。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.84,
  "end": 258.6
 },
 {
  "input": "Notice that in their starting positions, j-hat is to the left of i-hat. ",
  "translatedText": "请注意，在它们的起始位置，j-hat 位于 i-hat 的左侧。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 259.16,
  "end": 263.06
 },
 {
  "input": "If after a transformation, j-hat is now on the right of i-hat, the orientation of space has been inverted. ",
  "translatedText": "如果变换后，j-hat 现在位于 i -hat 的右侧，则空间方向已反转。",
  "model": "google_nmt",
  "from_community_srt": "j帽在i帽的左边 如果在变换之后，",
  "n_reviews": 0,
  "start": 263.62,
  "end": 270.2
 },
 {
  "input": "Whenever this happens, whenever the orientation of space is inverted, the determinant will be negative. ",
  "translatedText": "每当这种情况发生时，每当空间 方向反转时，行列式将为负。",
  "model": "google_nmt",
  "from_community_srt": "j帽处于i帽的右边 那么空间定向就发生了改变 当空间定向改变的情况发生时，",
  "n_reviews": 0,
  "start": 272.12,
  "end": 276.58
 },
 {
  "input": "The absolute value of the determinant, though, still tells you the factor by which areas have been scaled. ",
  "translatedText": "不过，行列式的绝对值仍然 告诉您缩放区域的因子。",
  "model": "google_nmt",
  "from_community_srt": "行列式为负 但是行列式的绝对值依然表示区域面积的缩放比例 比如说，",
  "n_reviews": 0,
  "start": 277.46,
  "end": 282.4
 },
 {
  "input": "For example, the matrix with columns 1, 1 and 2, negative 1 encodes a transformation that has determinant, I'll just tell you, negative 3. ",
  "translatedText": "例如，具有第 1、1 和 2 列的矩阵，负 1 编码具有行列式的变换，我只是告诉你，负 3。",
  "model": "google_nmt",
  "from_community_srt": "我告诉你由(1, 1)和(2, -1)为列的矩阵所代表的线性变换的行列式是-3 这就是说变换后空间被翻转，",
  "n_reviews": 0,
  "start": 283.02,
  "end": 290.68
 },
 {
  "input": "And what this means is that space gets flipped over and areas are scaled by a factor of 3. ",
  "translatedText": "这意味着空间被翻转， 面积被缩放 3 倍。",
  "model": "google_nmt",
  "from_community_srt": "并且面积放大为原来的3倍 那么，",
  "n_reviews": 0,
  "start": 291.46,
  "end": 296.28
 },
 {
  "input": "So why would this idea of a negative area scaling factor be a natural way to describe orientation flipping? ",
  "translatedText": "那么为什么负面积缩放因子的想法 是描述方向翻转的自然方式呢？",
  "model": "google_nmt",
  "from_community_srt": "负的面积缩放比例为什么会自然地用来描述定向改变呢？",
  "n_reviews": 0,
  "start": 297.78,
  "end": 303.7
 },
 {
  "input": "Think about the series of transformations you get by slowly letting i-hat get closer and closer to j-hat. ",
  "translatedText": "想一想通过慢慢让 i-hat 越来越接 近 j-hat 所获得的一系列转变。",
  "model": "google_nmt",
  "from_community_srt": "考虑i帽逐渐接近j帽所形成的一系列变换 当i帽靠近j帽时，",
  "n_reviews": 0,
  "start": 304.26,
  "end": 310.14
 },
 {
  "input": "As i-hat gets closer, all of the areas in space are getting squished more and more, meaning the determinant approaches 0. ",
  "translatedText": "随着 i-hat 越来越近，空间中的所有区 域都被越来越挤压，这意味着行列式接近 0。",
  "model": "google_nmt",
  "from_community_srt": "空间也被压缩地更严重 这意味着行列式趋近于0 当i帽与j帽完全重合时，",
  "n_reviews": 0,
  "start": 310.72,
  "end": 317.1
 },
 {
  "input": "Once i-hat lines up perfectly with j-hat, the determinant is 0. ",
  "translatedText": "一旦 i-hat 与 j-hat 完美对齐，行列式就是 0。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 317.82,
  "end": 321.64
 },
 {
  "input": "Then, if i-hat continues the way that it was going, doesn't it kind of feel natural for the determinant to keep decreasing into the negative numbers? ",
  "translatedText": "那么，如果 i-hat 继续这样下去，行列式不断 减小为负数不是很自然吗？",
  "model": "google_nmt",
  "from_community_srt": "行列式为0 如果i帽继续沿着这个方向运动 行列式继续减小为负值难道不是一件很自然的事吗？",
  "n_reviews": 0,
  "start": 322.44,
  "end": 329.28
 },
 {
  "input": "So that's the understanding of determinants in two dimensions. ",
  "translatedText": "这就是对二维行列式的理解。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 330.68,
  "end": 333.56
 },
 {
  "input": "What do you think it should mean for three dimensions? ",
  "translatedText": "你认为这对于三维来说意味着什么？",
  "model": "google_nmt",
  "from_community_srt": "以上就是在二维空间中对行列式的理解 你觉得行列式在三维空间中是什么意义？",
  "n_reviews": 0,
  "start": 333.56,
  "end": 335.94
 },
 {
  "input": "It also tells you how much a transformation scales things, but this time it tells you how much volumes get scaled. ",
  "translatedText": "它还告诉您转换对事物的缩放程度， 但这次它告诉您缩放了多少体积。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.92,
  "end": 343.24
 },
 {
  "input": "Just as in two dimensions, where this is easiest to think about by focusing on one particular square with an area 1 and watching only what happens to it, in three dimensions it helps to focus your attention on the specific 1 by 1 by 1 cube whose edges are resting on the basis vectors i-hat, j-hat, and k-hat. ",
  "translatedText": "就像在二维中一样，在二维中，通过关注一个面 积为 1 的特定正方形并仅观察它发生的情 况，最容易想到这一点，在三维中，它有助于 将您的注意力集中在特定的 1 x 1 x 1 立方体上，该立方体的边基于基向量 i-hat、j-hat 和 k-hat。",
  "model": "google_nmt",
  "from_community_srt": "它告诉你的依然是变换前后的缩放比例 不过这次它说的是体积的缩放 二维空间中 我们最容易考虑一个面积为1的特殊正方形 并观察变换对它的影响 在三维空间中， 你聚焦于一个特定的1×1×1立方体 它的棱处于基向量i帽、j帽和k帽上 在变换后，",
  "n_reviews": 0,
  "start": 345.34,
  "end": 363.44
 },
 {
  "input": "After the transformation, that cube might get warped into some kind of slanty slanty cube. ",
  "translatedText": "转换后，该立方体可能会扭 曲成某种倾斜的立方体。",
  "model": "google_nmt",
  "from_community_srt": "这个立方体可能就变成了一个斜不拉几的形状 顺带一提，",
  "n_reviews": 0,
  "start": 364.32,
  "end": 369.3
 },
 {
  "input": "This shape, by the way, has the best name ever, parallel a pipette, a name that's made even more delightful when your professor has a nice thick Russian accent. ",
  "translatedText": "顺便说一句，这种形状有一个有史以来最好的名 字，平行移液器，当你的教授有一口浓重的俄 罗斯口音时，这个名字就变得更加令人愉快。",
  "model": "google_nmt",
  "from_community_srt": "这个形状有个好听的名字——平行六面体 如果你的教授有着浓重的俄国口音，",
  "n_reviews": 0,
  "start": 370.34,
  "end": 377.44
 },
 {
  "input": "Since this cube starts out with a volume of 1 and the determinant gives the factor by which any volume is scaled, you can think of the determinant simply as being the volume of that parallel a pipette that the cube turns into. ",
  "translatedText": "由于该立方体的起始体积为 1，并 且行列式给出了任何体积缩放的系 数，因此您可以将行列式简单地视 为立方体变成的平行吸管的体积。",
  "model": "google_nmt",
  "from_community_srt": "这个名字就会显得更滑稽 因为这个立方体的初始体积为1 而行列式给出的是体积缩放比例 所以你可以把行列式简单看作这个平行六面体的体积 行列式为0则意味着整个空间被压缩为零体积的东西 也就是一个平面或一条直线，",
  "n_reviews": 0,
  "start": 378.52,
  "end": 390.64
 },
 {
  "input": "A determinant of 0 would mean that all of space is squished onto something with 0 volume, meaning either a flat plane, a line, or, in the most extreme case, onto a single point. ",
  "translatedText": "行列式为 0 意味着所有空间都被 压缩到体积为 0 的物体上，这意 味着要么是一个平面、一条线，要么 在最极端的情况下压缩到一个点上。",
  "model": "google_nmt",
  "from_community_srt": "或者更极端的情况下，",
  "n_reviews": 0,
  "start": 392.38,
  "end": 402.5
 },
 {
  "input": "Those of you who watched chapter 2 will recognize this as meaning that the columns of the matrix are linearly dependent. ",
  "translatedText": "看过第二章的人会认识到这意 味着矩阵的列是线性相关的。",
  "model": "google_nmt",
  "from_community_srt": "一个点 看过第二章视频的朋友 可能会认得这是在说矩阵的列线性相关 你明白为什么吗？",
  "n_reviews": 0,
  "start": 403.76,
  "end": 409.24
 },
 {
  "input": "Can you see why? ",
  "translatedText": "你能明白为什么吗？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.76,
  "end": 410.42
 },
 {
  "input": "What about negative determinants? ",
  "translatedText": "负面决定因素又如何呢？",
  "model": "google_nmt",
  "from_community_srt": "那么对于负值行列式呢？",
  "n_reviews": 0,
  "start": 414.92,
  "end": 416.64
 },
 {
  "input": "What should that mean for three dimensions? ",
  "translatedText": "这对于三维来说意味着什么？",
  "model": "google_nmt",
  "from_community_srt": "它在三维下是什么意思？",
  "n_reviews": 0,
  "start": 416.78,
  "end": 418.1
 },
 {
  "input": "One way to describe orientation in 3D is with the right hand rule. ",
  "translatedText": "描述 3D 方向的一种方法是使用右手定则。",
  "model": "google_nmt",
  "from_community_srt": "有一种方法来描述三维空间的定向，",
  "n_reviews": 0,
  "start": 418.78,
  "end": 422.68
 },
 {
  "input": "Point the forefinger of your right hand in the direction of i-hat, stick out your middle finger in the direction of j-hat, and notice how when you point your thumb up, it's in the direction of k-hat. ",
  "translatedText": "将右手的食指指向 i-hat 的方向，将 中指指向 j-hat 的方向，注意当你 将拇指向上指向 k-hat 的方向时。",
  "model": "google_nmt",
  "from_community_srt": "那就是“右手定则” 右手食指指向i帽的方向 伸出中指指向j帽的方向 当你把大拇指竖起来时，",
  "n_reviews": 0,
  "start": 423.3,
  "end": 432.76
 },
 {
  "input": "If you can still do that after the transformation, orientation has not changed, and the determinant is positive. ",
  "translatedText": "如果变换后仍能做到这一点，则 方向没有改变，行列式为正。",
  "model": "google_nmt",
  "from_community_srt": "它就正好指向k帽的方向 如果在变换后你仍然可以这么做 那么定向没有发生改变，",
  "n_reviews": 0,
  "start": 434.88,
  "end": 440.9
 },
 {
  "input": "Otherwise, if after the transformation it only makes sense to do that with your left hand, orientation has been flipped, and the determinant is negative. ",
  "translatedText": "否则，如果在转换之后只有用左 手执行此操作才有意义，则方 向已翻转，并且行列式为负。",
  "model": "google_nmt",
  "from_community_srt": "行列式为正 否则， 如果在变换后你只能用左手这么做 说明定向发生了改变，",
  "n_reviews": 0,
  "start": 441.54,
  "end": 449.38
 },
 {
  "input": "So if you haven't seen it before, you're probably wondering by now, how do you actually compute the determinant? ",
  "translatedText": "因此，如果您以前没有见过它，那么您现 在可能想知道，如何实际计算行列式？",
  "model": "google_nmt",
  "from_community_srt": "行列式为负 如果你以前没有学过行列式， 那你现在大概就在想 到底怎么计算行列式呢？",
  "n_reviews": 0,
  "start": 451.9,
  "end": 457.04
 },
 {
  "input": "For a 2x2 matrix with entries a, b, c, d, the formula is a times d minus b times c. ",
  "translatedText": "对于包含 a、b、c、d 项的 2x2 矩阵，公式为 a 乘以 d 减去 b 乘以 c。",
  "model": "google_nmt",
  "from_community_srt": "对于一个2×2的矩阵[[a, b], [c, d]]，",
  "n_reviews": 0,
  "start": 457.56,
  "end": 464.42
 },
 {
  "input": "Here's part of an intuition for where this formula comes from. ",
  "translatedText": "这是这个公式的来源的部分直觉。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.74,
  "end": 468.5
 },
 {
  "input": "Let's say that the terms b and c both happened to be 0. ",
  "translatedText": "假设 b 和 c 项恰好都是 0。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.88,
  "end": 471.78
 },
 {
  "input": "Then the term a tells you how much i-hat is stretched in the x direction, and the term d tells you how much j-hat is stretched in the y direction. ",
  "translatedText": "然后项 a 告诉您 i-hat 在 x 方向上拉伸了多少 ，项 d 告诉您 j-hat 在 y 方向上拉伸了多少。",
  "model": "google_nmt",
  "from_community_srt": "公式是ad-bc 以下是对这个公式来源的直观理解 假如b和c均恰好为0 那么a告诉你i帽在x轴方向的伸缩比例 d告诉你j帽在y轴方向的伸缩比例 因为其他项均为0，",
  "n_reviews": 0,
  "start": 471.78,
  "end": 481.16
 },
 {
  "input": "So since those other terms are 0, it should make sense that a times d gives the area of the rectangle that our favorite unit square turns into, kind of like the 3, 0, 0, 2 example from earlier. ",
  "translatedText": "因此，由于其他项都是 0，所以 a 乘以 d 给出了我们最喜欢的单位正方形变成的矩形的面积 ，有点像前面的 3, 0, 0, 2 示例。",
  "model": "google_nmt",
  "from_community_srt": "所以ad给出的是单位正方形伸缩后形成的矩形的面积 就像之前[[3, 0], [0, 2]]那个例子一样，",
  "n_reviews": 0,
  "start": 482.76,
  "end": 493.36
 },
 {
  "input": "Even if only one of b or c are 0, you'll have a parallelogram with a base a and a height d, so the area should still be a times d. ",
  "translatedText": "即使 b 或 c 中只有一个为 0，您也会得到一个底为 a 、高为 d 的平行四边形，因此面积仍应为 a 乘以 d。",
  "model": "google_nmt",
  "from_community_srt": "这一点合情合理 即便b和c其中只有一项为0 那么最后得到的是一个平行四边形， 底为a且高为d，",
  "n_reviews": 0,
  "start": 495.36,
  "end": 504.5
 },
 {
  "input": "Loosely speaking, if both b and c are non-zero, then that b times c term tells you how much this parallelogram is stretched or squished in the diagonal direction. ",
  "translatedText": "宽松地说，如果 b 和 c 均非零，则 b 乘以 c 项会告诉您该平行四边形在对角线方向上拉伸或挤压了多少。",
  "model": "google_nmt",
  "from_community_srt": "面积应该仍旧为ad 粗略地说， 如果b和c均不为0 那么bc项就会告诉你平行四边形在对角方向上拉伸或压缩了多少 对于那些想迫切知道bc项精确含义的人 如果你愿意暂停思考的话，",
  "n_reviews": 0,
  "start": 505.46,
  "end": 515.46
 },
 {
  "input": "For those of you hungry for a more precise description of this b times c term, here's a helpful diagram if you'd like to pause and ponder. ",
  "translatedText": "对于那些渴望更准确地描述这个 b 乘 c 术语的人来 说，如果您想停下来思考一下，这里有一个有用的图表。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 516.66,
  "end": 522.88
 },
 {
  "input": "Now if you feel like computing determinants by hand is something that you need to know, the only way to get it down is to just practice it with a few. ",
  "translatedText": "现在，如果您觉得手动计算行列式是您需要知道的事情， 那么将其记下来的唯一方法就是通过一些练习来实现。",
  "model": "google_nmt",
  "from_community_srt": "这里有个简图可以帮忙 如果你觉得徒手计算行列式是你必须掌握的 那么唯一的方法就是用一些矩阵来练习 训练计算的过程实在没有太多东西需要我讲解或是用动画演示的 对三阶行列式来说，",
  "n_reviews": 0,
  "start": 523.98,
  "end": 531.2
 },
 {
  "input": "There's really not that much I can say or animate that's going to drill in the computation. ",
  "translatedText": "我确实没有太多可以说或动画的内容来深入计算。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 531.2,
  "end": 535.18
 },
 {
  "input": "This is all triply true for three-dimensional determinants. ",
  "translatedText": "对于三维行列式来说，这都是三重正确的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.12,
  "end": 538.64
 },
 {
  "input": "There is a formula, and if you feel like that's something you need to know, you should practice with a few matrices, or, you know, go watch Sal Khan work through a few. ",
  "translatedText": "有一个公式，如果你觉得这是你需要知道的东西，你应该练习一些矩阵， 或者，你知道，去看萨尔·汗（Sal Khan）如何计算一些矩阵。",
  "model": "google_nmt",
  "from_community_srt": "这个观点“三倍”正确 这里有个计算公式， 如果你觉得你必须掌握它 那就应该用一些矩阵来练习， 或者去看看Sal Khan是怎么计算的 说实话，",
  "n_reviews": 0,
  "start": 539.04,
  "end": 546.34
 },
 {
  "input": "Honestly, though, I don't think that those computations fall within the essence of linear algebra, but I definitely think that understanding what the determinant represents falls within that essence. ",
  "translatedText": "但老实说，我不认为这些计算属于线性代数的本质 ，但我绝对认为理解行列式代表什么属于该本质。",
  "model": "google_nmt",
  "from_community_srt": "我不认为这些计算过程属于线性代数的本质 但是我确实认为理解行列式所代表的意义在它当中 下期视频开始前，",
  "n_reviews": 0,
  "start": 547.24,
  "end": 556.46
 },
 {
  "input": "Here's kind of a fun question to think about before the next video. ",
  "translatedText": "在下一个视频之前要考虑一个有趣的问题。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 558.06,
  "end": 560.64
 },
 {
  "input": "If you multiply two matrices together, the determinant of the resulting matrix is the same as the product of the determinants of the original two matrices. ",
  "translatedText": "如果将两个矩阵相乘，所得矩阵的行列式 与原始两个矩阵的行列式的乘积相同。",
  "model": "google_nmt",
  "from_community_srt": "你可以考虑这个有趣的问题 如果你将两个矩阵相乘 它们乘积的行列式 等于这两个矩阵的行列式的乘积 如果你想用数值方法证明，",
  "n_reviews": 0,
  "start": 560.64,
  "end": 570.08
 },
 {
  "input": "If you tried to justify this with numbers, it would take a really long time, but see if you can explain why this makes sense in just one sentence. ",
  "translatedText": "如果你试图用数字来证明这一点，那将需要很长时间， 但看看你是否能用一句话解释为什么这是有意义的。",
  "model": "google_nmt",
  "from_community_srt": "那要耗费你不少时间 不过，",
  "n_reviews": 0,
  "start": 571.1,
  "end": 577.88
 },
 {
  "input": "Next up, I'll be relating the idea of linear transformations covered so far to one of the areas where linear algebra is most useful, linear systems of equations. ",
  "translatedText": "接下来，我将把到目前为止所涉及的线性变换的概念与 线性代数最有用的领域之一——线性方程组联系起来。",
  "model": "google_nmt",
  "from_community_srt": "看看你能不能只用一句话就解释清楚它的道理 下期视频中， 我会将目前涉及的线性变换的思想 与线性代数中最有用的领域之一——线性方程组相结合 到时候再见！",
  "n_reviews": 0,
  "start": 582.0,
  "end": 590.96
 },
 {
  "input": "See you then! ",
  "translatedText": "回头见！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 591.48,
  "end": 591.6
 }
]