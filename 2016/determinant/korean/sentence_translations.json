[
 {
  "input": "Hello, hello again.",
  "translatedText": "안녕, 또 안녕.",
  "model": "google_nmt",
  "from_community_srt": "안녕, 또 만났네. (이번에도 자막싱크가 안맞네요.",
  "n_reviews": 0,
  "start": 11.98,
  "end": 13.0
 },
 {
  "input": "So moving forward, I'll be assuming that you have a visual understanding of linear transformations and how they're represented with matrices, the way that I've been talking about in the last few videos.",
  "translatedText": "앞으로는 여러분이 선형 변환과 선형 변환이 행렬로 표현되는 방식, 즉 제가 지난 몇 개의 비디오에서 이야기했던 방식을 시각적으로 이해했다고 가정하겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "자막이 훨빨라요;) 그럼, 이제는 너에게 충분히 설명했다고 생각해. 선형변환에 대한 시각적 이해가 됬을거고, 선형변환을 행렬로 표현하는 방법도 알거야. 지난 동영상들에서 통해서 계속 얘기했었지.",
  "n_reviews": 0,
  "start": 13.52,
  "end": 21.84
 },
 {
  "input": "If you think about a couple of these linear transformations, you might notice how some of them seem to stretch space out, while others squish it on in.",
  "translatedText": "이러한 선형 변환 몇 가지에 대해 생각해 보면 일부는 공간을 늘리고 다른 일부는 공간을 찌그러뜨리는 것처럼 보일 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "근데 너가 선형변환을 다루다보면, 어떤 것들은 공간을 확대시키는 것 같을거고, 또 어떤 것들은 공간을 축소시키는 것 같을거야.",
  "n_reviews": 0,
  "start": 22.66,
  "end": 30.42
 },
 {
  "input": "One thing that turns out to be pretty useful for understanding one of these transformations is to measure exactly how much it stretches or squishes things.",
  "translatedText": "이러한 변환 중 하나를 이해하는 데 매우 유용한 것으로 밝혀진 한 가지는 그것이 얼마나 늘어나거나 찌그러지는지 정확히 측정하는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "이런 변환을 이해하는데 꽤 도움이 되는 방법 한가지가 있어. 바로 물체를 얼마나 확장되거나 축소되는지 특정해보는거야.",
  "n_reviews": 0,
  "start": 31.14,
  "end": 38.92
 },
 {
  "input": "More specifically, to measure the factor by which the area of a given region increases or decreases.",
  "translatedText": "보다 구체적으로 말하면, 특정 지역의 면적이 증가하거나 감소하는 요인을 측정합니다.",
  "model": "google_nmt",
  "from_community_srt": "더 구체적으로 설명하자면, 특정 지역의 크기를 증가하거나 감소시키는 팩터(factor 요인)값을 측정해보는 거야.",
  "n_reviews": 0,
  "start": 39.52,
  "end": 45.82
 },
 {
  "input": "For example, look at the matrix with columns 3, 0 and 0, 2.",
  "translatedText": "예를 들어 열 3, 0과 0, 2가 있는 행렬을 살펴보세요.",
  "model": "google_nmt",
  "from_community_srt": "예를 들어 볼게. 열 (3, 0), (0, 2) 로 이루어진 행렬을 봐봐.",
  "n_reviews": 0,
  "start": 47.18,
  "end": 50.88
 },
 {
  "input": "It scales i-hat by a factor of 3 and scales j-hat by a factor of 2.",
  "translatedText": "i-hat은 3배로 확장되고 j-hat은 2배로 확장됩니다.",
  "model": "google_nmt",
  "from_community_srt": "이 행렬은 i-hat(x 축 단위벡터) 를 팩터 3으로 확장시키고, j-hat(y축 단위벡터)를 팩터2 로 확장시키고 있어.",
  "n_reviews": 0,
  "start": 51.32,
  "end": 56.18
 },
 {
  "input": "Now, if we focus our attention on the 1 by 1 square whose bottom sits on i-hat and whose left side sits on j-hat, after the transformation, this turns into a 2 by 3 rectangle.",
  "translatedText": "이제 바닥이 i-hat에 있고 왼쪽이 j-hat에 있는 1x1 정사각형에 주의를 집중하면 변환 후 이는 2x3 직사각형으로 변합니다.",
  "model": "google_nmt",
  "from_community_srt": "이번에는, 1x1 짜리 정사각형을 집중해서 봐봐. 이 정사각형 아래는 i-hat 벡터고 왼쪽은 j-hat 벡터야. 변환 후를 보면, 2x3 크기의 직사각형이 되었어.",
  "n_reviews": 0,
  "start": 56.7,
  "end": 67.52
 },
 {
  "input": "Since this region started out with area 1 and ended up with area 6, we can say the linear transformation has scaled its area by a factor of 6.",
  "translatedText": "이 영역은 영역 1에서 시작하여 영역 6으로 끝났으므로 선형 변환으로 해당 영역이 6배로 확장되었다고 말할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "처음엔 영역(area) 1로 시작했는데, 나중엔 영역(area)크기가 6으로 바뀌었어. 그럼 우리는 이 선형변환은 팩터 6 으로 영역(area) 를 확장시킨다고 말할 수 있어.",
  "n_reviews": 0,
  "start": 68.38,
  "end": 77.28
 },
 {
  "input": "Compare that to a shear, whose matrix has columns 1, 0 and 1, 1, meaning i-hat stays in place and j-hat moves over to 1, 1.",
  "translatedText": "이를 행렬에 1, 0 및 1, 1 열이 있는 전단기와 비교해 보세요. 즉, i-hat은 제자리에 있고 j-hat은 1, 1로 이동한다는 의미입니다.",
  "model": "google_nmt",
  "from_community_srt": "기울이기?(shear) 변환과 비교해보자. 기울이기(shear) 변환을 타나내는 행렬은 (1,0), (1,1) 이야. 좀 풀어서 말하자면, i-hat 은 변하지 않고 j-hat 은 (1,1) 로 이동시켜.",
  "n_reviews": 0,
  "start": 78.18,
  "end": 86.1
 },
 {
  "input": "That same unit square determined by i-hat and j-hat gets slanted and turned into a parallelogram, but the area of that parallelogram is still 1, since its base and height each continue to have length 1.",
  "translatedText": "i-hat과 j-hat에 의해 결정된 동일한 단위 정사각형은 기울어져 평행사변형으로 바뀌지만 평행사변형의 넓이는 여전히 1입니다. 밑변과 높이가 각각 계속 길이 1을 가지기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "그러면 i-hat 과 j-hat 에 의해 결정된 단위 정사각형이 기울여지는 변형 후에는 평행사변형이 돼. 그래도 평행사변형의 영역(area) 크기는 여전히 1이야. 밑과 높이 길이가 여전히 1이기 때문이지.",
  "n_reviews": 0,
  "start": 87.0,
  "end": 98.38
 },
 {
  "input": "So even though this transformation smushes things about, it seems to leave areas unchanged, at least in the case of that 1 unit square.",
  "translatedText": "따라서 이 변환으로 인해 상황이 혼란스러워지더라도 적어도 1제곱 단위의 경우에는 면적이 변경되지 않은 것처럼 보입니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서, 이 변환이 마치 눌러서 찌그려뜨리는 것 같아도, 영역(넓이)는 바뀌지 않아. 흠, 적어도 단위 정사각형은 그렇지.",
  "n_reviews": 0,
  "start": 99.18,
  "end": 105.62
 },
 {
  "input": "Actually though, if you know how much the area of that one single unit square changes, it can tell you how the area of any possible region in space changes.",
  "translatedText": "하지만 실제로 단일 단위 정사각형의 면적이 얼마나 변하는지 알면 공간에서 가능한 영역의 면적이 어떻게 변하는지 알 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "사실은 너가 하나의 단위 정사각형의 영역이 얼마나 변하는지만 알면 공간 상 어떤 지역이 어떻게 변할지를 예측할 수 있게 돼.",
  "n_reviews": 0,
  "start": 106.82,
  "end": 115.52
 },
 {
  "input": "For starters, notice that whatever happens to one square in the grid has to happen to any other square in the grid, no matter the size.",
  "translatedText": "우선, 그리드의 한 사각형에 발생하는 모든 일은 크기에 관계없이 그리드의 다른 사각형에도 발생해야 한다는 점에 유의하세요.",
  "model": "google_nmt",
  "from_community_srt": "우선 격자에 한 정사각형이 어떻게 바뀌는지 살펴봐봐. 격자의 다른 정사각형들에도 마찬가지 변화가 똑같이 일어난다는 것을 깨닫게 될거야. 크기는 중요하지 않아.",
  "n_reviews": 0,
  "start": 116.3,
  "end": 123.58
 },
 {
  "input": "This follows from the fact that grid lines remain parallel and evenly spaced.",
  "translatedText": "이는 그리드 선이 평행하고 균일한 간격으로 유지된다는 사실에서 비롯됩니다.",
  "model": "google_nmt",
  "from_community_srt": "이건 격자선이 평행하고 균등한 거리를 유지한 채 변화하기 때문이야.",
  "n_reviews": 0,
  "start": 124.34,
  "end": 128.04
 },
 {
  "input": "Then, any shape that's not a grid square can be approximated by grid squares pretty well, with arbitrarily good approximations if you use small enough grid squares.",
  "translatedText": "그런 다음 격자 사각형이 아닌 모든 모양은 격자 사각형으로 꽤 잘 근사화될 수 있으며, 충분히 작은 격자 사각형을 사용하면 임의로 좋은 근사값을 얻을 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "그럼 이제는, 정사각이 아닌 임의 형태를 살펴보자. 임의 형태를 격자의 정사각형으로 꽤 잘 근사할 수 있어. 격자 정사각형을 충분히 작게 만들면,",
  "n_reviews": 0,
  "start": 128.76,
  "end": 137.52
 },
 {
  "input": "So, since the areas of all those tiny grid squares are being scaled by some single amount, the area of the blob as a whole will also be scaled by that same single amount.",
  "translatedText": "따라서 모든 작은 격자 사각형의 영역이 단일 크기로 조정되므로 전체 얼룩의 영역도 동일한 단일 크기로 크기가 조정됩니다.",
  "model": "google_nmt",
  "from_community_srt": "원하는 만큼 정확한 근사를 얻을 수 있어. 그리고, 작은 격자 정사각형들이 이루는 영역은 하나의 영역과 동일하게 스케일링되기 때문에, 전체 영역도 또한 한개와 같은 비율만큼 스케일링 되지.",
  "n_reviews": 0,
  "start": 137.52,
  "end": 147.82
 },
 {
  "input": "This very special scaling factor, the factor by which a linear transformation changes any area, is called the determinant of that transformation.",
  "translatedText": "선형 변환이 모든 영역을 변경하는 요소인 이 매우 특별한 스케일링 요소를 해당 변환의 행렬식이라고 합니다.",
  "model": "google_nmt",
  "from_community_srt": "이 특별한 스케일링 팩터는 선형변환에 의한 영역의 변화를 나타내는 팩터로서 행렬식(determinant) 라고 불러. (역자:",
  "n_reviews": 0,
  "start": 148.9,
  "end": 157.12
 },
 {
  "input": "I'll show how to compute the determinant of a transformation using its matrix later on in this video, which is also important in the computation.",
  "translatedText": "나중에 이 비디오에서 행렬을 사용하여 변환의 행렬식을 계산하는 방법을 보여 드리겠습니다. 이는 계산에서도 중요합니다.",
  "model": "google_nmt",
  "from_community_srt": "행렬식보다 determinant 자체가 이해하기 쉬운듯;) 뒤에서 이 선형변환의 행렬식(determinant) 를 계산하는 방법을 보여줄건데, 그런데 이게 무엇인지 이해하는 것은 계산법을 이해하는 것보다 훨~씬 중요해. 날 믿어봐.",
  "n_reviews": 0,
  "start": 159.12,
  "end": 168.42
 },
 {
  "input": "For example, the determinant of a transformation would be 3 if that transformation increases the area of a region by a factor of 3.",
  "translatedText": "예를 들어, 변환으로 인해 영역의 면적이 3배 증가하는 경우 변환의 행렬식은 3이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "예를 들어, 한 변환의 행렬식(determinant) 값이 3 이라면, 특정 지역의 크기는 팩터 3 만큼 증가해.",
  "n_reviews": 0,
  "start": 169.58,
  "end": 177.04
 },
 {
  "input": "The determinant of a transformation would be ½ if it squishes down all areas by a factor of ½.",
  "translatedText": "모든 영역을 1/2배로 압축하면 변환의 결정 요인은 1/2이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "행렬식(determinant) 값이 1/2 라면, 영역크기를 1/2 크기로 축소시키는 것을 의미해.",
  "n_reviews": 0,
  "start": 178.18,
  "end": 184.34
 },
 {
  "input": "And the determinant of a 2D transformation is 0 if it squishes all of space onto a line, or even onto a single point.",
  "translatedText": "그리고 2D 변환의 행렬식은 모든 공간을 선으로 압축하거나 단일 점으로 압축하는 경우 0입니다.",
  "model": "google_nmt",
  "from_community_srt": "2차원 변환의 행렬식이 0 이라면, 모든 공간이 찌부려뜨려져서 선이 될 수도 있어. 아니, 어쩌면 한 점이 될 수도 있지.",
  "n_reviews": 0,
  "start": 186.0,
  "end": 193.5
 },
 {
  "input": "Since then, the area of any region would become zero.",
  "translatedText": "그 이후로 모든 지역의 면적은 0이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "그럼 당연히, 어느 영역이든 크기가 0 이 될 거야.",
  "n_reviews": 0,
  "start": 194.0,
  "end": 196.76
 },
 {
  "input": "That last example will prove to be pretty important.",
  "translatedText": "마지막 예는 매우 중요할 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 마지막 예는 매우 중요한 것이라고 증명됐어.",
  "n_reviews": 0,
  "start": 197.62,
  "end": 199.6
 },
 {
  "input": "It means that checking if the determinant of a given matrix is zero will give a way of computing whether or not the transformation associated with that matrix squishes everything into a smaller dimension.",
  "translatedText": "이는 주어진 행렬의 행렬식이 0인지 확인하면 해당 행렬과 관련된 변환이 모든 것을 더 작은 차원으로 압축하는지 여부를 계산할 수 있는 방법을 제공한다는 것을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "주어진 행렬의 행렬식(determinant)값이 0 인지 확인하는 것은 계산할 수 있는지 없는지를 알려주는 거야. 이 변환과 관련된 행렬이 모든 것들을 더 작은 차원으로 뭉게버리는지를 말야.",
  "n_reviews": 0,
  "start": 200.02,
  "end": 209.74
 },
 {
  "input": "You'll see in the next few videos why this is even a useful thing to think about, but for now, I just want to lay down all of the visual intuition, which, in and of itself, is a beautiful thing to think about.",
  "translatedText": "다음 몇 개의 비디오에서 이것이 왜 생각해 보는 것이 유용한지 알게 될 것입니다. 하지만 지금은 그 자체로 생각하기에 아름다운 시각적 직관을 모두 내려놓고 싶습니다. .",
  "model": "google_nmt",
  "from_community_srt": "앞으로 다음 동영상에서 좀 더 보여줄거야. 왜 이렇게 생각하는게 유용한 방법인지를. 하지만 당장은, 모든 시각적 직관을 잠시 내려놓았으면 좋겠어. 매우 아름다운 것이지만, 지금 일단은 그러자.",
  "n_reviews": 0,
  "start": 210.52,
  "end": 220.1
 },
 {
  "input": "Okay, I need to confess that what I've said so far is not quite right.",
  "translatedText": "좋아요, 제가 지금까지 말한 내용이 완전히 옳지 않다는 점을 고백해야겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "좋아, 근데 난 사실 고백할게 있어. 지금까지 꽤 틀리게 말한게 있어.",
  "n_reviews": 0,
  "start": 222.12,
  "end": 225.56
 },
 {
  "input": "The full concept of the determinant allows for negative values.",
  "translatedText": "행렬식의 전체 개념은 음수 값을 허용합니다.",
  "model": "google_nmt",
  "from_community_srt": "행렬식의 온전한 개념으로 볼때, 음수(-) 값을 허용해.",
  "n_reviews": 0,
  "start": 225.88,
  "end": 229.28
 },
 {
  "input": "But what would the idea of scaling an area by a negative amount even mean?",
  "translatedText": "하지만 영역을 음수만큼 확장한다는 아이디어는 무엇을 의미할까요?",
  "model": "google_nmt",
  "from_community_srt": "그럼 영역을 스케일링할때 음수값은 무엇을 의미하는 걸까? 바로 방향(orientation)과 관계가 있어.",
  "n_reviews": 0,
  "start": 229.72,
  "end": 233.48
 },
 {
  "input": "This has to do with the idea of orientation.",
  "translatedText": "이는 방향의 개념과 관련이 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 234.94,
  "end": 236.96
 },
 {
  "input": "For example, notice how this transformation gives the sensation of flipping space over.",
  "translatedText": "예를 들어, 이러한 변형이 어떻게 공간을 뒤집는 듯한 느낌을 주는지 살펴보세요.",
  "model": "google_nmt",
  "from_community_srt": "예를 들면 이런 변환을 살펴봐봐. 공간을 뒤집는 느낌을 주고 있어.",
  "n_reviews": 0,
  "start": 237.8,
  "end": 242.68
 },
 {
  "input": "If you were thinking of 2D space as a sheet of paper, a transformation like that one seems to turn over that sheet onto the other side.",
  "translatedText": "2D 공간을 종이 한 장으로 생각했다면, 그런 변형은 그 시트를 반대편으로 뒤집는 것처럼 보입니다.",
  "model": "google_nmt",
  "from_community_srt": "만약 너가 2차원 공간에 있는 한 종이조각을 떠올렸다면, 변환이 마치 종이를 뒤집는 것과 같아.",
  "n_reviews": 0,
  "start": 243.24,
  "end": 249.86
 },
 {
  "input": "Any transformations that do this are said to invert the orientation of space.",
  "translatedText": "이를 수행하는 모든 변환은 공간의 방향을 반전시킨다고 합니다.",
  "model": "google_nmt",
  "from_community_srt": "이런 종류의 변환들을 일컬어 \"공간의 방향(orientation) 뒤집기\" 라고 불러. (역자: orientation 방향? 방위?",
  "n_reviews": 0,
  "start": 250.64,
  "end": 255.04
 },
 {
  "input": "Another way to think about it is in terms of i-hat and j-hat.",
  "translatedText": "그것에 대해 생각하는 또 다른 방법은 i-hat과 j-hat의 관점에서 보는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "원점?) i-hat 과 j-hat 을 통해 설명하는 방법도 있어.",
  "n_reviews": 0,
  "start": 255.84,
  "end": 258.6
 },
 {
  "input": "Notice that in their starting positions, j-hat is to the left of i-hat.",
  "translatedText": "시작 위치에서 j-hat은 i-hat의 왼쪽에 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 벡터들이 시작위치를 봐봐. i-hat 의 왼쪽에 j-hat 이 있어.",
  "n_reviews": 0,
  "start": 259.16,
  "end": 263.06
 },
 {
  "input": "If, after a transformation, j-hat is now on the right of i-hat, the orientation of space has been inverted.",
  "translatedText": "변환 후 j-hat이 이제 i-hat의 오른쪽에 있으면 공간의 방향이 반전된 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "변환 후에는, i-hat 의 오른쪽에 j-hat 이 있어. 공간의 방위이 반전되고 있지.",
  "n_reviews": 0,
  "start": 263.62,
  "end": 270.2
 },
 {
  "input": "Whenever this happens, whenever the orientation of space is inverted, the determinant will be negative.",
  "translatedText": "이런 일이 일어날 때마다, 공간의 방향이 반전될 때마다 행렬식은 음수가 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "이런 일이 발생할때마다, 공간의 방위(orientation) 이 뒤집힐때마다, 행렬식은 음수가 될 거야.",
  "n_reviews": 0,
  "start": 272.12,
  "end": 276.58
 },
 {
  "input": "The absolute value of the determinant, though, still tells you the factor by which areas have been scaled.",
  "translatedText": "그러나 행렬식의 절대값은 여전히 영역의 크기가 조정된 요소를 알려줍니다.",
  "model": "google_nmt",
  "from_community_srt": "그래도 행렬식의 절대값은 여전히 영역 스케일링 관한 팩터로 볼 수 있어.",
  "n_reviews": 0,
  "start": 277.46,
  "end": 282.4
 },
 {
  "input": "For example, the matrix with columns 1,1 and 2,-1 encodes a transformation that has determinant, I'll just tell you, negative 3.",
  "translatedText": "예를 들어, 열 1,1과 2,-1이 있는 행렬은 행렬식을 갖는 변환을 인코딩합니다. 그냥 말씀드리자면 마이너스 3입니다.",
  "model": "google_nmt",
  "from_community_srt": "예를 들면 열 (1,1), (2,-1) 로 구성된 행렬이 나타내는 변환의 행렬값은 바로 -3 이야.",
  "n_reviews": 0,
  "start": 283.02,
  "end": 290.68
 },
 {
  "input": "And what this means is that space gets flipped over and areas are scaled by a factor of 3.",
  "translatedText": "이것이 의미하는 바는 공간이 뒤집어지고 영역이 3배로 확장된다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "그리고 이것이 의미하는 것은 그 공간이 뒤집어졌다는 거야. 그리고 영역크기는 팩터3 으로 스케일링 됬어.",
  "n_reviews": 0,
  "start": 291.46,
  "end": 296.28
 },
 {
  "input": "So why would this idea of a negative area scaling factor be a natural way to describe orientation flipping?",
  "translatedText": "그렇다면 음의 영역 배율 인수에 대한 아이디어가 방향 뒤집기를 설명하는 자연스러운 방법인 이유는 무엇일까요?",
  "model": "google_nmt",
  "from_community_srt": "그럼 왜 음수 스케일링 팩터라는 개념이 방향 반전을 설명하는 자연스런 방법인지 알겠지? 이런 연속된 변환들을 생각해보자.",
  "n_reviews": 0,
  "start": 297.78,
  "end": 303.7
 },
 {
  "input": "Think about the series of transformations you get by slowly letting i-hat get closer and closer to j-hat.",
  "translatedText": "i-hat이 j-hat에 점점 더 가까워지도록 천천히 놔두면 얻을 수 있는 일련의 변화에 대해 생각해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "i-hat 과 j-hat 이 점점 가까워 지고 있어.",
  "n_reviews": 0,
  "start": 304.26,
  "end": 310.14
 },
 {
  "input": "As i-hat gets closer, all of the areas in space are getting squished more and more, meaning the determinant approaches 0.",
  "translatedText": "i-hat이 가까워질수록 공간의 모든 영역이 점점 더 찌그러지며 이는 행렬식이 0에 가까워진다는 것을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "i-hat 이 가까워 지면서, 공간의 모든 영역이 점점 찌부려뜨려지고 있어. 그 동안에 행렬식 값은 점점 0 에 가까워져.",
  "n_reviews": 0,
  "start": 310.72,
  "end": 317.1
 },
 {
  "input": "Once i-hat lines up perfectly with j-hat, the determinant is 0.",
  "translatedText": "i-hat이 j-hat과 완벽하게 일치하면 행렬식은 0이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "i-hat 과 j-hat 이 완전히 한 선을 이루게 되면 행렬식은 0 이야.",
  "n_reviews": 0,
  "start": 317.82,
  "end": 321.64
 },
 {
  "input": "Then, if i-hat continues the way that it was going, doesn't it kind of feel natural for the determinant to keep decreasing into the negative numbers?",
  "translatedText": "그렇다면, i-hat이 계속해서 진행된다면 행렬식은 계속해서 음수로 감소하는 것이 자연스럽게 느껴지지 않나요?",
  "model": "google_nmt",
  "from_community_srt": "근데, i-hat 이 계속 이동하게 하면 행렬식 값이 음수가 되는게 자연스럽지 않을까? 자,",
  "n_reviews": 0,
  "start": 322.44,
  "end": 329.28
 },
 {
  "input": "So that's the understanding of determinants in two dimensions.",
  "translatedText": "이것이 2차원의 행렬식에 대한 이해입니다.",
  "model": "google_nmt",
  "from_community_srt": "이것이 2차원에서 행렬식에 대한 설명이야.",
  "n_reviews": 0,
  "start": 330.68,
  "end": 333.56
 },
 {
  "input": "What do you think it should mean for three dimensions?",
  "translatedText": "3차원에서는 이것이 무엇을 의미한다고 생각하시나요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 333.56,
  "end": 335.94
 },
 {
  "input": "It also tells you how much a transformation scales things, but this time, it tells you how much volumes get scaled.",
  "translatedText": "또한 변환이 사물을 얼마나 확장하는지 알려주지만 이번에는 볼륨이 얼마나 확장되는지 알려줍니다.",
  "model": "google_nmt",
  "from_community_srt": "3차원에서는 어떨것 같아? 3 × 3 행렬의 행렬식 값도 역시 얼마나 스케일링 하는지를 알려주지만, 하지만, 이번에는 부피(volume)가 얼마나 스케일링 되는지 알려줘.",
  "n_reviews": 0,
  "start": 336.92,
  "end": 343.24
 },
 {
  "input": "Just as in two dimensions, where this is easiest to think about by focusing on one particular square with an area 1 and watching only what happens to it, in three dimensions, it helps to focus your attention on the specific 1 by 1 by 1 cube whose edges are resting on the basis vectors, i-hat, j-hat and k-hat.",
  "translatedText": "면적이 1인 하나의 특정 사각형에 초점을 맞추고 그 사각형에 무슨 일이 일어나는지 관찰함으로써 생각하기 가장 쉬운 2차원에서와 마찬가지로, 3차원에서는 특정 1x1x1 큐브에 주의를 집중하는 데 도움이 됩니다. 그 가장자리는 기본 벡터 i-hat, j-hat 및 k-hat에 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "2차원에서 이것을 가장 쉽게 생각해보는 방법은, 영역크기 1 에 해당하는 한 정사각형을 떠올려보는 거야. 그리고 그것의 변화를 쳐다봤지. 3차원에서도 이 방법은 상당히 도움이 돼. 하나의 1x1x1 큐브(정육면체)에 집중하는 거야. 이 정육면체 모서리에 각 단위벡터가 놓여있어. i-hat, j-hat, k-hat 벡터가.",
  "n_reviews": 0,
  "start": 345.34,
  "end": 363.44
 },
 {
  "input": "After the transformation, that cube might get warped into some kind of slanty slanty cube.",
  "translatedText": "변환 후 해당 큐브는 일종의 비스듬한 비스듬한 큐브로 뒤틀릴 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "변환 후 이 큐브는 기울어지고 기울어진 큐브가 돼.",
  "n_reviews": 0,
  "start": 364.32,
  "end": 369.3
 },
 {
  "input": "This shape, by the way, has the best name ever, parallelipiped, a name that's made even more delightful when your professor has a nice thick Russian accent.",
  "translatedText": "그건 그렇고, 이 모양은 평행육면체라는 최고의 이름을 가지고 있습니다. 교수님이 두껍고 굵은 러시아 억양을 가지고 계시다면 더욱 기분 좋은 이름이 될 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "근데 이 모양에 딱 맞는 이름이 있어. 평행육면체 (parallelepiped). 두꺼운 러시아 억양을 가진 발음처럼 들려.",
  "n_reviews": 0,
  "start": 370.34,
  "end": 377.44
 },
 {
  "input": "Since this cube starts out with a volume of 1, and the determinant gives the factor by which any volume is scaled, you can think of the determinant simply as being the volume of that parallelipiped that the cube turns into.",
  "translatedText": "이 입방체는 부피 1로 시작하고 행렬식은 부피의 크기가 조정되는 요소를 제공하므로 행렬식은 단순히 입방체가 변하는 평행육면체의 부피라고 생각할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 큐브는 부피가 1이고, 그리고 행렬식 값은 어떤 부피이든지 스케일링 팩터를 알려주니까 행렬식 값을 마치 평행육면체의 부피값으로 생각해도 될거야. 부피 1짜리 큐브가 바뀐 후의 부피로 말야.",
  "n_reviews": 0,
  "start": 378.52,
  "end": 390.64
 },
 {
  "input": "A determinant of 0 would mean that all of space is squished onto something with 0 volume, meaning either a flat plane, a line, or, in the most extreme case, onto a single point.",
  "translatedText": "행렬식이 0이라는 것은 모든 공간이 부피가 0인 어떤 것, 즉 평면, 선 또는 가장 극단적인 경우 단일 점에 눌려 있음을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "행렬식 값이 0 이라면 모든 공간이 찌부려뜨려서 부피 0을 만든다는 의미이고, 찌부려져서 평면이나, 선, 가장 극단적인 경우에는 단일 점이 되는 경우야.",
  "n_reviews": 0,
  "start": 392.38,
  "end": 402.5
 },
 {
  "input": "Those of you who watched chapter 2 will recognize this as meaning that the columns of the matrix are linearly dependent.",
  "translatedText": "2장을 보신 분들은 이것이 행렬의 열이 선형 종속적이라는 의미로 인식하실 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "챕터 2장을 본 사람들은 이 말을 받아들일때 그 행렬의 열들은 선형의존(linearly dependent) 하다라고 할거야.",
  "n_reviews": 0,
  "start": 403.76,
  "end": 409.24
 },
 {
  "input": "Can you see why?",
  "translatedText": "이유를 알 수 있나요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.76,
  "end": 410.42
 },
 {
  "input": "What about negative determinants?",
  "translatedText": "부정적인 결정 요인은 어떻습니까?",
  "model": "google_nmt",
  "from_community_srt": "왜 그런지 알겠어? 음수 행렬식값일때는 어떨까?",
  "n_reviews": 0,
  "start": 414.92,
  "end": 416.64
 },
 {
  "input": "What should that mean for three dimensions?",
  "translatedText": "3차원에서는 이것이 무엇을 의미할까요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.78,
  "end": 418.1
 },
 {
  "input": "One way to describe orientation in 3D is with the right hand rule.",
  "translatedText": "3D에서 방향을 설명하는 한 가지 방법은 오른손 법칙을 사용하는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "3차원에서는 무슨 의미이여야 할까? 3차원에서 방향(orientation)을 설명하는 방법으로 오른손 규칙이 있어.",
  "n_reviews": 0,
  "start": 418.78,
  "end": 422.68
 },
 {
  "input": "Point the forefinger of your right hand in the direction of i-hat, stick out your middle finger in the direction of j-hat, and notice how when you point your thumb up, it's in the direction of k-hat.",
  "translatedText": "오른손 집게손가락을 i-hat 방향으로 가리키고 가운데 손가락을 j-hat 방향으로 내밀고, 엄지손가락을 위로 향하면 k-hat 방향이 되는 것을 확인하세요.",
  "model": "google_nmt",
  "from_community_srt": "오른손의 집게손가락이 가리키는 방향이 i-hat 의 방향이 돼. 가운데 손가락이 가리키는 방향은 j-hat 방향, 그리고 알아차렸겠지만, 엄지손가락이 가리키는 방향은 k-hat 의 방향이야.",
  "n_reviews": 0,
  "start": 423.3,
  "end": 432.76
 },
 {
  "input": "If you can still do that after the transformation, orientation has not changed, and the determinant is positive.",
  "translatedText": "변환 후에도 여전히 그렇게 할 수 있다면 방향은 변경되지 않았으며 행렬식은 양수입니다.",
  "model": "google_nmt",
  "from_community_srt": "여전히 변환 이후에도 이 방향이 유지되려면, 방향(orientation) 이 바뀌지 않고, 양수 행렬식 값을 가진 경우야.",
  "n_reviews": 0,
  "start": 434.88,
  "end": 440.9
 },
 {
  "input": "Otherwise, if after the transformation it only makes sense to do that with your left hand, orientation has been flipped, and the determinant is negative.",
  "translatedText": "그렇지 않고 변환 후 왼손으로만 수행하는 것이 타당하다면 방향이 반전되어 행렬식은 음수입니다.",
  "model": "google_nmt",
  "from_community_srt": "그렇지 않으면 변환 이후에 왼손으로 바꿔야 하는 경우에는 방향(orientation)이 반전된거고, 행렬식 값은 음수가 돼.",
  "n_reviews": 0,
  "start": 441.54,
  "end": 449.38
 },
 {
  "input": "So, if you haven't seen it before, you're probably wondering by now, how do you actually compute the determinant?",
  "translatedText": "이전에 본 적이 없다면 행렬식을 실제로 어떻게 계산하는지 궁금할 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "만약 너가 전에 본 적 없다면 아마 지금쯤 꽤 궁금한게 생겼을꺼야.",
  "n_reviews": 0,
  "start": 451.9,
  "end": 457.04
 },
 {
  "input": "For a 2x2 matrix with entries a, b, c, d, the formula is a times d minus b times c.",
  "translatedText": "항목 a, b, c, d가 있는 2x2 행렬의 경우 공식은 a 곱하기 d 빼기 b 곱하기 c입니다.",
  "model": "google_nmt",
  "from_community_srt": "\"실제로 어떻게 행렬식을 계산하는 걸까?\" a,b,c,d 변수로 이루어진 2x2 행렬에서 공식은 (a * d) - (b * c) 였어.",
  "n_reviews": 0,
  "start": 457.56,
  "end": 464.42
 },
 {
  "input": "Here's part of an intuition for where this formula comes from.",
  "translatedText": "이 공식의 출처에 대한 직관의 일부는 다음과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 공식이 어떻게 나왔는지는 직관을 발휘할 부분이야.",
  "n_reviews": 0,
  "start": 465.74,
  "end": 468.5
 },
 {
  "input": "Let's say that the terms b and c both happened to be 0.",
  "translatedText": "b와 c 항이 모두 0이었다고 가정해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "b, c 둘 다 0 이라고 해보자.",
  "n_reviews": 0,
  "start": 468.88,
  "end": 471.78
 },
 {
  "input": "Then, the term a tells you how much i-hat is stretched in the x direction, and the term d tells you how much j-hat is stretched in the y direction.",
  "translatedText": "그런 다음 a 항은 i-hat이 x 방향으로 얼마나 늘어나는지 나타내고, d 항은 j-hat이 y 방향으로 얼마나 늘어나는지를 나타냅니다.",
  "model": "google_nmt",
  "from_community_srt": "그리고, a 를 i-hat 을 x축 방향으로 스케일링하는 요소로 보고, d 의 경우에는 j-hat 을 y 축방향으로 스케일링하는 요소로 보자.",
  "n_reviews": 0,
  "start": 471.78,
  "end": 481.16
 },
 {
  "input": "So, since those other terms are 0, it should make sense that a times d gives the area of the rectangle that our favorite unit square turns into, kind of like the 3, 0, 0, 2 example from earlier.",
  "translatedText": "따라서 다른 항은 0이므로 a 곱하기 d는 이전의 3, 0, 0, 2 예와 같이 우리가 가장 좋아하는 단위 정사각형이 변하는 직사각형의 면적을 제공한다는 것이 의미가 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "다른 값들(b,c)은 모두 0이기 때문에 행렬식 결과는 a * d 가 될거야. 그럼 단위 정사각형이 변환 후에 직사각형이 될거야. 앞서나온 행렬 (3, 0, 0, 2) 의 경우와 똑같아.",
  "n_reviews": 0,
  "start": 482.76,
  "end": 493.36
 },
 {
  "input": "Even if only one of b or c are 0, you'll have a parallelogram with a base a and a height d.",
  "translatedText": "b나 c 중 하나만 0이더라도 밑변이 a이고 높이가 d인 평행사변형이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "b, c 값 중 하나만 0 이라도, 평행사변형을 얻게될거야. 밑변 길이가 a 이고, 높이가 d 야.",
  "n_reviews": 0,
  "start": 495.36,
  "end": 501.76
 },
 {
  "input": "So, the area should still be a times d.",
  "translatedText": "따라서 면적은 여전히 d의 곱이어야 합니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서 영역 크기는 똑같이 a * d 가 돼.",
  "n_reviews": 0,
  "start": 501.78,
  "end": 504.5
 },
 {
  "input": "Loosely speaking, if both b and c are non-zero, then that b times c term tells you how much this parallelogram is stretched or squished in the diagonal direction.",
  "translatedText": "느슨하게 말하면, b와 c가 모두 0이 아니면 b 곱하기 c 항은 이 평행사변형이 대각선 방향으로 얼마나 늘어나거나 찌그러졌는지 알려줍니다.",
  "model": "google_nmt",
  "from_community_srt": "비공식적으로 말하자면 b, c 가 둘 다 0 이 아닌 경우엔 b * c 값이 알려주는 것은 이 평행사변형의 얼마나 대각선 방향으로 늘려지거나 찌그러지를 말해줘.",
  "n_reviews": 0,
  "start": 505.46,
  "end": 515.46
 },
 {
  "input": "For those of you hungry for a more precise description of this b times c term, here's a helpful diagram if you'd like to pause and ponder.",
  "translatedText": "이 b 곱하기 c 용어에 대한 더 정확한 설명을 원하는 분들을 위해 잠시 멈춰서 숙고하고 싶다면 여기에 유용한 다이어그램이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "아마 b * c 에 대한 좀 더 정확한 설명을 듣고 싶은 사람도 있을텐데 여기에 그런 사람들을 위해 도움될만한 다이어그램을 준비했어.",
  "n_reviews": 0,
  "start": 516.66,
  "end": 522.88
 },
 {
  "input": "Now, if you feel like computing determinants by hand is something that you need to know, the only way to get it down is to just practice it with a few.",
  "translatedText": "이제 손으로 행렬식을 계산하는 것이 알아야 할 사항이라고 생각되면 이를 이해하는 유일한 방법은 몇 가지를 가지고 연습하는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "만약 손으로 직접 행렬식 값을 계산할 생각이라면 너가 알아야만 할 것이 있어. 그것을 익히는 유일한 방법은 몇번 연습해보는 방법밖에 없어.",
  "n_reviews": 0,
  "start": 523.98,
  "end": 531.2
 },
 {
  "input": "There's really not that much I can say or animate that's going to drill in the computation.",
  "translatedText": "실제로 계산을 드릴링할 수 있는 말이나 애니메이션이 많지 않습니다.",
  "model": "google_nmt",
  "from_community_srt": "계산에 관해서는 영상이나 준비한 말은 별로 없어.",
  "n_reviews": 0,
  "start": 531.2,
  "end": 535.18
 },
 {
  "input": "This is all triply true for three-dimensional determinants.",
  "translatedText": "이것은 3차원 행렬식에 대해 모두 삼중으로 사실입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 모든 것들은 3차원에서 행렬식에 대해서도 모두 참이야.",
  "n_reviews": 0,
  "start": 536.12,
  "end": 538.64
 },
 {
  "input": "There is a formula, and if you feel like that's something you need to know, you should practice with a few matrices, or, you know, go watch Sal Khan work through a few.",
  "translatedText": "공식이 있으며, 그것이 알아야 할 것이라고 생각되면 몇 가지 행렬을 사용하여 연습하거나 Sal Khan이 몇 가지를 수행하는 것을 시청해야 합니다.",
  "model": "google_nmt",
  "from_community_srt": "여기 공식이 있어. 만약 너가 이 공식에 대해 뭔가 알아내고 싶다면 몇가지 행렬들로 연습을 해야만 해. 아니면 살만 칸(Sal Khan) 의 동영상 몇개를 찾아봐봐.",
  "n_reviews": 0,
  "start": 539.04,
  "end": 546.34
 },
 {
  "input": "Honestly, though, I don't think that those computations fall within the essence of linear algebra, but I definitely think that understanding what the determinant represents falls within that essence.",
  "translatedText": "솔직히 말해서, 나는 그 계산이 선형 대수학의 본질에 속한다고 생각하지 않습니다. 그러나 행렬식이 무엇을 나타내는지 이해하는 것은 분명히 그 본질에 속한다고 생각합니다.",
  "model": "google_nmt",
  "from_community_srt": "솔직히 말해서 이런 계산은 선형대수의 본질은 아닌 것 같아. 난 행렬식값이 무엇을 나타내는지 아는 것이 본질에 해당한다고 생각해.",
  "n_reviews": 0,
  "start": 547.24,
  "end": 556.46
 },
 {
  "input": "Here's kind of a fun question to think about before the next video.",
  "translatedText": "다음 비디오를 보기 전에 생각해 볼 만한 재미있는 질문이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "다음 동영상으로 가기전에 생각해볼만한 재밌는 퀴즈가 있어.",
  "n_reviews": 0,
  "start": 558.06,
  "end": 560.64
 },
 {
  "input": "If you multiply two matrices together, the determinant of the resulting matrix is the same as the product of the determinants of the original two matrices.",
  "translatedText": "두 행렬을 함께 곱하면 결과 행렬의 행렬식은 원래 두 행렬의 행렬식의 곱과 동일합니다.",
  "model": "google_nmt",
  "from_community_srt": "두 개의 행렬을 곱한 후 얻어지는 행렬식값은 따로 두 행렬의 행렬식값을 구해서 곱하는 것과 같을까? 만약 숫자로 이것을 증명하려 한다면,",
  "n_reviews": 0,
  "start": 560.64,
  "end": 570.08
 },
 {
  "input": "If you tried to justify this with numbers, it would take a really long time, but see if you can explain why this makes sense in just one sentence.",
  "translatedText": "이것을 숫자로 정당화하려고 하면 시간이 정말 오래 걸리겠지만, 이것이 왜 의미가 있는지 한 문장으로 설명할 수 있는지 살펴보세요.",
  "model": "google_nmt",
  "from_community_srt": "꽤 오랜 시간이 걸릴거야. 한 문장으로 왜그런지 설명을 한번 해봐.",
  "n_reviews": 0,
  "start": 571.1,
  "end": 577.88
 },
 {
  "input": "Next up, I'll be relating the idea of linear transformations covered so far to one of the areas where linear algebra is most useful, linear systems of equations.",
  "translatedText": "다음으로, 지금까지 다룬 선형 변환의 아이디어를 선형 대수가 가장 유용한 영역 중 하나인 선형 방정식 시스템과 연관지을 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "다음에는 지금까지 다룬 선형변환 개념을 다른 것과 엮어볼거야. 선형대수가 가장 유용한 분야들 하나야. 선형 방정식계를 사용하는 분야지.",
  "n_reviews": 0,
  "start": 582.0,
  "end": 591.6
 }
]