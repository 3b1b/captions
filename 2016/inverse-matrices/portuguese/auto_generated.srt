1
00:00:10,240 --> 00:00:13,273
Como você provavelmente já pode perceber, a maior parte desta 

2
00:00:13,273 --> 00:00:16,404
série trata da compreensão das operações matriciais e vetoriais 

3
00:00:16,404 --> 00:00:19,340
através das lentes mais visuais das transformações lineares.

4
00:00:19,980 --> 00:00:23,883
Este vídeo não é exceção, descrevendo os conceitos de matrizes inversas, 

5
00:00:23,883 --> 00:00:27,520
espaço de coluna, classificação e espaço nulo através dessas lentes.

6
00:00:27,520 --> 00:00:31,738
Porém, um aviso: não vou falar sobre os métodos para realmente calcular essas coisas, 

7
00:00:31,738 --> 00:00:34,240
e alguns argumentariam que isso é muito importante.

8
00:00:34,840 --> 00:00:38,839
Existem muitos recursos muito bons para aprender esses métodos fora desta série, 

9
00:00:38,839 --> 00:00:42,000
palavras-chave eliminação gaussiana e forma escalonada de linha.

10
00:00:42,540 --> 00:00:44,399
Acho que a maior parte do valor que realmente 

11
00:00:44,399 --> 00:00:46,340
tenho a agregar aqui está na metade da intuição.

12
00:00:46,900 --> 00:00:48,623
Além disso, na prática, geralmente obtemos software 

13
00:00:48,623 --> 00:00:50,480
para calcular essas coisas para nós de qualquer maneira.

14
00:00:51,500 --> 00:00:53,920
Primeiro, algumas palavras sobre a utilidade da álgebra linear.

15
00:00:54,300 --> 00:00:57,891
Até agora, você já tem uma dica de como ele é usado para descrever a manipulação 

16
00:00:57,891 --> 00:01:01,040
do espaço, o que é útil para coisas como computação gráfica e robótica.

17
00:01:01,500 --> 00:01:04,384
Mas uma das principais razões pelas quais a álgebra linear é mais 

18
00:01:04,384 --> 00:01:07,531
amplamente aplicável e necessária para praticamente qualquer disciplina 

19
00:01:07,531 --> 00:01:10,460
técnica é que ela nos permite resolver certos sistemas de equações.

20
00:01:11,380 --> 00:01:14,799
Quando digo sistema de equações, quero dizer que você tem uma lista de variáveis, 

21
00:01:14,799 --> 00:01:17,760
coisas que você não conhece, e uma lista de equações que as relacionam.

22
00:01:18,340 --> 00:01:21,600
Em muitas situações, essas equações podem ficar muito complicadas.

23
00:01:22,120 --> 00:01:25,300
Mas, se você tiver sorte, eles poderão assumir uma certa forma especial.

24
00:01:26,440 --> 00:01:29,889
Dentro de cada equação, a única coisa que acontece com cada variável é que 

25
00:01:29,889 --> 00:01:33,430
ela é escalonada por alguma constante, e a única coisa que acontece com cada 

26
00:01:33,430 --> 00:01:36,880
uma dessas variáveis escalonadas é que elas são adicionadas umas às outras.

27
00:01:37,540 --> 00:01:41,657
Portanto, nada de expoentes, funções sofisticadas ou multiplicação de duas variáveis, 

28
00:01:41,657 --> 00:01:42,280
coisas assim.

29
00:01:43,420 --> 00:01:47,701
A maneira típica de organizar esse tipo de sistema especial de equações é colocar 

30
00:01:47,701 --> 00:01:52,140
todas as variáveis à esquerda e colocar quaisquer constantes remanescentes à direita.

31
00:01:53,600 --> 00:01:56,200
Também é bom alinhar verticalmente as variáveis comuns e, 

32
00:01:56,200 --> 00:01:59,249
para fazer isso, talvez seja necessário inserir alguns coeficientes 

33
00:01:59,249 --> 00:02:01,940
zero sempre que a variável não aparecer em uma das equações.

34
00:02:04,540 --> 00:02:07,240
Isso é chamado de sistema linear de equações.

35
00:02:08,100 --> 00:02:11,180
Você pode notar que isso se parece muito com a multiplicação de vetores de matrizes.

36
00:02:11,820 --> 00:02:16,826
Na verdade, você pode empacotar todas as equações em uma única equação vetorial onde 

37
00:02:16,826 --> 00:02:21,891
você tem a matriz contendo todos os coeficientes constantes e um vetor contendo todas 

38
00:02:21,891 --> 00:02:26,780
as variáveis, e seu produto matriz-vetor é igual a algum vetor constante diferente.

39
00:02:28,640 --> 00:02:32,852
Vamos chamar essa matriz constante de A, denotar o vetor que contém as 

40
00:02:32,852 --> 00:02:37,480
variáveis com um X em negrito e chamar o vetor constante no lado direito de V.

41
00:02:38,860 --> 00:02:41,002
Isso é mais do que apenas um truque de notação para 

42
00:02:41,002 --> 00:02:42,980
escrever nosso sistema de equações em uma linha.

43
00:02:43,340 --> 00:02:46,780
Ele lança luz sobre uma interpretação geométrica muito interessante para o problema.

44
00:02:47,620 --> 00:02:50,853
A matriz A corresponde a alguma transformação linear, 

45
00:02:50,853 --> 00:02:55,524
então resolver Ax igual a V significa que estamos procurando um vetor X, que, 

46
00:02:55,524 --> 00:02:57,920
após aplicar a transformação, chega a V.

47
00:02:59,940 --> 00:03:01,780
Pense no que está acontecendo aqui por um momento.

48
00:03:02,060 --> 00:03:05,969
Você pode ter em mente essa ideia realmente complicada de múltiplas variáveis, 

49
00:03:05,969 --> 00:03:09,334
todas misturadas umas com as outras, apenas pensando em comprimir e 

50
00:03:09,334 --> 00:03:12,600
transformar o espaço e tentar descobrir qual vetor pousa em outro.

51
00:03:13,160 --> 00:03:13,760
Legal certo?

52
00:03:14,600 --> 00:03:16,539
Para começar de forma simples, digamos que você 

53
00:03:16,539 --> 00:03:18,680
tenha um sistema com duas equações e duas incógnitas.

54
00:03:19,000 --> 00:03:23,960
Isso significa que a matriz A é uma matriz 2x2 e V e X são vetores bidimensionais.

55
00:03:25,600 --> 00:03:29,817
Agora, a forma como pensamos sobre as soluções para esta equação depende se a 

56
00:03:29,817 --> 00:03:33,871
transformação associada a A comprime todo o espaço numa dimensão inferior, 

57
00:03:33,871 --> 00:03:38,197
como uma reta ou um ponto, ou se deixa tudo que abrange todas as duas dimensões 

58
00:03:38,197 --> 00:03:38,900
onde começou.

59
00:03:40,320 --> 00:03:44,012
Na linguagem do último vídeo, subdividimos nos casos em que A tem 

60
00:03:44,012 --> 00:03:48,040
determinante zero e no caso em que A tem determinante diferente de zero.

61
00:03:51,300 --> 00:03:54,654
Vamos começar com o caso mais provável, onde o determinante é diferente de zero, 

62
00:03:54,654 --> 00:03:57,720
o que significa que o espaço não fica comprimido numa região de área zero.

63
00:03:58,600 --> 00:04:02,231
Nesse caso, sempre haverá um e apenas um vetor que cai em V, 

64
00:04:02,231 --> 00:04:06,160
e você pode encontrá-lo reproduzindo a transformação ao contrário.

65
00:04:06,700 --> 00:04:10,196
Seguindo para onde V vai enquanto rebobinamos a fita assim, 

66
00:04:10,196 --> 00:04:13,460
você encontrará o vetor x tal que A vezes x é igual a V.

67
00:04:15,400 --> 00:04:17,958
Quando você reproduz a transformação ao contrário, 

68
00:04:17,958 --> 00:04:21,168
ela na verdade corresponde a uma transformação linear separada, 

69
00:04:21,168 --> 00:04:24,680
comumente chamada de inversa de A, denotada por A elevado ao negativo.

70
00:04:25,360 --> 00:04:29,137
Por exemplo, se A fosse uma rotação de 90 graus no sentido anti-horário, 

71
00:04:29,137 --> 00:04:32,760
então o inverso de A seria uma rotação de 90 graus no sentido horário.

72
00:04:34,320 --> 00:04:38,159
Se A fosse um cisalhamento para a direita que empurra J-Hat uma unidade para a direita, 

73
00:04:38,159 --> 00:04:40,996
o inverso de A seria um cisalhamento para a esquerda que empurra 

74
00:04:40,996 --> 00:04:42,480
J-Hat uma unidade para a esquerda.

75
00:04:44,100 --> 00:04:47,209
Em geral, A inverso é a transformação única com a propriedade 

76
00:04:47,209 --> 00:04:51,272
de que se você primeiro aplicar A e depois seguir com a transformação A inversa, 

77
00:04:51,272 --> 00:04:53,480
você terminará de volta ao ponto de partida.

78
00:04:54,540 --> 00:04:58,718
A aplicação de uma transformação após a outra é capturada algebricamente com a 

79
00:04:58,718 --> 00:05:02,897
multiplicação de matrizes, portanto, a propriedade central desta transformação 

80
00:05:02,897 --> 00:05:07,340
A inversa é que A inversa vezes A é igual à matriz que corresponde a não fazer nada.

81
00:05:08,200 --> 00:05:11,320
A transformação que não faz nada é chamada de transformação de identidade.

82
00:05:11,780 --> 00:05:18,080
Ele deixa i-hat e j-hat onde estão, imóveis, então suas colunas são 1,0 e 0,1.

83
00:05:19,980 --> 00:05:24,104
Depois de encontrar essa inversa, o que na prática você faria com um computador, 

84
00:05:24,104 --> 00:05:27,720
você pode resolver sua equação multiplicando essa matriz inversa por v.

85
00:05:29,960 --> 00:05:33,253
E, novamente, o que isso significa geometricamente é que você 

86
00:05:33,253 --> 00:05:36,440
está reproduzindo a transformação ao contrário e seguindo v.

87
00:05:40,200 --> 00:05:44,301
Este caso de determinante diferente de zero, que para uma escolha aleatória de 

88
00:05:44,301 --> 00:05:48,402
matriz é de longe o mais provável, corresponde à ideia de que se tivermos duas 

89
00:05:48,402 --> 00:05:52,400
incógnitas e duas equações, é quase certo que exista uma única solução única.

90
00:05:53,680 --> 00:05:56,320
Esta ideia também faz sentido em dimensões superiores, 

91
00:05:56,320 --> 00:05:59,200
quando o número de equações é igual ao número de incógnitas.

92
00:05:59,380 --> 00:06:03,663
Novamente, o sistema de equações pode ser traduzido para a 

93
00:06:03,663 --> 00:06:09,400
interpretação geométrica onde você tem alguma transformação A e algum vetor v, 

94
00:06:09,400 --> 00:06:12,740
e você está procurando o vetor x que cai em v.

95
00:06:15,740 --> 00:06:20,187
Contanto que a transformação A não comprima todo o espaço em uma dimensão inferior, 

96
00:06:20,187 --> 00:06:23,257
o que significa que seu determinante é diferente de zero, 

97
00:06:23,257 --> 00:06:27,069
haverá uma transformação inversa A inversa, com a propriedade de que se 

98
00:06:27,069 --> 00:06:31,040
você fizer A primeiro, então fará A inverso , é o mesmo que não fazer nada.

99
00:06:33,540 --> 00:06:36,398
E para resolver sua equação, basta multiplicar 

100
00:06:36,398 --> 00:06:39,440
essa matriz de transformação reversa pelo vetor v.

101
00:06:43,500 --> 00:06:47,966
Mas quando o determinante é zero e a transformação associada ao sistema 

102
00:06:47,966 --> 00:06:52,060
de equações comprime o espaço numa dimensão menor, não há inverso.

103
00:06:52,480 --> 00:06:55,460
Você não pode cancelar uma linha para transformá-la em um avião.

104
00:06:55,980 --> 00:06:58,060
Pelo menos isso não é algo que uma função possa fazer.

105
00:06:58,360 --> 00:07:02,980
Isso exigiria transformar cada vetor individual em uma linha inteira cheia de vetores.

106
00:07:03,740 --> 00:07:06,740
Mas as funções só podem levar uma única entrada para uma única saída.

107
00:07:08,400 --> 00:07:11,484
Da mesma forma, para três equações e três incógnitas, 

108
00:07:11,484 --> 00:07:16,340
não haverá inverso se a transformação correspondente comprimir o espaço 3D no plano, 

109
00:07:16,340 --> 00:07:19,140
ou mesmo se o comprimir sobre uma linha ou ponto.

110
00:07:19,920 --> 00:07:22,540
Tudo isso corresponde a um determinante zero, já que 

111
00:07:22,540 --> 00:07:25,160
qualquer região é comprimida em algo com volume zero.

112
00:07:26,700 --> 00:07:30,640
Ainda é possível que exista uma solução mesmo quando não há inverso.

113
00:07:30,720 --> 00:07:34,866
Acontece que quando sua transformação comprime o espaço em, digamos, uma reta, 

114
00:07:34,866 --> 00:07:39,380
você precisa ter sorte o suficiente para que o vetor v viva em algum lugar dessa reta.

115
00:07:43,300 --> 00:07:45,941
Você pode notar que alguns desses casos de determinante 

116
00:07:45,941 --> 00:07:48,300
zero parecem muito mais restritivos do que outros.

117
00:07:48,840 --> 00:07:52,504
Dada uma matriz 3x3, por exemplo, parece muito mais difícil existir uma 

118
00:07:52,504 --> 00:07:56,321
solução quando ela comprime o espaço em uma linha em comparação com quando 

119
00:07:56,321 --> 00:08:00,240
ela comprime as coisas em um plano, mesmo que ambos sejam determinantes zero.

120
00:08:02,600 --> 00:08:06,100
Temos uma linguagem um pouco mais específica do que apenas dizer determinante zero.

121
00:08:06,520 --> 00:08:09,034
Quando a saída de uma transformação é uma linha, 

122
00:08:09,034 --> 00:08:13,500
o que significa que é unidimensional, dizemos que a transformação tem classificação um.

123
00:08:15,140 --> 00:08:17,972
Se todos os vetores pousarem em algum plano bidimensional, 

124
00:08:17,972 --> 00:08:20,420
dizemos que a transformação tem classificação dois.

125
00:08:22,920 --> 00:08:25,104
Portanto, a palavra classificação significa o 

126
00:08:25,104 --> 00:08:27,480
número de dimensões na saída de uma transformação.

127
00:08:28,400 --> 00:08:32,720
Por exemplo, no caso de matrizes 2x2, a classificação dois é o melhor que pode ser.

128
00:08:33,080 --> 00:08:35,976
Isso significa que os vetores de base continuam a abranger 

129
00:08:35,976 --> 00:08:39,020
todas as duas dimensões do espaço e o determinante não é zero.

130
00:08:39,419 --> 00:08:42,853
Mas para matrizes 3x3, a classificação dois significa que entramos em colapso, 

131
00:08:42,853 --> 00:08:46,460
mas não tanto quanto teriam entrado em colapso em uma situação de classificação um.

132
00:08:47,240 --> 00:08:50,200
Se uma transformação 3D tiver um determinante diferente de zero e 

133
00:08:50,200 --> 00:08:53,340
sua saída preencher todo o espaço 3D, ela terá uma classificação três.

134
00:08:54,520 --> 00:08:58,542
Este conjunto de todas as saídas possíveis para a sua matriz, seja uma linha, 

135
00:08:58,542 --> 00:09:02,720
um plano, um espaço 3D, seja o que for, é chamado de espaço coluna da sua matriz.

136
00:09:04,140 --> 00:09:06,280
Você provavelmente pode adivinhar de onde vem esse nome.

137
00:09:06,560 --> 00:09:10,643
As colunas da sua matriz informam onde os vetores de base pousam, 

138
00:09:10,643 --> 00:09:15,840
e a extensão desses vetores de base transformados fornece todas as saídas possíveis.

139
00:09:16,360 --> 00:09:21,140
Em outras palavras, o espaço da coluna é a extensão das colunas da sua matriz.

140
00:09:23,300 --> 00:09:26,019
Portanto, uma definição mais precisa de classificação 

141
00:09:26,019 --> 00:09:28,940
seria que ela é o número de dimensões no espaço da coluna.

142
00:09:29,940 --> 00:09:32,194
Quando essa classificação é tão alta quanto possível, 

143
00:09:32,194 --> 00:09:34,282
o que significa que é igual ao número de colunas, 

144
00:09:34,282 --> 00:09:36,120
chamamos a matriz de classificação completa.

145
00:09:38,540 --> 00:09:42,079
Observe que o vetor zero sempre será incluído no espaço coluna, 

146
00:09:42,079 --> 00:09:45,840
pois as transformações lineares devem manter a origem fixa no lugar.

147
00:09:46,900 --> 00:09:49,286
Para uma transformação de classificação completa, 

148
00:09:49,286 --> 00:09:51,960
o único vetor que chega à origem é o próprio vetor zero.

149
00:09:52,460 --> 00:09:54,870
Mas para matrizes que não são de classificação completa, 

150
00:09:54,870 --> 00:09:57,998
que se comprimem em uma dimensão menor, você pode ter um monte de vetores 

151
00:09:57,998 --> 00:09:58,760
que chegam a zero.

152
00:10:01,640 --> 00:10:05,424
Se uma transformação 2D comprime o espaço em uma linha, por exemplo, 

153
00:10:05,424 --> 00:10:10,031
há uma linha separada em uma direção diferente cheia de vetores que são comprimidas 

154
00:10:10,031 --> 00:10:10,580
na origem.

155
00:10:11,780 --> 00:10:14,573
Se uma transformação 3D comprime o espaço em um plano, 

156
00:10:14,573 --> 00:10:17,620
há também uma linha completa de vetores que pousa na origem.

157
00:10:20,520 --> 00:10:23,829
Se uma transformação 3D comprimir todo o espaço em uma linha, 

158
00:10:23,829 --> 00:10:27,460
então haverá um plano inteiro cheio de vetores que pousam na origem.

159
00:10:32,800 --> 00:10:35,757
Esse conjunto de vetores que chega à origem é 

160
00:10:35,757 --> 00:10:38,780
chamado de espaço nulo ou núcleo da sua matriz.

161
00:10:39,360 --> 00:10:42,114
É o espaço de todos os vetores que se tornam nulos, 

162
00:10:42,114 --> 00:10:44,180
no sentido de que pousam no vetor zero.

163
00:10:45,680 --> 00:10:49,690
Em termos do sistema linear de equações, quando v é o vetor zero, 

164
00:10:49,690 --> 00:10:53,640
o espaço nulo fornece todas as soluções possíveis para a equação.

165
00:10:56,420 --> 00:10:59,069
Essa é uma visão geral de alto nível de como pensar 

166
00:10:59,069 --> 00:11:01,720
geometricamente sobre sistemas lineares de equações.

167
00:11:02,300 --> 00:11:05,766
Cada sistema tem algum tipo de transformação linear associada a ele, 

168
00:11:05,766 --> 00:11:10,137
e quando essa transformação tem uma inversa, você pode usar essa inversa para resolver 

169
00:11:10,137 --> 00:11:10,740
seu sistema.

170
00:11:12,280 --> 00:11:16,111
Caso contrário, a ideia de espaço de colunas permite-nos compreender 

171
00:11:16,111 --> 00:11:19,664
quando existe uma solução, e a ideia de espaço nulo ajuda-nos a 

172
00:11:19,664 --> 00:11:23,440
compreender como pode ser o conjunto de todas as soluções possíveis.

173
00:11:24,980 --> 00:11:27,371
Novamente, há muitas coisas que não abordei aqui, 

174
00:11:27,371 --> 00:11:29,380
principalmente como calcular essas coisas.

175
00:11:29,800 --> 00:11:32,280
Também tive que limitar meu escopo a exemplos em que 

176
00:11:32,280 --> 00:11:34,760
o número de equações é igual ao número de incógnitas.

177
00:11:34,880 --> 00:11:37,285
Mas o objetivo aqui não é tentar ensinar tudo, 

178
00:11:37,285 --> 00:11:40,357
é que você tenha uma forte intuição para matrizes inversas, 

179
00:11:40,357 --> 00:11:43,991
espaço de colunas e espaço nulo, e que essas intuições tornem qualquer 

180
00:11:43,991 --> 00:11:46,500
aprendizado futuro que você fizer mais frutífero.

181
00:11:47,660 --> 00:11:49,904
O próximo vídeo, a pedido popular, será uma breve 

182
00:11:49,904 --> 00:11:51,880
nota de rodapé sobre matrizes não quadradas.

183
00:11:51,880 --> 00:11:55,770
Depois disso, vou dar minha opinião sobre produtos escalares e algo muito 

184
00:11:55,770 --> 00:11:59,660
legal que acontece quando você os vê sob a luz de transformações lineares.

