1
00:00:10,240 --> 00:00:14,846
Som du sikkert kan se nu, handler hovedparten af denne serie om at forstå matrix- 

2
00:00:14,846 --> 00:00:19,340
og vektoroperationer gennem den mere visuelle linse af lineære transformationer.

3
00:00:19,980 --> 00:00:24,587
Denne video er ingen undtagelse, der beskriver begreberne omvendte matricer, 

4
00:00:24,587 --> 00:00:27,520
søjlerum, rangering og nulrum gennem denne linse.

5
00:00:27,520 --> 00:00:32,000
En forvarsel dog, jeg vil ikke tale om metoderne til faktisk at beregne disse ting, 

6
00:00:32,000 --> 00:00:34,240
og nogle vil hævde, at det er ret vigtigt.

7
00:00:34,840 --> 00:00:39,308
Der er mange meget gode ressourcer til at lære disse metoder uden for denne serie, 

8
00:00:39,308 --> 00:00:42,000
nøgleord Gaussian elimination og row echelon form.

9
00:00:42,540 --> 00:00:45,248
Jeg tror, at det meste af den værdi, jeg faktisk skal tilføje her, 

10
00:00:45,248 --> 00:00:46,340
er på intuitionens halvdel.

11
00:00:46,900 --> 00:00:50,480
Plus, i praksis får vi som regel software til at beregne disse ting for os alligevel.

12
00:00:51,500 --> 00:00:53,920
Først et par ord om anvendeligheden af lineær algebra.

13
00:00:54,300 --> 00:00:56,264
På nuværende tidspunkt har du allerede et tip til, 

14
00:00:56,264 --> 00:00:58,536
hvordan det bruges til at beskrive manipulation af rummet, 

15
00:00:58,536 --> 00:01:01,040
hvilket er nyttigt til ting som computergrafik og robotteknologi.

16
00:01:01,500 --> 00:01:05,979
Men en af hovedårsagerne til, at lineær algebra er mere bredt anvendelig og påkrævet 

17
00:01:05,979 --> 00:01:10,460
for næsten enhver teknisk disciplin, er, at den lader os løse visse ligningssystemer.

18
00:01:11,380 --> 00:01:14,818
Når jeg siger ligningssystem, mener jeg, at du har en liste over variabler, 

19
00:01:14,818 --> 00:01:17,760
ting, du ikke ved, og en liste over ligninger, der relaterer dem.

20
00:01:18,340 --> 00:01:21,600
I mange situationer kan disse ligninger blive meget komplicerede.

21
00:01:22,120 --> 00:01:25,300
Men hvis du er heldig, kan de antage en vis speciel form.

22
00:01:26,440 --> 00:01:30,047
Inden for hver ligning er det eneste, der sker med hver variabel, 

23
00:01:30,047 --> 00:01:32,671
at den skaleres med en konstant, og det eneste, 

24
00:01:32,671 --> 00:01:36,880
der sker med hver af disse skalerede variabler, er, at de føjes til hinanden.

25
00:01:37,540 --> 00:01:41,716
Så ingen eksponenter eller smarte funktioner eller multiplikation af to variable sammen, 

26
00:01:41,716 --> 00:01:42,280
sådan noget.

27
00:01:43,420 --> 00:01:47,887
Den typiske måde at organisere denne slags specielle ligningssystem på er at kaste 

28
00:01:47,887 --> 00:01:52,140
alle variablerne til venstre og sætte eventuelle dvælende konstanter til højre.

29
00:01:53,600 --> 00:01:56,507
Det er også rart at opstille de almindelige variable lodret, 

30
00:01:56,507 --> 00:01:59,795
og for at gøre det skal du muligvis indsætte nogle nulkoefficienter, 

31
00:01:59,795 --> 00:02:01,940
når variablen ikke vises i en af ligningerne.

32
00:02:04,540 --> 00:02:07,240
Dette kaldes et lineært ligningssystem.

33
00:02:08,100 --> 00:02:11,180
Du vil måske bemærke, at dette ligner matrix-vektor multiplikation.

34
00:02:11,820 --> 00:02:16,144
Faktisk kan du pakke alle ligningerne sammen til en enkelt vektorligning, 

35
00:02:16,144 --> 00:02:20,994
hvor du har matrixen, der indeholder alle de konstante koefficienter og en vektor, 

36
00:02:20,994 --> 00:02:25,845
der indeholder alle variablerne, og deres matrix-vektorprodukt er lig med en anden 

37
00:02:25,845 --> 00:02:26,780
konstant vektor.

38
00:02:28,640 --> 00:02:31,903
Lad os navngive den konstantmatrix A, angive vektoren, 

39
00:02:31,903 --> 00:02:34,988
der indeholder variablerne med et X med fed skrift, 

40
00:02:34,988 --> 00:02:37,480
og kalde konstantvektoren på højre side V.

41
00:02:38,860 --> 00:02:41,009
Dette er mere end blot et notationstrick til at 

42
00:02:41,009 --> 00:02:42,980
få vores ligningssystem skrevet på én linje.

43
00:02:43,340 --> 00:02:46,780
Det kaster lys over en ret cool geometrisk fortolkning af problemet.

44
00:02:47,620 --> 00:02:50,924
Matricen A svarer til en eller anden lineær transformation, 

45
00:02:50,924 --> 00:02:54,780
så løsning af Ax er lig med V betyder, at vi leder efter en vektor X, 

46
00:02:54,780 --> 00:02:57,920
som, efter at have anvendt transformationen, lander på V.

47
00:02:59,940 --> 00:03:01,780
Tænk over, hvad der sker her et øjeblik.

48
00:03:02,060 --> 00:03:05,604
Du kan holde i dit hoved denne virkelig komplicerede idé om flere variabler, 

49
00:03:05,604 --> 00:03:08,917
der alle blander sig med hinanden, bare ved at tænke på at squishing og 

50
00:03:08,917 --> 00:03:12,600
morphing space og forsøge at finde ud af, hvilken vektor der lander på en anden.

51
00:03:13,160 --> 00:03:13,760
Fedt, ikke?

52
00:03:14,600 --> 00:03:18,680
For at starte enkelt, lad os sige, at du har et system med to ligninger og to ukendte.

53
00:03:19,000 --> 00:03:23,960
Det betyder, at matrixen A er en 2x2 matrix, og V og X er hver todimensionelle vektorer.

54
00:03:25,600 --> 00:03:30,000
Hvordan vi tænker på løsningerne til denne ligning afhænger nu af, om transformationen, 

55
00:03:30,000 --> 00:03:33,550
der er forbundet med A, klemmer hele rummet ind i en lavere dimension, 

56
00:03:33,550 --> 00:03:38,000
som en linje eller et punkt, eller om den lader alt spænde over de fulde to dimensioner, 

57
00:03:38,000 --> 00:03:38,900
hvor den startede.

58
00:03:40,320 --> 00:03:43,593
På sproget i den sidste video opdeler vi i tilfælde, 

59
00:03:43,593 --> 00:03:48,040
hvor A har nul determinant og tilfælde, hvor A har ikke-nul determinant.

60
00:03:51,300 --> 00:03:54,766
Lad os starte med det mest sandsynlige tilfælde, hvor determinanten er ikke-nul, 

61
00:03:54,766 --> 00:03:57,720
hvilket betyder, at rummet ikke bliver klemt ind i et område med nul.

62
00:03:58,600 --> 00:04:02,806
I dette tilfælde vil der altid være én og kun én vektor, der lander på V, 

63
00:04:02,806 --> 00:04:06,160
og du kan finde den ved at spille transformationen omvendt.

64
00:04:06,700 --> 00:04:10,249
Efter hvor V går, mens vi spoler båndet tilbage på denne måde, 

65
00:04:10,249 --> 00:04:13,460
vil du finde vektoren x sådan, at A gange x er lig med V.

66
00:04:15,400 --> 00:04:20,011
Når du spiller transformationen omvendt, svarer den faktisk til en separat lineær 

67
00:04:20,011 --> 00:04:24,680
transformation, almindeligvis kaldet den inverse af A, betegnet A til den negative.

68
00:04:25,360 --> 00:04:28,886
For eksempel, hvis A var en rotation mod uret med 90 grader, 

69
00:04:28,886 --> 00:04:32,760
så ville det omvendte af A være en rotation med uret med 90 grader.

70
00:04:34,320 --> 00:04:37,898
Hvis A var en forskydning til højre, der skubber j-hat en enhed til højre, 

71
00:04:37,898 --> 00:04:40,618
ville det omvendte af A være en forskydning til venstre, 

72
00:04:40,618 --> 00:04:42,480
der skubber j-hat en enhed til venstre.

73
00:04:44,100 --> 00:04:47,604
Generelt er A invers den unikke transformation med den egenskab, 

74
00:04:47,604 --> 00:04:51,593
at hvis du først anvender A, så følger den med transformationen A invers, 

75
00:04:51,593 --> 00:04:53,480
ender du tilbage, hvor du startede.

76
00:04:54,540 --> 00:04:58,862
Anvendelse af den ene transformation efter den anden indfanges algebraisk med 

77
00:04:58,862 --> 00:05:03,128
matrixmultiplikation, så kerneegenskaben i denne transformation A invers er, 

78
00:05:03,128 --> 00:05:07,340
at A invers gange A er lig med den matrix, der svarer til at gøre ingenting.

79
00:05:08,200 --> 00:05:11,320
Den transformation, der ikke gør noget, kaldes identitetstransformationen.

80
00:05:11,780 --> 00:05:15,066
Den efterlader i-hat og j-hat hver, hvor de er, 

81
00:05:15,066 --> 00:05:18,080
ubevægelige, så dens kolonner er 1,0 og 0,1.

82
00:05:19,980 --> 00:05:24,231
Når du har fundet denne inverse, som du i praksis ville gøre med en computer, 

83
00:05:24,231 --> 00:05:27,720
kan du løse din ligning ved at gange denne inverse matrix med v.

84
00:05:29,960 --> 00:05:33,131
Og igen, hvad dette betyder geometrisk er, at 

85
00:05:33,131 --> 00:05:36,440
du spiller transformationen omvendt og følger v.

86
00:05:40,200 --> 00:05:44,163
Dette ikke-nul determinant tilfælde, som for et tilfældigt valg af matrix er 

87
00:05:44,163 --> 00:05:46,686
langt den mest sandsynlige, svarer til ideen om, 

88
00:05:46,686 --> 00:05:50,701
at hvis du har to ubekendte og to ligninger, er det næsten sikkert tilfældet, 

89
00:05:50,701 --> 00:05:52,400
at der er en enkelt unik løsning.

90
00:05:53,680 --> 00:05:56,259
Denne idé giver også mening i højere dimensioner, 

91
00:05:56,259 --> 00:05:59,200
når antallet af ligninger er lig med antallet af ukendte.

92
00:05:59,380 --> 00:06:04,604
Igen kan ligningssystemet oversættes til den geometriske fortolkning, 

93
00:06:04,604 --> 00:06:09,306
hvor du har en eller anden transformation A og noget vektor v, 

94
00:06:09,306 --> 00:06:12,740
og du leder efter vektoren x, der lander på v.

95
00:06:15,740 --> 00:06:20,111
Så længe transformationen A ikke klemmer hele rummet ind i en lavere dimension, 

96
00:06:20,111 --> 00:06:22,843
hvilket betyder, at dens determinant er ikke-nul, 

97
00:06:22,843 --> 00:06:26,450
vil der være en invers transformation A invers, med den egenskab, 

98
00:06:26,450 --> 00:06:31,040
at hvis du først gør A, så gør du A invers , det er det samme som at gøre ingenting.

99
00:06:33,540 --> 00:06:36,367
Og for at løse din ligning skal du bare gange 

100
00:06:36,367 --> 00:06:39,440
den omvendte transformationsmatrix med vektoren v.

101
00:06:43,500 --> 00:06:47,331
Men når determinanten er nul, og transformationen forbundet med 

102
00:06:47,331 --> 00:06:52,060
ligningssystemet klemmer rummet ind i en mindre dimension, er der ingen invers.

103
00:06:52,480 --> 00:06:55,460
Du kan ikke frigøre en linje for at gøre den til et fly.

104
00:06:55,980 --> 00:06:58,060
Det er i hvert fald ikke noget, en funktion kan.

105
00:06:58,360 --> 00:07:02,980
Det ville kræve at transformere hver enkelt vektor til en hel linje fuld af vektorer.

106
00:07:03,740 --> 00:07:06,740
Men funktioner kan kun tage et enkelt input til et enkelt output.

107
00:07:08,400 --> 00:07:12,554
Tilsvarende, for tre ligninger og tre ubekendte, vil der ikke være nogen omvendt, 

108
00:07:12,554 --> 00:07:15,897
hvis den tilsvarende transformation klemmer 3D-rum ind på planet, 

109
00:07:15,897 --> 00:07:19,140
eller endda hvis den klemmer det ind på en linje eller et punkt.

110
00:07:19,920 --> 00:07:22,565
De svarer alle til en determinant på nul, da enhver 

111
00:07:22,565 --> 00:07:25,160
region er presset sammen til noget med nul volumen.

112
00:07:26,700 --> 00:07:30,640
Det er stadig muligt, at der findes en løsning, selvom der ikke er nogen omvendt.

113
00:07:30,720 --> 00:07:35,301
Det er bare, at når din transformation klemmer plads på f.eks. en linje, 

114
00:07:35,301 --> 00:07:39,380
skal du være så heldig, at vektoren v lever et sted på den linje.

115
00:07:43,300 --> 00:07:45,992
Du vil måske bemærke, at nogle af disse nuldeterminante 

116
00:07:45,992 --> 00:07:48,300
tilfælde føles meget mere restriktive end andre.

117
00:07:48,840 --> 00:07:51,335
Givet en 3x3 matrix, for eksempel, ser det ud til, 

118
00:07:51,335 --> 00:07:55,640
at det er meget sværere for en løsning at eksistere, når den klemmer plads på en linje, 

119
00:07:55,640 --> 00:07:58,282
sammenlignet med når den klemmer ting ind på et plan, 

120
00:07:58,282 --> 00:08:00,240
selvom begge disse er nul-determinanter.

121
00:08:02,600 --> 00:08:06,100
Vi har et sprog, der er lidt mere specifikt end blot at sige nul determinant.

122
00:08:06,520 --> 00:08:09,807
Når outputtet af en transformation er en linje, hvilket betyder, 

123
00:08:09,807 --> 00:08:13,500
at den er endimensionel, siger vi, at transformationen har en rang på én.

124
00:08:15,140 --> 00:08:17,959
Hvis alle vektorerne lander på et todimensionalt plan, 

125
00:08:17,959 --> 00:08:20,420
siger vi, at transformationen har en rang på to.

126
00:08:22,920 --> 00:08:27,480
Så ordet rang betyder antallet af dimensioner i outputtet af en transformation.

127
00:08:28,400 --> 00:08:32,720
For eksempel, i tilfælde af 2x2 matricer, er rang 2 den bedste, den kan være.

128
00:08:33,080 --> 00:08:36,002
Det betyder, at basisvektorerne fortsætter med at spænde over 

129
00:08:36,002 --> 00:08:39,020
de fulde to dimensioner af rummet, og determinanten er ikke nul.

130
00:08:39,419 --> 00:08:42,560
Men for 3x3-matricer betyder rang to, at vi er kollapset, 

131
00:08:42,560 --> 00:08:46,460
men ikke så meget, som de ville have kollapset for en rang én situation.

132
00:08:47,240 --> 00:08:50,132
Hvis en 3D-transformation har en ikke-nul determinant, 

133
00:08:50,132 --> 00:08:53,340
og dens output fylder hele 3D-rummet, har den en rang på tre.

134
00:08:54,520 --> 00:08:58,817
Dette sæt af alle mulige output for din matrix, hvad enten det er en linje, 

135
00:08:58,817 --> 00:09:02,720
et plan, 3D-rum, hvad som helst, kaldes kolonnerummet for din matrix.

136
00:09:04,140 --> 00:09:06,280
Du kan sikkert gætte, hvor det navn kommer fra.

137
00:09:06,560 --> 00:09:10,698
Søjlerne i din matrix fortæller dig, hvor basisvektorerne lander, 

138
00:09:10,698 --> 00:09:15,840
og spændvidden af disse transformerede basisvektorer giver dig alle mulige output.

139
00:09:16,360 --> 00:09:21,140
Med andre ord er kolonnerummet spændvidden af kolonnerne i din matrix.

140
00:09:23,300 --> 00:09:26,091
Så en mere præcis definition af rang ville være, 

141
00:09:26,091 --> 00:09:28,940
at det er antallet af dimensioner i kolonnerummet.

142
00:09:29,940 --> 00:09:32,933
Når denne rang er så høj som den kan være, hvilket betyder at 

143
00:09:32,933 --> 00:09:36,120
den er lig med antallet af kolonner, kalder vi matrixen fuld rang.

144
00:09:38,540 --> 00:09:42,217
Bemærk, at nulvektoren altid vil være inkluderet i kolonnerummet, 

145
00:09:42,217 --> 00:09:45,840
da lineære transformationer skal holde oprindelsen fast på plads.

146
00:09:46,900 --> 00:09:49,806
For en fuld rang transformation er den eneste vektor, 

147
00:09:49,806 --> 00:09:51,960
der lander ved origo, selve nulvektoren.

148
00:09:52,460 --> 00:09:56,211
Men for matricer, der ikke er fuld rang, som svirper til en mindre dimension, 

149
00:09:56,211 --> 00:09:58,760
kan du have en hel masse vektorer, der lander på nul.

150
00:10:01,640 --> 00:10:05,448
Hvis en 2D-transformation klemmer rummet ind på en linje, for eksempel, 

151
00:10:05,448 --> 00:10:08,675
er der en separat linje i en anden retning fuld af vektorer, 

152
00:10:08,675 --> 00:10:10,580
der bliver klemt ind på oprindelsen.

153
00:10:11,780 --> 00:10:14,505
Hvis en 3D-transformation klemmer rummet ind på et fly, 

154
00:10:14,505 --> 00:10:17,620
er der også en hel række af vektorer, der lander på oprindelsen.

155
00:10:20,520 --> 00:10:23,629
Hvis en 3D-transformation klemmer al plads på en linje, 

156
00:10:23,629 --> 00:10:27,460
så er der et helt plan fyldt med vektorer, der lander på oprindelsen.

157
00:10:32,800 --> 00:10:35,980
Dette sæt af vektorer, der lander på oprindelsen, 

158
00:10:35,980 --> 00:10:38,780
kaldes nullrummet eller kernen i din matrix.

159
00:10:39,360 --> 00:10:41,899
Det er rummet for alle vektorer, der bliver nul, 

160
00:10:41,899 --> 00:10:44,180
i den forstand, at de lander på nulvektoren.

161
00:10:45,680 --> 00:10:50,313
Med hensyn til det lineære ligningssystem, når v tilfældigvis er nulvektoren, 

162
00:10:50,313 --> 00:10:53,640
giver nulrummet dig alle mulige løsninger til ligningen.

163
00:10:56,420 --> 00:10:58,833
Så det er et meget højt niveau overblik over, 

164
00:10:58,833 --> 00:11:01,720
hvordan man tænker lineære ligningssystemer geometrisk.

165
00:11:02,300 --> 00:11:06,347
Hvert system har en eller anden form for lineær transformation forbundet med sig, 

166
00:11:06,347 --> 00:11:10,740
og når den transformation har en invers, kan du bruge den inverse til at løse dit system.

167
00:11:12,280 --> 00:11:17,528
Ellers lader ideen om kolonnerum os forstå, hvornår en løsning overhovedet eksisterer, 

168
00:11:17,528 --> 00:11:20,423
og ideen om et nulrum hjælper os til at forstå, 

169
00:11:20,423 --> 00:11:23,440
hvordan sættet af alle mulige løsninger kan se ud.

170
00:11:24,980 --> 00:11:29,380
Igen, der er meget, som jeg ikke har dækket her, især hvordan man beregner disse ting.

171
00:11:29,800 --> 00:11:32,322
Jeg var også nødt til at begrænse mit omfang til eksempler, 

172
00:11:32,322 --> 00:11:34,760
hvor antallet af ligninger er lig med antallet af ukendte.

173
00:11:34,880 --> 00:11:37,771
Men målet her er ikke at forsøge at lære alt, det er, 

174
00:11:37,771 --> 00:11:41,305
at du kommer derfra med en stærk intuition for omvendte matricer, 

175
00:11:41,305 --> 00:11:45,321
kolonnerum og nulrum, og at disse intuitioner gør enhver fremtidig læring, 

176
00:11:45,321 --> 00:11:46,500
du gør, mere frugtbar.

177
00:11:47,660 --> 00:11:49,815
Næste video, efter populær anmodning, vil være 

178
00:11:49,815 --> 00:11:51,880
en kort fodnote om ikke-kvadratiske matricer.

179
00:11:51,880 --> 00:11:56,064
Derefter vil jeg give dig mit bud på dot-produkter, og noget ret cool, 

180
00:11:56,064 --> 00:11:59,660
der sker, når du ser dem i lyset af lineære transformationer.

