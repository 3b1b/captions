[
 {
  "input": "As you can probably tell by now, the bulk of this series is on understanding matrix and vector operations through that more visual lens of linear transformations. ",
  "translatedText": "همانطور که احتمالاً تا به حال می توانید بگویید، بخش عمده ای از این سری در درک عملیات ماتریس و برداری از طریق آن لنز بصری تر تبدیلات خطی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.24,
  "end": 19.34
 },
 {
  "input": "This video is no exception, describing the concepts of inverse matrices, column space, rank, and null space through that lens. ",
  "translatedText": "این ویدیو از این قاعده مستثنی نیست و مفاهیم ماتریس های معکوس، فضای ستون، رتبه و فضای تهی را از طریق آن لنز توصیف می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.98,
  "end": 27.52
 },
 {
  "input": "A forewarning though, I'm not going to talk about the methods for actually computing these things, and some would argue that that's pretty important. ",
  "translatedText": "با این حال، یک هشدار قبلی، من در مورد روش های محاسبه واقعی این چیزها صحبت نمی کنم، و برخی استدلال می کنند که این بسیار مهم است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.52,
  "end": 34.24
 },
 {
  "input": "There are a lot of very good resources for learning those methods outside this series, keywords Gaussian elimination and row echelon form. ",
  "translatedText": "منابع بسیار خوبی برای یادگیری آن روش ها در خارج از این مجموعه وجود دارد، کلمات کلیدی حذف گاوسی و فرم ردیف ردیف. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.84,
  "end": 42.0
 },
 {
  "input": "I think most of the value that I actually have to add here is on the intuition half. ",
  "translatedText": "من فکر می کنم بیشتر ارزشی که در اینجا باید اضافه کنم به نیمه شهود است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.54,
  "end": 46.34
 },
 {
  "input": "Plus, in practice, we usually get software to compute this stuff for us anyway. ",
  "translatedText": "به علاوه، در عمل، ما معمولا نرم افزاری دریافت می کنیم که به هر حال این موارد را برای ما محاسبه کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.9,
  "end": 50.48
 },
 {
  "input": "First, a few words on the usefulness of linear algebra. ",
  "translatedText": "ابتدا چند کلمه در مورد سودمندی جبر خطی. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.5,
  "end": 53.92
 },
 {
  "input": "By now you already have a hint for how it's used in describing the manipulation of space, which is useful for things like computer graphics and robotics, but one of the main reasons that linear algebra is more broadly applicable and required for just about any technical discipline is that it lets us solve certain systems of equations. ",
  "translatedText": "در حال حاضر شما قبلاً اشاره ای به نحوه استفاده از آن در توصیف دستکاری فضا دارید، که برای مواردی مانند گرافیک کامپیوتری و روباتیک مفید است، اما یکی از دلایل اصلی این است که جبر خطی به طور گسترده تری کاربرد دارد و تقریباً برای هر رشته فنی مورد نیاز است. این است که به ما اجازه می دهد سیستم های معینی از معادلات را حل کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 54.3,
  "end": 70.46
 },
 {
  "input": "When I say system of equations, I mean you have a list of variables, things you don't know, and a list of equations relating them. ",
  "translatedText": "وقتی می‌گویم سیستم معادلات، منظورم این است که شما فهرستی از متغیرها، چیزهایی که نمی‌دانید و فهرستی از معادلات مربوط به آنها را دارید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.38,
  "end": 77.76
 },
 {
  "input": "In a lot of situations, those equations can get very complicated, but if you're lucky, they might take on a certain special form. ",
  "translatedText": "در بسیاری از موقعیت ها، این معادلات می توانند بسیار پیچیده شوند، اما اگر خوش شانس باشید، ممکن است شکل خاصی به خود بگیرند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.34,
  "end": 85.3
 },
 {
  "input": "Within each equation, the only thing happening to each variable is that it's scaled by some constant, and the only thing happening to each of those scaled variables is that they're added to each other. ",
  "translatedText": "در هر معادله، تنها چیزی که برای هر متغیر اتفاق می‌افتد این است که با مقداری ثابت مقیاس می‌شود، و تنها چیزی که برای هر یک از آن متغیرهای مقیاس‌شده اتفاق می‌افتد این است که آنها به یکدیگر اضافه می‌شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 86.44,
  "end": 96.88
 },
 {
  "input": "So no exponents or fancy functions or multiplying two variables together, things like that. ",
  "translatedText": "بنابراین هیچ نما یا توابع فانتزی یا ضرب دو متغیر با هم، چیزهایی مانند آن. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 97.54,
  "end": 102.28
 },
 {
  "input": "The typical way to organize this sort of special system of equations is to throw all the variables on the left and put any lingering constants on the right. ",
  "translatedText": "روش معمولی برای سازماندهی این نوع سیستم ویژه معادلات این است که همه متغیرها را در سمت چپ قرار دهید و هر ثابتی را در سمت راست قرار دهید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.42,
  "end": 112.14
 },
 {
  "input": "It's also nice to vertically line up the common variables, and to do that you might need to throw in some zero coefficients whenever the variable doesn't show up in one of the equations. ",
  "translatedText": "همچنین خوب است که متغیرهای رایج را به صورت عمودی ردیف کنید، و برای انجام این کار ممکن است لازم باشد هر زمان که متغیر در یکی از معادلات نشان داده نشد، ضرایب صفر را وارد کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.6,
  "end": 121.94
 },
 {
  "input": "This is called a linear system of equations. ",
  "translatedText": "به این سیستم معادلات خطی می گویند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.54,
  "end": 127.24
 },
 {
  "input": "You might notice that this looks a lot like matrix-vector multiplication. ",
  "translatedText": "ممکن است متوجه شوید که این بسیار شبیه ضرب ماتریس-بردار است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 128.1,
  "end": 131.18
 },
 {
  "input": "In fact, you can package all of the equations together into a single vector equation where you have the matrix containing all of the constant coefficients and a vector containing all of the variables, and their matrix-vector product equals some different constant vector. ",
  "translatedText": "در واقع، شما می توانید همه معادلات را با هم در یک معادله برداری واحد بسته بندی کنید که در آن ماتریس حاوی همه ضرایب ثابت و بردار حاوی همه متغیرها است و حاصلضرب ماتریس-بردار آنها برابر با بردار ثابت متفاوت است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.82,
  "end": 146.78
 },
 {
  "input": "Let's name that constant matrix A, denote the vector holding the variables with a bold-faced x, and call the constant vector on the right-hand side v. ",
  "translatedText": "بیایید آن ماتریس ثابت A را نامگذاری کنیم، بردار نگهدارنده متغیرها را با یک x پررنگ نشان دهیم و بردار ثابت سمت راست را v صدا کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 148.64,
  "end": 157.48
 },
 {
  "input": "This is more than just a notational trick to get our system of equations written on one line. ",
  "translatedText": "این چیزی بیش از یک ترفند نمادین برای نوشتن سیستم معادلات ما در یک خط است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.86,
  "end": 162.98
 },
 {
  "input": "It sheds light on a pretty cool geometric interpretation for the problem. ",
  "translatedText": "این یک تفسیر هندسی بسیار جالب برای مسئله را روشن می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.34,
  "end": 166.78
 },
 {
  "input": "The matrix A corresponds with some linear transformation, so solving Ax equals v means we're looking for a vector x which, after applying the transformation, lands on v. ",
  "translatedText": "ماتریس A با مقداری تبدیل خطی مطابقت دارد، بنابراین حل Ax برابر با v به این معنی است که ما به دنبال بردار x هستیم که پس از اعمال تبدیل، بر روی v قرار می گیرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 167.62,
  "end": 177.92
 },
 {
  "input": "Think about what's happening here for a moment. ",
  "translatedText": "یک لحظه به آنچه اینجا اتفاق می افتد فکر کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.94,
  "end": 181.78
 },
 {
  "input": "You can hold in your head this really complicated idea of multiple variables all intermingling with each other just by thinking about squishing and morphing space and trying to figure out which vector lands on another. ",
  "translatedText": "شما می توانید این ایده واقعاً پیچیده از متغیرهای متعدد را در ذهن خود نگه دارید، فقط با فکر کردن در مورد له کردن و تغییر شکل فضا و تلاش برای کشف اینکه کدام بردار روی دیگری قرار می گیرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 182.06,
  "end": 192.6
 },
 {
  "input": "Cool, right? ",
  "translatedText": "باحال، درسته؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.16,
  "end": 193.76
 },
 {
  "input": "To start simple, let's say you have a system with two equations and two unknowns. ",
  "translatedText": "برای شروع ساده، فرض کنید یک سیستم با دو معادله و دو مجهول دارید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 194.6,
  "end": 198.68
 },
 {
  "input": "This means the matrix A is a 2x2 matrix and v and x are each two-dimensional vectors. ",
  "translatedText": "این بدان معناست که ماتریس A یک ماتریس 2x2 است و v و x هر یک بردارهای دو بعدی هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.0,
  "end": 203.96
 },
 {
  "input": "Now, how we think about the solutions to this equation depends on whether the transformation associated with A squishes all of space into a lower dimension, like a line or a point, or if it leaves everything spanning the full two dimensions where it started. ",
  "translatedText": "حال، اینکه چگونه در مورد راه حل های این معادله فکر می کنیم، بستگی به این دارد که آیا تبدیل مرتبط با A، تمام فضا را به یک بعد پایین تر، مانند یک خط یا یک نقطه، فشرده می کند، یا اینکه همه چیز را در دو بعد کامل از جایی که شروع کرده است، رها می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 205.6,
  "end": 218.9
 },
 {
  "input": "In the language of the last video, we subdivide into the cases where A has zero determinant and the case where A has non-zero determinant. ",
  "translatedText": "در زبان آخرین ویدیو، مواردی را که A دارای تعیین کننده صفر است و مواردی که A دارای تعیین کننده غیر صفر هستند، تقسیم می کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 220.32,
  "end": 228.04
 },
 {
  "input": "Let's start with the most likely case, where the determinant is non-zero, meaning space does not get squished into a zero-area region. ",
  "translatedText": "بیایید با محتمل ترین حالت شروع کنیم، که در آن تعیین کننده غیر صفر است، به این معنی که فضا به ناحیه صفر تبدیل نمی شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.3,
  "end": 237.72
 },
 {
  "input": "In this case, there will always be one and only one vector that lands on v, and you can find it by playing the transformation in reverse. ",
  "translatedText": "در این حالت، همیشه یک و تنها یک بردار وجود خواهد داشت که بر روی v قرار می‌گیرد و شما می‌توانید آن را با اجرای معکوس تبدیل پیدا کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.6,
  "end": 246.16
 },
 {
  "input": "Following where v goes as we rewind the tape like this, you'll find the vector x such that A times x equals v. ",
  "translatedText": "وقتی نوار را به این شکل به عقب برمی‌گردانیم، به دنبال جایی که v می‌رود، بردار x را به‌گونه‌ای خواهید یافت که A ضربدر x برابر با v است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.7,
  "end": 253.46
 },
 {
  "input": "When you play the transformation in reverse, it actually corresponds to a separate linear transformation commonly called the inverse of A, denoted A to the negative one. ",
  "translatedText": "هنگامی که تبدیل را به صورت معکوس اجرا می کنید، در واقع با تبدیل خطی جداگانه ای مطابقت دارد که معمولاً معکوس A نامیده می شود و A به منفی نشان داده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.4,
  "end": 264.68
 },
 {
  "input": "For example, if A was a counterclockwise rotation by 90 degrees, then the inverse of A would be a clockwise rotation by 90 degrees. ",
  "translatedText": "به عنوان مثال، اگر A یک چرخش 90 درجه در خلاف جهت عقربه های ساعت باشد، معکوس A یک چرخش در جهت عقربه های ساعت 90 درجه خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 265.36,
  "end": 272.76
 },
 {
  "input": "If A was a rightward shear that pushes j-hat one unit to the right, the inverse of A would be a leftward shear that pushes j-hat one unit to the left. ",
  "translatedText": "اگر A برشی به سمت راست بود که j-hat را یک واحد به سمت راست هل می داد، معکوس A یک برشی به سمت چپ بود که j-hat را یک واحد به سمت چپ هل می داد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.32,
  "end": 282.48
 },
 {
  "input": "In general, A inverse is the unique transformation with the property that if you first apply A, then follow it with the transformation A inverse, you end up back where you started. ",
  "translatedText": "به طور کلی، A inverse تبدیل منحصر به فرد با ویژگی است که اگر ابتدا A را اعمال کنید، سپس آن را با تبدیل A معکوس دنبال کنید، در نهایت به همان جایی که شروع کرده اید باز می گردید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 284.1,
  "end": 293.48
 },
 {
  "input": "Applying one transformation after another is captured algebraically with matrix multiplication. ",
  "translatedText": "اعمال یک تبدیل پس از دیگری به صورت جبری با ضرب ماتریس گرفته می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 294.54,
  "end": 298.94
 },
 {
  "input": "So the core property of this transformation A inverse is that A inverse times A equals the matrix that corresponds to doing nothing. ",
  "translatedText": "بنابراین ویژگی اصلی این تبدیل A معکوس این است که A برعکس A برابر با ماتریسی است که با انجام هیچ کاری مطابقت دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.42,
  "end": 307.34
 },
 {
  "input": "The transformation that does nothing is called the identity transformation. ",
  "translatedText": "تحولی که هیچ کاری انجام نمی دهد، تبدیل هویت نامیده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 308.2,
  "end": 311.32
 },
 {
  "input": "It leaves i-hat and j-hat each where they are, unmoved, so its columns are 1,0 and 0,1. ",
  "translatedText": "i-hat و j-hat هر کدام را بدون حرکت رها می کند، بنابراین ستون های آن 1،0 و 0،1 هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.78,
  "end": 318.08
 },
 {
  "input": "Once you find this inverse, which in practice you'd do with a computer, you can solve your equation by multiplying this inverse matrix by v. ",
  "translatedText": "هنگامی که این معکوس را پیدا کردید، که در عمل با کامپیوتر انجام می‌دادید، می‌توانید معادله خود را با ضرب این ماتریس معکوس در v حل کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.98,
  "end": 327.72
 },
 {
  "input": "And again, what this means geometrically is that you're playing the transformation in reverse and following v. ",
  "translatedText": "و باز هم، معنای هندسی این است که شما تبدیل را به صورت معکوس اجرا می کنید و v را دنبال می کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 329.96,
  "end": 336.44
 },
 {
  "input": "This non-zero determinant case, which for a random choice of matrix is by far the most likely one, corresponds with the idea that if you have two unknowns and two equations, it's almost certainly the case that there's a single unique solution. ",
  "translatedText": "این حالت تعیین‌کننده غیرصفر، که برای انتخاب تصادفی ماتریس محتمل‌ترین مورد است، با این ایده مطابقت دارد که اگر دو مجهول و دو معادله دارید، تقریباً مطمئناً یک راه‌حل منحصر به فرد وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.2,
  "end": 352.4
 },
 {
  "input": "This idea also makes sense in higher dimensions, when the number of equations equals the number of unknowns. ",
  "translatedText": "این ایده همچنین در ابعاد بالاتر، زمانی که تعداد معادلات با تعداد مجهولات برابر باشد، معنا پیدا می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.68,
  "end": 359.2
 },
 {
  "input": "Again, the system of equations can be translated to the geometric interpretation where you have some transformation A and some vector v, and you're looking for the vector x that lands on v. ",
  "translatedText": "باز هم، سیستم معادلات را می توان به تفسیر هندسی ترجمه کرد که در آن شما مقداری تبدیل A و مقداری بردار v دارید، و به دنبال بردار x هستید که روی v قرار می گیرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 359.38,
  "end": 372.74
 },
 {
  "input": "As long as the transformation A doesn't squish all of space into a lower dimension, meaning its determinant is non-zero, there will be an inverse transformation A inverse, with the property that if you first do A, then you do A inverse, it's the same as doing nothing. ",
  "translatedText": "تا زمانی که تبدیل A تمام فضا را به یک بعد پایین تر تبدیل نکند، یعنی تعیین کننده آن غیر صفر است، یک تبدیل معکوس A معکوس خواهد بود، با این ویژگی که اگر ابتدا A را انجام دهید، سپس A را معکوس کنید. ، مانند هیچ کاری نیست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 391.04
 },
 {
  "input": "And to solve your equation, you just have to multiply that reverse transformation matrix by the vector v. ",
  "translatedText": "و برای حل معادله خود، فقط باید آن ماتریس تبدیل معکوس را در بردار v ضرب کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 393.54,
  "end": 399.44
 },
 {
  "input": "But when the determinant is zero, and the transformation associated with the system of equations squishes space into a smaller dimension, there is no inverse. ",
  "translatedText": "اما زمانی که دترمینان صفر است، و تبدیل مرتبط با سیستم معادلات، فضا را به ابعاد کوچک‌تری تبدیل می‌کند، معکوس وجود ندارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.5,
  "end": 412.06
 },
 {
  "input": "You cannot unsquish a line to turn it into a plane. ",
  "translatedText": "شما نمی توانید یک خط را از بین ببرید تا آن را به یک هواپیما تبدیل کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.48,
  "end": 415.46
 },
 {
  "input": "At least that's not something that a function can do. ",
  "translatedText": "حداقل این چیزی نیست که یک تابع بتواند انجام دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 415.98,
  "end": 418.06
 },
 {
  "input": "That would require transforming each individual vector into a whole line full of vectors. ",
  "translatedText": "این امر مستلزم تبدیل هر بردار جداگانه به یک خط کامل پر از بردار است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.36,
  "end": 422.98
 },
 {
  "input": "But functions can only take a single input to a single output. ",
  "translatedText": "اما توابع تنها می توانند یک ورودی را به یک خروجی واحد ببرند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.74,
  "end": 426.74
 },
 {
  "input": "Similarly, for three equations and three unknowns, there will be no inverse if the corresponding transformation squishes 3D space onto the plane, or even if it squishes it onto a line or a point. ",
  "translatedText": "به طور مشابه، برای سه معادله و سه مجهول، اگر تبدیل متناظر فضای سه بعدی را بر روی صفحه بکوبد، یا حتی اگر آن را روی یک خط یا یک نقطه فشار دهد، معکوس وجود نخواهد داشت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.4,
  "end": 439.14
 },
 {
  "input": "Those all correspond to a determinant of zero, since any region is squished into something with zero volume. ",
  "translatedText": "همه آنها با یک تعیین کننده صفر مطابقت دارند، زیرا هر منطقه به چیزی با حجم صفر فشرده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 439.92,
  "end": 445.16
 },
 {
  "input": "It's still possible that a solution exists even when there is no inverse. ",
  "translatedText": "حتی زمانی که معکوس وجود ندارد، هنوز ممکن است راه حلی وجود داشته باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.7,
  "end": 450.64
 },
 {
  "input": "It's just that when your transformation squishes space onto, say, a line, you have to be lucky enough that the vector v lives somewhere on that line. ",
  "translatedText": "فقط این است که وقتی تبدیل شما فضا را مثلاً روی یک خط می زند، باید به اندازه کافی خوش شانس باشید که بردار v در جایی روی آن خط زندگی کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.72,
  "end": 459.38
 },
 {
  "input": "You might notice that some of these zero determinant cases feel a lot more restrictive than others. ",
  "translatedText": "ممکن است متوجه شوید که برخی از این موارد تعیین کننده صفر بسیار محدودتر از بقیه هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 463.3,
  "end": 468.3
 },
 {
  "input": "Given a 3x3 matrix, for example, it seems a lot harder for a solution to exist when it squishes space onto a line compared to when it squishes things onto a plane, even though both of those are zero determinant. ",
  "translatedText": "برای مثال، با توجه به یک ماتریس 3x3، به نظر می‌رسد که وجود یک راه‌حل زمانی که فضا را روی یک خط فشار می‌دهد، در مقایسه با زمانی که چیزها را روی یک صفحه فشار می‌دهد، بسیار سخت‌تر به نظر می‌رسد، حتی اگر هر دوی آن‌ها تعیین‌کننده صفر هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.84,
  "end": 480.24
 },
 {
  "input": "We have some language that's a bit more specific than just saying zero determinant. ",
  "translatedText": "ما زبانی داریم که کمی خاص تر از گفتن فقط تعیین کننده صفر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 482.6,
  "end": 486.1
 },
 {
  "input": "When the output of a transformation is a line, meaning it's one-dimensional, we say the transformation has a rank of one. ",
  "translatedText": "وقتی خروجی یک تبدیل یک خط است، یعنی یک بعدی است، می گوییم تبدیل دارای رتبه یک است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.52,
  "end": 493.5
 },
 {
  "input": "If all the vectors land on some two-dimensional plane, we say the transformation has a rank of two. ",
  "translatedText": "اگر همه بردارها روی یک صفحه دوبعدی فرود آیند، می گوییم تبدیل دارای رتبه دو است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.14,
  "end": 500.42
 },
 {
  "input": "So the word rank means the number of dimensions in the output of a transformation. ",
  "translatedText": "بنابراین کلمه رتبه به معنای تعداد ابعاد در خروجی یک تبدیل است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.92,
  "end": 507.48
 },
 {
  "input": "For instance, in the case of 2x2 matrices, rank 2 is the best that it can be. ",
  "translatedText": "به عنوان مثال، در مورد ماتریس های 2x2، رتبه 2 بهترین است که می تواند باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.4,
  "end": 512.72
 },
 {
  "input": "It means the basis vectors continue to span the full two dimensions of space and the determinant is non-zero. ",
  "translatedText": "این بدان معناست که بردارهای پایه به دو بعد کامل فضا ادامه می دهند و تعیین کننده غیر صفر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 513.08,
  "end": 519.02
 },
 {
  "input": "But for 3x3 matrices, rank 2 means that we've collapsed, but not as much as they would have collapsed for a rank 1 situation. ",
  "translatedText": "اما برای ماتریس‌های 3x3، رتبه 2 به این معنی است که ما سقوط کرده‌ایم، اما نه به اندازه‌ای که برای وضعیت رتبه 1 فرو می‌رفتند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 519.42,
  "end": 526.46
 },
 {
  "input": "If a 3D transformation has a non-zero determinant and its output fills all of 3D space, it has a rank of 3. ",
  "translatedText": "اگر یک تبدیل سه بعدی دارای یک تعیین کننده غیر صفر باشد و خروجی آن تمام فضای سه بعدی را پر کند، دارای رتبه 3 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 527.24,
  "end": 533.34
 },
 {
  "input": "This set of all possible outputs for your matrix, whether it's a line, a plane, 3D space, whatever, is called the column space of your matrix. ",
  "translatedText": "این مجموعه از تمام خروجی های ممکن برای ماتریس شما، خواه یک خط، یک صفحه، فضای سه بعدی و هر چه باشد، فضای ستونی ماتریس شما نامیده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 534.52,
  "end": 542.72
 },
 {
  "input": "You can probably guess where that name comes from. ",
  "translatedText": "احتمالاً می توانید حدس بزنید که این نام از کجا آمده است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.14,
  "end": 546.28
 },
 {
  "input": "The columns of your matrix tell you where the basis vectors land, and the span of those transformed basis vectors gives you all possible outputs. ",
  "translatedText": "ستون‌های ماتریس به شما می‌گویند که بردارهای پایه کجا قرار می‌گیرند، و گستره آن بردارهای مبنا تبدیل شده همه خروجی‌های ممکن را به شما می‌دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.56,
  "end": 555.84
 },
 {
  "input": "In other words, the column space is the span of the columns of your matrix. ",
  "translatedText": "به عبارت دیگر، فضای ستون، دهانه ستون های ماتریس شما است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.36,
  "end": 561.14
 },
 {
  "input": "So a more precise definition of rank would be that it's the number of dimensions in the column space. ",
  "translatedText": "بنابراین یک تعریف دقیق تر از رتبه این است که تعداد ابعاد در فضای ستون است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 568.94
 },
 {
  "input": "When this rank is as high as it can be, meaning it equals the number of columns, we call the matrix full rank. ",
  "translatedText": "زمانی که این رتبه تا جایی که می تواند بالا باشد، یعنی با تعداد ستون ها برابر باشد، ماتریس را رتبه کامل می نامیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.94,
  "end": 576.12
 },
 {
  "input": "Notice, the zero vector will always be included in the column space, since linear transformations must keep the origin fixed in place. ",
  "translatedText": "توجه داشته باشید، بردار صفر همیشه در فضای ستون گنجانده می شود، زیرا تبدیل های خطی باید مبدا را در جای خود ثابت نگه دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.54,
  "end": 585.84
 },
 {
  "input": "For a full rank transformation, the only vector that lands at the origin is the zero vector itself. ",
  "translatedText": "برای تبدیل رتبه کامل، تنها بردار که در مبدا قرار می گیرد خود بردار صفر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.9,
  "end": 591.96
 },
 {
  "input": "But for matrices that aren't full rank, which squish to a smaller dimension, you can have a whole bunch of vectors that land on zero. ",
  "translatedText": "اما برای ماتریس‌هایی که رتبه کاملی ندارند و به ابعاد کوچک‌تری می‌رسند، می‌توانید یک دسته کامل از بردارها داشته باشید که روی صفر قرار می‌گیرند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.46,
  "end": 598.76
 },
 {
  "input": "If a 2D transformation squishes space onto a line, for example, there is a separate line in a different direction full of vectors that get squished onto the origin. ",
  "translatedText": "برای مثال، اگر یک تبدیل دوبعدی فضا را روی یک خط فشار دهد، یک خط جداگانه در جهتی متفاوت پر از بردارهایی وجود دارد که روی مبدأ له می‌شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 601.64,
  "end": 610.58
 },
 {
  "input": "If a 3D transformation squishes space onto a plane, there's also a full line of vectors that land on the origin. ",
  "translatedText": "اگر یک تبدیل سه‌بعدی فضا را روی یک هواپیما بکوبد، یک خط کامل از بردارها نیز وجود دارد که روی مبدا فرود می‌آیند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 611.78,
  "end": 617.62
 },
 {
  "input": "If a 3D transformation squishes all of space onto a line, then there's a whole plane full of vectors that land on the origin. ",
  "translatedText": "اگر یک تبدیل سه‌بعدی تمام فضا را روی یک خط بکوبد، یک صفحه کامل پر از بردارها وجود دارد که روی مبدا فرود می‌آیند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 620.52,
  "end": 627.46
 },
 {
  "input": "This set of vectors that lands on the origin is called the null space, or the kernel of your matrix. ",
  "translatedText": "این مجموعه از بردارها که در مبدا قرار می گیرند، فضای خالی یا هسته ماتریس شما نامیده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.8,
  "end": 638.78
 },
 {
  "input": "It's the space of all vectors that become null, in the sense that they land on the zero vector. ",
  "translatedText": "فضای همه بردارها تهی می شود، به این معنا که بر روی بردار صفر قرار می گیرند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.36,
  "end": 644.18
 },
 {
  "input": "In terms of the linear system of equations, when v happens to be the zero vector, the null space gives you all of the possible solutions to the equation. ",
  "translatedText": "از نظر سیستم خطی معادلات، زمانی که v بردار صفر باشد، فضای تهی همه راه حل های ممکن برای معادله را به شما می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 645.68,
  "end": 653.64
 },
 {
  "input": "So that's a very high-level overview of how to think about linear systems of equations geometrically. ",
  "translatedText": "بنابراین این یک نمای کلی سطح بالایی از نحوه تفکر هندسی در مورد سیستم های خطی معادلات است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.42,
  "end": 661.72
 },
 {
  "input": "Each system has some kind of linear transformation associated with it, and when that transformation has an inverse, you can use that inverse to solve your system. ",
  "translatedText": "هر سیستم دارای نوعی تبدیل خطی مرتبط با خود است و زمانی که آن تبدیل دارای معکوس باشد، می توانید از آن معکوس برای حل سیستم خود استفاده کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 662.3,
  "end": 670.74
 },
 {
  "input": "Otherwise, the idea of column space lets us understand when a solution even exists, and the idea of a null space helps us to understand what the set of all possible solutions can look like. ",
  "translatedText": "در غیر این صورت، ایده فضای ستون به ما امکان می دهد بفهمیم که چه زمانی یک راه حل وجود دارد، و ایده فضای خالی به ما کمک می کند تا بفهمیم مجموعه تمام راه حل های ممکن چگونه می تواند باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 672.28,
  "end": 683.44
 },
 {
  "input": "Again, there's a lot that I haven't covered here, most notably how to compute these things. ",
  "translatedText": "باز هم، بسیاری از موارد وجود دارد که من در اینجا به آنها پرداخته‌ام، از جمله مهم‌ترین آنها نحوه محاسبه این موارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 684.98,
  "end": 689.38
 },
 {
  "input": "I also had to limit my scope to examples where the number of equations equals the number of unknowns. ",
  "translatedText": "همچنین مجبور شدم دامنه خود را به مثال هایی محدود کنم که تعداد معادلات با تعداد مجهولات برابر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.8,
  "end": 694.76
 },
 {
  "input": "But the goal here is not to try to teach everything, it's that you come away with a strong intuition for inverse matrices, column space, and null space, and that those intuitions make any future learning that you do more fruitful. ",
  "translatedText": "اما هدف در اینجا تلاش برای آموزش همه چیز نیست، بلکه این است که شما با یک شهود قوی برای ماتریس‌های معکوس، فضای ستون‌ها و فضای تهی کنار می‌آیید، و این شهودات باعث می‌شود هر یادگیری در آینده که انجام می‌دهید مثمر ثمرتر باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 694.88,
  "end": 706.5
 },
 {
  "input": "Next video, by popular request, will be a brief footnote about non-square matrices. ",
  "translatedText": "ویدیوی بعدی، بنا به درخواست مردمی، پاورقی مختصری در مورد ماتریس های غیر مربعی خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 707.66,
  "end": 711.88
 },
 {
  "input": "Then after that, I'm going to give you my take on dot products, and something pretty cool that happens when you view them under the light of linear transformations. ",
  "translatedText": "سپس بعد از آن، می‌خواهم دیدگاه خود را در مورد محصولات نقطه‌ای به شما ارائه دهم، و چیزی بسیار جالب که وقتی آنها را زیر نور تبدیل‌های خطی مشاهده می‌کنید، اتفاق می‌افتد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 711.88,
  "end": 718.92
 },
 {
  "input": "See you then! ",
  "translatedText": "بعدا می بینمت! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 719.48,
  "end": 719.66
 }
]