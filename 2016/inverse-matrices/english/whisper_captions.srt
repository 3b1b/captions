1
00:00:10,240 --> 00:00:15,120
As you can probably tell by now, the bulk of this series is on understanding matrix

2
00:00:15,120 --> 00:00:19,340
and vector operations through that more visual lens of linear transformations.

3
00:00:19,980 --> 00:00:24,420
This video is no exception, describing the concepts of inverse matrices, column space,

4
00:00:24,840 --> 00:00:27,520
rank, and null space through that lens.

5
00:00:27,520 --> 00:00:32,080
A forewarning though, I'm not going to talk about the methods for actually computing these

6
00:00:32,080 --> 00:00:34,240
things, and some would argue that that's pretty important.

7
00:00:34,840 --> 00:00:38,680
There are a lot of very good resources for learning those methods outside this series,

8
00:00:39,080 --> 00:00:42,000
keywords Gaussian elimination and row echelon form.

9
00:00:42,540 --> 00:00:46,340
I think most of the value that I actually have to add here is on the intuition half.

10
00:00:46,900 --> 00:00:50,480
Plus, in practice, we usually get software to compute this stuff for us anyway.

11
00:00:51,500 --> 00:00:53,920
First, a few words on the usefulness of linear algebra.

12
00:00:54,300 --> 00:00:58,300
By now, you already have a hint for how it's used in describing the manipulation of space,

13
00:00:58,560 --> 00:01:01,040
which is useful for things like computer graphics and robotics.

14
00:01:01,500 --> 00:01:05,900
But one of the main reasons that linear algebra is more broadly applicable and required for

15
00:01:05,900 --> 00:01:10,460
just about any technical discipline is that it lets us solve certain systems of equations.

16
00:01:11,380 --> 00:01:15,440
When I say system of equations, I mean you have a list of variables, things you don't

17
00:01:15,440 --> 00:01:17,760
know, and a list of equations relating them.

18
00:01:18,340 --> 00:01:21,600
In a lot of situations, those equations can get very complicated.

19
00:01:22,120 --> 00:01:25,300
But, if you're lucky, they might take on a certain special form.

20
00:01:26,440 --> 00:01:31,420
Within each equation, the only thing happening to each variable is that it's scaled by some

21
00:01:31,420 --> 00:01:35,820
constant, and the only thing happening to each of those scaled variables is that they're

22
00:01:35,820 --> 00:01:36,880
added to each other.

23
00:01:37,540 --> 00:01:42,280
So no exponents or fancy functions or multiplying two variables together, things like that.

24
00:01:43,420 --> 00:01:47,820
The typical way to organize this sort of special system of equations is to throw all the variables

25
00:01:47,820 --> 00:01:52,140
on the left and put any lingering constants on the right.

26
00:01:53,600 --> 00:01:57,700
It's also nice to vertically line up the common variables, and to do that, you might need

27
00:01:57,700 --> 00:02:01,460
to throw in some zero coefficients whenever the variable doesn't show up in one of the

28
00:02:01,460 --> 00:02:01,940
equations.

29
00:02:04,540 --> 00:02:07,240
This is called a linear system of equations.

30
00:02:08,100 --> 00:02:11,180
You might notice that this looks a lot like matrix-vector multiplication.

31
00:02:11,820 --> 00:02:17,260
In fact, you can package all of the equations together into a single vector equation where

32
00:02:17,260 --> 00:02:21,440
you have the matrix containing all of the constant coefficients and a vector containing

33
00:02:21,440 --> 00:02:26,780
all of the variables, and their matrix-vector product equals some different constant vector.

34
00:02:28,640 --> 00:02:33,820
Let's name that constant matrix A, denote the vector holding the variables with a bold-faced

35
00:02:33,820 --> 00:02:37,480
X, and call the constant vector on the right-hand side V.

36
00:02:38,860 --> 00:02:42,440
This is more than just a notational trick to get our system of equations written on

37
00:02:42,440 --> 00:02:42,980
one line.

38
00:02:43,340 --> 00:02:46,780
It sheds light on a pretty cool geometric interpretation for the problem.

39
00:02:47,620 --> 00:02:52,960
The matrix A corresponds with some linear transformation, so solving Ax equals V means

40
00:02:52,960 --> 00:02:57,920
we're looking for a vector X, which, after applying the transformation, lands on V.

41
00:02:59,940 --> 00:03:01,780
Think about what's happening here for a moment.

42
00:03:02,060 --> 00:03:07,120
You can hold in your head this really complicated idea of multiple variables all intermingling

43
00:03:07,120 --> 00:03:11,340
with each other just by thinking about squishing and morphing space and trying to figure out

44
00:03:11,340 --> 00:03:12,600
which vector lands on another.

45
00:03:13,160 --> 00:03:13,760
Cool, right?

46
00:03:14,600 --> 00:03:18,680
To start simple, let's say you have a system with two equations and two unknowns.

47
00:03:19,000 --> 00:03:23,960
This means the matrix A is a 2x2 matrix, and V and X are each two-dimensional vectors.

48
00:03:25,600 --> 00:03:30,200
Now, how we think about the solutions to this equation depends on whether the transformation

49
00:03:30,200 --> 00:03:35,140
associated with A squishes all of space into a lower dimension, like a line or a point,

50
00:03:35,660 --> 00:03:38,900
or if it leaves everything spanning the full two dimensions where it started.

51
00:03:40,320 --> 00:03:45,260
In the language of the last video, we subdivide into the cases where A has zero determinant

52
00:03:45,260 --> 00:03:48,040
and the case where A has non-zero determinant.

53
00:03:51,300 --> 00:03:55,500
Let's start with the most likely case, where the determinant is non-zero, meaning space

54
00:03:55,500 --> 00:03:57,720
does not get squished into a zero-area region.

55
00:03:58,600 --> 00:04:03,840
In this case, there will always be one and only one vector that lands on V, and you can

56
00:04:03,840 --> 00:04:06,160
find it by playing the transformation in reverse.

57
00:04:06,700 --> 00:04:12,000
Following where V goes as we rewind the tape like this, you'll find the vector x such that

58
00:04:12,000 --> 00:04:13,460
A times x equals V.

59
00:04:15,400 --> 00:04:19,840
When you play the transformation in reverse, it actually corresponds to a separate linear

60
00:04:19,840 --> 00:04:24,680
transformation, commonly called the inverse of A, denoted A to the negative one.

61
00:04:25,360 --> 00:04:30,480
For example, if A was a counterclockwise rotation by 90 degrees, then the inverse of A would

62
00:04:30,480 --> 00:04:32,760
be a clockwise rotation by 90 degrees.

63
00:04:34,320 --> 00:04:39,240
If A was a rightward shear that pushes j-hat one unit to the right, the inverse of A would

64
00:04:39,240 --> 00:04:42,480
be a leftward shear that pushes j-hat one unit to the left.

65
00:04:44,100 --> 00:04:48,940
In general, A inverse is the unique transformation with the property that if you first apply

66
00:04:48,940 --> 00:04:53,480
A, then follow it with the transformation A inverse, you end up back where you started.

67
00:04:54,540 --> 00:04:58,940
Applying one transformation after another is captured algebraically with matrix multiplication,

68
00:04:59,420 --> 00:05:05,480
so the core property of this transformation A inverse is that A inverse times A equals

69
00:05:05,480 --> 00:05:07,340
the matrix that corresponds to doing nothing.

70
00:05:08,200 --> 00:05:11,320
The transformation that does nothing is called the identity transformation.

71
00:05:11,780 --> 00:05:18,080
It leaves i-hat and j-hat each where they are, unmoved, so its columns are 1,0 and 0,1.

72
00:05:19,980 --> 00:05:24,320
Once you find this inverse, which in practice you'd do with a computer, you can solve your

73
00:05:24,320 --> 00:05:27,720
equation by multiplying this inverse matrix by v.

74
00:05:29,960 --> 00:05:34,560
And again, what this means geometrically is that you're playing the transformation in

75
00:05:34,560 --> 00:05:36,440
reverse and following v.

76
00:05:40,200 --> 00:05:44,680
This non-zero determinant case, which for a random choice of matrix is by far the most

77
00:05:44,680 --> 00:05:49,220
likely one, corresponds with the idea that if you have two unknowns and two equations,

78
00:05:49,440 --> 00:05:52,400
it's almost certainly the case that there's a single unique solution.

79
00:05:53,680 --> 00:05:57,820
This idea also makes sense in higher dimensions, when the number of equations equals the number

80
00:05:57,820 --> 00:05:59,200
of unknowns.

81
00:05:59,380 --> 00:06:04,420
Again, the system of equations can be translated to the geometric interpretation where you

82
00:06:04,420 --> 00:06:11,780
have some transformation A and some vector v, and you're looking for the vector x that

83
00:06:11,780 --> 00:06:12,740
lands on v.

84
00:06:15,740 --> 00:06:20,580
As long as the transformation A doesn't squish all of space into a lower dimension, meaning

85
00:06:20,580 --> 00:06:26,100
its determinant is non-zero, there will be an inverse transformation A inverse, with

86
00:06:26,100 --> 00:06:31,040
the property that if you first do A, then you do A inverse, it's the same as doing nothing.

87
00:06:33,540 --> 00:06:38,100
And to solve your equation, you just have to multiply that reverse transformation matrix

88
00:06:38,100 --> 00:06:39,440
by the vector v.

89
00:06:43,500 --> 00:06:47,620
But when the determinant is zero, and the transformation associated with the system

90
00:06:47,620 --> 00:06:52,060
of equations squishes space into a smaller dimension, there is no inverse.

91
00:06:52,480 --> 00:06:55,460
You cannot unsquish a line to turn it into a plane.

92
00:06:55,980 --> 00:06:58,060
At least that's not something that a function can do.

93
00:06:58,360 --> 00:07:02,980
That would require transforming each individual vector into a whole line full of vectors.

94
00:07:03,740 --> 00:07:06,740
But functions can only take a single input to a single output.

95
00:07:08,400 --> 00:07:13,300
Similarly, for three equations and three unknowns, there will be no inverse if the corresponding

96
00:07:13,300 --> 00:07:18,560
transformation squishes 3D space onto the plane, or even if it squishes it onto a line

97
00:07:18,560 --> 00:07:19,140
or a point.

98
00:07:19,920 --> 00:07:24,180
Those all correspond to a determinant of zero, since any region is squished into something

99
00:07:24,180 --> 00:07:25,160
with zero volume.

100
00:07:26,700 --> 00:07:30,640
It's still possible that a solution exists even when there is no inverse.

101
00:07:30,720 --> 00:07:35,780
It's just that when your transformation squishes space onto, say, a line, you have to be lucky

102
00:07:35,780 --> 00:07:39,380
enough that the vector v lives somewhere on that line.

103
00:07:43,300 --> 00:07:48,300
You might notice that some of these zero determinant cases feel a lot more restrictive than others.

104
00:07:48,840 --> 00:07:53,620
Given a 3x3 matrix, for example, it seems a lot harder for a solution to exist when

105
00:07:53,620 --> 00:07:58,420
it squishes space onto a line compared to when it squishes things onto a plane, even

106
00:07:58,420 --> 00:08:00,240
though both of those are zero determinant.

107
00:08:02,600 --> 00:08:06,100
We have some language that's a bit more specific than just saying zero determinant.

108
00:08:06,520 --> 00:08:11,900
When the output of a transformation is a line, meaning it's one-dimensional, we say the transformation

109
00:08:11,900 --> 00:08:13,500
has a rank of one.

110
00:08:15,140 --> 00:08:19,740
If all the vectors land on some two-dimensional plane, we say the transformation has a rank

111
00:08:19,740 --> 00:08:20,420
of two.

112
00:08:22,920 --> 00:08:27,480
So the word rank means the number of dimensions in the output of a transformation.

113
00:08:28,400 --> 00:08:32,720
For instance, in the case of 2x2 matrices, rank two is the best that it can be.

114
00:08:33,080 --> 00:08:37,920
It means the basis vectors continue to span the full two dimensions of space, and the

115
00:08:37,920 --> 00:08:39,020
determinant is not zero.

116
00:08:39,420 --> 00:08:44,500
But for 3x3 matrices, rank two means that we've collapsed, but not as much as they would

117
00:08:44,500 --> 00:08:46,460
have collapsed for a rank one situation.

118
00:08:47,240 --> 00:08:52,680
If a 3D transformation has a non-zero determinant and its output fills all of 3D space, it has

119
00:08:52,680 --> 00:08:53,340
a rank of three.

120
00:08:54,520 --> 00:08:59,180
This set of all possible outputs for your matrix, whether it's a line, a plane, 3D space,

121
00:08:59,340 --> 00:09:02,720
whatever, is called the column space of your matrix.

122
00:09:04,140 --> 00:09:06,280
You can probably guess where that name comes from.

123
00:09:06,560 --> 00:09:12,220
The columns of your matrix tell you where the basis vectors land, and the span of those

124
00:09:12,220 --> 00:09:15,840
transformed basis vectors gives you all possible outputs.

125
00:09:16,360 --> 00:09:21,140
In other words, the column space is the span of the columns of your matrix.

126
00:09:23,300 --> 00:09:28,220
So a more precise definition of rank would be that it's the number of dimensions in the

127
00:09:28,220 --> 00:09:28,940
column space.

128
00:09:29,940 --> 00:09:34,920
When this rank is as high as it can be, meaning it equals the number of columns, we call the

129
00:09:34,920 --> 00:09:36,120
matrix full rank.

130
00:09:38,540 --> 00:09:43,920
Notice the zero vector will always be included in the column space, since linear transformations

131
00:09:43,920 --> 00:09:45,840
must keep the origin fixed in place.

132
00:09:46,900 --> 00:09:51,460
For a full rank transformation, the only vector that lands at the origin is the zero vector

133
00:09:51,460 --> 00:09:51,960
itself.

134
00:09:52,460 --> 00:09:56,740
But for matrices that aren't full rank, which squish to a smaller dimension, you can have

135
00:09:56,740 --> 00:09:58,760
a whole bunch of vectors that land on zero.

136
00:10:01,640 --> 00:10:06,700
If a 2D transformation squishes space onto a line, for example, there is a separate line

137
00:10:06,700 --> 00:10:10,580
in a different direction full of vectors that get squished onto the origin.

138
00:10:11,780 --> 00:10:16,440
If a 3D transformation squishes space onto a plane, there's also a full line of vectors

139
00:10:16,440 --> 00:10:17,620
that land on the origin.

140
00:10:20,520 --> 00:10:25,620
If a 3D transformation squishes all of space onto a line, then there's a whole plane full

141
00:10:25,620 --> 00:10:27,460
of vectors that land on the origin.

142
00:10:32,800 --> 00:10:38,240
This set of vectors that lands on the origin is called the null space, or the kernel of

143
00:10:38,240 --> 00:10:38,780
your matrix.

144
00:10:39,360 --> 00:10:43,800
It's the space of all vectors that become null, in the sense that they land on the zero

145
00:10:43,800 --> 00:10:44,180
vector.

146
00:10:45,680 --> 00:10:50,380
In terms of the linear system of equations, when v happens to be the zero vector, the

147
00:10:50,380 --> 00:10:53,640
null space gives you all of the possible solutions to the equation.

148
00:10:56,420 --> 00:11:00,920
So that's a very high level overview of how to think about linear systems of equations

149
00:11:00,920 --> 00:11:01,720
geometrically.

150
00:11:02,300 --> 00:11:06,900
Each system has some kind of linear transformation associated with it, and when that transformation

151
00:11:06,900 --> 00:11:10,740
has an inverse, you can use that inverse to solve your system.

152
00:11:12,280 --> 00:11:18,220
Otherwise, the idea of column space lets us understand when a solution even exists, and

153
00:11:18,220 --> 00:11:22,660
the idea of a null space helps us to understand what the set of all possible solutions can

154
00:11:22,660 --> 00:11:23,440
look like.

155
00:11:24,980 --> 00:11:29,380
Again, there's a lot that I haven't covered here, most notably how to compute these things.

156
00:11:29,800 --> 00:11:33,720
I also had to limit my scope to examples where the number of equations equals the number

157
00:11:33,720 --> 00:11:34,760
of unknowns.

158
00:11:34,880 --> 00:11:39,380
But the goal here is not to try to teach everything, it's that you come away with a strong intuition

159
00:11:39,380 --> 00:11:44,800
for inverse matrices, column space, and null space, and that those intuitions make any

160
00:11:44,800 --> 00:11:46,500
future learning that you do more fruitful.

161
00:11:47,660 --> 00:11:51,880
Next video, by popular request, will be a brief footnote about non-square matrices.

162
00:11:51,880 --> 00:11:55,580
Then after that, I'm going to give you my take on dot products, and something pretty

163
00:11:55,580 --> 00:11:58,920
cool that happens when you view them under the light of linear transformations.

164
00:11:59,480 --> 00:11:59,900
See you then!

