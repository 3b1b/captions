1
00:00:03,560 --> 00:00:08,120
Hacer la pregunta correcta es más difícil que responderla.

2
00:00:08,120 --> 00:00:10,960
-George Cantor

3
00:00:11,160 --> 00:00:16,400
Como se habrán dado cuenta, la mayor parte de la serie consiste en entender las operaciones matriciales y vectoriales

4
00:00:16,400 --> 00:00:19,540
a través del lente de las transformaciones lineales.

5
00:00:19,940 --> 00:00:23,740
Este video no es ninguna excepción, describiendo los conceptos de matriz inversa,

6
00:00:23,740 --> 00:00:27,340
espacio de columna, rango y núcleo a través de ese lente.

7
00:00:27,840 --> 00:00:32,500
Pero antes les advierto, no voy a hablar de los métodos para en realidad computar estas cosas,

8
00:00:32,500 --> 00:00:34,720
y algunos argumentarán que eso es bastante importante.

9
00:00:34,720 --> 00:00:38,800
Hay muchos buenos recursos para aprender esos métodos fuera de esta serie.

10
00:00:38,800 --> 00:00:42,140
Busquen: "eliminasión gaussiana" y "forma escalonada reducida por columnas".

11
00:00:42,500 --> 00:00:46,720
Yo creo que el valor que de verdad puedo agregar
 es en la parte de la intuición.

12
00:00:46,720 --> 00:00:51,160
Además, en la práctica, usualmente tenemos software que computan estas cosas por nosotros de todas maneras.

13
00:00:51,380 --> 00:00:54,000
Primero, unas palabras sobre la utilidad del álgebra lineal.

14
00:00:54,300 --> 00:00:58,280
A estas alturas ya tienen una idea de cómo se usa para describir la manipulación del espacio,

15
00:00:58,280 --> 00:01:01,280
lo cual es útil para cosas como gráficos de computadora y robótica,

16
00:01:01,280 --> 00:01:05,020
pero una de las razones por la que el álgebra lineal es ampliamente aplicable

17
00:01:05,020 --> 00:01:07,560
y requerida para casi cualquier disciplina técnica,

18
00:01:07,560 --> 00:01:10,580
es que nos permite resolver ciertos sistemas de ecuaciones.

19
00:01:11,320 --> 00:01:16,060
Cuando digo "sistemas de ecuaciones", quiero decir que tienen una lista de variables, cosas que no conocen,

20
00:01:16,120 --> 00:01:18,200
y una lista de ecuaciones vinculándolas.

21
00:01:18,300 --> 00:01:21,820
En muchas situaciones esas ecuaciones se pueden volver muy complicadas,

22
00:01:22,200 --> 00:01:25,720
pero, si tienes suerte, puede que tomen una forma especial.

23
00:01:26,340 --> 00:01:32,060
Dentro de cada ecuación, lo único que le ocurre a cada variables es que es "escalada" por alguna constante

24
00:01:32,060 --> 00:01:36,700
y lo único que le ocurre a cada una de esas variables escaladas es que se suman mutuamente.

25
00:01:37,540 --> 00:01:42,560
Por lo tanto, no hay exponentes o funciones  complicadas, o dos variables multiplicándose; cosas como esa.

26
00:01:43,460 --> 00:01:46,460
La forma típica de organizar este tipo especial de sistemas de ecuaciones

27
00:01:46,540 --> 00:01:49,260
es poner todas las variables a la izquierda

28
00:01:49,440 --> 00:01:52,480
y dejar las constantes sobrantes a la derecha.

29
00:01:53,700 --> 00:01:56,620
También es bueno alinear las variables comunes verticalmente

30
00:01:56,620 --> 00:02:02,060
y para hacer eso puede que requieran escribir algún coeficiente cero cuando sea que la variable no aparezca en una de las ecuaciones.

31
00:02:04,480 --> 00:02:07,600
A esto se le llama un "sistema lineal de ecuaciones".

32
00:02:07,940 --> 00:02:11,560
Puede que se den cuenta de que esto se parece mucho al producto matriz-vector.

33
00:02:11,800 --> 00:02:16,740
De hecho, pueden escribir todas las ecuaciones juntas en una sola ecuación vectorial,

34
00:02:16,740 --> 00:02:22,400
donde tienen la matriz que contiene todos los coeficientes constantes  y un vector que contiene las variables

35
00:02:22,400 --> 00:02:27,040
y su producto matriz-vector es igual a un vector constante.

36
00:02:28,660 --> 00:02:30,780
Llamemos a la matriz constante "A",

37
00:02:30,780 --> 00:02:34,400
denotemos al vector que contiene las variables con una "x" en negrillas

38
00:02:34,400 --> 00:02:37,900
y llamemos al vector constante de la derecha "v".

39
00:02:38,840 --> 00:02:43,300
Esto es más que un truco notacional para que nuestro sistema de ecuaciones quede escrito en una línea.

40
00:02:43,300 --> 00:02:47,020
Arroja luz a una interesante interpretación geométrica del problema.

41
00:02:47,640 --> 00:02:52,420
La matriz "A" corresponde a alguna transformación lineal, entonces resolver Ax = v

42
00:02:52,480 --> 00:02:58,400
significa que buscamos un vector "x" el cual, después de aplicar la transformación, cae en "v".

43
00:03:00,180 --> 00:03:02,080
Piensen un momento en lo que está ocurriendo.

44
00:03:02,080 --> 00:03:07,660
Pueden tener en su cabeza esta idea complicada de varias variables entremezcladas unas con otras,

45
00:03:07,660 --> 00:03:12,760
pensando en cómo se deforma el espacio e intentando averiguar qué vector cae en otro vector dado.

46
00:03:13,100 --> 00:03:13,980
¿Fino no?

47
00:03:14,700 --> 00:03:18,460
Para empezar por lo fácil, digamos que tienen un sistema de dos ecuaciones y dos incógnitas.

48
00:03:18,880 --> 00:03:21,760
Esto quiere decir que la matriz es una matriz de 2x2,

49
00:03:21,760 --> 00:03:24,080
y que "v" y "x" son dos vectores bidimensionales.

50
00:03:25,420 --> 00:03:28,560
Ahora, cómo pensar en la solución de estas ecuaciones

51
00:03:28,560 --> 00:03:33,780
depende de si la transformación asociada a "A" comprime todo el espacio a una dimensión menor,

52
00:03:33,780 --> 00:03:35,120
como una línea o un punto,

53
00:03:35,840 --> 00:03:39,100
o de si termina generando todo el espacio de dos dimensiones en el cual empezó.

54
00:03:40,520 --> 00:03:45,120
Usando el lenguaje del último video, subdividimos en el caso donde el determinante de "A" es cero

55
00:03:45,560 --> 00:03:48,060
y el caso donde el determinante es distinto de cero.

56
00:03:51,340 --> 00:03:54,620
Empecemos por el caso más probable: que el determinante es distinto de cero,

57
00:03:54,620 --> 00:03:57,740
por lo cual el espacio no es comprimido en una región de área cero.

58
00:03:58,420 --> 00:04:03,220
En este caso, siempre habrá uno y sólo un vector que cae en "v"

59
00:04:03,220 --> 00:04:06,400
y pueden hallarlo reproduciendo la transformación en sentido contrario.

60
00:04:06,900 --> 00:04:09,940
Siguiendo a dónde va "v" mientras rebobinamos la cinta,

61
00:04:10,080 --> 00:04:13,740
hallaran el vector "x" tal que Ax = v.

62
00:04:15,740 --> 00:04:20,500
Aplicar la transformación en reversa, se corresponde con una transformación lineal independiente,

63
00:04:20,500 --> 00:04:22,820
comunmente llamada la inversa de "A"

64
00:04:22,920 --> 00:04:24,920
denotada por "A" a la menos uno.

65
00:04:25,400 --> 00:04:28,720
Por ejemplo,  si "A" fuera una rotación de 90º en contra de las agujas del reloj,

66
00:04:29,200 --> 00:04:33,040
entonces la inversa de "A" sería una rotación de 90º en el sentido de las agujas del reloj.

67
00:04:34,560 --> 00:04:37,900
Si "A" fuera un "shear" que empuja a "j" una unidad hacia la derecha,

68
00:04:38,220 --> 00:04:42,980
la inversa de "A" sería un "shear" que empuja a "j" una unidad hacia la izquierda.

69
00:04:44,200 --> 00:04:49,360
En general,  la inversa de "A" es la única transformación con la propiedad de que si primero aplican "A"

70
00:04:49,360 --> 00:04:51,860
y luego aplican la transformación inversa de "A",

71
00:04:51,860 --> 00:04:53,580
terminan de vuelta en donde empezaron.

72
00:04:54,400 --> 00:04:58,920
Aplicar una transformación después de la otra se define algebráicamente con el producto matricial,

73
00:04:59,640 --> 00:05:04,840
así que la propiedad principal de esta transformación inversa de "A" es que la inversa de "A" por "A"

74
00:05:04,840 --> 00:05:07,420
es igual a la matriz que corresponde a no hacer nada.

75
00:05:08,140 --> 00:05:11,560
La transformación que no hace nada se llama la "transformación identidad".

76
00:05:11,560 --> 00:05:14,620
Deja a "i" y a "j" donde estaban, sin moverlos,

77
00:05:14,920 --> 00:05:17,800
por lo que sus columnas son [1,0] y [0,1].

78
00:05:19,980 --> 00:05:23,320
Una vez hallan esta inversa, la cual en la práctica, lo hacen con una computadora,

79
00:05:23,500 --> 00:05:28,060
pueden resolver su ecuación multiplicando esta matriz inversa por "v".

80
00:05:29,660 --> 00:05:36,640
Y de nuevo, lo que esto quiere decir, geométricamente, es que están reproduciendo la transformación en sentido contrario y siguiendo a "v".

81
00:05:40,180 --> 00:05:45,280
Este caso con determinante distinto de cero, el cual para una matriz aleatoria es de lejos el más probable,

82
00:05:45,280 --> 00:05:49,180
corresponde con la idea de que si tienen dos incógnitas y dos ecuaciones,

83
00:05:49,180 --> 00:05:52,920
es casi seguro que hay una solución única.

84
00:05:53,980 --> 00:05:56,180
Esta idea también tiene sentido en dimensiones mayores,

85
00:05:56,180 --> 00:05:58,700
cuando el número de ecuaciones es igual al número de incógnitas.

86
00:05:59,240 --> 00:06:03,560
De nuevo, el sistema de ecuaciones puede ser trasladado a la interpretación geométrica

87
00:06:03,800 --> 00:06:06,440
donde tienen alguna transformación "A"

88
00:06:08,300 --> 00:06:09,580
y algún vector"v",

89
00:06:09,780 --> 00:06:12,820
y buscan el vector "x" que caiga en "v".

90
00:06:15,900 --> 00:06:20,040
Siempre que la transformación "A" no comprima el espacio a una dimensión menor,

91
00:06:20,040 --> 00:06:22,520
es decir, su determinante es distinto de cero,

92
00:06:22,520 --> 00:06:25,620
habrá una transformación inversa, inversa de "A",

93
00:06:25,620 --> 00:06:28,040
con la propiedad de que si primero aplican "A"

94
00:06:28,220 --> 00:06:29,660
y luego aplican la inversa de "A",

95
00:06:29,840 --> 00:06:31,360
será lo mismo que no hacer nada.

96
00:06:33,620 --> 00:06:40,060
Y para resolver su sistema de ecuaciones, sólo tienen que multiplicar esa matriz de transformación inversa por el vector "v".

97
00:06:43,520 --> 00:06:48,380
Pero cuando el determinante es cero y la transformación asociada con el sistema de ecuaciones

98
00:06:48,380 --> 00:06:52,320
comprime el espacio a una dimensión menor, no hay inversa.

99
00:06:52,320 --> 00:06:55,600
No pueden descomprimir una línea y volverla un plano,

100
00:06:55,600 --> 00:06:58,140
o por lo menos, eso no es algo que pueda hacer una función.

101
00:06:58,140 --> 00:07:00,880
Eso requeriría transformar cada vector individual

102
00:07:01,040 --> 00:07:03,100
en toda una línea llena de vectores.

103
00:07:03,620 --> 00:07:07,240
Pero las funciones sólo pueden llevar una sola entrada a una sola salida.

104
00:07:08,260 --> 00:07:10,840
De manera similar para tres ecuaciones y tres incógnitas,

105
00:07:10,840 --> 00:07:14,280
no habrá inversa si la transformación correspondiente

106
00:07:14,420 --> 00:07:16,660
comprime el espacio 3-D en un plano,

107
00:07:16,660 --> 00:07:19,260
o inclusive si lo comprime a una línea o un punto.

108
00:07:19,760 --> 00:07:22,280
Todos esos corresponden a un determinante cero,

109
00:07:22,280 --> 00:07:25,360
dado que cualquier región es comprimida a algo de volumen cero.

110
00:07:26,860 --> 00:07:30,780
Puede que haya una solución inclusive cuando no haya inversa,

111
00:07:30,780 --> 00:07:34,780
sólo que cuando su transformación comprima el espacio en, digamos, una línea,

112
00:07:34,780 --> 00:07:39,360
tienen que tener la suerte suficiente para que ese vector "v" esté sobre esa línea.

113
00:07:43,320 --> 00:07:48,460
Puede que se den cuenta que algunos de estos casos con determinante 0 parecen más restrictivos que otros.

114
00:07:48,460 --> 00:07:53,400
Dada una matriz de 3x3, por ejemplo, pareciera más difícil que exista una solución

115
00:07:53,400 --> 00:07:58,040
cuando comprima el espacio en una línea comparado con cuando lo comprime en un plano,

116
00:07:58,040 --> 00:08:00,040
aún cuando en ambos casos el determinante es cero.

117
00:08:02,500 --> 00:08:06,400
Tenemos un lenguaje que es un poco más específico que sólo decir que tiene "determinante igual a cero".

118
00:08:06,400 --> 00:08:10,700
Cuando la salida de una transformación es una línea, 
es decir que es unidimensional,

119
00:08:10,700 --> 00:08:13,700
Decimos que la transformación tiene "rango" 1.

120
00:08:15,240 --> 00:08:17,860
Si todos los vectores caen en algún plano bidimensional,

121
00:08:18,140 --> 00:08:20,960
decimos que la transformación tiene rango 2.

122
00:08:22,820 --> 00:08:27,640
Así que la palabra "rango" quiere decir el número de dimensiones de la salida de una transformación.

123
00:08:28,080 --> 00:08:32,960
Por ejemplo, en el caso de las matrices de 2x2, el rango 2 es el mejor que puede ser.

124
00:08:33,000 --> 00:08:39,100
Quiere decir que los vectores base siguen generando todo el espacio bidimensional y su determinante no es cero.

125
00:08:39,540 --> 00:08:43,320
Pero para matrices de 3x3, rango 2 quiere decir que hemos colapsado,

126
00:08:43,320 --> 00:08:46,580
pero no tanto como hubiéramos colapsado en un caso de rango 1.

127
00:08:47,140 --> 00:08:52,060
Si una transformación 3-D posee determinante distinto de cero o su salida abarca todo el espacio 3-D,

128
00:08:52,300 --> 00:08:53,580
tiene rango 3.

129
00:08:54,340 --> 00:08:56,960
Este conjunto de todas las posibles salidas de su matriz,

130
00:08:56,960 --> 00:08:59,920
no importa si es una línea, un plano, el espacio 3-D o lo que sea,

131
00:08:59,940 --> 00:09:02,920
es llamado el "espacio de columna" de su matriz.

132
00:09:04,100 --> 00:09:06,520
Pueden adivinar de dónde sale el nombre.

133
00:09:06,680 --> 00:09:10,020
Las columnas de su matriz les dicen dónde caen sus vectores base

134
00:09:10,980 --> 00:09:15,860
y el espacio generado por las transformadas de los vectores base les da todas las posibles salidas.

135
00:09:16,480 --> 00:09:21,609
En otras palabras, el espacio de columna es el espacio generado por las columnas de tu matriz.

136
00:09:23,680 --> 00:09:25,500
Entonces, una definición más precisa de rango sería

137
00:09:25,580 --> 00:09:29,040
el número de dimensiones en el espacio de columna.

138
00:09:30,020 --> 00:09:32,200
Cuando este rango es el valor más alto que puede tener,

139
00:09:32,200 --> 00:09:36,070
es decir, es igual al número de columnas, decimos que la matriz es de "rango completo".

140
00:09:38,160 --> 00:09:42,480
Fíjense que el vector cero siempre estará incluido en el espacio de columna,

141
00:09:42,480 --> 00:09:45,840
dado que las transformaciones lineales siempre mantienen el origen fijo.

142
00:09:46,940 --> 00:09:51,860
Para una transformación de rango completo, el único vector que cae en el origen es el mismo vector cero,

143
00:09:52,460 --> 00:09:56,120
pero para matrices que no son de rango completo, que comprimen a una dimensión menor,

144
00:09:56,120 --> 00:09:59,420
pueden tener un montón de vectores que caigan en el cero.

145
00:10:01,880 --> 00:10:05,200
Si una transformación 2-D comprime el espacio en una línea, por ejemplo,

146
00:10:05,420 --> 00:10:07,940
hay una línea distinta con una dirección diferente,

147
00:10:08,140 --> 00:10:11,120
llena de vectores que son comprimidos al origen.

148
00:10:11,480 --> 00:10:14,839
Si una transformación 3-D comprime el espacio a un plano,

149
00:10:14,839 --> 00:10:17,839
hay también una línea llena de vectores que caen en el origen.

150
00:10:20,540 --> 00:10:24,100
Si una transformación 3-D comprime todo el espacio en una línea,

151
00:10:24,160 --> 00:10:27,660
hay un plano completo lleno de vectores que cae en el origen.

152
00:10:32,960 --> 00:10:39,080
Este conjunto de vectores que cae en el origen es llamado el "núcleo" o el "kernel" de la matriz.

153
00:10:39,200 --> 00:10:42,340
Es el espacio de todos los vectores que se hacen nulos,

154
00:10:42,350 --> 00:10:44,830
en el sentido de que caen en el vector cero.

155
00:10:45,540 --> 00:10:49,920
En términos de sistemas de ecuaciones lineales, cuando sucede que "v"es el vector cero,

156
00:10:50,240 --> 00:10:53,841
el núcleo les da todas las posibles soluciones de la ecuación.

157
00:10:56,660 --> 00:10:58,380
Esa es una visión general subida de nivel

158
00:10:58,380 --> 00:11:01,980
sobre cómo pensar en los sistemas de ecuaciones lineales geométricamente.

159
00:11:02,260 --> 00:11:05,680
Cada sistema tiene algún tipo de transformación lineal asociado a él,

160
00:11:05,940 --> 00:11:08,120
y cuando esa transformación tiene una inversa

161
00:11:08,120 --> 00:11:11,320
pueden usar esa inversa para resolver su sistema.

162
00:11:12,420 --> 00:11:17,520
En caso contrario, la idea de espacio de columna nos permite entender cuándo una solución puede existir,

163
00:11:17,920 --> 00:11:21,380
y la idea del núcleo nos ayuda a entender cómo se puede ver el conjunto de

164
00:11:21,389 --> 00:11:23,849
todas las posibles soluciones.

165
00:11:24,940 --> 00:11:27,380
De nuevo, hay mucho que no he cubierto aquí,

166
00:11:27,380 --> 00:11:29,380
la más notable siendo cómo computar estas cosas.

167
00:11:29,760 --> 00:11:32,200
Además tuve que limitar mi alcance a ejemplos en los que el número de ecuaciones

168
00:11:32,200 --> 00:11:34,280
es igual al número de incógnitas.

169
00:11:35,000 --> 00:11:37,000
Pero el objetivo acá no es tratar de enseñar todo,

170
00:11:37,100 --> 00:11:42,720
sino que queden con una intuición sólida de las matrices inversas, el espacio de columna y el núcleo,

171
00:11:43,220 --> 00:11:46,840
y que esa intuición haga que cualquier aprendizaje que tengan en el futuro sea más rico

172
00:11:47,700 --> 00:11:51,980
El próximo video, por demanda popular,  será un breve comentario sobre las matrices no cuadradas.

173
00:11:52,260 --> 00:11:54,940
Luego, después de eso, les daré mi perspectiva del producto punto,

174
00:11:55,000 --> 00:11:57,000
y una consecuencia bastante interesante de verlo

175
00:11:57,000 --> 00:11:59,240
bajo la perspectiva de las transformaciones lineales.

176
00:11:59,340 --> 00:12:00,200
¡Nos vemos entonces!

