1
00:00:00,000 --> 00:00:15,160
As you can probably tell by now, the bulk of this series is on understanding matrix

2
00:00:15,160 --> 00:00:20,040
and vector operations through that more visual lens of linear transformations.

3
00:00:20,040 --> 00:00:24,760
This video is no exception, describing the concepts of inverse matrices, column space,

4
00:00:24,760 --> 00:00:28,080
rank, and null space through that lens.

5
00:00:28,080 --> 00:00:32,000
A forewarning though, I'm not going to talk about the methods for actually computing these

6
00:00:32,000 --> 00:00:34,920
things, and some would argue that that's pretty important.

7
00:00:34,920 --> 00:00:38,960
There are a lot of very good resources for learning those methods outside this series,

8
00:00:38,960 --> 00:00:42,440
keywords Gaussian elimination and row echelon form.

9
00:00:42,440 --> 00:00:46,640
I think most of the value that I actually have to add here is on the intuition half.

10
00:00:46,640 --> 00:00:50,760
Plus, in practice, we usually get software to compute this stuff for us anyway.

11
00:00:50,760 --> 00:00:54,460
First, a few words on the usefulness of linear algebra.

12
00:00:54,460 --> 00:00:58,580
By now you already have a hint for how it's used in describing the manipulation of space,

13
00:00:58,580 --> 00:01:02,580
which is useful for things like computer graphics and robotics, but one of the main reasons

14
00:01:02,580 --> 00:01:06,920
that linear algebra is more broadly applicable and required for just about any technical

15
00:01:06,920 --> 00:01:11,500
discipline is that it lets us solve certain systems of equations.

16
00:01:11,500 --> 00:01:15,500
When I say system of equations, I mean you have a list of variables, things you don't

17
00:01:15,500 --> 00:01:18,500
know, and a list of equations relating them.

18
00:01:18,500 --> 00:01:23,600
In a lot of situations, those equations can get very complicated, but if you're lucky,

19
00:01:23,600 --> 00:01:26,520
they might take on a certain special form.

20
00:01:26,520 --> 00:01:31,920
Within each equation, the only thing happening to each variable is that it's scaled by some

21
00:01:31,920 --> 00:01:35,740
constant, and the only thing happening to each of those scaled variables is that they're

22
00:01:35,740 --> 00:01:37,700
added to each other.

23
00:01:37,700 --> 00:01:43,560
So no exponents or fancy functions or multiplying two variables together, things like that.

24
00:01:43,560 --> 00:01:47,820
The typical way to organize this sort of special system of equations is to throw all the variables

25
00:01:47,820 --> 00:01:54,020
on the left and put any lingering constants on the right.

26
00:01:54,020 --> 00:01:57,720
It's also nice to vertically line up the common variables, and to do that you might need to

27
00:01:57,720 --> 00:02:04,940
throw in some zero coefficients whenever the variable doesn't show up in one of the equations.

28
00:02:04,940 --> 00:02:08,160
This is called a linear system of equations.

29
00:02:08,160 --> 00:02:11,940
You might notice that this looks a lot like matrix-vector multiplication.

30
00:02:11,940 --> 00:02:17,220
In fact, you can package all of the equations together into a single vector equation where

31
00:02:17,220 --> 00:02:21,460
you have the matrix containing all of the constant coefficients and a vector containing

32
00:02:21,460 --> 00:02:29,020
all of the variables, and their matrix-vector product equals some different constant vector.

33
00:02:29,020 --> 00:02:33,940
Let's name that constant matrix A, denote the vector holding the variables with a bold-faced

34
00:02:33,940 --> 00:02:39,020
x, and call the constant vector on the right-hand side v.

35
00:02:39,020 --> 00:02:42,360
This is more than just a notational trick to get our system of equations written on

36
00:02:42,360 --> 00:02:43,540
one line.

37
00:02:43,540 --> 00:02:47,620
It sheds light on a pretty cool geometric interpretation for the problem.

38
00:02:47,620 --> 00:02:52,940
The matrix A corresponds with some linear transformation, so solving Ax equals v means

39
00:02:52,940 --> 00:03:00,420
we're looking for a vector x which, after applying the transformation, lands on v.

40
00:03:00,420 --> 00:03:02,180
Think about what's happening here for a moment.

41
00:03:02,180 --> 00:03:07,120
You can hold in your head this really complicated idea of multiple variables all intermingling

42
00:03:07,120 --> 00:03:11,200
with each other just by thinking about squishing and morphing space and trying to figure out

43
00:03:11,200 --> 00:03:12,900
which vector lands on another.

44
00:03:12,940 --> 00:03:14,860
Cool, right?

45
00:03:14,860 --> 00:03:19,060
To start simple, let's say you have a system with two equations and two unknowns.

46
00:03:19,060 --> 00:03:24,780
This means the matrix A is a 2x2 matrix and v and x are each two-dimensional vectors.

47
00:03:24,780 --> 00:03:30,240
Now, how we think about the solutions to this equation depends on whether the transformation

48
00:03:30,240 --> 00:03:35,820
associated with A squishes all of space into a lower dimension, like a line or a point,

49
00:03:35,820 --> 00:03:40,780
or if it leaves everything spanning the full two dimensions where it started.

50
00:03:40,780 --> 00:03:45,540
In the language of the last video, we subdivide into the cases where A has zero determinant

51
00:03:45,540 --> 00:03:48,180
and the case where A has non-zero determinant.

52
00:03:51,740 --> 00:03:55,460
Let's start with the most likely case, where the determinant is non-zero, meaning space

53
00:03:55,460 --> 00:03:58,660
does not get squished into a zero-area region.

54
00:03:58,660 --> 00:04:03,740
In this case, there will always be one and only one vector that lands on v, and you can

55
00:04:03,740 --> 00:04:06,940
find it by playing the transformation in reverse.

56
00:04:06,940 --> 00:04:11,940
Following where v goes as we rewind the tape like this, you'll find the vector x such that

57
00:04:11,940 --> 00:04:15,900
A times x equals v.

58
00:04:15,900 --> 00:04:19,780
When you play the transformation in reverse, it actually corresponds to a separate linear

59
00:04:19,780 --> 00:04:25,420
transformation commonly called the inverse of A, denoted A to the negative one.

60
00:04:25,420 --> 00:04:30,440
For example, if A was a counterclockwise rotation by 90 degrees, then the inverse of A would

61
00:04:30,440 --> 00:04:34,780
be a clockwise rotation by 90 degrees.

62
00:04:34,780 --> 00:04:39,200
If A was a rightward shear that pushes j-hat one unit to the right, the inverse of A would

63
00:04:39,200 --> 00:04:44,340
be a leftward shear that pushes j-hat one unit to the left.

64
00:04:44,340 --> 00:04:48,860
In general, A inverse is the unique transformation with the property that if you first apply

65
00:04:48,860 --> 00:04:54,660
A, then follow it with the transformation A inverse, you end up back where you started.

66
00:04:54,660 --> 00:04:59,640
Applying one transformation after another is captured algebraically with matrix multiplication.

67
00:04:59,640 --> 00:05:05,480
So the core property of this transformation A inverse is that A inverse times A equals

68
00:05:05,480 --> 00:05:08,180
the matrix that corresponds to doing nothing.

69
00:05:08,180 --> 00:05:11,840
The transformation that does nothing is called the identity transformation.

70
00:05:11,840 --> 00:05:20,160
It leaves i-hat and j-hat each where they are, unmoved, so its columns are 1,0 and 0,1.

71
00:05:20,160 --> 00:05:24,240
Once you find this inverse, which in practice you'd do with a computer, you can solve your

72
00:05:24,240 --> 00:05:30,120
equation by multiplying this inverse matrix by v.

73
00:05:30,120 --> 00:05:34,400
And again, what this means geometrically is that you're playing the transformation in

74
00:05:34,400 --> 00:05:40,560
reverse and following v.

75
00:05:40,560 --> 00:05:44,640
This non-zero determinant case, which for a random choice of matrix is by far the most

76
00:05:44,640 --> 00:05:49,680
likely one, corresponds with the idea that if you have two unknowns and two equations,

77
00:05:49,680 --> 00:05:54,160
it's almost certainly the case that there's a single unique solution.

78
00:05:54,160 --> 00:05:57,780
This idea also makes sense in higher dimensions, when the number of equations equals the number

79
00:05:57,780 --> 00:05:58,960
of unknowns.

80
00:05:58,960 --> 00:06:04,360
Again, the system of equations can be translated to the geometric interpretation where you

81
00:06:04,360 --> 00:06:11,700
have some transformation A and some vector v, and you're looking for the vector x that

82
00:06:11,700 --> 00:06:16,180
lands on v.

83
00:06:16,180 --> 00:06:20,720
As long as the transformation A doesn't squish all of space into a lower dimension, meaning

84
00:06:20,720 --> 00:06:26,060
its determinant is non-zero, there will be an inverse transformation A inverse, with

85
00:06:26,060 --> 00:06:33,760
the property that if you first do A, then you do A inverse, it's the same as doing nothing.

86
00:06:33,760 --> 00:06:38,280
And to solve your equation, you just have to multiply that reverse transformation matrix

87
00:06:38,280 --> 00:06:43,640
by the vector v.

88
00:06:43,640 --> 00:06:47,600
But when the determinant is zero, and the transformation associated with the system

89
00:06:47,600 --> 00:06:52,520
of equations squishes space into a smaller dimension, there is no inverse.

90
00:06:52,520 --> 00:06:56,040
You cannot unsquish a line to turn it into a plane.

91
00:06:56,040 --> 00:06:58,500
At least that's not something that a function can do.

92
00:06:58,500 --> 00:07:03,880
That would require transforming each individual vector into a whole line full of vectors.

93
00:07:03,880 --> 00:07:07,720
But functions can only take a single input to a single output.

94
00:07:07,720 --> 00:07:13,320
Similarly, for three equations and three unknowns, there will be no inverse if the corresponding

95
00:07:13,320 --> 00:07:18,560
transformation squishes 3D space onto the plane, or even if it squishes it onto a line

96
00:07:18,560 --> 00:07:19,880
or a point.

97
00:07:19,880 --> 00:07:24,200
Those all correspond to a determinant of zero, since any region is squished into something

98
00:07:24,200 --> 00:07:27,140
with zero volume.

99
00:07:27,140 --> 00:07:31,160
It's still possible that a solution exists even when there is no inverse.

100
00:07:31,160 --> 00:07:35,780
It's just that when your transformation squishes space onto, say, a line, you have to be lucky

101
00:07:35,780 --> 00:07:43,540
enough that the vector v lives somewhere on that line.

102
00:07:43,540 --> 00:07:49,020
You might notice that some of these zero determinant cases feel a lot more restrictive than others.

103
00:07:49,020 --> 00:07:53,620
Given a 3x3 matrix, for example, it seems a lot harder for a solution to exist when

104
00:07:53,620 --> 00:07:58,460
it squishes space onto a line compared to when it squishes things onto a plane, even

105
00:07:58,460 --> 00:08:02,780
though both of those are zero determinant.

106
00:08:02,780 --> 00:08:06,660
We have some language that's a bit more specific than just saying zero determinant.

107
00:08:06,660 --> 00:08:11,300
When the output of a transformation is a line, meaning it's one-dimensional, we say the

108
00:08:11,300 --> 00:08:15,340
transformation has a rank of one.

109
00:08:15,340 --> 00:08:19,840
If all the vectors land on some two-dimensional plane, we say the transformation has a rank

110
00:08:19,840 --> 00:08:23,100
of two.

111
00:08:23,100 --> 00:08:28,500
So the word rank means the number of dimensions in the output of a transformation.

112
00:08:28,500 --> 00:08:33,200
For instance, in the case of 2x2 matrices, rank 2 is the best that it can be.

113
00:08:33,200 --> 00:08:38,340
It means the basis vectors continue to span the full two dimensions of space and the determinant

114
00:08:38,340 --> 00:08:39,680
is non-zero.

115
00:08:39,680 --> 00:08:44,580
But for 3x3 matrices, rank 2 means that we've collapsed, but not as much as they would have

116
00:08:44,580 --> 00:08:47,320
collapsed for a rank 1 situation.

117
00:08:47,320 --> 00:08:52,660
If a 3D transformation has a non-zero determinant and its output fills all of 3D space, it has

118
00:08:52,660 --> 00:08:54,700
a rank of 3.

119
00:08:54,700 --> 00:08:59,900
This set of all possible outputs for your matrix, whether it's a line, a plane, 3D space,

120
00:08:59,900 --> 00:09:04,480
whatever, is called the column space of your matrix.

121
00:09:04,480 --> 00:09:06,780
You can probably guess where that name comes from.

122
00:09:06,780 --> 00:09:12,160
The columns of your matrix tell you where the basis vectors land, and the span of those

123
00:09:12,160 --> 00:09:16,620
transformed basis vectors gives you all possible outputs.

124
00:09:16,620 --> 00:09:23,800
In other words, the column space is the span of the columns of your matrix.

125
00:09:23,800 --> 00:09:28,040
So a more precise definition of rank would be that it's the number of dimensions in the

126
00:09:28,040 --> 00:09:30,240
column space.

127
00:09:30,240 --> 00:09:34,840
When this rank is as high as it can be, meaning it equals the number of columns, we call the

128
00:09:34,840 --> 00:09:37,640
matrix full rank.

129
00:09:37,640 --> 00:09:44,040
Notice, the zero vector will always be included in the column space, since linear transformations

130
00:09:44,040 --> 00:09:47,060
must keep the origin fixed in place.

131
00:09:47,060 --> 00:09:51,640
For a full rank transformation, the only vector that lands at the origin is the zero vector

132
00:09:51,640 --> 00:09:52,640
itself.

133
00:09:52,680 --> 00:09:56,720
But for matrices that aren't full rank, which squish to a smaller dimension, you can have

134
00:09:56,720 --> 00:10:02,160
a whole bunch of vectors that land on zero.

135
00:10:02,160 --> 00:10:06,760
If a 2D transformation squishes space onto a line, for example, there is a separate line

136
00:10:06,760 --> 00:10:11,920
in a different direction full of vectors that get squished onto the origin.

137
00:10:11,920 --> 00:10:16,460
If a 3D transformation squishes space onto a plane, there's also a full line of vectors

138
00:10:16,460 --> 00:10:20,800
that land on the origin.

139
00:10:20,800 --> 00:10:25,540
If a 3D transformation squishes all of space onto a line, then there's a whole plane full

140
00:10:25,540 --> 00:10:33,380
of vectors that land on the origin.

141
00:10:33,380 --> 00:10:38,160
This set of vectors that lands on the origin is called the null space, or the kernel of

142
00:10:38,160 --> 00:10:39,360
your matrix.

143
00:10:39,360 --> 00:10:43,760
It's the space of all vectors that become null, in the sense that they land on the zero

144
00:10:43,760 --> 00:10:45,740
vector.

145
00:10:45,740 --> 00:10:50,320
In terms of the linear system of equations, when v happens to be the zero vector, the

146
00:10:50,360 --> 00:10:56,920
null space gives you all of the possible solutions to the equation.

147
00:10:56,920 --> 00:11:00,920
So that's a very high-level overview of how to think about linear systems of equations

148
00:11:00,920 --> 00:11:02,420
geometrically.

149
00:11:02,420 --> 00:11:06,980
Each system has some kind of linear transformation associated with it, and when that transformation

150
00:11:06,980 --> 00:11:11,720
has an inverse, you can use that inverse to solve your system.

151
00:11:11,720 --> 00:11:18,240
Otherwise, the idea of column space lets us understand when a solution even exists, and

152
00:11:18,240 --> 00:11:22,640
the idea of a null space helps us to understand what the set of all possible solutions can

153
00:11:22,640 --> 00:11:24,200
look like.

154
00:11:24,200 --> 00:11:29,800
Again, there's a lot that I haven't covered here, most notably how to compute these things.

155
00:11:29,800 --> 00:11:33,680
I also had to limit my scope to examples where the number of equations equals the number

156
00:11:33,680 --> 00:11:35,200
of unknowns.

157
00:11:35,200 --> 00:11:39,700
But the goal here is not to try to teach everything, it's that you come away with a strong intuition

158
00:11:39,700 --> 00:11:44,720
for inverse matrices, column space, and null space, and that those intuitions make any

159
00:11:44,720 --> 00:11:47,760
future learning that you do more fruitful.

160
00:11:47,800 --> 00:11:52,480
Next video, by popular request, will be a brief footnote about non-square matrices.

161
00:11:52,480 --> 00:11:55,580
Then after that, I'm going to give you my take on dot products, and something pretty

162
00:11:55,580 --> 00:11:59,440
cool that happens when you view them under the light of linear transformations.

163
00:11:59,440 --> 00:11:59,940
See you then!

