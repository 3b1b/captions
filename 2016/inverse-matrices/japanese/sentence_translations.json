[
 {
  "input": "As you can probably tell by now, the bulk of this series is on understanding matrix and vector operations through that more visual lens of linear transformations. ",
  "translatedText": "もうおわかりかと思いますが、このシリーズの大部分は、線形変換のより視 覚的なレンズを通して行列とベクトルの演算を理解することにあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.24,
  "end": 19.34
 },
 {
  "input": "This video is no exception, describing the concepts of inverse matrices, column space, rank, and null space through that lens. ",
  "translatedText": "このビデオも例外ではなく、そのレンズを通して逆行列 、列空間、ランク、ヌル空間の概念を説明しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.98,
  "end": 27.52
 },
 {
  "input": "A forewarning though, I'm not going to talk about the methods for actually computing these things, and some would argue that that's pretty important. ",
  "translatedText": "ただし、事前に警告しておきますが、これらを実際に計算する方法については話 すつもりはありません。それは非常に重要であると主張する人もいるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 27.52,
  "end": 34.24
 },
 {
  "input": "There are a lot of very good resources for learning those methods outside this series, keywords Gaussian elimination and row echelon form. ",
  "translatedText": "このシリーズ以外にも、ガウス消去法や行階層形式などのキーワー ドを学習するための非常に優れたリソースがたくさんあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.84,
  "end": 42.0
 },
 {
  "input": "I think most of the value that I actually have to add here is on the intuition half. ",
  "translatedText": "ここで実際に追加しなければならない価値の大部分は、半分は直感に基づいていると思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.54,
  "end": 46.34
 },
 {
  "input": "Plus, in practice, we usually get software to compute this stuff for us anyway. ",
  "translatedText": "さらに、実際には、通常、このようなことを計算してくれるソフトウェアを入手します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.9,
  "end": 50.48
 },
 {
  "input": "First, a few words on the usefulness of linear algebra. ",
  "translatedText": "まず、線形代数の有用性について少しお話します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.5,
  "end": 53.92
 },
 {
  "input": "By now you already have a hint for how it's used in describing the manipulation of space, which is useful for things like computer graphics and robotics, but one of the main reasons that linear algebra is more broadly applicable and required for just about any technical discipline is that it lets us solve certain systems of equations. ",
  "translatedText": "ここまでで、線形代数が空間の操作を記述する際にどのように使用されるかについてのヒント はすでに得ています。これはコンピューター グラフィックスやロボット工学などに役立ち ますが、線形代数がより広範囲に適用可能であり、ほぼすべての技術分野で必要とされる主な 理由の 1 つはこれです。それは、特定の方程式系を解くことができるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 54.3,
  "end": 70.46
 },
 {
  "input": "When I say system of equations, I mean you have a list of variables, things you don't know, and a list of equations relating them. ",
  "translatedText": "私が方程式系と言うときは、変数のリスト、未知のもの、およ びそれらに関連する方程式のリストがあることを意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.38,
  "end": 77.76
 },
 {
  "input": "In a lot of situations, those equations can get very complicated, but if you're lucky, they might take on a certain special form. ",
  "translatedText": "多くの状況で、これらの方程式は非常に複雑になる可能性がありま すが、運が良ければ、特定の特別な形をとる可能性があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.34,
  "end": 85.3
 },
 {
  "input": "Within each equation, the only thing happening to each variable is that it's scaled by some constant, and the only thing happening to each of those scaled variables is that they're added to each other. ",
  "translatedText": "各方程式内で、各変数に起こることは、ある定数によってスケ ールされることだけであり、スケールされた変数のそれぞれ に起こることは、それらが互いに加算されることだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 86.44,
  "end": 96.88
 },
 {
  "input": "So no exponents or fancy functions or multiplying two variables together, things like that. ",
  "translatedText": "したがって、指数や派手な関数、2 つの変数の乗算などは必要ありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 97.54,
  "end": 102.28
 },
 {
  "input": "The typical way to organize this sort of special system of equations is to throw all the variables on the left and put any lingering constants on the right. ",
  "translatedText": "この種の特殊な方程式系を整理する一般的な方法は、すべての 変数を左側に配置し、残りの定数を右側に配置することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.42,
  "end": 112.14
 },
 {
  "input": "It's also nice to vertically line up the common variables, and to do that you might need to throw in some zero coefficients whenever the variable doesn't show up in one of the equations. ",
  "translatedText": "共通の変数を縦に並べるのも効果的です。そのためには、変数が方程式の 1 つ に現れない場合には、ゼロ係数をいくつか投入する必要があるかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.6,
  "end": 121.94
 },
 {
  "input": "This is called a linear system of equations. ",
  "translatedText": "これを線形方程式系と呼びます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.54,
  "end": 127.24
 },
 {
  "input": "You might notice that this looks a lot like matrix-vector multiplication. ",
  "translatedText": "これは行列とベクトルの乗算によく似ていることに気づくかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 128.1,
  "end": 131.18
 },
 {
  "input": "In fact, you can package all of the equations together into a single vector equation where you have the matrix containing all of the constant coefficients and a vector containing all of the variables, and their matrix-vector product equals some different constant vector. ",
  "translatedText": "実際、すべての方程式を 1 つのベクトル方程式にパッケージ化することがで きます。この場合、すべての定数係数を含む行列とすべての変数を含むベクトル があり、それらの行列とベクトルの積は別の定数ベクトルと等しくなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.82,
  "end": 146.78
 },
 {
  "input": "Let's name that constant matrix A, denote the vector holding the variables with a bold-faced x, and call the constant vector on the right-hand side v. ",
  "translatedText": "その定数行列を A と名付け、変数を保持するベクトルを太 字の x で示し、右側の定数ベクトルを v と呼びます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 148.64,
  "end": 157.48
 },
 {
  "input": "This is more than just a notational trick to get our system of equations written on one line. ",
  "translatedText": "これは、方程式系を 1 行で記述するため の単なる表記上のトリックではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.86,
  "end": 162.98
 },
 {
  "input": "It sheds light on a pretty cool geometric interpretation for the problem. ",
  "translatedText": "それは、この問題に対する非常にクールな幾何学的な解釈に光を当てます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.34,
  "end": 166.78
 },
 {
  "input": "The matrix A corresponds with some linear transformation, so solving Ax equals v means we're looking for a vector x which, after applying the transformation, lands on v. ",
  "translatedText": "行列 A は線形変換に対応するため、Ax が v に等しいという解を求めることは 、変換を適用した後に v に到達するベクトル x を探していることを意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 167.62,
  "end": 177.92
 },
 {
  "input": "Think about what's happening here for a moment. ",
  "translatedText": "ここで何が起こっているのか少し考えてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.94,
  "end": 181.78
 },
 {
  "input": "You can hold in your head this really complicated idea of multiple variables all intermingling with each other just by thinking about squishing and morphing space and trying to figure out which vector lands on another. ",
  "translatedText": "空間を潰したりモーフィングしたり、どのベクトルが別のベクトルに着 地するかを理解しようとするだけで、複数の変数がすべて互いに混ざり 合うというこの非常に複雑な概念を頭の中に保持することができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 182.06,
  "end": 192.6
 },
 {
  "input": "Cool, right? ",
  "translatedText": "クールですよね？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.16,
  "end": 193.76
 },
 {
  "input": "To start simple, let's say you have a system with two equations and two unknowns. ",
  "translatedText": "まず簡単に、2 つの方程式と 2 つの未知数を含むシステムがあるとします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 194.6,
  "end": 198.68
 },
 {
  "input": "This means the matrix A is a 2x2 matrix and v and x are each two-dimensional vectors. ",
  "translatedText": "これは、行列 A が 2x2 行列であり、v と x がそれぞれ 2 次元ベクトルであることを意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.0,
  "end": 203.96
 },
 {
  "input": "Now, how we think about the solutions to this equation depends on whether the transformation associated with A squishes all of space into a lower dimension, like a line or a point, or if it leaves everything spanning the full two dimensions where it started. ",
  "translatedText": "さて、この方程式の解をどのように考えるかは、A に関連付けられた変 換が空間全体を線や点などの低次元に押しつぶすのか、それとも開始位置 の完全な 2 次元にまたがるすべてを残すのかによって決まります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 205.6,
  "end": 218.9
 },
 {
  "input": "In the language of the last video, we subdivide into the cases where A has zero determinant and the case where A has non-zero determinant. ",
  "translatedText": "最後のビデオの言語では、A の行列式がゼロの場合と、 A の行列式がゼロ以外の場合にさらに細分化されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 220.32,
  "end": 228.04
 },
 {
  "input": "Let's start with the most likely case, where the determinant is non-zero, meaning space does not get squished into a zero-area region. ",
  "translatedText": "最も可能性の高いケースから始めましょう。行列式が非ゼロである場合 、つまり、空間が面積ゼロの領域に押しつぶされないということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.3,
  "end": 237.72
 },
 {
  "input": "In this case, there will always be one and only one vector that lands on v, and you can find it by playing the transformation in reverse. ",
  "translatedText": "この場合、v に到達するベクトルは常に 1 つだけあり、 変換を逆に実行することでそれを見つけることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.6,
  "end": 246.16
 },
 {
  "input": "Following where v goes as we rewind the tape like this, you'll find the vector x such that A times x equals v. ",
  "translatedText": "このようにテープを巻き戻しながら v の位置を追跡すると、A と x の積が v に等しくなるようなベクトル x が見つかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.7,
  "end": 253.46
 },
 {
  "input": "When you play the transformation in reverse, it actually corresponds to a separate linear transformation commonly called the inverse of A, denoted A to the negative one. ",
  "translatedText": "変換を逆に実行すると、実際には、一般に A の逆と呼 ばれる別の線形変換に対応し、A を負の値に表します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.4,
  "end": 264.68
 },
 {
  "input": "For example, if A was a counterclockwise rotation by 90 degrees, then the inverse of A would be a clockwise rotation by 90 degrees. ",
  "translatedText": "たとえば、A が反時計回りに 90 度回転した場 合、A の逆は時計回りに 90 度回転します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 265.36,
  "end": 272.76
 },
 {
  "input": "If A was a rightward shear that pushes j-hat one unit to the right, the inverse of A would be a leftward shear that pushes j-hat one unit to the left. ",
  "translatedText": "A が j-hat を 1 単位右に押す右方向のシアーである場合、A の逆は、j-hat を 1 単位左に押す左方向のシアーになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.32,
  "end": 282.48
 },
 {
  "input": "In general, A inverse is the unique transformation with the property that if you first apply A, then follow it with the transformation A inverse, you end up back where you started. ",
  "translatedText": "一般に、A 逆変換は、最初に A を適用し、次に変換 A 逆変換を適用 すると、最終的には開始した場所に戻るという特性を持つ固有の変換です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 284.1,
  "end": 293.48
 },
 {
  "input": "Applying one transformation after another is captured algebraically with matrix multiplication. ",
  "translatedText": "変換を次々に適用することは、行列の乗算によって代数的に捉えられます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 294.54,
  "end": 298.94
 },
 {
  "input": "So the core property of this transformation A inverse is that A inverse times A equals the matrix that corresponds to doing nothing. ",
  "translatedText": "したがって、この変換 A 逆数の中核となる特性は、A 逆数と A の積が何もしないことに対応する行列に等しいということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.42,
  "end": 307.34
 },
 {
  "input": "The transformation that does nothing is called the identity transformation. ",
  "translatedText": "何もしない変換を恒等変換といいます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 308.2,
  "end": 311.32
 },
 {
  "input": "It leaves i-hat and j-hat each where they are, unmoved, so its columns are 1,0 and 0,1. ",
  "translatedText": "i-hat と j-hat はそれぞれ移動されずにそのまま残されるため、その列は 1,0 と 0,1 になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.78,
  "end": 318.08
 },
 {
  "input": "Once you find this inverse, which in practice you'd do with a computer, you can solve your equation by multiplying this inverse matrix by v. ",
  "translatedText": "この逆行列を見つけたら (実際にはコンピューターで行うことになります が)、この逆行列に v を乗算することで方程式を解くことができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.98,
  "end": 327.72
 },
 {
  "input": "And again, what this means geometrically is that you're playing the transformation in reverse and following v. ",
  "translatedText": "繰り返しになりますが、これが幾何学的に意味するの は、変換を逆に実行し、v に従うということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 329.96,
  "end": 336.44
 },
 {
  "input": "This non-zero determinant case, which for a random choice of matrix is by far the most likely one, corresponds with the idea that if you have two unknowns and two equations, it's almost certainly the case that there's a single unique solution. ",
  "translatedText": "この非ゼロの行列式のケースは、行列をランダムに選択した場合に最も可 能性が高いケースであり、2 つの未知数と 2 つの方程式がある場合 、ほぼ確実に 1 つの一意の解が存在するという考えに対応します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.2,
  "end": 352.4
 },
 {
  "input": "This idea also makes sense in higher dimensions, when the number of equations equals the number of unknowns. ",
  "translatedText": "この考え方は、方程式の数が未知数の数と 等しい場合、高次元でも意味を成します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.68,
  "end": 359.2
 },
 {
  "input": "Again, the system of equations can be translated to the geometric interpretation where you have some transformation A and some vector v, and you're looking for the vector x that lands on v. ",
  "translatedText": "繰り返しますが、方程式系は幾何学的な解釈に変換で きます。ここでは、変換 A とベクトル v が あり、v に到達するベクトル x を探します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 359.38,
  "end": 372.74
 },
 {
  "input": "As long as the transformation A doesn't squish all of space into a lower dimension, meaning its determinant is non-zero, there will be an inverse transformation A inverse, with the property that if you first do A, then you do A inverse, it's the same as doing nothing. ",
  "translatedText": "変換 A が空間全体をより低い次元に押しつぶさない限り、つまり行列式がゼ ロ以外である限り、最初に A を行うと、次に A 逆を行うという性質を 持つ、逆変換 A 逆が存在します。, それは何もしないのと同じです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 391.04
 },
 {
  "input": "And to solve your equation, you just have to multiply that reverse transformation matrix by the vector v. ",
  "translatedText": "方程式を解くには、その逆変換行列に ベクトル v を乗算するだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 393.54,
  "end": 399.44
 },
 {
  "input": "But when the determinant is zero, and the transformation associated with the system of equations squishes space into a smaller dimension, there is no inverse. ",
  "translatedText": "しかし、行列式がゼロで、連立方程式に関連する変換によって空 間がより小さな次元に押しつぶされる場合、逆は存在しません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.5,
  "end": 412.06
 },
 {
  "input": "You cannot unsquish a line to turn it into a plane. ",
  "translatedText": "線を押し広げて平面にすることはできません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.48,
  "end": 415.46
 },
 {
  "input": "At least that's not something that a function can do. ",
  "translatedText": "少なくともそれは関数でできることではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 415.98,
  "end": 418.06
 },
 {
  "input": "That would require transforming each individual vector into a whole line full of vectors. ",
  "translatedText": "そのためには、個々のベクトルをベクトルで満たされた行全体に変換する必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.36,
  "end": 422.98
 },
 {
  "input": "But functions can only take a single input to a single output. ",
  "translatedText": "ただし、関数は 1 つの出力に対して 1 つの入力のみを取ることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.74,
  "end": 426.74
 },
 {
  "input": "Similarly, for three equations and three unknowns, there will be no inverse if the corresponding transformation squishes 3D space onto the plane, or even if it squishes it onto a line or a point. ",
  "translatedText": "同様に、3 つの方程式と 3 つの未知数の場合、対応する変換 によって 3D 空間が平面上に押しつぶされる場合、または線 または点上に押しつぶされる場合でも、逆変換は行われません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.4,
  "end": 439.14
 },
 {
  "input": "Those all correspond to a determinant of zero, since any region is squished into something with zero volume. ",
  "translatedText": "どの領域も体積ゼロの何かに押しつぶされるため 、これらはすべてゼロの行列式に対応します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 439.92,
  "end": 445.16
 },
 {
  "input": "It's still possible that a solution exists even when there is no inverse. ",
  "translatedText": "逆が存在しない場合でも、解が存在する可能性は依然としてあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.7,
  "end": 450.64
 },
 {
  "input": "It's just that when your transformation squishes space onto, say, a line, you have to be lucky enough that the vector v lives somewhere on that line. ",
  "translatedText": "ただ、変換によって空間がたとえば直線上に押しつぶされるとき、幸運 にもベクトル v がその直線上のどこかに存在する必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.72,
  "end": 459.38
 },
 {
  "input": "You might notice that some of these zero determinant cases feel a lot more restrictive than others. ",
  "translatedText": "これらのゼロ決定要因のケースの中には、他のケースよりもはるかに制限的であるように感じられるものがあることに気づくかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 463.3,
  "end": 468.3
 },
 {
  "input": "Given a 3x3 matrix, for example, it seems a lot harder for a solution to exist when it squishes space onto a line compared to when it squishes things onto a plane, even though both of those are zero determinant. ",
  "translatedText": "たとえば、3x3 の行列を考えると、空間を直線上に押しつぶす場合は 、物体を平面上に押し込む場合に比べて、どちらも決定要因がゼロである にもかかわらず、解が存在するのははるかに困難であるように見えます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.84,
  "end": 480.24
 },
 {
  "input": "We have some language that's a bit more specific than just saying zero determinant. ",
  "translatedText": "単に決定要因がゼロというだけではなく、もう少し具体的な表現があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 482.6,
  "end": 486.1
 },
 {
  "input": "When the output of a transformation is a line, meaning it's one-dimensional, we say the transformation has a rank of one. ",
  "translatedText": "変換の出力が線である場合、つまり 1 次元であ る場合、変換のランクは 1 であると言います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.52,
  "end": 493.5
 },
 {
  "input": "If all the vectors land on some two-dimensional plane, we say the transformation has a rank of two. ",
  "translatedText": "すべてのベクトルが何らかの 2 次元平面上にあ る場合、変換のランクは 2 であると言います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.14,
  "end": 500.42
 },
 {
  "input": "So the word rank means the number of dimensions in the output of a transformation. ",
  "translatedText": "したがって、ランクという言葉は、変換の出力の次元数を意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.92,
  "end": 507.48
 },
 {
  "input": "For instance, in the case of 2x2 matrices, rank 2 is the best that it can be. ",
  "translatedText": "たとえば、2x2 行列の場合、ランク 2 が可能な限り最高です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.4,
  "end": 512.72
 },
 {
  "input": "It means the basis vectors continue to span the full two dimensions of space and the determinant is non-zero. ",
  "translatedText": "これは、基底ベクトルが空間の完全な 2 次元に広 がり続け、行列式が非ゼロであることを意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 513.08,
  "end": 519.02
 },
 {
  "input": "But for 3x3 matrices, rank 2 means that we've collapsed, but not as much as they would have collapsed for a rank 1 situation. ",
  "translatedText": "しかし、3x3 行列の場合、ランク 2 は崩壊していることを意 味しますが、ランク 1 の状況で崩壊するほどではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 519.42,
  "end": 526.46
 },
 {
  "input": "If a 3D transformation has a non-zero determinant and its output fills all of 3D space, it has a rank of 3. ",
  "translatedText": "3D 変換にゼロ以外の行列式があり、その出力が 3D 空間全体を満たす場合、ランクは 3 になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 527.24,
  "end": 533.34
 },
 {
  "input": "This set of all possible outputs for your matrix, whether it's a line, a plane, 3D space, whatever, is called the column space of your matrix. ",
  "translatedText": "行列の可能なすべての出力のセットは、線、平面 、3D 空間など、行列の列空間と呼ばれます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 534.52,
  "end": 542.72
 },
 {
  "input": "You can probably guess where that name comes from. ",
  "translatedText": "おそらくその名前の由来は推測できるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.14,
  "end": 546.28
 },
 {
  "input": "The columns of your matrix tell you where the basis vectors land, and the span of those transformed basis vectors gives you all possible outputs. ",
  "translatedText": "行列の列は、基底ベクトルがどこに到達するかを示し、それらの変換さ れた基底ベクトルの範囲から、考えられるすべての出力が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.56,
  "end": 555.84
 },
 {
  "input": "In other words, the column space is the span of the columns of your matrix. ",
  "translatedText": "言い換えれば、列空間は行列の列の範囲です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.36,
  "end": 561.14
 },
 {
  "input": "So a more precise definition of rank would be that it's the number of dimensions in the column space. ",
  "translatedText": "したがって、ランクのより正確な定義は、列 空間の次元数であるということになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 568.94
 },
 {
  "input": "When this rank is as high as it can be, meaning it equals the number of columns, we call the matrix full rank. ",
  "translatedText": "このランクが可能な限り高い場合、つまり列の数 と等しい場合、行列をフル ランクと呼びます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.94,
  "end": 576.12
 },
 {
  "input": "Notice, the zero vector will always be included in the column space, since linear transformations must keep the origin fixed in place. ",
  "translatedText": "線形変換では原点を所定の位置に固定しておく必要があるため、ゼ ロ ベクトルは常に列空間に含まれることに注意してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 578.54,
  "end": 585.84
 },
 {
  "input": "For a full rank transformation, the only vector that lands at the origin is the zero vector itself. ",
  "translatedText": "フルランク変換の場合、原点に到達する唯 一のベクトルはゼロ ベクトル自体です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.9,
  "end": 591.96
 },
 {
  "input": "But for matrices that aren't full rank, which squish to a smaller dimension, you can have a whole bunch of vectors that land on zero. ",
  "translatedText": "しかし、フルランクではない行列の場合は、より小さい次元に押しつぶさ れるため、ゼロに到達するベクトルが大量に存在する可能性があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.46,
  "end": 598.76
 },
 {
  "input": "If a 2D transformation squishes space onto a line, for example, there is a separate line in a different direction full of vectors that get squished onto the origin. ",
  "translatedText": "たとえば、2D 変換によって空間がライン上に押しつぶされる場合、原点上 に押しつぶされるベクトルで満たされた別の方向の別のラインが存在します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 601.64,
  "end": 610.58
 },
 {
  "input": "If a 3D transformation squishes space onto a plane, there's also a full line of vectors that land on the origin. ",
  "translatedText": "3D 変換によって空間が平面上に押しつぶされる 場合、原点に到達するベクトルの列も存在します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 611.78,
  "end": 617.62
 },
 {
  "input": "If a 3D transformation squishes all of space onto a line, then there's a whole plane full of vectors that land on the origin. ",
  "translatedText": "3D 変換によって空間全体が 1 つの直線上に押しつぶされる と、原点に到達するベクトルで満たされた平面全体が存在します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 620.52,
  "end": 627.46
 },
 {
  "input": "This set of vectors that lands on the origin is called the null space, or the kernel of your matrix. ",
  "translatedText": "原点に到達するこのベクトルのセットは、ヌル 空間、または行列のカーネルと呼ばれます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.8,
  "end": 638.78
 },
 {
  "input": "It's the space of all vectors that become null, in the sense that they land on the zero vector. ",
  "translatedText": "これは、ゼロ ベクトルに到達するという意味 で、ヌルになるすべてのベクトルの空間です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 639.36,
  "end": 644.18
 },
 {
  "input": "In terms of the linear system of equations, when v happens to be the zero vector, the null space gives you all of the possible solutions to the equation. ",
  "translatedText": "線形方程式系に関して言えば、v がたまたまゼロ ベクトルであ る場合、ヌル空間は方程式の考えられるすべての解を与えます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 645.68,
  "end": 653.64
 },
 {
  "input": "So that's a very high-level overview of how to think about linear systems of equations geometrically. ",
  "translatedText": "これは、線形方程式系を幾何学的に考える 方法についての非常に高度な概要です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.42,
  "end": 661.72
 },
 {
  "input": "Each system has some kind of linear transformation associated with it, and when that transformation has an inverse, you can use that inverse to solve your system. ",
  "translatedText": "各システムには何らかの線形変換が関連付けられており、その変換に逆関 数がある場合は、その逆関数を使用してシステムを解くことができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 662.3,
  "end": 670.74
 },
 {
  "input": "Otherwise, the idea of column space lets us understand when a solution even exists, and the idea of a null space helps us to understand what the set of all possible solutions can look like. ",
  "translatedText": "それ以外の場合は、列空間の概念により、解が存在する場合を 理解することができ、ヌル空間の概念は、考えられるすべての 解のセットがどのようなものかを理解するのに役立ちます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 672.28,
  "end": 683.44
 },
 {
  "input": "Again, there's a lot that I haven't covered here, most notably how to compute these things. ",
  "translatedText": "繰り返しになりますが、ここで取り上げていないことはたくさんありますが、最も注目すべきはこれらの計算方法です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 684.98,
  "end": 689.38
 },
 {
  "input": "I also had to limit my scope to examples where the number of equations equals the number of unknowns. ",
  "translatedText": "また、方程式の数が未知数の数と等しい 例に範囲を限定する必要もありました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.8,
  "end": 694.76
 },
 {
  "input": "But the goal here is not to try to teach everything, it's that you come away with a strong intuition for inverse matrices, column space, and null space, and that those intuitions make any future learning that you do more fruitful. ",
  "translatedText": "しかし、ここでの目標は、すべてを教えようとすることではなく、 逆行列、列空間、NULL 空間についての強い直感を身につけ、 その直感によって今後の学習がより実りあるものになることです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 694.88,
  "end": 706.5
 },
 {
  "input": "Next video, by popular request, will be a brief footnote about non-square matrices. ",
  "translatedText": "次のビデオは、多くのリクエストに応えて、非正方行列に関する簡単な脚注になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 707.66,
  "end": 711.88
 },
 {
  "input": "Then after that, I'm going to give you my take on dot products, and something pretty cool that happens when you view them under the light of linear transformations. ",
  "translatedText": "その後、ドット積についての私の見解と、線形変換の光の下でドット 積を観察すると起こる非常に素晴らしいことについて説明します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 711.88,
  "end": 718.92
 },
 {
  "input": "See you then! ",
  "translatedText": "それではまた！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 719.48,
  "end": 719.66
 }
]