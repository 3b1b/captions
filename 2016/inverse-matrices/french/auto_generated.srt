1
00:00:10,240 --> 00:00:12,599
Comme vous pouvez probablement le constater maintenant, 

2
00:00:12,599 --> 00:00:15,506
l'essentiel de cette série porte sur la compréhension des opérations 

3
00:00:15,506 --> 00:00:18,244
matricielles et vectorielles à travers le prisme plus visuel des 

4
00:00:18,244 --> 00:00:19,340
transformations linéaires.

5
00:00:19,980 --> 00:00:24,001
Cette vidéo ne fait pas exception, décrivant les concepts de matrices inverses, 

6
00:00:24,001 --> 00:00:27,520
d'espace de colonnes, de rang et d'espace nul à travers cette optique.

7
00:00:27,520 --> 00:00:30,677
Attention cependant, je ne vais pas parler des méthodes permettant de 

8
00:00:30,677 --> 00:00:34,240
calculer réellement ces choses, et certains diraient que c'est assez important.

9
00:00:34,840 --> 00:00:38,506
Il existe de nombreuses très bonnes ressources pour apprendre ces méthodes en dehors 

10
00:00:38,506 --> 00:00:42,000
de cette série, les mots-clés élimination gaussienne et forme d'échelon de ligne.

11
00:00:42,540 --> 00:00:44,459
Je pense que l’essentiel de la valeur que je dois 

12
00:00:44,459 --> 00:00:46,340
ajouter ici réside dans la moitié de l’intuition.

13
00:00:46,900 --> 00:00:48,706
De plus, dans la pratique, nous obtenons généralement 

14
00:00:48,706 --> 00:00:50,480
un logiciel pour calculer ces éléments à notre place.

15
00:00:51,500 --> 00:00:53,920
Tout d’abord, quelques mots sur l’utilité de l’algèbre linéaire.

16
00:00:54,300 --> 00:00:57,442
À présent, vous avez déjà une idée de la façon dont il est utilisé pour décrire la 

17
00:00:57,442 --> 00:01:00,661
manipulation de l’espace, ce qui est utile pour des choses comme l’infographie et la 

18
00:01:00,661 --> 00:01:01,040
robotique.

19
00:01:01,500 --> 00:01:04,459
Mais l’une des principales raisons pour lesquelles l’algèbre linéaire est 

20
00:01:04,459 --> 00:01:07,380
plus largement applicable et nécessaire à presque toutes les disciplines 

21
00:01:07,380 --> 00:01:10,460
techniques est qu’elle nous permet de résoudre certains systèmes d’équations.

22
00:01:11,380 --> 00:01:14,860
Quand je parle de système d'équations, je veux dire que vous avez une liste de variables, 

23
00:01:14,860 --> 00:01:17,760
de choses que vous ne connaissez pas, et une liste d'équations les reliant.

24
00:01:18,340 --> 00:01:21,600
Dans de nombreuses situations, ces équations peuvent devenir très compliquées.

25
00:01:22,120 --> 00:01:25,300
Mais si vous avez de la chance, ils pourraient prendre une forme particulière.

26
00:01:26,440 --> 00:01:29,847
Dans chaque équation, la seule chose qui arrive à chaque variable est qu'elle 

27
00:01:29,847 --> 00:01:33,298
est mise à l'échelle par une constante, et la seule chose qui arrive à chacune 

28
00:01:33,298 --> 00:01:36,880
de ces variables mises à l'échelle est qu'elles sont ajoutées les unes aux autres.

29
00:01:37,540 --> 00:01:39,770
Donc pas d'exposants ou de fonctions fantaisistes ou de 

30
00:01:39,770 --> 00:01:42,280
multiplication de deux variables ensemble, des choses comme ça.

31
00:01:43,420 --> 00:01:47,780
La manière typique d’organiser ce type de système spécial d’équations consiste 

32
00:01:47,780 --> 00:01:52,140
à placer toutes les variables à gauche et les constantes persistantes à droite.

33
00:01:53,600 --> 00:01:56,703
Il est également intéressant d'aligner verticalement les variables communes et, 

34
00:01:56,703 --> 00:01:59,573
pour ce faire, vous devrez peut-être ajouter des coefficients nuls chaque 

35
00:01:59,573 --> 00:02:01,940
fois que la variable n'apparaît pas dans l'une des équations.

36
00:02:04,540 --> 00:02:07,240
C'est ce qu'on appelle un système linéaire d'équations.

37
00:02:08,100 --> 00:02:09,639
Vous remarquerez peut-être que cela ressemble 

38
00:02:09,639 --> 00:02:11,180
beaucoup à une multiplication matrice-vecteur.

39
00:02:11,820 --> 00:02:15,349
En fait, vous pouvez regrouper toutes les équations dans une seule 

40
00:02:15,349 --> 00:02:19,089
équation vectorielle dans laquelle vous avez la matrice contenant tous 

41
00:02:19,089 --> 00:02:22,934
les coefficients constants et un vecteur contenant toutes les variables, 

42
00:02:22,934 --> 00:02:26,780
et leur produit matrice-vecteur est égal à un vecteur constant différent.

43
00:02:28,640 --> 00:02:32,878
Nommons cette matrice constante A, désignons le vecteur contenant les 

44
00:02:32,878 --> 00:02:37,480
variables avec un X en gras et appelons le vecteur constant du côté droit V.

45
00:02:38,860 --> 00:02:40,839
C’est plus qu’une simple astuce de notation pour 

46
00:02:40,839 --> 00:02:42,980
écrire notre système d’équations sur une seule ligne.

47
00:02:43,340 --> 00:02:46,780
Cela met en lumière une interprétation géométrique plutôt sympa du problème.

48
00:02:47,620 --> 00:02:50,633
La matrice A correspond à une transformation linéaire, 

49
00:02:50,633 --> 00:02:54,906
donc résoudre Ax est égal à V signifie que nous recherchons un vecteur X qui, 

50
00:02:54,906 --> 00:02:57,920
après avoir appliqué la transformation, atterrit sur V.

51
00:02:59,940 --> 00:03:01,780
Pensez un instant à ce qui se passe ici.

52
00:03:02,060 --> 00:03:05,314
Vous pouvez garder en tête cette idée très compliquée de plusieurs 

53
00:03:05,314 --> 00:03:09,054
variables qui s'entremêlent simplement en pensant à écraser et à transformer 

54
00:03:09,054 --> 00:03:12,600
l'espace et en essayant de déterminer quel vecteur atterrit sur un autre.

55
00:03:13,160 --> 00:03:13,760
Cool, non ?

56
00:03:14,600 --> 00:03:16,661
Pour commencer simple, disons que vous avez un 

57
00:03:16,661 --> 00:03:18,680
système avec deux équations et deux inconnues.

58
00:03:19,000 --> 00:03:21,526
Cela signifie que la matrice A est une matrice 2x2 et 

59
00:03:21,526 --> 00:03:23,960
que V et X sont chacun des vecteurs bidimensionnels.

60
00:03:25,600 --> 00:03:29,071
Maintenant, la façon dont nous réfléchissons aux solutions de cette équation 

61
00:03:29,071 --> 00:03:32,272
dépend du fait que la transformation associée à A écrase tout l'espace 

62
00:03:32,272 --> 00:03:34,977
dans une dimension inférieure, comme une ligne ou un point, 

63
00:03:34,977 --> 00:03:38,900
ou si elle laisse tout s'étendre sur les deux dimensions complètes là où il a commencé.

64
00:03:40,320 --> 00:03:44,090
Dans le langage de la dernière vidéo, nous subdivisons les cas 

65
00:03:44,090 --> 00:03:48,040
où A a un déterminant nul et le cas où A a un déterminant non nul.

66
00:03:51,300 --> 00:03:54,400
Commençons par le cas le plus probable, où le déterminant est non nul, 

67
00:03:54,400 --> 00:03:57,720
ce qui signifie que l'espace n'est pas écrasé dans une région de zone nulle.

68
00:03:58,600 --> 00:04:02,565
Dans ce cas, il y aura toujours un et un seul vecteur qui atterrira sur V, 

69
00:04:02,565 --> 00:04:06,160
et vous pourrez le retrouver en jouant la transformation à l'envers.

70
00:04:06,700 --> 00:04:10,272
En suivant où va V lorsque nous rembobinons la bande comme ceci, 

71
00:04:10,272 --> 00:04:13,460
vous trouverez le vecteur x tel que A fois x est égal à V.

72
00:04:15,400 --> 00:04:18,090
Lorsque vous jouez la transformation à l'envers, 

73
00:04:18,090 --> 00:04:21,659
elle correspond en fait à une transformation linéaire distincte, 

74
00:04:21,659 --> 00:04:24,680
communément appelée l'inverse de A, notée A au négatif.

75
00:04:25,360 --> 00:04:27,775
Par exemple, si A était une rotation de 90 degrés dans le sens 

76
00:04:27,775 --> 00:04:30,229
inverse des aiguilles d’une montre, alors l’inverse de A serait 

77
00:04:30,229 --> 00:04:32,760
une rotation de 90 degrés dans le sens des aiguilles d’une montre.

78
00:04:34,320 --> 00:04:38,178
Si A était un cisaillement vers la droite qui pousse j-hat d’une unité vers la droite, 

79
00:04:38,178 --> 00:04:41,016
l’inverse de A serait un cisaillement vers la gauche qui pousse 

80
00:04:41,016 --> 00:04:42,480
j-hat d’une unité vers la gauche.

81
00:04:44,100 --> 00:04:46,993
En général, A inverse est l'unique transformation avec la 

82
00:04:46,993 --> 00:04:50,137
propriété que si vous appliquez d'abord A, puis suivez-la avec 

83
00:04:50,137 --> 00:04:53,480
la transformation A inverse, vous revenez là où vous avez commencé.

84
00:04:54,540 --> 00:04:58,823
L'application d'une transformation après l'autre est capturée algébriquement avec la 

85
00:04:58,823 --> 00:05:03,006
multiplication matricielle, donc la propriété principale de cette transformation A 

86
00:05:03,006 --> 00:05:07,340
inverse est que A inverse fois A est égal à la matrice qui correspond à ne rien faire.

87
00:05:08,200 --> 00:05:11,320
La transformation qui ne fait rien s’appelle la transformation identitaire.

88
00:05:11,780 --> 00:05:15,031
Il laisse i-hat et j-hat chacun là où ils sont, 

89
00:05:15,031 --> 00:05:18,080
immobiles, donc ses colonnes sont 1,0 et 0,1.

90
00:05:19,980 --> 00:05:23,648
Une fois que vous avez trouvé cet inverse, ce que vous feriez en pratique avec un 

91
00:05:23,648 --> 00:05:27,451
ordinateur, vous pouvez résoudre votre équation en multipliant cette matrice inverse 

92
00:05:27,451 --> 00:05:27,720
par v.

93
00:05:29,960 --> 00:05:32,990
Et encore une fois, ce que cela signifie géométriquement, 

94
00:05:32,990 --> 00:05:36,440
c'est que vous jouez la transformation à l'envers et en suivant v.

95
00:05:40,200 --> 00:05:44,125
Ce cas déterminant non nul, qui pour un choix aléatoire de matrice est de 

96
00:05:44,125 --> 00:05:48,103
loin le plus probable, correspond à l'idée que si vous avez deux inconnues 

97
00:05:48,103 --> 00:05:52,400
et deux équations, il est presque certain qu'il existe une seule solution unique.

98
00:05:53,680 --> 00:05:56,506
Cette idée a également du sens dans les dimensions supérieures, 

99
00:05:56,506 --> 00:05:59,200
lorsque le nombre d’équations est égal au nombre d’inconnues.

100
00:05:59,380 --> 00:06:03,669
Encore une fois, le système d'équations peut être traduit en 

101
00:06:03,669 --> 00:06:09,153
interprétation géométrique où vous avez une transformation A et un vecteur v, 

102
00:06:09,153 --> 00:06:12,740
et vous recherchez le vecteur x qui atterrit sur v.

103
00:06:15,740 --> 00:06:20,162
Tant que la transformation A n'écrase pas tout l'espace dans une dimension inférieure, 

104
00:06:20,162 --> 00:06:22,652
ce qui signifie que son déterminant est non nul, 

105
00:06:22,652 --> 00:06:25,092
il y aura une transformation inverse A inverse, 

106
00:06:25,092 --> 00:06:29,006
avec la propriété que si vous faites d'abord A, alors vous faites A inverse. 

107
00:06:29,006 --> 00:06:31,040
, c'est la même chose que ne rien faire.

108
00:06:33,540 --> 00:06:36,613
Et pour résoudre votre équation, il vous suffit de multiplier 

109
00:06:36,613 --> 00:06:39,440
cette matrice de transformation inverse par le vecteur v.

110
00:06:43,500 --> 00:06:47,806
Mais lorsque le déterminant est nul et que la transformation associée au système 

111
00:06:47,806 --> 00:06:52,060
d’équations réduit l’espace à une dimension plus petite, il n’y a pas d’inverse.

112
00:06:52,480 --> 00:06:55,460
Vous ne pouvez pas défaire une ligne pour la transformer en avion.

113
00:06:55,980 --> 00:06:58,060
Au moins, ce n'est pas quelque chose qu'une fonction peut faire.

114
00:06:58,360 --> 00:07:00,601
Cela nécessiterait de transformer chaque vecteur 

115
00:07:00,601 --> 00:07:02,980
individuel en une ligne entière remplie de vecteurs.

116
00:07:03,740 --> 00:07:06,740
Mais les fonctions ne peuvent prendre qu’une seule entrée vers une seule sortie.

117
00:07:08,400 --> 00:07:11,139
De même, pour trois équations et trois inconnues, 

118
00:07:11,139 --> 00:07:14,537
il n'y aura pas d'inverse si la transformation correspondante 

119
00:07:14,537 --> 00:07:19,140
écrase l'espace 3D sur le plan, ou même si elle l'écrase sur une droite ou un point.

120
00:07:19,920 --> 00:07:22,138
Tout cela correspond à un déterminant de zéro, 

121
00:07:22,138 --> 00:07:25,160
puisque toute région est écrasée en quelque chose de volume nul.

122
00:07:26,700 --> 00:07:30,640
Il est toujours possible qu'une solution existe même s'il n'y a pas d'inverse.

123
00:07:30,720 --> 00:07:35,235
C'est juste que lorsque votre transformation écrase l'espace sur, disons, une ligne, 

124
00:07:35,235 --> 00:07:39,380
vous devez avoir la chance que le vecteur v vive quelque part sur cette ligne.

125
00:07:43,300 --> 00:07:45,545
Vous remarquerez peut-être que certains de ces cas à 

126
00:07:45,545 --> 00:07:48,300
déterminant zéro semblent beaucoup plus restrictifs que d’autres.

127
00:07:48,840 --> 00:07:52,558
Étant donné une matrice 3x3, par exemple, il semble beaucoup plus difficile 

128
00:07:52,558 --> 00:07:56,179
pour une solution d'exister lorsqu'elle écrase l'espace sur une ligne que 

129
00:07:56,179 --> 00:08:00,240
lorsqu'elle écrase les choses sur un plan, même si les deux sont déterminants nuls.

130
00:08:02,600 --> 00:08:06,100
Nous avons un langage un peu plus précis que de simplement dire un déterminant zéro.

131
00:08:06,520 --> 00:08:09,272
Lorsque le résultat d’une transformation est une ligne, 

132
00:08:09,272 --> 00:08:13,500
c’est-à-dire qu’elle est unidimensionnelle, on dit que la transformation a un rang un.

133
00:08:15,140 --> 00:08:18,143
Si tous les vecteurs atterrissent sur un plan bidimensionnel, 

134
00:08:18,143 --> 00:08:20,420
on dit que la transformation a un rang de deux.

135
00:08:22,920 --> 00:08:27,480
Ainsi, le mot rang désigne le nombre de dimensions dans le résultat d’une transformation.

136
00:08:28,400 --> 00:08:32,720
Par exemple, dans le cas de matrices 2x2, le rang deux est le meilleur possible.

137
00:08:33,080 --> 00:08:36,073
Cela signifie que les vecteurs de base continuent de couvrir les 

138
00:08:36,073 --> 00:08:39,020
deux dimensions de l’espace et que le déterminant n’est pas nul.

139
00:08:39,419 --> 00:08:43,268
Mais pour les matrices 3x3, le rang deux signifie que nous nous sommes effondrés, 

140
00:08:43,268 --> 00:08:46,460
mais pas autant qu'ils l'auraient été dans une situation de rang un.

141
00:08:47,240 --> 00:08:50,211
Si une transformation 3D a un déterminant non nul et que 

142
00:08:50,211 --> 00:08:53,340
sa sortie remplit tout l’espace 3D, elle a un rang de trois.

143
00:08:54,520 --> 00:08:57,514
Cet ensemble de toutes les sorties possibles pour votre matrice, 

144
00:08:57,514 --> 00:09:00,646
qu'il s'agisse d'une ligne, d'un plan, d'un espace 3D, peu importe, 

145
00:09:00,646 --> 00:09:02,720
est appelé l'espace colonne de votre matrice.

146
00:09:04,140 --> 00:09:06,280
Vous pouvez probablement deviner d'où vient ce nom.

147
00:09:06,560 --> 00:09:11,038
Les colonnes de votre matrice vous indiquent où atterrissent les vecteurs de base, 

148
00:09:11,038 --> 00:09:15,840
et l'étendue de ces vecteurs de base transformés vous donne toutes les sorties possibles.

149
00:09:16,360 --> 00:09:21,140
En d’autres termes, l’espace des colonnes est l’étendue des colonnes de votre matrice.

150
00:09:23,300 --> 00:09:26,019
Une définition plus précise du rang serait donc qu'il 

151
00:09:26,019 --> 00:09:28,940
s'agit du nombre de dimensions dans l'espace des colonnes.

152
00:09:29,940 --> 00:09:32,915
Lorsque ce rang est aussi élevé que possible, c’est-à-dire qu’il 

153
00:09:32,915 --> 00:09:36,120
est égal au nombre de colonnes, nous appelons la matrice rang complet.

154
00:09:38,540 --> 00:09:42,165
Notez que le vecteur zéro sera toujours inclus dans l'espace des colonnes, 

155
00:09:42,165 --> 00:09:45,840
car les transformations linéaires doivent maintenir l'origine fixe en place.

156
00:09:46,900 --> 00:09:49,498
Pour une transformation de rang complet, le seul vecteur 

157
00:09:49,498 --> 00:09:51,960
qui atterrit à l’origine est le vecteur zéro lui-même.

158
00:09:52,460 --> 00:09:54,523
Mais pour les matrices qui ne sont pas de rang complet, 

159
00:09:54,523 --> 00:09:56,254
qui s'écrasent dans une dimension plus petite, 

160
00:09:56,254 --> 00:09:58,760
vous pouvez avoir tout un tas de vecteurs qui atterrissent sur zéro.

161
00:10:01,640 --> 00:10:05,086
Si une transformation 2D écrase l'espace sur une ligne, par exemple, 

162
00:10:05,086 --> 00:10:09,481
il existe une ligne distincte dans une direction différente pleine de vecteurs qui sont 

163
00:10:09,481 --> 00:10:10,580
écrasés sur l'origine.

164
00:10:11,780 --> 00:10:14,098
Si une transformation 3D écrase l'espace sur un plan, 

165
00:10:14,098 --> 00:10:17,620
il existe également une ligne complète de vecteurs qui atterrissent sur l'origine.

166
00:10:20,520 --> 00:10:23,610
Si une transformation 3D écrase tout l'espace sur une ligne, 

167
00:10:23,610 --> 00:10:27,460
alors il y a tout un plan rempli de vecteurs qui atterrissent sur l'origine.

168
00:10:32,800 --> 00:10:37,143
Cet ensemble de vecteurs qui atterrit sur l'origine est appelé l'espace nul, 

169
00:10:37,143 --> 00:10:38,780
ou le noyau de votre matrice.

170
00:10:39,360 --> 00:10:41,857
C'est l'espace de tous les vecteurs qui deviennent nuls, 

171
00:10:41,857 --> 00:10:44,180
dans le sens où ils atterrissent sur le vecteur zéro.

172
00:10:45,680 --> 00:10:50,102
En termes de système linéaire d'équations, lorsque v se trouve être le vecteur zéro, 

173
00:10:50,102 --> 00:10:53,640
l'espace nul vous donne toutes les solutions possibles à l'équation.

174
00:10:56,420 --> 00:10:59,023
Voilà donc un aperçu de très haut niveau de la façon de 

175
00:10:59,023 --> 00:11:01,720
penser géométriquement les systèmes d’équations linéaires.

176
00:11:02,300 --> 00:11:05,549
Chaque système est associé à une sorte de transformation linéaire, 

177
00:11:05,549 --> 00:11:09,624
et lorsque cette transformation a un inverse, vous pouvez utiliser cet inverse pour 

178
00:11:09,624 --> 00:11:10,740
résoudre votre système.

179
00:11:12,280 --> 00:11:17,141
Sinon, l’idée d’espace de colonnes nous permet de comprendre quand une solution existe, 

180
00:11:17,141 --> 00:11:20,953
et l’idée d’espace nul nous aide à comprendre à quoi peut ressembler 

181
00:11:20,953 --> 00:11:23,440
l’ensemble de toutes les solutions possibles.

182
00:11:24,980 --> 00:11:27,873
Encore une fois, il y a beaucoup de choses que je n'ai pas abordées ici, 

183
00:11:27,873 --> 00:11:29,380
notamment comment calculer ces choses.

184
00:11:29,800 --> 00:11:32,425
J'ai également dû limiter mon champ d'application aux exemples 

185
00:11:32,425 --> 00:11:34,760
où le nombre d'équations est égal au nombre d'inconnues.

186
00:11:34,880 --> 00:11:37,406
Mais le but ici n'est pas d'essayer de tout enseigner, 

187
00:11:37,406 --> 00:11:40,988
c'est que vous repartiez avec une forte intuition pour les matrices inverses, 

188
00:11:40,988 --> 00:11:44,800
l'espace colonne et l'espace nul, et que ces intuitions rendent tout apprentissage 

189
00:11:44,800 --> 00:11:46,500
futur que vous faites plus fructueux.

190
00:11:47,660 --> 00:11:49,710
La prochaine vidéo, à la demande générale, sera une 

191
00:11:49,710 --> 00:11:51,880
brève note de bas de page sur les matrices non carrées.

192
00:11:51,880 --> 00:11:54,926
Ensuite, je vais vous donner mon point de vue sur les produits scalaires, 

193
00:11:54,926 --> 00:11:57,478
et quelque chose d'assez cool qui se produit lorsque vous les 

194
00:11:57,478 --> 00:11:59,660
visualisez à la lumière de transformations linéaires.

