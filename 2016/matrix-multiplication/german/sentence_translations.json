[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices.",
  "translatedText": "Hallo zusammen, wo wir zuletzt aufgehört haben, habe ich gezeigt, wie lineare Transformationen aussehen und wie man sie mithilfe von Matrizen darstellt.",
  "model": "google_nmt",
  "from_community_srt": "\"Meiner Erfahrung nach kann man Beweise, die auf Matrizen beruhen, halb so kurz gestalten, indem man die Matrizen weglässt.\" - Emil Artin Hallo zusammen! Das letzte mal habe ich euch gezeigt, wie lineare Transformationen aussehen und wie man sie mithilfe von Matrizen darstellen kann.",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video.",
  "translatedText": "Dies ist einen kurzen Rückblick wert, weil es wirklich wichtig ist, aber wenn es sich natürlich nach mehr als nur einem Rückblick anfühlt, gehen Sie zurück und schauen Sie sich das vollständige Video an.",
  "model": "google_nmt",
  "from_community_srt": "Da es so wichtig ist, sollten wir es auch nochmal schnell wiederholen. Ist dir jedoch in der Wiederholung einiges unbekannt, schaue dir lieber das letzte Video nochmal an.",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Technically speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed.",
  "translatedText": "Technisch gesehen sind lineare Transformationen Funktionen mit Vektoren als Eingaben und Vektoren als Ausgaben, aber ich habe letztes Mal gezeigt, wie wir sie uns visuell so vorstellen können, als würden sie sich im Raum bewegen, sodass die Gitterlinien parallel und gleichmäßig beabstandet bleiben und der Ursprung erhalten bleibt bleibt behoben.",
  "model": "google_nmt",
  "from_community_srt": "Genau genommen sind lineare Transformationen nichts anderes als Funktionen mit Vektoren als Ein- und Ausgangsgröße. Aber das letzte Mal habe ich gezeigt, wie man sie grafisch durch Verzerrung des Raumes darstellen kann, wobei alle Gitterlinien parallel und gleichmäßig verteilt bleiben; so, dass der Ursprung an seiner Position bleibt.",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat.",
  "translatedText": "Die wichtigste Erkenntnis war, dass eine lineare Transformation vollständig dadurch bestimmt wird, wo sie die Basisvektoren des Raums annimmt, was für zwei Dimensionen i-hat und j-hat bedeutet.",
  "model": "google_nmt",
  "from_community_srt": "Die wichtigste Schlussfolgerung war, dass eine lineare Transformation komplett durch die \"Lage\" der Basisvektoren im Raum bestimmt ist, was im 2-Dimensionalen Raum auf i-hat und j-hat zutrifft.",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors.",
  "translatedText": "Dies liegt daran, dass jeder andere Vektor als lineare Kombination dieser Basisvektoren beschrieben werden könnte.",
  "model": "google_nmt",
  "from_community_srt": "Das kommt daher, dass jeder Vektor durch eine lineare Kombination mit diesen Basisvektoren beschrieben werden kann.",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat.",
  "translatedText": "Ein Vektor mit den Koordinaten x, y ist x mal i-hat plus y mal j-hat.",
  "model": "google_nmt",
  "from_community_srt": "Ein Vektor mit den Koordinaten x und y ist x mal i-hat plus y mal j-hat.",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence.",
  "translatedText": "Nach der Transformation hat diese Eigenschaft, dass die Gitterlinien parallel und gleichmäßig verteilt bleiben, eine wunderbare Konsequenz.",
  "model": "google_nmt",
  "from_community_srt": "Nach der Ausführung der Transformation hat die Eigenschaft,",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat.",
  "translatedText": "Der Ort, an dem Ihr Vektor landet, ist x-mal die transformierte Version von i-hat plus y-mal die transformierte Version von j-hat.",
  "model": "google_nmt",
  "from_community_srt": "dass alle Linien parallel und gleichmäßig verteilt bleiben eine wundervolle Auswirkung: Der neue Ort für den Vektor ist dann x mal die transformierte Version von i-hat + y mal die transformierte Version von j-hat.",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat.",
  "translatedText": "Das heißt, wenn Sie die Koordinaten, an denen i-hat landet, und die Koordinaten, an denen j-hat landet, aufzeichnen, können Sie berechnen, dass ein Vektor, der bei x, y beginnt, auf x mal den neuen Koordinaten von i-hat plus y landen muss mal die neuen Koordinaten von j-hat.",
  "model": "google_nmt",
  "from_community_srt": "Wenn man also alle Koordinaten aufnimmt, auf denen jeweils i-hat und j-hat landen werden, kann man ausrechnen, dass ein Vektoren, der bei (x, y) beginnt, nun bei x mal der neuen Koordinaten von i-hat + y mal der neuen Koordinaten von j-hat landen muss.",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication.",
  "translatedText": "Die Konvention besteht darin, die Koordinaten der Landpunkte von i-hat und j-hat als Spalten einer Matrix aufzuzeichnen und diese Summe der skalierten Versionen dieser Spalten durch x und y als Matrix-Vektor-Multiplikation zu definieren.",
  "model": "google_nmt",
  "from_community_srt": "Die neuen Koordinaten von jeweils i-hat und j-hat schreibt man dann immer als eine Spalte einer Matrix und die Summe der mit x und y skalierten Versionen der Spalten nennt man eine Matrix-Vektor Multiplikation.",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector.",
  "translatedText": "Auf diese Weise stellt eine Matrix eine bestimmte lineare Transformation dar, und die Multiplikation einer Matrix mit einem Vektor bedeutet rechnerisch, diese Transformation auf diesen Vektor anzuwenden.",
  "model": "google_nmt",
  "from_community_srt": "Das heißt also, dass eine Matrix eine bestimmte lineare Transformation darstellt und diese mit einem Vektor zu multiplizieren ist dasselbe, wie diese Transformation an einem Vektor auszuführen.",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff.",
  "translatedText": "Okay, rekapitulieren wir noch mal, weiter zu den neuen Sachen.",
  "model": "google_nmt",
  "from_community_srt": "Ok, Wiederholung geschafft! Auf zu dem neuen Zeug!",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes, you find yourself wanting to describe the effects of applying one transformation and then another.",
  "translatedText": "Oft möchten Sie die Auswirkungen der Anwendung einer Transformation und dann einer anderen beschreiben.",
  "model": "google_nmt",
  "from_community_srt": "Oftmals wirst du dich fragen, was passiert, wenn man eine Transformation ausführt und im Anschluss eine weitere.",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear.",
  "translatedText": "Vielleicht möchten Sie beispielsweise beschreiben, was passiert, wenn Sie die Ebene zunächst um 90 Grad gegen den Uhrzeigersinn drehen und dann eine Scherung anwenden.",
  "model": "google_nmt",
  "from_community_srt": "Zum Beispiel: Vielleicht willst du wissen, was passiert, wenn du nach einer 90° Rotation gegen den Uhrzeigersinn eine Verformung (Shear) ausführst.",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear.",
  "translatedText": "Der Gesamteffekt hier ist von Anfang bis Ende eine weitere lineare Transformation, die sich von der Rotation und der Scherung unterscheidet.",
  "model": "google_nmt",
  "from_community_srt": "Letztendlich ist es vom Anfang bis zum Ende nur eine weitere lineare Transformation, bestehend aus einer Rotation und einer Verformung.",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied.",
  "translatedText": "Diese neue lineare Transformation wird allgemein als Zusammensetzung der beiden von uns angewendeten separaten Transformationen bezeichnet.",
  "model": "google_nmt",
  "from_community_srt": "Diese neue lineare Transformation wird die Komposition der beiden angewandten, separaten Transformationen genannt.",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat.",
  "translatedText": "Und wie jede lineare Transformation kann sie mit einer eigenen Matrix beschrieben werden, indem man i-hat und j-hat folgt.",
  "model": "google_nmt",
  "from_community_srt": "Und wie bei jeder anderen linearen Transformation kann man diese vollständig durch eine Matrix beschreiben, indem man sich an i-hat und j-hat orientiert.",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix.",
  "translatedText": "In diesem Beispiel ist der endgültige Landepunkt für i-hat nach beiden Transformationen 1,1, also machen wir das zur ersten Spalte einer Matrix.",
  "model": "google_nmt",
  "from_community_srt": "In diesem Beispiel ist der endgültige Landepunkt für i-hat nach beiden Transformationen bei (1, 1). Lass uns das also in die erste Spalte der Matrix schreiben.",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix.",
  "translatedText": "Ebenso landet j-hat letztendlich an der Position minus 1,0, also machen wir das zur zweiten Spalte der Matrix.",
  "model": "google_nmt",
  "from_community_srt": "Nach diesem Prinzip landet j-hat deshalb letztendlich bei (-1, 0); das wird dann die zweite Spalte der Matrix.",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones.",
  "translatedText": "Diese neue Matrix erfasst den Gesamteffekt der Anwendung einer Rotation und dann einer Scherung, jedoch als eine einzige Aktion und nicht als zwei aufeinanderfolgende.",
  "model": "google_nmt",
  "from_community_srt": "Diese neue Matrix zeigt nun den allgemeinen Effekt der Rotation, gefolgt von der Verformung, jedoch als eine einzelne Aktion,",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix.",
  "translatedText": "Hier ist eine Möglichkeit, über diese neue Matrix nachzudenken.",
  "model": "google_nmt",
  "from_community_srt": "statt zwei aufeinander folgende.",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix, then take whatever you get and multiply that on the left by the shear matrix.",
  "translatedText": "Wenn Sie einen Vektor nehmen und ihn durch die Rotation, also die Scherung, pumpen würden, besteht der lange Weg, um zu berechnen, wo er landet, darin, ihn zuerst links mit der Rotationsmatrix zu multiplizieren, dann das zu nehmen, was Sie erhalten, und das mit der Rotationsmatrix zu multiplizieren von der Schermatrix hinterlassen.",
  "model": "google_nmt",
  "from_community_srt": "So könnte man sich diese neue Matrix noch vorstellen: Wenn man einen Vektor nimmt und übt die Rotation und dann die Verformung aus, wäre der lange Weg zur Berechnung des Endpunktes zunächst die Multiplikation mit der Rotations-Matrix zur Linken; Dann multiplizierst du letztlich das Resultat noch mit der Verformungs-Matrix.",
  "n_reviews": 0,
  "start": 185.42,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector.",
  "translatedText": "Das ist numerisch gesehen, was es bedeutet, auf einen gegebenen Vektor eine Drehung und dann eine Scherung anzuwenden.",
  "model": "google_nmt",
  "from_community_srt": "Das ist, numerisch gesagt, was es bedeutet, die Rotation und dann Verformung an einen gegebenen Vektor anzuwenden.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action.",
  "translatedText": "Was auch immer Sie erhalten, sollte jedoch dasselbe sein wie die Anwendung dieser neuen Kompositionsmatrix, die wir gerade anhand desselben Vektors gefunden haben, unabhängig davon, welchen Vektor Sie ausgewählt haben, da diese neue Matrix den gleichen Gesamteffekt wie die Rotations- und dann die Scherwirkung erfassen soll.",
  "model": "google_nmt",
  "from_community_srt": "Aber: Das Ergebnis sollte immer dasselbe sein, wie bei der Verwendung der Kompositions-Matrix, die wir für diesen Vektor gefunden haben, egal, welchen Vektor man nimmt, zumal diese neue Matrix den gleichen Endeffekt haben muss wie die Rotation-Dann-Verformung Aktion.",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you?",
  "translatedText": "Basierend auf der Art und Weise, wie die Dinge hier niedergeschrieben werden, denke ich, dass es vernünftig ist, diese neue Matrix das Produkt der beiden ursprünglichen Matrizen zu nennen, nicht wahr?",
  "model": "google_nmt",
  "from_community_srt": "Basierend auf der vorliegenden Schreibweise, ist es angemessen, die neue Matrix das \"Produkt\" der beiden ursprünglichen Matrizen zu nennen.",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers.",
  "translatedText": "Wir können gleich darüber nachdenken, wie wir dieses Produkt allgemeiner berechnen können, aber es ist viel zu leicht, sich im Wald der Zahlen zu verlieren.",
  "model": "google_nmt",
  "from_community_srt": "Oder? Wie man dieses Produkt allgemein berechnet besprechen wir gleich, aber man verirrt sich viel zu leicht in diesem Zahlenwirrwarr.",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another.",
  "translatedText": "Denken Sie immer daran, dass die Multiplikation zweier Matrizen auf diese Weise die geometrische Bedeutung hat, eine Transformation und dann eine andere anzuwenden.",
  "model": "google_nmt",
  "from_community_srt": "Denk immer daran, dass die Multiplikation zweier Matrizen der geometrischen Anwendung einer Transformation ist,",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left.",
  "translatedText": "Was hier irgendwie seltsam ist, ist, dass wir hier von rechts nach links lesen.",
  "model": "google_nmt",
  "from_community_srt": "gefolgt von der anderen. Was hier aber ziemlich komisch ist,",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left.",
  "translatedText": "Sie wenden zuerst die Transformation an, die durch die Matrix auf der rechten Seite dargestellt wird, und wenden dann die Transformation an, die durch die Matrix auf der linken Seite dargestellt wird.",
  "model": "google_nmt",
  "from_community_srt": "ist die Leserichtung von Rechts nach Links; Man macht erst die Transformation, dargestellt durch die Matrix zur Rechten. Dann die Transformation, dargestellt durch die Matrix zur Linken.",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left.",
  "translatedText": "Dies liegt an der Funktionsnotation, da wir Funktionen links von Variablen schreiben. Daher müssen Sie jedes Mal, wenn Sie zwei Funktionen zusammensetzen, diese immer von rechts nach links lesen.",
  "model": "google_nmt",
  "from_community_srt": "Das kommt von der Funktions-Schreibweise, zumal wir Funktionen zur Linken von Variablen schreiben; Immer wenn du Funktionen zusammenstellst, musst du sie von Rechts nach Links lesen.",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us.",
  "translatedText": "Gute Nachrichten für die hebräischen Leser, schlechte Nachrichten für den Rest von uns.",
  "model": "google_nmt",
  "from_community_srt": "Gute Nachricht für die hebräischen Leser,",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example.",
  "translatedText": "Schauen wir uns ein anderes Beispiel an.",
  "model": "google_nmt",
  "from_community_srt": "schlechte Nachricht für den Rest von uns. Lass uns ein anderes Beispiel betrachten.",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.",
  "translatedText": "Nehmen Sie die Matrix mit den Spalten 1,1 und minus 2,0, deren Transformation so aussieht.",
  "model": "google_nmt",
  "from_community_srt": "Nimm eine Matrix mit den Spalten (1, 1) und (-2, 0), deren Transformation so aussieht,",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it M1.",
  "translatedText": "Und nennen wir es M1.",
  "model": "google_nmt",
  "from_community_srt": "und wir nennen sie M1.",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.",
  "translatedText": "Als nächstes nehmen wir die Matrix mit den Spalten 0,1 und 2,0, deren Transformation so aussieht.",
  "model": "google_nmt",
  "from_community_srt": "Jetzt nehmen wir eine Matrix mit den Spalten (0, 1) und (2,",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy M2.",
  "translatedText": "Und nennen wir diesen Kerl M2.",
  "model": "google_nmt",
  "from_community_srt": "0), deren Transformation so aussieht, und wir nennen diese nun M2.",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying M1 then M2 gives us a new transformation, so let's find its matrix.",
  "translatedText": "Der Gesamteffekt der Anwendung von M1 und dann M2 führt zu einer neuen Transformation. Finden wir also deren Matrix.",
  "model": "google_nmt",
  "from_community_srt": "Nach der Anwendung von M1 und dann M2 haben wir eine neue Transformation.",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix.",
  "translatedText": "Aber dieses Mal wollen wir sehen, ob wir es schaffen können, ohne die Animationen anzusehen und stattdessen einfach die numerischen Einträge in jeder Matrix zu verwenden.",
  "model": "google_nmt",
  "from_community_srt": "Lass uns deren Matrix herausfinden! Aber dieses mal schauen wir nicht auf die Animation, sondern benutzen nur die Zahlen in den einzelnen Matrizen.",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes.",
  "translatedText": "Zuerst müssen wir herausfinden, wohin i-hat geht.",
  "model": "google_nmt",
  "from_community_srt": "Zuerst müssen wir wissen,",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying M1, the new coordinates of i-hat, by definition, are given by that first column of M1, namely 1,1.",
  "translatedText": "Nach der Anwendung von M1 sind die neuen Koordinaten von i-hat per Definition durch die erste Spalte von M1 gegeben, nämlich 1,1.",
  "model": "google_nmt",
  "from_community_srt": "wo i-hat hingeht. Nach der Verwendung von M1 sind die neuen Koordinaten von i-hat - so die Definition - durch die erste Spalte von M1 gegeben; und zwar (1,",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying M2, multiply the matrix for M2 by that vector 1,1.",
  "translatedText": "Um zu sehen, was nach der Anwendung von M2 passiert, multiplizieren Sie die Matrix für M2 mit diesem Vektor 1,1.",
  "model": "google_nmt",
  "from_community_srt": "1). Um zu sehen, was nach M2 passiert, muss man die Matrix M2 mit dem Vektor (1,",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1.",
  "translatedText": "Wenn Sie es so ausrechnen, wie ich es im letzten Video beschrieben habe, erhalten Sie den Vektor 2,1.",
  "model": "google_nmt",
  "from_community_srt": "1) multiplizieren. Nach der Berechnung, wie sie im letzten Video erklärt wurde, bekommt man den Vektor (2,",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix.",
  "translatedText": "Dies ist die erste Spalte der Kompositionsmatrix.",
  "model": "google_nmt",
  "from_community_srt": "1). Dieser wird die erste Spalte der Kompositions-Matrix.",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of M1 tells us that it first lands on negative 2,0.",
  "translatedText": "Um j-hat zu folgen, sagt uns die zweite Spalte von M1, dass es zuerst bei minus 2,0 landet.",
  "model": "google_nmt",
  "from_community_srt": "Gleichermaßen, für j-hat, beschreibt die zweite Spalte von M1, dass es zunächst auf (-2,",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply M2 to that vector, you can work out the matrix-vector product to get 0, negative 2, which becomes the second column of our composition matrix.",
  "translatedText": "Wenn wir dann M2 auf diesen Vektor anwenden, können Sie das Matrix-Vektor-Produkt berechnen, um 0, minus 2 zu erhalten, was zur zweiten Spalte unserer Kompositionsmatrix wird.",
  "model": "google_nmt",
  "from_community_srt": "0) landet. Wenn wir dann M2 an diesen Vektor anwenden, ergibt das Matrix-Vektor Produkt dieser (0, -2), was in die zweite Spalte unserer Kompositions-Matrix geschrieben wird.",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices.",
  "translatedText": "Lassen Sie mich den gleichen Prozess noch einmal durchgehen, aber dieses Mal werde ich Variableneinträge in jeder Matrix zeigen, nur um zu zeigen, dass die gleiche Argumentation für alle Matrizen funktioniert.",
  "model": "google_nmt",
  "from_community_srt": "Lass uns diesen Prozess nochmal durchgehen, aber dieses Mal nehmen wir variable Einträge in jeder Matrix, um zu zeigen, dass diese Erklärung auf alle Matrizen zutrifft.",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way.",
  "translatedText": "Dies ist symbollastiger und erfordert etwas mehr Platz, dürfte aber für jeden, der schon einmal die Matrixmultiplikation auf die eher auswendig gelernte Art und Weise gelernt hat, ziemlich zufriedenstellend sein.",
  "model": "google_nmt",
  "from_community_srt": "Das ist viel Zeichenlastiger und benötigt mehr Platz, jedoch wird es ziemlich befriedigend sein für diejenigen, die zuvor schon Matrix Multiplikation in \"geschriebener\" Form gelernt haben.",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands.",
  "translatedText": "Um zu verfolgen, wohin i-hat geht, schauen Sie sich zunächst die erste Spalte der Matrix auf der rechten Seite an, da i-hat dort zunächst landet.",
  "model": "google_nmt",
  "from_community_srt": "Um i-hat zu folgen, beginnen wir mit der ersten Spalte von der rechten Matrix, zumal das der End-Landepunkt für i-hat ist.",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation.",
  "translatedText": "Durch Multiplizieren dieser Spalte mit der Matrix auf der linken Seite können Sie feststellen, wo die Zwischenversion von i-hat nach Anwendung der zweiten Transformation landet.",
  "model": "google_nmt",
  "from_community_srt": "Die Multiplikation dieser Spalte mit der Matrix zur Linken zeigt, wo die zwischenzeitliche Version von i-hat landet, nachdem man die zweite Transformation angewendet hat.",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix.",
  "translatedText": "Die erste Spalte der Kompositionsmatrix entspricht also immer der linken Matrix mal der ersten Spalte der rechten Matrix.",
  "model": "google_nmt",
  "from_community_srt": "Die erste Spalte der Kompositions-Matrix ist also immer gleich der linken Matrix mal der ersten Spalte der rechten Matrix.",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix.",
  "translatedText": "Ebenso landet j-hat zunächst immer in der zweiten Spalte der rechten Matrix.",
  "model": "google_nmt",
  "from_community_srt": "Gleichermaßen wird j-hat anfangs immer auf der zweiten Spalte der rechten Matrix landen.",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix.",
  "translatedText": "Wenn man also die linke Matrix mit dieser zweiten Spalte multipliziert, erhält man ihre endgültige Position, und daher ist dies die zweite Spalte der Kompositionsmatrix.",
  "model": "google_nmt",
  "from_community_srt": "Nach der Multiplikation mit der linken Matrix bekommt man also seine finale Position und das ist folglich die zweite Spalte der Kompositions-Matrix.",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to help remember it.",
  "translatedText": "Beachten Sie, dass es hier viele Symbole gibt, und es ist üblich, diese Formel als etwas zum Auswendiglernen beizubringen, zusammen mit einem bestimmten algorithmischen Prozess, der dabei hilft, sich daran zu erinnern.",
  "model": "google_nmt",
  "from_community_srt": "Man sieht, dass hier viele Zeichen sind und es ist normal, diese Formel zum auswendig-lernen vorgelegt zu bekommen. Zusammen mit einem algorithmischen Prozess um es sich besser zu merken.",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another.",
  "translatedText": "Aber ich denke wirklich, dass man sich, bevor man sich diesen Prozess einprägt, angewöhnen sollte, darüber nachzudenken, was die Matrixmultiplikation wirklich darstellt, indem man eine Transformation nach der anderen anwendet.",
  "model": "google_nmt",
  "from_community_srt": "Aber bevor man diesen Prozess auswendig lernt, sollte man darüber nachdenken, was eine Matrix Multiplikation eigentlich beschreibt: Anwendung einer Transformation nach der anderen.",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand.",
  "translatedText": "Vertrauen Sie mir, dies wird Ihnen einen viel besseren konzeptionellen Rahmen geben, der das Verständnis der Eigenschaften der Matrixmultiplikation viel einfacher macht.",
  "model": "google_nmt",
  "from_community_srt": "Glaub mir, das gibt dir eine viel bessere Gedankenstütze, um die Eigenschaften von Matrix Multiplikation einfacher zu verstehen.",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question.",
  "translatedText": "Hier ist zum Beispiel eine Frage.",
  "model": "google_nmt",
  "from_community_srt": "Zum Beispiel,",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them?",
  "translatedText": "Spielt es eine Rolle, in welche Reihenfolge wir die beiden Matrizen bringen, wenn wir sie multiplizieren?",
  "model": "google_nmt",
  "from_community_srt": "hier ist eine Frage: Macht es einen Unterschied, in welcher Reihenfolge wir die Matrizen multiplizieren? Ok,",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier.",
  "translatedText": "Nun, lassen Sie uns ein einfaches Beispiel durchdenken, wie das von vorhin.",
  "model": "google_nmt",
  "from_community_srt": "lass uns an ein einfaches Beispiel denken, wie das von zuvor:",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smooshes j-hat over to the right, and a 90 degree rotation.",
  "translatedText": "Nehmen Sie eine Schere, die den I-Hat fixiert und ihn nach rechts glättet, und eine 90-Grad-Drehung.",
  "model": "google_nmt",
  "from_community_srt": "Mach eine Verformung, welche i-hat fixiert und j-hat nach rechts verzerrt und dann eine 90°-Rotation.",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1.",
  "translatedText": "Wenn Sie zuerst die Scherung und dann die Drehung durchführen, können wir sehen, dass i-hat bei 0,1 und j-hat bei minus 1,1 landet.",
  "model": "google_nmt",
  "from_community_srt": "Wenn man erst verformt und dann rotiert, ist i-hat nun auf (0, 1) und j-hat auf (-1,",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together.",
  "translatedText": "Beide zeigen im Allgemeinen nahe beieinander.",
  "model": "google_nmt",
  "from_community_srt": "1); beide sind in etwa nah bei einander.",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing, you know, farther apart.",
  "translatedText": "Wenn Sie zuerst drehen und dann scheren, endet i-hat bei 1,1 und j-hat ist bei minus 1,0 in eine andere Richtung verschoben, und sie zeigen, wissen Sie, weiter auseinander.",
  "model": "google_nmt",
  "from_community_srt": "Wenn man erst rotiert und dann verformt, ist i-hat dann auf (1, 1) und j-hat auf (-1, 0); einer völlig anderen Richtung. und sie zeigen, naja,",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently, order totally does matter.",
  "translatedText": "Der Gesamteffekt ist hier eindeutig anders, daher ist die Reihenfolge offensichtlich völlig wichtig.",
  "model": "google_nmt",
  "from_community_srt": "viel weiter von einander weg. Der Endeffekt ist hier eindeutig anders, also hat die Reihenfolge offensichtlich eine Auswirkung.",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice, by thinking in terms of transformations, that's the kind of thing that you can do in your head by visualizing.",
  "translatedText": "Beachten Sie, dass Sie, wenn Sie in Transformationen denken, solche Dinge durch Visualisierung in Ihrem Kopf bewirken können.",
  "model": "google_nmt",
  "from_community_srt": "Merk dir, dass die Transformation komplett in deinem Kopf abspielen kann, um sie zu visualisieren.",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary.",
  "translatedText": "Keine Matrixmultiplikation erforderlich.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative.",
  "translatedText": "Ich erinnere mich, als ich mich zum ersten Mal mit linearer Algebra befasste, gab es diese eine Hausaufgabe, bei der wir beweisen mussten, dass die Matrixmultiplikation assoziativ ist.",
  "model": "google_nmt",
  "from_community_srt": "Man braucht keine Matrix Multiplikation. Ich weiß noch, als ich das erste mal lineare Algebra hatte: Es gab ein Hausaufgaben-Problem, bei welchem wir beweisen mussten, dass Matrix Multiplikationen assoziativ sind.",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left.",
  "translatedText": "Das heißt, wenn Sie drei Matrizen haben, A, B und C, und Sie sie alle miteinander multiplizieren, sollte es keine Rolle spielen, ob Sie zuerst A mal B berechnen und dann das Ergebnis mit C multiplizieren oder ob Sie zuerst B mal multiplizieren C, dann multiplizieren Sie das Ergebnis links mit A.",
  "model": "google_nmt",
  "from_community_srt": "Das heißt, wenn man drei Matrizen A, B, und C hat und man alle miteinander multipliziert, ist es egal, ob man zuerst A mal B und dann das Ergebnis mal C berechnet oder ob man zuert B mal C multipliziert und das Ergebnis dann mal A nimmt.",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses.",
  "translatedText": "Mit anderen Worten: Es spielt keine Rolle, wo Sie die Klammern setzen.",
  "model": "google_nmt",
  "from_community_srt": "In anderen Worten: Es ist egal,",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter.",
  "translatedText": "Wenn man nun versucht, das numerisch durchzurechnen, wie ich es damals getan habe, ist es schrecklich, einfach schrecklich und im Übrigen auch nicht aufschlussreich.",
  "model": "google_nmt",
  "from_community_srt": "wo die Klammern stehen. Wenn du das nun versuchst, numerisch zu erarbeiten, wie ich es zuvor gemacht habe: es ist schrecklich, einfach schrecklich und  unnütz in diesem Fall.",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial.",
  "translatedText": "Aber wenn man sich die Matrixmultiplikation als die Anwendung einer Transformation nach der anderen vorstellt, ist diese Eigenschaft einfach trivial.",
  "model": "google_nmt",
  "from_community_srt": "Aber wenn man Matrix Multiplikation als angewandte Transformation nach der anderen sieht, ist diese Eigenschaft nur belanglos.",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why?",
  "translatedText": "Können Sie verstehen, warum?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C, then B, then A, it's the same as applying C, then B, then A.",
  "translatedText": "Darin heißt es: Wenn Sie zuerst C, dann B und dann A anwenden, ist das dasselbe, als würden Sie C, dann B und dann A anwenden.",
  "model": "google_nmt",
  "from_community_srt": "Erkennst du warum? Da heißt: Wenn man erst C anwendet, dann B und dann A, ist es das gleiche wie C, dann B,",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove.",
  "translatedText": "Ich meine, es gibt nichts zu beweisen.",
  "model": "google_nmt",
  "from_community_srt": "dann A.",
  "n_reviews": 0,
  "start": 552.82,
  "end": 554.38
 },
 {
  "input": "You're just applying the same three things one after the other, all in the same order.",
  "translatedText": "Sie wenden einfach die gleichen drei Dinge nacheinander an, alle in der gleichen Reihenfolge.",
  "model": "google_nmt",
  "from_community_srt": "Eigentlich muss man da nichts beweisen, man wendet einfach die gleichen drei Dinge eins nach dem anderen an.",
  "n_reviews": 0,
  "start": 554.54,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not.",
  "translatedText": "Das mag sich wie Betrug anfühlen, ist es aber nicht.",
  "model": "google_nmt",
  "from_community_srt": "In der gleichen Reihenfolge. Es kommt einem vielleicht wie Schummeln vor.",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative, and even better than that, it's a good explanation for why that property should be true.",
  "translatedText": "Dies ist ein wahrer Beweis dafür, dass die Matrixmultiplikation assoziativ ist, und noch besser: Es ist eine gute Erklärung dafür, warum diese Eigenschaft wahr sein sollte.",
  "model": "google_nmt",
  "from_community_srt": "Aber ist es nicht! Das ist ein echter Beweis dafür, dass Matrix Multiplikation assoziativ ist und darüber hinaus ist es eine gute Erklärung dafür, warum es wahr sein muss.",
  "n_reviews": 0,
  "start": 561.54,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically.",
  "translatedText": "Ich ermutige Sie wirklich, mehr mit dieser Idee herumzuspielen, sich zwei verschiedene Transformationen vorzustellen, darüber nachzudenken, was passiert, wenn Sie eine nach der anderen anwenden, und dann das Matrixprodukt numerisch zu berechnen.",
  "model": "google_nmt",
  "from_community_srt": "Ich empfehle euch wirklich, mit dieser Idee weiter herumzuspielen: Vorstellung zweier verschiedener Transformationen; Nachdenken darüber, was nach der Anwendung nacheinander passiert; Und dann das Matrix Produkt numerisch ausarbeiten.",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in.",
  "translatedText": "Vertrauen Sie mir, das ist die Art von Spielzeit, bei der die Idee wirklich greift.",
  "model": "google_nmt",
  "from_community_srt": "Vertraue mir, diese Art von Spielchen machen die Idee erst richtig verständlich.",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions.",
  "translatedText": "Im nächsten Video werde ich über die Erweiterung dieser Ideen über nur zwei Dimensionen hinaus sprechen.",
  "model": "google_nmt",
  "from_community_srt": "Im nächsten Video werde ich beginnen,",
  "n_reviews": 0,
  "start": 587.2,
  "end": 592.18
 }
]