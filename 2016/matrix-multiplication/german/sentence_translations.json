[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices.",
  "translatedText": "Hallo zusammen, wo wir das letzte Mal aufgehört haben, habe ich gezeigt, wie lineare Transformationen aussehen und wie man sie mit Matrizen darstellen kann.",
  "model": "DeepL",
  "from_community_srt": "\"Meiner Erfahrung nach kann man Beweise, die auf Matrizen beruhen, halb so kurz gestalten, indem man die Matrizen weglässt.\" - Emil Artin Hallo zusammen! Das letzte mal habe ich euch gezeigt, wie lineare Transformationen aussehen und wie man sie mithilfe von Matrizen darstellen kann.",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video.",
  "translatedText": "Das ist eine kurze Zusammenfassung wert, weil es einfach sehr wichtig ist. Aber wenn du mehr als nur eine Zusammenfassung möchtest, kannst du dir natürlich das ganze Video ansehen.",
  "model": "DeepL",
  "from_community_srt": "Da es so wichtig ist, sollten wir es auch nochmal schnell wiederholen. Ist dir jedoch in der Wiederholung einiges unbekannt, schaue dir lieber das letzte Video nochmal an.",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Technically speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed.",
  "translatedText": "Technisch gesehen sind lineare Transformationen Funktionen mit Vektoren als Eingänge und Vektoren als Ausgänge, aber ich habe letztes Mal gezeigt, wie wir sie uns visuell vorstellen können, indem wir sie so durch den Raum schieben, dass die Gitterlinien parallel und in gleichmäßigen Abständen bleiben und der Ursprung fest bleibt.",
  "model": "DeepL",
  "from_community_srt": "Genau genommen sind lineare Transformationen nichts anderes als Funktionen mit Vektoren als Ein- und Ausgangsgröße. Aber das letzte Mal habe ich gezeigt, wie man sie grafisch durch Verzerrung des Raumes darstellen kann, wobei alle Gitterlinien parallel und gleichmäßig verteilt bleiben; so, dass der Ursprung an seiner Position bleibt.",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat.",
  "translatedText": "Die wichtigste Erkenntnis war, dass eine lineare Transformation vollständig davon abhängt, wo sie die Basisvektoren des Raums nimmt, was bei zwei Dimensionen i-hat und j-hat bedeutet.",
  "model": "DeepL",
  "from_community_srt": "Die wichtigste Schlussfolgerung war, dass eine lineare Transformation komplett durch die \"Lage\" der Basisvektoren im Raum bestimmt ist, was im 2-Dimensionalen Raum auf i-hat und j-hat zutrifft.",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors.",
  "translatedText": "Das liegt daran, dass jeder andere Vektor als Linearkombination dieser Basisvektoren beschrieben werden kann.",
  "model": "DeepL",
  "from_community_srt": "Das kommt daher, dass jeder Vektor durch eine lineare Kombination mit diesen Basisvektoren beschrieben werden kann.",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat.",
  "translatedText": "Ein Vektor mit den Koordinaten x, y ist x mal i-hat plus y mal j-hat.",
  "model": "DeepL",
  "from_community_srt": "Ein Vektor mit den Koordinaten x und y ist x mal i-hat plus y mal j-hat.",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence.",
  "translatedText": "Nach der Umwandlung hat diese Eigenschaft, dass die Gitterlinien parallel und gleichmäßig verteilt bleiben, eine wunderbare Folge.",
  "model": "DeepL",
  "from_community_srt": "Nach der Ausführung der Transformation hat die Eigenschaft,",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat.",
  "translatedText": "Der Ort, an dem dein Vektor landet, ist x mal die transformierte Version von i-hat plus y mal die transformierte Version von j-hat.",
  "model": "DeepL",
  "from_community_srt": "dass alle Linien parallel und gleichmäßig verteilt bleiben eine wundervolle Auswirkung: Der neue Ort für den Vektor ist dann x mal die transformierte Version von i-hat + y mal die transformierte Version von j-hat.",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat.",
  "translatedText": "Das heißt, wenn du die Koordinaten aufzeichnest, an denen i-hat und j-hat landet, kannst du berechnen, dass ein Vektor, der bei x, y beginnt, auf x mal den neuen Koordinaten von i-hat plus y mal den neuen Koordinaten von j-hat landen muss.",
  "model": "DeepL",
  "from_community_srt": "Wenn man also alle Koordinaten aufnimmt, auf denen jeweils i-hat und j-hat landen werden, kann man ausrechnen, dass ein Vektoren, der bei (x, y) beginnt, nun bei x mal der neuen Koordinaten von i-hat + y mal der neuen Koordinaten von j-hat landen muss.",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication.",
  "translatedText": "Die Konvention besteht darin, die Koordinaten, auf denen i-hat und j-hat landen, als Spalten einer Matrix aufzuzeichnen und die Summe der skalierten Versionen dieser Spalten mit x und y als Matrix-Vektor-Multiplikation zu definieren.",
  "model": "DeepL",
  "from_community_srt": "Die neuen Koordinaten von jeweils i-hat und j-hat schreibt man dann immer als eine Spalte einer Matrix und die Summe der mit x und y skalierten Versionen der Spalten nennt man eine Matrix-Vektor Multiplikation.",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector.",
  "translatedText": "Auf diese Weise stellt eine Matrix eine bestimmte lineare Transformation dar, und die Multiplikation einer Matrix mit einem Vektor bedeutet rechnerisch, dass diese Transformation auf diesen Vektor angewendet wird.",
  "model": "DeepL",
  "from_community_srt": "Das heißt also, dass eine Matrix eine bestimmte lineare Transformation darstellt und diese mit einem Vektor zu multiplizieren ist dasselbe, wie diese Transformation an einem Vektor auszuführen.",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff.",
  "translatedText": "Okay, das war's mit der Zusammenfassung, jetzt zu den neuen Sachen.",
  "model": "DeepL",
  "from_community_srt": "Ok, Wiederholung geschafft! Auf zu dem neuen Zeug!",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes, you find yourself wanting to describe the effects of applying one transformation and then another.",
  "translatedText": "Oft ertappst du dich dabei, dass du die Auswirkungen der Anwendung einer Transformation und dann einer anderen beschreiben willst.",
  "model": "DeepL",
  "from_community_srt": "Oftmals wirst du dich fragen, was passiert, wenn man eine Transformation ausführt und im Anschluss eine weitere.",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear.",
  "translatedText": "Vielleicht willst du zum Beispiel beschreiben, was passiert, wenn du die Ebene zuerst um 90 Grad gegen den Uhrzeigersinn drehst und dann eine Scherung anwendest.",
  "model": "DeepL",
  "from_community_srt": "Zum Beispiel: Vielleicht willst du wissen, was passiert, wenn du nach einer 90° Rotation gegen den Uhrzeigersinn eine Verformung (Shear) ausführst.",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear.",
  "translatedText": "Der Gesamteffekt ist hier von Anfang bis Ende eine weitere lineare Transformation, die sich von der Drehung und der Scherung unterscheidet.",
  "model": "DeepL",
  "from_community_srt": "Letztendlich ist es vom Anfang bis zum Ende nur eine weitere lineare Transformation, bestehend aus einer Rotation und einer Verformung.",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied.",
  "translatedText": "Diese neue lineare Transformation wird gemeinhin als Komposition der beiden separaten Transformationen bezeichnet, die wir angewendet haben.",
  "model": "DeepL",
  "from_community_srt": "Diese neue lineare Transformation wird die Komposition der beiden angewandten, separaten Transformationen genannt.",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat.",
  "translatedText": "Und wie jede lineare Transformation kann sie mit einer eigenen Matrix beschrieben werden, indem man i-hat und j-hat.",
  "model": "DeepL",
  "from_community_srt": "Und wie bei jeder anderen linearen Transformation kann man diese vollständig durch eine Matrix beschreiben, indem man sich an i-hat und j-hat orientiert.",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix.",
  "translatedText": "In diesem Beispiel ist der endgültige Landepunkt für i-hat nach beiden Transformationen 1,1, also machen wir das zur ersten Spalte einer Matrix.",
  "model": "DeepL",
  "from_community_srt": "In diesem Beispiel ist der endgültige Landepunkt für i-hat nach beiden Transformationen bei (1, 1). Lass uns das also in die erste Spalte der Matrix schreiben.",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix.",
  "translatedText": "Auch j-hat landet am Ende an der Stelle negativ 1,0, also machen wir das zur zweiten Spalte der Matrix.",
  "model": "DeepL",
  "from_community_srt": "Nach diesem Prinzip landet j-hat deshalb letztendlich bei (-1, 0); das wird dann die zweite Spalte der Matrix.",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones.",
  "translatedText": "Diese neue Matrix erfasst den Gesamteffekt der Anwendung einer Drehung und einer Scherung, aber als eine einzige Aktion und nicht als zwei aufeinanderfolgende Aktionen.",
  "model": "DeepL",
  "from_community_srt": "Diese neue Matrix zeigt nun den allgemeinen Effekt der Rotation, gefolgt von der Verformung, jedoch als eine einzelne Aktion,",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix.",
  "translatedText": "Hier ist eine Möglichkeit, über diese neue Matrix nachzudenken.",
  "model": "DeepL",
  "from_community_srt": "statt zwei aufeinander folgende.",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix, then take whatever you get and multiply that on the left by the shear matrix.",
  "translatedText": "Wenn du einen Vektor nimmst und ihn durch die Rotation und dann die Scherung pumpen würdest, ist der lange Weg, um zu berechnen, wo er am Ende landet, dass du ihn zuerst links mit der Rotationsmatrix multiplizierst, dann nimmst du das, was du bekommst und multiplizierst es links mit der Schermatrix.",
  "model": "DeepL",
  "from_community_srt": "So könnte man sich diese neue Matrix noch vorstellen: Wenn man einen Vektor nimmt und übt die Rotation und dann die Verformung aus, wäre der lange Weg zur Berechnung des Endpunktes zunächst die Multiplikation mit der Rotations-Matrix zur Linken; Dann multiplizierst du letztlich das Resultat noch mit der Verformungs-Matrix.",
  "n_reviews": 0,
  "start": 185.42,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector.",
  "translatedText": "Numerisch ausgedrückt bedeutet das, dass auf einen bestimmten Vektor erst eine Drehung und dann eine Scherung angewendet wird.",
  "model": "DeepL",
  "from_community_srt": "Das ist, numerisch gesagt, was es bedeutet, die Rotation und dann Verformung an einen gegebenen Vektor anzuwenden.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action.",
  "translatedText": "Aber was auch immer du bekommst, sollte dasselbe sein, wie wenn du die neue Kompositionsmatrix, die wir gerade gefunden haben, auf denselben Vektor anwendest, egal welchen Vektor du wählst, da diese neue Matrix denselben Gesamteffekt wie die Rotation und dann die Scherung haben soll.",
  "model": "DeepL",
  "from_community_srt": "Aber: Das Ergebnis sollte immer dasselbe sein, wie bei der Verwendung der Kompositions-Matrix, die wir für diesen Vektor gefunden haben, egal, welchen Vektor man nimmt, zumal diese neue Matrix den gleichen Endeffekt haben muss wie die Rotation-Dann-Verformung Aktion.",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you?",
  "translatedText": "So wie die Dinge hier niedergeschrieben sind, denke ich, dass es vernünftig ist, diese neue Matrix als Produkt der beiden ursprünglichen Matrizen zu bezeichnen, meinst du nicht?",
  "model": "DeepL",
  "from_community_srt": "Basierend auf der vorliegenden Schreibweise, ist es angemessen, die neue Matrix das \"Produkt\" der beiden ursprünglichen Matrizen zu nennen.",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers.",
  "translatedText": "Wir können gleich darüber nachdenken, wie man dieses Produkt allgemeiner berechnen kann, aber es ist viel zu einfach, sich im Wald der Zahlen zu verlieren.",
  "model": "DeepL",
  "from_community_srt": "Oder? Wie man dieses Produkt allgemein berechnet besprechen wir gleich, aber man verirrt sich viel zu leicht in diesem Zahlenwirrwarr.",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another.",
  "translatedText": "Denke immer daran, dass die Multiplikation zweier Matrizen die geometrische Bedeutung hat, dass eine Transformation auf die andere folgt.",
  "model": "DeepL",
  "from_community_srt": "Denk immer daran, dass die Multiplikation zweier Matrizen der geometrischen Anwendung einer Transformation ist,",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left.",
  "translatedText": "Eine Sache, die hier etwas seltsam ist, ist, dass wir von rechts nach links lesen.",
  "model": "DeepL",
  "from_community_srt": "gefolgt von der anderen. Was hier aber ziemlich komisch ist,",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left.",
  "translatedText": "Du wendest zuerst die Transformation an, die durch die Matrix auf der rechten Seite dargestellt wird, und dann die Transformation, die durch die Matrix auf der linken Seite dargestellt wird.",
  "model": "DeepL",
  "from_community_srt": "ist die Leserichtung von Rechts nach Links; Man macht erst die Transformation, dargestellt durch die Matrix zur Rechten. Dann die Transformation, dargestellt durch die Matrix zur Linken.",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left.",
  "translatedText": "Das liegt an der Funktionsnotation, denn wir schreiben Funktionen links von den Variablen. Wenn du also zwei Funktionen zusammensetzt, musst du sie immer von rechts nach links lesen.",
  "model": "DeepL",
  "from_community_srt": "Das kommt von der Funktions-Schreibweise, zumal wir Funktionen zur Linken von Variablen schreiben; Immer wenn du Funktionen zusammenstellst, musst du sie von Rechts nach Links lesen.",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us.",
  "translatedText": "Gute Nachrichten für die hebräischen Leser, schlechte Nachrichten für den Rest von uns.",
  "model": "DeepL",
  "from_community_srt": "Gute Nachricht für die hebräischen Leser,",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example.",
  "translatedText": "Schauen wir uns ein anderes Beispiel an.",
  "model": "DeepL",
  "from_community_srt": "schlechte Nachricht für den Rest von uns. Lass uns ein anderes Beispiel betrachten.",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.",
  "translatedText": "Nimm die Matrix mit den Spalten 1,1 und dem Negativ 2,0, deren Transformation wie folgt aussieht.",
  "model": "DeepL",
  "from_community_srt": "Nimm eine Matrix mit den Spalten (1, 1) und (-2, 0), deren Transformation so aussieht,",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it M1.",
  "translatedText": "Nennen wir sie M1.",
  "model": "DeepL",
  "from_community_srt": "und wir nennen sie M1.",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.",
  "translatedText": "Als Nächstes nimmst du die Matrix mit den Spalten 0,1 und 2,0, deren Transformation wie folgt aussieht.",
  "model": "DeepL",
  "from_community_srt": "Jetzt nehmen wir eine Matrix mit den Spalten (0, 1) und (2,",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy M2.",
  "translatedText": "Und den nennen wir M2.",
  "model": "DeepL",
  "from_community_srt": "0), deren Transformation so aussieht, und wir nennen diese nun M2.",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying M1 then M2 gives us a new transformation, so let's find its matrix.",
  "translatedText": "Der Gesamteffekt der Anwendung von M1 und M2 ergibt eine neue Transformation, also lass uns ihre Matrix finden.",
  "model": "DeepL",
  "from_community_srt": "Nach der Anwendung von M1 und dann M2 haben wir eine neue Transformation.",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix.",
  "translatedText": "Aber dieses Mal wollen wir sehen, ob wir es schaffen, ohne die Animationen anzusehen und stattdessen nur die numerischen Einträge in jeder Matrix zu verwenden.",
  "model": "DeepL",
  "from_community_srt": "Lass uns deren Matrix herausfinden! Aber dieses mal schauen wir nicht auf die Animation, sondern benutzen nur die Zahlen in den einzelnen Matrizen.",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes.",
  "translatedText": "Zuerst müssen wir herausfinden, wo i-hat hingehört.",
  "model": "DeepL",
  "from_community_srt": "Zuerst müssen wir wissen,",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying M1, the new coordinates of i-hat, by definition, are given by that first column of M1, namely 1,1.",
  "translatedText": "Nach der Anwendung von M1 sind die neuen Koordinaten von i-hat per Definition durch die erste Spalte von M1 gegeben, nämlich 1,1.",
  "model": "DeepL",
  "from_community_srt": "wo i-hat hingeht. Nach der Verwendung von M1 sind die neuen Koordinaten von i-hat - so die Definition - durch die erste Spalte von M1 gegeben; und zwar (1,",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying M2, multiply the matrix for M2 by that vector 1,1.",
  "translatedText": "Um zu sehen, was nach der Anwendung von M2 passiert, multipliziere die Matrix für M2 mit dem Vektor 1,1.",
  "model": "DeepL",
  "from_community_srt": "1). Um zu sehen, was nach M2 passiert, muss man die Matrix M2 mit dem Vektor (1,",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1.",
  "translatedText": "Wenn du es so machst, wie ich es im letzten Video beschrieben habe, bekommst du den Vektor 2,1.",
  "model": "DeepL",
  "from_community_srt": "1) multiplizieren. Nach der Berechnung, wie sie im letzten Video erklärt wurde, bekommt man den Vektor (2,",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix.",
  "translatedText": "Dies wird die erste Spalte der Zusammensetzungsmatrix sein.",
  "model": "DeepL",
  "from_community_srt": "1). Dieser wird die erste Spalte der Kompositions-Matrix.",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of M1 tells us that it first lands on negative 2,0.",
  "translatedText": "Um j-hat zu folgen, sagt uns die zweite Spalte von M1, dass er zuerst auf dem negativen Wert 2,0 landet.",
  "model": "DeepL",
  "from_community_srt": "Gleichermaßen, für j-hat, beschreibt die zweite Spalte von M1, dass es zunächst auf (-2,",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply M2 to that vector, you can work out the matrix-vector product to get 0, negative 2, which becomes the second column of our composition matrix.",
  "translatedText": "Wenn wir dann M2 auf diesen Vektor anwenden, kannst du das Matrix-Vektor-Produkt berechnen und erhältst 0, negativ 2, was die zweite Spalte unserer Kompositionsmatrix ist.",
  "model": "DeepL",
  "from_community_srt": "0) landet. Wenn wir dann M2 an diesen Vektor anwenden, ergibt das Matrix-Vektor Produkt dieser (0, -2), was in die zweite Spalte unserer Kompositions-Matrix geschrieben wird.",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices.",
  "translatedText": "Lass mich den gleichen Prozess noch einmal durchgehen, aber dieses Mal zeige ich die variablen Einträge in jeder Matrix, nur um zu zeigen, dass die gleiche Argumentation für alle Matrizen funktioniert.",
  "model": "DeepL",
  "from_community_srt": "Lass uns diesen Prozess nochmal durchgehen, aber dieses Mal nehmen wir variable Einträge in jeder Matrix, um zu zeigen, dass diese Erklärung auf alle Matrizen zutrifft.",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way.",
  "translatedText": "Diese Methode ist symbollastiger und erfordert etwas mehr Platz, aber sie sollte für alle, die die Multiplikation mit Matrizen bisher eher auswendig gelernt haben, ziemlich befriedigend sein.",
  "model": "DeepL",
  "from_community_srt": "Das ist viel Zeichenlastiger und benötigt mehr Platz, jedoch wird es ziemlich befriedigend sein für diejenigen, die zuvor schon Matrix Multiplikation in \"geschriebener\" Form gelernt haben.",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands.",
  "translatedText": "Um nachzuvollziehen, wohin i-hat geht, schau dir zunächst die erste Spalte der Matrix auf der rechten Seite an, denn dort landet i-hat zunächst.",
  "model": "DeepL",
  "from_community_srt": "Um i-hat zu folgen, beginnen wir mit der ersten Spalte von der rechten Matrix, zumal das der End-Landepunkt für i-hat ist.",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation.",
  "translatedText": "Wenn du diese Spalte mit der Matrix auf der linken Seite multiplizierst, kannst du sehen, wo die Zwischenversion von i-hat nach der zweiten Transformation landet.",
  "model": "DeepL",
  "from_community_srt": "Die Multiplikation dieser Spalte mit der Matrix zur Linken zeigt, wo die zwischenzeitliche Version von i-hat landet, nachdem man die zweite Transformation angewendet hat.",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix.",
  "translatedText": "Die erste Spalte der Kompositionsmatrix wird also immer gleich der linken Matrix mal der ersten Spalte der rechten Matrix sein.",
  "model": "DeepL",
  "from_community_srt": "Die erste Spalte der Kompositions-Matrix ist also immer gleich der linken Matrix mal der ersten Spalte der rechten Matrix.",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix.",
  "translatedText": "Genauso landet j-hat anfangs immer auf der zweiten Spalte der rechten Matrix.",
  "model": "DeepL",
  "from_community_srt": "Gleichermaßen wird j-hat anfangs immer auf der zweiten Spalte der rechten Matrix landen.",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix.",
  "translatedText": "Wenn du also die linke Matrix mit dieser zweiten Spalte multiplizierst, erhältst du ihre endgültige Position, und das ist die zweite Spalte der Kompositionsmatrix.",
  "model": "DeepL",
  "from_community_srt": "Nach der Multiplikation mit der linken Matrix bekommt man also seine finale Position und das ist folglich die zweite Spalte der Kompositions-Matrix.",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to help remember it.",
  "translatedText": "Beachte, dass es hier viele Symbole gibt und dass es üblich ist, diese Formel auswendig zu lernen, zusammen mit einem bestimmten algorithmischen Prozess, der dabei hilft, sie sich zu merken.",
  "model": "DeepL",
  "from_community_srt": "Man sieht, dass hier viele Zeichen sind und es ist normal, diese Formel zum auswendig-lernen vorgelegt zu bekommen. Zusammen mit einem algorithmischen Prozess um es sich besser zu merken.",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another.",
  "translatedText": "Aber ich denke, bevor du diesen Prozess auswendig lernst, solltest du dir angewöhnen, darüber nachzudenken, was Matrixmultiplikation wirklich bedeutet, indem du eine Transformation nach der anderen anwendest.",
  "model": "DeepL",
  "from_community_srt": "Aber bevor man diesen Prozess auswendig lernt, sollte man darüber nachdenken, was eine Matrix Multiplikation eigentlich beschreibt: Anwendung einer Transformation nach der anderen.",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand.",
  "translatedText": "Glaube mir, dadurch bekommst du einen viel besseren konzeptionellen Rahmen, der dir die Eigenschaften der Matrixmultiplikation viel leichter verständlich macht.",
  "model": "DeepL",
  "from_community_srt": "Glaub mir, das gibt dir eine viel bessere Gedankenstütze, um die Eigenschaften von Matrix Multiplikation einfacher zu verstehen.",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question.",
  "translatedText": "Hier ist zum Beispiel eine Frage.",
  "model": "DeepL",
  "from_community_srt": "Zum Beispiel,",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them?",
  "translatedText": "Spielt es eine Rolle, in welcher Reihenfolge wir die beiden Matrizen bei der Multiplikation anordnen?",
  "model": "DeepL",
  "from_community_srt": "hier ist eine Frage: Macht es einen Unterschied, in welcher Reihenfolge wir die Matrizen multiplizieren? Ok,",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier.",
  "translatedText": "Nun, lass uns ein einfaches Beispiel durchdenken, wie das von vorhin.",
  "model": "DeepL",
  "from_community_srt": "lass uns an ein einfaches Beispiel denken, wie das von zuvor:",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smooshes j-hat over to the right, and a 90 degree rotation.",
  "translatedText": "Nimm eine Schere, die den i-Hut fixiert und den j-Hut nach rechts drückt, und eine 90-Grad-Drehung.",
  "model": "DeepL",
  "from_community_srt": "Mach eine Verformung, welche i-hat fixiert und j-hat nach rechts verzerrt und dann eine 90°-Rotation.",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1.",
  "translatedText": "Wenn du zuerst die Scherung und dann die Drehung durchführst, sehen wir, dass i-hat bei 0,1 und j-hat bei negativ 1,1 endet.",
  "model": "DeepL",
  "from_community_srt": "Wenn man erst verformt und dann rotiert, ist i-hat nun auf (0, 1) und j-hat auf (-1,",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together.",
  "translatedText": "Beide zeigen in der Regel dicht beieinander.",
  "model": "DeepL",
  "from_community_srt": "1); beide sind in etwa nah bei einander.",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing, you know, farther apart.",
  "translatedText": "Wenn du zuerst die Drehung und dann die Scherung durchführst, landet der i-Hut bei 1,1 und der j-Hut in einer anderen Richtung bei minus 1,0, und sie zeigen weiter auseinander.",
  "model": "DeepL",
  "from_community_srt": "Wenn man erst rotiert und dann verformt, ist i-hat dann auf (1, 1) und j-hat auf (-1, 0); einer völlig anderen Richtung. und sie zeigen, naja,",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently, order totally does matter.",
  "translatedText": "Die Gesamtwirkung ist hier ganz anders, also spielt die Reihenfolge offensichtlich eine große Rolle.",
  "model": "DeepL",
  "from_community_srt": "viel weiter von einander weg. Der Endeffekt ist hier eindeutig anders, also hat die Reihenfolge offensichtlich eine Auswirkung.",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice, by thinking in terms of transformations, that's the kind of thing that you can do in your head by visualizing.",
  "translatedText": "Beachte, dass du in Form von Transformationen denken kannst, indem du dir diese Dinge in deinem Kopf vorstellst.",
  "model": "DeepL",
  "from_community_srt": "Merk dir, dass die Transformation komplett in deinem Kopf abspielen kann, um sie zu visualisieren.",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary.",
  "translatedText": "Keine Matrixmultiplikation notwendig.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative.",
  "translatedText": "Als ich zum ersten Mal lineare Algebra belegte, gab es eine Hausaufgabe, bei der wir beweisen sollten, dass die Matrixmultiplikation assoziativ ist.",
  "model": "DeepL",
  "from_community_srt": "Man braucht keine Matrix Multiplikation. Ich weiß noch, als ich das erste mal lineare Algebra hatte: Es gab ein Hausaufgaben-Problem, bei welchem wir beweisen mussten, dass Matrix Multiplikationen assoziativ sind.",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left.",
  "translatedText": "Das heißt, wenn du drei Matrizen hast, A, B und C, und sie alle miteinander multiplizierst, sollte es keinen Unterschied machen, ob du zuerst A mal B berechnest und das Ergebnis dann mit C multiplizierst oder ob du zuerst B mal C multiplizierst und das Ergebnis dann mit A auf der linken Seite multiplizierst.",
  "model": "DeepL",
  "from_community_srt": "Das heißt, wenn man drei Matrizen A, B, und C hat und man alle miteinander multipliziert, ist es egal, ob man zuerst A mal B und dann das Ergebnis mal C berechnet oder ob man zuert B mal C multipliziert und das Ergebnis dann mal A nimmt.",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses.",
  "translatedText": "Mit anderen Worten: Es spielt keine Rolle, wo du die Klammern setzt.",
  "model": "DeepL",
  "from_community_srt": "In anderen Worten: Es ist egal,",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter.",
  "translatedText": "Wenn du jetzt versuchst, das numerisch durchzuarbeiten, wie ich es damals getan habe, ist das schrecklich, einfach schrecklich und vor allem nicht erhellend.",
  "model": "DeepL",
  "from_community_srt": "wo die Klammern stehen. Wenn du das nun versuchst, numerisch zu erarbeiten, wie ich es zuvor gemacht habe: es ist schrecklich, einfach schrecklich und  unnütz in diesem Fall.",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial.",
  "translatedText": "Aber wenn du dir die Matrixmultiplikation als Anwendung einer Transformation nach der anderen vorstellst, ist diese Eigenschaft einfach trivial.",
  "model": "DeepL",
  "from_community_srt": "Aber wenn man Matrix Multiplikation als angewandte Transformation nach der anderen sieht, ist diese Eigenschaft nur belanglos.",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why?",
  "translatedText": "Verstehst du, warum?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C, then B, then A, it's the same as applying C, then B, then A.",
  "translatedText": "Wenn du erst C, dann B und dann A anwendest, ist das dasselbe wie wenn du erst C, dann B und dann A anwendest.",
  "model": "DeepL",
  "from_community_srt": "Erkennst du warum? Da heißt: Wenn man erst C anwendet, dann B und dann A, ist es das gleiche wie C, dann B,",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove.",
  "translatedText": "Ich meine, es gibt nichts zu beweisen.",
  "model": "DeepL",
  "from_community_srt": "dann A.",
  "n_reviews": 0,
  "start": 552.82,
  "end": 554.38
 },
 {
  "input": "You're just applying the same three things one after the other, all in the same order.",
  "translatedText": "Du wendest einfach die gleichen drei Dinge nacheinander an, alle in der gleichen Reihenfolge.",
  "model": "DeepL",
  "from_community_srt": "Eigentlich muss man da nichts beweisen, man wendet einfach die gleichen drei Dinge eins nach dem anderen an.",
  "n_reviews": 0,
  "start": 554.54,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not.",
  "translatedText": "Das mag sich wie Betrug anfühlen, ist es aber nicht.",
  "model": "DeepL",
  "from_community_srt": "In der gleichen Reihenfolge. Es kommt einem vielleicht wie Schummeln vor.",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative, and even better than that, it's a good explanation for why that property should be true.",
  "translatedText": "Dies ist ein echter Beweis dafür, dass die Matrixmultiplikation assoziativ ist, und noch besser: Es ist eine gute Erklärung dafür, warum diese Eigenschaft wahr sein sollte.",
  "model": "DeepL",
  "from_community_srt": "Aber ist es nicht! Das ist ein echter Beweis dafür, dass Matrix Multiplikation assoziativ ist und darüber hinaus ist es eine gute Erklärung dafür, warum es wahr sein muss.",
  "n_reviews": 0,
  "start": 561.54,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically.",
  "translatedText": "Ich möchte dich ermutigen, mehr mit dieser Idee zu spielen, indem du dir zwei verschiedene Transformationen vorstellst, überlegst, was passiert, wenn du eine nach der anderen anwendest, und dann das Matrixprodukt numerisch berechnest.",
  "model": "DeepL",
  "from_community_srt": "Ich empfehle euch wirklich, mit dieser Idee weiter herumzuspielen: Vorstellung zweier verschiedener Transformationen; Nachdenken darüber, was nach der Anwendung nacheinander passiert; Und dann das Matrix Produkt numerisch ausarbeiten.",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in.",
  "translatedText": "Glaub mir, das ist die Art von Spielzeit, bei der man die Idee wirklich begreift.",
  "model": "DeepL",
  "from_community_srt": "Vertraue mir, diese Art von Spielchen machen die Idee erst richtig verständlich.",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions.",
  "translatedText": "Im nächsten Video werde ich darüber sprechen, wie man diese Ideen auf mehr als nur zwei Dimensionen ausweiten kann.",
  "model": "DeepL",
  "from_community_srt": "Im nächsten Video werde ich beginnen,",
  "n_reviews": 0,
  "start": 587.2,
  "end": 592.18
 }
]