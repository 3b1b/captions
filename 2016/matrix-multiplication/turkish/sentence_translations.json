[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices.",
  "translatedText": "Herkese merhaba, en son kaldığımız yerden doğrusal dönüşümlerin neye benzediğini ve matrislerle nasıl temsil edileceğini gösterdim.",
  "model": "google_nmt",
  "from_community_srt": "\"\"\" kişisel tecrübelerime göre, matris içeren ispatlar matrisler çıkarılarak %50 kısa hale getirilebilir \"\"\" Herkese merhaba! Gecen videoda, lineer donusumun ne oldugunu ve matrislerle nasıl temsil edilebileceklerini göstermiştim.",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video.",
  "translatedText": "Bu kısa bir özetlemeye değer çünkü gerçekten önemli, ancak elbette bu bir özetten daha fazlası gibi geliyorsa geri dönün ve videonun tamamını izleyin.",
  "model": "google_nmt",
  "from_community_srt": "Cok önemli olduğu için hızlı bir şekilde gözden geçirmeye değer! Ama tabii ki, özetten fazlasına ihtiyaç duyarsanız geri dönüp tüm bölümü izleyin.",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Technically speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed.",
  "translatedText": "Teknik olarak konuşursak, doğrusal dönüşümler girdi olarak vektörlerin ve çıktı olarak vektörlerin olduğu fonksiyonlardır, ancak geçen sefer bunları ızgara çizgilerinin paralel ve eşit aralıklı kalacağı ve böylece başlangıç noktasının eşit olacağı şekilde uzayın etrafında hareket ederek görsel olarak nasıl düşünebileceğimizi göstermiştim. sabit kalır.",
  "model": "google_nmt",
  "from_community_srt": "Teknik olarak, doğrusal dönüşümler; kendilerine vektör girdiler verip vektör çıktılar aldığımız işlevlerdir. (fonksiyonlar) Fakat geçen seferde görsel olarak anlamlandırmak için uzayı nasıl eğip büktüğümüzü ve üstelik bunu yarparken de ızgara çizgilerini paralel ve orijin i ise sabit tuttuğumuzu göstermiştim.",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat.",
  "translatedText": "Temel çıkarım, doğrusal bir dönüşümün tamamen uzayın temel vektörlerini aldığı yere göre belirlendiğiydi; bu, iki boyut için i-hat ve j-hat anlamına gelir.",
  "model": "google_nmt",
  "from_community_srt": "En önemli çıkarım şu idi: doğrusal dönüşüm, tamamen asıl vektörlerle tanımlanmakta idi, ki iki boyut için bunların da isimleri i ve j idi.",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors.",
  "translatedText": "Bunun nedeni, diğer herhangi bir vektörün bu temel vektörlerin doğrusal bir kombinasyonu olarak tanımlanabilmesidir.",
  "model": "google_nmt",
  "from_community_srt": "Bunun nedeni ise diğer tüm vektörlerin bu iki vektörün birleşimi olarak tarif edilebilmesi idi.",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat.",
  "translatedText": "Koordinatları x, y olan bir vektör x çarpı i-hat artı y çarpı j-hat'tır.",
  "model": "google_nmt",
  "from_community_srt": "zira (x,y) koordinatlı bir vektör x kere i artı y kere j dir.",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence.",
  "translatedText": "Dönüşümden geçtikten sonra ızgara çizgilerinin paralel ve eşit aralıklı kalması özelliği harika bir sonuç doğurur.",
  "model": "google_nmt",
  "from_community_srt": "Dönüşümleri bu ölçütle gerçekleştirince ızgara çizgilerinin paralel ve eşit aralıklı olması özelliği şahane bir sonuca ulaştırdır bizi.",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat.",
  "translatedText": "Vektörünüzün düştüğü yer x çarpı i-hat'ın dönüştürülmüş hali artı y çarpı j-hat'ın dönüştürülmüş hali olacaktır.",
  "model": "google_nmt",
  "from_community_srt": "Vektörün dönüşüm sonrası konumu, i nin dönüşüm sonrası konumu çarpı x ile y çarpı  j nin dönüşüm sonrası konumu nun toplamına eşit oldu.",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat.",
  "translatedText": "Bu, i-hat'in indiği koordinatların ve j-hat'ın indiği koordinatların kaydını tutarsanız, x, y'de başlayan bir vektörün x çarpı i-hat artı y'nin yeni koordinatları üzerine inmesi gerektiğini hesaplayabileceğiniz anlamına gelir. çarpı j-hat'ın yeni koordinatları.",
  "model": "google_nmt",
  "from_community_srt": "i nin ve j nin koordinatlarının kaydını tutarsak i nin ve j nin koordinatlarının kaydını tutarsak (x,y) noktasındaki vektörün i kere x + y kere j konumunda olması gerektiğini hesaplayabilirsin.",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication.",
  "translatedText": "Kural, i-hat ve j-hat'ın düştüğü yerin koordinatlarını bir matrisin sütunları olarak kaydetmek ve bu sütunların ölçeklendirilmiş versiyonlarının toplamını x ve y ile matris-vektör çarpımı olarak tanımlamaktır.",
  "model": "google_nmt",
  "from_community_srt": "Mutabak, i ve j nin koordinatlarını kolon matrix olarak tutmak ve bu matrix ile x y şeklindeki vektörlerle vektör matrix çarpımı yapmak şeklindedir.",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector.",
  "translatedText": "Bu şekilde, bir matris belirli bir doğrusal dönüşümü temsil eder ve bir matrisin bir vektörle çarpılması, bu dönüşümün o vektöre hesaplamalı olarak uygulanması anlamına gelir.",
  "model": "google_nmt",
  "from_community_srt": "Bu şekilde, matrix belirli bir doğrusal dönüşümü temsil eder ve hesap açısından, matrix i vektör ile çarpmak ilgili vektöre bu dönüşümü uygulamak anlamına gelir.",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff.",
  "translatedText": "Tamam, yeni konulara geçelim.",
  "model": "google_nmt",
  "from_community_srt": "Pekala, gözden geçirme bitti! Gelelim yeni konuya.",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes, you find yourself wanting to describe the effects of applying one transformation and then another.",
  "translatedText": "Çoğu zaman, kendinizi bir dönüşümün ardından diğerinin uygulanmasının etkilerini tanımlamak isterken bulursunuz.",
  "model": "google_nmt",
  "from_community_srt": "Genelde kendini bir dönüşüm ardından başka bir dönüşüm uygulama işlemininin tesirini açıklamak  durumunda bulursun.",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear.",
  "translatedText": "Örneğin, düzlemi önce saat yönünün tersine 90 derece döndürdüğünüzde, sonra da kesme uyguladığınızda ne olacağını açıklamak isteyebilirsiniz.",
  "model": "google_nmt",
  "from_community_srt": "Ne demek yani? Belki açıklamak istediğin şey şu: eğer önce  saatin tersi yönünde 90° döndürüp sonra sheer(büküm) uygularsam ne olur? baştan sona ,",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear.",
  "translatedText": "Buradaki genel etki, baştan sona, dönme ve kaymadan farklı bir başka doğrusal dönüşümdür.",
  "model": "google_nmt",
  "from_community_srt": "toplamda olan şey bu durumda, başka bir doğrusal dönüşüm olurdu,",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied.",
  "translatedText": "Bu yeni doğrusal dönüşüme genellikle uyguladığımız iki ayrı dönüşümün bileşimi denir.",
  "model": "google_nmt",
  "from_community_srt": "ne döndürme ne de sheer olan! Bu doğrusal dönüşüm genel olarak \"kompozisyon\", iki ayrı uygulanmış dönüşümün kompozisyonu olarak adlandırılır.",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat.",
  "translatedText": "Ve herhangi bir doğrusal dönüşüm gibi, i-hat ve j-hat izlenerek tamamen kendine ait bir matrisle tanımlanabilir.",
  "model": "google_nmt",
  "from_community_srt": "Ve tıpkı tüm doğrusal dönüşümler gibi i ve j yi takip ederek elde edilecek, tek bir matrix olarak tarif edilebilir.",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix.",
  "translatedText": "Bu örnekte, her iki dönüşümden sonra i-hat için nihai iniş noktası 1,1'dir, o halde bunu bir matrisin ilk sütunu yapalım.",
  "model": "google_nmt",
  "from_community_srt": "Bu örnekte, i nin nihai erek, her iki dönüşüm sonrası(1,1)dir. O halde bu değeri, yeni dönüşüm matriksi için birinci sütun yapalım.",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix.",
  "translatedText": "Benzer şekilde, j-hat sonuçta negatif 1,0 konumunda biter, dolayısıyla bunu matrisin ikinci sütunu yaparız.",
  "model": "google_nmt",
  "from_community_srt": "Aynı şekilde, j son durumda (-1,0) noktasına gelir, haliyle bunu da ikinci sütun yapalım.",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones.",
  "translatedText": "Bu yeni matris, bir döndürme ve ardından bir kesme uygulamasının genel etkisini yakalar, ancak bunu iki ardışık eylem yerine tek bir eylem olarak gerçekleştirir.",
  "model": "google_nmt",
  "from_community_srt": "İşte bu yeni matrix, önce döndürme ve sonra sheer uygulama etkisini iki peş peşe hareket yerine,",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix.",
  "translatedText": "İşte bu yeni matris hakkında düşünmenin bir yolu.",
  "model": "google_nmt",
  "from_community_srt": "tek bir hareket olarak ifade eder. Yeni matrix i şöyle de düşünebiliriz: eğer bir vektörü alır,",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix, then take whatever you get and multiply that on the left by the shear matrix.",
  "translatedText": "Eğer bir vektör alıp onu dönme boyunca pompalarsanız, o zaman kesme kuvvetinin nerede bittiğini hesaplamanın uzun yolu önce onu soldan dönme matrisiyle çarpmak, sonra ne bulursanız onu alıp çarpmaktır. kayma matrisi tarafından bırakılmıştır.",
  "model": "google_nmt",
  "from_community_srt": "önce döndürme sonra da sheer işleminden geçirirsen, bunu hesaplamanın uzun yolu: evvela onu sola döndürme matrix ini koyup onunla çarppıp, sonra elde ettiğin sonucu da sola koyacağın sheer matrix i ile çarpmak olurdu.",
  "n_reviews": 0,
  "start": 185.42,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector.",
  "translatedText": "Bu, sayısal olarak konuşursak, belirli bir vektöre bir döndürme ve ardından bir kesme uygulamanın anlamıdır.",
  "model": "google_nmt",
  "from_community_srt": "Bu, sayısal ifade etmek gerekirse; verilen bir vektörü önce döndürme sonra da sheer etmenin ta kendisidir.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action.",
  "translatedText": "Ancak elde ettiğiniz sonuç, hangi vektörü seçerseniz seçin, az önce bulduğumuz bu yeni bileşim matrisini aynı vektörle uygulamakla aynı olmalıdır, çünkü bu yeni matrisin dönme ve kayma hareketi ile aynı genel etkiyi yakalaması gerekir.",
  "model": "google_nmt",
  "from_community_srt": "Şimdi, bulmamız gereken öyle bir kompozisyon işlemi olmalı ki uyguladığımızda, aynı vektör üzerinde, hangi vektörü seçersek de seçelim üstelik, tanım gereği bu matrixin, toplamdaki etkiyi meydana getirmesi gerektiğinden, döndürme-sheer etme işlemlerine eşit olsun.",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you?",
  "translatedText": "Burada yazılanlara bakılırsa, bu yeni matrise orijinal iki matrisin çarpımı demek mantıklı sanırım, değil mi?",
  "model": "google_nmt",
  "from_community_srt": "Buraya yazdıklarmıza dayanarak; sanırım, bu yeni matrix i, iki dönüşüm matrix inin çarpımı olarak tarif etmek makul olsa gerek? Ne dersin?",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers.",
  "translatedText": "Bir anda bu çarpımı daha genel olarak nasıl hesaplayacağımızı düşünebiliriz, ancak sayılar ormanında kaybolmak çok kolaydır.",
  "model": "google_nmt",
  "from_community_srt": "Genel olarak bu iki matrix in çarpımını nasıl yapacağımızı kısa bi düşününce bile insan gerçekten,",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another.",
  "translatedText": "İki matrisi bu şekilde çarpmanın, bir dönüşümün ardından diğerine uygulanmasının geometrik anlamına sahip olduğunu her zaman unutmayın.",
  "model": "google_nmt",
  "from_community_srt": "sayılar dehlizlerinde kayboluyor! Daima iki matrix i çarpmayı şöyle hatırla: bu işlem,",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left.",
  "translatedText": "Burada tuhaf olan şey, bunun bizi sağdan sola doğru okuması.",
  "model": "google_nmt",
  "from_community_srt": "peşpeşe iki dönüşüm gerçekleştirmek demek geometrik olarak ! Burada tek tuhaf olan, sağdan sola okumak Önce,",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left.",
  "translatedText": "Önce sağdaki matrisin temsil ettiği dönüşümü uygularsınız, ardından soldaki matrisin temsil ettiği dönüşümü uygularsınız.",
  "model": "google_nmt",
  "from_community_srt": "sağdaki matrix ile ifade edilen dönüşümü uygulayacan. Sonra soldaki matrix ile olanı.",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left.",
  "translatedText": "Bu, fonksiyon gösteriminden kaynaklanmaktadır, çünkü fonksiyonları değişkenlerin soluna yazıyoruz, dolayısıyla her iki fonksiyon oluşturduğunuzda, onu her zaman sağdan sola okumak zorunda kalırsınız.",
  "model": "google_nmt",
  "from_community_srt": "Bu fonksiyonlardaki gösterimden gelmekte, çünkü fonksiyonları yazarken değişkenleri sağa, isimlerini ise sola yazarız, haliyle her iki fonksiyonu birleştirirken daima sağdan sola doğru okumak durumundasınızdır.",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us.",
  "translatedText": "İbrani okuyucular için iyi haber, geri kalanımız için kötü haber.",
  "model": "google_nmt",
  "from_community_srt": "Yahudi(Arap) seyirciler için iyi, geri kalanlar için kötü haber.",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example.",
  "translatedText": "Başka bir örneğe bakalım.",
  "model": "google_nmt",
  "from_community_srt": "Bir diğer örneğe bakalım.",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.",
  "translatedText": "Dönüşümü şuna benzeyen, sütunları 1,1 ve negatif 2,0 olan matrisi alın.",
  "model": "google_nmt",
  "from_community_srt": "(1,1) ve (-2,0) sütunlu dönüşümü şu şekilde görünen matrisi ele alalım ve M1 ismini verelim bu matrise.",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it M1.",
  "translatedText": "Ve buna M1 diyelim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.",
  "translatedText": "Daha sonra, dönüşümü şu şekilde görünen 0,1 ve 2,0 sütunlu matrisi alın.",
  "model": "google_nmt",
  "from_community_srt": "(0,1) ve (2,0) sütunlu diğer, dönüşümü şu şekilde olan bir",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy M2.",
  "translatedText": "Ve bu adama M2 diyelim.",
  "model": "google_nmt",
  "from_community_srt": "M2 isimli matrisi alalım.",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying M1 then M2 gives us a new transformation, so let's find its matrix.",
  "translatedText": "M1'i ve ardından M2'yi uygulamanın toplam etkisi bize yeni bir dönüşüm verir, o halde bunun matrisini bulalım.",
  "model": "google_nmt",
  "from_community_srt": "Önce M1 i sonra da M2 yi uygulamanın toplam etkisi bize yeni dönüşümü verir. O halde hadi nihai matrisi bulalım.",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix.",
  "translatedText": "Ama bu sefer, animasyonları izlemeden, bunun yerine sadece her matristeki sayısal girişleri kullanarak bunu yapıp yapamayacağımızı görelim.",
  "model": "google_nmt",
  "from_community_srt": "Fakat bu defa animasyonu izlemeden bu işlemi yapmaya çalışalım ve sadece matrislerdeki sayısal değerleri kullanalım.",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes.",
  "translatedText": "Öncelikle i-hat'ın nereye gittiğini bulmamız gerekiyor.",
  "model": "google_nmt",
  "from_community_srt": "Evvela i asıl vektörünün nereye konumlanacağına bulmalıyız.",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying M1, the new coordinates of i-hat, by definition, are given by that first column of M1, namely 1,1.",
  "translatedText": "M1 uygulandıktan sonra i-hat'ın yeni koordinatları tanım gereği M1'in ilk sütunu, yani 1,1 tarafından verilir.",
  "model": "google_nmt",
  "from_community_srt": "M1 dönüşümü sonrası i nin koordinatları tanım gereği, M1 matrisinin ilk sütunundaki değer olmalıdır. ki gördüğümüz gibi bu (1,1) dir.",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying M2, multiply the matrix for M2 by that vector 1,1.",
  "translatedText": "M2'yi uyguladıktan sonra ne olacağını görmek için M2'nin matrisini bu 1,1 vektörüyle çarpın.",
  "model": "google_nmt",
  "from_community_srt": "M2 dönüşümü sonrasında bu vektöre ne olduğunu görmek için M2 matrisi ile (1,1) vektörünü çarpalım.",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1.",
  "translatedText": "Geçen videoda anlattığım şekilde çalışırsanız 2,1 vektörünü elde edersiniz.",
  "model": "google_nmt",
  "from_community_srt": "Daha önceki videoda anlattığım şekilde hesabı yaparsak elde edeceğimiz vektör (2,1) olur.",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix.",
  "translatedText": "Bu, kompozisyon matrisinin ilk sütunu olacaktır.",
  "model": "google_nmt",
  "from_community_srt": "Kompozisyon matrisimizin birinci sütunu bu vektör olacaktır.",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of M1 tells us that it first lands on negative 2,0.",
  "translatedText": "Benzer şekilde, j-hat'ı takip etmek gerekirse, M1'in ikinci sütunu bize bunun ilk önce negatif 2,0'a indiğini söyler.",
  "model": "google_nmt",
  "from_community_srt": "Aynı şekilde, j vektörünün nereye gideceğini de önce M1 matrisinin ikinci sütunu söylüyor,",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply M2 to that vector, you can work out the matrix-vector product to get 0, negative 2, which becomes the second column of our composition matrix.",
  "translatedText": "Daha sonra M2'yi bu vektöre uyguladığımızda matris-vektör çarpımını hesaplayarak 0, negatif 2'yi elde edebilirsiniz, bu da bileşim matrisimizin ikinci sütunu olur.",
  "model": "google_nmt",
  "from_community_srt": "(-2,0) sonra ise bu vektöre M2 matrisi uygulanınca matris vektör çarpımı ile (0,-2) değerine ulaşırız. Ki bu da kompozisyon matrisimizin ikinci sütunu olur.",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices.",
  "translatedText": "Aynı süreçten tekrar bahsetmeme izin verin, ancak bu kez her matristeki değişken girdileri göstereceğim, sadece aynı mantığın tüm matrisler için işe yaradığını göstermek için.",
  "model": "google_nmt",
  "from_community_srt": "Müsadenizle aynı işlemi bu defa farklı bir şekilde anlatayım: her matristeki değerleri değişkenlerle tarif edeceğim ki aynı işlemler türlü türlü matris için de geçerli olduğunu farkedelim.",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way.",
  "translatedText": "Bu daha sembol ağırlıklıdır ve biraz daha fazla alan gerektirir, ancak daha önce matris çarpımını daha ezberci bir şekilde öğretmiş olan herkes için oldukça tatmin edici olmalıdır.",
  "model": "google_nmt",
  "from_community_srt": "Bu defa daha sembol ağırlıklı ve alan gereksinimli bir açıklama yapacağım, fakat bu, matris çarpımlarını daha klasik şekilde öğrenmiş kimseler için tatmin edici olacaktır.",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands.",
  "translatedText": "i-hat'ın nereye gittiğini takip etmek için sağdaki matrisin ilk sütununa bakarak başlayın, çünkü burası i-hat'in başlangıçta indiği yerdir.",
  "model": "google_nmt",
  "from_community_srt": "i vektörünü takip etmek için birinci matrisin ilk sütununa bakın, zira bu i nin başlangıç konumudur.",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation.",
  "translatedText": "Bu sütunu soldaki matrisle çarpmak, ikinci dönüşümü uyguladıktan sonra i-hat'ın ara versiyonunun nerede biteceğini nasıl anlayacağınızı gösterir.",
  "model": "google_nmt",
  "from_community_srt": "Bu vektörü soldaki matris ile çarpmak, i nin dönüşüm ortasındaki değerinin, ikinci dönüşüm işlemi sonrası nerede olacağını söylemektir.",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix.",
  "translatedText": "Yani bileşim matrisinin ilk sütunu her zaman sol matris çarpı sağ matrisin ilk sütununa eşit olacaktır.",
  "model": "google_nmt",
  "from_community_srt": "Dolayısıyla, kompozisyon matrisinin ilk sütunu daima birinci matrisin ilk sütunun ile ikinci matrisin çarpımına eşit olacaktır.",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix.",
  "translatedText": "Benzer şekilde, j-hat her zaman başlangıçta sağ matrisin ikinci sütununa yerleşecektir.",
  "model": "google_nmt",
  "from_community_srt": "Aynı şekilde j vektörü daima sağdaki matrisin ikinci sütunundaki başlangıç değerine sahip olacaktır.",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix.",
  "translatedText": "Soldaki matrisi bu ikinci sütunla çarpmak onun son konumunu verecektir ve dolayısıyla bu, bileşim matrisinin ikinci sütunu olur.",
  "model": "google_nmt",
  "from_community_srt": "Dolayısıyla, soldaki matrisi bu ikinci sütun ile çarpmak nihai konumu verir ki bu da kompozisyon matrisin ikinci sütunudur.",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to help remember it.",
  "translatedText": "Burada çok sayıda sembol olduğuna dikkat edin ve bu formülün ezberlenmesi gereken bir şey olarak öğretilmesinin yanı sıra hatırlamaya yardımcı olacak belirli bir algoritmik süreç de yaygındır.",
  "model": "google_nmt",
  "from_community_srt": "farkettiysen bir sürü sembol oldu genel yanılgımız ise bunun ezberlenecek bir formül olduğunu sanmak şeklinde, üstelik kendimizce bunu hatırlamak için de kimi hatırlatıcı metotlar da kullanırız.",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another.",
  "translatedText": "Ancak bu süreci ezberlemeden önce, matris çarpımının gerçekte neyi temsil ettiğini düşünme, dönüşümleri ardı ardına uygulama alışkanlığı edinmeniz gerektiğini düşünüyorum.",
  "model": "google_nmt",
  "from_community_srt": "Ama gerçekten bence, bunları ezberlemeden önce matris çarpımının gerçekten şu anlama geldiğini düşünme alışkanlığı kazanmalısınız: \"bir dönüşüm peşi sıra başka bir dönüşüm uygulamak\" Bana güvenin,",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand.",
  "translatedText": "İnanın bana, bu size matris çarpımının özelliklerinin anlaşılmasını çok daha kolay hale getiren çok daha iyi bir kavramsal çerçeve sağlayacaktır.",
  "model": "google_nmt",
  "from_community_srt": "bu size daha iyi bir kavram temeli verecektir öyle ki matris çarpımın özelliklerini anlamak da çok daha kolay hale getirecektir.",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question.",
  "translatedText": "Mesela şöyle bir soru var.",
  "model": "google_nmt",
  "from_community_srt": "Örneğin,",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them?",
  "translatedText": "İki matrisi çarparken hangi sıraya koyduğumuz önemli mi?",
  "model": "google_nmt",
  "from_community_srt": "buyrun bir soru: Matris çarpımı yaparken sıralama önemli midir?",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier.",
  "translatedText": "Peki, daha önce olduğu gibi basit bir örnek üzerinden düşünelim.",
  "model": "google_nmt",
  "from_community_srt": "Pekala, basit bir örnek üzerinden düşünelim.",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smooshes j-hat over to the right, and a 90 degree rotation.",
  "translatedText": "i-hat'i sabitleyen ve j-hat'i sağa doğru yumuşatan bir makası ve 90 derecelik bir dönüş alın.",
  "model": "google_nmt",
  "from_community_srt": "tıpkı önceki gibi: Shear dönüşümü uygulalayım, i yerinde kalsın, y sağa doğru kaysın. ve sonra 90° dönüş yapalım.",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1.",
  "translatedText": "Önce kesmeyi yaparsanız, sonra döndürürseniz, i-hat'ın 0,1'de ve j-hat'ın negatif 1,1'de bittiğini görebiliriz.",
  "model": "google_nmt",
  "from_community_srt": "Şayet önce shear edip, sonra döndürürsek i nin (0,1) e geldiğini ve j nin de (-1,1) e geldiğini görürüz.",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together.",
  "translatedText": "Her ikisi de genellikle birbirine yakın işaret ediyor.",
  "model": "google_nmt",
  "from_community_srt": "ikisi de genel olarak hemen hemen aynı yöne bakıyor.",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing, you know, farther apart.",
  "translatedText": "İlk önce döndürürseniz, sonra kesmeyi yapın, i-hat 1,1'de biter ve j-hat, negatif 1,0'da farklı bir yöne doğru ayrılır ve onlar, bilirsiniz, daha uzak bir mesafeyi işaret ederler.",
  "model": "google_nmt",
  "from_community_srt": "Yok eğer önce döndürüp sonra shear uygulasakdı i (1,1) noktasına, j ise farklı bir yöne (-1,0) noktasına gelirdi. ve ikisi bir birine göre, ne bileyim,",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently, order totally does matter.",
  "translatedText": "Buradaki genel etki açıkça farklıdır, dolayısıyla düzenin tamamen önemli olduğu açıktır.",
  "model": "google_nmt",
  "from_community_srt": "uzağa bakarlardı. Toplamdaki etki açıkça görülüyor ki farklı haliyle, besbelli bir şekilde sıralama kesinlikle önemlidir.",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice, by thinking in terms of transformations, that's the kind of thing that you can do in your head by visualizing.",
  "translatedText": "Dikkat edin, dönüşümler açısından düşünerek, bu, kafanızda görselleştirerek yapabileceğiniz türden bir şeydir.",
  "model": "google_nmt",
  "from_community_srt": "Dİkkat et, dönüşümler olarak düşününce bu işlemleri kafanda kolayca canlandırıp yapılabilecek iş oldu bunlar.",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary.",
  "translatedText": "Matris çarpımına gerek yoktur.",
  "model": "google_nmt",
  "from_community_srt": "Matrix çarpımına filan gerek yok.",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative.",
  "translatedText": "Doğrusal cebiri ilk aldığımda, matris çarpımının ilişkisel olduğunu kanıtlamamızı isteyen bir ev ödevi problemi olduğunu hatırlıyorum.",
  "model": "google_nmt",
  "from_community_srt": "Doğrusal cebir dersini aldığım ilk ders geldi aklıma, bir ödev vardı, hoca bizden matris çarpımlarının birleşimlilik özelliğinin olduğunu ispatlamamamızı istemişti.",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left.",
  "translatedText": "Bu şu anlama gelir: A, B ve C olmak üzere üç matrisiniz varsa ve bunları birbiriyle çarparsanız, önce A ile B'yi hesaplamanız, ardından sonucu C ile çarpmanız veya önce B'yi çarpmanız fark etmez. C, sonra bu sonucu soldaki A ile çarpın.",
  "model": "google_nmt",
  "from_community_srt": "Birleşişimlilik özelliği şu demek: A, B ve C matrisleri varsa elimizde, hepsini çarpmak gerekiyorsa, önce A ile B yi sonra da C yi çarpmak ile önce B ile C yi çarpıp sonra A ile çarpmak fark meydana getirmemeli.",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses.",
  "translatedText": "Başka bir deyişle parantezleri nereye koyduğunuz önemli değil.",
  "model": "google_nmt",
  "from_community_srt": "Başka bir deyişle parantezi nereye koyduğumuz önemsiz olmalı.",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter.",
  "translatedText": "Şimdi, eğer bunu sayısal olarak çözmeye çalışırsanız, benim o zamanlar yaptığım gibi, bu korkunçtur, sadece korkunçtur ve bu konuda aydınlatıcı değildir.",
  "model": "google_nmt",
  "from_community_srt": "Şimdi eğer bunu formüllerle, sayılarla yapacak olursan, bizim geçmişte yaptığımız gibi, korkunç, direk korkunç ve konuyu anlamakla alakasız bir iş yapmış olursun..",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial.",
  "translatedText": "Ancak matris çarpımını dönüşümlerin ardı ardına uygulanması olarak düşündüğünüzde, bu özellik önemsizdir.",
  "model": "google_nmt",
  "from_community_srt": "Matris çarpımını bir dönüşüm ardına başka dönüşüm gibi düşünürsen, bu özellik çocuk oyuncağı.",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why?",
  "translatedText": "Nedenini görebiliyor musun?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C, then B, then A, it's the same as applying C, then B, then A.",
  "translatedText": "Demek istediği, önce C'yi, sonra B'yi, sonra A'yı uygularsanız, bu C, sonra B, sonra A'yı uygulamakla aynı şeydir.",
  "model": "google_nmt",
  "from_community_srt": "Nedenini görebiliyor musun? Söylenen şey: önce C sonra B ve daha sonra A, önce C sonra B ve en son A uygulamakla aynı şeydir.",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove.",
  "translatedText": "Demek istediğim, kanıtlanacak hiçbir şey yok.",
  "model": "google_nmt",
  "from_community_srt": "Demek istediğim,",
  "n_reviews": 0,
  "start": 552.82,
  "end": 554.38
 },
 {
  "input": "You're just applying the same three things one after the other, all in the same order.",
  "translatedText": "Aynı üç şeyi birbiri ardına, aynı sırayla uyguluyorsunuz.",
  "model": "google_nmt",
  "from_community_srt": "ispatlamaya ihtiyaç bile yok! aynı üç şeyi, aynı sıra ile uyguluyorsun.",
  "n_reviews": 0,
  "start": 554.54,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not.",
  "translatedText": "Bu hile yapmak gibi gelebilir ama değil.",
  "model": "google_nmt",
  "from_community_srt": "Hile yapıyoruz gibi değil mi? Ama değil!",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative, and even better than that, it's a good explanation for why that property should be true.",
  "translatedText": "Bu, matris çarpımının ilişkisel olduğunun dürüst bir kanıtıdır ve bundan daha da iyisi, bu özelliğin neden doğru olması gerektiğine dair iyi bir açıklamadır.",
  "model": "google_nmt",
  "from_community_srt": "Bu matris çarpımı işlemlerinde birleşimliliğin olduğuna dair dos-doğru bir ispat ve daha da iyisi,",
  "n_reviews": 0,
  "start": 561.54,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically.",
  "translatedText": "Bu fikirle daha fazla oynamanızı, iki farklı dönüşümü hayal etmenizi, birbiri ardına uyguladığınızda ne olacağını düşünmenizi ve ardından matris çarpımını sayısal olarak hesaplamanızı gerçekten tavsiye ediyorum.",
  "model": "google_nmt",
  "from_community_srt": "bu özelliğin neden doğru olması gerektiğini anlatan güzel bir açıklama! Bu fikirleri biraz daha didiklemenizi şiddetle öneriyorum, iki farklı dönüşümü hayal edin, biri ardında diğeri uygulanınca ne olacağını düşünün ve sonra matris çarpımını sayısal işlem olarak da yapın.",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in.",
  "translatedText": "İnan bana, bu fikrin gerçekten yerleşmesini sağlayan türden bir oyun zamanı.",
  "model": "google_nmt",
  "from_community_srt": "İnanın ki; bu, taşların yerine oturması için gerekli bir eğlence.",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions.",
  "translatedText": "Bir sonraki videoda bu fikirleri iki boyutun ötesine taşımaktan bahsetmeye başlayacağım.",
  "model": "google_nmt",
  "from_community_srt": "Gelecek videoda, bu fikirleri iki boyutun ötesine taşımak hakkında konuşmaya başlayacağım.",
  "n_reviews": 0,
  "start": 587.2,
  "end": 592.18
 }
]