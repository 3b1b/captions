1
00:00:10,940 --> 00:00:13,910
Привет всем, с того места, где мы остановились в последний раз, я показал, 

2
00:00:13,910 --> 00:00:16,880
как выглядят линейные преобразования и как их представить с помощью матриц.

3
00:00:18,320 --> 00:00:20,692
Стоит кратко подвести итог, потому что это действительно важно, 

4
00:00:20,692 --> 00:00:22,693
но, конечно, если вам кажется, что это нечто большее, 

5
00:00:22,693 --> 00:00:25,140
чем просто подведение итогов, вернитесь и посмотрите полное видео.

6
00:00:25,780 --> 00:00:28,783
Технически говоря, линейные преобразования — это функции с векторами в 

7
00:00:28,783 --> 00:00:31,491
качестве входных данных и векторами в качестве выходных данных, 

8
00:00:31,491 --> 00:00:34,453
но в прошлый раз я показал, как мы можем представить их визуально как 

9
00:00:34,453 --> 00:00:37,203
сглаживание пространства таким образом, что линии сетки остаются 

10
00:00:37,203 --> 00:00:40,206
параллельными и равномерно расположенными, а начало координат остается 

11
00:00:40,206 --> 00:00:41,180
остается фиксированным.

12
00:00:41,820 --> 00:00:46,424
Ключевой вывод заключался в том, что линейное преобразование полностью определяется тем, 

13
00:00:46,424 --> 00:00:49,425
где оно берет базисные векторы пространства, что для двух 

14
00:00:49,425 --> 00:00:51,340
измерений означает i-шляпу и j-шляпу.

15
00:00:51,340 --> 00:00:54,166
Это связано с тем, что любой другой вектор можно 

16
00:00:54,166 --> 00:00:57,340
описать как линейную комбинацию этих базисных векторов.

17
00:00:57,940 --> 00:01:00,841
Вектор с координатами x, y равен x, умноженному на i-шляпу, 

18
00:01:00,841 --> 00:01:02,340
плюс y, умноженному на j-шляпу.

19
00:01:03,460 --> 00:01:06,027
После прохождения трансформации это свойство, заключающееся в том, 

20
00:01:06,027 --> 00:01:08,633
что линии сетки остаются параллельными и равномерно расположенными, 

21
00:01:08,633 --> 00:01:09,860
имеет замечательные последствия.

22
00:01:10,500 --> 00:01:13,662
Место, где приземлится ваш вектор, будет в x раз больше 

23
00:01:13,662 --> 00:01:17,560
преобразованной версии i-hat плюс y раз преобразованной версии j-hat.

24
00:01:18,240 --> 00:01:22,212
Это означает, что если вы записываете координаты, где приземляется i-шляпа, 

25
00:01:22,212 --> 00:01:26,028
и координаты, где приземляется j-шляпа, вы можете вычислить, что вектор, 

26
00:01:26,028 --> 00:01:29,635
который начинается в точках x, y, должен приземлиться в x раз больше 

27
00:01:29,635 --> 00:01:32,720
новых координат i-шляпы плюс y. раз новые координаты j-hat.

28
00:01:33,560 --> 00:01:37,021
Соглашение состоит в том, чтобы записывать координаты того места, 

29
00:01:37,021 --> 00:01:40,010
где приземляются i-шляпа и j-шляпа, как столбцы матрицы, 

30
00:01:40,010 --> 00:01:43,891
и определять эту сумму масштабированных версий этих столбцов по x и y как 

31
00:01:43,891 --> 00:01:45,360
умножение матрицы на вектор.

32
00:01:46,050 --> 00:01:50,484
Таким образом, матрица представляет собой конкретное линейное преобразование, 

33
00:01:50,484 --> 00:01:54,350
а умножение матрицы на вектор — это то, что означает вычислительное 

34
00:01:54,350 --> 00:01:57,080
применение этого преобразования к этому вектору.

35
00:01:58,800 --> 00:02:00,880
Хорошо, подведем итоги, переходим к новому.

36
00:02:01,600 --> 00:02:07,000
Часто вам хочется описать эффект применения одного преобразования, а затем другого.

37
00:02:07,620 --> 00:02:09,978
Например, возможно, вы хотите описать, что происходит, 

38
00:02:09,978 --> 00:02:13,408
когда вы сначала поворачиваете плоскость на 90 градусов против часовой стрелки, 

39
00:02:13,408 --> 00:02:14,480
а затем применяете сдвиг.

40
00:02:15,260 --> 00:02:18,397
Общий эффект здесь, от начала до конца, представляет собой 

41
00:02:18,397 --> 00:02:21,800
еще одно линейное преобразование, отличное от вращения и сдвига.

42
00:02:22,280 --> 00:02:25,510
Это новое линейное преобразование обычно называют композицией 

43
00:02:25,510 --> 00:02:28,220
двух отдельных преобразований, которые мы применили.

44
00:02:28,920 --> 00:02:34,172
И, как любое линейное преобразование, его можно описать с помощью собственной матрицы, 

45
00:02:34,172 --> 00:02:35,440
следуя i-hat и j-hat.

46
00:02:36,020 --> 00:02:41,067
В этом примере конечная точка приземления для i-hat после обоих преобразований — 1,1, 

47
00:02:41,067 --> 00:02:44,120
поэтому давайте сделаем это первым столбцом матрицы.

48
00:02:44,960 --> 00:02:49,340
Аналогично, j-шляпа в конечном итоге оказывается в отрицательном положении 1,0, 

49
00:02:49,340 --> 00:02:51,860
поэтому мы делаем это вторым столбцом матрицы.

50
00:02:52,680 --> 00:02:56,622
Эта новая матрица отражает общий эффект применения вращения, 

51
00:02:56,622 --> 00:03:01,340
а затем сдвига, но как одно действие, а не два последовательных действия.

52
00:03:03,040 --> 00:03:04,880
Вот один из способов подумать об этой новой матрице.

53
00:03:05,420 --> 00:03:09,699
Если бы вы взяли какой-то вектор и прокачали его через вращение, то сдвиг, 

54
00:03:09,699 --> 00:03:14,664
длинный способ вычислить, где он окажется, — это сначала умножить его слева на матрицу 

55
00:03:14,664 --> 00:03:19,800
вращения, затем взять все, что вы получите, и умножить это на оставленный матрицей сдвига.

56
00:03:20,460 --> 00:03:26,060
Говоря численно, это означает применение вращения, а затем сдвига к заданному вектору.

57
00:03:26,800 --> 00:03:29,028
Но все, что вы получите, должно быть таким же, 

58
00:03:29,028 --> 00:03:32,965
как простое применение этой новой матрицы композиции, которую мы только что нашли, 

59
00:03:32,965 --> 00:03:36,000
к тому же вектору, независимо от того, какой вектор вы выбрали, 

60
00:03:36,000 --> 00:03:39,083
поскольку эта новая матрица должна отражать тот же общий эффект, 

61
00:03:39,083 --> 00:03:40,980
что и действие вращения, а затем сдвига.

62
00:03:42,480 --> 00:03:44,955
Судя по тому, как здесь все написано, я думаю, 

63
00:03:44,955 --> 00:03:49,380
что разумно назвать эту новую матрицу произведением двух исходных матриц, не так ли?

64
00:03:50,420 --> 00:03:54,633
Мы можем за мгновение подумать о том, как вычислить это произведение в более общем плане, 

65
00:03:54,633 --> 00:03:56,600
но слишком легко заблудиться в лесу чисел.

66
00:03:56,600 --> 00:04:00,019
Всегда помните, что подобное умножение двух матриц имеет 

67
00:04:00,019 --> 00:04:04,280
геометрический смысл применения одного преобразования, а затем другого.

68
00:04:05,860 --> 00:04:09,660
Одна вещь, которая здесь немного странная, это то, что мы читаем справа налево.

69
00:04:10,040 --> 00:04:13,555
Сначала вы применяете преобразование, представленное матрицей справа, 

70
00:04:13,555 --> 00:04:16,720
затем применяете преобразование, представленное матрицей слева.

71
00:04:17,399 --> 00:04:21,079
Это связано с обозначением функций, поскольку мы пишем функции слева от переменных, 

72
00:04:21,079 --> 00:04:23,444
поэтому каждый раз, когда вы составляете две функции, 

73
00:04:23,444 --> 00:04:25,460
вам всегда приходится читать их справа налево.

74
00:04:25,920 --> 00:04:28,980
Хорошие новости для читателей на иврите, плохие новости для всех нас.

75
00:04:29,880 --> 00:04:31,100
Давайте посмотрим на другой пример.

76
00:04:31,760 --> 00:04:34,431
Возьмем матрицу со столбцами 1,1 и отрицательными 2,0, 

77
00:04:34,431 --> 00:04:36,860
преобразование которой выглядит следующим образом.

78
00:04:37,980 --> 00:04:39,060
И назовем его М1.

79
00:04:40,100 --> 00:04:42,783
Далее возьмем матрицу со столбцами 0,1 и 2,0, 

80
00:04:42,783 --> 00:04:45,700
преобразование которой выглядит следующим образом.

81
00:04:47,520 --> 00:04:49,240
И назовем этого парня М2.

82
00:04:49,920 --> 00:04:53,813
Общий эффект от применения M1, а затем M2 дает нам новое преобразование, 

83
00:04:53,813 --> 00:04:55,680
поэтому давайте найдем его матрицу.

84
00:04:56,280 --> 00:04:59,575
Но на этот раз давайте посмотрим, сможем ли мы сделать это, 

85
00:04:59,575 --> 00:05:03,860
не просматривая анимацию, а просто используя числовые записи в каждой матрице.

86
00:05:04,740 --> 00:05:07,140
Для начала нам нужно выяснить, куда девается i-hat.

87
00:05:08,040 --> 00:05:11,518
После применения M1 новые координаты i-hat по 

88
00:05:11,518 --> 00:05:15,980
определению задаются этим первым столбцом M1, а именно 1,1.

89
00:05:16,780 --> 00:05:23,500
Чтобы увидеть, что произойдет после применения M2, умножьте матрицу M2 на этот вектор 1,1.

90
00:05:25,300 --> 00:05:29,880
Сделав это так, как я описал в прошлом видео, вы получите вектор 2,1.

91
00:05:30,700 --> 00:05:33,100
Это будет первый столбец матрицы композиции.

92
00:05:34,520 --> 00:05:37,665
Аналогично, следуя j-hat, второй столбец M1 сообщает нам, 

93
00:05:37,665 --> 00:05:40,540
что сначала он достигает отрицательного значения 2,0.

94
00:05:42,700 --> 00:05:48,880
Затем, когда мы применим M2 к этому вектору, вы можете вычислить произведение матрицы на 

95
00:05:48,880 --> 00:05:54,436
вектор, чтобы получить 0, минус 2, что становится вторым столбцом нашей матрицы 

96
00:05:54,436 --> 00:05:55,200
композиции.

97
00:05:56,640 --> 00:05:58,956
Позвольте мне еще раз рассказать об этом же процессе, 

98
00:05:58,956 --> 00:06:01,230
но на этот раз я покажу переменные в каждой матрице, 

99
00:06:01,230 --> 00:06:04,920
просто чтобы показать, что одна и та же цепочка рассуждений работает для любых матриц.

100
00:06:05,540 --> 00:06:07,961
Это более тяжелый символ и потребует больше места, 

101
00:06:07,961 --> 00:06:10,620
но он должен быть вполне удовлетворительным для любого, 

102
00:06:10,620 --> 00:06:13,660
кто ранее обучался умножению матриц более механическим способом.

103
00:06:14,460 --> 00:06:17,607
Чтобы проследить, куда движется i-шляпа, начните с рассмотрения первого 

104
00:06:17,607 --> 00:06:21,060
столбца матрицы справа, поскольку именно здесь изначально приземляется i-шляпа.

105
00:06:22,000 --> 00:06:25,623
Умножив этот столбец на матрицу слева, вы сможете определить, 

106
00:06:25,623 --> 00:06:30,300
где окажется промежуточная версия i-hat после применения второго преобразования.

107
00:06:31,620 --> 00:06:34,758
Таким образом, первый столбец матрицы композиции всегда будет 

108
00:06:34,758 --> 00:06:38,100
равен произведению левой матрицы на первый столбец правой матрицы.

109
00:06:42,160 --> 00:06:47,140
Аналогично, j-hat всегда изначально будет находиться во втором столбце правой матрицы.

110
00:06:48,940 --> 00:06:53,290
Таким образом, умножение левой матрицы на этот второй столбец даст ее окончательное 

111
00:06:53,290 --> 00:06:57,020
местоположение, и, следовательно, это второй столбец матрицы композиции.

112
00:07:00,620 --> 00:07:04,735
Обратите внимание, что здесь много символов, и эту формулу обычно преподают как нечто, 

113
00:07:04,735 --> 00:07:07,904
что нужно запомнить, а также определенный алгоритмический процесс, 

114
00:07:07,904 --> 00:07:09,040
помогающий ее запомнить.

115
00:07:09,160 --> 00:07:12,455
Но я действительно считаю, что прежде чем запоминать этот процесс, 

116
00:07:12,455 --> 00:07:15,800
вам следует привыкнуть думать о том, что на самом деле представляет 

117
00:07:15,800 --> 00:07:18,900
собой умножение матриц, применяя одно преобразование за другим.

118
00:07:19,620 --> 00:07:22,985
Поверьте мне, это даст вам гораздо лучшую концептуальную основу, 

119
00:07:22,985 --> 00:07:26,300
которая значительно облегчит понимание свойств умножения матриц.

120
00:07:27,060 --> 00:07:28,360
Например, вот вопрос.

121
00:07:28,880 --> 00:07:32,840
Имеет ли значение, в каком порядке мы помещаем две матрицы при их умножении?

122
00:07:33,620 --> 00:07:37,000
Что ж, давайте рассмотрим простой пример, подобный приведенному ранее.

123
00:07:37,640 --> 00:07:41,164
Возьмите ножницы, которые фиксируют i-хэт и смещают J-хэт вправо, 

124
00:07:41,164 --> 00:07:42,820
и поверните его на 90 градусов.

125
00:07:43,600 --> 00:07:47,222
Если вы сначала выполните сдвиг, а затем повернете, мы увидим, 

126
00:07:47,222 --> 00:07:50,960
что i-hat окажется на уровне 0,1, а j-hat — на отрицательном 1,1.

127
00:07:51,320 --> 00:07:53,060
Оба обычно направлены близко друг к другу.

128
00:07:53,860 --> 00:07:58,458
Если вы сначала повернете, а затем выполните сдвиг, i-шляпа окажется на уровне 1,1, 

129
00:07:58,458 --> 00:08:02,454
а j-шляпа сместится в другом направлении при отрицательном значении 1,0, 

130
00:08:02,454 --> 00:08:05,520
и они будут направлены, вы знаете, дальше друг от друга.

131
00:08:06,380 --> 00:08:10,660
Общий эффект здесь явно иной, поэтому очевидно, что порядок имеет значение.

132
00:08:12,200 --> 00:08:17,106
Обратите внимание: думая о трансформациях, вы можете делать такие вещи в своей голове, 

133
00:08:17,106 --> 00:08:17,840
визуализируя.

134
00:08:18,220 --> 00:08:19,900
Матричное умножение не требуется.

135
00:08:21,480 --> 00:08:25,551
Помню, когда я впервые изучал линейную алгебру, у нас была одна домашняя задача, 

136
00:08:25,551 --> 00:08:29,120
в которой нам предлагалось доказать, что умножение матриц ассоциативно.

137
00:08:29,560 --> 00:08:34,328
Это означает, что если у вас есть три матрицы: A, B и C, и вы умножаете их все вместе, 

138
00:08:34,328 --> 00:08:37,782
не имеет значения, вычислите ли вы сначала A, умноженное на B, 

139
00:08:37,782 --> 00:08:42,112
а затем умножив результат на C, или сначала вы умножите B, умноженное на B. C, 

140
00:08:42,112 --> 00:08:44,360
затем умножьте этот результат на A слева.

141
00:08:44,940 --> 00:08:47,400
Другими словами, не имеет значения, где вы поставите скобки.

142
00:08:48,380 --> 00:08:52,349
Теперь, если вы попытаетесь проработать это численно, как я это сделал тогда, 

143
00:08:52,349 --> 00:08:55,760
это будет ужасно, просто ужасно и, если уж на то пошло, бесполезно.

144
00:08:55,760 --> 00:09:00,474
Но когда вы думаете об умножении матриц как о применении одного преобразования за другим, 

145
00:09:00,474 --> 00:09:02,780
это свойство оказывается просто тривиальным.

146
00:09:03,300 --> 00:09:04,000
Вы понимаете, почему?

147
00:09:04,860 --> 00:09:08,620
Речь идет о том, что если вы сначала применяете C, затем B, 

148
00:09:08,620 --> 00:09:12,380
затем A, это то же самое, что применять C, затем B, затем A.

149
00:09:12,820 --> 00:09:14,380
Я имею в виду, что нечего доказывать.

150
00:09:14,540 --> 00:09:18,660
Вы просто применяете одни и те же три вещи одну за другой, все в одном и том же порядке.

151
00:09:19,460 --> 00:09:21,540
Это может показаться мошенничеством, но это не так.

152
00:09:21,540 --> 00:09:26,081
Это честное доказательство того, что умножение матриц является ассоциативным, и, 

153
00:09:26,081 --> 00:09:30,680
более того, это хорошее объяснение того, почему это свойство должно быть истинным.

154
00:09:31,560 --> 00:09:34,815
Я действительно советую вам больше экспериментировать с этой идеей, 

155
00:09:34,815 --> 00:09:38,070
представляя два разных преобразования, думая о том, что происходит, 

156
00:09:38,070 --> 00:09:42,140
когда вы применяете одно за другим, а затем численно вычисляя матричное произведение.

157
00:09:42,600 --> 00:09:46,440
Поверьте мне, это тот вид игры, который действительно помогает усвоить идею.

158
00:09:47,200 --> 00:09:52,180
В следующем видео я начну говорить о расширении этих идей за пределы двух измерений.

