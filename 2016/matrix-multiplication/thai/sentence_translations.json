[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices.",
  "translatedText": "สวัสดีทุกคน จากที่ค้างไว้ล่าสุด ผมได้แสดงให้เห็นว่าการแปลงเชิงเส้นมีหน้าตาเป็นอย่างไร และจะแสดงมันอย่างไรโดยใช้เมทริกซ์ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video.",
  "translatedText": "การสรุปสั้นๆ นี้คุ้มค่าแก่การสรุปสั้นๆ เพราะมันสำคัญจริงๆ แต่แน่นอนว่าหากรู้สึกเหมือนเป็นมากกว่าการสรุป ให้ย้อนกลับไปดูวิดีโอฉบับเต็ม ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Technically speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed.",
  "translatedText": "ในทางเทคนิคแล้ว การแปลงเชิงเส้นเป็นฟังก์ชันที่มีเวกเตอร์เป็นอินพุต และเวกเตอร์เป็นเอาท์พุต แต่ฉันได้แสดงไปแล้วครั้งล่าสุดว่าเราจะคิดถึงพวกมันได้อย่างไรด้วยการทำให้เรียบไปรอบๆ อวกาศ ในลักษณะที่เส้นกริดยังคงขนานกันและมีระยะห่างเท่าๆ กัน และเพื่อให้จุดกำเนิด ยังคงคงที่",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat.",
  "translatedText": "ประเด็นสำคัญคือการแปลงเชิงเส้นถูกกำหนดโดยสมบูรณ์โดยที่เวกเตอร์พื้นฐานของปริภูมิ ซึ่งสำหรับสองมิติจะหมายถึง i-hat และ j-hat ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors.",
  "translatedText": "เนื่องจากเวกเตอร์อื่นๆ สามารถอธิบายได้ว่าเป็นผลรวมเชิงเส้นของเวกเตอร์พื้นฐานเหล่านั้น ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat.",
  "translatedText": "เวกเตอร์ที่มีพิกัด x, y คือ x คูณ i-hat บวก y คูณ j-hat ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence.",
  "translatedText": "หลังจากผ่านการเปลี่ยนแปลง คุณสมบัตินี้ซึ่งเส้นกริดยังคงขนานกันและมีระยะห่างเท่าๆ กัน ส่งผลที่ตามมาอย่างยอดเยี่ยม ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat.",
  "translatedText": "จุดที่เวกเตอร์ของคุณตกลงมา จะเป็น x คูณ i-hat เวอร์ชันที่แปลงแล้ว บวก y คูณด้วย j-hat เวอร์ชันที่แปลงแล้ว ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat.",
  "translatedText": "ซึ่งหมายความว่าหากคุณเก็บบันทึกพิกัดที่ i-hat ตกลงและพิกัดที่ j-hat ตกลง คุณสามารถคำนวณเวกเตอร์ที่เริ่มต้นที่ x ได้ y ต้องลงบน x คูณพิกัดใหม่ของ i-hat บวก y คูณพิกัดใหม่ของ j-hat ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication.",
  "translatedText": "แบบแผนคือการบันทึกพิกัดของที่ i-hat และ j-hat ลงจอดเป็นคอลัมน์ของเมทริกซ์ และเพื่อนิยามผลรวมของเวอร์ชันที่ปรับขนาดของคอลัมน์เหล่านั้นด้วย x และ y เพื่อให้เป็นการคูณเมทริกซ์-เวกเตอร์ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector.",
  "translatedText": "ด้วยวิธีนี้ เมทริกซ์แทนการแปลงเชิงเส้นเฉพาะ และการคูณเมทริกซ์ด้วยเวกเตอร์คือความหมายในการคำนวณเพื่อใช้การแปลงนั้นกับเวกเตอร์นั้น ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff.",
  "translatedText": "เอาล่ะ สรุปเรื่องใหม่กันต่อ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes, you find yourself wanting to describe the effects of applying one transformation and then another.",
  "translatedText": "บ่อยครั้ง คุณพบว่าตัวเองต้องการอธิบายผลกระทบของการใช้การเปลี่ยนแปลงครั้งแล้วครั้งเล่า",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear.",
  "translatedText": "ตัวอย่างเช่น คุณอาจต้องการอธิบายว่าจะเกิดอะไรขึ้นเมื่อคุณหมุนเครื่องบิน 90 องศาทวนเข็มนาฬิกาเป็นครั้งแรก จากนั้นจึงใช้แรงเฉือน ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear.",
  "translatedText": "ผลกระทบโดยรวมตั้งแต่ต้นจนจบคือการเปลี่ยนแปลงเชิงเส้นอีกรูปแบบหนึ่ง แตกต่างจากการหมุนและแรงเฉือน ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied.",
  "translatedText": "การแปลงเชิงเส้นใหม่นี้มักเรียกว่าองค์ประกอบของการแปลงสองแบบแยกกันที่เรานำไปใช้ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat.",
  "translatedText": "และเช่นเดียวกับการแปลงเชิงเส้นใดๆ, มันสามารถอธิบายได้ด้วยเมทริกซ์ของมันเอง โดยทำตาม i-hat และ j-hat ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix.",
  "translatedText": "ในตัวอย่างนี้ จุดลงจอดสุดท้ายสำหรับ i-hat หลังจากการแปลงทั้งสองคือ 1,1 ดังนั้นเรามาสร้างคอลัมน์แรกของเมทริกซ์กันดีกว่า ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix.",
  "translatedText": "ในทำนองเดียวกัน j-hat สุดท้ายก็จบลงที่ตำแหน่งลบ 1,0 เราจึงสร้างคอลัมน์ที่สองของเมทริกซ์ขึ้นมา ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones.",
  "translatedText": "เมทริกซ์ใหม่นี้จับผลกระทบโดยรวมของการหมุนตามด้วยแรงเฉือน แต่เป็นการกระทำเดี่ยวๆ แทนที่จะเป็นสองครั้งติดต่อกัน ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix.",
  "translatedText": "นี่เป็นวิธีคิดอย่างหนึ่งเกี่ยวกับเมทริกซ์ใหม่นั้น ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix, then take whatever you get and multiply that on the left by the shear matrix.",
  "translatedText": "หากคุณต้องใช้เวกเตอร์และปั๊มมันผ่านการหมุน จากนั้นแรงเฉือน วิธียาวในการคำนวณว่ามันจะจบลงที่ตรงไหน คือการคูณมันทางด้านซ้ายด้วยเมทริกซ์การหมุน จากนั้นนำสิ่งที่คุณได้มาคูณกับมันบน เหลือไว้ด้วยเมทริกซ์เฉือน",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 185.42,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector.",
  "translatedText": "หากพูดเป็นตัวเลขแล้ว การใช้การหมุนตามด้วยแรงเฉือนกับเวกเตอร์ที่กำหนดหมายความว่าอย่างไร ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action.",
  "translatedText": "แต่สิ่งที่คุณได้ควรจะเหมือนกับการใช้เมทริกซ์การจัดองค์ประกอบใหม่ ที่เราเพิ่งพบโดยเวกเตอร์เดียวกันนั้น ไม่ว่าคุณจะเลือกเวกเตอร์ใดก็ตาม เนื่องจากเมทริกซ์ใหม่นี้ควรจะจับเอฟเฟกต์โดยรวมเหมือนกับการหมุนและแรงเฉือน ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you?",
  "translatedText": "จากวิธีที่เขียนไว้ตรงนี้ ฉันคิดว่ามันสมเหตุสมผลที่จะเรียกเมทริกซ์ใหม่นี้ว่าผลคูณของเมทริกซ์สองตัวดั้งเดิม ใช่ไหม? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers.",
  "translatedText": "เราสามารถคิดถึงวิธีคำนวณผลิตภัณฑ์นั้นโดยทั่วไปได้ในเวลาเพียงชั่วครู่ แต่มันง่ายเกินไปที่จะหลงทางในป่าแห่งตัวเลข ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another.",
  "translatedText": "โปรดจำไว้เสมอว่าการคูณเมทริกซ์สองตัวแบบนี้มีความหมายทางเรขาคณิตของการแปลงหนึ่งและอีกการแปลงหนึ่ง ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left.",
  "translatedText": "สิ่งหนึ่งที่แปลกตรงนี้ก็คือ เราอ่านจากขวาไปซ้าย ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left.",
  "translatedText": "ขั้นแรกคุณใช้การแปลงที่แสดงโดยเมทริกซ์ทางขวา จากนั้นจึงใช้การแปลงที่แสดงโดยเมทริกซ์ทางด้านซ้าย ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left.",
  "translatedText": "สิ่งนี้มีต้นกำเนิดมาจากสัญลักษณ์ฟังก์ชัน เนื่องจากเราเขียนฟังก์ชันทางด้านซ้ายของตัวแปร ดังนั้นทุกครั้งที่คุณเขียนสองฟังก์ชัน คุณจะต้องอ่านจากขวาไปซ้ายเสมอ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us.",
  "translatedText": "ข่าวดีสำหรับผู้อ่านชาวฮีบรู ข่าวร้ายสำหรับพวกเราที่เหลือ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example.",
  "translatedText": "ลองดูอีกตัวอย่างหนึ่ง ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.",
  "translatedText": "หาเมทริกซ์ที่มีคอลัมน์ 1,1 และลบ 2,0 ซึ่งการแปลงจะเป็นดังนี้ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it M1.",
  "translatedText": "และเรียกมันว่า M1",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.",
  "translatedText": "ต่อไป นำเมทริกซ์ที่มีคอลัมน์ 0,1 และ 2,0 ซึ่งการเปลี่ยนแปลงมีลักษณะเช่นนี้ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy M2.",
  "translatedText": "และลองเรียกเจ้านั่นว่า M2",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying M1 then M2 gives us a new transformation, so let's find its matrix.",
  "translatedText": "ผลรวมของการใช้ M1 แล้ว M2 ให้การแปลงใหม่ ลองหาเมทริกซ์ของมันกัน",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix.",
  "translatedText": "แต่คราวนี้ เรามาดูกันว่าเราจะสามารถทำได้โดยไม่ต้องดูภาพเคลื่อนไหว และใช้เพียงรายการตัวเลขในแต่ละเมทริกซ์แทน ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes.",
  "translatedText": "ก่อนอื่น เราต้องหาก่อนว่า i-hat ไปไหน ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying M1, the new coordinates of i-hat, by definition, are given by that first column of M1, namely 1,1.",
  "translatedText": "หลังจากใช้ M1 แล้ว ตามคำจำกัดความ พิกัดใหม่ของ i-hat จะได้รับจากคอลัมน์แรกของ M1 ซึ่งก็คือ 1,1",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying M2, multiply the matrix for M2 by that vector 1,1.",
  "translatedText": "หากต้องการดูว่าเกิดอะไรขึ้นหลังจากใช้ M2 ให้คูณเมทริกซ์สำหรับ M2 ด้วยเวกเตอร์ 1,1",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1.",
  "translatedText": "ลองดูอย่างที่ผมอธิบายในวิดีโอที่แล้ว คุณจะได้เวกเตอร์ 2,1 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix.",
  "translatedText": "นี่จะเป็นคอลัมน์แรกของเมทริกซ์องค์ประกอบ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of M1 tells us that it first lands on negative 2,0.",
  "translatedText": "ในทำนองเดียวกัน เพื่อติดตาม j-hat คอลัมน์ที่สองของ M1 บอกเราว่ามันตกลงไปที่ลบ 2,0 ก่อน",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply M2 to that vector, you can work out the matrix-vector product to get 0, negative 2, which becomes the second column of our composition matrix.",
  "translatedText": "จากนั้น เมื่อเราใส่ M2 กับเวกเตอร์นั้น คุณสามารถหาผลคูณเมทริกซ์-เวกเตอร์ได้ 0 ลบ 2 ซึ่งกลายเป็นคอลัมน์ที่สองของเมทริกซ์ประกอบของเรา",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices.",
  "translatedText": "ขอผมพูดถึงกระบวนการเดิมอีกครั้ง แต่คราวนี้ ผมจะแสดงรายการตัวแปรในแต่ละเมทริกซ์ เพื่อแสดงว่าการใช้เหตุผลบรรทัดเดียวกันใช้ได้กับเมทริกซ์ใดๆ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way.",
  "translatedText": "นี่เป็นสัญลักษณ์ที่หนักกว่าและจะต้องมีพื้นที่เพิ่มขึ้น แต่ก็น่าพอใจสำหรับทุกคนที่เคยสอนการคูณเมทริกซ์ด้วยวิธีท่องจำมากกว่ามาก่อน ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands.",
  "translatedText": "หากต้องการติดตามว่า i-hat ไปตรงไหน ให้เริ่มด้วยการดูคอลัมน์แรกของเมทริกซ์ทางด้านขวา เนื่องจากนี่คือจุดที่ i-hat ตกลงมาในตอนแรก ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation.",
  "translatedText": "การคูณคอลัมน์นั้นด้วยเมทริกซ์ทางด้านซ้ายคือวิธีที่คุณสามารถบอกได้ว่า i-hat เวอร์ชันกลางจบลงที่ใดหลังจากใช้การแปลงครั้งที่สอง ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix.",
  "translatedText": "คอลัมน์แรกของเมทริกซ์องค์ประกอบจะเท่ากับเมทริกซ์ด้านซ้ายเสมอ คูณคอลัมน์แรกของเมทริกซ์ด้านขวาเสมอ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix.",
  "translatedText": "ในทำนองเดียวกัน j-hat จะลงบนคอลัมน์ที่สองของเมทริกซ์ด้านขวาเสมอ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix.",
  "translatedText": "ดังนั้นการคูณเมทริกซ์ทางซ้ายด้วยคอลัมน์ที่สองจะได้ตำแหน่งสุดท้าย และนั่นคือคอลัมน์ที่สองของเมทริกซ์องค์ประกอบ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to help remember it.",
  "translatedText": "สังเกตว่ามีสัญลักษณ์มากมายที่นี่ และเป็นเรื่องปกติที่จะต้องสอนสูตรนี้ให้เป็นสิ่งที่ต้องจดจำ ควบคู่ไปกับกระบวนการอัลกอริธึมบางอย่างเพื่อช่วยจดจำ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another.",
  "translatedText": "แต่ฉันคิดจริงๆ ว่าก่อนที่จะจำกระบวนการนั้น คุณควรมีนิสัยคิดว่าการคูณเมทริกซ์แทนจริงๆ แล้วใช้การแปลงครั้งแล้วครั้งเล่า ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand.",
  "translatedText": "เชื่อฉันเถอะ นี่จะทำให้คุณมีกรอบแนวคิดที่ดีขึ้นมาก ซึ่งทำให้คุณสมบัติของการคูณเมทริกซ์เข้าใจง่ายขึ้นมาก ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question.",
  "translatedText": "ตัวอย่างเช่น นี่คือคำถาม ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them?",
  "translatedText": "สำคัญไหมที่เราใส่เมทริกซ์สองตัวนั้นลงไปตามลำดับอะไรเมื่อเราคูณมัน? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier.",
  "translatedText": "ลองคิดถึงตัวอย่างง่ายๆ เหมือนกับตัวอย่างก่อนหน้านี้ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smooshes j-hat over to the right, and a 90 degree rotation.",
  "translatedText": "ใช้แรงเฉือน ซึ่งจะยึด i-hat และเลื่อน j-hat ไปทางขวา และหมุน 90 องศา",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1.",
  "translatedText": "หากคุณทำแรงเฉือนก่อน แล้วหมุน เราจะเห็นว่า i-hat จบลงที่ 0,1 และ j-hat จบลงที่ลบ 1,1 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together.",
  "translatedText": "โดยทั่วไปทั้งสองจะชี้ใกล้กัน ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing, you know, farther apart.",
  "translatedText": "หากคุณหมุนครั้งแรก, แล้วทำแรงเฉือน, i-hat จบลงที่ 1,1, และ j-hat ออกไปอีกทางหนึ่งที่ลบ 1,0 และพวกมันชี้ออกจากกันมากขึ้น ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently, order totally does matter.",
  "translatedText": "ผลกระทบโดยรวมที่นี่แตกต่างอย่างชัดเจน ดังนั้นเห็นได้ชัดว่าลำดับมีความสำคัญโดยสิ้นเชิง",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice, by thinking in terms of transformations, that's the kind of thing that you can do in your head by visualizing.",
  "translatedText": "สังเกตว่า ด้วยการคิดในแง่ของการเปลี่ยนแปลง นั่นคือสิ่งที่คุณสามารถทำได้ในหัวด้วยการมองเห็นภาพ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary.",
  "translatedText": "ไม่จำเป็นต้องคูณเมทริกซ์ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative.",
  "translatedText": "ผมจำได้ว่าตอนผมเรียนพีชคณิตเชิงเส้นครั้งแรก มีปัญหาการบ้านข้อหนึ่งที่ขอให้เราพิสูจน์ว่าการคูณเมทริกซ์มีความสัมพันธ์กัน ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left.",
  "translatedText": "ซึ่งหมายความว่า หากคุณมีเมทริกซ์ 3 ตัวคือ A, B และ C และคุณคูณเมทริกซ์ทั้งหมดเข้าด้วยกัน ไม่สำคัญว่าคุณจะคำนวณ A คูณ B เป็นครั้งแรก จากนั้นจึงคูณผลลัพธ์ด้วย C หรือหากคุณคูณ B เป็นครั้งแรก C แล้วคูณผลลัพธ์นั้นด้วย A ทางด้านซ้าย ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses.",
  "translatedText": "กล่าวอีกนัยหนึ่ง ไม่สำคัญว่าคุณจะใส่วงเล็บไว้ที่ใด ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter.",
  "translatedText": "ทีนี้ ถ้าคุณพยายามที่จะจัดการกับสิ่งนี้ในเชิงตัวเลข เหมือนอย่างที่ฉันเคยทำในตอนนั้น มันแย่มาก แย่มาก และไม่กระจ่างแจ้งสำหรับเรื่องนั้น ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial.",
  "translatedText": "แต่เมื่อคุณคิดถึงการคูณเมทริกซ์ ว่าเป็นการแปลงครั้งแล้วครั้งเล่า คุณสมบัตินี้ไม่สำคัญ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why?",
  "translatedText": "คุณเห็นไหมว่าทำไม? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C, then B, then A, it's the same as applying C, then B, then A.",
  "translatedText": "สิ่งที่บอกคือว่า ถ้าคุณใส่ C ครั้งแรก แล้วก็ B แล้วก็ A มันก็เหมือนกับการใส่ C แล้วก็ B แล้วก็ A",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove.",
  "translatedText": "ฉันหมายความว่าไม่มีอะไรที่จะพิสูจน์",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 552.82,
  "end": 554.38
 },
 {
  "input": "You're just applying the same three things one after the other, all in the same order.",
  "translatedText": "คุณแค่ใช้สามสิ่งเดียวกันติดกัน ทั้งหมดอยู่ในลำดับเดียวกัน",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 554.54,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not.",
  "translatedText": "นี่อาจรู้สึกเหมือนเป็นการโกง แต่ก็ไม่เป็นเช่นนั้น ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative, and even better than that, it's a good explanation for why that property should be true.",
  "translatedText": "นี่เป็นข้อพิสูจน์โดยสุจริตว่าการคูณเมทริกซ์มีความเชื่อมโยง และที่ดียิ่งกว่านั้น มันเป็นคำอธิบายที่ดีว่าทำไมคุณสมบัตินั้นจึงควรเป็นจริง",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 561.54,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically.",
  "translatedText": "ฉันขอแนะนำให้คุณลองใช้แนวคิดนี้มากขึ้น ลองจินตนาการถึงการแปลงสองแบบที่ต่างกัน ลองนึกถึงสิ่งที่เกิดขึ้นเมื่อคุณใช้ทีละอัน แล้วหาผลคูณเมทริกซ์เป็นตัวเลข ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in.",
  "translatedText": "เชื่อฉันเถอะ นี่เป็นช่วงเวลาเล่นที่ทำให้ไอเดียนี้ฝังลึกลงไปจริงๆ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions. See you then!",
  "translatedText": "ในวิดีโอหน้า ฉันจะเริ่มพูดถึงการขยายแนวคิดเหล่านี้ให้มากกว่าแค่สองมิติ งั้นไว้เจอกันใหม่!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.2,
  "end": 592.18
 }
]