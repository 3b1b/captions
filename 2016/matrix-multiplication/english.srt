1
00:00:00,000 --> 00:00:15,240
Hey everyone, where we last left off, I showed what linear transformations look like and

2
00:00:15,240 --> 00:00:18,360
how to represent them using matrices.

3
00:00:18,360 --> 00:00:22,320
This is worth a quick recap because it's just really important, but of course if this feels

4
00:00:22,320 --> 00:00:26,280
like more than just a recap, go back and watch the full video.

5
00:00:26,280 --> 00:00:30,700
Generally speaking, linear transformations are functions with vectors as inputs and vectors

6
00:00:30,700 --> 00:00:34,760
as outputs, but I showed last time how we can think about them visually as smooshing

7
00:00:34,760 --> 00:00:39,760
around space in such a way that grid lines stay parallel and evenly spaced, and so that

8
00:00:39,760 --> 00:00:41,840
the origin remains fixed.

9
00:00:41,840 --> 00:00:46,860
The key takeaway was that a linear transformation is completely determined by where it takes

10
00:00:46,860 --> 00:00:52,260
the basis vectors of the space, which for two dimensions means i-hat and j-hat.

11
00:00:52,260 --> 00:00:56,500
This is because any other vector could be described as a linear combination of those

12
00:00:56,500 --> 00:00:57,820
basis vectors.

13
00:00:57,820 --> 00:01:03,460
A vector with coordinates x, y is x times i-hat plus y times j-hat.

14
00:01:03,460 --> 00:01:07,540
After going through the transformation, this property that grid lines remain parallel and

15
00:01:07,540 --> 00:01:10,600
evenly spaced has a wonderful consequence.

16
00:01:10,600 --> 00:01:15,180
The place where your vector lands will be x times the transformed version of i-hat plus

17
00:01:15,180 --> 00:01:18,440
y times the transformed version of j-hat.

18
00:01:18,440 --> 00:01:22,960
This means if you keep a record of the coordinates where i-hat lands and the coordinates where

19
00:01:22,960 --> 00:01:28,940
j-hat lands, you can compute that a vector which starts at x, y must land on x times

20
00:01:28,940 --> 00:01:33,600
the new coordinates of i-hat plus y times the new coordinates of j-hat.

21
00:01:33,600 --> 00:01:38,000
The convention is to record the coordinates of where i-hat and j-hat land as the columns

22
00:01:38,000 --> 00:01:42,820
of a matrix, and to define this sum of the scaled versions of those columns by x and

23
00:01:42,820 --> 00:01:46,280
y to be matrix-vector multiplication.

24
00:01:46,280 --> 00:01:51,320
In this way, a matrix represents a specific linear transformation, and multiplying a matrix

25
00:01:51,320 --> 00:01:58,040
by a vector is what it means computationally to apply that transformation to that vector.

26
00:01:58,040 --> 00:02:01,760
Alright, recap over, on to the new stuff.

27
00:02:01,760 --> 00:02:06,160
Oftentimes you find yourself wanting to describe the effects of applying one transformation

28
00:02:06,160 --> 00:02:07,680
and then another.

29
00:02:07,680 --> 00:02:11,760
For example, maybe you want to describe what happens when you first rotate the plane 90

30
00:02:11,760 --> 00:02:15,440
degrees counterclockwise, then apply a shear.

31
00:02:15,440 --> 00:02:20,360
The overall effect here, from start to finish, is another linear transformation, distinct

32
00:02:20,360 --> 00:02:22,540
from the rotation and the shear.

33
00:02:22,540 --> 00:02:26,920
This new linear transformation is commonly called the composition of the two separate

34
00:02:26,920 --> 00:02:29,040
transformations we applied.

35
00:02:29,040 --> 00:02:33,480
And like any linear transformation, it can be described with a matrix all of its own

36
00:02:33,480 --> 00:02:36,280
by following i-hat and j-hat.

37
00:02:36,280 --> 00:02:42,360
In this example, the ultimate landing spot for i-hat after both transformations is 1,1,

38
00:02:42,360 --> 00:02:44,800
so let's make that the first column of a matrix.

39
00:02:44,840 --> 00:02:50,320
Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the

40
00:02:50,320 --> 00:02:52,800
second column of the matrix.

41
00:02:52,800 --> 00:02:58,300
This new matrix captures the overall effect of applying a rotation then a shear, but as

42
00:02:58,300 --> 00:03:03,400
one single action, rather than two successive ones.

43
00:03:03,400 --> 00:03:05,480
Here's one way to think about that new matrix.

44
00:03:05,480 --> 00:03:09,760
If you were to take some vector and pump it through the rotation, then the shear, the

45
00:03:09,760 --> 00:03:14,360
long way to compute where it ends up is to first multiply it on the left by the rotation

46
00:03:14,400 --> 00:03:15,400
matrix.

47
00:03:15,400 --> 00:03:20,520
Then, take whatever you get and multiply that on the left by the shear matrix.

48
00:03:20,520 --> 00:03:26,000
This is, numerically speaking, what it means to apply a rotation then a shear to a given

49
00:03:26,000 --> 00:03:27,000
vector.

50
00:03:27,000 --> 00:03:30,720
But whatever you get should be the same as just applying this new composition matrix

51
00:03:30,720 --> 00:03:35,560
that we just found by that same vector, no matter what vector you chose, since this new

52
00:03:35,560 --> 00:03:42,720
matrix is supposed to capture the same overall effect as the rotation then shear action.

53
00:03:42,720 --> 00:03:45,940
Based on how things are written down here, I think it's reasonable to call this new

54
00:03:45,940 --> 00:03:50,640
matrix the product of the original two matrices, don't you?

55
00:03:50,640 --> 00:03:54,460
We can think about how to compute that product more generally in just a moment, but it's

56
00:03:54,460 --> 00:03:57,440
way too easy to get lost in the forest of numbers.

57
00:03:57,440 --> 00:04:02,280
Always remember that multiplying two matrices like this has the geometric meaning of applying

58
00:04:02,280 --> 00:04:06,340
one transformation then another.

59
00:04:06,340 --> 00:04:10,080
One thing that's kind of weird here is that this has us reading from right to left.

60
00:04:10,080 --> 00:04:14,160
You first apply the transformation represented by the matrix on the right, then you apply

61
00:04:14,160 --> 00:04:17,600
the transformation represented by the matrix on the left.

62
00:04:17,600 --> 00:04:21,940
This stems from function notation, since we write functions on the left of variables,

63
00:04:21,940 --> 00:04:26,160
so every time you compose two functions, you always have to read it right to left.

64
00:04:26,160 --> 00:04:30,080
Good news for the Hebrew readers, bad news for the rest of us.

65
00:04:30,080 --> 00:04:31,880
Let's look at another example.

66
00:04:31,880 --> 00:04:38,160
Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.

67
00:04:38,240 --> 00:04:40,000
And let's call it m1.

68
00:04:40,000 --> 00:04:46,000
Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.

69
00:04:47,840 --> 00:04:50,040
And let's call that guy m2.

70
00:04:50,040 --> 00:04:55,560
The total effect of applying m1 then m2 gives us a new transformation, so let's find its

71
00:04:55,560 --> 00:04:56,560
matrix.

72
00:04:56,560 --> 00:05:00,940
But this time, let's see if we can do it without watching the animations, and instead

73
00:05:00,940 --> 00:05:04,480
just using the numerical entries in each matrix.

74
00:05:04,480 --> 00:05:08,040
First, we need to figure out where i-hat goes.

75
00:05:08,280 --> 00:05:13,560
After applying m1, the new coordinates of i-hat, by definition, are given by that first

76
00:05:13,560 --> 00:05:16,960
column of m1, namely 1,1.

77
00:05:16,960 --> 00:05:23,960
To see what happens after applying m2, multiply the matrix for m2 by that vector 1,1.

78
00:05:25,720 --> 00:05:30,860
Working it out, the way I described last video, you'll get the vector 2,1.

79
00:05:30,860 --> 00:05:33,960
This will be the first column of the composition matrix.

80
00:05:34,160 --> 00:05:40,000
Likewise, to follow j-hat, the second column of m1 tells us that it first lands on negative

81
00:05:40,000 --> 00:05:42,000
2,0.

82
00:05:42,000 --> 00:05:50,000
Then, when we apply m2 to that vector, you can work out the matrix vector product to

83
00:05:50,240 --> 00:05:57,040
get 0, negative 2, which becomes the second column of our composition matrix.

84
00:05:57,040 --> 00:06:01,060
Let me talk through that same process again, but this time I'll show variable entries

85
00:06:01,060 --> 00:06:05,620
in each matrix, just to show that the same line of reasoning works for any matrices.

86
00:06:05,620 --> 00:06:09,560
This is more symbol-heavy and will require some more room, but it should be pretty satisfying

87
00:06:09,560 --> 00:06:14,580
for anyone who has previously been taught matrix multiplication the more rote way.

88
00:06:14,580 --> 00:06:19,180
To follow where i-hat goes, start by looking at the first column of the matrix on the right,

89
00:06:19,180 --> 00:06:22,440
since this is where i-hat initially lands.

90
00:06:22,440 --> 00:06:26,860
Multiplying that column by the matrix on the left is how you can tell where the intermediate

91
00:06:26,860 --> 00:06:31,780
version of i-hat ends up after applying the second transformation.

92
00:06:31,780 --> 00:06:36,380
So the first column of the composition matrix will always equal the left matrix times the

93
00:06:36,380 --> 00:06:39,380
first column of the right matrix.

94
00:06:39,380 --> 00:06:46,380
Likewise, j-hat will always initially land on the second column of the right matrix.

95
00:06:48,960 --> 00:06:54,540
So multiplying the left matrix by this second column will give its final location, and hence

96
00:06:54,740 --> 00:07:00,740
that's the second column of the composition matrix.

97
00:07:00,740 --> 00:07:04,460
Notice there's a lot of symbols here, and it's common to be taught this formula as something

98
00:07:04,460 --> 00:07:09,320
to memorize, along with a certain algorithmic process to kind of help remember it.

99
00:07:09,320 --> 00:07:13,100
But I really do think that before memorizing that process, you should get in the habit

100
00:07:13,100 --> 00:07:18,140
of thinking about what matrix multiplication really represents, applying one transformation

101
00:07:18,140 --> 00:07:19,660
after another.

102
00:07:19,660 --> 00:07:23,600
Trust me, this will give you a much better conceptual framework that makes the properties

103
00:07:23,640 --> 00:07:27,160
of matrix multiplication much easier to understand.

104
00:07:27,160 --> 00:07:29,080
For example, here's a question.

105
00:07:29,080 --> 00:07:33,480
Does it matter what order we put the two matrices in when we multiply them?

106
00:07:33,480 --> 00:07:37,760
Well, let's think through a simple example, like the one from earlier.

107
00:07:37,760 --> 00:07:43,700
Take a shear, which fixes i-hat and smushes j-hat over to the right, and a 90 degree rotation.

108
00:07:43,700 --> 00:07:49,560
If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat

109
00:07:49,600 --> 00:07:51,480
ends up at negative 1,1.

110
00:07:51,480 --> 00:07:54,000
Both are generally pointing close together.

111
00:07:54,000 --> 00:08:01,000
If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a

112
00:08:01,420 --> 00:08:06,440
different direction at negative 1,0, and they're pointing farther apart.

113
00:08:06,440 --> 00:08:12,480
The overall effect here is clearly different, so evidently order totally does matter.

114
00:08:12,480 --> 00:08:16,520
Notice by thinking in terms of transformations, that's the kind of thing you can do in your

115
00:08:16,520 --> 00:08:18,360
head by visualizing.

116
00:08:18,360 --> 00:08:21,800
No matrix multiplication necessary.

117
00:08:21,800 --> 00:08:26,020
I remember when I first took linear algebra, there was this one homework problem that asked

118
00:08:26,020 --> 00:08:29,780
us to prove that matrix multiplication is associative.

119
00:08:29,780 --> 00:08:34,660
This means that if you have three matrices, A, B, and C, and you multiply them all together,

120
00:08:34,660 --> 00:08:39,840
it shouldn't matter if you first compute A times B, then multiply the result by C, or

121
00:08:39,840 --> 00:08:45,060
if you first multiply B times C, then multiply that result by A on the left.

122
00:08:45,060 --> 00:08:48,100
In other words, it doesn't matter where you put the parentheses.

123
00:08:48,100 --> 00:08:53,340
Now, if you try to work through this numerically, like I did back then, it's horrible, just

124
00:08:53,340 --> 00:08:56,420
horrible, and unenlightening for that matter.

125
00:08:56,420 --> 00:09:01,380
But when you think about matrix multiplication as applying one transformation after another,

126
00:09:01,380 --> 00:09:03,460
this property is just trivial.

127
00:09:03,460 --> 00:09:05,060
Can you see why?

128
00:09:05,060 --> 00:09:10,700
What it's saying is that if you first apply C then B, then A, it's the same as applying

129
00:09:10,700 --> 00:09:13,060
C, then B, then A.

130
00:09:13,060 --> 00:09:16,940
I mean, there's nothing to prove, you're just applying the same three things one after

131
00:09:16,940 --> 00:09:19,680
the other, all in the same order.

132
00:09:19,680 --> 00:09:22,080
This might feel like cheating, but it's not.

133
00:09:22,080 --> 00:09:26,360
This is an honest-to-goodness proof that matrix multiplication is associative.

134
00:09:26,360 --> 00:09:31,820
And even better than that, it's a good explanation for why that property should be true.

135
00:09:31,820 --> 00:09:37,020
I really do encourage you to play around more with this idea, imagining two different transformations,

136
00:09:37,020 --> 00:09:40,560
thinking about what happens when you apply one after the other, and then working out

137
00:09:40,560 --> 00:09:42,700
the matrix product numerically.

138
00:09:42,700 --> 00:09:47,460
Trust me, this is the kind of playtime that really makes the idea sink in.

139
00:09:47,460 --> 00:09:52,060
In the next video, I'll start talking about extending these ideas beyond just two dimensions.

140
00:09:52,060 --> 00:09:52,340
See you then!

