[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices.",
  "translatedText": "Усім привіт! Там, де ми зупинилися, я показав, як виглядають лінійні перетворення та як їх представити за допомогою матриць.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video.",
  "translatedText": "Це варто короткого підсумку, оскільки це просто дуже важливо, але, звісно, якщо вам здається, що це більше, ніж просто підсумок, поверніться назад і подивіться повне відео.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Technically speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed.",
  "translatedText": "Технічно кажучи, лінійні перетворення - це функції з векторами на входах і векторами на виходах, але минулого разу я показав, як ми можемо думати про них візуально, як про згладжування простору таким чином, щоб лінії сітки залишалися паралельними і рівномірно розподіленими, і щоб початок координат залишався фіксованим.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat.",
  "translatedText": "Ключовим висновком було те, що лінійне перетворення повністю визначається тим, де воно бере базисні вектори простору, що для двох вимірів означає i-hat і j-hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors.",
  "translatedText": "Це тому, що будь-який інший вектор можна описати як лінійну комбінацію цих базисних векторів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat.",
  "translatedText": "Вектор із координатами x, y дорівнює x, помноженому на i-hat плюс y, помноженому на j-hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence.",
  "translatedText": "Після проходження трансформації ця властивість, що лінії сітки залишаються паралельними та рівномірно розташованими, має чудовий наслідок.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat.",
  "translatedText": "Місце, куди приземляється ваш вектор, буде перетвореною версією i-hat у х разів плюс перетвореною версією j-hat у y разів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat.",
  "translatedText": "Це означає, що якщо ви записуєте координати, де приземляється i-hat, і координати, де приземляється j-hat, ви можете обчислити, що вектор, який починається в x, y, повинен приземлитися на x, помножених на нові координати i-hat плюс y помножити на нові координати j-hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication.",
  "translatedText": "Конвенція полягає в тому, щоб записати координати місця розташування i-hat і j-hat як стовпці матриці, і визначити цю суму масштабованих версій цих стовпців на x і y як множення матриці на вектор.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector.",
  "translatedText": "Таким чином, матриця представляє конкретне лінійне перетворення, і множення матриці на вектор – це те, що обчислювально означає застосувати це перетворення до цього вектора.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff.",
  "translatedText": "Гаразд, підсумовуємо, переходимо до нового матеріалу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes, you find yourself wanting to describe the effects of applying one transformation and then another.",
  "translatedText": "Часто ви відчуваєте, що хочете описати ефекти від застосування однієї трансформації, а потім іншої.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear.",
  "translatedText": "Наприклад, можливо, ви хочете описати, що відбувається, коли ви спочатку повертаєте площину на 90 градусів проти годинникової стрілки, а потім застосовуєте зсув.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear.",
  "translatedText": "Загальний ефект тут, від початку до кінця, є іншою лінійною трансформацією, відмінною від обертання та зсуву.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied.",
  "translatedText": "Це нове лінійне перетворення зазвичай називають композицією двох окремих перетворень, які ми застосували.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat.",
  "translatedText": "І, як будь-яке лінійне перетворення, його можна описати за допомогою власної матриці, дотримуючись i-hat і j-hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix.",
  "translatedText": "У цьому прикладі кінцева точка посадки для i-hat після обох перетворень дорівнює 1,1, тому давайте зробимо це першим стовпцем матриці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix.",
  "translatedText": "Подібним чином, j-hat врешті-решт опиняється в положенні мінус 1,0, тому ми робимо це другим стовпцем матриці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones.",
  "translatedText": "Ця нова матриця фіксує загальний ефект застосування обертання, а потім зсуву, але як одну дію, а не дві послідовні.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix.",
  "translatedText": "Ось один спосіб подумати про цю нову матрицю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix, then take whatever you get and multiply that on the left by the shear matrix.",
  "translatedText": "Якщо ви візьмете якийсь вектор і прокачаєте його через обертання, а потім зсув, то довгий спосіб обчислити, де він опиниться, - це спочатку помножити його зліва на матрицю обертання, потім взяти те, що вийшло, і помножити це зліва на матрицю зсуву.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 185.42,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector.",
  "translatedText": "Це, чисельно кажучи, що означає застосувати обертання, а потім зсув до даного вектора.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action.",
  "translatedText": "Але все, що ви отримаєте, має бути таким же, як і застосування цієї нової композиційної матриці, яку ми щойно знайшли, за тим самим вектором, незалежно від того, який вектор ви обрали, оскільки ця нова матриця має фіксувати той самий загальний ефект, що й обертання, а потім зсув.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you?",
  "translatedText": "Виходячи з того, як тут записані речі, я вважаю розумним назвати цю нову матрицю добутком двох оригінальних матриць, чи не так?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers.",
  "translatedText": "Ми можемо подумати про те, як обчислити цей добуток більш загально за мить, але заблукати в лісі чисел дуже легко.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another.",
  "translatedText": "Завжди пам’ятайте, що множення двох матриць, як це, має геометричний сенс застосування одного перетворення, а потім іншого.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left.",
  "translatedText": "Одна річ, яка є трохи дивною, це те, що ми читаємо справа наліво.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left.",
  "translatedText": "Спочатку ви застосовуєте перетворення, представлене матрицею праворуч, а потім застосовуєте перетворення, представлене матрицею ліворуч.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left.",
  "translatedText": "Це випливає з нотації функцій, оскільки ми пишемо функції ліворуч від змінних, тому щоразу, коли ви створюєте дві функції, вам завжди потрібно читати їх справа наліво.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us.",
  "translatedText": "Хороші новини для читачів івритом, погані новини для всіх нас.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example.",
  "translatedText": "Давайте розглянемо інший приклад.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.",
  "translatedText": "Візьмемо матрицю зі стовпцями 1,1 і мінус 2,0, перетворення якої виглядає так.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it M1.",
  "translatedText": "Назвемо його М1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.",
  "translatedText": "Далі беремо матрицю зі стовпцями 0,1 і 2,0, перетворення якої виглядає так.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy M2.",
  "translatedText": "Назвемо його М2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying M1 then M2 gives us a new transformation, so let's find its matrix.",
  "translatedText": "Сумарний ефект від застосування M1 та M2 дає нам нове перетворення, тож знайдемо його матрицю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix.",
  "translatedText": "Але цього разу давайте подивимося, чи зможемо ми зробити це без перегляду анімації, а замість цього просто використовуючи числові записи в кожній матриці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes.",
  "translatedText": "По-перше, нам потрібно з’ясувати, куди йде i-hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying M1, the new coordinates of i-hat, by definition, are given by that first column of M1, namely 1,1.",
  "translatedText": "Після застосування M1 нові координати i-ї хати, за визначенням, задаються першим стовпчиком M1, а саме 1,1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying M2, multiply the matrix for M2 by that vector 1,1.",
  "translatedText": "Щоб побачити, що відбувається після застосування M2, помножте матрицю для M2 на вектор 1,1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1.",
  "translatedText": "Опрацювавши це, як я описав останнє відео, ви отримаєте вектор 2,1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix.",
  "translatedText": "Це буде перший стовпець композиційної матриці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of M1 tells us that it first lands on negative 2,0.",
  "translatedText": "Аналогічно, якщо слідувати j-шапці, другий стовпчик M1 показує нам, що вона спочатку потрапляє на від'ємне значення 2,0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply M2 to that vector, you can work out the matrix-vector product to get 0, negative 2, which becomes the second column of our composition matrix.",
  "translatedText": "Потім, коли ми застосуємо M2 до цього вектора, ви можете обчислити матрично-векторний добуток, щоб отримати 0, від'ємне 2, що стане другим стовпчиком нашої матриці композиції.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices.",
  "translatedText": "Дозвольте мені знову розповісти про той самий процес, але цього разу я покажу записи змінних у кожній матриці, щоб показати, що одна і та ж лінія міркувань працює для будь-яких матриць.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way.",
  "translatedText": "Це більше символів і вимагатиме більше місця, але це має бути досить задовільним для тих, хто раніше навчався множення матриці більш запам’ятовуванням.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands.",
  "translatedText": "Щоб зрозуміти, куди йде i-hat, почніть із першого стовпця матриці праворуч, оскільки саме тут спочатку потрапляє i-hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation.",
  "translatedText": "Помноживши цей стовпець на матрицю ліворуч, ви можете визначити, де закінчується проміжна версія i-hat після застосування другого перетворення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix.",
  "translatedText": "Отже, перший стовпець композиційної матриці завжди дорівнюватиме лівій матриці, помноженій на перший стовпець правої матриці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix.",
  "translatedText": "Подібним чином j-hat завжди спочатку потраплятиме у другий стовпець правої матриці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix.",
  "translatedText": "Отже, множення лівої матриці на цей другий стовпець дасть її остаточне розташування, а отже, це другий стовпець композиційної матриці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to help remember it.",
  "translatedText": "Зверніть увагу, що тут багато символів, і зазвичай цю формулу викладають як щось, що потрібно запам'ятати, разом з певним алгоритмічним процесом, який допоможе її запам'ятати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another.",
  "translatedText": "Але я справді вважаю, що перш ніж запам’ятовувати цей процес, ви повинні виробити звичку думати про те, що насправді представляє множення матриць, застосовуючи одне перетворення за іншим.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand.",
  "translatedText": "Повірте мені, це дасть вам набагато кращу концептуальну структуру, яка полегшить розуміння властивостей множення матриць.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question.",
  "translatedText": "Наприклад, ось таке запитання.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them?",
  "translatedText": "Чи має значення, у якому порядку ми розміщуємо дві матриці, коли ми їх множимо?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier.",
  "translatedText": "Ну, давайте поміркуємо на простому прикладі, як той, що був раніше.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smooshes j-hat over to the right, and a 90 degree rotation.",
  "translatedText": "Візьміть ножиці, які фіксують капелюшок i і зсувають капелюшок j вправо, і поверніть на 90 градусів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1.",
  "translatedText": "Якщо ви спочатку виконуєте зсув, а потім повертаєте, ми можемо побачити, що i-hat закінчується на 0,1, а j-hat закінчується на мінус 1,1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together.",
  "translatedText": "Обидва, як правило, спрямовані близько один до одного.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing, you know, farther apart.",
  "translatedText": "Якщо ви спочатку обертаєте, а потім виконуєте зсув, i-hat закінчується на 1,1, а j-hat змінюється в іншому напрямку на мінус 1,0, і вони спрямовані далі один від одного.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently, order totally does matter.",
  "translatedText": "Загальний ефект тут явно відрізняється, тож очевидно, що порядок має велике значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice, by thinking in terms of transformations, that's the kind of thing that you can do in your head by visualizing.",
  "translatedText": "Зауважте, мислячи в термінах трансформацій, ви можете робити такі речі в своїй голові, візуалізуючи їх.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary.",
  "translatedText": "Множення матриці не потрібне.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative.",
  "translatedText": "Я пам’ятаю, коли я вперше пройшов курс лінійної алгебри, було одне домашнє завдання, яке просили нас довести, що множення матриць є асоціативним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left.",
  "translatedText": "Це означає, що якщо у вас є три матриці, A, B і C, і ви множите їх усі разом, не повинно мати значення, чи ви спочатку обчислюєте A помножити на B, а потім помножити результат на C, або якщо ви спочатку помножите B раз C, потім помножте отриманий результат на A зліва.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses.",
  "translatedText": "Іншими словами, не має значення, де ви ставите дужки.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter.",
  "translatedText": "Тепер, якщо ви спробуєте пропрацювати це чисельно, як я тоді, це буде жахливо, просто жахливо і непросвітницько.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial.",
  "translatedText": "Але коли ви думаєте про множення матриць як про застосування одного перетворення за іншим, ця властивість просто тривіальна.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why?",
  "translatedText": "Ви бачите чому?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C, then B, then A, it's the same as applying C, then B, then A.",
  "translatedText": "Це означає, що якщо ви спочатку застосуєте C, потім B, потім A, це те саме, що застосувати C, потім B, потім A.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove.",
  "translatedText": "Тобто, нічого не треба доводити.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 552.82,
  "end": 554.38
 },
 {
  "input": "You're just applying the same three things one after the other, all in the same order.",
  "translatedText": "Ви просто застосовуєте ті ж самі три речі одну за одною, все в тому ж порядку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 554.54,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not.",
  "translatedText": "Це може здатися обманом, але це не так.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative, and even better than that, it's a good explanation for why that property should be true.",
  "translatedText": "Це чесний доказ того, що множення матриць є асоціативним, і навіть краще, це гарне пояснення того, чому ця властивість має бути істинною.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 561.54,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically.",
  "translatedText": "Я дійсно заохочую вас більше пограти з цією ідеєю, уявивши два різні перетворення, подумавши про те, що станеться, коли ви застосовуєте одне за одним, а потім розробите матричний добуток чисельно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in.",
  "translatedText": "Повірте мені, це той вид гри, який справді захоплює ідею.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions. See you then!",
  "translatedText": "У наступному відео я почну говорити про те, як розширити ці ідеї за межі двох вимірів. До зустрічі!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.2,
  "end": 592.18
 }
]