1
00:00:10,940 --> 00:00:15,280
Hey everyone, where we last left off, I showed what linear transformations look like and

2
00:00:15,280 --> 00:00:16,880
how to represent them using matrices.

3
00:00:18,320 --> 00:00:22,340
This is worth a quick recap because it's just really important, but of course if this feels

4
00:00:22,340 --> 00:00:25,140
like more than just a recap, go back and watch the full video.

5
00:00:25,780 --> 00:00:30,720
Technically speaking, linear transformations are functions with vectors as inputs and vectors

6
00:00:30,720 --> 00:00:34,800
as outputs, but I showed last time how we can think about them visually as smooshing

7
00:00:34,800 --> 00:00:39,820
around space in such a way that grid lines stay parallel and evenly spaced, and so that

8
00:00:39,820 --> 00:00:41,180
the origin remains fixed.

9
00:00:41,820 --> 00:00:46,900
The key takeaway was that a linear transformation is completely determined by where it takes

10
00:00:46,900 --> 00:00:51,340
the basis vectors of the space, which for two dimensions means i-hat and j-hat.

11
00:00:51,340 --> 00:00:56,560
This is because any other vector could be described as a linear combination of those

12
00:00:56,560 --> 00:00:57,340
basis vectors.

13
00:00:57,940 --> 00:01:02,340
A vector with coordinates x, y is x times i-hat plus y times j-hat.

14
00:01:03,460 --> 00:01:07,580
After going through the transformation, this property that grid lines remain parallel and

15
00:01:07,580 --> 00:01:09,860
evenly spaced has a wonderful consequence.

16
00:01:10,500 --> 00:01:15,180
The place where your vector lands will be x times the transformed version of i-hat plus

17
00:01:15,180 --> 00:01:17,560
y times the transformed version of j-hat.

18
00:01:18,240 --> 00:01:23,040
This means if you keep a record of the coordinates where i-hat lands and the coordinates where

19
00:01:23,040 --> 00:01:28,960
j-hat lands, you can compute that a vector which starts at x, y must land on x times

20
00:01:28,960 --> 00:01:32,720
the new coordinates of i-hat plus y times the new coordinates of j-hat.

21
00:01:33,560 --> 00:01:37,980
The convention is to record the coordinates of where i-hat and j-hat land as the columns

22
00:01:37,980 --> 00:01:42,880
of a matrix, and to define this sum of the scaled versions of those columns by x and

23
00:01:42,880 --> 00:01:45,360
y to be matrix-vector multiplication.

24
00:01:46,050 --> 00:01:51,320
In this way, a matrix represents a specific linear transformation, and multiplying a matrix

25
00:01:51,320 --> 00:01:57,080
by a vector is what it means computationally to apply that transformation to that vector.

26
00:01:58,800 --> 00:02:00,880
Alright, recap over, on to the new stuff.

27
00:02:01,600 --> 00:02:05,900
Oftentimes, you find yourself wanting to describe the effects of applying one transformation

28
00:02:05,900 --> 00:02:07,000
and then another.

29
00:02:07,620 --> 00:02:11,740
For example, maybe you want to describe what happens when you first rotate the plane 90

30
00:02:11,740 --> 00:02:14,480
degrees counterclockwise, then apply a shear.

31
00:02:15,260 --> 00:02:20,280
The overall effect here, from start to finish, is another linear transformation, distinct

32
00:02:20,280 --> 00:02:21,800
from the rotation and the shear.

33
00:02:22,280 --> 00:02:26,940
This new linear transformation is commonly called the composition of the two separate

34
00:02:26,940 --> 00:02:28,220
transformations we applied.

35
00:02:28,920 --> 00:02:33,340
And like any linear transformation, it can be described with a matrix all of its own

36
00:02:33,340 --> 00:02:35,440
by following i-hat and j-hat.

37
00:02:36,020 --> 00:02:41,820
In this example, the ultimate landing spot for i-hat after both transformations is 1,1,

38
00:02:42,180 --> 00:02:44,120
so let's make that the first column of a matrix.

39
00:02:44,960 --> 00:02:50,460
Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the

40
00:02:50,460 --> 00:02:51,860
second column of the matrix.

41
00:02:52,680 --> 00:02:58,380
This new matrix captures the overall effect of applying a rotation then a shear, but as

42
00:02:58,380 --> 00:03:01,340
one single action, rather than two successive ones.

43
00:03:03,040 --> 00:03:04,880
Here's one way to think about that new matrix.

44
00:03:05,420 --> 00:03:09,840
If you were to take some vector and pump it through the rotation, then the shear, the

45
00:03:09,840 --> 00:03:14,360
long way to compute where it ends up is to first multiply it on the left by the rotation

46
00:03:14,360 --> 00:03:19,800
matrix, then take whatever you get and multiply that on the left by the shear matrix.

47
00:03:20,460 --> 00:03:25,680
This is, numerically speaking, what it means to apply a rotation then a shear to a given

48
00:03:25,680 --> 00:03:26,060
vector.

49
00:03:26,800 --> 00:03:30,700
But whatever you get should be the same as just applying this new composition matrix

50
00:03:30,700 --> 00:03:35,640
that we just found by that same vector, no matter what vector you chose, since this new

51
00:03:35,640 --> 00:03:40,980
matrix is supposed to capture the same overall effect as the rotation then shear action.

52
00:03:42,480 --> 00:03:46,020
Based on how things are written down here, I think it's reasonable to call this new

53
00:03:46,020 --> 00:03:49,380
matrix the product of the original two matrices, don't you?

54
00:03:50,420 --> 00:03:54,400
We can think about how to compute that product more generally in just a moment, but it's

55
00:03:54,400 --> 00:03:56,600
way too easy to get lost in the forest of numbers.

56
00:03:56,600 --> 00:04:02,260
Always remember that multiplying two matrices like this has the geometric meaning of applying

57
00:04:02,260 --> 00:04:04,280
one transformation then another.

58
00:04:05,860 --> 00:04:09,660
One thing that's kind of weird here is that this has us reading from right to left.

59
00:04:10,040 --> 00:04:14,200
You first apply the transformation represented by the matrix on the right, then you apply

60
00:04:14,200 --> 00:04:16,720
the transformation represented by the matrix on the left.

61
00:04:17,400 --> 00:04:21,540
This stems from function notation, since we write functions on the left of variables,

62
00:04:21,580 --> 00:04:25,460
so every time you compose two functions, you always have to read it right to left.

63
00:04:25,920 --> 00:04:28,980
Good news for the Hebrew readers, bad news for the rest of us.

64
00:04:29,880 --> 00:04:31,100
Let's look at another example.

65
00:04:31,760 --> 00:04:36,860
Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.

66
00:04:37,980 --> 00:04:39,060
And let's call it M1.

67
00:04:40,100 --> 00:04:45,700
Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.

68
00:04:47,520 --> 00:04:49,240
And let's call that guy M2.

69
00:04:49,920 --> 00:04:55,260
The total effect of applying M1 then M2 gives us a new transformation, so let's find its

70
00:04:55,260 --> 00:04:55,680
matrix.

71
00:04:56,280 --> 00:05:00,720
But this time, let's see if we can do it without watching the animations, and instead

72
00:05:00,720 --> 00:05:03,860
just using the numerical entries in each matrix.

73
00:05:04,740 --> 00:05:07,140
First, we need to figure out where i-hat goes.

74
00:05:08,040 --> 00:05:13,560
After applying M1, the new coordinates of i-hat, by definition, are given by that first

75
00:05:13,560 --> 00:05:15,980
column of M1, namely 1,1.

76
00:05:16,780 --> 00:05:23,500
To see what happens after applying M2, multiply the matrix for M2 by that vector 1,1.

77
00:05:25,300 --> 00:05:29,880
Working it out, the way I described last video, you'll get the vector 2,1.

78
00:05:30,700 --> 00:05:33,100
This will be the first column of the composition matrix.

79
00:05:34,520 --> 00:05:39,800
Likewise, to follow j-hat, the second column of M1 tells us that it first lands on negative

80
00:05:39,800 --> 00:05:40,540
2,0.

81
00:05:42,700 --> 00:05:50,320
Then, when we apply M2 to that vector, you can work out the matrix-vector product to

82
00:05:50,320 --> 00:05:55,200
get 0, negative 2, which becomes the second column of our composition matrix.

83
00:05:56,640 --> 00:06:01,020
Let me talk through that same process again, but this time I'll show variable entries

84
00:06:01,020 --> 00:06:04,920
in each matrix, just to show that the same line of reasoning works for any matrices.

85
00:06:05,540 --> 00:06:09,540
This is more symbol-heavy and will require some more room, but it should be pretty satisfying

86
00:06:09,540 --> 00:06:13,660
for anyone who has previously been taught matrix multiplication the more rote way.

87
00:06:14,460 --> 00:06:18,840
To follow where i-hat goes, start by looking at the first column of the matrix on the right,

88
00:06:18,840 --> 00:06:21,060
since this is where i-hat initially lands.

89
00:06:22,000 --> 00:06:26,820
Multiplying that column by the matrix on the left is how you can tell where the intermediate

90
00:06:26,820 --> 00:06:30,300
version of i-hat ends up after applying the second transformation.

91
00:06:31,620 --> 00:06:36,480
So the first column of the composition matrix will always equal the left matrix times the

92
00:06:36,480 --> 00:06:38,100
first column of the right matrix.

93
00:06:42,160 --> 00:06:47,140
Likewise, j-hat will always initially land on the second column of the right matrix.

94
00:06:48,940 --> 00:06:54,400
So multiplying the left matrix by this second column will give its final location, and hence

95
00:06:54,400 --> 00:06:57,020
that's the second column of the composition matrix.

96
00:07:00,620 --> 00:07:04,500
Notice there's a lot of symbols here, and it's common to be taught this formula as something

97
00:07:04,500 --> 00:07:09,040
to memorize, along with a certain algorithmic process to help remember it.

98
00:07:09,160 --> 00:07:13,080
But I really do think that before memorizing that process, you should get in the habit

99
00:07:13,080 --> 00:07:17,980
of thinking about what matrix multiplication really represents, applying one transformation

100
00:07:17,980 --> 00:07:18,900
after another.

101
00:07:19,620 --> 00:07:23,600
Trust me, this will give you a much better conceptual framework that makes the properties

102
00:07:23,600 --> 00:07:26,300
of matrix multiplication much easier to understand.

103
00:07:27,060 --> 00:07:28,360
For example, here's a question.

104
00:07:28,880 --> 00:07:32,840
Does it matter what order we put the two matrices in when we multiply them?

105
00:07:33,620 --> 00:07:37,000
Well, let's think through a simple example, like the one from earlier.

106
00:07:37,640 --> 00:07:42,820
Take a shear, which fixes i-hat and smooshes j-hat over to the right, and a 90 degree rotation.

107
00:07:43,600 --> 00:07:49,580
If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat

108
00:07:49,580 --> 00:07:50,960
ends up at negative 1,1.

109
00:07:51,320 --> 00:07:53,060
Both are generally pointing close together.

110
00:07:53,860 --> 00:08:01,520
If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a

111
00:08:01,520 --> 00:08:05,520
different direction at negative 1,0, and they're pointing, you know, farther apart.

112
00:08:06,380 --> 00:08:10,660
The overall effect here is clearly different, so evidently, order totally does matter.

113
00:08:12,200 --> 00:08:16,400
Notice, by thinking in terms of transformations, that's the kind of thing that you can do in

114
00:08:16,400 --> 00:08:17,840
your head by visualizing.

115
00:08:18,220 --> 00:08:19,900
No matrix multiplication necessary.

116
00:08:21,480 --> 00:08:26,000
I remember when I first took linear algebra, there was this one homework problem that asked

117
00:08:26,000 --> 00:08:29,120
us to prove that matrix multiplication is associative.

118
00:08:29,560 --> 00:08:34,060
This means that if you have three matrices, A, B, and C, and you multiply them all together,

119
00:08:34,380 --> 00:08:39,860
it shouldn't matter if you first compute A times B, then multiply the result by C, or

120
00:08:39,860 --> 00:08:44,360
if you first multiply B times C, then multiply that result by A on the left.

121
00:08:44,940 --> 00:08:47,400
In other words, it doesn't matter where you put the parentheses.

122
00:08:48,380 --> 00:08:53,740
Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible,

123
00:08:53,740 --> 00:08:55,760
and unenlightening for that matter.

124
00:08:55,760 --> 00:09:00,780
But when you think about matrix multiplication as applying one transformation after another,

125
00:09:01,200 --> 00:09:02,780
this property is just trivial.

126
00:09:03,300 --> 00:09:04,000
Can you see why?

127
00:09:04,860 --> 00:09:10,520
What it's saying is that if you first apply C, then B, then A, it's the same as applying

128
00:09:10,520 --> 00:09:12,380
C, then B, then A.

129
00:09:12,820 --> 00:09:14,380
I mean, there's nothing to prove.

130
00:09:14,540 --> 00:09:18,660
You're just applying the same three things one after the other, all in the same order.

131
00:09:19,460 --> 00:09:21,540
This might feel like cheating, but it's not.

132
00:09:21,540 --> 00:09:26,800
This is an honest-to-goodness proof that matrix multiplication is associative, and even better

133
00:09:26,800 --> 00:09:30,680
than that, it's a good explanation for why that property should be true.

134
00:09:31,560 --> 00:09:36,300
I really do encourage you to play around more with this idea, imagining two different transformations,

135
00:09:36,900 --> 00:09:40,560
thinking about what happens when you apply one after the other, and then working out

136
00:09:40,560 --> 00:09:42,140
the matrix product numerically.

137
00:09:42,600 --> 00:09:46,440
Trust me, this is the kind of playtime that really makes the idea sink in.

138
00:09:47,200 --> 00:09:51,420
In the next video, I'll start talking about extending these ideas beyond just two dimensions.

139
00:09:52,020 --> 00:09:52,380
See you then!

