[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices. ",
  "translatedText": "皆さん、前回の続きで、線形変換がどのようなものなのか、そ して行列を使用して線形変換を表現する方法を説明しました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video. ",
  "translatedText": "これは本当に重要なことなので、簡単に要約する価値がありますが、もちろん、こ れが単なる要約以上のものだと感じた場合は、戻ってビデオ全体を見てください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Generally speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed. ",
  "translatedText": "一般的に、線形変換は入力としてベクト ル、出力としてベクトルを持つ関数です が、前回、グリッド線が平行で等間隔に 保たれ、原点が固定されたままです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat. ",
  "translatedText": "重要な点は、線形変換は空間の基底ベクトル (2 次元の場合は i-hat と j -hat を意味します) をどこに取るかによって完全に決定されるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors. ",
  "translatedText": "これは、他のベクトルはそれらの基底ベクト ルの線形結合として記述できるためです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat. ",
  "translatedText": "座標 x、y を持つベクトルは、x に i-hat を掛けたものに、y に j-hat を掛けたものです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence. ",
  "translatedText": "変換を経た後、グリッド線が平行かつ等間隔に保たれる というこの特性は、素晴らしい結果をもたらします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat. ",
  "translatedText": "ベクトルが着地する場所は、i-hat の変換バージョンの x 倍に 、j-hat の変換バージョンの y 倍を加えた場所になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat. ",
  "translatedText": "これは、i-hat が着地する座標と j-hat が着地する座標を記録しておくと、 x、y で始まるベクトルは、i-hat に y を加えた新しい座標の x 倍の位 置に着地する必要があることを計算できます。j-hat の新しい座標を倍します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication. ",
  "translatedText": "慣例では、i ハットと j ハットが配置される場所の座標を 行列の列として記録し、これらの列を x と y でスケーリ ングしたものの合計を行列とベクトルの乗算として定義します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector. ",
  "translatedText": "このように、行列は特定の線形変換を表し、行列にベクトルを乗算するこ とは、その変換をそのベクトルに適用することを計算的に意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff. ",
  "translatedText": "さて、おさらいして新しい内容に移りましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes you find yourself wanting to describe the effects of applying one transformation and then another. ",
  "translatedText": "ある変換を適用してから別の変換を適用した場 合の効果を説明したい場合がよくあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear. ",
  "translatedText": "たとえば、最初に平面を反時計回りに 90 度回転し、次 にシアーを適用すると何が起こるかを説明したいとします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear. ",
  "translatedText": "ここでの全体的な効果は、最初から最後まで、 回転やせん断とは異なる別の線形変換です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied. ",
  "translatedText": "この新しい線形変換は、一般に、適用した 2 つの別々の変換の合成と呼ばれます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat. ",
  "translatedText": "そして、他の線形変換と同様に、i-hat と j-ha t に従って、それ自体の行列で記述することができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix. ",
  "translatedText": "この例では、両方の変換後の i-hat の最終的な着地 点は 1,1 なので、これを行列の最初の列にします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix. ",
  "translatedText": "同様に、j-hat は最終的にマイナス 1,0 の位 置に到達するため、それを行列の 2 列目にします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones. ",
  "translatedText": "この新しいマトリックスは、回転とせん断を適用した場合の全体的な効果を、連続 した 2 つのアクションではなく 1 つのアクションとして捉えています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix. ",
  "translatedText": "この新しいマトリックスについて考える 1 つの方法を次に示します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix. ",
  "translatedText": "ベクトルを取得し、それを回転、次にせん断を通してポ ンプする場合、最終的にどこに到達するかを計算する長 い方法は、まず左側で回転行列を乗算することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.42,
  "end": 194.82
 },
 {
  "input": "Then, take whatever you get and multiply that on the left by the shear matrix. ",
  "translatedText": "次に、得られたものをすべて取り、左側にせん断行列を掛けます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 195.32,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector. ",
  "translatedText": "これは、数値的に言えば、特定のベクトルに回転を 加えてからせん断を適用することを意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action. ",
  "translatedText": "しかし、得られるものは何であれ、選択したベクトルに関係なく、同じベクトルで見つ けたこの新しい合成行列を適用したのと同じであるはずです。この新しい行列は、回転 とせん断アクションと同じ全体的な効果をキャプチャすることになっているためです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you? ",
  "translatedText": "ここでの書き方からすると、この新しい行列を元の 2 つの行列の積と呼ぶのが妥当だと思いますね。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers. ",
  "translatedText": "その積をより一般的に計算する方法についてはすぐに考えることができ ますが、あまりにも簡単に数値の森の中で迷子になってしまいます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another. ",
  "translatedText": "このように 2 つの行列を乗算することには、ある変換を適用してから別の変 換を適用するという幾何学的な意味があることを常に覚えておいてください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left. ",
  "translatedText": "ここで少し奇妙なのは、右から左に読むことになることです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left. ",
  "translatedText": "まず右側の行列で表される変換を適用し、次 に左側の行列で表される変換を適用します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left. ",
  "translatedText": "これは関数の表記法に由来しています。変数の左側に関数を記述するため 、2 つの関数を構成するたびに、常に右から左に読む必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us. ",
  "translatedText": "ヘブライ語の読者にとっては良いニュースですが、それ以外の私たちにとっては悪いニュースです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example. ",
  "translatedText": "別の例を見てみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this. ",
  "translatedText": "列 1,1 と負の 2,0 を持つ行列を考えます。その変換は次のようになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it m1. ",
  "translatedText": "それを m1 と呼びます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this. ",
  "translatedText": "次に、列 0,1 および 2,0 を持つ行列を取得します。その変換は次のようになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy m2. ",
  "translatedText": "その男を m2 と呼びましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying m1 then m2 gives us a new transformation, so let's find its matrix. ",
  "translatedText": "m1 を適用してから m2 を適用する合計の効果により、 新しい変換が得られるので、その行列を見つけてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix. ",
  "translatedText": "ただし今回は、アニメーションを見ずに、各行列の数値入力 だけを使用してそれができるかどうかを見てみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes. ",
  "translatedText": "まず、i-hat がどこに行くのかを把握する必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying m1, the new coordinates of i-hat, by definition, are given by that first column of m1, namely 1,1. ",
  "translatedText": "m1 を適用した後、i-hat の新しい座標は、定義により 、m1 の最初の列、つまり 1,1 によって与えられます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying m2, multiply the matrix for m2 by that vector 1,1. ",
  "translatedText": "m2 を適用した後に何が起こるかを確認するには、m2 の行列にそのベクトル 1,1 を乗算します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1. ",
  "translatedText": "前回のビデオで説明した方法で計算すると、ベクトル 2,1 が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix. ",
  "translatedText": "これは、構成マトリックスの最初の列になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of m1 tells us that it first lands on negative 2,0. ",
  "translatedText": "同様に、j-hat に従うと、m1 の 2 番目の列は 、最初に負の 2,0 に到達することを示しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply m2 to that vector, you can work out the matrix vector product to get 0, negative 2, which becomes the second column of our composition matrix. ",
  "translatedText": "次に、そのベクトルに m2 を適用すると、行列ベクトルの積を計算して 0、負の 2 を得ることができ、これが合成行列の 2 列目になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices. ",
  "translatedText": "同じプロセスをもう一度説明しますが、今回は、同じ推論がどの行列 でも機能することを示すために、各行列の変数エントリを示します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way. ",
  "translatedText": "これはシンボルが多くなり、もう少しスペースが必要になりますが、以前に行列の 乗算をより暗記的な方法で教えられた人にとっては、かなり満足できるはずです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands. ",
  "translatedText": "i-hat がどこへ行くのかを追跡するには、まず右側の行列の最初の 列を調べます。これは、i-hat が最初に着地する場所だからです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation. ",
  "translatedText": "その列に左側の行列を乗算すると、2 番目の変換を適用した後に i-hat の中間バージョンがどこに到達するかがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix. ",
  "translatedText": "したがって、合成行列の最初の列は常に、左行 列と右行列の最初の列の積に等しくなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix. ",
  "translatedText": "同様に、j-hat は常に最初は右行列の 2 列目に配置されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix. ",
  "translatedText": "したがって、左の行列にこの 2 番目の列を乗算すると、その最終的 な位置が得られるため、それが合成行列の 2 番目の列になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to kind of help remember it. ",
  "translatedText": "ここにはたくさんの記号があることに注意してください。この公式は暗記するものとして教えられる のが一般的であり、それを覚えるのに役立つ特定のアルゴリズムのプロセスも一緒に教えられます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another. ",
  "translatedText": "しかし、そのプロセスを暗記する前に、行列の乗算 が実際に何を表し、変換を次々と適用するかを考え る習慣を身につけるべきだと私は心から思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand. ",
  "translatedText": "信じてください。これにより、行列の乗算の特性をはるかに理解 しやすくする、より優れた概念フレームワークが得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question. ",
  "translatedText": "たとえば、こんな質問があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them? ",
  "translatedText": "2 つの行列を乗算するときにそれらをどの順序で置くかは重要ですか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier. ",
  "translatedText": "さて、先ほどのような簡単な例で考えてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smushes j-hat over to the right, and a 90 degree rotation. ",
  "translatedText": "i-hat を固定し、j-hat を右に押しつぶすハサミを 90 度回転させます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1. ",
  "translatedText": "最初にせん断を行ってから回転すると、i-hat が 0,1 になり、j-hat が負の 1,1 になることがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together. ",
  "translatedText": "両方は通常、近くを指しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing farther apart. ",
  "translatedText": "最初に回転してからせん断を行うと、i-hat は 1,1 で終了し、j- hat は負の 1,0 で別の方向にずれて、さらに離れた方向を指します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently order totally does matter. ",
  "translatedText": "ここでの全体的な効果は明らかに異なるため、明らかに順序が完全に重要です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice by thinking in terms of transformations, that's the kind of thing you can do in your head by visualizing. ",
  "translatedText": "変換という観点から考えると、視覚化する ことで頭の中でできるようなことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary. ",
  "translatedText": "行列の乗算は必要ありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative. ",
  "translatedText": "私が初めて線形代数を履修したとき、行列の乗算が結合的 であることを証明するという宿題が 1 つありました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left. ",
  "translatedText": "これは、A、B、C という 3 つの行列があり、それらをすべて乗算する場合、最初に A と B を掛けてからその結果を C で乗算するか、最初に B を乗算するかは 問題ではないことを意味します。C を計算し、その結果に左側の A を掛けます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses. ",
  "translatedText": "つまり、括弧をどこに入れても問題ありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter. ",
  "translatedText": "さて、当時の私のようにこれを数値的に解明しようとすると、それは恐 ろしい、まったく恐ろしく、その点においては啓発的ではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial. ",
  "translatedText": "しかし、行列の乗算を 1 つの変換を次々に適用する ものと考えると、この性質は単なる些細なものです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why? ",
  "translatedText": "理由がわかりますか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C then B, then A, it's the same as applying C, then B, then A. ",
  "translatedText": "何を言っているのかというと、最初に C を適用し、次に B、次に A を適用した 場合、それは C、次に B、次に A を適用したのと同じであるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove, you're just applying the same three things one after the other, all in the same order. ",
  "translatedText": "つまり、証明するものは何もなく、同じ 3 つのこ とをすべて同じ順序で順番に適用しているだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 552.82,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not. ",
  "translatedText": "これは不正行為のように感じるかもしれませんが、そうではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative. ",
  "translatedText": "これは、行列の乗算が結合的であるという正直な証明です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 561.54,
  "end": 565.9
 },
 {
  "input": "And even better than that, it's a good explanation for why that property should be true. ",
  "translatedText": "そしてそれ以上に、それはその特性が真である理由をうまく説明しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.9,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically. ",
  "translatedText": "このアイデアをもっと試してみることをお勧めします 。2 つの異なる変換を想像し、交互に適用すると何 が起こるかを考え、行列の積を数値的に計算します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in. ",
  "translatedText": "信じてください、これはアイデアを本当に浸透させる一種の遊びです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions. ",
  "translatedText": "次のビデオでは、これらのアイデアを単なる 2 次元を超えて拡張することについて話し始めます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 587.2,
  "end": 591.42
 },
 {
  "input": "See you then! ",
  "translatedText": "それではまた！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.02,
  "end": 592.18
 }
]