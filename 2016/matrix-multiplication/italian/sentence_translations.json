[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices.",
  "translatedText": "Ciao a tutti, da dove ci eravamo interrotti l'ultima volta, ho mostrato come appaiono le trasformazioni lineari e come rappresentarle utilizzando le matrici.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video.",
  "translatedText": "Vale la pena fare un breve riepilogo perché è davvero importante, ma ovviamente se ti sembra qualcosa di più di un semplice riepilogo, torna indietro e guarda il video completo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Generally speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed.",
  "translatedText": "In generale, le trasformazioni lineari sono funzioni con vettori come input e vettori come output, ma l'ultima volta ho mostrato come possiamo pensarle visivamente come se si muovessero nello spazio in modo tale che le linee della griglia rimangano parallele e uniformemente distanziate, e in modo che l'origine rimane fisso.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat.",
  "translatedText": "La conclusione fondamentale è che una trasformazione lineare è completamente determinata da dove prendono i vettori base dello spazio, che per due dimensioni significa i-hat e j-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors.",
  "translatedText": "Questo perché qualsiasi altro vettore potrebbe essere descritto come una combinazione lineare di questi vettori base.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat.",
  "translatedText": "Un vettore con coordinate x, y è x per i-hat più y per j-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence.",
  "translatedText": "Dopo aver attraversato la trasformazione, questa proprietà secondo cui le linee della griglia rimangono parallele e spaziate uniformemente ha una conseguenza meravigliosa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat.",
  "translatedText": "Il luogo in cui atterrerà il tuo vettore sarà x volte la versione trasformata di i-hat più y volte la versione trasformata di j-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat.",
  "translatedText": "Ciò significa che se tieni un registro delle coordinate dove atterra i-hat e delle coordinate dove atterra j-hat, puoi calcolare che un vettore che inizia da x, y deve atterrare su x volte le nuove coordinate di i-hat più y volte le nuove coordinate di j-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication.",
  "translatedText": "La convenzione è di registrare le coordinate di dove si trovano i-hat e j-hat come colonne di una matrice e di definire questa somma delle versioni scalate di quelle colonne per x e y come moltiplicazione di vettori di matrice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector.",
  "translatedText": "In questo modo, una matrice rappresenta una trasformazione lineare specifica e moltiplicare una matrice per un vettore è ciò che significa, dal punto di vista computazionale, applicare quella trasformazione a quel vettore.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff.",
  "translatedText": "Va bene, ricapitoliamo, passiamo alle novità.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes you find yourself wanting to describe the effects of applying one transformation and then another.",
  "translatedText": "Spesso ti ritrovi a voler descrivere gli effetti dell'applicazione di una trasformazione e poi di un'altra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear.",
  "translatedText": "Ad esempio, potresti voler descrivere cosa succede quando ruoti per la prima volta il piano di 90 gradi in senso antiorario e poi applichi un taglio.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear.",
  "translatedText": "L'effetto complessivo qui, dall'inizio alla fine, è un'altra trasformazione lineare, distinta dalla rotazione e dal taglio.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied.",
  "translatedText": "Questa nuova trasformazione lineare è comunemente chiamata la composizione delle due trasformazioni separate che abbiamo applicato.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat.",
  "translatedText": "E come ogni trasformazione lineare, può essere descritta con una matrice tutta sua seguendo i-hat e j-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix.",
  "translatedText": "In questo esempio, il punto di destinazione finale per i-hat dopo entrambe le trasformazioni è 1,1, quindi rendiamolo la prima colonna di una matrice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix.",
  "translatedText": "Allo stesso modo, j-hat alla fine finisce nella posizione negativa 1,0, quindi la rendiamo la seconda colonna della matrice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones.",
  "translatedText": "Questa nuova matrice cattura l'effetto complessivo dell'applicazione di una rotazione e poi di un taglio, ma come una singola azione, anziché due successive.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix.",
  "translatedText": "Ecco un modo di pensare a quella nuova matrice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix.",
  "translatedText": "Se dovessi prendere un vettore e pomparlo attraverso la rotazione, quindi il taglio, la strada più lunga per calcolare dove va a finire è moltiplicarlo prima a sinistra per la matrice di rotazione.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.42,
  "end": 194.82
 },
 {
  "input": "Then, take whatever you get and multiply that on the left by the shear matrix.",
  "translatedText": "Quindi prendi quello che ottieni e moltiplica quello a sinistra per la matrice di taglio.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 195.32,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector.",
  "translatedText": "Questo è, numericamente parlando, ciò che significa applicare una rotazione e poi un taglio ad un dato vettore.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action.",
  "translatedText": "Ma qualunque cosa ottieni dovrebbe essere uguale all'applicazione di questa nuova matrice di composizione che abbiamo appena trovato con lo stesso vettore, non importa quale vettore hai scelto, poiché questa nuova matrice dovrebbe catturare lo stesso effetto complessivo dell'azione di rotazione e poi di taglio.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you?",
  "translatedText": "Basandosi su come sono scritte le cose qui, penso che sia ragionevole chiamare questa nuova matrice il prodotto delle due matrici originali, non è vero?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers.",
  "translatedText": "Possiamo pensare a come calcolare quel prodotto più in generale in un attimo, ma è troppo facile perdersi nella foresta dei numeri.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another.",
  "translatedText": "Ricorda sempre che moltiplicare due matrici come questa ha il significato geometrico di applicare una trasformazione poi un'altra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left.",
  "translatedText": "Una cosa un po' strana qui è che ci fa leggere da destra a sinistra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left.",
  "translatedText": "Prima applichi la trasformazione rappresentata dalla matrice a destra, quindi applichi la trasformazione rappresentata dalla matrice a sinistra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left.",
  "translatedText": "Ciò deriva dalla notazione delle funzioni, poiché scriviamo le funzioni a sinistra delle variabili, quindi ogni volta che componi due funzioni, devi sempre leggerle da destra a sinistra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us.",
  "translatedText": "Buone notizie per i lettori ebrei, cattive notizie per il resto di noi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example.",
  "translatedText": "Diamo un'occhiata a un altro esempio.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.",
  "translatedText": "Prendi la matrice con le colonne 1,1 e negativo 2,0, la cui trasformazione assomiglia a questa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it m1.",
  "translatedText": "E chiamiamolo m1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.",
  "translatedText": "Successivamente, prendi la matrice con le colonne 0,1 e 2,0, la cui trasformazione assomiglia a questa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy m2.",
  "translatedText": "E chiamiamo quel ragazzo m2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying m1 then m2 gives us a new transformation, so let's find its matrix.",
  "translatedText": "L'effetto totale dell'applicazione di m1 e poi di m2 ci dà una nuova trasformazione, quindi troviamo la sua matrice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix.",
  "translatedText": "Ma questa volta vediamo se possiamo farlo senza guardare le animazioni, e utilizzando invece solo le voci numeriche in ciascuna matrice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes.",
  "translatedText": "Per prima cosa dobbiamo capire dove va a finire l'i-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying m1, the new coordinates of i-hat, by definition, are given by that first column of m1, namely 1,1.",
  "translatedText": "Dopo aver applicato m1, le nuove coordinate di i-hat, per definizione, sono date da quella prima colonna di m1, ovvero 1,1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying m2, multiply the matrix for m2 by that vector 1,1.",
  "translatedText": "Per vedere cosa succede dopo aver applicato m2, moltiplica la matrice per m2 per quel vettore 1,1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1.",
  "translatedText": "Elaborandolo, nel modo in cui ho descritto l'ultimo video, otterrai il vettore 2,1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix.",
  "translatedText": "Questa sarà la prima colonna della matrice di composizione.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of m1 tells us that it first lands on negative 2,0.",
  "translatedText": "Allo stesso modo, per seguire j-hat, la seconda colonna di m1 ci dice che prima si ferma su meno 2,0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply m2 to that vector, you can work out the matrix vector product to get 0, negative 2, which becomes the second column of our composition matrix.",
  "translatedText": "Quindi, quando applichiamo m2 a quel vettore, puoi calcolare il prodotto del vettore della matrice per ottenere 0, meno 2, che diventa la seconda colonna della nostra matrice di composizione.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices.",
  "translatedText": "Lasciatemi parlare di nuovo dello stesso processo, ma questa volta mostrerò le voci variabili in ciascuna matrice, solo per mostrare che la stessa linea di ragionamento funziona per qualsiasi matrice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way.",
  "translatedText": "Questo è più ricco di simboli e richiederà un po' più di spazio, ma dovrebbe essere abbastanza soddisfacente per chiunque abbia già imparato la moltiplicazione di matrici in modo più meccanico.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands.",
  "translatedText": "Per seguire dove va i-hat, inizia guardando la prima colonna della matrice a destra, poiché è qui che inizialmente si ferma i-hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation.",
  "translatedText": "Moltiplicando quella colonna per la matrice a sinistra è come puoi sapere dove finisce la versione intermedia di i-hat dopo aver applicato la seconda trasformazione.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix.",
  "translatedText": "Quindi la prima colonna della matrice di composizione sarà sempre uguale alla matrice di sinistra moltiplicata per la prima colonna della matrice di destra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix.",
  "translatedText": "Allo stesso modo, j-hat atterrerà sempre inizialmente sulla seconda colonna della matrice di destra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix.",
  "translatedText": "Quindi moltiplicando la matrice di sinistra per questa seconda colonna si otterrà la sua posizione finale, e quindi quella sarà la seconda colonna della matrice di composizione.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to kind of help remember it.",
  "translatedText": "Nota che ci sono molti simboli qui, ed è normale che questa formula venga insegnata come qualcosa da memorizzare, insieme a un certo processo algoritmico per aiutare a ricordarla.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another.",
  "translatedText": "Ma penso davvero che prima di memorizzare quel processo, dovresti abituarti a pensare a cosa rappresenta realmente la moltiplicazione di matrici, applicando una trasformazione dopo l'altra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand.",
  "translatedText": "Credimi, questo ti fornirà un quadro concettuale molto migliore che renderà le proprietà della moltiplicazione di matrici molto più facili da comprendere.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question.",
  "translatedText": "Ad esempio, ecco una domanda.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them?",
  "translatedText": "È importante l'ordine in cui inseriamo le due matrici quando le moltiplichiamo?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier.",
  "translatedText": "Bene, riflettiamo su un semplice esempio, come quello di prima.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smushes j-hat over to the right, and a 90 degree rotation.",
  "translatedText": "Prendi un taglio, che fissa l'i-hat e spinge il j-hat verso destra, e una rotazione di 90 gradi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1.",
  "translatedText": "Se prima esegui il taglio e poi ruoti, possiamo vedere che i-hat finisce a 0,1 e j-hat finisce a meno 1,1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together.",
  "translatedText": "Entrambi generalmente puntano vicini.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing farther apart.",
  "translatedText": "Se prima ruoti, poi esegui il taglio, i-hat finisce a 1,1 e j-hat si sposta in una direzione diversa a meno 1,0 e puntano più distanti.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently order totally does matter.",
  "translatedText": "L’effetto complessivo qui è chiaramente diverso, quindi evidentemente l’ordine ha tutta la sua importanza.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice by thinking in terms of transformations, that's the kind of thing you can do in your head by visualizing.",
  "translatedText": "Nota che pensando in termini di trasformazioni, questo è il genere di cose che puoi fare nella tua testa visualizzando.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary.",
  "translatedText": "Nessuna moltiplicazione di matrici necessaria.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative.",
  "translatedText": "Ricordo che quando ho studiato per la prima volta l'algebra lineare, c'era questo compito a casa che ci chiedeva di dimostrare che la moltiplicazione di matrici è associativa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left.",
  "translatedText": "Ciò significa che se hai tre matrici, A, B e C, e le moltiplichi tutte insieme, non dovrebbe importare se prima calcoli A per B, poi moltiplichi il risultato per C, o se prima moltiplichi B per C, quindi moltiplica il risultato per A a sinistra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses.",
  "translatedText": "In altre parole, non importa dove metti le parentesi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter.",
  "translatedText": "Ora, se provi a elaborare questo numericamente, come ho fatto allora, è orribile, semplicemente orribile e poco illuminante per quella materia.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial.",
  "translatedText": "Ma se si pensa alla moltiplicazione di matrici come all'applicazione di una trasformazione dopo l'altra, questa proprietà è semplicemente banale.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why?",
  "translatedText": "Riesci a vedere perché?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C then B, then A, it's the same as applying C, then B, then A.",
  "translatedText": "Quello che sta dicendo è che se applichi prima C poi B, poi A, è come applicare C, poi B, poi A.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove, you're just applying the same three things one after the other, all in the same order.",
  "translatedText": "Voglio dire, non c'è niente da dimostrare, stai solo applicando le stesse tre cose una dopo l'altra, tutte nello stesso ordine.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 552.82,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not.",
  "translatedText": "Potrebbe sembrare un imbroglio, ma non lo è.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative.",
  "translatedText": "Questa è una prova onesta che la moltiplicazione di matrici è associativa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 561.54,
  "end": 565.9
 },
 {
  "input": "And even better than that, it's a good explanation for why that property should be true.",
  "translatedText": "E, ancora meglio, è una buona spiegazione del motivo per cui quella proprietà dovrebbe essere vera.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.9,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically.",
  "translatedText": "Ti incoraggio davvero a giocare di più con questa idea, immaginando due diverse trasformazioni, pensando a cosa succede quando ne applichi una dopo l'altra e poi elaborando numericamente il prodotto della matrice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in.",
  "translatedText": "Credimi, questo è il tipo di momento di gioco che fa davvero affondare l'idea.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions.",
  "translatedText": "Nel prossimo video inizierò a parlare di come estendere queste idee oltre le sole due dimensioni.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 587.2,
  "end": 591.42
 },
 {
  "input": "See you then!",
  "translatedText": "Ci vediamo!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.02,
  "end": 592.18
 }
]