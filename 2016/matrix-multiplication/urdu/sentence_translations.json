[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices. ",
  "translatedText": "ارے سب، جہاں ہم نے آخری بار چھوڑا تھا، میں نے دکھایا کہ لکیری تبدیلیاں کیسی نظر آتی ہیں اور میٹرکس کا استعمال کرتے ہوئے ان کی نمائندگی کیسے کی جائے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video. ",
  "translatedText": "یہ ایک فوری ریکپ کے قابل ہے کیونکہ یہ واقعی بہت اہم ہے، لیکن یقیناً اگر یہ صرف ایک ریکپ سے زیادہ محسوس ہوتا ہے، تو واپس جائیں اور پوری ویڈیو دیکھیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Generally speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed. ",
  "translatedText": "عام طور پر، لکیری تبدیلیاں ویکٹرز کے ساتھ ان پٹ کے طور پر اور ویکٹرز آؤٹ پٹ کے طور پر کام کرتی ہیں، لیکن میں نے پچھلی بار دکھایا تھا کہ ہم ان کے بارے میں بصری طور پر کیسے سوچ سکتے ہیں جیسے خلا کے گرد اس طرح سے کہ گرڈ لائنیں متوازی اور یکساں فاصلہ پر رہیں، اور تاکہ اصل مقرر رہتا ہے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat. ",
  "translatedText": "اہم بات یہ تھی کہ ایک لکیری تبدیلی مکمل طور پر اس بات سے طے کی جاتی ہے کہ یہ خلا کے بنیادی ویکٹرز کو کہاں لیتی ہے، جس کا دو جہتوں کا مطلب ہے i-hat اور j-hat۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors. ",
  "translatedText": "اس کی وجہ یہ ہے کہ کسی بھی دوسرے ویکٹر کو ان بنیادی ویکٹروں کے لکیری مجموعہ کے طور پر بیان کیا جا سکتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat. ",
  "translatedText": "کوآرڈینیٹ x، y کے ساتھ ایک ویکٹر x گنا i-hat کے علاوہ y گنا j-hat ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence. ",
  "translatedText": "تبدیلی سے گزرنے کے بعد، یہ خاصیت جو گرڈ لائنیں متوازی رہتی ہیں اور یکساں طور پر فاصلہ رکھتی ہیں اس کا ایک شاندار نتیجہ ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat. ",
  "translatedText": "وہ جگہ جہاں آپ کا ویکٹر لینڈ کرتا ہے وہ i-hat کے تبدیل شدہ ورژن سے x گنا اور j-ہیٹ کے تبدیل شدہ ورژن سے y گنا زیادہ ہوگا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat. ",
  "translatedText": "اس کا مطلب ہے کہ اگر آپ ان نقاط کا ریکارڈ رکھتے ہیں جہاں i-hat اترتا ہے اور وہ نقاط جہاں j-hat اترتا ہے، تو آپ حساب کر سکتے ہیں کہ ایک ویکٹر جو x, y سے شروع ہوتا ہے اسے i-hat جمع y کے نئے نقاط کے x گنا پر اترنا چاہیے۔ j-hat کے نئے نقاط کی اوقات۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication. ",
  "translatedText": "کنونشن کا مقصد ان نقاط کو ریکارڈ کرنا ہے جہاں i-hat اور j-hat ایک میٹرکس کے کالموں کے طور پر اترتے ہیں، اور ان کالموں کے سکیل شدہ ورژن کے اس مجموعہ کو x اور y سے میٹرکس ویکٹر ضرب ہونے کے لیے متعین کرنا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector. ",
  "translatedText": "اس طرح، ایک میٹرکس ایک مخصوص لکیری تبدیلی کی نمائندگی کرتا ہے، اور میٹرکس کو ویکٹر سے ضرب دینے کا مطلب کمپیوٹیشنل طور پر اس تبدیلی کو اس ویکٹر پر لاگو کرنا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff. ",
  "translatedText": "ٹھیک ہے، نئی چیزوں پر دوبارہ نظر ڈالیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes you find yourself wanting to describe the effects of applying one transformation and then another. ",
  "translatedText": "اکثر اوقات آپ خود کو ایک تبدیلی اور پھر دوسری تبدیلی کو لاگو کرنے کے اثرات کو بیان کرنا چاہتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear. ",
  "translatedText": "مثال کے طور پر، ہوسکتا ہے کہ آپ یہ بیان کرنا چاہیں کہ جب آپ پہلی بار ہوائی جہاز کو 90 ڈگری مخالف گھڑی کی سمت گھماتے ہیں، پھر قینچ لگائیں تو کیا ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear. ",
  "translatedText": "یہاں کا مجموعی اثر، شروع سے آخر تک، ایک اور لکیری تبدیلی ہے، جو گردش اور قینچ سے الگ ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied. ",
  "translatedText": "اس نئی لکیری تبدیلی کو عام طور پر ان دو الگ الگ تبدیلیوں کی ترکیب کہا جاتا ہے جن کا ہم نے اطلاق کیا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat. ",
  "translatedText": "اور کسی بھی لکیری تبدیلی کی طرح، اسے i-hat اور j-hat کی پیروی کرکے اپنے تمام میٹرکس کے ساتھ بیان کیا جا سکتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix. ",
  "translatedText": "اس مثال میں، دونوں تبدیلیوں کے بعد i-hat کے لیے حتمی لینڈنگ اسپاٹ 1,1 ہے، تو آئیے اسے میٹرکس کا پہلا کالم بناتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix. ",
  "translatedText": "اسی طرح، j-hat بالآخر منفی 1,0 مقام پر ختم ہوتا ہے، لہذا ہم اسے میٹرکس کا دوسرا کالم بناتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones. ",
  "translatedText": "یہ نیا میٹرکس ایک گھماؤ پھر قینچ لگانے کے مجموعی اثر کو حاصل کرتا ہے، لیکن لگاتار دو کے بجائے ایک واحد عمل کے طور پر۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix. ",
  "translatedText": "اس نئے میٹرکس کے بارے میں سوچنے کا ایک طریقہ یہ ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix. ",
  "translatedText": "اگر آپ کو کچھ ویکٹر لینا ہے اور اسے گردش کے ذریعے پمپ کرنا ہے، تو قینچ، شمار کرنے کا طویل راستہ جہاں یہ ختم ہوتا ہے پہلے اسے بائیں طرف گردش میٹرکس سے ضرب دینا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.42,
  "end": 194.82
 },
 {
  "input": "Then, take whatever you get and multiply that on the left by the shear matrix. ",
  "translatedText": "پھر، جو کچھ بھی آپ کو ملے اسے لے لیں اور اسے بائیں جانب قینچ میٹرکس سے ضرب دیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 195.32,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector. ",
  "translatedText": "یہ ہے، عددی طور پر، ایک گھماؤ پھر ایک دیے گئے ویکٹر پر قینچ لگانے کا کیا مطلب ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action. ",
  "translatedText": "لیکن جو کچھ بھی آپ کو ملتا ہے وہی ہونا چاہئے اس نئے کمپوزیشن میٹرکس کو لاگو کرنے جیسا کہ ہمیں ابھی اسی ویکٹر سے ملا ہے، اس سے کوئی فرق نہیں پڑتا ہے کہ آپ نے کون سا ویکٹر منتخب کیا ہے، کیونکہ یہ نیا میٹرکس وہی مجموعی اثر حاصل کرے گا جیسا کہ گردش پھر قینچ ایکشن۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you? ",
  "translatedText": "یہاں چیزوں کو کیسے لکھا گیا ہے اس کی بنیاد پر، میرے خیال میں اس نئے میٹرکس کو اصل دو میٹرکس کی پیداوار کہنا مناسب ہے، کیا آپ نہیں؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers. ",
  "translatedText": "ہم اس بارے میں سوچ سکتے ہیں کہ عام طور پر صرف ایک لمحے میں اس پروڈکٹ کی گنتی کیسے کی جائے، لیکن اعداد کے جنگل میں کھو جانا بہت آسان ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another. ",
  "translatedText": "ہمیشہ یاد رکھیں کہ اس طرح دو میٹرکس کو ضرب کرنے کا ہندسی معنی ہے کہ ایک تبدیلی کے بعد دوسری کو لاگو کرنا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left. ",
  "translatedText": "ایک چیز جو یہاں عجیب قسم کی ہے وہ یہ ہے کہ یہ ہمیں دائیں سے بائیں پڑھتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left. ",
  "translatedText": "آپ سب سے پہلے دائیں طرف میٹرکس کے ذریعہ ظاہر کردہ تبدیلی کو لاگو کرتے ہیں، پھر آپ بائیں طرف میٹرکس کے ذریعہ پیش کردہ تبدیلی کو لاگو کرتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left. ",
  "translatedText": "یہ فنکشن نوٹیشن سے پیدا ہوتا ہے، چونکہ ہم متغیرات کے بائیں طرف فنکشن لکھتے ہیں، اس لیے جب بھی آپ دو فنکشنز کمپوز کرتے ہیں، آپ کو اسے ہمیشہ دائیں سے بائیں پڑھنا پڑتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us. ",
  "translatedText": "عبرانی قارئین کے لیے اچھی خبر، ہم سب کے لیے بری خبر۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example. ",
  "translatedText": "آئیے ایک اور مثال دیکھتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this. ",
  "translatedText": "کالم 1,1 اور منفی 2,0 کے ساتھ میٹرکس لیں، جن کی تبدیلی اس طرح نظر آتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it m1. ",
  "translatedText": "اور آئیے اسے m1 کہتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this. ",
  "translatedText": "اس کے بعد، کالم 0،1 اور 2،0 کے ساتھ میٹرکس لیں، جن کی تبدیلی اس طرح نظر آتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy m2. ",
  "translatedText": "اور چلو اس آدمی کو M2 کہتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying m1 then m2 gives us a new transformation, so let's find its matrix. ",
  "translatedText": "m1 پھر m2 لگانے کا کل اثر ہمیں ایک نئی تبدیلی دیتا ہے، تو آئیے اس کا میٹرکس تلاش کرتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix. ",
  "translatedText": "لیکن اس بار، آئیے دیکھتے ہیں کہ کیا ہم متحرک تصاویر کو دیکھے بغیر، اور اس کے بجائے ہر میٹرکس میں عددی اندراجات کا استعمال کر سکتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes. ",
  "translatedText": "سب سے پہلے، ہمیں یہ معلوم کرنا ہوگا کہ i-hat کہاں جاتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying m1, the new coordinates of i-hat, by definition, are given by that first column of m1, namely 1,1. ",
  "translatedText": "m1 لگانے کے بعد، i-hat کے نئے نقاط، تعریف کے مطابق، m1 کے پہلے کالم، یعنی 1,1 کے ذریعے دیے جاتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying m2, multiply the matrix for m2 by that vector 1,1. ",
  "translatedText": "یہ دیکھنے کے لیے کہ m2 لگانے کے بعد کیا ہوتا ہے، m2 کے لیے میٹرکس کو اس ویکٹر 1,1 سے ضرب دیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1. ",
  "translatedText": "اس پر کام کرتے ہوئے، جس طرح میں نے پچھلی ویڈیو کو بیان کیا، آپ کو ویکٹر 2,1 ملے گا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix. ",
  "translatedText": "یہ کمپوزیشن میٹرکس کا پہلا کالم ہوگا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of m1 tells us that it first lands on negative 2,0. ",
  "translatedText": "اسی طرح، j-hat کی پیروی کرنے کے لیے، m1 کا دوسرا کالم ہمیں بتاتا ہے کہ یہ پہلے منفی 2,0 پر اترتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply m2 to that vector, you can work out the matrix vector product to get 0, negative 2, which becomes the second column of our composition matrix. ",
  "translatedText": "پھر، جب ہم اس ویکٹر پر m2 لاگو کرتے ہیں، تو آپ میٹرکس ویکٹر پروڈکٹ کو 0، منفی 2 حاصل کرنے کے لیے کام کر سکتے ہیں، جو ہمارے کمپوزیشن میٹرکس کا دوسرا کالم بن جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices. ",
  "translatedText": "مجھے اسی عمل کے ذریعے دوبارہ بات کرنے دو، لیکن اس بار میں ہر میٹرکس میں متغیر اندراجات دکھاؤں گا، صرف یہ دکھانے کے لیے کہ استدلال کی ایک ہی لائن کسی بھی میٹرکس کے لیے کام کرتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way. ",
  "translatedText": "یہ زیادہ علامت والا ہے اور اس کے لیے کچھ اور جگہ درکار ہوگی، لیکن یہ ہر اس شخص کے لیے کافی اطمینان بخش ہونا چاہیے جسے پہلے میٹرکس ضرب زیادہ روٹ طریقہ سے سکھایا گیا ہو۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands. ",
  "translatedText": "i-ہیٹ کہاں جاتی ہے اس کی پیروی کرنے کے لیے، دائیں جانب میٹرکس کے پہلے کالم کو دیکھ کر شروع کریں، کیونکہ یہ وہ جگہ ہے جہاں i-hat شروع ہوتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation. ",
  "translatedText": "اس کالم کو بائیں طرف میٹرکس سے ضرب دینا یہ ہے کہ آپ یہ کیسے بتا سکتے ہیں کہ دوسری تبدیلی کو لاگو کرنے کے بعد i-hat کا انٹرمیڈیٹ ورژن کہاں ختم ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix. ",
  "translatedText": "لہٰذا کمپوزیشن میٹرکس کا پہلا کالم ہمیشہ بائیں میٹرکس کے دائیں میٹرکس کے پہلے کالم کے برابر ہوگا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix. ",
  "translatedText": "اسی طرح، j-hat ہمیشہ ابتدائی طور پر دائیں میٹرکس کے دوسرے کالم پر اترے گا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix. ",
  "translatedText": "لہٰذا بائیں میٹرکس کو اس دوسرے کالم سے ضرب کرنے سے اس کا حتمی مقام حاصل ہو جائے گا، اور اس وجہ سے یہ مرکب میٹرکس کا دوسرا کالم ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to kind of help remember it. ",
  "translatedText": "نوٹ کریں کہ یہاں بہت ساری علامتیں ہیں، اور یہ عام بات ہے کہ اس فارمولے کو حفظ کرنے کے لیے کچھ سکھایا جائے، اس کے ساتھ ساتھ اسے یاد رکھنے میں مدد کے لیے ایک مخصوص الگورتھمک عمل بھی۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another. ",
  "translatedText": "لیکن میں واقعی میں سوچتا ہوں کہ اس عمل کو یاد کرنے سے پہلے، آپ کو یہ سوچنے کی عادت ڈالنی چاہیے کہ میٹرکس ضرب واقعی کس چیز کی نمائندگی کرتی ہے، ایک کے بعد دوسری تبدیلی کو لاگو کرتے ہوئے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand. ",
  "translatedText": "مجھ پر بھروسہ کریں، یہ آپ کو ایک بہت بہتر تصوراتی فریم ورک دے گا جو میٹرکس ضرب کی خصوصیات کو سمجھنے میں بہت آسان بنا دیتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question. ",
  "translatedText": "مثال کے طور پر، یہاں ایک سوال ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them? ",
  "translatedText": "کیا اس سے کوئی فرق پڑتا ہے کہ جب ہم دونوں میٹرکس کو ضرب دیتے ہیں تو ہم کس ترتیب میں رکھتے ہیں؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier. ",
  "translatedText": "ٹھیک ہے، آئیے ایک سادہ سی مثال کے ذریعے سوچتے ہیں، جیسا کہ پہلے والی مثال تھی۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smushes j-hat over to the right, and a 90 degree rotation. ",
  "translatedText": "ایک قینچ لیں، جو i-ہیٹ کو ٹھیک کرتا ہے اور j-ہیٹ کو دائیں طرف مسح کرتا ہے، اور 90 ڈگری گھماؤ۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1. ",
  "translatedText": "اگر آپ پہلے شیئر کرتے ہیں، پھر گھمائیں، ہم دیکھ سکتے ہیں کہ i-hat 0,1 پر ختم ہوتا ہے اور j-hat منفی 1,1 پر ختم ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together. ",
  "translatedText": "دونوں عام طور پر ایک دوسرے کے قریب کی طرف اشارہ کر رہے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing farther apart. ",
  "translatedText": "اگر آپ پہلے گھماتے ہیں، تو قینچی کریں، i-ہیٹ 1,1 پر ختم ہوتا ہے، اور j-hat منفی 1,0 پر مختلف سمت میں بند ہوتا ہے، اور وہ دور کی طرف اشارہ کر رہے ہوتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently order totally does matter. ",
  "translatedText": "یہاں مجموعی اثر واضح طور پر مختلف ہے، لہذا ظاہر ہے کہ آرڈر بالکل اہمیت رکھتا ہے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice by thinking in terms of transformations, that's the kind of thing you can do in your head by visualizing. ",
  "translatedText": "تبدیلیوں کے لحاظ سے سوچتے ہوئے نوٹس کریں، یہ اس قسم کی چیز ہے جسے آپ تصور کرکے اپنے دماغ میں کرسکتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary. ",
  "translatedText": "میٹرکس ضرب کی ضرورت نہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative. ",
  "translatedText": "مجھے یاد ہے جب میں نے پہلی بار لکیری الجبرا لیا تھا، تو ہوم ورک کا یہ ایک مسئلہ تھا جس نے ہم سے یہ ثابت کرنے کے لیے کہا تھا کہ میٹرکس ضرب ملحقہ ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left. ",
  "translatedText": "اس کا مطلب یہ ہے کہ اگر آپ کے پاس تین میٹرکس ہیں، A، B، اور C، اور آپ ان سب کو ایک ساتھ ضرب دیتے ہیں، تو اس سے کوئی فرق نہیں پڑے گا کہ آپ پہلے A ضرب بی کی گنتی کریں، پھر نتیجہ کو C سے ضرب دیں، یا اگر آپ پہلے B ضرب کریں C، پھر اس نتیجے کو بائیں طرف A سے ضرب دیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses. ",
  "translatedText": "دوسرے لفظوں میں، اس سے کوئی فرق نہیں پڑتا کہ آپ قوسین کہاں رکھتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter. ",
  "translatedText": "اب، اگر آپ عددی طور پر اس کے ذریعے کام کرنے کی کوشش کرتے ہیں، جیسا کہ میں نے اس وقت کیا تھا، تو یہ اس معاملے کے لیے خوفناک، صرف خوفناک، اور غیر روشن ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial. ",
  "translatedText": "لیکن جب آپ میٹرکس ضرب کے بارے میں سوچتے ہیں کہ ایک کے بعد ایک تبدیلی کا اطلاق ہوتا ہے، تو یہ خاصیت محض معمولی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why? ",
  "translatedText": "کیا آپ دیکھ سکتے ہیں کیوں؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C then B, then A, it's the same as applying C, then B, then A. ",
  "translatedText": "یہ کیا کہہ رہا ہے کہ اگر آپ پہلے C، پھر B، پھر A، لگاتے ہیں تو یہ C، پھر B، پھر A لگانے کے مترادف ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove, you're just applying the same three things one after the other, all in the same order. ",
  "translatedText": "میرا مطلب ہے، ثابت کرنے کے لیے کچھ نہیں ہے، آپ صرف ایک کے بعد ایک وہی تین چیزیں لگا رہے ہیں، سب ایک ہی ترتیب میں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 552.82,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not. ",
  "translatedText": "یہ دھوکہ دہی کی طرح محسوس ہوسکتا ہے، لیکن ایسا نہیں ہے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative. ",
  "translatedText": "یہ ایمانداری سے نیکی کا ثبوت ہے کہ میٹرکس ضرب ملحقہ ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 561.54,
  "end": 565.9
 },
 {
  "input": "And even better than that, it's a good explanation for why that property should be true. ",
  "translatedText": "اور اس سے بھی بہتر، یہ اس بات کی اچھی وضاحت ہے کہ وہ جائیداد کیوں درست ہونی چاہیے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.9,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically. ",
  "translatedText": "میں واقعی میں آپ کو اس خیال کے ساتھ مزید کھیلنے کی ترغیب دیتا ہوں، دو مختلف تبدیلیوں کا تصور کرتے ہوئے، یہ سوچتے ہوئے کہ جب آپ ایک کے بعد ایک درخواست دیتے ہیں تو کیا ہوتا ہے، اور پھر میٹرکس پروڈکٹ کو عددی طور پر تیار کرتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in. ",
  "translatedText": "مجھ پر بھروسہ کریں، یہ اس طرح کا پلے ٹائم ہے جو واقعی میں خیال کو ڈوبتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions. ",
  "translatedText": "اگلی ویڈیو میں، میں ان خیالات کو صرف دو جہتوں سے آگے بڑھانے کے بارے میں بات کرنا شروع کروں گا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 587.2,
  "end": 591.42
 },
 {
  "input": "See you then! ",
  "translatedText": "پھر آپ دیکھیں! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.02,
  "end": 592.18
 }
]