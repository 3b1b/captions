[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices.",
  "translatedText": "Sziasztok, ahol legutóbb abbahagytuk, megmutattam, hogy néznek ki a lineáris transzformációk, és hogyan lehet őket mátrixok segítségével ábrázolni.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video.",
  "translatedText": "Ez megér egy gyors összefoglalót, mert egyszerűen nagyon fontos, de természetesen, ha ez többnek tűnik, mint egy egyszerű összefoglaló, menj vissza és nézd meg a teljes videót.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Technically speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed.",
  "translatedText": "Technikailag a lineáris transzformációk olyan függvények, amelyeknek bemenete vektorok, kimenete pedig vektorok, de a múltkor megmutattam, hogy vizuálisan úgy gondolhatunk rájuk, mint a térben való mozgásra úgy, hogy a rácsvonalak párhuzamosak és egyenletes távolságban maradnak, és hogy az origó fix marad.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat.",
  "translatedText": "A legfontosabb tanulság az volt, hogy egy lineáris transzformációt teljes mértékben meghatározza, hogy hova veszi a tér alapvektorait, ami két dimenzió esetén i-hat és j-hat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors.",
  "translatedText": "Ez azért van így, mert bármely más vektor leírható ezen alapvektorok lineáris kombinációjaként.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat.",
  "translatedText": "Az x, y koordinátájú vektor x-szer i-hat plusz y-szer j-hat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence.",
  "translatedText": "Az átalakítás után ennek a tulajdonságnak, hogy a rácsvonalak párhuzamosak és egyenletes távolságban maradnak, csodálatos következménye van.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat.",
  "translatedText": "A hely, ahol a vektorod landol, az i-hat transzformált változatának x-szerese és a j-hat transzformált változatának y-szorosa lesz.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat.",
  "translatedText": "Ez azt jelenti, hogy ha feljegyezzük azokat a koordinátákat, ahol az i-kalap landol, és azokat a koordinátákat, ahol a j-kalap landol, akkor kiszámíthatjuk, hogy az x, y pontból induló vektornak az i-kalap új koordinátáinak x-szeresén és a j-kalap új koordinátáinak y-szorosán kell landolnia.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication.",
  "translatedText": "A konvenció az, hogy az i-hat és j-hat koordinátáit egy mátrix oszlopaként rögzítjük, és ezen oszlopok x-szel és y-jal skálázott változatainak összegét mátrix-vektor szorzásként definiáljuk.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector.",
  "translatedText": "Ily módon egy mátrix egy adott lineáris transzformációt reprezentál, és a mátrixnak egy vektorral való szorzása az, amit számításilag az adott transzformációnak az adott vektorra való alkalmazása jelent.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Alright, recap over, on to the new stuff.",
  "translatedText": "Rendben, vége az összefoglalónak, jöjjenek az új dolgok.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Oftentimes, you find yourself wanting to describe the effects of applying one transformation and then another.",
  "translatedText": "Gyakran azon kapja magát, hogy szeretné leírni az egyik, majd a másik átalakítás alkalmazásának hatásait.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear.",
  "translatedText": "Például, talán azt szeretné leírni, hogy mi történik, ha először elforgatja a síkot 90 fokkal az óramutató járásával ellentétes irányban, majd nyírást alkalmaz.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear.",
  "translatedText": "Az általános hatás itt az elejétől a végéig egy másik lineáris transzformáció, amely különbözik a forgástól és a nyírástól.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied.",
  "translatedText": "Ezt az új lineáris transzformációt általában az általunk alkalmazott két különálló transzformáció kompozíciójának nevezik.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat.",
  "translatedText": "És mint minden lineáris transzformáció, ez is leírható egy saját mátrixszal az i-hat és j-hat követésével.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix.",
  "translatedText": "Ebben a példában az i-hat végső célpontja mindkét transzformáció után az 1,1, ezért tegyük ezt egy mátrix első oszlopává.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix.",
  "translatedText": "Hasonlóképpen, a j-hat végül a negatív 1,0 helyére kerül, így ez lesz a mátrix második oszlopa.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones.",
  "translatedText": "Ez az új mátrix a forgatás, majd a nyírás általános hatását érzékelteti, de nem két egymást követő műveletként, hanem egyetlen műveletként.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Here's one way to think about that new matrix.",
  "translatedText": "Az új mátrixról például így gondolkodhatunk.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix, then take whatever you get and multiply that on the left by the shear matrix.",
  "translatedText": "Ha veszünk egy vektort, és átpumpáljuk a forgatáson, majd a nyíráson, akkor a hosszú módja annak, hogy kiszámítsuk, hogy hova kerül, az, hogy először balra megszorozzuk a forgatási mátrixszal, majd bármit is kapunk, aztán balra megszorozzuk a nyírási mátrixszal.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector.",
  "translatedText": "Számszerűen ez azt jelenti, hogy egy adott vektorra először elforgatást, majd nyírást alkalmazunk.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action.",
  "translatedText": "De bármit is kapunk, annak ugyanannak kell lennie, mintha csak ezt az új összetételi mátrixot alkalmaznánk, amit az imént találtunk ugyanazon a vektoron, függetlenül attól, hogy milyen vektort választottunk, mivel ennek az új mátrixnak ugyanazt az általános hatást kell megragadnia, mint a forgatásnak, majd a nyírási műveletnek.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you?",
  "translatedText": "Az alapján, ahogy a dolgok itt le vannak írva, azt hiszem, ésszerű ezt az új mátrixot az eredeti két mátrix szorzatának nevezni, nem gondolja?",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers.",
  "translatedText": "Egy pillanat múlva elgondolkodhatunk azon, hogyan lehet ezt a szorzatot általánosabban kiszámítani, de túl könnyű elveszni a számok erdejében.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another.",
  "translatedText": "Ne feledjük, hogy két mátrix ilyen módon történő szorzása geometriai értelemben azt jelenti, hogy először az egyik, majd a másik transzformációt alkalmazzuk.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left.",
  "translatedText": "Az egyik dolog, ami itt egy kicsit furcsa, az az, hogy itt jobbról balra olvasunk.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left.",
  "translatedText": "Először a jobb oldali mátrix által ábrázolt transzformációt alkalmazza, majd a bal oldali mátrix által ábrázolt transzformációt.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left.",
  "translatedText": "Ez a függvényjelölésből ered, mivel a függvényeket a változók bal oldalán írjuk, így minden alkalommal, amikor két függvényt állítunk össze, mindig jobbról balra kell olvasni.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us.",
  "translatedText": "Jó hír a héber olvasóknak, rossz hír nekünk, többieknek.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Let's look at another example.",
  "translatedText": "Nézzünk egy másik példát.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.",
  "translatedText": "Vegyük az 1,1 oszlopú és a 2,0 negatív oszlopú mátrixot, amelynek transzformációja így néz ki.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "And let's call it M1.",
  "translatedText": "És nevezzük M1-nek.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.",
  "translatedText": "Ezután vegyük a 0,1 és 2,0 oszlopú mátrixot, amelynek transzformációja így néz ki.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "And let's call that guy M2.",
  "translatedText": "És hívjuk ezt a fickót M2-nek.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The total effect of applying M1 then M2 gives us a new transformation, so let's find its matrix.",
  "translatedText": "Az M1, majd az M2 alkalmazásának összhatása egy új transzformációt ad, keressük meg tehát annak mátrixát.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix.",
  "translatedText": "De ezúttal nézzük meg, hogy meg tudjuk-e csinálni anélkül, hogy az animációkat néznénk, és helyette csak az egyes mátrixok numerikus bejegyzéseit használjuk.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "First, we need to figure out where i-hat goes.",
  "translatedText": "Először is ki kell találnunk, hova kerül az i-hat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "After applying M1, the new coordinates of i-hat, by definition, are given by that first column of M1, namely 1,1.",
  "translatedText": "Az M1 alkalmazása után az i-hat új koordinátáit definíció szerint az M1 első oszlopa adja, azaz 1,1.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "To see what happens after applying M2, multiply the matrix for M2 by that vector 1,1.",
  "translatedText": "Hogy lássuk, mi történik az M2 alkalmazása után, szorozzuk meg az M2 mátrixot az 1,1 vektorral.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1.",
  "translatedText": "Ha úgy dolgozod ki, ahogy az előző videóban leírtam, akkor a 2,1 vektort kapod.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This will be the first column of the composition matrix.",
  "translatedText": "Ez lesz az összetételi mátrix első oszlopa.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Likewise, to follow j-hat, the second column of M1 tells us that it first lands on negative 2,0.",
  "translatedText": "Hasonlóképpen, hogy kövessük a j-hat-ot, az M1 második oszlopából megtudhatjuk, hogy először a negatív 2,0-ra érkezik.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Then, when we apply M2 to that vector, you can work out the matrix-vector product to get 0, negative 2, which becomes the second column of our composition matrix.",
  "translatedText": "Ezután, amikor az M2-t alkalmazzuk erre a vektorra, a mátrix-vektor szorzatot kiszámítva megkapjuk a 0, negatív 2 értéket, ami a kompozíciós mátrixunk második oszlopa lesz.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices.",
  "translatedText": "Hadd beszéljem át még egyszer ugyanezt a folyamatot, de ezúttal minden mátrixban változó bejegyzéseket fogok mutatni, csak hogy megmutassam, hogy ugyanaz az érvelés bármely mátrix esetében működik.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way.",
  "translatedText": "Ez sokkal szimbólumdúsabb, és némileg több helyet igényel, de elég kielégítő lehet mindazok számára, akiknek korábban a mátrixszorzást rutinszerűen tanították.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands.",
  "translatedText": "Ha követni szeretnénk, hogy hová megy az i-hat, kezdjük a mátrix első oszlopával a jobb oldalon, mivel az i-hat eredetileg itt landol.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation.",
  "translatedText": "Ha ezt az oszlopot megszorozzuk a bal oldali mátrixszal, akkor meg tudjuk mondani, hogy a második transzformáció alkalmazása után hova kerül az i-hat köztes változata.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix.",
  "translatedText": "Tehát az összetételi mátrix első oszlopa mindig egyenlő a bal mátrix és a jobb mátrix első oszlopának szorzatával.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix.",
  "translatedText": "Hasonlóképpen, a j-hat kezdetben mindig a jobb oldali mátrix második oszlopában fog landolni.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix.",
  "translatedText": "Tehát a bal oldali mátrixot megszorozva ezzel a második oszloppal megkapjuk a végső helyét, és így ez lesz az összetételi mátrix második oszlopa.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to help remember it.",
  "translatedText": "Vegye észre, hogy itt sok szimbólum van, és gyakori, hogy ezt a képletet úgy tanítják, mint valami megjegyzendő dolgot, egy bizonyos algoritmikus eljárással együtt, amely segít emlékezni rá.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another.",
  "translatedText": "De tényleg úgy gondolom, hogy mielőtt ezt a folyamatot memorizálnád, szokj rá arra, hogy átgondolod, mit is jelent valójában a mátrixszorzás, és egyik transzformációt a másik után alkalmazod.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand.",
  "translatedText": "Higgye el, ez sokkal jobb fogalmi keretet ad, amely sokkal könnyebben érthetővé teszi a mátrixszorzás tulajdonságait.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "For example, here's a question.",
  "translatedText": "Itt van például egy kérdés.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them?",
  "translatedText": "Számít-e, hogy milyen sorrendbe állítjuk a két mátrixot, amikor megszorozzuk őket?",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier.",
  "translatedText": "Nos, gondoljunk végig egy egyszerű példát, mint az előbbi.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Take a shear, which fixes i-hat and smooshes j-hat over to the right, and a 90 degree rotation.",
  "translatedText": "Vegyünk egy nyírást, amely rögzíti az i-kalapot és jobbra tolja a j-kalapot, valamint egy 90 fokos elforgatást.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1.",
  "translatedText": "Ha először a nyírást végezzük el, majd elforgatjuk, láthatjuk, hogy az i-kalap 0,1-nél, a j-kalap pedig negatív 1,1-nél végződik.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Both are generally pointing close together.",
  "translatedText": "Mindkettő általában közel van egymáshoz.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing, you know, farther apart.",
  "translatedText": "Ha először elforgatjuk, majd elvégezzük a nyírást, akkor az i-kalap az 1,1 ponton köt ki, a j-kalap pedig egy másik irányba, negatív 1,0 ponton, és távolabb mutatnak egymástól.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The overall effect here is clearly different, so evidently, order totally does matter.",
  "translatedText": "Az összhatás itt egyértelműen más, tehát nyilvánvaló, hogy a sorrend nem mindegy.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Notice, by thinking in terms of transformations, that's the kind of thing that you can do in your head by visualizing.",
  "translatedText": "Vegyétek észre, hogy az átalakulásokban gondolkodva, ez az a fajta dolog, amit a fejetekben vizualizálással meg tudtok tenni.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "No matrix multiplication necessary.",
  "translatedText": "Nincs szükség mátrixszorzásra.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative.",
  "translatedText": "Emlékszem, amikor először tanultam lineáris algebrát, volt egy házi feladat, amelyben azt kellett bebizonyítanunk, hogy a mátrixszorzás asszociatív.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left.",
  "translatedText": "Ez azt jelenti, hogy ha van három mátrixunk, A, B és C, és ezeket összeszorozzuk, akkor nem számít, hogy először kiszámítjuk A szorozva B-vel, majd az eredményt megszorozzuk C-vel, vagy először megszorozzuk B-vel C-t, majd az eredményt balra megszorozzuk A-val.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses.",
  "translatedText": "Más szóval, nem számít, hogy hova teszed a zárójeleket.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter.",
  "translatedText": "Ha most megpróbálod ezt számszerűen feldolgozni, ahogy én is tettem akkoriban, az borzalmas, egyszerűen borzalmas, és ami azt illeti, felvilágosulatlan.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial.",
  "translatedText": "De ha a mátrixszorzásra úgy gondolunk, mint az egyik transzformáció alkalmazására a másik után, akkor ez a tulajdonság egyszerűen triviális.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Can you see why?",
  "translatedText": "Látod, miért?",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "What it's saying is that if you first apply C, then B, then A, it's the same as applying C, then B, then A.",
  "translatedText": "Azt mondja, hogy ha először a C-t, majd a B-t, majd az A-t alkalmazzuk, az ugyanaz, mintha a C-t, majd a B-t, majd az A-t alkalmaznánk.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "I mean, there's nothing to prove.",
  "translatedText": "Úgy értem, nincs mit bizonyítani.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "You're just applying the same three things one after the other, all in the same order.",
  "translatedText": "Csak ugyanazt a három dolgot alkalmazod egymás után, ugyanabban a sorrendben.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This might feel like cheating, but it's not.",
  "translatedText": "Ez csalásnak tűnhet, de nem az.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative, and even better than that, it's a good explanation for why that property should be true.",
  "translatedText": "Ez egy őszinte bizonyíték arra, hogy a mátrixszorzás asszociatív, és ami még ennél is jobb, ez egy jó magyarázat arra, hogy miért kell ennek a tulajdonságnak igaznak lennie.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically.",
  "translatedText": "Tényleg arra bátorítom, hogy játsszon többet ezzel az ötlettel, képzelje el a két különböző transzformációt, gondolkodjon el azon, mi történik, ha az egyiket a másik után alkalmazza, majd számszerűen dolgozza ki a mátrixszorzatot.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in.",
  "translatedText": "Higgye el, ez az a fajta játékidő, amely igazán megérteti a gondolatot.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions.",
  "translatedText": "A következő videóban arról fogok beszélni, hogyan lehet ezeket az ötleteket két dimenzión túl is kiterjeszteni.",
  "model": "DeepL",
  "n_reviews": 0
 }
]