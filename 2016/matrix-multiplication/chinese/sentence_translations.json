[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices. ",
  "translatedText": "大家好，在我们上次停下的地方，我展示了线 性变换的样子以及如何使用矩阵来表示它们。",
  "model": "google_nmt",
  "from_community_srt": "嘿 大家好！ 上期视频结束前 我展示了线性变换长什么样，",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video. ",
  "translatedText": "这值得快速回顾一下，因为它非常重要，但当然，如 果这感觉不仅仅是回顾，请返回并观看完整的视频。",
  "model": "google_nmt",
  "from_community_srt": "以及如何用矩阵描述它们 这值得我们快速回顾一下， 因为它们实在是很重要 当然， 如果你觉得不是“回顾”这么简单，",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Generally speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed. ",
  "translatedText": "一般来说，线性变换是以向量作为输入、以向量 作为输出的函数，但我上次展示了如何在视觉 上将它们视为在空间中平滑移动，从而使网格 线保持平行且均匀间隔，并且原点保持固定。",
  "model": "google_nmt",
  "from_community_srt": "那就再回去看看上期视频吧 严格意义上说， 线性变换是将向量作为输入和输出的一类函数 但是上期视频中， 我说过可以将线性变换看作对空间的挤压伸展 它保持网格线平行且等距分布，",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat. ",
  "translatedText": "关键要点是线性变换完全取决于它采用空间基向量的位置 ，这对于二维意味着 i-hat 和 j-hat。",
  "model": "google_nmt",
  "from_community_srt": "并且保持原点不变 关键的一点在于， 线性变换由它对空间的基向量的作用完全决定 在二维空间中，",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors. ",
  "translatedText": "这是因为任何其他向量都可以描 述为这些基向量的线性组合。",
  "model": "google_nmt",
  "from_community_srt": "基向量就是i帽和j帽 这是因为其他任意向量都能表示为基向量的线性组合 坐标为(x,",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat. ",
  "translatedText": "坐标为 x、y 的向量等于 x 乘以 i-hat 加上 y 乘以 j-hat。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence. ",
  "translatedText": "经过变换后，网格线保持平行且间 隔均匀的特性产生了奇妙的结果。",
  "model": "google_nmt",
  "from_community_srt": "y)的向量就是x乘以i帽加上y乘以j帽 在线性变换之后 网格线保持平行且等距分布这一性质有个绝妙的推论 向量(x,",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat. ",
  "translatedText": "矢量降落的位置将是 i-hat 变换版本的 x 倍加上 j-hat 变换版本的 y 倍。",
  "model": "google_nmt",
  "from_community_srt": "y)变换之后的结果， 将是x乘以变换后的i帽，",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat. ",
  "translatedText": "这意味着，如果您记录 i-hat 落在的坐标和 j-hat 落 在的坐标，您可以计算出从 x, y 开始的向量必须落在 x 乘 以 i-hat 的新坐标加 y 上乘以 j-hat 的新坐标。",
  "model": "google_nmt",
  "from_community_srt": "加上y乘以变换后的j帽 这意味着只要记录下i帽和j帽变换后的位置 你就能计算出一个坐标为(x, y)的向量变换后的坐标 就是x乘以变换后i帽的坐标， 加上y乘以变换后j帽的坐标 习惯上，",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication. ",
  "translatedText": "约定是将 i-hat 和 j-hat 落地 的坐标记录为矩阵的列，并将这些列按 x 和 y 缩放后的总和定义为矩阵向量乘法。",
  "model": "google_nmt",
  "from_community_srt": "我们将变换后i帽和j帽的坐标作为一个矩阵的列 并且将两列分别与x和y相乘后加和的结果定义为矩阵向量乘积 这样，",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector. ",
  "translatedText": "这样，矩阵表示特定的线性变换，将矩阵乘以 向量就是将该变换应用于该向量的计算意义。",
  "model": "google_nmt",
  "from_community_srt": "矩阵代表一个特定的线性变换 而矩阵与向量相乘 就是将线性变换作用于那个向量 回顾结束！",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff. ",
  "translatedText": "好了，回顾一下，开始新内容。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes you find yourself wanting to describe the effects of applying one transformation and then another. ",
  "translatedText": "通常，您会发现自己想要描述应用一 种转换然后应用另一种转换的效果。",
  "model": "google_nmt",
  "from_community_srt": "开始新的内容 很多时候你发现你想描述这样一种作用：一个变换之后再进行另一个变换 比如说，",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear. ",
  "translatedText": "例如，也许您想要描述当您首先将平面逆时针旋 转 90 度，然后应用剪切时会发生什么。",
  "model": "google_nmt",
  "from_community_srt": "你想描述 将整个平面逆时针旋转90度后，",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear. ",
  "translatedText": "这里的整体效果，从开始到结束，是另 一种线性变换，与旋转和剪切不同。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied. ",
  "translatedText": "这种新的线性变换通常称为我们 应用的两个单独变换的组合。",
  "model": "google_nmt",
  "from_community_srt": "再进行一次剪切变换会发生什么 从头到尾的总体作用是另一个线性变换 它与旋转和剪切明显不同 这个新的线性变换通常被称为前两个独立变换的“复合变换” 和其他线性变换一样 我们也能通过追踪i帽和j帽，",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat. ",
  "translatedText": "与任何线性变换一样，它可以通过遵循 i-h at 和 j-hat 用自己的矩阵来描述。",
  "model": "google_nmt",
  "from_community_srt": "并用矩阵完全描述这个复合变换 在这个例子中，",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix. ",
  "translatedText": "在此示例中，两次转换后 i-hat 的最终着陆 点是 1,1，因此我们将其作为矩阵的第一列。",
  "model": "google_nmt",
  "from_community_srt": "i帽在两个线性变换之后的最终落点是(1, 1) 我们将它作为矩阵的第一列 类似地，",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix. ",
  "translatedText": "同样，j-hat 最终位于负 1,0 位置，因此我们将其作为矩阵的第二列。",
  "model": "google_nmt",
  "from_community_srt": "j帽最终落在(-1,",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones. ",
  "translatedText": "这个新矩阵捕捉了应用旋转然后剪切的整体效果， 但作为一个单一动作，而不是两个连续的动作。",
  "model": "google_nmt",
  "from_community_srt": "0) 我们将它作为矩阵的第二列 这一新的矩阵捕捉到了旋转然后剪切的总体效应 但它是一个单独的作用，",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix. ",
  "translatedText": "这是思考新矩阵的一种方法。",
  "model": "google_nmt",
  "from_community_srt": "而不是两个相继作用的合成 这里有种方法来考虑这个新矩阵 如果你有一个向量，",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix. ",
  "translatedText": "如果你要获取一些向量并通过旋转泵送 它，那么剪切，计算它最终的位置的长 方法是首先将它在左侧乘以旋转矩阵。",
  "model": "google_nmt",
  "from_community_srt": "将它进行旋转然后剪切 一个麻烦的计算方法是： 首先将它左乘旋转矩阵 然后将得到的结果再左乘剪切矩阵 从数值角度看，",
  "n_reviews": 0,
  "start": 185.42,
  "end": 194.82
 },
 {
  "input": "Then, take whatever you get and multiply that on the left by the shear matrix. ",
  "translatedText": "然后，将得到的值乘以左侧的剪切矩阵。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 195.32,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector. ",
  "translatedText": "从数字上来说，这就是对给定向 量应用旋转然后剪切的含义。",
  "model": "google_nmt",
  "from_community_srt": "这意味着对一个给定向量进行旋转然后剪切 但是无论所选向量是什么，",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action. ",
  "translatedText": "但是无论您得到什么，都应该与应用我们刚刚通过同一向 量找到的新合成矩阵相同，无论您选择什么向量，因为这 个新矩阵应该捕获与旋转然后剪切动作相同的整体效果。",
  "model": "google_nmt",
  "from_community_srt": "结果都应该与复合变换作用的结果完全相同 因为新矩阵应当捕捉到了旋转然后剪切的相同总体效应 根据我们这里所写下的内容 我认为将这个新矩阵称为最初两个矩阵的积是合理的，",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you? ",
  "translatedText": "根据这里的写法，我认为将这个新矩阵称为 原始两个矩阵的乘积是合理的，不是吗？",
  "model": "google_nmt",
  "from_community_srt": "不是吗？",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers. ",
  "translatedText": "我们可以立即考虑如何更普遍地计算该 乘积，但很容易迷失在数字的森林中。",
  "model": "google_nmt",
  "from_community_srt": "我们很快就能搞清楚如何在更普遍的情况下计算这个乘积 但是这样太容易迷失在数字丛林当中 时刻记得 两个矩阵相乘有着几何意义，",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another. ",
  "translatedText": "永远记住，像这样的两个矩阵相乘具有先应 用一种变换再应用另一种变换的几何意义。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left. ",
  "translatedText": "这里有点奇怪的一件事是我们从右向左阅读。",
  "model": "google_nmt",
  "from_community_srt": "也就是两个线性变换相继作用 这里有件奇怪的事，",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left. ",
  "translatedText": "首先应用右侧矩阵表示的变换， 然后应用左侧矩阵表示的变换。",
  "model": "google_nmt",
  "from_community_srt": "就是这个乘积需要从右向左读 首先应用右侧矩阵所描述的变换 然后再应用左侧矩阵所描述的变换 它起源于函数的记号，",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left. ",
  "translatedText": "这源于函数表示法，因为我们将函数写在变量的左侧，所 以每次组合两个函数时，您总是必须从右到左读取它。",
  "model": "google_nmt",
  "from_community_srt": "因为我们将函数写在变量左侧 所以每次将两个函数复合时，",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us. ",
  "translatedText": "对于希伯来语读者来说是个好消息，对于我们其他人来说则是坏消息。",
  "model": "google_nmt",
  "from_community_srt": "你总是要从右向左读 对希伯来读者是好消息，",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example. ",
  "translatedText": "让我们看另一个例子。",
  "model": "google_nmt",
  "from_community_srt": "对其他人则是坏消息 我们再看一个例子 一个矩阵，",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this. ",
  "translatedText": "取第 1,1 列和第 2,0 列为负的矩阵，其变换如下所示。",
  "model": "google_nmt",
  "from_community_srt": "两列为(1, 1)和(-2,",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it m1. ",
  "translatedText": "我们称之为 m1。",
  "model": "google_nmt",
  "from_community_srt": "0) 它所代表的变换长这样 我们称它为M1 另一个矩阵，",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this. ",
  "translatedText": "接下来，采用具有 0,1 和 2,0 列的矩阵，其变换如下所示。",
  "model": "google_nmt",
  "from_community_srt": "两列为(0, 1)和(2,",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy m2. ",
  "translatedText": "我们称那个人为m2。",
  "model": "google_nmt",
  "from_community_srt": "0) 它所代表的变换长这样 我们称它为M2 M1和M2先后作用，",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying m1 then m2 gives us a new transformation, so let's find its matrix. ",
  "translatedText": "应用 m1 然后 m2 的总效果为我们提供 了一个新的变换，所以让我们找到它的矩阵。",
  "model": "google_nmt",
  "from_community_srt": "总体效果是一个新的变换，",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix. ",
  "translatedText": "但这一次，让我们看看是否可以在不观看动画的情况 下做到这一点，而只使用每个矩阵中的数字条目。",
  "model": "google_nmt",
  "from_community_srt": "我们来求解它的矩阵 但是这一次， 我们来尝试一下不通过观看动画 只使用每个矩阵的数值来求解 首先，",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes. ",
  "translatedText": "首先，我们需要弄清楚 i-hat 的去向。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying m1, the new coordinates of i-hat, by definition, are given by that first column of m1, namely 1,1. ",
  "translatedText": "应用 m1 后，根据定义，i-hat 的新 坐标由 m1 的第一列（即 1,1）给出。",
  "model": "google_nmt",
  "from_community_srt": "我们得算出i帽的去向 在M1作用之后 按照定义， i帽的新坐标由M1的第一列给出， 也就是(1,",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying m2, multiply the matrix for m2 by that vector 1,1. ",
  "translatedText": "要查看应用 m2 后会发生什么，请将 m2 的矩阵乘以该向量 1,1。",
  "model": "google_nmt",
  "from_community_srt": "1) 要看看M2作用之后会发生什么， 将矩阵M2乘以向量(1,",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1. ",
  "translatedText": "按照我上一个视频描述的方式进行计算，您将得到向量 2,1。",
  "model": "google_nmt",
  "from_community_srt": "1) 用上期视频中我讲述的方法计算出结果， 你会得到向量(2,",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix. ",
  "translatedText": "这将是组成矩阵的第一列。",
  "model": "google_nmt",
  "from_community_srt": "1) 这就是复合矩阵的第一列 类似地，",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of m1 tells us that it first lands on negative 2,0. ",
  "translatedText": "同样，遵循 j-hat，m1 的第二 列告诉我们它首先落在负 2,0 上。",
  "model": "google_nmt",
  "from_community_srt": "M1的第二列告诉我们j帽首先落在(-2,",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply m2 to that vector, you can work out the matrix vector product to get 0, negative 2, which becomes the second column of our composition matrix. ",
  "translatedText": "然后，当我们将 m2 应用于该向量时，您可以计算出矩阵向 量乘积，得到 0，负 2，这成为我们的复合矩阵的第二列。",
  "model": "google_nmt",
  "from_community_srt": "0) 然后将M2作用于这个向量 你能根据矩阵向量乘法计算得到(0,",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices. ",
  "translatedText": "让我再次讨论相同的过程，但这次我将显示每个矩阵中的变 量条目，只是为了表明相同的推理过程适用于任何矩阵。",
  "model": "google_nmt",
  "from_community_srt": "-2) 这就是复合矩阵的第二列 我再重复一次同样的过程， 不过这次我们用变量代替数值 只是为了说明这一推理过程对于任意矩阵都适用 这种方法符号繁多，",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way. ",
  "translatedText": "这需要更多的符号，并且需要更多的空间，但是对于以前以更死记 硬背的方式学习过矩阵乘法的人来说，它应该是非常令人满意的。",
  "model": "google_nmt",
  "from_community_srt": "也需要更多空间 但是那些曾经通过书面计算学习矩阵乘法的人来说是非常满足的 要跟踪i帽的去向，",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands. ",
  "translatedText": "要跟踪 i-hat 的去向，请首先查看右侧矩阵的 第一列，因为这是 i-hat 最初落地的位置。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation. ",
  "translatedText": "将该列乘以左侧的矩阵，您就可以知道应用第二次转 换后 i-hat 的中间版本最终出现在哪里。",
  "model": "google_nmt",
  "from_community_srt": "首先找右侧矩阵的第一列 因为这是i帽首先到达的地方 将这一列左乘左侧的矩阵 结果就是i帽在第二个变换作用后的结果 所以复合矩阵的第一列 就是左侧矩阵与右侧矩阵第一列的乘积 类似地，",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix. ",
  "translatedText": "因此，合成矩阵的第一列始终等 于左矩阵乘以右矩阵的第一列。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix. ",
  "translatedText": "同样，j-hat 最初总是落在右侧矩阵的第二列上。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix. ",
  "translatedText": "因此，将左侧矩阵乘以第二列将给出其最 终位置，因此这是合成矩阵的第二列。",
  "model": "google_nmt",
  "from_community_srt": "j帽首先落在右侧矩阵第二列所代表的位置上 左侧矩阵与这一列相乘就能得到j帽的最终位置 因此这一乘积就是复合矩阵的第二列 注意，",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to kind of help remember it. ",
  "translatedText": "请注意，这里有很多符号，通常会教授这个公式作为 要记住的东西，以及某种帮助记住它的算法过程。",
  "model": "google_nmt",
  "from_community_srt": "这里有不少符号，",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another. ",
  "translatedText": "但我确实认为，在记住该过程之前， 您应该养成思考矩阵乘法真正代表什 么的习惯，应用一个又一个变换。",
  "model": "google_nmt",
  "from_community_srt": "通常学生需要记住这个公式 并通过一些特定的运算加强记忆 但是在记忆这个过程前 我认为你应该养成思考矩阵乘法意义的习惯 也就是两个变换相继作用 相信我，",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand. ",
  "translatedText": "相信我，这将为您提供一个更好的概念 框架，使矩阵乘法的属性更容易理解。",
  "model": "google_nmt",
  "from_community_srt": "这能给你一个更好的概念性框架 并让你更容易理解矩阵乘积的性质 比如说下面这个问题 矩阵相乘时，",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question. ",
  "translatedText": "例如，这是一个问题。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them? ",
  "translatedText": "当我们将两个矩阵相乘时，它们的顺序重要吗？",
  "model": "google_nmt",
  "from_community_srt": "它们的先后顺序影响结果吗？",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier. ",
  "translatedText": "好吧，让我们考虑一个简单的例子，就像前面的例子一样。",
  "model": "google_nmt",
  "from_community_srt": "我们先来想一个简单的例子，",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smushes j-hat over to the right, and a 90 degree rotation. ",
  "translatedText": "拿一把剪刀，它可以固定 i-hat 并将 j-hat 压到右侧，然后旋转 90 度。",
  "model": "google_nmt",
  "from_community_srt": "比如之前提到的 一个是剪切， 它保持i帽不变， 将j帽挤到右边 一个是90度旋转 如果你首先剪切，",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1. ",
  "translatedText": "如果你先进行剪切，然后旋转，我们可以看到 i-ha t 最终为 0,1，j-hat 最终为负 1,1。",
  "model": "google_nmt",
  "from_community_srt": "然后旋转 你会发现i帽落在(0, 1)， j帽落在(-1,",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together. ",
  "translatedText": "两者通常都指向一起。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing farther apart. ",
  "translatedText": "如果你先旋转，然后进行剪切，i-hat 最终会在 1,1 处结束，而 j-hat 在负 1,0 处朝不同方向偏离，并且它们指向的距离更远。",
  "model": "google_nmt",
  "from_community_srt": "1) 它们彼此靠得很近 如果你首先旋转， 然后剪切 i帽落在(1, 1)， 而j帽落在一个不同的方向(-1,",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently order totally does matter. ",
  "translatedText": "这里的整体效果明显不同，所以显然顺序很重要。",
  "model": "google_nmt",
  "from_community_srt": "0) 它们的指向分隔很远 二者总体效应明显不同， 所以乘积顺序显然会有影响 注意，",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice by thinking in terms of transformations, that's the kind of thing you can do in your head by visualizing. ",
  "translatedText": "请注意，通过从转换的角度思考，这就是 你可以通过想象在头脑中完成的事情。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary. ",
  "translatedText": "不需要矩阵乘法。",
  "model": "google_nmt",
  "from_community_srt": "我们用在变换来思考 这一过程可以在脑中形象地进行 完全不需要做矩阵乘法 我记得我首次学习线性代数时，",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative. ",
  "translatedText": "我记得当我第一次学习线性代数时，有一个作 业问题要求我们证明矩阵乘法是结合律的。",
  "model": "google_nmt",
  "from_community_srt": "有一道作业题是 让我们证明矩阵乘法具有结合性 这是在说，",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left. ",
  "translatedText": "这意味着，如果您有三个矩阵 A、B 和 C，并且将它们全部相 乘，则无论您先计算 A 乘以 B，然后将结果乘以 C，还是先 将 B 乘以，都没有关系。C，然后将该结果乘以左边的 A。",
  "model": "google_nmt",
  "from_community_srt": "如果你有三个矩阵A B C， 然后将它们相乘 无论是首先计算A乘以B， 然后将结果乘以C 还是先算B乘以C， 然后将结果左乘A，",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses. ",
  "translatedText": "换句话说，括号放在哪里并不重要。",
  "model": "google_nmt",
  "from_community_srt": "二者结果应该相同 换句话讲，",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter. ",
  "translatedText": "现在，如果你尝试用数字来解决这个问题，就像我当时所 做的那样，那就太可怕了，太可怕了，而且毫无启发性。",
  "model": "google_nmt",
  "from_community_srt": "添加括号与结果无关 现在如果你尝试用数值方法证明， 就像我当时所做的 这个过程会非常糟糕，",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial. ",
  "translatedText": "但是，当您将矩阵乘法视为应用一个又一 个变换时，这个属性就显得微不足道了。",
  "model": "google_nmt",
  "from_community_srt": "而且对你毫无启发 但是如果你用变换相继作用的思想去考虑矩阵乘积 这一性质就变得很平凡了 你能看出为什么吗？",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why? ",
  "translatedText": "你能明白为什么吗？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C then B, then A, it's the same as applying C, then B, then A. ",
  "translatedText": "它的意思是，如果您先应用 C，然后应用 B，然后应用 A，则与应用 C、然后 B、然后 A 的效果相同。",
  "model": "google_nmt",
  "from_community_srt": "这是在说， 首先应用C变换和B变换， 然后应用A变换 与先应用C变换，",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove, you're just applying the same three things one after the other, all in the same order. ",
  "translatedText": "我的意思是，没有什么可以证明的，你只是 按照相同的顺序依次应用相同的三件事。",
  "model": "google_nmt",
  "from_community_srt": "然后应用B变换和A变换的结果相同 完全没有需要证明的东西 你只是将同样的三个变换用同样的顺序依次作用而已 可能这看起来是在耍花招，",
  "n_reviews": 0,
  "start": 552.82,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not. ",
  "translatedText": "这可能感觉像是作弊，但事实并非如此。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative. ",
  "translatedText": "这是矩阵乘法具有结合性的诚实证明。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 561.54,
  "end": 565.9
 },
 {
  "input": "And even better than that, it's a good explanation for why that property should be true. ",
  "translatedText": "甚至更好的是，它很好地解释了为什么该属性应该是正确的。",
  "model": "google_nmt",
  "from_community_srt": "但并不是 这是证明矩阵乘法具有结合性的一个实实在在的证明 它甚至很好地解释了为什么这个性质应该是正确的 我真的鼓励你在这种想法上多做尝试 想象两个不同的变换 思考他们依次作用后会发生什么",
  "n_reviews": 0,
  "start": 565.9,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically. ",
  "translatedText": "我真的鼓励你更多地尝试这个想法，想象两种 不同的变换，思考当你一个接一个地应用时 会发生什么，然后用数字计算出矩阵乘积。",
  "model": "google_nmt",
  "from_community_srt": "最后用数值方法计算出矩阵乘积 相信我，",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in. ",
  "translatedText": "相信我，这样的游戏时间才能真正让这个想法深入人心。",
  "model": "google_nmt",
  "from_community_srt": "这段时间能让你完全理解这一想法 下期视频中，",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions. ",
  "translatedText": "在下一个视频中，我将开始讨论将这些想法扩展到二维之外。",
  "model": "google_nmt",
  "from_community_srt": "我会将二维空间中的这些想法拓展至三维空间中 到时候再见！",
  "n_reviews": 0,
  "start": 587.2,
  "end": 591.42
 },
 {
  "input": "See you then! ",
  "translatedText": "回头见！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.02,
  "end": 592.18
 }
]