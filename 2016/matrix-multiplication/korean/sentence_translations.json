[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices.",
  "translatedText": "안녕하세요 여러분, 지난번에 이어 선형 변환이 어떤 모습인지, 그리고 행렬을 사용하여 이를 표현하는 방법을 보여드렸습니다.",
  "model": "google_nmt",
  "from_community_srt": "안녕하세요 여러분? 지난 시간에는 선형변환이 어떤 것인지 설명해드렸고 선형변환을 행렬을 이용해 표현하는 방법도 소개해드렸습니다.",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video.",
  "translatedText": "이것은 매우 중요하기 때문에 빠르게 요약할 가치가 있지만, 물론 이것이 단순한 요약 이상의 것처럼 느껴지면 돌아가서 전체 비디오를 시청하십시오.",
  "model": "google_nmt",
  "from_community_srt": "지난 번에 다룬 내용을 다시 요약해드리겠습니다. 왜냐하면 이건 정말 중요한 것이거든요. 물론, 요약만으로 부족하다고 느낀다면 다시 이전 동영상 전체를 시청하는 것도 좋은 방법입니다.",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Technically speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed.",
  "translatedText": "기술적으로 말하면 선형 변환은 벡터를 입력으로, 벡터를 출력으로 사용하는 함수이지만 지난 시간에 그리드 선이 평행하고 균일한 간격을 유지하고 원점을 유지하는 방식으로 공간 주위를 스무딩하는 것으로 시각적으로 생각할 수 있는 방법을 보여주었습니다. 고정된 상태로 유지됩니다.",
  "model": "google_nmt",
  "from_community_srt": "기술적으로 말하자면, 선형변환은 한마디로 함수입니다. 벡터를 집어넣으면[벡터가 정의역] 벡터가 나오는 것이지요.[벡터가 치역] 지난번에 제가 이것을 시각적으로 보여드렸습니다. 어떻게  선형변환을 생각할 수 있는 지를요. 공간을 이리저리 비틀면서 말이죠. 물론 격자선들은 여전히 평행하고 균등간격을 유지한 채로요. 그리고 원점은 고정되어 있습니다.",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat.",
  "translatedText": "핵심 내용은 선형 변환이 공간의 기본 벡터를 취하는 위치에 따라 완전히 결정된다는 것입니다. 이는 2차원의 경우 i-hat과 j-hat을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "여기서 중요한 점은, 기저벡터가 선형변환에 의해 어떻게 옮겨졌는지를 알면, 그 선형 변환이 무엇인지 파악할 수 있다는 것입니다. 예를 들어, 2차원 공간에서는 i-hat 벡터와 j-hat 벡터로요.",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors.",
  "translatedText": "이는 다른 벡터가 해당 기저 벡터의 선형 조합으로 설명될 수 있기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "왜냐하면 모든 벡터들은 이 i-hat 벡터와 j-hat 벡터의 선형 결합으로 모두 표현이 가능하기 때문이죠.",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat.",
  "translatedText": "x, y 좌표를 갖는 벡터는 x 곱하기 i-hat 더하기 y 곱하기 j-hat입니다.",
  "model": "google_nmt",
  "from_community_srt": "좌표값이 (x,y) 인 벡터는 i-hat 벡터의 x 배 + j-hat 벡터의 y 배로 표현됩니다.",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence.",
  "translatedText": "변환을 거친 후에도 그리드 선이 평행하고 균일한 간격을 유지하는 이 속성은 놀라운 결과를 가져옵니다.",
  "model": "google_nmt",
  "from_community_srt": "선형변환을 하고 나서도, 격자선들이 여전히 평행을 유지하고 균등간격을 유지한다는 멋진 결론을 얻었죠.",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat.",
  "translatedText": "벡터가 착지하는 장소는 변환된 i-hat 버전의 x배와 변환된 j-hat 버전의 y배가 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "변환된 벡터는 변환된 i-hat 의 x배 + 변환된 j-hat 의 y 배로 표현된다는 것도요.",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat.",
  "translatedText": "즉, i-hat이 착지하는 좌표와 j-hat이 착지하는 좌표를 기록해 두면 x, y에서 시작하는 벡터가 i-hat과 y의 새 좌표의 x배에 착지해야 한다는 것을 계산할 수 있습니다. j-hat의 새 좌표를 곱합니다.",
  "model": "google_nmt",
  "from_community_srt": "이 말의 뜻은 만약 우리가 변환된 i-hat 의 좌표값과 변환된 j-hat의 좌표값을 안다면, 초기값 (x, y) 에서 시작한 벡터가 변환  후 어디로 갈지 계산해낼 수 있다는 것입니다. 바로 변환된 i-hat 좌표값의 x 배와 변환된 j-hat 좌표값의 y 배를 이용해서 말이죠.",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication.",
  "translatedText": "규칙은 i-hat과 j-hat이 위치하는 좌표를 행렬의 열로 기록하고 해당 열의 스케일링된 버전의 합계를 x와 y로 정의하여 행렬-벡터 곱셈으로 정의하는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "관례적으로 i-hat 과 j-hat 의 변형후 좌표값을 행렬로 표현합니다. 그리고 이 열들(벡터) 각각을 x, y로 스케일링한 것을 행렬-벡터 곱셈으로 정의합니다.",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector.",
  "translatedText": "이러한 방식으로 행렬은 특정 선형 변환을 나타내며, 행렬에 벡터를 곱하는 것은 해당 변환을 해당 벡터에 적용한다는 것이 계산상 의미하는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 방법으로 보면, 한 행렬은 하나의 선형변환을 나타냅니다. 그리고 벡터에 행렬을 곱하는 것은 수식적으로 그 벡터를 선형변환하는 것과 같습니다.",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff.",
  "translatedText": "좋습니다. 새로운 내용을 요약해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "됐습니다. 요약 끝. 새 주제로 넘어갑시다.",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes, you find yourself wanting to describe the effects of applying one transformation and then another.",
  "translatedText": "한 변환을 적용한 후 다른 변환을 적용할 때의 효과를 설명하고 싶은 경우가 종종 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "아마도 선형변환을 하고 나서 거기에 다시 선형변환을 하는 것도 설명하고 싶어하는 사람도 있을 것입니다.",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear.",
  "translatedText": "예를 들어, 먼저 평면을 시계 반대 방향으로 90도 회전한 다음 전단을 적용하면 어떤 일이 발생하는지 설명할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "예를 들면, 시계방향으로 90도 회전시키고, 그리고 나서 옆으로 밀면(shearing) 어떻게 되는 지를요.",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear.",
  "translatedText": "여기에서 처음부터 끝까지 전반적인 효과는 회전 및 전단과는 다른 또 다른 선형 변환입니다.",
  "model": "google_nmt",
  "from_community_srt": "전체 효과는 또 다른 하나의 선형변환이라는 것입니다. 회전과 밂과는 구별되는 무언가라는 것이죠.",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied.",
  "translatedText": "이 새로운 선형 변환은 일반적으로 우리가 적용한 두 가지 개별 변환의 구성이라고 합니다.",
  "model": "google_nmt",
  "from_community_srt": "이렇게 새로 생겨난 선형변환을 흔히 두 개의 선형변환의 합성이라고 일컫습니다.",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat.",
  "translatedText": "그리고 모든 선형 변환과 마찬가지로 i-hat과 j-hat을 따르면서 자체 행렬로 설명할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "다른 선형변환과 마찬가지로, 이 선형변환도 행렬로 표현이 가능합니다. 위와 같이 i-hat,",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix.",
  "translatedText": "이 예에서 두 변환 후 i-hat의 최종 착지 지점은 1,1이므로 이를 행렬의 첫 번째 열로 만들어 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "j-hat을 이용해서요. 여기서 i-hat 의 최종도착지는 (1,1)입니다. 그럼 이것을 행렬의 첫번째 열로 적으면 됩니다.",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix.",
  "translatedText": "마찬가지로 j-hat은 궁극적으로 음수 1,0 위치에서 끝나므로 이를 행렬의 두 번째 열로 만듭니다.",
  "model": "google_nmt",
  "from_community_srt": "마찬가지로, j-hat 의 최종도착지는 (-1, 0)입니다. 그럼 이걸 행렬의 두번째 열로 적으면 됩니다.",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones.",
  "translatedText": "이 새로운 매트릭스는 회전을 적용한 다음 전단을 적용하는 전체적인 효과를 포착합니다. 단, 두 개의 연속 동작이 아닌 하나의 단일 동작으로 수행됩니다.",
  "model": "google_nmt",
  "from_community_srt": "이 새 행렬이 바로 회전하고 미는 변환의 최종 효과을 나타냅니다. 하지만 연속되는 변환이 아니라,",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix.",
  "translatedText": "여기에 새로운 행렬에 대해 생각하는 한 가지 방법이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "단지 하나의 변환으로서만 표현하고 있죠. 새 행렬을 생각해보는 방법이 있습니다.",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix, then take whatever you get and multiply that on the left by the shear matrix.",
  "translatedText": "벡터를 가져와서 회전과 전단을 통해 펌핑하려는 경우 벡터가 끝나는 위치를 계산하는 긴 방법은 먼저 왼쪽에 회전 행렬을 곱한 다음 얻은 값을 가져와서 벡터에 곱하는 것입니다. 전단 매트릭스에 의해 남겨집니다.",
  "model": "google_nmt",
  "from_community_srt": "어떤 벡터를 가져다가 회전시키고 미는 변환을 시키면, 결과가 무엇인기 계산해야되는 이 긴 과정은 다음과 같습니다. 우선 회전을 나타내는 행렬에 벡터를 곱합니다. 그리고 그 결과를 미는 것을 나타내는 행렬 오른쪽에다 놓고 곱하면 이것이 수치적으로 표현된,",
  "n_reviews": 0,
  "start": 185.42,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector.",
  "translatedText": "이는 수치적으로 말하면 주어진 벡터에 회전을 적용한 다음 전단을 적용한다는 의미입니다.",
  "model": "google_nmt",
  "from_community_srt": "어떤 벡터에다가 회전 시키고 민 것을 적용한 뒤의 결과입니다.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action.",
  "translatedText": "그러나 무엇을 얻든, 어떤 벡터를 선택하든 동일한 벡터로 방금 찾은 이 새로운 구성 행렬을 적용하는 것과 동일해야 합니다. 왜냐하면 이 새로운 행렬은 회전 및 전단 동작과 동일한 전체 효과를 캡처해야 하기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "하지만 우리가 방금 구한 행렬을 곱해도  같은 결과값을 얻습니다. 어떤 벡터를 고르던지 말이죠. 우리가 구한 새 행렬은 회전하고 미는 것과 같은 효과를 나타내기 때문입니다.",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you?",
  "translatedText": "여기에 적힌 내용을 토대로 이 새로운 행렬을 원래 두 행렬의 곱이라고 부르는 것이 합리적이라고 생각합니다. 그렇지 않습니까?",
  "model": "google_nmt",
  "from_community_srt": "지금까지 설명한 것들을 기반으로 해서, 이 새 행렬을, 두 원본 행렬의 곱(product)이라고 불러도 될 것 같습니다.",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers.",
  "translatedText": "우리는 그 곱을 좀 더 일반적으로 계산하는 방법을 잠시 생각해 볼 수 있지만, 숫자의 숲에서 길을 잃기가 너무 쉽습니다.",
  "model": "google_nmt",
  "from_community_srt": "그렇지 않나요? 그럼 잠시, 행렬의 곱을 좀 더 일반적으로 계산하는 방법에 대해 생각해보겠습니다. 근데 숫자들 사이에서 헤매기 쉬우므로 조심하십시오.",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another.",
  "translatedText": "이와 같이 두 행렬을 곱하는 것은 하나의 변환을 적용한 다음 다른 변환을 적용한다는 기하학적 의미를 갖는다는 것을 항상 기억하십시오.",
  "model": "google_nmt",
  "from_community_srt": "항상 여러분들이 기억해야 할 것은, 두 행렬의 곱셈은 기하학적으로 한 변환을 적용하고나서 다른 변환을 적용한 것과 같다는 것입니다.",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left.",
  "translatedText": "여기서 좀 이상한 점 중 하나는 오른쪽에서 왼쪽으로 읽게 된다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "근데 이상한 점이 있는데, 읽을 때 오른쪽에서 왼쪽방향으로 봐야합니다.",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left.",
  "translatedText": "먼저 오른쪽 행렬로 표시되는 변환을 적용한 다음 왼쪽 행렬로 표시되는 변환을 적용합니다.",
  "model": "google_nmt",
  "from_community_srt": "우측의 행렬이 첫번째 변환을 의미하고, 좌측의 행렬로 그 다음 변환 적용을 나타내죠.",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left.",
  "translatedText": "이는 함수 표기법에서 유래합니다. 변수의 왼쪽에 함수를 작성하므로 두 함수를 작성할 때마다 항상 오른쪽에서 왼쪽으로 읽어야 합니다.",
  "model": "google_nmt",
  "from_community_srt": "이것은 함수 표기법에서 유래한 것입니다. 함수를 변수의 왼쪽에다 적기 때문이죠. 그래서 두 함수를 합성할 때마다, 오른쪽에서 왼쪽으로 읽어야 합니다.",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us.",
  "translatedText": "히브리어 독자들에게는 좋은 소식이고 나머지 우리에게는 나쁜 소식입니다.",
  "model": "google_nmt",
  "from_community_srt": "히브리어 독자들에게 좋은 소식일 테지만, 우리에게는 나쁜 소식입니다.",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example.",
  "translatedText": "또 다른 예를 살펴보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "다른 예를 살펴 보죠.",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.",
  "translatedText": "열 1,1과 음수 2,0이 있는 행렬을 가져오면 변환은 다음과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "행렬 (1, 1), (-2, 0) 이 있을때, 이 변환은 이렇게 보일텐데,",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it M1.",
  "translatedText": "그리고 그것을 M1이라고 부르자.",
  "model": "google_nmt",
  "from_community_srt": "이 변환을 M1 이라고 합시다.",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.",
  "translatedText": "다음으로, 0,1과 2,0 열이 있는 행렬을 가져옵니다. 그 변환은 다음과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "그 다음 행렬 (0, 1), (2, 0) 을  있을때, 이 변환은 이렇게 보일텐데,",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy M2.",
  "translatedText": "그리고 그 사람을 M2라고 부르자.",
  "model": "google_nmt",
  "from_community_srt": "이번엔 M2 라고 합시다.",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying M1 then M2 gives us a new transformation, so let's find its matrix.",
  "translatedText": "M1과 M2를 적용한 총 효과는 새로운 변환을 제공하므로 해당 행렬을 찾아보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "M1을 적용하고 나서 M2를 적용한 결과는 우리에게 새로운 변환을 나타내죠. 자, 이 행렬을 찾아봅시다.",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix.",
  "translatedText": "하지만 이번에는 애니메이션을 보지 않고 대신 각 행렬의 숫자 항목을 사용하여 이를 수행할 수 있는지 살펴보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "하지만 이번엔, 애니메이션을 보지 않고 찾아내봅시다. 대신에 각 행렬을 나타내는 수치만 가지고 찾아내봅시다.",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes.",
  "translatedText": "먼저, i-hat이 어디로 가는지 알아내야 합니다.",
  "model": "google_nmt",
  "from_community_srt": "우선 i-hat 벡터가 어떻게 되는지 부터 봅시다.",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying M1, the new coordinates of i-hat, by definition, are given by that first column of M1, namely 1,1.",
  "translatedText": "M1을 적용한 후 정의에 따라 i-hat의 새 좌표는 M1의 첫 번째 열, 즉 1,1에 의해 제공됩니다.",
  "model": "google_nmt",
  "from_community_srt": "M1 변환 적용 후 나타나는 새로운 i-hat 좌표는 정의에 의해, M1의 첫 번째 열에 나타나죠. 즉, (1,",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying M2, multiply the matrix for M2 by that vector 1,1.",
  "translatedText": "M2를 적용한 후 어떤 일이 발생하는지 확인하려면 M2의 행렬에 해당 벡터 1,1을 곱합니다.",
  "model": "google_nmt",
  "from_community_srt": "1) M2를 적용한 후를 살펴보려면, M2 행렬에 이 벡터 (1,1)를 곱하면 됩니다.",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1.",
  "translatedText": "이를 해결하면 지난 비디오에서 설명한 대로 벡터 2,1을 얻게 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "지난번에 설명한 방법으로 계산하면, 벡터 (2, 1) 라는 값을 얻을 수 있습니다.",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix.",
  "translatedText": "이는 컴포지션 매트릭스의 첫 번째 열이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "이 벡터가 합성행렬의 첫 번째 열입니다.",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of M1 tells us that it first lands on negative 2,0.",
  "translatedText": "마찬가지로 j-hat에 따르면 M1의 두 번째 열은 먼저 음수 2,0에 도달했음을 알려줍니다.",
  "model": "google_nmt",
  "from_community_srt": "마찬가지로, j-hat 에도 적용해보면, M1의 두 번째 열이 첫번째 변환 후인 좌표 (-2,",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply M2 to that vector, you can work out the matrix-vector product to get 0, negative 2, which becomes the second column of our composition matrix.",
  "translatedText": "그런 다음 해당 벡터에 M2를 적용하면 행렬-벡터 곱을 계산하여 0, 음수 2를 얻을 수 있으며 이는 구성 행렬의 두 번째 열이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "0)가 되고 다음, M2 행렬을 이 벡터에다 곱하면, 행렬-벡터 곱으로 계산해서 (0, -2)이라는 값을 얻을 수 있습니다. 이것이 합성행렬의 두번재 열입니다.",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices.",
  "translatedText": "동일한 과정을 다시 설명하겠습니다. 하지만 이번에는 동일한 추론 방식이 모든 행렬에 적용된다는 것을 보여주기 위해 각 행렬의 변수 항목을 표시하겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "다시 한 번 설명드리겠습니다. 하지만 이번에는 각 행렬 안에 숫자를 변수로 대체해서 보여드리겠습니다. 아까 전과 같이 진행할 것이라서, 기호가 더 많고,",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way.",
  "translatedText": "이는 기호가 더 많고 더 많은 공간이 필요하지만 이전에 더 암기적인 방식으로 행렬 곱셈을 배운 사람에게는 꽤 만족스러울 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "쓸 공간도 더 많이 필요하겠죠. 하지만 이전에 행렬 곱셈을 요령으로 배웠던 사람이라면 꽤 만족할 것입니다.",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands.",
  "translatedText": "i-hat이 어디로 가는지 따라가려면 먼저 오른쪽 행렬의 첫 번째 열을 살펴보세요. i-hat이 처음에 도착하는 곳이 바로 이곳이기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "i-hat 벡터가 어떻게 되는 지부터 봅시다. 오른쪽 행렬의 첫째 열부터 보죠. 이것은 첫번째 변환 후의 i-hat의 위치입니다.",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation.",
  "translatedText": "해당 열에 왼쪽 행렬을 곱하면 두 번째 변환을 적용한 후 i-hat의 중간 버전이 끝나는 위치를 알 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 열을 왼쪽 행렬에다가 곱하면, 한 번 변환을 거친 i-hat이 최종적으로 어디에 도달하는지 알 수 있습니다.",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix.",
  "translatedText": "따라서 구성 행렬의 첫 번째 열은 항상 왼쪽 행렬과 오른쪽 행렬의 첫 번째 열을 곱한 것과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서 합성행렬의 첫번째 열은 항상 왼쪽 행렬과 오른쪽 행렬의 첫째열의 곱셈과 같습니다.",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix.",
  "translatedText": "마찬가지로, j-hat은 항상 처음에 오른쪽 행렬의 두 번째 열에 착륙합니다.",
  "model": "google_nmt",
  "from_community_srt": "마찬가지로,",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix.",
  "translatedText": "따라서 왼쪽 행렬에 이 두 번째 열을 곱하면 최종 위치가 제공되므로 이것이 구성 행렬의 두 번째 열이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "j-hat 은 오른쪽 행렬의 두번째 열의 값을 거쳐 왼쪽 행렬을 곱하여서 최종 위치가 나옵니다. 따라서, 이 값이 합성 행렬의 두번째 열입니다.",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to help remember it.",
  "translatedText": "여기에는 많은 기호가 있으며, 이 공식을 기억하는 데 도움이 되는 특정 알고리즘 프로세스와 함께 외워야 할 것으로 배우는 것이 일반적입니다.",
  "model": "google_nmt",
  "from_community_srt": "주의할 점은, 기호가 너무 많기 때문에 이것을 기억하기 위해서 어떤 요령으로 많이들 가르치죠. 특정 알고리즘 순서로서 기억하기 쉽게 말이요.",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another.",
  "translatedText": "하지만 저는 그 과정을 기억하기 전에 행렬 곱셈이 실제로 무엇을 나타내는지 생각하고 변환을 하나씩 적용하는 습관을 들여야 한다고 생각합니다.",
  "model": "google_nmt",
  "from_community_srt": "하지만 그렇게 암기하기 전에 꼭, 행렬 곱셈이 무엇을 나타낸는지 생각해보는 습관을 가졌으면 좋겠습니다. 그것은 한 변환을 적용한 후, 다른 변환을 적용한다는 것.",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand.",
  "translatedText": "저를 믿으십시오. 이것은 행렬 곱셈의 속성을 훨씬 더 쉽게 이해할 수 있게 해주는 훨씬 더 나은 개념적 프레임워크를 제공할 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "방금 제가 설명드린 것들은 여러분들이  행렬의 곱셈에 대해 더 쉽게 이해할 수 있도록 어떤 틀을 제공해 줄것입니다.",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question.",
  "translatedText": "예를 들어, 여기에 질문이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "예를들어, 다음과 같은 질문이 있습니다.",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them?",
  "translatedText": "두 행렬을 곱할 때 두 행렬을 어떤 순서로 넣는 것이 중요한가요?",
  "model": "google_nmt",
  "from_community_srt": "우리가 두 행렬을 곱할 때 그 두 행렬을 곱하는 순서가 상관있을까요? 간단한 예제를 통해 알아봅시다.",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier.",
  "translatedText": "자, 앞서 말한 것과 같은 간단한 예를 생각해 봅시다.",
  "model": "google_nmt",
  "from_community_srt": "앞에서 했었던 방법으로 말이죠.",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smooshes j-hat over to the right, and a 90 degree rotation.",
  "translatedText": "i-hat을 고정하고 j-hat을 오른쪽으로 밀어내는 가위를 사용하고 90도 회전합니다.",
  "model": "google_nmt",
  "from_community_srt": "i-hat 은 고정이고 j-hat 만 오른쪽으로 밀어지는 미는 (shear) 변환과 90 ° 회전 변환을 이용해서요.",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1.",
  "translatedText": "먼저 전단을 수행한 다음 회전하면 i-hat이 0,1로 끝나고 j-hat이 -1,1로 끝나는 것을 볼 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "만약 먼저 민 후 회전시키면, i-hat 의 결과는 (0, 1)이고 j-hat 의 결과는 (-1, 1) 가 된다는 것을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together.",
  "translatedText": "둘 다 일반적으로 서로 가깝게 가리키고 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "두 벡터가 서로 가까이 위치하고 있네요.",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing, you know, farther apart.",
  "translatedText": "먼저 회전한 다음 전단을 수행하면 i-hat은 1,1에서 끝나고 j-hat은 -1,0에서 다른 방향으로 벗어나서 더 멀리 떨어져 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이번엔 먼저 회전을 하고 나서 밀면, i-hat 은 (1, 1) j-hat 은 (-1, 0) 위치가 된다는 것을 알 수 있습니다. 이 둘은 보시다시피 서로 멀리 떨어져 있네요.",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently, order totally does matter.",
  "translatedText": "여기서 전체적인 효과는 분명히 다르므로 순서가 전적으로 중요합니다.",
  "model": "google_nmt",
  "from_community_srt": "곱하는 순서에 따라 결과가 달라지기 때문에 결론은 곱하는 순서가 중요합니다.",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice, by thinking in terms of transformations, that's the kind of thing that you can do in your head by visualizing.",
  "translatedText": "변형의 관점에서 생각함으로써 시각화를 통해 머리 속에서 할 수 있는 일이 있다는 점에 주목하세요.",
  "model": "google_nmt",
  "from_community_srt": "주목할 점은 변환에 대해서 떠올릴 때 변환은 시각화함으로써 머릿속으로 떠올릴 수 있는 것들이라는 것이죠.",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary.",
  "translatedText": "행렬 곱셈이 필요하지 않습니다.",
  "model": "google_nmt",
  "from_community_srt": "행렬 곱셈하는 것 그 자체는 중요하지 않아요.",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative.",
  "translatedText": "제가 처음 선형대수학을 수강했을 때 행렬 곱셈이 결합적이라는 것을 증명하라는 숙제가 있었습니다.",
  "model": "google_nmt",
  "from_community_srt": "제가 선형대수학을 처음 접했을 때가 기억나는 데, 행렬 곱셈의 결합법칙(Asoociativity)에 대해 증명하라는 숙제가 있었습니다.",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left.",
  "translatedText": "이는 A, B, C라는 세 개의 행렬이 있고 이를 모두 곱하는 경우 먼저 A 곱하기 B를 계산한 다음 그 결과에 C를 곱하거나 먼저 B를 곱해도 문제가 되지 않음을 의미합니다. C를 구하고 그 결과에 왼쪽의 A를 곱합니다.",
  "model": "google_nmt",
  "from_community_srt": "결합법칙은 행렬 A, B, C가 있어서 이것을 모두 곱할 때, AB 먼저 곱하고 나서 오른쪽에 C를 곱하거나 BC 먼저 곱하고나서 왼쪽에  A를 곱하거나 그 순서는 상관이 없다는 것을 의미합니다.",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses.",
  "translatedText": "즉, 괄호를 어디에 넣는지는 중요하지 않습니다.",
  "model": "google_nmt",
  "from_community_srt": "다시 말해서,",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter.",
  "translatedText": "자, 만약 제가 그때 그랬던 것처럼 이것을 수치적으로 해결하려고 한다면, 그것은 끔찍하고, 끔찍하고, 그 문제에 있어서는 깨달음이 없습니다.",
  "model": "google_nmt",
  "from_community_srt": "괄호를 어디다 집어넣든 크게 상관이 없다는 거죠 여러분들이 결합법칙을 수치적으로 제가 방금 아까 했던 것처럼 증명하려 한다면 그 방법은 아주 끔찍하고 결합법칙이 무엇인지 깨우치는 데 좋지 못합니다.",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial.",
  "translatedText": "그러나 행렬 곱셈을 하나의 변환을 차례로 적용하는 것으로 생각하면 이 속성은 아주 사소한 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "하지만 행렬의 곱셈을 한 변환을 적용하고 나서 다시 다른 변환을 적용하는 것이라고 생각한다면, 이 문제는 정말 간단합니다.",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why?",
  "translatedText": "이유를 알 수 있나요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C, then B, then A, it's the same as applying C, then B, then A.",
  "translatedText": "즉, C를 먼저 적용한 다음 B를 적용하고 A를 적용하면 C를 적용한 다음 B를 적용하고 A를 적용하는 것과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "왜 그런지 이해가 되나요? 결합법칙이 나타내는 것은 CB, A 순서로 적용하는 것이 C, BA 순서로 적용하는 것과 같다는 겁니다.",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove.",
  "translatedText": "즉, 증명할 것이 아무것도 없습니다.",
  "model": "google_nmt",
  "from_community_srt": "더이상 증명할 게 없어요.",
  "n_reviews": 0,
  "start": 552.82,
  "end": 554.38
 },
 {
  "input": "You're just applying the same three things one after the other, all in the same order.",
  "translatedText": "동일한 세 가지 항목을 모두 동일한 순서로 하나씩 적용하면 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "그냥 동일한 세 변환을 같은 순서대로 적용하는 것에 불과합니다.",
  "n_reviews": 0,
  "start": 554.54,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not.",
  "translatedText": "부정행위처럼 느껴질 수도 있지만 그렇지 않습니다.",
  "model": "google_nmt",
  "from_community_srt": "꼼수처럼 느껴질 지도 모르지만 전혀 그렇지 않아요.",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative, and even better than that, it's a good explanation for why that property should be true.",
  "translatedText": "이는 행렬 곱셈이 결합적이라는 사실을 증명하는 것이며, 그보다 더 좋은 점은 해당 속성이 왜 참이어야 하는지에 대한 좋은 설명입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 방법은 행렬 곱셈이 결합법칙이 성립된다는 것을 증명하는 아주 좋은 방법입니다. 게다가 이렇게 하면 왜 결합법칙이 참인지 아주 잘 설명해주죠.",
  "n_reviews": 0,
  "start": 561.54,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically.",
  "translatedText": "두 가지 다른 변환을 상상하고 하나씩 적용하면 어떤 일이 일어나는지 생각한 다음 행렬 곱을 수치적으로 계산하면서 이 아이디어를 더 많이 시도해 보시기 바랍니다.",
  "model": "google_nmt",
  "from_community_srt": "저는 여러분들이 이 아이디어를 잘 활용하시기를 바랍니다. 서로 다른 두 개의 변환을 떠올려서 한 변환을 적용한 후 다른 변환을 적용하는 것과 행렬 곱셈을 수치적으로 하는 것에 대해 한 번 궁구해보십시오.",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in.",
  "translatedText": "저를 믿으십시오. 이것은 정말 아이디어를 깊이있게 만드는 일종의 놀이 시간입니다.",
  "model": "google_nmt",
  "from_community_srt": "이렇게 함으로써 행렬 곱셈에 대해 정말로 깊게 이해할 수 있게 될 것입니다.",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions.",
  "translatedText": "다음 비디오에서는 이러한 아이디어를 2차원 이상으로 확장하는 방법에 대해 이야기하겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "다음 동영상에서는 이 아이디어를 2차원 이상으로 확장해보겠습니다.",
  "n_reviews": 0,
  "start": 587.2,
  "end": 592.18
 }
]