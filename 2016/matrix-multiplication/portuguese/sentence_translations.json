[
 {
  "input": "Hey everyone, where we last left off, I showed what linear transformations look like and how to represent them using matrices.",
  "translatedText": "Olá a todos, de onde paramos, mostrei como são as transformações lineares e como representá-las usando matrizes.",
  "model": "google_nmt",
  "from_community_srt": "\"Em minha experiência, provas envolvendo matrizes podem ser diminuídas em 50% se jogarmos as matrizes fora\". (Emil Artin) Olá todo mundo! No lugar onde paramos da última vez, eu mostrei com o que as transformações lineares se parecem e como representá-las usando matrizes.",
  "n_reviews": 0,
  "start": 10.94,
  "end": 16.88
 },
 {
  "input": "This is worth a quick recap because it's just really important, but of course if this feels like more than just a recap, go back and watch the full video.",
  "translatedText": "Vale a pena recapitular rapidamente porque é muito importante, mas é claro que se parecer mais do que apenas uma recapitulação, volte e assista ao vídeo completo.",
  "model": "google_nmt",
  "from_community_srt": "Isso vale uma rápida revisão, porque é simplesmente muito importante. Mas claro, se isto parecer mais do que só uma revisão, volte atrás e assista o vídeo inteiro.",
  "n_reviews": 0,
  "start": 18.32,
  "end": 25.14
 },
 {
  "input": "Technically speaking, linear transformations are functions with vectors as inputs and vectors as outputs, but I showed last time how we can think about them visually as smooshing around space in such a way that grid lines stay parallel and evenly spaced, and so that the origin remains fixed.",
  "translatedText": "Tecnicamente falando, as transformações lineares são funções com vetores como entradas e vetores como saídas, mas mostrei da última vez como podemos pensar nelas visualmente como se deslizando pelo espaço de tal forma que as linhas da grade permaneçam paralelas e uniformemente espaçadas, e para que a origem permanece fixo.",
  "model": "google_nmt",
  "from_community_srt": "Falando tecnicamente, transformações lineares são funções, onde entram vetores e saem vetores. Porém, eu mostrei na última vez que nós podemos pensar sobre a visualização como remodelação do espaço, de tal maneira que as grades de linhas permaneçam paralelas e igualmente espaçadas, e que a origem permaneça fixada.",
  "n_reviews": 0,
  "start": 25.78,
  "end": 41.18
 },
 {
  "input": "The key takeaway was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means i-hat and j-hat.",
  "translatedText": "A principal conclusão foi que uma transformação linear é completamente determinada por onde ela leva os vetores de base do espaço, o que para duas dimensões significa i-hat e j-hat.",
  "model": "google_nmt",
  "from_community_srt": "O ponto chave dado foi que uma transformação linear é completamente determinada, pela movimentação dos vetores básicos que, para duas dimensões,",
  "n_reviews": 0,
  "start": 41.82,
  "end": 51.34
 },
 {
  "input": "This is because any other vector could be described as a linear combination of those basis vectors.",
  "translatedText": "Isto ocorre porque qualquer outro vetor poderia ser descrito como uma combinação linear desses vetores de base.",
  "model": "google_nmt",
  "from_community_srt": "significa î e ĵ. Isto é porque qualquer outro vetor pode ser descrito como uma combinação linear destes vetores básicos.",
  "n_reviews": 0,
  "start": 51.34,
  "end": 57.34
 },
 {
  "input": "A vector with coordinates x, y is x times i-hat plus y times j-hat.",
  "translatedText": "Um vetor com coordenadas x, y é x vezes i-hat mais y vezes j-hat.",
  "model": "google_nmt",
  "from_community_srt": "Um vetor com coordenadas (x,y) é x·î+y·ĵ.",
  "n_reviews": 0,
  "start": 57.94,
  "end": 62.34
 },
 {
  "input": "After going through the transformation, this property that grid lines remain parallel and evenly spaced has a wonderful consequence.",
  "translatedText": "Depois de passar pela transformação, essa propriedade de que as linhas da grade permanecem paralelas e espaçadas uniformemente tem uma consequência maravilhosa.",
  "model": "google_nmt",
  "from_community_srt": "Depois de percorrer a transformação essa propriedade, que as grades de linhas permaneçam paralelas e igualmente espaçadas, possui uma maravilhosa consequência.",
  "n_reviews": 0,
  "start": 63.46,
  "end": 69.86
 },
 {
  "input": "The place where your vector lands will be x times the transformed version of i-hat plus y times the transformed version of j-hat.",
  "translatedText": "O local onde seu vetor pousar será x vezes a versão transformada de i-hat mais y vezes a versão transformada de j-hat.",
  "model": "google_nmt",
  "from_community_srt": "O local onde seu vetor estacionará será x vezes a versão transformada de î mais y vezes a versão transformada de ĵ.",
  "n_reviews": 0,
  "start": 70.5,
  "end": 77.56
 },
 {
  "input": "This means if you keep a record of the coordinates where i-hat lands and the coordinates where j-hat lands, you can compute that a vector which starts at x, y must land on x times the new coordinates of i-hat plus y times the new coordinates of j-hat.",
  "translatedText": "Isso significa que se você mantiver um registro das coordenadas onde i-hat pousa e das coordenadas onde j-hat pousa, você pode calcular que um vetor que começa em x, y deve pousar em x vezes as novas coordenadas de i-hat mais y vezes as novas coordenadas de j-hat.",
  "model": "google_nmt",
  "from_community_srt": "Isso significa que se você manter um registro das coordenadas onde î parou e as coordenadas onde ĵ parou você pode computar isto como um vetor que começa em (x,y) mas estaciona em x vezes as novas coordenadas de î mais y vezes as novas coordenadas de ĵ.",
  "n_reviews": 0,
  "start": 78.24,
  "end": 92.72
 },
 {
  "input": "The convention is to record the coordinates of where i-hat and j-hat land as the columns of a matrix, and to define this sum of the scaled versions of those columns by x and y to be matrix-vector multiplication.",
  "translatedText": "A convenção é registrar as coordenadas de onde i-hat e j-hat pousam como as colunas de uma matriz e definir esta soma das versões em escala dessas colunas por x e y como multiplicação de vetor de matriz.",
  "model": "google_nmt",
  "from_community_srt": "A convenção é de recordar as coordenadas onde î e ĵ estacionam como as colunas de uma matrizes e para definir isso como a soma das versões escalares destas colunas por x e y para uma multiplicação matriz-vetor.",
  "n_reviews": 0,
  "start": 93.56,
  "end": 105.36
 },
 {
  "input": "In this way, a matrix represents a specific linear transformation, and multiplying a matrix by a vector is what it means computationally to apply that transformation to that vector.",
  "translatedText": "Desta forma, uma matriz representa uma transformação linear específica, e multiplicar uma matriz por um vetor é o que significa computacionalmente aplicar essa transformação a esse vetor.",
  "model": "google_nmt",
  "from_community_srt": "Desta maneira, uma matriz representa uma transformação linear especifica e multiplicar uma matriz por um vetor é, o que significa computacionalmente, aplicar esta transformação linear para este vetor.",
  "n_reviews": 0,
  "start": 106.05,
  "end": 117.08
 },
 {
  "input": "Alright, recap over, on to the new stuff.",
  "translatedText": "Tudo bem, recapitulando, vamos às novidades.",
  "model": "google_nmt",
  "from_community_srt": "Tudo bem, revisão terminada. Pronto para  os novos assuntos.",
  "n_reviews": 0,
  "start": 118.8,
  "end": 120.88
 },
 {
  "input": "Oftentimes, you find yourself wanting to describe the effects of applying one transformation and then another.",
  "translatedText": "Muitas vezes, você deseja descrever os efeitos da aplicação de uma transformação e depois de outra.",
  "model": "google_nmt",
  "from_community_srt": "De vez em quando você se encontra querendo descrever o efeito da aplicação de uma transformação e depois outra .",
  "n_reviews": 0,
  "start": 121.6,
  "end": 127.0
 },
 {
  "input": "For example, maybe you want to describe what happens when you first rotate the plane 90 degrees counterclockwise, then apply a shear.",
  "translatedText": "Por exemplo, talvez você queira descrever o que acontece quando você primeiro gira o plano 90 graus no sentido anti-horário e depois aplica um cisalhamento.",
  "model": "google_nmt",
  "from_community_srt": "Por exemplo, Talvez você queira descrever o que acontece quando você primeiramente rotaciona o plano em 90º graus no sentido anti-horário e então, aplica um cisalhamento.",
  "n_reviews": 0,
  "start": 127.62,
  "end": 134.48
 },
 {
  "input": "The overall effect here, from start to finish, is another linear transformation, distinct from the rotation and the shear.",
  "translatedText": "O efeito geral aqui, do início ao fim, é outra transformação linear, distinta da rotação e do cisalhamento.",
  "model": "google_nmt",
  "from_community_srt": "O efeito total aqui, do começo ao fim, é uma outra transformação linear, distinta da rotação e do cisalhamento.",
  "n_reviews": 0,
  "start": 135.26,
  "end": 141.8
 },
 {
  "input": "This new linear transformation is commonly called the composition of the two separate transformations we applied.",
  "translatedText": "Esta nova transformação linear é comumente chamada de composição das duas transformações separadas que aplicamos.",
  "model": "google_nmt",
  "from_community_srt": "Esta nova transformação é comumente chamada como a  \"composição\" das duas transformações separadas que aplicamos",
  "n_reviews": 0,
  "start": 142.28,
  "end": 148.22
 },
 {
  "input": "And like any linear transformation, it can be described with a matrix all of its own by following i-hat and j-hat.",
  "translatedText": "E como qualquer transformação linear, ela pode ser descrita com uma matriz própria seguindo i-hat e j-hat.",
  "model": "google_nmt",
  "from_community_srt": "E como qualquer transformação linear Pode ser completamente descrita por uma matriz, acompanhando î e ĵ.",
  "n_reviews": 0,
  "start": 148.92,
  "end": 155.44
 },
 {
  "input": "In this example, the ultimate landing spot for i-hat after both transformations is 1,1, so let's make that the first column of a matrix.",
  "translatedText": "Neste exemplo, o ponto de aterrissagem final para i-hat após ambas as transformações é 1,1, então vamos fazer disso a primeira coluna de uma matriz.",
  "model": "google_nmt",
  "from_community_srt": "Neste exemplo, o último ponto de parada para o î depois das duas transformações é (1,1) Então vamos fazer disto a primeira coluna da matriz",
  "n_reviews": 0,
  "start": 156.02,
  "end": 164.12
 },
 {
  "input": "Likewise, j-hat ultimately ends up at the location negative 1,0, so we make that the second column of the matrix.",
  "translatedText": "Da mesma forma, j-hat acaba no local menos 1,0, então fazemos disso a segunda coluna da matriz.",
  "model": "google_nmt",
  "from_community_srt": "analogamente, ĵ termina parando nas coordenadas (-1,0) Logo, nós fazemos destas a segunda coluna da matriz.",
  "n_reviews": 0,
  "start": 164.96,
  "end": 171.86
 },
 {
  "input": "This new matrix captures the overall effect of applying a rotation then a shear, but as one single action, rather than two successive ones.",
  "translatedText": "Esta nova matriz captura o efeito geral da aplicação de uma rotação e depois de um cisalhamento, mas como uma única ação, em vez de duas ações sucessivas.",
  "model": "google_nmt",
  "from_community_srt": "Esta nova matriz captura o efeito total da aplicação da rotação e depois do cisalhamento porém em um único movimento,",
  "n_reviews": 0,
  "start": 172.68,
  "end": 181.34
 },
 {
  "input": "Here's one way to think about that new matrix.",
  "translatedText": "Aqui está uma maneira de pensar sobre essa nova matriz.",
  "model": "google_nmt",
  "from_community_srt": "ao invés de dois sucessivos.",
  "n_reviews": 0,
  "start": 183.04,
  "end": 184.88
 },
 {
  "input": "If you were to take some vector and pump it through the rotation, then the shear, the long way to compute where it ends up is to first multiply it on the left by the rotation matrix, then take whatever you get and multiply that on the left by the shear matrix.",
  "translatedText": "Se você pegar algum vetor e bombeá-lo através da rotação, então o cisalhamento, o longo caminho para calcular onde ele vai parar é primeiro multiplicá-lo à esquerda pela matriz de rotação, então pegar o que você conseguir e multiplicar isso no deixado pela matriz de cisalhamento.",
  "model": "google_nmt",
  "from_community_srt": "Aqui está uma maneira de pensar sobre esta nova matriz: se você fosse pegar um vetor pra fazê-lo passar por uma rotação e depois por um cisalhamento, a maneira mais longa de calcular onde ele vai parar é primeiro multiplicar na esquerda pela matriz de rotação, então pegar o resultado que você obter, e multiplicar ele pela esquerda pela matriz de cisalhamento.",
  "n_reviews": 0,
  "start": 185.42,
  "end": 199.8
 },
 {
  "input": "This is, numerically speaking, what it means to apply a rotation then a shear to a given vector.",
  "translatedText": "Isto é, numericamente falando, o que significa aplicar uma rotação e depois um cisalhamento a um determinado vetor.",
  "model": "google_nmt",
  "from_community_srt": "Isso é, numericamente falando, o que significa aplicar uma rotação e então um cisalhamento a um vetor.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 206.06
 },
 {
  "input": "But whatever you get should be the same as just applying this new composition matrix that we just found by that same vector, no matter what vector you chose, since this new matrix is supposed to capture the same overall effect as the rotation then shear action.",
  "translatedText": "Mas o que você obtiver deve ser o mesmo que aplicar esta nova matriz de composição que acabamos de encontrar pelo mesmo vetor, não importa qual vetor você escolheu, já que esta nova matriz deve capturar o mesmo efeito geral da rotação e depois da ação de cisalhamento.",
  "model": "google_nmt",
  "from_community_srt": "Mas o que você obter deve ser o mesmo que aplicar essa nova matriz composta que acabamos de encontrar no mesmo vetor. Não importa o vetor que você escolher, já que esta nova matriz supostamente captura o mesmo efeito geral da rotação seguida do cisalhamento.",
  "n_reviews": 0,
  "start": 206.8,
  "end": 220.98
 },
 {
  "input": "Based on how things are written down here, I think it's reasonable to call this new matrix the product of the original two matrices, don't you?",
  "translatedText": "Com base em como as coisas estão escritas aqui, acho razoável chamar essa nova matriz de produto das duas matrizes originais, não é?",
  "model": "google_nmt",
  "from_community_srt": "Baseado em como as coisas foram escritas aqui, acho que é razoável chamar essa nova matriz como sendo o produto das duas matrizes originais,",
  "n_reviews": 0,
  "start": 222.48,
  "end": 229.38
 },
 {
  "input": "We can think about how to compute that product more generally in just a moment, but it's way too easy to get lost in the forest of numbers.",
  "translatedText": "Podemos pensar em como calcular esse produto de forma mais geral em apenas um momento, mas é muito fácil nos perdermos na floresta de números.",
  "model": "google_nmt",
  "from_community_srt": "não acha? Podemos pensar em como calcular esse produto de maneira mais geral num momento, mas é muito fácil se perder no monte de números.",
  "n_reviews": 0,
  "start": 230.42,
  "end": 236.6
 },
 {
  "input": "Always remember that multiplying two matrices like this has the geometric meaning of applying one transformation then another.",
  "translatedText": "Lembre-se sempre de que multiplicar duas matrizes como esta tem o significado geométrico de aplicar uma transformação depois de outra.",
  "model": "google_nmt",
  "from_community_srt": "Lembre sempre que multiplicar duas matrizes assim tem o significado geométrico de aplicar uma transformação,",
  "n_reviews": 0,
  "start": 236.6,
  "end": 244.28
 },
 {
  "input": "One thing that's kind of weird here is that this has us reading from right to left.",
  "translatedText": "Uma coisa meio estranha aqui é que isso nos faz ler da direita para a esquerda.",
  "model": "google_nmt",
  "from_community_srt": "e depois outra. Uma coisa que é estranha aqui,",
  "n_reviews": 0,
  "start": 245.86,
  "end": 249.66
 },
 {
  "input": "You first apply the transformation represented by the matrix on the right, then you apply the transformation represented by the matrix on the left.",
  "translatedText": "Primeiro você aplica a transformação representada pela matriz à direita e depois aplica a transformação representada pela matriz à esquerda.",
  "model": "google_nmt",
  "from_community_srt": "é que isso se lê da direita para a esquerda, você primeiro aplica a transformação representada pela matriz na direita, então você aplica a transformação representada pela matriz na esquerda.",
  "n_reviews": 0,
  "start": 250.04,
  "end": 256.72
 },
 {
  "input": "This stems from function notation, since we write functions on the left of variables, so every time you compose two functions, you always have to read it right to left.",
  "translatedText": "Isso decorre da notação de função, já que escrevemos funções à esquerda das variáveis, então toda vez que você compõe duas funções, você sempre terá que lê-las da direita para a esquerda.",
  "model": "google_nmt",
  "from_community_srt": "Isso vem da notação de funções, já que escrevemos funções à esquerda das variáveis, Então toda vez que você compõe duas funções, você sempre tem que ler da direita para a esquerda.",
  "n_reviews": 0,
  "start": 257.4,
  "end": 265.46
 },
 {
  "input": "Good news for the Hebrew readers, bad news for the rest of us.",
  "translatedText": "Boas notícias para os leitores de hebraico, más notícias para o resto de nós.",
  "model": "google_nmt",
  "from_community_srt": "Boas notícias para os leitores de Hebreu, más notícias para o resto de nós.",
  "n_reviews": 0,
  "start": 265.92,
  "end": 268.98
 },
 {
  "input": "Let's look at another example.",
  "translatedText": "Vejamos outro exemplo.",
  "model": "google_nmt",
  "from_community_srt": "Vamos ver outro exemplo.",
  "n_reviews": 0,
  "start": 269.88,
  "end": 271.1
 },
 {
  "input": "Take the matrix with columns 1,1 and negative 2,0, whose transformation looks like this.",
  "translatedText": "Pegue a matriz com colunas 1,1 e menos 2,0, cuja transformação se parece com esta.",
  "model": "google_nmt",
  "from_community_srt": "Pegue a matriz com as colunas (1,1) e (-2,0) cuja transformação se parece com isso, e vamos chamá-la de M1.",
  "n_reviews": 0,
  "start": 271.76,
  "end": 276.86
 },
 {
  "input": "And let's call it M1.",
  "translatedText": "E vamos chamá-lo de M1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.98,
  "end": 279.06
 },
 {
  "input": "Next, take the matrix with columns 0,1 and 2,0, whose transformation looks like this.",
  "translatedText": "A seguir, pegue a matriz com colunas 0,1 e 2,0, cuja transformação se parece com esta.",
  "model": "google_nmt",
  "from_community_srt": "Depois, pegue a matriz com colunas (0,1) e (2,0) cuja transformação se parece com isso, e vamos chamar esse cara de M2.",
  "n_reviews": 0,
  "start": 280.1,
  "end": 285.7
 },
 {
  "input": "And let's call that guy M2.",
  "translatedText": "E vamos chamar aquele cara de M2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.52,
  "end": 289.24
 },
 {
  "input": "The total effect of applying M1 then M2 gives us a new transformation, so let's find its matrix.",
  "translatedText": "O efeito total da aplicação de M1 e M2 nos dá uma nova transformação, então vamos encontrar sua matriz.",
  "model": "google_nmt",
  "from_community_srt": "O efeito total de aplicar M1 e depois M2 nos dá uma nova transformação, então vamos achar essa matriz.",
  "n_reviews": 0,
  "start": 289.92,
  "end": 295.68
 },
 {
  "input": "But this time, let's see if we can do it without watching the animations, and instead just using the numerical entries in each matrix.",
  "translatedText": "Mas desta vez, vamos ver se conseguimos fazer isso sem assistir às animações e, em vez disso, usando apenas as entradas numéricas em cada matriz.",
  "model": "google_nmt",
  "from_community_srt": "Mas dessa vez, vamos ver se conseguimos fazer sem olhar as animações, as invés disso, vamos usar somente as entradas numéricas de cada matriz.",
  "n_reviews": 0,
  "start": 296.28,
  "end": 303.86
 },
 {
  "input": "First, we need to figure out where i-hat goes.",
  "translatedText": "Primeiro, precisamos descobrir para onde vai o i-hat.",
  "model": "google_nmt",
  "from_community_srt": "Primeiro, devemos descobrir onde î vai.",
  "n_reviews": 0,
  "start": 304.74,
  "end": 307.14
 },
 {
  "input": "After applying M1, the new coordinates of i-hat, by definition, are given by that first column of M1, namely 1,1.",
  "translatedText": "Depois de aplicar M1, as novas coordenadas de i-hat, por definição, são dadas por aquela primeira coluna de M1, nomeadamente 1,1.",
  "model": "google_nmt",
  "from_community_srt": "Após aplicarmos M1, as novas coordenadas de î por definição, são dadas pela primeira coluna de M1, no caso,",
  "n_reviews": 0,
  "start": 308.04,
  "end": 315.98
 },
 {
  "input": "To see what happens after applying M2, multiply the matrix for M2 by that vector 1,1.",
  "translatedText": "Para ver o que acontece após aplicar M2, multiplique a matriz de M2 por aquele vetor 1,1.",
  "model": "google_nmt",
  "from_community_srt": "(1,1). Para ver o que acontece depois de aplicarmos M2, multiplique essa matriz M2 pelo vetor (1,1).",
  "n_reviews": 0,
  "start": 316.78,
  "end": 323.5
 },
 {
  "input": "Working it out, the way I described last video, you'll get the vector 2,1.",
  "translatedText": "Resolvendo como descrevi no último vídeo, você obterá o vetor 2,1.",
  "model": "google_nmt",
  "from_community_srt": "Calculando do jeito que eu descrevi no último vídeo, você obtém o vetor (2,1).",
  "n_reviews": 0,
  "start": 325.3,
  "end": 329.88
 },
 {
  "input": "This will be the first column of the composition matrix.",
  "translatedText": "Esta será a primeira coluna da matriz de composição.",
  "model": "google_nmt",
  "from_community_srt": "Este será a primeira coluna da matriz composta.",
  "n_reviews": 0,
  "start": 330.7,
  "end": 333.1
 },
 {
  "input": "Likewise, to follow j-hat, the second column of M1 tells us that it first lands on negative 2,0.",
  "translatedText": "Da mesma forma, para seguir o J-hat, a segunda coluna de M1 nos diz que primeiro chega a menos 2,0.",
  "model": "google_nmt",
  "from_community_srt": "Da mesmo maneira, para seguir ĵ, a segunda coluna de M1 nos diz que ele para no vetor (-2,0).",
  "n_reviews": 0,
  "start": 334.52,
  "end": 340.54
 },
 {
  "input": "Then, when we apply M2 to that vector, you can work out the matrix-vector product to get 0, negative 2, which becomes the second column of our composition matrix.",
  "translatedText": "Então, quando aplicamos M2 a esse vetor, você pode calcular o produto matriz-vetor para obter 0, menos 2, que se torna a segunda coluna da nossa matriz de composição.",
  "model": "google_nmt",
  "from_community_srt": "Então, quando aplicamos M2 neste vetor, você pode calcular o produto da matriz com o vetor para obter (0,-2), que se torna a segunda coluna da nossa matriz composta.",
  "n_reviews": 0,
  "start": 342.7,
  "end": 355.2
 },
 {
  "input": "Let me talk through that same process again, but this time I'll show variable entries in each matrix, just to show that the same line of reasoning works for any matrices.",
  "translatedText": "Deixe-me falar sobre o mesmo processo novamente, mas desta vez mostrarei entradas de variáveis em cada matriz, apenas para mostrar que a mesma linha de raciocínio funciona para qualquer matriz.",
  "model": "google_nmt",
  "from_community_srt": "Deixe-me ir por esse processo de novo, mas dessa vez, eu mostrarei variáveis nas entradas das matrizes, apenas para mostrar que a mesma linha de pensamento funciona para qualquer matriz.",
  "n_reviews": 0,
  "start": 356.64,
  "end": 364.92
 },
 {
  "input": "This is more symbol-heavy and will require some more room, but it should be pretty satisfying for anyone who has previously been taught matrix multiplication the more rote way.",
  "translatedText": "Isso tem mais símbolos e exigirá mais espaço, mas deve ser bastante satisfatório para qualquer pessoa que já tenha aprendido multiplicação de matrizes de maneira mais mecânica.",
  "model": "google_nmt",
  "from_community_srt": "Isso é mais pesado simbolicamente e precisaremos de mais espaço, mas deve ser muito satisfatório para qualquer um que anteriormente aprendeu multiplicação de matrizes do jeito mais bruto.",
  "n_reviews": 0,
  "start": 365.54,
  "end": 373.66
 },
 {
  "input": "To follow where i-hat goes, start by looking at the first column of the matrix on the right, since this is where i-hat initially lands.",
  "translatedText": "Para acompanhar para onde vai o i-hat, comece olhando para a primeira coluna da matriz à direita, pois é aqui que o i-hat inicialmente pousa.",
  "model": "google_nmt",
  "from_community_srt": "Para seguir onde î vai comece olhando pela primeira coluna na matriz da direita já que ali é onde î inicialmente para.",
  "n_reviews": 0,
  "start": 374.46,
  "end": 381.06
 },
 {
  "input": "Multiplying that column by the matrix on the left is how you can tell where the intermediate version of i-hat ends up after applying the second transformation.",
  "translatedText": "Multiplicar essa coluna pela matriz à esquerda é como você pode saber onde a versão intermediária do i-hat termina após aplicar a segunda transformação.",
  "model": "google_nmt",
  "from_community_srt": "Multiplicando essa coluna pela matriz a esquerda é como você pode dizer onde a versão intermediária de î para após aplicarmos a segunda transformação.",
  "n_reviews": 0,
  "start": 382.0,
  "end": 390.3
 },
 {
  "input": "So the first column of the composition matrix will always equal the left matrix times the first column of the right matrix.",
  "translatedText": "Portanto, a primeira coluna da matriz composição será sempre igual à matriz esquerda vezes a primeira coluna da matriz direita.",
  "model": "google_nmt",
  "from_community_srt": "Então a segunda coluna da matriz composta sempre será igual a matriz da esquerda vezes a primeira coluna da matriz a direita.",
  "n_reviews": 0,
  "start": 391.62,
  "end": 398.1
 },
 {
  "input": "Likewise, j-hat will always initially land on the second column of the right matrix.",
  "translatedText": "Da mesma forma, j-hat sempre pousará inicialmente na segunda coluna da matriz direita.",
  "model": "google_nmt",
  "from_community_srt": "Da mesma maneira, ĵ sempre irá inicialmente parar na segunda coluna da matriz a direita.",
  "n_reviews": 0,
  "start": 402.16,
  "end": 407.14
 },
 {
  "input": "So multiplying the left matrix by this second column will give its final location, and hence that's the second column of the composition matrix.",
  "translatedText": "Portanto, multiplicar a matriz esquerda por esta segunda coluna dará sua localização final e, portanto, essa é a segunda coluna da matriz de composição.",
  "model": "google_nmt",
  "from_community_srt": "Então multiplicar a matriz da esquerda por essa segunda coluna dará a sua localização final, então essa será a segunda coluna da matriz composta.",
  "n_reviews": 0,
  "start": 408.94,
  "end": 417.02
 },
 {
  "input": "Notice there's a lot of symbols here, and it's common to be taught this formula as something to memorize, along with a certain algorithmic process to help remember it.",
  "translatedText": "Observe que há muitos símbolos aqui, e é comum aprender essa fórmula como algo para memorizar, junto com um certo processo algorítmico para ajudar a lembrá-la.",
  "model": "google_nmt",
  "from_community_srt": "Perceba, tem vários símbolos aqui, e é comum ensinarem essa fórmula como uma coisa a ser memorizada, junto com um certo processo algorítmico pra ajudar a lembrar, mas eu realmente acho que antes de memorizar esse processo",
  "n_reviews": 0,
  "start": 420.62,
  "end": 429.04
 },
 {
  "input": "But I really do think that before memorizing that process, you should get in the habit of thinking about what matrix multiplication really represents, applying one transformation after another.",
  "translatedText": "Mas eu realmente acho que antes de memorizar esse processo, você deveria adquirir o hábito de pensar sobre o que realmente representa a multiplicação de matrizes, aplicando uma transformação após a outra.",
  "model": "google_nmt",
  "from_community_srt": "você deve obter o hábito de pensar o que a multiplicação de matrizes realmente representa, que é aplicar uma transformação e depois outra.",
  "n_reviews": 0,
  "start": 429.16,
  "end": 438.9
 },
 {
  "input": "Trust me, this will give you a much better conceptual framework that makes the properties of matrix multiplication much easier to understand.",
  "translatedText": "Acredite em mim, isso lhe dará uma estrutura conceitual muito melhor que torna as propriedades da multiplicação de matrizes muito mais fáceis de entender.",
  "model": "google_nmt",
  "from_community_srt": "Confie em mim, isto lhe dará um contexto conceitual muito melhor que faz as propriedades da multiplicação de matrizes muito mais fáceis de entender.",
  "n_reviews": 0,
  "start": 439.62,
  "end": 446.3
 },
 {
  "input": "For example, here's a question.",
  "translatedText": "Por exemplo, aqui está uma pergunta.",
  "model": "google_nmt",
  "from_community_srt": "Por exemplo,",
  "n_reviews": 0,
  "start": 447.06,
  "end": 448.36
 },
 {
  "input": "Does it matter what order we put the two matrices in when we multiply them?",
  "translatedText": "Faz diferença a ordem em que colocamos as duas matrizes quando as multiplicamos?",
  "model": "google_nmt",
  "from_community_srt": "aqui está uma pergunta: importa em que ordem nós colocamos as duas matrizes quando as multiplicamos?",
  "n_reviews": 0,
  "start": 448.88,
  "end": 452.84
 },
 {
  "input": "Well, let's think through a simple example, like the one from earlier.",
  "translatedText": "Bem, vamos pensar em um exemplo simples, como o anterior.",
  "model": "google_nmt",
  "from_community_srt": "Bem, vamos pensar num simples exemplo, como o anterior.",
  "n_reviews": 0,
  "start": 453.62,
  "end": 457.0
 },
 {
  "input": "Take a shear, which fixes i-hat and smooshes j-hat over to the right, and a 90 degree rotation.",
  "translatedText": "Pegue uma tesoura, que fixa o i-hat e alisa o j-hat para a direita, e uma rotação de 90 graus.",
  "model": "google_nmt",
  "from_community_srt": "Pegue o cisalhamento, que fixa î e move ĵ para a direita e uma rotação de 90 graus.",
  "n_reviews": 0,
  "start": 457.64,
  "end": 462.82
 },
 {
  "input": "If you first do the shear, then rotate, we can see that i-hat ends up at 0,1 and j-hat ends up at negative 1,1.",
  "translatedText": "Se você primeiro fizer o cisalhamento e depois girar, podemos ver que i-hat termina em 0,1 e j-hat termina em menos 1,1.",
  "model": "google_nmt",
  "from_community_srt": "Se você fizer o cisalhamento e depois a rotação podemos ver que î para em (0,1) e ĵ para em (-1,1).",
  "n_reviews": 0,
  "start": 463.6,
  "end": 470.96
 },
 {
  "input": "Both are generally pointing close together.",
  "translatedText": "Ambos geralmente estão apontando próximos um do outro.",
  "model": "google_nmt",
  "from_community_srt": "Ambos geralmente então apontando em direções próximas.",
  "n_reviews": 0,
  "start": 471.32,
  "end": 473.06
 },
 {
  "input": "If you first rotate, then do the shear, i-hat ends up over at 1,1, and j-hat is off in a different direction at negative 1,0, and they're pointing, you know, farther apart.",
  "translatedText": "Se você girar primeiro, então faça o cisalhamento, i-hat termina em 1,1, e j-hat está em uma direção diferente em menos 1,0, e eles estão apontando, você sabe, mais distantes um do outro.",
  "model": "google_nmt",
  "from_community_srt": "Se você primeiro rotacionar e então fizer o cisalhamento, î para em (1,1) e ĵ está em uma direção diferente, em (-1,0), e eles estão apontando, você sabe,",
  "n_reviews": 0,
  "start": 473.86,
  "end": 485.52
 },
 {
  "input": "The overall effect here is clearly different, so evidently, order totally does matter.",
  "translatedText": "O efeito geral aqui é claramente diferente, então, evidentemente, a ordem importa totalmente.",
  "model": "google_nmt",
  "from_community_srt": "distantes um do outro. O efeito geral aqui é claramente diferente, então, evidentemente,",
  "n_reviews": 0,
  "start": 486.38,
  "end": 490.66
 },
 {
  "input": "Notice, by thinking in terms of transformations, that's the kind of thing that you can do in your head by visualizing.",
  "translatedText": "Observe que, ao pensar em termos de transformações, esse é o tipo de coisa que você pode fazer mentalmente através da visualização.",
  "model": "google_nmt",
  "from_community_srt": "a ordem totalmente importa. Perceba, pensando em termos de transformações, este é o tipo de coisa que você pode fazer na sua cabeça,",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.84
 },
 {
  "input": "No matrix multiplication necessary.",
  "translatedText": "Nenhuma multiplicação de matrizes é necessária.",
  "model": "google_nmt",
  "from_community_srt": "visualizando, nenhuma multiplicação de matrizes é necessária.",
  "n_reviews": 0,
  "start": 498.22,
  "end": 499.9
 },
 {
  "input": "I remember when I first took linear algebra, there was this one homework problem that asked us to prove that matrix multiplication is associative.",
  "translatedText": "Lembro-me de quando estudei álgebra linear pela primeira vez, havia um problema de lição de casa que nos pedia para provar que a multiplicação de matrizes é associativa.",
  "model": "google_nmt",
  "from_community_srt": "Eu lembro quando eu aprendi Álgebra Linear pela primeira vez, e tinha esse exercício que pedia pra provar que a multiplicação de matrizes é associativa.",
  "n_reviews": 0,
  "start": 501.48,
  "end": 509.12
 },
 {
  "input": "This means that if you have three matrices, A, B, and C, and you multiply them all together, it shouldn't matter if you first compute A times B, then multiply the result by C, or if you first multiply B times C, then multiply that result by A on the left.",
  "translatedText": "Isso significa que se você tem três matrizes, A, B e C, e multiplica todas elas, não importa se você primeiro calcula A vezes B e depois multiplica o resultado por C, ou se primeiro multiplica B vezes C e multiplique esse resultado por A à esquerda.",
  "model": "google_nmt",
  "from_community_srt": "Isso quer dizer que se você tem três matrizes, A, B e C, e você multiplicá-las juntas não deve importar se você primeiro calcular A vezes B, e multiplicar o resultado por C, ou se você primeiro multiplicar B vezes C, e depois multiplicar esse resultado por A na esquerda.",
  "n_reviews": 0,
  "start": 509.56,
  "end": 524.36
 },
 {
  "input": "In other words, it doesn't matter where you put the parentheses.",
  "translatedText": "Em outras palavras, não importa onde você coloca os parênteses.",
  "model": "google_nmt",
  "from_community_srt": "Em outras palavras, não importa onde você coloca os parênteses.",
  "n_reviews": 0,
  "start": 524.94,
  "end": 527.4
 },
 {
  "input": "Now, if you try to work through this numerically, like I did back then, it's horrible, just horrible, and unenlightening for that matter.",
  "translatedText": "Agora, se você tentar resolver isso numericamente, como eu fiz naquela época, é horrível, simplesmente horrível e pouco esclarecedor, nesse caso.",
  "model": "google_nmt",
  "from_community_srt": "Agora, se você tentar trabalhar isso numericamente como eu fiz na época, é horrível, simplesmente horrível, e só serve pra confundir.",
  "n_reviews": 0,
  "start": 528.38,
  "end": 535.76
 },
 {
  "input": "But when you think about matrix multiplication as applying one transformation after another, this property is just trivial.",
  "translatedText": "Mas quando você pensa na multiplicação de matrizes como a aplicação de uma transformação após a outra, essa propriedade é simplesmente trivial.",
  "model": "google_nmt",
  "from_community_srt": "Mas quando você pensa sobre a multiplicação de matrizes em termos de aplicar uma transformação e depois outra, essa propriedade é trivial,",
  "n_reviews": 0,
  "start": 535.76,
  "end": 542.78
 },
 {
  "input": "Can you see why?",
  "translatedText": "Você pode ver por quê?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.3,
  "end": 544.0
 },
 {
  "input": "What it's saying is that if you first apply C, then B, then A, it's the same as applying C, then B, then A.",
  "translatedText": "O que está dizendo é que se você aplicar primeiro C, depois B, depois A, é o mesmo que aplicar C, depois B e depois A.",
  "model": "google_nmt",
  "from_community_srt": "você consegue ver o por quê? O que ela está dizendo é que se você primeiro aplicar C, depois B, depois A, é a mesma coisa de aplicar C, depois B,",
  "n_reviews": 0,
  "start": 544.86,
  "end": 552.38
 },
 {
  "input": "I mean, there's nothing to prove.",
  "translatedText": "Quero dizer, não há nada a provar.",
  "model": "google_nmt",
  "from_community_srt": "depois A. Quero dizer,",
  "n_reviews": 0,
  "start": 552.82,
  "end": 554.38
 },
 {
  "input": "You're just applying the same three things one after the other, all in the same order.",
  "translatedText": "Você está apenas aplicando as mesmas três coisas, uma após a outra, todas na mesma ordem.",
  "model": "google_nmt",
  "from_community_srt": "não tem nada pra provar! Você só está aplicando as mesmas três coisas, uma depois da outra,",
  "n_reviews": 0,
  "start": 554.54,
  "end": 558.66
 },
 {
  "input": "This might feel like cheating, but it's not.",
  "translatedText": "Isso pode parecer trapaça, mas não é.",
  "model": "google_nmt",
  "from_community_srt": "todas na mesma ordem. Isso pode parecer trapaça, mas,",
  "n_reviews": 0,
  "start": 559.46,
  "end": 561.54
 },
 {
  "input": "This is an honest-to-goodness proof that matrix multiplication is associative, and even better than that, it's a good explanation for why that property should be true.",
  "translatedText": "Esta é uma prova honesta de que a multiplicação de matrizes é associativa e, melhor ainda, é uma boa explicação de por que essa propriedade deveria ser verdadeira.",
  "model": "google_nmt",
  "from_community_srt": "não é! Essa é uma prova legítima que a multiplicação de matrizes é associativa, e melhor do que isso, é uma boa explicação do por que essa propriedade deve ser verdadeira.",
  "n_reviews": 0,
  "start": 561.54,
  "end": 570.68
 },
 {
  "input": "I really do encourage you to play around more with this idea, imagining two different transformations, thinking about what happens when you apply one after the other, and then working out the matrix product numerically.",
  "translatedText": "Eu realmente encorajo você a brincar mais com essa ideia, imaginando duas transformações diferentes, pensando no que acontece quando você aplica uma após a outra e depois calculando numericamente o produto da matriz.",
  "model": "google_nmt",
  "from_community_srt": "Eu realmente lhe encorajo a brincar mais com essa idéia, imaginando duas transformações diferentes, e pensando sobre o que acontece quando você aplica uma depois da outra, e então trabalhar o produto das matrizes numericamente.",
  "n_reviews": 0,
  "start": 571.56,
  "end": 582.14
 },
 {
  "input": "Trust me, this is the kind of playtime that really makes the idea sink in.",
  "translatedText": "Acredite em mim, esse é o tipo de brincadeira que realmente faz a ideia penetrar.",
  "model": "google_nmt",
  "from_community_srt": "Confie em mim, esse é o tipo de exercício que realmente faz as idéias fixarem.",
  "n_reviews": 0,
  "start": 582.6,
  "end": 586.44
 },
 {
  "input": "In the next video, I'll start talking about extending these ideas beyond just two dimensions.",
  "translatedText": "No próximo vídeo, começarei falando sobre como estender essas ideias além de apenas duas dimensões.",
  "model": "google_nmt",
  "from_community_srt": "No próximo vídeo, eu começarei a falar sobre estender essas idéias além de somente duas dimensões.",
  "n_reviews": 0,
  "start": 587.2,
  "end": 592.18
 }
]