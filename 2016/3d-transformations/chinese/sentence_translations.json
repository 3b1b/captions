[
 {
  "input": "Hey folks, I've got a relatively quick video for you today, just sort of a footnote between chapters.",
  "translatedText": "嘿伙计们，我今天为你们准备了一个相 对较快的视频，只是章节之间的脚注。",
  "model": "google_nmt",
  "from_community_srt": "嘿 大家好！ 今天我带来了一个相对较短的视频作为章节之间的补充说明 前两期视频中，",
  "n_reviews": 0,
  "start": 13.46,
  "end": 18.52
 },
 {
  "input": "In the last two videos I talked about linear transformations and matrices, but I only showed the specific case of transformations that take two-dimensional vectors to other two-dimensional vectors.",
  "translatedText": "在上两个视频中，我讨论了线性变换和 矩阵，但我只展示了将二维向量转换 为其他二维向量的变换的具体情况。",
  "model": "google_nmt",
  "from_community_srt": "我谈论了关于线性变换与矩阵的话题 但是我只说明了一类将二维向量变换为其他二维向量的特殊变换 总体而言，",
  "n_reviews": 0,
  "start": 19.06,
  "end": 28.38
 },
 {
  "input": "The general throughout this series will work mainly in two dimensions, mostly because it's easier to actually see on the screen and wrap your mind around.",
  "translatedText": "本系列的一般内容将主要在二维空间中工作，主要是因为 它更容易在屏幕上实际看到并让您的思维更容易理解。",
  "model": "google_nmt",
  "from_community_srt": "整个系列里我们主要在二维空间中进行讨论 主要是因为这样更容易在屏幕上展现，",
  "n_reviews": 0,
  "start": 28.92,
  "end": 36.06
 },
 {
  "input": "But more importantly than that, once you get all the core ideas in two dimensions, they carry over pretty seamlessly to higher dimensions.",
  "translatedText": "但更重要的是，一旦你获得了二维的所有核心 思想，它们就可以无缝地转移到更高的维度。",
  "model": "google_nmt",
  "from_community_srt": "也更容易理解 但是更重要的一点在于， 一旦你掌握了二维空间里的核心概念 这些概念就能完美推广至高维空间 然而，",
  "n_reviews": 0,
  "start": 36.5,
  "end": 42.8
 },
 {
  "input": "Nevertheless, it's good to peek our heads outside of flatland now and then to, you know, see what it means to apply these ideas in more than just those two dimensions.",
  "translatedText": "尽管如此，时不时地把我们的头脑抛向平地之外是件好事，你知 道，看看将这些想法应用到不仅仅是这两个维度意味着什么。",
  "model": "google_nmt",
  "from_community_srt": "我们还是应该不时探出头看看平面外的世界 了解这些概念在二维空间外意味着什么 比如说，",
  "n_reviews": 0,
  "start": 43.8,
  "end": 51.0
 },
 {
  "input": "For example, consider a linear transformation with three-dimensional vectors as inputs and three-dimensional vectors as outputs.",
  "translatedText": "例如，考虑以三维向量作为输入、 以三维向量作为输出的线性变换。",
  "model": "google_nmt",
  "from_community_srt": "考虑这样一个线性变换 它以三维向量为输入，",
  "n_reviews": 0,
  "start": 52.34,
  "end": 58.88
 },
 {
  "input": "We can visualize this by smooshing around all the points in three-dimensional space, as represented by a grid, in such a way that keeps the grid lines parallel and evenly spaced, and which fixes the origin in place.",
  "translatedText": "我们可以通过平滑网格表示的三维空间中的所 有点来可视化这一点，以保持网格线平行且均 匀分布的方式，并将原点固定在适当的位置。",
  "model": "google_nmt",
  "from_community_srt": "并以三维向量为输出 我们可以想象它在移动三维空间中的所有点（这里用网格代表） 保持网格线平行且等距分布，",
  "n_reviews": 0,
  "start": 60.16,
  "end": 72.52
 },
 {
  "input": "And just as with two dimensions, every point of space that we see moving around is really just a proxy for a vector who has its tip at that point, and what we're really doing is thinking about input vectors moving over to their corresponding outputs.",
  "translatedText": "就像二维一样，我们看到移动的每个空间点实际上只 是一个向量的代理，该向量的尖端位于该点，而我们 真正要做的是考虑输入向量移动到其相应的输出。",
  "model": "google_nmt",
  "from_community_srt": "并保持原点不动 和二维情形一样， 我们看到的三维空间的每一个点 实际上只是用来代表以它本身为终点的一个向量 而我们所做的变换只是将输入向量移动至对应的输出向量 还是和二维情形相同 三维线性变换由基向量的去向完全决定 不过现在我们有三个通常使用的标准基向量",
  "n_reviews": 0,
  "start": 73.46,
  "end": 87.16
 },
 {
  "input": "And just as with two dimensions, one of these transformations is completely described by where the basis vectors go.",
  "translatedText": "就像二维一样，这些变换之一 完全由基向量的走向来描述。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.9,
  "end": 93.56
 },
 {
  "input": "But now, there are three standard basis vectors that we typically use, the unit vector in the x direction, i-hat, the unit vector in the y direction, j-hat, and a new guy, the unit vector in the z direction, called k-hat.",
  "translatedText": "但是现在，我们通常使用三个标准基向量，x 方向的单位向 量 i-hat，y 方向的单位向量 j-hat，以及 一个新的家伙，z 方向的单位向量，称为 k-hat。",
  "model": "google_nmt",
  "from_community_srt": "x方向的单位向量“i帽” y方向的单位向量“j帽” 外加一个新来的， z方向的单位向量“k帽” 实际上我认为，",
  "n_reviews": 0,
  "start": 94.16,
  "end": 106.56
 },
 {
  "input": "In fact, I think it's easier to think about these transformations by only following those basis vectors, since the full 3D grid representing all points can get kind of messy.",
  "translatedText": "事实上，我认为仅通过遵循这些基本向量来考虑这些变换会更容 易，因为表示所有点的完整 3D 网格可能会变得有点混乱。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.14,
  "end": 116.02
 },
 {
  "input": "By leaving a copy of the original axes in the background, we can think about the coordinates of where each of these three basis vectors lands.",
  "translatedText": "通过在背景中保留原始轴的副本，我们可以 考虑这三个基本向量中的每一个的坐标。",
  "model": "google_nmt",
  "from_community_srt": "只考虑跟踪这些基向量的话会更容易观察这些线性变换 因为使用三维网格会显得很凌乱 在背景中留下原始坐标轴的副本 我们就能观察到三个基向量变换后的位置 将变换后三个基向量的坐标记录在一个3×3的矩阵中",
  "n_reviews": 0,
  "start": 116.92,
  "end": 124.0
 },
 {
  "input": "Record the coordinates of these three vectors as the columns of a 3x3 matrix.",
  "translatedText": "将这三个向量的坐标记录为 3x3 矩阵的列。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.82,
  "end": 130.46
 },
 {
  "input": "This gives a matrix that completely describes the transformation using only nine numbers.",
  "translatedText": "这给出了一个仅使用九个数字就可以完整描述变换的矩阵。",
  "model": "google_nmt",
  "from_community_srt": "仅仅使用九个数字，",
  "n_reviews": 0,
  "start": 131.26,
  "end": 136.16
 },
 {
  "input": "As a simple example, consider the transformation that rotates space 90 degrees around the y axis.",
  "translatedText": "作为一个简单的示例，请考虑围绕 y 轴将空间旋转 90 度的变换。",
  "model": "google_nmt",
  "from_community_srt": "这个矩阵就完全描述了一个线性变换 举个简单的例子，",
  "n_reviews": 0,
  "start": 137.2,
  "end": 143.96
 },
 {
  "input": "So that would mean that it takes i-hat to the coordinates 0, 0, negative 1 on the z axis.",
  "translatedText": "所以这意味着它需要 i-hat 到 z 轴上的坐标 0, 0, 负 1。",
  "model": "google_nmt",
  "from_community_srt": "考虑沿着y轴旋转90度的变换 这个变换将i帽移动到z轴上的(0, 0,",
  "n_reviews": 0,
  "start": 144.86,
  "end": 150.1
 },
 {
  "input": "It doesn't move j-hat, so it stays at the coordinates 0, 1, 0.",
  "translatedText": "它不会移动 j-hat，因此它停留在坐标 0, 1, 0。",
  "model": "google_nmt",
  "from_community_srt": "-1) 它不移动j帽， 所以j帽仍旧在(0, 1,",
  "n_reviews": 0,
  "start": 150.82,
  "end": 154.28
 },
 {
  "input": "And then k-hat moves over to the x axis at 1, 0, 0.",
  "translatedText": "然后 k-hat 移动到 x 轴 1,0,0。",
  "model": "google_nmt",
  "from_community_srt": "0) 而k帽被移动至x轴上的(1, 0,",
  "n_reviews": 0,
  "start": 154.88,
  "end": 158.84
 },
 {
  "input": "Those three sets of coordinates become the columns of a matrix that describes that rotation To see where a vector with coordinates x, y, z lands, the reasoning is almost identical to what it was for two dimensions.",
  "translatedText": "这三组坐标成为描述旋转的矩阵的列。 要查看坐标为 x、y、z 的向量落 在哪里，推理几乎与二维的推理相同。",
  "model": "google_nmt",
  "from_community_srt": "0) 这三组坐标就成为了描述这一旋转变换的矩阵的三列 要想知道(x, y,",
  "n_reviews": 0,
  "start": 160.99,
  "end": 176.22
 },
 {
  "input": "Each of those coordinates can be thought of as instructions for how to scale each basis vector so that they add together to get your vector.",
  "translatedText": "这些坐标中的每一个都可以被认为是如何缩放每个 基本向量的指令，以便它们加在一起得到向量。",
  "model": "google_nmt",
  "from_community_srt": "z)所代表的向量在变换后的去向 推理过程与二维下几乎相同 它的每个坐标都可以看作对相应基向量的缩放 从而使缩放结果的和为你选择的向量 和二维类似，",
  "n_reviews": 0,
  "start": 176.94,
  "end": 184.04
 },
 {
  "input": "And the important part, just like the 2D case, is that this scaling and adding process works both before and after the transformation.",
  "translatedText": "与 2D 情况一样，重要的部分是此缩 放和添加过程在转换之前和之后都有效。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 186.26,
  "end": 194.0
 },
 {
  "input": "So to see where your vector lands, you multiply those coordinates by the corresponding columns of the matrix, and then you add together the three results.",
  "translatedText": "因此，要查看向量落在哪里，请将这些坐标乘 以矩阵的相应列，然后将三个结果加在一起。",
  "model": "google_nmt",
  "from_community_srt": "重要的部分在于 这一“缩放再相加”的过程在变换前后均适用 所以要找到向量变换后的位置 你将它的坐标与矩阵的对应列相乘 再将结果相加即可 两个矩阵相乘也是类似的 当你看到两个3×3矩阵相乘时",
  "n_reviews": 0,
  "start": 196.38,
  "end": 204.82
 },
 {
  "input": "Multiplying two matrices is also similar.",
  "translatedText": "两个矩阵相乘也类似。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.6,
  "end": 211.5
 },
 {
  "input": "Whenever you see two 3x3 matrices getting multiplied together, you should imagine first applying the transformation encoded by the right one, then applying the transformation encoded by the left one.",
  "translatedText": "每当您看到两个 3x3 矩阵相乘时， 您应该想象首先应用由右侧矩阵编码的变 换，然后应用由左侧矩阵编码的变换。",
  "model": "google_nmt",
  "from_community_srt": "你应该想象首先应用右侧矩阵代表的变换 然后应用左侧矩阵代表的变换 实际上，",
  "n_reviews": 0,
  "start": 212.02,
  "end": 223.26
 },
 {
  "input": "It turns out that 3D matrix multiplication is actually pretty important for fields like computer graphics and robotics, since things like rotations and three dimensions can be pretty hard to describe, but they're easier to wrap your mind around if you can break them down as the composition of separate, easier-to-think-about rotations.",
  "translatedText": "事实证明，3D 矩阵乘法实际上对于计算机图 形学和机器人学等领域非常重要，因为像旋转和 三维这样的东西可能很难描述，但如果你能将它 们分解为单独的、更容易思考的轮换的组成。",
  "model": "google_nmt",
  "from_community_srt": "三维矩阵相乘在部分领域有着非常重要的应用 比如计算机图形学与机器人学 虽然三维空间中的旋转很难直接表述 但是如果将它分解为简单分立的旋转的复合，",
  "n_reviews": 0,
  "start": 224.06,
  "end": 241.16
 },
 {
  "input": "Performing this matrix multiplication numerically is, once again, pretty similar to the two-dimensional case.",
  "translatedText": "以数字方式执行此矩阵乘法 再次与二维情况非常相似。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.36,
  "end": 249.86
 },
 {
  "input": "In fact, a good way to test your understanding of the last video would be to try to reason through what specifically this matrix multiplication should look like, thinking closely about how it relates to the idea of applying two successive transformations in space.",
  "translatedText": "事实上，测试您对上一个视频的理解的一个好方法是 尝试推理这个矩阵乘法具体应该是什么样子，仔细思 考它与在空间中应用两个连续变换的想法有何关系。",
  "model": "google_nmt",
  "from_community_srt": "这一过程就很容易理解了 三维下通过数值计算矩阵乘法再次与二维情形类似 这里有一个好方法来检测你对上期视频的理解程度 那就是根据“两个线性变换依次作用”这一想法 自己尝试推理上面这个矩阵乘法应该是什么样的",
  "n_reviews": 0,
  "start": 250.48,
  "end": 263.82
 },
 {
  "input": "In the next video, I'll start getting into the determinant.",
  "translatedText": "在下一个视频中，我将开始讨论行列式。",
  "model": "google_nmt",
  "from_community_srt": "下期视频中，",
  "n_reviews": 0,
  "start": 272.14,
  "end": 274.5
 }
]