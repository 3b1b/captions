[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Wektory własne i wartości własne to jeden z tych tematów, który wielu uczniów uważa za szczególnie nieintuicyjny.",
  "model": "google_nmt",
  "from_community_srt": "\"Matematyka to sztuka nadawania tych samych imion różnym rzeczom\" Jeśli mamy wektor w przestrzeni dwuwymiarowej, to istnieje standardowy sposób opisywania go współrzędnymi. W tym przypadku, wektor ma współrzędne [3,2] co oznacza, że przejście od jego początku do grotu wymaga przejścia 3 jednostek w prawo i 2 w górę.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Pytania takie jak: dlaczego to robimy i co to właściwie oznacza, zbyt często pozostawiane są po prostu w morzu obliczeń bez odpowiedzi.",
  "model": "google_nmt",
  "from_community_srt": "Jednak bardziej \"algebrowym\" sposobem opisywania współrzędnych jest myślenie o tych liczbach jako skalarach,",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "Kiedy publikowałem filmy z tej serii, wielu z Was komentowało, że nie może się doczekać wizualizacji tego tematu.",
  "model": "google_nmt",
  "from_community_srt": "czyli rzeczach które rozciągają albo ściskają wektor. Wtedy pierwsza współrzędna skaluje i-z-daszkiem czyli wektor o długości 1,",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Podejrzewam, że powodem tego nie jest to, że kwestie własne są szczególnie skomplikowane lub słabo wyjaśnione.",
  "model": "google_nmt",
  "from_community_srt": "wskazujący w prawo podczas gdy drugi skaluje j-z-daszkiem, czyli wektor o długości jeden,",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "W rzeczywistości jest to stosunkowo proste i myślę, że większość książek świetnie to wyjaśnia.",
  "model": "google_nmt",
  "from_community_srt": "wskazujący w górę.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Problem w tym, że ma to naprawdę sens tylko wtedy, gdy masz solidne wizualne zrozumienie wielu poprzedzających go tematów.",
  "model": "google_nmt",
  "from_community_srt": "Suma tych wektorów połączonych \"grot jednego do początku drugiego\" to właśnie wektor, który te współrzędne opisują.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Najważniejsze jest to, abyś wiedział, jak myśleć o macierzach jako o przekształceniach liniowych, ale musisz także oswoić się z takimi kwestiami, jak wyznaczniki, liniowe układy równań i zmiana podstawy.",
  "model": "google_nmt",
  "from_community_srt": "Można myśleć o tych dwóch specjalnych wektorach jako opisujących wszystkie nieme założenia naszego układu współrzędnych. Czyli między innymi fakt że pierwsza liczba opisuje kierunek w prawo, a druga kierunek w górę dokładnie tyle, ile jest we współrzędnych.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Zamieszanie wokół rzeczy własnych ma zwykle więcej wspólnego z niepewnymi podstawami w jednym z tych tematów niż z wektorami własnymi i samymi wartościami własnymi.",
  "model": "google_nmt",
  "from_community_srt": "To wszystko zapisane jest w wyborze i-z-daszkiem i j-z-daszkiem jako wektorów które te skalarne współrzędne mają rozciągać.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Na początek rozważ transformację liniową w dwóch wymiarach, taką jak ta pokazana tutaj.",
  "model": "google_nmt",
  "from_community_srt": "To przetłumaczenie pomiędzy wektorami a listami liczb określa się układem współrzędnych, a te dwa specjalne wektory,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Przesuwa wektor bazowy i-hat do współrzędnych 3, 0, a j-hat do 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "i-z-daszkiem i j-z-daszkiem nazywamy wektorami bazowymi z bazy standardowej.",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Jest to więc reprezentowane przez macierz, której kolumny to 3, 0 i 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "(w Polsce często nazywane e1 i e2) Chciałbym teraz powiedzieć trochę o koncepcie używania różnych wektorów bazowych.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Skoncentruj się na tym, co robi z jednym konkretnym wektorem i pomyśl o rozpiętości tego wektora, linii przechodzącej przez jego początek i wierzchołek.",
  "model": "google_nmt",
  "from_community_srt": "Na przykład powiedzmy, że mamy przyjaciółkę Jennifer która używa innych wektorów bazowych, które nazwę b1 i b2.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "Większość wektorów zostanie wyrzucona ze swojego zakresu podczas transformacji.",
  "model": "google_nmt",
  "from_community_srt": "Jej pierwszy wektor z bazy, b1,",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "To znaczy, wydawałoby się całkiem przypadkowe, gdyby miejsce, w którym wylądował wektor, również znajdowało się gdzieś na tej linii.",
  "model": "google_nmt",
  "from_community_srt": "wskazuje w górę i ciut w prawo, a drugi wektor wskazuje lewo i w górę. Spójrzmy teraz na ten wektor który pokazywałem wcześniej, ten,",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Jednak niektóre wektory specjalne pozostają na swoim własnym obszarze, co oznacza, że macierz wywiera wpływ na taki wektor po prostu go rozciągając lub zgniatając, jak skalar.",
  "model": "google_nmt",
  "from_community_srt": "który opisaliśmy współrzędnymi [3, 2] używając wektorów bazowych i-z-daszkiem i j-z-daszkiem. Jennifer natomiast opisałaby ten wektor współrzędnymi [5/3, 1/3] Co znaczy,",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "W tym konkretnym przykładzie wektor bazowy i-hat jest jednym z takich wektorów specjalnych.",
  "model": "google_nmt",
  "from_community_srt": "że aby otrzymać ten wektor za pomocą jej dwóch wektorów bazowych trzeba przeskalować b1 przez 5/3,",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "Rozpiętość i-hat to oś x, a z pierwszej kolumny macierzy widzimy, że i-hat przesuwa się do 3-krotności siebie, wciąż na tej osi x.",
  "model": "google_nmt",
  "from_community_srt": "b2 przez 1/3 i dodać je do siebie. Za chwilkę pokażę wam, jak wymyślić, że to miały być liczby dokładnie 5/3 oraz 1/3.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Co więcej, ze względu na sposób działania transformacji liniowych, każdy inny wektor na osi x jest również rozciągany 3-krotnie, a zatem pozostaje na swoim własnym zakresie.",
  "model": "google_nmt",
  "from_community_srt": "Ogólnie, jeżeli Jennifer opisuje współrzędne jakiegoś wektora, to myśli o pierwszej współrzędnej jako o rozciąganiu b1 a o drugiej, jako rozciąganiu b2 i następnie dodaje te dwa rozciągnięte wektory.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Nieco bardziej przebiegły wektor, który pozostaje na swoim własnym obszarze podczas tej transformacji, to minus 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "To, co otrzyma będzie zwykle kompletnie inne niż wektor który ja i ty otrzymalibyśmy, używając tych współrzędnych.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Skończyło się na rozciągnięciu 2-krotnym.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "I znowu, liniowość będzie oznaczać, że każdy inny wektor na linii ukośnej rozpiętej przez tego gościa zostanie rozciągnięty dwukrotnie.",
  "model": "google_nmt",
  "from_community_srt": "Dla nieco większej precyzji, jej pierwszy wektor bazowy, b1, opisalibyśmy u nas jako [2, 1] a drugi wektor b2 opisalibyśmy jako [-1,",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "I dla tej transformacji są to wszystkie wektory posiadające tę szczególną właściwość pozostawania na swojej rozpiętości.",
  "model": "google_nmt",
  "from_community_srt": "1]. Co ważne, trzeba zauważyć że z jej perspektywy, wektory b1 i b2 mają współrzędne [1,",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Te na osi x zostaną rozciągnięte 3-krotnie, a te na tej ukośnej linii zostaną rozciągnięte 2-krotnie.",
  "model": "google_nmt",
  "from_community_srt": "0] oraz [0, 1]. To właśnie b1 i b2 definiują, co w jej świecie oznacza [1, 0] i [0,",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Każdy inny wektor zostanie nieco obrócony podczas transformacji i wyrzucony z linii, którą obejmuje.",
  "model": "google_nmt",
  "from_community_srt": "1]. Więc w rezultacie rozmawiamy w innych językach.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Jak można się już domyślić, te specjalne wektory nazywane są wektorami własnymi transformacji, a każdy wektor własny ma przypisaną tak zwaną wartość własną, która jest po prostu czynnikiem, przez który jest on rozciągany lub zgniatany podczas transformacji.",
  "model": "google_nmt",
  "from_community_srt": "Wszyscy patrzymy na te same wektory w przestrzeni, lecz Jennifer używa innych słów i liczb na ich opisanie. Teraz szybkie słówko o reprezentacji przestrzeni, której używam w 2D. Zwykle używam tej kwadratowej kraty, ale ta krata to tylko umowa, sposób wizualizacji naszego układu współrzędnych, który zależy od wyboru bazy.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Oczywiście nie ma nic specjalnego w rozciąganiu i zgniataniu lub w fakcie, że te wartości własne są dodatnie.",
  "model": "google_nmt",
  "from_community_srt": "Sama przestrzeń nie ma własnej kraty. Jennifer może narysować własną kratownicę, tak samo wymyśloną jak moja,",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "W innym przykładzie możesz mieć wektor własny z wartością własną ujemną o 1 połowę, co oznacza, że wektor zostanie odwrócony i zgnieciony 1-krotnie.",
  "model": "google_nmt",
  "from_community_srt": "która tak samo nie oznacza więcej niż tylko sposób wizualizacji, który pomaga nam używać jej współrzędnych. Początek układu, natomiast,",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Ale ważną częścią jest to, że pozostaje na linii, na którą się rozciąga, bez obracania się z niej.",
  "model": "google_nmt",
  "from_community_srt": "pokrywałby się z naszym, skoro wszyscy zgadzamy się, co oznacza [0, 0]. Jest to wynik który otrzymamy, gdy skalujemy dowolny wektor przez 0.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Aby zobaczyć, dlaczego warto się nad tym zastanowić, rozważmy trójwymiarową rotację.",
  "model": "google_nmt",
  "from_community_srt": "Jednak kierunek jej osi oraz odstępy między jej liniami kraty będzie inny, zależnie od wyboru wektorów bazowych.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Jeśli możesz znaleźć wektor własny dla tego obrotu, wektor, który pozostaje na swoim własnym rozpiętości, to tym, co znalazłeś, jest oś obrotu.",
  "model": "google_nmt",
  "from_community_srt": "Zatem, po całym tym wstępnie, dość naturalnym pytaniem jest: Jak przetłumaczać między różnymi układami współrzędnych? Jeżeli na przykład Jennifer opisze wektor współrzędnymi [-1,",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "O wiele łatwiej jest myśleć o obrocie 3D w kategoriach jakiejś osi obrotu i kąta, o jaki się obraca, niż myśleć o pełnej macierzy 3x3 powiązanej z tą transformacją.",
  "model": "google_nmt",
  "from_community_srt": "2] to jak zapisać go w naszym układzie współrzędnych? Jak tłumaczyć z jej języka na nasz? Cóż, jej współrzędne mówią, że ten wektor to -1 b1 + 2 b2.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "Nawiasem mówiąc, w tym przypadku odpowiadająca wartość własna musiałaby wynosić 1, ponieważ obroty nigdy niczego nie rozciągają ani nie zgniatają, więc długość wektora pozostanie taka sama.",
  "model": "google_nmt",
  "from_community_srt": "A z naszej perspektywy b1 ma współrzędne [2,",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Ten wzór często pojawia się w algebrze liniowej.",
  "model": "google_nmt",
  "from_community_srt": "1] i b2 ma współrzędne [-1,",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "W przypadku dowolnej transformacji liniowej opisanej przez macierz można zrozumieć, co ona robi, odczytując kolumny tej macierzy jako miejsca lądowania dla wektorów bazowych.",
  "model": "google_nmt",
  "from_community_srt": "1] Więc możemy po prostu obliczyć -1 b1 + 2 b2 w naszym układzie współrzędnych.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Często jednak lepszym sposobem na dotarcie do sedna tego, co faktycznie robi transformacja liniowa, mniej zależnego od konkretnego układu współrzędnych, jest znalezienie wektorów własnych i wartości własnych.",
  "model": "google_nmt",
  "from_community_srt": "Obliczając to otrzymamy wektor [-4, 1] Tak zatem opisalibyśmy wektor o którym ona myśli jako [-1,",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Nie będę tutaj omawiał wszystkich szczegółów metod obliczania wektorów własnych i wartości własnych, ale spróbuję przedstawić przegląd pomysłów obliczeniowych, które są najważniejsze dla zrozumienia pojęciowego.",
  "model": "google_nmt",
  "from_community_srt": "2] Ten proces skalowania każdego z jej wektorów bazowych przez odpowiednie współrzędne pewnego wektora a następnie dodawanie ich może wydawać się znajome To mnożenie wektora przez macierz gdzie kolumny macierzy reprezentują wektory z bazy Jennifer w naszym języku",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Symbolicznie, oto jak wygląda idea wektora własnego.",
  "model": "google_nmt",
  "from_community_srt": "Tak naprawdę, jeżeli zrozumiesz mnożenie macierzy przez wektor jako przykładanie pewnego przekształcenia liniowego",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A jest macierzą reprezentującą pewną transformację, gdzie v jest wektorem własnym, a lambda jest liczbą, a mianowicie odpowiednią wartością własną.",
  "model": "google_nmt",
  "from_community_srt": "na przykład, przez film który uważam za najważniejszy z całego kursu, Rozdział 3, to okaże się, że to co się tu dzieje jest dość intuicyjne.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "To wyrażenie mówi, że iloczyn wektora macierzowego A razy v daje taki sam wynik, jak samo skalowanie wektora własnego v o pewną wartość lambda.",
  "model": "google_nmt",
  "from_community_srt": "Macierz której kolumny opisują wektory z bazy Jennifer może być interpretowana jako przekształcenie które wysyła naszą bazę, i-z-daszkiem i j-z-daszkiem czyli wektory o których myślimy,",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Zatem znalezienie wektorów własnych i ich wartości własnych macierzy A sprowadza się do znalezienia wartości v i lambda, które sprawiają, że to wyrażenie jest prawdziwe.",
  "model": "google_nmt",
  "from_community_srt": "mówiąc [1,0] i [0,1] na wektory z bazy Jennifer czyli rzeczy o których myśli ona mówiąc [1, 0] i [0, 1] Żeby pokazać jak to działa przejdźmy powoli przez to,",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "Na początku praca z tym jest trochę niewygodna, ponieważ lewa strona reprezentuje mnożenie wektorów macierzowych, a prawa strona to mnożenie wektorów skalarnych.",
  "model": "google_nmt",
  "from_community_srt": "co oznacza wzięcie wektora o współrzędnych [-1, 2] w naszej bazie i przyłożenia do niego tego przekształcenia.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Zacznijmy więc od przepisania tej prawej strony jako pewnego rodzaju mnożenia macierzy przez wektor, używając macierzy, która powoduje skalowanie dowolnego wektora przez współczynnik lambda.",
  "model": "google_nmt",
  "from_community_srt": "Przed przekształceniem myślimy o tym wektorze jako o pewnej kombinacji liniowej naszych wektorów bazowych: -1 razy i-z-daszkiem + 2 razy j-z-daszkiem A główną własnością przekształcenia liniowego jest fakt że wektor wyjściowy będzie tą samą liniową kombinacją",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Kolumny takiej macierzy będą przedstawiać, co dzieje się z każdym wektorem bazowym, a każdy wektor bazowy jest po prostu mnożony przez lambda, zatem w tej macierzy liczba lambda będzie znajdować się wzdłuż przekątnej, z zerami wszędzie indziej.",
  "model": "google_nmt",
  "from_community_srt": "ale nowych wektorów bazowych -1 razy obraz i-z-daszkiem + 2 razy obraz j-z-daszkiem. Więc to,",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "Powszechnym sposobem zapisywania tego faceta jest rozłożenie tej lambdy na czynniki i zapisanie jej jako lambda razy i, gdzie i jest macierzą tożsamości z jedynkami wzdłuż przekątnej.",
  "model": "google_nmt",
  "from_community_srt": "co robi macierz to przekształca coś, co jest naszym złym przekonaniem o czym myśli Jennifer w prawdziwy wektor, o którym ona myśli. Pamiętam,",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Ponieważ obie strony wyglądają jak mnożenie macierzy przez wektor, możemy odjąć tę prawą stronę i wyliczyć v.",
  "model": "google_nmt",
  "from_community_srt": "że gdy pierwszy raz się o tym uczyłem, wydało mi się to kompletnie pomieszane. Geometrycznie, macierz przekształca naszą kratę w kratę Jennifer.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Mamy więc nową macierz, A minus lambda razy tożsamość i szukamy wektora v takiego, że ta nowa macierz razy v daje wektor zerowy.",
  "model": "google_nmt",
  "from_community_srt": "Jednak obliczeniowo, przekształca opis wektora z jej języka na nasz. To, co w końcu otworzyło mi oczy, to pomyślenie o tym jak bierze ten zły,",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "To zawsze będzie prawdą, jeśli v samo w sobie jest wektorem zerowym, ale to jest nudne.",
  "model": "google_nmt",
  "from_community_srt": "początkowy wektor wektor który otrzymujemy używając tych samych współrzędnych,",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "To, czego chcemy, to niezerowy wektor własny.",
  "model": "google_nmt",
  "from_community_srt": "tylko w naszym układzie, i przekształca go w wektor o który naprawdę chodziło.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "A jeśli obejrzysz rozdziały 5 i 6, będziesz wiedział, że jedyny sposób, w jaki iloczyn macierzy z niezerowym wektorem może stać się zerem, polega na tym, że transformacja związana z tą macierzą zgniata przestrzeń do niższego wymiaru.",
  "model": "google_nmt",
  "from_community_srt": "A co z przekształceniem z powrotem? W przypadku którego użyłem wcześniej w tym video jeżeli mam wektor o współrzędnych [3, 2] w naszym układzie jak w końcu obliczyłem że będzie miał współrzędne [5/3,",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "I to zgniatanie odpowiada zerowej wyznacznikowi macierzy.",
  "model": "google_nmt",
  "from_community_srt": "1/3] w naszym układzie? Zaczynamy z macierzą zmiany bazy",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Mówiąc konkretnie, załóżmy, że macierz A ma kolumny 2, 1 i 2, 3 i pomyśl o odjęciu zmiennej wartości lambda od każdego wpisu po przekątnej.",
  "model": "google_nmt",
  "from_community_srt": "która przetłumacza język Jennifer na nasz i bierzemy jej odwrotność. Pamiętamy, że przekształcenie odwrotne to nowe przekształcenie,",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Teraz wyobraź sobie, że poprawiasz lambdę, obracając pokrętło, aby zmienić jej wartość.",
  "model": "google_nmt",
  "from_community_srt": "któremu odpowiada wzięcie tego początkowego na odwrót.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "Gdy zmienia się wartość lambda, zmienia się sama macierz, a zatem zmienia się wyznacznik macierzy.",
  "model": "google_nmt",
  "from_community_srt": "W praktyce, szczególnie gdy działamy w więcej niż 2 wymiarach używalibyśmy komputera do obliczania macierzy odwrotnej.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Celem jest znalezienie wartości lambda, która sprawi, że ten wyznacznik będzie zerowy, co oznacza, że poprawiona transformacja zgniata przestrzeń w niższy wymiar.",
  "model": "google_nmt",
  "from_community_srt": "W tym przypadku, macierz odwrotna do macierzy zmiany bazy która ma bazę Jennifer jako kolumny okazuje się być macierzą o kolumnach  [1/3,",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "W tym przypadku optymalny moment występuje, gdy lambda wynosi 1.",
  "model": "google_nmt",
  "from_community_srt": "-1/3] i [1/3, 2/3] Zatem,",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Oczywiście gdybyśmy wybrali inną macierz, wartość własna niekoniecznie wynosiłaby 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Najlepszym punktem może być inna wartość lambda.",
  "model": "google_nmt",
  "from_community_srt": "dla przykładu żeby zobaczyć jak wygląda wektor [3,2] w układzie współrzędnych Jennifer mnożymy odwrotność macierzy zmiany bazy przez [3,",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "To dość dużo, ale spójrzmy, co to mówi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Gdy lambda jest równa 1, macierz A minus lambda razy tożsamość spację spacji na linii.",
  "model": "google_nmt",
  "from_community_srt": "2] co okazuje się być [5/3, 1/3] Więc tak,",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "To oznacza, że istnieje niezerowy wektor v taki, że A minus lambda razy identyczność v równa się wektorowi zerowemu.",
  "model": "google_nmt",
  "from_community_srt": "pokrótce, przetłumaczamy opis wektorów z jednego układu do drugiego i z powrotem.",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "I pamiętajcie, przejmujemy się tym, ponieważ oznacza to, że A razy v równa się lambda razy v, co można odczytać w ten sposób, że wektor v jest wektorem własnym A, pozostającym na swoim własnym rozpiętości podczas transformacji A.",
  "model": "google_nmt",
  "from_community_srt": "Macierz której kolumny opisują wektory z bazy Jennifer ale opisane w naszych współrzędnych przekształca wektory z jej języka na nasz. A macierz odwrotna robi rzecz odwrotną. Jednak wektory nie są jedyną rzeczą jaką opisujemy współrzędnymi. Przed tą kolejną częścią, ważne jest że umiecie posługiwać się",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "W tym przykładzie odpowiadająca wartość własna wynosi 1, więc v faktycznie pozostanie na miejscu.",
  "model": "google_nmt",
  "from_community_srt": "reprezentacją przekształceń macierzami i wiecie, jak mnożenie macierzy odpowiada złożeniu przekształceń.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Zatrzymaj się i zastanów, czy chcesz się upewnić, że taki tok rozumowania jest dobry.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "To jest ten rodzaj rzeczy, o którym wspomniałem we wstępie.",
  "model": "google_nmt",
  "from_community_srt": "Z pewnością nie zaszkodzi zapauzować i spojrzeć na rozdziały 3 i 4 jeżeli coś z tego nie wydaje się wam jasne.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Gdybyś nie miał solidnego pojęcia o wyznacznikach i o tym, dlaczego odnoszą się one do liniowych układów równań mających rozwiązania niezerowe, takie wyrażenie wydawałoby się zupełnie niespodziewane.",
  "model": "google_nmt",
  "from_community_srt": "Rozpatrzmy pewne przekształcenie liniowe, na przykład obrót o 90° przeciwnie do wskazówek zegara. jeżeli wyrazimy to macierzą, patrzymy gdzie trafiają i-z-daszkiem i j-z-daszkiem.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Aby zobaczyć to w akcji, wróćmy do przykładu od początku, z macierzą, której kolumny to 3, 0 i 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "i ląduje na wektorze o współrzędnych [0,1] a ja na wektorze [-1, 0] Zatem te współrzędne stają się kolumnami naszej macierzy.",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Aby sprawdzić, czy wartość lambda jest wartością własną, odejmij ją od przekątnych tej macierzy i oblicz wyznacznik.",
  "model": "google_nmt",
  "from_community_srt": "Jednak ta reprezentacja jest mocno związana z wyborem bazy, poczynając od tego, że patrzymy,",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Robiąc to, otrzymamy pewien wielomian kwadratowy w lambdzie, 3 minus lambda razy 2 minus lambda.",
  "model": "google_nmt",
  "from_community_srt": "gdzie lądują i oraz j, a kończąc na tym, że opisujemy wektory na jakie przechodzą w naszym układzie współrzędnych. Jak Jennifer opisałaby ten sam obrót? Może cię kusić,",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Ponieważ lambda może być wartością własną tylko wtedy, gdy ten wyznacznik ma wartość zero, można stwierdzić, że jedynymi możliwymi wartościami własnymi są lambda równe 2 i lambda równe 3.",
  "model": "google_nmt",
  "from_community_srt": "aby po prostu przetłumaczyć kolumny naszej macierzy na język Jennifer. Ale nie jest dobry pomysł. Te kolumny pokazują,",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Aby dowiedzieć się, jakie wektory własne faktycznie mają jedną z tych wartości własnych, powiedzmy, że lambda równa się 2, podłącz tę wartość lambda do macierzy, a następnie oblicz, dla jakich wektorów ta zmieniona po przekątnej macierz ma wartość zero.",
  "model": "google_nmt",
  "from_community_srt": "gdzie wylądują nasze wektory i oraz j. A Jennifer chce macierz, która reprezentuje gdzie lądują jej wektory bazowe i musi opisać te wektory na których lądują w jej języku. Pokażę popularny sposób myślenia, jak to robić.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Jeśli obliczysz to w taki sam sposób, jak każdy inny układ liniowy, zobaczysz, że rozwiązaniami są wszystkie wektory na linii ukośnej rozpiętej przez -1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Zacznijmy od dowolnego wektora opisanego w języku Jennifer. Zamiast myśleć co się z nim w stanie w języku Jennifer, najpierw przechodzimy do naszego języka,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Odpowiada to faktowi, że niezmieniona macierz 3, 0, 1, 2 powoduje dwukrotne rozciągnięcie wszystkich wektorów.",
  "model": "google_nmt",
  "from_community_srt": "używając macierzy zmiany bazy, tej, której kolumny reprezentują jej wektory bazowe w naszym języku. To daje nam ten sam wektor ale opisany w naszym języku.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Transformacja 2D nie musi teraz mieć wektorów własnych.",
  "model": "google_nmt",
  "from_community_srt": "Następnie użyj macierzy przekształcenia do wyniku przez przemnożenie z lewej strony.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Rozważmy na przykład obrót o 90 stopni.",
  "model": "google_nmt",
  "from_community_srt": "To mówi nam gdzie ląduje wektor, ale nadal w naszym języku.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "To nie ma żadnych wektorów własnych, ponieważ obraca każdy wektor poza swój własny zakres.",
  "model": "google_nmt",
  "from_community_srt": "Zatem ostatnim krokiem jest powrót do języka Jennifer przez przemnożenie przez odwrotność macierzy zmiany bazy",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Jeśli faktycznie spróbujesz obliczyć wartości własne takiego obrotu, zwróć uwagę, co się stanie.",
  "model": "google_nmt",
  "from_community_srt": "z lewej strony aby otrzymać przekształcony wektor już w języku Jennifer.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Jej macierz ma kolumny 0, 1 i ujemne 1, 0.",
  "model": "google_nmt",
  "from_community_srt": "Skoro możemy to zrobić z dowolnym wektorem napisanym w jej języku najpierw zmieniając bazę,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Odejmij lambdę od elementów przekątnych i sprawdź, kiedy wyznacznik wynosi zero.",
  "model": "google_nmt",
  "from_community_srt": "potem obracając, i znowu wracając do starej bazy,",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "W tym przypadku otrzymasz wielomian lambda do kwadratu plus 1.",
  "model": "google_nmt",
  "from_community_srt": "to złożenie trzech macierzy daje nam macierz przekształcenia w języku Jennifer.",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Jedynymi pierwiastkami tego wielomianu są liczby urojone i oraz ujemne i.",
  "model": "google_nmt",
  "from_community_srt": "Bierze ono wektor w jej języku i wypluwa jego obrót, też w jej języku.",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Brak rozwiązań liczb rzeczywistych oznacza, że nie ma wektorów własnych.",
  "model": "google_nmt",
  "from_community_srt": "w tym przykładzie jeśli wektory z bazy Jennifer to [2,1] i [-1,1] w naszym języku i rotacja jest o 90 stopni",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Innym całkiem interesującym przykładem, który warto mieć z tyłu głowy, jest ścinanie.",
  "model": "google_nmt",
  "from_community_srt": "to iloczyn tych trzech macierzy jeżeli to obliczyć ma kolumny [1/3,",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "To ustawia i-hat na miejscu i przesuwa j-hat 1, więc jego macierz ma kolumny 1, 0 i 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "5/3] i [-2/3,",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Wszystkie wektory na osi x są wektorami własnymi o wartości własnej 1, ponieważ pozostają nieruchome.",
  "model": "google_nmt",
  "from_community_srt": "-1/3] zatem jeżeli Jennifer przemnoży tę macierz przez współrzędne wektora w jej układzie to dostanie wersję przekręconą o 90 stopni",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "W rzeczywistości są to jedyne wektory własne.",
  "model": "google_nmt",
  "from_community_srt": "wyrażoną w jej układzie współrzędnych.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Kiedy odejmiemy lambdę od przekątnych i obliczymy wyznacznik, otrzymamy 1 minus lambda do kwadratu.",
  "model": "google_nmt",
  "from_community_srt": "w ogólności, kiedykolwiek widzisz wyrażenie typu A^(-1) M A to sugeruje to pewną matematyczną \"zmianę perspektywy\"",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "Jedynym pierwiastkiem tego wyrażenia jest lambda równa 1.",
  "model": "google_nmt",
  "from_community_srt": "macierz środkowa to pewne przekształcenie tak,",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Zgadza się to z tym, co widzimy geometrycznie, że wszystkie wektory własne mają wartość własną 1.",
  "model": "google_nmt",
  "from_community_srt": "jak my je widzimy, a dwie zewnętrzne mówią o pewnej empatii - zmianie perspektywy,",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Należy jednak pamiętać, że możliwe jest również posiadanie tylko jednej wartości własnej, ale z więcej niż tylko linią pełną wektorów własnych.",
  "model": "google_nmt",
  "from_community_srt": "a wszystkie trzy mówią o tym samym przekształceniu, tylko z cudzej perspektywy. Dla tych którzy zastanawiają się, po co nam inne układy współrzędnych - następny film o wektorach własnych i wartościach własnych",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Prostym przykładem jest macierz, która skaluje wszystko przez 2.",
  "model": "google_nmt",
  "from_community_srt": "da nam bardzo ważny tego przykład.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "Jedyną wartością własną jest 2, ale każdy wektor na płaszczyźnie staje się wektorem własnym z tą wartością własną.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Teraz jest dobry moment, aby zatrzymać się i zastanowić nad niektórymi z tych kwestii, zanim przejdę do ostatniego tematu.",
  "model": "google_nmt",
  "from_community_srt": "Do zobaczenia!",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Chcę w tym miejscu zakończyć koncepcją podstawy własnej, która w dużej mierze opiera się na pomysłach z poprzedniego filmu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Przyjrzyj się, co się stanie, jeśli nasze wektory bazowe okażą się wektorami własnymi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Na przykład może i-hat jest skalowany przez -1, a j-hat jest skalowany przez 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Zapisując ich nowe współrzędne jako kolumny macierzy, zauważ, że te wielokrotności skalarne, ujemne 1 i 2, które są wartościami własnymi i-hat i j-hat, leżą na przekątnej naszej macierzy, a każdy inny wpis to 0 .",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Za każdym razem, gdy macierz ma zera wszędzie poza przekątną, nazywa się ją, całkiem rozsądnie, macierzą diagonalną.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "Można to zinterpretować w ten sposób, że wszystkie wektory bazowe są wektorami własnymi, a elementy diagonalne tej macierzy są ich wartościami własnymi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Jest wiele rzeczy, które sprawiają, że praca z macierzami diagonalnymi jest o wiele przyjemniejsza.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Najważniejszą z nich jest to, że łatwiej jest obliczyć, co się stanie, jeśli pomnożysz tę macierz przez samą siebie wiele razy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Ponieważ wszystkie te macierze skalują każdy wektor bazowy o pewną wartość własną, wielokrotne zastosowanie tej macierzy, powiedzmy 100 razy, będzie po prostu odpowiadać skalowaniu każdego wektora bazowego przez setną potęgę odpowiedniej wartości własnej.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Dla kontrastu spróbuj obliczyć setną potęgę macierzy niediagonalnej.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Naprawdę, spróbuj przez chwilę.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "To koszmar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Oczywiście rzadko będziesz miał tyle szczęścia, aby wektory bazowe były również wektorami własnymi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Ale jeśli twoja transformacja ma wiele wektorów własnych, takich jak ten z początku tego filmu, wystarczająco dużo, abyś mógł wybrać zbiór obejmujący całą przestrzeń, możesz zmienić swój układ współrzędnych tak, aby te wektory własne były wektorami bazowymi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Mówiłem o zmianie podstawy w poprzednim filmie, ale tutaj bardzo szybko przypomnę, jak wyrazić transformację aktualnie zapisaną w naszym układzie współrzędnych na inny układ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Weź współrzędne wektorów, których chcesz użyć jako nowej podstawy, co w tym przypadku oznacza nasze dwa wektory własne, a następnie uczyń te współrzędne kolumnami macierzy, znanej jako macierz zmiany podstawy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Kiedy umieścisz pierwotną transformację, umieszczając zmianę macierzy bazowej po jej prawej stronie i odwrotność zmiany macierzy bazowej po jej lewej stronie, wynikiem będzie macierz reprezentująca tę samą transformację, ale z punktu widzenia współrzędnych nowych wektorów bazowych system.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Cały sens robienia tego z wektorami własnymi polega na tym, że ta nowa macierz ma gwarancję przekątnej z odpowiadającymi jej wartościami własnymi wzdłuż tej przekątnej.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Dzieje się tak, ponieważ reprezentuje pracę w układzie współrzędnych, w którym wektory bazowe ulegają skalowaniu podczas transformacji.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Zbiór wektorów bazowych, które są również wektorami własnymi, nazywany jest, znowu, całkiem rozsądnie, bazą własną.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Jeśli więc na przykład trzeba byłoby obliczyć setną potęgę tej macierzy, znacznie łatwiej byłoby przejść na podstawę własną, obliczyć setną potęgę w tym układzie, a następnie przekonwertować z powrotem do naszego standardowego systemu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Nie można tego zrobić w przypadku wszystkich transformacji.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Na przykład ścinanie nie ma wystarczającej liczby wektorów własnych, aby objąć całą przestrzeń.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Ale jeśli potrafisz znaleźć bazę własną, operacje na macierzach stają się naprawdę piękne.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Dla tych z Was, którzy chcą rozwiązać całkiem niezłą łamigłówkę, aby zobaczyć, jak to wygląda w akcji i jak można ją wykorzystać do uzyskania zaskakujących rezultatów, zostawię podpowiedź tutaj na ekranie.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Wymaga to trochę pracy, ale myślę, że będziesz zadowolony.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Następny i ostatni film z tej serii będzie dotyczył abstrakcyjnych przestrzeni wektorowych.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]