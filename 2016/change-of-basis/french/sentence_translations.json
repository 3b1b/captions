[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Les vecteurs propres et les valeurs propres font partie de ces sujets que beaucoup d'étudiants trouvent particulièrement peu intuitifs.",
  "from_community_srt": "\"Les mathématiques est l’art de donner le même nom à des choses différentes.\" Si j'ai ici un vecteur dans un espace 2D on a un moyen standard de le décrire avec des coordonnées Dans ce cas, le vecteur a comme coordonnées [3,2], ce qui signifie que d'aller de son origine à sa pointe implique un déplacement de 3 unités vers la droite et 2 vers le haut.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Des questions telles que « pourquoi faisons-nous cela et qu'est-ce que cela signifie réellement » restent trop souvent flottantes dans une mer de calculs sans réponse.",
  "from_community_srt": "A présent, le moyen le plus axé algèbre linéaire pour décrire des coordonnées est de pensé à chacun de ces nombres comme étant des scalaires,",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "Et au fur et à mesure que j'ai publié les vidéos de cette série, beaucoup d'entre vous ont exprimé leur impatience de visualiser ce sujet en particulier.",
  "from_community_srt": "quelque chose que étire ou compresse des vecteurs. On voit cette première coordonnée comme une mise à l'échelle de î le vecteur de longueur 1,",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Je soupçonne que la raison en est pas tant que les choses soient particulièrement compliquées ou mal expliquées.",
  "from_community_srt": "pointant vers la droite tandis que la seconde coordonnée met à l'échelle ĵ le vecteur de longueur 1,",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "En fait, c’est relativement simple, et je pense que la plupart des livres l’expliquent très bien.",
  "from_community_srt": "pointant vers le haut.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Le problème est que cela n’a vraiment de sens que si vous avez une solide compréhension visuelle de la plupart des sujets qui le précèdent.",
  "from_community_srt": "La mise bout à bout de ces deux vecteurs mis à l'échelle est ce que les coordonnées décrivent.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Le plus important ici est que vous sachiez considérer les matrices comme des transformations linéaires, mais vous devez également être à l'aise avec des éléments tels que les déterminants, les systèmes d'équations linéaires et le changement de base.",
  "from_community_srt": "On peut voir ces deux vecteurs spéciaux comme un moyen de décrire toutes significations implicites de notre système de coordonnées. Le fait que le premier nombre indique un mouvement vers la droite, que le deuxième indique un mouvement vers le haut",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "La confusion à propos des choses propres a généralement plus à voir avec des fondations fragiles dans l'un de ces sujets qu'avec les vecteurs propres et les valeurs propres elles-mêmes.",
  "from_community_srt": "exactement à combien d'unités de distance, tout cela est lié au choix de î et ĵ qui sont les vecteurs dont les coordonnées mettent à l'échelle.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Pour commencer, considérons une transformation linéaire en deux dimensions, comme celle présentée ici.",
  "from_community_srt": "Toute façon de traduire entre les vecteurs et ensembles de nombres est appelé un système de coordonnées et les deux vecteurs spéciaux,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Il déplace le vecteur de base i-hat vers les coordonnées 3, 0 et j-hat vers 1, 2.",
  "from_community_srt": "î et ĵ, sont appelés les vecteurs de base de notre système de coordonnées standard.",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Il est donc représenté par une matrice dont les colonnes sont 3, 0 et 1, 2.",
  "from_community_srt": "Ce dont j'aimerais parler ici est l'idée d'utiliser un ensemble différent de base vecteurs.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Concentrez-vous sur ce qu'il fait à un vecteur particulier et pensez à l'étendue de ce vecteur, à la ligne passant par son origine et sa pointe.",
  "from_community_srt": "Par exemple, disons que vous avez un ami, Jennifer qui utilise un ensemble différent de vecteurs de base que je vais appeler b1 et b2",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "La plupart des vecteurs vont perdre leur portée pendant la transformation.",
  "from_community_srt": "Son premier vecteur de base b1 pointe vers le haut,",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Je veux dire, cela semblerait une coïncidence si l'endroit où le vecteur a atterri se trouvait également quelque part sur cette ligne.",
  "from_community_srt": "un peu à droite juste un peu et son deuxième vecteur b2 pointe en haut à gauche Maintenant, jetez un autre coup d'oeil à ce vecteur que j'ai montré plus tôt Celui que vous et moi décririons en utilisant les coordonnées [3,",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Mais certains vecteurs spéciaux restent sur leur propre étendue, ce qui signifie que l'effet de la matrice sur un tel vecteur est simplement de l'étirer ou de l'écraser, comme un scalaire.",
  "from_community_srt": "2] en utilisant nos vecteurs de base î et ĵ. Jennifer décrirait plutôt ce vecteur avec les coordonnées [5/3,",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Pour cet exemple spécifique, le vecteur de base i-hat est l’un de ces vecteurs spéciaux.",
  "from_community_srt": "1/3] ce qui signifie que la façon particulière pour arriver à ce vecteur en utilisant ses deux vecteurs de base",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "L'étendue de i-hat est l'axe des x, et à partir de la première colonne de la matrice, nous pouvons voir que i-hat se déplace jusqu'à 3 fois lui-même, toujours sur cet axe des x.",
  "from_community_srt": "est de mettre à l'échelle b1 par 5/3 et b2 par 1/3 puis les ajouter ensemble. Dans peu de temps, je vais vous montrer comment vous auriez pu trouver ces deux nombres 5/3 et 1/3.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "De plus, en raison du fonctionnement des transformations linéaires, tout autre vecteur sur l'axe des x est également simplement étiré d'un facteur 3 et reste donc sur sa propre étendue.",
  "from_community_srt": "En général, chaque fois que Jennifer utilise des coordonnées pour décrire un vecteur elle pense à sa première coordonnée comme une mise à l'échelle de b1 la deuxième coordonnée, une mise à l'échelle de b2 et elle additionne les résultats.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Un vecteur légèrement plus sournois qui reste sur sa propre étendue pendant cette transformation est moins 1, 1.",
  "from_community_srt": "Ce qu'elle obtient sera généralement complètement différent du vecteur que vous et moi penserions en ayant ces coordonnées.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Il finit par être étiré d'un facteur 2.",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "Et encore une fois, la linéarité impliquera que tout autre vecteur sur la diagonale parcourue par ce type sera simplement étiré d'un facteur 2.",
  "from_community_srt": "Pour être un peu plus précis sur la configuration ici son premier vecteur de base b1 est quelque chose que nous décririons avec le coordonnées [2, 1] et son deuxième vecteur de base b2 est quelque chose que nous décririons comme [-1,",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "Et pour cette transformation, ce sont tous les vecteurs qui ont cette propriété particulière de rester sur leur portée.",
  "from_community_srt": "1]. Mais il est important de réaliser que de son point de vue, dans son système ces vecteurs ont des coordonnées [1,",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Ceux sur l'axe des x sont étirés d'un facteur 3, et ceux sur cette ligne diagonale sont étirés d'un facteur 2.",
  "from_community_srt": "0] et [0, 1] Ils sont ce qui définit la signification des coordonnées [1, 0] et [0, 1] dans son monde.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Tout autre vecteur va subir une légère rotation pendant la transformation, et être retiré de la ligne qu'il couvre.",
  "from_community_srt": "Donc, en effet,",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Comme vous l'avez peut-être deviné maintenant, ces vecteurs spéciaux sont appelés vecteurs propres de la transformation, et chaque vecteur propre est associé à ce qu'on appelle une valeur propre, qui est simplement le facteur par lequel il est étiré ou écrasé pendant la transformation.",
  "from_community_srt": "nous parlons des langues différentes Nous voyons tous les mêmes vecteurs dans l'espace mais Jennifer utilise des mots et des nombres différents pour les décrire. Laissez-moi vous dire un mot sur la façon dont je représente les choses ici quand j'anime l'espace 2D J'utilise généralement cette grille carrée Mais cette grille est juste une construction un moyen de visualiser notre système de coordonnées et cela dépend de notre choix de base.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Bien sûr, il n'y a rien de spécial entre l'étirement et l'écrasement, ou le fait que ces valeurs propres se révèlent positives.",
  "from_community_srt": "L'espace lui-même n'a pas de grille intrinsèque. Jennifer pourrait dessiner sa propre grille ce qui serait une construction inventée pareillement",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "Dans un autre exemple, vous pourriez avoir un vecteur propre avec une valeur propre négative de 1 moitié, ce qui signifie que le vecteur est inversé et écrasé d'un facteur de 1 moitié.",
  "from_community_srt": "qui n'est rien de plus qu'un outil visuel pour aider à suivre la signification de ses coordonnées. Son origine, cependant,",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Mais ce qui est important ici, c'est qu'il reste sur la ligne qu'il s'étend sans en sortir.",
  "from_community_srt": "serait en fait alignée avec la nôtre puisque tout le monde est d'accord sur ce que les coordonnées [0, 0] doivent signifier. C'est ce que vous obtenez lorsque vous mettez à l'échelle un vecteur par 0.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Pour avoir un aperçu de la raison pour laquelle cela pourrait être une chose utile à considérer, envisagez une rotation tridimensionnelle.",
  "from_community_srt": "Mais la direction de ses axes et l'espacement de ses lignes de la grille sera différent, en fonction de son choix de vecteurs de base.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Si vous pouvez trouver un vecteur propre pour cette rotation, un vecteur qui reste sur sa propre étendue, ce que vous avez trouvé est l'axe de rotation.",
  "from_community_srt": "Donc, après tout cela mis en place une question assez naturelle à poser est Comment nous traduisons entre les systèmes de coordonnées ? Si, par exemple,",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "Et il est beaucoup plus facile de penser à une rotation 3D en termes d'un axe de rotation et d'un angle de rotation, plutôt que de penser à la matrice 3x3 complète associée à cette transformation.",
  "from_community_srt": "Jennifer décrit un vecteur avec des coordonnées [-1, 2] que serait-ce dans notre système de coordonnées ? Comment traduiriez-vous de sa langue à la nôtre ? Eh bien,",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "Dans ce cas, d'ailleurs, la valeur propre correspondante devrait être 1, puisque les rotations ne s'étirent ni n'écrasent jamais quoi que ce soit, donc la longueur du vecteur resterait la même.",
  "from_community_srt": "ce que disent nos coordonnées est que ce vecteur est -1*b1 + 2*b2. Et de notre point de vue b1 a les coordonnées [2,",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Ce modèle apparaît souvent en algèbre linéaire.",
  "from_community_srt": "1] et b2 a les coordonnées [-1,",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Avec toute transformation linéaire décrite par une matrice, vous pouvez comprendre ce qu'elle fait en lisant les colonnes de cette matrice comme points d'atterrissage pour les vecteurs de base.",
  "from_community_srt": "1] Nous pouvons donc calculer -1*b1 + 2*b2 comme ils sont représentés dans notre système de coordonnées",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Mais souvent, une meilleure façon d'aller au cœur de ce que fait réellement la transformation linéaire, moins dépendante de votre système de coordonnées particulier, est de trouver les vecteurs propres et les valeurs propres.",
  "from_community_srt": "Et en effectuant vous obtenez un vecteur avec des coordonnées [-4, 1] Donc, voilà comment nous décririons le vecteur qu'elle pense comme [-1,",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Je ne couvrirai pas ici tous les détails sur les méthodes de calcul des vecteurs propres et des valeurs propres, mais je vais essayer de donner un aperçu des idées informatiques les plus importantes pour une compréhension conceptuelle.",
  "from_community_srt": "2] Ce processus ici de mise à l'échelle de chacun de ses vecteurs de base par les coordonnées correspondantes de certains vecteurs puis en les ajoutant ensemble pourrait sembler un peu familier C'est un multiplication de matrices vectorielles avec une matrice dont les colonnes représentent les vecteurs de base de Jennifer dans notre langue En fait,",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Symboliquement, voici à quoi ressemble l'idée d'un vecteur propre.",
  "from_community_srt": "une fois que vous comprenez la multiplication de matrices vectorielles comme une application de certaine transformation linéaire",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A est la matrice représentant une transformation, avec v comme vecteur propre, et lambda est un nombre, à savoir la valeur propre correspondante.",
  "from_community_srt": "disons, en ayant regardé ce que je vous ai dit qui la vidéo la plus importante de cette série, le chapitre 3. Il y a une façon assez intuitive de penser à ce qui se passe ici.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Ce que dit cette expression, c'est que le produit matrice-vecteur, A fois v, donne le même résultat qu'une simple mise à l'échelle du vecteur propre v par une certaine valeur lambda.",
  "from_community_srt": "Une matrice dont les colonnes représentent les vecteurs de base de Jennifer peut être considéré comme une transformation qui déplace nos vecteurs de base, î et ĵ les choses auquelles nous pensons quand nous disons [1,0] et [0,",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Ainsi, trouver les vecteurs propres et leurs valeurs propres d'une matrice A revient à trouver les valeurs de v et lambda qui rendent cette expression vraie.",
  "from_community_srt": "1] vers les vecteurs de base de Jennifer les choses auxquelles elle pense quand elle dit [1,0] et [0,",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "C'est un peu difficile à utiliser au début, car le côté gauche représente la multiplication matrice-vecteur, mais le côté droit ici est la multiplication scalaire-vecteur.",
  "from_community_srt": "1] Pour montrer comment cela fonctionne regardons étape par étape ce que signifierait de prendre le vecteur que nous pensons avoir les coordonnées [-1, 2] et en appliquant cette transformation.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Commençons donc par réécrire ce membre de droite comme une sorte de multiplication matrice-vecteur, en utilisant une matrice qui a pour effet de mettre à l'échelle n'importe quel vecteur par un facteur lambda.",
  "from_community_srt": "Avant la transformation linéaire nous pensons à ce vecteur comme une certaine combinaison linéaire de notre base vecteurs -1*î + 2*ĵ.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Les colonnes d'une telle matrice représenteront ce qui arrive à chaque vecteur de base, et chaque vecteur de base est simplement multiplié par lambda, donc cette matrice aura le nombre lambda sur la diagonale, avec des zéros partout ailleurs.",
  "from_community_srt": "Et la caractéristique clé d'une transformation linéaire est que le vecteur résultant sera cette même combinaison linéaire mais des les nouveaux vecteurs de base -1 fois l'endroit où î atterrit + 2 fois l'endroit où atterrit ĵ. Alors,",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "La façon courante d'écrire ce type est de prendre en compte ce lambda et de l'écrire sous la forme lambda fois i, où i est la matrice d'identité avec des 1 sur la diagonale.",
  "from_community_srt": "ce que fait cette matrice est transformer notre fausse idée de ce que Jennifer veux dire dans le vrai vecteur auquel elle réfère.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Les deux côtés ressemblant à une multiplication matrice-vecteur, nous pouvons soustraire ce côté droit et factoriser le v.",
  "from_community_srt": "Je me souviens que quand je commençais à apprendre ça ça me paraissait toujours un peu à l'envers. Géométriquement, cette matrice transforme notre grille dans la grille de Jennifer.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Nous avons donc maintenant une nouvelle matrice, A moins lambda fois l'identité, et nous recherchons un vecteur v tel que cette nouvelle matrice multipliée par v donne le vecteur zéro.",
  "from_community_srt": "Mais numériquement, ça traduit un vecteur décrit dans sa langue à notre langue. Ce qui m'a finalement fait comprendre était en prenant que la transformation prend notre fausse idée de ce que veut dire Jennifer,",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Maintenant, cela sera toujours vrai si v lui-même est le vecteur zéro, mais c'est ennuyeux.",
  "from_community_srt": "le vecteur que nous obtenons en utilisant les mêmes coordonnées mais dans notre système et le transforme en le vecteur qu'elle voulait vraiment dire.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Ce que nous voulons, c'est un vecteur propre non nul.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "Et si vous regardez les chapitres 5 et 6, vous saurez que la seule façon pour le produit d'une matrice avec un vecteur non nul de devenir nul est si la transformation associée à cette matrice écrase l'espace dans une dimension inférieure.",
  "from_community_srt": "Qu'en est-il de l'inverse ? Dans l'exemple que j'ai utilisé plus tôt cette vidéo quand j'ai le vecteur avec les coordonnées [3,2] dans notre système Comment ai-je calculé qu'il aurait des coordonnées [5/3,",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "Et cette squishification correspond à un déterminant nul pour la matrice.",
  "from_community_srt": "1/3] dans le système Jennifer ? Vous commencez avec cette matrice de passage",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Pour être concret, disons que votre matrice A comporte les colonnes 2, 1 et 2, 3, et pensez à soustraire un montant variable, lambda, de chaque entrée diagonale.",
  "from_community_srt": "qui traduit la langue de Jennifer dans la nôtre ensuite vous prenez son inverse. Rappelez-vous,",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Imaginez maintenant que vous modifiez lambda, en tournant un bouton pour modifier sa valeur.",
  "from_community_srt": "l'inverse d'une transformation est une nouvelle transformation qui correspond à jouer la première à l'envers.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "À mesure que cette valeur de lambda change, la matrice elle-même change, et donc le déterminant de la matrice change.",
  "from_community_srt": "En pratique, surtout quand vous travaillez en plus de deux dimensions vous utiliseriez un ordinateur pour calculer la matrice qui représente cette inverse.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Le but ici est de trouver une valeur de lambda qui rendra ce déterminant nul, ce qui signifie que la transformation modifiée écrase l'espace dans une dimension inférieure.",
  "from_community_srt": "Dans ce cas, l'inverse de la matrice de passage qui a la base de Jennifer comme colonnes se trouve avoir pour colonnes [1/3,",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "Dans ce cas, le point idéal survient lorsque lambda est égal à 1.",
  "from_community_srt": "-1/3] et [1/3, 2/3] Ainsi,",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Bien entendu, si nous avions choisi une autre matrice, la valeur propre ne serait pas nécessairement 1.",
  "from_community_srt": "par exemple pour voir à quoi ressemble le vecteur [3,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Le point idéal pourrait être atteint à une autre valeur de lambda.",
  "from_community_srt": "2] dans le système de Jennifer nous multiplions la matrice de passage inverse par le vecteur [3,",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "C'est donc beaucoup, mais voyons ce que cela veut dire.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Lorsque lambda est égal à 1, la matrice A moins lambda multipliée par l'identité écrase l'espace sur une ligne.",
  "from_community_srt": "2] ce qui fait [5/3, 1/3] Alors,",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Cela signifie qu'il existe un vecteur v non nul tel que A moins lambda fois l'identité fois v est égal au vecteur zéro.",
  "from_community_srt": "en un mot c'est comme ça qu'on traduit la description de vecteurs individuels entre les systèmes de coordonnées dans un sens et dans l'autre. La matrice dont les colonnes représentent les vecteurs de base de Jennifer mais écrits dans nos coordonnées",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "Et rappelez-vous, la raison pour laquelle nous nous soucions de cela est que cela signifie que A fois v est égal à lambda fois v, ce qui peut être lu comme disant que le vecteur v est un vecteur propre de A, restant sur sa propre étendue pendant la transformation A.",
  "from_community_srt": "traduit les vecteurs de sa langue en notre langue. Et la matrice inverse fait le contraire. Mais les vecteurs ne sont pas les seules choses que nous décrivons en utilisant des coordonnées.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "Dans cet exemple, la valeur propre correspondante est 1, donc v resterait simplement fixe en place.",
  "from_community_srt": "Pour la prochaine partie il est important que vous soyez tous à l'aise pour représenter des transformations avec des matrices et que vous savez comment la multiplication matricielle correspond à composer des transformations successives.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Faites une pause et réfléchissez si vous devez vous assurer que ce raisonnement vous convient.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "C'est le genre de chose que j'ai mentionné dans l'introduction.",
  "from_community_srt": "Mettez vraiment en pause et jetez un œil aux chapitres 3 et 4 si vous n'êtes pas à l'aise avec un de ces choses.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Si vous n'aviez pas une solide compréhension des déterminants et de la raison pour laquelle ils se rapportent à des systèmes d'équations linéaires ayant des solutions non nulles, une expression comme celle-ci semblerait complètement inattendue.",
  "from_community_srt": "Considérez une transformation linéaire comme une rotation de 90 ° dans le sens antihoraire. Quand vous et moi représentons cela avec a matrice nous suivons où les vecteurs de base î et ĵ se retrouvent.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Pour voir cela en action, reprenons l'exemple du début, avec une matrice dont les colonnes sont 3, 0 et 1, 2.",
  "from_community_srt": "î finit à l'endroit avec les coordonnées [0, 1] et ĵ finit à l'endroit avec les coordonnées [-1, 0] Donc,",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Pour savoir si une valeur lambda est une valeur propre, soustrayez-la des diagonales de cette matrice et calculez le déterminant.",
  "from_community_srt": "ces coordonnées deviennent les colonnes de notre matrice mais cette représentation est fortement lié à notre choix de vecteurs de base",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "En faisant cela, nous obtenons un certain polynôme quadratique en lambda, 3 moins lambda fois 2 moins lambda.",
  "from_community_srt": "du fait que nous suivons î et ĵ en premier lieu au fait que nous enregistrons où ils aterrissent dans notre propre système de coordonnées. Comment Jennifer décrirait-elle cette même rotation de l'espace de 90° ? Vous pourriez être tenté de juste",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Puisque lambda ne peut être une valeur propre que si ce déterminant est nul, vous pouvez conclure que les seules valeurs propres possibles sont lambda égale à 2 et lambda égale à 3.",
  "from_community_srt": "traduire les colonnes de notre matrice de rotation dans la langue de Jennifer. Mais ce n'est pas tout à fait correct.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Pour déterminer quels sont les vecteurs propres qui ont réellement l'une de ces valeurs propres, disons que lambda est égal à 2, branchez cette valeur de lambda à la matrice, puis déterminez pour quels vecteurs cette matrice modifiée en diagonale envoie à zéro.",
  "from_community_srt": "Ces colonnes représentent où nos vecteurs de base î et ĵ vont. Mais la matrice que veut Jennifer devrait représenter où ses vecteurs de base atterrissent et il doit décrire ces points dans sa langue. Voici une façon commune de penser à comment ça marche.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Si vous calculiez cela comme vous le feriez avec n'importe quel autre système linéaire, vous verriez que les solutions sont tous les vecteurs sur la diagonale engendrée par moins 1, 1.",
  "from_community_srt": "Commencez avec n'importe quel vecteur écrit dans la langue de Jennifer. Plutôt que d'essayer de suivre ce qui se passe dans sa langue d'abord,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Cela correspond au fait que la matrice inchangée, 3, 0, 1, 2, a pour effet d'étirer tous ces vecteurs d'un facteur 2.",
  "from_community_srt": "nous allons le traduire dans notre langue en utilisant la matrice passage celui dont les colonnes représentent ses vecteurs de base dans notre langue. Cela nous donne le même vecteur mais maintenant écrit dans notre langue.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Désormais, une transformation 2D n'a pas besoin d'avoir de vecteurs propres.",
  "from_community_srt": "Ensuite, appliquez la matrice de transformation à ce que vous obtenez en le multipliant à gauche.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Par exemple, considérons une rotation de 90 degrés.",
  "from_community_srt": "Cela nous dit où ce vecteur atterrit mais toujours dans notre langue.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Cela n'a pas de vecteurs propres puisqu'il fait pivoter chaque vecteur hors de sa propre étendue.",
  "from_community_srt": "Donc, comme dernière étape appliquez la matrice de passage inverse multipliée à gauche comme d'habitude",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Si vous essayez réellement de calculer les valeurs propres d’une rotation comme celle-ci, remarquez ce qui se passe.",
  "from_community_srt": "pour obtenir le vecteur transformé mais maintenant dans la langue de Jennifer.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Sa matrice comporte les colonnes 0, 1 et moins 1, 0.",
  "from_community_srt": "Puisque nous pourrions le faire avec n'importe quel vecteur écrit dans sa langue d'abord, en appliquant le changement de base puis, la transformation puis,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Soustrayez lambda des éléments diagonaux et recherchez quand le déterminant est zéro.",
  "from_community_srt": "le changement inverse de base Cette composition de trois matrices nous donne la matrice de transformation dans la langue de Jennifer.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "Dans ce cas, vous obtenez le polynôme lambda au carré plus 1.",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Les seules racines de ce polynôme sont les nombres imaginaires i et i négatif.",
  "from_community_srt": "Elle prend en entrée un vecteur dans la langue de Jennifer et ressort la version transformée de ce vecteur dans sa langue",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Le fait qu’il n’y ait pas de solutions numériques réelles indique qu’il n’y a pas de vecteurs propres.",
  "from_community_srt": "Pour cet exemple spécifique quand les vecteurs de base de Jennifer sont [2, 1] et [-1,",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Un autre exemple assez intéressant qui mérite d’être gardé à l’esprit est une cisaille.",
  "from_community_srt": "1] dans notre langue et quand la transformation est une rotation de 90° le produit de ces trois matrices si vous l'effectuez a les colonnes [1/3,",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Cela fixe i-hat en place et déplace j-hat 1, de sorte que sa matrice a les colonnes 1, 0 et 1, 1.",
  "from_community_srt": "5/3] et [-2/3, -1/3] Donc,",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Tous les vecteurs sur l'axe des x sont des vecteurs propres de valeur propre 1 puisqu'ils restent fixes.",
  "from_community_srt": "si Jennifer multiplie cette matrice par les coordonnées d'un vecteur dans son système il retournera la version pivotée de 90° de ce vecteur",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "En fait, ce sont les seuls vecteurs propres.",
  "from_community_srt": "exprimé dans son système de coordonnées.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Lorsque vous soustrayez lambda des diagonales et calculez le déterminant, vous obtenez 1 moins lambda au carré.",
  "from_community_srt": "En général, chaque fois que vous voyez une expression comme A^(-1) M A cela suggère une sorte d'empathie mathématique.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "Et la seule racine de cette expression est lambda égal à 1.",
  "from_community_srt": "Cette matrice du milieu représente une certaine transformation, comme vous la voyez et les deux matrices externes représentent l'empathie,",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Cela correspond à ce que nous voyons géométriquement, à savoir que tous les vecteurs propres ont une valeur propre 1.",
  "from_community_srt": "le changement de perspective et le produit matriciel complet représente cette même transformation",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Gardez cependant à l’esprit qu’il est également possible d’avoir une seule valeur propre, mais avec plus qu’une simple ligne remplie de vecteurs propres.",
  "from_community_srt": "mais comme quelqu'un d'autre le voit. Pour ceux d'entre vous qui se demandent pourquoi nous nous soucions de systèmes de coordonnées alternatifs",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Un exemple simple est une matrice qui met tout à l’échelle par 2.",
  "from_community_srt": "la prochaine vidéo sur les vecteurs propres et valeurs propres donnera un exemple vraiment important de cela.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "La seule valeur propre est 2, mais chaque vecteur du plan devient un vecteur propre avec cette valeur propre.",
  "from_community_srt": "À la prochaine !",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "C’est maintenant un autre bon moment pour faire une pause et réfléchir à tout cela avant de passer au dernier sujet.",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Je veux terminer ici avec l'idée d'une base propre, qui s'appuie fortement sur les idées de la dernière vidéo.",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Jetez un œil à ce qui se passe si nos vecteurs de base se révèlent être des vecteurs propres.",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Par exemple, peut-être que i-hat est mis à l'échelle de moins 1 et j-hat est mis à l'échelle de 2.",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "En écrivant leurs nouvelles coordonnées sous forme de colonnes d'une matrice, notez que ces multiples scalaires, négatifs 1 et 2, qui sont les valeurs propres de i-hat et j-hat, se trouvent sur la diagonale de notre matrice et que chaque autre entrée est un 0. .",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Chaque fois qu'une matrice a des zéros partout ailleurs que sur la diagonale, on l'appelle, assez raisonnablement, une matrice diagonale.",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "Et la façon d'interpréter cela est que tous les vecteurs de base sont des vecteurs propres, les entrées diagonales de cette matrice étant leurs valeurs propres.",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Il y a beaucoup de choses qui rendent les matrices diagonales beaucoup plus agréables à utiliser.",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Le plus important est qu'il est plus facile de calculer ce qui se passera si vous multipliez cette matrice par elle-même plusieurs fois.",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Puisque toutes ces matrices ne font que mettre à l'échelle chaque vecteur de base par une valeur propre, appliquer cette matrice plusieurs fois, disons 100 fois, va simplement correspondre à la mise à l'échelle de chaque vecteur de base par la puissance 100 de la valeur propre correspondante.",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "En revanche, essayez de calculer la puissance 100 d’une matrice non diagonale.",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Vraiment, essayez-le un instant.",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "C'est un cauchemar.",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Bien sûr, vous aurez rarement la chance que vos vecteurs de base soient également des vecteurs propres.",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Mais si votre transformation comporte un grand nombre de vecteurs propres, comme celui du début de cette vidéo, suffisamment pour que vous puissiez choisir un ensemble qui s'étend sur tout l'espace, vous pouvez alors modifier votre système de coordonnées afin que ces vecteurs propres soient vos vecteurs de base.",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "J'ai parlé du changement de base dans la dernière vidéo, mais je vais faire ici un rappel très rapide de la façon d'exprimer une transformation actuellement écrite dans notre système de coordonnées dans un système différent.",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Prenez les coordonnées des vecteurs que vous souhaitez utiliser comme nouvelle base, ce qui signifie dans ce cas nos deux vecteurs propres, puis faites de ces coordonnées les colonnes d'une matrice, connue sous le nom de matrice de changement de base.",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Lorsque vous prenez en sandwich la transformation d'origine, en plaçant la matrice de changement de base à sa droite et l'inverse de la matrice de changement de base à sa gauche, le résultat sera une matrice représentant cette même transformation, mais du point de vue des nouvelles coordonnées des vecteurs de base. système.",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "L’intérêt de faire cela avec les vecteurs propres est que cette nouvelle matrice est garantie d’être diagonale avec ses valeurs propres correspondantes sur cette diagonale.",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "En effet, cela représente un travail dans un système de coordonnées où les vecteurs de base sont mis à l'échelle lors de la transformation.",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Un ensemble de vecteurs de base qui sont également des vecteurs propres est appelé, encore une fois, assez raisonnablement, une base propre.",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Ainsi, si, par exemple, vous deviez calculer la 100e puissance de cette matrice, il serait beaucoup plus facile de passer à une base propre, de calculer la 100e puissance dans ce système, puis de revenir à notre système standard.",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Vous ne pouvez pas faire cela avec toutes les transformations.",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Une cisaille, par exemple, n'a pas suffisamment de vecteurs propres pour couvrir tout l'espace.",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Mais si vous pouvez trouver une base propre, cela rend les opérations matricielles vraiment agréables.",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Pour ceux d'entre vous qui souhaitent résoudre un casse-tête assez soigné pour voir à quoi cela ressemble en action et comment il peut être utilisé pour produire des résultats surprenants, je vais laisser une invite ici à l'écran.",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Cela demande un peu de travail, mais je pense que vous l'apprécierez.",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "La prochaine et dernière vidéo de cette série portera sur les espaces vectoriels abstraits.",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]