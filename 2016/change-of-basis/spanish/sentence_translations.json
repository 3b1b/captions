[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Los vectores propios y los valores propios es uno de esos temas que muchos estudiantes encuentran particularmente poco intuitivo.",
  "from_community_srt": "Si tengo un vector en R2 tenemos una manera estándar de describirlo con coordenadas. En este caso, el vector tiene coordenadas [3,2] Lo que significa que ir desde su origen a su punta requiere moverse 3 unidades hacia la derecha y 2 hacia arriba.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Preguntas como por qué hacemos esto y qué significa realmente, con demasiada frecuencia quedan flotando en un mar de cálculos sin respuesta.",
  "from_community_srt": "Pero la forma más correcta en algebra lineal de describir coordenadas es la de pensar en cada uno de estos números como escalares:",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "Y a medida que publiqué los videos de esta serie, muchos de ustedes han comentado que esperan visualizar este tema en particular.",
  "from_community_srt": "algo que estira o encoge vectores.",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Sospecho que la razón de esto no es tanto que las cosas propias sean particularmente complicadas o estén mal explicadas.",
  "from_community_srt": "Pensamos en esa primera coordenada como el escalar de i, el vector de longitud 1 que apunta hacia la derecha,",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "De hecho, es comparativamente sencillo y creo que la mayoría de los libros lo explican muy bien.",
  "from_community_srt": "mientras que la segunda coordenada es el escalar de j, el vector de longuitud 1 que apunta hacia arriba.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "El problema es que sólo tiene sentido si tienes una comprensión visual sólida de muchos de los temas que lo preceden.",
  "from_community_srt": "La suma vectorial de esos dos vectores escalados es lo que describen dichas coordenadas.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Lo más importante aquí es que sepas pensar en las matrices como transformaciones lineales, pero también debes sentirte cómodo con cosas como determinantes, sistemas lineales de ecuaciones y cambios de base.",
  "from_community_srt": "Puedes pensar en estos dos vectores especiales como capsulas de toda la información implicita a nuestro sistema de coordenadas. El echo de que el primer número indica movimiento hacia la derecha, el segundo número movimiento hacia arriba, cuanta distancia es exactamente una unidad...",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "La confusión sobre las cosas propias suele tener más que ver con una base inestable en uno de estos temas que con los vectores propios y los valores propios.",
  "from_community_srt": "Todo eso esta asociado a la elección de i y j como los vectores cuyas coordenadas se supone que debemos escalar",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Para empezar, considere alguna transformación lineal en dos dimensiones, como la que se muestra aquí.",
  "from_community_srt": "Cualquier forma de traducir entre vectores y un conjunto de números es lo que llamamos un sistema de coordenadas. y los dos vectores especiales,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Mueve el vector base i-hat a las coordenadas 3, 0 y j-hat a 1, 2.",
  "from_community_srt": "i y j,son los llamados vectores base de nuestro sistema de coordenadas estándar.",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Entonces se representa con una matriz cuyas columnas son 3, 0 y 1, 2.",
  "from_community_srt": "De lo que me gustaría hablar aquí es de la idea de utilizar un conjunto distinto de vectores base.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Concéntrese en lo que le hace a un vector en particular y piense en el alcance de ese vector, la línea que pasa por su origen y su punta.",
  "from_community_srt": "Por ejemplo, supongamos que tienes una amiga, Jennifer que usa un conjunto distinto de vectores base a los que llamaré b1 y b2 Su primer vector base b1 apunta hacia arriba y un poco hacia la derecha",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "La mayoría de los vectores quedarán fuera de su alcance durante la transformación.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Quiero decir, parecería bastante coincidente si el lugar donde aterrizó el vector también estuviera en algún lugar de esa línea.",
  "from_community_srt": "y el segundo apunta hacia la izquierda y hacia arriba. Fíjate en el vector que he mostrado antes ese que tú y yo describiríamos con las coordenadas [3,2]",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Pero algunos vectores especiales permanecen en su propio lapso, lo que significa que el efecto que tiene la matriz sobre dicho vector es simplemente estirarlo o aplastarlo, como un escalar.",
  "from_community_srt": "al usar nuestros vectores base i y j. Jennifer sin embargo describiria a este vector con las coordenadas [5/3,1/3] Lo que esto significa es que la forma particular con la que obtenemos ese vector",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Para este ejemplo específico, el vector base i-hat es uno de esos vectores especiales.",
  "from_community_srt": "usando sus dos vectores base es escalando b1 por 5/3,",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "El intervalo de i-hat es el eje x, y desde la primera columna de la matriz, podemos ver que i-hat se mueve 3 veces él mismo, todavía en ese eje x.",
  "from_community_srt": "escalando b2 por 1/3 y después sumando ambos resultados. En unos momentos os enseñaré como podríais haber calculado esos dos números:",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Es más, debido a la forma en que funcionan las transformaciones lineales, cualquier otro vector en el eje x también se estira en un factor de 3 y, por lo tanto, permanece en su propio tramo.",
  "from_community_srt": "5/3 y 1/3. En general, cuando Jennifer usa coordenadas para describir un vector piensa en la primera coordenada como un escalar de b1, la segunda coordenada como un escalar de b2 y suma los resultados.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Un vector un poco más astuto que permanece en su propio lapso durante esta transformación es negativo 1, 1.",
  "from_community_srt": "Lo que ella obtiene será completamente diferente al vector que tú y yo pensaríamos con esas coordenadas.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Termina estirándose por un factor de 2.",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "Y nuevamente, la linealidad implicará que cualquier otro vector en la línea diagonal trazada por este tipo simplemente se estirará en un factor de 2.",
  "from_community_srt": "Siendo más precisos en este ejemplo su vector base b1 es aquel que nosotros describiríamos con las coordenadas [2,1] y su segundo vector base b2 es aquel que describiríamos como [-1,1].",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "Y para esta transformación, esos son todos los vectores con esta propiedad especial de permanecer en su tramo.",
  "from_community_srt": "Pero es importante darnos cuenta que desde su perspectiva en su sistema esos vectores tienen las coordenadas [1,0] y [0,1]",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Los que están en el eje x se estiran en un factor de 3 y los que están en esta línea diagonal se estiran en un factor de 2.",
  "from_community_srt": "Son lo que define para ella el significado de las coordenadas [1,0] y [0,1].",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Cualquier otro vector será rotado un poco durante la transformación, eliminado de la línea que abarca.",
  "from_community_srt": "Así que, en efecto, hablamos idiomas diferentes.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Como ya habrás adivinado, estos vectores especiales se llaman vectores propios de la transformación, y cada vector propio tiene asociado lo que se llama un valor propio, que es simplemente el factor por el cual se estira o se aplasta durante la transformación.",
  "from_community_srt": "Miramos a los mismos vectores pero Jennifer usa palabras y números diferentes para describirlos. Permitidme una aclaración rápida sobre como represento las cosas aquí. Cuando animo el espacio bidimensional (R2) suelo utilizar esta cuadricula. pero esa cuadricula es solo una interpretación, una forma de visualizar nuestro sistema de coordenadas y por tanto depende de nuestra elección de la base.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Por supuesto, no hay nada especial en estirar versus aplastar, o en el hecho de que estos valores propios resulten ser positivos.",
  "from_community_srt": "El espacio en si mismo no tiene ninguna cuadricula intrinsica. Jennifer podría dibujar su propia cuadricula que sería igualmente tan sólo una interpretación,",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "En otro ejemplo, podría tener un vector propio con valor propio negativo 1 mitad, lo que significa que el vector se voltea y se aplasta en un factor de 1 mitad.",
  "from_community_srt": "nada más que una herramienta visual que ayuda a captar el significado de sus coordenadas.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Pero la parte importante aquí es que permanece en la línea que se extiende sin salirse de ella.",
  "from_community_srt": "Sin embargo su origen coincidiría con el nuestro ya que todo el mundo está de acuerdo en lo que deben significar las coordenadas [0,0]: es lo que obtienes cuando escalas cualquier vector por 0.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Para tener una idea de por qué podría ser útil pensar en esto, considere una rotación tridimensional.",
  "from_community_srt": "Pero la dirección de sus ejes y el espacio entre sus líneas de cuadrícula será diferente dependiendo de su elección de los vectores de la base",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Si puedes encontrar un vector propio para esa rotación, un vector que permanece en su propio lapso, lo que has encontrado es el eje de rotación.",
  "from_community_srt": "Así que tras presentar todo esto la pregunta natural que surge es: ¿Cómo traducimos entre sistemas de coordenadas? Si, por ejemplo,",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "Y es mucho más fácil pensar en una rotación 3D en términos de algún eje de rotación y un ángulo según el cual gira, en lugar de pensar en la matriz completa de 3x3 asociada con esa transformación.",
  "from_community_srt": "Jennifer describe el vector con coordenadas [-1,2] ¿Cuál sería ese vector en nustro sistema de coordenadas? ¿Cómo traducimos de su lenguaje al nuestro? Bueno,",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "En este caso, por cierto, el valor propio correspondiente tendría que ser 1, ya que las rotaciones nunca estiran ni aplastan nada, por lo que la longitud del vector seguiría siendo la misma.",
  "from_community_srt": "lo que dicen sus coordenadas Es que el vector es -1b1 + 2b2. Y desde nuestra perspectiva b1 tiene coordenadas [2,1] y b2 tiene coordenadas [-1,1].",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Este patrón aparece mucho en álgebra lineal.",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Con cualquier transformación lineal descrita por una matriz, se puede entender lo que hace leyendo las columnas de esta matriz como puntos de aterrizaje para los vectores base.",
  "from_community_srt": "Así que podemos calcular -1b1 + 2b2 por como están representados en nuestro sistema de coordenadas.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Pero a menudo, una mejor manera de llegar al núcleo de lo que realmente hace la transformación lineal, menos dependiente de su sistema de coordenadas particular, es encontrar los vectores propios y los valores propios.",
  "from_community_srt": "Y operando así Obtienes el vector con coordenadas  [-4,1]. Así es como nosotros describiríamos el vector en el que ella piensa como [-1,2].",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "No cubriré todos los detalles sobre los métodos para calcular vectores propios y valores propios aquí, pero intentaré dar una visión general de las ideas computacionales que son más importantes para una comprensión conceptual.",
  "from_community_srt": "Este proceso de escalar cada vector de su base por las coordenadas correspondientes a un vector y después sumarlas se siente familiar. Es una multiplicación matriz-vector en la que las columnas de la matriz representan los vectores de la base de  Jennifer en nuestro idioma. De echo,",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Simbólicamente, así es como se ve la idea de un vector propio.",
  "from_community_srt": "una vez que entiendes la multiplicación matriz-vector como la aplicación de una aplicación lineal,",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A es la matriz que representa alguna transformación, con v como vector propio y lambda es un número, es decir, el valor propio correspondiente.",
  "from_community_srt": "como por ejemplo al ver el vídeo que os he dicho es el más importante de la serie, el capítulo 3, hay una forma bastante intuitiva de pensar en lo que está ocurriendo aquí.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Lo que esta expresión dice es que el producto matriz-vector, A multiplicado por v, da el mismo resultado que simplemente escalar el vector propio v por algún valor lambda.",
  "from_community_srt": "Una matriz cuyas columnas representan los vectores de la base de Jennifer puede ser interpretada como una transformación que mueve nuestros vectores base, i y j, las cosas en las que nosotros pensamos al decir [1,0] y [0,1],",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Entonces, encontrar los vectores propios y sus valores propios de una matriz A se reduce a encontrar los valores de v y lambda que hacen que esta expresión sea verdadera.",
  "from_community_srt": "a los vectores de la base de Jennifer, las cosas en las que ella piensa al decir [1,0] y [0,1].",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "Es un poco incómodo trabajar con él al principio, porque el lado izquierdo representa la multiplicación de matriz-vector, pero el lado derecho aquí es la multiplicación de vector escalar.",
  "from_community_srt": "Para mostraros como esto funciona vamos a ver lo que significaría tomar el vector que nosotros pensamos tiene las coordenadas [-1,2] y aplicar esa transformación.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Entonces, comencemos reescribiendo ese lado derecho como una especie de multiplicación matriz-vector, usando una matriz que tiene el efecto de escalar cualquier vector por un factor lambda.",
  "from_community_srt": "Antes de la aplicación lineal pensamos en este vector como una combinación de nuestros vectores base     -1i + 2j",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Las columnas de dicha matriz representarán lo que sucede con cada vector base, y cada vector base simplemente se multiplica por lambda, por lo que esta matriz tendrá el número lambda en la diagonal, con ceros en el resto.",
  "from_community_srt": "Y la característica clave de una aplicación lineal es que el vector resultante será esa misma combinación lineal pero de los nuevos vectores base -1 veces el vector en el que termina i + 2 veces el vector en el que termina j",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "La forma común de escribir este tipo es factorizar esa lambda y escribirla como lambda multiplicada por i, donde i es la matriz de identidad con 1 en la diagonal.",
  "from_community_srt": "Así que lo que hace esta matriz es transformar nuestra equivocada interpretación de lo que nos decía Jennifer en el vector al que realmente se refería.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Como ambos lados parecen una multiplicación de matriz-vector, podemos restar ese lado derecho y factorizar v.",
  "from_community_srt": "Recuerdo que cuando aprendí esto por primera vez sentía como que funcionaba al revés de como debería.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Entonces, lo que ahora tenemos es una nueva matriz, A menos lambda multiplicada por la identidad, y estamos buscando un vector v tal que esta nueva matriz multiplicada por v dé el vector cero.",
  "from_community_srt": "Geometricamente esta matriz cambia nuestra cuadricula en la de Jennifer pero numéricamente traduce un vector de su idioma a el nuestro. Lo que hizo que finalmente tuviera sentido para mi fue esta forma de pensar en como lleva nuestra interpretación incorrecta de lo que Jennifer quiere decir,",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Ahora bien, esto siempre será cierto si v mismo es el vector cero, pero eso es aburrido.",
  "from_community_srt": "el vector que obtenemos usando las mismas coordenadas pero en nuestro sistema de referencia",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Lo que queremos es un vector propio distinto de cero.",
  "from_community_srt": "y después transformándolo en el vector al que realmente se refería.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "Y si miras los capítulos 5 y 6, sabrás que la única forma en que es posible que el producto de una matriz con un vector distinto de cero se convierta en cero es si la transformación asociada con esa matriz reduce el espacio a una dimensión inferior.",
  "from_community_srt": "¿Qué ocurre en el sentido contrario? En el ejemplo que he usado al principio del video cuando tenemos un vector de coordenadas [3,2] en nuestro sistema ¿Cómo he calculado que tendría las coordenadas [5/3,",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "Y esa compresión corresponde a un determinante cero para la matriz.",
  "from_community_srt": "1/3] en el sistema de Jennifer? Empezamos con una matriz de cambio de base",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Para ser concretos, digamos que su matriz A tiene las columnas 2, 1 y 2, 3, y piense en restar una cantidad variable, lambda, de cada entrada diagonal.",
  "from_community_srt": "que traduce el idioma de Jennifer al nuestro y después tomamos la inversa Recuerda,",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Ahora imagina ajustar lambda, girando una perilla para cambiar su valor.",
  "from_community_srt": "la inversa de una transformación es una nueva transformación que corresponde al cambio de la primera a la inversa.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "A medida que cambia ese valor de lambda, la matriz misma cambia y, por lo tanto, cambia el determinante de la matriz.",
  "from_community_srt": "En practica, especialmente cuando trabajas en más de dos dimensiones usarías un ordenador (Con Mathematica aibalahostia) para calcular la matriz inversa.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "El objetivo aquí es encontrar un valor de lambda que haga que este determinante sea cero, lo que significa que la transformación modificada aplasta el espacio a una dimensión inferior.",
  "from_community_srt": "En este caso, la matriz inversa de la matriz de cambio de base que tiene las bases de Jennifer como columnas termina teniendo esta forma.",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "En este caso, el punto óptimo se produce cuando lambda es igual a 1.",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Por supuesto, si hubiéramos elegido otra matriz, el valor propio no necesariamente sería 1.",
  "from_community_srt": "Así por ejemplo para ver que forma tiene el vector [3,2] en el sistema de Jennifer multiplicamos la matriz inversa de cambio de base por el vector [3,2]",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "El punto óptimo podría alcanzarse con algún otro valor de lambda.",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Esto es mucho, pero analicemos lo que dice.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Cuando lambda es igual a 1, la matriz A menos lambda multiplicada por la identidad aplasta el espacio en una línea.",
  "from_community_srt": "lo que nos da como resultado [5/3,1/3] Asi es como,",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Eso significa que hay un vector v distinto de cero tal que A menos lambda multiplicado por la identidad multiplicado por v es igual al vector cero.",
  "from_community_srt": "en resumidas cuentas 'traducimos' la descripcion de vectores individuales entre sistemas de coordenadas. La matriz cuyas columnas representan los vectores de la base de Jennifer pero escritos en nuestro sistema de coordenadas",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "Y recuerde, la razón por la que esto nos importa es porque significa que A multiplicado por v es igual a lambda multiplicado por v, lo que se puede interpretar como que el vector v es un vector propio de A, que permanece en su propio lapso durante la transformación A.",
  "from_community_srt": "\"traduce\" vectores de su lenguaje al nuestro y la inversa de dicha matriz hace lo contrario. Pero los vectores no son la única cosa que describimos con coordenadas.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "En este ejemplo, el valor propio correspondiente es 1, por lo que v en realidad permanecería fijo en su lugar.",
  "from_community_srt": "Para la parte que viene es importante que esteis cómodos representando transformaciones lineales con matrices y que sepáis como la multiplicación de matrices corresponde a la composición sucesiva de estas transformaciones.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Haga una pausa y reflexione si necesita asegurarse de que esa línea de razonamiento se sienta bien.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Este es el tipo de cosas que mencioné en la introducción.",
  "from_community_srt": "Definitivamente tomate un momento y echa un ojo a los capitulos 3 y 4 si algo de esto parece complicado.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Si no tuvieras una comprensión sólida de los determinantes y de por qué se relacionan con sistemas lineales de ecuaciones que tienen soluciones distintas de cero, una expresión como esta parecería completamente inesperada.",
  "from_community_srt": "Considera un transformación lineal como un giro 90 grados en la dirección de las agujas del reloj. Cuando representamos esta transformación con la matriz correspondiente seguimos los vectores i y j y a donde va cada uno tras aplicar la aplicación lineal.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Para ver esto en acción, volvamos al ejemplo desde el principio, con una matriz cuyas columnas son 3, 0 y 1, 2.",
  "from_community_srt": "i termina en el punto de coordenadas [0,1] y j termina en el punto de coordenadas [-1,0]. Estas coordenadas son las columnas de nuestra matriz asociada.",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Para encontrar si un valor lambda es un valor propio, réstalo de las diagonales de esta matriz y calcula el determinante.",
  "from_community_srt": "Pero esta representación esta fuertemente asociada a nuestra elección de la base desde el hecho de que estamos siguiendo a i y j en primer lugar",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Al hacer esto, obtenemos un determinado polinomio cuadrático en lambda, 3 menos lambda por 2 menos lambda.",
  "from_community_srt": "a el  hecho de que medimos su lugar de aterrizaje en nuestro sistema de coordenadas. ¿Cómo describiría Jennifer esta misma rotación? Lo que te pide el cuerpo es directamente",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Dado que lambda solo puede ser un valor propio si este determinante es cero, se puede concluir que los únicos valores propios posibles son lambda igual a 2 y lambda igual a 3.",
  "from_community_srt": "traducir las columnas de nuestra matriz de rotación al lenguaje de Jennifer pero eso no es del todo correcto.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Para descubrir cuáles son los vectores propios que realmente tienen uno de estos valores propios, digamos que lambda es igual a 2, conecte ese valor de lambda a la matriz y luego resuelva qué vectores esta matriz alterada diagonalmente envía a cero.",
  "from_community_srt": "Estas columnas representan a dónde van nuestros vectores de la base i y j pero la matriz que Jennifer quiere debería representar los vectores de su base y los puntos en los que aterrizan deben ser descritos en su lenguaje también. Esta es una manera común de pensar en como se hace esto.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Si calculas esto de la misma manera que lo harías con cualquier otro sistema lineal, verás que las soluciones son todos los vectores en la línea diagonal atravesada por menos 1, 1.",
  "from_community_srt": "Empezamos con cualquier vector escrito en el lenguaje de Jennifer. En vez de intentar seguir que le ocurre en su idioma primero vamos a traducir dicho vector a el nuestro",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Esto corresponde al hecho de que la matriz inalterada, 3, 0, 1, 2, tiene el efecto de estirar todos esos vectores por un factor de 2.",
  "from_community_srt": "usando la matriz de cambio de base, la matriz cuyas columnas representan sus vectores base en nuestro idioma. Esto nos da el mismo vector pero ahora está escrito en nuestro idioma.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Ahora bien, una transformación 2D no tiene por qué tener vectores propios.",
  "from_community_srt": "Podemos entonces aplicarle la matriz asociada a la transformación multiplicando a la izquierda.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Por ejemplo, considere una rotación de 90 grados.",
  "from_community_srt": "Esto nos dice dónde termina dicho vector tras la aplicación pero sigue en nuestro lenguaje.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Esto no tiene vectores propios ya que rota cada vector fuera de su propio intervalo.",
  "from_community_srt": "Debemos como último paso aplicar la inversa de la matriz cambio de base multiplicando por la izquierda como de costumbre",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Si realmente intentas calcular los valores propios de una rotación como esta, observa lo que sucede.",
  "from_community_srt": "para obtener el vector transformado en el idioma de Jennifer Ya que podemos hacer esto con cualquier vector en su lenguaje",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Su matriz tiene columnas 0, 1 y negativo 1, 0.",
  "from_community_srt": "primero aplicando el cambio de base después la transformación y por último la inversa de la matriz de cambio de base",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Resta lambda de los elementos de la diagonal y busca cuándo el determinante es cero.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "En este caso, obtienes el polinomio lambda al cuadrado más 1.",
  "from_community_srt": "la composición de esas 3 matrices nos da la matriz transformación en el idioma de Jennifer.",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Las únicas raíces de ese polinomio son los números imaginarios, i y i negativo.",
  "from_community_srt": "Toma un vector en su idioma y da por salida la versión transformada de ese vector en su idioma",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "El hecho de que no haya soluciones de números reales indica que no hay vectores propios.",
  "from_community_srt": "Para este ejemplo especifico en el que la base de Jennifer tiene la forma [2,1] y [-1,1]  en nuestro idioma",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Otro ejemplo bastante interesante que vale la pena tener en cuenta es una cizalla.",
  "from_community_srt": "y la transformación es un giro de 90 grados el producto de estas 3 matrices si lo calculas tiene esta forma",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Esto fija i-hat en su lugar y mueve j-hat 1, por lo que su matriz tiene las columnas 1, 0 y 1, 1.",
  "from_community_srt": "Así que si Jennifer multiplica dicha matriz por las coordenadas de cualquier vector en su sistema",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Todos los vectores en el eje x son vectores propios con valor propio 1 ya que permanecen fijos en su lugar.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "De hecho, estos son los únicos vectores propios.",
  "from_community_srt": "Obtiene la versión girada 90 grados de dicho vector expresado en su sistema de coordenadas.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Cuando restas lambda de las diagonales y calculas el determinante, lo que obtienes es 1 menos lambda al cuadrado.",
  "from_community_srt": "En general, siempre que veas una expresión de la forma A^(-1) M A sugiere cierta conexión matemática.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "Y la única raíz de esta expresión es lambda igual a 1.",
  "from_community_srt": "La matriz del medio representa una transformación de cierto tipo, de la forma en que tú la ves y las otras dos matrices exteriores representan la conexión,",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Esto se alinea con lo que vemos geométricamente, que todos los vectores propios tienen valor propio 1.",
  "from_community_srt": "el cambio de perspectiva, y la matriz producto completa representa la misma transfomación",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Sin embargo, tenga en cuenta que también es posible tener un solo valor propio, pero con más que una simple línea llena de vectores propios.",
  "from_community_srt": "pero como otra persona lo ve. Para aquellos que se preguntan por que nos preocupamos por sistemas de coordinadas alternativos",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Un ejemplo sencillo es una matriz que escala todo en 2.",
  "from_community_srt": "el siguiente vídeo sobre valores y vectores propios os dará un ejemplo importante.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "El único valor propio es 2, pero cada vector en el plano llega a ser un vector propio con ese valor propio.",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Ahora es otro buen momento para hacer una pausa y reflexionar sobre algo de esto antes de pasar al último tema.",
  "from_community_srt": "Nos vemos allí",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Quiero terminar aquí con la idea de una base propia, que se basa en gran medida en las ideas del último vídeo.",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Eche un vistazo a lo que sucede si nuestros vectores base resultan ser vectores propios.",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Por ejemplo, tal vez i-hat esté escalado por 1 negativo y j-hat esté escalado por 2.",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Al escribir sus nuevas coordenadas como las columnas de una matriz, observe que esos múltiplos escalares, negativos 1 y 2, que son los valores propios de i-hat y j-hat, se ubican en la diagonal de nuestra matriz, y todas las demás entradas son 0. .",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Cada vez que una matriz tiene ceros en todas partes excepto en la diagonal, se llama, razonablemente, matriz diagonal.",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "Y la forma de interpretar esto es que todos los vectores base son vectores propios, siendo las entradas diagonales de esta matriz sus valores propios.",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Hay muchas cosas que hacen que sea mucho más agradable trabajar con matrices diagonales.",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Uno de los más importantes es que es más fácil calcular lo que sucederá si multiplicas esta matriz por sí misma muchas veces.",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Dado que lo único que hace una de estas matrices es escalar cada vector base según algún valor propio, aplicar esa matriz muchas veces, digamos 100 veces, corresponderá a escalar cada vector base según la centésima potencia del valor propio correspondiente.",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Por el contrario, intente calcular la potencia número 100 de una matriz no diagonal.",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "De verdad, pruébalo por un momento.",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Es una pesadilla.",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Por supuesto, rara vez tendrás tanta suerte como para que tus vectores base también sean vectores propios.",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Pero si tu transformación tiene muchos vectores propios, como el del inicio de este vídeo, suficientes para que puedas elegir un conjunto que abarque todo el espacio, entonces podrías cambiar tu sistema de coordenadas para que estos vectores propios sean tus vectores base.",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Hablé sobre el cambio de base en el último video, pero aquí haré un recordatorio súper rápido de cómo expresar una transformación actualmente escrita en nuestro sistema de coordenadas en un sistema diferente.",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Tome las coordenadas de los vectores que desea usar como una nueva base, que en este caso significa nuestros dos vectores propios, luego convierta esas coordenadas en las columnas de una matriz, conocida como matriz de cambio de base.",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Cuando intercala la transformación original, poniendo la matriz de cambio de base a su derecha y la inversa de la matriz de cambio de base a su izquierda, el resultado será una matriz que representa esa misma transformación, pero desde la perspectiva de las nuevas coordenadas de los vectores base. sistema.",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "El objetivo de hacer esto con vectores propios es que se garantiza que esta nueva matriz será diagonal con sus valores propios correspondientes en esa diagonal.",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Esto se debe a que representa trabajar en un sistema de coordenadas donde lo que sucede con los vectores base es que se escalan durante la transformación.",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Un conjunto de vectores base que también son vectores propios se denomina, nuevamente, razonablemente, base propia.",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Entonces, si, por ejemplo, necesitara calcular la potencia número 100 de esta matriz, sería mucho más fácil cambiar a una base propia, calcular la potencia número 100 en ese sistema y luego volver a convertir a nuestro sistema estándar.",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "No puedes hacer esto con todas las transformaciones.",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Un corte, por ejemplo, no tiene suficientes vectores propios para abarcar todo el espacio.",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Pero si puedes encontrar una base propia, las operaciones matriciales son realmente hermosas.",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Para aquellos de ustedes que estén dispuestos a resolver un rompecabezas bastante interesante para ver cómo se ve en acción y cómo se puede usar para producir resultados sorprendentes, dejaré un mensaje aquí en la pantalla.",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Requiere un poco de trabajo, pero creo que lo disfrutarás.",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "El siguiente y último vídeo de esta serie tratará sobre espacios vectoriales abstractos.",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]