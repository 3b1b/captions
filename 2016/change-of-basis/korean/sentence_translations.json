[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "",
  "from_community_srt": "만일 우리가 2D 공간에 있는 하나의 벡터를 놓는다면 우리는 좌표를 사용해서 이 벡터를 묘사하는 기본적인 방법을 갖게 됩니다. 이 경우에, 벡터는 [3, 2]의 좌표를 갖는다. 이것은 꼬리에서 머리까지 오른쪽으로 3칸 움직이고,",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "",
  "from_community_srt": "위로 2칸 움직인다는 것을 의미한다. 그럼, 좌표를 설명하기 위한 더욱 더 선형 대수학에 근거한 방법은 각각의 숫자는 스칼라(scalar)로써 이루어져 있다고 생각하는 것이다.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "",
  "from_community_srt": "이 스칼라를 늘어나거나 쪼그라든 벡터라고 생각하면서, 당신은 첫번째 좌표를 i hat 이라 부르고,",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "",
  "from_community_srt": "그것은 길이가 1이고 오른쪽을 가르킨다고 생각한다. 반면 두번째 좌표 스케일인 j hat은",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "",
  "from_community_srt": "길이가 1이고 위를 가르킨다고 생각한다.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "",
  "from_community_srt": "이 두개의 벡터를 머리와 꼬리를 이으면서 더하는 것은 좌표를 묘사하는 방법을 의미하는 것이다. 당신은 이러한 두개의 특별한 벡터를",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "",
  "from_community_srt": "모든 좌표계의 임의의 가정을 포함하는 것으로 생각할 수 있다. 첫번째 숫자는 오른쪽을 가르킨다는 사실과, 두번째 숫자는 윗 방향을 가르킨다는 사실은 정확히 단위 길이를 표시한다.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "",
  "from_community_srt": "i hat 과 j hat의 선택을 묶는 모든것은 벡터로되는 좌표는 스칼라 실제로 확장하기위한 것입니다.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "",
  "from_community_srt": "어쨌든 벡터와 수의 집합을 연결하는 것을 좌표계라고 부른다. 그리고, 두 특수 벡터,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "",
  "from_community_srt": "i hat 과 j hat는 기저 벡터 (basis vector)라고 부른다. 우리의 표준 좌표계에서는 말이다. 여기에 대해 이야기하고 싶은 것은",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "",
  "from_community_srt": "다른 기저 벡터의 집합을 사용하는 아이디어이다.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "",
  "from_community_srt": "예를 들어, 당신이 친구, 제니퍼가 있다고 가정 해 보자 그녀는 b1과 b2라고 불리는 다른 집합의 기저 벡터를 사용한다.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "",
  "from_community_srt": "그녀의 첫번째 기저 벡터 b1은 약간 오른쪽 위를 가르키고",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "",
  "from_community_srt": "그녀의 두 번째 기저 벡터 b2는 왼쪽 위를 가르킨다. 이제, 내가 방금 보여주였던 벡터를 살펴보자.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "",
  "from_community_srt": "당신과 내가 묘사하고 싶은 [3, 2]는 우리가 사용했던 기저 벡터 i hat과 j hat을 사용해서 나타냈다. 제니퍼는 이 벡터를 기술 할 것 좌표 [5/3, 1/3]라고 기술할 것이다.",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "",
  "from_community_srt": "이것이 의미하는 것은 벡터를 얻는 특정한 방법, 즉 그녀의 2개의 기저벡터를 사용하면, b1에서 5/3,",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "",
  "from_community_srt": "b2에서 1/3을 나타내고, 그 다음 모두를 합하는 것이다. 잠시동안, 나는 당신이 어떻게 두개의 숫자, 5/3과 1/3을 계산할 수 있는지를 보여줄것이다.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "",
  "from_community_srt": "일반적으로, 제니퍼 좌표를 사용해 벡터를 이야기할 때 마다 그녀는 처음에는 b1을 생각하고 두번째 좌표 b2를 생각할 것이다. 그리고,",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "",
  "from_community_srt": "그녀는 그 결과들을 더합니다. 그녀가 얻게 될 것은 우리가 생각한 좌표를 이용하여 얻은 좌표계와 완전히 다를 것이다.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "여기서, 설정에 대해 좀 더 정확하게하려면",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "그녀의 첫 기저 벡터 b1는 우리의 좌표계에서는 [2, 1]s로 표현된다. (역자 주: 표준좌표계는 [x,y]s로 표기) 그리고 그녀의 두번째 기저 벡터 b2는 우리의 좌표계에서 [-1,",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "",
  "from_community_srt": "1]s로 표현된다. 하지만 그녀의 시스템에서 그녀의 직관으로부터 이해하는 것은 중요합니다. 이 벡터의 좌표가 [1, 0]j 이고, [0, 1]j 이라는 것 말이죠.",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "(역자 주: 제니퍼좌표계는 [x,y]j로 표기) 그 좌표들은 그녀의 세상에서는 [0, 1]j이고 [1, 0]j입니다.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "",
  "from_community_srt": "그래서, 사실은, 우리는 다른 언어를 사용하고 있습니다 우리는 모두, 공간안에서 같은 벡터를 보고있지만",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "",
  "from_community_srt": "제니퍼는 그것을 설명하기 위해 다른 단어와 숫자를 사용한다. 내가 여기서 설명하고 있는 몇가지 단어들을 빠르게 설명하고 지나가도록 하자. 나는 2 차원 공간의 애니메이션을 그릴 때, 나는 일반적으로이 사각형 격자를 사용한다. 하지만, 이 그리드는 그냥 구조물일 뿐이다. 우리의 좌표 시스템을 시각화하는 방법으로써, 그리고 그리드는 우리가 선택하는 기저에 따라 달라집니다.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "",
  "from_community_srt": "공간 자체는 본질적으로 그리드를 갖고 있지 않습니다. 제니퍼가 그녀만의 그리드를 그린다고 해도 이것은 단순히 만들어진 구조이며",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "",
  "from_community_srt": "그리드의 의미는 시각적 인 도구에 지나지 않는다. 이건 그녀의 좌표의 의미를 이해하는데에 도움이 되지만요. 하지만, 그녀의 원점은 실제로 우리와 같을 것이다.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "",
  "from_community_srt": "왜냐하면 어떤 좌표에서도 [0, 0]은 같은 의미를 갖기 때문이다. 이 사실은 당신이 어느 벡터를 가지고 크기를 0으로 줄였을 때를 의미하는 곳이기 때문이다.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "",
  "from_community_srt": "그러나 그녀의 축 방향 그녀의 그리드 라인의 간격 그녀가 선택한 기저 벡터에 따라 달라집니다.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "",
  "from_community_srt": "그래서, 이 모든 설정 후 자연스럽게 나오는 질문 중 하나는 우리는 어떻게 다른 좌표계 사이를 해석 해야합니까? 일겁니다. 만일, 예를 들어,",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "",
  "from_community_srt": "제니퍼는 벡터를 설명하는 경우 좌표 값 [-1, 2]j를 이용하는데 그것은 우리 좌표계에서는 어떻게 될 것인가? 당신은 어떻게 그녀의 언어를 우리의 언어로 해석할 수 있을까? 글쎄, 우리의 좌표가 말하는 것은 이 벡터가 b1에 -1을 곱하고,",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "",
  "from_community_srt": "b2로 +2를 곱하라는 것이다. 그리고 우리의 관점으로부터 b1은 좌표 [2, 1]s를 갖고, b2는 [-1,",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "",
  "from_community_srt": "1]s의 좌표를 갖는다",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "",
  "from_community_srt": "그래서 우리는 실제 -1*b1 + 2*b2를 계산할 수 있다. 우리의 좌표 시스템에 표시하고 있는 벡터를 사용해서 말이죠. 그리고 이렇게 하면",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "",
  "from_community_srt": "당신은 좌표 [-4, 1]s에 있는 벡터를 얻을 수 있다. 그래서, 이것이 그녀가 생각하는 [-1, 2]j를 우리가 표현하는 방법이다. 여기서, 그녀의 각 기저벡터의 스케일[b1,",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "",
  "from_community_srt": "b2]j은 일부 벡터의 좌표[(2,1)s, (-1, 1)s]에 해당한다. 그리고 그것들을 더함으로 얻을 수 있다. 아마 다음의 것과 비슷해 보일 수 있다. 바로 행렬과 벡터의 곱 이다. 우리의 언어로 표현된 제니퍼의 기저 상태를 나타내는 열들을 갖고있는 행렬들의 곱 말이다. 사실,",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "",
  "from_community_srt": "당신이 한번 (행렬 * 벡터) 곱셈을 임의의 선형 변환을 가하는 것을써 이해하고 있다면,",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "",
  "from_community_srt": "즉, 이 시리즈에서 가장 중요한 비디오인 챕터 3을 보면, 여기서 일어나는 일에 대해서 생각하는 직관을 갖고 있을지도 모른다.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "",
  "from_community_srt": "제니퍼의 기저 벡터를 나타내는 열을 갖고 있는 행렬은 선형 변환으로 간주 될 수있다 우리의 기저 벡터 i hat 과 j hat을 움직이는 변환으로 말이다. 우리가 말하는 [1,",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "",
  "from_community_srt": "0]s과 [0, 1]s을 말하는 것은 제니퍼의 기저 벡터에서는 그녀가 말하는 [1, 0]j과 [0, 1]j이 되어버린다. 이것이 어떻게 작동하는지 보려면,",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "",
  "from_community_srt": "우리가 가진 좌표에서 [-1, 2]s를 갖는 벡터의 의미가 무엇인지 같이 따라가보자. 그리고 변환을 가해보자. 선형 변환 전에 우리는이 벡터를",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "",
  "from_community_srt": "우리 기준의 특정 선형 조합으로서 벡터 -1*i hat + 2 j hat로 나타낼 수 있다. 선형 변환의 주요 기능은",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "",
  "from_community_srt": "그 결과가 다른 기저를 이용하여도 동일한 선형 결합 벡터가 된다는 점입니다. i hat이 있던 장소에서 -1을 곱하고, j hat이 있던 장소에서 2를 곱함으로써 말이죠.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "",
  "from_community_srt": "그래서 이 행렬이하는 일은 우리의 오해를 제니퍼가 의미하는 바로 변환시키는 것입니다. 그녀가 생각하는 실제의 벡터가 있는 곳으로 변환시키면서 말이죠.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "",
  "from_community_srt": "내가 이것을 처음 배웠을 때 반대로 말하는 느낌이 들었다. 기하학적으로,이 행렬은 우리의 그리드를 제니퍼의 격자로 변환시키는 것입니다.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "",
  "from_community_srt": "그러나 수치학적으로, 그녀의 언어에서 우리의 언어로 번역하는 것이죠. 나를 마침내 두드린 것은 제니퍼가 말하는 것의 우리의 오해를 어떻게 다루는지 생각하는 것이다.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "",
  "from_community_srt": "우리의 좌표계에서 벡터를 생각하고",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "",
  "from_community_srt": "그녀가 실제로 의미하는 벡터로의 변환하는 것이다.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "",
  "from_community_srt": "다른 방법으로 생각하는 건 어떨까요? 내가 이전에 사용했던 비디오를 예로 들어서 우리의 좌표계에서 [3, 2]s를 가질 때 어떻게 나는 제니퍼의 좌표계에서는 [5/3,",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "",
  "from_community_srt": "1/3]j라고 계산할까? 당신은 기초 매트릭스의 변화로 시작한다.",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "",
  "from_community_srt": "그것은 제니퍼의 언어를 우리의 언어로  번역한다. 그리고 그것의 역을 취한다. 기억해봐라, 역변환은",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "",
  "from_community_srt": "처음에 있던 곳으로 되돌리는 것에 대응하는 새로운 변환이다.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "",
  "from_community_srt": "실제로, 특히 2차원보다 큰 곳에서 생각할 때 이 역행렬을 구하려면 컴퓨터를 사용하는 것이 좋다.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "",
  "from_community_srt": "이 경우에, 제니퍼의 기저 벡터를 열행렬로 가지고 있는 기저 행렬의 역변환은 [1/3, -1/3],",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "",
  "from_community_srt": "[1/3, 2/3]가 된다. 따라서,",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "",
  "from_community_srt": "예를 들어 제니퍼의 좌표계에서 [3, 2]s를 보는 것은",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "",
  "from_community_srt": "우리는 이 기저 행렬의 역행렬을 [3,",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "",
  "from_community_srt": "2]s 에 곱해야한다.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "",
  "from_community_srt": "그러면  [5/3, 1/3]j이 나온다. 그래서, 간단히 말해서",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "",
  "from_community_srt": "이것이 개개의 벡터의 모습을 변환하는 방법입니다. 좌표계 사이를 왔다 갔다 하면서요. 제니퍼의 기저 벡터를 가지고 있는 행렬은,",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "",
  "from_community_srt": "우리의 좌표계로 나타내어 있지만, 그녀의 언어에서 우리의 언어로 벡터를 변환시켜 줍니다. 그리고 역행렬은 반대로 작용한다. 그러나 벡터는 좌표계를 이용해서만 묘사되는 것이 아니다. 이 다음부터는 말이죠.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "",
  "from_community_srt": "변환을 행렬로 생각하는 방법은 중요합니다. 그리고 행렬 곱이 어떻게",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "",
  "from_community_srt": "연속적인 변환과 연관되는지 알게 될 것입니다. 잠시 멈추고 챕터3과 4를 다시보세요.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "",
  "from_community_srt": "만약 불안하다면요. 일부 선형 변환을 고려해보죠.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "",
  "from_community_srt": "반 시계 방향으로 90 ° 회전된것 처럼요. 당신과 나는 이 행렬이 표현하는 것은 기저 벡터 i hat 과 j hat이 각각 어디로 가야하는지를 나타냅니다. i hat은 [0,",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "",
  "from_community_srt": "1]s에서 끝나고, j hat은 [-1, 0]s에서 끝납니다. 그렇기 때문에 각 좌표는 우리의 행렬의 열행렬로써 쓰여질 수 있습니다.",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "",
  "from_community_srt": "그러나 이 표현은 우리의 초기 기저 벡터에 제한되어있습니다. i hat 과 j hat 이 처음에 있던 자리를 알아야하고,",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "",
  "from_community_srt": "그것들이 어디로 가야하는지 알아야만  합니다. 그것도 우리 좌표계 안에서 말이죠. 제니퍼는 같은 90° 변환을 어떻게 표현할 수 있을까요?",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "",
  "from_community_srt": "당신은 이렇게 말하고 싶을지도 모릅니다. 우리의 회전행렬을 그녀의 언어로 변환하는 것으로써 나타낼 수 있다고 말이죠. 그러나 그것은 옳지 않습니다.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "",
  "from_community_srt": "그 행렬의 각각의 열 벡터는 우리의 기저벡터 i hat 과 j hat이 어떻게 가는지를 나타낼 뿐이고, 하지만 제니퍼가 원하는 행렬은 그녀의 기저 벡터가 어디로 가야하는지를 나타내야만 한다. 그리고 그 도착 지점 또한 그녀의 언어로 표시해야만 하죠. 다음은, 어떻게 이것이 실행되는지 가장 일반적인 방법을 설명한다.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "",
  "from_community_srt": "제니퍼의 언어로 쓰여진 임의의 벡터로부터 시작한다. 그녀의 언어에서 무엇이 일어나는지 따라가기 보다는 먼저, 우리는 우리의 언어로 번역할 것이다.",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "기저 행렬의 변환을 사용해서 말이죠. 열 벡터의 요소는 그녀의 기저 벡터가 우리 언어의 무엇을 말하는지 나타냅니다. 이것은 우리에게 같은 벡터를 제공합니다 우리의 언어로 쓰여있는 벡터로 말이죠.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "",
  "from_community_srt": "그 다음에 얻어진 변환 행렬을 왼쪽에 곱합니다.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "",
  "from_community_srt": "이것은 어디로 벡터가 움직일지를 알려줍니다. 하지만 여전히 우리의 언어이다.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "",
  "from_community_srt": "마지막 단계로써 기저 행렬의 역함수를 평범하게 왼쪽에 곱하는 것으로써,",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "",
  "from_community_srt": "변환 된 벡터를 얻을 수 있습니다 제니퍼의 언어로 말이죠. 우리는이 작업을 수행 할 수 있습니다.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "",
  "from_community_srt": "그녀의 언어로 작성된 모든 벡터에 대해서요. 먼저, 기저의 변환을 적용하고 그리고, 선형 변환 후,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "",
  "from_community_srt": "그리고, 기저 벡터의 역변환을 통해서 말이죠. 세 행렬의 구성은",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "",
  "from_community_srt": "우리에게 제니퍼의 변환 행렬을 제니퍼의 언어로 제공합니다 이것은 그녀의 언어로 쓰인 벡터에 작용해서",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "",
  "from_community_srt": "변환 후, 그녀의 언어로 이루어진 벡터를 뱉습니다. 이 구체적인 예를 들어",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "",
  "from_community_srt": "우리의 언어에서 [2, 1]s 및 [-1, 1]s인 제니퍼의 기저 벡터가",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "",
  "from_community_srt": "90 ° 회전 될 때 이 세 가지 행렬의 곱으로써 나타내어지고 당신은 그것을 통해 계산할 경우 열이 [1/3,",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "",
  "from_community_srt": "5/3] 및 [-2/3, -1/3]가 됩니다. 그래서 제니퍼가 그 행렬을",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "",
  "from_community_srt": "그녀의 좌표계에 있는 벡터에 곱할 때 그것의 90 ° 회전 된 버전을 반환합니다.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "",
  "from_community_srt": "그녀의 좌표계로 표현되면서 말이죠.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "",
  "from_community_srt": "일반적으로, 당신이 A^(-1)MA와 같은 표현을 볼 때 마다 방금까지와 같은 수학적인 느낌을 떠올리기 바란다.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "",
  "from_community_srt": "중간의 행렬 M 은 선형 변환을 나타내고,",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "",
  "from_community_srt": "그리고 바깥쪽 행렬 A와 A^(-1)은 관점의 변환을 나타낸다. 그리고 총 행렬의 곱은 같은 변환을 나태지만,",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "",
  "from_community_srt": "다만 기저가 다를 뿐이다. 좌표계의 변환을 왜 해야하는지 궁금증을 갖는 당신을 위해, 고유 벡터와 고유 값을 다룬 다음 비디오가",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "",
  "from_community_srt": "정말 중요한 예제를 제공 할 것입니다.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "",
  "from_community_srt": "그때 만나!",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]