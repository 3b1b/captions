[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "고유벡터와 고유값은 많은 학생들이 특히 직관적이지 않다고 생각하는 주제 중 하나입니다.",
  "model": "google_nmt",
  "from_community_srt": "만일 우리가 2D 공간에 있는 하나의 벡터를 놓는다면 우리는 좌표를 사용해서 이 벡터를 묘사하는 기본적인 방법을 갖게 됩니다. 이 경우에, 벡터는 [3, 2]의 좌표를 갖는다. 이것은 꼬리에서 머리까지 오른쪽으로 3칸 움직이고,",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "우리가 이것을 하는 이유와 이것이 실제로 무엇을 의미하는지와 같은 질문은 답이 없는 계산의 바다에 떠다니는 경우가 너무 많습니다.",
  "model": "google_nmt",
  "from_community_srt": "위로 2칸 움직인다는 것을 의미한다. 그럼, 좌표를 설명하기 위한 더욱 더 선형 대수학에 근거한 방법은 각각의 숫자는 스칼라(scalar)로써 이루어져 있다고 생각하는 것이다.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "그리고 제가 이 시리즈의 영상을 공개하면서 많은 분들이 이 주제를 특히 시각화할 수 있기를 기대한다는 의견을 많이 주셨습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 스칼라를 늘어나거나 쪼그라든 벡터라고 생각하면서, 당신은 첫번째 좌표를 i hat 이라 부르고,",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "나는 그 이유가 고유 사물이 특별히 복잡하거나 제대로 설명되지 않았기 때문이 아니라고 생각합니다.",
  "model": "google_nmt",
  "from_community_srt": "그것은 길이가 1이고 오른쪽을 가르킨다고 생각한다.",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "사실, 그것은 비교적 간단하고, 대부분의 책이 그것을 잘 설명하고 있다고 생각합니다.",
  "model": "google_nmt",
  "from_community_srt": "반면 두번째 좌표 스케일인 j hat은 길이가 1이고 위를 가르킨다고 생각한다.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "문제는 앞에 나오는 많은 주제에 대해 확실한 시각적 이해가 있는 경우에만 실제로 의미가 있다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 두개의 벡터를 머리와 꼬리를 이으면서 더하는 것은 좌표를 묘사하는 방법을 의미하는 것이다.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "여기서 가장 중요한 것은 행렬을 선형 변환으로 생각하는 방법을 아는 것입니다. 하지만 행렬식, 방정식의 선형 시스템 및 기저 변경과 같은 사항에도 익숙해야 합니다.",
  "model": "google_nmt",
  "from_community_srt": "당신은 이러한 두개의 특별한 벡터를 모든 좌표계의 임의의 가정을 포함하는 것으로 생각할 수 있다. 첫번째 숫자는 오른쪽을 가르킨다는 사실과, 두번째 숫자는 윗 방향을 가르킨다는 사실은 정확히 단위 길이를 표시한다.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "고유량에 대한 혼란은 일반적으로 고유벡터 및 고유값 자체보다는 이러한 주제 중 하나의 불안정한 기초와 더 관련이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "i hat 과 j hat의 선택을 묶는 모든것은 벡터로되는 좌표는 스칼라 실제로 확장하기위한 것입니다.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "시작하려면 여기에 표시된 것과 같은 2차원의 선형 변환을 고려해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "어쨌든 벡터와 수의 집합을 연결하는 것을 좌표계라고 부른다. 그리고, 두 특수 벡터,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "기본 벡터 i-hat을 좌표 3, 0으로 이동하고 j-hat을 1, 2로 이동합니다.",
  "model": "google_nmt",
  "from_community_srt": "i hat 과 j hat는 기저 벡터 (basis vector)라고 부른다. 우리의 표준 좌표계에서는 말이다.",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "따라서 열이 3, 0, 1, 2인 행렬로 표현됩니다.",
  "model": "google_nmt",
  "from_community_srt": "여기에 대해 이야기하고 싶은 것은 다른 기저 벡터의 집합을 사용하는 아이디어이다.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "하나의 특정 벡터에 어떤 역할을 하는지에 집중하고 해당 벡터의 범위, 원점과 끝을 통과하는 선에 대해 생각해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "예를 들어, 당신이 친구, 제니퍼가 있다고 가정 해 보자 그녀는 b1과 b2라고 불리는 다른 집합의 기저 벡터를 사용한다.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "대부분의 벡터는 변환 중에 해당 범위를 벗어나게 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "그녀의 첫번째 기저 벡터 b1은 약간 오른쪽 위를 가르키고 그녀의 두 번째 기저 벡터 b2는 왼쪽 위를 가르킨다.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "내 말은, 벡터가 착륙한 장소도 우연히 그 선 어딘가에 있었다면 그것은 꽤 우연의 일치처럼 보일 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "이제, 내가 방금 보여주였던 벡터를 살펴보자.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "그러나 일부 특수 벡터는 자체 범위에 남아 있습니다. 즉, 행렬이 그러한 벡터에 미치는 영향은 스칼라처럼 단순히 늘리거나 찌그러뜨리는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "당신과 내가 묘사하고 싶은 [3, 2]는 우리가 사용했던 기저 벡터 i hat과 j hat을 사용해서 나타냈다. 제니퍼는 이 벡터를 기술 할 것 좌표 [5/3, 1/3]라고 기술할 것이다.",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "이 특정 예에서 기본 벡터 i-hat은 그러한 특수 벡터 중 하나입니다.",
  "model": "google_nmt",
  "from_community_srt": "이것이 의미하는 것은 벡터를 얻는 특정한 방법, 즉 그녀의 2개의 기저벡터를 사용하면, b1에서 5/3,",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "i-hat의 범위는 x축이고 행렬의 첫 번째 열에서 i-hat이 여전히 x축에 있는 3배로 이동하는 것을 볼 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "b2에서 1/3을 나타내고, 그 다음 모두를 합하는 것이다. 잠시동안, 나는 당신이 어떻게 두개의 숫자, 5/3과 1/3을 계산할 수 있는지를 보여줄것이다.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "게다가 선형 변환이 작동하는 방식으로 인해 x축의 다른 벡터도 3배만큼 늘어나서 자체 범위를 유지합니다.",
  "model": "google_nmt",
  "from_community_srt": "일반적으로, 제니퍼 좌표를 사용해 벡터를 이야기할 때 마다 그녀는 처음에는 b1을 생각하고 두번째 좌표 b2를 생각할 것이다. 그리고,",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "이 변환 중에 자체 범위에 남아 있는 약간 더 교묘한 벡터는 음수 1, 1입니다.",
  "model": "google_nmt",
  "from_community_srt": "그녀는 그 결과들을 더합니다. 그녀가 얻게 될 것은 우리가 생각한 좌표를 이용하여 얻은 좌표계와 완전히 다를 것이다.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "결국 2배로 늘어나게 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "여기서,",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "그리고 다시 선형성은 이 사람이 가로지르는 대각선의 다른 벡터가 2배만큼 늘어나게 된다는 것을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "설정에 대해 좀 더 정확하게하려면 그녀의 첫 기저 벡터 b1는 우리의 좌표계에서는 [2, 1]s로 표현된다. (역자 주: 표준좌표계는 [x,y]s로 표기) 그리고 그녀의 두번째 기저 벡터 b2는 우리의 좌표계에서 [-1,",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "그리고 이 변환의 경우, 그것들은 범위를 유지하는 특별한 속성을 가진 모든 벡터입니다.",
  "model": "google_nmt",
  "from_community_srt": "1]s로 표현된다. 하지만 그녀의 시스템에서 그녀의 직관으로부터 이해하는 것은 중요합니다. 이 벡터의 좌표가 [1, 0]j 이고, [0, 1]j 이라는 것 말이죠.",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "x축에 있는 것들은 3배로 늘어나고, 이 대각선에 있는 것들은 2배로 늘어납니다.",
  "model": "google_nmt",
  "from_community_srt": "(역자 주: 제니퍼좌표계는 [x,y]j로 표기) 그 좌표들은 그녀의 세상에서는 [0, 1]j이고 [1, 0]j입니다.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "다른 모든 벡터는 변환 중에 어느 정도 회전하여 해당 선을 벗어나게 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서, 사실은, 우리는 다른 언어를 사용하고 있습니다 우리는 모두,",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "지금쯤 추측할 수 있듯이 이러한 특수 벡터를 변환의 고유 벡터라고 하며 각 고유 벡터는 고유값이라고 불리는 것과 연관되어 있습니다. 이는 변환 중에 늘어나거나 찌그러지는 요소일 뿐입니다.",
  "model": "google_nmt",
  "from_community_srt": "공간안에서 같은 벡터를 보고있지만 제니퍼는 그것을 설명하기 위해 다른 단어와 숫자를 사용한다. 내가 여기서 설명하고 있는 몇가지 단어들을 빠르게 설명하고 지나가도록 하자. 나는 2 차원 공간의 애니메이션을 그릴 때, 나는 일반적으로이 사각형 격자를 사용한다. 하지만, 이 그리드는 그냥 구조물일 뿐이다. 우리의 좌표 시스템을 시각화하는 방법으로써, 그리고 그리드는 우리가 선택하는 기저에 따라 달라집니다.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "물론, 스트레칭과 스퀴싱에 대해 특별한 것은 없으며 이러한 고유값이 양수라는 사실도 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "공간 자체는 본질적으로 그리드를 갖고 있지 않습니다. 제니퍼가 그녀만의 그리드를 그린다고 해도 이것은 단순히 만들어진 구조이며 그리드의 의미는 시각적 인 도구에 지나지 않는다.",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "또 다른 예로, 고유값이 1/2인 고유벡터가 있을 수 있습니다. 이는 벡터가 1/2만큼 뒤집히고 찌그러진다는 의미입니다.",
  "model": "google_nmt",
  "from_community_srt": "이건 그녀의 좌표의 의미를 이해하는데에 도움이 되지만요. 하지만, 그녀의 원점은 실제로 우리와 같을 것이다.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "하지만 여기서 중요한 점은 회전하지 않고 뻗어나가는 선에 머물러 있다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "왜냐하면 어떤 좌표에서도 [0, 0]은 같은 의미를 갖기 때문이다. 이 사실은 당신이 어느 벡터를 가지고 크기를 0으로 줄였을 때를 의미하는 곳이기 때문이다.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "이것이 생각하기에 유용한 이유를 엿볼 수 있도록 3차원 회전을 고려해보세요.",
  "model": "google_nmt",
  "from_community_srt": "그러나 그녀의 축 방향 그녀의 그리드 라인의 간격 그녀가 선택한 기저 벡터에 따라 달라집니다.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "해당 회전에 대한 고유벡터(자체 범위에 남아 있는 벡터)를 찾을 수 있다면 회전축을 찾은 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서, 이 모든 설정 후 자연스럽게 나오는 질문 중 하나는 우리는 어떻게 다른 좌표계 사이를 해석 해야합니까? 일겁니다. 만일, 예를 들어,",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "그리고 해당 변환과 관련된 전체 3x3 행렬에 대해 생각하는 것보다 일부 회전 축과 회전 각도 측면에서 3D 회전을 생각하는 것이 훨씬 쉽습니다.",
  "model": "google_nmt",
  "from_community_srt": "제니퍼는 벡터를 설명하는 경우 좌표 값 [-1, 2]j를 이용하는데 그것은 우리 좌표계에서는 어떻게 될 것인가? 당신은 어떻게 그녀의 언어를 우리의 언어로 해석할 수 있을까? 글쎄, 우리의 좌표가 말하는 것은 이 벡터가 b1에 -1을 곱하고,",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "그런데 이 경우 해당 고유값은 1이어야 합니다. 왜냐하면 회전은 아무것도 늘어나거나 찌그러지지 않으므로 벡터의 길이는 동일하게 유지되기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "b2로 +2를 곱하라는 것이다. 그리고 우리의 관점으로부터 b1은 좌표 [2, 1]s를 갖고, b2는 [-1,",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "이 패턴은 선형대수학에서 많이 나타납니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "행렬로 설명되는 모든 선형 변환을 사용하면 이 행렬의 열을 기저 벡터의 착지 지점으로 읽어서 무엇을 하는지 이해할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "1]s의 좌표를 갖는다 그래서 우리는 실제 -1*b1 + 2*b2를 계산할 수 있다. 우리의 좌표 시스템에 표시하고 있는 벡터를 사용해서 말이죠. 그리고 이렇게 하면 당신은 좌표 [-4,",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "그러나 특정 좌표계에 덜 의존하면서 선형 변환이 실제로 수행하는 작업의 핵심을 파악하는 더 좋은 방법은 고유벡터와 고유값을 찾는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "1]s에 있는 벡터를 얻을 수 있다. 그래서, 이것이 그녀가 생각하는 [-1, 2]j를 우리가 표현하는 방법이다. 여기서, 그녀의 각 기저벡터의 스케일[b1,",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "여기서는 고유벡터와 고유값을 계산하는 방법에 대해 자세히 다루지는 않지만 개념적 이해에 가장 중요한 계산 아이디어에 대한 개요를 제공하려고 노력할 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "b2]j은 일부 벡터의 좌표[(2,1)s, (-1, 1)s]에 해당한다. 그리고 그것들을 더함으로 얻을 수 있다. 아마 다음의 것과 비슷해 보일 수 있다. 바로 행렬과 벡터의 곱 이다. 우리의 언어로 표현된 제니퍼의 기저 상태를 나타내는 열들을 갖고있는 행렬들의 곱 말이다. 사실,",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "상징적으로 고유벡터의 아이디어는 다음과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "당신이 한번 (행렬 * 벡터) 곱셈을 임의의 선형 변환을 가하는 것을써 이해하고 있다면, 즉,",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A는 v가 고유벡터인 일부 변환을 나타내는 행렬이고, 람다는 숫자, 즉 해당 고유값입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 시리즈에서 가장 중요한 비디오인 챕터 3을 보면, 여기서 일어나는 일에 대해서 생각하는 직관을 갖고 있을지도 모른다.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "이 표현식이 말하는 것은 행렬-벡터 곱 A 곱하기 v가 고유벡터 v를 일부 값 람다로 스케일링하는 것과 동일한 결과를 제공한다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "제니퍼의 기저 벡터를 나타내는 열을 갖고 있는 행렬은 선형 변환으로 간주 될 수있다 우리의 기저 벡터 i hat 과 j hat을 움직이는 변환으로 말이다. 우리가 말하는 [1,",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "따라서 행렬 A의 고유벡터와 고유값을 찾는 것은 이 표현식을 참으로 만드는 v와 람다의 값을 찾는 것으로 귀결됩니다.",
  "model": "google_nmt",
  "from_community_srt": "0]s과 [0, 1]s을 말하는 것은 제니퍼의 기저 벡터에서는 그녀가 말하는 [1, 0]j과 [0, 1]j이 되어버린다.",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "처음에는 작업하기가 약간 어색합니다. 왜냐하면 왼쪽은 행렬-벡터 곱셈을 나타내지만 오른쪽은 스칼라-벡터 곱셈을 나타내기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "이것이 어떻게 작동하는지 보려면, 우리가 가진 좌표에서 [-1, 2]s를 갖는 벡터의 의미가 무엇인지 같이 따라가보자. 그리고 변환을 가해보자.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "그럼 우변을 일종의 행렬-벡터 곱셈으로 다시 작성하는 것부터 시작하겠습니다. 행렬을 사용하면 모든 벡터를 람다 배율로 스케일링하는 효과가 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "선형 변환 전에 우리는이 벡터를 우리 기준의 특정 선형 조합으로서 벡터 -1*i hat + 2 j hat로 나타낼 수 있다.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "그러한 행렬의 열은 각 기저 벡터에 어떤 일이 일어나는지 나타내며, 각 기저 벡터는 단순히 람다와 곱해집니다. 따라서 이 행렬의 대각선 아래 숫자는 람다이고 다른 곳은 모두 0입니다.",
  "model": "google_nmt",
  "from_community_srt": "선형 변환의 주요 기능은 그 결과가 다른 기저를 이용하여도 동일한 선형 결합 벡터가 된다는 점입니다. i hat이 있던 장소에서 -1을 곱하고, j hat이 있던 장소에서 2를 곱함으로써 말이죠.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "이 함수를 작성하는 일반적인 방법은 람다를 인수분해하여 람다 곱하기 i로 작성하는 것입니다. 여기서 i는 대각선 아래에 1이 있는 단위 행렬입니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서 이 행렬이하는 일은 우리의 오해를 제니퍼가 의미하는 바로 변환시키는 것입니다. 그녀가 생각하는 실제의 벡터가 있는 곳으로 변환시키면서 말이죠.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "양쪽 변이 행렬-벡터 곱셈처럼 보이면 우변을 빼고 v를 인수분해할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "내가 이것을 처음 배웠을 때 반대로 말하는 느낌이 들었다. 기하학적으로,이 행렬은 우리의 그리드를 제니퍼의 격자로 변환시키는 것입니다.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "이제 우리가 가진 것은 새로운 행렬 A - 람다 곱하기 항등식입니다. 그리고 우리는 이 새로운 행렬 곱하기 v가 0 벡터를 제공하는 벡터 v를 찾고 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "그러나 수치학적으로, 그녀의 언어에서 우리의 언어로 번역하는 것이죠. 나를 마침내 두드린 것은 제니퍼가 말하는 것의 우리의 오해를 어떻게 다루는지 생각하는 것이다.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "이제, v 자체가 0 벡터라면 이는 항상 참이 될 것입니다. 그러나 그것은 지루합니다.",
  "model": "google_nmt",
  "from_community_srt": "우리의 좌표계에서 벡터를 생각하고 그녀가 실제로 의미하는 벡터로의 변환하는 것이다.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "우리가 원하는 것은 0이 아닌 고유벡터입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "그리고 5장과 6장을 보면 0이 아닌 벡터를 가진 행렬의 곱이 0이 되는 유일한 방법은 해당 행렬과 관련된 변환이 공간을 더 낮은 차원으로 압축하는 것이라는 것을 알게 될 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "다른 방법으로 생각하는 건 어떨까요? 내가 이전에 사용했던 비디오를 예로 들어서 우리의 좌표계에서 [3, 2]s를 가질 때 어떻게 나는 제니퍼의 좌표계에서는 [5/3,",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "그리고 그 찌그러짐은 행렬의 행렬식 0에 해당합니다.",
  "model": "google_nmt",
  "from_community_srt": "1/3]j라고 계산할까? 당신은 기초 매트릭스의 변화로 시작한다.",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "구체적으로 행렬 A에 열 2, 1과 2, 3이 있다고 가정하고 각 대각선 항목에서 가변 양인 람다를 빼는 것을 생각해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "그것은 제니퍼의 언어를 우리의 언어로  번역한다. 그리고 그것의 역을 취한다. 기억해봐라,",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "이제 람다를 조정하고 손잡이를 돌려 값을 변경하는 것을 상상해 보십시오.",
  "model": "google_nmt",
  "from_community_srt": "역변환은 처음에 있던 곳으로 되돌리는 것에 대응하는 새로운 변환이다.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "람다 값이 변경되면 행렬 자체도 변경되므로 행렬의 행렬식도 변경됩니다.",
  "model": "google_nmt",
  "from_community_srt": "실제로, 특히 2차원보다 큰 곳에서 생각할 때 이 역행렬을 구하려면 컴퓨터를 사용하는 것이 좋다.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "여기서 목표는 이 행렬식을 0으로 만드는 람다 값을 찾는 것입니다. 즉, 조정된 변환이 공간을 더 낮은 차원으로 압축한다는 의미입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 경우에, 제니퍼의 기저 벡터를 열행렬로 가지고 있는 기저 행렬의 역변환은 [1/3, -1/3],",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "이 경우 최적 지점은 람다가 1일 때 발생합니다.",
  "model": "google_nmt",
  "from_community_srt": "[1/3, 2/3]가 된다. 따라서,",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "물론, 다른 행렬을 선택했다면 고유값이 반드시 1이 아닐 수도 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "예를 들어 제니퍼의 좌표계에서 [3,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "최적의 지점은 람다의 다른 값에 도달할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "2]s를 보는 것은 우리는 이 기저 행렬의 역행렬을 [3,",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "내용이 좀 많지만 이것이 무엇을 말하는지 풀어보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "2]s 에 곱해야한다.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "람다가 1이면 행렬 A에서 람다를 곱하고 항등식을 곱하여 공간을 선으로 압축합니다.",
  "model": "google_nmt",
  "from_community_srt": "그러면  [5/3, 1/3]j이 나온다. 그래서,",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "이는 A 마이너스 람다 곱하기 항등 시간 v가 0 벡터와 같은 0이 아닌 벡터 v가 있다는 것을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "간단히 말해서 이것이 개개의 벡터의 모습을 변환하는 방법입니다. 좌표계 사이를 왔다 갔다 하면서요. 제니퍼의 기저 벡터를 가지고 있는 행렬은, 우리의 좌표계로 나타내어 있지만,",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "그리고 우리가 그것에 관심을 갖는 이유는 A 곱하기 v가 람다 곱하기 v와 같다는 것을 의미하기 때문이라는 것을 기억하세요. 이는 벡터 v가 A의 고유 벡터이며 변환 A 동안 자체 범위에 머무르는 것으로 읽을 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "그녀의 언어에서 우리의 언어로 벡터를 변환시켜 줍니다. 그리고 역행렬은 반대로 작용한다. 그러나 벡터는 좌표계를 이용해서만 묘사되는 것이 아니다. 이 다음부터는 말이죠.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "이 예에서 해당 고유값은 1이므로 v는 실제로 고정된 상태로 유지됩니다.",
  "model": "google_nmt",
  "from_community_srt": "변환을 행렬로 생각하는 방법은 중요합니다. 그리고 행렬 곱이 어떻게 연속적인 변환과 연관되는지 알게 될 것입니다.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "해당 추론 방식이 좋은지 확인해야 하는지 잠시 멈추고 숙고해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "잠시 멈추고 챕터3과 4를 다시보세요.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "서문에서 언급한 내용이 바로 이런 내용입니다.",
  "model": "google_nmt",
  "from_community_srt": "만약 불안하다면요. 일부 선형 변환을 고려해보죠.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "행렬식을 확실히 이해하지 못하고 왜 행렬식이 0이 아닌 해를 갖는 선형 방정식 시스템과 관련되어 있는지 알지 못한다면 이와 같은 표현은 전혀 예상치 못한 일처럼 느껴질 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "반 시계 방향으로 90 ° 회전된것 처럼요. 당신과 나는 이 행렬이 표현하는 것은 기저 벡터 i hat 과 j hat이 각각 어디로 가야하는지를 나타냅니다. i hat은 [0,",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "이것이 실제로 작동하는 모습을 보려면 열이 3, 0 및 1, 2인 행렬을 사용하여 처음부터 예제를 다시 살펴보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "1]s에서 끝나고, j hat은 [-1, 0]s에서 끝납니다. 그렇기 때문에 각 좌표는 우리의 행렬의 열행렬로써 쓰여질 수 있습니다.",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "람다 값이 고유값인지 확인하려면 이 행렬의 대각선에서 이를 빼고 행렬식을 계산하세요.",
  "model": "google_nmt",
  "from_community_srt": "그러나 이 표현은 우리의 초기 기저 벡터에 제한되어있습니다.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "이렇게 하면 우리는 람다에서 3 - 람다 곱하기 2 - 람다라는 특정 이차 다항식을 얻습니다.",
  "model": "google_nmt",
  "from_community_srt": "i hat 과 j hat 이 처음에 있던 자리를 알아야하고, 그것들이 어디로 가야하는지 알아야만  합니다. 그것도 우리 좌표계 안에서 말이죠. 제니퍼는 같은 90° 변환을 어떻게 표현할 수 있을까요? 당신은 이렇게 말하고 싶을지도 모릅니다.",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "람다는 이 행렬식이 0인 경우에만 고유값이 될 수 있으므로 가능한 고유값은 람다가 2이고 람다가 3이라는 결론을 내릴 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "우리의 회전행렬을 그녀의 언어로 변환하는 것으로써 나타낼 수 있다고 말이죠. 그러나 그것은 옳지 않습니다.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "실제로 이러한 고유값 중 하나(예: 람다가 2)를 갖는 고유벡터가 무엇인지 알아내기 위해 해당 람다 값을 행렬에 연결한 다음 대각선으로 변경된 행렬이 0으로 보내는 벡터를 해결합니다.",
  "model": "google_nmt",
  "from_community_srt": "그 행렬의 각각의 열 벡터는 우리의 기저벡터 i hat 과 j hat이 어떻게 가는지를 나타낼 뿐이고, 하지만 제니퍼가 원하는 행렬은 그녀의 기저 벡터가 어디로 가야하는지를 나타내야만 한다. 그리고 그 도착 지점 또한 그녀의 언어로 표시해야만 하죠. 다음은, 어떻게 이것이 실행되는지 가장 일반적인 방법을 설명한다.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "다른 선형 시스템과 같은 방식으로 이를 계산하면 해는 -1, 1 범위의 대각선에 있는 모든 벡터라는 것을 알 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "제니퍼의 언어로 쓰여진 임의의 벡터로부터 시작한다. 그녀의 언어에서 무엇이 일어나는지 따라가기 보다는 먼저, 우리는 우리의 언어로 번역할 것이다.",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "이는 변경되지 않은 행렬 3, 0, 1, 2가 모든 벡터를 2배로 늘리는 효과가 있다는 사실에 해당합니다.",
  "model": "google_nmt",
  "from_community_srt": "기저 행렬의 변환을 사용해서 말이죠. 열 벡터의 요소는 그녀의 기저 벡터가 우리 언어의 무엇을 말하는지 나타냅니다. 이것은 우리에게 같은 벡터를 제공합니다 우리의 언어로 쓰여있는 벡터로 말이죠.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "이제 2D 변환에는 고유벡터가 필요하지 않습니다.",
  "model": "google_nmt",
  "from_community_srt": "그 다음에 얻어진 변환 행렬을 왼쪽에 곱합니다.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "예를 들어 90도 회전을 가정해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "이것은 어디로 벡터가 움직일지를 알려줍니다. 하지만 여전히 우리의 언어이다.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "이것은 자체 범위에서 모든 벡터를 회전시키기 때문에 고유 벡터가 없습니다.",
  "model": "google_nmt",
  "from_community_srt": "마지막 단계로써 기저 행렬의 역함수를 평범하게 왼쪽에 곱하는 것으로써, 변환 된 벡터를 얻을 수 있습니다",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "실제로 이와 같이 회전의 고유값을 계산해 보면 어떤 일이 발생하는지 확인하세요.",
  "model": "google_nmt",
  "from_community_srt": "제니퍼의 언어로 말이죠. 우리는이 작업을 수행 할 수 있습니다.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "해당 행렬에는 열 0, 1과 음수 1, 0이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "그녀의 언어로 작성된 모든 벡터에 대해서요. 먼저, 기저의 변환을 적용하고 그리고,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "대각선 요소에서 람다를 빼고 행렬식이 0이 되는 시점을 찾습니다.",
  "model": "google_nmt",
  "from_community_srt": "선형 변환 후, 그리고, 기저 벡터의 역변환을 통해서 말이죠.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "이 경우 다항식 람다 제곱에 1을 더한 값을 얻습니다.",
  "model": "google_nmt",
  "from_community_srt": "세 행렬의 구성은 우리에게 제니퍼의 변환 행렬을 제니퍼의 언어로 제공합니다 이것은 그녀의 언어로 쓰인 벡터에 작용해서 변환 후,",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "해당 다항식의 유일한 근은 허수 i와 음수 i입니다.",
  "model": "google_nmt",
  "from_community_srt": "그녀의 언어로 이루어진 벡터를 뱉습니다. 이 구체적인 예를 들어 우리의 언어에서 [2,",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "실수 해가 없다는 사실은 고유벡터가 없다는 것을 나타냅니다.",
  "model": "google_nmt",
  "from_community_srt": "1]s 및 [-1,",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "마음 속에 간직할 가치가 있는 또 다른 매우 흥미로운 예는 가위입니다.",
  "model": "google_nmt",
  "from_community_srt": "1]s인 제니퍼의 기저 벡터가 90 ° 회전 될 때 이 세 가지 행렬의 곱으로써 나타내어지고 당신은 그것을 통해 계산할 경우 열이 [1/3,",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "그러면 i-hat이 제자리에 고정되고 j-hat 1이 위로 이동하므로 해당 행렬에는 열 1, 0과 1, 1이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "5/3] 및 [-2/3, -1/3]가 됩니다.",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "x축의 모든 벡터는 제자리에 고정되어 있으므로 고유값 1을 갖는 고유벡터입니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서 제니퍼가 그 행렬을 그녀의 좌표계에 있는 벡터에 곱할 때 그것의 90 ° 회전 된 버전을 반환합니다.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "사실, 이것들은 유일한 고유벡터입니다.",
  "model": "google_nmt",
  "from_community_srt": "그녀의 좌표계로 표현되면서 말이죠.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "대각선에서 람다를 빼고 행렬식을 계산하면 1 빼기 람다 제곱이 나옵니다.",
  "model": "google_nmt",
  "from_community_srt": "일반적으로, 당신이 A^(-1)MA와 같은 표현을 볼 때 마다 방금까지와 같은 수학적인 느낌을 떠올리기 바란다.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "그리고 이 표현식의 유일한 근은 람다가 1과 같다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "중간의 행렬 M 은 선형 변환을 나타내고, 그리고 바깥쪽 행렬 A와 A^(-1)은 관점의 변환을 나타낸다.",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "이는 우리가 기하학적으로 보는 것과 일치합니다. 모든 고유벡터는 고유값 1을 갖습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "그러나 고유값은 하나만 가질 수도 있지만 고유벡터로 가득 찬 선 이상을 가질 수도 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "그리고 총 행렬의 곱은 같은 변환을 나태지만, 다만 기저가 다를 뿐이다. 좌표계의 변환을 왜 해야하는지 궁금증을 갖는 당신을 위해, 고유 벡터와 고유 값을 다룬 다음 비디오가",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "간단한 예는 모든 것을 2로 확장하는 행렬입니다.",
  "model": "google_nmt",
  "from_community_srt": "정말 중요한 예제를 제공 할 것입니다.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "유일한 고유값은 2이지만 평면의 모든 벡터는 해당 고유값을 갖는 고유벡터가 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "이제 마지막 주제로 넘어가기 전에 잠시 멈춰서 이에 대해 생각해 볼 좋은 시간입니다.",
  "model": "google_nmt",
  "from_community_srt": "그때 만나!",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "지난 비디오의 아이디어에 크게 의존하는 고유기초 아이디어로 여기서 마무리하고 싶습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "우리의 기저 벡터가 우연히 고유 벡터가 된다면 무슨 일이 일어나는지 살펴보세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "예를 들어, i-hat은 -1로 스케일링되고 j-hat은 2로 스케일링될 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "새 좌표를 행렬의 열로 작성하면 i-hat과 j-hat의 고유값인 음수 1과 2의 스칼라 배수가 행렬의 대각선에 있고 다른 모든 항목은 0입니다. .",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "행렬의 대각선 이외의 모든 부분에서 0이 있을 때마다 이를 대각 행렬이라고 부르는 것이 합리적입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "그리고 이것을 해석하는 방법은 모든 기본 벡터가 고유 벡터이고 이 행렬의 대각선 항목이 고유값이라는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "대각 행렬을 작업하기 훨씬 더 좋게 만드는 많은 것들이 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "한 가지 큰 점은 이 행렬 자체를 여러 번 곱하면 어떤 일이 일어날지 계산하는 것이 더 쉽다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "이러한 행렬 중 하나는 각 기본 벡터를 일부 고유값만큼 스케일링하므로 해당 행렬을 여러 번 적용하는 것(가령 100번)은 각 기본 벡터를 해당 고유값의 100승으로 스케일링하는 것과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "이와 대조적으로, 비대각선 행렬의 100제곱을 계산해 보십시오.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "정말로, 한번 시도해 보세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "악몽이다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "물론, 기본 벡터가 고유벡터가 될 정도로 운이 좋은 경우는 거의 없습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "그러나 변환에 이 비디오의 시작 부분과 같이 전체 공간에 걸쳐 있는 집합을 선택할 수 있을 만큼 고유벡터가 많은 경우 이러한 고유벡터가 기본 벡터가 되도록 좌표계를 변경할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "지난 영상에서 기저 변경에 대해 이야기했지만, 여기서는 현재 좌표계에 쓰여진 변환을 다른 시스템으로 표현하는 방법에 대해 매우 빠르게 설명하겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "새 기저로 사용하려는 벡터의 좌표(이 경우에는 두 개의 고유 벡터를 의미)를 선택한 다음 해당 좌표를 기저 행렬의 변경이라고 알려진 행렬의 열로 만듭니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "원래 변환을 끼우고 기본 행렬의 변경 사항을 오른쪽에 배치하고 기본 행렬 변경의 역수를 왼쪽에 배치하면 결과는 동일한 변환을 나타내는 행렬이 되지만 새 기본 벡터 좌표의 관점에서 보면 체계.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "고유벡터를 사용하여 이 작업을 수행하는 요점은 이 새로운 행렬이 해당 대각선 아래에 해당하는 고유값과 함께 대각선이 되도록 보장된다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "이는 기본 벡터에 발생하는 일이 변환 중에 크기가 조정되는 좌표계에서의 작업을 나타내기 때문입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "고유벡터이기도 한 기저 벡터 세트를 다시 고유기저라고 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "따라서 예를 들어 이 행렬의 100제곱을 계산해야 하는 경우 고유기저로 변경하고 해당 시스템에서 100제곱을 계산한 다음 표준 시스템으로 다시 변환하는 것이 훨씬 쉬울 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "모든 변환에 대해 이 작업을 수행할 수는 없습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "예를 들어 전단에는 전체 공간을 포괄할 만큼 고유벡터가 충분하지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "그러나 고유기저를 찾을 수 있다면 행렬 연산이 정말 멋질 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "이것이 실제로 어떻게 보이는지, 그리고 이것이 놀라운 결과를 생성하는 데 어떻게 사용될 수 있는지 알아보기 위해 매우 깔끔한 퍼즐을 풀고자 하는 분들을 위해 여기 화면에 프롬프트를 남겨 두겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "약간의 노력이 필요하지만, 즐기시면 될 것 같아요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "이 시리즈의 다음이자 마지막 비디오는 추상적인 벡터 공간에 관한 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]