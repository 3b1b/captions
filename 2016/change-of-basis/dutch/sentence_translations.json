[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Eigenvectoren en eigenwaarden is een van de onderwerpen die veel studenten bijzonder niet intuïtief vinden.",
  "model": "google_nmt",
  "from_community_srt": "Als ik een vector heb in een 2D-ruimte, hebben we een standaard manier om hem te bespreken met coördinaten In dit geval heeft de vector de coördinaten  [3, 2], wat betekent, gaande van zijn staart tot top, beweeg je 3 eenheden naar rechts en 2 eenheden naar boven.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Vragen als: waarom doen we dit en wat betekent dit eigenlijk, blijven maar al te vaak ronddrijven in een onbeantwoorde zee van berekeningen.",
  "model": "google_nmt",
  "from_community_srt": "Nu, de meer lineaire-algebra georiënteerde manier om de coördinaten te bespreken is ieder van deze nummers beschouwen als een schaalfactor,",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "En terwijl ik de video's van deze serie heb uitgebracht, hebben veel van jullie opgemerkt dat ze ernaar uitkijken om dit onderwerp in het bijzonder te visualiseren.",
  "model": "google_nmt",
  "from_community_srt": "een ding dat de vector uitrekt of samendrukt. Je denkt aan de eerste coördinaat als schaling van i-hoedje, de vector met lengte 1,",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Ik vermoed dat de reden hiervoor niet zozeer is dat eigendingen bijzonder ingewikkeld of slecht uitgelegd zijn.",
  "model": "google_nmt",
  "from_community_srt": "wijzende naar rechts met de tweede coördinaat schalende j-hoedje, de vector met lengte 1,",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "In feite is het relatief eenvoudig, en ik denk dat de meeste boeken het prima uitleggen.",
  "model": "google_nmt",
  "from_community_srt": "wijzend naar boven.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Het probleem is dat het alleen echt zinvol is als je een goed visueel inzicht hebt in veel van de onderwerpen die eraan voorafgaan.",
  "model": "google_nmt",
  "from_community_srt": "De som van deze 2 geschaalde vectors is wat de coördinaten moeten beschrijven.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Het belangrijkste hier is dat je weet hoe je matrices als lineaire transformaties moet beschouwen, maar dat je ook vertrouwd moet zijn met zaken als determinanten, lineaire stelsels van vergelijkingen en verandering van basis.",
  "model": "google_nmt",
  "from_community_srt": "Je kan denken aan deze 2 speciale vectoren als een verzameling van alle impliciete veronderstellingen van ons coördinaten systeem. Het feit dat het eerste getal voor de beweging naar rechts staat en de tweede voor de opwaartse beweging",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Verwarring over eigenstuffs heeft meestal meer te maken met een wankele basis in een van deze onderwerpen dan met eigenvectoren en eigenwaarden zelf.",
  "model": "google_nmt",
  "from_community_srt": "exact hoeveel eenheden of afstanden Al dat is samengevat in de keuze van i-hoedje en j-hoedje, alsof de vectoren welke schalingfactoren  zijn echt bedoelt zijn om te schalen. Hoe dan ook,",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Overweeg om te beginnen een lineaire transformatie in twee dimensies, zoals hier getoond.",
  "model": "google_nmt",
  "from_community_srt": "om te vertalen tussen vectoren en aantal nummers wordt een coördinaten systeem genoemd en de 2 speciale vectoren,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Het verplaatst de basisvector i-hat naar de coördinaten 3, 0, en j-hat naar 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "i-hoedje en j-hoedje worden basis vectoren genoemd van ons standaard coördinaten systeem.",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Het wordt dus weergegeven met een matrix waarvan de kolommen 3, 0 en 1, 2 zijn.",
  "model": "google_nmt",
  "from_community_srt": "Waar ik over wil praten is het idee van het gebruik van verschillende basis vectoren.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Concentreer u op wat het met een bepaalde vector doet, en denk na over de reikwijdte van die vector, de lijn die door zijn oorsprong en zijn punt loopt.",
  "model": "google_nmt",
  "from_community_srt": "Bijvoorbeeld, laten we zeggen dat je een vriendin hebt, Jennifer die andere basis vectoren gebruikt, welke ik b1 en b2 zal noemen.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "De meeste vectoren zullen tijdens de transformatie uit hun bereik worden gehaald.",
  "model": "google_nmt",
  "from_community_srt": "Haar eerste basis vector b1 wijst een klein beetje naar rechts boven en haar tweede vector b2 wijst naar links boven.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Ik bedoel, het zou behoorlijk toevallig lijken als de plaats waar de vector landde zich ook ergens op die lijn zou bevinden.",
  "model": "google_nmt",
  "from_community_srt": "Nu, her bekijk de vector dat ik eerder heb laten zien.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Maar sommige speciale vectoren blijven op hun eigen bereik, wat betekent dat het effect dat de matrix op zo'n vector heeft, alleen maar is dat deze wordt uitgerekt of platgedrukt, zoals bij een scalaire vector.",
  "model": "google_nmt",
  "from_community_srt": "Degene die je wil beschrijven met het gebruik van de coördinaten  [3, 2] nemende onze basis vectoren i-hoedje en j-hoedje. Jennifer zou echter deze vector beschrijven met de coördinaten  [5/3, 1/3].",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Voor dit specifieke voorbeeld is de basisvector i-hat zo'n speciale vector.",
  "model": "google_nmt",
  "from_community_srt": "Wat betekent dat de manier om die vector te bekomen gebruikende haar 2 basis vectoren is het schalen van b1 met 5/3 en b2 met 1/3",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "De spanwijdte van i-hat is de x-as, en uit de eerste kolom van de matrix kunnen we zien dat i-hat drie keer zichzelf verplaatst, nog steeds op die x-as.",
  "model": "google_nmt",
  "from_community_srt": "en dan hun optellen. Binnenkort zal ik jullie laten zien hoe je deze 2 nummers 5/3 en 1/3 kan bekomen Hoofdzakelijk,",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Bovendien wordt, vanwege de manier waarop lineaire transformaties werken, elke andere vector op de x-as ook gewoon met een factor 3 uitgerekt, en blijft dus op zijn eigen spanwijdte.",
  "model": "google_nmt",
  "from_community_srt": "wanneer Jennifer haar coördinaten gebruikt om de vector te bespreken denkt ze aan haar eerste coördinaat als schaling van b1 en de tweede coördinaat als schaling van b2 en breng dan de resultaten tesamen Wat zij bekomt zal normaal gezien volledig anders zijn dan",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Een iets geniepiger vector die tijdens deze transformatie op zijn eigen bereik blijft, is negatief 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "de vector waar jij en ik zouden aan denken nemende deze coördinaten.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Het wordt uiteindelijk met een factor 2 uitgerekt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "En nogmaals, lineariteit impliceert dat elke andere vector op de diagonale lijn die door deze man wordt overspannen, gewoon met een factor 2 zal worden uitgerekt.",
  "model": "google_nmt",
  "from_community_srt": "Om een beetje meer precies over deze opzet hier haar eerste basis vector b1 is iets wat we zouden beschrijven met de coördinaten [2, 1] en haar tweede basis vector b2 is iets wat we zouden beschrijven als  [-1,",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "En voor deze transformatie zijn dat alle vectoren met de speciale eigenschap om binnen hun bereik te blijven.",
  "model": "google_nmt",
  "from_community_srt": "1]. Maar het is belangrijk om te beseffen dat van haar perspectief, in haar systeem deze vectoren coördinaten  [1,",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Die op de x-as worden met een factor 3 uitgerekt, en die op deze diagonale lijn worden met een factor 2 uitgerekt.",
  "model": "google_nmt",
  "from_community_srt": "0] en [0, 1] bezitten Deze zijn wat de betekenis van coördinaten  [1, 0] en [0, 1] definieert in haar wereld.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Elke andere vector zal tijdens de transformatie enigszins worden geroteerd en van de lijn worden geslagen die hij overspant.",
  "model": "google_nmt",
  "from_community_srt": "In werkelijkheid zijn we over 2 verschillende talen aan het spreken.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Zoals je misschien al geraden hebt, worden deze speciale vectoren de eigenvectoren van de transformatie genoemd, en aan elke eigenvector is een zogenaamde eigenwaarde gekoppeld, wat precies de factor is waarmee deze wordt uitgerekt of platgedrukt tijdens de transformatie.",
  "model": "google_nmt",
  "from_community_srt": "We kijken naar dezelfde vectoren in de ruimte, maar Jennifer gebruikt verschillende woorden en nummers om ze te beschrijven. Laat me snel iets zeggen over hoe ik de dingen voorstel hier als ik animeer in een 2D ruimte. Meestal gebruik ik een vierkant raster maar het raster is enkel een constructie, een manier om ons coördinaten systeem te visualiseren en zo hangt het af van onze keuze van basis vectoren.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Er is natuurlijk niets bijzonders aan uitrekken versus samendrukken, of aan het feit dat deze eigenwaarden positief zijn.",
  "model": "google_nmt",
  "from_community_srt": "De ruimte zelf heeft geen wezenlijk raster Jennifer mag misschien haar eigen raster tekenen welke ook een verzonnen constructie zou zijn aantonende dat het niets meer is dan een visueel hulpmiddel",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "In een ander voorbeeld zou je een eigenvector kunnen hebben met een eigenwaarde negatief 1 half, wat betekent dat de vector wordt omgedraaid en geplet met een factor 1 half.",
  "model": "google_nmt",
  "from_community_srt": "om haar te helpen met de betekenis van haar coördinaten.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Maar het belangrijkste hier is dat het op de lijn blijft die het uitstrekt, zonder er vanaf te draaien.",
  "model": "google_nmt",
  "from_community_srt": "Haar oorsprong zou echter samenvallen met de onze doordat iedereen het eens is met wat de coördinaten [0, 0] moeten beteken Het is hetgene wat je krijgt als je eender welke vector schaalt met 0.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Om een idee te krijgen van waarom dit nuttig zou kunnen zijn om over na te denken, kun je een driedimensionale rotatie overwegen.",
  "model": "google_nmt",
  "from_community_srt": "maar de richting van haar assen en de plaatsing van haar raster lijnen zullen anders zijn, afhankelijk van haar keuze van basis vectoren Zo,",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Als je voor die rotatie een eigenvector kunt vinden, een vector die op zijn eigen bereik blijft, dan heb je de rotatie-as gevonden.",
  "model": "google_nmt",
  "from_community_srt": "na al datgene gezegd een natuurlijke vraag om te stellen is Hoe vertalen we tussen de verschillende coördinaten systemen?",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "En het is veel gemakkelijker om aan een 3D-rotatie te denken in termen van een bepaalde rotatie-as en een hoek waarover deze roteert, dan te denken aan de volledige 3x3-matrix die bij die transformatie hoort.",
  "model": "google_nmt",
  "from_community_srt": "Als bijvoorbeeld Jennifer een vector beschrijft met coördinaten [-1, 2] wat zou dat dan zijn in ons coördinaten systeem? Hoe vertaal je haar taal in de onze? Wel, wat onze coördinaten zeggen is dat deze vector  -1 b1 + 2 b2 is.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "In dit geval zou de corresponderende eigenwaarde trouwens 1 moeten zijn, aangezien rotaties nooit iets uitrekken of samendrukken, zodat de lengte van de vector hetzelfde zou blijven.",
  "model": "google_nmt",
  "from_community_srt": "En van ons perspectief heeft b1 als coördinaten [2,",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Dit patroon komt veel voor in de lineaire algebra.",
  "model": "google_nmt",
  "from_community_srt": "1] en b2 als coördinaten [-1,",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Bij elke lineaire transformatie die door een matrix wordt beschreven, kun je begrijpen wat deze doet door de kolommen van deze matrix af te lezen als de landingsplaatsen voor basisvectoren.",
  "model": "google_nmt",
  "from_community_srt": "1] We kunnen eigenlijk  -1 b1 + 2 b2 berekenen als ze zijn gepresenteerd in ons coördinaten systeem",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Maar vaak is een betere manier om tot de kern te komen van wat de lineaire transformatie feitelijk doet, minder afhankelijk van uw specifieke coördinatensysteem, het vinden van de eigenvectoren en eigenwaarden.",
  "model": "google_nmt",
  "from_community_srt": "En dit uitwerkende zal je een vector krijgen met coördinaten [-4, 1] Zo, dat is hoe we de vector zouden beschrijven als zij denkt aan [-1,",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Ik zal hier niet de volledige details behandelen over methoden voor het berekenen van eigenvectoren en eigenwaarden, maar ik zal proberen een overzicht te geven van de computationele ideeën die het belangrijkst zijn voor een conceptueel begrip.",
  "model": "google_nmt",
  "from_community_srt": "2] Dit proces hier van schalen ieder van haar basis vectoren met de samenhangende coördinaten van sommige vectoren dan ze samen brengen zal misschien wat bekend aanvoelen Het is matrix-vector vermenigvuldiging met een matrix wiens kolommen, Jennifers basis vectoren representeren in onze taal In feite,",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Symbolisch gezien ziet dit er zo uit hoe het idee van een eigenvector eruit ziet.",
  "model": "google_nmt",
  "from_community_srt": "eens je matrix-vector vermenigvuldiging begrijpt als het toepassen van een zekere lineaire transformatie.",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A is de matrix die een transformatie vertegenwoordigt, met v als de eigenvector, en lambda is een getal, namelijk de overeenkomstige eigenwaarde.",
  "model": "google_nmt",
  "from_community_srt": "zeg, bij het kijken van de meeste belangrijke vide in deze serie, hoofdstuk 3 Er is een intuïtieve manier om na te denken over wat er hier gebeurt.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Wat deze uitdrukking zegt is dat het matrix-vectorproduct, A maal v, hetzelfde resultaat geeft als het schalen van de eigenvector v met een bepaalde waarde lambda.",
  "model": "google_nmt",
  "from_community_srt": "Een matrix wiens kolommen Jennifers basis vectoren representeren kan  worden gezien als een transformatie die onze basis vectoren, i-hoedje en j-hoedje beweegt de dingen waaraan we denken wanneer we [1,",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Het vinden van de eigenvectoren en hun eigenwaarden van een matrix A komt dus neer op het vinden van de waarden van v en lambda die deze uitdrukking waar maken.",
  "model": "google_nmt",
  "from_community_srt": "0] en [0, 1] zeggen naar Jennifers basis vectoren de dingen waaraan zij denkt als ze zegt  [1, 0] en [0,",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "In het begin is het een beetje lastig om mee te werken, omdat de linkerkant de matrix-vectorvermenigvuldiging vertegenwoordigt, maar de rechterkant hier de scalaire vectorvermenigvuldiging.",
  "model": "google_nmt",
  "from_community_srt": "1] Om je te laten zien hoe dit werkt laten we zien wat het moet betekenen om de vector met coördinaten [-1, 2] nemen en toepassen op die transformatie voor de lineaire transformatie we zijn denkende aan deze vector",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Laten we beginnen met het herschrijven van die rechterkant als een soort matrix-vectorvermenigvuldiging, met behulp van een matrix die het effect heeft dat elke vector met een factor lambda wordt geschaald.",
  "model": "google_nmt",
  "from_community_srt": "als een zekere lineaire combinatie van onze basis vectoren -1 x i-hoedje + 2 x j-hoedje en het sleutel kenmerk van een lineaire transformatie",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "De kolommen van zo'n matrix vertegenwoordigen wat er met elke basisvector gebeurt, en elke basisvector wordt eenvoudigweg vermenigvuldigd met lambda, dus deze matrix zal het getal lambda onderaan de diagonaal hebben, met overal nullen.",
  "model": "google_nmt",
  "from_community_srt": "is dat de uiteindelijke vector diezelfde lineaire combinatie zal zijn maar van de nieuwe basis vectoren -1 keer de plaats waar i-hoedje valt + 2 keer de plaats waar j-hoedje valt Zo wat deze matrix doet",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "De gebruikelijke manier om deze man te schrijven is door die lambda eruit te halen en het op te schrijven als lambda maal i, waarbij i de identiteitsmatrix is met 1s langs de diagonaal.",
  "model": "google_nmt",
  "from_community_srt": "is onze misverstand van wat Jennifer bedoelt veranderen in de echte vector waar ze naar verwijst.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Omdat beide zijden lijken op matrix-vectorvermenigvuldiging, kunnen we die rechterkant aftrekken en de v wegwerken.",
  "model": "google_nmt",
  "from_community_srt": "Ik herinner dat wanneer Ik dit voor eerst leerde het altijd een soort omgekeerde voelde voor mij. geometrisch gezien transformeert deze matrix ons raster in Jennifers raster maar numeriek,",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Dus wat we nu hebben is een nieuwe matrix, A minus lambda maal de identiteit, en we zoeken naar een vector v zodat deze nieuwe matrix maal v de nulvector oplevert.",
  "model": "google_nmt",
  "from_community_srt": "vertaalt het een vector beschreven in haar taal naar onze taal wat het me uiteindelijk liet snappen was voor mij",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Dit zal altijd waar zijn als v zelf de nulvector is, maar dat is saai.",
  "model": "google_nmt",
  "from_community_srt": "denken over hoe het neemt wat onze misconceptie was over wat Jennifer bodoelde de vector die we krijgen gebruikende dezelfde coördinaten maar in ons systeem",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Wat we willen is een eigenvector die niet nul is.",
  "model": "google_nmt",
  "from_community_srt": "dan transformeren het naar de vector dat ze echt bedoelde.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "En als je hoofdstuk 5 en 6 bekijkt, weet je dat de enige manier waarop het product van een matrix met een vector die niet nul is, nul kan worden, is als de transformatie die bij die matrix hoort de ruimte in een lagere dimensie perst.",
  "model": "google_nmt",
  "from_community_srt": "Wat over dat omgekeerd gaan? In het voorbeeld dat ik eerder deze video gebruikte waar ik de vector heb met coördinaten [3, 2] in ons systeem Hoe heb ik berekend dat het de coördinaten [5/3,",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "En die verkleining komt overeen met een nuldeterminant voor de matrix.",
  "model": "google_nmt",
  "from_community_srt": "1/3]  zou hebben in Jennifers systeem? Je begint met het veranderen van basis matrix",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Om concreet te zijn, laten we zeggen dat matrix A de kolommen 2, 1 en 2, 3 heeft, en denk erover na om een variabel bedrag, lambda, af te trekken van elke diagonale invoer.",
  "model": "google_nmt",
  "from_community_srt": "dat vertaalt Jennifers taal in de onze dan neem je de inverse herinner je dat de inverse van een transformatie",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Stel je nu voor dat je lambda aanpast, aan een knop draait om de waarde ervan te veranderen.",
  "model": "google_nmt",
  "from_community_srt": "een nieuwe transformatie is die gelijk is aan de eerste achterstevoren In praktijk,",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "Naarmate de waarde van lambda verandert, verandert de matrix zelf, en dus verandert de determinant van de matrix.",
  "model": "google_nmt",
  "from_community_srt": "voornamelijk als je werkt in meer dan 2 dimensies zou je een computer gebruiken om de matrix te berekenen die eigenlijk de inverse voorstelt",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Het doel hier is om een waarde van lambda te vinden die deze determinant nul maakt, wat betekent dat de aangepaste transformatie de ruimte in een lagere dimensie drukt.",
  "model": "google_nmt",
  "from_community_srt": "In dit geval, de inverse van de verandering van de basis matrix heeft Jennifers basis als zijn colommen eindigen met zijnde kolommen  [1/3,",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "In dit geval komt de goede plek wanneer lambda gelijk is aan 1.",
  "model": "google_nmt",
  "from_community_srt": "-1/3] en [1/3,",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Als we een andere matrix hadden gekozen, zou de eigenwaarde uiteraard niet noodzakelijkerwijs 1 zijn.",
  "model": "google_nmt",
  "from_community_srt": "2/3] Bijvoorbeeld om te zien wat de vector [3,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "De goede plek zou kunnen worden bereikt door een andere waarde van lambda.",
  "model": "google_nmt",
  "from_community_srt": "2] eruit ziet in Jennifers systeem vermenigvuldigen we de inverse verandering van basis matrix met de vector  [3,",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Dit is dus nogal veel, maar laten we ontrafelen wat dit zegt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Wanneer lambda gelijk is aan 1, drukt de matrix A minus lambda maal de identiteit de ruimte op een lijn.",
  "model": "google_nmt",
  "from_community_srt": "2] wat [5/3, 1/3] wordt Zo dat in een notendop is hoe we de beschrijving van individuele vectoren vertalen",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Dat betekent dat er een vector v is die niet nul is, zodat A minus lambda maal de identiteit maal v gelijk is aan de nulvector.",
  "model": "google_nmt",
  "from_community_srt": "heen en terug tussen coördinaten systemen De matrix wiens kolommen Jennifers basis vectoren representern",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "En onthoud, de reden dat dit ons interesseert, is omdat het betekent dat A maal v gelijk is aan lambda maal v, wat je kunt aflezen als te zeggen dat de vector v een eigenvector is van A, die op zijn eigen span blijft tijdens de transformatie A.",
  "model": "google_nmt",
  "from_community_srt": "maar geschreven in onze coördinaten vertaalt vectoren van haar taal naar onze taal En de inverse matrix doet het tegen over gestelde. maar vectoren zijn niet het enigste ding dat we gebruiken om coördinaten te beschrijven Voor het volgende deel",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "In dit voorbeeld is de corresponderende eigenwaarde 1, dus v zou eigenlijk gewoon op zijn plaats blijven.",
  "model": "google_nmt",
  "from_community_srt": "is het belangrijk dat je comfortabel bent met het representeren van transformaties met matrices en dat je weet hoe matrix vermenigvuldigin overeenkomt met samenstellen van opeenvolgende transformaties",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Pauzeer en denk na of je ervoor moet zorgen dat die redenering goed voelt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Dit is het soort dingen dat ik in de inleiding noemde.",
  "model": "google_nmt",
  "from_community_srt": "Pauzeer en neem een kijkje naar hoofdstuk 3 en 4 als iets van dat ongemakkelijk voelt Bekijk een paar lineaire transformaties",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Als je geen goed inzicht had in de determinanten en waarom ze betrekking hebben op lineaire stelsels van vergelijkingen met oplossingen die niet nul zijn, zou een uitdrukking als deze volkomen uit de lucht komen vallen.",
  "model": "google_nmt",
  "from_community_srt": "zoals een 90° draaiing tegen de klok in wanneer jij en ik dit representeren met de matrix volgen we waar de basis vectoren i-hoedje en j-hoedje gaan",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Om dit in actie te zien, gaan we het voorbeeld vanaf het begin opnieuw bekijken, met een matrix waarvan de kolommen 3, 0 en 1, 2 zijn.",
  "model": "google_nmt",
  "from_community_srt": "i-hoedje eindigt op het punt met coördinaten [0, 1] en j-hoedje eindigt op het punt met coördinaten  [-1,",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Om te bepalen of een waarde lambda een eigenwaarde is, trekt u deze af van de diagonalen van deze matrix en berekent u de determinant.",
  "model": "google_nmt",
  "from_community_srt": "0] Deze coördinaten worden de kolommen van onze matrix maar deze representatie is zwaar vast hangend aan onze keuze van basis vectoren van het feit dat we i-hoedje en j-hoedje volgen in de eerste plaats.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Als we dit doen, krijgen we een bepaald kwadratisch polynoom in lambda, 3 min lambda maal 2 min lambda.",
  "model": "google_nmt",
  "from_community_srt": "tot het feit dat we aan het opnemen zijn waar ze terecht komen on ons eigen coördinaten systeem. Hoe zou Jennifer deze zelfde 90° rotatie van ruimte beschrijven? Je zou misschien geneigd zijn om gewoon",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Omdat lambda alleen een eigenwaarde kan zijn als deze determinant nul is, kun je concluderen dat de enige mogelijke eigenwaarden zijn: lambda is gelijk aan 2 en lambda is gelijk aan 3.",
  "model": "google_nmt",
  "from_community_srt": "de kolommen van onze rotatie matrix te vertalen in Jennifers taal. maar dat is niet volledig juist.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Om erachter te komen wat de eigenvectoren zijn die daadwerkelijk een van deze eigenwaarden hebben, zeg lambda is gelijk aan 2, plug je die waarde van lambda in de matrix in en los je vervolgens op voor welke vectoren deze diagonaal gewijzigde matrix naar nul stuurt.",
  "model": "google_nmt",
  "from_community_srt": "Deze kolommen representeren waar onze basis vectoren i-hoedje en j-hoedje gaan. maar de matrix dat Jennifer wil zou moeten voorstellen waar haar basis vectoren landen en het moet beschrijven waar deze landingsplaatsen  zijn in haar taal. Here is een veelvoorkomende manier om te denken over hoe het is gebeurt.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Als je dit op dezelfde manier zou berekenen als elk ander lineair systeem, zou je zien dat de oplossingen alle vectoren op de diagonale lijn zijn, opgespannen door negatief 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Begin met eender welke vector geschreven in Jennifers taal. Eerder dan proberen te volgen wat er gebeurt in termen van haar taal eerst,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Dit komt overeen met het feit dat de ongewijzigde matrix, 3, 0, 1, 2, het effect heeft dat al deze vectoren met een factor 2 worden uitgerekt.",
  "model": "google_nmt",
  "from_community_srt": "gaan we het vertalen in onze taal gebruikende de verandering van basis matrix de ene wiens kolommen haar basis vectoren representeren in onze taal Dit geeft ons dezelfde vector, maar nu geschreven in onze taal.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Nu hoeft een 2D-transformatie geen eigenvectoren te hebben.",
  "model": "google_nmt",
  "from_community_srt": "Dan, pas je de transformatie matrix toe op wat je krijgt bij het vermenigvuldigen aan de linkerkant;",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Beschouw bijvoorbeeld een rotatie van 90 graden.",
  "model": "google_nmt",
  "from_community_srt": "Dit vertelt ons waar de vector valt maar nog steeds in onze taal.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Dit heeft geen eigenvectoren omdat het elke vector buiten zijn eigen bereik roteert.",
  "model": "google_nmt",
  "from_community_srt": "Zo als laatste stap pas de inverse verandering van basis matrix toe vermenigvuldigt aan de linkerkant,",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Als je daadwerkelijk probeert de eigenwaarden van een rotatie als deze te berekenen, kijk dan eens wat er gebeurt.",
  "model": "google_nmt",
  "from_community_srt": "zoals gewoonlijk om de getransformeerde vector te krijgen maar nu in Jennifers taal.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "De matrix heeft kolommen 0, 1 en negatief 1, 0.",
  "model": "google_nmt",
  "from_community_srt": "Sinds we dit kunnen doen met eender elke vector geschreven in haar taal eerst,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Trek lambda af van de diagonale elementen en zoek wanneer de determinant nul is.",
  "model": "google_nmt",
  "from_community_srt": "de verandering van basis toe passende dan de transformatie dan de inverse verandering van basis",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "In dit geval krijg je de polynoom lambda in het kwadraat plus 1.",
  "model": "google_nmt",
  "from_community_srt": "die samenstelling van drie matrices geeft ons de transformatie matrix in Jennifers taal het neemt in een vector van haar taal",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "De enige wortels van dat polynoom zijn de denkbeeldige getallen, i en negatieve i.",
  "model": "google_nmt",
  "from_community_srt": "en spuugt de getransformeerde versie van die vector uit in onze taal voor dit specifiek voorbeeld",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Het feit dat er geen oplossingen voor reële getallen zijn, geeft aan dat er geen eigenvectoren zijn.",
  "model": "google_nmt",
  "from_community_srt": "wanneer Jennifers basis vectoren eruit zien als [2, 1] en [-1,",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Een ander behoorlijk interessant voorbeeld dat de moeite waard is om in je achterhoofd te houden, is een schaar.",
  "model": "google_nmt",
  "from_community_srt": "1] en wanneer de transformatie een 90° rotatie is het product van deze drie matrices als je het uitwerkt",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Hierdoor wordt i-hat op zijn plaats gezet en wordt j-hat 1 verplaatst, zodat de matrix kolommen 1, 0 en 1, 1 heeft.",
  "model": "google_nmt",
  "from_community_srt": "heeft kolommen  [1/3, 5/3] en [-2/3, -1/3] Zo als Jennifer die matrix vermenigvuldigt met de coördinaten als een vector in haar systeem",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Alle vectoren op de x-as zijn eigenvectoren met eigenwaarde 1, aangezien ze op hun plaats blijven.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "In feite zijn dit de enige eigenvectoren.",
  "model": "google_nmt",
  "from_community_srt": "zal het de 90° geroteerde versie van die vector terug keren uitgesproken in haar coördinaten systeem.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Als je lambda aftrekt van de diagonalen en de determinant berekent, krijg je 1 minus lambda kwadraat.",
  "model": "google_nmt",
  "from_community_srt": "Hoofdzakelijk, wanneer je een uitdrukking zie zoals   A^(-1) M A stelt het een wiskundige manier van empathie voor.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "En de enige wortel van deze uitdrukking is dat lambda gelijk is aan 1.",
  "model": "google_nmt",
  "from_community_srt": "De middelste matrix stelt een soort transformatie voor,",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Dit komt overeen met wat we geometrisch zien, dat alle eigenvectoren een eigenwaarde 1 hebben.",
  "model": "google_nmt",
  "from_community_srt": "zoals je ziet en de twee buitenste matrices stellen de empathie voor, de shift in perspectief en het volledig matrix product stelt dezelfde transformatie voor maar zoals iemand ander het ziet.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Houd er echter rekening mee dat het ook mogelijk is om slechts één eigenwaarde te hebben, maar dan met meer dan alleen een lijn vol eigenvectoren.",
  "model": "google_nmt",
  "from_community_srt": "Voor deze van jullie die zicht afvraagt waarom we geven om alternatieve coördinaten systemen",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Een eenvoudig voorbeeld is een matrix die alles met 2 schaalt.",
  "model": "google_nmt",
  "from_community_srt": "de volgende video over eigen vectoren en eigenwaarden zal je een zeer belangrijk voorbeeld geven van dit.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "De enige eigenwaarde is 2, maar elke vector in het vlak wordt een eigenvector met die eigenwaarde.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Dit is weer een goed moment om even stil te staan en hierover na te denken voordat ik verder ga met het laatste onderwerp.",
  "model": "google_nmt",
  "from_community_srt": "Zie jullie dan!",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Ik wil hier afsluiten met het idee van een eigenbasis, die sterk leunt op ideeën uit de laatste video.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Kijk eens wat er gebeurt als onze basisvectoren toevallig eigenvectoren zijn.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Misschien wordt i-hat bijvoorbeeld geschaald met negatief 1 en wordt j-hat geschaald met 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Als je hun nieuwe coördinaten schrijft als de kolommen van een matrix, merk dan op dat die scalaire veelvouden, negatief 1 en 2, die de eigenwaarden zijn van i-hat en j-hat, op de diagonaal van onze matrix zitten, en elke andere invoer is een 0 .",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Elke keer dat een matrix overal nullen heeft, behalve op de diagonaal, wordt dit redelijkerwijs een diagonaalmatrix genoemd.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "En de manier om dit te interpreteren is dat alle basisvectoren eigenvectoren zijn, waarbij de diagonale ingangen van deze matrix hun eigenwaarden zijn.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Er zijn veel dingen die diagonale matrices veel leuker maken om mee te werken.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Een grote daarvan is dat het gemakkelijker is om te berekenen wat er zal gebeuren als je deze matrix een aantal keer met zichzelf vermenigvuldigt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Omdat al deze matrices elke basisvector met een eigenwaarde schalen, komt het vele malen toepassen van die matrix, bijvoorbeeld 100 keer, overeen met het schalen van elke basisvector met de 100ste macht van de overeenkomstige eigenwaarde.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Probeer daarentegen de 100e macht van een niet-diagonale matrix te berekenen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Probeer het echt even.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Het is een nachtmerrie.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Natuurlijk zul je zelden zoveel geluk hebben dat je basisvectoren ook eigenvectoren zijn.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Maar als uw transformatie veel eigenvectoren heeft, zoals die uit het begin van deze video, genoeg zodat u een verzameling kunt kiezen die de volledige ruimte bestrijkt, dan kunt u uw coördinatensysteem zo wijzigen dat deze eigenvectoren uw basisvectoren zijn.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Ik had het in de vorige video over het veranderen van de basis, maar ik zal hier een supersnelle herinnering doornemen over hoe je een transformatie die momenteel in ons coördinatensysteem is geschreven, in een ander systeem kunt uitdrukken.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Neem de coördinaten van de vectoren die u als nieuwe basis wilt gebruiken, wat in dit geval onze twee eigenvectoren betekent, en maak van die coördinaten de kolommen van een matrix, ook wel de verandering van de basismatrix genoemd.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Wanneer je de oorspronkelijke transformatie in een sandwich plaatst, waarbij je de verandering van de basismatrix aan de rechterkant plaatst en het omgekeerde van de verandering van de basismatrix aan de linkerkant, zal het resultaat een matrix zijn die dezelfde transformatie vertegenwoordigt, maar vanuit het perspectief van de nieuwe basisvectoren coördineren systeem.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Het hele punt van dit doen met eigenvectoren is dat deze nieuwe matrix gegarandeerd diagonaal is met de bijbehorende eigenwaarden beneden die diagonaal.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Dit komt omdat het het werken in een coördinatensysteem vertegenwoordigt, waarbij wat er met de basisvectoren gebeurt, is dat ze tijdens de transformatie worden geschaald.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Een reeks basisvectoren die ook eigenvectoren zijn, wordt, wederom, redelijkerwijs, een eigenbasis genoemd.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Dus als je bijvoorbeeld de 100e macht van deze matrix zou moeten berekenen, zou het veel gemakkelijker zijn om naar een eigenbasis te gaan, de 100e macht in dat systeem te berekenen en vervolgens terug te converteren naar ons standaardsysteem.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Je kunt dit niet bij alle transformaties doen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Een afschuiving heeft bijvoorbeeld niet genoeg eigenvectoren om de volledige ruimte te overspannen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Maar als je een eigenbasis kunt vinden, worden matrixbewerkingen heel mooi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Voor degenen onder jullie die bereid zijn een mooie puzzel uit te werken om te zien hoe dit er in actie uitziet en hoe het kan worden gebruikt om verrassende resultaten te produceren, zal ik hier op het scherm een prompt achterlaten.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Het vergt wat werk, maar ik denk dat je er veel plezier aan zult beleven.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "De volgende en laatste video van deze serie gaat over abstracte vectorruimten.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]