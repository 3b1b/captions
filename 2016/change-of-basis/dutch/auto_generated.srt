1
00:00:19,920 --> 00:00:22,976
Eigenvectoren en eigenwaarden is een van de onderwerpen 

2
00:00:22,976 --> 00:00:25,760
die veel studenten bijzonder niet intuïtief vinden.

3
00:00:25,760 --> 00:00:29,081
Vragen als: waarom doen we dit en wat betekent dit eigenlijk, 

4
00:00:29,081 --> 00:00:33,260
blijven maar al te vaak ronddrijven in een onbeantwoorde zee van berekeningen.

5
00:00:33,920 --> 00:00:36,054
En terwijl ik de video's van deze serie heb uitgebracht, 

6
00:00:36,054 --> 00:00:39,086
hebben veel van jullie opgemerkt dat ze ernaar uitkijken om dit onderwerp in het 

7
00:00:39,086 --> 00:00:40,060
bijzonder te visualiseren.

8
00:00:40,680 --> 00:00:43,340
Ik vermoed dat de reden hiervoor niet zozeer is dat 

9
00:00:43,340 --> 00:00:46,360
eigendingen bijzonder ingewikkeld of slecht uitgelegd zijn.

10
00:00:46,860 --> 00:00:51,180
In feite is het relatief eenvoudig, en ik denk dat de meeste boeken het prima uitleggen.

11
00:00:51,520 --> 00:00:54,814
Het probleem is dat het alleen echt zinvol is als je een goed 

12
00:00:54,814 --> 00:00:58,480
visueel inzicht hebt in veel van de onderwerpen die eraan voorafgaan.

13
00:00:59,060 --> 00:01:03,007
Het belangrijkste hier is dat je weet hoe je matrices als lineaire transformaties 

14
00:01:03,007 --> 00:01:06,955
moet beschouwen, maar dat je ook vertrouwd moet zijn met zaken als determinanten, 

15
00:01:06,955 --> 00:01:09,940
lineaire stelsels van vergelijkingen en verandering van basis.

16
00:01:10,720 --> 00:01:14,837
Verwarring over eigenstuffs heeft meestal meer te maken met een wankele 

17
00:01:14,837 --> 00:01:19,240
basis in een van deze onderwerpen dan met eigenvectoren en eigenwaarden zelf.

18
00:01:19,980 --> 00:01:24,840
Overweeg om te beginnen een lineaire transformatie in twee dimensies, zoals hier getoond.

19
00:01:25,460 --> 00:01:31,040
Het verplaatst de basisvector i-hat naar de coördinaten 3, 0, en j-hat naar 1, 2.

20
00:01:31,780 --> 00:01:35,640
Het wordt dus weergegeven met een matrix waarvan de kolommen 3, 0 en 1, 2 zijn.

21
00:01:36,600 --> 00:01:39,335
Concentreer u op wat het met een bepaalde vector doet, 

22
00:01:39,335 --> 00:01:43,215
en denk na over de reikwijdte van die vector, de lijn die door zijn oorsprong 

23
00:01:43,215 --> 00:01:44,160
en zijn punt loopt.

24
00:01:44,920 --> 00:01:48,380
De meeste vectoren zullen tijdens de transformatie uit hun bereik worden gehaald.

25
00:01:48,780 --> 00:01:51,997
Ik bedoel, het zou behoorlijk toevallig lijken als de plaats 

26
00:01:51,997 --> 00:01:55,320
waar de vector landde zich ook ergens op die lijn zou bevinden.

27
00:01:57,400 --> 00:02:00,154
Maar sommige speciale vectoren blijven op hun eigen bereik, 

28
00:02:00,154 --> 00:02:03,092
wat betekent dat het effect dat de matrix op zo'n vector heeft, 

29
00:02:03,092 --> 00:02:07,040
alleen maar is dat deze wordt uitgerekt of platgedrukt, zoals bij een scalaire vector.

30
00:02:09,460 --> 00:02:14,100
Voor dit specifieke voorbeeld is de basisvector i-hat zo'n speciale vector.

31
00:02:14,640 --> 00:02:19,192
De spanwijdte van i-hat is de x-as, en uit de eerste kolom van de matrix 

32
00:02:19,192 --> 00:02:24,120
kunnen we zien dat i-hat drie keer zichzelf verplaatst, nog steeds op die x-as.

33
00:02:26,320 --> 00:02:30,450
Bovendien wordt, vanwege de manier waarop lineaire transformaties werken, 

34
00:02:30,450 --> 00:02:34,302
elke andere vector op de x-as ook gewoon met een factor 3 uitgerekt, 

35
00:02:34,302 --> 00:02:36,480
en blijft dus op zijn eigen spanwijdte.

36
00:02:38,500 --> 00:02:43,134
Een iets geniepiger vector die tijdens deze transformatie op zijn eigen bereik blijft, 

37
00:02:43,134 --> 00:02:44,040
is negatief 1, 1.

38
00:02:44,660 --> 00:02:47,140
Het wordt uiteindelijk met een factor 2 uitgerekt.

39
00:02:49,000 --> 00:02:53,553
En nogmaals, lineariteit impliceert dat elke andere vector op de diagonale lijn 

40
00:02:53,553 --> 00:02:58,220
die door deze man wordt overspannen, gewoon met een factor 2 zal worden uitgerekt.

41
00:02:59,820 --> 00:03:02,475
En voor deze transformatie zijn dat alle vectoren met 

42
00:03:02,475 --> 00:03:05,180
de speciale eigenschap om binnen hun bereik te blijven.

43
00:03:05,620 --> 00:03:08,409
Die op de x-as worden met een factor 3 uitgerekt, 

44
00:03:08,409 --> 00:03:11,980
en die op deze diagonale lijn worden met een factor 2 uitgerekt.

45
00:03:12,760 --> 00:03:15,548
Elke andere vector zal tijdens de transformatie enigszins worden 

46
00:03:15,548 --> 00:03:18,080
geroteerd en van de lijn worden geslagen die hij overspant.

47
00:03:22,520 --> 00:03:26,289
Zoals je misschien al geraden hebt, worden deze speciale vectoren de 

48
00:03:26,289 --> 00:03:30,004
eigenvectoren van de transformatie genoemd, en aan elke eigenvector 

49
00:03:30,004 --> 00:03:33,610
is een zogenaamde eigenwaarde gekoppeld, wat precies de factor is 

50
00:03:33,610 --> 00:03:37,380
waarmee deze wordt uitgerekt of platgedrukt tijdens de transformatie.

51
00:03:40,280 --> 00:03:43,507
Er is natuurlijk niets bijzonders aan uitrekken versus samendrukken, 

52
00:03:43,507 --> 00:03:45,940
of aan het feit dat deze eigenwaarden positief zijn.

53
00:03:46,380 --> 00:03:50,876
In een ander voorbeeld zou je een eigenvector kunnen hebben met een eigenwaarde negatief 

54
00:03:50,876 --> 00:03:55,120
1 half, wat betekent dat de vector wordt omgedraaid en geplet met een factor 1 half.

55
00:03:56,980 --> 00:04:01,244
Maar het belangrijkste hier is dat het op de lijn blijft die het uitstrekt, 

56
00:04:01,244 --> 00:04:02,760
zonder er vanaf te draaien.

57
00:04:04,460 --> 00:04:07,895
Om een idee te krijgen van waarom dit nuttig zou kunnen zijn om over na te denken, 

58
00:04:07,895 --> 00:04:09,800
kun je een driedimensionale rotatie overwegen.

59
00:04:11,660 --> 00:04:15,236
Als je voor die rotatie een eigenvector kunt vinden, 

60
00:04:15,236 --> 00:04:20,500
een vector die op zijn eigen bereik blijft, dan heb je de rotatie-as gevonden.

61
00:04:22,600 --> 00:04:26,743
En het is veel gemakkelijker om aan een 3D-rotatie te denken in termen 

62
00:04:26,743 --> 00:04:30,420
van een bepaalde rotatie-as en een hoek waarover deze roteert, 

63
00:04:30,420 --> 00:04:34,740
dan te denken aan de volledige 3x3-matrix die bij die transformatie hoort.

64
00:04:37,000 --> 00:04:40,553
In dit geval zou de corresponderende eigenwaarde trouwens 1 moeten zijn, 

65
00:04:40,553 --> 00:04:43,328
aangezien rotaties nooit iets uitrekken of samendrukken, 

66
00:04:43,328 --> 00:04:45,860
zodat de lengte van de vector hetzelfde zou blijven.

67
00:04:48,080 --> 00:04:50,020
Dit patroon komt veel voor in de lineaire algebra.

68
00:04:50,440 --> 00:04:53,758
Bij elke lineaire transformatie die door een matrix wordt beschreven, 

69
00:04:53,758 --> 00:04:56,792
kun je begrijpen wat deze doet door de kolommen van deze matrix 

70
00:04:56,792 --> 00:04:59,400
af te lezen als de landingsplaatsen voor basisvectoren.

71
00:05:00,020 --> 00:05:03,414
Maar vaak is een betere manier om tot de kern te komen van wat de 

72
00:05:03,414 --> 00:05:06,757
lineaire transformatie feitelijk doet, minder afhankelijk van uw 

73
00:05:06,757 --> 00:05:10,820
specifieke coördinatensysteem, het vinden van de eigenvectoren en eigenwaarden.

74
00:05:15,460 --> 00:05:19,008
Ik zal hier niet de volledige details behandelen over methoden voor het berekenen 

75
00:05:19,008 --> 00:05:22,427
van eigenvectoren en eigenwaarden, maar ik zal proberen een overzicht te geven 

76
00:05:22,427 --> 00:05:26,020
van de computationele ideeën die het belangrijkst zijn voor een conceptueel begrip.

77
00:05:27,180 --> 00:05:30,480
Symbolisch gezien ziet dit er zo uit hoe het idee van een eigenvector eruit ziet.

78
00:05:31,040 --> 00:05:35,900
A is de matrix die een transformatie vertegenwoordigt, met v als de eigenvector, 

79
00:05:35,900 --> 00:05:39,740
en lambda is een getal, namelijk de overeenkomstige eigenwaarde.

80
00:05:40,680 --> 00:05:44,582
Wat deze uitdrukking zegt is dat het matrix-vectorproduct, A maal v, 

81
00:05:44,582 --> 00:05:49,108
hetzelfde resultaat geeft als het schalen van de eigenvector v met een bepaalde 

82
00:05:49,108 --> 00:05:49,900
waarde lambda.

83
00:05:51,000 --> 00:05:55,436
Het vinden van de eigenvectoren en hun eigenwaarden van een matrix A komt dus 

84
00:05:55,436 --> 00:06:00,100
neer op het vinden van de waarden van v en lambda die deze uitdrukking waar maken.

85
00:06:01,920 --> 00:06:04,474
In het begin is het een beetje lastig om mee te werken, 

86
00:06:04,474 --> 00:06:07,757
omdat de linkerkant de matrix-vectorvermenigvuldiging vertegenwoordigt, 

87
00:06:07,757 --> 00:06:10,540
maar de rechterkant hier de scalaire vectorvermenigvuldiging.

88
00:06:11,120 --> 00:06:14,209
Laten we beginnen met het herschrijven van die rechterkant als een 

89
00:06:14,209 --> 00:06:17,345
soort matrix-vectorvermenigvuldiging, met behulp van een matrix die 

90
00:06:17,345 --> 00:06:20,620
het effect heeft dat elke vector met een factor lambda wordt geschaald.

91
00:06:21,680 --> 00:06:26,144
De kolommen van zo'n matrix vertegenwoordigen wat er met elke basisvector gebeurt, 

92
00:06:26,144 --> 00:06:29,748
en elke basisvector wordt eenvoudigweg vermenigvuldigd met lambda, 

93
00:06:29,748 --> 00:06:34,320
dus deze matrix zal het getal lambda onderaan de diagonaal hebben, met overal nullen.

94
00:06:36,180 --> 00:06:39,042
De gebruikelijke manier om deze man te schrijven is door die 

95
00:06:39,042 --> 00:06:42,044
lambda eruit te halen en het op te schrijven als lambda maal i, 

96
00:06:42,044 --> 00:06:44,860
waarbij i de identiteitsmatrix is met 1s langs de diagonaal.

97
00:06:45,860 --> 00:06:49,042
Omdat beide zijden lijken op matrix-vectorvermenigvuldiging, 

98
00:06:49,042 --> 00:06:51,860
kunnen we die rechterkant aftrekken en de v wegwerken.

99
00:06:54,160 --> 00:06:59,308
Dus wat we nu hebben is een nieuwe matrix, A minus lambda maal de identiteit, 

100
00:06:59,308 --> 00:07:04,920
en we zoeken naar een vector v zodat deze nieuwe matrix maal v de nulvector oplevert.

101
00:07:06,380 --> 00:07:11,100
Dit zal altijd waar zijn als v zelf de nulvector is, maar dat is saai.

102
00:07:11,340 --> 00:07:13,640
Wat we willen is een eigenvector die niet nul is.

103
00:07:14,420 --> 00:07:18,778
En als je hoofdstuk 5 en 6 bekijkt, weet je dat de enige manier waarop het 

104
00:07:18,778 --> 00:07:22,905
product van een matrix met een vector die niet nul is, nul kan worden, 

105
00:07:22,905 --> 00:07:28,020
is als de transformatie die bij die matrix hoort de ruimte in een lagere dimensie perst.

106
00:07:29,300 --> 00:07:34,220
En die verkleining komt overeen met een nuldeterminant voor de matrix.

107
00:07:35,480 --> 00:07:40,266
Om concreet te zijn, laten we zeggen dat matrix A de kolommen 2, 1 en 2, 3 heeft, 

108
00:07:40,266 --> 00:07:45,520
en denk erover na om een variabel bedrag, lambda, af te trekken van elke diagonale invoer.

109
00:07:46,480 --> 00:07:48,421
Stel je nu voor dat je lambda aanpast, aan een 

110
00:07:48,421 --> 00:07:50,280
knop draait om de waarde ervan te veranderen.

111
00:07:50,940 --> 00:07:54,675
Naarmate de waarde van lambda verandert, verandert de matrix zelf, 

112
00:07:54,675 --> 00:07:57,240
en dus verandert de determinant van de matrix.

113
00:07:58,220 --> 00:08:02,730
Het doel hier is om een waarde van lambda te vinden die deze determinant nul maakt, 

114
00:08:02,730 --> 00:08:07,240
wat betekent dat de aangepaste transformatie de ruimte in een lagere dimensie drukt.

115
00:08:08,160 --> 00:08:11,160
In dit geval komt de goede plek wanneer lambda gelijk is aan 1.

116
00:08:12,180 --> 00:08:14,052
Als we een andere matrix hadden gekozen, zou de 

117
00:08:14,052 --> 00:08:16,120
eigenwaarde uiteraard niet noodzakelijkerwijs 1 zijn.

118
00:08:16,240 --> 00:08:18,600
De goede plek zou kunnen worden bereikt door een andere waarde van lambda.

119
00:08:20,080 --> 00:08:22,960
Dit is dus nogal veel, maar laten we ontrafelen wat dit zegt.

120
00:08:22,960 --> 00:08:26,133
Wanneer lambda gelijk is aan 1, drukt de matrix A 

121
00:08:26,133 --> 00:08:29,560
minus lambda maal de identiteit de ruimte op een lijn.

122
00:08:30,440 --> 00:08:33,828
Dat betekent dat er een vector v is die niet nul is, 

123
00:08:33,828 --> 00:08:38,559
zodat A minus lambda maal de identiteit maal v gelijk is aan de nulvector.

124
00:08:40,480 --> 00:08:43,716
En onthoud, de reden dat dit ons interesseert, 

125
00:08:43,716 --> 00:08:48,122
is omdat het betekent dat A maal v gelijk is aan lambda maal v, 

126
00:08:48,122 --> 00:08:53,355
wat je kunt aflezen als te zeggen dat de vector v een eigenvector is van A, 

127
00:08:53,355 --> 00:08:57,280
die op zijn eigen span blijft tijdens de transformatie A.

128
00:08:58,320 --> 00:09:01,305
In dit voorbeeld is de corresponderende eigenwaarde 1, 

129
00:09:01,305 --> 00:09:04,020
dus v zou eigenlijk gewoon op zijn plaats blijven.

130
00:09:06,220 --> 00:09:09,500
Pauzeer en denk na of je ervoor moet zorgen dat die redenering goed voelt.

131
00:09:13,380 --> 00:09:15,640
Dit is het soort dingen dat ik in de inleiding noemde.

132
00:09:16,220 --> 00:09:19,580
Als je geen goed inzicht had in de determinanten en waarom ze betrekking 

133
00:09:19,580 --> 00:09:23,354
hebben op lineaire stelsels van vergelijkingen met oplossingen die niet nul zijn, 

134
00:09:23,354 --> 00:09:26,300
zou een uitdrukking als deze volkomen uit de lucht komen vallen.

135
00:09:28,320 --> 00:09:32,079
Om dit in actie te zien, gaan we het voorbeeld vanaf het begin opnieuw bekijken, 

136
00:09:32,079 --> 00:09:34,540
met een matrix waarvan de kolommen 3, 0 en 1, 2 zijn.

137
00:09:35,350 --> 00:09:38,654
Om te bepalen of een waarde lambda een eigenwaarde is, 

138
00:09:38,654 --> 00:09:43,400
trekt u deze af van de diagonalen van deze matrix en berekent u de determinant.

139
00:09:50,580 --> 00:09:54,872
Als we dit doen, krijgen we een bepaald kwadratisch polynoom in lambda, 

140
00:09:54,872 --> 00:09:56,720
3 min lambda maal 2 min lambda.

141
00:09:57,800 --> 00:10:02,240
Omdat lambda alleen een eigenwaarde kan zijn als deze determinant nul is, 

142
00:10:02,240 --> 00:10:05,900
kun je concluderen dat de enige mogelijke eigenwaarden zijn: 

143
00:10:05,900 --> 00:10:08,840
lambda is gelijk aan 2 en lambda is gelijk aan 3.

144
00:10:09,640 --> 00:10:13,821
Om erachter te komen wat de eigenvectoren zijn die daadwerkelijk een van deze 

145
00:10:13,821 --> 00:10:16,448
eigenwaarden hebben, zeg lambda is gelijk aan 2, 

146
00:10:16,448 --> 00:10:21,273
plug je die waarde van lambda in de matrix in en los je vervolgens op voor welke vectoren 

147
00:10:21,273 --> 00:10:23,900
deze diagonaal gewijzigde matrix naar nul stuurt.

148
00:10:24,940 --> 00:10:28,883
Als je dit op dezelfde manier zou berekenen als elk ander lineair systeem, 

149
00:10:28,883 --> 00:10:32,669
zou je zien dat de oplossingen alle vectoren op de diagonale lijn zijn, 

150
00:10:32,669 --> 00:10:34,300
opgespannen door negatief 1, 1.

151
00:10:35,220 --> 00:10:39,281
Dit komt overeen met het feit dat de ongewijzigde matrix, 3, 0, 1, 2, 

152
00:10:39,281 --> 00:10:43,460
het effect heeft dat al deze vectoren met een factor 2 worden uitgerekt.

153
00:10:46,320 --> 00:10:50,200
Nu hoeft een 2D-transformatie geen eigenvectoren te hebben.

154
00:10:50,720 --> 00:10:53,400
Beschouw bijvoorbeeld een rotatie van 90 graden.

155
00:10:53,660 --> 00:10:58,200
Dit heeft geen eigenvectoren omdat het elke vector buiten zijn eigen bereik roteert.

156
00:11:00,800 --> 00:11:04,349
Als je daadwerkelijk probeert de eigenwaarden van een rotatie als deze te berekenen, 

157
00:11:04,349 --> 00:11:05,560
kijk dan eens wat er gebeurt.

158
00:11:06,300 --> 00:11:10,140
De matrix heeft kolommen 0, 1 en negatief 1, 0.

159
00:11:11,100 --> 00:11:15,800
Trek lambda af van de diagonale elementen en zoek wanneer de determinant nul is.

160
00:11:18,140 --> 00:11:21,940
In dit geval krijg je de polynoom lambda in het kwadraat plus 1.

161
00:11:22,680 --> 00:11:27,920
De enige wortels van dat polynoom zijn de denkbeeldige getallen, i en negatieve i.

162
00:11:28,840 --> 00:11:31,648
Het feit dat er geen oplossingen voor reële getallen zijn, 

163
00:11:31,648 --> 00:11:33,600
geeft aan dat er geen eigenvectoren zijn.

164
00:11:35,540 --> 00:11:37,718
Een ander behoorlijk interessant voorbeeld dat de moeite 

165
00:11:37,718 --> 00:11:39,820
waard is om in je achterhoofd te houden, is een schaar.

166
00:11:40,560 --> 00:11:45,054
Hierdoor wordt i-hat op zijn plaats gezet en wordt j-hat 1 verplaatst, 

167
00:11:45,054 --> 00:11:47,840
zodat de matrix kolommen 1, 0 en 1, 1 heeft.

168
00:11:48,740 --> 00:11:52,468
Alle vectoren op de x-as zijn eigenvectoren met eigenwaarde 1, 

169
00:11:52,468 --> 00:11:54,540
aangezien ze op hun plaats blijven.

170
00:11:55,680 --> 00:11:57,820
In feite zijn dit de enige eigenvectoren.

171
00:11:58,760 --> 00:12:03,998
Als je lambda aftrekt van de diagonalen en de determinant berekent, 

172
00:12:03,998 --> 00:12:06,540
krijg je 1 minus lambda kwadraat.

173
00:12:09,320 --> 00:12:12,860
En de enige wortel van deze uitdrukking is dat lambda gelijk is aan 1.

174
00:12:14,560 --> 00:12:17,085
Dit komt overeen met wat we geometrisch zien, 

175
00:12:17,085 --> 00:12:19,720
dat alle eigenvectoren een eigenwaarde 1 hebben.

176
00:12:21,080 --> 00:12:25,358
Houd er echter rekening mee dat het ook mogelijk is om slechts één eigenwaarde te hebben, 

177
00:12:25,358 --> 00:12:28,020
maar dan met meer dan alleen een lijn vol eigenvectoren.

178
00:12:29,900 --> 00:12:33,180
Een eenvoudig voorbeeld is een matrix die alles met 2 schaalt.

179
00:12:33,900 --> 00:12:37,161
De enige eigenwaarde is 2, maar elke vector in 

180
00:12:37,161 --> 00:12:40,700
het vlak wordt een eigenvector met die eigenwaarde.

181
00:12:42,000 --> 00:12:44,520
Dit is weer een goed moment om even stil te staan en hierover 

182
00:12:44,520 --> 00:12:46,960
na te denken voordat ik verder ga met het laatste onderwerp.

183
00:13:03,540 --> 00:13:06,958
Ik wil hier afsluiten met het idee van een eigenbasis, 

184
00:13:06,958 --> 00:13:09,880
die sterk leunt op ideeën uit de laatste video.

185
00:13:11,480 --> 00:13:16,380
Kijk eens wat er gebeurt als onze basisvectoren toevallig eigenvectoren zijn.

186
00:13:17,120 --> 00:13:19,952
Misschien wordt i-hat bijvoorbeeld geschaald met 

187
00:13:19,952 --> 00:13:22,380
negatief 1 en wordt j-hat geschaald met 2.

188
00:13:23,420 --> 00:13:27,391
Als je hun nieuwe coördinaten schrijft als de kolommen van een matrix, 

189
00:13:27,391 --> 00:13:30,635
merk dan op dat die scalaire veelvouden, negatief 1 en 2, 

190
00:13:30,635 --> 00:13:35,390
die de eigenwaarden zijn van i-hat en j-hat, op de diagonaal van onze matrix zitten, 

191
00:13:35,390 --> 00:13:37,180
en elke andere invoer is een 0 .

192
00:13:38,880 --> 00:13:42,624
Elke keer dat een matrix overal nullen heeft, behalve op de diagonaal, 

193
00:13:42,624 --> 00:13:45,420
wordt dit redelijkerwijs een diagonaalmatrix genoemd.

194
00:13:45,840 --> 00:13:50,545
En de manier om dit te interpreteren is dat alle basisvectoren eigenvectoren zijn, 

195
00:13:50,545 --> 00:13:54,400
waarbij de diagonale ingangen van deze matrix hun eigenwaarden zijn.

196
00:13:57,100 --> 00:14:01,060
Er zijn veel dingen die diagonale matrices veel leuker maken om mee te werken.

197
00:14:01,780 --> 00:14:05,060
Een grote daarvan is dat het gemakkelijker is om te berekenen wat er zal 

198
00:14:05,060 --> 00:14:08,340
gebeuren als je deze matrix een aantal keer met zichzelf vermenigvuldigt.

199
00:14:09,420 --> 00:14:13,820
Omdat al deze matrices elke basisvector met een eigenwaarde schalen, 

200
00:14:13,820 --> 00:14:18,221
komt het vele malen toepassen van die matrix, bijvoorbeeld 100 keer, 

201
00:14:18,221 --> 00:14:22,814
overeen met het schalen van elke basisvector met de 100ste macht van de 

202
00:14:22,814 --> 00:14:24,600
overeenkomstige eigenwaarde.

203
00:14:25,700 --> 00:14:29,680
Probeer daarentegen de 100e macht van een niet-diagonale matrix te berekenen.

204
00:14:29,680 --> 00:14:31,320
Probeer het echt even.

205
00:14:31,740 --> 00:14:32,440
Het is een nachtmerrie.

206
00:14:36,080 --> 00:14:41,260
Natuurlijk zul je zelden zoveel geluk hebben dat je basisvectoren ook eigenvectoren zijn.

207
00:14:42,040 --> 00:14:44,940
Maar als uw transformatie veel eigenvectoren heeft, 

208
00:14:44,940 --> 00:14:49,791
zoals die uit het begin van deze video, genoeg zodat u een verzameling kunt kiezen die 

209
00:14:49,791 --> 00:14:54,532
de volledige ruimte bestrijkt, dan kunt u uw coördinatensysteem zo wijzigen dat deze 

210
00:14:54,532 --> 00:14:56,540
eigenvectoren uw basisvectoren zijn.

211
00:14:57,140 --> 00:14:59,769
Ik had het in de vorige video over het veranderen van de basis, 

212
00:14:59,769 --> 00:15:03,260
maar ik zal hier een supersnelle herinnering doornemen over hoe je een transformatie 

213
00:15:03,260 --> 00:15:05,520
die momenteel in ons coördinatensysteem is geschreven, 

214
00:15:05,520 --> 00:15:07,040
in een ander systeem kunt uitdrukken.

215
00:15:08,440 --> 00:15:11,996
Neem de coördinaten van de vectoren die u als nieuwe basis wilt gebruiken, 

216
00:15:11,996 --> 00:15:14,414
wat in dit geval onze twee eigenvectoren betekent, 

217
00:15:14,414 --> 00:15:17,069
en maak van die coördinaten de kolommen van een matrix, 

218
00:15:17,069 --> 00:15:19,440
ook wel de verandering van de basismatrix genoemd.

219
00:15:20,180 --> 00:15:23,215
Wanneer je de oorspronkelijke transformatie in een sandwich plaatst, 

220
00:15:23,215 --> 00:15:26,514
waarbij je de verandering van de basismatrix aan de rechterkant plaatst en 

221
00:15:26,514 --> 00:15:29,681
het omgekeerde van de verandering van de basismatrix aan de linkerkant, 

222
00:15:29,681 --> 00:15:33,156
zal het resultaat een matrix zijn die dezelfde transformatie vertegenwoordigt, 

223
00:15:33,156 --> 00:15:36,500
maar vanuit het perspectief van de nieuwe basisvectoren coördineren systeem.

224
00:15:37,440 --> 00:15:41,756
Het hele punt van dit doen met eigenvectoren is dat deze nieuwe matrix 

225
00:15:41,756 --> 00:15:46,680
gegarandeerd diagonaal is met de bijbehorende eigenwaarden beneden die diagonaal.

226
00:15:46,860 --> 00:15:50,088
Dit komt omdat het het werken in een coördinatensysteem vertegenwoordigt, 

227
00:15:50,088 --> 00:15:53,883
waarbij wat er met de basisvectoren gebeurt, is dat ze tijdens de transformatie worden 

228
00:15:53,883 --> 00:15:54,320
geschaald.

229
00:15:55,800 --> 00:15:58,599
Een reeks basisvectoren die ook eigenvectoren zijn, 

230
00:15:58,599 --> 00:16:01,560
wordt, wederom, redelijkerwijs, een eigenbasis genoemd.

231
00:16:02,340 --> 00:16:06,512
Dus als je bijvoorbeeld de 100e macht van deze matrix zou moeten berekenen, 

232
00:16:06,512 --> 00:16:10,025
zou het veel gemakkelijker zijn om naar een eigenbasis te gaan, 

233
00:16:10,025 --> 00:16:14,527
de 100e macht in dat systeem te berekenen en vervolgens terug te converteren naar 

234
00:16:14,527 --> 00:16:15,680
ons standaardsysteem.

235
00:16:16,620 --> 00:16:18,320
Je kunt dit niet bij alle transformaties doen.

236
00:16:18,320 --> 00:16:20,532
Een afschuiving heeft bijvoorbeeld niet genoeg 

237
00:16:20,532 --> 00:16:22,980
eigenvectoren om de volledige ruimte te overspannen.

238
00:16:23,460 --> 00:16:28,160
Maar als je een eigenbasis kunt vinden, worden matrixbewerkingen heel mooi.

239
00:16:29,120 --> 00:16:31,794
Voor degenen onder jullie die bereid zijn een mooie puzzel uit te werken om 

240
00:16:31,794 --> 00:16:34,680
te zien hoe dit er in actie uitziet en hoe het kan worden gebruikt om verrassende 

241
00:16:34,680 --> 00:16:37,320
resultaten te produceren, zal ik hier op het scherm een prompt achterlaten.

242
00:16:37,600 --> 00:16:40,280
Het vergt wat werk, maar ik denk dat je er veel plezier aan zult beleven.

243
00:16:40,840 --> 00:16:46,120
De volgende en laatste video van deze serie gaat over abstracte vectorruimten.

