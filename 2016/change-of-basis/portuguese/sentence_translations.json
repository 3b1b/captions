[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Autovetores e autovalores é um daqueles tópicos que muitos estudantes consideram particularmente pouco intuitivo.",
  "model": "google_nmt",
  "from_community_srt": "\"Matemática é a arte de dar o mesmo nome a coisas diferentes.\" -- Henri Poincaré Se eu tenho um vetor sentado aqui no espaço 2D temos uma maneira padrão para descrevê-lo com coordenadas. Neste caso, o vetor tem coordenadas [3, 2], o que significa ir de sua base para sua ponta envolve mover 3 unidades para a direita e 2 unidades para cima.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Perguntas como por que estamos fazendo isso e o que isso realmente significa são muitas vezes deixadas flutuando em um mar sem resposta de cálculos.",
  "model": "google_nmt",
  "from_community_srt": "Agora, a maneira mais orientada à Álgebra Linear de se descrever coordenadas é pensar em cada um desses números como um escalar",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "E enquanto eu lançava os vídeos desta série, muitos de vocês comentaram sobre o desejo de visualizar este tópico em particular.",
  "model": "google_nmt",
  "from_community_srt": "uma coisa que se estende ou aperta vetores. Você imagina a primeira coordenada escalando î, o vetor com o comprimento 1,",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Suspeito que a razão para isso não seja tanto o fato de as coisas serem particularmente complicadas ou mal explicadas.",
  "model": "google_nmt",
  "from_community_srt": "que aponta para a direita, enquanto que a segunda coordenada escala ĵ,",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "Na verdade, é comparativamente simples, e acho que a maioria dos livros faz um bom trabalho ao explicá-lo.",
  "model": "google_nmt",
  "from_community_srt": "o vetor com o comprimento 1, que aponta pra cima.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "A questão é que isso só faz sentido se você tiver um conhecimento visual sólido de muitos dos tópicos que o precedem.",
  "model": "google_nmt",
  "from_community_srt": "A soma de base a ponta desses dois vetores escalados é o que as coordenadas são destinadas a descrever.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "O mais importante aqui é que você saiba pensar em matrizes como transformações lineares, mas também precisa estar confortável com coisas como determinantes, sistemas lineares de equações e mudança de base.",
  "model": "google_nmt",
  "from_community_srt": "Você pode pensar nesses dois vetores especiais como encapsular todos os pressupostos implícitos do nosso sistema de coordenadas. O fato de que o primeiro número indica a direita movimento que o segundo indica movimento ascendente exatamente o quão longe unidade de distâncias.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "A confusão sobre materiais próprios geralmente tem mais a ver com uma base instável em um desses tópicos do que com os próprios vetores e valores próprios.",
  "model": "google_nmt",
  "from_community_srt": "Tudo isso está amarrado na escolha do î e do ĵ como os vetores que as coordenadas escalares são destinadas a escalar realmente.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Para começar, considere alguma transformação linear em duas dimensões, como a mostrada aqui.",
  "model": "google_nmt",
  "from_community_srt": "Qualquer forma de traduzir entre os vetores e conjuntos de números é chamada um sistema de coordenadas, e os dois vetores especiais,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Ele move o vetor base i-hat para as coordenadas 3, 0 e j-hat para 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "î e ĵ, são chamados os vetores de base do nosso sistema padrão de coordenadas.",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Portanto, é representado por uma matriz cujas colunas são 3, 0 e 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "O que eu gostaria de falar aqui é a ideia de usar um conjunto diferente de vetores de base.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Concentre-se no que ele faz com um vetor específico e pense na extensão desse vetor, na reta que passa por sua origem e sua ponta.",
  "model": "google_nmt",
  "from_community_srt": "Por exemplo, digamos que você tem uma amiga, Jennifer que usa um conjunto diferente de vetores de base, que chamarei de b1 e b2.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "A maioria dos vetores será eliminada durante a transformação.",
  "model": "google_nmt",
  "from_community_srt": "Seu primeiro vetor de base b1 aponta para a direita um pouco e seu segundo vetor,",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Quero dizer, pareceria bastante coincidência se o local onde o vetor pousou também estivesse em algum lugar nessa linha.",
  "model": "google_nmt",
  "from_community_srt": "b2, aponta para a  esquerda e para cima. Agora, veja novamente aquele outro vetor que eu mostrei mais cedo, o que você e eu descreveríamos usando as coordenadas [3,",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Mas alguns vetores especiais permanecem em sua própria extensão, o que significa que o efeito que a matriz tem sobre tal vetor é apenas esticá-lo ou comprimi-lo, como um escalar.",
  "model": "google_nmt",
  "from_community_srt": "2], usando nossos vetores de base î e ĵ. Jennifer na verdade descreveria este vetor com as coordenadas [5/3, 1/3].",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Para este exemplo específico, o vetor base i-hat é um desses vetores especiais.",
  "model": "google_nmt",
  "from_community_srt": "O que isto significa é que o modo particular para chegar a esse vetor usando os dois vetores de base dela",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "A extensão de i-hat é o eixo x, e da primeira coluna da matriz, podemos ver que i-hat se move 3 vezes, ainda nesse eixo x.",
  "model": "google_nmt",
  "from_community_srt": "é escalar b1 por 5/3, escalar b2 por 1/3, em seguida, somar os dois. Em um pouco, eu vou lhe mostrar como você poderia ter descoberto esses dois números 5/3 e 1/3.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Além do mais, devido à forma como as transformações lineares funcionam, qualquer outro vetor no eixo x também é esticado por um fator de 3 e, portanto, permanece em seu próprio vão.",
  "model": "google_nmt",
  "from_community_srt": "Em geral, sempre que Jennifer usa coordenadas para descrever um vetor, ela pensa na sua primeira coordenada como escalando b1, na segunda coordenada escalando b2, e ela adiciona os resultados.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Um vetor um pouco mais sorrateiro que permanece em sua própria extensão durante esta transformação é negativo 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "O que ela obtém normalmente será completamente diferente do vetor que você e eu pensaríamos como tendo essas coordenadas.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Acaba sendo esticado por um fator de 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "E, novamente, a linearidade implicará que qualquer outro vetor na reta diagonal gerada por esse cara será esticado por um fator de 2.",
  "model": "google_nmt",
  "from_community_srt": "Vamos ser um pouco mais precisos com relação à configuração aqui: o primeiro vetor de base b1 dela é algo que gostaríamos de descrever com as coordenadas [2, 1] e o segundo vetor de base b2 dela, é algo que nós descreveríamos como [-1,",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "E para esta transformação, estes são todos os vetores com esta propriedade especial de permanecer no seu vão.",
  "model": "google_nmt",
  "from_community_srt": "1]. Mas é importante perceber que, a partir da perspectiva do sistema dela, esses vetores têm coordenadas [1,",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Aqueles no eixo x sendo esticados por um fator de 3, e aqueles nesta linha diagonal sendo esticados por um fator de 2.",
  "model": "google_nmt",
  "from_community_srt": "0] e [0, 1]. Eles são o que define o significado das coordenadas [1, 0] e [0, 1] no mundo dela.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Qualquer outro vetor será girado um pouco durante a transformação, saindo da linha que ele abrange.",
  "model": "google_nmt",
  "from_community_srt": "Então, na verdade, estamos falando línguas diferentes! Estamos todos olhando para os mesmos vetores no espaço",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Como você já deve ter adivinhado, esses vetores especiais são chamados de autovetores da transformação, e cada autovetor tem associado a ele o que é chamado de autovalor, que é apenas o fator pelo qual ele é esticado ou comprimido durante a transformação.",
  "model": "google_nmt",
  "from_community_srt": "mas Jennifer usa diferentes palavras e números para descrevê-los. Deixe-me dizer uma palavra rápida sobre como eu estou representando coisas aqui: quando eu animo o espaço 2D, Eu normalmente uso esta grade quadrada mas essa grade é apenas uma construção, uma forma de visualizar o nosso  sistema de coordenadas, e por isso depende da nossa escolha da base.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "É claro que não há nada de especial em esticar versus esmagar, ou no fato de que esses autovalores são positivos.",
  "model": "google_nmt",
  "from_community_srt": "O próprio espaço não tem grade intrínseca. Jennifer pode desenhar a sua própria grade que seria uma construção igualmente confeccionada,",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "Em outro exemplo, você poderia ter um autovetor com autovalor negativo 1 metade, o que significa que o vetor é invertido e comprimido por um fator de 1 metade.",
  "model": "google_nmt",
  "from_community_srt": "significando nada mais que uma ferramenta visual para ajudar a seguir o significado de suas coordenadas. A origem dela, no entanto,",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Mas a parte importante aqui é que ele permanece na linha que se estende sem ser girado para fora dela.",
  "model": "google_nmt",
  "from_community_srt": "seria a mesma da nossa, desde que todos concordam com o que as coordenadas [0, 0], devem significar. É a coisa que você obtém quando você dimensiona qualquer vetor por 0.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Para entender por que isso pode ser útil para se pensar, considere alguma rotação tridimensional.",
  "model": "google_nmt",
  "from_community_srt": "Mas a direção dos eixos dela e o espaçamento de suas linhas de grade será diferente, dependendo de sua escolha de vetores de base.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Se você puder encontrar um autovetor para essa rotação, um vetor que permaneça em seu próprio vão, o que você encontrará é o eixo de rotação.",
  "model": "google_nmt",
  "from_community_srt": "Então, depois que tudo isso é configurado, uma pergunta muito natural a se fazer é: \"Como podemos traduzir entre sistemas de coordenadas?\" Se, por exemplo,",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "E é muito mais fácil pensar em uma rotação 3D em termos de algum eixo de rotação e um ângulo pelo qual ela gira, em vez de pensar na matriz 3x3 completa associada a essa transformação.",
  "model": "google_nmt",
  "from_community_srt": "Jennifer descreve um vetor com coordenadas [-1, 2], o que seria isso no nosso sistema de coordenadas? Como você traduzir de sua linguagem para a nossa? Bem, o que nossas coordenadas estão dizendo é que este vetor é -1 b1 + 2 b2.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "Nesse caso, aliás, o autovalor correspondente teria que ser 1, já que as rotações nunca esticam ou comprimem nada, então o comprimento do vetor permaneceria o mesmo.",
  "model": "google_nmt",
  "from_community_srt": "E a partir de nossa perspectiva, b1 tem coordenadas [2,",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Esse padrão aparece muito na álgebra linear.",
  "model": "google_nmt",
  "from_community_srt": "1] e b2 tem coordenadas [-1, 1], então,",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Com qualquer transformação linear descrita por uma matriz, você pode entender o que ela está fazendo lendo as colunas dessa matriz como pontos de aterrissagem para vetores de base.",
  "model": "google_nmt",
  "from_community_srt": "podemos calcular realmente -1 b1 + 2 b2, como eles são representados em nosso sistema de coordenadas,",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Mas muitas vezes, a melhor maneira de chegar ao cerne do que a transformação linear realmente faz, menos dependente do seu sistema de coordenadas específico, é encontrar os autovetores e autovalores.",
  "model": "google_nmt",
  "from_community_srt": "E fazendo esta conta, você termina com um vetor com coordenadas [-4, 1]. Então, é assim que nós descreveríamos o vetor que ela entende como [-1, 2].",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Não cobrirei todos os detalhes sobre métodos para calcular autovetores e autovalores aqui, mas tentarei fornecer uma visão geral das ideias computacionais que são mais importantes para uma compreensão conceitual.",
  "model": "google_nmt",
  "from_community_srt": "Este processo aqui de escalar cada um dos vetores da base dela pelas coordenadas correspondentes de algum vetor e, em seguida, adicionando-os juntos pode parecer algo familiar. É a multiplicação matriz-vector, com uma matriz cujas colunas representam os vetores de base de Jennifer em nossa língua. Na verdade,",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Simbolicamente, esta é a aparência da ideia de um autovetor.",
  "model": "google_nmt",
  "from_community_srt": "depois que você entende a multiplicação matriz-vector como a aplicação de uma certa transformação linear,",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A é a matriz que representa alguma transformação, com v como autovetor, e lambda é um número, ou seja, o autovalor correspondente.",
  "model": "google_nmt",
  "from_community_srt": "digamos, observando o que eu entendo como o mais importante desta série, capítulo 3, há uma maneira muito intuitiva para se pensar o que está acontecendo aqui.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "O que esta expressão está dizendo é que o produto matriz-vetor, A vezes v, dá o mesmo resultado que apenas dimensionar o autovetor v por algum valor lambda.",
  "model": "google_nmt",
  "from_community_srt": "A matriz cujas colunas representam os vetores de base de Jennifer pode ser pensada como uma transformação que move nossos vetores de base, î e ĵ (que são as coisas em que pensamos  quando dizemos [1,0] e [0,",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Portanto, encontrar os autovetores e seus autovalores de uma matriz A se resume a encontrar os valores de v e lambda que tornam essa expressão verdadeira.",
  "model": "google_nmt",
  "from_community_srt": "1]) para os vetores de base de Jennifer (que são as coisas em que ela pensa quando ela diz [1,0] e [0, 1]).",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "É um pouco estranho trabalhar com isso no início, porque o lado esquerdo representa a multiplicação de vetores de matrizes, mas o lado direito aqui é a multiplicação de vetores escalares.",
  "model": "google_nmt",
  "from_community_srt": "Para mostrar como isso funciona, vamos percorrer o que significaria tomar o vetor que nós pensamos como tendo coordenadas [-1, 2], e aplicar essa transformação. Antes da transformação linear, nós estamos pensando neste vetor",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Então, vamos começar reescrevendo o lado direito como algum tipo de multiplicação matriz-vetor, usando uma matriz que tem o efeito de escalonar qualquer vetor por um fator lambda.",
  "model": "google_nmt",
  "from_community_srt": "como uma certa combinação linear de nossos vetores de base, -1 î + 2 ĵ.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "As colunas dessa matriz representarão o que acontece com cada vetor de base, e cada vetor de base é simplesmente multiplicado por lambda, então essa matriz terá o número lambda na diagonal, com zeros em todos os outros lugares.",
  "model": "google_nmt",
  "from_community_srt": "E o elemento-chave de uma transformação linear é que o vetor resultante que será a mesma combinação linear mas dos novos vetores de base -1 vezes o lugar onde î vai parar + 2 vezes o lugar onde ĵ vai parar. Então,",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "A maneira comum de escrever esse cara é fatorar esse lambda e escrevê-lo como lambda vezes i, onde i é a matriz identidade com 1s na diagonal.",
  "model": "google_nmt",
  "from_community_srt": "o que esta matriz faz é transformar nosso equívoco do que Jennifer quer dizer no vetor real a que ela está se referindo.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Com ambos os lados parecendo uma multiplicação de matrizes e vetores, podemos subtrair o lado direito e fatorar v.",
  "model": "google_nmt",
  "from_community_srt": "Lembro-me que quando eu aprendia isso pela primeira vez, me parecia meio de trás pra frente. Geometricamente, esta matriz transforma a nossa grade na grade de Jennifer.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Então o que temos agora é uma nova matriz, A menos lambda vezes a identidade, e estamos procurando um vetor v tal que esta nova matriz vezes v dê o vetor zero.",
  "model": "google_nmt",
  "from_community_srt": "Mas numericamente, está traduzindo um vetor descrito no idioma dela para um no nosso. O que fez tudo finalmente fazer sentido para mim foi pensar em como ele leva nosso equívoco do que Jennifer quer dizer,",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Agora, isso sempre será verdade se v for o vetor zero, mas isso é chato.",
  "model": "google_nmt",
  "from_community_srt": "isto é, o vetor a que chegamos usando as mesmas coordenadas mas em nosso sistema, e em seguida,",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "O que queremos é um autovetor diferente de zero.",
  "model": "google_nmt",
  "from_community_srt": "transforma-o no vector a que ela realmente se referia.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "E se você assistir aos capítulos 5 e 6, saberá que a única maneira de o produto de uma matriz com um vetor diferente de zero se tornar zero é se a transformação associada a essa matriz comprimir o espaço em uma dimensão inferior.",
  "model": "google_nmt",
  "from_community_srt": "Que tal ir no outro sentido? No exemplo que eu usei no início deste vídeo, quando tenho o vector com coordenadas [3,2] em nosso sistema, Como é que eu calculei que ele teria coordenadas [5/3,",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "E esse esmagamento corresponde a um determinante zero para a matriz.",
  "model": "google_nmt",
  "from_community_srt": "1/3] no sistema de Jennifer? Você começa com aquela matriz de  mudança de base",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Para ser concreto, digamos que sua matriz A tenha colunas 2, 1 e 2, 3, e pense em subtrair um valor variável, lambda, de cada entrada diagonal.",
  "model": "google_nmt",
  "from_community_srt": "que traduz a linguagem de Jennifer para a nossa, e então você toma a sua inversa. Lembre-se,",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Agora imagine ajustar o lambda, girando um botão para alterar seu valor.",
  "model": "google_nmt",
  "from_community_srt": "a inversa de uma transformação é uma nova transformação que corresponde a reproduzir a primeira ao contrário. Na prática,",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "À medida que o valor de lambda muda, a própria matriz muda e, portanto, o determinante da matriz muda.",
  "model": "google_nmt",
  "from_community_srt": "especialmente quando você está trabalhando em mais de duas dimensões, você usaria um computador para calcular a matriz que representa esta inversa.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "O objetivo aqui é encontrar um valor de lambda que torne esse determinante zero, o que significa que a transformação ajustada comprime o espaço em uma dimensão inferior.",
  "model": "google_nmt",
  "from_community_srt": "Neste caso, a inversa da matriz de mudança de base, que tem os vetores de base de Jennifer como suas colunas, acaba tendo por colunas [1/3,-1/3] e [1/3,",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "Nesse caso, o ponto ideal ocorre quando lambda é igual a 1.",
  "model": "google_nmt",
  "from_community_srt": "2/3]. Assim,",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "É claro que, se tivéssemos escolhido alguma outra matriz, o autovalor poderia não ser necessariamente 1.",
  "model": "google_nmt",
  "from_community_srt": "por exemplo, para ver como que o vetor [3,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "O ponto ideal pode ser atingido por algum outro valor de lambda.",
  "model": "google_nmt",
  "from_community_srt": "2] fica no sistema de Jennifer, multiplicamos esta matriz de mudança de base inversa pelo vetor [3,",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Então isso é bastante, mas vamos desvendar o que isso quer dizer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Quando lambda é igual a 1, a matriz A menos lambda vezes a identidade comprime o espaço em uma linha.",
  "model": "google_nmt",
  "from_community_srt": "2] que termina como [5/3, 1/3]. Então isso,",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Isso significa que existe um vetor diferente de zero v tal que A menos lambda vezes a identidade vezes v é igual ao vetor zero.",
  "model": "google_nmt",
  "from_community_srt": "em poucas palavras, é como traduzir a descrição de vetores individuais entre os sistemas de coordenadas. A matriz cujas colunas representam os vetores de base de Jennifer mas escritos em nossas coordenadas",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "E lembre-se, a razão pela qual nos preocupamos com isso é porque significa A vezes v é igual a lambda vezes v, o que você pode interpretar como dizendo que o vetor v é um autovetor de A, permanecendo em seu próprio intervalo durante a transformação A.",
  "model": "google_nmt",
  "from_community_srt": "traduz vetores da língua dela para a nossa língua. E a matriz inversa faz o oposto. Mas vetores não são a única coisa que nós descrevemos utilizando coordenadas.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "Neste exemplo, o autovalor correspondente é 1, então v permaneceria fixo no lugar.",
  "model": "google_nmt",
  "from_community_srt": "Para esta parte seguinte é importante que vocês estejam todos confortáveis representando transformações com matrizes e que vocês saibam como a multiplicação de matrizes corresponde à composição de transformações sucessivas.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Faça uma pausa e pondere se você precisa ter certeza de que essa linha de raciocínio é boa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Esse é o tipo de coisa que mencionei na introdução.",
  "model": "google_nmt",
  "from_community_srt": "Definitivamente pare e dê uma olhada nos capítulos 3 e 4 se você não estiver confortável com algum desses temas.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Se você não tivesse uma compreensão sólida dos determinantes e por que eles se relacionam com sistemas lineares de equações com soluções diferentes de zero, uma expressão como essa pareceria completamente inesperada.",
  "model": "google_nmt",
  "from_community_srt": "Considere alguma transformação linear como uma rotação anti-horária de 90°. Quando você e eu a representamos com a matriz, seguimos onde os vetores de base î e ĵ vão.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Para ver isso em ação, vamos revisitar o exemplo desde o início, com uma matriz cujas colunas são 3, 0 e 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "î acaba no ponto com coordenadas [0, 1] e ĵ acaba no local com coordenadas [-1, 0] Então,",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Para descobrir se um valor lambda é um autovalor, subtraia-o das diagonais desta matriz e calcule o determinante.",
  "model": "google_nmt",
  "from_community_srt": "essas coordenadas tornam-se as colunas de nossa matriz, mas esta representação é fortemente amarrada em nossa escolha de vetores de base, devido ao fato de que estamos seguindo î e ĵ em primeiro lugar",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Fazendo isso, obtemos um certo polinômio quadrático em lambda, 3 menos lambda vezes 2 menos lambda.",
  "model": "google_nmt",
  "from_community_srt": "e que nós estamos gravando seu ponto de chegada em nosso próprio sistema de coordenadas. Como Jennifer descreveria essa mesma rotação de 90° no espaço? Você pode ser tentado a apenas",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Como lambda só pode ser um autovalor se esse determinante for zero, você pode concluir que os únicos autovalores possíveis são lambda igual a 2 e lambda igual a 3.",
  "model": "google_nmt",
  "from_community_srt": "traduzir as colunas de nossa matriz de rotação para a linguagem de Jennifer. Mas isso não está bem certo.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Para descobrir quais são os autovetores que realmente possuem um desses autovalores, digamos que lambda é igual a 2, insira esse valor de lambda na matriz e, em seguida, resolva quais vetores essa matriz alterada diagonalmente envia para zero.",
  "model": "google_nmt",
  "from_community_srt": "Essas colunas representam onde nossos vetores de base, î e ĵ vão. Mas a matriz que Jennifer quer deve representar onde os vetores da base dela vão parar, e ele precisa descrever os pontos de pouso dos vetores na língua dela. Aqui está uma maneira comum de pensar em como isto é feito.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Se você calculasse isso da mesma forma que faria com qualquer outro sistema linear, veria que as soluções são todos os vetores na linha diagonal medido por menos 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Comece com qualquer vector escrito na linguagem de Jennifer. Ao invés de tentar seguir o que acontece a ele em termos de sua linguagem em primeiro lugar,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Isto corresponde ao fato de que a matriz inalterada, 3, 0, 1, 2, tem o efeito de esticar todos esses vetores por um fator de 2.",
  "model": "google_nmt",
  "from_community_srt": "vamos traduzi-lo em nossa língua utilizando a matriz de mudança de base, aquela cujas colunas representam sua seus vetores de base em nossa língua. Isso nos dá o mesmo vetor, mas agora escrito em nossa língua.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Agora, uma transformação 2D não precisa ter vetores próprios.",
  "model": "google_nmt",
  "from_community_srt": "Em seguida, aplique a matriz de transformação  para o que você obteve multiplicando-a à esquerda.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Por exemplo, considere uma rotação de 90 graus.",
  "model": "google_nmt",
  "from_community_srt": "Isto nos diz onde aquele vetor vai parar mas ainda em nossa língua.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Isso não possui vetores próprios, pois gira cada vetor fora de seu próprio intervalo.",
  "model": "google_nmt",
  "from_community_srt": "Assim, como uma última etapa, aplique a inversa da matriz de mudança de base,",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Se você realmente tentar calcular os autovalores de uma rotação como essa, observe o que acontece.",
  "model": "google_nmt",
  "from_community_srt": "multiplicada à esquerda como de costume para obter o vetor transformado mas agora, na linguagem de Jennifer.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Sua matriz possui colunas 0, 1 e negativo 1, 0.",
  "model": "google_nmt",
  "from_community_srt": "Uma vez que poderíamos fazer isso com qualquer vetor escrito em sua língua primeiro, aplicando a mudança de base, em seguida,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Subtraia lambda dos elementos diagonais e procure quando o determinante é zero.",
  "model": "google_nmt",
  "from_community_srt": "a transformação e em seguida, a inversa da mudança de base; essa composição de três matrizes",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "Nesse caso, você obtém o polinômio lambda ao quadrado mais 1.",
  "model": "google_nmt",
  "from_community_srt": "nos dá a matriz de transformação na linguagem de Jennifer, que leva um vetor,",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "As únicas raízes desse polinômio são os números imaginários, i e negativo i.",
  "model": "google_nmt",
  "from_community_srt": "escrito na linguagem dela à versão transformada do vetor, na linguagem dela.",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "O fato de não existirem soluções de números reais indica que não existem autovetores.",
  "model": "google_nmt",
  "from_community_srt": "Para este exemplo específico, quando vetores da base de Jennifer são [2,1] e [-1, 1] no nosso idioma, e quando a transformação é uma rotação de 90°,",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Outro exemplo bastante interessante que vale a pena manter em mente é uma tesoura.",
  "model": "google_nmt",
  "from_community_srt": "o produto destas três matrizes se você fizer a conta, tem colunas [1/3,",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Isso fixa o i-hat no lugar e move o j-hat 1, de modo que sua matriz tenha as colunas 1, 0 e 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "5/3] e [-2/3, -1/3]. Então,",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Todos os vetores no eixo x são autovetores com autovalor 1, pois permanecem fixos no lugar.",
  "model": "google_nmt",
  "from_community_srt": "se Jennifer multiplica essa matriz pelas coordenadas de um vetor em seu sistema, ele irá retornar a versão rodada de 90° daquele vetor,",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "Na verdade, esses são os únicos autovetores.",
  "model": "google_nmt",
  "from_community_srt": "expressa em seu sistema de coordenadas.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Quando você subtrai lambda das diagonais e calcula o determinante, o que você obtém é 1 menos lambda ao quadrado.",
  "model": "google_nmt",
  "from_community_srt": "Em geral, sempre que você vê uma expressão como A ^ (- 1) MA ele sugere uma espécie matemática de \"empatia\".",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "E a única raiz desta expressão é lambda igual a 1.",
  "model": "google_nmt",
  "from_community_srt": "Essa matriz do meio representa uma transformação de algum tipo, como você a vê, e as duas matrizes exteriores representam a empatia,",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Isso está de acordo com o que vemos geometricamente, que todos os autovetores têm autovalor 1.",
  "model": "google_nmt",
  "from_community_srt": "a mudança de perspectiva e o produto matricial completo representa aquela mesma transformação,",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Tenha em mente, porém, que também é possível ter apenas um autovalor, mas com mais do que apenas uma linha cheia de autovetores.",
  "model": "google_nmt",
  "from_community_srt": "mas como alguém a vê. Para aqueles de vocês perguntando por que nos preocupamos com alternar sistemas de coordenadas",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Um exemplo simples é uma matriz que dimensiona tudo por 2.",
  "model": "google_nmt",
  "from_community_srt": "o próximo vídeo sobre autovalores e autovetores vai dar um exemplo muito importante disto.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "O único autovalor é 2, mas todo vetor no plano passa a ser um autovetor com esse autovalor.",
  "model": "google_nmt",
  "from_community_srt": "Vejo vocês lá!",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Agora é outro bom momento para fazer uma pausa e refletir sobre isso antes de passar para o último tópico.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Quero terminar aqui com a ideia de uma base própria, que se baseia muito nas ideias do último vídeo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Dê uma olhada no que acontece se nossos vetores de base forem autovetores.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Por exemplo, talvez i-hat seja dimensionado em menos 1 e j-hat seja dimensionado em 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Escrevendo suas novas coordenadas como as colunas de uma matriz, observe que esses múltiplos escalares, menos 1 e 2, que são os autovalores de i-hat e j-hat, ficam na diagonal de nossa matriz, e todas as outras entradas são 0 .",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Sempre que uma matriz tem zeros em todos os lugares, exceto na diagonal, ela é chamada, razoavelmente, de matriz diagonal.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "E a maneira de interpretar isto é que todos os vetores de base são autovetores, sendo as entradas diagonais desta matriz os seus autovalores.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Há muitas coisas que tornam as matrizes diagonais muito mais agradáveis de trabalhar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Um grande problema é que é mais fácil calcular o que acontecerá se você multiplicar essa matriz por ela mesma várias vezes.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Como tudo o que uma dessas matrizes faz é dimensionar cada vetor de base por algum autovalor, aplicar essa matriz muitas vezes, digamos 100 vezes, corresponderá apenas a dimensionar cada vetor de base pela 100ª potência do autovalor correspondente.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Em contraste, tente calcular a centésima potência de uma matriz não diagonal.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Sério, experimente por um momento.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "É um pesadelo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "É claro que você raramente terá a sorte de ter seus vetores de base também como autovetores.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Mas se a sua transformação tiver muitos autovetores, como o do início deste vídeo, o suficiente para que você possa escolher um conjunto que abranja todo o espaço, então você poderá alterar seu sistema de coordenadas para que esses autovetores sejam seus vetores de base.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Falei sobre mudança de base no vídeo passado, mas vou fazer um lembrete super rápido aqui de como expressar uma transformação atualmente escrita em nosso sistema de coordenadas em um sistema diferente.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Pegue as coordenadas dos vetores que deseja usar como uma nova base, que neste caso significa nossos dois autovetores, e depois transforme essas coordenadas nas colunas de uma matriz, conhecida como matriz de mudança de base.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Quando você imprensa a transformação original, colocando a matriz de mudança de base à sua direita e o inverso da matriz de mudança de base à sua esquerda, o resultado será uma matriz representando essa mesma transformação, mas da perspectiva da nova coordenada dos vetores de base sistema.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "O objetivo de fazer isso com autovetores é que essa nova matriz tem a garantia de ser diagonal com seus autovalores correspondentes nessa diagonal.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Isso ocorre porque representa trabalhar em um sistema de coordenadas onde o que acontece com os vetores de base é que eles são escalonados durante a transformação.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Um conjunto de vetores de base que também são autovetores é chamado, novamente, razoavelmente, de autobase.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Portanto, se, por exemplo, você precisasse calcular a centésima potência desta matriz, seria muito mais fácil mudar para uma base própria, calcular a centésima potência nesse sistema e depois converter novamente para o nosso sistema padrão.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Você não pode fazer isso com todas as transformações.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Um cisalhamento, por exemplo, não possui vetores próprios suficientes para abranger todo o espaço.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Mas se você puder encontrar uma base própria, isso tornará as operações matriciais realmente adoráveis.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Para aqueles que desejam resolver um quebra-cabeça bem bacana para ver como ele funciona em ação e como pode ser usado para produzir alguns resultados surpreendentes, deixarei um aviso aqui na tela.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Dá um pouco de trabalho, mas acho que você vai gostar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "O próximo e último vídeo desta série será sobre espaços vetoriais abstratos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]