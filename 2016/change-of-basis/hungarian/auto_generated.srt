1
00:00:19,920 --> 00:00:23,052
A sajátvektorok és sajátértékek egyike azoknak a témáknak, 

2
00:00:23,052 --> 00:00:25,760
amelyeket sok diák különösen nem talál intuitívnak.

3
00:00:25,760 --> 00:00:29,997
Az olyan kérdések, mint például, hogy miért csináljuk ezt, és mit jelent ez valójában, 

4
00:00:29,997 --> 00:00:33,260
túl gyakran csak lebegnek a számítások megválaszolatlan tengerében.

5
00:00:33,920 --> 00:00:37,209
És ahogy a sorozat videóit közzétettem, sokan megjegyezték, 

6
00:00:37,209 --> 00:00:40,060
hogy alig várják, hogy ezt a témát vizualizálhassák.

7
00:00:40,680 --> 00:00:43,773
Gyanítom, hogy ennek nem annyira az az oka, hogy a tulajdonképpeni 

8
00:00:43,773 --> 00:00:46,360
dolgok különösen bonyolultak vagy rosszul magyarázottak.

9
00:00:46,860 --> 00:00:51,180
Valójában ez viszonylag egyszerű, és szerintem a legtöbb könyv jól elmagyarázza.

10
00:00:51,520 --> 00:00:54,668
A probléma az, hogy ennek csak akkor van igazán értelme, 

11
00:00:54,668 --> 00:00:58,480
ha az azt megelőző témák közül soknak van szilárd vizuális megértése.

12
00:00:59,060 --> 00:01:02,241
A legfontosabb, hogy tudd, hogyan kell a mátrixokról lineáris 

13
00:01:02,241 --> 00:01:06,193
transzformációként gondolkodni, de olyan dolgokkal is tisztában kell lenned, 

14
00:01:06,193 --> 00:01:09,940
mint a determinánsok, a lineáris egyenletrendszerek és az alapváltozások.

15
00:01:10,720 --> 00:01:14,903
A sajátértékekkel kapcsolatos zavar általában inkább a fenti témák valamelyikének 

16
00:01:14,903 --> 00:01:19,240
ingatag alapozásával függ össze, mint magukkal a sajátvektorokkal és sajátértékekkel.

17
00:01:19,980 --> 00:01:24,840
Kezdetnek tekintsünk egy kétdimenziós lineáris transzformációt, például az itt láthatót.

18
00:01:25,460 --> 00:01:29,402
Az i-hat alapvektort a 3, 0 koordinátákra, a j-hatot pedig az 1, 

19
00:01:29,402 --> 00:01:31,040
2 koordinátákra helyezi át.

20
00:01:31,780 --> 00:01:35,640
Tehát egy olyan mátrixszal ábrázoljuk, amelynek oszlopai a 3, 0 és az 1, 2.

21
00:01:36,600 --> 00:01:39,657
Koncentráljon arra, hogy mit tesz egy adott vektorral, 

22
00:01:39,657 --> 00:01:44,160
és gondoljon a vektor kiterjedésére, az origóján és a csúcsán áthaladó egyenesre.

23
00:01:44,920 --> 00:01:48,380
A legtöbb vektor a transzformáció során ki fog esni az átfogásból.

24
00:01:48,780 --> 00:01:53,058
Úgy értem, elég véletlennek tűnne, ha a hely, ahol a vektor leszállt, 

25
00:01:53,058 --> 00:01:55,320
szintén valahol ezen a vonalon lenne.

26
00:01:57,400 --> 00:02:00,699
Néhány speciális vektor azonban megmarad a saját tartományában, 

27
00:02:00,699 --> 00:02:04,256
ami azt jelenti, hogy a mátrix hatása egy ilyen vektorra csak annyi, 

28
00:02:04,256 --> 00:02:07,040
hogy megnyújtja vagy összenyomja azt, mint egy skalár.

29
00:02:09,460 --> 00:02:14,100
Ebben a konkrét példában az i-hat alapvektor egy ilyen speciális vektor.

30
00:02:14,640 --> 00:02:19,914
Az i-hat kiterjedése az x-tengely, és a mátrix első oszlopából láthatjuk, 

31
00:02:19,914 --> 00:02:24,120
hogy az i-hat 3-szorosára mozog, még mindig az x-tengelyen.

32
00:02:26,320 --> 00:02:31,296
Ráadásul a lineáris transzformációk működése miatt az x-tengely bármely 

33
00:02:31,296 --> 00:02:36,480
más vektora is csak 3-szorosára nyúlik, és így a saját tartományában marad.

34
00:02:38,500 --> 00:02:43,257
Egy kissé sunyibb vektor, amely az átalakítás során a saját tartományán marad, 

35
00:02:43,257 --> 00:02:44,040
negatív 1, 1.

36
00:02:44,660 --> 00:02:47,140
A végeredmény a 2-szeresére nyúlik.

37
00:02:49,000 --> 00:02:53,758
És ismét, a linearitás azt jelenti, hogy a fickó által felölelt 

38
00:02:53,758 --> 00:02:58,220
átlós egyenes bármely más vektora 2-szeresére fog megnyúlni.

39
00:02:59,820 --> 00:03:02,003
És ennél a transzformációnál ezek mind olyan vektorok, 

40
00:03:02,003 --> 00:03:05,180
amelyeknek megvan az a különleges tulajdonságuk, hogy a tartományukban maradnak.

41
00:03:05,620 --> 00:03:11,980
Az x-tengelyen lévőket 3-szorosára, az átlós vonalon lévőket pedig 2-szeresére nyújtják.

42
00:03:12,760 --> 00:03:16,085
Bármely más vektor a transzformáció során némileg elfordul, 

43
00:03:16,085 --> 00:03:18,080
lekerül a vonalról, amelyen áthalad.

44
00:03:22,520 --> 00:03:26,638
Ahogyan már kitalálhattad, ezeket a speciális vektorokat a transzformáció 

45
00:03:26,638 --> 00:03:31,257
sajátvektorainak nevezzük, és minden egyes sajátvektorhoz tartozik egy úgynevezett 

46
00:03:31,257 --> 00:03:35,599
sajátérték, amely nem más, mint az a tényező, amellyel a transzformáció során 

47
00:03:35,599 --> 00:03:37,380
megnyújtották vagy összenyomták.

48
00:03:40,280 --> 00:03:43,327
Természetesen semmi különös nincs a nyújtás és az összenyomás között, 

49
00:03:43,327 --> 00:03:45,940
vagy abban, hogy ezek a sajátértékek történetesen pozitívak.

50
00:03:46,380 --> 00:03:51,406
Egy másik példában egy olyan sajátvektorunk lehet, amelynek sajátértéke negatív 1 fele, 

51
00:03:51,406 --> 00:03:55,120
ami azt jelenti, hogy a vektort megfordítjuk és 1 fele szorozzuk.

52
00:03:56,980 --> 00:04:00,078
De a fontos része itt az, hogy a vonalban maradjon, 

53
00:04:00,078 --> 00:04:02,760
amit kifeszít, anélkül, hogy lefordulna róla.

54
00:04:04,460 --> 00:04:07,377
Hogy meglássuk, miért lehet ez egy hasznos gondolat, 

55
00:04:07,377 --> 00:04:09,800
gondoljunk néhány háromdimenziós forgatásra.

56
00:04:11,660 --> 00:04:15,983
Ha találunk egy sajátvektort ehhez a forgáshoz, egy olyan vektort, 

57
00:04:15,983 --> 00:04:20,500
amely a saját tartományában marad, akkor megtaláltuk a forgástengelyt.

58
00:04:22,600 --> 00:04:28,285
És sokkal könnyebb egy 3D-s forgatásról a forgástengely és a forgási szög 

59
00:04:28,285 --> 00:04:34,740
szempontjából gondolkodni, mint a transzformációhoz tartozó teljes 3x3-as mátrixról.

60
00:04:37,000 --> 00:04:40,937
Ebben az esetben egyébként a megfelelő sajátértéknek 1-nek kell lennie, 

61
00:04:40,937 --> 00:04:45,860
mivel a forgatás soha nem nyújt vagy szorít semmit, így a vektor hossza változatlan marad.

62
00:04:48,080 --> 00:04:50,020
Ez a minta gyakran megjelenik a lineáris algebrában.

63
00:04:50,440 --> 00:04:54,813
Bármely lineáris transzformáció esetében, amelyet egy mátrix ír le, megérthetjük, 

64
00:04:54,813 --> 00:04:59,400
hogy mit csinál, ha a mátrix oszlopait a bázisvektorok leszállóhelyeiként olvassuk le.

65
00:05:00,020 --> 00:05:03,744
De gyakran jobb módja annak, hogy a lineáris transzformáció tényleges 

66
00:05:03,744 --> 00:05:08,000
működésének lényegéhez jussunk, és kevésbé függ az adott koordinátarendszertől, 

67
00:05:08,000 --> 00:05:10,820
ha megkeressük a sajátvektorokat és a sajátértékeket.

68
00:05:15,460 --> 00:05:19,196
Nem fogok itt a sajátvektorok és sajátértékek számítási módszereinek 

69
00:05:19,196 --> 00:05:22,500
minden részletére kitérni, de megpróbálok áttekintést adni a 

70
00:05:22,500 --> 00:05:26,020
fogalmi megértés szempontjából legfontosabb számítási ötletekről.

71
00:05:27,180 --> 00:05:30,480
Szimbolikusan így néz ki egy sajátvektor gondolata.

72
00:05:31,040 --> 00:05:36,002
A valamilyen transzformációt reprezentáló mátrix, amelynek v a sajátvektora, 

73
00:05:36,002 --> 00:05:39,740
lambda pedig egy szám, nevezetesen a megfelelő sajátérték.

74
00:05:40,680 --> 00:05:44,836
Ez a kifejezés azt mondja ki, hogy a mátrix-vektor szorzat, A szorozva v-vel, 

75
00:05:44,836 --> 00:05:49,313
ugyanazt az eredményt adja, mintha a v sajátvektort csak valamilyen lambda értékkel 

76
00:05:49,313 --> 00:05:49,900
skáláznánk.

77
00:05:51,000 --> 00:05:55,636
Tehát az A mátrix sajátvektorainak és sajátértékeinek megtalálása a v és lambda 

78
00:05:55,636 --> 00:06:00,100
azon értékeinek megtalálását jelenti, amelyek igazzá teszik ezt a kifejezést.

79
00:06:01,920 --> 00:06:07,578
Elsőre kissé nehézkes a munka, mert a bal oldali oldal mátrix-vektor szorzást jelent, 

80
00:06:07,578 --> 00:06:10,540
de a jobb oldali oldal skalár-vektor szorzás.

81
00:06:11,120 --> 00:06:15,581
Kezdjük tehát azzal, hogy a jobb oldalt átírjuk valamiféle mátrix-vektor szorzásnak, 

82
00:06:15,581 --> 00:06:18,625
egy olyan mátrixot használva, amely bármely vektor lambda 

83
00:06:18,625 --> 00:06:20,620
faktorral való skálázását eredményezi.

84
00:06:21,680 --> 00:06:26,069
Egy ilyen mátrix oszlopai azt jelölik, hogy mi történik az egyes bázisvektorokkal, 

85
00:06:26,069 --> 00:06:29,137
és minden bázisvektor egyszerűen megszorozódik lambdával, 

86
00:06:29,137 --> 00:06:32,627
így ez a mátrix az átló mentén a lambda számot fogja tartalmazni, 

87
00:06:32,627 --> 00:06:34,320
mindenhol máshol pedig nullákat.

88
00:06:36,180 --> 00:06:40,416
Ezt a fickót általában úgy írjuk le, hogy a lambdát faktorozzuk ki, és úgy írjuk, 

89
00:06:40,416 --> 00:06:44,860
hogy lambda szorozva i-vel, ahol i az azonossági mátrix az átló mentén lévő 1-esekkel.

90
00:06:45,860 --> 00:06:48,889
Mivel mindkét oldal mátrix-vektor szorzásnak tűnik, 

91
00:06:48,889 --> 00:06:51,860
kivonhatjuk a jobb oldalt, és faktorálhatjuk a v-t.

92
00:06:54,160 --> 00:06:59,300
Tehát most van egy új mátrixunk, A mínusz lambda szorozva az azonossággal, 

93
00:06:59,300 --> 00:07:04,920
és olyan vektort keresünk, hogy ez az új mátrix szorozva v-vel adja a nullvektort.

94
00:07:06,380 --> 00:07:11,100
Ez mindig igaz lesz, ha v maga a nullvektor, de ez unalmas.

95
00:07:11,340 --> 00:07:13,640
Mi egy nem nulla sajátvektort akarunk.

96
00:07:14,420 --> 00:07:17,803
És ha megnézed az 5. és 6. fejezetet, tudni fogod, 

97
00:07:17,803 --> 00:07:22,845
hogy egy mátrix és egy nem nulla vektor szorzata csak akkor válhat nullává, 

98
00:07:22,845 --> 00:07:28,020
ha a mátrixhoz tartozó transzformáció a teret alacsonyabb dimenzióba szorítja.

99
00:07:29,300 --> 00:07:34,220
És ez a squishification megfelel a mátrix nulla determinánsának.

100
00:07:35,480 --> 00:07:39,914
Konkrétan, tegyük fel, hogy az A mátrixunknak 2, 1 és 2, 3 oszlopa van, 

101
00:07:39,914 --> 00:07:44,965
és gondoljunk arra, hogy minden átlós bejegyzésből levonunk egy változó összeget, 

102
00:07:44,965 --> 00:07:45,520
lambda-t.

103
00:07:46,480 --> 00:07:48,520
Most képzelje el, hogy a lambda értékét egy gombot 

104
00:07:48,520 --> 00:07:50,280
elforgatva változtatja meg a lambda értékét.

105
00:07:50,940 --> 00:07:54,657
Ahogy a lambda értéke változik, maga a mátrix is változik, 

106
00:07:54,657 --> 00:07:57,240
és így a mátrix determinánsa is változik.

107
00:07:58,220 --> 00:08:00,901
A cél itt az, hogy megtaláljuk a lambda olyan értékét, 

108
00:08:00,901 --> 00:08:03,680
amely ezt a determinánst nullává teszi, ami azt jelenti, 

109
00:08:03,680 --> 00:08:07,240
hogy a módosított transzformáció a teret alacsonyabb dimenzióba szorítja.

110
00:08:08,160 --> 00:08:11,160
Ebben az esetben a legjobb pont akkor jön el, amikor a lambda egyenlő 1-gyel.

111
00:08:12,180 --> 00:08:16,120
Természetesen, ha más mátrixot választottunk volna, a sajátérték nem feltétlenül lenne 1.

112
00:08:16,240 --> 00:08:18,600
Az édes pontot a lambda más értékénél lehet elérni.

113
00:08:20,080 --> 00:08:22,960
Szóval ez elég sok, de bontsuk ki, mit is jelent ez.

114
00:08:22,960 --> 00:08:26,377
Ha lambda egyenlő 1-gyel, akkor az A mátrix mínusz lambda 

115
00:08:26,377 --> 00:08:29,560
szorozva az azonossággal egy vonalra szorítja a teret.

116
00:08:30,440 --> 00:08:33,585
Ez azt jelenti, hogy van egy olyan nem nulla vektor v, 

117
00:08:33,585 --> 00:08:38,559
hogy A mínusz lambda szorozva az azonossággal szorozva v-vel egyenlő a nulla vektorral.

118
00:08:40,480 --> 00:08:45,331
És ne feledjük, hogy ez azért érdekel minket, mert ez azt jelenti, 

119
00:08:45,331 --> 00:08:50,980
hogy A szorozva v-vel egyenlő lambda szorozva v-vel, amit úgy olvashatunk le, 

120
00:08:50,980 --> 00:08:57,280
hogy a vektor A sajátvektora, és az A transzformáció során a saját tartományában marad.

121
00:08:58,320 --> 00:09:04,020
Ebben a példában a megfelelő sajátérték 1, így v valójában csak a helyén marad.

122
00:09:06,220 --> 00:09:08,549
Tartson szünetet, és gondolkodjon el azon, hogy meg kell-e győződnie arról, 

123
00:09:08,549 --> 00:09:09,500
hogy ez a gondolatsor jól esik.

124
00:09:13,380 --> 00:09:15,640
Ez az a fajta dolog, amit a bevezetőben említettem.

125
00:09:16,220 --> 00:09:19,035
Ha nem ismernéd a determinánsokat, és nem tudnád, 

126
00:09:19,035 --> 00:09:23,428
hogy miért kapcsolódnak a nem nulla megoldású lineáris egyenletrendszerekhez, 

127
00:09:23,428 --> 00:09:26,300
egy ilyen kifejezés teljesen váratlanul érne téged.

128
00:09:28,320 --> 00:09:31,881
Hogy ezt a gyakorlatban is lássuk, nézzük meg újra a példát az elején, 

129
00:09:31,881 --> 00:09:34,540
egy olyan mátrixszal, amelynek oszlopai 3, 0 és 1, 2.

130
00:09:35,350 --> 00:09:39,341
Ahhoz, hogy megtudjuk, hogy egy lambda érték sajátérték-e, 

131
00:09:39,341 --> 00:09:43,400
vonjuk ki a mátrix átlóiból, és számítsuk ki a determinánst.

132
00:09:50,580 --> 00:09:54,468
Ha ezt megtesszük, akkor egy bizonyos négyzetes polinomot kapunk lambdában, 

133
00:09:54,468 --> 00:09:56,720
3 mínusz lambda szorozva 2 mínusz lambdával.

134
00:09:57,800 --> 00:10:02,615
Mivel lambda csak akkor lehet sajátérték, ha ez a determináns történetesen nulla, 

135
00:10:02,615 --> 00:10:06,432
arra következtethetünk, hogy az egyetlen lehetséges sajátértékek 

136
00:10:06,432 --> 00:10:08,840
a lambda egyenlő 2 és a lambda egyenlő 3.

137
00:10:09,640 --> 00:10:14,200
Hogy kitaláljuk, melyek azok a sajátvektorok, amelyeknek valóban van egy ilyen 

138
00:10:14,200 --> 00:10:19,050
sajátértékük, mondjuk lambda egyenlő 2-vel, adjuk meg a lambda értékét a mátrixhoz, 

139
00:10:19,050 --> 00:10:23,900
majd oldjuk meg, hogy ez az átlósan módosított mátrix mely vektorokat küldi nullára.

140
00:10:24,940 --> 00:10:29,709
Ha ezt úgy számolnád ki, mint bármely más lineáris rendszert, akkor azt látnád, 

141
00:10:29,709 --> 00:10:34,300
hogy a megoldások a negatív 1, 1 által felölelt átlós egyenes összes vektora.

142
00:10:35,220 --> 00:10:39,134
Ez megfelel annak a ténynek, hogy a változatlan 3, 0, 1, 

143
00:10:39,134 --> 00:10:43,460
2 mátrix hatására az összes említett vektor 2-szeresére nyúlik.

144
00:10:46,320 --> 00:10:50,200
Egy 2D transzformációnak nem kell, hogy legyenek sajátvektorai.

145
00:10:50,720 --> 00:10:53,400
Vegyük például a 90 fokos elforgatást.

146
00:10:53,660 --> 00:10:58,200
Ennek nincsenek sajátvektorai, mivel minden vektort elforgat a saját tartományából.

147
00:11:00,800 --> 00:11:04,235
Ha valóban megpróbáljuk kiszámítani egy ilyen forgatás sajátértékeit, 

148
00:11:04,235 --> 00:11:05,560
figyeljük meg, mi történik.

149
00:11:06,300 --> 00:11:10,140
Mátrixának oszlopai 0, 1 és negatív 1, 0.

150
00:11:11,100 --> 00:11:15,800
Vonjuk ki a lambdát az átlós elemekből, és keressük meg, hogy a determináns mikor nulla.

151
00:11:18,140 --> 00:11:21,940
Ebben az esetben a polinom lambda négyzete plusz 1.

152
00:11:22,680 --> 00:11:27,920
Ennek a polinomnak az egyetlen gyöke a képzeletbeli számok, az i és a negatív i.

153
00:11:28,840 --> 00:11:33,600
Az a tény, hogy nincsenek valós számú megoldások, azt jelzi, hogy nincsenek sajátvektorok.

154
00:11:35,540 --> 00:11:39,820
Egy másik elég érdekes példa, amit érdemes a fejünkben tartani, az egy nyírógép.

155
00:11:40,560 --> 00:11:45,161
Ez rögzíti az i-kalapot a helyén, és áthelyezi a j-kalapot 1-gyel, 

156
00:11:45,161 --> 00:11:47,840
így a mátrixának oszlopai 1, 0 és 1, 1.

157
00:11:48,740 --> 00:11:54,540
Az x-tengelyen lévő összes vektor 1 sajátértékű sajátvektor, mivel a helyükön maradnak.

158
00:11:55,680 --> 00:11:57,820
Valójában ezek az egyetlen sajátvektorok.

159
00:11:58,760 --> 00:12:03,622
Ha kivonjuk a lambdát az átlóból, és kiszámítjuk a determinánst, 

160
00:12:03,622 --> 00:12:06,540
akkor 1 mínusz lambda négyzetet kapunk.

161
00:12:09,320 --> 00:12:12,860
És ennek a kifejezésnek az egyetlen gyökere a lambda egyenlő 1.

162
00:12:14,560 --> 00:12:17,549
Ez összhangban van azzal, amit geometriai szempontból látunk, 

163
00:12:17,549 --> 00:12:19,720
hogy minden sajátvektornak 1 sajátértéke van.

164
00:12:21,080 --> 00:12:25,686
Ne feledjük azonban, hogy az is lehetséges, hogy csak egy sajátértékünk van, 

165
00:12:25,686 --> 00:12:28,020
de több mint egy sornyi sajátvektorral.

166
00:12:29,900 --> 00:12:33,180
Egy egyszerű példa egy mátrix, amely mindent 2-vel skáláz.

167
00:12:33,900 --> 00:12:37,336
Az egyetlen sajátérték a 2, de a síkban minden 

168
00:12:37,336 --> 00:12:40,700
vektor sajátvektor lesz ezzel a sajátértékkel.

169
00:12:42,000 --> 00:12:45,165
Most itt az ideje, hogy megálljunk és elgondolkodjunk ezen, 

170
00:12:45,165 --> 00:12:46,960
mielőtt rátérnék az utolsó témára.

171
00:13:03,540 --> 00:13:06,327
Szeretném itt befejezni a sajátbázis gondolatával, 

172
00:13:06,327 --> 00:13:09,880
amely nagyban támaszkodik az előző videóban elhangzott ötletekre.

173
00:13:11,480 --> 00:13:16,380
Nézzük meg, mi történik, ha az alapvektoraink történetesen sajátvektorok.

174
00:13:17,120 --> 00:13:22,380
Például, lehet, hogy az i-hat negatív 1, a j-hat pedig 2 értékkel van skálázva.

175
00:13:23,420 --> 00:13:28,406
Ha az új koordinátáikat egy mátrix oszlopaként írjuk fel, akkor észrevehetjük, 

176
00:13:28,406 --> 00:13:33,203
hogy az i-hat és j-hat sajátértékei, a negatív 1 és 2 skaláris többszörösei 

177
00:13:33,203 --> 00:13:37,180
a mátrixunk átlóján helyezkednek el, és minden más bejegyzés 0.

178
00:13:38,880 --> 00:13:42,501
Ha egy mátrixnak az átlótól eltérő helyen mindenhol vannak nullái, 

179
00:13:42,501 --> 00:13:45,420
akkor azt - ésszerűen - diagonális mátrixnak nevezzük.

180
00:13:45,840 --> 00:13:50,400
Ezt úgy kell értelmezni, hogy az összes bázisvektor sajátvektor, 

181
00:13:50,400 --> 00:13:54,400
és ennek a mátrixnak az átlós bejegyzései a sajátértékek.

182
00:13:57,100 --> 00:14:01,060
Sok minden van, ami miatt a diagonális mátrixokkal sokkal szebb dolgozni.

183
00:14:01,780 --> 00:14:05,406
Az egyik nagy dolog az, hogy könnyebb kiszámítani, mi fog történni, 

184
00:14:05,406 --> 00:14:08,340
ha ezt a mátrixot egy csomószor megszorozzuk önmagával.

185
00:14:09,420 --> 00:14:14,617
Mivel az egyik ilyen mátrix csak annyit tesz, hogy minden egyes bázisvektort valamilyen 

186
00:14:14,617 --> 00:14:19,697
sajátértékkel skáláz, a mátrix többszöri, mondjuk 100-szoros alkalmazása minden egyes 

187
00:14:19,697 --> 00:14:24,600
bázisvektornak a megfelelő sajátérték 100. hatványával való skálázásának felel meg.

188
00:14:25,700 --> 00:14:29,680
Ezzel szemben próbálja meg kiszámítani egy nem diagonális mátrix 100. hatványát.

189
00:14:29,680 --> 00:14:31,320
Tényleg, próbáld ki egy pillanatra.

190
00:14:31,740 --> 00:14:32,440
Ez egy rémálom.

191
00:14:36,080 --> 00:14:38,497
Természetesen ritkán leszünk olyan szerencsések, 

192
00:14:38,497 --> 00:14:41,260
hogy az alapvektoraink egyben sajátvektorok is legyenek.

193
00:14:42,040 --> 00:14:46,260
De ha a transzformációdnak sok sajátvektora van, mint például a videó elején, 

194
00:14:46,260 --> 00:14:50,859
elég sok ahhoz, hogy kiválaszthass egy olyan halmazt, amely a teljes térre kiterjed, 

195
00:14:50,859 --> 00:14:53,780
akkor megváltoztathatod a koordinátarendszeredet úgy, 

196
00:14:53,780 --> 00:14:56,540
hogy ezek a sajátvektorok legyenek az alapvektorok.

197
00:14:57,140 --> 00:14:59,815
Az előző videóban már beszéltem az alap megváltoztatásáról, 

198
00:14:59,815 --> 00:15:02,357
de itt egy szuper gyors emlékeztetővel végigmegyek azon, 

199
00:15:02,357 --> 00:15:05,390
hogyan fejezhetünk ki egy jelenleg a koordinátarendszerünkben leírt 

200
00:15:05,390 --> 00:15:07,040
transzformációt egy másik rendszerbe.

201
00:15:08,440 --> 00:15:12,477
Vegyük azoknak a vektoroknak a koordinátáit, amelyeket új bázisként akarunk használni, 

202
00:15:12,477 --> 00:15:14,937
ami ebben az esetben a két sajátvektorunkat jelenti, 

203
00:15:14,937 --> 00:15:17,583
majd ezeket a koordinátákat tegyük egy mátrix oszlopává, 

204
00:15:17,583 --> 00:15:19,440
amelyet bázisváltási mátrixnak nevezünk.

205
00:15:20,180 --> 00:15:25,171
Ha az eredeti transzformációt szendvicsezzük, a bázisváltozási mátrixot a jobb oldalára, 

206
00:15:25,171 --> 00:15:28,536
a bázisváltozási mátrix inverzét pedig a bal oldalára téve, 

207
00:15:28,536 --> 00:15:33,191
az eredmény egy olyan mátrix lesz, amely ugyanazt a transzformációt reprezentálja, 

208
00:15:33,191 --> 00:15:36,500
de az új bázisvektorok koordinátarendszerének szemszögéből.

209
00:15:37,440 --> 00:15:41,851
A sajátvektorokkal való művelet lényege, hogy ez az új mátrix garantáltan 

210
00:15:41,851 --> 00:15:46,680
diagonális lesz, és a megfelelő sajátértékek a diagonális mentén helyezkednek el.

211
00:15:46,860 --> 00:15:50,042
Ez azért van így, mert olyan koordinátarendszerben dolgozik, 

212
00:15:50,042 --> 00:15:54,320
ahol az alapvektorokkal az történik, hogy azok a transzformáció során skálázódnak.

213
00:15:55,800 --> 00:15:59,255
Az olyan bázisvektorok halmazát, amelyek egyben sajátvektorok is, 

214
00:15:59,255 --> 00:16:01,560
ismét csak ésszerűen sajátbázisnak nevezzük.

215
00:16:02,340 --> 00:16:06,746
Tehát ha például ennek a mátrixnak a 100. hatványát kellene kiszámítani, 

216
00:16:06,746 --> 00:16:11,032
sokkal egyszerűbb lenne átváltani egy sajátbázisra, kiszámítani a 100. 

217
00:16:11,032 --> 00:16:15,680
hatványt abban a rendszerben, majd visszaváltani a mi standard rendszerünkre.

218
00:16:16,620 --> 00:16:18,320
Ezt nem teheti meg minden átalakítással.

219
00:16:18,320 --> 00:16:22,980
Egy nyírásnak például nincs elég sajátvektora ahhoz, hogy a teljes teret átfogja.

220
00:16:23,460 --> 00:16:28,160
De ha találsz egy saját bázist, akkor a mátrixműveletek nagyon szépek lesznek.

221
00:16:29,120 --> 00:16:32,062
Azok számára, akik hajlandóak egy elég szép rejtvényen keresztül dolgozni, 

222
00:16:32,062 --> 00:16:34,612
hogy lássák, hogyan néz ki ez a működésben, és hogyan lehet vele 

223
00:16:34,612 --> 00:16:37,320
meglepő eredményeket elérni, itt hagyok egy felszólítást a képernyőn.

224
00:16:37,600 --> 00:16:40,280
Egy kis munkát igényel, de szerintem élvezni fogod.

225
00:16:40,840 --> 00:16:46,120
A sorozat következő és egyben utolsó videója az absztrakt vektorterekről fog szólni.

