1
00:00:19,920 --> 00:00:22,956
Eigenvektoren und Eigenwerte gehören zu den Themen, 

2
00:00:22,956 --> 00:00:25,760
die für viele Schüler besonders unintuitiv sind.

3
00:00:25,760 --> 00:00:29,283
Fragen wie "Warum machen wir das?" und "Was bedeutet das eigentlich?" 

4
00:00:29,283 --> 00:00:33,260
bleiben allzu oft in einem Meer von unbeantworteten Berechnungen unbeantwortet.

5
00:00:33,920 --> 00:00:37,417
Und während ich die Videos dieser Serie veröffentlicht habe, haben viele von euch gesagt, 

6
00:00:37,417 --> 00:00:40,060
dass sie sich besonders auf die Visualisierung dieses Themas freuen.

7
00:00:40,680 --> 00:00:43,408
Ich vermute, dass der Grund dafür nicht so sehr darin liegt, 

8
00:00:43,408 --> 00:00:46,360
dass Eigenthings besonders kompliziert oder schlecht erklärt sind.

9
00:00:46,860 --> 00:00:49,554
Eigentlich ist es vergleichsweise einfach, und ich denke, 

10
00:00:49,554 --> 00:00:51,180
die meisten Bücher erklären es gut.

11
00:00:51,520 --> 00:00:54,273
Das Problem ist, dass es nur dann wirklich Sinn macht, 

12
00:00:54,273 --> 00:00:58,480
wenn du ein solides visuelles Verständnis für viele der vorangegangenen Themen hast.

13
00:00:59,060 --> 00:01:02,996
Das Wichtigste dabei ist, dass du Matrizen als lineare Transformationen 

14
00:01:02,996 --> 00:01:06,714
betrachten kannst, aber du musst auch mit Dingen wie Determinanten, 

15
00:01:06,714 --> 00:01:09,940
linearen Gleichungssystemen und Basiswechsel vertraut sein.

16
00:01:10,720 --> 00:01:15,033
Die Verwirrung über die Eigenwerte hat meist mehr mit einem wackeligen Fundament 

17
00:01:15,033 --> 00:01:19,240
in einem dieser Themen zu tun als mit den Eigenvektoren und Eigenwerten selbst.

18
00:01:19,980 --> 00:01:23,495
Betrachte zunächst eine lineare Transformation in zwei Dimensionen, 

19
00:01:23,495 --> 00:01:24,840
wie sie hier gezeigt wird.

20
00:01:25,460 --> 00:01:31,040
Er verschiebt den Basisvektor i-hat auf die Koordinaten 3, 0 und j-hat auf 1, 2.

21
00:01:31,780 --> 00:01:35,640
Sie wird also mit einer Matrix dargestellt, deren Spalten 3, 0 und 1, 2 sind.

22
00:01:36,600 --> 00:01:39,424
Konzentriere dich darauf, was es mit einem bestimmten Vektor macht, 

23
00:01:39,424 --> 00:01:42,000
und denke über die Spannweite dieses Vektors nach, die Linie, 

24
00:01:42,000 --> 00:01:44,160
die durch seinen Ursprung und seine Spitze verläuft.

25
00:01:44,920 --> 00:01:48,380
Die meisten Vektoren werden bei der Umwandlung aus ihrer Spanne gerissen.

26
00:01:48,780 --> 00:01:51,554
Ich meine, es wäre ein ziemlicher Zufall, wenn der Ort, 

27
00:01:51,554 --> 00:01:55,320
an dem der Vektor gelandet ist, auch irgendwo auf dieser Linie liegen würde.

28
00:01:57,400 --> 00:02:00,825
Einige spezielle Vektoren bleiben jedoch in ihrer eigenen Spannweite, 

29
00:02:00,825 --> 00:02:04,642
d.h. die Wirkung der Matrix auf einen solchen Vektor besteht lediglich darin, 

30
00:02:04,642 --> 00:02:07,040
ihn zu strecken oder zu stauchen, wie ein Skalar.

31
00:02:09,460 --> 00:02:14,100
Für dieses spezielle Beispiel ist der Basisvektor i-hat ein solcher spezieller Vektor.

32
00:02:14,640 --> 00:02:17,800
Die Spannweite von i-hat ist die x-Achse, und aus der ersten 

33
00:02:17,800 --> 00:02:21,011
Spalte der Matrix können wir ersehen, dass sich i-hat bis zum 

34
00:02:21,011 --> 00:02:24,120
Dreifachen seiner selbst bewegt, immer noch auf der x-Achse.

35
00:02:26,320 --> 00:02:31,734
Außerdem wird jeder andere Vektor auf der x-Achse aufgrund der linearen Transformationen 

36
00:02:31,734 --> 00:02:36,480
einfach um den Faktor 3 gestreckt und bleibt somit auf seiner eigenen Strecke.

37
00:02:38,500 --> 00:02:41,561
Ein etwas raffinierterer Vektor, der bei dieser Transformation 

38
00:02:41,561 --> 00:02:44,040
auf seiner eigenen Spanne bleibt, ist negativ 1, 1.

39
00:02:44,660 --> 00:02:47,140
Am Ende wird er um den Faktor 2 gestreckt.

40
00:02:49,000 --> 00:02:54,115
Und wieder bedeutet die Linearität, dass jeder andere Vektor auf der Diagonalen, 

41
00:02:54,115 --> 00:02:58,220
die dieser Typ aufspannt, einfach um den Faktor 2 gestreckt wird.

42
00:02:59,820 --> 00:03:02,148
Und für diese Transformation sind das alle Vektoren, 

43
00:03:02,148 --> 00:03:05,180
die die besondere Eigenschaft haben, auf ihrer Spannweite zu bleiben.

44
00:03:05,620 --> 00:03:08,772
Die auf der X-Achse werden um den Faktor 3 gestreckt und 

45
00:03:08,772 --> 00:03:11,980
die auf dieser diagonalen Linie um den Faktor 2 gestreckt.

46
00:03:12,760 --> 00:03:16,624
Jeder andere Vektor wird während der Transformation etwas gedreht und von der Linie, 

47
00:03:16,624 --> 00:03:18,080
die er überspannt, abgeschlagen.

48
00:03:22,520 --> 00:03:27,017
Wie du dir vielleicht schon gedacht hast, heißen diese speziellen Vektoren die 

49
00:03:27,017 --> 00:03:31,914
Eigenvektoren der Transformation, und jedem Eigenvektor ist ein sogenannter Eigenwert 

50
00:03:31,914 --> 00:03:36,525
zugeordnet, also der Faktor, um den er während der Transformation gestreckt oder 

51
00:03:36,525 --> 00:03:37,380
gestaucht wird.

52
00:03:40,280 --> 00:03:43,704
Natürlich ist es nichts Besonderes, dass diese Eigenwerte positiv sind, 

53
00:03:43,704 --> 00:03:45,940
oder dass sie gestreckt oder gequetscht werden.

54
00:03:46,380 --> 00:03:50,774
In einem anderen Beispiel könntest du einen Eigenvektor mit dem Eigenwert negativ 1 halb 

55
00:03:50,774 --> 00:03:55,120
haben, was bedeutet, dass der Vektor um den Faktor 1 halb gespiegelt und quadriert wird.

56
00:03:56,980 --> 00:03:59,677
Wichtig dabei ist, dass er auf der Linie bleibt, 

57
00:03:59,677 --> 00:04:02,760
die er überspannt, ohne dass er von ihr weggedreht wird.

58
00:04:04,460 --> 00:04:07,253
Um einen Eindruck davon zu bekommen, warum es sinnvoll sein könnte, 

59
00:04:07,253 --> 00:04:09,800
darüber nachzudenken, betrachte eine dreidimensionale Drehung.

60
00:04:11,660 --> 00:04:16,226
Wenn du einen Eigenvektor für diese Drehung finden kannst, also einen Vektor, 

61
00:04:16,226 --> 00:04:20,500
der auf seiner eigenen Spannweite bleibt, hast du die Drehachse gefunden.

62
00:04:22,600 --> 00:04:26,884
Und es ist viel einfacher, über eine 3D-Drehung in Form einer Drehachse 

63
00:04:26,884 --> 00:04:30,157
und eines Winkels nachzudenken, um den sie sich dreht, 

64
00:04:30,157 --> 00:04:34,740
als über die gesamte 3x3-Matrix, die mit dieser Transformation verbunden ist.

65
00:04:37,000 --> 00:04:40,757
In diesem Fall müsste der entsprechende Eigenwert übrigens 1 sein, 

66
00:04:40,757 --> 00:04:43,448
da Rotationen nie etwas strecken oder stauchen, 

67
00:04:43,448 --> 00:04:45,860
sodass die Länge des Vektors gleich bleibt.

68
00:04:48,080 --> 00:04:50,020
Dieses Muster taucht häufig in der linearen Algebra auf.

69
00:04:50,440 --> 00:04:54,072
Bei jeder linearen Transformation, die durch eine Matrix beschrieben wird, 

70
00:04:54,072 --> 00:04:57,026
kannst du verstehen, was sie tut, wenn du die Spalten dieser 

71
00:04:57,026 --> 00:04:59,400
Matrix als Landeplätze für Basisvektoren abliest.

72
00:05:00,020 --> 00:05:03,654
Oft ist es aber besser, die Eigenvektoren und Eigenwerte zu ermitteln, 

73
00:05:03,654 --> 00:05:07,288
um herauszufinden, was die lineare Transformation tatsächlich bewirkt, 

74
00:05:07,288 --> 00:05:10,820
und weniger von deinem speziellen Koordinatensystem abhängig zu sein.

75
00:05:15,460 --> 00:05:19,036
Ich werde hier nicht auf alle Details der Methoden zur Berechnung von Eigenvektoren 

76
00:05:19,036 --> 00:05:21,250
und Eigenwerten eingehen, aber ich werde versuchen, 

77
00:05:21,250 --> 00:05:23,465
einen Überblick über die Berechnungsideen zu geben, 

78
00:05:23,465 --> 00:05:26,020
die für ein konzeptionelles Verständnis am wichtigsten sind.

79
00:05:27,180 --> 00:05:30,480
Symbolisch sieht die Idee eines Eigenvektors folgendermaßen aus.

80
00:05:31,040 --> 00:05:35,831
A ist die Matrix, die eine Transformation darstellt, mit v als Eigenvektor, 

81
00:05:35,831 --> 00:05:39,740
und lambda ist eine Zahl, nämlich der entsprechende Eigenwert.

82
00:05:40,680 --> 00:05:44,648
Dieser Ausdruck besagt, dass das Matrix-Vektor-Produkt, A mal v, 

83
00:05:44,648 --> 00:05:49,900
dasselbe Ergebnis liefert wie die Skalierung des Eigenvektors v mit einem Wert lambda.

84
00:05:51,000 --> 00:05:55,119
Um die Eigenvektoren und ihre Eigenwerte einer Matrix A zu finden, 

85
00:05:55,119 --> 00:06:00,100
musst du also die Werte von v und lambda finden, die diesen Ausdruck wahr machen.

86
00:06:01,920 --> 00:06:06,266
Die linke Seite steht für die Matrix-Vektor-Multiplikation, 

87
00:06:06,266 --> 00:06:10,540
aber die rechte Seite ist die Skalar-Vektor-Multiplikation.

88
00:06:11,120 --> 00:06:13,944
Beginnen wir also damit, die rechte Seite als eine Art 

89
00:06:13,944 --> 00:06:17,898
Matrix-Vektor-Multiplikation umzuschreiben, indem wir eine Matrix verwenden, 

90
00:06:17,898 --> 00:06:20,620
die jeden Vektor um einen Faktor von Lambda skaliert.

91
00:06:21,680 --> 00:06:26,128
Die Spalten einer solchen Matrix stellen dar, was mit den einzelnen Basisvektoren 

92
00:06:26,128 --> 00:06:29,763
passiert. Jeder Basisvektor wird einfach mit Lambda multipliziert, 

93
00:06:29,763 --> 00:06:34,320
sodass diese Matrix auf der Diagonalen die Zahl Lambda und überall sonst Nullen hat.

94
00:06:36,180 --> 00:06:38,484
Die übliche Art, diesen Typ zu schreiben, ist, 

95
00:06:38,484 --> 00:06:41,966
das Lambda herauszufaktorisieren und es als Lambda mal i zu schreiben, 

96
00:06:41,966 --> 00:06:44,860
wobei i die Identitätsmatrix mit 1en auf der Diagonale ist.

97
00:06:45,860 --> 00:06:48,836
Da beide Seiten wie eine Matrix-Vektor-Multiplikation aussehen, 

98
00:06:48,836 --> 00:06:51,860
können wir die rechte Seite subtrahieren und das v herausrechnen.

99
00:06:54,160 --> 00:06:59,100
Wir haben jetzt also eine neue Matrix, A minus Lambda mal die Identität, 

100
00:06:59,100 --> 00:07:04,920
und suchen nach einem Vektor v, bei dem diese neue Matrix mal v den Nullvektor ergibt.

101
00:07:06,380 --> 00:07:11,100
Das ist immer dann der Fall, wenn v selbst der Nullvektor ist, aber das ist langweilig.

102
00:07:11,340 --> 00:07:13,640
Was wir wollen, ist ein Eigenvektor ungleich Null.

103
00:07:14,420 --> 00:07:17,591
Und wenn du dir Kapitel 5 und 6 ansiehst, wirst du wissen, 

104
00:07:17,591 --> 00:07:22,160
dass das Produkt einer Matrix mit einem Nicht-Null-Vektor nur dann Null werden kann, 

105
00:07:22,160 --> 00:07:25,493
wenn die Transformation, die mit dieser Matrix verbunden ist, 

106
00:07:25,493 --> 00:07:28,020
den Raum in eine niedrigere Dimension quetscht.

107
00:07:29,300 --> 00:07:34,220
Und diese Squishification entspricht einer Null-Determinante für die Matrix.

108
00:07:35,480 --> 00:07:40,306
Nehmen wir an, deine Matrix A hat die Spalten 2, 1 und 2, 3. Überlege dir, 

109
00:07:40,306 --> 00:07:45,520
wie du einen variablen Betrag, Lambda, von jedem Diagonaleintrag abziehen kannst.

110
00:07:46,480 --> 00:07:50,280
Jetzt stell dir vor, du drehst an einem Knopf, um den Wert von Lambda zu ändern.

111
00:07:50,940 --> 00:07:54,144
Wenn sich der Wert von Lambda ändert, ändert sich auch die 

112
00:07:54,144 --> 00:07:57,240
Matrix selbst und damit auch die Determinante der Matrix.

113
00:07:58,220 --> 00:08:02,418
Das Ziel ist es, einen Lambda-Wert zu finden, bei dem die Determinante Null ist, 

114
00:08:02,418 --> 00:08:06,877
was bedeutet, dass die geänderte Transformation den Raum in eine niedrigere Dimension 

115
00:08:06,877 --> 00:08:07,240
drückt.

116
00:08:08,160 --> 00:08:11,160
In diesem Fall ist der Sweet Spot erreicht, wenn lambda gleich 1 ist.

117
00:08:12,180 --> 00:08:14,128
Hätten wir eine andere Matrix gewählt, müsste 

118
00:08:14,128 --> 00:08:16,120
der Eigenwert natürlich nicht unbedingt 1 sein.

119
00:08:16,240 --> 00:08:18,600
Der Sweet Spot könnte auch bei einem anderen Lambda-Wert erreicht werden.

120
00:08:20,080 --> 00:08:22,960
Das ist ganz schön viel, aber lass uns enträtseln, was das bedeutet.

121
00:08:22,960 --> 00:08:26,035
Wenn lambda gleich 1 ist, quetscht die Matrix A 

122
00:08:26,035 --> 00:08:29,560
minus lambda mal die Identität den Raum auf eine Linie.

123
00:08:30,440 --> 00:08:33,865
Das bedeutet, dass es einen Nicht-Null-Vektor v gibt, 

124
00:08:33,865 --> 00:08:38,559
so dass A minus Lambda mal die Identität mal v gleich dem Null-Vektor ist.

125
00:08:40,480 --> 00:08:45,130
Der Grund, warum uns das interessiert, ist, dass es bedeutet, 

126
00:08:45,130 --> 00:08:49,030
dass A mal v gleich Lambda mal v ist, was bedeutet, 

127
00:08:49,030 --> 00:08:54,655
dass der Vektor v ein Eigenvektor von A ist und während der Transformation 

128
00:08:54,655 --> 00:08:57,280
A auf seiner eigenen Spanne bleibt.

129
00:08:58,320 --> 00:09:01,251
In diesem Beispiel ist der entsprechende Eigenwert 1, 

130
00:09:01,251 --> 00:09:04,020
so dass v eigentlich an seinem Platz bleiben würde.

131
00:09:06,220 --> 00:09:08,034
Halte inne und überlege, ob du sicherstellen musst, 

132
00:09:08,034 --> 00:09:09,500
dass sich diese Argumentation gut anfühlt.

133
00:09:13,380 --> 00:09:15,640
Das ist die Art von Dingen, die ich in der Einleitung erwähnt habe.

134
00:09:16,220 --> 00:09:19,690
Wenn du kein solides Verständnis von Determinanten hättest und wüsstest, 

135
00:09:19,690 --> 00:09:23,589
warum sie sich auf lineare Gleichungssysteme mit Lösungen ungleich Null beziehen, 

136
00:09:23,589 --> 00:09:26,300
würde dir ein Ausdruck wie dieser völlig fremd vorkommen.

137
00:09:28,320 --> 00:09:32,002
Um dies in Aktion zu sehen, lass uns das Beispiel vom Anfang wiederholen, 

138
00:09:32,002 --> 00:09:34,540
mit einer Matrix, deren Spalten 3, 0 und 1, 2 sind.

139
00:09:35,350 --> 00:09:38,651
Um herauszufinden, ob ein Wert lambda ein Eigenwert ist, 

140
00:09:38,651 --> 00:09:43,400
ziehst du ihn von den Diagonalen dieser Matrix ab und berechnest die Determinante.

141
00:09:50,580 --> 00:09:54,839
Auf diese Weise erhalten wir ein bestimmtes quadratisches Polynom in Lambda, 

142
00:09:54,839 --> 00:09:56,720
3 minus Lambda mal 2 minus Lambda.

143
00:09:57,800 --> 00:10:02,429
Da lambda nur dann ein Eigenwert sein kann, wenn diese Determinante Null ist, 

144
00:10:02,429 --> 00:10:06,406
kannst du daraus schließen, dass die einzigen möglichen Eigenwerte 

145
00:10:06,406 --> 00:10:08,840
lambda gleich 2 und lambda gleich 3 sind.

146
00:10:09,640 --> 00:10:14,698
Um herauszufinden, welche Eigenvektoren tatsächlich einen dieser Eigenwerte haben, 

147
00:10:14,698 --> 00:10:20,060
z. B. Lambda gleich 2, fügst du diesen Wert von Lambda in die Matrix ein und löst dann, 

148
00:10:20,060 --> 00:10:23,900
welche Vektoren diese diagonal veränderte Matrix zu Null macht.

149
00:10:24,940 --> 00:10:28,498
Wenn du dieses System wie jedes andere lineare System berechnen würdest, 

150
00:10:28,498 --> 00:10:32,155
würdest du sehen, dass die Lösungen alle Vektoren auf der Diagonalen sind, 

151
00:10:32,155 --> 00:10:34,300
die von der negativen 1, 1 aufgespannt wird.

152
00:10:35,220 --> 00:10:39,741
Dies entspricht der Tatsache, dass die unveränderte Matrix 3, 

153
00:10:39,741 --> 00:10:43,460
0, 1, 2 all diese Vektoren um den Faktor 2 streckt.

154
00:10:46,320 --> 00:10:50,200
Eine 2D-Transformation muss also keine Eigenvektoren haben.

155
00:10:50,720 --> 00:10:53,400
Betrachte zum Beispiel eine Drehung um 90 Grad.

156
00:10:53,660 --> 00:10:58,200
Diese hat keine Eigenvektoren, da sie jeden Vektor aus seiner eigenen Spanne herausdreht.

157
00:11:00,800 --> 00:11:04,344
Wenn du versuchst, die Eigenwerte einer solchen Drehung zu berechnen, 

158
00:11:04,344 --> 00:11:05,560
merkst du, was passiert.

159
00:11:06,300 --> 00:11:10,140
Seine Matrix hat die Spalten 0, 1 und negativ 1, 0.

160
00:11:11,100 --> 00:11:15,800
Subtrahiere lambda von den Diagonalelementen und suche, wann die Determinante Null ist.

161
00:11:18,140 --> 00:11:21,940
In diesem Fall erhältst du das Polynom lambda Quadrat plus 1.

162
00:11:22,680 --> 00:11:27,920
Die einzigen Wurzeln dieses Polynoms sind die imaginären Zahlen, i und negativ i.

163
00:11:28,840 --> 00:11:31,650
Die Tatsache, dass es keine Lösungen mit reellen Zahlen gibt, 

164
00:11:31,650 --> 00:11:33,600
bedeutet, dass es keine Eigenvektoren gibt.

165
00:11:35,540 --> 00:11:39,083
Ein weiteres interessantes Beispiel, das du im Hinterkopf behalten solltest, 

166
00:11:39,083 --> 00:11:39,820
ist eine Schere.

167
00:11:40,560 --> 00:11:44,806
Dadurch wird i-hat an seinem Platz fixiert und j-hat um 1 verschoben, 

168
00:11:44,806 --> 00:11:47,840
sodass seine Matrix die Spalten 1, 0 und 1, 1 hat.

169
00:11:48,740 --> 00:11:52,800
Alle Vektoren auf der x-Achse sind Eigenvektoren mit dem Eigenwert 1, 

170
00:11:52,800 --> 00:11:54,540
da sie an ihrem Platz bleiben.

171
00:11:55,680 --> 00:11:57,820
Tatsächlich sind dies die einzigen Eigenvektoren.

172
00:11:58,760 --> 00:12:03,901
Wenn du lambda von den Diagonalen abziehst und die Determinante berechnest, 

173
00:12:03,901 --> 00:12:06,540
erhältst du 1 minus lambda zum Quadrat.

174
00:12:09,320 --> 00:12:12,860
Und die einzige Wurzel dieses Ausdrucks ist lambda gleich 1.

175
00:12:14,560 --> 00:12:16,988
Dies entspricht dem, was wir geometrisch sehen, 

176
00:12:16,988 --> 00:12:19,720
nämlich dass alle Eigenvektoren den Eigenwert 1 haben.

177
00:12:21,080 --> 00:12:25,128
Bedenke aber, dass es auch möglich ist, nur einen Eigenwert zu haben, 

178
00:12:25,128 --> 00:12:28,020
aber mehr als nur eine Linie voller Eigenvektoren.

179
00:12:29,900 --> 00:12:33,180
Ein einfaches Beispiel ist eine Matrix, die alles mit 2 skaliert.

180
00:12:33,900 --> 00:12:37,331
Der einzige Eigenwert ist 2, aber jeder Vektor in der 

181
00:12:37,331 --> 00:12:40,700
Ebene wird zu einem Eigenvektor mit diesem Eigenwert.

182
00:12:42,000 --> 00:12:45,480
Jetzt ist ein weiterer guter Zeitpunkt, um innezuhalten und über einiges nachzudenken, 

183
00:12:45,480 --> 00:12:46,960
bevor ich zum letzten Thema übergehe.

184
00:13:03,540 --> 00:13:06,737
Ich möchte hier mit der Idee einer Eigenbasis abschließen, 

185
00:13:06,737 --> 00:13:09,880
die sich stark auf die Ideen aus dem letzten Video stützt.

186
00:13:11,480 --> 00:13:16,380
Sieh dir an, was passiert, wenn unsere Basisvektoren zufällig Eigenvektoren sind.

187
00:13:17,120 --> 00:13:19,943
Zum Beispiel könnte i-hat eine negative Skalierung 

188
00:13:19,943 --> 00:13:22,380
von 1 und j-hat eine Skalierung von 2 haben.

189
00:13:23,420 --> 00:13:27,726
Wenn du die neuen Koordinaten als Spalten einer Matrix schreibst, siehst du, 

190
00:13:27,726 --> 00:13:30,411
dass die skalaren Vielfachen, negative 1 und 2, 

191
00:13:30,411 --> 00:13:35,166
die die Eigenwerte von i-hat und j-hat sind, auf der Diagonale unserer Matrix stehen 

192
00:13:35,166 --> 00:13:37,180
und jeder andere Eintrag eine 0 ist.

193
00:13:38,880 --> 00:13:42,462
Wenn eine Matrix überall Nullen hat, außer auf der Diagonalen, 

194
00:13:42,462 --> 00:13:45,420
nennt man sie vernünftigerweise eine Diagonalmatrix.

195
00:13:45,840 --> 00:13:50,010
Das bedeutet, dass alle Basisvektoren Eigenvektoren sind 

196
00:13:50,010 --> 00:13:54,400
und die Diagonaleinträge dieser Matrix ihre Eigenwerte sind.

197
00:13:57,100 --> 00:14:01,060
Es gibt eine Menge Dinge, die die Arbeit mit diagonalen Matrizen viel angenehmer machen.

198
00:14:01,780 --> 00:14:05,085
Eine davon ist, dass es einfacher zu berechnen ist, was passiert, 

199
00:14:05,085 --> 00:14:08,340
wenn du diese Matrix ein paar Mal mit sich selbst multiplizierst.

200
00:14:09,420 --> 00:14:14,435
Da eine dieser Matrizen jeden Basisvektor nur um einen Eigenwert skaliert, 

201
00:14:14,435 --> 00:14:18,848
entspricht die Anwendung dieser Matrix viele Male, z. B. 100 Mal, 

202
00:14:18,848 --> 00:14:24,600
der Skalierung jedes Basisvektors um die 100-te Potenz des entsprechenden Eigenwertes.

203
00:14:25,700 --> 00:14:29,680
Versuche dagegen, die 100. Potenz einer nicht diagonalen Matrix zu berechnen.

204
00:14:29,680 --> 00:14:31,320
Probiere es doch mal aus.

205
00:14:31,740 --> 00:14:32,440
Es ist ein Alptraum.

206
00:14:36,080 --> 00:14:38,782
Natürlich wirst du selten das Glück haben, dass 

207
00:14:38,782 --> 00:14:41,260
deine Basisvektoren auch Eigenvektoren sind.

208
00:14:42,040 --> 00:14:47,059
Wenn deine Transformation aber viele Eigenvektoren hat, wie die vom Anfang dieses Videos, 

209
00:14:47,059 --> 00:14:51,018
so dass du eine Menge auswählen kannst, die den gesamten Raum abdeckt, 

210
00:14:51,018 --> 00:14:53,751
dann kannst du dein Koordinatensystem so ändern, 

211
00:14:53,751 --> 00:14:56,540
dass diese Eigenvektoren deine Basisvektoren sind.

212
00:14:57,140 --> 00:14:59,974
Im letzten Video habe ich über den Wechsel der Basis gesprochen, 

213
00:14:59,974 --> 00:15:03,289
aber ich werde hier noch einmal kurz erklären, wie man eine Transformation, 

214
00:15:03,289 --> 00:15:07,040
die in unserem Koordinatensystem geschrieben wurde, in einem anderen System ausdrückt.

215
00:15:08,440 --> 00:15:12,224
Nimm die Koordinaten der Vektoren, die du als neue Basis verwenden willst, 

216
00:15:12,224 --> 00:15:14,696
also in diesem Fall unsere beiden Eigenvektoren, 

217
00:15:14,696 --> 00:15:17,573
und mache diese Koordinaten zu den Spalten einer Matrix, 

218
00:15:17,573 --> 00:15:19,440
der sogenannten Basisänderungsmatrix.

219
00:15:20,180 --> 00:15:23,736
Wenn du die ursprüngliche Umwandlung in eine Sandwich-Matrix umwandelst, 

220
00:15:23,736 --> 00:15:27,487
indem du die Basisänderungsmatrix auf die rechte Seite und die Umkehrung der 

221
00:15:27,487 --> 00:15:31,287
Basisänderungsmatrix auf die linke Seite legst, ist das Ergebnis eine Matrix, 

222
00:15:31,287 --> 00:15:35,330
die dieselbe Umwandlung darstellt, aber aus der Perspektive des Koordinatensystems 

223
00:15:35,330 --> 00:15:36,500
der neuen Basisvektoren.

224
00:15:37,440 --> 00:15:40,297
Der Sinn dieser Methode mit Eigenvektoren ist, 

225
00:15:40,297 --> 00:15:45,099
dass die neue Matrix garantiert diagonal ist und die entsprechenden Eigenwerte 

226
00:15:45,099 --> 00:15:46,680
auf der Diagonalen liegen.

227
00:15:46,860 --> 00:15:50,590
Das liegt daran, dass es sich um ein Koordinatensystem handelt, 

228
00:15:50,590 --> 00:15:54,320
in dem die Basisvektoren bei der Transformation skaliert werden.

229
00:15:55,800 --> 00:15:59,099
Eine Menge von Basisvektoren, die auch Eigenvektoren sind, 

230
00:15:59,099 --> 00:16:01,560
nennt man vernünftigerweise eine Eigenbasis.

231
00:16:02,340 --> 00:16:06,767
Wenn du also zum Beispiel die 100. Potenz dieser Matrix berechnen müsstest, 

232
00:16:06,767 --> 00:16:10,611
wäre es viel einfacher, zu einer Eigenbasis zu wechseln, die 100. 

233
00:16:10,611 --> 00:16:15,680
Potenz in diesem System zu berechnen und dann zu unserem Standardsystem zurückzukehren.

234
00:16:16,620 --> 00:16:18,320
Du kannst das nicht mit allen Transformationen machen.

235
00:16:18,320 --> 00:16:21,290
Eine Scherung zum Beispiel hat nicht genug Eigenvektoren, 

236
00:16:21,290 --> 00:16:22,980
um den gesamten Raum zu erfassen.

237
00:16:23,460 --> 00:16:28,160
Aber wenn du eine Eigenbasis finden kannst, sind Matrixoperationen wirklich schön.

238
00:16:29,120 --> 00:16:32,151
Für diejenigen unter euch, die bereit sind, sich durch ein hübsches Rätsel zu arbeiten, 

239
00:16:32,151 --> 00:16:35,011
um zu sehen, wie das in Aktion aussieht und wie man damit überraschende Ergebnisse 

240
00:16:35,011 --> 00:16:37,320
erzielen kann, lasse ich hier eine Aufforderung auf dem Bildschirm.

241
00:16:37,600 --> 00:16:40,280
Es ist ein bisschen Arbeit, aber ich glaube, es wird dir Spaß machen.

242
00:16:40,840 --> 00:16:43,508
Das nächste und letzte Video dieser Reihe wird 

243
00:16:43,508 --> 00:16:46,120
sich mit abstrakten Vektorräumen beschäftigen.

