[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Eigenvektoren und Eigenwerte gehören zu den Themen, die für viele Schüler besonders unintuitiv sind.",
  "model": "DeepL",
  "from_community_srt": "Wenn ich einen Vektor im zweidimensionalen Raum habe, haben wir einen Standard Weg, um diesen mit Koordinaten zu beschreiben. In diesem Fall hat der Vektor die Koordinaten [3,2] Das bedeutet von seinem hinteren Teil, zu seiner Spitze gehen wir drei Einheiten nach rechts und zwei nach oben.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Fragen wie \"Warum machen wir das?\" und \"Was bedeutet das eigentlich?\" bleiben allzu oft in einem Meer von unbeantworteten Berechnungen unbeantwortet.",
  "model": "DeepL",
  "from_community_srt": "Der Weg der linearen Algebra um Koordinaten zu beschreiben ist, sich jede dieser Zahlen als Skalar vorzustellen.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "Und während ich die Videos dieser Serie veröffentlicht habe, haben viele von euch gesagt, dass sie sich besonders auf die Visualisierung dieses Themas freuen.",
  "model": "DeepL",
  "from_community_srt": "Ein Ding, das Vektoren in die Länge zieht oder zusammendrückt. Du stellst dir die erste Koordinate vor als Skalierung von i-Hut, dem Vektor mit Länge 1,",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Ich vermute, dass der Grund dafür nicht so sehr darin liegt, dass Eigenthings besonders kompliziert oder schlecht erklärt sind.",
  "model": "DeepL",
  "from_community_srt": "der nach rechts zeigt, während die zweite Koordinate j-Hut skaliert,",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "Eigentlich ist es vergleichsweise einfach, und ich denke, die meisten Bücher erklären es gut.",
  "model": "DeepL",
  "from_community_srt": "den Vektor mit Länge 1, der nach oben zeigt. Die Summe dieser zwei Vektoren vom hinteren Teil,",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Das Problem ist, dass es nur dann wirklich Sinn macht, wenn du ein solides visuelles Verständnis für viele der vorangegangenen Themen hast.",
  "model": "DeepL",
  "from_community_srt": "bis zur Spitze ist, was diese Koordinaten beschreiben sollen.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Das Wichtigste dabei ist, dass du Matrizen als lineare Transformationen betrachten kannst, aber du musst auch mit Dingen wie Determinanten, linearen Gleichungssystemen und Basiswechsel vertraut sein.",
  "model": "DeepL",
  "from_community_srt": "Du kannst dir diese zwei speziellen Vektoren vorstellen, indem du alle impliziten Annahmen unseres Koordinatensystems einkapselst. Der Fakt, dass die erste Zahl eine Bewegung nach rechts impliziert und die zweite eine Bewegung nach oben impliziert, mit exakter Länge dieser Distanz.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Die Verwirrung über die Eigenwerte hat meist mehr mit einem wackeligen Fundament in einem dieser Themen zu tun als mit den Eigenvektoren und Eigenwerten selbst.",
  "model": "DeepL",
  "from_community_srt": "All dies liegt in der Wahl von i-Hut und j-Hut als die Vektoren, die Skalar Koordinaten eigentlich skalieren sollen. Jedenfalls,",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Betrachte zunächst eine lineare Transformation in zwei Dimensionen, wie sie hier gezeigt wird.",
  "model": "DeepL",
  "from_community_srt": "um zwischen Vektoren und einer Ansammlung von Zahlen zu übersetzen wird dies ein Koordinatensystem genannt",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Er verschiebt den Basisvektor i-hat auf die Koordinaten 3, 0 und j-hat auf 1, 2.",
  "model": "DeepL",
  "from_community_srt": "und die zwei speziellen Vektoren i-Hut und j-Hut sind die sogenannten Basisvektoren unseres Standard Koordinatensystems",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Sie wird also mit einer Matrix dargestellt, deren Spalten 3, 0 und 1, 2 sind.",
  "model": "DeepL",
  "from_community_srt": "Worüber ich hierbei gerne sprechen möchte, ist die Idee unterschiedliche Systeme von Basisvektoren zu nutzen.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Konzentriere dich darauf, was es mit einem bestimmten Vektor macht, und denke über die Spannweite dieses Vektors nach, die Linie, die durch seinen Ursprung und seine Spitze verläuft.",
  "model": "DeepL",
  "from_community_srt": "Zum Beispiel, lass uns sagen du hast eine Freundin, Jennifer, diese nutzt ein anderes System von Basisvektoren, die ich b1 und b2 nennen werde.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "Die meisten Vektoren werden bei der Umwandlung aus ihrer Spanne gerissen.",
  "model": "DeepL",
  "from_community_srt": "Ihr erster Basisvektor b1 zeigt ein bisschen nach oben rechts und ihr zweiter Basisvektor b2 zeigt nach oben links.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Ich meine, es wäre ein ziemlicher Zufall, wenn der Ort, an dem der Vektor gelandet ist, auch irgendwo auf dieser Linie liegen würde.",
  "model": "DeepL",
  "from_community_srt": "Schau dir nun nochmal den Vektor an, den ich vorhin gezeigt habe. Den,",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Einige spezielle Vektoren bleiben jedoch in ihrer eigenen Spannweite, d.h. die Wirkung der Matrix auf einen solchen Vektor besteht lediglich darin, ihn zu strecken oder zu stauchen, wie ein Skalar.",
  "model": "DeepL",
  "from_community_srt": "den du und ich mit den Koordinaten [3 , 2] beschreiben würden, wenn wir unsere Basisvektoren i-Hut und j-Hut nutzen. Jennifer würde diesen Vektor mit den Koordinaten  [5/3, 1/3] beschreiben. Das bedeutet,",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Für dieses spezielle Beispiel ist der Basisvektor i-hat ein solcher spezieller Vektor.",
  "model": "DeepL",
  "from_community_srt": "dass der Weg, um zu diesem Vektor zu kommen, mit ihren zwei Basisvektoren ist,",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "Die Spannweite von i-hat ist die x-Achse, und aus der ersten Spalte der Matrix können wir ersehen, dass sich i-hat bis zum Dreifachen seiner selbst bewegt, immer noch auf der x-Achse.",
  "model": "DeepL",
  "from_community_srt": "b1 mit 5/3 und b2 mit 1/3 zu skalieren und sie dann zu addieren. Ich zeige euch später,",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Außerdem wird jeder andere Vektor auf der x-Achse aufgrund der linearen Transformationen einfach um den Faktor 3 gestreckt und bleibt somit auf seiner eigenen Strecke.",
  "model": "DeepL",
  "from_community_srt": "wie ihr die beiden Zahlen 5/3 und 1/3 herausfinden könnt. Im Allgemeinen, wenn Jennifer ihr Koordinatensystem nutzt, um einen Vektor zu beschreiben, stellt sie sich die erste Koordinate als Skalierung von b1 und die zweite Koordinate als Skalierung von b2 vor und addiert die Ergebnisse.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Ein etwas raffinierterer Vektor, der bei dieser Transformation auf seiner eigenen Spanne bleibt, ist negativ 1, 1.",
  "model": "DeepL",
  "from_community_srt": "Was sie herausbekommt wird normalerweise komplett anders sein, als der Vektor den du und ich herausbekommen, wenn wir uns diese Koordinaten vorstellen.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Am Ende wird er um den Faktor 2 gestreckt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "Und wieder bedeutet die Linearität, dass jeder andere Vektor auf der Diagonalen, die dieser Typ aufspannt, einfach um den Faktor 2 gestreckt wird.",
  "model": "DeepL",
  "from_community_srt": "Um ein bisschen präziser zu sein Ihren erster Basisvektor b1 würden wir mit den Koordinaten [2, 1] beschreiben. Und ihren zweiten Basisvektor b2 würden wir beschreiben als  [-1,",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "Und für diese Transformation sind das alle Vektoren, die die besondere Eigenschaft haben, auf ihrer Spannweite zu bleiben.",
  "model": "DeepL",
  "from_community_srt": "1]. Aber es ist wichtig zu verstehen, dass aus Sicht ihres Systems, diese Vektoren die Koordinaten [1,",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Die auf der X-Achse werden um den Faktor 3 gestreckt und die auf dieser diagonalen Linie um den Faktor 2 gestreckt.",
  "model": "DeepL",
  "from_community_srt": "0] und [0, 1] haben. SIe beschreiben in ihrer Welt die Bedeutung von [1, 0] und [0,",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Jeder andere Vektor wird während der Transformation etwas gedreht und von der Linie, die er überspannt, abgeschlagen.",
  "model": "DeepL",
  "from_community_srt": "1] Also sprechen wir gewissermaßen unterschiedliche Sprachen Wir betrachten alle die selben Vektoren im Raum,",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Wie du dir vielleicht schon gedacht hast, heißen diese speziellen Vektoren die Eigenvektoren der Transformation, und jedem Eigenvektor ist ein sogenannter Eigenwert zugeordnet, also der Faktor, um den er während der Transformation gestreckt oder gestaucht wird.",
  "model": "DeepL",
  "from_community_srt": "aber Jennifer nutzt andere Wörter und Zahlen um diese zu beschreiben Lass mich kurz etwas darüber sagen, wie ich die Dinge hier darstelle, wenn ich den 2D Raum animiere. Ich benutze normalerweise dieses quadratische Raster, aber dieses Raster ist nur ein Konstrukt, ein Weg, um unser Koordinatensystem darzustellen, also ist es abhängig von der Wahl unserer Basisvektoren.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Natürlich ist es nichts Besonderes, dass diese Eigenwerte positiv sind, oder dass sie gestreckt oder gequetscht werden.",
  "model": "DeepL",
  "from_community_srt": "Der Raum selbst hat kein inneres Raster Jennifer wird vielleicht ihr eigenes Raster zeichnen welches ein genauso erdachtes Konstrukt wäre, welches nichts weiter als ein visuelles Werkzeug wäre,",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "In einem anderen Beispiel könntest du einen Eigenvektor mit dem Eigenwert negativ 1 halb haben, was bedeutet, dass der Vektor um den Faktor 1 halb gespiegelt und quadriert wird.",
  "model": "DeepL",
  "from_community_srt": "um der Bedeutung ihrer Koordinaten zu folgen.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Wichtig dabei ist, dass er auf der Linie bleibt, die er überspannt, ohne dass er von ihr weggedreht wird.",
  "model": "DeepL",
  "from_community_srt": "Ihr Ursprung würde jedoch mit unserem zusammenfallen, weil wir uns alle darüber einig sind, was [0 , 0] bedeuten soll. Es ist das was du herausbekommst, wenn du irgendeinen Vektor mit 0 skalierst.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Um einen Eindruck davon zu bekommen, warum es sinnvoll sein könnte, darüber nachzudenken, betrachte eine dreidimensionale Drehung.",
  "model": "DeepL",
  "from_community_srt": "Aber die Richtung ihrer Achsen und der Abstand ihrer Rasterlinien werden sich von unseren unterscheiden, abhängig von der Wahl ihrer Basisvektoren.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Wenn du einen Eigenvektor für diese Drehung finden kannst, also einen Vektor, der auf seiner eigenen Spannweite bleibt, hast du die Drehachse gefunden.",
  "model": "DeepL",
  "from_community_srt": "Jetzt wo all dies geklärt ist, ist eine ziemlich natürliche Frage, wie wir zwischen Koordinatensystemen übersetzen. Wenn zum Beispiel Jennifer einen Vektor mit den Koordinaten  [-1,",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "Und es ist viel einfacher, über eine 3D-Drehung in Form einer Drehachse und eines Winkels nachzudenken, um den sie sich dreht, als über die gesamte 3x3-Matrix, die mit dieser Transformation verbunden ist.",
  "model": "DeepL",
  "from_community_srt": "2] beschreibt, was wäre dieser Vektor in unserem Koordinatensystem? Wie übersetzt du von ihrer Sprache in unsere? Nun, was unserer Koordinaten sagen ist,",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "In diesem Fall müsste der entsprechende Eigenwert übrigens 1 sein, da Rotationen nie etwas strecken oder stauchen, sodass die Länge des Vektors gleich bleibt.",
  "model": "DeepL",
  "from_community_srt": "dass dieser Vektor -1 mal b1 + 2 mal b2 ist. Aus unserer Perspektive hat b1 die Koordinaten [2,",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Dieses Muster taucht häufig in der linearen Algebra auf.",
  "model": "DeepL",
  "from_community_srt": "1] und b2 die Koordinaten [-1, 1].",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Bei jeder linearen Transformation, die durch eine Matrix beschrieben wird, kannst du verstehen, was sie tut, wenn du die Spalten dieser Matrix als Landeplätze für Basisvektoren abliest.",
  "model": "DeepL",
  "from_community_srt": "Also können wir wirklich -1 b1 + 2 b2 ausführen so wie sie in unserem Koordinatensystem herauskommen.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Oft ist es aber besser, die Eigenvektoren und Eigenwerte zu ermitteln, um herauszufinden, was die lineare Transformation tatsächlich bewirkt, und weniger von deinem speziellen Koordinatensystem abhängig zu sein.",
  "model": "DeepL",
  "from_community_srt": "Wenn du dies durcharbeitest, kriegst du einen Vektor mit den Koordinaten  [-4, 1] . Das ist also wie wir den Vektor, den sie sich als [-1, 2] vorstellt,",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Ich werde hier nicht auf alle Details der Methoden zur Berechnung von Eigenvektoren und Eigenwerten eingehen, aber ich werde versuchen, einen Überblick über die Berechnungsideen zu geben, die für ein konzeptionelles Verständnis am wichtigsten sind.",
  "model": "DeepL",
  "from_community_srt": "beschreiben würden Der Prozess der Skalierung ihrer Basisvektoren durch die korrespondierenden Koordinanten irgendeines Vektors und sie dann zu addieren, fühlt sich vielleicht bekannt an. Es ist die Matrix-Vektor Multiplikation mit einer Matrix, deren Spalten ihre Basisvektoren in unserer Sprache beschreiben.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Symbolisch sieht die Idee eines Eigenvektors folgendermaßen aus.",
  "model": "DeepL",
  "from_community_srt": "Wenn du also Matrix-Vektor Multiplikation verstehst, als die Anwendung einer bestimmten Linear-Transformation",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A ist die Matrix, die eine Transformation darstellt, mit v als Eigenvektor, und lambda ist eine Zahl, nämlich der entsprechende Eigenwert.",
  "model": "DeepL",
  "from_community_srt": "sagen wir durch Anschauen dessen, was ich als das wichtigste Video dieser Reihe betrachte, Kapitel 3. Es gibt einen ziemlich intuitiven Weg, sich vorzustellen, was hier passiert.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Dieser Ausdruck besagt, dass das Matrix-Vektor-Produkt, A mal v, dasselbe Ergebnis liefert wie die Skalierung des Eigenvektors v mit einem Wert lambda.",
  "model": "DeepL",
  "from_community_srt": "Eine Matrix deren Spalten Jennifers Basisvektoren beschreiben kann man sich vorstellen als Transformation die unsere Basisvektoren i-Hut und j-Hut, die Dinger, die wir uns vorstellen,",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Um die Eigenvektoren und ihre Eigenwerte einer Matrix A zu finden, musst du also die Werte von v und lambda finden, die diesen Ausdruck wahr machen.",
  "model": "DeepL",
  "from_community_srt": "wenn wir sagen  [1,0] und [0, 1], zu Jennifers Basisvektoren bewegt, den Dingern, die sie sich vorstellt, wenn sie sagt  [1,0] und [0, 1]. Um zu zeigen,",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "Die linke Seite steht für die Matrix-Vektor-Multiplikation, aber die rechte Seite ist die Skalar-Vektor-Multiplikation.",
  "model": "DeepL",
  "from_community_srt": "wie das funktioniert, lass uns durchgehen, was es bedeuten würde einen Vektor zu nehmen mit den Koordinaten [-1 , 2,] und dieseTransformation anzuwenden.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Beginnen wir also damit, die rechte Seite als eine Art Matrix-Vektor-Multiplikation umzuschreiben, indem wir eine Matrix verwenden, die jeden Vektor um einen Faktor von Lambda skaliert.",
  "model": "DeepL",
  "from_community_srt": "Vor der Linear-Transformation stellen wir uns diesen Vektor vor, als eine bestimmte Linearkombination unserer Basisvektoren -1 mal i-Hut + 2 mal j-Hut. Und das Schlüsselkonzept einer Linear-Transformation, ist,",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Die Spalten einer solchen Matrix stellen dar, was mit den einzelnen Basisvektoren passiert. Jeder Basisvektor wird einfach mit Lambda multipliziert, sodass diese Matrix auf der Diagonalen die Zahl Lambda und überall sonst Nullen hat.",
  "model": "DeepL",
  "from_community_srt": "dass der resultierende Vektor die selbe Linearkombination sein wird, aber durch die neuen Basisvektoren -1 mal der Ort, an dem i-Hut landet + 2 mal der Ort, an dem j-Hut landet.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "Die übliche Art, diesen Typ zu schreiben, ist, das Lambda herauszufaktorisieren und es als Lambda mal i zu schreiben, wobei i die Identitätsmatrix mit 1en auf der Diagonale ist.",
  "model": "DeepL",
  "from_community_srt": "Was also diese Matrix tut, ist, sie transformiert unserer falsche Vorstellung davon, was Jennifer meint, in den tatsächlichen Vektor, auf den sie sich bezieht. Ich erinnere mich daran,",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Da beide Seiten wie eine Matrix-Vektor-Multiplikation aussehen, können wir die rechte Seite subtrahieren und das v herausrechnen.",
  "model": "DeepL",
  "from_community_srt": "dass als ich das das erste Mal gelernt habe, es sich irgendwie rückwärts angefühlt hat. Geometrisch gesehen, transformiert diese Matrix unser Raster in Jennifers Raster.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Wir haben jetzt also eine neue Matrix, A minus Lambda mal die Identität, und suchen nach einem Vektor v, bei dem diese neue Matrix mal v den Nullvektor ergibt.",
  "model": "DeepL",
  "from_community_srt": "Aber numerisch übersetzt sie einen Vektor aus ihrer Sprache in unsere Sprache. Wodurch es schließlich bei mir Klick gemacht hat, war sich vorzustellen, wie es unsere falsche Vorstellung darüber,",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Das ist immer dann der Fall, wenn v selbst der Nullvektor ist, aber das ist langweilig.",
  "model": "DeepL",
  "from_community_srt": "was Jennifer meint, den Vektor, den wir bekommen, wenn wir die selben Koordinaten in unserem System nutzen,",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Was wir wollen, ist ein Eigenvektor ungleich Null.",
  "model": "DeepL",
  "from_community_srt": "nimmt und ihn dann transformiert in den Vektor,",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "Und wenn du dir Kapitel 5 und 6 ansiehst, wirst du wissen, dass das Produkt einer Matrix mit einem Nicht-Null-Vektor nur dann Null werden kann, wenn die Transformation, die mit dieser Matrix verbunden ist, den Raum in eine niedrigere Dimension quetscht.",
  "model": "DeepL",
  "from_community_srt": "den sie tatsächlich meint. Wie ist es aber andersherum? Im Beispiel, dass ich zu Anfang des Videos benutzte, als ich den Vektor mit den Koordinaten  [3, 2] in unserem System habe, wie habe ich berechnet, dass er die Koordinaten [5/3,",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "Und diese Squishification entspricht einer Null-Determinante für die Matrix.",
  "model": "DeepL",
  "from_community_srt": "1/3] in Jennifers Raster haben würde? Du startest mit der Basiswechselmatrix,",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Nehmen wir an, deine Matrix A hat die Spalten 2, 1 und 2, 3. Überlege dir, wie du einen variablen Betrag, Lambda, von jedem Diagonaleintrag abziehen kannst.",
  "model": "DeepL",
  "from_community_srt": "die Jennifers Sprache in unsere Sprache übersetzt Dann nimmst du dessen ihre Inverse Denk daran, die Inverse einer Transformation ist die neue Transformation,",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Jetzt stell dir vor, du drehst an einem Knopf, um den Wert von Lambda zu ändern.",
  "model": "DeepL",
  "from_community_srt": "die bedeutet, die erste rückwärts ablaufen zu lassen.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "Wenn sich der Wert von Lambda ändert, ändert sich auch die Matrix selbst und damit auch die Determinante der Matrix.",
  "model": "DeepL",
  "from_community_srt": "In der Anwendung, speziell dann, wenn du in mehr als zwei DImensionen arbeitest, würdest du einen Computer nutzen, um die inverse Matrix zu bestimmen.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Das Ziel ist es, einen Lambda-Wert zu finden, bei dem die Determinante Null ist, was bedeutet, dass die geänderte Transformation den Raum in eine niedrigere Dimension drückt.",
  "model": "DeepL",
  "from_community_srt": "In diesem Fall ist die Inverse der Basiswechselmatrix, die Jennifers Basisvektoren als Spalten hat, die Matrix mit den Spalten  [1/3,",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "In diesem Fall ist der Sweet Spot erreicht, wenn lambda gleich 1 ist.",
  "model": "DeepL",
  "from_community_srt": "-1/3] und [1/3, 2/3].",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Hätten wir eine andere Matrix gewählt, müsste der Eigenwert natürlich nicht unbedingt 1 sein.",
  "model": "DeepL",
  "from_community_srt": "Also zum Beispiel, wenn wir wissen wollen,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Der Sweet Spot könnte auch bei einem anderen Lambda-Wert erreicht werden.",
  "model": "DeepL",
  "from_community_srt": "wie der Vektor [3 ,2] in Jennifers System aussieht, multiplizieren wir diese inverse Basiswechselmatrix mit dem Vektor [3 ,",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Das ist ganz schön viel, aber lass uns enträtseln, was das bedeutet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Wenn lambda gleich 1 ist, quetscht die Matrix A minus lambda mal die Identität den Raum auf eine Linie.",
  "model": "DeepL",
  "from_community_srt": "2] was herauskommt ist [5/3, 1/3].",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Das bedeutet, dass es einen Nicht-Null-Vektor v gibt, so dass A minus Lambda mal die Identität mal v gleich dem Null-Vektor ist.",
  "model": "DeepL",
  "from_community_srt": "Also ist dies in Kürze, wie man die Beschreibung individueller Vektoren vor und zurück zwischen Koordinatensystemen übersetzt. Die Matrix mit den Jennifers Basisvektoren als Spalten, aber beschrieben durch unsere Koordinaten,",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "Der Grund, warum uns das interessiert, ist, dass es bedeutet, dass A mal v gleich Lambda mal v ist, was bedeutet, dass der Vektor v ein Eigenvektor von A ist und während der Transformation A auf seiner eigenen Spanne bleibt.",
  "model": "DeepL",
  "from_community_srt": "übersetzt Vektoren von ihrer Sprache in unsere Sprache. Und die Inverse tut das Gegenteil. Aber Vektoren sind nicht das EInzige, das wir mit Koordinaten beschreiben. Für den nächsten Teil, ist es wichtig that du vertraut damit bist,",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "In diesem Beispiel ist der entsprechende Eigenwert 1, so dass v eigentlich an seinem Platz bleiben würde.",
  "model": "DeepL",
  "from_community_srt": "Transformationen mit Matrizen zu beschreiben, dass du weißt wie Matrixmultiplikation",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Halte inne und überlege, ob du sicherstellen musst, dass sich diese Argumentation gut anfühlt.",
  "model": "DeepL",
  "from_community_srt": "zusammenhängt mit der Komposition von Transformationen.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Das ist die Art von Dingen, die ich in der Einleitung erwähnt habe.",
  "model": "DeepL",
  "from_community_srt": "Pausiere definitiv und schaue dir Kapitel 3 und 4 an, wenn sich irgendwas davon nicht einfach anfühlt.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Wenn du kein solides Verständnis von Determinanten hättest und wüsstest, warum sie sich auf lineare Gleichungssysteme mit Lösungen ungleich Null beziehen, würde dir ein Ausdruck wie dieser völlig fremd vorkommen.",
  "model": "DeepL",
  "from_community_srt": "Nimm eine Lineartransformation wie eine 90° Drehung gegen den Uhrzeigersinn. Wenn du und ich diese mit der Matrix beschreiben, schauen wir, wo unsere Basisvektoren i-Hut und j-Hut landen.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Um dies in Aktion zu sehen, lass uns das Beispiel vom Anfang wiederholen, mit einer Matrix, deren Spalten 3, 0 und 1, 2 sind.",
  "model": "DeepL",
  "from_community_srt": "i-Hut landet auf dem Punkt mit den Koordinaten [0 , 1] und j-Hut landet an dem Punkt mit den Koordinaten [.1 , 0] Also werden diese Koordinaten die Spalten unserer Matrix.",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Um herauszufinden, ob ein Wert lambda ein Eigenwert ist, ziehst du ihn von den Diagonalen dieser Matrix ab und berechnest die Determinante.",
  "model": "DeepL",
  "from_community_srt": "Aber diese Darstellung, ist stark abhängig von der Wahl unserer Basisvektoren. Dadurch, dass wir in erster Linie i-Hut und j-Hut betrachten, hinzu dem Fakt,",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Auf diese Weise erhalten wir ein bestimmtes quadratisches Polynom in Lambda, 3 minus Lambda mal 2 minus Lambda.",
  "model": "DeepL",
  "from_community_srt": "dass wir ihre Landepunkte in unserem Koordinatensystem verfolgen. Wie würde Jennifer die selbe 90° Drehung des Raumes beschreiben? Du bist vielleicht verleitet dazu,",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Da lambda nur dann ein Eigenwert sein kann, wenn diese Determinante Null ist, kannst du daraus schließen, dass die einzigen möglichen Eigenwerte lambda gleich 2 und lambda gleich 3 sind.",
  "model": "DeepL",
  "from_community_srt": "nur die Spalten unserer Rotationsmatrix in Jennifers Sprache zu übersetzen, aber das stimmt nicht ganz Diese Spalten repräsentieren,",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Um herauszufinden, welche Eigenvektoren tatsächlich einen dieser Eigenwerte haben, z. B. Lambda gleich 2, fügst du diesen Wert von Lambda in die Matrix ein und löst dann, welche Vektoren diese diagonal veränderte Matrix zu Null macht.",
  "model": "DeepL",
  "from_community_srt": "wo unsere Basisvektoren i-Hut und j-Hut landen. Aber die Matrix, die Jennifer will, sollte zeigen, wo ihre Basisvektoren landen und es muss die Landepunkte in ihrer Sprache beschreiben. Hier ist ein normaler Weg, sich vorzustellen, wie das passiert.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Wenn du dieses System wie jedes andere lineare System berechnen würdest, würdest du sehen, dass die Lösungen alle Vektoren auf der Diagonalen sind, die von der negativen 1, 1 aufgespannt wird.",
  "model": "DeepL",
  "from_community_srt": "Starte mit irgendeinem Vektor geschrieben in Jennifers Sprache. Eher als zu versuchen, zu schauen, was mit diesem in ihrer Sprache passiert, werden wir ihn zuerst in unsere Sprache übersetzen,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Dies entspricht der Tatsache, dass die unveränderte Matrix 3, 0, 1, 2 all diese Vektoren um den Faktor 2 streckt.",
  "model": "DeepL",
  "from_community_srt": "indem wir die Basiswechselmatrix nutzen, deren Spalten ihre Basisvektoren in unserer Sprache beschreiben. Dies gibt uns den selben Vektor, aber nun geschrieben in unserer Sprache.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Eine 2D-Transformation muss also keine Eigenvektoren haben.",
  "model": "DeepL",
  "from_community_srt": "Dann nutze die Transformationsmatrix an dem was du herausbekommst, indem du sie von links multiplizierst. Das sagt uns,",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Betrachte zum Beispiel eine Drehung um 90 Grad.",
  "model": "DeepL",
  "from_community_srt": "wo der Vektor landet, aber immernoch in unserer Sprache.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Diese hat keine Eigenvektoren, da sie jeden Vektor aus seiner eigenen Spanne herausdreht.",
  "model": "DeepL",
  "from_community_srt": "Also als letzten Schritt, füge die inverse Basiswechselmatrix hinzu,",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Wenn du versuchst, die Eigenwerte einer solchen Drehung zu berechnen, merkst du, was passiert.",
  "model": "DeepL",
  "from_community_srt": "wie gewöhnlich bon links multipliziert, um den transformierten Vektor zu bekommen, aber nun in Jennifers Sprache. Da wir dies mit jedem Vektor,",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Seine Matrix hat die Spalten 0, 1 und negativ 1, 0.",
  "model": "DeepL",
  "from_community_srt": "geschrieben in ihrer Sprache, tun können Als erstes,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Subtrahiere lambda von den Diagonalelementen und suche, wann die Determinante Null ist.",
  "model": "DeepL",
  "from_community_srt": "füg die Basiswechselmatrix hinzu dann die Transformation dann die Inverse des Basiswechsels",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "In diesem Fall erhältst du das Polynom lambda Quadrat plus 1.",
  "model": "DeepL",
  "from_community_srt": "Die Komposition dieser drei Matrizen gibt uns die Transformationsmatrix in Jennifers Sprache.",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Die einzigen Wurzeln dieses Polynoms sind die imaginären Zahlen, i und negativ i.",
  "model": "DeepL",
  "from_community_srt": "Sie nimmt einen Vektor in ihrer Sprache, und gibt uns die transformierte Version des Vektors in ihrer Sprache",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Die Tatsache, dass es keine Lösungen mit reellen Zahlen gibt, bedeutet, dass es keine Eigenvektoren gibt.",
  "model": "DeepL",
  "from_community_srt": "In diesem expliziten Beispiel Wen Jennifers Basisvektoren aussehen wie  [2, 1] und [-1,",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Ein weiteres interessantes Beispiel, das du im Hinterkopf behalten solltest, ist eine Schere.",
  "model": "DeepL",
  "from_community_srt": "1] in unserer Sprache und die Transformation eine 90° Rotation ist, dann ist das Produkt dieser drei Matrizen",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Dadurch wird i-hat an seinem Platz fixiert und j-hat um 1 verschoben, sodass seine Matrix die Spalten 1, 0 und 1, 1 hat.",
  "model": "DeepL",
  "from_community_srt": "wenn du dich durcharbeitest, die Matrix mit den Spalten  [1/3, 5/3] and [-2/3, -1/3]. Wenn also Jennifer die Matrix multipliziert mit den Koordinaten eines Vektors in ihrem System",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Alle Vektoren auf der x-Achse sind Eigenvektoren mit dem Eigenwert 1, da sie an ihrem Platz bleiben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "Tatsächlich sind dies die einzigen Eigenvektoren.",
  "model": "DeepL",
  "from_community_srt": "wird dies eine um 90° gedrehte Version dieses Vektors ausgeben, ausgedrückt in ihrem Koordinaten System.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Wenn du lambda von den Diagonalen abziehst und die Determinante berechnest, erhältst du 1 minus lambda zum Quadrat.",
  "model": "DeepL",
  "from_community_srt": "Im Allgemeinen, immer wenn du einen Ausdruck siehst, wie A^(-1) M A bedeutet dies eine mathematische Art der Einfühlung.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "Und die einzige Wurzel dieses Ausdrucks ist lambda gleich 1.",
  "model": "DeepL",
  "from_community_srt": "Die mittlere Matrix repräsentiert in bestimmter Art und Weise eine Transformation,",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Dies entspricht dem, was wir geometrisch sehen, nämlich dass alle Eigenvektoren den Eigenwert 1 haben.",
  "model": "DeepL",
  "from_community_srt": "wie du sie siehst und die äußeren beiden repräsentieren die Einfühlung, den Sichtwechsel und die die gesamte Matrix repräsentiert die selbe Transformation, aber wie sie jemand anderes sieht.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Bedenke aber, dass es auch möglich ist, nur einen Eigenwert zu haben, aber mehr als nur eine Linie voller Eigenvektoren.",
  "model": "DeepL",
  "from_community_srt": "Für die, die sich Gedanken machen, wieso wir uns für alternative Koordinatensysteme interessieren, das nächste Video über EIgenvektoren und Eigenwerte,",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Ein einfaches Beispiel ist eine Matrix, die alles mit 2 skaliert.",
  "model": "DeepL",
  "from_community_srt": "wird ein sehr wichtiges Beispiel dafür zeigen.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "Der einzige Eigenwert ist 2, aber jeder Vektor in der Ebene wird zu einem Eigenvektor mit diesem Eigenwert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Jetzt ist ein weiterer guter Zeitpunkt, um innezuhalten und über einiges nachzudenken, bevor ich zum letzten Thema übergehe.",
  "model": "DeepL",
  "from_community_srt": "Auf Wiedersehen!",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Ich möchte hier mit der Idee einer Eigenbasis abschließen, die sich stark auf die Ideen aus dem letzten Video stützt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Sieh dir an, was passiert, wenn unsere Basisvektoren zufällig Eigenvektoren sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Zum Beispiel könnte i-hat eine negative Skalierung von 1 und j-hat eine Skalierung von 2 haben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Wenn du die neuen Koordinaten als Spalten einer Matrix schreibst, siehst du, dass die skalaren Vielfachen, negative 1 und 2, die die Eigenwerte von i-hat und j-hat sind, auf der Diagonale unserer Matrix stehen und jeder andere Eintrag eine 0 ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Wenn eine Matrix überall Nullen hat, außer auf der Diagonalen, nennt man sie vernünftigerweise eine Diagonalmatrix.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "Das bedeutet, dass alle Basisvektoren Eigenvektoren sind und die Diagonaleinträge dieser Matrix ihre Eigenwerte sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Es gibt eine Menge Dinge, die die Arbeit mit diagonalen Matrizen viel angenehmer machen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Eine davon ist, dass es einfacher zu berechnen ist, was passiert, wenn du diese Matrix ein paar Mal mit sich selbst multiplizierst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Da eine dieser Matrizen jeden Basisvektor nur um einen Eigenwert skaliert, entspricht die Anwendung dieser Matrix viele Male, z. B. 100 Mal, der Skalierung jedes Basisvektors um die 100-te Potenz des entsprechenden Eigenwertes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Versuche dagegen, die 100. Potenz einer nicht diagonalen Matrix zu berechnen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Probiere es doch mal aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Es ist ein Alptraum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Natürlich wirst du selten das Glück haben, dass deine Basisvektoren auch Eigenvektoren sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Wenn deine Transformation aber viele Eigenvektoren hat, wie die vom Anfang dieses Videos, so dass du eine Menge auswählen kannst, die den gesamten Raum abdeckt, dann kannst du dein Koordinatensystem so ändern, dass diese Eigenvektoren deine Basisvektoren sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Im letzten Video habe ich über den Wechsel der Basis gesprochen, aber ich werde hier noch einmal kurz erklären, wie man eine Transformation, die in unserem Koordinatensystem geschrieben wurde, in einem anderen System ausdrückt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Nimm die Koordinaten der Vektoren, die du als neue Basis verwenden willst, also in diesem Fall unsere beiden Eigenvektoren, und mache diese Koordinaten zu den Spalten einer Matrix, der sogenannten Basisänderungsmatrix.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Wenn du die ursprüngliche Umwandlung in eine Sandwich-Matrix umwandelst, indem du die Basisänderungsmatrix auf die rechte Seite und die Umkehrung der Basisänderungsmatrix auf die linke Seite legst, ist das Ergebnis eine Matrix, die dieselbe Umwandlung darstellt, aber aus der Perspektive des Koordinatensystems der neuen Basisvektoren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Der Sinn dieser Methode mit Eigenvektoren ist, dass die neue Matrix garantiert diagonal ist und die entsprechenden Eigenwerte auf der Diagonalen liegen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Das liegt daran, dass es sich um ein Koordinatensystem handelt, in dem die Basisvektoren bei der Transformation skaliert werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Eine Menge von Basisvektoren, die auch Eigenvektoren sind, nennt man vernünftigerweise eine Eigenbasis.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Wenn du also zum Beispiel die 100. Potenz dieser Matrix berechnen müsstest, wäre es viel einfacher, zu einer Eigenbasis zu wechseln, die 100. Potenz in diesem System zu berechnen und dann zu unserem Standardsystem zurückzukehren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Du kannst das nicht mit allen Transformationen machen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Eine Scherung zum Beispiel hat nicht genug Eigenvektoren, um den gesamten Raum zu erfassen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Aber wenn du eine Eigenbasis finden kannst, sind Matrixoperationen wirklich schön.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Für diejenigen unter euch, die bereit sind, sich durch ein hübsches Rätsel zu arbeiten, um zu sehen, wie das in Aktion aussieht und wie man damit überraschende Ergebnisse erzielen kann, lasse ich hier eine Aufforderung auf dem Bildschirm.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Es ist ein bisschen Arbeit, aber ich glaube, es wird dir Spaß machen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Das nächste und letzte Video dieser Reihe wird sich mit abstrakten Vektorräumen beschäftigen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]