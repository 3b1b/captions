[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Eigenvektoren und Eigenwerte gehören zu den Themen, die viele Studierende als besonders unintuitiv empfinden.",
  "model": "google_nmt",
  "from_community_srt": "Wenn ich einen Vektor im zweidimensionalen Raum habe, haben wir einen Standard Weg, um diesen mit Koordinaten zu beschreiben. In diesem Fall hat der Vektor die Koordinaten [3,2] Das bedeutet von seinem hinteren Teil, zu seiner Spitze gehen wir drei Einheiten nach rechts und zwei nach oben.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Fragen wie „Warum tun wir das und was bedeutet das eigentlich?“ bleiben allzu oft in einem Meer unbeantworteter Berechnungen unbeantwortet.",
  "model": "google_nmt",
  "from_community_srt": "Der Weg der linearen Algebra um Koordinaten zu beschreiben ist, sich jede dieser Zahlen als Skalar vorzustellen.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "Und während ich die Videos dieser Serie veröffentlicht habe, haben viele von Ihnen kommentiert, dass sie sich besonders auf die Visualisierung dieses Themas freuen.",
  "model": "google_nmt",
  "from_community_srt": "Ein Ding, das Vektoren in die Länge zieht oder zusammendrückt. Du stellst dir die erste Koordinate vor als Skalierung von i-Hut, dem Vektor mit Länge 1,",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Ich vermute, dass der Grund dafür nicht so sehr darin liegt, dass Eigendinge besonders kompliziert oder schlecht erklärt sind.",
  "model": "google_nmt",
  "from_community_srt": "der nach rechts zeigt, während die zweite Koordinate j-Hut skaliert,",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "Tatsächlich ist es vergleichsweise einfach und ich denke, dass die meisten Bücher es gut erklären.",
  "model": "google_nmt",
  "from_community_srt": "den Vektor mit Länge 1, der nach oben zeigt. Die Summe dieser zwei Vektoren vom hinteren Teil,",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Das Problem ist, dass es nur dann wirklich Sinn macht, wenn man ein solides visuelles Verständnis für viele der vorangehenden Themen hat.",
  "model": "google_nmt",
  "from_community_srt": "bis zur Spitze ist, was diese Koordinaten beschreiben sollen.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Das Wichtigste dabei ist, dass Sie wissen, wie man sich Matrizen als lineare Transformationen vorstellt, aber Sie müssen sich auch mit Dingen wie Determinanten, linearen Gleichungssystemen und Basiswechseln auskennen.",
  "model": "google_nmt",
  "from_community_srt": "Du kannst dir diese zwei speziellen Vektoren vorstellen, indem du alle impliziten Annahmen unseres Koordinatensystems einkapselst. Der Fakt, dass die erste Zahl eine Bewegung nach rechts impliziert und die zweite eine Bewegung nach oben impliziert, mit exakter Länge dieser Distanz.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Verwirrung über Eigenstoffe hat in der Regel mehr mit einer wackeligen Grundlage in einem dieser Themen zu tun als mit Eigenvektoren und Eigenwerten selbst.",
  "model": "google_nmt",
  "from_community_srt": "All dies liegt in der Wahl von i-Hut und j-Hut als die Vektoren, die Skalar Koordinaten eigentlich skalieren sollen. Jedenfalls,",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Betrachten Sie zunächst eine lineare Transformation in zwei Dimensionen, wie die hier gezeigte.",
  "model": "google_nmt",
  "from_community_srt": "um zwischen Vektoren und einer Ansammlung von Zahlen zu übersetzen wird dies ein Koordinatensystem genannt",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Es verschiebt den Basisvektor i-hat zu den Koordinaten 3, 0 und j-hat zu 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "und die zwei speziellen Vektoren i-Hut und j-Hut sind die sogenannten Basisvektoren unseres Standard Koordinatensystems",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Es wird also durch eine Matrix dargestellt, deren Spalten 3, 0 und 1, 2 sind.",
  "model": "google_nmt",
  "from_community_srt": "Worüber ich hierbei gerne sprechen möchte, ist die Idee unterschiedliche Systeme von Basisvektoren zu nutzen.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Konzentrieren Sie sich darauf, was es mit einem bestimmten Vektor macht, und denken Sie an die Spanne dieses Vektors, die Linie, die durch seinen Ursprung und seine Spitze verläuft.",
  "model": "google_nmt",
  "from_community_srt": "Zum Beispiel, lass uns sagen du hast eine Freundin, Jennifer, diese nutzt ein anderes System von Basisvektoren, die ich b1 und b2 nennen werde.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "Die meisten Vektoren werden während der Transformation aus ihrer Spanne gerissen.",
  "model": "google_nmt",
  "from_community_srt": "Ihr erster Basisvektor b1 zeigt ein bisschen nach oben rechts und ihr zweiter Basisvektor b2 zeigt nach oben links.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Ich meine, es wäre ziemlich zufällig, wenn der Ort, an dem der Vektor gelandet ist, zufällig auch irgendwo auf dieser Linie liegen würde.",
  "model": "google_nmt",
  "from_community_srt": "Schau dir nun nochmal den Vektor an, den ich vorhin gezeigt habe. Den,",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Einige spezielle Vektoren bleiben jedoch in ihrer eigenen Spanne, was bedeutet, dass die Wirkung der Matrix auf einen solchen Vektor lediglich darin besteht, ihn zu strecken oder zu stauchen, wie bei einem Skalar.",
  "model": "google_nmt",
  "from_community_srt": "den du und ich mit den Koordinaten [3 , 2] beschreiben würden, wenn wir unsere Basisvektoren i-Hut und j-Hut nutzen. Jennifer würde diesen Vektor mit den Koordinaten  [5/3, 1/3] beschreiben. Das bedeutet,",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Für dieses spezielle Beispiel ist der Basisvektor i-hat ein solcher spezieller Vektor.",
  "model": "google_nmt",
  "from_community_srt": "dass der Weg, um zu diesem Vektor zu kommen, mit ihren zwei Basisvektoren ist,",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "Die Spanne von i-hat ist die x-Achse, und aus der ersten Spalte der Matrix können wir sehen, dass sich i-hat auf das Dreifache seiner selbst bewegt, immer noch auf dieser x-Achse.",
  "model": "google_nmt",
  "from_community_srt": "b1 mit 5/3 und b2 mit 1/3 zu skalieren und sie dann zu addieren. Ich zeige euch später,",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Darüber hinaus wird aufgrund der Funktionsweise linearer Transformationen auch jeder andere Vektor auf der x-Achse nur um den Faktor 3 gedehnt und bleibt somit auf seiner eigenen Spanne.",
  "model": "google_nmt",
  "from_community_srt": "wie ihr die beiden Zahlen 5/3 und 1/3 herausfinden könnt. Im Allgemeinen, wenn Jennifer ihr Koordinatensystem nutzt, um einen Vektor zu beschreiben, stellt sie sich die erste Koordinate als Skalierung von b1 und die zweite Koordinate als Skalierung von b2 vor und addiert die Ergebnisse.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Ein etwas hinterhältigerer Vektor, der während dieser Transformation auf seiner eigenen Spanne bleibt, ist negativ 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Was sie herausbekommt wird normalerweise komplett anders sein, als der Vektor den du und ich herausbekommen, wenn wir uns diese Koordinaten vorstellen.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Am Ende wird es um den Faktor 2 gedehnt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "Und wiederum bedeutet Linearität, dass jeder andere Vektor auf der von diesem Kerl aufgespannten Diagonalen einfach um den Faktor 2 gedehnt wird.",
  "model": "google_nmt",
  "from_community_srt": "Um ein bisschen präziser zu sein Ihren erster Basisvektor b1 würden wir mit den Koordinaten [2, 1] beschreiben. Und ihren zweiten Basisvektor b2 würden wir beschreiben als  [-1,",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "Und für diese Transformation sind das alles Vektoren mit der besonderen Eigenschaft, auf ihrer Spanne zu bleiben.",
  "model": "google_nmt",
  "from_community_srt": "1]. Aber es ist wichtig zu verstehen, dass aus Sicht ihres Systems, diese Vektoren die Koordinaten [1,",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Die auf der X-Achse werden um den Faktor 3 gestreckt, die auf dieser Diagonalen um den Faktor 2.",
  "model": "google_nmt",
  "from_community_srt": "0] und [0, 1] haben. SIe beschreiben in ihrer Welt die Bedeutung von [1, 0] und [0,",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Jeder andere Vektor wird während der Transformation etwas gedreht und von der Linie entfernt, die er aufspannt.",
  "model": "google_nmt",
  "from_community_srt": "1] Also sprechen wir gewissermaßen unterschiedliche Sprachen Wir betrachten alle die selben Vektoren im Raum,",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Wie Sie vielleicht schon erraten haben, werden diese speziellen Vektoren Eigenvektoren der Transformation genannt, und jedem Eigenvektor ist ein sogenannter Eigenwert zugeordnet, bei dem es sich lediglich um den Faktor handelt, um den er während der Transformation gedehnt oder gestaucht wird.",
  "model": "google_nmt",
  "from_community_srt": "aber Jennifer nutzt andere Wörter und Zahlen um diese zu beschreiben Lass mich kurz etwas darüber sagen, wie ich die Dinge hier darstelle, wenn ich den 2D Raum animiere. Ich benutze normalerweise dieses quadratische Raster, aber dieses Raster ist nur ein Konstrukt, ein Weg, um unser Koordinatensystem darzustellen, also ist es abhängig von der Wahl unserer Basisvektoren.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Natürlich ist das Dehnen im Vergleich zum Quetschen nichts Besonderes oder die Tatsache, dass diese Eigenwerte zufällig positiv sind.",
  "model": "google_nmt",
  "from_community_srt": "Der Raum selbst hat kein inneres Raster Jennifer wird vielleicht ihr eigenes Raster zeichnen welches ein genauso erdachtes Konstrukt wäre, welches nichts weiter als ein visuelles Werkzeug wäre,",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "In einem anderen Beispiel könnten Sie einen Eigenvektor mit einem Eigenwert von minus 1 Hälfte haben, was bedeutet, dass der Vektor um den Faktor 1 Hälfte gespiegelt und gestaucht wird.",
  "model": "google_nmt",
  "from_community_srt": "um der Bedeutung ihrer Koordinaten zu folgen.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Aber der wichtige Teil hier ist, dass es auf der Linie bleibt, die es überspannt, ohne dass es von dieser abgedreht wird.",
  "model": "google_nmt",
  "from_community_srt": "Ihr Ursprung würde jedoch mit unserem zusammenfallen, weil wir uns alle darüber einig sind, was [0 , 0] bedeuten soll. Es ist das was du herausbekommst, wenn du irgendeinen Vektor mit 0 skalierst.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Um einen Eindruck davon zu bekommen, warum es sinnvoll sein könnte, darüber nachzudenken, betrachten Sie eine dreidimensionale Rotation.",
  "model": "google_nmt",
  "from_community_srt": "Aber die Richtung ihrer Achsen und der Abstand ihrer Rasterlinien werden sich von unseren unterscheiden, abhängig von der Wahl ihrer Basisvektoren.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Wenn Sie einen Eigenvektor für diese Rotation finden können, einen Vektor, der auf seiner eigenen Spanne bleibt, haben Sie die Rotationsachse gefunden.",
  "model": "google_nmt",
  "from_community_srt": "Jetzt wo all dies geklärt ist, ist eine ziemlich natürliche Frage, wie wir zwischen Koordinatensystemen übersetzen. Wenn zum Beispiel Jennifer einen Vektor mit den Koordinaten  [-1,",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "Und es ist viel einfacher, sich eine 3D-Rotation im Hinblick auf eine Rotationsachse und einen Winkel vorzustellen, um den sie sich dreht, als über die vollständige 3x3-Matrix nachzudenken, die mit dieser Transformation verbunden ist.",
  "model": "google_nmt",
  "from_community_srt": "2] beschreibt, was wäre dieser Vektor in unserem Koordinatensystem? Wie übersetzt du von ihrer Sprache in unsere? Nun, was unserer Koordinaten sagen ist,",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "In diesem Fall müsste der entsprechende Eigenwert übrigens 1 sein, da Rotationen nie etwas strecken oder stauchen, die Länge des Vektors also gleich bleiben würde.",
  "model": "google_nmt",
  "from_community_srt": "dass dieser Vektor -1 mal b1 + 2 mal b2 ist. Aus unserer Perspektive hat b1 die Koordinaten [2,",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Dieses Muster kommt in der linearen Algebra häufig vor.",
  "model": "google_nmt",
  "from_community_srt": "1] und b2 die Koordinaten [-1, 1].",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Bei jeder linearen Transformation, die durch eine Matrix beschrieben wird, können Sie verstehen, was sie bewirkt, indem Sie die Spalten dieser Matrix als Landepunkte für Basisvektoren ablesen.",
  "model": "google_nmt",
  "from_community_srt": "Also können wir wirklich -1 b1 + 2 b2 ausführen so wie sie in unserem Koordinatensystem herauskommen.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Eine bessere Möglichkeit, den Kern dessen zu verstehen, was die lineare Transformation tatsächlich bewirkt, und die weniger von Ihrem speziellen Koordinatensystem abhängt, besteht jedoch häufig darin, die Eigenvektoren und Eigenwerte zu ermitteln.",
  "model": "google_nmt",
  "from_community_srt": "Wenn du dies durcharbeitest, kriegst du einen Vektor mit den Koordinaten  [-4, 1] . Das ist also wie wir den Vektor, den sie sich als [-1, 2] vorstellt,",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Ich werde hier nicht alle Details zu Methoden zur Berechnung von Eigenvektoren und Eigenwerten behandeln, aber ich werde versuchen, einen Überblick über die Berechnungsideen zu geben, die für ein konzeptionelles Verständnis am wichtigsten sind.",
  "model": "google_nmt",
  "from_community_srt": "beschreiben würden Der Prozess der Skalierung ihrer Basisvektoren durch die korrespondierenden Koordinanten irgendeines Vektors und sie dann zu addieren, fühlt sich vielleicht bekannt an. Es ist die Matrix-Vektor Multiplikation mit einer Matrix, deren Spalten ihre Basisvektoren in unserer Sprache beschreiben.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Symbolisch gesehen sieht die Idee eines Eigenvektors wie folgt aus.",
  "model": "google_nmt",
  "from_community_srt": "Wenn du also Matrix-Vektor Multiplikation verstehst, als die Anwendung einer bestimmten Linear-Transformation",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A ist die Matrix, die eine Transformation darstellt, mit v als Eigenvektor und Lambda ist eine Zahl, nämlich der entsprechende Eigenwert.",
  "model": "google_nmt",
  "from_community_srt": "sagen wir durch Anschauen dessen, was ich als das wichtigste Video dieser Reihe betrachte, Kapitel 3. Es gibt einen ziemlich intuitiven Weg, sich vorzustellen, was hier passiert.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Dieser Ausdruck besagt, dass das Matrix-Vektor-Produkt A mal v das gleiche Ergebnis liefert, als würde man den Eigenvektor v einfach um einen Lambda-Wert skalieren.",
  "model": "google_nmt",
  "from_community_srt": "Eine Matrix deren Spalten Jennifers Basisvektoren beschreiben kann man sich vorstellen als Transformation die unsere Basisvektoren i-Hut und j-Hut, die Dinger, die wir uns vorstellen,",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Um die Eigenvektoren und ihre Eigenwerte einer Matrix A zu finden, kommt es also darauf an, die Werte von v und Lambda zu finden, die diesen Ausdruck wahr machen.",
  "model": "google_nmt",
  "from_community_srt": "wenn wir sagen  [1,0] und [0, 1], zu Jennifers Basisvektoren bewegt, den Dingern, die sie sich vorstellt, wenn sie sagt  [1,0] und [0, 1]. Um zu zeigen,",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "Es ist zunächst etwas umständlich, damit zu arbeiten, da die linke Seite die Matrix-Vektor-Multiplikation darstellt, die rechte Seite hier jedoch die Skalar-Vektor-Multiplikation.",
  "model": "google_nmt",
  "from_community_srt": "wie das funktioniert, lass uns durchgehen, was es bedeuten würde einen Vektor zu nehmen mit den Koordinaten [-1 , 2,] und dieseTransformation anzuwenden.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Beginnen wir also damit, die rechte Seite als eine Art Matrix-Vektor-Multiplikation umzuschreiben und dabei eine Matrix zu verwenden, die den Effekt hat, jeden Vektor um einen Lambda-Faktor zu skalieren.",
  "model": "google_nmt",
  "from_community_srt": "Vor der Linear-Transformation stellen wir uns diesen Vektor vor, als eine bestimmte Linearkombination unserer Basisvektoren -1 mal i-Hut + 2 mal j-Hut. Und das Schlüsselkonzept einer Linear-Transformation, ist,",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Die Spalten einer solchen Matrix stellen dar, was mit jedem Basisvektor passiert, und jeder Basisvektor wird einfach mit Lambda multipliziert, sodass diese Matrix entlang der Diagonale die Zahl Lambda hat und an allen anderen Stellen Nullen.",
  "model": "google_nmt",
  "from_community_srt": "dass der resultierende Vektor die selbe Linearkombination sein wird, aber durch die neuen Basisvektoren -1 mal der Ort, an dem i-Hut landet + 2 mal der Ort, an dem j-Hut landet.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "Die übliche Art, diesen Kerl zu schreiben, besteht darin, das Lambda herauszurechnen und es als Lambda mal i zu schreiben, wobei i die Identitätsmatrix mit Einsen entlang der Diagonale ist.",
  "model": "google_nmt",
  "from_community_srt": "Was also diese Matrix tut, ist, sie transformiert unserer falsche Vorstellung davon, was Jennifer meint, in den tatsächlichen Vektor, auf den sie sich bezieht. Ich erinnere mich daran,",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Da beide Seiten wie eine Matrix-Vektor-Multiplikation aussehen, können wir diese rechte Seite subtrahieren und v herausrechnen.",
  "model": "google_nmt",
  "from_community_srt": "dass als ich das das erste Mal gelernt habe, es sich irgendwie rückwärts angefühlt hat. Geometrisch gesehen, transformiert diese Matrix unser Raster in Jennifers Raster.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Was wir jetzt haben, ist eine neue Matrix, A minus Lambda multipliziert mit der Identität, und wir suchen nach einem Vektor v, so dass diese neue Matrix multipliziert mit v den Nullvektor ergibt.",
  "model": "google_nmt",
  "from_community_srt": "Aber numerisch übersetzt sie einen Vektor aus ihrer Sprache in unsere Sprache. Wodurch es schließlich bei mir Klick gemacht hat, war sich vorzustellen, wie es unsere falsche Vorstellung darüber,",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Das gilt zwar immer, wenn v selbst der Nullvektor ist, aber das ist langweilig.",
  "model": "google_nmt",
  "from_community_srt": "was Jennifer meint, den Vektor, den wir bekommen, wenn wir die selben Koordinaten in unserem System nutzen,",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Was wir wollen, ist ein Eigenvektor ungleich Null.",
  "model": "google_nmt",
  "from_community_srt": "nimmt und ihn dann transformiert in den Vektor,",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "Und wenn Sie sich Kapitel 5 und 6 ansehen, wissen Sie, dass das Produkt einer Matrix mit einem Vektor ungleich Null nur dann zu Null werden kann, wenn die mit dieser Matrix verbundene Transformation den Raum in eine niedrigere Dimension quetscht.",
  "model": "google_nmt",
  "from_community_srt": "den sie tatsächlich meint. Wie ist es aber andersherum? Im Beispiel, dass ich zu Anfang des Videos benutzte, als ich den Vektor mit den Koordinaten  [3, 2] in unserem System habe, wie habe ich berechnet, dass er die Koordinaten [5/3,",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "Und diese Quetschung entspricht einer Nulldeterminante für die Matrix.",
  "model": "google_nmt",
  "from_community_srt": "1/3] in Jennifers Raster haben würde? Du startest mit der Basiswechselmatrix,",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Um konkret zu sein: Nehmen wir an, Ihre Matrix A hat die Spalten 2, 1 und 2, 3 und überlegen Sie, von jedem diagonalen Eintrag einen variablen Betrag, Lambda, abzuziehen.",
  "model": "google_nmt",
  "from_community_srt": "die Jennifers Sprache in unsere Sprache übersetzt Dann nimmst du dessen ihre Inverse Denk daran, die Inverse einer Transformation ist die neue Transformation,",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Stellen Sie sich nun vor, Sie optimieren Lambda und drehen einen Knopf, um seinen Wert zu ändern.",
  "model": "google_nmt",
  "from_community_srt": "die bedeutet, die erste rückwärts ablaufen zu lassen.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "Wenn sich dieser Lambda-Wert ändert, ändert sich auch die Matrix selbst und damit auch die Determinante der Matrix.",
  "model": "google_nmt",
  "from_community_srt": "In der Anwendung, speziell dann, wenn du in mehr als zwei DImensionen arbeitest, würdest du einen Computer nutzen, um die inverse Matrix zu bestimmen.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Das Ziel besteht hier darin, einen Lambda-Wert zu finden, der diese Determinante auf Null setzt, was bedeutet, dass die optimierte Transformation den Raum in eine niedrigere Dimension zerquetscht.",
  "model": "google_nmt",
  "from_community_srt": "In diesem Fall ist die Inverse der Basiswechselmatrix, die Jennifers Basisvektoren als Spalten hat, die Matrix mit den Spalten  [1/3,",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "In diesem Fall entsteht der Sweet Spot, wenn Lambda gleich 1 ist.",
  "model": "google_nmt",
  "from_community_srt": "-1/3] und [1/3, 2/3].",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Wenn wir eine andere Matrix gewählt hätten, wäre der Eigenwert natürlich nicht unbedingt 1.",
  "model": "google_nmt",
  "from_community_srt": "Also zum Beispiel, wenn wir wissen wollen,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Der Sweet Spot könnte bei einem anderen Lambda-Wert erreicht werden.",
  "model": "google_nmt",
  "from_community_srt": "wie der Vektor [3 ,2] in Jennifers System aussieht, multiplizieren wir diese inverse Basiswechselmatrix mit dem Vektor [3 ,",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Das ist also ziemlich viel, aber lassen Sie uns herausfinden, was damit gemeint ist.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Wenn Lambda gleich 1 ist, quetscht die Matrix A minus Lambda multipliziert mit der Identität Platz auf eine Linie.",
  "model": "google_nmt",
  "from_community_srt": "2] was herauskommt ist [5/3, 1/3].",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Das bedeutet, dass es einen Vektor v ungleich Null gibt, sodass A minus Lambda mal Identität mal v gleich dem Nullvektor ist.",
  "model": "google_nmt",
  "from_community_srt": "Also ist dies in Kürze, wie man die Beschreibung individueller Vektoren vor und zurück zwischen Koordinatensystemen übersetzt. Die Matrix mit den Jennifers Basisvektoren als Spalten, aber beschrieben durch unsere Koordinaten,",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "Und denken Sie daran, der Grund, warum uns das interessiert, ist, dass es bedeutet, dass A mal v gleich Lambda mal v ist, was man so interpretieren kann, dass der Vektor v ein Eigenvektor von A ist und während der Transformation A auf seiner eigenen Spanne bleibt.",
  "model": "google_nmt",
  "from_community_srt": "übersetzt Vektoren von ihrer Sprache in unsere Sprache. Und die Inverse tut das Gegenteil. Aber Vektoren sind nicht das EInzige, das wir mit Koordinaten beschreiben. Für den nächsten Teil, ist es wichtig that du vertraut damit bist,",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "In diesem Beispiel ist der entsprechende Eigenwert 1, sodass v eigentlich einfach an seinem Platz bleiben würde.",
  "model": "google_nmt",
  "from_community_srt": "Transformationen mit Matrizen zu beschreiben, dass du weißt wie Matrixmultiplikation",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Halten Sie inne und überlegen Sie, ob Sie sicherstellen müssen, dass sich diese Argumentation gut anfühlt.",
  "model": "google_nmt",
  "from_community_srt": "zusammenhängt mit der Komposition von Transformationen.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "So etwas habe ich in der Einleitung erwähnt.",
  "model": "google_nmt",
  "from_community_srt": "Pausiere definitiv und schaue dir Kapitel 3 und 4 an, wenn sich irgendwas davon nicht einfach anfühlt.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Wenn Sie kein solides Verständnis für Determinanten hätten und wissen würden, warum sie sich auf lineare Gleichungssysteme mit Lösungen ungleich Null beziehen, würde ein Ausdruck wie dieser völlig aus heiterem Himmel erscheinen.",
  "model": "google_nmt",
  "from_community_srt": "Nimm eine Lineartransformation wie eine 90° Drehung gegen den Uhrzeigersinn. Wenn du und ich diese mit der Matrix beschreiben, schauen wir, wo unsere Basisvektoren i-Hut und j-Hut landen.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Um dies in Aktion zu sehen, schauen wir uns das Beispiel noch einmal von Anfang an an, mit einer Matrix, deren Spalten 3, 0 und 1, 2 sind.",
  "model": "google_nmt",
  "from_community_srt": "i-Hut landet auf dem Punkt mit den Koordinaten [0 , 1] und j-Hut landet an dem Punkt mit den Koordinaten [.1 , 0] Also werden diese Koordinaten die Spalten unserer Matrix.",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Um herauszufinden, ob ein Wert Lambda ein Eigenwert ist, subtrahieren Sie ihn von den Diagonalen dieser Matrix und berechnen Sie die Determinante.",
  "model": "google_nmt",
  "from_community_srt": "Aber diese Darstellung, ist stark abhängig von der Wahl unserer Basisvektoren. Dadurch, dass wir in erster Linie i-Hut und j-Hut betrachten, hinzu dem Fakt,",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Dadurch erhalten wir ein bestimmtes quadratisches Polynom in Lambda, 3 minus Lambda mal 2 minus Lambda.",
  "model": "google_nmt",
  "from_community_srt": "dass wir ihre Landepunkte in unserem Koordinatensystem verfolgen. Wie würde Jennifer die selbe 90° Drehung des Raumes beschreiben? Du bist vielleicht verleitet dazu,",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Da Lambda nur dann ein Eigenwert sein kann, wenn diese Determinante zufällig Null ist, können Sie daraus schließen, dass die einzig möglichen Eigenwerte Lambda gleich 2 und Lambda gleich 3 sind.",
  "model": "google_nmt",
  "from_community_srt": "nur die Spalten unserer Rotationsmatrix in Jennifers Sprache zu übersetzen, aber das stimmt nicht ganz Diese Spalten repräsentieren,",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Um herauszufinden, welche Eigenvektoren tatsächlich einen dieser Eigenwerte haben, sagen wir, Lambda ist gleich 2, fügen Sie diesen Lambda-Wert in die Matrix ein und lösen Sie dann, welche Vektoren diese diagonal veränderte Matrix auf Null sendet.",
  "model": "google_nmt",
  "from_community_srt": "wo unsere Basisvektoren i-Hut und j-Hut landen. Aber die Matrix, die Jennifer will, sollte zeigen, wo ihre Basisvektoren landen und es muss die Landepunkte in ihrer Sprache beschreiben. Hier ist ein normaler Weg, sich vorzustellen, wie das passiert.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Wenn Sie dies wie jedes andere lineare System berechnen würden, würden Sie sehen, dass die Lösungen alle Vektoren auf der Diagonalen sind, die von minus 1, 1 aufgespannt werden.",
  "model": "google_nmt",
  "from_community_srt": "Starte mit irgendeinem Vektor geschrieben in Jennifers Sprache. Eher als zu versuchen, zu schauen, was mit diesem in ihrer Sprache passiert, werden wir ihn zuerst in unsere Sprache übersetzen,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Dies entspricht der Tatsache, dass die unveränderte Matrix 3, 0, 1, 2 den Effekt hat, alle diese Vektoren um den Faktor 2 zu strecken.",
  "model": "google_nmt",
  "from_community_srt": "indem wir die Basiswechselmatrix nutzen, deren Spalten ihre Basisvektoren in unserer Sprache beschreiben. Dies gibt uns den selben Vektor, aber nun geschrieben in unserer Sprache.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Nun muss eine 2D-Transformation keine Eigenvektoren haben.",
  "model": "google_nmt",
  "from_community_srt": "Dann nutze die Transformationsmatrix an dem was du herausbekommst, indem du sie von links multiplizierst. Das sagt uns,",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Betrachten Sie beispielsweise eine Drehung um 90 Grad.",
  "model": "google_nmt",
  "from_community_srt": "wo der Vektor landet, aber immernoch in unserer Sprache.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Dies hat keine Eigenvektoren, da es jeden Vektor aus seiner eigenen Spanne dreht.",
  "model": "google_nmt",
  "from_community_srt": "Also als letzten Schritt, füge die inverse Basiswechselmatrix hinzu,",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Wenn Sie tatsächlich versuchen, die Eigenwerte einer solchen Drehung zu berechnen, beachten Sie, was passiert.",
  "model": "google_nmt",
  "from_community_srt": "wie gewöhnlich bon links multipliziert, um den transformierten Vektor zu bekommen, aber nun in Jennifers Sprache. Da wir dies mit jedem Vektor,",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Seine Matrix hat die Spalten 0, 1 und negativ 1, 0.",
  "model": "google_nmt",
  "from_community_srt": "geschrieben in ihrer Sprache, tun können Als erstes,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Subtrahieren Sie Lambda von den Diagonalelementen und suchen Sie, wann die Determinante Null ist.",
  "model": "google_nmt",
  "from_community_srt": "füg die Basiswechselmatrix hinzu dann die Transformation dann die Inverse des Basiswechsels",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "In diesem Fall erhält man das Polynom Lambda zum Quadrat plus 1.",
  "model": "google_nmt",
  "from_community_srt": "Die Komposition dieser drei Matrizen gibt uns die Transformationsmatrix in Jennifers Sprache.",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Die einzigen Wurzeln dieses Polynoms sind die imaginären Zahlen i und negativ i.",
  "model": "google_nmt",
  "from_community_srt": "Sie nimmt einen Vektor in ihrer Sprache, und gibt uns die transformierte Version des Vektors in ihrer Sprache",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Die Tatsache, dass es keine reellen Zahlenlösungen gibt, weist darauf hin, dass es keine Eigenvektoren gibt.",
  "model": "google_nmt",
  "from_community_srt": "In diesem expliziten Beispiel Wen Jennifers Basisvektoren aussehen wie  [2, 1] und [-1,",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Ein weiteres ziemlich interessantes Beispiel, das es wert ist, im Hinterkopf zu behalten, ist eine Schere.",
  "model": "google_nmt",
  "from_community_srt": "1] in unserer Sprache und die Transformation eine 90° Rotation ist, dann ist das Produkt dieser drei Matrizen",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Dadurch wird i-hat an seinem Platz fixiert und j-hat 1 verschoben, sodass seine Matrix die Spalten 1, 0 und 1, 1 hat.",
  "model": "google_nmt",
  "from_community_srt": "wenn du dich durcharbeitest, die Matrix mit den Spalten  [1/3, 5/3] and [-2/3, -1/3]. Wenn also Jennifer die Matrix multipliziert mit den Koordinaten eines Vektors in ihrem System",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Alle Vektoren auf der x-Achse sind Eigenvektoren mit Eigenwert 1, da sie ortsfest bleiben.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "Tatsächlich sind dies die einzigen Eigenvektoren.",
  "model": "google_nmt",
  "from_community_srt": "wird dies eine um 90° gedrehte Version dieses Vektors ausgeben, ausgedrückt in ihrem Koordinaten System.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Wenn Sie Lambda von den Diagonalen subtrahieren und die Determinante berechnen, erhalten Sie 1 minus Lambda im Quadrat.",
  "model": "google_nmt",
  "from_community_srt": "Im Allgemeinen, immer wenn du einen Ausdruck siehst, wie A^(-1) M A bedeutet dies eine mathematische Art der Einfühlung.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "Und die einzige Wurzel dieses Ausdrucks ist Lambda gleich 1.",
  "model": "google_nmt",
  "from_community_srt": "Die mittlere Matrix repräsentiert in bestimmter Art und Weise eine Transformation,",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Dies steht im Einklang mit dem, was wir geometrisch sehen, dass alle Eigenvektoren den Eigenwert 1 haben.",
  "model": "google_nmt",
  "from_community_srt": "wie du sie siehst und die äußeren beiden repräsentieren die Einfühlung, den Sichtwechsel und die die gesamte Matrix repräsentiert die selbe Transformation, aber wie sie jemand anderes sieht.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Beachten Sie jedoch, dass es auch möglich ist, nur einen Eigenwert zu haben, jedoch mit mehr als nur einer Linie voller Eigenvektoren.",
  "model": "google_nmt",
  "from_community_srt": "Für die, die sich Gedanken machen, wieso wir uns für alternative Koordinatensysteme interessieren, das nächste Video über EIgenvektoren und Eigenwerte,",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Ein einfaches Beispiel ist eine Matrix, die alles um 2 skaliert.",
  "model": "google_nmt",
  "from_community_srt": "wird ein sehr wichtiges Beispiel dafür zeigen.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "Der einzige Eigenwert ist 2, aber jeder Vektor in der Ebene wird ein Eigenvektor mit diesem Eigenwert.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Jetzt ist ein weiterer guter Zeitpunkt, innezuhalten und darüber nachzudenken, bevor ich zum letzten Thema übergehe.",
  "model": "google_nmt",
  "from_community_srt": "Auf Wiedersehen!",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Ich möchte hier mit der Idee einer Eigenbasis abschließen, die stark auf Ideen aus dem letzten Video basiert.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Schauen Sie sich an, was passiert, wenn unsere Basisvektoren zufällig Eigenvektoren sind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Zum Beispiel könnte i-hat mit minus 1 skaliert werden und j-hat mit minus 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Wenn Sie ihre neuen Koordinaten als Spalten einer Matrix schreiben, beachten Sie, dass diese skalaren Vielfachen, negativ 1 und 2, die die Eigenwerte von i-hat und j-hat sind, auf der Diagonale unserer Matrix liegen und jeder andere Eintrag eine 0 ist .",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Immer wenn eine Matrix überall außer der Diagonale Nullen hat, wird sie vernünftigerweise Diagonalmatrix genannt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "Dies lässt sich so interpretieren, dass alle Basisvektoren Eigenvektoren sind und die diagonalen Einträge dieser Matrix ihre Eigenwerte sind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Es gibt viele Dinge, die die Arbeit mit Diagonalmatrizen viel angenehmer machen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Ein großer Nachteil ist, dass es einfacher ist zu berechnen, was passiert, wenn man diese Matrix mehrmals mit sich selbst multipliziert.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Da eine dieser Matrizen lediglich jeden Basisvektor um einen Eigenwert skaliert, entspricht die mehrmalige Anwendung dieser Matrix, beispielsweise 100 Mal, lediglich einer Skalierung jedes Basisvektors um die 100. Potenz des entsprechenden Eigenwerts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Versuchen Sie im Gegensatz dazu, die 100. Potenz einer nichtdiagonalen Matrix zu berechnen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Probieren Sie es wirklich einmal aus.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Es ist ein Albtraum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Natürlich werden Sie selten das Glück haben, dass Ihre Basisvektoren auch Eigenvektoren sind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Wenn Ihre Transformation jedoch viele Eigenvektoren hat, wie die vom Anfang dieses Videos, so viele, dass Sie einen Satz auswählen können, der den gesamten Raum abdeckt, können Sie Ihr Koordinatensystem so ändern, dass diese Eigenvektoren Ihre Basisvektoren sind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Ich habe im letzten Video über den Basiswechsel gesprochen, aber ich werde hier ganz kurz daran erinnern, wie man eine Transformation, die derzeit in unserem Koordinatensystem geschrieben ist, in ein anderes System ausdrückt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Nehmen Sie die Koordinaten der Vektoren, die Sie als neue Basis verwenden möchten, was in diesem Fall unsere beiden Eigenvektoren bedeutet, und machen Sie diese Koordinaten dann zu den Spalten einer Matrix, die als Basisänderungsmatrix bezeichnet wird.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Wenn Sie die ursprüngliche Transformation einordnen und die Änderung der Basismatrix auf der rechten Seite und die Umkehrung der Änderung der Basismatrix auf der linken Seite platzieren, ist das Ergebnis eine Matrix, die dieselbe Transformation darstellt, jedoch aus der Perspektive der neuen Basisvektorkoordinaten System.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Der Sinn dieser Vorgehensweise mit Eigenvektoren besteht darin, dass diese neue Matrix mit ihren entsprechenden Eigenwerten garantiert diagonal verläuft.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Dies liegt daran, dass es sich um die Arbeit in einem Koordinatensystem handelt, bei dem die Basisvektoren während der Transformation skaliert werden.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Eine Menge von Basisvektoren, die auch Eigenvektoren sind, wird wiederum sinnvollerweise Eigenbasis genannt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Wenn Sie beispielsweise die 100. Potenz dieser Matrix berechnen müssten, wäre es viel einfacher, zu einer Eigenbasis zu wechseln, die 100. Potenz in diesem System zu berechnen und dann wieder in unser Standardsystem umzuwandeln.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Das ist nicht bei allen Transformationen möglich.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Eine Scherung hat beispielsweise nicht genügend Eigenvektoren, um den gesamten Raum zu überspannen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Aber wenn man eine Eigenbasis findet, macht das Matrixoperationen wirklich schön.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Für diejenigen unter Ihnen, die bereit sind, ein ziemlich nettes Puzzle durchzuarbeiten, um zu sehen, wie das in Aktion aussieht und wie es zu überraschenden Ergebnissen führen kann, lasse ich hier auf dem Bildschirm eine Eingabeaufforderung.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Es erfordert ein wenig Arbeit, aber ich denke, es wird Ihnen Spaß machen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Das nächste und letzte Video dieser Serie wird sich mit abstrakten Vektorräumen befassen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]