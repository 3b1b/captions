[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Egenvektorer och egenvärden är ett av de ämnen som många elever tycker är särskilt ointuitiva.",
  "model": "google_nmt",
  "from_community_srt": "Om jag har en vektor i två dimensioner har vi ett standardsätt att beskriva det med koordinater. I detta fallet har vektorn koordinaterna [3,2], vilket betyder att för att gå från dess startpunkt till dess spets behöver vi flytta 3 enheter åt höger och 2 enheter upp.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Frågor som varför gör vi det här och vad betyder detta egentligen, lämnas alltför ofta bara flytande i ett obesvarat hav av beräkningar.",
  "model": "google_nmt",
  "from_community_srt": "Det mer linjär-algerbra-orienterade sättet att beskriva koordinater är att betrakta var och en av dessa siffror som en skalär,",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "Och när jag har lagt ut videorna i den här serien, har många av er kommenterat om att se fram emot att visualisera detta ämne i synnerhet.",
  "model": "google_nmt",
  "from_community_srt": "något som sträcker ut eller trycker ihop vektorer.",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Jag misstänker att orsaken till detta inte är så mycket att egensaker är särskilt komplicerade eller dåligt förklarade.",
  "model": "google_nmt",
  "from_community_srt": "Du tänker dig den första koordinaten som i-hatts skalär, vektorn med längden ett som pekar åt höger,",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "Faktum är att det är relativt okomplicerat, och jag tror att de flesta böcker gör ett bra jobb med att förklara det.",
  "model": "google_nmt",
  "from_community_srt": "medan den andra koordinaten skalar j-hatt, vektorn med längd ett som pekar rakt upp. Summan av dessa två skalade vektorer,",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Problemet är att det bara är vettigt om du har en solid visuell förståelse för många av de ämnen som föregår det.",
  "model": "google_nmt",
  "from_community_srt": "från startpunkt till spets, är vad koordinaterna representerar.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Viktigast här är att du vet hur du tänker på matriser som linjära transformationer, men du måste också vara bekväm med saker som determinanter, linjära ekvationssystem och förändring av bas.",
  "model": "google_nmt",
  "from_community_srt": "Du kan tänka dig att dessa två speciella vektorer innehåller alla de implicita antagandena om vårt koordinatsystem. Faktumet att den första siffran indikerar rörelse åt höger, att den andra indikerar rörelse uppåt exakt hur långt uttryckt i längdenheter.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Förvirring om egenmaterial har vanligtvis mer att göra med en skakig grund i ett av dessa ämnen än med egenvektorer och egenvärden i sig.",
  "model": "google_nmt",
  "from_community_srt": "Allt det beror på valet av i-hatt och j-hatt som vektorerna koordinaterna är avsedda att skala. Hur som helst,",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Till att börja med, överväg en linjär transformation i två dimensioner, som den som visas här.",
  "model": "google_nmt",
  "from_community_srt": "att översätta mellan vektorer och uppsättningar av siffror kallas för ett koordinatsystem, och de två speciella vektorerna,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Den flyttar basvektorn i-hat till koordinaterna 3, 0 och j-hat till 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "i-hatt och j-hatt, kallas för basvektorer i vårt standardkoordinatsystem.",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Så det representeras med en matris vars kolumner är 3, 0 och 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "Vad jag skulle vilja prata om här är idén att använda en annan uppsättning vektorer som basvektorer.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Fokusera på vad den gör med en viss vektor och tänk på spännvidden för den vektorn, linjen som går genom dess ursprung och dess spets.",
  "model": "google_nmt",
  "from_community_srt": "Till exempel, låt oss säga att du har en vän, Jennifer, som använder en annan uppsättning basvektorer, som jag kommer kalla b1 och b2.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "De flesta vektorer kommer att slås ur sitt spann under transformationen.",
  "model": "google_nmt",
  "from_community_srt": "Hennes första basvektor b1 pekar uppåt och lite till höger, och hennes andra vektor b2 pekar vänster och uppåt.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Jag menar, det skulle verka ganska tillfälligt om platsen där vektorn landade också råkade vara någonstans på den linjen.",
  "model": "google_nmt",
  "from_community_srt": "Ta nu ännu en titt på vektorn jag visade tidigare.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Men vissa speciella vektorer förblir på sitt eget span, vilket betyder att effekten som matrisen har på en sådan vektor bara är att sträcka den eller klämma ihop den, som en skalär.",
  "model": "google_nmt",
  "from_community_srt": "Den som du och jag skulle beskriva med koordinaterna [3,2] i våra basvektorer i-hatt och j-hatt. Jennifer skulle faktiskt beskriva denna  vektor med koordinaterna [5/3, 1/3].",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "För detta specifika exempel är basvektorn i-hat en sådan speciell vektor.",
  "model": "google_nmt",
  "from_community_srt": "Vad detta innebär är att det speciella sättet att få den vektorn med hjälp av hennes två basvektorer",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "Spännvidden för i-hat är x-axeln, och från den första kolumnen i matrisen kan vi se att i-hat rör sig över till 3 gånger sig själv, fortfarande på den x-axeln.",
  "model": "google_nmt",
  "from_community_srt": "är att skala b1 med 5/3, skala b2 med 1/3 och sedan lägga ihop dem. Om en liten stund kommer jag visa hur du skulle kunna komma fram till de två siffrorna 5/3 och",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Dessutom, på grund av hur linjära transformationer fungerar, sträcks alla andra vektorer på x-axeln bara ut med en faktor 3, och förblir därför på sitt eget span.",
  "model": "google_nmt",
  "from_community_srt": "1/3. I allmänhet när Jennifer använder koordinater för att beskriva en vektor tänker hon på den första koordinaten som en skalning av b1 den andra som en skalning av b2 och sedan lägger hon ihop resultaten.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "En något smygare vektor som förblir på sitt eget span under denna transformation är negativ 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Vad hon får kommer i allmänhet bli helt annorlunda från den vektor du och jag skulle anse ha de koordinaterna.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Det slutar med att det sträcks ut med en faktor 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "Och återigen, linearitet kommer att innebära att vilken annan vektor som helst på den diagonala linjen som spänner över av den här killen bara kommer att sträckas ut med en faktor 2.",
  "model": "google_nmt",
  "from_community_srt": "För att vara lite mer precis angående upplägget här: hennes första basvektor b1 är något vi skulle beskriva med koordinaterna [2,1] och hennes andra basvektor b2 är något vi skulle beskriva som [-1,1].",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "Och för denna transformation är det alla vektorer med den här speciella egenskapen att hålla sig på sin spännvidd.",
  "model": "google_nmt",
  "from_community_srt": "Men det är viktigt att inse att från hennes perspektiv i hennes system har de vektorerna koordinaterna [1,0] och [0,1].",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "De på x-axeln sträcks ut med en faktor 3, och de på den här diagonala linjen sträcks ut med en faktor 2.",
  "model": "google_nmt",
  "from_community_srt": "De är vad som definierar meningen av koordinaterna [1,0] och [0,1] i hennes värld.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Vilken annan vektor som helst kommer att roteras något under transformationen, slås av linjen som den sträcker sig över.",
  "model": "google_nmt",
  "from_community_srt": "Så egentligen pratar vi olika språk.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Som du kanske har gissat vid det här laget kallas dessa speciella vektorer för transformationens egenvektorer, och varje egenvektor har associerat med det vad som kallas ett egenvärde, vilket bara är den faktor med vilken den sträcks ut eller kläms ihop under transformationen.",
  "model": "google_nmt",
  "from_community_srt": "Vi tittar alla på samma vektorer i rummet men Jennifer använder andra ord och siffror för att beskriva dem. Låt mig säga några korta ord om hur jag representerar saker här när jag animerar 2D-rummet använder jag vanligtvis det här rutnätet Men det rutnätet är bara en konstruktion ett sätt att visualisera vårt koordinatsystem och därför beror det på vårt val av bas.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Naturligtvis finns det inget speciellt med stretching kontra squishing, eller det faktum att dessa egenvärden råkar vara positiva.",
  "model": "google_nmt",
  "from_community_srt": "Rummet självt har inget inneboende rutnät. Jennifer skulle kunna rita sitt eget rutnät som skulle vara en lika påhittad konstruktion som inte är något annat än ett visuellt verktyg",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "I ett annat exempel kan du ha en egenvektor med egenvärde negativt 1 halv, vilket betyder att vektorn vänds och kläms med en faktor på 1 halv.",
  "model": "google_nmt",
  "from_community_srt": "för att hjälpa till att visa vad hennes koordinater betyder.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Men den viktiga delen här är att den stannar på linjen som den sträcker sig ut utan att roteras bort från den.",
  "model": "google_nmt",
  "from_community_srt": "Hennes origo skulle dock faktiskt sammanfalla med vårt eftersom alla är överens om vad koordinaterna [0,0] skulle innebära. Det är vad du får när du skalar vilken godtycklig vektor som helst med 0.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "För en glimt av varför detta kan vara en bra sak att tänka på, överväg lite tredimensionell rotation.",
  "model": "google_nmt",
  "from_community_srt": "Men riktningen  på hennes axlar och avståndet mellan hennes rutnätslinjer kommer vara annorlunda, beroende på hur hon väljer basvektorer.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Om du kan hitta en egenvektor för den rotationen, en vektor som stannar kvar på sitt eget span, är det du har hittat rotationsaxeln.",
  "model": "google_nmt",
  "from_community_srt": "Så, efter att allt detta är upprättat är det ganska naturligt att fråga: hur översätter vi mellan koordinatsystem? Om, till exempel,",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "Och det är mycket lättare att tänka på en 3D-rotation i termer av någon rotationsaxel och en vinkel med vilken den roterar, snarare än att tänka på hela 3x3-matrisen som är förknippad med den transformationen.",
  "model": "google_nmt",
  "from_community_srt": "Jennifer beskriver en vektor med koordinaterna [-1,2] vad skulle det vara i vårt koordinatsystem? Hur översätter vi från hennes språk till vårt? Nå, det våra koordinater säger är att denna vektor är -1 b1 + 2 b2.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "I det här fallet måste förresten motsvarande egenvärde vara 1, eftersom rotationer aldrig sträcker eller klämmer ihop någonting, så längden på vektorn skulle förbli densamma.",
  "model": "google_nmt",
  "from_community_srt": "Och från vårt perspektiv har b1 koordinaterna [2,1] och b2 har koordinaterna [-1,1].",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Detta mönster visar sig mycket i linjär algebra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Med vilken linjär transformation som helst som beskrivs av en matris kan du förstå vad den gör genom att läsa av kolumnerna i denna matris som landningspunkter för basvektorer.",
  "model": "google_nmt",
  "from_community_srt": "Så vi kan faktiskt beräkna -1 b1 + 2 b2 som de representeras i vårt i koordinatsystem.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Men ofta är ett bättre sätt att komma till kärnan i vad den linjära transformationen faktiskt gör, mindre beroende av ditt specifika koordinatsystem, att hitta egenvektorerna och egenvärdena.",
  "model": "google_nmt",
  "from_community_srt": "Och om du räknar ut detta får du en vektor med koordinaterna [-4,1]. Det är alltså hur vi skulle beskriva den vektor hon tänker på som [-1,2]. Denna process,",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Jag kommer inte att täcka alla detaljer om metoder för att beräkna egenvektorer och egenvärden här, men jag ska försöka ge en översikt över de beräkningsidéer som är viktigast för en konceptuell förståelse.",
  "model": "google_nmt",
  "from_community_srt": "att skala var och en av hennes basvektor med de korresponderande koordinaterna för någon vektor och sedan addera dem kanske känns något bekant. Det är matrismultiplikation med en matris vars kolonner representerar Jennifers basvektorer i vårt språk.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Symboliskt, så här ser idén med en egenvektor ut.",
  "model": "google_nmt",
  "from_community_srt": "Faktum är att när du förstått att matrismultiplikation är att applicera en speciell linjär transformation",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A är matrisen som representerar någon transformation, med v som egenvektor, och lambda är ett tal, nämligen motsvarande egenvärde.",
  "model": "google_nmt",
  "from_community_srt": "till exempel genom att titta på den video jag anser vara den viktigaste i denna serien, kapitel 3, finns det ett ganska intuitivt sätt att tänka på vad som händer här.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Vad detta uttryck säger är att matris-vektorprodukten, A gånger v, ger samma resultat som att bara skala egenvektorn v med något värde lambda.",
  "model": "google_nmt",
  "from_community_srt": "En matris vars kolonner representeras Jennifers basvektorer kan ses som en transformation som flyttar våra basvektorer,",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Så att hitta egenvektorerna och deras egenvärden för en matris A handlar om att hitta värdena på v och lambda som gör detta uttryck sant.",
  "model": "google_nmt",
  "from_community_srt": "i-hatt och j-hatt de saker vi tänker på när vi säger [1,0] och [0,1] till Jennifers basvektorer de saker hon tänker på när hon säger [1,0] och [0,1].",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "Det är lite besvärligt att arbeta med till en början, eftersom den vänstra sidan representerar matris-vektor multiplikation, men den högra sidan här är skalär-vektor multiplikation.",
  "model": "google_nmt",
  "from_community_srt": "För att visa hur detta fungerar, låt oss gå igenom vad det skulle innebära att ta vektorn vi tänker oss har koordinaterna [-1,2] och applicera den transformationen.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Så låt oss börja med att skriva om den högra sidan som någon slags matris-vektormultiplikation, med hjälp av en matris som har effekten att skala vilken vektor som helst med en faktor lambda.",
  "model": "google_nmt",
  "from_community_srt": "Innan den linjära transformationen ser vi denna vektor som en speciell linjärkombination av våra basvektorer -1 x i-hatt + 2x j-hatt.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Kolumnerna i en sådan matris kommer att representera vad som händer med varje basvektor, och varje basvektor multipliceras helt enkelt med lambda, så denna matris kommer att ha talet lambda nedåt diagonalen, med nollor överallt annars.",
  "model": "google_nmt",
  "from_community_srt": "Och den viktigaste egenskapen hos en linjärkombination är att den resulterande vektorn kommer vara samma linjärkombination men av de nya basvektorerna -1 gånger stället där i-hat landar plus 2 gånger stället där j-hatt landar.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "Det vanliga sättet att skriva den här killen är att faktorisera den lambdan och skriva den som lambda gånger i, där i är identitetsmatrisen med 1:or nedåt diagonalen.",
  "model": "google_nmt",
  "from_community_srt": "Så vad denna matris gör är att transformera vår missuppfattning av vad Jennifer menar till den vektor hon faktiskt refererar till.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "När båda sidorna ser ut som matris-vektormultiplikation kan vi subtrahera den högra sidan och faktorisera v.",
  "model": "google_nmt",
  "from_community_srt": "Jag kommer ihåg att första gången jag lärde mig detta kändes det alltid ganska bakvänt. Geometriskt sett tar denna matristransformation vårt rutnät till Jennifers rutnät.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Så vad vi nu har är en ny matris, A minus lambda gånger identiteten, och vi letar efter en vektor v så att denna nya matris gånger v ger nollvektorn.",
  "model": "google_nmt",
  "from_community_srt": "Men numeriskt översätter den en vektor beskriven i hennes språk till vårt språk. Vad som fick bitarna att falla på plats för mig var att tänka på hur det tar vår missuppfattning av vad Jennifer menar,",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Nu kommer detta alltid att vara sant om v i sig är nollvektorn, men det är tråkigt.",
  "model": "google_nmt",
  "from_community_srt": "vektorn vi får om vi använder samma koordinater men i vårt system och sedan transformerar det till den vektorn hon faktiskt menar.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Vad vi vill ha är en egenvektor som inte är noll.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "Och om du tittar på kapitel 5 och 6, kommer du att veta att det enda sättet det är möjligt för produkten av en matris med en vektor som inte är noll att bli noll är om transformationen som är associerad med den matrisen klämmer ihop rymden till en lägre dimension.",
  "model": "google_nmt",
  "from_community_srt": "Åt andra hållet då? I exemplet jag använde tidigare i denna videon när jag har vektorn med koordinaterna [3,2] i vårt system., Hur räknade jag ut att det skulle ha koordinaterna i [5/3,1/3] i Jennifers system.",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "Och den squishifieringen motsvarar en nolldeterminant för matrisen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "För att vara konkret, låt oss säga att din matris A har kolumnerna 2, 1 och 2, 3, och tänk på att subtrahera en variabel mängd, lambda, från varje diagonal inmatning.",
  "model": "google_nmt",
  "from_community_srt": "Du startar med den basbytesmatrisen som översätter Jennifers språk till vårt och sedan tar du dess invers. Kom ihåg,",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Föreställ dig nu att du justerar lambda, vrider på en ratt för att ändra dess värde.",
  "model": "google_nmt",
  "from_community_srt": "inversen av en transformation är en ny transformation som motsvarar att spela den första baklänges.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "När värdet på lambda ändras ändras själva matrisen, och så ändras matrisens determinant.",
  "model": "google_nmt",
  "from_community_srt": "I praktiken, speciellt om du arbetar i mer än två dimensioner skulle du använda  en dator till att beräkna matrisen som faktiskt representerar denna invers.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Målet här är att hitta ett värde på lambda som kommer att göra denna determinant till noll, vilket betyder att den justerade transformationen klämmer ihop rymden till en lägre dimension.",
  "model": "google_nmt",
  "from_community_srt": "I det här fallet visar det sig att inversen av basbytesmatrisen som har Jennifers bas som sina kolonner har kolonnerna [1/3,",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "I det här fallet kommer sweet spot när lambda är lika med 1.",
  "model": "google_nmt",
  "from_community_srt": "-1/3] och [1/3,2/3]. Så,",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Naturligtvis, om vi hade valt någon annan matris, kanske egenvärdet inte nödvändigtvis är 1.",
  "model": "google_nmt",
  "from_community_srt": "till exempel, för att se hur vektorn [3,2] ser ut i Jennifers system",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Sweet spot kan drabbas av något annat värde av lambda.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Så det här är ganska mycket, men låt oss reda ut vad det här säger.",
  "model": "google_nmt",
  "from_community_srt": "multiplicerar vi inversen av basbytesmatrisen med vektorn [3,2] vilket visar sig bli [5/3,1/3].",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "När lambda är lika med 1, pressar matrisen A minus lambda gånger identiteten utrymme på en linje.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Det betyder att det finns en vektor v som inte är noll så att A minus lambda gånger identiteten gånger v är lika med nollvektorn.",
  "model": "google_nmt",
  "from_community_srt": "Det är i ett nötskal hur man översätter beskrivningen av individuella vektorer fram och tillbaka mellan koordinatsystem. Matrisen vars kolonner representerar Jennifers basvektorer men är skriven i våra koordinater",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "Och kom ihåg, anledningen till att vi bryr oss om det är för att det betyder att A gånger v är lika med lambda gånger v, vilket du kan läsa som att vektorn v är en egenvektor till A, som stannar på sitt eget span under transformationen A.",
  "model": "google_nmt",
  "from_community_srt": "översätter vektorer från hennes språk till vårt språk. Och matrisens inverse gör det motsatta. Men vektorer är inte det enda vi beskriver med hjälp av koordinater.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "I det här exemplet är motsvarande egenvärde 1, så v skulle faktiskt bara stanna kvar på plats.",
  "model": "google_nmt",
  "from_community_srt": "För nästa del är det viktigt att du är bekväm med att representera transformationer med matriser och att du vet hur matrismultiplikation korresponderar mot att sammansätta påföljande transformationer.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Pausa och fundera över om du behöver se till att det resonemanget känns bra.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Det är sånt jag nämnde i inledningen.",
  "model": "google_nmt",
  "from_community_srt": "Du borde absolut pausa och ta en titt på kapitel 3 och 4 om något av detta känns svårt.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Om du inte hade ett gediget grepp om determinanter och varför de relaterar till linjära ekvationssystem som har lösningar som inte är noll, skulle ett uttryck som detta kännas helt ur det blå.",
  "model": "google_nmt",
  "from_community_srt": "Betrakta en linjär transformation så som en 90-gradig moturs rotation. När du och jag representerar detta med matrisen följer vi var basvektorerna i-hatt och j-hatt hamnar.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "För att se detta i praktiken, låt oss återgå till exemplet från början, med en matris vars kolumner är 3, 0 och 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "i-hatt hamnar på punkten med koordinaterna [0,1] och j-hatt hamnar på punkten med koordinaterna [-1,0].",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "För att ta reda på om ett värde lambda är ett egenvärde, subtrahera det från diagonalerna i denna matris och beräkna determinanten.",
  "model": "google_nmt",
  "from_community_srt": "Så de koordinaterna blir kolonnerna i vår matris men denna representation är starkt knuten till vårt val av basvektorer",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Genom att göra detta får vi ett visst kvadratiskt polynom i lambda, 3 minus lambda gånger 2 minus lambda.",
  "model": "google_nmt",
  "from_community_srt": "från det faktum att vi följer i-hatt och j-hatt till att börja med till det faktum att vi ser var de landar i vårt eget koordinatsystem. Hur skulle Jennifer beskriva samma 90-gradiga rotation av rummet? Du skulle kunna frestat att bara",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Eftersom lambda bara kan vara ett egenvärde om denna determinant råkar vara noll, kan du dra slutsatsen att de enda möjliga egenvärdena är lambda lika med 2 och lambda lika med 3.",
  "model": "google_nmt",
  "from_community_srt": "översätta kolonnerna av vår rotationsmatris till Jennifers språk. Men det är inte helt rätt.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "För att ta reda på vilka egenvektorer det är som faktiskt har ett av dessa egenvärden, säg att lambda är lika med 2, koppla in det värdet på lambda till matrisen och lös sedan för vilka vektorer denna diagonalt förändrade matris skickar till noll.",
  "model": "google_nmt",
  "from_community_srt": "De kolonnerna representerar var våra basvektorer i-hatt och j-hatt hamnar. Men matrisen Jennifer vill ha borde representera var hennes basvektorer landar och den behöver beskriva dessa landningspunkter i hennes språk. Här är ett vanligt sätt att tänka på hur detta görs.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Om du beräknade detta på samma sätt som du skulle göra med vilket annat linjärt system som helst, skulle du se att lösningarna är alla vektorer på den diagonala linjen som sträcks av negativ 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Starta med en godtycklig vektor skriven i Jennifers språk. Snarare än att försöka följa vad som händer med den i termer av hennes språk kommer vi först att översätta den till vårt språk",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Detta motsvarar det faktum att den oförändrade matrisen, 3, 0, 1, 2, har effekten att sträcka ut alla dessa vektorer med en faktor 2.",
  "model": "google_nmt",
  "from_community_srt": "genom att använda basbytesmatrisen den vars kolonner representerar hennes basvektorer i vårt språk. Detta ger oss samma vektor men nu skriven i vårt språk.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Nu behöver en 2D-transformation inte ha egenvektorer.",
  "model": "google_nmt",
  "from_community_srt": "Sedan applicerar vi transformationsmatrisen till vad du får genom att multiplicera den från vänster.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Tänk till exempel en rotation med 90 grader.",
  "model": "google_nmt",
  "from_community_srt": "Detta berättar var vektorn landar men fortfarande i vårt språk.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Detta har inga egenvektorer eftersom det roterar varje vektor från sitt eget span.",
  "model": "google_nmt",
  "from_community_srt": "Så som ett sista steg, applicera basbytesmatrisens invers,",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Om du faktiskt försöker beräkna egenvärdena för en rotation som denna, lägg märke till vad som händer.",
  "model": "google_nmt",
  "from_community_srt": "multiplicerad från vänster som vanligt, för att få den transformerade vektorn men nu i Jennifers språk.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Dess matris har kolumnerna 0, 1 och negativa 1, 0.",
  "model": "google_nmt",
  "from_community_srt": "Eftersom vi kunde göra detta med vilken vektor som helst skriven i hennes språk Applicera först basbytesmatrisen",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Subtrahera lambda från de diagonala elementen och leta efter när determinanten är noll.",
  "model": "google_nmt",
  "from_community_srt": "sedan transformationen sedan inversen av basbytesmatrisen.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "I det här fallet får du polynomet lambda i kvadrat plus 1.",
  "model": "google_nmt",
  "from_community_srt": "Denna komposition av tre matriser ger oss transformationsmatrisen i Jennifers språk.",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "De enda rötterna till det polynomet är de imaginära talen, i och negativa i.",
  "model": "google_nmt",
  "from_community_srt": "Den tar in en vektor i hennes språk och spottar ut den transformerade versionen av vektorn i hennes språk.",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Det faktum att det inte finns några reella tallösningar tyder på att det inte finns några egenvektorer.",
  "model": "google_nmt",
  "from_community_srt": "För detta specifika exempel när Jennifers basvektorer är [2,1] och [-1,1] i vårt språk och när transformationen är en 90-gradig rotation",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Ett annat ganska intressant exempel värt att ha i bakhuvudet är en sax.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Detta fixerar i-hat på plats och flyttar j-hat 1 över, så dess matris har kolumnerna 1, 0 och 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "har produkten av dessa tre matrisen om du arbetar genom det kolonnerna [1/3,5/3] och [-2/3,-1/3]-",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Alla vektorer på x-axeln är egenvektorer med egenvärde 1 eftersom de förblir fixerade.",
  "model": "google_nmt",
  "from_community_srt": "Så om Jennifer multiplicerar den matrisen med koordinaterna till en vektor i hennes system kommer den att returnera den 90-gradiga roterande versionen av den vektorn",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "I själva verket är dessa de enda egenvektorerna.",
  "model": "google_nmt",
  "from_community_srt": "uttryckt i hennes koordinatsystem.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "När du subtraherar lambda från diagonalerna och beräknar determinanten, får du 1 minus lambda i kvadrat.",
  "model": "google_nmt",
  "from_community_srt": "Generellt sett, när du ser ett uttryck som A^(-1) M A implicerar det en viss matematisk empati.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "Och den enda roten till detta uttryck är lambda lika med 1.",
  "model": "google_nmt",
  "from_community_srt": "Mittenmatrisen representerar någon typ av transformation,",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Detta stämmer överens med vad vi ser geometriskt, att alla egenvektorer har egenvärde 1.",
  "model": "google_nmt",
  "from_community_srt": "som du ser den och de två yttre matriserna representerar empatin, skiftet av perspektiv och hela matrisen representerar samma transformation men som någon annan ser den.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Kom dock ihåg att det också är möjligt att ha bara ett egenvärde, men med mer än bara en linje full av egenvektorer.",
  "model": "google_nmt",
  "from_community_srt": "För alla er som undrar varför vi bryr oss om alternativa koordinatsystem, kommer nästa video om egenvektorer och egenvärden",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Ett enkelt exempel är en matris som skalar allt med 2.",
  "model": "google_nmt",
  "from_community_srt": "ge ett riktigt viktigt exempel på detta.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "Det enda egenvärdet är 2, men varje vektor i planet får vara en egenvektor med det egenvärdet.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Nu är ytterligare ett bra tillfälle att pausa och fundera över en del av detta innan jag går vidare till det sista ämnet.",
  "model": "google_nmt",
  "from_community_srt": "Vi ses då!",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Jag vill avsluta här med idén om en egenbas, som är mycket beroende av idéer från den senaste videon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Ta en titt på vad som händer om våra basvektorer bara råkar vara egenvektorer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Till exempel kanske i-hat skalas med negativ 1 och j-hat skalas med 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Genom att skriva sina nya koordinater som kolumnerna i en matris, lägg märke till att dessa skalära multipler, negativ 1 och 2, som är egenvärdena för i-hat och j-hat, sitter på diagonalen av vår matris, och varannan post är en 0 .",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Varje gång en matris har nollor överallt förutom diagonalen, kallas den, rimligen nog, en diagonal matris.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "Och sättet att tolka detta är att alla basvektorer är egenvektorer, där de diagonala ingångarna i denna matris är deras egenvärden.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Det finns många saker som gör diagonala matriser mycket trevligare att arbeta med.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "En stor sak är att det är lättare att beräkna vad som kommer att hända om du multiplicerar den här matrisen med sig själv en hel massa gånger.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Eftersom allt en av dessa matriser gör är att skala varje basvektor med något egenvärde, att tillämpa den matrisen många gånger, säg 100 gånger, kommer bara att motsvara att skala varje basvektor med 100:e potensen av motsvarande egenvärde.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Försök däremot att beräkna 100:e potensen av en icke-diagonal matris.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Verkligen, prova det ett ögonblick.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Det är en mardröm.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Naturligtvis kommer du sällan att ha så tur att dina basvektorer också är egenvektorer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Men om din transformation har många egenvektorer, som den från början av den här videon, tillräckligt så att du kan välja en uppsättning som spänner över hela rymden, då kan du ändra ditt koordinatsystem så att dessa egenvektorer är dina basvektorer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Jag pratade om ändring av grund förra videon, men jag ska gå igenom en supersnabb påminnelse här om hur man uttrycker en transformation som för närvarande är skriven i vårt koordinatsystem till ett annat system.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Ta koordinaterna för vektorerna som du vill använda som en ny bas, vilket i det här fallet betyder våra två egenvektorer, gör sedan dessa koordinater till kolumnerna i en matris, känd som förändring av basmatrisen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "När du lägger in den ursprungliga transformationen, placerar förändringen av basmatrisen till höger och inversen av förändringen av basmatrisen till vänster, blir resultatet en matris som representerar samma transformation, men ur perspektivet av de nya basvektorernas koordinater systemet.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Hela poängen med att göra det här med egenvektorer är att denna nya matris garanterat är diagonal med dess motsvarande egenvärden nedåt den diagonalen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Detta beror på att det representerar att arbeta i ett koordinatsystem där det som händer med basvektorerna är att de skalas under transformationen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "En uppsättning basvektorer som också är egenvektorer kallas, återigen, rimligen nog, en egenbas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Så om du till exempel behövde beräkna den 100:e potensen av denna matris, skulle det vara mycket lättare att ändra till en egenbas, beräkna den 100:e potensen i det systemet och sedan konvertera tillbaka till vårt standardsystem.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Du kan inte göra det här med alla transformationer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "En skjuvning, till exempel, har inte tillräckligt med egenvektorer för att spänna över hela utrymmet.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Men om du kan hitta en egenbas gör det matrisoperationer riktigt härliga.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "För de av er som är villiga att arbeta igenom ett ganska snyggt pussel för att se hur det här ser ut i aktion och hur det kan användas för att ge några överraskande resultat, lämnar jag en uppmaning här på skärmen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Det kräver lite arbete, men jag tror att du kommer att trivas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Nästa och sista video i den här serien kommer att vara på abstrakta vektorutrymmen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]