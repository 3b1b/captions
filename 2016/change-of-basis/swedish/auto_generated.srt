1
00:00:19,920 --> 00:00:22,840
Egenvektorer och egenvärden är ett av de ämnen 

2
00:00:22,840 --> 00:00:25,760
som många elever tycker är särskilt ointuitiva.

3
00:00:25,760 --> 00:00:29,454
Frågor som varför gör vi det här och vad betyder detta egentligen, 

4
00:00:29,454 --> 00:00:33,260
lämnas alltför ofta bara flytande i ett obesvarat hav av beräkningar.

5
00:00:33,920 --> 00:00:36,189
Och när jag har lagt ut videorna i den här serien, 

6
00:00:36,189 --> 00:00:40,060
har många av er kommenterat om att se fram emot att visualisera detta ämne i synnerhet.

7
00:00:40,680 --> 00:00:43,543
Jag misstänker att orsaken till detta inte är så mycket att 

8
00:00:43,543 --> 00:00:46,360
egensaker är särskilt komplicerade eller dåligt förklarade.

9
00:00:46,860 --> 00:00:48,982
Faktum är att det är relativt okomplicerat, och jag tror 

10
00:00:48,982 --> 00:00:51,180
att de flesta böcker gör ett bra jobb med att förklara det.

11
00:00:51,520 --> 00:00:54,969
Problemet är att det bara är vettigt om du har en solid 

12
00:00:54,969 --> 00:00:58,480
visuell förståelse för många av de ämnen som föregår det.

13
00:00:59,060 --> 00:01:03,870
Viktigast här är att du vet hur du tänker på matriser som linjära transformationer, 

14
00:01:03,870 --> 00:01:07,305
men du måste också vara bekväm med saker som determinanter, 

15
00:01:07,305 --> 00:01:09,940
linjära ekvationssystem och förändring av bas.

16
00:01:10,720 --> 00:01:15,011
Förvirring om egenmaterial har vanligtvis mer att göra med en skakig 

17
00:01:15,011 --> 00:01:19,240
grund i ett av dessa ämnen än med egenvektorer och egenvärden i sig.

18
00:01:19,980 --> 00:01:23,702
Till att börja med, överväg en linjär transformation i två dimensioner, 

19
00:01:23,702 --> 00:01:24,840
som den som visas här.

20
00:01:25,460 --> 00:01:31,040
Den flyttar basvektorn i-hat till koordinaterna 3, 0 och j-hat till 1, 2.

21
00:01:31,780 --> 00:01:35,640
Så det representeras med en matris vars kolumner är 3, 0 och 1, 2.

22
00:01:36,600 --> 00:01:41,339
Fokusera på vad den gör med en viss vektor och tänk på spännvidden för den vektorn, 

23
00:01:41,339 --> 00:01:44,160
linjen som går genom dess ursprung och dess spets.

24
00:01:44,920 --> 00:01:48,380
De flesta vektorer kommer att slås ur sitt spann under transformationen.

25
00:01:48,780 --> 00:01:51,914
Jag menar, det skulle verka ganska tillfälligt om platsen 

26
00:01:51,914 --> 00:01:55,320
där vektorn landade också råkade vara någonstans på den linjen.

27
00:01:57,400 --> 00:02:00,382
Men vissa speciella vektorer förblir på sitt eget span, 

28
00:02:00,382 --> 00:02:03,418
vilket betyder att effekten som matrisen har på en sådan 

29
00:02:03,418 --> 00:02:07,040
vektor bara är att sträcka den eller klämma ihop den, som en skalär.

30
00:02:09,460 --> 00:02:14,100
För detta specifika exempel är basvektorn i-hat en sådan speciell vektor.

31
00:02:14,640 --> 00:02:19,290
Spännvidden för i-hat är x-axeln, och från den första kolumnen i matrisen kan 

32
00:02:19,290 --> 00:02:24,120
vi se att i-hat rör sig över till 3 gånger sig själv, fortfarande på den x-axeln.

33
00:02:26,320 --> 00:02:30,145
Dessutom, på grund av hur linjära transformationer fungerar, 

34
00:02:30,145 --> 00:02:34,159
sträcks alla andra vektorer på x-axeln bara ut med en faktor 3, 

35
00:02:34,159 --> 00:02:36,480
och förblir därför på sitt eget span.

36
00:02:38,500 --> 00:02:41,298
En något smygare vektor som förblir på sitt eget 

37
00:02:41,298 --> 00:02:44,040
span under denna transformation är negativ 1, 1.

38
00:02:44,660 --> 00:02:47,140
Det slutar med att det sträcks ut med en faktor 2.

39
00:02:49,000 --> 00:02:52,140
Och återigen, linearitet kommer att innebära att vilken annan 

40
00:02:52,140 --> 00:02:55,231
vektor som helst på den diagonala linjen som spänner över av 

41
00:02:55,231 --> 00:02:58,220
den här killen bara kommer att sträckas ut med en faktor 2.

42
00:02:59,820 --> 00:03:02,547
Och för denna transformation är det alla vektorer med den 

43
00:03:02,547 --> 00:03:05,180
här speciella egenskapen att hålla sig på sin spännvidd.

44
00:03:05,620 --> 00:03:08,800
De på x-axeln sträcks ut med en faktor 3, och de på 

45
00:03:08,800 --> 00:03:11,980
den här diagonala linjen sträcks ut med en faktor 2.

46
00:03:12,760 --> 00:03:16,262
Vilken annan vektor som helst kommer att roteras något under transformationen, 

47
00:03:16,262 --> 00:03:18,080
slås av linjen som den sträcker sig över.

48
00:03:22,520 --> 00:03:26,060
Som du kanske har gissat vid det här laget kallas dessa speciella 

49
00:03:26,060 --> 00:03:29,815
vektorer för transformationens egenvektorer, och varje egenvektor har 

50
00:03:29,815 --> 00:03:32,444
associerat med det vad som kallas ett egenvärde, 

51
00:03:32,444 --> 00:03:36,146
vilket bara är den faktor med vilken den sträcks ut eller kläms ihop 

52
00:03:36,146 --> 00:03:37,380
under transformationen.

53
00:03:40,280 --> 00:03:43,414
Naturligtvis finns det inget speciellt med stretching kontra squishing, 

54
00:03:43,414 --> 00:03:45,940
eller det faktum att dessa egenvärden råkar vara positiva.

55
00:03:46,380 --> 00:03:50,996
I ett annat exempel kan du ha en egenvektor med egenvärde negativt 1 halv, 

56
00:03:50,996 --> 00:03:55,120
vilket betyder att vektorn vänds och kläms med en faktor på 1 halv.

57
00:03:56,980 --> 00:03:59,870
Men den viktiga delen här är att den stannar på linjen 

58
00:03:59,870 --> 00:04:02,760
som den sträcker sig ut utan att roteras bort från den.

59
00:04:04,460 --> 00:04:07,790
För en glimt av varför detta kan vara en bra sak att tänka på, 

60
00:04:07,790 --> 00:04:09,800
överväg lite tredimensionell rotation.

61
00:04:11,660 --> 00:04:15,008
Om du kan hitta en egenvektor för den rotationen, 

62
00:04:15,008 --> 00:04:20,500
en vektor som stannar kvar på sitt eget span, är det du har hittat rotationsaxeln.

63
00:04:22,600 --> 00:04:26,433
Och det är mycket lättare att tänka på en 3D-rotation i termer av 

64
00:04:26,433 --> 00:04:29,802
någon rotationsaxel och en vinkel med vilken den roterar, 

65
00:04:29,802 --> 00:04:34,740
snarare än att tänka på hela 3x3-matrisen som är förknippad med den transformationen.

66
00:04:37,000 --> 00:04:40,207
I det här fallet måste förresten motsvarande egenvärde vara 1, 

67
00:04:40,207 --> 00:04:43,568
eftersom rotationer aldrig sträcker eller klämmer ihop någonting, 

68
00:04:43,568 --> 00:04:45,860
så längden på vektorn skulle förbli densamma.

69
00:04:48,080 --> 00:04:50,020
Detta mönster visar sig mycket i linjär algebra.

70
00:04:50,440 --> 00:04:54,869
Med vilken linjär transformation som helst som beskrivs av en matris kan du förstå vad 

71
00:04:54,869 --> 00:04:59,400
den gör genom att läsa av kolumnerna i denna matris som landningspunkter för basvektorer.

72
00:05:00,020 --> 00:05:03,412
Men ofta är ett bättre sätt att komma till kärnan i vad den 

73
00:05:03,412 --> 00:05:08,501
linjära transformationen faktiskt gör, mindre beroende av ditt specifika koordinatsystem, 

74
00:05:08,501 --> 00:05:10,820
att hitta egenvektorerna och egenvärdena.

75
00:05:15,460 --> 00:05:18,894
Jag kommer inte att täcka alla detaljer om metoder för att beräkna 

76
00:05:18,894 --> 00:05:22,380
egenvektorer och egenvärden här, men jag ska försöka ge en översikt 

77
00:05:22,380 --> 00:05:26,020
över de beräkningsidéer som är viktigast för en konceptuell förståelse.

78
00:05:27,180 --> 00:05:30,480
Symboliskt, så här ser idén med en egenvektor ut.

79
00:05:31,040 --> 00:05:34,653
A är matrisen som representerar någon transformation, 

80
00:05:34,653 --> 00:05:39,740
med v som egenvektor, och lambda är ett tal, nämligen motsvarande egenvärde.

81
00:05:40,680 --> 00:05:45,030
Vad detta uttryck säger är att matris-vektorprodukten, A gånger v, 

82
00:05:45,030 --> 00:05:49,900
ger samma resultat som att bara skala egenvektorn v med något värde lambda.

83
00:05:51,000 --> 00:05:55,317
Så att hitta egenvektorerna och deras egenvärden för en matris A 

84
00:05:55,317 --> 00:06:00,100
handlar om att hitta värdena på v och lambda som gör detta uttryck sant.

85
00:06:01,920 --> 00:06:04,491
Det är lite besvärligt att arbeta med till en början, 

86
00:06:04,491 --> 00:06:07,873
eftersom den vänstra sidan representerar matris-vektor multiplikation, 

87
00:06:07,873 --> 00:06:10,540
men den högra sidan här är skalär-vektor multiplikation.

88
00:06:11,120 --> 00:06:14,154
Så låt oss börja med att skriva om den högra sidan som någon 

89
00:06:14,154 --> 00:06:17,237
slags matris-vektormultiplikation, med hjälp av en matris som 

90
00:06:17,237 --> 00:06:20,620
har effekten att skala vilken vektor som helst med en faktor lambda.

91
00:06:21,680 --> 00:06:26,467
Kolumnerna i en sådan matris kommer att representera vad som händer med varje basvektor, 

92
00:06:26,467 --> 00:06:29,586
och varje basvektor multipliceras helt enkelt med lambda, 

93
00:06:29,586 --> 00:06:34,320
så denna matris kommer att ha talet lambda nedåt diagonalen, med nollor överallt annars.

94
00:06:36,180 --> 00:06:40,363
Det vanliga sättet att skriva den här killen är att faktorisera den lambdan och 

95
00:06:40,363 --> 00:06:44,860
skriva den som lambda gånger i, där i är identitetsmatrisen med 1:or nedåt diagonalen.

96
00:06:45,860 --> 00:06:48,971
När båda sidorna ser ut som matris-vektormultiplikation 

97
00:06:48,971 --> 00:06:51,860
kan vi subtrahera den högra sidan och faktorisera v.

98
00:06:54,160 --> 00:06:59,142
Så vad vi nu har är en ny matris, A minus lambda gånger identiteten, 

99
00:06:59,142 --> 00:07:04,920
och vi letar efter en vektor v så att denna nya matris gånger v ger nollvektorn.

100
00:07:06,380 --> 00:07:11,100
Nu kommer detta alltid att vara sant om v i sig är nollvektorn, men det är tråkigt.

101
00:07:11,340 --> 00:07:13,640
Vad vi vill ha är en egenvektor som inte är noll.

102
00:07:14,420 --> 00:07:18,695
Och om du tittar på kapitel 5 och 6, kommer du att veta att det enda sättet det är 

103
00:07:18,695 --> 00:07:23,074
möjligt för produkten av en matris med en vektor som inte är noll att bli noll är om 

104
00:07:23,074 --> 00:07:27,504
transformationen som är associerad med den matrisen klämmer ihop rymden till en lägre 

105
00:07:27,504 --> 00:07:28,020
dimension.

106
00:07:29,300 --> 00:07:34,220
Och den squishifieringen motsvarar en nolldeterminant för matrisen.

107
00:07:35,480 --> 00:07:40,439
För att vara konkret, låt oss säga att din matris A har kolumnerna 2, 1 och 2, 3, 

108
00:07:40,439 --> 00:07:45,520
och tänk på att subtrahera en variabel mängd, lambda, från varje diagonal inmatning.

109
00:07:46,480 --> 00:07:50,280
Föreställ dig nu att du justerar lambda, vrider på en ratt för att ändra dess värde.

110
00:07:50,940 --> 00:07:57,240
När värdet på lambda ändras ändras själva matrisen, och så ändras matrisens determinant.

111
00:07:58,220 --> 00:08:01,275
Målet här är att hitta ett värde på lambda som kommer att göra 

112
00:08:01,275 --> 00:08:04,281
denna determinant till noll, vilket betyder att den justerade 

113
00:08:04,281 --> 00:08:07,240
transformationen klämmer ihop rymden till en lägre dimension.

114
00:08:08,160 --> 00:08:11,160
I det här fallet kommer sweet spot när lambda är lika med 1.

115
00:08:12,180 --> 00:08:14,321
Naturligtvis, om vi hade valt någon annan matris, 

116
00:08:14,321 --> 00:08:16,120
kanske egenvärdet inte nödvändigtvis är 1.

117
00:08:16,240 --> 00:08:18,600
Sweet spot kan drabbas av något annat värde av lambda.

118
00:08:20,080 --> 00:08:22,960
Så det här är ganska mycket, men låt oss reda ut vad det här säger.

119
00:08:22,960 --> 00:08:26,430
När lambda är lika med 1, pressar matrisen A minus 

120
00:08:26,430 --> 00:08:29,560
lambda gånger identiteten utrymme på en linje.

121
00:08:30,440 --> 00:08:34,468
Det betyder att det finns en vektor v som inte är noll så att A 

122
00:08:34,468 --> 00:08:38,559
minus lambda gånger identiteten gånger v är lika med nollvektorn.

123
00:08:40,480 --> 00:08:45,915
Och kom ihåg, anledningen till att vi bryr oss om det är för att det betyder 

124
00:08:45,915 --> 00:08:51,491
att A gånger v är lika med lambda gånger v, vilket du kan läsa som att vektorn 

125
00:08:51,491 --> 00:08:57,280
v är en egenvektor till A, som stannar på sitt eget span under transformationen A.

126
00:08:58,320 --> 00:09:01,170
I det här exemplet är motsvarande egenvärde 1, 

127
00:09:01,170 --> 00:09:04,020
så v skulle faktiskt bara stanna kvar på plats.

128
00:09:06,220 --> 00:09:09,500
Pausa och fundera över om du behöver se till att det resonemanget känns bra.

129
00:09:13,380 --> 00:09:15,640
Det är sånt jag nämnde i inledningen.

130
00:09:16,220 --> 00:09:19,632
Om du inte hade ett gediget grepp om determinanter och varför de 

131
00:09:19,632 --> 00:09:23,569
relaterar till linjära ekvationssystem som har lösningar som inte är noll, 

132
00:09:23,569 --> 00:09:26,300
skulle ett uttryck som detta kännas helt ur det blå.

133
00:09:28,320 --> 00:09:32,147
För att se detta i praktiken, låt oss återgå till exemplet från början, 

134
00:09:32,147 --> 00:09:34,540
med en matris vars kolumner är 3, 0 och 1, 2.

135
00:09:35,350 --> 00:09:38,852
För att ta reda på om ett värde lambda är ett egenvärde, 

136
00:09:38,852 --> 00:09:43,400
subtrahera det från diagonalerna i denna matris och beräkna determinanten.

137
00:09:50,580 --> 00:09:54,556
Genom att göra detta får vi ett visst kvadratiskt polynom i lambda, 

138
00:09:54,556 --> 00:09:56,720
3 minus lambda gånger 2 minus lambda.

139
00:09:57,800 --> 00:10:02,857
Eftersom lambda bara kan vara ett egenvärde om denna determinant råkar vara noll, 

140
00:10:02,857 --> 00:10:08,161
kan du dra slutsatsen att de enda möjliga egenvärdena är lambda lika med 2 och lambda 

141
00:10:08,161 --> 00:10:08,840
lika med 3.

142
00:10:09,640 --> 00:10:14,662
För att ta reda på vilka egenvektorer det är som faktiskt har ett av dessa egenvärden, 

143
00:10:14,662 --> 00:10:19,454
säg att lambda är lika med 2, koppla in det värdet på lambda till matrisen och lös 

144
00:10:19,454 --> 00:10:23,900
sedan för vilka vektorer denna diagonalt förändrade matris skickar till noll.

145
00:10:24,940 --> 00:10:28,091
Om du beräknade detta på samma sätt som du skulle göra med vilket 

146
00:10:28,091 --> 00:10:31,148
annat linjärt system som helst, skulle du se att lösningarna är 

147
00:10:31,148 --> 00:10:34,300
alla vektorer på den diagonala linjen som sträcks av negativ 1, 1.

148
00:10:35,220 --> 00:10:39,309
Detta motsvarar det faktum att den oförändrade matrisen, 3, 0, 1, 

149
00:10:39,309 --> 00:10:43,460
2, har effekten att sträcka ut alla dessa vektorer med en faktor 2.

150
00:10:46,320 --> 00:10:50,200
Nu behöver en 2D-transformation inte ha egenvektorer.

151
00:10:50,720 --> 00:10:53,400
Tänk till exempel en rotation med 90 grader.

152
00:10:53,660 --> 00:10:58,200
Detta har inga egenvektorer eftersom det roterar varje vektor från sitt eget span.

153
00:11:00,800 --> 00:11:04,113
Om du faktiskt försöker beräkna egenvärdena för en rotation som denna, 

154
00:11:04,113 --> 00:11:05,560
lägg märke till vad som händer.

155
00:11:06,300 --> 00:11:10,140
Dess matris har kolumnerna 0, 1 och negativa 1, 0.

156
00:11:11,100 --> 00:11:15,800
Subtrahera lambda från de diagonala elementen och leta efter när determinanten är noll.

157
00:11:18,140 --> 00:11:21,940
I det här fallet får du polynomet lambda i kvadrat plus 1.

158
00:11:22,680 --> 00:11:27,920
De enda rötterna till det polynomet är de imaginära talen, i och negativa i.

159
00:11:28,840 --> 00:11:31,427
Det faktum att det inte finns några reella tallösningar 

160
00:11:31,427 --> 00:11:33,600
tyder på att det inte finns några egenvektorer.

161
00:11:35,540 --> 00:11:39,820
Ett annat ganska intressant exempel värt att ha i bakhuvudet är en sax.

162
00:11:40,560 --> 00:11:44,604
Detta fixerar i-hat på plats och flyttar j-hat 1 över, 

163
00:11:44,604 --> 00:11:47,840
så dess matris har kolumnerna 1, 0 och 1, 1.

164
00:11:48,740 --> 00:11:54,540
Alla vektorer på x-axeln är egenvektorer med egenvärde 1 eftersom de förblir fixerade.

165
00:11:55,680 --> 00:11:57,820
I själva verket är dessa de enda egenvektorerna.

166
00:11:58,760 --> 00:12:04,146
När du subtraherar lambda från diagonalerna och beräknar determinanten, 

167
00:12:04,146 --> 00:12:06,540
får du 1 minus lambda i kvadrat.

168
00:12:09,320 --> 00:12:12,860
Och den enda roten till detta uttryck är lambda lika med 1.

169
00:12:14,560 --> 00:12:19,720
Detta stämmer överens med vad vi ser geometriskt, att alla egenvektorer har egenvärde 1.

170
00:12:21,080 --> 00:12:25,028
Kom dock ihåg att det också är möjligt att ha bara ett egenvärde, 

171
00:12:25,028 --> 00:12:28,020
men med mer än bara en linje full av egenvektorer.

172
00:12:29,900 --> 00:12:33,180
Ett enkelt exempel är en matris som skalar allt med 2.

173
00:12:33,900 --> 00:12:37,661
Det enda egenvärdet är 2, men varje vektor i planet 

174
00:12:37,661 --> 00:12:40,700
får vara en egenvektor med det egenvärdet.

175
00:12:42,000 --> 00:12:44,377
Nu är ytterligare ett bra tillfälle att pausa och fundera 

176
00:12:44,377 --> 00:12:46,960
över en del av detta innan jag går vidare till det sista ämnet.

177
00:13:03,540 --> 00:13:06,615
Jag vill avsluta här med idén om en egenbas, som 

178
00:13:06,615 --> 00:13:09,880
är mycket beroende av idéer från den senaste videon.

179
00:13:11,480 --> 00:13:16,380
Ta en titt på vad som händer om våra basvektorer bara råkar vara egenvektorer.

180
00:13:17,120 --> 00:13:22,380
Till exempel kanske i-hat skalas med negativ 1 och j-hat skalas med 2.

181
00:13:23,420 --> 00:13:27,351
Genom att skriva sina nya koordinater som kolumnerna i en matris, 

182
00:13:27,351 --> 00:13:31,044
lägg märke till att dessa skalära multipler, negativ 1 och 2, 

183
00:13:31,044 --> 00:13:35,571
som är egenvärdena för i-hat och j-hat, sitter på diagonalen av vår matris, 

184
00:13:35,571 --> 00:13:37,180
och varannan post är en 0 .

185
00:13:38,880 --> 00:13:42,643
Varje gång en matris har nollor överallt förutom diagonalen, 

186
00:13:42,643 --> 00:13:45,420
kallas den, rimligen nog, en diagonal matris.

187
00:13:45,840 --> 00:13:50,283
Och sättet att tolka detta är att alla basvektorer är egenvektorer, 

188
00:13:50,283 --> 00:13:54,400
där de diagonala ingångarna i denna matris är deras egenvärden.

189
00:13:57,100 --> 00:14:01,060
Det finns många saker som gör diagonala matriser mycket trevligare att arbeta med.

190
00:14:01,780 --> 00:14:05,060
En stor sak är att det är lättare att beräkna vad som kommer att hända 

191
00:14:05,060 --> 00:14:08,340
om du multiplicerar den här matrisen med sig själv en hel massa gånger.

192
00:14:09,420 --> 00:14:15,049
Eftersom allt en av dessa matriser gör är att skala varje basvektor med något egenvärde, 

193
00:14:15,049 --> 00:14:18,591
att tillämpa den matrisen många gånger, säg 100 gånger, 

194
00:14:18,591 --> 00:14:23,967
kommer bara att motsvara att skala varje basvektor med 100:e potensen av motsvarande 

195
00:14:23,967 --> 00:14:24,600
egenvärde.

196
00:14:25,700 --> 00:14:29,680
Försök däremot att beräkna 100:e potensen av en icke-diagonal matris.

197
00:14:29,680 --> 00:14:31,320
Verkligen, prova det ett ögonblick.

198
00:14:31,740 --> 00:14:32,440
Det är en mardröm.

199
00:14:36,080 --> 00:14:41,260
Naturligtvis kommer du sällan att ha så tur att dina basvektorer också är egenvektorer.

200
00:14:42,040 --> 00:14:47,202
Men om din transformation har många egenvektorer, som den från början av den här videon, 

201
00:14:47,202 --> 00:14:51,726
tillräckligt så att du kan välja en uppsättning som spänner över hela rymden, 

202
00:14:51,726 --> 00:14:56,540
då kan du ändra ditt koordinatsystem så att dessa egenvektorer är dina basvektorer.

203
00:14:57,140 --> 00:15:00,471
Jag pratade om ändring av grund förra videon, men jag ska gå igenom en 

204
00:15:00,471 --> 00:15:03,708
supersnabb påminnelse här om hur man uttrycker en transformation som 

205
00:15:03,708 --> 00:15:07,040
för närvarande är skriven i vårt koordinatsystem till ett annat system.

206
00:15:08,440 --> 00:15:11,883
Ta koordinaterna för vektorerna som du vill använda som en ny bas, 

207
00:15:11,883 --> 00:15:14,711
vilket i det här fallet betyder våra två egenvektorer, 

208
00:15:14,711 --> 00:15:17,640
gör sedan dessa koordinater till kolumnerna i en matris, 

209
00:15:17,640 --> 00:15:19,440
känd som förändring av basmatrisen.

210
00:15:20,180 --> 00:15:23,106
När du lägger in den ursprungliga transformationen, 

211
00:15:23,106 --> 00:15:27,439
placerar förändringen av basmatrisen till höger och inversen av förändringen 

212
00:15:27,439 --> 00:15:31,547
av basmatrisen till vänster, blir resultatet en matris som representerar 

213
00:15:31,547 --> 00:15:36,500
samma transformation, men ur perspektivet av de nya basvektorernas koordinater systemet.

214
00:15:37,440 --> 00:15:42,029
Hela poängen med att göra det här med egenvektorer är att denna nya matris 

215
00:15:42,029 --> 00:15:46,680
garanterat är diagonal med dess motsvarande egenvärden nedåt den diagonalen.

216
00:15:46,860 --> 00:15:50,615
Detta beror på att det representerar att arbeta i ett koordinatsystem där 

217
00:15:50,615 --> 00:15:54,320
det som händer med basvektorerna är att de skalas under transformationen.

218
00:15:55,800 --> 00:15:59,459
En uppsättning basvektorer som också är egenvektorer kallas, 

219
00:15:59,459 --> 00:16:01,560
återigen, rimligen nog, en egenbas.

220
00:16:02,340 --> 00:16:06,650
Så om du till exempel behövde beräkna den 100:e potensen av denna matris, 

221
00:16:06,650 --> 00:16:10,029
skulle det vara mycket lättare att ändra till en egenbas, 

222
00:16:10,029 --> 00:16:14,514
beräkna den 100:e potensen i det systemet och sedan konvertera tillbaka till 

223
00:16:14,514 --> 00:16:15,680
vårt standardsystem.

224
00:16:16,620 --> 00:16:18,320
Du kan inte göra det här med alla transformationer.

225
00:16:18,320 --> 00:16:20,626
En skjuvning, till exempel, har inte tillräckligt 

226
00:16:20,626 --> 00:16:22,980
med egenvektorer för att spänna över hela utrymmet.

227
00:16:23,460 --> 00:16:28,160
Men om du kan hitta en egenbas gör det matrisoperationer riktigt härliga.

228
00:16:29,120 --> 00:16:31,840
För de av er som är villiga att arbeta igenom ett ganska snyggt pussel 

229
00:16:31,840 --> 00:16:34,599
för att se hur det här ser ut i aktion och hur det kan användas för att 

230
00:16:34,599 --> 00:16:37,320
ge några överraskande resultat, lämnar jag en uppmaning här på skärmen.

231
00:16:37,600 --> 00:16:40,280
Det kräver lite arbete, men jag tror att du kommer att trivas.

232
00:16:40,840 --> 00:16:46,120
Nästa och sista video i den här serien kommer att vara på abstrakta vektorutrymmen.

