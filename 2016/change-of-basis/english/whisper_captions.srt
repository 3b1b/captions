1
00:00:19,920 --> 00:00:24,640
Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly

2
00:00:25,200 --> 00:00:25,760
unintuitive.

3
00:00:25,760 --> 00:00:30,100
Questions like, why are we doing this and what does this actually mean, are too often

4
00:00:30,100 --> 00:00:33,260
left just floating away in an unanswered sea of computations.

5
00:00:33,920 --> 00:00:37,360
And as I've put out the videos of this series, a lot of you have commented about looking

6
00:00:37,360 --> 00:00:40,060
forward to visualizing this topic in particular.

7
00:00:40,680 --> 00:00:44,620
I suspect that the reason for this is not so much that eigenthings are particularly

8
00:00:44,620 --> 00:00:46,360
complicated or poorly explained.

9
00:00:46,860 --> 00:00:50,840
In fact, it's comparatively straightforward, and I think most books do a fine job explaining

10
00:00:50,840 --> 00:00:51,180
it.

11
00:00:51,520 --> 00:00:56,700
The issue is that it only really makes sense if you have a solid visual understanding for

12
00:00:56,700 --> 00:00:58,480
many of the topics that precede it.

13
00:00:59,060 --> 00:01:03,760
Most important here is that you know how to think about matrices as linear transformations,

14
00:01:04,040 --> 00:01:08,560
but you also need to be comfortable with things like determinants, linear systems of equations,

15
00:01:08,980 --> 00:01:09,940
and change of basis.

16
00:01:10,720 --> 00:01:15,640
Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these

17
00:01:15,640 --> 00:01:19,240
topics than it does with eigenvectors and eigenvalues themselves.

18
00:01:19,980 --> 00:01:24,840
To start, consider some linear transformation in two dimensions, like the one shown here.

19
00:01:25,460 --> 00:01:31,040
It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.

20
00:01:31,780 --> 00:01:35,640
So it's represented with a matrix whose columns are 3, 0, and 1, 2.

21
00:01:36,600 --> 00:01:41,620
Focus in on what it does to one particular vector, and think about the span of that vector,

22
00:01:41,900 --> 00:01:44,160
the line passing through its origin and its tip.

23
00:01:44,920 --> 00:01:48,380
Most vectors are going to get knocked off their span during the transformation.

24
00:01:48,780 --> 00:01:53,160
I mean, it would seem pretty coincidental if the place where the vector landed also

25
00:01:53,160 --> 00:01:55,320
happened to be somewhere on that line.

26
00:01:57,400 --> 00:02:02,340
But some special vectors do remain on their own span, meaning the effect that the matrix

27
00:02:02,340 --> 00:02:07,040
has on such a vector is just to stretch it or squish it, like a scalar.

28
00:02:09,460 --> 00:02:14,100
For this specific example, the basis vector i-hat is one such special vector.

29
00:02:14,640 --> 00:02:20,020
The span of i-hat is the x-axis, and from the first column of the matrix, we can see

30
00:02:20,020 --> 00:02:24,120
that i-hat moves over to 3 times itself, still on that x-axis.

31
00:02:26,320 --> 00:02:32,540
What's more, because of the way linear transformations work, any other vector on the x-axis is also

32
00:02:32,540 --> 00:02:36,480
just stretched by a factor of 3, and hence remains on its own span.

33
00:02:38,500 --> 00:02:42,820
A slightly sneakier vector that remains on its own span during this transformation is

34
00:02:42,820 --> 00:02:44,040
negative 1, 1.

35
00:02:44,660 --> 00:02:47,140
It ends up getting stretched by a factor of 2.

36
00:02:49,000 --> 00:02:54,740
And again, linearity is going to imply that any other vector on the diagonal line spanned

37
00:02:54,740 --> 00:02:58,220
by this guy is just going to get stretched out by a factor of 2.

38
00:02:59,820 --> 00:03:04,260
And for this transformation, those are all the vectors with this special property of

39
00:03:04,260 --> 00:03:05,180
staying on their span.

40
00:03:05,620 --> 00:03:09,980
Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal

41
00:03:09,980 --> 00:03:11,980
line getting stretched by a factor of 2.

42
00:03:12,760 --> 00:03:17,100
Any other vector is going to get rotated somewhat during the transformation, knocked off the

43
00:03:17,100 --> 00:03:18,080
line that it spans.

44
00:03:22,520 --> 00:03:27,480
As you might have guessed by now, these special vectors are called the eigenvectors of the

45
00:03:27,480 --> 00:03:33,460
transformation, and each eigenvector has associated with it what's called an eigenvalue, which

46
00:03:33,460 --> 00:03:37,380
is just the factor by which it's stretched or squished during the transformation.

47
00:03:40,280 --> 00:03:44,920
Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues

48
00:03:44,920 --> 00:03:45,940
happen to be positive.

49
00:03:46,380 --> 00:03:51,760
In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that

50
00:03:51,760 --> 00:03:55,120
the vector gets flipped and squished by a factor of 1 half.

51
00:03:56,980 --> 00:04:01,620
But the important part here is that it stays on the line that it spans out without getting

52
00:04:01,620 --> 00:04:02,760
rotated off of it.

53
00:04:04,460 --> 00:04:09,160
For a glimpse of why this might be a useful thing to think about, consider some three-dimensional

54
00:04:09,540 --> 00:04:09,800
rotation.

55
00:04:11,660 --> 00:04:17,800
If you can find an eigenvector for that rotation, a vector that remains on its own span, what

56
00:04:17,800 --> 00:04:20,500
you have found is the axis of rotation.

57
00:04:22,600 --> 00:04:28,540
And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle

58
00:04:28,540 --> 00:04:33,860
by which it's rotating, rather than thinking about the full 3x3 matrix associated with

59
00:04:33,860 --> 00:04:34,740
that transformation.

60
00:04:37,000 --> 00:04:41,980
In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations

61
00:04:41,980 --> 00:04:45,860
never stretch or squish anything, so the length of the vector would remain the same.

62
00:04:48,080 --> 00:04:50,020
This pattern shows up a lot in linear algebra.

63
00:04:50,440 --> 00:04:55,220
With any linear transformation described by a matrix, you could understand what it's doing

64
00:04:55,220 --> 00:04:59,400
by reading off the columns of this matrix as the landing spots for basis vectors.

65
00:05:00,020 --> 00:05:04,100
But often, a better way to get at the heart of what the linear transformation actually

66
00:05:04,100 --> 00:05:10,820
does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.

67
00:05:15,460 --> 00:05:19,680
I won't cover the full details on methods for computing eigenvectors and eigenvalues

68
00:05:19,680 --> 00:05:24,540
here, but I'll try to give an overview of the computational ideas that are most important

69
00:05:24,540 --> 00:05:26,020
for a conceptual understanding.

70
00:05:27,180 --> 00:05:30,480
Symbolically, here's what the idea of an eigenvector looks like.

71
00:05:31,040 --> 00:05:37,320
A is the matrix representing some transformation, with v as the eigenvector, and lambda is a

72
00:05:37,320 --> 00:05:39,740
number, namely the corresponding eigenvalue.

73
00:05:40,680 --> 00:05:45,580
What this expression is saying is that the matrix-vector product, A times v, gives the

74
00:05:45,580 --> 00:05:49,900
same result as just scaling the eigenvector v by some value lambda.

75
00:05:51,000 --> 00:05:56,920
So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values

76
00:05:56,920 --> 00:06:00,100
of v and lambda that make this expression true.

77
00:06:01,920 --> 00:06:06,340
It's a little awkward to work with at first, because that left-hand side represents matrix-vector

78
00:06:06,340 --> 00:06:10,540
multiplication, but the right-hand side here is scalar-vector multiplication.

79
00:06:11,120 --> 00:06:15,980
So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication,

80
00:06:15,980 --> 00:06:20,620
using a matrix which has the effect of scaling any vector by a factor of lambda.

81
00:06:21,680 --> 00:06:26,300
The columns of such a matrix will represent what happens to each basis vector, and each

82
00:06:26,300 --> 00:06:31,280
basis vector is simply multiplied by lambda, so this matrix will have the number lambda

83
00:06:31,280 --> 00:06:34,320
down the diagonal, with zeros everywhere else.

84
00:06:36,180 --> 00:06:40,920
The common way to write this guy is to factor that lambda out and write it as lambda times

85
00:06:40,920 --> 00:06:44,860
i, where i is the identity matrix with 1s down the diagonal.

86
00:06:45,860 --> 00:06:50,280
With both sides looking like matrix-vector multiplication, we can subtract off that right-hand

87
00:06:50,280 --> 00:06:51,860
side and factor out the v.

88
00:06:54,160 --> 00:06:59,360
So what we now have is a new matrix, A minus lambda times the identity, and we're looking

89
00:06:59,360 --> 00:07:04,920
for a vector v such that this new matrix times v gives the zero vector.

90
00:07:06,380 --> 00:07:11,100
Now, this will always be true if v itself is the zero vector, but that's boring.

91
00:07:11,340 --> 00:07:13,640
What we want is a non-zero eigenvector.

92
00:07:14,420 --> 00:07:18,860
And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product

93
00:07:18,860 --> 00:07:25,020
of a matrix with a non-zero vector to become zero is if the transformation associated with

94
00:07:25,020 --> 00:07:28,020
that matrix squishes space into a lower dimension.

95
00:07:29,300 --> 00:07:34,220
And that squishification corresponds to a zero determinant for the matrix.

96
00:07:35,480 --> 00:07:41,800
To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting

97
00:07:41,800 --> 00:07:45,520
off a variable amount, lambda, from each diagonal entry.

98
00:07:46,480 --> 00:07:50,280
Now imagine tweaking lambda, turning a knob to change its value.

99
00:07:50,940 --> 00:07:56,340
As that value of lambda changes, the matrix itself changes, and so the determinant of

100
00:07:56,340 --> 00:07:57,240
the matrix changes.

101
00:07:58,220 --> 00:08:03,580
The goal here is to find a value of lambda that will make this determinant zero, meaning

102
00:08:03,580 --> 00:08:07,240
the tweaked transformation squishes space into a lower dimension.

103
00:08:08,160 --> 00:08:11,160
In this case, the sweet spot comes when lambda equals 1.

104
00:08:12,180 --> 00:08:16,120
Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.

105
00:08:16,240 --> 00:08:18,600
The sweet spot might be hit at some other value of lambda.

106
00:08:20,080 --> 00:08:22,960
So this is kind of a lot, but let's unravel what this is saying.

107
00:08:22,960 --> 00:08:29,560
When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.

108
00:08:30,440 --> 00:08:36,400
That means there's a non-zero vector v such that A minus lambda times the identity times

109
00:08:36,400 --> 00:08:38,560
v equals the zero vector.

110
00:08:40,480 --> 00:08:46,160
And remember, the reason we care about that is because it means A times v equals lambda

111
00:08:46,160 --> 00:08:53,420
times v, which you can read off as saying that the vector v is an eigenvector of A,

112
00:08:53,840 --> 00:08:57,280
staying on its own span during the transformation A.

113
00:08:58,320 --> 00:09:03,420
In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed

114
00:09:03,420 --> 00:09:04,020
in place.

115
00:09:06,220 --> 00:09:09,500
Pause and ponder if you need to make sure that that line of reasoning feels good.

116
00:09:13,380 --> 00:09:15,640
This is the kind of thing I mentioned in the introduction.

117
00:09:16,220 --> 00:09:21,240
If you didn't have a solid grasp of determinants and why they relate to linear systems of equations

118
00:09:21,240 --> 00:09:26,300
having non-zero solutions, an expression like this would feel completely out of the blue.

119
00:09:28,320 --> 00:09:32,740
To see this in action, let's revisit the example from the start, with a matrix whose columns

120
00:09:32,740 --> 00:09:34,540
are 3, 0 and 1, 2.

121
00:09:35,350 --> 00:09:41,240
To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix

122
00:09:41,240 --> 00:09:43,400
and compute the determinant.

123
00:09:50,580 --> 00:09:56,720
Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.

124
00:09:57,800 --> 00:10:03,060
Since lambda can only be an eigenvalue if this determinant happens to be zero, you can

125
00:10:03,060 --> 00:10:08,840
conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.

126
00:10:09,640 --> 00:10:14,240
To figure out what the eigenvectors are that actually have one of these eigenvalues, say

127
00:10:14,240 --> 00:10:20,680
lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors

128
00:10:20,680 --> 00:10:23,900
this diagonally altered matrix sends to zero.

129
00:10:24,940 --> 00:10:29,580
If you computed this the way you would any other linear system, you'd see that the solutions

130
00:10:29,580 --> 00:10:34,300
are all the vectors on the diagonal line spanned by negative 1, 1.

131
00:10:35,220 --> 00:10:41,180
This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching

132
00:10:41,180 --> 00:10:43,460
all those vectors by a factor of 2.

133
00:10:46,320 --> 00:10:50,200
Now, a 2D transformation doesn't have to have eigenvectors.

134
00:10:50,720 --> 00:10:53,400
For example, consider a rotation by 90 degrees.

135
00:10:53,660 --> 00:10:58,200
This doesn't have any eigenvectors since it rotates every vector off of its own span.

136
00:11:00,800 --> 00:11:05,560
If you actually try computing the eigenvalues of a rotation like this, notice what happens.

137
00:11:06,300 --> 00:11:10,140
Its matrix has columns 0, 1 and negative 1, 0.

138
00:11:11,100 --> 00:11:15,800
Subtract off lambda from the diagonal elements and look for when the determinant is zero.

139
00:11:18,140 --> 00:11:21,940
In this case, you get the polynomial lambda squared plus 1.

140
00:11:22,680 --> 00:11:27,920
The only roots of that polynomial are the imaginary numbers, i and negative i.

141
00:11:28,840 --> 00:11:33,600
The fact that there are no real number solutions indicates that there are no eigenvectors.

142
00:11:35,540 --> 00:11:39,820
Another pretty interesting example worth holding in the back of your mind is a shear.

143
00:11:40,560 --> 00:11:47,240
This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and

144
00:11:47,240 --> 00:11:47,840
1, 1.

145
00:11:48,740 --> 00:11:54,040
All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed

146
00:11:54,040 --> 00:11:54,540
in place.

147
00:11:55,680 --> 00:11:57,820
In fact, these are the only eigenvectors.

148
00:11:58,760 --> 00:12:04,020
When you subtract off lambda from the diagonals and compute the determinant, what you get

149
00:12:04,020 --> 00:12:06,540
is 1 minus lambda squared.

150
00:12:09,320 --> 00:12:12,860
And the only root of this expression is lambda equals 1.

151
00:12:14,560 --> 00:12:19,720
This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.

152
00:12:21,080 --> 00:12:26,280
Keep in mind though, it's also possible to have just one eigenvalue, but with more than

153
00:12:26,280 --> 00:12:28,020
just a line full of eigenvectors.

154
00:12:29,900 --> 00:12:33,180
A simple example is a matrix that scales everything by 2.

155
00:12:33,900 --> 00:12:39,600
The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with

156
00:12:39,600 --> 00:12:40,700
that eigenvalue.

157
00:12:42,000 --> 00:12:46,960
Now is another good time to pause and ponder some of this before I move on to the last topic.

158
00:13:03,540 --> 00:13:08,880
I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas

159
00:13:08,880 --> 00:13:09,880
from the last video.

160
00:13:11,480 --> 00:13:16,380
Take a look at what happens if our basis vectors just so happen to be eigenvectors.

161
00:13:17,120 --> 00:13:22,380
For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.

162
00:13:23,420 --> 00:13:28,340
Writing their new coordinates as the columns of a matrix, notice that those scalar multiples,

163
00:13:28,580 --> 00:13:34,560
negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of

164
00:13:34,560 --> 00:13:37,180
our matrix, and every other entry is a 0.

165
00:13:38,880 --> 00:13:43,780
Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably

166
00:13:43,780 --> 00:13:45,420
enough, a diagonal matrix.

167
00:13:45,840 --> 00:13:50,580
And the way to interpret this is that all the basis vectors are eigenvectors, with the

168
00:13:50,580 --> 00:13:54,400
diagonal entries of this matrix being their eigenvalues.

169
00:13:57,100 --> 00:14:01,060
There are a lot of things that make diagonal matrices much nicer to work with.

170
00:14:01,780 --> 00:14:06,340
One big one is that it's easier to compute what will happen if you multiply this matrix

171
00:14:06,340 --> 00:14:08,340
by itself a whole bunch of times.

172
00:14:09,420 --> 00:14:15,100
Since all one of these matrices does is scale each basis vector by some eigenvalue, applying

173
00:14:15,100 --> 00:14:20,560
that matrix many times, say 100 times, is just going to correspond to scaling each basis

174
00:14:20,560 --> 00:14:24,600
vector by the 100th power of the corresponding eigenvalue.

175
00:14:25,700 --> 00:14:29,680
In contrast, try computing the 100th power of a non-diagonal matrix.

176
00:14:29,680 --> 00:14:31,320
Really, try it for a moment.

177
00:14:31,740 --> 00:14:32,440
It's a nightmare.

178
00:14:36,080 --> 00:14:41,260
Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.

179
00:14:42,040 --> 00:14:46,340
But if your transformation has a lot of eigenvectors, like the one from the start of this video,

180
00:14:46,840 --> 00:14:52,320
enough so that you can choose a set that spans the full space, then you could change your

181
00:14:52,320 --> 00:14:56,540
coordinate system so that these eigenvectors are your basis vectors.

182
00:14:57,140 --> 00:15:01,160
I talked about change of basis last video, but I'll go through a super quick reminder

183
00:15:01,160 --> 00:15:06,560
here of how to express a transformation currently written in our coordinate system into a different

184
00:15:06,560 --> 00:15:07,040
system.

185
00:15:08,440 --> 00:15:12,500
Take the coordinates of the vectors that you want to use as a new basis, which in this

186
00:15:12,500 --> 00:15:16,980
case means our two eigenvectors, then make those coordinates the columns of a matrix,

187
00:15:17,440 --> 00:15:19,440
known as the change of basis matrix.

188
00:15:20,180 --> 00:15:24,540
When you sandwich the original transformation, putting the change of basis matrix on its

189
00:15:24,540 --> 00:15:30,100
right and the inverse of the change of basis matrix on its left, the result will be a

190
00:15:30,100 --> 00:15:35,180
matrix representing that same transformation, but from the perspective of the new basis

191
00:15:35,180 --> 00:15:36,500
vectors coordinate system.

192
00:15:37,440 --> 00:15:42,820
The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be

193
00:15:42,820 --> 00:15:46,680
diagonal with its corresponding eigenvalues down that diagonal.

194
00:15:46,860 --> 00:15:51,220
This is because it represents working in a coordinate system where what happens to the

195
00:15:51,220 --> 00:15:54,320
basis vectors is that they get scaled during the transformation.

196
00:15:55,800 --> 00:16:01,560
A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.

197
00:16:02,340 --> 00:16:07,780
So if, for example, you needed to compute the 100th power of this matrix, it would be

198
00:16:07,780 --> 00:16:14,060
much easier to change to an eigenbasis, compute the 100th power in that system, then convert

199
00:16:14,060 --> 00:16:15,680
back to our standard system.

200
00:16:16,620 --> 00:16:18,320
You can't do this with all transformations.

201
00:16:18,320 --> 00:16:22,980
A shear, for example, doesn't have enough eigenvectors to span the full space.

202
00:16:23,460 --> 00:16:28,160
But if you can find an eigenbasis, it makes matrix operations really lovely.

203
00:16:29,120 --> 00:16:32,080
For those of you willing to work through a pretty neat puzzle to see what this looks

204
00:16:32,080 --> 00:16:36,420
like in action and how it can be used to produce some surprising results, I'll leave up a prompt

205
00:16:36,420 --> 00:16:37,320
here on the screen.

206
00:16:37,600 --> 00:16:40,280
It takes a bit of work, but I think you'll enjoy it.

207
00:16:40,840 --> 00:16:45,380
The next and final video of this series is going to be on abstract vector spaces.

208
00:16:45,900 --> 00:16:46,440
See you then!

