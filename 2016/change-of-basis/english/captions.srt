1
00:00:19,920 --> 00:00:22,812
Eigenvectors and eigenvalues is one of those topics 

2
00:00:22,812 --> 00:00:25,760
that a lot of students find particularly unintuitive.

3
00:00:25,760 --> 00:00:29,433
Questions like, why are we doing this and what does this actually mean, 

4
00:00:29,433 --> 00:00:33,260
are too often left just floating away in an unanswered sea of computations.

5
00:00:33,920 --> 00:00:36,026
And as I've put out the videos of this series, 

6
00:00:36,026 --> 00:00:40,060
a lot of you have commented about looking forward to visualizing this topic in particular.

7
00:00:40,680 --> 00:00:43,373
I suspect that the reason for this is not so much that 

8
00:00:43,373 --> 00:00:46,360
eigenthings are particularly complicated or poorly explained.

9
00:00:46,860 --> 00:00:49,065
In fact, it's comparatively straightforward, and 

10
00:00:49,065 --> 00:00:51,180
I think most books do a fine job explaining it.

11
00:00:51,520 --> 00:00:54,805
The issue is that it only really makes sense if you have a 

12
00:00:54,805 --> 00:00:58,480
solid visual understanding for many of the topics that precede it.

13
00:00:59,060 --> 00:01:02,616
Most important here is that you know how to think about matrices as 

14
00:01:02,616 --> 00:01:06,383
linear transformations, but you also need to be comfortable with things 

15
00:01:06,383 --> 00:01:09,940
like determinants, linear systems of equations, and change of basis.

16
00:01:10,720 --> 00:01:14,979
Confusion about eigenstuffs usually has more to do with a shaky foundation in 

17
00:01:14,979 --> 00:01:19,240
one of these topics than it does with eigenvectors and eigenvalues themselves.

18
00:01:19,980 --> 00:01:24,840
To start, consider some linear transformation in two dimensions, like the one shown here.

19
00:01:25,460 --> 00:01:27,936
st number indicates rightward motion, that the second one indicates upward motion, 

20
00:01:27,936 --> 00:01:29,786
exactly how far a unit of distance is, all of that is tied up 

21
00:01:29,786 --> 00:01:31,040
in the choice of i-hat and j-hat as the ve

22
00:01:31,780 --> 00:01:42,148
ctors which are scalar coordinates are meant to actually scale. 

23
00:01:42,148 --> 00:01:56,405
Any way to translate between vectors and sets of numbers is called a coordinate system, 

24
00:01:56,405 --> 00:01:57,540
and the

25
00:01:57,540 --> 00:02:01,017
two special vectors i-hat and j-hat are called the basis vectors of our standard 

26
00:02:01,017 --> 00:02:04,236
coordinate system. What I'd like to talk about here is the idea of using a 

27
00:02:04,236 --> 00:02:07,842
different set of basis vectors. For example, let's say you have a friend, Jennifer, 

28
00:02:07,842 --> 00:02:11,320
who uses a different set of basis vectors, which I'll call b1 and b2. Her first b

29
00:02:11,320 --> 00:02:24,183
asis vector, b1, points up and to the right a little bit, and her second vector, b2, 

30
00:02:24,183 --> 00:02:36,290
points left and up. Now take another look at that vector that I showed earlier, 

31
00:02:36,290 --> 00:02:46,126
the one that you and I would describe using the coordinates 3,2, 

32
00:02:46,126 --> 00:02:52,180
using our basis vectors i-hat and j-hat.

33
00:02:52,180 --> 00:02:57,318
Jennifer would actually describe this vector with the coordinates 5 thirds and 1 third. 

34
00:02:57,318 --> 00:03:01,113
What this means is that the particular way to get to that vector 

35
00:03:01,113 --> 00:03:05,609
using her two basis vectors is to scale b1 by 5 thirds, scale b2 by 1 third, 

36
00:03:05,609 --> 00:03:08,120
then add them both together. In a little bi

37
00:03:08,120 --> 00:03:08,466
t, I'll show you how you could have figured out those two numbers, 

38
00:03:08,466 --> 00:03:08,823
5 thirds and 1 third. In general, whenever Jennifer uses coordinates 

39
00:03:08,823 --> 00:03:09,160
to describe a vector, she thinks of her first coordinate as scali

40
00:03:09,160 --> 00:03:25,060
For this specific example, the basis vector i-hat is one such special vector.

41
00:03:25,060 --> 00:03:38,400
The span of i-hat is the x-axis, and from the first column of the matrix, 

42
00:03:38,400 --> 00:03:51,560
we can see that i-hat moves over to 3 times itself, still on that x-axis.

43
00:03:51,560 --> 00:04:00,881
What's more, because of the way linear transformations work, 

44
00:04:00,881 --> 00:04:11,884
any other vector on the x-axis is also just stretched by a factor of 3, 

45
00:04:11,884 --> 00:04:17,079
and hence remains on its own span.

46
00:04:17,620 --> 00:04:21,822
A slightly sneakier vector that remains on its own 

47
00:04:21,822 --> 00:04:25,860
span during this transformation is negative 1, 1.

48
00:04:25,860 --> 00:04:39,020
Let me say a quick word about how I'm representing things here. When I animate 2D space, 

49
00:04:39,020 --> 00:04:48,780
I typically use this square grid. But that grid is just a construc

50
00:04:48,780 --> 00:04:54,161
t, a way to visualize our coordinate system, and so it depends on our choice of basis. 

51
00:04:54,161 --> 00:04:58,492
Space itself has no intrinsic grid. Jennifer might draw her own grid, 

52
00:04:58,492 --> 00:05:02,080
which would be an equally made up construct meant as nothi

53
00:05:02,080 --> 00:05:10,784
ng more than a visual tool to help follow the meaning of her coordinates. 

54
00:05:10,784 --> 00:05:16,900
Her origin though would actually line up with ours, 

55
00:05:16,900 --> 00:05:24,428
since everybody agrees on what the coordinates 0,0 should mean. 

56
00:05:24,428 --> 00:05:31,838
It's the thing that you get when you scale any vector by zero. 

57
00:05:31,838 --> 00:05:35,720
But the direction of her axes and

58
00:05:36,260 --> 00:05:41,475
Those on the x-axis getting stretched out by a factor of 3, 

59
00:05:41,475 --> 00:05:47,300
and those on this diagonal line getting stretched by a factor of 2.

60
00:05:47,300 --> 00:06:05,010
Any other vector is going to get rotated somewhat during the transformation, 

61
00:06:05,010 --> 00:06:13,060
knocked off the line that it spans.

62
00:06:13,060 --> 00:06:14,406
ks of as negative 1, 2. This process here of scaling each of her basis vectors 

63
00:06:14,406 --> 00:06:15,702
by the corresponding coordinates of some vector, then adding them together, 

64
00:06:15,702 --> 00:06:16,811
might feel somewhat familiar. It's matrix vector multiplication, 

65
00:06:16,811 --> 00:06:18,174
with a matrix whose columns represent Jennifer's basis vectors in our language. 

66
00:06:18,174 --> 00:06:19,539
In fact, once you understand matrix vector multiplication as applying a certain 

67
00:06:19,539 --> 00:06:19,880
linear transformatio

68
00:06:19,880 --> 00:06:22,680
Of course, there's nothing special about stretching versus squishing, 

69
00:06:22,680 --> 00:06:24,960
or the fact that these eigenvalues happen to be positive.

70
00:06:24,960 --> 00:06:32,349
In another example, you could have an eigenvector with eigenvalue negative 1 half, 

71
00:06:32,349 --> 00:06:38,760
meaning that the vector gets flipped and squished by a factor of 1 half.

72
00:06:38,760 --> 00:06:41,398
pretty intuitive way to think about what's going on here. 

73
00:06:41,398 --> 00:06:44,718
A matrix whose columns represent Jennifer's basis vectors can be thought 

74
00:06:44,718 --> 00:06:47,902
of as a transformation that moves our basis vectors, i-hat and j-hat, 

75
00:06:47,902 --> 00:06:51,860
the things we think of when we say 1, 0 and 0, 1, to Jennifer's basis vectors, the thin

76
00:06:54,160 --> 00:07:03,933
gs she thinks of when she says 1, 0 and 0, 1. To show how this works, 

77
00:07:03,933 --> 00:07:13,847
let's walk through what it would mean to take the vector that we think 

78
00:07:13,847 --> 00:07:23,900
of as having coordinates negative 1, 2 and applying that transformation.

79
00:07:23,900 --> 00:07:25,448
If you can find an eigenvector for that rotation, 

80
00:07:25,448 --> 00:07:28,020
a vector that remains on its own span, what you have found is the axis of rotation.

81
00:07:28,540 --> 00:07:28,020
And it's much easier to think about a 3D rotation in terms of some 

82
00:07:28,879 --> 00:07:28,540
axis of rotation and an angle by which it's rotating, 

83
00:07:29,300 --> 00:07:28,879
rather than thinking about the full 3x3 matrix associated with that transformation.

84
00:07:29,300 --> 00:07:41,274
In this case, by the way, the corresponding eigenvalue would have to be 1, 

85
00:07:41,274 --> 00:07:49,257
since rotations never stretch or squish anything, 

86
00:07:49,257 --> 00:07:57,240
so the length of the vector would remain the same.

87
00:07:58,220 --> 00:08:07,240
This pattern shows up a lot in linear algebra.

88
00:08:08,160 --> 00:08:15,833
With any linear transformation described by a matrix, 

89
00:08:15,833 --> 00:08:28,053
you could understand what it's doing by reading off the columns of this matrix as the 

90
00:08:28,053 --> 00:08:32,600
landing spots for basis vectors.

91
00:08:32,600 --> 00:08:39,284
But often, a better way to get at the heart of what the linear 

92
00:08:39,284 --> 00:08:48,091
transformation actually does, less dependent on your particular coordinate system, 

93
00:08:48,091 --> 00:08:52,760
is to find the eigenvectors and eigenvalues.

94
00:08:52,760 --> 00:08:56,585
we get using the same coordinates but in our system, 

95
00:08:56,585 --> 00:09:00,988
then it transforms it into the vector that she really meant. 

96
00:09:00,988 --> 00:09:06,834
What about going the other way around? In the example I used earlier this video, 

97
00:09:06,834 --> 00:09:11,092
when I had the vector with coordinates 3, 2 in our system, 

98
00:09:11,092 --> 00:09:15,640
how did I compute that it would have coordinates 5 thirds and 1

99
00:09:16,220 --> 00:09:29,640
Symbolically, here's what the idea of an eigenvector looks like.

100
00:09:29,640 --> 00:09:34,552
A is the matrix representing some transformation, with v as the eigenvector, 

101
00:09:34,552 --> 00:09:38,380
and lambda is a number, namely the corresponding eigenvalue.

102
00:09:39,000 --> 00:09:39,495
e. In this case, the inverse of the change of basis matrix that has Jennifer's 

103
00:09:39,495 --> 00:09:39,909
basis as its columns ends up working out to have columns 1 third, 

104
00:09:39,909 --> 00:09:40,429
negative 1 third, and 1 third, 2 thirds. So for example, to see what the vector 3, 

105
00:09:40,429 --> 00:09:40,900
2 looks like in Jennifer's system, we multiply this inverse change of basis

106
00:09:40,900 --> 00:09:53,667
matrix by the vector 3, 2, which works out to be 5 thirds, 1 third. So that, 

107
00:09:53,667 --> 00:10:06,600
in a nutshell, is how to translate the description of individual vectors back 

108
00:10:06,600 --> 00:10:19,700
and forth between coordinate systems. The matrix whose columns represent Jennif

109
00:10:19,700 --> 00:10:21,400
er's basis vectors, but written in our coordinates, 

110
00:10:21,400 --> 00:10:23,231
translates vectors from her language into our language. 

111
00:10:23,231 --> 00:10:25,717
And the inverse matrix does the opposite. But vectors aren't the only thing 

112
00:10:25,717 --> 00:10:27,548
that we describe using coordinates. For this next part, 

113
00:10:27,548 --> 00:10:30,393
it's important that you're all comfortable representing transformations with matrices, 

114
00:10:30,393 --> 00:10:31,800
and that you know how matrix multiplication

115
00:10:31,800 --> 00:10:38,562
So let's start by rewriting that right-hand side as some kind of matrix-vector 

116
00:10:38,562 --> 00:10:45,924
multiplication, using a matrix which has the effect of scaling any vector by a factor 

117
00:10:45,924 --> 00:10:46,780
of lambda.

118
00:10:46,980 --> 00:10:54,011
The columns of such a matrix will represent what happens to each basis vector, 

119
00:10:54,011 --> 00:10:58,818
and each basis vector is simply multiplied by lambda, 

120
00:10:58,818 --> 00:11:06,740
so this matrix will have the number lambda down the diagonal, with zeros everywhere else.

121
00:11:06,740 --> 00:11:09,771
the columns of our matrix. But this representation is heavily tied up in our choice 

122
00:11:09,771 --> 00:11:12,840
of basis vectors, from the fact that we're following i-hat and j-hat in the first pla

123
00:11:12,840 --> 00:11:20,758
With both sides looking like matrix-vector multiplication, 

124
00:11:20,758 --> 00:11:29,080
we can subtract off that right-hand side and factor out the v.

125
00:11:29,080 --> 00:11:31,101
So what we now have is a new matrix, A minus lambda times the identity, 

126
00:11:31,101 --> 00:11:33,600
and we're looking for a vector v such that this new matrix times v gives the zero vector.

127
00:11:35,540 --> 00:11:51,563
s land, and it needs to describe those landing spots in her language. 

128
00:11:51,563 --> 00:12:02,780
Here's a common way to think of how this is done.

129
00:12:03,460 --> 00:12:28,020
What we want is a non-zero eigenvector.

130
00:12:29,900 --> 00:12:32,960
And if you watch chapter 5 and 6, you'll know that the only way it's possible 

131
00:12:32,960 --> 00:12:35,942
for the product of a matrix with a non-zero vector to become zero is if the 

132
00:12:35,942 --> 00:12:39,120
transformation associated with that matrix squishes space into a lower dimension.

133
00:12:39,120 --> 00:13:09,460
And that squishification corresponds to a zero determinant for the matrix.

134
00:13:09,460 --> 00:13:20,676
To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, 

135
00:13:20,676 --> 00:13:34,740
and think about subtracting off a variable amount, lambda, from each diagonal entry.

136
00:13:34,740 --> 00:13:54,400
Now imagine tweaking lambda, turning a knob to change its value.

137
00:13:57,100 --> 00:14:02,734
As that value of lambda changes, the matrix itself changes, 

138
00:14:02,734 --> 00:14:06,960
and so the determinant of the matrix changes.

139
00:14:06,960 --> 00:14:12,063
ou work through it, has columns one third, five thirds, and negative two thirds, 

140
00:14:12,063 --> 00:14:16,158
negative one third. So if Jennifer multiplies that matrix by the 

141
00:14:16,158 --> 00:14:20,442
coordinates of a vector in her system, it will return the 90 degree 

142
00:14:20,442 --> 00:14:24,600
rotated version of that vector expressed in her coordinate system.

143
00:14:25,700 --> 00:14:32,440
In this case, the sweet spot comes when lambda equals 1.

144
00:14:36,080 --> 00:14:37,860
Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.

145
00:14:37,860 --> 00:14:40,966
A inverse times M times A, it suggests a mathematical sort of empathy. 

146
00:14:40,966 --> 00:14:42,760
That middle matrix represents a transform

147
00:14:42,760 --> 00:14:47,676
ation of some kind as you see it, and the outer two matrices represent the empathy, 

148
00:14:47,676 --> 00:14:49,140
the shift in perspective.

149
00:14:49,140 --> 00:14:54,174
And the full matrix product represents that same transformation, 

150
00:14:54,174 --> 00:15:00,680
but as someone else sees it. For those of you wondering why we care about alternate 

151
00:15:00,680 --> 00:15:07,341
coordinate systems, the next video on eigenvectors and eigenvalues will give a really 

152
00:15:07,341 --> 00:15:09,820
important example of this. See y

153
00:15:09,820 --> 00:15:09,820
That means there's a non-zero vector v such that A minus 

154
00:15:09,820 --> 00:15:09,820
lambda times the identity times v equals the zero vector.

155
00:15:09,820 --> 00:15:13,982
And remember, the reason we care about that is because it means A times v 

156
00:15:13,982 --> 00:15:18,145
equals lambda times v, which you can read off as saying that the vector v 

157
00:15:18,145 --> 00:15:22,420
is an eigenvector of A, staying on its own span during the transformation A.

158
00:15:22,800 --> 00:15:23,850
In this example, the corresponding eigenvalue is 1, 

159
00:15:23,850 --> 00:15:24,760
so v would actually just stay fixed in place.

160
00:15:24,760 --> 00:15:36,500
Pause and ponder if you need to make sure that that line of reasoning feels good.

161
00:15:37,440 --> 00:15:54,320
This is the kind of thing I mentioned in the introduction.

162
00:15:55,800 --> 00:15:59,034
If you didn't have a solid grasp of determinants and why they 

163
00:15:59,034 --> 00:16:02,425
relate to linear systems of equations having non-zero solutions, 

164
00:16:02,425 --> 00:16:05,660
an expression like this would feel completely out of the blue.

165
00:16:05,660 --> 00:16:11,527
To see this in action, let's revisit the example from the start, 

166
00:16:11,527 --> 00:16:15,680
with a matrix whose columns are 3, 0 and 1, 2.

167
00:16:16,620 --> 00:16:22,585
To find if a value lambda is an eigenvalue, subtract it from 

168
00:16:22,585 --> 00:16:28,160
the diagonals of this matrix and compute the determinant.

169
00:16:29,120 --> 00:16:37,873
Doing this, we get a certain quadratic polynomial in lambda, 

170
00:16:37,873 --> 00:16:43,040
3 minus lambda times 2 minus lambda.

171
00:16:43,040 --> 00:16:44,121
Since lambda can only be an eigenvalue if this determinant happens to be zero, 

172
00:16:44,121 --> 00:16:45,256
you can conclude that the only possible eigenvalues are lambda equals 2 and lambda 

173
00:16:45,256 --> 00:16:45,380
equals 3.

174
00:16:45,435 --> 00:16:45,380
To figure out what the eigenvectors are that actually have one of these eigenvalues, 

175
00:16:45,494 --> 00:16:45,380
say lambda equals 2, plug in that value of lambda to the matrix and then 

176
00:16:45,538 --> 00:16:45,380
solve for which vectors this diagonally altered matrix sends to zero.

177
00:16:45,643 --> 00:16:45,380
If you computed this the way you would any other linear system, 

178
00:16:45,690 --> 00:16:45,380
you'd see that the solutions are all the vectors on the diagonal line spanned 

179
00:16:45,705 --> 00:16:45,380
by negative 1, 1.

180
00:16:45,900 --> 00:16:45,380
This corresponds to the fact that the unaltered matrix, 3, 0, 1, 

181
00:16:45,900 --> 00:16:45,380
2, has the effect of stretching all those vectors by a factor of 2.

182
00:16:45,900 --> 00:16:45,380
Now, a 2D transformation doesn't have to have eigenvectors.

183
00:16:45,900 --> 00:16:45,435
For example, consider a rotation by 90 degrees.

184
00:16:45,900 --> 00:16:45,494
This doesn't have any eigenvectors since it rotates every vector off of its own span.

185
00:16:45,900 --> 00:16:45,538
If you actually try computing the eigenvalues of a rotation like this, 

186
00:16:45,900 --> 00:16:45,643
notice what happens.

187
00:16:45,900 --> 00:16:45,690
Its matrix has columns 0, 1 and negative 1, 0.

188
00:16:45,900 --> 00:16:45,705
Subtract off lambda from the diagonal elements and look for when the determinant is zero.

189
00:16:45,900 --> 00:16:46,119
In this case, you get the polynomial lambda squared plus 1.

190
00:16:46,119 --> 00:16:46,119
The only roots of that polynomial are the imaginary numbers, i and negative i.

191
00:16:46,119 --> 00:16:46,119
The fact that there are no real number solutions indicates that there are no eigenvectors.

192
00:16:46,119 --> 00:16:46,119
Another pretty interesting example worth holding in the back of your mind is a shear.

193
00:16:46,119 --> 00:16:46,119
This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.

194
00:16:46,119 --> 00:16:46,120
All of the vectors on the x-axis are eigenvectors 

195
00:16:46,120 --> 00:16:46,120
with eigenvalue 1 since they remain fixed in place.

196
00:16:46,120 --> 00:16:46,120
In fact, these are the only eigenvectors.

197
00:16:46,120 --> 00:16:46,120
When you subtract off lambda from the diagonals and compute the determinant, 

198
00:16:46,120 --> 00:16:46,120
what you get is 1 minus lambda squared.

199
00:16:46,120 --> 00:16:46,120
And the only root of this expression is lambda equals 1.

200
00:16:46,120 --> 00:16:46,120
This lines up with what we see geometrically, 

201
00:16:46,120 --> 00:16:46,120
that all of the eigenvectors have eigenvalue 1.

202
00:16:46,120 --> 00:16:46,120
Keep in mind though, it's also possible to have just one eigenvalue, 

203
00:16:46,120 --> 00:16:46,120
but with more than just a line full of eigenvectors.

204
00:16:46,120 --> 00:16:46,120
A simple example is a matrix that scales everything by 2.

205
00:16:46,120 --> 00:16:46,120
The only eigenvalue is 2, but every vector in the 

206
00:16:46,120 --> 00:16:46,120
plane gets to be an eigenvector with that eigenvalue.

207
00:16:46,120 --> 00:16:46,120
Now is another good time to pause and ponder some 

208
00:16:46,120 --> 00:16:46,120
of this before I move on to the last topic.

209
00:16:46,120 --> 00:16:46,120
I want to finish off here with the idea of an eigenbasis, 

210
00:16:46,120 --> 00:16:46,120
which relies heavily on ideas from the last video.

211
00:16:46,120 --> 00:16:46,120
Take a look at what happens if our basis vectors just so happen to be eigenvectors.

212
00:16:46,120 --> 00:16:46,120
For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.

213
00:16:46,120 --> 00:16:46,120
Writing their new coordinates as the columns of a matrix, 

214
00:16:46,120 --> 00:16:46,120
notice that those scalar multiples, negative 1 and 2, 

215
00:16:46,120 --> 00:16:46,120
which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, 

216
00:16:46,120 --> 00:16:46,120
and every other entry is a 0.

217
00:16:46,120 --> 00:16:46,120
Any time a matrix has zeros everywhere other than the diagonal, 

218
00:16:46,120 --> 00:16:46,120
it's called, reasonably enough, a diagonal matrix.

219
00:16:46,120 --> 00:16:46,120
And the way to interpret this is that all the basis vectors are eigenvectors, 

220
00:16:46,120 --> 00:16:46,120
with the diagonal entries of this matrix being their eigenvalues.

221
00:16:46,120 --> 00:16:46,120
There are a lot of things that make diagonal matrices much nicer to work with.

222
00:16:46,120 --> 00:16:46,120
One big one is that it's easier to compute what will happen 

223
00:16:46,120 --> 00:16:46,120
if you multiply this matrix by itself a whole bunch of times.

224
00:16:46,120 --> 00:16:46,120
Since all one of these matrices does is scale each basis vector by some eigenvalue, 

225
00:16:46,120 --> 00:16:46,120
applying that matrix many times, say 100 times, 

226
00:16:46,120 --> 00:16:46,120
is just going to correspond to scaling each basis vector by the 100th power of 

227
00:16:46,120 --> 00:16:46,120
the corresponding eigenvalue.

228
00:16:46,120 --> 00:16:46,120
In contrast, try computing the 100th power of a non-diagonal matrix.

229
00:16:46,120 --> 00:16:46,120
Really, try it for a moment.

230
00:16:46,120 --> 00:16:46,120
It's a nightmare.

231
00:16:46,120 --> 00:16:46,120
Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.

232
00:16:46,120 --> 00:16:46,120
But if your transformation has a lot of eigenvectors, 

233
00:16:46,120 --> 00:16:46,120
like the one from the start of this video, enough so that you can choose a set that 

234
00:16:46,120 --> 00:16:46,120
spans the full space, then you could change your coordinate system so that these 

235
00:16:46,120 --> 00:16:46,120
eigenvectors are your basis vectors.

236
00:16:46,120 --> 00:16:46,120
I talked about change of basis last video, but I'll go through 

237
00:16:46,120 --> 00:16:46,120
a super quick reminder here of how to express a transformation 

238
00:16:46,120 --> 00:16:46,120
currently written in our coordinate system into a different system.

239
00:16:46,120 --> 00:16:46,120
Take the coordinates of the vectors that you want to use as a new basis, 

240
00:16:46,120 --> 00:16:46,120
which in this case means our two eigenvectors, 

241
00:16:46,120 --> 00:16:46,120
then make those coordinates the columns of a matrix, known as the change of basis matrix.

242
00:16:46,120 --> 00:16:46,120
When you sandwich the original transformation, 

243
00:16:46,120 --> 00:16:46,120
putting the change of basis matrix on its right and the inverse of the 

244
00:16:46,120 --> 00:16:46,120
change of basis matrix on its left, the result will be a matrix representing 

245
00:16:46,120 --> 00:16:46,120
that same transformation, but from the perspective of the new basis 

246
00:16:46,120 --> 00:16:46,120
vectors coordinate system.

247
00:16:46,120 --> 00:16:46,120
The whole point of doing this with eigenvectors is that this new matrix is 

248
00:16:46,120 --> 00:16:46,120
guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.

249
00:16:46,120 --> 00:16:46,120
This is because it represents working in a coordinate system where what 

250
00:16:46,120 --> 00:16:46,120
happens to the basis vectors is that they get scaled during the transformation.

251
00:16:46,120 --> 00:16:46,120
A set of basis vectors which are also eigenvectors is called, 

252
00:16:46,120 --> 00:16:46,120
again, reasonably enough, an eigenbasis.

253
00:16:46,120 --> 00:16:46,120
So if, for example, you needed to compute the 100th power of this matrix, 

254
00:16:46,120 --> 00:16:46,120
it would be much easier to change to an eigenbasis, 

255
00:16:46,120 --> 00:16:46,120
compute the 100th power in that system, then convert back to our standard system.

256
00:16:46,120 --> 00:16:46,120
You can't do this with all transformations.

257
00:16:46,120 --> 00:16:46,120
A shear, for example, doesn't have enough eigenvectors to span the full space.

258
00:16:46,120 --> 00:16:46,120
But if you can find an eigenbasis, it makes matrix operations really lovely.

259
00:16:46,120 --> 00:16:46,120
For those of you willing to work through a pretty neat puzzle to 

260
00:16:46,120 --> 00:16:46,120
see what this looks like in action and how it can be used to produce 

261
00:16:46,120 --> 00:16:46,120
some surprising results, I'll leave up a prompt here on the screen.

262
00:16:46,120 --> 00:16:46,120
It takes a bit of work, but I think you'll enjoy it.

263
00:16:46,120 --> 00:16:46,120
The next and final video of this series is going to be on abstract vector spaces.

