1
00:00:19,920 --> 00:00:22,812
Eigenvectors and eigenvalues is one of those topics 

2
00:00:22,812 --> 00:00:25,760
that a lot of students find particularly unintuitive.

3
00:00:25,760 --> 00:00:29,433
Questions like, why are we doing this and what does this actually mean, 

4
00:00:29,433 --> 00:00:33,260
are too often left just floating away in an unanswered sea of computations.

5
00:00:33,920 --> 00:00:36,026
And as I've put out the videos of this series, 

6
00:00:36,026 --> 00:00:40,060
a lot of you have commented about looking forward to visualizing this topic in particular.

7
00:00:40,680 --> 00:00:43,373
I suspect that the reason for this is not so much that 

8
00:00:43,373 --> 00:00:46,360
eigenthings are particularly complicated or poorly explained.

9
00:00:46,860 --> 00:00:49,065
In fact, it's comparatively straightforward, and 

10
00:00:49,065 --> 00:00:51,180
I think most books do a fine job explaining it.

11
00:00:51,520 --> 00:00:54,805
The issue is that it only really makes sense if you have a 

12
00:00:54,805 --> 00:00:58,480
solid visual understanding for many of the topics that precede it.

13
00:00:59,060 --> 00:01:02,616
Most important here is that you know how to think about matrices as 

14
00:01:02,616 --> 00:01:06,383
linear transformations, but you also need to be comfortable with things 

15
00:01:06,383 --> 00:01:09,940
like determinants, linear systems of equations, and change of basis.

16
00:01:10,720 --> 00:01:14,979
Confusion about eigenstuffs usually has more to do with a shaky foundation in 

17
00:01:14,979 --> 00:01:19,240
one of these topics than it does with eigenvectors and eigenvalues themselves.

18
00:01:19,980 --> 00:01:24,840
To start, consider some linear transformation in two dimensions, like the one shown here.

19
00:01:25,460 --> 00:01:32,472
st number indicates rightward motion, that the second one indicates upward motion, 

20
00:01:32,472 --> 00:01:37,711
exactly how far a unit of distance is, all of that is tied up 

21
00:01:37,711 --> 00:01:41,260
in the choice of i-hat and j-hat as the ve

22
00:01:41,260 --> 00:01:45,381
ctors which are scalar coordinates are meant to actually scale. 

23
00:01:45,381 --> 00:01:51,049
Any way to translate between vectors and sets of numbers is called a coordinate system, 

24
00:01:51,049 --> 00:01:51,500
and the

25
00:01:51,500 --> 00:01:56,501
two special vectors i-hat and j-hat are called the basis vectors of our standard 

26
00:01:56,501 --> 00:02:01,132
coordinate system. What I'd like to talk about here is the idea of using a 

27
00:02:01,132 --> 00:02:06,318
different set of basis vectors. For example, let's say you have a friend, Jennifer, 

28
00:02:06,318 --> 00:02:11,320
who uses a different set of basis vectors, which I'll call b1 and b2. Her first b

29
00:02:11,320 --> 00:02:15,349
asis vector, b1, points up and to the right a little bit, and her second vector, b2, 

30
00:02:15,349 --> 00:02:19,142
points left and up. Now take another look at that vector that I showed earlier, 

31
00:02:19,142 --> 00:02:22,223
the one that you and I would describe using the coordinates 3,2, 

32
00:02:22,223 --> 00:02:24,120
using our basis vectors i-hat and j-hat.

33
00:02:26,320 --> 00:02:32,921
Jennifer would actually describe this vector with the coordinates 5 thirds and 1 third. 

34
00:02:32,921 --> 00:02:37,797
What this means is that the particular way to get to that vector 

35
00:02:37,797 --> 00:02:43,574
using her two basis vectors is to scale b1 by 5 thirds, scale b2 by 1 third, 

36
00:02:43,574 --> 00:02:46,800
then add them both together. In a little bi

37
00:02:46,800 --> 00:02:52,073
t, I'll show you how you could have figured out those two numbers, 

38
00:02:52,073 --> 00:02:57,504
5 thirds and 1 third. In general, whenever Jennifer uses coordinates 

39
00:02:57,504 --> 00:03:02,620
to describe a vector, she thinks of her first coordinate as scali

40
00:03:02,620 --> 00:03:04,260
For this specific example, the basis vector i-hat is one such special vector.

41
00:03:04,260 --> 00:03:05,427
The span of i-hat is the x-axis, and from the first column of the matrix, 

42
00:03:05,427 --> 00:03:06,580
we can see that i-hat moves over to 3 times itself, still on that x-axis.

43
00:03:06,580 --> 00:03:10,780
What's more, because of the way linear transformations work, 

44
00:03:10,780 --> 00:03:15,738
any other vector on the x-axis is also just stretched by a factor of 3, 

45
00:03:15,738 --> 00:03:18,080
and hence remains on its own span.

46
00:03:20,255 --> 00:03:18,080
A slightly sneakier vector that remains on its own 

47
00:03:22,520 --> 00:03:20,255
span during this transformation is negative 1, 1.

48
00:03:22,520 --> 00:03:29,375
Let me say a quick word about how I'm representing things here. When I animate 2D space, 

49
00:03:29,375 --> 00:03:34,460
I typically use this square grid. But that grid is just a construc

50
00:03:34,460 --> 00:03:38,045
t, a way to visualize our coordinate system, and so it depends on our choice of basis. 

51
00:03:38,045 --> 00:03:40,929
Space itself has no intrinsic grid. Jennifer might draw her own grid, 

52
00:03:40,929 --> 00:03:43,320
which would be an equally made up construct meant as nothi

53
00:03:43,400 --> 00:03:48,952
ng more than a visual tool to help follow the meaning of her coordinates. 

54
00:03:48,952 --> 00:03:52,854
Her origin though would actually line up with ours, 

55
00:03:52,854 --> 00:03:57,656
since everybody agrees on what the coordinates 0,0 should mean. 

56
00:03:57,656 --> 00:04:02,383
It's the thing that you get when you scale any vector by zero. 

57
00:04:02,383 --> 00:04:04,860
But the direction of her axes and

58
00:04:04,860 --> 00:04:08,526
Those on the x-axis getting stretched out by a factor of 3, 

59
00:04:08,526 --> 00:04:12,620
and those on this diagonal line getting stretched by a factor of 2.

60
00:04:12,620 --> 00:04:16,414
Any other vector is going to get rotated somewhat during the transformation, 

61
00:04:16,414 --> 00:04:18,140
knocked off the line that it spans.

62
00:04:18,140 --> 00:04:24,795
ks of as negative 1, 2. This process here of scaling each of her basis vectors 

63
00:04:24,795 --> 00:04:31,198
by the corresponding coordinates of some vector, then adding them together, 

64
00:04:31,198 --> 00:04:36,674
might feel somewhat familiar. It's matrix vector multiplication, 

65
00:04:36,674 --> 00:04:43,414
with a matrix whose columns represent Jennifer's basis vectors in our language. 

66
00:04:43,414 --> 00:04:50,155
In fact, once you understand matrix vector multiplication as applying a certain 

67
00:04:50,155 --> 00:04:51,840
linear transformatio

68
00:04:51,840 --> 00:04:57,142
Of course, there's nothing special about stretching versus squishing, 

69
00:04:57,142 --> 00:05:01,460
or the fact that these eigenvalues happen to be positive.

70
00:05:01,460 --> 00:05:04,437
In another example, you could have an eigenvector with eigenvalue negative 1 half, 

71
00:05:04,437 --> 00:05:07,020
meaning that the vector gets flipped and squished by a factor of 1 half.

72
00:05:07,020 --> 00:05:11,317
pretty intuitive way to think about what's going on here. 

73
00:05:11,317 --> 00:05:16,726
A matrix whose columns represent Jennifer's basis vectors can be thought 

74
00:05:16,726 --> 00:05:21,913
of as a transformation that moves our basis vectors, i-hat and j-hat, 

75
00:05:21,913 --> 00:05:28,360
the things we think of when we say 1, 0 and 0, 1, to Jennifer's basis vectors, the thin

76
00:05:28,360 --> 00:05:32,638
gs she thinks of when she says 1, 0 and 0, 1. To show how this works, 

77
00:05:32,638 --> 00:05:36,978
let's walk through what it would mean to take the vector that we think 

78
00:05:36,978 --> 00:05:41,380
of as having coordinates negative 1, 2 and applying that transformation.

79
00:05:41,380 --> 00:05:46,432
If you can find an eigenvector for that rotation, 

80
00:05:46,432 --> 00:05:54,820
a vector that remains on its own span, what you have found is the axis of rotation.

81
00:05:54,820 --> 00:05:59,982
And it's much easier to think about a 3D rotation in terms of some 

82
00:05:59,982 --> 00:06:04,144
axis of rotation and an angle by which it's rotating, 

83
00:06:04,144 --> 00:06:10,540
rather than thinking about the full 3x3 matrix associated with that transformation.

84
00:06:11,120 --> 00:06:15,191
In this case, by the way, the corresponding eigenvalue would have to be 1, 

85
00:06:15,191 --> 00:06:17,905
since rotations never stretch or squish anything, 

86
00:06:17,905 --> 00:06:20,620
so the length of the vector would remain the same.

87
00:06:21,680 --> 00:06:20,620
This pattern shows up a lot in linear algebra.

88
00:06:21,680 --> 00:06:23,048
With any linear transformation described by a matrix, 

89
00:06:23,048 --> 00:06:25,228
you could understand what it's doing by reading off the columns of this matrix as the 

90
00:06:25,228 --> 00:06:26,040
landing spots for basis vectors.

91
00:06:26,040 --> 00:06:30,973
But often, a better way to get at the heart of what the linear 

92
00:06:30,973 --> 00:06:37,474
transformation actually does, less dependent on your particular coordinate system, 

93
00:06:37,474 --> 00:06:40,920
is to find the eigenvectors and eigenvalues.

94
00:06:40,920 --> 00:06:43,611
we get using the same coordinates but in our system, 

95
00:06:43,611 --> 00:06:46,709
then it transforms it into the vector that she really meant. 

96
00:06:46,709 --> 00:06:50,823
What about going the other way around? In the example I used earlier this video, 

97
00:06:50,823 --> 00:06:53,820
when I had the vector with coordinates 3, 2 in our system, 

98
00:06:53,820 --> 00:06:57,020
how did I compute that it would have coordinates 5 thirds and 1

99
00:06:57,020 --> 00:07:01,280
Symbolically, here's what the idea of an eigenvector looks like.

100
00:07:01,280 --> 00:07:08,226
A is the matrix representing some transformation, with v as the eigenvector, 

101
00:07:08,226 --> 00:07:13,640
and lambda is a number, namely the corresponding eigenvalue.

102
00:07:14,420 --> 00:07:19,478
e. In this case, the inverse of the change of basis matrix that has Jennifer's 

103
00:07:19,478 --> 00:07:23,703
basis as its columns ends up working out to have columns 1 third, 

104
00:07:23,703 --> 00:07:29,018
negative 1 third, and 1 third, 2 thirds. So for example, to see what the vector 3, 

105
00:07:29,018 --> 00:07:33,820
2 looks like in Jennifer's system, we multiply this inverse change of basis

106
00:07:33,820 --> 00:07:39,038
matrix by the vector 3, 2, which works out to be 5 thirds, 1 third. So that, 

107
00:07:39,038 --> 00:07:44,325
in a nutshell, is how to translate the description of individual vectors back 

108
00:07:44,325 --> 00:07:49,680
and forth between coordinate systems. The matrix whose columns represent Jennif

109
00:07:49,680 --> 00:07:53,227
er's basis vectors, but written in our coordinates, 

110
00:07:53,227 --> 00:07:57,047
translates vectors from her language into our language. 

111
00:07:57,047 --> 00:08:02,231
And the inverse matrix does the opposite. But vectors aren't the only thing 

112
00:08:02,231 --> 00:08:06,051
that we describe using coordinates. For this next part, 

113
00:08:06,051 --> 00:08:11,986
it's important that you're all comfortable representing transformations with matrices, 

114
00:08:11,986 --> 00:08:14,920
and that you know how matrix multiplication

115
00:08:14,920 --> 00:08:16,581
So let's start by rewriting that right-hand side as some kind of matrix-vector 

116
00:08:16,581 --> 00:08:18,389
multiplication, using a matrix which has the effect of scaling any vector by a factor 

117
00:08:18,389 --> 00:08:18,600
of lambda.

118
00:08:20,080 --> 00:08:26,656
The columns of such a matrix will represent what happens to each basis vector, 

119
00:08:26,656 --> 00:08:31,151
and each basis vector is simply multiplied by lambda, 

120
00:08:31,151 --> 00:08:38,559
so this matrix will have the number lambda down the diagonal, with zeros everywhere else.

121
00:08:40,480 --> 00:08:45,977
the columns of our matrix. But this representation is heavily tied up in our choice 

122
00:08:45,977 --> 00:08:51,540
of basis vectors, from the fact that we're following i-hat and j-hat in the first pla

123
00:08:51,540 --> 00:08:59,156
With both sides looking like matrix-vector multiplication, 

124
00:08:59,156 --> 00:09:07,160
we can subtract off that right-hand side and factor out the v.

125
00:09:07,160 --> 00:09:10,952
So what we now have is a new matrix, A minus lambda times the identity, 

126
00:09:10,952 --> 00:09:15,640
and we're looking for a vector v such that this new matrix times v gives the zero vector.

127
00:09:16,220 --> 00:09:22,149
s land, and it needs to describe those landing spots in her language. 

128
00:09:22,149 --> 00:09:26,300
Here's a common way to think of how this is done.

129
00:09:28,320 --> 00:09:34,540
What we want is a non-zero eigenvector.

130
00:09:35,350 --> 00:09:43,505
And if you watch chapter 5 and 6, you'll know that the only way it's possible 

131
00:09:43,505 --> 00:09:51,451
for the product of a matrix with a non-zero vector to become zero is if the 

132
00:09:51,451 --> 00:09:59,920
transformation associated with that matrix squishes space into a lower dimension.

133
00:09:59,920 --> 00:10:02,880
And that squishification corresponds to a zero determinant for the matrix.

134
00:10:02,880 --> 00:10:05,524
To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, 

135
00:10:05,524 --> 00:10:08,840
and think about subtracting off a variable amount, lambda, from each diagonal entry.

136
00:10:09,640 --> 00:10:16,160
Now imagine tweaking lambda, turning a knob to change its value.

137
00:10:16,160 --> 00:10:17,977
As that value of lambda changes, the matrix itself changes, 

138
00:10:17,977 --> 00:10:19,340
and so the determinant of the matrix changes.

139
00:10:19,340 --> 00:10:22,666
ou work through it, has columns one third, five thirds, and negative two thirds, 

140
00:10:22,666 --> 00:10:25,336
negative one third. So if Jennifer multiplies that matrix by the 

141
00:10:25,336 --> 00:10:28,129
coordinates of a vector in her system, it will return the 90 degree 

142
00:10:28,129 --> 00:10:30,840
rotated version of that vector expressed in her coordinate system.

143
00:10:30,840 --> 00:10:34,300
In this case, the sweet spot comes when lambda equals 1.

144
00:10:35,220 --> 00:10:34,300
Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.

145
00:10:35,220 --> 00:10:43,308
A inverse times M times A, it suggests a mathematical sort of empathy. 

146
00:10:43,308 --> 00:10:47,980
That middle matrix represents a transform

147
00:10:47,980 --> 00:10:49,690
ation of some kind as you see it, and the outer two matrices represent the empathy, 

148
00:10:49,690 --> 00:10:50,200
the shift in perspective.

149
00:10:50,720 --> 00:10:54,172
And the full matrix product represents that same transformation, 

150
00:10:54,172 --> 00:10:58,633
but as someone else sees it. For those of you wondering why we care about alternate 

151
00:10:58,633 --> 00:11:03,200
coordinate systems, the next video on eigenvectors and eigenvalues will give a really 

152
00:11:03,200 --> 00:11:04,900
important example of this. See y

153
00:11:04,900 --> 00:11:11,760
That means there's a non-zero vector v such that A minus 

154
00:11:11,760 --> 00:11:18,620
lambda times the identity times v equals the zero vector.

155
00:11:18,620 --> 00:11:21,692
And remember, the reason we care about that is because it means A times v 

156
00:11:21,692 --> 00:11:24,764
equals lambda times v, which you can read off as saying that the vector v 

157
00:11:24,764 --> 00:11:27,920
is an eigenvector of A, staying on its own span during the transformation A.

158
00:11:28,840 --> 00:11:35,744
In this example, the corresponding eigenvalue is 1, 

159
00:11:35,744 --> 00:11:41,720
so v would actually just stay fixed in place.

160
00:11:41,720 --> 00:11:47,840
Pause and ponder if you need to make sure that that line of reasoning feels good.

161
00:11:48,740 --> 00:11:54,540
This is the kind of thing I mentioned in the introduction.

162
00:11:55,680 --> 00:12:00,482
If you didn't have a solid grasp of determinants and why they 

163
00:12:00,482 --> 00:12:05,517
relate to linear systems of equations having non-zero solutions, 

164
00:12:05,517 --> 00:12:10,320
an expression like this would feel completely out of the blue.

165
00:12:10,320 --> 00:12:15,824
To see this in action, let's revisit the example from the start, 

166
00:12:15,824 --> 00:12:19,720
with a matrix whose columns are 3, 0 and 1, 2.

167
00:12:21,080 --> 00:12:22,972
To find if a value lambda is an eigenvalue, subtract it from 

168
00:12:22,972 --> 00:12:24,740
the diagonals of this matrix and compute the determinant.

169
00:12:25,340 --> 00:12:28,723
Doing this, we get a certain quadratic polynomial in lambda, 

170
00:12:28,723 --> 00:12:30,720
3 minus lambda times 2 minus lambda.

171
00:12:30,720 --> 00:12:34,323
Since lambda can only be an eigenvalue if this determinant happens to be zero, 

172
00:12:34,323 --> 00:12:38,109
you can conclude that the only possible eigenvalues are lambda equals 2 and lambda 

173
00:12:38,109 --> 00:12:38,520
equals 3.

174
00:12:38,520 --> 00:12:50,262
To figure out what the eigenvectors are that actually have one of these eigenvalues, 

175
00:12:50,262 --> 00:13:00,347
say lambda equals 2, plug in that value of lambda to the matrix and then 

176
00:13:00,347 --> 00:13:09,880
solve for which vectors this diagonally altered matrix sends to zero.

177
00:13:11,480 --> 00:13:15,207
If you computed this the way you would any other linear system, 

178
00:13:15,207 --> 00:13:19,749
you'd see that the solutions are all the vectors on the diagonal line spanned 

179
00:13:19,749 --> 00:13:20,740
by negative 1, 1.

180
00:13:20,740 --> 00:13:23,330
This corresponds to the fact that the unaltered matrix, 3, 0, 1, 

181
00:13:23,330 --> 00:13:26,000
2, has the effect of stretching all those vectors by a factor of 2.

182
00:13:26,620 --> 00:13:31,140
Now, a 2D transformation doesn't have to have eigenvectors.

183
00:13:31,140 --> 00:13:36,420
For example, consider a rotation by 90 degrees.

184
00:13:36,420 --> 00:13:46,720
This doesn't have any eigenvectors since it rotates every vector off of its own span.

185
00:13:46,720 --> 00:13:55,848
If you actually try computing the eigenvalues of a rotation like this, 

186
00:13:55,848 --> 00:13:58,420
notice what happens.

187
00:13:58,420 --> 00:14:01,060
Its matrix has columns 0, 1 and negative 1, 0.

188
00:14:01,780 --> 00:14:03,020
Subtract off lambda from the diagonal elements and look for when the determinant is zero.

189
00:14:03,020 --> 00:14:08,340
In this case, you get the polynomial lambda squared plus 1.

190
00:14:09,420 --> 00:14:09,580
The only roots of that polynomial are the imaginary numbers, i and negative i.

191
00:14:09,580 --> 00:14:20,920
The fact that there are no real number solutions indicates that there are no eigenvectors.

192
00:14:20,920 --> 00:14:31,320
Another pretty interesting example worth holding in the back of your mind is a shear.

193
00:14:31,740 --> 00:14:43,620
This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.

194
00:14:43,620 --> 00:14:48,313
All of the vectors on the x-axis are eigenvectors 

195
00:14:48,313 --> 00:14:53,100
with eigenvalue 1 since they remain fixed in place.

196
00:14:53,100 --> 00:14:54,860
In fact, these are the only eigenvectors.

197
00:14:54,860 --> 00:15:00,475
When you subtract off lambda from the diagonals and compute the determinant, 

198
00:15:00,475 --> 00:15:03,320
what you get is 1 minus lambda squared.

199
00:15:03,320 --> 00:15:11,620
And the only root of this expression is lambda equals 1.

200
00:15:11,980 --> 00:15:13,028
This lines up with what we see geometrically, 

201
00:15:13,028 --> 00:15:14,100
that all of the eigenvectors have eigenvalue 1.

202
00:15:14,500 --> 00:15:16,290
Keep in mind though, it's also possible to have just one eigenvalue, 

203
00:15:16,290 --> 00:15:17,640
but with more than just a line full of eigenvectors.

204
00:15:17,640 --> 00:15:24,540
A simple example is a matrix that scales everything by 2.

205
00:15:24,540 --> 00:15:28,044
The only eigenvalue is 2, but every vector in the 

206
00:15:28,044 --> 00:15:31,760
plane gets to be an eigenvector with that eigenvalue.

207
00:15:31,760 --> 00:15:34,308
Now is another good time to pause and ponder some 

208
00:15:34,308 --> 00:15:36,500
of this before I move on to the last topic.

209
00:15:37,440 --> 00:15:42,402
I want to finish off here with the idea of an eigenbasis, 

210
00:15:42,402 --> 00:15:46,680
which relies heavily on ideas from the last video.

211
00:15:46,860 --> 00:15:51,900
Take a look at what happens if our basis vectors just so happen to be eigenvectors.

212
00:15:51,900 --> 00:15:53,200
For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.

213
00:15:53,200 --> 00:15:56,962
Writing their new coordinates as the columns of a matrix, 

214
00:15:56,962 --> 00:16:00,464
notice that those scalar multiples, negative 1 and 2, 

215
00:16:00,464 --> 00:16:05,718
which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, 

216
00:16:05,718 --> 00:16:07,600
and every other entry is a 0.

217
00:16:07,600 --> 00:16:09,194
Any time a matrix has zeros everywhere other than the diagonal, 

218
00:16:09,194 --> 00:16:10,440
it's called, reasonably enough, a diagonal matrix.

219
00:16:10,920 --> 00:16:13,287
And the way to interpret this is that all the basis vectors are eigenvectors, 

220
00:16:13,287 --> 00:16:15,260
with the diagonal entries of this matrix being their eigenvalues.

221
00:16:15,260 --> 00:16:15,680
There are a lot of things that make diagonal matrices much nicer to work with.

222
00:16:16,620 --> 00:16:17,066
One big one is that it's easier to compute what will happen 

223
00:16:17,066 --> 00:16:17,520
if you multiply this matrix by itself a whole bunch of times.

224
00:16:17,520 --> 00:16:20,495
Since all one of these matrices does is scale each basis vector by some eigenvalue, 

225
00:16:20,495 --> 00:16:22,194
applying that matrix many times, say 100 times, 

226
00:16:22,194 --> 00:16:24,992
is just going to correspond to scaling each basis vector by the 100th power of 

227
00:16:24,992 --> 00:16:26,020
the corresponding eigenvalue.

228
00:16:26,020 --> 00:16:27,160
In contrast, try computing the 100th power of a non-diagonal matrix.

229
00:16:27,160 --> 00:16:30,700
Really, try it for a moment.

230
00:16:30,700 --> 00:16:30,860
It's a nightmare.

231
00:16:30,860 --> 00:16:39,060
Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.

232
00:16:39,060 --> 00:16:39,060
But if your transformation has a lot of eigenvectors, 

233
00:16:39,060 --> 00:16:39,060
like the one from the start of this video, enough so that you can choose a set that 

234
00:16:39,060 --> 00:16:39,060
spans the full space, then you could change your coordinate system so that these 

235
00:16:39,060 --> 00:16:39,060
eigenvectors are your basis vectors.

236
00:16:39,060 --> 00:16:41,123
I talked about change of basis last video, but I'll go through 

237
00:16:41,123 --> 00:16:43,186
a super quick reminder here of how to express a transformation 

238
00:16:43,186 --> 00:16:45,380
currently written in our coordinate system into a different system.

239
00:16:45,426 --> 00:16:45,380
Take the coordinates of the vectors that you want to use as a new basis, 

240
00:16:45,549 --> 00:16:45,380
which in this case means our two eigenvectors, 

241
00:16:45,601 --> 00:16:45,380
then make those coordinates the columns of a matrix, known as the change of basis matrix.

242
00:16:45,648 --> 00:16:45,380
When you sandwich the original transformation, 

243
00:16:45,652 --> 00:16:45,426
putting the change of basis matrix on its right and the inverse of the 

244
00:16:45,687 --> 00:16:45,549
change of basis matrix on its left, the result will be a matrix representing 

245
00:16:45,718 --> 00:16:45,601
that same transformation, but from the perspective of the new basis 

246
00:16:45,815 --> 00:16:45,648
vectors coordinate system.

247
00:16:45,900 --> 00:16:45,652
The whole point of doing this with eigenvectors is that this new matrix is 

248
00:16:45,900 --> 00:16:45,687
guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.

249
00:16:45,900 --> 00:16:45,718
This is because it represents working in a coordinate system where what 

250
00:16:45,900 --> 00:16:45,815
happens to the basis vectors is that they get scaled during the transformation.

251
00:16:45,900 --> 00:16:46,033
A set of basis vectors which are also eigenvectors is called, 

252
00:16:46,033 --> 00:16:46,119
again, reasonably enough, an eigenbasis.

253
00:16:46,119 --> 00:16:46,120
So if, for example, you needed to compute the 100th power of this matrix, 

254
00:16:46,120 --> 00:16:46,120
it would be much easier to change to an eigenbasis, 

255
00:16:46,120 --> 00:16:46,120
compute the 100th power in that system, then convert back to our standard system.

256
00:16:46,120 --> 00:16:46,120
You can't do this with all transformations.

257
00:16:46,120 --> 00:16:46,120
A shear, for example, doesn't have enough eigenvectors to span the full space.

258
00:16:46,120 --> 00:16:46,120
But if you can find an eigenbasis, it makes matrix operations really lovely.

259
00:16:46,120 --> 00:16:46,120
For those of you willing to work through a pretty neat puzzle to 

260
00:16:46,120 --> 00:16:46,120
see what this looks like in action and how it can be used to produce 

261
00:16:46,120 --> 00:16:46,120
some surprising results, I'll leave up a prompt here on the screen.

262
00:16:46,120 --> 00:16:46,120
It takes a bit of work, but I think you'll enjoy it.

263
00:16:46,120 --> 00:16:46,120
The next and final video of this series is going to be on abstract vector spaces.

