[
 [
  "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  19.92,
  25.76
 ],
 [
  "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  25.76,
  33.26
 ],
 [
  "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  33.92,
  40.06
 ],
 [
  "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  40.68,
  46.36
 ],
 [
  "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  46.86,
  51.18
 ],
 [
  "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  51.52,
  58.48
 ],
 [
  "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  59.06,
  69.94
 ],
 [
  "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  70.72,
  79.24
 ],
 [
  "To start, consider some linear transformation in two dimensions, like the one shown here.",
  79.98,
  84.84
 ],
 [
  "st number indicates rightward motion, that the second one indicates upward motion, exactly how far a unit of distance is, all of that is tied up in the choice of i-hat and j-hat as the ve",
  85.46,
  101.26
 ],
 [
  "ctors which are scalar coordinates are meant to actually scale. Any way to translate between vectors and sets of numbers is called a coordinate system, and the",
  101.26,
  111.5
 ],
 [
  "two special vectors i-hat and j-hat are called the basis vectors of our standard coordinate system. What I'd like to talk about here is the idea of using a different set of basis vectors. For example, let's say you have a friend, Jennifer, who uses a different set of basis vectors, which I'll call b1 and b2. Her first b",
  111.5,
  131.32
 ],
 [
  "asis vector, b1, points up and to the right a little bit, and her second vector, b2, points left and up. Now take another look at that vector that I showed earlier, the one that you and I would describe using the coordinates 3,2, using our basis vectors i-hat and j-hat.",
  131.32,
  144.12
 ],
 [
  "Jennifer would actually describe this vector with the coordinates 5 thirds and 1 third. What this means is that the particular way to get to that vector using her two basis vectors is to scale b1 by 5 thirds, scale b2 by 1 third, then add them both together. In a little bi",
  146.32,
  166.8
 ],
 [
  "t, I'll show you how you could have figured out those two numbers, 5 thirds and 1 third. In general, whenever Jennifer uses coordinates to describe a vector, she thinks of her first coordinate as scali",
  166.8,
  182.62
 ],
 [
  "For this specific example, the basis vector i-hat is one such special vector.",
  182.62,
  184.26
 ],
 [
  "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  184.26,
  186.58
 ],
 [
  "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  186.58,
  198.08
 ],
 [
  "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  202.52,
  198.08
 ],
 [
  "Let me say a quick word about how I'm representing things here. When I animate 2D space, I typically use this square grid. But that grid is just a construc",
  202.52,
  214.46
 ],
 [
  "t, a way to visualize our coordinate system, and so it depends on our choice of basis. Space itself has no intrinsic grid. Jennifer might draw her own grid, which would be an equally made up construct meant as nothi",
  214.46,
  223.32
 ],
 [
  "ng more than a visual tool to help follow the meaning of her coordinates. Her origin though would actually line up with ours, since everybody agrees on what the coordinates 0,0 should mean. It's the thing that you get when you scale any vector by zero. But the direction of her axes and",
  223.4,
  244.86
 ],
 [
  "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  244.86,
  252.62
 ],
 [
  "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  252.62,
  258.14
 ],
 [
  "ks of as negative 1, 2. This process here of scaling each of her basis vectors by the corresponding coordinates of some vector, then adding them together, might feel somewhat familiar. It's matrix vector multiplication, with a matrix whose columns represent Jennifer's basis vectors in our language. In fact, once you understand matrix vector multiplication as applying a certain linear transformatio",
  258.14,
  291.84
 ],
 [
  "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  291.84,
  301.46
 ],
 [
  "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  301.46,
  307.02
 ],
 [
  "pretty intuitive way to think about what's going on here. A matrix whose columns represent Jennifer's basis vectors can be thought of as a transformation that moves our basis vectors, i-hat and j-hat, the things we think of when we say 1, 0 and 0, 1, to Jennifer's basis vectors, the thin",
  307.02,
  328.36
 ],
 [
  "gs she thinks of when she says 1, 0 and 0, 1. To show how this works, let's walk through what it would mean to take the vector that we think of as having coordinates negative 1, 2 and applying that transformation.",
  328.36,
  341.38
 ],
 [
  "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  341.38,
  354.82
 ],
 [
  "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  354.82,
  370.54
 ],
 [
  "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  371.12,
  380.62
 ],
 [
  "This pattern shows up a lot in linear algebra.",
  381.68,
  380.62
 ],
 [
  "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  381.68,
  386.04
 ],
 [
  "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  386.04,
  400.92
 ],
 [
  "we get using the same coordinates but in our system, then it transforms it into the vector that she really meant. What about going the other way around? In the example I used earlier this video, when I had the vector with coordinates 3, 2 in our system, how did I compute that it would have coordinates 5 thirds and 1",
  400.92,
  417.02
 ],
 [
  "Symbolically, here's what the idea of an eigenvector looks like.",
  417.02,
  421.28
 ],
 [
  "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  421.28,
  433.64
 ],
 [
  "e. In this case, the inverse of the change of basis matrix that has Jennifer's basis as its columns ends up working out to have columns 1 third, negative 1 third, and 1 third, 2 thirds. So for example, to see what the vector 3, 2 looks like in Jennifer's system, we multiply this inverse change of basis",
  434.42,
  453.82
 ],
 [
  "matrix by the vector 3, 2, which works out to be 5 thirds, 1 third. So that, in a nutshell, is how to translate the description of individual vectors back and forth between coordinate systems. The matrix whose columns represent Jennif",
  453.82,
  469.68
 ],
 [
  "er's basis vectors, but written in our coordinates, translates vectors from her language into our language. And the inverse matrix does the opposite. But vectors aren't the only thing that we describe using coordinates. For this next part, it's important that you're all comfortable representing transformations with matrices, and that you know how matrix multiplication",
  469.68,
  494.92
 ],
 [
  "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  494.92,
  498.6
 ],
 [
  "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  500.08,
  518.56
 ],
 [
  "the columns of our matrix. But this representation is heavily tied up in our choice of basis vectors, from the fact that we're following i-hat and j-hat in the first pla",
  520.48,
  531.54
 ],
 [
  "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  531.54,
  547.16
 ],
 [
  "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  547.16,
  555.64
 ],
 [
  "s land, and it needs to describe those landing spots in her language. Here's a common way to think of how this is done.",
  556.22,
  566.3
 ],
 [
  "What we want is a non-zero eigenvector.",
  568.32,
  574.54
 ],
 [
  "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  575.35,
  599.92
 ],
 [
  "And that squishification corresponds to a zero determinant for the matrix.",
  599.92,
  602.88
 ],
 [
  "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  602.88,
  608.84
 ],
 [
  "Now imagine tweaking lambda, turning a knob to change its value.",
  609.64,
  616.16
 ],
 [
  "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  616.16,
  619.34
 ],
 [
  "ou work through it, has columns one third, five thirds, and negative two thirds, negative one third. So if Jennifer multiplies that matrix by the coordinates of a vector in her system, it will return the 90 degree rotated version of that vector expressed in her coordinate system.",
  619.34,
  630.84
 ],
 [
  "In this case, the sweet spot comes when lambda equals 1.",
  630.84,
  634.3
 ],
 [
  "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  635.22,
  634.3
 ],
 [
  "A inverse times M times A, it suggests a mathematical sort of empathy. That middle matrix represents a transform",
  635.22,
  647.98
 ],
 [
  "ation of some kind as you see it, and the outer two matrices represent the empathy, the shift in perspective.",
  647.98,
  650.2
 ],
 [
  "And the full matrix product represents that same transformation, but as someone else sees it. For those of you wondering why we care about alternate coordinate systems, the next video on eigenvectors and eigenvalues will give a really important example of this. See y",
  650.72,
  664.9
 ],
 [
  "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  664.9,
  678.62
 ],
 [
  "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  678.62,
  687.92
 ],
 [
  "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  688.84,
  701.72
 ],
 [
  "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  701.72,
  707.84
 ],
 [
  "This is the kind of thing I mentioned in the introduction.",
  708.74,
  714.54
 ],
 [
  "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  715.68,
  730.32
 ],
 [
  "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  730.32,
  739.72
 ],
 [
  "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  741.08,
  744.74
 ],
 [
  "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  745.34,
  750.72
 ],
 [
  "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  750.72,
  758.52
 ],
 [
  "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  758.52,
  789.88
 ],
 [
  "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  791.48,
  800.74
 ],
 [
  "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  800.74,
  806.0
 ],
 [
  "Now, a 2D transformation doesn't have to have eigenvectors.",
  806.62,
  811.14
 ],
 [
  "For example, consider a rotation by 90 degrees.",
  811.14,
  816.42
 ],
 [
  "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  816.42,
  826.72
 ],
 [
  "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  826.72,
  838.42
 ],
 [
  "Its matrix has columns 0, 1 and negative 1, 0.",
  838.42,
  841.06
 ],
 [
  "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  841.78,
  843.02
 ],
 [
  "In this case, you get the polynomial lambda squared plus 1.",
  843.02,
  848.34
 ],
 [
  "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  849.42,
  849.58
 ],
 [
  "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  849.58,
  860.92
 ],
 [
  "Another pretty interesting example worth holding in the back of your mind is a shear.",
  860.92,
  871.32
 ],
 [
  "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  871.74,
  883.62
 ],
 [
  "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  883.62,
  893.1
 ],
 [
  "In fact, these are the only eigenvectors.",
  893.1,
  894.86
 ],
 [
  "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  894.86,
  903.32
 ],
 [
  "And the only root of this expression is lambda equals 1.",
  903.32,
  911.62
 ],
 [
  "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  911.98,
  914.1
 ],
 [
  "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  914.5,
  917.64
 ],
 [
  "A simple example is a matrix that scales everything by 2.",
  917.64,
  924.54
 ],
 [
  "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  924.54,
  931.76
 ],
 [
  "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  931.76,
  936.5
 ],
 [
  "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  937.44,
  946.68
 ],
 [
  "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  946.86,
  951.9
 ],
 [
  "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  951.9,
  953.2
 ],
 [
  "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  953.2,
  967.6
 ],
 [
  "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  967.6,
  970.44
 ],
 [
  "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  970.92,
  975.26
 ],
 [
  "There are a lot of things that make diagonal matrices much nicer to work with.",
  975.26,
  975.68
 ],
 [
  "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  976.62,
  977.52
 ],
 [
  "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  977.52,
  986.02
 ],
 [
  "In contrast, try computing the 100th power of a non-diagonal matrix.",
  986.02,
  987.16
 ],
 [
  "Really, try it for a moment.",
  987.16,
  990.7
 ],
 [
  "It's a nightmare.",
  990.7,
  990.86
 ],
 [
  "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  990.86,
  999.06
 ],
 [
  "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  999.06,
  999.06
 ],
 [
  "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  999.06,
  1005.38
 ],
 [
  "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  1005.9,
  1005.38
 ],
 [
  "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  1005.9,
  1005.38
 ],
 [
  "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  1005.9,
  1005.38
 ],
 [
  "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  1005.9,
  1005.38
 ],
 [
  "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  1005.9,
  1006.12
 ],
 [
  "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  1006.12,
  1006.12
 ],
 [
  "You can't do this with all transformations.",
  1006.12,
  1006.12
 ],
 [
  "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  1006.12,
  1006.12
 ],
 [
  "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  1006.12,
  1006.12
 ],
 [
  "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  1006.12,
  1006.12
 ],
 [
  "It takes a bit of work, but I think you'll enjoy it.",
  1006.12,
  1006.12
 ],
 [
  "The next and final video of this series is going to be on abstract vector spaces.",
  1006.12,
  1006.12
 ]
]