[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "",
  "from_community_srt": "في هذه الحالة ، يقوم المتجه بتنسيق [3 ، 2]، مما يعني الذهاب من ذيله إلى طرفه ينطوي على نقل 3 وحدات إلى اليمين و 2 يصل وحدات.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "",
  "from_community_srt": "الآن ، فإن الطريقة الخطية والجبر أكثر لوصف الإحداثيات هو التفكير في كل من هذه الأرقام ك العددية شيء يمتد أو يسحق المتجهات.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "",
  "from_community_srt": "أنت تفكر في هذا التنسيق الأول كتقويم أنا قبعة المتجه مع طول 1، مشيرا إلى حق في حين أن تنسيق الإحداثي الثاني j-hat المتجه مع طول 1 ، مشيرا على التوالي فوق.",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "",
  "from_community_srt": "غيض إلى مجموع ذيل تلك المتجهات المقياس هو ما تهدف الإحداثيات لوصفه.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "",
  "from_community_srt": "حقيقة أن الرقم الأول يشير إلى اليمين اقتراح أن الثاني يشير إلى الحركة التصاعدية بالضبط كم وحدة من المسافات.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "",
  "from_community_srt": "كل ذلك مرتبط باختيار آي-هات و j-hat مثل المتجهات التي هي إحداثيات العددية من المفترض أن نطاقها في الواقع.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "",
  "from_community_srt": "على أي حال للترجمة بين المتجهات والمجموعات من الأرقام يسمى نظام الإحداثيات واثنين من المتجهات الخاصة ، آي قبعة وجي هات ، تسمى المتجهات الأساسية لدينا نظام الإحداثيات القياسي.",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "",
  "from_community_srt": "ما أود التحدث عنه هنا هي فكرة استخدام مجموعة مختلفة من الأساس ثلاثة أبعاد.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "",
  "from_community_srt": "على سبيل المثال ، لنفترض أن لديك صديقًا ، جنيفر الذي يستخدم مجموعة مختلفة من المتجهات الأساسية والتي سأطلق عليها b1 و b2 أول نقطة أساسها ناقل B1 تصل إلى صحيح قليلا ونقطتها الثانية من ناقلات b2 تركت وأعلى الآن ، إلقاء نظرة أخرى على هذا المتجه ذلك لقد اظهرت في وقت سابق الشخص الذي نود وصفه باستخدامه الإحداثيات [3 ، 2] باستخدام لدينا ناقلات أساس i-hat و j-hat.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "",
  "from_community_srt": "جنيفر تصف فعلا هذا الناقل مع الإحداثيات [5/3، 1/3] ما يعنيه هذا هو أن طريقة معينة للوصول إلى هذا المتجه باستخدام اثنين من ناقلات أساسها هو قياس b1 بمقدار 5/3 ، المقياس b2 بمقدار 1/3 ثم نضيفهما معًا.",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "",
  "from_community_srt": "في القليل ، سأريك كيف يمكنك لقد برزت هذين الرقمين 5/3 و 1/3.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "",
  "from_community_srt": "بشكل عام ، عندما تستخدم جينيفر الإحداثيات لوصف المتجه تفكر في تنسيقها الأول كقياس B1 الإحداثي الثاني هو التحجيم b2 وتضيف النتائج.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "",
  "from_community_srt": "ما ستحصل عليه سيكون طبيعيا تماما مختلف من المتجه الذي نفكر به أنا وأنت من وجود تلك الإحداثيات.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "لتكون أكثر دقة حول الإعداد هنا لها أول ناقلات أساس b1 هو الشيء الذي نود وصفه مع إحداثيات [2 ، 1] و أساسها الثاني ناقلات b2 هو شيء نود وصفه بـ [-1 ، 1].",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "ولكن من المهم أن ندرك من وجهة نظرها في نظامها تلك المتجهات لها إحداثيات [1، 0] و [0 ، 1] هم ما يعرف معنى الإحداثيات [1 ، 0] و [0 ، 1] في عالمها.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "",
  "from_community_srt": "لذلك ، في الواقع ، نحن نتحدث لغات مختلفة نحن جميعا ننظر إلى نفس المتجهات في الفضاء لكن جنيفر تستخدم كلمات وأرقام مختلفة لوصفها.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "",
  "from_community_srt": "اسمحوا لي أن أقول كلمة سريعة حول كيف أنا أمثل الأشياء هنا عندما تحرك الفضاء 2D أنا عادة استخدام هذه الشبكة المربعة لكن هذه الشبكة هي مجرد بناء طريقة لتصور نظام الإحداثيات لدينا وذلك يعتمد على اختيارنا من الأساس.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "",
  "from_community_srt": "جنيفر قد ترسم شبكتها الخاصة والتي ستكون بناء متساوٍ يعني ليس أكثر من أداة بصرية للمساعدة في متابعة معنى إحداثياتها.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "",
  "from_community_srt": "هذا هو الشيء الذي تحصل عليه عند قياس أي متجه بنسبة 0.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "",
  "from_community_srt": "لكن اتجاه محاورها والتباعد بين خطوط الشبكة الخاصة بها سيكون مختلفًا ، اعتمادًا على اختيارها من ناقلات الأساس.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "",
  "from_community_srt": "لذلك ، بعد كل هذا تم إعداده سؤال طبيعي جدا أن نسأل هو كيف نترجم بين أنظمة الإحداثيات؟ إذا ، على سبيل المثال ، تصف جينيفر متجه مع الإحداثيات [-1 ، 2] ماذا سيكون ذلك في نظام الإحداثيات لدينا؟ كيف تترجم من لغتها إلى لنا؟ حسناً ، ما هي إحداثياتنا هو أن هذا المتجه هو -1 b1 + 2 b2.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "",
  "from_community_srt": "ومن وجهة نظرنا يحتوي b1 على إحداثيات [2، 1] و b2 لديه إحداثيات [-1، 1] حتى يمكننا حساب -1 b1 + 2 b2 بالفعل كما هي ممثلة في نظام الإحداثيات لدينا ويعمل هذا تحصل على متجه بإحداثيات [-4، 1] إذن ، هكذا سنصف المتجه انها تفكر في [-1 ، 2] هذه العملية هنا من تسلق كل أساس لها ثلاثة أبعاد من الإحداثيات المقابلة لبعض المتجهات ثم إضافتها معًا قد تبدو مألوفة إلى حد ما انها مضاعفة مكافحة ناقلات مع مصفوفة تمثل أعمدةها جينيفر ناقلات الأساس في لغتنا في الواقع ، بمجرد فهمك مصفوفة متجه عمليه الضرب كتطبيق تحول خطي معين قل ، من خلال مشاهدة ما كنت لك أن تكون أكثر من غيرها فيديو مهم في هذه السلسلة ، الفصل 3.",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "",
  "from_community_srt": "هناك طريقة بديهية للتفكير ماذا يجري هنا.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "",
  "from_community_srt": "مصفوفة تمثل أعمدةها جينيفر ناقلات الأساس يمكن اعتباره بمثابة تحول التي تحرك ناقلات الأساس لدينا ، أنا قبعة وجي هات الأشياء التي نفكر بها عندما نقول [1،0] و [0 ، 1] لمتجهات أساس جنيفر الأشياء التي تفكر بها عندما تقول [1 ، 0] و [0 ، 1] لإظهار كيف يعمل هذا دعونا نمشي ما سيعنيه لأخذ المتجه الذي نفكر فيه إحداثيات [-1 ، 2] وتطبيق هذا التحول.",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "",
  "from_community_srt": "قبل التحول الخطي نحن نفكر في هذا الناقل كمجموعة خطية معينة من أساسنا vectors -1 x i-hat + 2 x j-hat.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "",
  "from_community_srt": "والميزة الرئيسية للتحول الخطي هو أن المتجه الناتج سيكون ذلك نفس التركيبة الخطية ولكن من ناقلات أساس جديد -1 أضعاف المكان الذي تهبط فيه القبعة + مرتين المكان الذي يوجد فيه j-hat.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "",
  "from_community_srt": "فماذا تفعل هذه المصفوفة غيرت مفهومنا الخاطئ لما جينيفر يعني في المتجه الفعلي الذي تشير إليه إلى.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "",
  "from_community_srt": "أتذكر أنني عندما كنت أتعلم لأول مرة هذه لقد شعرت دائما بالوراء. هندسيا ، هذه المصفوفة يحول لدينا الشبكة في شبكة جنيفر.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "",
  "from_community_srt": "لكن من الناحية العددية ، إنها تترجم المتجه موصوفة بلغتها إلى لغتنا.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "",
  "from_community_srt": "ما الذي جعله ينقر في النهاية بالنسبة لي كان يفكر في كيف يأخذ الفهم الخاطئ لدينا مما تعنيه جينيفر المتجه نحصل على نفس الإحداثيات لكن في نظامنا ثم يحوله إلى المتجه ذلك انها تعني حقا.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "",
  "from_community_srt": "تذكر ، معكوس التحول هو تحول جديد يتوافق مع لعب ذلك أول واحد إلى الوراء.",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "",
  "from_community_srt": "في الواقع ، خاصة عندما تعمل في أكثر من بعدين كنت تستخدم جهاز كمبيوتر لحساب المصفوفة هذا يمثل هذا العكس.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "",
  "from_community_srt": "في هذه الحالة ، معكوس التغيير أساس الأساس لديها أساس جنيفر كأعمدة لها ينتهي بالعمل على وجود أعمدة [1/3 ، -1/3] و [1/3 ، 2/3] هكذا ، على سبيل المثال لمعرفة ما يبدو عليه المتجه [3 ، 2] نظام جنيفر نضرب هذا التغيير العكسي لمصفوفة الأساس من قبل المتجه [3 ، 2] الذي يعمل ليكون [5/3 ، 1/3] لذلك ، باختصار هو كيفية ترجمة وصف الفرد ثلاثة أبعاد ذهابا وإيابا بين أنظمة الإحداثيات.",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "",
  "from_community_srt": "المصفوفة التي تمثل الأعمدة جينيفر ناقلات الأساس لكن مكتوب في إحداثياتنا يترجم نواقل من لغتها إلى لغتنا. والمصفوفة العكسية تفعل العكس. لكن المتجهات ليست الشيء الوحيد الذي نحن وصف باستخدام الاحداثيات.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "",
  "from_community_srt": "لهذا الجزء التالي من المهم أن تكون مرتاحًا تمثل التحولات مع المصفوفات وأنك تعرف كيف الضرب المصفوفة يتوافق مع تأليف التحولات المتتالية.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "",
  "from_community_srt": "عندما كنت وأنا نمثل هذا مع المصفوفة نتابع حيث المتجهات أساس i-hat و ي-ك كل ذهاب.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "",
  "from_community_srt": "أنا قبعة ينتهي في الحال مع الإحداثيات [0 ، 1] و j-hat في نهاية المطاف مع إحداثيات [- 1 - 0] بحيث تصبح تلك الإحداثيات أعمدة لدينا المصفوفة لكن هذا التمثيل ترتبط بشدة في اختيارنا من الأساس ثلاثة أبعاد من حقيقة أننا نتبع i-hat و ي-قبعة في المقام الأول إلى حقيقة أننا نسجل هبوطهم بقع في نظام الإحداثيات الخاص بنا.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "",
  "from_community_srt": "كيف تصف جينيفر هذا الدوران 90 درجة نفسه من الفضاء؟ قد يميل إلى مجرد ترجم أعمدة مصفوفة الدوران الخاصة بنا في لغة جنيفر. لكن هذا ليس صحيحًا تمامًا.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "",
  "from_community_srt": "تمثل تلك الأعمدة أين متجهنا الأساسي أنا قبعة وجي هات. لكن المصفوفة التي تريدها جنيفر يجب أن تمثل حيث نواقل أساسها الأرض وتحتاج إلى وصف تلك النقاط الهبوط في لغتها. إليك طريقة شائعة للتفكير في كيفية حدوث ذلك فعله.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "",
  "from_community_srt": "تبدأ مع أي ناقلات مكتوبة في جنيفر لغة. بدلا من محاولة متابعة ما يحدث لها من حيث لغتها أولا ، سنقوم بترجمته إلى موقعنا لغة باستخدام تغيير مصفوفة الأساس الشخص الذي تمثل أعمدته أساسًا المتجهات في لغتنا.",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "هذا يعطينا نفس المتجه لكن الآن مكتوبة بلغتنا.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "",
  "from_community_srt": "ثم ، تطبيق مصفوفة التحويل على ما لقد حصلت بضربه على اليسار.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "",
  "from_community_srt": "هذا يخبرنا أين المتجهات الأراضي ولكن لا يزال في لغتنا.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "",
  "from_community_srt": "بما أننا يمكن أن نفعل هذا مع أي متجه مكتوب بلغتها أولا ، تطبيق تغيير الأساس ثم التحول ثم ، وتغير معكوس من الأساس هذا التكوين من ثلاث المصفوفات يعطينا مصفوفة التحويل في جنيفر لغة.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "",
  "from_community_srt": "يأخذ في متجه من لغتها وتبصق النسخة المحولة لذلك متجه بلغتها لهذا المثال بالتحديد عندما تبدو متجهات جينيفر الأساسية مثل [2 ، 1] و [-1 ، 1] بلغتنا وعندما يكون التحول دوران 90 درجة نتاج هذه المصفوفات الثلاثة إذا كنت تعمل من خلال ذلك يحتوي على أعمدة [1/3 ، 5/3] و [-2 / 3 ، -1/3] لذلك إذا ضربت جنيفر تلك المصفوفة من إحداثيات ناقل في نظامها فإنه يعود 90 درجة استدارة من هذا المتجه أعرب في نظام الإحداثيات الخاص بها.",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "",
  "from_community_srt": "بشكل عام ، عندما ترى تعبيرًا مثل A ^ (- 1) MA يقترح نوعا رياضيا من التعاطف.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "",
  "from_community_srt": "تمثل تلك المصفوفة الوسطى تحولًا من نوع ما ، كما ترونه والمصفوفات الخارجية تمثل التعاطف ، التحول في المنظور ومنتج المصفوفة الكامل يمثل ذلك نفس التحول ولكن كما يراه شخص آخر.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "",
  "from_community_srt": "لأولئك من أنت تتساءل لماذا نهتم أنظمة إحداثيات بديلة الفيديو التالي على ناقلات eigen و eigen القيم سوف يعطي مثالا هاما حقا من هذا.",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]