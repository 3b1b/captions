[
 {
  "input": "Hey everyone!",
  "translatedText": "안녕 모두들!",
  "model": "google_nmt",
  "from_community_srt": "불행하게도, 누구도 매트릭스가 무엇인지 말할 수 없습니다. 당신 스스로 찾아야만 합니다. - 모피어스 (영화 매트릭스 중에서) (행렬 연산을 시각적으로 이해시키는 놀라울정도로 적절한 문장) 안녕 모두들!",
  "n_reviews": 0,
  "start": 12.04,
  "end": 12.92
 },
 {
  "input": "If I had to choose just one topic that makes all of the others in linear algebra start to click, and which too often goes unlearned the first time a student takes linear algebra, it would be this one.",
  "translatedText": "선형 대수학의 다른 모든 주제를 클릭하게 만들고 학생이 선형 대수학을 처음 수강할 때 너무 자주 배우지 않게 되는 주제를 하나만 선택해야 한다면 바로 이 주제일 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "제가 선형대수에서 단지 하나의 주제를 선택해야 한다면, 특히 선형대수에 대해 하나도 모르는 학생을 위해서 하나 선택해야 한다면,",
  "n_reviews": 0,
  "start": 13.32,
  "end": 22.28
 },
 {
  "input": "The idea of a linear transformation and its relation to matrices.",
  "translatedText": "선형 변환의 아이디어와 행렬과의 관계.",
  "model": "google_nmt",
  "from_community_srt": "그것은 선형변환(linear transformation) 과 행렬과의 관계입니다.",
  "n_reviews": 0,
  "start": 22.7,
  "end": 26.2
 },
 {
  "input": "For this video, I'm just going to focus on what these transformations look like in the case of two dimensions, and how they relate to the idea of matrix vector multiplication.",
  "translatedText": "이 비디오에서는 2차원의 경우 이러한 변환이 어떻게 나타나는지, 그리고 행렬 벡터 곱셈 아이디어와 어떤 관련이 있는지에 중점을 둘 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 동영상에서는 2차원 예제를 통해 선형변환이 무엇인지에 관해 집중해보겠습니다. 그리고 행렬-벡터 곱셈과 어떤 관련이 있는지도 알아보겠습니다.",
  "n_reviews": 0,
  "start": 26.95,
  "end": 35.06
 },
 {
  "input": "In particular, I want to show you a way to think about matrix vector multiplication that doesn't rely on memorization.",
  "translatedText": "특히, 암기에 의존하지 않는 행렬 벡터 곱셈에 대해 생각하는 방법을 보여주고 싶습니다.",
  "model": "google_nmt",
  "from_community_srt": "특히, 행렬-벡터 곱셈을 단순암기말고 가능한 다른 방법도 있다는 것을 보여드리겠습니다.",
  "n_reviews": 0,
  "start": 35.88,
  "end": 42.08
 },
 {
  "input": "To start, let's just parse this term, linear transformation.",
  "translatedText": "시작하려면 선형 변환이라는 용어를 분석해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "시작하기 앞서, \"선형 변환\" 이라는 용어를 알아봅시다.",
  "n_reviews": 0,
  "start": 43.16,
  "end": 46.58
 },
 {
  "input": "Transformation is essentially a fancy word for function.",
  "translatedText": "변환은 본질적으로 기능에 대한 멋진 단어입니다.",
  "model": "google_nmt",
  "from_community_srt": "\"변환\" 은 근본적으로 \"함수\"의 다른말일 뿐입니다.",
  "n_reviews": 0,
  "start": 47.42,
  "end": 49.88
 },
 {
  "input": "It's something that takes in inputs and spits out an output for each one.",
  "translatedText": "그것은 입력을 받아들이고 각각에 대한 출력을 뱉어내는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "입력을 받고 결과물을 반환하는 그 무엇입니다.",
  "n_reviews": 0,
  "start": 50.26,
  "end": 53.98
 },
 {
  "input": "Specifically, in the context of linear algebra, we like to think about transformations that take in some vector and spit out another vector.",
  "translatedText": "특히 선형 대수학의 맥락에서 우리는 일부 벡터를 받아들이고 다른 벡터를 뱉어내는 변환에 대해 생각하는 것을 좋아합니다.",
  "model": "google_nmt",
  "from_community_srt": "선형대수 맥락으로 보자면, 특정 벡터를 다른 벡터로 바꾸는 변환같은 것입니다.",
  "n_reviews": 0,
  "start": 53.98,
  "end": 61.08
 },
 {
  "input": "So why use the word transformation instead of function if they mean the same thing?",
  "translatedText": "그렇다면 동일한 의미인 경우 함수 대신 변환이라는 단어를 사용하는 이유는 무엇입니까?",
  "model": "google_nmt",
  "from_community_srt": "그런데 같은 의미라면, 왜 굳이 \"함수(funcction)\" 라는 말대신 \"변환(transformation)\" 이라는 말을 사용하는 것일까요? 글쎄요,",
  "n_reviews": 0,
  "start": 62.5,
  "end": 66.38
 },
 {
  "input": "Well, it's to be suggestive of a certain way to visualize this input-output relation.",
  "translatedText": "글쎄요, 이는 이러한 입출력 관계를 시각화하는 특정 방법을 암시하는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "입력 - 출력 관계를 시각화하는 특정 방법을 암시해줍니다.",
  "n_reviews": 0,
  "start": 67.12,
  "end": 71.34
 },
 {
  "input": "You see, a great way to understand functions of vectors is to use movement.",
  "translatedText": "벡터의 기능을 이해하는 가장 좋은 방법은 움직임을 이용하는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "알다시피, 벡터 함수를 이해하는 가장 좋은 방법은 움직임으로 이해하는 것입니다.",
  "n_reviews": 0,
  "start": 71.86,
  "end": 75.8
 },
 {
  "input": "If a transformation takes some input vector to some output vector, we imagine that input vector moving over to the output vector.",
  "translatedText": "변환이 일부 입력 벡터를 일부 출력 벡터로 가져오는 경우 입력 벡터가 출력 벡터로 이동한다고 상상합니다.",
  "model": "google_nmt",
  "from_community_srt": "어떤 변환이 입력벡터를 출력벡터로 바꾼다면, 우리는 이것을 입력벡터를 이동시켜서 출력벡터로 만드는 것으로 생각해볼 수 있습니다.",
  "n_reviews": 0,
  "start": 76.78,
  "end": 84.86
 },
 {
  "input": "Then to understand the transformation as a whole, we might imagine watching every possible input vector move over to its corresponding output vector.",
  "translatedText": "그런 다음 변환을 전체적으로 이해하기 위해 가능한 모든 입력 벡터가 해당 출력 벡터로 이동하는 것을 상상해 볼 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 변환을 벡터들 모두에 적용한다고 생각해보면, 모든 가능한 입력벡터들을 가져다 움직여 그에 상응하는 결과벡터를 만들어내는 것을 상상해볼 수 있습니다.",
  "n_reviews": 0,
  "start": 85.68,
  "end": 94.08
 },
 {
  "input": "It gets really crowded to think about all of the vectors all at once, each one as an arrow.",
  "translatedText": "모든 벡터를 한꺼번에, 각각을 화살표로 생각하는 것은 정말 복잡해집니다.",
  "model": "google_nmt",
  "from_community_srt": "화살표로 그려진 모든 벡터들의 움직임을 한번에 생각하는 것은 혼란스럽습니다.",
  "n_reviews": 0,
  "start": 94.98,
  "end": 99.12
 },
 {
  "input": "So as I mentioned last video, a nice trick is to conceptualize each vector not as an arrow, but as a single point, the point where its tip sits.",
  "translatedText": "그래서 제가 지난 비디오에서 언급했듯이, 좋은 비결은 각 벡터를 화살표가 아닌 단일 점, 즉 끝이 있는 점으로 개념화하는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "제가 지난번 동영상에서 언급했다시피, 각 벡터를 개념화하는 방법은 화살표가 아니라, 하나의 점으로 생각하는 것입니다. 점 하나가 벡터 하나의 끝을 가리킵니다.",
  "n_reviews": 0,
  "start": 99.5,
  "end": 107.42
 },
 {
  "input": "That way, to think about a transformation taking every possible input vector to some output vector, we watch every point in space moving to some other point.",
  "translatedText": "그런 식으로 가능한 모든 입력 벡터를 일부 출력 벡터로 변환하는 변환을 생각하기 위해 공간의 모든 지점이 다른 지점으로 이동하는 것을 관찰합니다.",
  "model": "google_nmt",
  "from_community_srt": "이 방법은 어떤 변환이 입력벡터들을 출력벡터로 바꾸는 것을 쉽게 생각하게 해줍니다. 마치 공간상의 모든 점들이 다른 점으로 이동하는 것처럼 생각하면 됩니다.",
  "n_reviews": 0,
  "start": 108.03,
  "end": 116.34
 },
 {
  "input": "In the case of transformations in two dimensions, to get a better feel for the whole shape of the transformation, I like to do this with all of the points on an infinite grid.",
  "translatedText": "2차원 변환의 경우 변환의 전체 모양에 대한 더 나은 느낌을 얻기 위해 무한 그리드의 모든 점을 사용하여 이 작업을 수행하는 것을 좋아합니다.",
  "model": "google_nmt",
  "from_community_srt": "2차원에서 변환을 예로 살펴보면, 변환에 대한 전체 \"형태\"이 어떤가를 좀 더 쉽게 와닿을 겁니다. 저는 무한한 크기의 격자선을 만들고, 그 위의 점을 가지고 살펴보는 것을 좋아합니다.",
  "n_reviews": 0,
  "start": 117.22,
  "end": 125.78
 },
 {
  "input": "I also sometimes like to keep a copy of the grid in the background, just to help keep track of where everything ends up relative to where it starts.",
  "translatedText": "또한 나는 때때로 시작 위치와 관련하여 모든 것이 끝나는 위치를 추적하는 데 도움이 되도록 백그라운드에 그리드의 복사본을 유지하는 것을 좋아합니다.",
  "model": "google_nmt",
  "from_community_srt": "또 때로는 변경전 격자선을 뒷배경에 남겨두는 방법도 좋아하는데, 이렇게 하면 움직임 전 후를 추적해 보는데 도움이 됩니다.",
  "n_reviews": 0,
  "start": 126.56,
  "end": 132.84
 },
 {
  "input": "The effect for various transformations moving around all of the points in space is, you've got to admit, beautiful.",
  "translatedText": "공간의 모든 지점 주위를 이동하는 다양한 변형의 효과는 아름답습니다.",
  "model": "google_nmt",
  "from_community_srt": "다양한 변환들의 효과로 인한 공간상에서 움직이는 점들을 보고 있노라면, 당신도 느끼겠지만,",
  "n_reviews": 0,
  "start": 134.46,
  "end": 141.08
 },
 {
  "input": "It gives the feeling of squishing and morphing space itself.",
  "translatedText": "공간 자체가 찌그러지고 변형되는 느낌을 줍니다.",
  "model": "google_nmt",
  "from_community_srt": "아름답습니다. 공간 그 자체가 특수효과처럼 비틀리고 수축하는 느낌을 줍니다.",
  "n_reviews": 0,
  "start": 141.88,
  "end": 144.64
 },
 {
  "input": "As you can imagine though, arbitrary transformations can look pretty complicated.",
  "translatedText": "상상할 수 있듯이 임의의 변환은 꽤 복잡해 보일 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "당신도 생각하다시피, 임의 변환의 결과가 상당히 복잡해 보이지만, 다행히도,",
  "n_reviews": 0,
  "start": 145.6,
  "end": 149.92
 },
 {
  "input": "But luckily, linear algebra limits itself to a special type of transformation, ones that are easier to understand, called linear transformations.",
  "translatedText": "그러나 다행히도 선형 대수학은 선형 변환이라고 불리는 이해하기 쉬운 특수한 유형의 변환으로 제한됩니다.",
  "model": "google_nmt",
  "from_community_srt": "선형대수에서는 특수한 형태의 변환으로만 제한됩니다. 이름도 기억하기 쉬운 \"선형\" 변환입니다.",
  "n_reviews": 0,
  "start": 150.38,
  "end": 158.28
 },
 {
  "input": "Visually speaking, a transformation is linear if it has two properties.",
  "translatedText": "시각적으로 말하면 두 가지 속성이 있는 변환은 선형입니다.",
  "model": "google_nmt",
  "from_community_srt": "시작적으로 볼 때, 변환이 선형적(linear) 하다는 것은 두 가지 속성을 의미합니다.",
  "n_reviews": 0,
  "start": 159.12,
  "end": 163.06
 },
 {
  "input": "All lines must remain lines without getting curved, and the origin must remain fixed in place.",
  "translatedText": "모든 선은 휘어지지 않고 선을 유지해야 하며, 원점은 제자리에 고정되어 있어야 합니다.",
  "model": "google_nmt",
  "from_community_srt": "모든 선들은 변환 이후에도 휘지 않고 직선이어야 하며, 원점은 변환 이후에도 여전히 원점이여야 합니다.",
  "n_reviews": 0,
  "start": 163.7,
  "end": 169.6
 },
 {
  "input": "For example, this right here would not be a linear transformation, since the lines get all curvy.",
  "translatedText": "예를 들어, 여기 있는 선은 모두 곡선이 되기 때문에 선형 변환이 아닙니다.",
  "model": "google_nmt",
  "from_community_srt": "예를들어, 보이는 것같이 선이 휘어지게 만들어 지는 변환은 선형변환이 아닙니다.",
  "n_reviews": 0,
  "start": 170.62,
  "end": 175.54
 },
 {
  "input": "And this one right here, although it keeps the lines straight, is not a linear transformation, because it moves the origin.",
  "translatedText": "그리고 바로 여기 있는 것은 선을 직선으로 유지하지만 원점을 이동시키기 때문에 선형 변환이 아닙니다.",
  "model": "google_nmt",
  "from_community_srt": "그리고 이 변환은 비록 직선은 유지하지만, 원점이 이동하기 때문에 선형 변환이 아닙니다.",
  "n_reviews": 0,
  "start": 176.1,
  "end": 181.86
 },
 {
  "input": "This one here fixes the origin, and it might look like it keeps lines straight, but that's just because I'm only showing the horizontal and vertical grid lines.",
  "translatedText": "여기 이건 원점을 수정해서 선을 직선으로 유지하는 것처럼 보일 수도 있지만, 가로 및 세로 격자선만 표시하고 있기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "이번 것은 원점도 고정되어 있고 라인도 직선을 유지하는 것처럼 보이지만, 하지만, 이것은 제가 단지 수직선과 수평선만을 그렸기 때문입니다.",
  "n_reviews": 0,
  "start": 182.68,
  "end": 189.24
 },
 {
  "input": "When you see what it does to a diagonal line, it becomes clear that it's not at all linear, since it turns that line all curvy.",
  "translatedText": "이것이 대각선에 어떤 영향을 미치는지 보면, 그 선이 모두 곡선으로 변하기 때문에 전혀 선형이 아니라는 것이 분명해집니다.",
  "model": "google_nmt",
  "from_community_srt": "대각선을 그려보면, 직선이 아니라 곡선으로 바뀌는 것을 볼 수 있습니다.",
  "n_reviews": 0,
  "start": 189.54,
  "end": 195.32
 },
 {
  "input": "In general, you should think of linear transformations as keeping grid lines parallel and evenly spaced.",
  "translatedText": "일반적으로 선형 변환은 그리드 선을 평행하고 일정한 간격으로 유지하는 것으로 생각해야 합니다.",
  "model": "google_nmt",
  "from_community_srt": "일반적으로, 선형변환이라면 격자 라인들이 변형 이후에도 여전히 \"평행\"하고 \"동일한 간격\"으로 있어야 합니다.",
  "n_reviews": 0,
  "start": 196.76,
  "end": 202.24
 },
 {
  "input": "Some linear transformations are simple to think about, like rotations about the origin.",
  "translatedText": "일부 선형 변환은 원점에 대한 회전과 같이 생각하기 쉽습니다.",
  "model": "google_nmt",
  "from_community_srt": "어떤 선형변환의 경우에는 원점 기준으로 회전처럼 간단합니다.",
  "n_reviews": 0,
  "start": 203.4,
  "end": 207.54
 },
 {
  "input": "Others are a little trickier to describe with words.",
  "translatedText": "다른 것들은 말로 설명하기가 조금 더 까다롭습니다.",
  "model": "google_nmt",
  "from_community_srt": "하지만 어떤 변환은 쉽게 설명하기 까다롭습니다.",
  "n_reviews": 0,
  "start": 208.12,
  "end": 210.6
 },
 {
  "input": "So, how do you think you could describe these transformations numerically?",
  "translatedText": "그렇다면 이러한 변환을 수치적으로 어떻게 설명할 수 있다고 생각하시나요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 212.04,
  "end": 215.48
 },
 {
  "input": "If you were, say, programming some animations to make a video teaching the topic, what formula do you give the computer so that if you give it the coordinates of a vector, it can give you the coordinates of where that vector lands?",
  "translatedText": "예를 들어, 주제를 가르치는 비디오를 만들기 위해 애니메이션을 프로그래밍하는 경우 벡터의 좌표를 제공하면 해당 벡터가 도달하는 위치의 좌표를 제공할 수 있도록 컴퓨터에 어떤 공식을 제공합니까?",
  "model": "google_nmt",
  "from_community_srt": "그럼 이런 변환들을 수치적으로는 어떻게 설명할 수 있을까요? 당신이 만약 이 주제를 설명하기 위해 애니메이션 동영상을 프로그래밍 한다면, 어떤 공식을 컴퓨터에 넣어야, 벡터의 좌표값을 입력해서 결과 벡터 좌표값이 나오도록 할 수 있을까요? 결론은 두 개의 기저벡터 (i-hat,",
  "n_reviews": 0,
  "start": 215.48,
  "end": 227.24
 },
 {
  "input": "It turns out that you only need to record where the two basis vectors, i-hat and j-hat, each land, and everything else will follow from that.",
  "translatedText": "두 개의 기본 벡터인 i-hat과 j-hat, 각 랜드와 그 밖의 모든 것이 어디에서 따라오는지 기록하면 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "j-hat) 가 어떻게 변하는지만 알면 해결됩니다. 다른 벡터들은 이 기저벡터들로 구하면 그만입니다.",
  "n_reviews": 0,
  "start": 228.48,
  "end": 236.6
 },
 {
  "input": "For example, consider the vector v with coordinates negative 1, 2, meaning that it equals negative 1 times i-hat plus 2 times j-hat.",
  "translatedText": "예를 들어, 좌표가 음수 1, 2인 벡터 v를 생각해 보세요. 즉, 음수 1 x i-hat 더하기 2 x j-hat과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "예를 들어, 벡터 v (-1,2) 를 생각해봅시다. 좌표값은 i-hat 벡터의 -1배,",
  "n_reviews": 0,
  "start": 237.5,
  "end": 245.7
 },
 {
  "input": "If we play some transformation and follow where all three of these vectors go, the property that grid lines remain parallel and evenly spaced has a really important consequence.",
  "translatedText": "변환을 수행하고 이 세 벡터가 모두 어디로 가는지 따라가면 그리드 선이 평행하고 균일한 간격을 유지한다는 속성이 정말 중요한 결과를 가져옵니다.",
  "model": "google_nmt",
  "from_community_srt": "j-hat 벡터의 2배를 의미합니다. 어떤 변환을 적용시켜, 그 결과 이 세 벡터가 어디로 이동하는지 따라가보면, 매우 중요한 결과로 격자 선들이 계속 평행하고 균등하게 분포한다는 속성을 발견하게 됩니다.",
  "n_reviews": 0,
  "start": 248.68,
  "end": 258.3
 },
 {
  "input": "The place where v lands will be negative 1 times the vector where i-hat landed plus 2 times the vector where j-hat landed.",
  "translatedText": "v가 착지하는 장소는 i-hat이 착지한 벡터의 음수 1배에 j-hat이 착지한 벡터의 2배를 더한 값이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "변환 후 v 는 변환된 i-hat 벡터의 -1배, 변환된 j-hat 벡터의 2배입니다.",
  "n_reviews": 0,
  "start": 259.1,
  "end": 265.4
 },
 {
  "input": "In other words, it started off as a certain linear combination of i-hat and j-hat, and it ends up as that same linear combination of where those two vectors landed.",
  "translatedText": "즉, i-hat과 j-hat의 특정 선형 조합으로 시작하여 두 벡터가 도달한 동일한 선형 조합으로 끝납니다.",
  "model": "google_nmt",
  "from_community_srt": "즉, 변환전에 v벡터를 이루는 i-hat 과 j-hat 의 어떤 선형 결합이 변환 후에도 같은 선형결합을 유지합니다.",
  "n_reviews": 0,
  "start": 265.98,
  "end": 274.58
 },
 {
  "input": "This means you can deduce where v must go based only on where i-hat and j-hat each land.",
  "translatedText": "이는 i-hat과 j-hat이 각각 착륙한 위치만을 기반으로 v가 어디로 가야 하는지 추론할 수 있음을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "이 말은 단순히 i-hat 과 j-hat 의 변형위치만 알면, 벡터 v 를 추론할 수 있다는 것을 의미합니다.",
  "n_reviews": 0,
  "start": 275.62,
  "end": 280.92
 },
 {
  "input": "This is why I like keeping a copy of the original grid in the background.",
  "translatedText": "이것이 내가 배경에 원본 그리드의 복사본을 유지하는 것을 좋아하는 이유입니다.",
  "model": "google_nmt",
  "from_community_srt": "이것이 변환 전 격자선을 배경에 계속 그려놓는 이유입니다.",
  "n_reviews": 0,
  "start": 281.58,
  "end": 284.54
 },
 {
  "input": "For the transformation shown here, we can read off that i-hat lands on the coordinates 1, negative 2, and j-hat lands on the x-axis over at the coordinates 3, 0.",
  "translatedText": "여기에 표시된 변환의 경우 i-hat은 좌표 1, -2에 있고 j-hat은 x축 좌표 3, 0에 있음을 읽을 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "지금 본 변환에서, 우리는 i-hat 벡터가 변환전 좌표계의 (1, -2) 위치로 옮겨진 것을 볼 수 있습니다. j-hat 은 변환전 좌표의 (3, 0)에 있게됩니다.",
  "n_reviews": 0,
  "start": 285.08,
  "end": 294.94
 },
 {
  "input": "This means that the vector represented by negative 1 i-hat plus 2 times j-hat ends up at negative 1 times the vector 1, negative 2 plus 2 times the vector 3, 0.",
  "translatedText": "이는 -1 i-hat + 2 곱하기 j-hat으로 표현되는 벡터가 벡터 1의 -1 곱하기, -2 더하기 벡터 3, 0의 2배로 끝나는 것을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "따라서 (-1) i-hat + 2 j-hat 으로 나타낸 벡터는 변환후에는 (-1) (1, -2) + 2 (3, 0) 바뀌었습니다.",
  "n_reviews": 0,
  "start": 295.54,
  "end": 306.14
 },
 {
  "input": "Adding that all together, you can deduce that it has to land on the vector 5, 2.",
  "translatedText": "이를 모두 합치면 벡터 5, 2에 도달해야 한다는 것을 추론할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이를 종합하면, (5,",
  "n_reviews": 0,
  "start": 307.1,
  "end": 311.68
 },
 {
  "input": "This is a good point to pause and ponder, because it's pretty important.",
  "translatedText": "이것은 꽤 중요하기 때문에 잠시 멈춰서 생각해 보는 것이 좋습니다.",
  "model": "google_nmt",
  "from_community_srt": "2) 벡터가 됩니다. 꽤 중요하기 때문에, 여기서 잠깐 멈춰서 숙고해봅시다.",
  "n_reviews": 0,
  "start": 314.26,
  "end": 317.24
 },
 {
  "input": "Now, given that I'm actually showing you the full transformation, you could have just looked to see that v has the coordinates 5, 2.",
  "translatedText": "이제 실제로 전체 변환을 보여주고 있으므로 v의 좌표가 5, 2라는 것을 볼 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "지금, 실제로 변환이 어떤지 전과정을 보여주고 있기 때문에 벡터v 가  (5,",
  "n_reviews": 0,
  "start": 318.52,
  "end": 325.28
 },
 {
  "input": "But the cool part here is that this gives us a technique to deduce where any vectors land so long as we have a record of where i-hat and j-hat each land without needing to watch the transformation itself.",
  "translatedText": "그러나 여기서 멋진 부분은 변환 자체를 볼 필요 없이 i-hat과 j-hat이 각각 어디에 착지하는지에 대한 기록이 있는 한 벡터가 어디에 착지하는지 추론할 수 있는 기술을 제공한다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "2) 로 변환된 것을 바로 알 수 있습니다, 여기 멋진 부분은 이 방법으로 어떤 벡터든지 변환후에 어디로 이동할지 알아낼 수 있다는 것입니다. i-hat, j-hat 벡터 좌표를 알고있는 한, 변환이 어떤지를 볼 필요도 없습니다.",
  "n_reviews": 0,
  "start": 325.76,
  "end": 337.38
 },
 {
  "input": "Write the vector with more general coordinates, x and y, and it will land on x times the vector where i-hat lands, 1, negative 2, plus y times the vector where j-hat lands, 3, 0.",
  "translatedText": "보다 일반적인 좌표인 x와 y를 사용하여 벡터를 작성하면 i-hat이 착지하는 벡터의 x배(1, -2)에 j-hat이 착지하는 벡터의 y배(3, 0)에 착지하게 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "좀 더 일반화하자면, 벡터의 좌표값을 x, y 라 하면, 변환후 i-hat 벡터 (1, -2) 로부터 x 배와 변환후 j-hat 벡터(3,0) 의 y 배한 것을 합하면,",
  "n_reviews": 0,
  "start": 338.6,
  "end": 350.6
 },
 {
  "input": "Carrying out that sum, you see that it lands at 1x plus 3y, negative 2x plus 0y.",
  "translatedText": "그 합계를 계산하면 1x + 3y, -2x + 0y에 도달하는 것을 알 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "변환후 벡터를 구할 수 있습니다. 이제 그 합을 계산하면, 변환후 벡터의 위치는 (1x+3y, -2x+0y) 가 됩니다.",
  "n_reviews": 0,
  "start": 351.86,
  "end": 358.1
 },
 {
  "input": "I give you any vector, and you can tell me where that vector lands using this formula.",
  "translatedText": "제가 여러분에게 임의의 벡터를 제공하고 여러분은 이 공식을 사용하여 그 벡터가 어디에 도달하는지 말해 줄 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "제가 어떤 벡터를 제시하든, 당신은 이 공식을 계산을 하면 바로 결과벡터를 말할 수 있습니다.",
  "n_reviews": 0,
  "start": 358.74,
  "end": 363.58
 },
 {
  "input": "What all of this is saying is that a two-dimensional linear transformation is completely described by just four numbers, the two coordinates for where i-hat lands and the two coordinates for where j-hat lands.",
  "translatedText": "이 모든 것이 말하는 것은 2차원 선형 변환이 단지 4개의 숫자, 즉 i-hat이 착지하는 위치에 대한 두 좌표와 j-hat이 착지하는 위치에 대한 두 좌표로 완전히 설명된다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "이처럼 2차원 선형 변환을 통해 이 모든 것들이 오로지 4개의 숫자면 설명 가능합니다. 바로 변환된 i-hat 의 두 개의 좌표값과 변환된 j-hat 의 두 개의 좌표값이 그것입니다.",
  "n_reviews": 0,
  "start": 364.86,
  "end": 376.5
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "멋지지 않나요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.08,
  "end": 377.64
 },
 {
  "input": "It's common to package these coordinates into a 2x2 grid of numbers called a 2x2 matrix, where you can interpret the columns as the two special vectors where i-hat and j-hat each land.",
  "translatedText": "이러한 좌표를 2x2 행렬이라고 하는 숫자의 2x2 격자로 패키징하는 것이 일반적입니다. 여기서 열을 i-hat과 j-hat이 각각 배치되는 두 개의 특수 벡터로 해석할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "정말 멋지지 않나요? 이 좌표값들을 2x2 숫자형태로 표현하는게 일반적 입니다. 바로 2x2 행렬입니다. 행렬의 컬럼들을 i-hat, j-hat 두개의 특별한 벡터로 해석할 수 있습니다.",
  "n_reviews": 0,
  "start": 378.38,
  "end": 389.64
 },
 {
  "input": "If you're given a 2x2 matrix describing a linear transformation and some specific vector, and you want to know where that linear transformation takes that vector, you can take the coordinates of the vector, multiply them by the corresponding columns of the matrix, then add together what you get.",
  "translatedText": "선형 변환과 일부 특정 벡터를 설명하는 2x2 행렬이 주어졌고 해당 선형 변환이 해당 벡터를 사용하는 위치를 알고 싶다면 벡터의 좌표를 가져와 행렬의 해당 열을 곱한 다음 당신이 얻는 것을 합치십시오.",
  "model": "google_nmt",
  "from_community_srt": "만약 당신이 선형 변환을 묘사하는 2x2 행렬과 어떤 벡터를 주어진다면, 당신은 선형변환이 이 벡터를 어디로 변환시킬지 궁금할 것입니다. 일단 벡터의 좌표값을 취한다음에, 행렬의 대응되는 컬럼에 곱해줍니다. 그리고나서 합치면 얻을 수 있습니다.",
  "n_reviews": 0,
  "start": 390.38,
  "end": 407.34
 },
 {
  "input": "This corresponds with the idea of adding the scaled versions of our new basis vectors.",
  "translatedText": "이는 새로운 기본 벡터의 확장된 버전을 추가한다는 아이디어와 일치합니다.",
  "model": "google_nmt",
  "from_community_srt": "이것은 변환후 새 기저 벡터들로 스케일링하고 합한다는 개념입니다.",
  "n_reviews": 0,
  "start": 408.18,
  "end": 412.72
 },
 {
  "input": "Let's see what this looks like in the most general case, where your matrix has entries A, B, C, D.",
  "translatedText": "행렬에 항목 A, B, C, D가 있는 가장 일반적인 경우에 이것이 어떻게 보이는지 살펴보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "일반적인 경우에 어떻게 되는지 살펴봅시다. 행렬의 인자가 a, b, c, d 로된 일반적인 경우를 말이죠.",
  "n_reviews": 0,
  "start": 414.72,
  "end": 420.54
 },
 {
  "input": "And remember, this matrix is just a way of packaging the information needed to describe a linear transformation.",
  "translatedText": "그리고 기억하세요. 이 행렬은 선형 변환을 설명하는 데 필요한 정보를 패키징하는 방법일 뿐입니다.",
  "model": "google_nmt",
  "from_community_srt": "기억하세요. 이 행렬은 단순히 형태를 잡은 것일 뿐입니다. 바로 선형 변환을 나타낼 뿐입니다.",
  "n_reviews": 0,
  "start": 421.1,
  "end": 426.24
 },
 {
  "input": "Always remember to interpret that first column, AC, as the place where the first basis vector lands, and that second column, BD, as the place where the second basis vector lands.",
  "translatedText": "항상 첫 번째 열인 AC를 첫 번째 기저 벡터가 있는 위치로 해석하고, 두 번째 열인 BD를 두 번째 기저 벡터가 있는 위치로 해석해야 한다는 점을 기억하세요.",
  "model": "google_nmt",
  "from_community_srt": "항상 해석할때는 다음을 기억하세요. 첫번째 열 (a, c) 은 첫번째 기저벡터의 도착점이고, 그리고 두 번째 열 (b, d) 는, 두 번째 기저벡터의 도착점입니다.",
  "n_reviews": 0,
  "start": 426.24,
  "end": 436.44
 },
 {
  "input": "When we apply this transformation to some vector xy, what do you get?",
  "translatedText": "이 변환을 일부 벡터 xy에 적용하면 무엇을 얻게 됩니까?",
  "model": "google_nmt",
  "from_community_srt": "이 변환을 어떤 벡터(x, y) 에 적용하면 어떨 결과를 얻게될까요? 글쎄요.",
  "n_reviews": 0,
  "start": 437.5,
  "end": 441.0
 },
 {
  "input": "Well, it'll be x times AC plus y times BD.",
  "translatedText": "음, x 곱하기 AC 더하기 y 곱하기 BD가 될 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "그것은 (a, c) 의 x 배, 더하기, (b, d) 의 y 배 일 것입니다.",
  "n_reviews": 0,
  "start": 442.06,
  "end": 446.98
 },
 {
  "input": "Putting this together, you get a vector Ax plus By, Cx plus Dy.",
  "translatedText": "이것을 종합하면 벡터 Ax + By, Cx + Dy를 얻게 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "결론은 (ax+by, cx+dy) 벡터를 얻습니다.",
  "n_reviews": 0,
  "start": 448.06,
  "end": 453.3
 },
 {
  "input": "You could even define this as matrix vector multiplication, when you put the matrix on the left of the vector like it's a function.",
  "translatedText": "행렬을 함수처럼 벡터의 왼쪽에 배치하면 이를 행렬 벡터 곱셈으로 정의할 수도 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "당신은 행렬-벡터 곱셈으로 이와 똑같은 연산을 할 수 있습니다. 벡터 왼쪽에 행렬을 놓게되면, 이 행렬은 함수와 같아집니다.",
  "n_reviews": 0,
  "start": 453.98,
  "end": 460.94
 },
 {
  "input": "Then, you could make high schoolers memorize this without showing them the crucial part that makes it feel intuitive.",
  "translatedText": "그러면 고등학생들에게 직관적으로 느껴지게 하는 중요한 부분을 보여주지 않고도 이것을 암기하게 만들 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "그리고, 당신이 고등과정 학생에게 이것을 암기하게 할 수 있을 겁니다. 직관적으로 느꼈던 핵심부분을 보여주지 않고도 말입니다.",
  "n_reviews": 0,
  "start": 461.66,
  "end": 466.62
 },
 {
  "input": "But, isn't it more fun to think about these columns as the transformed versions of your basis vectors, and to think about the result as the appropriate linear combination of those vectors?",
  "translatedText": "하지만 이러한 열을 기저 벡터의 변환된 버전으로 생각하고 결과를 해당 벡터의 적절한 선형 조합으로 생각하는 것이 더 재미있지 않습니까?",
  "model": "google_nmt",
  "from_community_srt": "그러나, 행렬의 열을 이렇게 생각하는게 더 재밌습니다. 기저벡터의 변환된 형태로서 말입니다. 그리고 그 결과를 생각할때는 이 변환된 벡터들의 선형조합으로 여기것이 더 재밌습니다.",
  "n_reviews": 0,
  "start": 468.3,
  "end": 477.96
 },
 {
  "input": "Let's practice describing a few linear transformations with matrices.",
  "translatedText": "행렬을 사용하여 몇 가지 선형 변환을 설명하는 연습을 해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "몇가지 선형 변환을 나타내는 행렬을 가지고 연습을 해봅시다.",
  "n_reviews": 0,
  "start": 480.72,
  "end": 483.78
 },
 {
  "input": "For example, if we rotate all of space 90 degrees counterclockwise, then i-hat lands on the coordinates 0, 1.",
  "translatedText": "예를 들어, 모든 공간을 시계 반대 방향으로 90도 회전하면 i-hat은 좌표 0, 1에 착지합니다.",
  "model": "google_nmt",
  "from_community_srt": "예를 들어, 만약 모든 공간을 90 ° 시계 반대 방향으로 회전시키면, i-hat 벡터의 좌표값은 (0,",
  "n_reviews": 0,
  "start": 484.58,
  "end": 492.24
 },
 {
  "input": "And j-hat lands on the coordinates negative 1, 0.",
  "translatedText": "그리고 j-hat은 음수 1, 0 좌표에 도달합니다.",
  "model": "google_nmt",
  "from_community_srt": "1) 가 되고, j-hat 벡터의 좌표값은 (-1,",
  "n_reviews": 0,
  "start": 493.98,
  "end": 497.18
 },
 {
  "input": "So the matrix we end up with has columns 0, 1, negative 1, 0.",
  "translatedText": "따라서 우리가 끝나는 행렬에는 열 0, 1, 음수 1, 0이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "0)이 됩니다. 그래서 결론으로 얻은 행렬은 (0, 1), (-1,",
  "n_reviews": 0,
  "start": 497.98,
  "end": 501.96
 },
 {
  "input": "To figure out what happens to any vector after a 90-degree rotation, you could just multiply its coordinates by this matrix.",
  "translatedText": "90도 회전 후 벡터에 어떤 일이 발생하는지 파악하려면 해당 좌표에 이 행렬을 곱하면 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "0) 입니다. 90 ° 회전 한 후 다른 벡터들이 어떻게되는지 파악하기 위해, 단순히 이 행렬을 곱하기만 하면 됩니다.",
  "n_reviews": 0,
  "start": 502.88,
  "end": 509.62
 },
 {
  "input": "Here's a fun transformation with a special name, called a shear.",
  "translatedText": "여기 shear라고 불리는 특별한 이름을 가진 재미있는 변형이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "여기 \"shear\"라는 특별한 이름을 가진 흥미로운 변환이 있습니다.",
  "n_reviews": 0,
  "start": 511.56,
  "end": 514.3
 },
 {
  "input": "In it, i-hat remains fixed, so the first column of the matrix is 1, 0.",
  "translatedText": "그 안에서 i-hat은 고정된 상태로 유지되므로 행렬의 첫 번째 열은 1, 0입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 변환에서, i-hat 은 변하지 않아서 따라서 행렬의 첫번째 열은 (0,",
  "n_reviews": 0,
  "start": 515.0,
  "end": 519.16
 },
 {
  "input": "But j-hat moves over to the coordinates 1, 1, which become the second column of the matrix.",
  "translatedText": "그러나 j-hat은 행렬의 두 번째 열이 되는 좌표 1, 1로 이동합니다.",
  "model": "google_nmt",
  "from_community_srt": "1) 이지만, 그러나 j-hat 은 (1,1) 위치로 변합니다. 이것이 매트릭스의 두 번째 열이 됩니다.",
  "n_reviews": 0,
  "start": 519.6,
  "end": 525.3
 },
 {
  "input": "And at the risk of being redundant here, figuring out how a shear transforms a given vector comes down to multiplying this matrix by that vector.",
  "translatedText": "여기서 중복될 위험이 있으므로 전단이 주어진 벡터를 어떻게 변환하는지 알아내는 것은 이 행렬에 해당 벡터를 곱하는 것으로 귀결됩니다.",
  "model": "google_nmt",
  "from_community_srt": "불필요하게 반복하는 것 같지만?, 이 shear 변환이 벡터를 어떻게 변환시키는지 알아내는 것은 이 행렬에 벡터를 곱해 나가는 것과 같습니다.",
  "n_reviews": 0,
  "start": 525.3,
  "end": 534.08
 },
 {
  "input": "Let's say we want to go the other way around, starting with a matrix, say with columns 1, 2 and 3, 1, and we want to deduce what its transformation looks like.",
  "translatedText": "행렬에서 시작하여 열 1, 2, 3, 1로 시작하여 행렬의 변환이 어떻게 생겼는지 추론하고 싶다고 가정해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "다른 방향으로 생각해봅시다. 열 (1, 2), (1, 3) 인 행렬을 가지고, 이 변환이 어떤것인지 추론해 봅시다.",
  "n_reviews": 0,
  "start": 535.76,
  "end": 544.52
 },
 {
  "input": "Pause and take a moment to see if you can imagine it.",
  "translatedText": "잠시 멈춰서 상상할 수 있는지 살펴보세요.",
  "model": "google_nmt",
  "from_community_srt": "잠깐 멈추고, 잠시 시간을 갖고 그 변환이 어떨지 생각해봅시다.",
  "n_reviews": 0,
  "start": 544.96,
  "end": 547.44
 },
 {
  "input": "One way to do this is to first move i-hat to 1, 2, then move j-hat to 3, 1.",
  "translatedText": "이를 수행하는 한 가지 방법은 먼저 i-hat을 1, 2로 이동한 다음 j-hat을 3, 1로 이동하는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "이렇게하는 한 가지 방법은 우선 i-hat 을 (1, 2) 로 이동시킵니다. 그런 다음, j-hat 을 (3, 1)으로 이동시킵니다.",
  "n_reviews": 0,
  "start": 548.42,
  "end": 555.1
 },
 {
  "input": "Always moving the rest of space in such a way that keeps gridlines parallel and evenly spaced.",
  "translatedText": "눈금선이 평행하고 일정한 간격을 유지하도록 항상 나머지 공간을 이동하십시오.",
  "model": "google_nmt",
  "from_community_srt": "공간의 나머지 부분도 움직이는 데 격자선은 여전히 평행하고 규등한 간격을 유지하면서 움직입니다.",
  "n_reviews": 0,
  "start": 555.1,
  "end": 560.22
 },
 {
  "input": "If the vectors that i-hat and j-hat land on are linearly dependent, which, if you recall from last video, means that one is a scaled version of the other, it means that the linear transformation squishes all of 2D space onto the line where those two vectors sit, also known as the one-dimensional span of those two linearly dependent vectors.",
  "translatedText": "i-hat과 j-hat이 착지하는 벡터가 선형 종속적이라면(지난 비디오를 떠올려 보면 하나가 다른 하나의 크기가 조정된 버전이라는 의미) 이는 선형 변환이 모든 2D 공간을 두 벡터가 위치하는 선으로, 두 선형 종속 벡터의 1차원 범위라고도 합니다.",
  "model": "google_nmt",
  "from_community_srt": "i-hat 과 j-hat 벡터가 선형 종속(linearly dependent) 이라면 , 동영상 마지막에 다시 한번 정리할텐데, 벡터 하나가 다른벡터의 스케일링 버전임을 뜻합니다. 즉, 이 선형 변환은 2차원 공간을 수축(squish) 시켜 두 벡터가 놓여있는 선으로 만드는 것을 의미합니다. 1차원 스팬(span)으로, 이 선형 종속적인 두 벡터의 스팬입니다.",
  "n_reviews": 0,
  "start": 561.68,
  "end": 582.42
 },
 {
  "input": "To sum up, linear transformations are a way to move around space such that gridlines remain parallel and evenly spaced, and such that the origin remains fixed.",
  "translatedText": "요약하자면, 선형 변환은 격자선이 평행하고 균일한 간격을 유지하며 원점이 고정된 상태로 유지되도록 공간을 이동하는 방법입니다.",
  "model": "google_nmt",
  "from_community_srt": "요약하면, 선형변환은 공간을 이동시키는 방법이며, 격자선이 여전히 평행하고 균등간격을 유지한 변형입니다. 그리고 원점은 고정되있음을 의미합니다.",
  "n_reviews": 0,
  "start": 584.42,
  "end": 593.94
 },
 {
  "input": "Delightfully, these transformations can be described using only a handful of numbers, the coordinates of where each basis vector lands.",
  "translatedText": "다행스럽게도 이러한 변환은 각 기본 벡터가 도달하는 좌표인 소수의 숫자만 사용하여 설명할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "기쁘게도, 이 변환들을 간단한 숫자들로 설명가능합니다. 바로 기저벡터들의 변형후 좌표값입니다.",
  "n_reviews": 0,
  "start": 594.54,
  "end": 601.53
 },
 {
  "input": "Matrices give us a language to describe these transformations, where the columns represent those coordinates, and matrix-vector multiplication is just a way to compute what that transformation does to a given vector.",
  "translatedText": "행렬은 이러한 변환을 설명하는 언어를 제공합니다. 여기서 열은 해당 좌표를 나타내며, 행렬-벡터 곱셈은 해당 변환이 주어진 벡터에 대해 수행하는 작업을 계산하는 방법일 뿐입니다.",
  "model": "google_nmt",
  "from_community_srt": "행렬은 우리에게 이러한 변환을 설명하는 언어를 제공해줍니다. 행렬의 열들은 이 좌표값을 나타내며, 행렬 - 벡터 곱셈은 단지 이것을 계산하는 방법입니다. 이 변환이 주어진 벡터에 적용한 결과를요.",
  "n_reviews": 0,
  "start": 602.76,
  "end": 614.66
 },
 {
  "input": "The important takeaway here is that every time you see a matrix, you can interpret it as a certain transformation of space.",
  "translatedText": "여기서 중요한 점은 행렬을 볼 때마다 이를 공간의 특정 변형으로 해석할 수 있다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "여기서 중요한 것은 바로 이것입니다. 당신이 행렬을 볼때마다 공간의 어떤 변환으로 생각하십시오.",
  "n_reviews": 0,
  "start": 615.36,
  "end": 621.88
 },
 {
  "input": "Once you really digest this idea, you're in a great position to understand linear algebra deeply.",
  "translatedText": "이 아이디어를 실제로 소화하면 선형 대수학을 깊이 이해할 수 있는 좋은 위치에 있게 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "이 아이디어를 잘 습득하면, 당신은 선형 대수를 깊게 이해할 수있는 좋은 위치에 있게 됩니다.",
  "n_reviews": 0,
  "start": 622.58,
  "end": 627.32
 },
 {
  "input": "Almost all of the topics coming up, from matrix multiplication to determinants, change of basis, eigenvalues, all of these will become easier to understand once you start thinking about matrices as transformations of space.",
  "translatedText": "행렬 곱셈부터 행렬식, 기저 변화, 고유값에 이르기까지 앞으로 나올 거의 모든 주제는 행렬을 공간 변환으로 생각하기 시작하면 이해하기가 더 쉬워질 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "앞으로 다룰 거의 모든 주제들은, , 행렬 곱셈부터 행렬식(determinant)까지, 기저의 변환, 고유값(eigenvalues), 이 모든 것을 이해하기가 쉬워질 것입니다. 당신이 일단 행렬을 공간의 변형과 같이 생각하게 된다면, 그 다음은 바로,",
  "n_reviews": 0,
  "start": 627.66,
  "end": 640.56
 },
 {
  "input": "Most immediately, in the next video, I'll be talking about multiplying two matrices together.",
  "translatedText": "가장 즉시 다음 비디오에서는 두 행렬을 곱하는 것에 대해 이야기하겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "다음 동영상에 나오는, 두 행렬의 곱셈에 대한 것입니다.",
  "n_reviews": 0,
  "start": 641.3,
  "end": 646.32
 }
]