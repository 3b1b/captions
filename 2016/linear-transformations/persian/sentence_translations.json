[
 {
  "input": "Hey everyone! ",
  "translatedText": "هی همه! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 12.04,
  "end": 12.92
 },
 {
  "input": "If I had to choose just one topic that makes all of the others in linear algebra start to click, and which too often goes unlearned the first time a student takes linear algebra, it would be this one. ",
  "translatedText": "اگر بخواهم فقط یک مبحث را انتخاب کنم که همه موضوعات دیگر در جبر خطی شروع به کلیک کردن کنند، و اغلب در اولین باری که دانش‌آموزی جبر خطی را می‌خواند ناآموخته می‌شود، آن موضوع همین موضوع است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.32,
  "end": 22.28
 },
 {
  "input": "The idea of a linear transformation and its relation to matrices. ",
  "translatedText": "ایده تبدیل خطی و رابطه آن با ماتریس ها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 22.7,
  "end": 26.2
 },
 {
  "input": "For this video, I'm just going to focus on what these transformations look like in the case of two dimensions, and how they relate to the idea of matrix vector multiplication. ",
  "translatedText": "برای این ویدیو، من فقط بر روی این موضوع تمرکز می کنم که این تبدیل ها در مورد دو بعد چگونه به نظر می رسند، و چگونه آنها با ایده ضرب بردار ماتریس ارتباط دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.95,
  "end": 35.06
 },
 {
  "input": "In particular, I want to show you a way to think about matrix vector multiplication that doesn't rely on memorization. ",
  "translatedText": "به طور خاص، می‌خواهم راهی را به شما نشان دهم تا در مورد ضرب برداری ماتریس فکر کنید که به حفظ کردن متکی نیست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 35.88,
  "end": 42.08
 },
 {
  "input": "To start, let's just parse this term, linear transformation. ",
  "translatedText": "برای شروع، اجازه دهید فقط این عبارت، تبدیل خطی را تجزیه کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.16,
  "end": 46.58
 },
 {
  "input": "Transformation is essentially a fancy word for function. ",
  "translatedText": "دگرگونی اساساً یک کلمه فانتزی برای عملکرد است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 47.42,
  "end": 49.88
 },
 {
  "input": "It's something that takes in inputs and spits out an output for each one. ",
  "translatedText": "این چیزی است که ورودی ها را می گیرد و برای هر یک خروجی می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 50.26,
  "end": 53.98
 },
 {
  "input": "Specifically, in the context of linear algebra, we like to think about transformations that take in some vector and spit out another vector. ",
  "translatedText": "به طور خاص، در زمینه جبر خطی، ما دوست داریم در مورد تبدیل هایی فکر کنیم که بردار دیگری را می گیرند و بردار دیگری را بیرون می اندازند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 53.98,
  "end": 61.08
 },
 {
  "input": "So why use the word transformation instead of function if they mean the same thing? ",
  "translatedText": "پس چرا از کلمه transformation به جای تابع استفاده می کنیم اگر معنی آنها یکسان است؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.5,
  "end": 66.38
 },
 {
  "input": "Well, it's to be suggestive of a certain way to visualize this input-output relation. ",
  "translatedText": "خوب، باید راه خاصی را برای تجسم این رابطه ورودی-خروجی پیشنهاد کرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.12,
  "end": 71.34
 },
 {
  "input": "You see, a great way to understand functions of vectors is to use movement. ",
  "translatedText": "می بینید، یک راه عالی برای درک عملکرد بردارها استفاده از حرکت است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.86,
  "end": 75.8
 },
 {
  "input": "If a transformation takes some input vector to some output vector, we imagine that input vector moving over to the output vector. ",
  "translatedText": "اگر یک تبدیل مقداری از بردار ورودی را به برخی از بردارهای خروجی ببرد، تصور می کنیم که آن بردار ورودی به سمت بردار خروجی حرکت می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.78,
  "end": 84.86
 },
 {
  "input": "Then to understand the transformation as a whole, we might imagine watching every possible input vector move over to its corresponding output vector. ",
  "translatedText": "سپس برای درک کل تبدیل، ممکن است تصور کنیم که هر بردار ورودی ممکن را به سمت بردار خروجی مربوطه خود حرکت دهیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.68,
  "end": 94.08
 },
 {
  "input": "It gets really crowded to think about all of the vectors all at once, each one as an arrow. ",
  "translatedText": "فکر کردن به همه بردارها به طور همزمان، هر یک به عنوان یک فلش، واقعاً شلوغ می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 94.98,
  "end": 99.12
 },
 {
  "input": "So, as I mentioned last video, a nice trick is to conceptualize each vector not as an arrow but as a single point, the point where its tip sits. ",
  "translatedText": "بنابراین، همانطور که در ویدیوی گذشته ذکر کردم، یک ترفند خوب این است که هر بردار را نه به عنوان یک فلش، بلکه به عنوان یک نقطه واحد، نقطه ای که نوک آن قرار دارد، مفهوم سازی کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.5,
  "end": 107.42
 },
 {
  "input": "That way, to think about a transformation taking every possible input vector to some output vector, we watch every point in space moving to some other point. ",
  "translatedText": "به این ترتیب، برای فکر کردن در مورد تبدیلی که هر بردار ورودی ممکن را به بردار خروجی می‌برد، هر نقطه در فضا را در حال حرکت به نقطه دیگری تماشا می‌کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.03,
  "end": 116.34
 },
 {
  "input": "In the case of transformations in two dimensions, to get a better feel for the whole shape of the transformation, I like to do this with all of the points on an infinite grid. ",
  "translatedText": "در مورد تبدیل‌های دو بعدی، برای اینکه احساس بهتری نسبت به کل شکل تبدیل داشته باشم، دوست دارم این کار را با تمام نقاط روی یک شبکه بی‌نهایت انجام دهم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.22,
  "end": 125.78
 },
 {
  "input": "I also sometimes like to keep a copy of the grid in the background just to help keep track of where everything ends up relative to where it starts. ",
  "translatedText": "من همچنین گاهی اوقات دوست دارم یک کپی از شبکه را در پس‌زمینه نگه دارم فقط برای کمک به پیگیری اینکه همه چیز به کجا ختم می‌شود نسبت به جایی که شروع می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.56,
  "end": 132.84
 },
 {
  "input": "The effect for various transformations moving around all of the points in space is, you've got to admit, beautiful. ",
  "translatedText": "باید اعتراف کنید که تأثیر دگرگونی های مختلف که در اطراف تمام نقاط فضا حرکت می کنند، زیباست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.46,
  "end": 141.08
 },
 {
  "input": "It gives the feeling of squishing and morphing space itself. ",
  "translatedText": "این احساس له شدن و تغییر شکل فضا را به خود می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.88,
  "end": 144.64
 },
 {
  "input": "As you can imagine though, arbitrary transformations can look pretty complicated. ",
  "translatedText": "همانطور که می توانید تصور کنید، تغییر شکل های دلخواه می تواند بسیار پیچیده به نظر برسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 145.6,
  "end": 149.92
 },
 {
  "input": "But luckily, linear algebra limits itself to a special type of transformation, ones that are easier to understand, called linear transformations. ",
  "translatedText": "اما خوشبختانه، جبر خطی خود را به نوع خاصی از تبدیل محدود می کند، آنهایی که درک آن آسان تر است، به نام تبدیل های خطی. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.38,
  "end": 158.28
 },
 {
  "input": "Visually speaking, a transformation is linear if it has two properties. ",
  "translatedText": "از نظر بصری، یک تبدیل خطی است اگر دارای دو ویژگی باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.12,
  "end": 163.06
 },
 {
  "input": "All lines must remain lines without getting curved, and the origin must remain fixed in place. ",
  "translatedText": "همه خطوط باید بدون منحنی شدن خطوط باقی بمانند و مبدا باید در جای خود ثابت بماند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.7,
  "end": 169.6
 },
 {
  "input": "For example, this right here would not be a linear transformation, since the lines get all curvy. ",
  "translatedText": "به عنوان مثال، این دقیقاً در اینجا یک تبدیل خطی نخواهد بود، زیرا خطوط کاملاً منحنی هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 170.62,
  "end": 175.54
 },
 {
  "input": "And this one right here, although it keeps the lines straight, is not a linear transformation, because it moves the origin. ",
  "translatedText": "و این یکی در اینجا، اگرچه خطوط را مستقیم نگه می دارد، یک تبدیل خطی نیست، زیرا مبدا را حرکت می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.1,
  "end": 181.86
 },
 {
  "input": "This one here fixes the origin, and it might look like it keeps lines straight, but that's just because I'm only showing the horizontal and vertical grid lines. ",
  "translatedText": "این یکی در اینجا مبدا را ثابت می کند، و ممکن است به نظر برسد که خطوط را مستقیم نگه می دارد، اما این فقط به این دلیل است که من فقط خطوط شبکه افقی و عمودی را نشان می دهم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 182.68,
  "end": 189.24
 },
 {
  "input": "When you see what it does to a diagonal line, it becomes clear that it's not at all linear, since it turns that line all curvy. ",
  "translatedText": "وقتی می‌بینید با یک خط مورب چه می‌کند، مشخص می‌شود که اصلاً خطی نیست، زیرا آن خط را کاملاً منحنی می‌کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.54,
  "end": 195.32
 },
 {
  "input": "In general, you should think of linear transformations as keeping grid lines parallel and evenly spaced. ",
  "translatedText": "به طور کلی، شما باید تبدیل های خطی را به صورت موازی و یکسان نگه داشتن خطوط شبکه در نظر بگیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.76,
  "end": 202.24
 },
 {
  "input": "Some linear transformations are simple to think about, like rotations about the origin. ",
  "translatedText": "برخی از تبدیل‌های خطی مانند چرخش‌های مربوط به مبدا، ساده هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.4,
  "end": 207.54
 },
 {
  "input": "Others are a little trickier to describe with words. ",
  "translatedText": "توصیف دیگران با کلمات کمی دشوارتر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 208.12,
  "end": 210.6
 },
 {
  "input": "So how do you think you could describe these transformations numerically? ",
  "translatedText": "بنابراین فکر می کنید چگونه می توانید این دگرگونی ها را به صورت عددی توصیف کنید؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 212.04,
  "end": 215.48
 },
 {
  "input": "If you were, say, programming some animations to make a video teaching the topic, what formula do you give the computer so that if you give it the coordinates of a vector, it can give you the coordinates of where that vector lands? ",
  "translatedText": "اگر مثلاً چند انیمیشن را برنامه‌نویسی می‌کردید تا یک ویدیوی آموزش موضوع را بسازید، چه فرمولی به رایانه می‌دهید تا اگر مختصات یک بردار را به آن بدهید، بتواند مختصات محل فرود آن بردار را به شما بدهد؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 215.48,
  "end": 227.24
 },
 {
  "input": "It turns out that you only need to record where the two basis vectors, i-hat and j-hat, each land, and everything else will follow from that. ",
  "translatedText": "به نظر می رسد که شما فقط باید ثبت کنید که دو بردار پایه، i-hat و j-hat، هر زمین و هر چیز دیگری از آن نتیجه می گیرند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 228.48,
  "end": 236.6
 },
 {
  "input": "For example, consider the vector v with coordinates negative 1, 2, meaning that it equals negative 1 times i-hat plus 2 times j-hat. ",
  "translatedText": "به عنوان مثال، بردار v را با مختصات منفی 1، 2 در نظر بگیرید، به این معنی که برابر است با منفی 1 برابر i-hat به اضافه 2 برابر j-hat. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 237.5,
  "end": 245.7
 },
 {
  "input": "If we play some transformation and follow where all three of these vectors go, the property that grid lines remain parallel and evenly spaced has a really important consequence. ",
  "translatedText": "اگر مقداری تبدیل را انجام دهیم و دنبال کنیم که هر سه این بردارها کجا می روند، این ویژگی که خطوط شبکه موازی و با فواصل مساوی باقی می مانند، پیامد بسیار مهمی دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.68,
  "end": 258.3
 },
 {
  "input": "The place where v lands will be negative 1 times the vector where i-hat landed plus 2 times the vector where j-hat landed. ",
  "translatedText": "جایی که v فرود می آید 1 برابر بردار جایی که i-hat فرود آمد، منفی خواهد بود به اضافه 2 برابر بردار جایی که j-hat فرود آمد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 259.1,
  "end": 265.4
 },
 {
  "input": "In other words, it started off as a certain linear combination of i-hat and j-hat, and it ends up as that same linear combination of where those two vectors landed. ",
  "translatedText": "به عبارت دیگر، به عنوان یک ترکیب خطی مشخص از i-hat و j-hat شروع شد و به همان ترکیب خطی که آن دو بردار در آن فرود آمدند، ختم می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 265.98,
  "end": 274.58
 },
 {
  "input": "This means you can deduce where v must go based only on where i-hat and j-hat each land. ",
  "translatedText": "این بدان معنی است که شما می توانید استنباط کنید که v باید به کجا برود فقط بر اساس جایی که i-hat و j-hat در هر زمین قرار دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 275.62,
  "end": 280.92
 },
 {
  "input": "This is why I like keeping a copy of the original grid in the background. ",
  "translatedText": "به همین دلیل است که من دوست دارم یک کپی از شبکه اصلی را در پس زمینه نگه دارم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.58,
  "end": 284.54
 },
 {
  "input": "For the transformation shown here, we can read off that i-hat lands on the coordinates 1, negative 2, and j-hat lands on the x-axis over at the coordinates 3, 0. ",
  "translatedText": "برای تبدیل نشان داده شده در اینجا، می‌توانیم بخوانیم که i-hat روی مختصات 1، منفی 2، و j-hat روی محور x در مختصات 3، 0 قرار می‌گیرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.08,
  "end": 294.94
 },
 {
  "input": "This means that the vector represented by negative 1 i-hat plus 2 times j-hat ends up at negative 1 times the vector 1, negative 2 plus 2 times the vector 3, 0. ",
  "translatedText": "این به این معنی است که بردار منفی 1 i-hat به اضافه 2 برابر j-hat به منفی 1 برابر بردار 1، منفی 2 به اضافه 2 برابر بردار 3، 0 ختم می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 295.54,
  "end": 306.14
 },
 {
  "input": "Adding that all together, you can deduce that it has to land on the vector 5, 2. ",
  "translatedText": "با اضافه کردن همه اینها، می توانید استنباط کنید که باید روی بردار 5، 2 فرود آید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.1,
  "end": 311.68
 },
 {
  "input": "This is a good point to pause and ponder, because it's pretty important. ",
  "translatedText": "این نکته خوبی برای مکث و تأمل است، زیرا بسیار مهم است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 314.26,
  "end": 317.24
 },
 {
  "input": "Now, given that I'm actually showing you the full transformation, you could have just looked to see that v has the coordinates 5, 2. ",
  "translatedText": "حالا، با توجه به اینکه من در واقع تبدیل کامل را به شما نشان می‌دهم، می‌توانید فقط نگاه کنید تا ببینید v مختصات 5، 2 را دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 318.52,
  "end": 325.28
 },
 {
  "input": "But the cool part here is that this gives us a technique to deduce where any vectors land so long as we have a record of where i-hat and j-hat each land, without needing to watch the transformation itself. ",
  "translatedText": "اما بخش جالب اینجاست که این تکنیک به ما می‌دهد تا جایی که بردارها در کجا قرار می‌گیرند، تا زمانی که رکوردی از مکان i-hat و j-hat هر زمین داشته باشیم، بدون نیاز به تماشای خود تبدیل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 325.76,
  "end": 337.38
 },
 {
  "input": "Write the vector with more general coordinates, x and y, and it will land on x times the vector where i-hat lands, 1, negative 2, plus y times the vector where j-hat lands, 3, 0. ",
  "translatedText": "بردار را با مختصات کلی تر، x و y بنویسید، و بر روی x برابر بردار جایی که i-hat فرود می آید، 1، منفی 2، به علاوه y برابر بردار جایی که j-hat فرود می آید، 3، 0 قرار می گیرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.6,
  "end": 350.6
 },
 {
  "input": "Carrying out that sum, you see that it lands at 1x plus 3y, negative 2x plus 0y. ",
  "translatedText": "با انجام این مجموع، می بینید که در 1x به علاوه 3y، منفی 2x به علاوه 0y فرود می آید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.86,
  "end": 358.1
 },
 {
  "input": "I give you any vector, and you can tell me where that vector lands using this formula. ",
  "translatedText": "من هر بردار را به شما می‌دهم، و می‌توانید با استفاده از این فرمول به من بگویید که آن بردار کجا قرار می‌گیرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.74,
  "end": 363.58
 },
 {
  "input": "What all of this is saying is that a two-dimensional linear transformation is completely described by just four numbers, the two coordinates for where i-hat lands, and the two coordinates for where j-hat lands. ",
  "translatedText": "آنچه همه اینها می گویند این است که یک تبدیل خطی دو بعدی به طور کامل تنها با چهار عدد توصیف می شود، دو مختصات برای جایی که i-hat فرود می آید و دو مختصات برای جایی که j-hat فرود می آید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 364.86,
  "end": 376.5
 },
 {
  "input": "Isn't that cool? ",
  "translatedText": "این باحال نیست؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.08,
  "end": 377.64
 },
 {
  "input": "It's common to package these coordinates into a 2x2 grid of numbers called a 2x2 matrix, where you can interpret the columns as the two special vectors where i-hat and j-hat each land. ",
  "translatedText": "معمول است که این مختصات را در یک شبکه 2x2 از اعداد به نام ماتریس 2x2 بسته بندی کنید، جایی که می توانید ستون ها را به عنوان دو بردار خاص تفسیر کنید که i-hat و j-hat هر کدام در آن قرار می گیرند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.38,
  "end": 389.64
 },
 {
  "input": "If you're given a 2x2 matrix describing a linear transformation and some specific vector, and you want to know where that linear transformation takes that vector, you can take the coordinates of the vector, multiply them by the corresponding columns of the matrix, then add together what you get. ",
  "translatedText": "اگر به شما یک ماتریس 2x2 داده می شود که یک تبدیل خطی و چند بردار خاص را توصیف می کند، و می خواهید بدانید که آن تبدیل خطی آن بردار را کجا می برد، می توانید مختصات بردار را بگیرید، آنها را در ستون های مربوطه ماتریس ضرب کنید، سپس آنچه به دست می آورید را با هم اضافه کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.38,
  "end": 407.34
 },
 {
  "input": "This corresponds with the idea of adding the scaled versions of our new basis vectors. ",
  "translatedText": "این با ایده اضافه کردن نسخه های مقیاس شده بردارهای پایه جدید ما مطابقت دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 408.18,
  "end": 412.72
 },
 {
  "input": "Let's see what this looks like in the most general case, where your matrix has entries A, B, C, D. ",
  "translatedText": "بیایید ببینیم در کلی‌ترین حالت، جایی که ماتریس شما دارای ورودی‌های A، B، C، D است، چگونه به نظر می‌رسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 414.72,
  "end": 420.54
 },
 {
  "input": "And remember, this matrix is just a way of packaging the information needed to describe a linear transformation. ",
  "translatedText": "و به یاد داشته باشید، این ماتریس فقط راهی برای بسته بندی اطلاعات مورد نیاز برای توصیف یک تبدیل خطی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 421.1,
  "end": 426.24
 },
 {
  "input": "Always remember to interpret that first column, AC, as the place where the first basis vector lands, and that second column, BD, as the place where the second basis vector lands. ",
  "translatedText": "همیشه به یاد داشته باشید که ستون اول، AC، را به عنوان مکانی که بردار پایه اول فرود می آید، و ستون دوم، BD، به عنوان مکانی که بردار پایه دوم فرود می آید، تفسیر کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 426.24,
  "end": 436.44
 },
 {
  "input": "When we apply this transformation to some vector x, y, what do you get? ",
  "translatedText": "وقتی این تبدیل را به برخی از بردارهای x، y اعمال می کنیم، چه چیزی به دست می آید؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.5,
  "end": 441.0
 },
 {
  "input": "Well, it'll be x times AC plus y times BD. ",
  "translatedText": "خوب، x برابر AC به اضافه y برابر BD خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.06,
  "end": 446.98
 },
 {
  "input": "Putting this together, you get a vector Ax plus By, Cx plus Dy. ",
  "translatedText": "با کنار هم قرار دادن اینها، یک بردار Ax به اضافه By، Cx به علاوه Dy دریافت می کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 448.06,
  "end": 453.3
 },
 {
  "input": "You could even define this as matrix-vector multiplication when you put the matrix on the left of the vector like it's a function. ",
  "translatedText": "وقتی ماتریس را مانند یک تابع در سمت چپ بردار قرار می دهید، حتی می توانید این را به عنوان ضرب ماتریس-بردار تعریف کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.98,
  "end": 460.94
 },
 {
  "input": "Then you could make high schoolers memorize this without showing them the crucial part that makes it feel intuitive. ",
  "translatedText": "سپس می‌توانید دانش‌آموزان دبیرستانی را وادار کنید تا این را حفظ کنند، بدون اینکه بخش مهمی را که باعث می‌شود آن احساس بصری ایجاد کند، به آنها نشان دهید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 461.66,
  "end": 466.62
 },
 {
  "input": "But isn't it more fun to think about these columns as the transformed versions of your basis vectors, and to think about the result as the appropriate linear combination of those vectors? ",
  "translatedText": "اما آیا فکر کردن به این ستون‌ها به‌عنوان نسخه‌های تبدیل‌شده بردارهای پایه‌تان و فکر کردن به نتیجه به‌عنوان ترکیب خطی مناسب از آن بردارها سرگرم‌کننده‌تر نیست؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 468.3,
  "end": 477.96
 },
 {
  "input": "Let's practice describing a few linear transformations with matrices. ",
  "translatedText": "بیایید توصیف چند تبدیل خطی با ماتریس را تمرین کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 480.72,
  "end": 483.78
 },
 {
  "input": "For example, if we rotate all of space 90 degrees counterclockwise, then I-hat lands on the coordinates 0, 1, and J-hat lands on the coordinates negative 1, 0. ",
  "translatedText": "به عنوان مثال، اگر تمام فضا را 90 درجه در خلاف جهت عقربه های ساعت بچرخانیم، I-hat روی مختصات 0، 1 و J-hat روی مختصات منفی 1، 0 قرار می گیرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.58,
  "end": 497.18
 },
 {
  "input": "So the matrix we end up with has columns 0, 1, negative 1, 0. ",
  "translatedText": "بنابراین ماتریسی که در پایان به آن می رسیم دارای ستون های 0، 1، منفی 1، 0 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.98,
  "end": 501.96
 },
 {
  "input": "To figure out what happens to any vector after a 90-degree rotation, you could just multiply its coordinates by this matrix. ",
  "translatedText": "برای اینکه بفهمید بعد از یک چرخش 90 درجه برای هر بردار چه اتفاقی می افتد، فقط می توانید مختصات آن را در این ماتریس ضرب کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.88,
  "end": 509.62
 },
 {
  "input": "Here's a fun transformation with a special name, called a shear. ",
  "translatedText": "در اینجا یک دگرگونی سرگرم کننده با یک نام خاص به نام برش وجود دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.56,
  "end": 514.3
 },
 {
  "input": "In it, I-hat remains fixed, so the first column of the matrix is 1, 0, but J-hat moves over to the coordinates 1, 1, which become the second column of the matrix. ",
  "translatedText": "در آن، I-hat ثابت می ماند، بنابراین اولین ستون ماتریس 1، 0 است، اما J-hat به سمت مختصات 1، 1 حرکت می کند که به ستون دوم ماتریس تبدیل می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 515.0,
  "end": 525.3
 },
 {
  "input": "And at the risk of being redundant here, figuring out how a shear transforms a given vector comes down to multiplying this matrix by that vector. ",
  "translatedText": "و با خطر زائد بودن در اینجا، فهمیدن اینکه چگونه یک برش یک بردار معین را تبدیل می‌کند به ضرب این ماتریس در آن بردار ختم می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.3,
  "end": 534.08
 },
 {
  "input": "Let's say we want to go the other way around, starting with a matrix, say with columns 1, 2, and 3, 1, and we want to deduce what its transformation looks like. ",
  "translatedText": "فرض کنید می‌خواهیم برعکس برویم، با یک ماتریس شروع کنیم، مثلاً با ستون‌های 1، 2، و 3، 1، و می‌خواهیم استنباط کنیم که تبدیل آن چگونه است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 535.76,
  "end": 544.52
 },
 {
  "input": "Pause and take a moment to see if you can imagine it. ",
  "translatedText": "مکث کنید و کمی وقت بگذارید تا ببینید آیا می توانید آن را تصور کنید یا خیر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.96,
  "end": 547.44
 },
 {
  "input": "One way to do this is to first move I-hat to 1, 2, then move J-hat to 3, 1, always moving the rest of space in such a way that keeps gridlines parallel and evenly spaced. ",
  "translatedText": "یک راه برای انجام این کار این است که ابتدا I-hat را به 1، 2 منتقل کنید، سپس J-hat را به 3، 1 منتقل کنید، همیشه بقیه فضا را به گونه ای جابجا کنید که خطوط شبکه را موازی و با فاصله یکسان نگه دارید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 548.42,
  "end": 560.22
 },
 {
  "input": "If the vectors that I-hat and J-hat land on are linearly dependent, which, if you recall from last video, means that one is a scaled version of the other, it means that the linear transformation squishes all of 2D space onto the line where those two vectors sit, also known as the one-dimensional span of those two linearly dependent vectors. ",
  "translatedText": "اگر بردارهایی که I-hat و J-hat روی آن ها فرود می آیند به صورت خطی وابسته باشند، که، اگر از آخرین ویدیو به خاطر بیاورید، به این معنی است که یکی نسخه مقیاس شده دیگری است، به این معنی است که تبدیل خطی، تمام فضای دو بعدی را بر روی خطی که آن دو بردار در آن قرار دارند، که به عنوان دهانه یک بعدی آن دو بردار وابسته خطی نیز شناخته می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 561.68,
  "end": 582.42
 },
 {
  "input": "To sum up, linear transformations are a way to move around space such that gridlines remain parallel and evenly spaced, and such that the origin remains fixed. ",
  "translatedText": "به طور خلاصه، تبدیل های خطی راهی برای حرکت در فضا است به طوری که خطوط شبکه موازی و با فاصله یکسان باقی می مانند و به گونه ای که مبدا ثابت می ماند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 584.42,
  "end": 593.94
 },
 {
  "input": "Flightfully, these transformations can be described using only a handful of numbers, the coordinates of where each basis vector lands. ",
  "translatedText": "این دگرگونی‌ها را می‌توان با استفاده از تعداد معدودی از اعداد، مختصات جایی که هر بردار پایه قرار می‌گیرد، توصیف کرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 594.54,
  "end": 601.53
 },
 {
  "input": "Matrices give us a language to describe these transformations, where the columns represent those coordinates, and matrix-vector multiplication is just a way to compute what that transformation does to a given vector. ",
  "translatedText": "ماتریس ها زبانی را برای توصیف این تبدیل ها به ما می دهند، جایی که ستون ها آن مختصات را نشان می دهند، و ضرب ماتریس-بردار فقط راهی برای محاسبه کاری است که آن تبدیل با یک بردار معین انجام می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.76,
  "end": 614.66
 },
 {
  "input": "The important takeaway here is that every time you see a matrix, you can interpret it as a certain transformation of space. ",
  "translatedText": "نکته مهم در اینجا این است که هر بار که یک ماتریس را می بینید، می توانید آن را به عنوان تغییر شکل خاصی از فضا تفسیر کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.36,
  "end": 621.88
 },
 {
  "input": "Once you really digest this idea, you're in a great position to understand linear algebra deeply. ",
  "translatedText": "وقتی واقعاً این ایده را هضم کردید، در موقعیت عالی برای درک عمیق جبر خطی هستید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 622.58,
  "end": 627.32
 },
 {
  "input": "Almost all of the topics coming up, from matrix multiplication to determinants, change of basis, eigenvalues, all of these will become easier to understand once you start thinking about matrices as transformations of space. ",
  "translatedText": "تقریباً همه موضوعات مطرح شده، از ضرب ماتریس گرفته تا عوامل تعیین کننده، تغییر مبنا، مقادیر ویژه، وقتی شروع به فکر کردن در مورد ماتریس ها به عنوان تبدیل فضا کنید، درک همه اینها آسان تر خواهد شد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 627.66,
  "end": 640.56
 },
 {
  "input": "Most immediately, in the next video, I'll be talking about multiplying two matrices together. ",
  "translatedText": "بلافاصله، در ویدیوی بعدی، در مورد ضرب دو ماتریس با هم صحبت خواهم کرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 641.3,
  "end": 645.66
 },
 {
  "input": "See you then! ",
  "translatedText": "بعدا می بینمت! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.12,
  "end": 645.66
 },
 {
  "input": "Thank you for watching! ",
  "translatedText": "ممنون که تماشا کردید! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.12,
  "end": 646.32
 }
]