[
 {
  "input": "Hey everyone! ",
  "translatedText": "こんにちは、みなさん！",
  "model": "google_nmt",
  "from_community_srt": "残念ながら、行列は教えられて分かるものではない。 自分の目で見るしかない。 - モーフィアス 視覚的に行列演算を理解することの重要性を示す、 驚くほどに適切な言葉 こんにちは！",
  "n_reviews": 0,
  "start": 12.04,
  "end": 12.92
 },
 {
  "input": "If I had to choose just one topic that makes all of the others in linear algebra start to click, and which too often goes unlearned the first time a student takes linear algebra, it would be this one. ",
  "translatedText": "線形代数の他のすべてのトピックがピンとくるトピックを 1 つだけ選 択しなければならないとしたら、学生が初めて線形代数を学ぶときにあ まりにも頻繁に学習されなくなるトピックは、このトピックでしょう。",
  "model": "google_nmt",
  "from_community_srt": "線形代数で他の全てのトピックを学ぶ 起点になるようなトピックを選ぶなら、 初めて線形代数を学ぶ学生がしばしば理解できていないもの、 それは、線形変換と行列との関係の考え方です。",
  "n_reviews": 0,
  "start": 13.32,
  "end": 22.28
 },
 {
  "input": "The idea of a linear transformation and its relation to matrices. ",
  "translatedText": "線形変換の考え方と行列との関係。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 22.7,
  "end": 26.2
 },
 {
  "input": "For this video, I'm just going to focus on what these transformations look like in the case of two dimensions, and how they relate to the idea of matrix vector multiplication. ",
  "translatedText": "このビデオでは、これらの変換が 2 次元の場合にどのように見えるか、および それらが行列ベクトルの乗算の概念にどのように関連するかに焦点を当てます。",
  "model": "google_nmt",
  "from_community_srt": "この動画では、 2次元の場合に線形変換はどのようにに見えるか そしてそれらは行列 と ベクトルの積の考え方とどのように関係するか、について考えていきます。",
  "n_reviews": 0,
  "start": 26.95,
  "end": 35.06
 },
 {
  "input": "In particular, I want to show you a way to think about matrix vector multiplication that doesn't rely on memorization. ",
  "translatedText": "特に、暗記に頼らない行列ベクトルの乗 算の考え方を紹介したいと思います。",
  "model": "google_nmt",
  "from_community_srt": "特に、暗記に頼らない 行列とベクトルの積の考え方を示したいと思います。",
  "n_reviews": 0,
  "start": 35.88,
  "end": 42.08
 },
 {
  "input": "To start, let's just parse this term, linear transformation. ",
  "translatedText": "まず、線形変換という用語を解析してみましょう。",
  "model": "google_nmt",
  "from_community_srt": "まずは、この用語 「線形変換」を解析してみましょう。",
  "n_reviews": 0,
  "start": 43.16,
  "end": 46.58
 },
 {
  "input": "Transformation is essentially a fancy word for function. ",
  "translatedText": "トランスフォーメーションとは、本質的には機能を意味する派手な言葉です。",
  "model": "google_nmt",
  "from_community_srt": "「変換」は、本質的には「関数」を気取って言っただけの言葉です。",
  "n_reviews": 0,
  "start": 47.42,
  "end": 49.88
 },
 {
  "input": "It's something that takes in inputs and spits out an output for each one. ",
  "translatedText": "入力を受け取り、それぞれに対して出力を吐き出すものです。",
  "model": "google_nmt",
  "from_community_srt": "それは入力を得て、それぞれに対して 出力を吐き出すものです。",
  "n_reviews": 0,
  "start": 50.26,
  "end": 53.98
 },
 {
  "input": "Specifically, in the context of linear algebra, we like to think about transformations that take in some vector and spit out another vector. ",
  "translatedText": "具体的には、線形代数のコンテキストでは、あるベクトルを取り込 んで別のベクトルを吐き出す変換について考えるのが好きです。",
  "model": "google_nmt",
  "from_community_srt": "特に線形代数においては、ベクトルを受け取って 別のベクトルを返す変換を考えます。",
  "n_reviews": 0,
  "start": 53.98,
  "end": 61.08
 },
 {
  "input": "So why use the word transformation instead of function if they mean the same thing? ",
  "translatedText": "では、同じ意味であるのに、なぜ関数ではなく変換という言葉を使うのでしょうか? ",
  "model": "google_nmt",
  "from_community_srt": "では、同じ意味なのにどうして「関数」の代わりに 「変換」の単語を用いるのでしょうか？",
  "n_reviews": 0,
  "start": 62.5,
  "end": 66.38
 },
 {
  "input": "Well, it's to be suggestive of a certain way to visualize this input-output relation. ",
  "translatedText": "そうですね、この入出力関係を視覚化するための特定の方法を示唆するためです。",
  "model": "google_nmt",
  "from_community_srt": "まあ、 それは、この入出力の関係を視覚化する 方法があることを示唆するためです。",
  "n_reviews": 0,
  "start": 67.12,
  "end": 71.34
 },
 {
  "input": "You see, a great way to understand functions of vectors is to use movement. ",
  "translatedText": "ベクトルの機能を理解するための優れた方法は、動きを利用することです。",
  "model": "google_nmt",
  "from_community_srt": "ご存知のように、 ベクトルの関数を理解するのに最適な方法は、 運動を考えることです。",
  "n_reviews": 0,
  "start": 71.86,
  "end": 75.8
 },
 {
  "input": "If a transformation takes some input vector to some output vector, we imagine that input vector moving over to the output vector. ",
  "translatedText": "変換によって入力ベクトルが出力ベクトルに変換される場合、そ の入力ベクトルが出力ベクトルに移動することを想像します。",
  "model": "google_nmt",
  "from_community_srt": "変換が入力ベクトルから 出力ベクトルを得たとき、 私たちは入力ベクトルが 出力ベクトルまで動く様子を想像します。",
  "n_reviews": 0,
  "start": 76.78,
  "end": 84.86
 },
 {
  "input": "Then to understand the transformation as a whole, we might imagine watching every possible input vector move over to its corresponding output vector. ",
  "translatedText": "次に、変換を全体として理解するために、考えられるすべての入力ベクトルが対 応する出力ベクトルに移動するのを観察することを想像するかもしれません。",
  "model": "google_nmt",
  "from_community_srt": "そこで、変換全体を理解するために、 入力ベクトルになり得るもの全てが それに対応する出力ベクトルへと移動する様子を 想像します。",
  "n_reviews": 0,
  "start": 85.68,
  "end": 94.08
 },
 {
  "input": "It gets really crowded to think about all of the vectors all at once, each one as an arrow. ",
  "translatedText": "すべてのベクトルを一度に、それぞれを 矢印として考えるのは非常に面倒です。",
  "model": "google_nmt",
  "from_community_srt": "全てのベクトルを、それぞれが矢印のまま 一度に考えるのは本当に混乱します。",
  "n_reviews": 0,
  "start": 94.98,
  "end": 99.12
 },
 {
  "input": "So, as I mentioned last video, a nice trick is to conceptualize each vector not as an arrow but as a single point, the point where its tip sits. ",
  "translatedText": "前回のビデオで述べたように、優れたトリックは、各ベクトルを矢印では なく単一の点、つまりその先端が位置する点として概念化することです。",
  "model": "google_nmt",
  "from_community_srt": "なので、前回の動画で述べた通り、良い考え方は ベクトルを矢印ではなくその終点が指す点として 概念化することです。",
  "n_reviews": 0,
  "start": 99.5,
  "end": 107.42
 },
 {
  "input": "That way, to think about a transformation taking every possible input vector to some output vector, we watch every point in space moving to some other point. ",
  "translatedText": "このようにして、考えられるすべての入力ベクトルを何らかの出力ベクトルに変換す ることを考えるために、空間内のすべての点が他の点に移動するのを観察します。",
  "model": "google_nmt",
  "from_community_srt": "この方法で全ての入力ベクトルを 出力ベクトルに変換することを考えると、 空間の全ての点が別の点に移動する様子が見えます。",
  "n_reviews": 0,
  "start": 108.03,
  "end": 116.34
 },
 {
  "input": "In the case of transformations in two dimensions, to get a better feel for the whole shape of the transformation, I like to do this with all of the points on an infinite grid. ",
  "translatedText": "2 次元での変換の場合、変換の全体的な形状をよりよく理解する ために、無限グリッド上のすべての点でこれを行うのが好きです。",
  "model": "google_nmt",
  "from_community_srt": "二次元の変換の場合には、 変換全体の「形」を感じやすくするために、 無限に並んだ格子上の点で これをやるのが好きです。",
  "n_reviews": 0,
  "start": 117.22,
  "end": 125.78
 },
 {
  "input": "I also sometimes like to keep a copy of the grid in the background just to help keep track of where everything ends up relative to where it starts. ",
  "translatedText": "また、開始位置に対するすべての終了位置を追跡しやすくするために、グ リッドのコピーをバックグラウンドに保存しておきたい場合もあります。",
  "model": "google_nmt",
  "from_community_srt": "時々、背景に元の格子を 残しておくこともあります。 終了時の様子を開始時の様子と比較できるように するために。",
  "n_reviews": 0,
  "start": 126.56,
  "end": 132.84
 },
 {
  "input": "The effect for various transformations moving around all of the points in space is, you've got to admit, beautiful. ",
  "translatedText": "空間内のすべての点を移動するさまざまな変換 の効果は、認められるとおり、美しいです。",
  "model": "google_nmt",
  "from_community_srt": "種々の変換、空間の全ての点を動かすものは、 見ての通り、美しいです。",
  "n_reviews": 0,
  "start": 134.46,
  "end": 141.08
 },
 {
  "input": "It gives the feeling of squishing and morphing space itself. ",
  "translatedText": "空間自体を押しつぶして変形させるような感覚を与えます。",
  "model": "google_nmt",
  "from_community_srt": "見ての通り、美しいです。 これは、空間自身を潰したり 変形させているような感じがします。",
  "n_reviews": 0,
  "start": 141.88,
  "end": 144.64
 },
 {
  "input": "As you can imagine though, arbitrary transformations can look pretty complicated. ",
  "translatedText": "ただし、ご想像のとおり、任意の変換は非常に複雑に見える場合があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 145.6,
  "end": 149.92
 },
 {
  "input": "But luckily, linear algebra limits itself to a special type of transformation, ones that are easier to understand, called linear transformations. ",
  "translatedText": "しかし幸いなことに、線形代数は、線形変換と呼ばれる、 より理解しやすい特殊な種類の変換に限定されています。",
  "model": "google_nmt",
  "from_community_srt": "想像の通り、 任意の変換はかなり複雑に見えることもありますが、 幸いにも、線形代数の変換は特殊なものに 制限されています。 理解するのが容易な、「線形」変換と呼ばれます。",
  "n_reviews": 0,
  "start": 150.38,
  "end": 158.28
 },
 {
  "input": "Visually speaking, a transformation is linear if it has two properties. ",
  "translatedText": "視覚的に言えば、変換に 2 つのプロパティがある場合、変換は線形です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.12,
  "end": 163.06
 },
 {
  "input": "All lines must remain lines without getting curved, and the origin must remain fixed in place. ",
  "translatedText": "すべての線は曲がらずに線のままでなければならず、原点は所定の位置に固定されていなければなりません。",
  "model": "google_nmt",
  "from_community_srt": "視覚的には、二つの性質をもつ変換を 線形と呼びます： 全ての直線が 曲がったりせず直線のままであること、 そして、原点は空間に固定されていること。",
  "n_reviews": 0,
  "start": 163.7,
  "end": 169.6
 },
 {
  "input": "For example, this right here would not be a linear transformation, since the lines get all curvy. ",
  "translatedText": "たとえば、ここでは線がすべて曲線になっ ているため、線形変換ではありません。",
  "model": "google_nmt",
  "from_community_srt": "例えば、これは全ての線が曲がってしまっているので 線形ではありません。",
  "n_reviews": 0,
  "start": 170.62,
  "end": 175.54
 },
 {
  "input": "And this one right here, although it keeps the lines straight, is not a linear transformation, because it moves the origin. ",
  "translatedText": "そして、ここにあるものは、線を真っ直ぐに保ちます が、原点を移動するため、線形変換ではありません。",
  "model": "google_nmt",
  "from_community_srt": "そして、これは直線をまっすぐに維持していますが、 原点を動かしているので線形変換ではありません。",
  "n_reviews": 0,
  "start": 176.1,
  "end": 181.86
 },
 {
  "input": "This one here fixes the origin, and it might look like it keeps lines straight, but that's just because I'm only showing the horizontal and vertical grid lines. ",
  "translatedText": "これは原点を固定しており、線がまっすぐに保たれているように見えるかもしれま せんが、それは水平と垂直のグリッド線だけを表示しているだけであるためです。",
  "model": "google_nmt",
  "from_community_srt": "これは原点を固定して、 直線もまっすぐに維持しているかのように見えます。 しかしそれは、水平あるいは鉛直な格子線しか 表示していないからです。",
  "n_reviews": 0,
  "start": 182.68,
  "end": 189.24
 },
 {
  "input": "When you see what it does to a diagonal line, it becomes clear that it's not at all linear, since it turns that line all curvy. ",
  "translatedText": "対角線がどうなるかを見ると、その線がすべて曲線に なるため、まったく直線ではないことがわかります。",
  "model": "google_nmt",
  "from_community_srt": "対角線の動きを見れば、 これが線形ではないことは明らかです。",
  "n_reviews": 0,
  "start": 189.54,
  "end": 195.32
 },
 {
  "input": "In general, you should think of linear transformations as keeping grid lines parallel and evenly spaced. ",
  "translatedText": "一般に、線形変換はグリッド線を平行かつ等 間隔に保つことと考える必要があります。",
  "model": "google_nmt",
  "from_community_srt": "一般的には、線形変換は格子線を 平行かつ等間隔に維持するものと考えます。",
  "n_reviews": 0,
  "start": 196.76,
  "end": 202.24
 },
 {
  "input": "Some linear transformations are simple to think about, like rotations about the origin. ",
  "translatedText": "原点を中心とした回転など、一部の線形変換は考えるのが簡単です。",
  "model": "google_nmt",
  "from_community_srt": "原点まわりの回転のような線形変換は、 考えるのが簡単です。",
  "n_reviews": 0,
  "start": 203.4,
  "end": 207.54
 },
 {
  "input": "Others are a little trickier to describe with words. ",
  "translatedText": "言葉で説明するのが少し難しいものもあります。",
  "model": "google_nmt",
  "from_community_srt": "他のものは、 言葉で説明するには少しトリッキーです。",
  "n_reviews": 0,
  "start": 208.12,
  "end": 210.6
 },
 {
  "input": "So how do you think you could describe these transformations numerically? ",
  "translatedText": "では、これらの変化を数値的に説明するにはどうすればよいでしょうか? ",
  "model": "google_nmt",
  "from_community_srt": "それでは、これらの変換を数値的に表現するには どうすればいいでしょうか？",
  "n_reviews": 0,
  "start": 212.04,
  "end": 215.48
 },
 {
  "input": "If you were, say, programming some animations to make a video teaching the topic, what formula do you give the computer so that if you give it the coordinates of a vector, it can give you the coordinates of where that vector lands? ",
  "translatedText": "たとえば、トピックを教えるビデオを作成するためにアニメーションをプログラミ ングしている場合、コンピュータにベクトルの座標を与えると、そのベクトルが到 達する位置の座標が得られるように、どのような式をコンピュータに与えますか? ",
  "model": "google_nmt",
  "from_community_srt": "例えば、このトピックを動画で説明するために、 プログラミングでアニメーションを 作ることになったとしたら、 ベクトルの座標を入力したら 変換先のベクトルの座標を出力してくれるように、 コンピュータにどんな式を 与えればいいのでしょうか？",
  "n_reviews": 0,
  "start": 215.48,
  "end": 227.24
 },
 {
  "input": "It turns out that you only need to record where the two basis vectors, i-hat and j-hat, each land, and everything else will follow from that. ",
  "translatedText": "2 つの基底ベクトル、i-hat と j-hat、それぞれの着地がどこに あるかを記録するだけでよく、その他すべてはそこから続くことがわかります。",
  "model": "google_nmt",
  "from_community_srt": "実は、2つの基底ベクトル、 i ベクトルと j ベクトルの行き先だけを 記録しておけばよいのです。 他の全てのベクトルはそこから計算できます。",
  "n_reviews": 0,
  "start": 228.48,
  "end": 236.6
 },
 {
  "input": "For example, consider the vector v with coordinates negative 1, 2, meaning that it equals negative 1 times i-hat plus 2 times j-hat. ",
  "translatedText": "たとえば、座標が負の 1、2 であるベクトル v を考えます。これは、負の 1 倍 i-hat と 2 倍 j-hat に等しいことを意味します。",
  "model": "google_nmt",
  "from_community_srt": "例えば、座標が (-1, 2) のベクトル v を考えます。 つまり、i ベクトルの-1倍と j ベクトルの2倍との和に等しいベクトルです。",
  "n_reviews": 0,
  "start": 237.5,
  "end": 245.7
 },
 {
  "input": "If we play some transformation and follow where all three of these vectors go, the property that grid lines remain parallel and evenly spaced has a really important consequence. ",
  "translatedText": "何らかの変換を実行して、これら 3 つのベクトルすべてがどこに行くのかを追跡すると 、グリッド線が平行かつ等間隔に保たれるという特性が非常に重要な結果をもたらします。",
  "model": "google_nmt",
  "from_community_srt": "これに何かしらの変換を適用して 3つのベクトルの行き先を調べると、 格子線が平行かつ等間隔のままという性質は 本当に重要な結果をもたらします。",
  "n_reviews": 0,
  "start": 248.68,
  "end": 258.3
 },
 {
  "input": "The place where v lands will be negative 1 times the vector where i-hat landed plus 2 times the vector where j-hat landed. ",
  "translatedText": "v が着地する場所は、i-hat が着地したベクトルの 1 倍に、 j-hat が着地したベクトルの 2 倍を加えた負の値になります。",
  "model": "google_nmt",
  "from_community_srt": "v の行き先は、i ベクトルの行き先の-1倍 + j ベクトルの行き先の2倍なのです。",
  "n_reviews": 0,
  "start": 259.1,
  "end": 265.4
 },
 {
  "input": "In other words, it started off as a certain linear combination of i-hat and j-hat, and it ends up as that same linear combination of where those two vectors landed. ",
  "translatedText": "言い換えれば、それは i-hat と j-hat の特定の線形結合として始 まり、最終的にはこれら 2 つのベクトルが着地した同じ線形結合になります。",
  "model": "google_nmt",
  "from_community_srt": "言い換えれば、i ベクトルと j ベクトルの 特定の線形結合から出発し、 それら2つのベクトルの行き先の 同じ線形結合に移ったのです。",
  "n_reviews": 0,
  "start": 265.98,
  "end": 274.58
 },
 {
  "input": "This means you can deduce where v must go based only on where i-hat and j-hat each land. ",
  "translatedText": "これは、i-hat と j-hat がそれぞれ着地する場所のみに基づいて v がどこに行くべきかを推測できることを意味します。",
  "model": "google_nmt",
  "from_community_srt": "これは、i ベクトルと j ベクトルの行き先だけで v がどこに行くべきか推定できることを意味します。",
  "n_reviews": 0,
  "start": 275.62,
  "end": 280.92
 },
 {
  "input": "This is why I like keeping a copy of the original grid in the background. ",
  "translatedText": "これが、私が元のグリッドのコピーをバックグラウンドに保持しておくのが好きな理由です。",
  "model": "google_nmt",
  "from_community_srt": "背景に元の格子を残しておいたのは このためです。",
  "n_reviews": 0,
  "start": 281.58,
  "end": 284.54
 },
 {
  "input": "For the transformation shown here, we can read off that i-hat lands on the coordinates 1, negative 2, and j-hat lands on the x-axis over at the coordinates 3, 0. ",
  "translatedText": "ここに示す変換では、i-hat が座標 1、負の 2 に着地し、j -hat が座標 3、0 の x 軸に着地することが読み取れます。",
  "model": "google_nmt",
  "from_community_srt": "この線形変換においては、i ベクトルは 座標 (1, -2) に移り、 j ベクトルはx軸に乗って座標 (3, 0) に移る ことがわかります。",
  "n_reviews": 0,
  "start": 285.08,
  "end": 294.94
 },
 {
  "input": "This means that the vector represented by negative 1 i-hat plus 2 times j-hat ends up at negative 1 times the vector 1, negative 2 plus 2 times the vector 3, 0. ",
  "translatedText": "これは、負の 1 i-hat と j-hat の 2 倍で表されるベクトルは、最終的にベクトル 1 の負の 1 倍、負の 2 プラス ベクトル 3 の 2 倍、0 になることを意味します。",
  "model": "google_nmt",
  "from_community_srt": "これは、(-1) (i ベクトル) + 2 (j ベクトル) で表されるベクトルは (-1) 倍のベクトル (1, -2) + 2倍のベクトル (3, 0) に行くことを意味します。",
  "n_reviews": 0,
  "start": 295.54,
  "end": 306.14
 },
 {
  "input": "Adding that all together, you can deduce that it has to land on the vector 5, 2. ",
  "translatedText": "これらをすべて合計すると、ベクトル 5、2 に着地する必要があると推測できます。",
  "model": "google_nmt",
  "from_community_srt": "全て足し合わせれば、 行き先はベクトル (5,",
  "n_reviews": 0,
  "start": 307.1,
  "end": 311.68
 },
 {
  "input": "This is a good point to pause and ponder, because it's pretty important. ",
  "translatedText": "これは非常に重要なことなので、立ち止まって熟考するのに良いポイントです。",
  "model": "google_nmt",
  "from_community_srt": "2)であると推定できます。 ここはかなり重要なので、 一度立ち止まって熟考してみることをお勧めします。",
  "n_reviews": 0,
  "start": 314.26,
  "end": 317.24
 },
 {
  "input": "Now, given that I'm actually showing you the full transformation, you could have just looked to see that v has the coordinates 5, 2. ",
  "translatedText": "ここで、実際に完全な変換を示していることを考えると、 v の座標が 5、2 であることがわかるはずです。",
  "model": "google_nmt",
  "from_community_srt": "今、私は完全な変換の様子を見せていますが、 v が座標 (5, 2) を持っていることしか 見えないかもしれません。",
  "n_reviews": 0,
  "start": 318.52,
  "end": 325.28
 },
 {
  "input": "But the cool part here is that this gives us a technique to deduce where any vectors land so long as we have a record of where i-hat and j-hat each land, without needing to watch the transformation itself. ",
  "translatedText": "しかし、ここでの素晴らしい点は、i-hat と j-hat がそれ ぞれどこに着地するか記録があれば、変換自体を観察する必要がなく、ベ クトルがどこに着地するかを推定するテクニックが得られることです。",
  "model": "google_nmt",
  "from_community_srt": "しかし、ここで素晴らしいのは、 これがあらゆるベクトルの行き先を推定する技術を 提供していることです。 i ベクトルと j ベクトルの行き先を記録しておけば、 変換自体を見る必要はないのです。",
  "n_reviews": 0,
  "start": 325.76,
  "end": 337.38
 },
 {
  "input": "Write the vector with more general coordinates, x and y, and it will land on x times the vector where i-hat lands, 1, negative 2, plus y times the vector where j-hat lands, 3, 0. ",
  "translatedText": "より一般的な座標 x と y を使用してベクトルを記述すると、i-hat が着地するベクトルの x 倍、1、負の 2、および j-hat が着地するベクトルの y 倍、3、0 に着地します。",
  "model": "google_nmt",
  "from_community_srt": "より一般的に、座標 (x, y) のベクトルを考えます。 このベクトルの行き先は、i ベクトルの行き先である ベクトル (1, -2) の x 倍に、 j ベクトルの行き先である ベクトル (3,",
  "n_reviews": 0,
  "start": 338.6,
  "end": 350.6
 },
 {
  "input": "Carrying out that sum, you see that it lands at 1x plus 3y, negative 2x plus 0y. ",
  "translatedText": "この合計を実行すると、1x プラス 3y、マイナス 2x プラス 0y になることがわかります。",
  "model": "google_nmt",
  "from_community_srt": "0) の y 倍を足したものになります。 その和をとると、その行き先が (1x+3y, -2x+0y) であることが分かります。",
  "n_reviews": 0,
  "start": 351.86,
  "end": 358.1
 },
 {
  "input": "I give you any vector, and you can tell me where that vector lands using this formula. ",
  "translatedText": "任意のベクトルを与えると、この公式を使用してそのベクトルがどこに着地するかを教えてもらえます。",
  "model": "google_nmt",
  "from_community_srt": "任意のベクトルが与えられたら、この式で そのベクトルの行き先が分かります。",
  "n_reviews": 0,
  "start": 358.74,
  "end": 363.58
 },
 {
  "input": "What all of this is saying is that a two-dimensional linear transformation is completely described by just four numbers, the two coordinates for where i-hat lands, and the two coordinates for where j-hat lands. ",
  "translatedText": "これらすべてが言っているのは、2 次元の線形変換は、i-hat が着地する 2 つの座標と j-hat が着地する 2 つの 座標という 4 つの数値だけで完全に記述できるということです。",
  "model": "google_nmt",
  "from_community_srt": "これはつまり、 2次元の線形変換はちょうど4つの数字で 完全に記述できるということです。 i ベクトルの行き先である2次元の座標、 そして j ベクトルの行き先である 2 次元の座標です。",
  "n_reviews": 0,
  "start": 364.86,
  "end": 376.5
 },
 {
  "input": "Isn't that cool? ",
  "translatedText": "それはクールじゃないですか？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.08,
  "end": 377.64
 },
 {
  "input": "It's common to package these coordinates into a 2x2 grid of numbers called a 2x2 matrix, where you can interpret the columns as the two special vectors where i-hat and j-hat each land. ",
  "translatedText": "これらの座標を 2x2 行列と呼ばれる数値の 2x2 グリッドにパッ ケージ化するのが一般的です。ここで、列を i-hat と j-ha t がそれぞれ着地する 2 つの特別なベクトルとして解釈できます。",
  "model": "google_nmt",
  "from_community_srt": "クールでしょう？ ふつう、これらの座標は 2×2 の数の格子に並べられ、 「2×2行列」と呼ばれます。 その各行は、i ベクトルと j ベクトル それぞれのの行き先である 2つの特別なベクトルだと解釈できます。",
  "n_reviews": 0,
  "start": 378.38,
  "end": 389.64
 },
 {
  "input": "If you're given a 2x2 matrix describing a linear transformation and some specific vector, and you want to know where that linear transformation takes that vector, you can take the coordinates of the vector, multiply them by the corresponding columns of the matrix, then add together what you get. ",
  "translatedText": "線形変換を記述する 2x2 行列と特定のベクトルが与 えられ、その線形変換がそのベクトルをどこに取るかを知 りたい場合は、ベクトルの座標を取得し、それらを行列の 対応する列で乗算します。得られたものを合計します。",
  "model": "google_nmt",
  "from_community_srt": "線形変換を記述した2×2行列と いくつかのベクトルを与えられ、 線形変換を記述した2×2行列と ベクトルを与えられ、 その線形変換がそのベクトルを どこに移すか知りたいときは、 ベクトルの座標を取り、 行列の対応する列に掛け、 得られたものを足し合わせます。",
  "n_reviews": 0,
  "start": 390.38,
  "end": 407.34
 },
 {
  "input": "This corresponds with the idea of adding the scaled versions of our new basis vectors. ",
  "translatedText": "これは、新しい基底ベクトルのスケーリングされたバージョンを追加するという考えに対応します。",
  "model": "google_nmt",
  "from_community_srt": "これは、新たな基底ベクトルのスカラー倍を 足し合わせるという考えに対応します。",
  "n_reviews": 0,
  "start": 408.18,
  "end": 412.72
 },
 {
  "input": "Let's see what this looks like in the most general case, where your matrix has entries A, B, C, D. ",
  "translatedText": "行列にエントリ A、B、C、D がある最も一般的な ケースでこれがどのようになるかを見てみましょう。",
  "model": "google_nmt",
  "from_community_srt": "最も一般的な場合ではどう見えるか見てみましょう。 行列の要素を a, b, c,",
  "n_reviews": 0,
  "start": 414.72,
  "end": 420.54
 },
 {
  "input": "And remember, this matrix is just a way of packaging the information needed to describe a linear transformation. ",
  "translatedText": "この行列は、線形変換を記述するために必要な情報をパッケ ージ化する単なる方法であることを覚えておいてください。",
  "model": "google_nmt",
  "from_community_srt": "d として。 この行列は、線形変換を記述するのに必要な情報を まとめただけのものであることに注意してください。 この行列は、線形変換を記述するのに必要な情報を まとめただけのものであることに注意してください。",
  "n_reviews": 0,
  "start": 421.1,
  "end": 426.24
 },
 {
  "input": "Always remember to interpret that first column, AC, as the place where the first basis vector lands, and that second column, BD, as the place where the second basis vector lands. ",
  "translatedText": "最初の列 AC を最初の基底ベクトルが到着する場所として解釈し、2 番目の列 BD を 2 番目の基底ベクトルが到着する場所として解釈することを常に忘れないでください。",
  "model": "google_nmt",
  "from_community_srt": "最初の列 (a, c) は 最初の基底ベクトルの行き先で、 そして、第2列 (b, d) は、 2番目の基底ベクトルの行き先です。",
  "n_reviews": 0,
  "start": 426.24,
  "end": 436.44
 },
 {
  "input": "When we apply this transformation to some vector x, y, what do you get? ",
  "translatedText": "この変換をベクトル x、y に適用すると、何が得られるでしょうか? ",
  "model": "google_nmt",
  "from_community_srt": "この線形変換をベクトル (x, y) に適用したら、 何が得られるでしょうか？",
  "n_reviews": 0,
  "start": 437.5,
  "end": 441.0
 },
 {
  "input": "Well, it'll be x times AC plus y times BD. ",
  "translatedText": "そうですね、AC の x 倍と BD の y 倍になります。",
  "model": "google_nmt",
  "from_community_srt": "まあ、 それは x (a, c) + y (b,",
  "n_reviews": 0,
  "start": 442.06,
  "end": 446.98
 },
 {
  "input": "Putting this together, you get a vector Ax plus By, Cx plus Dy. ",
  "translatedText": "これをまとめると、ベクトル Ax と By、Cx と Dy が得られます。",
  "model": "google_nmt",
  "from_community_srt": "d) になるでしょう。 これをまとめると、ベクトル (ax+by, cx+dy) が得られます。",
  "n_reviews": 0,
  "start": 448.06,
  "end": 453.3
 },
 {
  "input": "You could even define this as matrix-vector multiplication when you put the matrix on the left of the vector like it's a function. ",
  "translatedText": "行列を関数のようにベクトルの左側に置くと、これを行 列とベクトルの乗算として定義することもできます。",
  "model": "google_nmt",
  "from_community_srt": "これを行列とベクトルの積として定義できます。 関数のように、 行列をベクトルの左側に置いたときに。 関数のように、 行列をベクトルの左側に置いたときに。",
  "n_reviews": 0,
  "start": 453.98,
  "end": 460.94
 },
 {
  "input": "Then you could make high schoolers memorize this without showing them the crucial part that makes it feel intuitive. ",
  "translatedText": "そうすれば、直感的に感じられる重要な部分を見せ ずに、高校生にこれを暗記させることができます。",
  "model": "google_nmt",
  "from_community_srt": "そして、これは高校生に 暗記してもらうこともできます。 これが直感的に感じられる重要な部分を 見せることなく。",
  "n_reviews": 0,
  "start": 461.66,
  "end": 466.62
 },
 {
  "input": "But isn't it more fun to think about these columns as the transformed versions of your basis vectors, and to think about the result as the appropriate linear combination of those vectors? ",
  "translatedText": "しかし、これらの列を基底ベクトルの変換バージョン として考え、その結果をそれらのベクトルの適切な 線形結合として考える方が楽しいと思いませんか? ",
  "model": "google_nmt",
  "from_community_srt": "だけど、 これらの列を基底ベクトルの行き先と考え、 これらの列を基底ベクトルの行き先と考え、 計算結果をそれらのベクトルの 適切な線形結合だと考えた方が、 楽しくはありませんか？ 計算結果をそれらのベクトルの 適切な線形結合だと考えた方が、 楽しくはありませんか？",
  "n_reviews": 0,
  "start": 468.3,
  "end": 477.96
 },
 {
  "input": "Let's practice describing a few linear transformations with matrices. ",
  "translatedText": "行列を使用していくつかの線形変換を記述する練習をしてみましょう。",
  "model": "google_nmt",
  "from_community_srt": "いくつかの線形変換を行列で記述してみましょう。",
  "n_reviews": 0,
  "start": 480.72,
  "end": 483.78
 },
 {
  "input": "For example, if we rotate all of space 90 degrees counterclockwise, then I-hat lands on the coordinates 0, 1, and J-hat lands on the coordinates negative 1, 0. ",
  "translatedText": "たとえば、空間全体を反時計回りに 90 度回転すると、I ハットは座標 0、1 に着地し、J ハットは座標のマイナス 1、0 に着地します。",
  "model": "google_nmt",
  "from_community_srt": "例えば、 空間全体を反時計回りに90度回転させると、 i ベクトルは座標 (0, 1) に、 j ベクトルは座標 (-1,",
  "n_reviews": 0,
  "start": 484.58,
  "end": 497.18
 },
 {
  "input": "So the matrix we end up with has columns 0, 1, negative 1, 0. ",
  "translatedText": "したがって、最終的に得られる行列の列は 0、1、負の 1、0 になります。",
  "model": "google_nmt",
  "from_community_srt": "0) に移ります。 よって、最終的に得られる行列は 列 (0, 1), (-1,",
  "n_reviews": 0,
  "start": 497.98,
  "end": 501.96
 },
 {
  "input": "To figure out what happens to any vector after a 90-degree rotation, you could just multiply its coordinates by this matrix. ",
  "translatedText": "90 度回転したベクトルに何が起こるかを調べる には、その座標にこの行列を乗算するだけです。",
  "model": "google_nmt",
  "from_community_srt": "0) を持っています。 90°回転した後、 任意のベクトルがどうなるかを知るには、 単にその座標にこの行列を掛ければ良いのです。",
  "n_reviews": 0,
  "start": 502.88,
  "end": 509.62
 },
 {
  "input": "Here's a fun transformation with a special name, called a shear. ",
  "translatedText": "ここでは、ハサミと呼ばれる特別な名前が付いた楽しい変身を紹介します。",
  "model": "google_nmt",
  "from_community_srt": "ここに、「せん断」と呼ばれる 面白い変換があります。",
  "n_reviews": 0,
  "start": 511.56,
  "end": 514.3
 },
 {
  "input": "In it, I-hat remains fixed, so the first column of the matrix is 1, 0, but J-hat moves over to the coordinates 1, 1, which become the second column of the matrix. ",
  "translatedText": "ここでは、I ハットは固定されたままであるため、行列の最初の列は 1, 0 で すが、J ハットは座標 1, 1 に移動し、それが行列の 2 列目になります。",
  "model": "google_nmt",
  "from_community_srt": "ここでは、i ベクトルは固定されたままで、 従って行列の最初の列は (1, 0) です。 しかし、j ベクトルは座標 (1, 1) に移動します。 これが行列の2列目になります。",
  "n_reviews": 0,
  "start": 515.0,
  "end": 525.3
 },
 {
  "input": "And at the risk of being redundant here, figuring out how a shear transforms a given vector comes down to multiplying this matrix by that vector. ",
  "translatedText": "そして、ここで冗長になる危険がありますが、せん断が与えられたベクトルをどのように変 換するかを理解することは、結局、この行列とそのベクトルを乗算することになります。",
  "model": "google_nmt",
  "from_community_srt": "そして、もう冗長かもしれませんが、 せん断が与えられたベクトルをどう変換するか 考えることは、 そのベクトルにこの行列を掛けることになります。",
  "n_reviews": 0,
  "start": 525.3,
  "end": 534.08
 },
 {
  "input": "Let's say we want to go the other way around, starting with a matrix, say with columns 1, 2, and 3, 1, and we want to deduce what its transformation looks like. ",
  "translatedText": "逆に、列 1、2、および 3、1 の行列から始めて 、その変換がどのようになるかを推定したいとします。",
  "model": "google_nmt",
  "from_community_srt": "別の方向から考えてみるとしましょう。 行列から始めます。 たとえば、列は (1, 2), (3, 1) だとしましょう。 その変換がどのように見えるか推測します。",
  "n_reviews": 0,
  "start": 535.76,
  "end": 544.52
 },
 {
  "input": "Pause and take a moment to see if you can imagine it. ",
  "translatedText": "少し立ち止まって、想像できるかどうかを確認してください。",
  "model": "google_nmt",
  "from_community_srt": "ちょっと立ち止まって、想像できるか 試してみてください。",
  "n_reviews": 0,
  "start": 544.96,
  "end": 547.44
 },
 {
  "input": "One way to do this is to first move I-hat to 1, 2, then move J-hat to 3, 1, always moving the rest of space in such a way that keeps gridlines parallel and evenly spaced. ",
  "translatedText": "これを行う 1 つの方法は、まず I ハットを 1、2 に移動し、次に J ハットを 3 、1 に移動し、常にグリッド線が平行かつ等間隔になるように残りのスペースを移動します。",
  "model": "google_nmt",
  "from_community_srt": "これを行う1つの方法は、 まず i ベクトルを (1, 2) に移動させ、 次に、j ベクトルを (3, 1) に移動させることです。 残りの空間は、 格子戦を平行で等間隔に保つように動かします。 残りの空間は、 格子戦を平行かつ等間隔に保つように動かします。",
  "n_reviews": 0,
  "start": 548.42,
  "end": 560.22
 },
 {
  "input": "If the vectors that I-hat and J-hat land on are linearly dependent, which, if you recall from last video, means that one is a scaled version of the other, it means that the linear transformation squishes all of 2D space onto the line where those two vectors sit, also known as the one-dimensional span of those two linearly dependent vectors. ",
  "translatedText": "I ハットと J ハットが着地するベクトルが線形依存している場合、これは、前回のビデオを思 い出していただけると、一方が他方のスケーリングされたバージョンであることを意味し、線形変換 によって 2D 空間全体が 2D 空間に押しつぶされることを意味します。これら 2 つのベ クトルが位置するライン。これら 2 つの線形依存ベクトルの 1 次元スパンとも呼ばれます。",
  "model": "google_nmt",
  "from_community_srt": "i ベクトルと j ベクトルの行き先が 線形従属だった場合、 前回の動画を思い出してくださいね、 一方が他方のスカラー倍であることを意味します。 これは、この線形変換が二次元空間の全てを これら2つのベクトルが収まる直線上に、 これは、この線形変換が二次元空間の全てを これら2つのベクトルが収まる直線上に、 言い換えれば これら2つの線形従属なベクトルのスパンに、 潰されてしまうことを意味します。 言い換えれば これら2つの線形従属なベクトルのスパンに、 潰してしまうことを意味します。",
  "n_reviews": 0,
  "start": 561.68,
  "end": 582.42
 },
 {
  "input": "To sum up, linear transformations are a way to move around space such that gridlines remain parallel and evenly spaced, and such that the origin remains fixed. ",
  "translatedText": "要約すると、線形変換は、グリッド線が平行かつ等間隔に保たれ、 原点が固定されたままになるように空間内を移動する方法です。",
  "model": "google_nmt",
  "from_community_srt": "まとめると、線形変換は空間を動かす手段です。 まとめると、線形変換は空間を動かす手段です。 格子線を平行かつ等間隔に保ち、 原点は固定するような。 格子線を平行かつ等間隔に保ち、 原点は固定するような。",
  "n_reviews": 0,
  "start": 584.42,
  "end": 593.94
 },
 {
  "input": "Flightfully, these transformations can be described using only a handful of numbers, the coordinates of where each basis vector lands. ",
  "translatedText": "これらの変換は、少数の数値、つまり各基底ベクトル が着地する座標のみを使用して簡単に記述できます。",
  "model": "google_nmt",
  "from_community_srt": "嬉しいことに、 これらの変換は、ほんの一握りの数字を使うことで 記述することができます。 すなわち、各基底ベクトルの行き先の座標です。",
  "n_reviews": 0,
  "start": 594.54,
  "end": 601.53
 },
 {
  "input": "Matrices give us a language to describe these transformations, where the columns represent those coordinates, and matrix-vector multiplication is just a way to compute what that transformation does to a given vector. ",
  "translatedText": "行列は、これらの変換を記述するための言語を提供します。列は それらの座標を表し、行列とベクトルの乗算は、その変換が特 定のベクトルに対して何を行うかを計算する単なる方法です。",
  "model": "google_nmt",
  "from_community_srt": "行列は、私たちにこれらの変換を記述するための言語を与えます。 列でその座標を表すことで。 そして、行列とベクトルの積は単に、 与えられたベクトルに変換が何をするかを 計算する方法です。",
  "n_reviews": 0,
  "start": 602.76,
  "end": 614.66
 },
 {
  "input": "The important takeaway here is that every time you see a matrix, you can interpret it as a certain transformation of space. ",
  "translatedText": "ここで重要なのは、マトリックスを見るたびに、それを 空間の特定の変換として解釈できるということです。",
  "model": "google_nmt",
  "from_community_srt": "ここで覚えておいてほしいことは、 行列を見れば、 それを空間の特定の変換として解釈できる ということです。 行列を見れば、 それを空間の特定の変換として解釈できる ということです。",
  "n_reviews": 0,
  "start": 615.36,
  "end": 621.88
 },
 {
  "input": "Once you really digest this idea, you're in a great position to understand linear algebra deeply. ",
  "translatedText": "この考え方をしっかり理解すると、線形 代数を深く理解できるようになります。",
  "model": "google_nmt",
  "from_community_srt": "この考え方を本当に消化できれば、 あなたは線形代数を深く理解するのに最適な位置にいます。",
  "n_reviews": 0,
  "start": 622.58,
  "end": 627.32
 },
 {
  "input": "Almost all of the topics coming up, from matrix multiplication to determinants, change of basis, eigenvalues, all of these will become easier to understand once you start thinking about matrices as transformations of space. ",
  "translatedText": "行列の乗算から行列式、基底の変更、固有値に至るまで、今 後出てくるほぼすべてのトピックは、行列を空間の変換とし て考え始めると、より簡単に理解できるようになります。",
  "model": "google_nmt",
  "from_community_srt": "現れるほとんど全てのトピックが、 行列の積から行列式に、 基底の変換、固有値まで… これらのすべてが理解しやすくなります。 ひとたび行列を空間の変換だと考えれば。",
  "n_reviews": 0,
  "start": 627.66,
  "end": 640.56
 },
 {
  "input": "Most immediately, in the next video, I'll be talking about multiplying two matrices together. ",
  "translatedText": "すぐに、次のビデオで 2 つの 行列の乗算について説明します。",
  "model": "google_nmt",
  "from_community_srt": "すぐに、次の動画で 2つの行列を掛け合わせることについて話します。",
  "n_reviews": 0,
  "start": 641.3,
  "end": 645.66
 },
 {
  "input": "See you then! ",
  "translatedText": "それではまた！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.12,
  "end": 645.66
 },
 {
  "input": "Thank you for watching! ",
  "translatedText": "ご清覧ありがとうございました！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.12,
  "end": 646.32
 }
]