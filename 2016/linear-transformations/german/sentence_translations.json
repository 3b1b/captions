[
 {
  "input": "Hey everyone!",
  "translatedText": "Hallo zusammen!",
  "model": "DeepL",
  "from_community_srt": "Dummerweise kann man niemandem erklären, was die Matrix ist. Du musst sie selbst erleben. Morpheus Erstaunlich zutreffende Worte über die Wichtigkeit des visuellen Verständnisses von Matrix–Operationen.",
  "n_reviews": 0,
  "start": 12.04,
  "end": 12.92
 },
 {
  "input": "If I had to choose just one topic that makes all of the others in linear algebra start to click, and which too often goes unlearned the first time a student takes linear algebra, it would be this one.",
  "translatedText": "Wenn ich nur ein Thema auswählen müsste, bei dem alle anderen Themen der linearen Algebra anfangen zu klicken, und das allzu oft vernachlässigt wird, wenn ein Schüler zum ersten Mal lineare Algebra lernt, dann wäre es dieses Thema.",
  "model": "DeepL",
  "from_community_srt": "Hallo zusammen! Wenn ich ein Thema auswählen müsste durch das alle anderen Themen der linearen Algebra auf einmal logisch erscheinen und das viel zu oft in den Einführungskursen nicht gelehrt wird dann wäre es dieses:",
  "n_reviews": 0,
  "start": 13.32,
  "end": 22.28
 },
 {
  "input": "The idea of a linear transformation and its relation to matrices.",
  "translatedText": "Die Idee der linearen Transformation und ihre Beziehung zu Matrizen.",
  "model": "DeepL",
  "from_community_srt": "die Idee einer linearen Transformation und ihre Verbindungen zu Matritzen.",
  "n_reviews": 0,
  "start": 22.7,
  "end": 26.2
 },
 {
  "input": "For this video, I'm just going to focus on what these transformations look like in the case of two dimensions, and how they relate to the idea of matrix vector multiplication.",
  "translatedText": "In diesem Video werde ich mich darauf konzentrieren, wie diese Transformationen im Falle von zwei Dimensionen aussehen und wie sie mit der Idee der Matrix-Vektor-Multiplikation zusammenhängen.",
  "model": "DeepL",
  "from_community_srt": "In diesem Video beschränke ich mich auf das Aussehen dieser Transformationen im zweidimensionalen Raum und wie sie mit Matrix–Vektor–Multiplikation zusammenhängen.",
  "n_reviews": 0,
  "start": 26.95,
  "end": 35.06
 },
 {
  "input": "In particular, I want to show you a way to think about matrix vector multiplication that doesn't rely on memorization.",
  "translatedText": "Ich möchte dir vor allem einen Weg zeigen, wie du über die Matrix-Vektor-Multiplikation nachdenken kannst, ohne sie auswendig lernen zu müssen.",
  "model": "DeepL",
  "from_community_srt": "Vor allem möchte ich euch eine Möglichkeit geben über Matrix–Vektor–Multiplikation zu denken die nicht auf Auswendiglernen basiert.",
  "n_reviews": 0,
  "start": 35.88,
  "end": 42.08
 },
 {
  "input": "To start, let's just parse this term, linear transformation.",
  "translatedText": "Zu Beginn wollen wir den Begriff \"lineare Transformation\" einmal analysieren.",
  "model": "DeepL",
  "from_community_srt": "Lasst uns zunächst erst einmal den Begriff \"Lineare Transformation\" genauer ansehen.",
  "n_reviews": 0,
  "start": 43.16,
  "end": 46.58
 },
 {
  "input": "Transformation is essentially a fancy word for function.",
  "translatedText": "Transformation ist im Grunde ein schickes Wort für Funktion.",
  "model": "DeepL",
  "from_community_srt": "\"Transformation\" ist letztendlich nur ein hochgestochenes Wort für \"Funktion\".",
  "n_reviews": 0,
  "start": 47.42,
  "end": 49.88
 },
 {
  "input": "It's something that takes in inputs and spits out an output for each one.",
  "translatedText": "Es ist etwas, das Eingaben entgegennimmt und für jede eine Ausgabe ausspuckt.",
  "model": "DeepL",
  "from_community_srt": "Es ist etwas, dass eine Eingabe akzeptiert und eine Ausgabe zurückgibt.",
  "n_reviews": 0,
  "start": 50.26,
  "end": 53.98
 },
 {
  "input": "Specifically, in the context of linear algebra, we like to think about transformations that take in some vector and spit out another vector.",
  "translatedText": "Im Zusammenhang mit der linearen Algebra denken wir gerne an Transformationen, die einen Vektor einnehmen und einen anderen Vektor ausspucken.",
  "model": "DeepL",
  "from_community_srt": "Im Kontext der linearen Algebra akzeptieren Transformationen in der Regel einen bestimmen Vektor als Eingabe",
  "n_reviews": 0,
  "start": 53.98,
  "end": 61.08
 },
 {
  "input": "So why use the word transformation instead of function if they mean the same thing?",
  "translatedText": "Warum also das Wort \"Transformation\" statt \"Funktion\" verwenden, wenn beide dasselbe bedeuten?",
  "model": "DeepL",
  "from_community_srt": "und geben einen anderen Vektor als Ausgabe zurück- Warum sagen wir dann überhaupt \"Transformation\" anstelle von \"Funktion\" – wenn doch beide dasselbe sind?",
  "n_reviews": 0,
  "start": 62.5,
  "end": 66.38
 },
 {
  "input": "Well, it's to be suggestive of a certain way to visualize this input-output relation.",
  "translatedText": "Nun, es soll eine bestimmte Art und Weise andeuten, wie man diese Input-Output-Beziehung visualisieren kann.",
  "model": "DeepL",
  "from_community_srt": "Nun ja, weil wir damit eine bestimmte Visualisierung dieses Eingabe–Ausgabe–Verhältnisses andeuten.",
  "n_reviews": 0,
  "start": 67.12,
  "end": 71.34
 },
 {
  "input": "You see, a great way to understand functions of vectors is to use movement.",
  "translatedText": "Du siehst, eine gute Möglichkeit, die Funktionen von Vektoren zu verstehen, ist die Bewegung.",
  "model": "DeepL",
  "from_community_srt": "Weißt du, eine schöne Art und Weise Vektor–Funktionen zu verstehen ist Bewegung.",
  "n_reviews": 0,
  "start": 71.86,
  "end": 75.8
 },
 {
  "input": "If a transformation takes some input vector to some output vector, we imagine that input vector moving over to the output vector.",
  "translatedText": "Wenn eine Transformation von einem Eingangsvektor zu einem Ausgangsvektor führt, stellen wir uns vor, dass der Eingangsvektor auf den Ausgangsvektor übergeht.",
  "model": "DeepL",
  "from_community_srt": "Wenn eine Transformation einen bestimmten Eingabe–Vektor auf einen bestimmten Ausgabe–Vektor abbildet, dann stellen wir uns vor, dass der Eingabe–Vektor sich zum Ausgabe–Vektor bewegt.",
  "n_reviews": 0,
  "start": 76.78,
  "end": 84.86
 },
 {
  "input": "Then to understand the transformation as a whole, we might imagine watching every possible input vector move over to its corresponding output vector.",
  "translatedText": "Um die Transformation als Ganzes zu verstehen, können wir uns vorstellen, wie jeder mögliche Eingangsvektor zu seinem entsprechenden Ausgangsvektor übergeht.",
  "model": "DeepL",
  "from_community_srt": "Um dann die Transformation als Ganzes zu verstehen, könnten wir uns jeden möglichen Eingabe–Vektor vorstellen, wie er sich zum entsprechenden Ausgabe–Vektor bewegt.",
  "n_reviews": 0,
  "start": 85.68,
  "end": 94.08
 },
 {
  "input": "It gets really crowded to think about all of the vectors all at once, each one as an arrow.",
  "translatedText": "Es ist sehr anstrengend, sich alle Vektoren auf einmal vorzustellen, jeden einzelnen als Pfeil.",
  "model": "DeepL",
  "from_community_srt": "Es wird ganz schön voll hier wenn wir uns alle Vektoren gleichzeitig als Pfeile vorstellen.",
  "n_reviews": 0,
  "start": 94.98,
  "end": 99.12
 },
 {
  "input": "So as I mentioned last video, a nice trick is to conceptualize each vector not as an arrow, but as a single point, the point where its tip sits.",
  "translatedText": "Wie ich bereits im letzten Video erwähnt habe, besteht ein guter Trick darin, sich jeden Vektor nicht als Pfeil vorzustellen, sondern als einen einzelnen Punkt, den Punkt, an dem seine Spitze sitzt.",
  "model": "DeepL",
  "from_community_srt": "Wie bereits im letzten Video erwähnt: Ein praktischer Trick um alle Vektoren zu visualisieren ist sie uns als Punkte vorzustellen, die an der Spitze dieser Pfeile liegen.",
  "n_reviews": 0,
  "start": 99.5,
  "end": 107.42
 },
 {
  "input": "That way, to think about a transformation taking every possible input vector to some output vector, we watch every point in space moving to some other point.",
  "translatedText": "Wenn wir uns eine Transformation vorstellen, die jeden möglichen Eingangsvektor in einen Ausgangsvektor umwandelt, sehen wir, wie sich jeder Punkt im Raum zu einem anderen Punkt bewegt.",
  "model": "DeepL",
  "from_community_srt": "Wenn wir und so vorstellen wie eine Transformation alle Eingabe–Vektoren zu ihren Ausgabe–Vektoren bewegt dann sehen wir jeden Punkt im Raum wie er sich zu einem anderen Punkt bewegt.",
  "n_reviews": 0,
  "start": 108.03,
  "end": 116.34
 },
 {
  "input": "In the case of transformations in two dimensions, to get a better feel for the whole shape of the transformation, I like to do this with all of the points on an infinite grid.",
  "translatedText": "Bei zweidimensionalen Transformationen mache ich das gerne mit allen Punkten auf einem unendlichen Gitter, um ein besseres Gefühl für die gesamte Form der Transformation zu bekommen.",
  "model": "DeepL",
  "from_community_srt": "Um bei zweidimensionalen Transformationen ein besseres Gefühl für die \"Form\" der Transformation zu bekommen tue ich genau das mit allen Punkten eines unendlichen Rasters.",
  "n_reviews": 0,
  "start": 117.22,
  "end": 125.78
 },
 {
  "input": "I also sometimes like to keep a copy of the grid in the background, just to help keep track of where everything ends up relative to where it starts.",
  "translatedText": "Manchmal behalte ich auch eine Kopie des Rasters im Hintergrund, um den Überblick zu behalten, wo alles im Verhältnis zum Anfang endet.",
  "model": "DeepL",
  "from_community_srt": "Manchmal behalte ich eine Kopie des Rasters im Hintergrund um im Kopf zu behalten wo alles landet,",
  "n_reviews": 0,
  "start": 126.56,
  "end": 132.84
 },
 {
  "input": "The effect for various transformations moving around all of the points in space is, you've got to admit, beautiful.",
  "translatedText": "Der Effekt für verschiedene Transformationen, die sich um alle Punkte im Raum bewegen, ist, das musst du zugeben, wunderschön.",
  "model": "DeepL",
  "from_community_srt": "relativ zum Anfangspunkt. Der Effekt einiger Transformationen die alle Punkte im Raum bewegen ist, das musst du zugeben,",
  "n_reviews": 0,
  "start": 134.46,
  "end": 141.08
 },
 {
  "input": "It gives the feeling of squishing and morphing space itself.",
  "translatedText": "Es gibt dir das Gefühl, den Raum zu zerquetschen und zu morphen.",
  "model": "DeepL",
  "from_community_srt": "schön. Es erweckt den Eindruck, den Raum selbst zu dehnen und zu verbiegen.",
  "n_reviews": 0,
  "start": 141.88,
  "end": 144.64
 },
 {
  "input": "As you can imagine though, arbitrary transformations can look pretty complicated.",
  "translatedText": "Wie du dir vorstellen kannst, können willkürliche Transformationen aber ziemlich kompliziert aussehen.",
  "model": "DeepL",
  "from_community_srt": "Wie du dir sicher vorstellen kannst, können arbiträre Transformationen ziemlich kompliziert aussehen.",
  "n_reviews": 0,
  "start": 145.6,
  "end": 149.92
 },
 {
  "input": "But luckily, linear algebra limits itself to a special type of transformation, ones that are easier to understand, called linear transformations.",
  "translatedText": "Aber zum Glück beschränkt sich die lineare Algebra auf eine spezielle Art von Transformationen, die einfacher zu verstehen sind, die sogenannten linearen Transformationen.",
  "model": "DeepL",
  "from_community_srt": "Aber glücklicherweise beschränkt sich die lineare Algebra auf eine bestimmte Art Transformationen. Transformationen, die einfacher zu verstehen sind. Gennant \"lineare\" Transformationen.",
  "n_reviews": 0,
  "start": 150.38,
  "end": 158.28
 },
 {
  "input": "Visually speaking, a transformation is linear if it has two properties.",
  "translatedText": "Visuell gesprochen ist eine Transformation linear, wenn sie zwei Eigenschaften hat.",
  "model": "DeepL",
  "from_community_srt": "Aus visueller Perspektive ist eine Transformation linear wenn sie folgende zwei Eigenschaften hat:",
  "n_reviews": 0,
  "start": 159.12,
  "end": 163.06
 },
 {
  "input": "All lines must remain lines without getting curved, and the origin must remain fixed in place.",
  "translatedText": "Alle Linien müssen Linien bleiben, ohne gekrümmt zu werden, und der Ursprung muss an seinem Platz bleiben.",
  "model": "DeepL",
  "from_community_srt": "Alle Linien müssen Linien bleiben ohne sich zu krümmen und der Ursprungspunkt muss fixiert bleiben.",
  "n_reviews": 0,
  "start": 163.7,
  "end": 169.6
 },
 {
  "input": "For example, this right here would not be a linear transformation, since the lines get all curvy.",
  "translatedText": "Das hier wäre zum Beispiel keine lineare Transformation, da die Linien sehr kurvig werden.",
  "model": "DeepL",
  "from_community_srt": "Das hier zum Beispiel ist keine lineare Transformation, weil alle Linien gekrümmt werden.",
  "n_reviews": 0,
  "start": 170.62,
  "end": 175.54
 },
 {
  "input": "And this one right here, although it keeps the lines straight, is not a linear transformation, because it moves the origin.",
  "translatedText": "Und diese hier, obwohl sie die Linien gerade hält, ist keine lineare Transformation, weil sie den Ursprung verschiebt.",
  "model": "DeepL",
  "from_community_srt": "Und diese hier, obwohl die Linien gerade bleiben, ist auch keine lineare Transformation, da sich der Ursprungspunkt bewegt.",
  "n_reviews": 0,
  "start": 176.1,
  "end": 181.86
 },
 {
  "input": "This one here fixes the origin, and it might look like it keeps lines straight, but that's just because I'm only showing the horizontal and vertical grid lines.",
  "translatedText": "Diese hier fixiert den Ursprung und es sieht vielleicht so aus, als ob die Linien gerade bleiben, aber das liegt nur daran, dass ich nur die horizontalen und vertikalen Gitterlinien zeige.",
  "model": "DeepL",
  "from_community_srt": "Diese hier fixiert den Ursprungspunkt und mag so aussehen als krümme sie die Linien nicht. Das liegt aber nur daran, dass ich nur horizontale und vertikale Linien zeige.",
  "n_reviews": 0,
  "start": 182.68,
  "end": 189.24
 },
 {
  "input": "When you see what it does to a diagonal line, it becomes clear that it's not at all linear, since it turns that line all curvy.",
  "translatedText": "Wenn du siehst, was sie mit einer diagonalen Linie macht, wird klar, dass sie überhaupt nicht linear ist, denn sie macht die Linie ganz krumm.",
  "model": "DeepL",
  "from_community_srt": "Wenn du siehst was diese Transformation mit diagonalen Linien macht wird klar, dass sie nicht linear ist, da sie diese Linie komplett verbiegt.",
  "n_reviews": 0,
  "start": 189.54,
  "end": 195.32
 },
 {
  "input": "In general, you should think of linear transformations as keeping grid lines parallel and evenly spaced.",
  "translatedText": "Generell solltest du dir vorstellen, dass lineare Transformationen dafür sorgen, dass die Gitterlinien parallel und in gleichmäßigen Abständen verlaufen.",
  "model": "DeepL",
  "from_community_srt": "Allgemein belassen lineare Transformationen die Linien parallel und gleichmäßig verteilt.",
  "n_reviews": 0,
  "start": 196.76,
  "end": 202.24
 },
 {
  "input": "Some linear transformations are simple to think about, like rotations about the origin.",
  "translatedText": "Einige lineare Transformationen sind einfach zu denken, z.B. Drehungen um den Ursprung.",
  "model": "DeepL",
  "from_community_srt": "Manche lineare Transformationen kann man sich leicht vorstellen, wie Rotationen um den Ursprungspunkt.",
  "n_reviews": 0,
  "start": 203.4,
  "end": 207.54
 },
 {
  "input": "Others are a little trickier to describe with words.",
  "translatedText": "Andere sind ein bisschen schwieriger mit Worten zu beschreiben.",
  "model": "DeepL",
  "from_community_srt": "Andere sind gar nicht so einfach mit Worten zu beschreiben.",
  "n_reviews": 0,
  "start": 208.12,
  "end": 210.6
 },
 {
  "input": "So, how do you think you could describe these transformations numerically?",
  "translatedText": "Wie könntest du diese Umwandlungen numerisch beschreiben?",
  "model": "DeepL",
  "from_community_srt": "Wie, glaubst du,",
  "n_reviews": 0,
  "start": 212.04,
  "end": 215.48
 },
 {
  "input": "If you were, say, programming some animations to make a video teaching the topic, what formula do you give the computer so that if you give it the coordinates of a vector, it can give you the coordinates of where that vector lands?",
  "translatedText": "Wenn du z.B. einige Animationen für ein Lehrvideo programmieren würdest, welche Formel gibst du dem Computer, damit er dir die Koordinaten eines Vektors geben kann, wenn du ihm die Koordinaten gibst, wo dieser Vektor landet?",
  "model": "DeepL",
  "from_community_srt": "könnte man diese Transformationen numerisch beschreiben? Sagen wir zum Beispiel du programmierst ein paar Animationen für ein Lehrvideo über das Thema – welche Formeln gibst du dem Computer damit er, wenn du ihm die Koordinaten eines Vektors gibst, dir sagen kann auf welchen Koordinaten dieser Vektor landet?",
  "n_reviews": 0,
  "start": 215.48,
  "end": 227.24
 },
 {
  "input": "It turns out that you only need to record where the two basis vectors, i-hat and j-hat, each land, and everything else will follow from that.",
  "translatedText": "Es stellt sich heraus, dass du nur aufzeichnen musst, wo die beiden Basisvektoren, i-hat und j-hat, jeweils landen, und alles andere ergibt sich dann von selbst.",
  "model": "DeepL",
  "from_community_srt": "Letztendlich musst du hierfür nur wissen wo die zwei Basisvektoren î und ĵ landen. Alles andere wird sich daraus ergeben.",
  "n_reviews": 0,
  "start": 228.48,
  "end": 236.6
 },
 {
  "input": "For example, consider the vector v with coordinates negative 1, 2, meaning that it equals negative 1 times i-hat plus 2 times j-hat.",
  "translatedText": "Betrachte zum Beispiel den Vektor v mit den Koordinaten negativ 1, 2, was bedeutet, dass er negativ 1 mal i-hat plus 2 mal j-hat ist.",
  "model": "DeepL",
  "from_community_srt": "Nimm als Beispiel den Vektor v mit den Koordinaten (–1, 2). Das bedeutet er ist gleich –1 · î + 2 · ĵ.",
  "n_reviews": 0,
  "start": 237.5,
  "end": 245.7
 },
 {
  "input": "If we play some transformation and follow where all three of these vectors go, the property that grid lines remain parallel and evenly spaced has a really important consequence.",
  "translatedText": "Wenn wir etwas Transformation spielen und verfolgen, wohin alle drei Vektoren gehen, hat die Eigenschaft, dass die Gitterlinien parallel und gleichmäßig verteilt bleiben, eine wirklich wichtige Folge.",
  "model": "DeepL",
  "from_community_srt": "Wenn wir eine Transformation abspielen und allen Vektoren folgen, dann hat die Eigenschaft, dass Rasterlinien parallel und gleichmäßig verteilt bleiben,",
  "n_reviews": 0,
  "start": 248.68,
  "end": 258.3
 },
 {
  "input": "The place where v lands will be negative 1 times the vector where i-hat landed plus 2 times the vector where j-hat landed.",
  "translatedText": "Der Ort, an dem v landet, ist das 1-fache des Vektors, auf dem i-hat gelandet ist, plus das 2-fache des Vektors, auf dem j-hat gelandet ist.",
  "model": "DeepL",
  "from_community_srt": "eine wichtige Konsequenz: Der Ort an dem v landet ist -1 mal der Vektor auf dem î landet plus 2 mal der Vektor auf dem  ĵ landet.",
  "n_reviews": 0,
  "start": 259.1,
  "end": 265.4
 },
 {
  "input": "In other words, it started off as a certain linear combination of i-hat and j-hat, and it ends up as that same linear combination of where those two vectors landed.",
  "translatedText": "Mit anderen Worten: Es begann als eine bestimmte Linearkombination von i-hat und j-hat und endet als dieselbe Linearkombination, in der diese beiden Vektoren gelandet sind.",
  "model": "DeepL",
  "from_community_srt": "Das heißt, die lineare Kombination von v ist gleich der linearen Kombination von î und ĵ.",
  "n_reviews": 0,
  "start": 265.98,
  "end": 274.58
 },
 {
  "input": "This means you can deduce where v must go based only on where i-hat and j-hat each land.",
  "translatedText": "Das bedeutet, dass du nur aus der Position von i-hat und j-hat ableiten kannst, wo v landen muss.",
  "model": "DeepL",
  "from_community_srt": "Das heißt wir können wissen wo v landet, nur durch unser Wissen wo î und ĵ landen.",
  "n_reviews": 0,
  "start": 275.62,
  "end": 280.92
 },
 {
  "input": "This is why I like keeping a copy of the original grid in the background.",
  "translatedText": "Deshalb behalte ich gerne eine Kopie des Originalrasters im Hintergrund.",
  "model": "DeepL",
  "from_community_srt": "Das ist der Grund weswegen ich gerne eine Kopie des ursprünglichen Rasters im Hintergrund behalte.",
  "n_reviews": 0,
  "start": 281.58,
  "end": 284.54
 },
 {
  "input": "For the transformation shown here, we can read off that i-hat lands on the coordinates 1, negative 2, and j-hat lands on the x-axis over at the coordinates 3, 0.",
  "translatedText": "Bei der hier gezeigten Transformation können wir ablesen, dass i-hat auf den Koordinaten 1, negativ 2, und j-hat auf der x-Achse bei den Koordinaten 3, 0 landet.",
  "model": "DeepL",
  "from_community_srt": "Für die hier gezeigte Transformation können wir ablesen, dass î auf den Koordinaten (1, –2) und ĵ auf der X-Achse auf den Koordinaten (3,",
  "n_reviews": 0,
  "start": 285.08,
  "end": 294.94
 },
 {
  "input": "This means that the vector represented by negative 1 i-hat plus 2 times j-hat ends up at negative 1 times the vector 1, negative 2 plus 2 times the vector 3, 0.",
  "translatedText": "Das bedeutet, dass der Vektor, der durch negative 1 i-hat plus 2 mal j-hat dargestellt wird, bei negativ 1 mal dem Vektor 1, negativ 2 plus 2 mal dem Vektor 3, 0 landet.",
  "model": "DeepL",
  "from_community_srt": "0) landet. Das heißt, dass der Vektor repräsentiert durch –1 · î + 2 · ĵ, landet bei –1 mal der Vektor (1, –2) plus 2 mal der Vektor (3,",
  "n_reviews": 0,
  "start": 295.54,
  "end": 306.14
 },
 {
  "input": "Adding that all together, you can deduce that it has to land on the vector 5, 2.",
  "translatedText": "Wenn du das alles zusammenzählst, kannst du ableiten, dass er auf dem Vektor 5, 2 landen muss.",
  "model": "DeepL",
  "from_community_srt": "0). Wenn wir das addieren, wissen wir, dass v auf dem Vektor (5,",
  "n_reviews": 0,
  "start": 307.1,
  "end": 311.68
 },
 {
  "input": "This is a good point to pause and ponder, because it's pretty important.",
  "translatedText": "Das ist ein guter Punkt zum Innehalten und Nachdenken, denn er ist ziemlich wichtig.",
  "model": "DeepL",
  "from_community_srt": "2) landet. Jetzt wäre ein guter Zeitpunkt um Kurz innezuhalten, denn das ist ziemlich wichtig.",
  "n_reviews": 0,
  "start": 314.26,
  "end": 317.24
 },
 {
  "input": "Now, given that I'm actually showing you the full transformation, you could have just looked to see that v has the coordinates 5, 2.",
  "translatedText": "Da ich dir jetzt die vollständige Transformation zeige, hättest du auch einfach nachsehen können, dass v die Koordinaten 5, 2 hat.",
  "model": "DeepL",
  "from_community_srt": "Da ich euch hier tatsächlich die komplette Transformation zeige, hättet ihr einfach schauen und direkt sehen können, dass v auf den Koordinaten (5,",
  "n_reviews": 0,
  "start": 318.52,
  "end": 325.28
 },
 {
  "input": "But the cool part here is that this gives us a technique to deduce where any vectors land so long as we have a record of where i-hat and j-hat each land without needing to watch the transformation itself.",
  "translatedText": "Das Tolle daran ist, dass wir auf diese Weise herausfinden können, wo die Vektoren landen, solange wir wissen, wo i-hat und j-hat jeweils landen, ohne dass wir die Transformation selbst beobachten müssen.",
  "model": "DeepL",
  "from_community_srt": "2) landet. Aber das coole an der Sache ist, dass wir mit dieser Technik wissen können wo irgendein beliebiger Vektor landet, solange wir nur wissen, wo î und ĵ landen, ohne die Transformation selbst sehen zu müssen.",
  "n_reviews": 0,
  "start": 325.76,
  "end": 337.38
 },
 {
  "input": "Write the vector with more general coordinates, x and y, and it will land on x times the vector where i-hat lands, 1, negative 2, plus y times the vector where j-hat lands, 3, 0.",
  "translatedText": "Wenn du den Vektor mit den allgemeineren Koordinaten x und y schreibst, landet er auf x mal dem Vektor, auf dem der i-Hut landet, 1, negativ 2, und y mal dem Vektor, auf dem der j-Hut landet, 3, 0.",
  "model": "DeepL",
  "from_community_srt": "Schreibt diesen Vektor mit den allgemeineren Koordinaten x und y und er landet auf x mal dem Vektor, auf dem î landet, plus y mal dem Vektor,",
  "n_reviews": 0,
  "start": 338.6,
  "end": 350.6
 },
 {
  "input": "Carrying out that sum, you see that it lands at 1x plus 3y, negative 2x plus 0y.",
  "translatedText": "Wenn du diese Summe ausrechnest, siehst du, dass sie bei 1x plus 3y, negativ 2x plus 0y liegt.",
  "model": "DeepL",
  "from_community_srt": "auf dem ĵ landet. Wenn wir diese Summe ausschreiben, sehen wir, dass der Vektor auf (1x + 3y, –2x + 0y) landet.",
  "n_reviews": 0,
  "start": 351.86,
  "end": 358.1
 },
 {
  "input": "I give you any vector, and you can tell me where that vector lands using this formula.",
  "translatedText": "Ich gebe dir einen beliebigen Vektor, und du kannst mir mit dieser Formel sagen, wo dieser Vektor landet.",
  "model": "DeepL",
  "from_community_srt": "Ich gebe euch irgendeinen Vektor und ihr könnt mir sagen wo er landen wird,",
  "n_reviews": 0,
  "start": 358.74,
  "end": 363.58
 },
 {
  "input": "What all of this is saying is that a two-dimensional linear transformation is completely described by just four numbers, the two coordinates for where i-hat lands and the two coordinates for where j-hat lands.",
  "translatedText": "Das bedeutet, dass eine zweidimensionale lineare Transformation vollständig durch nur vier Zahlen beschrieben wird, nämlich durch die beiden Koordinaten für den Ort, an dem i-hat, und die beiden Koordinaten für den Ort, an dem j-hat landet.",
  "model": "DeepL",
  "from_community_srt": "nur indem ihr diese Formel benutzt. Das alles sagt letztendlich aus, dass eine zweidimensionale lineare Transformation mit nur vier Zahlen vollständig beschrieben ist: die zwei Koordinaten für î und die zwei Koordinaten für ĵ.",
  "n_reviews": 0,
  "start": 364.86,
  "end": 376.5
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "Ist das nicht cool?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.08,
  "end": 377.64
 },
 {
  "input": "It's common to package these coordinates into a 2x2 grid of numbers called a 2x2 matrix, where you can interpret the columns as the two special vectors where i-hat and j-hat each land.",
  "translatedText": "Es ist üblich, diese Koordinaten in eine 2x2-Matrix zu packen, in der du die Spalten als die beiden speziellen Vektoren interpretieren kannst, auf denen i-hat und j-hat jeweils landen.",
  "model": "DeepL",
  "from_community_srt": "Ist das nicht cool? Allgemein schreibt man diese beiden Koordinaten in einem 2x2 Netz, genannt eine 2x2 Matrix, deren Spalten ihr interpretieren könnt als die besonderen Vektoren auf denen î und ĵ landen.",
  "n_reviews": 0,
  "start": 378.38,
  "end": 389.64
 },
 {
  "input": "If you're given a 2x2 matrix describing a linear transformation and some specific vector, and you want to know where that linear transformation takes that vector, you can take the coordinates of the vector, multiply them by the corresponding columns of the matrix, then add together what you get.",
  "translatedText": "Wenn du eine 2x2-Matrix hast, die eine lineare Transformation und einen bestimmten Vektor beschreibt, und du wissen willst, wohin die lineare Transformation den Vektor bringt, kannst du die Koordinaten des Vektors nehmen, sie mit den entsprechenden Spalten der Matrix multiplizieren und dann das Ergebnis addieren.",
  "model": "DeepL",
  "from_community_srt": "Wenn dir eine 2x2 Matrix gegeben ist, die eine lineare Transformation beschreibt und irgendeinen bestimmten Vektor und du wissen willst, wo die lineare Transformation diesen Vektor hinschiebt, kannst du die Koordinaten des Vektors nehmen und sie mit den entsprechenden Spalten der Matrix multiplizieren und dann die Ergebnisse addieren.",
  "n_reviews": 0,
  "start": 390.38,
  "end": 407.34
 },
 {
  "input": "This corresponds with the idea of adding the scaled versions of our new basis vectors.",
  "translatedText": "Dies entspricht der Idee, die skalierten Versionen unserer neuen Basisvektoren hinzuzufügen.",
  "model": "DeepL",
  "from_community_srt": "Das entspricht der Idee, die skalierten Basisvektoren zu addieren.",
  "n_reviews": 0,
  "start": 408.18,
  "end": 412.72
 },
 {
  "input": "Let's see what this looks like in the most general case, where your matrix has entries A, B, C, D.",
  "translatedText": "Schauen wir uns an, wie das im allgemeinsten Fall aussieht, wenn deine Matrix die Einträge A, B, C, D hat.",
  "model": "DeepL",
  "from_community_srt": "Lasst uns das mal für den allgemeinsten Fall betrachten, wo die Matrix die Einträge a, b, c und d besitzt.",
  "n_reviews": 0,
  "start": 414.72,
  "end": 420.54
 },
 {
  "input": "And remember, this matrix is just a way of packaging the information needed to describe a linear transformation.",
  "translatedText": "Und vergiss nicht, dass diese Matrix nur eine Möglichkeit ist, die Informationen zu verpacken, die zur Beschreibung einer linearen Transformation benötigt werden.",
  "model": "DeepL",
  "from_community_srt": "Und vergiss nicht, diese Matrix ist nur ein Weg um die notwendigen Information einer linearen Transformation zu verpacken.",
  "n_reviews": 0,
  "start": 421.1,
  "end": 426.24
 },
 {
  "input": "Always remember to interpret that first column, AC, as the place where the first basis vector lands, and that second column, BD, as the place where the second basis vector lands.",
  "translatedText": "Denke immer daran, die erste Spalte AC als den Ort zu interpretieren, an dem der erste Basisvektor landet, und die zweite Spalte BD als den Ort, an dem der zweite Basisvektor landet.",
  "model": "DeepL",
  "from_community_srt": "Erinnere dich immer daran, die erste Spalte der Matrix (a,c) als Landeort für den ersten Basisvektor und die zweite Spalte, (b,d) als Landeort des zweiten Basisvektors,",
  "n_reviews": 0,
  "start": 426.24,
  "end": 436.44
 },
 {
  "input": "When we apply this transformation to some vector xy, what do you get?",
  "translatedText": "Wenn wir diese Transformation auf einen Vektor xy anwenden, was erhältst du dann?",
  "model": "DeepL",
  "from_community_srt": "zu interpretieren. Wenn wir diese Transformation auf eine Vektor (x,y) anwenden, was erhalten wir dann?",
  "n_reviews": 0,
  "start": 437.5,
  "end": 441.0
 },
 {
  "input": "Well, it'll be x times AC plus y times BD.",
  "translatedText": "Nun, es wird x mal AC plus y mal BD sein.",
  "model": "DeepL",
  "from_community_srt": "Es kommt x*(a,c)+y*(b,d) heraus.",
  "n_reviews": 0,
  "start": 442.06,
  "end": 446.98
 },
 {
  "input": "Putting this together, you get a vector Ax plus By, Cx plus Dy.",
  "translatedText": "Setzt man dies zusammen, erhält man einen Vektor Ax plus By, Cx plus Dy.",
  "model": "DeepL",
  "from_community_srt": "Wenn man dies zusammenfasst, erhält man den Vektor (ax+by,",
  "n_reviews": 0,
  "start": 448.06,
  "end": 453.3
 },
 {
  "input": "You could even define this as matrix vector multiplication, when you put the matrix on the left of the vector like it's a function.",
  "translatedText": "Du könntest dies sogar als Matrix-Vektor-Multiplikation definieren, wenn du die Matrix wie eine Funktion links vom Vektor ansetzt.",
  "model": "DeepL",
  "from_community_srt": "cx+dy) Man kann das auch als Matrix-Vektor-Multiplikation definieren, wenn man die Matrix links vom Vektor setzt, als wäre es eine Funktion.",
  "n_reviews": 0,
  "start": 453.98,
  "end": 460.94
 },
 {
  "input": "Then, you could make high schoolers memorize this without showing them the crucial part that makes it feel intuitive.",
  "translatedText": "Dann könntest du Oberstufenschüler dazu bringen, dies auswendig zu lernen, ohne ihnen den entscheidenden Teil zu zeigen, der es intuitiv erscheinen lässt.",
  "model": "DeepL",
  "from_community_srt": "Dann kann man das Abiturienten zum Auswendiglernen geben, ohne ihnen den entscheidenden Teil zu zeigen, der es intuitiv erscheinen lässt.",
  "n_reviews": 0,
  "start": 461.66,
  "end": 466.62
 },
 {
  "input": "But, isn't it more fun to think about these columns as the transformed versions of your basis vectors, and to think about the result as the appropriate linear combination of those vectors?",
  "translatedText": "Aber macht es nicht mehr Spaß, sich diese Spalten als die transformierten Versionen deiner Basisvektoren vorzustellen und das Ergebnis als die entsprechende lineare Kombination dieser Vektoren zu betrachten?",
  "model": "DeepL",
  "from_community_srt": "Ist es aber nicht interessanter, die Spalten als die transformierten Versionen der Basisvektoren anzusehen? Und das Ergebnis als die entsprechende",
  "n_reviews": 0,
  "start": 468.3,
  "end": 477.96
 },
 {
  "input": "Let's practice describing a few linear transformations with matrices.",
  "translatedText": "Lass uns üben, ein paar lineare Transformationen mit Matrizen zu beschreiben.",
  "model": "DeepL",
  "from_community_srt": "lineare Kombination der Vektoren anzusehen? Lasst uns mal üben, wie man lineare Transformationen mit Matrizen beschreibt.",
  "n_reviews": 0,
  "start": 480.72,
  "end": 483.78
 },
 {
  "input": "For example, if we rotate all of space 90 degrees counterclockwise, then i-hat lands on the coordinates 0, 1.",
  "translatedText": "Wenn wir zum Beispiel den gesamten Raum um 90 Grad gegen den Uhrzeigersinn drehen, dann landet i-hat auf den Koordinaten 0, 1.",
  "model": "DeepL",
  "from_community_srt": "Zum Beispiel: Wenn wir den gesamten Raum um 90° gegen den Uhrzeigersinn drehen",
  "n_reviews": 0,
  "start": 484.58,
  "end": 492.24
 },
 {
  "input": "And j-hat lands on the coordinates negative 1, 0.",
  "translatedText": "Und j-hat landet auf den Koordinaten negativ 1, 0.",
  "model": "DeepL",
  "from_community_srt": "dann landet î auf den Koordinaten (0,1) und ĵ landet auf den Koordinaten (-1,0).",
  "n_reviews": 0,
  "start": 493.98,
  "end": 497.18
 },
 {
  "input": "So the matrix we end up with has columns 0, 1, negative 1, 0.",
  "translatedText": "Die Matrix, die wir erhalten, hat also die Spalten 0, 1, negativ 1, 0.",
  "model": "DeepL",
  "from_community_srt": "Also ergibt sich die Matrix mit den Spalten (0,1) und (-1,0) Um herauszufinden,",
  "n_reviews": 0,
  "start": 497.98,
  "end": 501.96
 },
 {
  "input": "To figure out what happens to any vector after a 90-degree rotation, you could just multiply its coordinates by this matrix.",
  "translatedText": "Um herauszufinden, was mit einem beliebigen Vektor nach einer 90-Grad-Drehung passiert, kannst du seine Koordinaten einfach mit dieser Matrix multiplizieren.",
  "model": "DeepL",
  "from_community_srt": "was mit irgendeinem Vektor nach einer 90° Drehung passiert, kann man einfach die Koordinaten des Vektors mit der Matrix multiplizieren.",
  "n_reviews": 0,
  "start": 502.88,
  "end": 509.62
 },
 {
  "input": "Here's a fun transformation with a special name, called a shear.",
  "translatedText": "Hier ist eine lustige Verwandlung mit einem besonderen Namen, der Schere.",
  "model": "DeepL",
  "from_community_srt": "Hier ist mal eine lustige Transformation, die den besonderen Namen \"Schere\" trägt.",
  "n_reviews": 0,
  "start": 511.56,
  "end": 514.3
 },
 {
  "input": "In it, i-hat remains fixed, so the first column of the matrix is 1, 0.",
  "translatedText": "Darin bleibt i-hat fest, also ist die erste Spalte der Matrix 1, 0.",
  "model": "DeepL",
  "from_community_srt": "In dieser, bleibt î fixiert, so dass die erste Spalte der Matrix (1,0) ist.",
  "n_reviews": 0,
  "start": 515.0,
  "end": 519.16
 },
 {
  "input": "But j-hat moves over to the coordinates 1, 1, which become the second column of the matrix.",
  "translatedText": "Aber j-hat geht zu den Koordinaten 1, 1 über, die zur zweiten Spalte der Matrix werden.",
  "model": "DeepL",
  "from_community_srt": "Aber ĵ landet bei den Koordinaten (1,1) was zur zweiten Spalte der Matrix wird.",
  "n_reviews": 0,
  "start": 519.6,
  "end": 525.3
 },
 {
  "input": "And at the risk of being redundant here, figuring out how a shear transforms a given vector comes down to multiplying this matrix by that vector.",
  "translatedText": "Auf die Gefahr hin, dass ich mich hier überflüssig mache: Um herauszufinden, wie eine Scherung einen bestimmten Vektor transformiert, musst du diese Matrix mit dem Vektor multiplizieren.",
  "model": "DeepL",
  "from_community_srt": "Ohne etwas wichtiges weglassen zu wollen, um herauszufinden wie die \"Schere\" einen gegebenen Vektor transformiert muss man die Matrix mit dem gegebenen Vektor multiplizieren.",
  "n_reviews": 0,
  "start": 525.3,
  "end": 534.08
 },
 {
  "input": "Let's say we want to go the other way around, starting with a matrix, say with columns 1, 2 and 3, 1, and we want to deduce what its transformation looks like.",
  "translatedText": "Nehmen wir an, wir wollen den umgekehrten Weg gehen und mit einer Matrix beginnen, z. B. mit den Spalten 1, 2 und 3, 1, und wir wollen daraus ableiten, wie ihre Transformation aussieht.",
  "model": "DeepL",
  "from_community_srt": "Sagen wir, wir wollen den umgekehrten Weg gehen. Sei die Matrix schon gegeben, mit den Spalten (1,2) und (3,1) und wir nun herausfinden wollen, wie dessen Transformation aussieht.",
  "n_reviews": 0,
  "start": 535.76,
  "end": 544.52
 },
 {
  "input": "Pause and take a moment to see if you can imagine it.",
  "translatedText": "Halte inne und nimm dir einen Moment Zeit, um zu sehen, ob du es dir vorstellen kannst.",
  "model": "DeepL",
  "from_community_srt": "Pausiere das Video und versuche dir vorzustellen wie sowas aussehen könnte.",
  "n_reviews": 0,
  "start": 544.96,
  "end": 547.44
 },
 {
  "input": "One way to do this is to first move i-hat to 1, 2, then move j-hat to 3, 1.",
  "translatedText": "Eine Möglichkeit ist, zuerst i-hat auf 1, 2 und dann j-hat auf 3, 1 zu setzen.",
  "model": "DeepL",
  "from_community_srt": "Ein möglicher Weg ist, î nach (1,2) zu verschieben und dann ĵ nach (3,1) zu verschieben.",
  "n_reviews": 0,
  "start": 548.42,
  "end": 555.1
 },
 {
  "input": "Always moving the rest of space in such a way that keeps gridlines parallel and evenly spaced.",
  "translatedText": "Bewege den Rest des Raums immer so, dass die Rasterlinien parallel und in gleichmäßigen Abständen verlaufen.",
  "model": "DeepL",
  "from_community_srt": "Dabei muss der restliche Raum so bewegt werden, so dass die Gitterlinien parallel und im gleichen Abstand bleiben.",
  "n_reviews": 0,
  "start": 555.1,
  "end": 560.22
 },
 {
  "input": "If the vectors that i-hat and j-hat land on are linearly dependent, which, if you recall from last video, means that one is a scaled version of the other, it means that the linear transformation squishes all of 2D space onto the line where those two vectors sit, also known as the one-dimensional span of those two linearly dependent vectors.",
  "translatedText": "Wenn die Vektoren, auf denen i-hat und j-hat landen, linear abhängig sind, was, wenn du dich an das letzte Video erinnerst, bedeutet, dass einer eine skalierte Version des anderen ist, bedeutet das, dass die lineare Transformation den gesamten 2D-Raum auf die Linie quetscht, auf der diese beiden Vektoren sitzen, auch bekannt als die eindimensionale Spannweite dieser beiden linear abhängigen Vektoren.",
  "model": "DeepL",
  "from_community_srt": "Wenn die Vektoren wo î und ĵ landen linear abhängig sind was - wenn man sich an das letzte Video erinnert bedeutet - dass der eine, eine skalierte Version des anderen ist, wird durch die lineare Transformation, der gesamte 2-Dimensionale Raum auf die Gerade zerdrückt auf welcher die Vektoren liegen. Auch bekannt, als der eindimensionale Span der linear abhängigen Vektoren.",
  "n_reviews": 0,
  "start": 561.68,
  "end": 582.42
 },
 {
  "input": "To sum up, linear transformations are a way to move around space such that gridlines remain parallel and evenly spaced, and such that the origin remains fixed.",
  "translatedText": "Zusammenfassend lässt sich sagen, dass lineare Transformationen eine Möglichkeit sind, sich so im Raum zu bewegen, dass die Gitternetzlinien parallel und in gleichmäßigen Abständen bleiben und der Ursprung fest bleibt.",
  "model": "DeepL",
  "from_community_srt": "Um es zusammenzufassen: Lineare Transformationen sind eine Möglichkeit um den Raum zu bewegen, so dass die Gitterlinien parallel und den gleichen Abstand zu einander haben und dass der Ursprung fixiert bleibt.",
  "n_reviews": 0,
  "start": 584.42,
  "end": 593.94
 },
 {
  "input": "Delightfully, these transformations can be described using only a handful of numbers, the coordinates of where each basis vector lands.",
  "translatedText": "Erfreulicherweise können diese Transformationen mit nur einer Handvoll Zahlen beschrieben werden, nämlich den Koordinaten, auf denen jeder Basisvektor landet.",
  "model": "DeepL",
  "from_community_srt": "Wunderbarer Weise, können diese Transformationen mit nur einer Handvoll Zahlen beschrieben werden, und zwar mit den Koordinaten der transformierten Basisvektoren.",
  "n_reviews": 0,
  "start": 594.54,
  "end": 601.53
 },
 {
  "input": "Matrices give us a language to describe these transformations, where the columns represent those coordinates, and matrix-vector multiplication is just a way to compute what that transformation does to a given vector.",
  "translatedText": "Matrizen geben uns eine Sprache, um diese Transformationen zu beschreiben, wobei die Spalten diese Koordinaten darstellen, und die Matrix-Vektor-Multiplikation ist nur ein Weg, um zu berechnen, was diese Transformation mit einem bestimmten Vektor macht.",
  "model": "DeepL",
  "from_community_srt": "Matrizen geben uns eine \"Sprache\" diese Transformationen zu beschrieben, wobei die Spalten die Koordinaten der Basisvektoren darstellen. Und die Matrix-Vektor-Multiplikation ist nur eine Methode, um berechnen zu können, was die Transformation mit einen gegebenen Vektor macht.",
  "n_reviews": 0,
  "start": 602.76,
  "end": 614.66
 },
 {
  "input": "The important takeaway here is that every time you see a matrix, you can interpret it as a certain transformation of space.",
  "translatedText": "Die wichtige Erkenntnis ist, dass du jedes Mal, wenn du eine Matrix siehst, diese als eine bestimmte Transformation des Raums interpretieren kannst.",
  "model": "DeepL",
  "from_community_srt": "Das wichtige zum mitnehmen ist, dass jedesmal wenn du eine Matrix siehst, du diese als eine Transformation des Raumes interpretieren kannst.",
  "n_reviews": 0,
  "start": 615.36,
  "end": 621.88
 },
 {
  "input": "Once you really digest this idea, you're in a great position to understand linear algebra deeply.",
  "translatedText": "Wenn du diese Idee erst einmal verdaut hast, bist du in der Lage, die lineare Algebra richtig zu verstehen.",
  "model": "DeepL",
  "from_community_srt": "Sobald du dir diese Vorstellung verinnerlicht hast, hast du sehr gute Voraussetzungen um lineare Algebra tiefgründig zu verstehen.",
  "n_reviews": 0,
  "start": 622.58,
  "end": 627.32
 },
 {
  "input": "Almost all of the topics coming up, from matrix multiplication to determinants, change of basis, eigenvalues, all of these will become easier to understand once you start thinking about matrices as transformations of space.",
  "translatedText": "Fast alle kommenden Themen, von der Matrizenmultiplikation bis hin zu Determinanten, Basiswechsel und Eigenwerten, werden leichter zu verstehen sein, wenn du anfängst, Matrizen als Transformationen des Raums zu betrachten.",
  "model": "DeepL",
  "from_community_srt": "Fast alle der kommenden Themen von Matrixmultiplikation über Determinanten, Basiswechsel, bis hin zu Eigenwerten. All diese Themen werden einfacher zu verstehen sein, sobald du anfängst, Matrizen als Transformationen im Raum zu verstehen.",
  "n_reviews": 0,
  "start": 627.66,
  "end": 640.56
 },
 {
  "input": "Most immediately, in the next video, I'll be talking about multiplying two matrices together.",
  "translatedText": "Im nächsten Video werde ich über die Multiplikation zweier Matrizen sprechen.",
  "model": "DeepL",
  "from_community_srt": "Im nächsten Video, bespreche ich wie man zwei Matrizen miteinander multipliziert.",
  "n_reviews": 0,
  "start": 641.3,
  "end": 646.32
 }
]