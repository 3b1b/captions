[
 {
  "input": "Hey everyone! ",
  "translatedText": "嘿大家！",
  "model": "google_nmt",
  "from_community_srt": "嘿 大家好！",
  "n_reviews": 0,
  "start": 12.04,
  "end": 12.92
 },
 {
  "input": "If I had to choose just one topic that makes all of the others in linear algebra start to click, and which too often goes unlearned the first time a student takes linear algebra, it would be this one. ",
  "translatedText": "如果我必须选择一个主题，让线性代数中的所有其 他主题都开始吸引人，并且学生第一次学习线性 代数时往往会忘记这个主题，那么它就是这个。",
  "model": "google_nmt",
  "from_community_srt": "如果要我选出一个主题， 它不仅让线性代数的其他内容一目了然 又经常被初次学习线性代数的人忽视 我会选择这个——线性变换的概念以及它和矩阵的关系 在这期视频中，",
  "n_reviews": 0,
  "start": 13.32,
  "end": 22.28
 },
 {
  "input": "The idea of a linear transformation and its relation to matrices. ",
  "translatedText": "线性变换的思想及其与矩阵的关系。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 22.7,
  "end": 26.2
 },
 {
  "input": "For this video, I'm just going to focus on what these transformations look like in the case of two dimensions, and how they relate to the idea of matrix vector multiplication. ",
  "translatedText": "在本视频中，我将重点介绍这些变换在二维情况下的 样子，以及它们与矩阵向量乘法的概念有何关系。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.95,
  "end": 35.06
 },
 {
  "input": "In particular, I want to show you a way to think about matrix vector multiplication that doesn't rely on memorization. ",
  "translatedText": "特别是，我想向您展示一种不依赖 于记忆的矩阵向量乘法思考方法。",
  "model": "google_nmt",
  "from_community_srt": "我只会集中讨论 这些变换在二维空间中长什么样 以及它们如何与矩阵向量乘法关联 尤其是展示一种不用死记硬背的考虑矩阵向量乘法的方法 首先，",
  "n_reviews": 0,
  "start": 35.88,
  "end": 42.08
 },
 {
  "input": "To start, let's just parse this term, linear transformation. ",
  "translatedText": "首先，我们来解析一下线性变换这个术语。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.16,
  "end": 46.58
 },
 {
  "input": "Transformation is essentially a fancy word for function. ",
  "translatedText": "转变本质上是功能的一个花哨的词。",
  "model": "google_nmt",
  "from_community_srt": "我们先来解析“线性变换”这个术语 “变换”本质上是“函数”的一种花哨的说法 它接收输入内容，",
  "n_reviews": 0,
  "start": 47.42,
  "end": 49.88
 },
 {
  "input": "It's something that takes in inputs and spits out an output for each one. ",
  "translatedText": "它接收输入并为每个输入输出一个输出。",
  "model": "google_nmt",
  "from_community_srt": "并输出对应结果 特别地，",
  "n_reviews": 0,
  "start": 50.26,
  "end": 53.98
 },
 {
  "input": "Specifically, in the context of linear algebra, we like to think about transformations that take in some vector and spit out another vector. ",
  "translatedText": "具体来说，在线性代数的背景下，我们喜欢考 虑接受某个向量并输出另一个向量的变换。",
  "model": "google_nmt",
  "from_community_srt": "在线性代数的情况下 我们考虑的是接收一个向量并且输出一个向量的变换 既然“变换”和“函数”意义相同，",
  "n_reviews": 0,
  "start": 53.98,
  "end": 61.08
 },
 {
  "input": "So why use the word transformation instead of function if they mean the same thing? ",
  "translatedText": "那么，如果它们的含义相同，为什么要使用“转换”一词而不是“函数”呢？",
  "model": "google_nmt",
  "from_community_srt": "为什么还要使用前者而不是后者？",
  "n_reviews": 0,
  "start": 62.5,
  "end": 66.38
 },
 {
  "input": "Well, it's to be suggestive of a certain way to visualize this input-output relation. ",
  "translatedText": "嗯，它暗示了一种可视化这种输入-输出关系的某种方式。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.12,
  "end": 71.34
 },
 {
  "input": "You see, a great way to understand functions of vectors is to use movement. ",
  "translatedText": "你看，理解向量函数的一个好方法是使用运动。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.86,
  "end": 75.8
 },
 {
  "input": "If a transformation takes some input vector to some output vector, we imagine that input vector moving over to the output vector. ",
  "translatedText": "如果转换将某个输入向量转换为某个输出向量 ，我们可以想象该输入向量移动到输出向量。",
  "model": "google_nmt",
  "from_community_srt": "因为使用“变换”是在暗示以特定方式来可视化这一输入-输出关系 一种理解“向量的函数”的方法是使用运动 如果一个变换接收一个向量并输出一个向量 我们想象这个输入向量移动到输出向量的位置",
  "n_reviews": 0,
  "start": 76.78,
  "end": 84.86
 },
 {
  "input": "Then to understand the transformation as a whole, we might imagine watching every possible input vector move over to its corresponding output vector. ",
  "translatedText": "然后，为了理解整个变换，我们可以想象观察每 个可能的输入向量移动到其相应的输出向量。",
  "model": "google_nmt",
  "from_community_srt": "接下来， 要理解整个变换 我们可以想象每一个输入向量都移动到对应输出向量的位置 因为将向量看作箭头时，",
  "n_reviews": 0,
  "start": 85.68,
  "end": 94.08
 },
 {
  "input": "It gets really crowded to think about all of the vectors all at once, each one as an arrow. ",
  "translatedText": "一次性考虑所有向量（每个向量 都是一支箭头）真的很拥挤。",
  "model": "google_nmt",
  "from_community_srt": "同时考虑所有二维向量会变得非常拥挤 所以按照我上期视频所说的，",
  "n_reviews": 0,
  "start": 94.98,
  "end": 99.12
 },
 {
  "input": "So, as I mentioned last video, a nice trick is to conceptualize each vector not as an arrow but as a single point, the point where its tip sits. ",
  "translatedText": "因此，正如我在上一个视频中提到的，一个很好的技巧是将每 个向量概念化为一个点，而不是箭头，即其尖端所在的点。",
  "model": "google_nmt",
  "from_community_srt": "一个好技巧是 将每一个向量看作它的终点，",
  "n_reviews": 0,
  "start": 99.5,
  "end": 107.42
 },
 {
  "input": "That way, to think about a transformation taking every possible input vector to some output vector, we watch every point in space moving to some other point. ",
  "translatedText": "这样，为了考虑将每个可能的输入向量转换为某个输出 向量的变换，我们观察空间中的每个点移动到其他点。",
  "model": "google_nmt",
  "from_community_srt": "而不是一个箭头 用这种方法考虑所有输入向量都移动到对应输出向量的位置时 我们只用看空间中的所有点移动到其他点的位置 二维空间变换这种情况下 为了更好地体会整个空间形状上的改变 我喜欢对无限网格上的所有点同时做变换",
  "n_reviews": 0,
  "start": 108.03,
  "end": 116.34
 },
 {
  "input": "In the case of transformations in two dimensions, to get a better feel for the whole shape of the transformation, I like to do this with all of the points on an infinite grid. ",
  "translatedText": "在二维变换的情况下，为了更好地了解变换的整体 形状，我喜欢对无限网格上的所有点执行此操作。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.22,
  "end": 125.78
 },
 {
  "input": "I also sometimes like to keep a copy of the grid in the background just to help keep track of where everything ends up relative to where it starts. ",
  "translatedText": "有时我还喜欢在后台保留网格的副本，以帮助 跟踪所有内容相对于其开始位置的结束位置。",
  "model": "google_nmt",
  "from_community_srt": "我有时也喜欢在背景中保留原始网格的副本 以便追踪终点与起点的相对关系 你得承认，",
  "n_reviews": 0,
  "start": 126.56,
  "end": 132.84
 },
 {
  "input": "The effect for various transformations moving around all of the points in space is, you've got to admit, beautiful. ",
  "translatedText": "你必须承认，围绕空间中所有点移 动的各种变换的效果是美丽的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.46,
  "end": 141.08
 },
 {
  "input": "It gives the feeling of squishing and morphing space itself. ",
  "translatedText": "它给人一种空间本身被挤压和变形的感觉。",
  "model": "google_nmt",
  "from_community_srt": "各种各样对空间的变换所产生的效果是很美妙的 它们能给你一种挤压和变形空间的感觉 你也能想象到，",
  "n_reviews": 0,
  "start": 141.88,
  "end": 144.64
 },
 {
  "input": "As you can imagine though, arbitrary transformations can look pretty complicated. ",
  "translatedText": "正如您可以想象的那样，任意转换可能看起来非常复杂。",
  "model": "google_nmt",
  "from_community_srt": "任意一个变换可以非常复杂 但幸运的是，",
  "n_reviews": 0,
  "start": 145.6,
  "end": 149.92
 },
 {
  "input": "But luckily, linear algebra limits itself to a special type of transformation, ones that are easier to understand, called linear transformations. ",
  "translatedText": "但幸运的是，线性代数将自己限制为一种特殊类型 的变换，这种变换更容易理解，称为线性变换。",
  "model": "google_nmt",
  "from_community_srt": "线性代数限制在一种特殊类型的变换上 这种变换更容易理解，",
  "n_reviews": 0,
  "start": 150.38,
  "end": 158.28
 },
 {
  "input": "Visually speaking, a transformation is linear if it has two properties. ",
  "translatedText": "从视觉上来说，如果一个变换具有两个属性，那么它就是线性的。",
  "model": "google_nmt",
  "from_community_srt": "称为“线性变换” 直观地说， 如果一个变换具有以下两条性质，",
  "n_reviews": 0,
  "start": 159.12,
  "end": 163.06
 },
 {
  "input": "All lines must remain lines without getting curved, and the origin must remain fixed in place. ",
  "translatedText": "所有线条都必须保持直线而不会弯曲，并且原点必须保持固定。",
  "model": "google_nmt",
  "from_community_srt": "我们就称它是线性的 一是直线在变换后仍然保持为直线， 不能有所弯曲 二是原点必须保持固定 举几个例子，",
  "n_reviews": 0,
  "start": 163.7,
  "end": 169.6
 },
 {
  "input": "For example, this right here would not be a linear transformation, since the lines get all curvy. ",
  "translatedText": "例如，这里不会是线性变换 ，因为线条都是弯曲的。",
  "model": "google_nmt",
  "from_community_srt": "现在所示的这个变换不是线性变换 因为直线变得弯曲了 而对于这一个变换，",
  "n_reviews": 0,
  "start": 170.62,
  "end": 175.54
 },
 {
  "input": "And this one right here, although it keeps the lines straight, is not a linear transformation, because it moves the origin. ",
  "translatedText": "这里的这个虽然保持了直线，但不 是线性变换，因为它移动了原点。",
  "model": "google_nmt",
  "from_community_srt": "即便保持直线平直， 它也不是一个线性变换 因为它移动了原点的位置 这一个变换保持原点不动，",
  "n_reviews": 0,
  "start": 176.1,
  "end": 181.86
 },
 {
  "input": "This one here fixes the origin, and it might look like it keeps lines straight, but that's just because I'm only showing the horizontal and vertical grid lines. ",
  "translatedText": "这里修复了原点，看起来可能使线条保持直线 ，但这只是因为我只显示水平和垂直网格线。",
  "model": "google_nmt",
  "from_community_srt": "乍一看它好像保持直线平直 但实际并非如此，",
  "n_reviews": 0,
  "start": 182.68,
  "end": 189.24
 },
 {
  "input": "When you see what it does to a diagonal line, it becomes clear that it's not at all linear, since it turns that line all curvy. ",
  "translatedText": "当你看到它对对角线的作用时，你会发现它根 本不是线性的，因为它使那条线变成了曲线。",
  "model": "google_nmt",
  "from_community_srt": "因为我只给你展示了水平和竖直的网格线 当你看看它对一条对角线作用时， 很明显它不是一个线性变换 因为这条线变弯曲了 总的来说，",
  "n_reviews": 0,
  "start": 189.54,
  "end": 195.32
 },
 {
  "input": "In general, you should think of linear transformations as keeping grid lines parallel and evenly spaced. ",
  "translatedText": "一般来说，您应该将线性变换视 为保持网格线平行且均匀分布。",
  "model": "google_nmt",
  "from_community_srt": "你应该把线性变换看作是“保持网格线平行且等距分布”的变换 部分线性变换很容易思考，",
  "n_reviews": 0,
  "start": 196.76,
  "end": 202.24
 },
 {
  "input": "Some linear transformations are simple to think about, like rotations about the origin. ",
  "translatedText": "一些线性变换很容易理解，例如绕原点的旋转。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.4,
  "end": 207.54
 },
 {
  "input": "Others are a little trickier to describe with words. ",
  "translatedText": "其他的则难以用语言来描述。",
  "model": "google_nmt",
  "from_community_srt": "比如关于原点的旋转 其他的稍显复杂，",
  "n_reviews": 0,
  "start": 208.12,
  "end": 210.6
 },
 {
  "input": "So how do you think you could describe these transformations numerically? ",
  "translatedText": "那么您认为如何用数字来描述这些转变呢？",
  "model": "google_nmt",
  "from_community_srt": "难以言表 你觉得应该如何用数值去描述这些线性变换呢？",
  "n_reviews": 0,
  "start": 212.04,
  "end": 215.48
 },
 {
  "input": "If you were, say, programming some animations to make a video teaching the topic, what formula do you give the computer so that if you give it the coordinates of a vector, it can give you the coordinates of where that vector lands? ",
  "translatedText": "比如说，如果你正在编写一些动画来制作一个教授该 主题的视频，你会给计算机什么公式，以便如果你给 它一个向量的坐标，它可以给你该向量落地的坐标？",
  "model": "google_nmt",
  "from_community_srt": "比如说， 你在通过编程制作动画和视频来教授这一主题 你应该给计算机什么样的计算公式 使得你给它一个向量的坐标， 它能给你变换后向量的坐标呢？",
  "n_reviews": 0,
  "start": 215.48,
  "end": 227.24
 },
 {
  "input": "It turns out that you only need to record where the two basis vectors, i-hat and j-hat, each land, and everything else will follow from that. ",
  "translatedText": "事实证明，您只需要记录两个基向量（i-hat 和 j -hat）各自的着陆点，其他所有内容都将随之而来。",
  "model": "google_nmt",
  "from_community_srt": "实际结果是， 你只需要记录两个基向量i帽和j帽变换后的位置 其他向量都会随之而动 比如说，",
  "n_reviews": 0,
  "start": 228.48,
  "end": 236.6
 },
 {
  "input": "For example, consider the vector v with coordinates negative 1, 2, meaning that it equals negative 1 times i-hat plus 2 times j-hat. ",
  "translatedText": "例如，考虑坐标为负 1, 2 的向量 v，这意味着它等 于负 1 乘以 i-hat 加 2 乘以 j-hat。",
  "model": "google_nmt",
  "from_community_srt": "考虑坐标为(-1, 2)的向量v 这个向量就是-1与i帽之积和2与j帽之积的和 如果我们运用一些变换，",
  "n_reviews": 0,
  "start": 237.5,
  "end": 245.7
 },
 {
  "input": "If we play some transformation and follow where all three of these vectors go, the property that grid lines remain parallel and evenly spaced has a really important consequence. ",
  "translatedText": "如果我们进行一些变换并跟踪所有这三个向量的走向，则网 格线保持平行且均匀分布的属性会产生非常重要的结果。",
  "model": "google_nmt",
  "from_community_srt": "并且跟随这三个向量的运动 网格线保持平行且等距分布的性质有一个重要的推论 变换后的向量v的位置，",
  "n_reviews": 0,
  "start": 248.68,
  "end": 258.3
 },
 {
  "input": "The place where v lands will be negative 1 times the vector where i-hat landed plus 2 times the vector where j-hat landed. ",
  "translatedText": "v 落地的位置将为负 1 乘以 i-hat 落 地向量加上 2 乘以 j-hat 落地向量。",
  "model": "google_nmt",
  "from_community_srt": "是-1与变换后的i帽之积， 加上2与变换后的j帽之积 换句话说，",
  "n_reviews": 0,
  "start": 259.1,
  "end": 265.4
 },
 {
  "input": "In other words, it started off as a certain linear combination of i-hat and j-hat, and it ends up as that same linear combination of where those two vectors landed. ",
  "translatedText": "换句话说，它开始时是 i-hat 和 j-hat 的 某种线性组合，最终是这两个向量落地的相同线性组合。",
  "model": "google_nmt",
  "from_community_srt": "向量v是i帽和j帽的一个特定线性组合 那么变换后的向量v也是变换后i帽和j帽的同样的线性组合 这意味着，",
  "n_reviews": 0,
  "start": 265.98,
  "end": 274.58
 },
 {
  "input": "This means you can deduce where v must go based only on where i-hat and j-hat each land. ",
  "translatedText": "这意味着您可以仅根据 i-hat 和 j-hat 各自着陆的位置来推断 v 必须去的位置。",
  "model": "google_nmt",
  "from_community_srt": "你可以只根据变换后的i帽和j帽，",
  "n_reviews": 0,
  "start": 275.62,
  "end": 280.92
 },
 {
  "input": "This is why I like keeping a copy of the original grid in the background. ",
  "translatedText": "这就是为什么我喜欢在后台保留原始网格的副本。",
  "model": "google_nmt",
  "from_community_srt": "就推断出变换后的v 这也是为什么我喜欢在背景中保留原始网格的副本 对于现在所示的变换，",
  "n_reviews": 0,
  "start": 281.58,
  "end": 284.54
 },
 {
  "input": "For the transformation shown here, we can read off that i-hat lands on the coordinates 1, negative 2, and j-hat lands on the x-axis over at the coordinates 3, 0. ",
  "translatedText": "对于此处显示的变换，我们可以读出 i-hat 落在坐标 1、负 2 上，而 j-hat 落在 x 轴上的坐标 3、0 处。",
  "model": "google_nmt",
  "from_community_srt": "我们可以看出i帽落在坐标(1, -2)上 j帽落在x轴上， 坐标为(3,",
  "n_reviews": 0,
  "start": 285.08,
  "end": 294.94
 },
 {
  "input": "This means that the vector represented by negative 1 i-hat plus 2 times j-hat ends up at negative 1 times the vector 1, negative 2 plus 2 times the vector 3, 0. ",
  "translatedText": "这意味着由负 1 i-hat 加上 2 乘以 j-hat 表示的向 量最终为负 1 乘以向量 1、负 2 加上 2 乘以向量 3、0。",
  "model": "google_nmt",
  "from_community_srt": "0) 也就是说， -1乘以i帽加上2乘以j帽所代表的向量 会落在-1乘以向量(1, -2)加上2乘以向量(3,",
  "n_reviews": 0,
  "start": 295.54,
  "end": 306.14
 },
 {
  "input": "Adding that all together, you can deduce that it has to land on the vector 5, 2. ",
  "translatedText": "将所有这些加在一起，您可以推断出它必须落在向量 5, 2 上。",
  "model": "google_nmt",
  "from_community_srt": "0)的位置上 简单运算之后， 你就能推断出向量v一定落在向量(5,",
  "n_reviews": 0,
  "start": 307.1,
  "end": 311.68
 },
 {
  "input": "This is a good point to pause and ponder, because it's pretty important. ",
  "translatedText": "这是一个值得停下来思考的好点，因为它非常重要。",
  "model": "google_nmt",
  "from_community_srt": "2)上 因为这个过程非常重要，",
  "n_reviews": 0,
  "start": 314.26,
  "end": 317.24
 },
 {
  "input": "Now, given that I'm actually showing you the full transformation, you could have just looked to see that v has the coordinates 5, 2. ",
  "translatedText": "现在，鉴于我实际上向您展示了完整的变换， 您可以只查看 v 的坐标为 5, 2。",
  "model": "google_nmt",
  "from_community_srt": "所以值得你停下来体会一番 实际上， 因为我给你展示了整个变换的样子 你完全可以直接读出向量v在变换后落在坐标(5,",
  "n_reviews": 0,
  "start": 318.52,
  "end": 325.28
 },
 {
  "input": "But the cool part here is that this gives us a technique to deduce where any vectors land so long as we have a record of where i-hat and j-hat each land, without needing to watch the transformation itself. ",
  "translatedText": "但这里最酷的部分是，这为我们提供了一种技术，只要我们 有 i-hat 和 j-hat 每个落地位置的记录， 就可以推断出任何向量落地的位置，而无需观察转换本身。",
  "model": "google_nmt",
  "from_community_srt": "2)上 但是更炫酷的是， 只要记录了变换后的i帽和j帽 我们就可以推断出任意向量在变换之后的位置 完全不必观察变换本身是什么样 一般的情况下，",
  "n_reviews": 0,
  "start": 325.76,
  "end": 337.38
 },
 {
  "input": "Write the vector with more general coordinates, x and y, and it will land on x times the vector where i-hat lands, 1, negative 2, plus y times the vector where j-hat lands, 3, 0. ",
  "translatedText": "用更通用的坐标 x 和 y 写入向量，它将落在 x 乘以 i-hat 落在 的向量上，1，负 2，加上 y 乘以 j-hat 落在的向量，3, 0。",
  "model": "google_nmt",
  "from_community_srt": "一个向量的坐标是(x, y) n,,0,0,0,,变换后的这个向量就是x乘以变换后的i帽(1, -2) n,,0,0,0,,加上y乘以变换后的j帽(3,",
  "n_reviews": 0,
  "start": 338.6,
  "end": 350.6
 },
 {
  "input": "Carrying out that sum, you see that it lands at 1x plus 3y, negative 2x plus 0y. ",
  "translatedText": "执行该求和，您会发现结果为 1x 加 3y，负 2x 加 0y。",
  "model": "google_nmt",
  "from_community_srt": "0) n,,0,0,0,,简单运算之后你就知道它落在坐标(1x+3y,",
  "n_reviews": 0,
  "start": 351.86,
  "end": 358.1
 },
 {
  "input": "I give you any vector, and you can tell me where that vector lands using this formula. ",
  "translatedText": "我给你任何向量，你可以使用这个公式告诉我该向量落在哪里。",
  "model": "google_nmt",
  "from_community_srt": "-2x+0y)上 运用这个公式， 我给你任意一个向量，",
  "n_reviews": 0,
  "start": 358.74,
  "end": 363.58
 },
 {
  "input": "What all of this is saying is that a two-dimensional linear transformation is completely described by just four numbers, the two coordinates for where i-hat lands, and the two coordinates for where j-hat lands. ",
  "translatedText": "所有这些的意思是，二维线性变换完全由四个数 字描述，即 i-hat 所在位置的两个坐标 ，以及 j-hat 所在位置的两个坐标。",
  "model": "google_nmt",
  "from_community_srt": "你都能告诉我它在变换后的位置 以上这些内容是在说 一个二维线性变换仅由四个数字完全确定 变换后i帽的两个坐标与变换后j帽的两个坐标 是不是很酷？",
  "n_reviews": 0,
  "start": 364.86,
  "end": 376.5
 },
 {
  "input": "Isn't that cool? ",
  "translatedText": "这不是很酷吗？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.08,
  "end": 377.64
 },
 {
  "input": "It's common to package these coordinates into a 2x2 grid of numbers called a 2x2 matrix, where you can interpret the columns as the two special vectors where i-hat and j-hat each land. ",
  "translatedText": "通常将这些坐标打包到称为 2x2 矩阵的 2x 2 数字网格中，您可以将其中的列解释为 i-h at 和 j-hat 各自所在的两个特殊向量。",
  "model": "google_nmt",
  "from_community_srt": "通常我们将这些坐标包装在一个2×2的格子中， 称它为2×2矩阵 你可以把它的列理解为两个特殊的向量，",
  "n_reviews": 0,
  "start": 378.38,
  "end": 389.64
 },
 {
  "input": "If you're given a 2x2 matrix describing a linear transformation and some specific vector, and you want to know where that linear transformation takes that vector, you can take the coordinates of the vector, multiply them by the corresponding columns of the matrix, then add together what you get. ",
  "translatedText": "如果给定一个描述线性变换和某个特定向量的 2x2 矩阵，并且您想知道该线性变换将该 向量带到哪里，则可以获取向量的坐标，将它们 乘以矩阵的相应列，然后把你得到的加起来。",
  "model": "google_nmt",
  "from_community_srt": "即变换后的i帽和j帽 如果你有一个描述线性变换的2×2矩阵， 以及一个给定向量 你想了解线性变换对这个向量的作用 你只需要取出向量的坐标 将它们分别与矩阵的特定列相乘，",
  "n_reviews": 0,
  "start": 390.38,
  "end": 407.34
 },
 {
  "input": "This corresponds with the idea of adding the scaled versions of our new basis vectors. ",
  "translatedText": "这与添加新基向量的缩放版本的想法相对应。",
  "model": "google_nmt",
  "from_community_srt": "然后将结果相加即可 这与“缩放基向量再相加”的思想一致 更一般的情况下，",
  "n_reviews": 0,
  "start": 408.18,
  "end": 412.72
 },
 {
  "input": "Let's see what this looks like in the most general case, where your matrix has entries A, B, C, D. ",
  "translatedText": "让我们看看在最一般的情况下， 矩阵有条目 A、B、C、D。",
  "model": "google_nmt",
  "from_community_srt": "我们来看看矩阵是[[a, b], [c, d]]时会发生什么 记住，",
  "n_reviews": 0,
  "start": 414.72,
  "end": 420.54
 },
 {
  "input": "And remember, this matrix is just a way of packaging the information needed to describe a linear transformation. ",
  "translatedText": "请记住，该矩阵只是打包描述线 性变换所需信息的一种方式。",
  "model": "google_nmt",
  "from_community_srt": "矩阵在这里只是一个记号，",
  "n_reviews": 0,
  "start": 421.1,
  "end": 426.24
 },
 {
  "input": "Always remember to interpret that first column, AC, as the place where the first basis vector lands, and that second column, BD, as the place where the second basis vector lands. ",
  "translatedText": "始终记住将第一列 AC 解释为第一个基向量落地的位 置，将第二列 BD 解释为第二个基向量落地的位置。",
  "model": "google_nmt",
  "from_community_srt": "它含有描述一个线性变换的信息 把第一列(a, c)看作是变换后的第一个基向量 把第二列(b,",
  "n_reviews": 0,
  "start": 426.24,
  "end": 436.44
 },
 {
  "input": "When we apply this transformation to some vector x, y, what do you get? ",
  "translatedText": "当我们将此变换应用于某个向量 x、y 时，您会得到什么？",
  "model": "google_nmt",
  "from_community_srt": "d)看作是变换后的第二个基向量 我们让这个变换作用于向量(x, y)， 结果是什么？",
  "n_reviews": 0,
  "start": 437.5,
  "end": 441.0
 },
 {
  "input": "Well, it'll be x times AC plus y times BD. ",
  "translatedText": "嗯，就是 x 乘以 AC 加 y 乘以 BD。",
  "model": "google_nmt",
  "from_community_srt": "那就应该是x乘以(a, c)加上y乘以(b,",
  "n_reviews": 0,
  "start": 442.06,
  "end": 446.98
 },
 {
  "input": "Putting this together, you get a vector Ax plus By, Cx plus Dy. ",
  "translatedText": "把它们放在一起，你得到一个向量 Ax 加 By，Cx 加 Dy。",
  "model": "google_nmt",
  "from_community_srt": "d) 合并之后， 得到向量(ax+by,",
  "n_reviews": 0,
  "start": 448.06,
  "end": 453.3
 },
 {
  "input": "You could even define this as matrix-vector multiplication when you put the matrix on the left of the vector like it's a function. ",
  "translatedText": "当您将矩阵像函数一样放在向量的左侧时 ，您甚至可以将其定义为矩阵向量乘法。",
  "model": "google_nmt",
  "from_community_srt": "cx+dy) 你甚至可以把它定义为矩阵向量乘法 这里矩阵放在向量左边，",
  "n_reviews": 0,
  "start": 453.98,
  "end": 460.94
 },
 {
  "input": "Then you could make high schoolers memorize this without showing them the crucial part that makes it feel intuitive. ",
  "translatedText": "然后你可以让高中生记住这一点，而不 向他们展示使其感觉直观的关键部分。",
  "model": "google_nmt",
  "from_community_srt": "类似一个函数 即便你不展示其中直观的关键部分，",
  "n_reviews": 0,
  "start": 461.66,
  "end": 466.62
 },
 {
  "input": "But isn't it more fun to think about these columns as the transformed versions of your basis vectors, and to think about the result as the appropriate linear combination of those vectors? ",
  "translatedText": "但是，将这些列视为基向量的转换 版本，并将结果视为这些向量的 适当线性组合，不是更有趣吗？",
  "model": "google_nmt",
  "from_community_srt": "高中生也能记住它 但是我们完全可以把矩阵的列看作变换后的基向量 把矩阵向量乘法看作它们的线性组合， 这样想不是更有意思吗？",
  "n_reviews": 0,
  "start": 468.3,
  "end": 477.96
 },
 {
  "input": "Let's practice describing a few linear transformations with matrices. ",
  "translatedText": "让我们练习描述一些矩阵线性变换。",
  "model": "google_nmt",
  "from_community_srt": "接下来我们练习用矩阵描述一些线性变换 比如说，",
  "n_reviews": 0,
  "start": 480.72,
  "end": 483.78
 },
 {
  "input": "For example, if we rotate all of space 90 degrees counterclockwise, then I-hat lands on the coordinates 0, 1, and J-hat lands on the coordinates negative 1, 0. ",
  "translatedText": "例如，如果我们将整个空间逆时针旋转 90 度，则 I-hat 落在坐标 0, 1 上，J-hat 落在坐标负 1, 0 上。",
  "model": "google_nmt",
  "from_community_srt": "我们将整个空间逆时针旋转90度 那么i帽落在坐标(0, 1)上 j帽落在坐标(-1,",
  "n_reviews": 0,
  "start": 484.58,
  "end": 497.18
 },
 {
  "input": "So the matrix we end up with has columns 0, 1, negative 1, 0. ",
  "translatedText": "所以我们最终得到的矩阵有列 0、1，负列 1、0。",
  "model": "google_nmt",
  "from_community_srt": "0)上 那么这个矩阵的列就分别是(0, 1)和(-1,",
  "n_reviews": 0,
  "start": 497.98,
  "end": 501.96
 },
 {
  "input": "To figure out what happens to any vector after a 90-degree rotation, you could just multiply its coordinates by this matrix. ",
  "translatedText": "要弄清楚任何向量旋转 90 度后会发 生什么，只需将其坐标乘以该矩阵即可。",
  "model": "google_nmt",
  "from_community_srt": "0) 如果想算出任意向量在逆时针旋转90度后的位置 你只需要把它与矩阵相乘即可 这里还有一个有趣的变换，",
  "n_reviews": 0,
  "start": 502.88,
  "end": 509.62
 },
 {
  "input": "Here's a fun transformation with a special name, called a shear. ",
  "translatedText": "这是一个有趣的变换，有一个特殊的名称，称为剪切。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.56,
  "end": 514.3
 },
 {
  "input": "In it, I-hat remains fixed, so the first column of the matrix is 1, 0, but J-hat moves over to the coordinates 1, 1, which become the second column of the matrix. ",
  "translatedText": "其中，I-hat 保持固定，因此矩阵的第一列是 1, 0， 但 J-hat 移动到坐标 1, 1，成为矩阵的第二列。",
  "model": "google_nmt",
  "from_community_srt": "它有个特殊的名称叫“剪切” 在这个变换里， i帽保持不变， 所以矩阵第一列为(1, 0) 但是j帽移动到了坐标(1, 1)， 所以矩阵第二列为(1,",
  "n_reviews": 0,
  "start": 515.0,
  "end": 525.3
 },
 {
  "input": "And at the risk of being redundant here, figuring out how a shear transforms a given vector comes down to multiplying this matrix by that vector. ",
  "translatedText": "这里冒着多余的风险，弄清楚剪切如何变换 给定向量可以归结为将该矩阵乘以该向量。",
  "model": "google_nmt",
  "from_community_srt": "1) （可能有些啰嗦， 但还是说一下） 为了计算出给定向量在剪切变换后的位置 只需要将矩阵与这个向量相乘即可 再比如说你想反过来看看问题 从一个矩阵出发，",
  "n_reviews": 0,
  "start": 525.3,
  "end": 534.08
 },
 {
  "input": "Let's say we want to go the other way around, starting with a matrix, say with columns 1, 2, and 3, 1, and we want to deduce what its transformation looks like. ",
  "translatedText": "假设我们想要反其道而行之，从一个矩阵开始，比如第 1, 2 和 3, 1 列，我们想要推断出它的变换是什么样的。",
  "model": "google_nmt",
  "from_community_srt": "比如说一个以(1, 2)和(3, 1)为列的矩阵 你想推测出它代表的线性变换是什么样的 暂停思考一下，",
  "n_reviews": 0,
  "start": 535.76,
  "end": 544.52
 },
 {
  "input": "Pause and take a moment to see if you can imagine it. ",
  "translatedText": "暂停一下，看看你是否能想象得到。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.96,
  "end": 547.44
 },
 {
  "input": "One way to do this is to first move I-hat to 1, 2, then move J-hat to 3, 1, always moving the rest of space in such a way that keeps gridlines parallel and evenly spaced. ",
  "translatedText": "一种方法是首先将 I-hat 移动到 1、2，然后将 J-hat 移动到 3、1，始终以保持网格线平行且均匀分布的方式移动其余空间。",
  "model": "google_nmt",
  "from_community_srt": "看看你能不能想象到 这里给出一种办法：首先将i帽移动到(1, 2)， 然后将j帽移动到(3, 1) 空间其他剩余部分随二者一起移动，",
  "n_reviews": 0,
  "start": 548.42,
  "end": 560.22
 },
 {
  "input": "If the vectors that I-hat and J-hat land on are linearly dependent, which, if you recall from last video, means that one is a scaled version of the other, it means that the linear transformation squishes all of 2D space onto the line where those two vectors sit, also known as the one-dimensional span of those two linearly dependent vectors. ",
  "translatedText": "如果 I-hat 和 J-hat 所落在的向量是线性相 关的（如果您还记得上一个视频），这意味着一个是另一个的 缩放版本，这意味着线性变换将所有 2D 空间压缩到这两 个向量所在的线，也称为这两个线性相关向量的一维跨度。",
  "model": "google_nmt",
  "from_community_srt": "以保持网格线平行且等距分布 如果变换后的i帽和变换后的j帽是线性相关的 回顾上期视频的内容， 意味着其中一个向量是另一个的倍数 那么这个线性变换将整个二维空间挤压到它们所在一条直线上 也就是这两个线性相关向量所张成的一维空间 总之，",
  "n_reviews": 0,
  "start": 561.68,
  "end": 582.42
 },
 {
  "input": "To sum up, linear transformations are a way to move around space such that gridlines remain parallel and evenly spaced, and such that the origin remains fixed. ",
  "translatedText": "总而言之，线性变换是一种在空间中移动的方法，使 得网格线保持平行且均匀分布，并且原点保持固定。",
  "model": "google_nmt",
  "from_community_srt": "线性变换是操纵空间的一种手段 它保持网格线平行且等距分布， 并且保持原点不动 令人高兴的是，",
  "n_reviews": 0,
  "start": 584.42,
  "end": 593.94
 },
 {
  "input": "Flightfully, these transformations can be described using only a handful of numbers, the coordinates of where each basis vector lands. ",
  "translatedText": "轻率地说，这些变换只需使用少数数字 即可描述，即每个基向量落地的坐标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 594.54,
  "end": 601.53
 },
 {
  "input": "Matrices give us a language to describe these transformations, where the columns represent those coordinates, and matrix-vector multiplication is just a way to compute what that transformation does to a given vector. ",
  "translatedText": "矩阵为我们提供了一种描述这些变换的语言， 其中列代表这些坐标，而矩阵向量乘法只是 计算该变换对给定向量的作用的一种方法。",
  "model": "google_nmt",
  "from_community_srt": "这种变换只需要几个数字就能描述清楚 这些数字就是变换后基向量的坐标 以这些坐标为列所构成的矩阵为我们提供了一种描述线性变换的语言 而矩阵向量乘法就是计算线性变换作用于给定向量的一种途径",
  "n_reviews": 0,
  "start": 602.76,
  "end": 614.66
 },
 {
  "input": "The important takeaway here is that every time you see a matrix, you can interpret it as a certain transformation of space. ",
  "translatedText": "这里重要的一点是，每次看到矩阵时， 您都可以将其解释为某种空间变换。",
  "model": "google_nmt",
  "from_community_srt": "这里重要的一点是， 每当你看到一个矩阵时 你都可以把它解读为对空间的一种特定变换 一旦真正消化了这些内容，",
  "n_reviews": 0,
  "start": 615.36,
  "end": 621.88
 },
 {
  "input": "Once you really digest this idea, you're in a great position to understand linear algebra deeply. ",
  "translatedText": "一旦你真正理解了这个想法， 你就能够深入理解线性代数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 622.58,
  "end": 627.32
 },
 {
  "input": "Almost all of the topics coming up, from matrix multiplication to determinants, change of basis, eigenvalues, all of these will become easier to understand once you start thinking about matrices as transformations of space. ",
  "translatedText": "几乎所有即将出现的主题，从矩阵乘法到行列 式、基础变化、特征值，一旦您开始将矩阵视 为空间变换，所有这些都会变得更容易理解。",
  "model": "google_nmt",
  "from_community_srt": "你就在深刻理解线性代数上占据了极佳的位置 当你将矩阵看作空间的变换之后， 此后几乎所有主题 从矩阵乘法， 到行列式、基变换、特征值等都会更加容易理解 下期视频中，",
  "n_reviews": 0,
  "start": 627.66,
  "end": 640.56
 },
 {
  "input": "Most immediately, in the next video, I'll be talking about multiplying two matrices together. ",
  "translatedText": "最直接的是，在下一个视频中 ，我将讨论两个矩阵相乘。",
  "model": "google_nmt",
  "from_community_srt": "我就会开始讨论两个矩阵的乘积 到时候再见！",
  "n_reviews": 0,
  "start": 641.3,
  "end": 645.66
 },
 {
  "input": "See you then! ",
  "translatedText": "回头见！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.12,
  "end": 645.66
 },
 {
  "input": "Thank you for watching! ",
  "translatedText": "感谢您的观看！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.12,
  "end": 646.32
 }
]