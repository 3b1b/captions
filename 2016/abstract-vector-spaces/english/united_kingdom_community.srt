1
00:00:17,280 --> 00:00:21,590
I'd like to revisit a deceptively simple question
that I asked in the very first video of this

2
00:00:21,590 --> 00:00:22,790
series,

3
00:00:22,790 --> 00:00:24,600
What are vectors?

4
00:00:24,600 --> 00:00:29,290
Is a two-dimensional vector for example, fundamentally
an arrow on a flat plane that we can describe

5
00:00:29,290 --> 00:00:30,990
with co-ordinates for convenience?

6
00:00:30,990 --> 00:00:36,340
Or, is it fundamentally that pair of real
numbers, which is just nicely visualised as

7
00:00:36,340 --> 00:00:38,660
an arrow on a flat plane?

8
00:00:38,660 --> 00:00:42,410
Or are both of these just manifestations of
something deeper?

9
00:00:42,410 --> 00:00:47,900
On the one hand, defining vectors as primarily
being a list of numbers feels clear-cut and

10
00:00:47,900 --> 00:00:49,090
unambiguous.

11
00:00:49,090 --> 00:00:52,470
It makes things like four-dimensional vectors
or one hundred-dimensional vectors

12
00:00:52,470 --> 00:00:56,010
sound like real, concrete ideas that you can
actually work with.

13
00:00:56,010 --> 00:01:01,280
When otherwise, an idea like four dimensions
is just a vague, geometric notion that's difficult

14
00:01:01,280 --> 00:01:02,280
to describe

15
00:01:02,280 --> 00:01:05,730
without waving your hands a bit.

16
00:01:05,730 --> 00:01:10,530
But on the other hand, a common sensation
for those who actually work with linear algebra,

17
00:01:10,530 --> 00:01:13,560
especially as you get more fluent with changing
your basis,

18
00:01:13,560 --> 00:01:17,530
is that you're dealing with a space that exists
independently from the co-ordinates that you

19
00:01:17,530 --> 00:01:18,530
give it,

20
00:01:18,530 --> 00:01:20,890
and that co-ordinates are actually somewhat
arbitrary,

21
00:01:20,890 --> 00:01:24,640
depending on what you happen to choose as
your basis vectors.

22
00:01:24,640 --> 00:01:28,320
Core topics in linear algebra, like determinants
and eigenvectors,

23
00:01:28,320 --> 00:01:31,420
seem indifferent to your choice of co-ordinate
systems.

24
00:01:31,420 --> 00:01:34,960
The determinant tells you how much a transformation
scales areas,

25
00:01:34,960 --> 00:01:40,180
and eigenvectors are the ones that stay on
their own span during a transformation.

26
00:01:40,180 --> 00:01:44,580
But both of these properties are inherently
spacial, and you can freely change your co-ordinate

27
00:01:44,580 --> 00:01:45,580
system

28
00:01:45,580 --> 00:01:49,840
without changing the underlying values of
either one.

29
00:01:49,840 --> 00:01:54,759
But, if vectors are fundamentally not lists
of real numbers,

30
00:01:54,759 --> 00:01:57,640
and if their underlying essence is something
more spacial,

31
00:01:57,640 --> 00:02:03,479
that just begs the question of what Mathematicians
mean when they use a word like space or spacial.

32
00:02:03,479 --> 00:02:07,000
To build up to where this is going, I'd actually
like to spend the bulk of this video talking

33
00:02:07,000 --> 00:02:08,000
about something

34
00:02:08,000 --> 00:02:12,980
which is neither an arrow, nor a list of numbers,
but also has vector-ish qualities:

35
00:02:12,980 --> 00:02:14,180
functions.

36
00:02:14,180 --> 00:02:19,840
You see, there's a sense in which functions
are actually just another type of vector.

37
00:02:19,840 --> 00:02:23,770
In the same way that you can add two vectors
together, there's also a sensible notion for

38
00:02:23,770 --> 00:02:24,900
adding two functions,

39
00:02:24,900 --> 00:02:28,569
f and g, to get a new function, (f+g).

40
00:02:28,569 --> 00:02:31,900
It's one of those thing where you kind of
already know what it's gonna be, but actually

41
00:02:31,900 --> 00:02:34,010
phrasing it is a mouthful.

42
00:02:34,010 --> 00:02:40,209
The output of this new function at any given
input, like -4, is the sum of the outputs

43
00:02:40,209 --> 00:02:41,489
of f and g,

44
00:02:41,489 --> 00:02:44,900
when you evaluate them each at that same input,
-4.

45
00:02:44,900 --> 00:02:51,569
Or, more generally, the value of the sum function
at any given input x is the sum of the values

46
00:02:51,569 --> 00:03:01,180
of f(x) + g(x).

47
00:03:01,180 --> 00:03:04,420
This is pretty similar to adding vectors co-ordinate
by co-ordinate,

48
00:03:04,420 --> 00:03:10,129
it's just that there are, in a sense, infinitely
many co-ordinates to deal with.

49
00:03:10,129 --> 00:03:15,480
Similarly, there's a sensible notion for scaling
a function by a real number,

50
00:03:15,480 --> 00:03:20,310
just scale all of the outputs by that number.

51
00:03:20,310 --> 00:03:24,099
And again, this is analogous to scaling a
vector co-ordinate by co-ordinate,

52
00:03:24,099 --> 00:03:27,780
it just feels like there's infinitely many
co-ordinates.

53
00:03:27,780 --> 00:03:34,290
Now, given that the only thing vectors can
really do is get added together or scaled,

54
00:03:34,290 --> 00:03:38,099
it feels like we should be able to take the
same useful constructs and problem solving

55
00:03:38,099 --> 00:03:39,099
techniques

56
00:03:39,099 --> 00:03:43,829
of linear algebra, that were originally thought
about in the context of arrows in space,

57
00:03:43,829 --> 00:03:46,580
and apply them to functions as well.

58
00:03:46,580 --> 00:03:52,530
For example, there's a perfectly reasonable
notion of a linear transformation for functions,

59
00:03:52,530 --> 00:04:00,080
something that takes in one function, and
turns it into another.

60
00:04:00,080 --> 00:04:03,049
One familiar example comes from calculus:
the derivative.

61
00:04:03,049 --> 00:04:09,060
It's something which transforms one function
into another function.

62
00:04:09,060 --> 00:04:12,870
Sometimes in this context, you'll hear these
called operators instead of transformations,

63
00:04:12,870 --> 00:04:14,939
but the meaning is the same.

64
00:04:14,939 --> 00:04:20,720
A natural question you might want to ask,
is what it means for a transformation of functions

65
00:04:20,720 --> 00:04:22,500
to be linear.

66
00:04:22,500 --> 00:04:27,280
The formal definition of linearity is relatively
abstract and symbolically driven

67
00:04:27,280 --> 00:04:31,090
compared to the way that I first talked about
it in chapter 3 of this series,

68
00:04:31,090 --> 00:04:35,690
but the reward of abstractness is that we'll
get something general enough to apply to functions,

69
00:04:35,690 --> 00:04:39,160
as well as arrows.

70
00:04:39,160 --> 00:04:44,350
A transformation is linear if it satisfies
two properties, commonly called additivity

71
00:04:44,350 --> 00:04:46,479
and scaling.

72
00:04:46,479 --> 00:04:51,530
Additivity means that if you add two vectors,
v and w, then apply a transformation to their

73
00:04:51,530 --> 00:04:55,670
sum,

74
00:04:55,670 --> 00:05:00,270
you get the same result as if you added the
transformed versions of v and w.

75
00:05:00,270 --> 00:05:09,990
The scaling property is that when you scale
a vector v by some number,

76
00:05:09,990 --> 00:05:13,310
then apply the transformation,

77
00:05:13,310 --> 00:05:18,220
you get the same ultimate vector as if you
scale the transformed version of v by that

78
00:05:18,220 --> 00:05:21,949
same amount.

79
00:05:21,949 --> 00:05:26,569
The way you'll often hear this described is
that linear transformations preserve the operations

80
00:05:26,569 --> 00:05:27,569
of

81
00:05:27,569 --> 00:05:32,460
vector addition and scalar multiplication.

82
00:05:32,460 --> 00:05:36,280
The idea of gridlines remaining parallel and
evenly spaced is that I've talked about in

83
00:05:36,280 --> 00:05:37,280
past videos

84
00:05:37,280 --> 00:05:42,810
is really just an illustration of what these
two properties mean in the specific case of

85
00:05:42,810 --> 00:05:45,270
2D points in space.

86
00:05:45,270 --> 00:05:47,890
One of the most important consequences of
these properties,

87
00:05:47,890 --> 00:05:53,400
which makes matrix-vector multiplication possible,
is that a linear transformation is completely

88
00:05:53,400 --> 00:05:54,400
described

89
00:05:54,400 --> 00:05:58,020
by where it takes the basis vectors.

90
00:05:58,020 --> 00:06:02,710
Since any vector can be expressed by scaling
and adding the basis vectors in some way,

91
00:06:02,710 --> 00:06:07,750
finding the transformed version of a vector
comes down to scaling and adding the transformed

92
00:06:07,750 --> 00:06:08,750
versions of

93
00:06:08,750 --> 00:06:12,610
the basis vectors in that same way.

94
00:06:12,610 --> 00:06:18,480
As you'll see in a moment, this is as true
for functions as it is for arrows.

95
00:06:18,480 --> 00:06:23,050
For example, calculus students are always
using the fact that the derivative is additive

96
00:06:23,050 --> 00:06:24,050
and

97
00:06:24,050 --> 00:06:28,270
has the scaling property, even f they haven't
heard it phrased that way.

98
00:06:28,270 --> 00:06:33,810
If you add two functions, then take the derivative,
it's the same as first taking the derivative

99
00:06:33,810 --> 00:06:34,810
of each one

100
00:06:34,810 --> 00:06:38,660
separately, then adding the result.

101
00:06:38,660 --> 00:06:44,669
Similarly, if you scale a function, then take
the derivative, it's the same as first taking

102
00:06:44,669 --> 00:06:45,740
the derivative,

103
00:06:45,740 --> 00:06:50,760
then scaling the result.

104
00:06:50,760 --> 00:06:55,300
To really drill in the parallel, let's see
what it might look like to describe the derivative

105
00:06:55,300 --> 00:06:57,020
with a matrix.

106
00:06:57,020 --> 00:07:01,490
This will be a little tricky, since function
spaces have a tendency to be infinite-dimensional,

107
00:07:01,490 --> 00:07:04,979
but I think this exercise is actually quite
satisfying.

108
00:07:04,979 --> 00:07:12,580
Let's limit ourselves to polynomials, things
like x^2 + 3x + 5 or 4x^7 - 5x^2.

109
00:07:12,580 --> 00:07:16,900
Each of the polynomials in our space will
only have finitely many terms,

110
00:07:16,900 --> 00:07:22,319
but the full space is going to include polynomials
with arbitrarily large degree.

111
00:07:22,319 --> 00:07:28,349
The first thing we need to do is give co-ordinates
to this space, which requires choosing a basis.

112
00:07:28,349 --> 00:07:32,860
Since polynomials are already written down
as the sum of scaled powers of the variable

113
00:07:32,860 --> 00:07:33,860
x,

114
00:07:33,860 --> 00:07:38,009
it's pretty natural to just choose pure powers
of x as the basis function.

115
00:07:38,009 --> 00:07:44,430
In other words, our first basis function will
be the constant function, b_0(x) = 1.

116
00:07:44,430 --> 00:07:53,960
The second basis function will be b_1(x) = x,
then b_2(x)=x^2, then b_3(x)=x^3, and so on.

117
00:07:53,960 --> 00:07:58,460
The role that these basis functions serve
will be similar to the roles of i-hat, j-hat

118
00:07:58,460 --> 00:07:59,460
and k-hat

119
00:07:59,460 --> 00:08:02,379
in the world of vectors as arrows.

120
00:08:02,379 --> 00:08:06,970
Since our polynomials can have arbitrarily
large degree, this set of basis functions

121
00:08:06,970 --> 00:08:08,190
is infinite.

122
00:08:08,190 --> 00:08:12,289
But that's okay, it just means that when we
treat out polynomials as vectors,

123
00:08:12,289 --> 00:08:14,539
they're going to have infinitely many co-ordinates.

124
00:08:14,539 --> 00:08:22,320
A polynomial like x^2 + 3x + 5, for example,
would be described with the co-ordiantes 5,

125
00:08:22,320 --> 00:08:23,720
3, 1,

126
00:08:23,720 --> 00:08:26,240
then infinitely many zeros.

127
00:08:26,240 --> 00:08:31,710
You'd read this as saying it's 5 times the
first basis function, plus 3 times that second

128
00:08:31,710 --> 00:08:33,040
basis function,

129
00:08:33,040 --> 00:08:37,760
plus 1 times the third basis function, and
then none of the other basis functions

130
00:08:37,760 --> 00:08:40,970
should be added from that point on.

131
00:08:40,970 --> 00:08:50,920
The polynomial 4x^7 - 5x^2 would have the
co-ordinates 0, 0, -5, 0, 0, 0, 0, 4, then

132
00:08:50,920 --> 00:08:53,410
an infinite string of zeros.

133
00:08:53,410 --> 00:08:59,140
In general, since every individual polynomial
has only finitely many terms, it's co-ordinates

134
00:08:59,140 --> 00:09:00,140
will be some finite

135
00:09:00,140 --> 00:09:07,280
string of numbers, with an infinite tail of
zeros.

136
00:09:07,280 --> 00:09:11,720
In this co-ordinate system, the derivative
is described with an infinite matrix, that's

137
00:09:11,720 --> 00:09:13,440
mostly full of zeors,

138
00:09:13,440 --> 00:09:17,570
but which has the positive integers counting
down on this offset diagonal.

139
00:09:17,570 --> 00:09:21,800
I'll talk about how you could find this matrix
in just a moment, but the best way to get

140
00:09:21,800 --> 00:09:22,800
a feel for it,

141
00:09:22,800 --> 00:09:25,250
is to just watch it in action.

142
00:09:25,250 --> 00:09:32,110
Take the co-ordinates representing the polynomial
x^3 + 5x^2 + 4x + 5,

143
00:09:32,110 --> 00:09:40,670
then put those co-ordinates on the right of
the matrix.

144
00:09:40,670 --> 00:09:45,830
The only term which contributes to the first
co-ordinate of the result is 1x4,

145
00:09:45,830 --> 00:09:50,670
which means the constant term in the result
will be 4.

146
00:09:50,670 --> 00:09:55,670
This corresponds to the fact that the derivative
of 4x is the constant 4.

147
00:09:55,670 --> 00:10:02,270
The only term contributing to the second co-ordinate
of the matrix-vector product is 2x5,

148
00:10:02,270 --> 00:10:06,590
which means the coefficient in front of x
in the derivative is 10.

149
00:10:06,590 --> 00:10:10,210
That one corresponds to the derivative of
5x^2.

150
00:10:10,210 --> 00:10:18,140
Similarly, the third co-ordinate in the matrix-vector
product comes down to taking 3x1.

151
00:10:18,140 --> 00:10:23,150
This one corresponds to the derivative of
x^3 being 3x^2.

152
00:10:23,150 --> 00:10:27,020
And after that, it'll be nothing but zeros.

153
00:10:27,020 --> 00:10:30,850
What makes this possible is that the derivative
is linear.

154
00:10:30,850 --> 00:10:35,880
And, for those of you who like to pause and
ponder, you could construct this matrix by

155
00:10:35,880 --> 00:10:36,880
taking the derivative

156
00:10:36,880 --> 00:10:48,080
of each basis function, and putting the co-ordinates
of the results in each column.

157
00:10:48,080 --> 00:11:05,520
So, surprisingly, matrix-vector multiplication
and taking a derivative, which at first seem

158
00:11:05,520 --> 00:11:06,860
like completely different

159
00:11:06,860 --> 00:11:10,510
animals, are both just really members of the
same family.

160
00:11:10,510 --> 00:11:15,460
In fact, most of the concepts I've talked
about in this series with respect to vectors

161
00:11:15,460 --> 00:11:17,260
as arrows in space,

162
00:11:17,260 --> 00:11:22,800
things like the dot product or eigenvectors,
have direct analogues in the world of functions.

163
00:11:22,800 --> 00:11:27,250
Though sometimes they go by different names,
things like 'inner product' or 'eigenfunction'.

164
00:11:27,250 --> 00:11:31,660
So, back to the question of what is a vector.

165
00:11:31,660 --> 00:11:36,570
The point I want to make here is that there
are lots of vector-ish things in maths,

166
00:11:36,570 --> 00:11:40,720
as long as you're dealing with a set of objects
where there's a reasonable notion of scaling

167
00:11:40,720 --> 00:11:41,840
and adding,

168
00:11:41,840 --> 00:11:46,550
whether that's a set of arrows in space, lists
of numbers, functions or whatever other crazy

169
00:11:46,550 --> 00:11:47,550
thing

170
00:11:47,550 --> 00:11:51,980
you choose to define, all of the tools developed
in linear algebra regarding vectors,

171
00:11:51,980 --> 00:11:57,580
linear transformations, and all that stuff,
should be able to apply.

172
00:11:57,580 --> 00:12:01,650
Take a moment to imagine yourself right now,
as a mathematician developing the theory of

173
00:12:01,650 --> 00:12:03,310
linear algebra.

174
00:12:03,310 --> 00:12:07,890
You want all of the definitions and discoveries
of your work to apply to all of the vector-ish

175
00:12:07,890 --> 00:12:08,890
things

176
00:12:08,890 --> 00:12:13,540
in full generality, not just to one specific
case.

177
00:12:13,540 --> 00:12:18,780
These sets of vector-ish things, like arrows
or lists of numbers or functions, are called

178
00:12:18,780 --> 00:12:20,660
vector spaces,

179
00:12:20,660 --> 00:12:23,370
and what you as the mathematician might want
to do is say,

180
00:12:23,370 --> 00:12:24,370
"Hey everyone!

181
00:12:24,370 --> 00:12:28,190
I don't want to think about all the different
types of crazy vector spaces that

182
00:12:28,190 --> 00:12:33,200
you all might come up with, so what you do
is establish a list of rules that vector addition

183
00:12:33,200 --> 00:12:34,200
and scaling

184
00:12:34,200 --> 00:12:36,580
have to abide by.

185
00:12:36,580 --> 00:12:40,780
These rules are called axioms, and in the
modern theory of linear algebra, there are

186
00:12:40,780 --> 00:12:41,820
8 axioms

187
00:12:41,820 --> 00:12:46,240
that any vector space must satisfy, if all
of the theory and constructs that we've discovered

188
00:12:46,240 --> 00:12:47,670
are going to apply.

189
00:12:47,670 --> 00:12:51,730
I'll leave them on the screen here for anyone
who wants to pause and ponder, but basically,

190
00:12:51,730 --> 00:12:56,080
it's just a checklist, to make sure that the
notions of vector addition and scalar multiplication

191
00:12:56,080 --> 00:12:59,170
do the things that you'd expect them to do.

192
00:12:59,170 --> 00:13:03,880
These axioms are not so much fundamental rules
of nature, as they are an interface between

193
00:13:03,880 --> 00:13:04,880
you,

194
00:13:04,880 --> 00:13:08,240
the mathematician discovering results, and
other people who might want to apply those

195
00:13:08,240 --> 00:13:09,240
results

196
00:13:09,240 --> 00:13:11,190
to new sorts of vectors spaces.

197
00:13:11,190 --> 00:13:15,680
If, for example, someone defines some crazy
type of vector space, like the set of all

198
00:13:15,680 --> 00:13:17,010
pi creatures,

199
00:13:17,010 --> 00:13:21,560
with some definition of adding and scaling
pi creatures, these axioms are like a checklist

200
00:13:21,560 --> 00:13:22,560
of things

201
00:13:22,560 --> 00:13:27,190
that they need to verify about their definitions,
before they can start applying the results

202
00:13:27,190 --> 00:13:28,890
of linear algebra.

203
00:13:28,890 --> 00:13:33,010
And you as the mathematician, never have to
think about all the possible crazy vector

204
00:13:33,010 --> 00:13:35,050
spaces people might define,

205
00:13:35,050 --> 00:13:39,980
you just have to prove your results in terms
of these axioms, so anyone who's definitions

206
00:13:39,980 --> 00:13:40,980
satisfy those

207
00:13:40,980 --> 00:13:47,060
axioms can happily apply you results, even
if you never thought about their situation.

208
00:13:47,060 --> 00:13:51,120
As a consequence, you'd tend to phrase all
of your results pretty abstractly, which is

209
00:13:51,120 --> 00:13:52,120
to say,

210
00:13:52,120 --> 00:13:54,110
only in terms of these axioms,

211
00:13:54,110 --> 00:14:02,050
rather than centring on a specific type of
vector, like arrows in space, or functions.

212
00:14:02,050 --> 00:14:07,310
For example, this is why just about every
textbook you'll find will define linear transformations

213
00:14:07,310 --> 00:14:12,200
in terms of additivity and scaling, rather
than talking about gridlines remaining parallel

214
00:14:12,200 --> 00:14:14,050
and evenly spaced,

215
00:14:14,050 --> 00:14:18,000
even though the latter is more intuitive,
and at least in my view, more helpful for

216
00:14:18,000 --> 00:14:19,380
first time learners,

217
00:14:19,380 --> 00:14:22,750
even if it is specific to one situation.

218
00:14:22,750 --> 00:14:27,560
So the mathematicians answer to "what are
vectors?" is to just ignore the question.

219
00:14:27,560 --> 00:14:32,380
In the modern theory, the form that vectors
take doesn't really matter, arrows, lists

220
00:14:32,380 --> 00:14:33,450
of numbers, functions,

221
00:14:33,450 --> 00:14:38,510
pi creatures, really it can be anything so
long as there is some notion of adding and

222
00:14:38,510 --> 00:14:39,660
scaling vectors

223
00:14:39,660 --> 00:14:42,350
that follows these rules.

224
00:14:42,350 --> 00:14:45,450
It's like asking what the number 'three' really
is.

225
00:14:45,450 --> 00:14:50,100
Whenever it comes up concretely, it's in the
context of some triplet of things, but in

226
00:14:50,100 --> 00:14:51,100
maths,

227
00:14:51,100 --> 00:14:55,520
it's treated as an abstraction for all possible
triplets of things, and lets you reason about

228
00:14:55,520 --> 00:14:57,050
all possible triplets,

229
00:14:57,050 --> 00:14:59,240
using a single idea.

230
00:14:59,240 --> 00:15:04,890
Same goes with vectors, which have many embodiments,
but maths abstracts them all into a single,

231
00:15:04,890 --> 00:15:08,160
intangible notion of a vector space.

232
00:15:08,160 --> 00:15:13,450
But, as anyone watching this series knows,
I think it's better to begin reasoning about

233
00:15:13,450 --> 00:15:15,610
vectors in a concrete,

234
00:15:15,610 --> 00:15:19,820
visualisable setting, like 2D space with arrows
rooted at the origin.

235
00:15:19,820 --> 00:15:24,450
But as you learn more linear algebra, know
that these tools apply much more generally,

236
00:15:24,450 --> 00:15:29,650
and that this is the underlying reason why
textbooks and lectures tend to be phrased,

237
00:15:29,650 --> 00:15:32,260
well, absractly.

238
00:15:32,260 --> 00:15:36,890
So with that, folks, I think I'll call it
an end to this essence of linear algebra series.

239
00:15:36,890 --> 00:15:40,680
If you've watched and understood the videos,
I really do believe that you have a solid

240
00:15:40,680 --> 00:15:41,680
foundation

241
00:15:41,680 --> 00:15:44,720
in the underlying intuitions of linear algebra.

242
00:15:44,720 --> 00:15:47,740
This is not the same thing as learning the
full topic, of course, that's something that

243
00:15:47,740 --> 00:15:48,830
can only really come from

244
00:15:48,830 --> 00:15:53,110
working through problems, but the learning
you do moving forward can be substantially

245
00:15:53,110 --> 00:15:54,110
more efficient

246
00:15:54,110 --> 00:15:56,650
if you have all the right intuitions in place.

247
00:15:56,650 --> 00:16:38,610
So, have fun applying those intuitions, and
best of luck with your future learning.

