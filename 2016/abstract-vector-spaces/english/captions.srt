1
00:00:16,880 --> 00:00:19,389
I'd like to revisit a deceptively simple question 

2
00:00:19,389 --> 00:00:22,000
that I asked in the very first video of this series.

3
00:00:22,700 --> 00:00:23,560
What are vectors?

4
00:00:24,480 --> 00:00:27,562
Is a two-dimensional vector, for example, fundamentally an arrow on 

5
00:00:27,562 --> 00:00:30,600
a flat plane that we can describe with coordinates for convenience?

6
00:00:30,860 --> 00:00:34,321
Or is it fundamentally that pair of real numbers which 

7
00:00:34,321 --> 00:00:37,720
is just nicely visualized as an arrow on a flat plane?

8
00:00:38,480 --> 00:00:41,360
Or are both of these just manifestations of something deeper?

9
00:00:42,300 --> 00:00:45,480
On the one hand, defining vectors as primarily being 

10
00:00:45,480 --> 00:00:48,480
a list of numbers feels clear-cut and unambiguous.

11
00:00:49,060 --> 00:00:53,435
It makes things like four-dimensional vectors or 100-dimensional vectors sound like real, 

12
00:00:53,435 --> 00:00:55,720
concrete ideas that you can actually work with.

13
00:00:55,720 --> 00:00:59,775
When otherwise, an idea like four dimensions is just a vague geometric 

14
00:00:59,775 --> 00:01:03,660
notion that's difficult to describe without waving your hands a bit.

15
00:01:05,540 --> 00:01:09,247
But on the other hand, a common sensation for those who actually work with 

16
00:01:09,247 --> 00:01:13,003
linear algebra, especially as you get more fluent with changing your basis, 

17
00:01:13,003 --> 00:01:16,512
is that you're dealing with a space that exists independently from the 

18
00:01:16,512 --> 00:01:20,664
coordinates that you give it, and that coordinates are actually somewhat arbitrary, 

19
00:01:20,664 --> 00:01:23,680
depending on what you happen to choose as your basis vectors.

20
00:01:24,480 --> 00:01:27,890
Core topics in linear algebra, like determinants and eigenvectors, 

21
00:01:27,890 --> 00:01:30,640
seem indifferent to your choice of coordinate systems.

22
00:01:31,440 --> 00:01:34,954
The determinant tells you how much a transformation scales areas, 

23
00:01:34,954 --> 00:01:39,320
and eigenvectors are the ones that stay on their own span during a transformation.

24
00:01:40,000 --> 00:01:42,851
But both of these properties are inherently spatial, 

25
00:01:42,851 --> 00:01:47,210
and you can freely change your coordinate system without changing the underlying 

26
00:01:47,210 --> 00:01:48,340
values of either one.

27
00:01:50,760 --> 00:01:53,974
But if vectors are not fundamentally lists of real numbers, 

28
00:01:53,974 --> 00:01:57,135
and if their underlying essence is something more spatial, 

29
00:01:57,135 --> 00:02:00,993
that just begs the question of what mathematicians mean when they use a 

30
00:02:00,993 --> 00:02:02,440
word like space or spatial.

31
00:02:03,400 --> 00:02:06,570
To build up to where this is going, I'd actually like to spend the 

32
00:02:06,570 --> 00:02:09,835
bulk of this video talking about something which is neither an arrow 

33
00:02:09,835 --> 00:02:13,100
nor a list of numbers, but also has vector-ish qualities â€“ functions.

34
00:02:13,740 --> 00:02:17,880
You see, there's a sense in which functions are actually just another type of vector.

35
00:02:19,760 --> 00:02:22,592
In the same way that you can add two vectors together, 

36
00:02:22,592 --> 00:02:25,991
there's also a sensible notion for adding two functions, f and g, 

37
00:02:25,991 --> 00:02:27,640
to get a new function, f plus g.

38
00:02:28,200 --> 00:02:31,507
It's one of those things where you kind of already know what it's going to be, 

39
00:02:31,507 --> 00:02:33,140
but actually phrasing it is a mouthful.

40
00:02:33,960 --> 00:02:38,693
The output of this new function at any given input, 

41
00:02:38,693 --> 00:02:44,520
like negative four, is the sum of the same input, negative four.

42
00:02:45,420 --> 00:02:49,507
Or more generally, the value of the sum function at any 

43
00:02:49,507 --> 00:02:53,740
given input x is the sum of the values f of x plus g of x.

44
00:03:00,700 --> 00:03:04,279
This is pretty similar to adding vectors coordinate by coordinate, 

45
00:03:04,279 --> 00:03:08,500
it's just that there are, in a sense, infinitely many coordinates to deal with.

46
00:03:11,100 --> 00:03:15,589
Similarly, there's a sensible notion for scaling a function by a real number, 

47
00:03:15,589 --> 00:03:18,180
just scale all of the outputs by that number.

48
00:03:20,240 --> 00:03:23,690
And again, this is analogous to scaling a vector coordinate by coordinate, 

49
00:03:23,690 --> 00:03:26,220
it just feels like there's infinitely many coordinates.

50
00:03:28,900 --> 00:03:33,546
Now, given that the only thing vectors can really do is get added together or scaled, 

51
00:03:33,546 --> 00:03:37,814
it feels like we should be able to take the same useful constructs and problem 

52
00:03:37,814 --> 00:03:41,866
solving techniques of linear algebra that were originally thought about in 

53
00:03:41,866 --> 00:03:45,540
the context of arrows and space and apply them to functions as well.

54
00:03:46,540 --> 00:03:51,070
For example, there's a perfectly reasonable notion of a linear transformation 

55
00:03:51,070 --> 00:03:55,600
for functions, something that takes in one function and turns it into another.

56
00:03:59,820 --> 00:04:02,780
One familiar example comes from calculus, the derivative.

57
00:04:03,420 --> 00:04:07,140
It's something which transforms one function into another function.

58
00:04:08,720 --> 00:04:12,721
Sometimes in this context you'll hear these called operators instead of transformations, 

59
00:04:12,721 --> 00:04:13,980
but the meaning is the same.

60
00:04:16,240 --> 00:04:18,864
A natural question you might want to ask is what it 

61
00:04:18,864 --> 00:04:21,540
means for a transformation of functions to be linear.

62
00:04:22,440 --> 00:04:26,565
The formal definition of linearity is relatively abstract and symbolically driven 

63
00:04:26,565 --> 00:04:30,440
compared to the way that I first talked about it in chapter 3 of this series.

64
00:04:30,440 --> 00:04:33,752
But the reward of abstractness is that we'll get something 

65
00:04:33,752 --> 00:04:36,840
general enough to apply to functions as well as arrows.

66
00:04:39,180 --> 00:04:42,683
A transformation is linear if it satisfies two properties, 

67
00:04:42,683 --> 00:04:45,000
commonly called additivity and scaling.

68
00:04:46,040 --> 00:04:50,554
Additivity means that if you add two vectors, v and w, 

69
00:04:50,554 --> 00:04:57,613
then apply a transformation to their sum, you get the same result as if you added the 

70
00:04:57,613 --> 00:05:00,240
transformed versions of v and w.

71
00:05:04,520 --> 00:05:09,606
The scaling property is that when you scale a vector v by some number, 

72
00:05:09,606 --> 00:05:14,406
then apply the transformation, you get the same ultimate vector as 

73
00:05:14,406 --> 00:05:18,920
if you scaled the transformed version of v by that same amount.

74
00:05:21,700 --> 00:05:25,478
The way you'll often hear this described is that linear transformations 

75
00:05:25,478 --> 00:05:29,100
preserve the operations of vector addition and scalar multiplication.

76
00:05:32,200 --> 00:05:36,171
The idea of gridlines remaining parallel and evenly spaced that I've 

77
00:05:36,171 --> 00:05:40,028
talked about in past videos is really just an illustration of what 

78
00:05:40,028 --> 00:05:44,000
these two properties mean in the specific case of points in 2D space.

79
00:05:44,880 --> 00:05:48,166
One of the most important consequences of these properties, 

80
00:05:48,166 --> 00:05:50,960
which makes matrix vector multiplication possible, 

81
00:05:50,960 --> 00:05:54,685
is that a linear transformation is completely described by where it 

82
00:05:54,685 --> 00:05:56,000
takes the basis vectors.

83
00:05:57,720 --> 00:06:02,595
Since any vector can be expressed by scaling and adding the basis vectors in some way, 

84
00:06:02,595 --> 00:06:06,909
finding the transformed version of a vector comes down to scaling and adding 

85
00:06:06,909 --> 00:06:10,440
the transformed versions of the basis vectors in that same way.

86
00:06:12,280 --> 00:06:16,780
As you'll see in just a moment, this is as true for functions as it is for arrows.

87
00:06:18,360 --> 00:06:22,437
For example, calculus students are always using the fact that the derivative is 

88
00:06:22,437 --> 00:06:26,820
additive and has the scaling property, even if they haven't heard it phrased that way.

89
00:06:28,140 --> 00:06:31,187
If you add two functions, then take the derivative, 

90
00:06:31,187 --> 00:06:35,231
it's the same as first taking the derivative of each one separately, 

91
00:06:35,231 --> 00:06:36,580
then adding the result.

92
00:06:40,140 --> 00:06:43,305
Similarly, if you scale a function, then take the derivative, 

93
00:06:43,305 --> 00:06:46,880
it's the same as first taking the derivative, then scaling the result.

94
00:06:50,280 --> 00:06:53,037
To really drill in the parallel, let's see what it 

95
00:06:53,037 --> 00:06:56,120
might look like to describe the derivative with a matrix.

96
00:06:56,980 --> 00:07:00,332
This will be a little tricky, since function spaces have a tendency to be 

97
00:07:00,332 --> 00:07:03,820
infinite dimensional, but I think this exercise is actually quite satisfying.

98
00:07:04,840 --> 00:07:09,520
Let's limit ourselves to polynomials, things like x squared plus 3x plus 5, 

99
00:07:09,520 --> 00:07:11,860
or 4x to the seventh minus 5x squared.

100
00:07:12,330 --> 00:07:16,439
Each of the polynomials in our space will only have finitely many terms, 

101
00:07:16,439 --> 00:07:21,000
but the full space is going to include polynomials with arbitrarily large degree.

102
00:07:22,220 --> 00:07:25,597
The first thing we need to do is give coordinates to this space, 

103
00:07:25,597 --> 00:07:27,260
which requires choosing a basis.

104
00:07:28,180 --> 00:07:33,393
Since polynomials are already written down as the sum of scaled powers of the variable x, 

105
00:07:33,393 --> 00:07:37,680
it's pretty natural to just choose pure powers of x as the basis function.

106
00:07:38,280 --> 00:07:43,700
In other words, our first basis function will be the constant function, b0 of x equals 1.

107
00:07:44,180 --> 00:07:48,075
The second basis function will be b1 of x equals x, 

108
00:07:48,075 --> 00:07:53,320
then b2 of x equals x squared, then b3 of x equals x cubed, and so on.

109
00:07:53,860 --> 00:07:58,229
The role that these basis functions serve will be similar to the roles of i-hat, 

110
00:07:58,229 --> 00:08:00,980
j-hat, and k-hat in the world of vectors as arrows.

111
00:08:02,120 --> 00:08:05,269
Since our polynomials can have arbitrarily large degree, 

112
00:08:05,269 --> 00:08:07,480
this set of basis functions is infinite.

113
00:08:08,240 --> 00:08:11,823
But that's okay, it just means that when we treat our polynomials as vectors, 

114
00:08:11,823 --> 00:08:14,120
they're going to have infinitely many coordinates.

115
00:08:15,600 --> 00:08:19,842
A polynomial like x squared plus 3x plus 5, for example, 

116
00:08:19,842 --> 00:08:25,500
would be described with the coordinates 5, 3, 1, then infinitely many zeros.

117
00:08:26,100 --> 00:08:30,121
You'd read this as saying that it's 5 times the first basis function, 

118
00:08:30,121 --> 00:08:34,718
plus 3 times that second basis function, plus 1 times the third basis function, 

119
00:08:34,718 --> 00:08:39,200
and then none of the other basis functions should be added from that point on.

120
00:08:40,620 --> 00:08:47,176
The polynomial 4x to the seventh minus 5x squared would have the coordinates 0, 

121
00:08:47,176 --> 00:08:52,340
0, negative 5, 0, 0, 0, 0, 4, then an infinite string of zeros.

122
00:08:53,260 --> 00:08:57,857
In general, since every individual polynomial has only finitely many terms, 

123
00:08:57,857 --> 00:09:03,000
its coordinates will be some finite string of numbers with an infinite tail of zeros.

124
00:09:06,900 --> 00:09:10,427
In this coordinate system, the derivative is described with 

125
00:09:10,427 --> 00:09:13,249
an infinite matrix that's mostly full of zeros, 

126
00:09:13,249 --> 00:09:17,600
but which has the positive integers counting down on this offset diagonal.

127
00:09:18,400 --> 00:09:21,312
I'll talk about how you could find this matrix in just a moment, 

128
00:09:21,312 --> 00:09:24,360
but the best way to get a feel for it is to just watch it in action.

129
00:09:24,970 --> 00:09:31,175
Take the coordinates representing the polynomial x cubed plus 5x squared plus 4x plus 5, 

130
00:09:31,175 --> 00:09:34,940
then put those coordinates on the right of the matrix.

131
00:09:40,410 --> 00:09:45,238
The only term that contributes to the first coordinate of the result is 1 times 4, 

132
00:09:45,238 --> 00:09:48,380
which means the constant term in the result will be 4.

133
00:09:50,100 --> 00:09:54,380
This corresponds to the fact that the derivative of 4x is the constant 4.

134
00:09:55,640 --> 00:10:00,721
The only term contributing to the second coordinate of the matrix vector product 

135
00:10:00,721 --> 00:10:05,740
is 2 times 5, which means the coefficient in front of x in the derivative is 10.

136
00:10:06,500 --> 00:10:09,280
That one corresponds to the derivative of 5x squared.

137
00:10:10,780 --> 00:10:13,430
Similarly, the third coordinate in the matrix 

138
00:10:13,430 --> 00:10:16,080
vector product comes down to taking 3 times 1.

139
00:10:17,660 --> 00:10:21,740
This one corresponds to the derivative of x cubed being 3x squared.

140
00:10:23,080 --> 00:10:25,020
And after that, it'll be nothing but zeros.

141
00:10:26,880 --> 00:10:29,800
What makes this possible is that the derivative is linear.

142
00:10:31,640 --> 00:10:34,300
And for those of you who like to pause and ponder, 

143
00:10:34,300 --> 00:10:37,691
you could construct this matrix by taking the derivative of each 

144
00:10:37,691 --> 00:10:41,500
basis function and putting the coordinates of the results in each column.

145
00:10:59,780 --> 00:11:03,918
So, surprisingly, matrix vector multiplication and taking a derivative, 

146
00:11:03,918 --> 00:11:07,080
which at first seem like completely different animals, 

147
00:11:07,080 --> 00:11:09,840
are both just really members of the same family.

148
00:11:11,220 --> 00:11:14,849
In fact, most of the concepts I've talked about in this series with 

149
00:11:14,849 --> 00:11:19,333
respect to vectors as arrows in space, things like the dot product or eigenvectors, 

150
00:11:19,333 --> 00:11:21,842
have direct analogs in the world of functions, 

151
00:11:21,842 --> 00:11:26,540
though sometimes they go by different names, things like inner product or eigenfunction.

152
00:11:28,400 --> 00:11:30,880
So back to the question of what is a vector.

153
00:11:31,560 --> 00:11:35,840
The point I want to make here is that there are lots of vectorish things in math.

154
00:11:35,840 --> 00:11:40,536
As long as you're dealing with a set of objects where there's a reasonable notion of 

155
00:11:40,536 --> 00:11:45,508
scaling and adding, whether that's a set of arrows in space, lists of numbers, functions, 

156
00:11:45,508 --> 00:11:48,382
or whatever other crazy thing you choose to define, 

157
00:11:48,382 --> 00:11:51,918
all of the tools developed in linear algebra regarding vectors, 

158
00:11:51,918 --> 00:11:55,620
linear transformations and all that stuff, should be able to apply.

159
00:11:57,480 --> 00:11:59,839
Take a moment to imagine yourself right now as a 

160
00:11:59,839 --> 00:12:02,440
mathematician developing the theory of linear algebra.

161
00:12:02,440 --> 00:12:06,723
You want all of the definitions and discoveries of your work to apply to 

162
00:12:06,723 --> 00:12:11,300
all of the vectorish things in full generality, not just to one specific case.

163
00:12:13,400 --> 00:12:18,186
These sets of vectorish things, like arrows or lists of numbers or functions, 

164
00:12:18,186 --> 00:12:19,720
are called vector spaces.

165
00:12:20,580 --> 00:12:23,304
And what you as the mathematician might want to do is say, 

166
00:12:23,304 --> 00:12:25,981
hey everyone, I don't want to have to think about all the 

167
00:12:25,981 --> 00:12:29,260
different types of crazy vector spaces that you all might come up with.

168
00:12:29,260 --> 00:12:32,387
So what you do is establish a list of rules that 

169
00:12:32,387 --> 00:12:35,260
vector addition and scaling have to abide by.

170
00:12:36,400 --> 00:12:40,199
These rules are called axioms, and in the modern theory of linear algebra, 

171
00:12:40,199 --> 00:12:43,645
there are eight axioms that any vector space must satisfy if all of 

172
00:12:43,645 --> 00:12:47,040
the theory and constructs that we've discovered are going to apply.

173
00:12:47,700 --> 00:12:51,180
I'll leave them on the screen here for anyone who wants to pause and ponder, 

174
00:12:51,180 --> 00:12:54,614
but basically it's just a checklist to make sure that the notions of vector 

175
00:12:54,614 --> 00:12:58,140
addition and scalar multiplication do the things that you'd expect them to do.

176
00:12:58,720 --> 00:13:02,568
These axioms are not so much fundamental rules of nature as they are an 

177
00:13:02,568 --> 00:13:05,936
interface between you, the mathematician, discovering results, 

178
00:13:05,936 --> 00:13:10,480
and other people who might want to apply those results to new sorts of vector spaces.

179
00:13:11,420 --> 00:13:14,979
If, for example, someone defines some crazy type of vector space, 

180
00:13:14,979 --> 00:13:19,833
like the set of all pi creatures with some definition of adding and scaling pi creatures, 

181
00:13:19,833 --> 00:13:23,879
these axioms are like a checklist of things that they need to verify about 

182
00:13:23,879 --> 00:13:28,140
their definitions before they can start applying the results of linear algebra.

183
00:13:28,820 --> 00:13:31,580
And you, as the mathematician, never have to think about 

184
00:13:31,580 --> 00:13:34,340
all the possible crazy vector spaces people might define.

185
00:13:34,860 --> 00:13:38,356
You just have to prove your results in terms of these axioms so 

186
00:13:38,356 --> 00:13:42,617
anyone whose definitions satisfy those axioms can happily apply your results, 

187
00:13:42,617 --> 00:13:45,240
even if you never thought about their situation.

188
00:13:46,520 --> 00:13:50,846
As a consequence, you'd tend to phrase all of your results pretty abstractly, 

189
00:13:50,846 --> 00:13:53,509
which is to say, only in terms of these axioms, 

190
00:13:53,509 --> 00:13:58,280
rather than centering on a specific type of vector, like arrows in space or functions.

191
00:14:01,860 --> 00:14:05,604
For example, this is why just about every textbook you'll find will 

192
00:14:05,604 --> 00:14:09,239
define linear transformations in terms of additivity and scaling, 

193
00:14:09,239 --> 00:14:13,260
rather than talking about gridlines remaining parallel and evenly spaced.

194
00:14:13,260 --> 00:14:16,956
Even though the latter is more intuitive, and at least in my view, 

195
00:14:16,956 --> 00:14:21,260
more helpful for first-time learners, even if it is specific to one situation.

196
00:14:22,620 --> 00:14:26,920
So the mathematician's answer to what are vectors is to just ignore the question.

197
00:14:27,500 --> 00:14:31,260
In the modern theory, the form that vectors take doesn't really matter.

198
00:14:31,860 --> 00:14:36,341
Arrows, lists of numbers, functions, pi creatures, really, it can be anything, 

199
00:14:36,341 --> 00:14:41,220
so long as there's some notion of adding and scaling vectors that follows these rules.

200
00:14:41,860 --> 00:14:44,880
It's like asking what the number 3 really is.

201
00:14:45,380 --> 00:14:49,816
Whenever it comes up concretely, it's in the context of some triplet of things, 

202
00:14:49,816 --> 00:14:54,308
but in math, it's treated as an abstraction for all possible triplets of things, 

203
00:14:54,308 --> 00:14:58,080
and lets you reason about all possible triplets using a single idea.

204
00:14:59,120 --> 00:15:02,283
Same goes with vectors, which have many embodiments, 

205
00:15:02,283 --> 00:15:07,000
but math abstracts them all into a single, intangible notion of a vector space.

206
00:15:08,860 --> 00:15:12,354
But, as anyone watching this series knows, I think it's better 

207
00:15:12,354 --> 00:15:16,237
to begin reasoning about vectors in a concrete, visualizable setting, 

208
00:15:16,237 --> 00:15:18,900
like 2D space, with arrows rooted at the origin.

209
00:15:19,660 --> 00:15:24,435
But as you learn more linear algebra, know that these tools apply much more generally, 

210
00:15:24,435 --> 00:15:29,156
and that this is the underlying reason why textbooks and lectures tend to be phrased, 

211
00:15:29,156 --> 00:15:30,090
well, abstractly.

212
00:15:31,940 --> 00:15:36,140
So with that, folks, I think I'll call it an in to this essence of linear algebra series.

213
00:15:36,140 --> 00:15:39,837
If you've watched and understood the videos, I really do believe that 

214
00:15:39,837 --> 00:15:43,800
you have a solid foundation in the underlying intuitions of linear algebra.

215
00:15:44,640 --> 00:15:47,495
This is not the same thing as learning the full topic, of course, 

216
00:15:47,495 --> 00:15:50,697
that's something that can only really come from working through problems, 

217
00:15:50,697 --> 00:15:54,548
but the learning you do moving forward could be substantially more efficient if you have 

218
00:15:54,548 --> 00:15:56,020
all the right intuitions in place.

219
00:15:56,660 --> 00:16:00,000
So, have fun applying those intuitions, and best of luck with your future learning.

