1
00:00:16,880 --> 00:00:21,260
I'd like to revisit a deceptively simple question that I asked in the very first video

2
00:00:21,260 --> 00:00:22,000
of this series.

3
00:00:22,700 --> 00:00:23,560
What are vectors?

4
00:00:24,480 --> 00:00:28,760
Is a two-dimensional vector, for example, fundamentally an arrow on a flat plane that

5
00:00:28,760 --> 00:00:30,600
we can describe with coordinates for convenience?

6
00:00:30,860 --> 00:00:36,820
Or is it fundamentally that pair of real numbers which is just nicely visualized as an arrow

7
00:00:36,820 --> 00:00:37,720
on a flat plane?

8
00:00:38,480 --> 00:00:41,360
Or are both of these just manifestations of something deeper?

9
00:00:42,300 --> 00:00:47,820
On the one hand, defining vectors as primarily being a list of numbers feels clear-cut and

10
00:00:47,820 --> 00:00:48,480
unambiguous.

11
00:00:49,060 --> 00:00:53,280
It makes things like four-dimensional vectors or 100-dimensional vectors sound like real,

12
00:00:53,440 --> 00:00:55,720
concrete ideas that you can actually work with.

13
00:00:55,720 --> 00:01:01,240
When otherwise, an idea like four dimensions is just a vague geometric notion that's difficult

14
00:01:01,240 --> 00:01:03,660
to describe without waving your hands a bit.

15
00:01:05,540 --> 00:01:10,100
But on the other hand, a common sensation for those who actually work with linear algebra,

16
00:01:10,500 --> 00:01:14,540
especially as you get more fluent with changing your basis, is that you're dealing with a

17
00:01:14,540 --> 00:01:19,180
space that exists independently from the coordinates that you give it, and that coordinates are

18
00:01:19,180 --> 00:01:23,680
actually somewhat arbitrary, depending on what you happen to choose as your basis vectors.

19
00:01:24,480 --> 00:01:29,520
Core topics in linear algebra, like determinants and eigenvectors, seem indifferent to your

20
00:01:29,520 --> 00:01:30,640
choice of coordinate systems.

21
00:01:31,440 --> 00:01:36,900
The determinant tells you how much a transformation scales areas, and eigenvectors are the ones

22
00:01:36,900 --> 00:01:39,320
that stay on their own span during a transformation.

23
00:01:40,000 --> 00:01:44,600
But both of these properties are inherently spatial, and you can freely change your coordinate

24
00:01:44,600 --> 00:01:48,340
system without changing the underlying values of either one.

25
00:01:50,760 --> 00:01:55,800
But if vectors are not fundamentally lists of real numbers, and if their underlying essence

26
00:01:55,800 --> 00:02:00,300
is something more spatial, that just begs the question of what mathematicians mean when

27
00:02:00,300 --> 00:02:02,440
they use a word like space or spatial.

28
00:02:03,400 --> 00:02:07,000
To build up to where this is going, I'd actually like to spend the bulk of this video talking

29
00:02:07,000 --> 00:02:11,560
about something which is neither an arrow nor a list of numbers, but also has vector-ish

30
00:02:11,560 --> 00:02:13,100
qualities â€“ functions.

31
00:02:13,740 --> 00:02:17,880
You see, there's a sense in which functions are actually just another type of vector.

32
00:02:19,760 --> 00:02:23,460
In the same way that you can add two vectors together, there's also a sensible notion

33
00:02:23,460 --> 00:02:27,640
for adding two functions, f and g, to get a new function, f plus g.

34
00:02:28,200 --> 00:02:31,540
It's one of those things where you kind of already know what it's going to be, but

35
00:02:31,540 --> 00:02:33,140
actually phrasing it is a mouthful.

36
00:02:33,960 --> 00:02:43,000
The output of this new function at any given input, like negative four, is the sum of the

37
00:02:43,000 --> 00:02:44,520
same input, negative four.

38
00:02:45,420 --> 00:02:51,260
Or more generally, the value of the sum function at any given input x is the sum of the values

39
00:02:51,260 --> 00:02:53,740
f of x plus g of x.

40
00:03:00,700 --> 00:03:05,260
This is pretty similar to adding vectors coordinate by coordinate, it's just that there are,

41
00:03:05,400 --> 00:03:08,500
in a sense, infinitely many coordinates to deal with.

42
00:03:11,100 --> 00:03:15,740
Similarly, there's a sensible notion for scaling a function by a real number, just

43
00:03:15,740 --> 00:03:18,180
scale all of the outputs by that number.

44
00:03:20,240 --> 00:03:24,560
And again, this is analogous to scaling a vector coordinate by coordinate, it just feels

45
00:03:24,560 --> 00:03:26,220
like there's infinitely many coordinates.

46
00:03:28,900 --> 00:03:33,740
Now, given that the only thing vectors can really do is get added together or scaled,

47
00:03:34,180 --> 00:03:38,140
it feels like we should be able to take the same useful constructs and problem solving

48
00:03:38,140 --> 00:03:42,600
techniques of linear algebra that were originally thought about in the context of arrows and

49
00:03:42,600 --> 00:03:45,540
space and apply them to functions as well.

50
00:03:46,540 --> 00:03:52,000
For example, there's a perfectly reasonable notion of a linear transformation for functions,

51
00:03:52,440 --> 00:03:55,600
something that takes in one function and turns it into another.

52
00:03:59,820 --> 00:04:02,780
One familiar example comes from calculus, the derivative.

53
00:04:03,420 --> 00:04:07,140
It's something which transforms one function into another function.

54
00:04:08,720 --> 00:04:12,440
Sometimes in this context you'll hear these called operators instead of transformations,

55
00:04:12,680 --> 00:04:13,980
but the meaning is the same.

56
00:04:16,240 --> 00:04:20,560
A natural question you might want to ask is what it means for a transformation of functions

57
00:04:20,560 --> 00:04:21,540
to be linear.

58
00:04:22,440 --> 00:04:27,560
The formal definition of linearity is relatively abstract and symbolically driven compared

59
00:04:27,560 --> 00:04:30,440
to the way that I first talked about it in chapter 3 of this series.

60
00:04:30,440 --> 00:04:35,460
But the reward of abstractness is that we'll get something general enough to apply to functions

61
00:04:35,460 --> 00:04:36,840
as well as arrows.

62
00:04:39,180 --> 00:04:44,120
A transformation is linear if it satisfies two properties, commonly called additivity

63
00:04:44,120 --> 00:04:45,000
and scaling.

64
00:04:46,040 --> 00:04:51,360
Additivity means that if you add two vectors, v and w, then apply a transformation to their

65
00:04:51,360 --> 00:05:00,240
sum, you get the same result as if you added the transformed versions of v and w.

66
00:05:04,520 --> 00:05:10,520
The scaling property is that when you scale a vector v by some number, then apply the

67
00:05:10,520 --> 00:05:17,080
transformation, you get the same ultimate vector as if you scaled the transformed version

68
00:05:17,080 --> 00:05:18,920
of v by that same amount.

69
00:05:21,700 --> 00:05:26,580
The way you'll often hear this described is that linear transformations preserve the operations

70
00:05:26,580 --> 00:05:29,100
of vector addition and scalar multiplication.

71
00:05:32,200 --> 00:05:36,560
The idea of gridlines remaining parallel and evenly spaced that I've talked about in past

72
00:05:36,560 --> 00:05:41,960
videos is really just an illustration of what these two properties mean in the specific

73
00:05:41,960 --> 00:05:44,000
case of points in 2D space.

74
00:05:44,880 --> 00:05:48,920
One of the most important consequences of these properties, which makes matrix vector

75
00:05:48,920 --> 00:05:54,400
multiplication possible, is that a linear transformation is completely described by

76
00:05:54,400 --> 00:05:56,000
where it takes the basis vectors.

77
00:05:57,720 --> 00:06:02,160
Since any vector can be expressed by scaling and adding the basis vectors in some way,

78
00:06:02,480 --> 00:06:07,380
finding the transformed version of a vector comes down to scaling and adding the transformed

79
00:06:07,380 --> 00:06:10,440
versions of the basis vectors in that same way.

80
00:06:12,280 --> 00:06:16,780
As you'll see in just a moment, this is as true for functions as it is for arrows.

81
00:06:18,360 --> 00:06:22,900
For example, calculus students are always using the fact that the derivative is additive

82
00:06:22,900 --> 00:06:26,820
and has the scaling property, even if they haven't heard it phrased that way.

83
00:06:28,140 --> 00:06:33,560
If you add two functions, then take the derivative, it's the same as first taking the derivative

84
00:06:33,560 --> 00:06:36,580
of each one separately, then adding the result.

85
00:06:40,140 --> 00:06:44,700
Similarly, if you scale a function, then take the derivative, it's the same as first taking

86
00:06:44,700 --> 00:06:46,880
the derivative, then scaling the result.

87
00:06:50,280 --> 00:06:54,960
To really drill in the parallel, let's see what it might look like to describe the derivative

88
00:06:54,960 --> 00:06:56,120
with a matrix.

89
00:06:56,980 --> 00:07:00,960
This will be a little tricky, since function spaces have a tendency to be infinite dimensional,

90
00:07:01,400 --> 00:07:03,820
but I think this exercise is actually quite satisfying.

91
00:07:04,840 --> 00:07:10,480
Let's limit ourselves to polynomials, things like x squared plus 3x plus 5, or 4x to the

92
00:07:10,480 --> 00:07:11,860
seventh minus 5x squared.

93
00:07:12,330 --> 00:07:17,260
Each of the polynomials in our space will only have finitely many terms, but the full

94
00:07:17,260 --> 00:07:21,000
space is going to include polynomials with arbitrarily large degree.

95
00:07:22,220 --> 00:07:27,260
The first thing we need to do is give coordinates to this space, which requires choosing a basis.

96
00:07:28,180 --> 00:07:32,780
Since polynomials are already written down as the sum of scaled powers of the variable

97
00:07:32,780 --> 00:07:37,680
x, it's pretty natural to just choose pure powers of x as the basis function.

98
00:07:38,280 --> 00:07:43,700
In other words, our first basis function will be the constant function, b0 of x equals 1.

99
00:07:44,180 --> 00:07:50,540
The second basis function will be b1 of x equals x, then b2 of x equals x squared, then

100
00:07:50,540 --> 00:07:53,320
b3 of x equals x cubed, and so on.

101
00:07:53,860 --> 00:07:58,300
The role that these basis functions serve will be similar to the roles of i-hat, j-hat,

102
00:07:58,460 --> 00:08:00,980
and k-hat in the world of vectors as arrows.

103
00:08:02,120 --> 00:08:06,700
Since our polynomials can have arbitrarily large degree, this set of basis functions

104
00:08:06,700 --> 00:08:07,480
is infinite.

105
00:08:08,240 --> 00:08:12,380
But that's okay, it just means that when we treat our polynomials as vectors, they're

106
00:08:12,380 --> 00:08:14,120
going to have infinitely many coordinates.

107
00:08:15,600 --> 00:08:21,180
A polynomial like x squared plus 3x plus 5, for example, would be described with the coordinates

108
00:08:21,180 --> 00:08:25,500
5, 3, 1, then infinitely many zeros.

109
00:08:26,100 --> 00:08:31,400
You'd read this as saying that it's 5 times the first basis function, plus 3 times that

110
00:08:31,400 --> 00:08:37,140
second basis function, plus 1 times the third basis function, and then none of the other

111
00:08:37,140 --> 00:08:39,200
basis functions should be added from that point on.

112
00:08:40,620 --> 00:08:47,180
The polynomial 4x to the seventh minus 5x squared would have the coordinates 0, 0, negative

113
00:08:47,180 --> 00:08:52,340
5, 0, 0, 0, 0, 4, then an infinite string of zeros.

114
00:08:53,260 --> 00:08:59,100
In general, since every individual polynomial has only finitely many terms, its coordinates

115
00:08:59,100 --> 00:09:03,000
will be some finite string of numbers with an infinite tail of zeros.

116
00:09:06,900 --> 00:09:11,880
In this coordinate system, the derivative is described with an infinite matrix that's

117
00:09:11,880 --> 00:09:17,600
mostly full of zeros, but which has the positive integers counting down on this offset diagonal.

118
00:09:18,400 --> 00:09:21,840
I'll talk about how you could find this matrix in just a moment, but the best way to get

119
00:09:21,840 --> 00:09:24,360
a feel for it is to just watch it in action.

120
00:09:24,970 --> 00:09:32,220
Take the coordinates representing the polynomial x cubed plus 5x squared plus 4x plus 5, then

121
00:09:32,220 --> 00:09:34,940
put those coordinates on the right of the matrix.

122
00:09:40,410 --> 00:09:45,940
The only term that contributes to the first coordinate of the result is 1 times 4, which

123
00:09:45,940 --> 00:09:48,380
means the constant term in the result will be 4.

124
00:09:50,100 --> 00:09:54,380
This corresponds to the fact that the derivative of 4x is the constant 4.

125
00:09:55,640 --> 00:10:00,600
The only term contributing to the second coordinate of the matrix vector product is 2 times 5,

126
00:10:01,940 --> 00:10:05,740
which means the coefficient in front of x in the derivative is 10.

127
00:10:06,500 --> 00:10:09,280
That one corresponds to the derivative of 5x squared.

128
00:10:10,780 --> 00:10:16,080
Similarly, the third coordinate in the matrix vector product comes down to taking 3 times 1.

129
00:10:17,660 --> 00:10:21,740
This one corresponds to the derivative of x cubed being 3x squared.

130
00:10:23,080 --> 00:10:25,020
And after that, it'll be nothing but zeros.

131
00:10:26,880 --> 00:10:29,800
What makes this possible is that the derivative is linear.

132
00:10:31,640 --> 00:10:35,960
And for those of you who like to pause and ponder, you could construct this matrix by

133
00:10:35,960 --> 00:10:41,500
taking the derivative of each basis function and putting the coordinates of the results in each column.

134
00:10:59,780 --> 00:11:07,200
So, surprisingly, matrix vector multiplication and taking a derivative, which at first seem like completely different animals,

135
00:11:07,600 --> 00:11:09,840
are both just really members of the same family.

136
00:11:11,220 --> 00:11:16,780
In fact, most of the concepts I've talked about in this series with respect to vectors as arrows in space,

137
00:11:17,200 --> 00:11:22,340
things like the dot product or eigenvectors, have direct analogs in the world of functions,

138
00:11:22,800 --> 00:11:26,540
though sometimes they go by different names, things like inner product or eigenfunction.

139
00:11:28,400 --> 00:11:30,880
So back to the question of what is a vector.

140
00:11:31,560 --> 00:11:35,840
The point I want to make here is that there are lots of vectorish things in math.

141
00:11:35,840 --> 00:11:41,420
As long as you're dealing with a set of objects where there's a reasonable notion of scaling and adding,

142
00:11:41,800 --> 00:11:47,700
whether that's a set of arrows in space, lists of numbers, functions, or whatever other crazy thing you choose to define,

143
00:11:48,340 --> 00:11:53,760
all of the tools developed in linear algebra regarding vectors, linear transformations and all that stuff,

144
00:11:54,420 --> 00:11:55,620
should be able to apply.

145
00:11:57,480 --> 00:12:02,440
Take a moment to imagine yourself right now as a mathematician developing the theory of linear algebra.

146
00:12:02,440 --> 00:12:09,520
You want all of the definitions and discoveries of your work to apply to all of the vectorish things in full generality,

147
00:12:09,720 --> 00:12:11,300
not just to one specific case.

148
00:12:13,400 --> 00:12:19,720
These sets of vectorish things, like arrows or lists of numbers or functions, are called vector spaces.

149
00:12:20,580 --> 00:12:23,280
And what you as the mathematician might want to do is say,

150
00:12:23,740 --> 00:12:29,260
hey everyone, I don't want to have to think about all the different types of crazy vector spaces that you all might come up with.

151
00:12:29,260 --> 00:12:35,260
So what you do is establish a list of rules that vector addition and scaling have to abide by.

152
00:12:36,400 --> 00:12:40,160
These rules are called axioms, and in the modern theory of linear algebra,

153
00:12:40,560 --> 00:12:47,040
there are eight axioms that any vector space must satisfy if all of the theory and constructs that we've discovered are going to apply.

154
00:12:47,700 --> 00:12:50,500
I'll leave them on the screen here for anyone who wants to pause and ponder,

155
00:12:50,820 --> 00:12:58,140
but basically it's just a checklist to make sure that the notions of vector addition and scalar multiplication do the things that you'd expect them to do.

156
00:12:58,720 --> 00:13:05,980
These axioms are not so much fundamental rules of nature as they are an interface between you, the mathematician, discovering results,

157
00:13:06,380 --> 00:13:10,480
and other people who might want to apply those results to new sorts of vector spaces.

158
00:13:11,420 --> 00:13:19,520
If, for example, someone defines some crazy type of vector space, like the set of all pi creatures with some definition of adding and scaling pi creatures,

159
00:13:19,520 --> 00:13:28,140
these axioms are like a checklist of things that they need to verify about their definitions before they can start applying the results of linear algebra.

160
00:13:28,820 --> 00:13:34,340
And you, as the mathematician, never have to think about all the possible crazy vector spaces people might define.

161
00:13:34,860 --> 00:13:42,840
You just have to prove your results in terms of these axioms so anyone whose definitions satisfy those axioms can happily apply your results,

162
00:13:43,080 --> 00:13:45,240
even if you never thought about their situation.

163
00:13:46,520 --> 00:13:53,720
As a consequence, you'd tend to phrase all of your results pretty abstractly, which is to say, only in terms of these axioms,

164
00:13:54,180 --> 00:13:58,280
rather than centering on a specific type of vector, like arrows in space or functions.

165
00:14:01,860 --> 00:14:09,400
For example, this is why just about every textbook you'll find will define linear transformations in terms of additivity and scaling,

166
00:14:09,780 --> 00:14:13,260
rather than talking about gridlines remaining parallel and evenly spaced.

167
00:14:13,260 --> 00:14:21,260
Even though the latter is more intuitive, and at least in my view, more helpful for first-time learners, even if it is specific to one situation.

168
00:14:22,620 --> 00:14:26,920
So the mathematician's answer to what are vectors is to just ignore the question.

169
00:14:27,500 --> 00:14:31,260
In the modern theory, the form that vectors take doesn't really matter.

170
00:14:31,860 --> 00:14:35,760
Arrows, lists of numbers, functions, pi creatures, really, it can be anything,

171
00:14:36,160 --> 00:14:41,220
so long as there's some notion of adding and scaling vectors that follows these rules.

172
00:14:41,860 --> 00:14:44,880
It's like asking what the number 3 really is.

173
00:14:45,380 --> 00:14:48,900
Whenever it comes up concretely, it's in the context of some triplet of things,

174
00:14:48,960 --> 00:14:54,000
but in math, it's treated as an abstraction for all possible triplets of things,

175
00:14:54,460 --> 00:14:58,080
and lets you reason about all possible triplets using a single idea.

176
00:14:59,120 --> 00:15:01,580
Same goes with vectors, which have many embodiments,

177
00:15:02,060 --> 00:15:07,000
but math abstracts them all into a single, intangible notion of a vector space.

178
00:15:08,860 --> 00:15:11,260
But, as anyone watching this series knows,

179
00:15:11,600 --> 00:15:16,220
I think it's better to begin reasoning about vectors in a concrete, visualizable setting,

180
00:15:16,500 --> 00:15:18,900
like 2D space, with arrows rooted at the origin.

181
00:15:19,660 --> 00:15:23,880
But as you learn more linear algebra, know that these tools apply much more generally,

182
00:15:24,080 --> 00:15:30,090
and that this is the underlying reason why textbooks and lectures tend to be phrased, well, abstractly.

183
00:15:31,940 --> 00:15:36,140
So with that, folks, I think I'll call it an in to this essence of linear algebra series.

184
00:15:36,140 --> 00:15:38,520
If you've watched and understood the videos,

185
00:15:38,780 --> 00:15:43,800
I really do believe that you have a solid foundation in the underlying intuitions of linear algebra.

186
00:15:44,640 --> 00:15:47,000
This is not the same thing as learning the full topic, of course,

187
00:15:47,140 --> 00:15:49,860
that's something that can only really come from working through problems,

188
00:15:49,880 --> 00:15:53,740
but the learning you do moving forward could be substantially more efficient

189
00:15:53,740 --> 00:15:56,020
if you have all the right intuitions in place.

190
00:15:56,660 --> 00:16:00,320
So, have fun applying those intuitions, and best of luck with your future learning.

191
00:16:35,480 --> 00:16:35,540
Thank you.

192
00:16:36,140 --> 00:16:37,760
.

