1
00:00:00,000 --> 00:00:21,240
I'd like to revisit a deceptively simple question that I asked in the very first video

2
00:00:21,240 --> 00:00:22,800
of this series.

3
00:00:22,800 --> 00:00:24,600
What are vectors?

4
00:00:24,600 --> 00:00:28,720
Is a two-dimensional vector, for example, fundamentally an arrow on a flat plane that

5
00:00:28,720 --> 00:00:31,280
we can describe with coordinates for convenience?

6
00:00:31,280 --> 00:00:36,760
Or is it fundamentally that pair of real numbers which is just nicely visualized as an arrow

7
00:00:36,760 --> 00:00:38,680
on a flat plane?

8
00:00:38,680 --> 00:00:42,400
Or are both of these just manifestations of something deeper?

9
00:00:42,400 --> 00:00:47,800
On the one hand, defining vectors as primarily being a list of numbers feels clear-cut and

10
00:00:47,800 --> 00:00:48,800
unambiguous.

11
00:00:48,800 --> 00:00:53,480
It makes things like four-dimensional vectors or 100-dimensional vectors sound like real,

12
00:00:53,480 --> 00:00:58,120
concrete ideas that you can actually work with, when otherwise an idea like four dimensions

13
00:00:58,120 --> 00:01:05,720
is just a vague geometric notion that's difficult to describe without waving your hands a bit.

14
00:01:05,720 --> 00:01:10,440
But on the other hand, a common sensation for those who actually work with linear algebra,

15
00:01:10,440 --> 00:01:14,380
especially as you get more fluent with changing your basis, is that you're dealing with a

16
00:01:14,380 --> 00:01:19,080
space that exists independently from the coordinates that you give it, and that coordinates are

17
00:01:19,080 --> 00:01:24,720
actually somewhat arbitrary, depending on what you happen to choose as your basis vectors.

18
00:01:24,720 --> 00:01:29,400
More topics in linear algebra, like determinants and eigenvectors, seem indifferent to your

19
00:01:29,400 --> 00:01:31,400
choice of coordinate systems.

20
00:01:31,400 --> 00:01:36,860
The determinant tells you how much a transformation scales areas, and eigenvectors are the ones

21
00:01:36,860 --> 00:01:40,160
that stay on their own span during a transformation.

22
00:01:40,160 --> 00:01:44,560
But both of these properties are inherently spatial, and you can freely change your coordinate

23
00:01:44,560 --> 00:01:51,220
system without changing the underlying values of either one.

24
00:01:51,220 --> 00:01:55,880
But if vectors are not fundamentally lists of real numbers, and if their underlying essence

25
00:01:55,880 --> 00:02:00,240
is something more spatial, that just begs the question of what mathematicians mean when

26
00:02:00,240 --> 00:02:03,480
they use a word like space or spatial.

27
00:02:03,480 --> 00:02:07,000
To build up to where this is going, I'd actually like to spend the bulk of this video talking

28
00:02:07,000 --> 00:02:11,940
about something which is neither an arrow nor a list of numbers, but also has vector-ish

29
00:02:11,940 --> 00:02:14,140
qualities, functions.

30
00:02:14,140 --> 00:02:19,820
You see, there's a sense in which functions are actually just another type of vector.

31
00:02:19,820 --> 00:02:23,780
In the same way that you can add two vectors together, there's also a sensible notion for

32
00:02:23,780 --> 00:02:28,420
adding two functions, f and g, to get a new function, f plus g.

33
00:02:28,420 --> 00:02:31,900
It's one of those things where you kind of already know what it's going to be, but actually

34
00:02:31,900 --> 00:02:34,020
phrasing it is a mouthful.

35
00:02:34,020 --> 00:02:39,720
The output of this new function at any given input, like negative four, is the sum of the

36
00:02:39,720 --> 00:02:45,580
outputs of f and g when you evaluate them each at that same input, negative four.

37
00:02:45,580 --> 00:02:51,360
Or more generally, the value of the sum function at any given input x is the sum of the values

38
00:02:51,360 --> 00:02:53,360
f of x plus g of x.

39
00:03:01,180 --> 00:03:04,420
This is pretty similar to adding vectors coordinate by coordinate.

40
00:03:04,420 --> 00:03:08,340
It's just that there are, in a sense, infinitely many coordinates to deal with.

41
00:03:08,340 --> 00:03:15,740
Similarly, there's a sensible notion for scaling a function by a real number.

42
00:03:15,740 --> 00:03:20,340
Just scale all of the outputs by that number.

43
00:03:20,340 --> 00:03:24,140
And again, this is analogous to scaling a vector coordinate by coordinate.

44
00:03:24,140 --> 00:03:27,780
It just feels like there's infinitely many coordinates.

45
00:03:27,780 --> 00:03:34,320
Now, given that the only thing vectors can really do is get added together or scaled,

46
00:03:34,320 --> 00:03:38,140
it feels like we should be able to take the same useful constructs and problem-solving

47
00:03:38,140 --> 00:03:42,580
techniques of linear algebra that were originally thought about in the context of arrows and

48
00:03:42,580 --> 00:03:46,620
space and apply them to functions as well.

49
00:03:46,620 --> 00:03:52,580
For example, there's a perfectly reasonable notion of a linear transformation for functions,

50
00:03:52,580 --> 00:04:00,100
something that takes in one function and turns it into another.

51
00:04:00,100 --> 00:04:03,720
One familiar example comes from calculus, the derivative.

52
00:04:03,720 --> 00:04:08,800
It's something which transforms one function into another function.

53
00:04:08,800 --> 00:04:12,860
Sometimes in this context, you'll hear these called operators instead of transformations,

54
00:04:12,860 --> 00:04:16,360
but the meaning is the same.

55
00:04:16,360 --> 00:04:20,760
A natural question you might want to ask is what it means for a transformation of functions

56
00:04:20,760 --> 00:04:22,480
to be linear.

57
00:04:22,480 --> 00:04:27,600
The formal definition of linearity is relatively abstract and symbolically driven compared

58
00:04:27,600 --> 00:04:31,100
to the way that I first talked about it in chapter 3 of this series.

59
00:04:31,100 --> 00:04:35,660
But the reward of abstractness is that we'll get something general enough to apply to functions

60
00:04:35,660 --> 00:04:39,140
as well as arrows.

61
00:04:39,140 --> 00:04:44,340
A transformation is linear if it satisfies two properties, commonly called additivity

62
00:04:44,340 --> 00:04:46,460
and scaling.

63
00:04:46,460 --> 00:04:51,600
Additivity means that if you add two vectors, v and w, then apply a transformation to their

64
00:04:51,600 --> 00:05:00,100
sum, you get the same result as if you added the transformed versions of v and w.

65
00:05:00,100 --> 00:05:10,420
The scaling property is that when you scale a vector v by some number, then apply the

66
00:05:10,420 --> 00:05:17,100
transformation, you get the same ultimate vector as if you scaled the transformed version

67
00:05:17,100 --> 00:05:21,960
of v by that same amount.

68
00:05:21,960 --> 00:05:26,580
The way you'll often hear this described is that linear transformations preserve the operations

69
00:05:26,580 --> 00:05:32,480
of vector addition and scalar multiplication.

70
00:05:32,480 --> 00:05:36,580
The idea of gridlines remaining parallel and evenly spaced that I've talked about in past

71
00:05:36,580 --> 00:05:42,060
videos is really just an illustration of what these two properties mean in the specific

72
00:05:42,060 --> 00:05:45,280
case of points in 2D space.

73
00:05:45,280 --> 00:05:48,900
One of the most important consequences of these properties, which makes matrix vector

74
00:05:48,900 --> 00:05:54,320
multiplication possible, is that a linear transformation is completely described by

75
00:05:54,320 --> 00:05:57,920
where it takes the basis vectors.

76
00:05:57,920 --> 00:06:02,720
Since any vector can be expressed by scaling and adding the basis vectors in some way,

77
00:06:02,720 --> 00:06:07,400
finding the transformed version of a vector comes down to scaling and adding the transformed

78
00:06:07,400 --> 00:06:12,640
versions of the basis vectors in that same way.

79
00:06:12,640 --> 00:06:18,520
As you'll see in just a moment, this is as true for functions as it is for arrows.

80
00:06:18,520 --> 00:06:23,100
For example, calculus students are always using the fact that the derivative is additive

81
00:06:23,100 --> 00:06:28,300
and has the scaling property, even if they haven't heard it phrased that way.

82
00:06:28,300 --> 00:06:33,820
If you add two functions, then take the derivative, it's the same as first taking the derivative

83
00:06:33,820 --> 00:06:38,540
of each one separately, then adding the result.

84
00:06:38,540 --> 00:06:44,700
Similarly, if you scale a function, then take the derivative, it's the same as first taking

85
00:06:44,700 --> 00:06:50,780
the derivative, then scaling the result.

86
00:06:50,780 --> 00:06:55,380
To really drill in the parallel, let's see what it might look like to describe the derivative

87
00:06:55,380 --> 00:06:57,060
with a matrix.

88
00:06:57,060 --> 00:07:01,520
This will be a little tricky since function spaces have a tendency to be infinite dimensional,

89
00:07:01,520 --> 00:07:05,020
but I think this exercise is actually quite satisfying.

90
00:07:05,020 --> 00:07:10,380
Let's limit ourselves to polynomials, things like x squared plus 3x plus 5, or 4x to the

91
00:07:10,380 --> 00:07:12,620
seventh minus 5x squared.

92
00:07:12,620 --> 00:07:17,220
Each of the polynomials in our space will only have finitely many terms, but the full

93
00:07:17,220 --> 00:07:22,340
space is going to include polynomials with arbitrarily large degree.

94
00:07:22,340 --> 00:07:28,380
The first thing we need to do is give coordinates to this space, which requires choosing a basis.

95
00:07:28,380 --> 00:07:32,780
Since polynomials are already written down as the sum of scaled powers of the variable

96
00:07:32,780 --> 00:07:38,540
x, it's pretty natural to just choose pure powers of x as the basis function.

97
00:07:38,540 --> 00:07:44,460
In other words, our first basis function will be the constant function, b0 of x equals 1.

98
00:07:44,460 --> 00:07:50,540
The second basis function will be b1 of x equals x, then b2 of x equals x squared, then

99
00:07:50,540 --> 00:07:54,000
b3 of x equals x cubed, and so on.

100
00:07:54,000 --> 00:07:58,500
The role that these basis functions serve will be similar to the roles of i-hat, j-hat,

101
00:07:58,500 --> 00:08:02,420
and k-hat in the world of vectors as arrows.

102
00:08:02,420 --> 00:08:07,000
Since our polynomials can have arbitrarily large degree, this set of basis functions

103
00:08:07,000 --> 00:08:08,380
is infinite.

104
00:08:08,380 --> 00:08:12,320
But that's okay, it just means that when we treat our polynomials as vectors, they're

105
00:08:12,320 --> 00:08:15,560
going to have infinitely many coordinates.

106
00:08:15,560 --> 00:08:21,160
A polynomial like x squared plus 3x plus 5, for example, would be described with the coordinates

107
00:08:21,160 --> 00:08:26,200
5, 3, 1, then infinitely many zeros.

108
00:08:26,200 --> 00:08:31,360
You'd read this as saying that it's 5 times the first basis function, plus 3 times that

109
00:08:31,360 --> 00:08:37,080
second basis function, plus 1 times the third basis function, and then none of the other

110
00:08:37,080 --> 00:08:41,000
basis functions should be added from that point on.

111
00:08:41,000 --> 00:08:47,240
The polynomial 4x to the seventh minus 5x squared would have the coordinates 0, 0, negative

112
00:08:47,240 --> 00:08:53,440
5, 0, 0, 0, 0, 4, then an infinite string of zeros.

113
00:08:53,440 --> 00:08:59,180
In general, since every individual polynomial has only finitely many terms, its coordinates

114
00:08:59,180 --> 00:09:07,320
will be some finite string of numbers with an infinite tail of zeros.

115
00:09:07,320 --> 00:09:11,760
In this coordinate system, the derivative is described with an infinite matrix that's

116
00:09:11,760 --> 00:09:18,400
mostly full of zeros, but which has the positive integers counting down on this offset diagonal.

117
00:09:18,400 --> 00:09:21,840
I'll talk about how you could find this matrix in just a moment, but the best way to get

118
00:09:21,840 --> 00:09:25,280
a feel for it is to just watch it in action.

119
00:09:25,280 --> 00:09:32,160
Take the coordinates representing the polynomial x cubed plus 5x squared plus 4x plus 5, then

120
00:09:32,160 --> 00:09:34,920
put those coordinates on the right of the matrix.

121
00:09:37,320 --> 00:09:45,920
The only term that contributes to the first coordinate of the result is 1 times 4, which

122
00:09:45,920 --> 00:09:50,720
means the constant term in the result will be 4.

123
00:09:50,720 --> 00:09:55,720
This corresponds to the fact that the derivative of 4x is the constant 4.

124
00:09:55,720 --> 00:10:02,320
The only term contributing to the second coordinate of the matrix vector product is 2 times 5,

125
00:10:02,320 --> 00:10:06,640
which means the coefficient in front of x in the derivative is 10.

126
00:10:06,640 --> 00:10:10,440
That one corresponds to the derivative of 5x squared.

127
00:10:10,440 --> 00:10:15,960
Similarly, the third coordinate in the matrix vector product comes down to taking 3 times

128
00:10:15,960 --> 00:10:18,160
1.

129
00:10:18,160 --> 00:10:23,200
This one corresponds to the derivative of x cubed being 3x squared.

130
00:10:23,200 --> 00:10:27,040
And after that, it'll be nothing but zeros.

131
00:10:27,040 --> 00:10:32,000
What makes this possible is that the derivative is linear.

132
00:10:32,000 --> 00:10:35,920
And for those of you who like to pause and ponder, you could construct this matrix by

133
00:10:35,920 --> 00:10:40,600
taking the derivative of each basis function and putting the coordinates of the results

134
00:10:40,600 --> 00:11:00,320
in each column.

135
00:11:00,320 --> 00:11:05,560
So surprisingly, matrix vector multiplication and taking a derivative, which at first seemed

136
00:11:05,600 --> 00:11:11,720
like completely different animals, are both just really members of the same family.

137
00:11:11,720 --> 00:11:15,480
In fact, most of the concepts I've talked about in this series with respect to vectors

138
00:11:15,480 --> 00:11:21,320
as arrows in space, things like the dot product or eigenvectors, have direct analogs in the

139
00:11:21,320 --> 00:11:25,520
world of functions, though sometimes they go by different names, things like inner product

140
00:11:25,520 --> 00:11:28,520
or eigenfunction.

141
00:11:28,520 --> 00:11:31,680
So back to the question of what is a vector.

142
00:11:31,680 --> 00:11:36,580
The point I want to make here is that there are lots of vector-ish things in math.

143
00:11:36,580 --> 00:11:40,740
As long as you're dealing with a set of objects where there's a reasonable notion of scaling

144
00:11:40,740 --> 00:11:45,680
and adding, whether that's a set of arrows in space, lists of numbers, functions, or

145
00:11:45,680 --> 00:11:50,440
whatever other crazy thing you choose to define, all of the tools developed in linear algebra

146
00:11:50,440 --> 00:11:57,600
regarding vectors, linear transformations, and all that stuff, should be able to apply.

147
00:11:57,600 --> 00:12:01,680
Take a moment to imagine yourself right now as a mathematician developing the theory of

148
00:12:01,680 --> 00:12:03,320
linear algebra.

149
00:12:03,320 --> 00:12:07,920
You want all of the definitions and discoveries of your work to apply to all of the vector-ish

150
00:12:07,920 --> 00:12:13,560
things in full generality, not just to one specific case.

151
00:12:13,560 --> 00:12:18,800
These sets of vector-ish things, like arrows or lists of numbers or functions, are called

152
00:12:18,800 --> 00:12:20,680
vector spaces.

153
00:12:20,680 --> 00:12:24,880
And what you as the mathematician might want to do is say, hey everyone, I don't want

154
00:12:24,880 --> 00:12:28,480
to have to think about all the different types of crazy vector spaces that y'all might

155
00:12:28,480 --> 00:12:29,800
come up with.

156
00:12:29,800 --> 00:12:35,000
So what you do is establish a list of rules that vector addition and scaling have to abide

157
00:12:35,000 --> 00:12:36,560
by.

158
00:12:36,560 --> 00:12:40,760
These rules are called axioms, and in the modern theory of linear algebra, there are

159
00:12:40,760 --> 00:12:45,760
eight axioms that any vector space must satisfy if all of the theory and constructs that we've

160
00:12:45,760 --> 00:12:47,640
discovered are going to apply.

161
00:12:47,640 --> 00:12:51,480
I'll leave them on the screen here for anyone who wants to pause and ponder, but basically

162
00:12:51,480 --> 00:12:56,080
it's just a checklist to make sure that the notions of vector addition and scalar multiplication

163
00:12:56,080 --> 00:12:59,160
do the things that you'd expect them to do.

164
00:12:59,160 --> 00:13:04,000
These axioms are not so much fundamental rules of nature as they are an interface between

165
00:13:04,000 --> 00:13:08,240
you, the mathematician discovering results, and other people who might want to apply those

166
00:13:08,240 --> 00:13:10,920
results to new sorts of vector spaces.

167
00:13:10,920 --> 00:13:15,680
If, for example, someone defines some crazy type of vector space, like the set of all

168
00:13:15,680 --> 00:13:20,880
pi creatures with some definition of adding and scaling pi creatures, these axioms are

169
00:13:20,880 --> 00:13:25,700
like a checklist of things that they need to verify about their definitions before they

170
00:13:25,700 --> 00:13:28,920
can start applying the results of linear algebra.

171
00:13:28,920 --> 00:13:33,020
And you, as the mathematician, never have to think about all the possible crazy vector

172
00:13:33,020 --> 00:13:35,060
spaces people might define.

173
00:13:35,060 --> 00:13:39,880
You just have to prove your results in terms of these axioms so anyone whose definitions

174
00:13:39,880 --> 00:13:44,720
satisfy those axioms can happily apply your results, even if you never thought about their

175
00:13:44,720 --> 00:13:47,080
situation.

176
00:13:47,080 --> 00:13:51,160
As a consequence, you'd tend to phrase all of your results pretty abstractly, which is

177
00:13:51,160 --> 00:13:56,640
to say, only in terms of these axioms, rather than centering on a specific type of vector,

178
00:13:56,640 --> 00:14:02,080
like arrows in space or functions.

179
00:14:02,080 --> 00:14:07,360
For example, this is why just about every textbook you'll find will define linear transformations

180
00:14:07,360 --> 00:14:12,240
in terms of additivity and scaling, rather than talking about gridlines remaining parallel

181
00:14:12,240 --> 00:14:14,080
and evenly spaced.

182
00:14:14,080 --> 00:14:18,020
Even though the latter is more intuitive, and at least in my view, more helpful for

183
00:14:18,020 --> 00:14:22,780
first time learners, even if it is specific to one situation.

184
00:14:22,780 --> 00:14:27,600
So the mathematician's answer to what are vectors is to just ignore the question.

185
00:14:27,600 --> 00:14:31,560
In the modern theory, the form that vectors take doesn't really matter.

186
00:14:31,560 --> 00:14:37,100
Arrows, lists of numbers, functions, pi creatures, really, it can be anything, so long as there's

187
00:14:37,100 --> 00:14:42,380
some notion of adding and scaling vectors that follows these rules.

188
00:14:42,380 --> 00:14:45,480
It's like asking what the number three really is.

189
00:14:45,480 --> 00:14:49,700
Whenever it comes up concretely, it's in the context of some triplet of things, but in

190
00:14:49,700 --> 00:14:54,840
math, it's treated as an abstraction for all possible triplets of things, and lets you

191
00:14:54,840 --> 00:14:59,280
reason about all possible triplets using a single idea.

192
00:14:59,280 --> 00:15:04,860
Same goes with vectors, which have many embodiments, but math abstracts them all into a single,

193
00:15:04,860 --> 00:15:09,420
intangible notion of a vector space.

194
00:15:09,460 --> 00:15:13,460
But as anyone watching this series knows, I think it's better to begin reasoning about

195
00:15:13,460 --> 00:15:19,840
vectors in a concrete, visualizable setting, like 2D space with arrows rooted at the origin.

196
00:15:19,840 --> 00:15:24,480
But as you learn more linear algebra, know that these tools apply much more generally,

197
00:15:24,480 --> 00:15:29,000
and that this is the underlying reason why textbooks and lectures tend to be phrased,

198
00:15:29,000 --> 00:15:32,280
well, abstractly.

199
00:15:32,280 --> 00:15:36,920
So with that, folks, I think I'll call it an in to this Essence of Linear Algebra series.

200
00:15:36,920 --> 00:15:40,700
If you've watched and understood the videos, I really do believe that you have a solid

201
00:15:40,700 --> 00:15:44,740
foundation in the underlying intuitions of linear algebra.

202
00:15:44,740 --> 00:15:47,760
This is not the same thing as learning the full topic, of course, that's something that

203
00:15:47,760 --> 00:15:52,140
can only really come from working through problems, but the learning you do moving forward

204
00:15:52,140 --> 00:15:56,880
could be substantially more efficient if you have all the right intuitions in place.

205
00:15:56,880 --> 00:16:00,260
So have fun applying those intuitions, and best of luck with your future learning.

