[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Собственные векторы и собственные значения — одна из тех тем, которые многие студенты находят особенно неинтуитивными.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Такие вопросы, как «почему мы это делаем и что это на самом деле означает», слишком часто просто уплывают в море вычислений без ответа.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "И когда я выпустил видео из этой серии, многие из вас высказались о том, что с нетерпением ждут возможности визуализировать именно эту тему.",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Я подозреваю, что причина этого не столько в том, что собственные вещи особенно сложны или плохо объяснены.",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "На самом деле, это сравнительно просто, и я думаю, что большинство книг прекрасно объясняют это.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Проблема в том, что это действительно имеет смысл только в том случае, если у вас есть четкое визуальное понимание многих тем, которые ему предшествуют.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Самое важное здесь то, что вы умеете воспринимать матрицы как линейные преобразования, но вам также необходимо хорошо разбираться в таких вещах, как определители, линейные системы уравнений и смена базиса.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Путаница в отношении собственных веществ обычно больше связана с шатким фундаментом одной из этих тем, чем с самими собственными векторами и собственными значениями.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Для начала рассмотрим некоторое линейное преобразование в двух измерениях, подобное тому, которое показано здесь.",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Он перемещает базисный вектор i-hat в координаты 3, 0 и j-hat в координаты 1, 2.",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Таким образом, он представлен матрицей, столбцы которой равны 3, 0 и 1, 2.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Сосредоточьтесь на том, что он делает с одним конкретным вектором, и подумайте о длине этого вектора, о линии, проходящей через его начало и кончик.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "Большинство векторов будут выбиты из своего диапазона во время преобразования.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Я имею в виду, что было бы довольно случайно, если бы место, где приземлился вектор, также оказалось где-то на этой линии.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Но некоторые специальные векторы остаются в своем собственном диапазоне, а это означает, что эффект, который матрица оказывает на такой вектор, заключается в его простом растягивании или сжатии, как скаляр.",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "В этом конкретном примере базисный вектор i-hat является одним из таких специальных векторов.",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "Диапазон i-hat — это ось X, и из первого столбца матрицы мы видим, что i-hat перемещается в 3 раза, все еще на этой оси X.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Более того, из-за того, как работают линейные преобразования, любой другой вектор на оси X также просто растягивается в 3 раза и, следовательно, остается на своем собственном интервале.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Немного более хитрый вектор, который во время этого преобразования остается в своем диапазоне, имеет отрицательное значение 1, 1.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "В итоге он растягивается в 2 раза.",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "И опять же, линейность будет означать, что любой другой вектор на диагональной линии, охватываемой этим парнем, просто растянется в 2 раза.",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "И для этого преобразования это все векторы с особым свойством оставаться в пределах своего диапазона.",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Те, что по оси X, растягиваются в 3 раза, а те, что на этой диагональной линии, растягиваются в 2 раза.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Любой другой вектор будет несколько повернут во время преобразования, сбитый с линии, которую он охватывает.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Как вы уже могли догадаться, эти специальные векторы называются собственными векторами преобразования, и с каждым собственным вектором связано так называемое собственное значение, которое является фактором, на который он растягивается или сжимается во время преобразования.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Конечно, нет ничего особенного в растяжении и сжатии или в том факте, что эти собственные значения оказываются положительными.",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "В другом примере у вас может быть собственный вектор с отрицательным собственным значением в 1 половину, что означает, что вектор переворачивается и сжимается в 1 половину.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Но важной частью здесь является то, что он остается на линии, по которой проходит, не сворачивая с нее.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Чтобы понять, почему об этом может быть полезно подумать, рассмотрим некоторое трехмерное вращение.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Если вы можете найти собственный вектор для этого вращения, вектор, который остается на своем собственном участке, то вы нашли ось вращения.",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "И гораздо проще думать о трехмерном вращении с точки зрения некоторой оси вращения и угла, на который оно вращается, а не думать о полной матрице 3x3, связанной с этим преобразованием.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "В этом случае, кстати, соответствующее собственное значение должно быть равно 1, поскольку вращение никогда ничего не растягивает и не сжимает, поэтому длина вектора останется прежней.",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Эта закономерность часто встречается в линейной алгебре.",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "При любом линейном преобразовании, описываемом матрицей, вы можете понять, что оно делает, прочитав столбцы этой матрицы как точки приземления базисных векторов.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Но зачастую лучший способ понять суть того, что на самом деле делает линейное преобразование, менее зависимый от вашей конкретной системы координат, — это найти собственные векторы и собственные значения.",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Я не буду здесь подробно описывать методы вычисления собственных векторов и собственных значений, но попытаюсь дать обзор вычислительных идей, которые наиболее важны для концептуального понимания.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Вот как символически выглядит идея собственного вектора.",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A — это матрица, представляющая некоторое преобразование, с v в качестве собственного вектора, а лямбда — это число, а именно соответствующее собственное значение.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Это выражение говорит о том, что произведение матрицы на вектор A, умноженное на v, дает тот же результат, что и простое масштабирование собственного вектора v на некоторое значение лямбда.",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Таким образом, поиск собственных векторов и их собственных значений матрицы A сводится к поиску значений v и лямбда, которые делают это выражение истинным.",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "Поначалу с ним немного неудобно работать, потому что левая часть представляет собой умножение матрицы на вектор, а правая часть здесь — умножение на скалярный вектор.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Итак, давайте начнем с того, что перепишем эту правую часть как своего рода умножение матрицы на вектор, используя матрицу, которая масштабирует любой вектор с коэффициентом лямбда.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Столбцы такой матрицы будут представлять то, что происходит с каждым базисным вектором, и каждый базисный вектор просто умножается на лямбда, поэтому эта матрица будет иметь число лямбда по диагонали, с нулями повсюду.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "Обычный способ написать этого парня — вынести эту лямбду на множитель и записать ее как лямбда, умноженную на i, где i — единичная матрица с единицами по диагонали.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Поскольку обе части выглядят как умножение матрицы на вектор, мы можем вычесть эту правую часть и исключить v.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Итак, теперь у нас есть новая матрица: A минус лямбда, умноженная на единицу, и мы ищем вектор v такой, чтобы эта новая матрица, умноженная на v, давала нулевой вектор.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Это всегда будет верно, если v само по себе является нулевым вектором, но это скучно.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Нам нужен ненулевой собственный вектор.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "И если вы посмотрите главы 5 и 6, вы поймете, что единственный способ, которым произведение матрицы с ненулевым вектором может стать нулевым, — это если преобразование, связанное с этой матрицей, сжимает пространство в более низкое измерение.",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "И это сжатие соответствует нулевому определителю матрицы.",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Чтобы быть конкретнее, предположим, что ваша матрица A имеет столбцы 2, 1 и 2, 3, и подумайте о вычитании переменной величины, лямбды, из каждой диагональной записи.",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Теперь представьте, что вы настраиваете лямбду, поворачивая ручку, чтобы изменить ее значение.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "По мере изменения этого значения лямбды меняется сама матрица, и, следовательно, меняется определитель матрицы.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Цель здесь — найти значение лямбды, при котором этот определитель будет равен нулю, а это означает, что измененное преобразование сжимает пространство в более низкое измерение.",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "В этом случае золотая середина наступает, когда лямбда равна 1.",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Конечно, если бы мы выбрали какую-то другую матрицу, собственное значение не обязательно было бы равно 1.",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Золотая середина может быть достигнута при каком-то другом значении лямбды.",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Так что это довольно много, но давайте разберемся, о чем идет речь.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Когда лямбда равна 1, матрица A минус лямбда, умноженная на единицу, сжимает пространство в строке.",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Это означает, что существует ненулевой вектор v такой, что A минус лямбда, умноженное на единицу, умноженное на v, равно нулевому вектору.",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "И помните, причина, по которой нас это волнует, заключается в том, что это означает, что A, умноженное на v, равно лямбда, умноженному на v, что вы можете прочитать как утверждение, что вектор v является собственным вектором A, оставаясь на своем собственном интервале во время преобразования A.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "В этом примере соответствующее собственное значение равно 1, поэтому v фактически просто останется на месте.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Сделайте паузу и подумайте, нужно ли вам убедиться, что эта линия рассуждений вам нравится.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Это то, о чем я упоминал во введении.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Если бы вы не имели четкого представления об определителях и о том, почему они связаны с линейными системами уравнений, имеющими ненулевые решения, такое выражение выглядело бы совершенно неожиданным.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Чтобы увидеть это в действии, давайте вернемся к примеру с самого начала, с матрицей, столбцы которой равны 3, 0 и 1, 2.",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Чтобы определить, является ли значение лямбда собственным значением, вычтите его из диагоналей этой матрицы и вычислите определитель.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Сделав это, мы получим некий квадратичный многочлен по лямбде, 3 минус лямбда, умноженный на 2 минус лямбда.",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Поскольку лямбда может быть собственным значением только в том случае, если этот определитель равен нулю, вы можете заключить, что единственными возможными собственными значениями являются лямбда, равная 2, и лямбда, равная 3.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Чтобы выяснить, какие собственные векторы на самом деле имеют одно из этих собственных значений, скажем, лямбда равна 2, подставьте это значение лямбда в матрицу, а затем решите, для каких векторов эта диагонально измененная матрица обнуляет.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Если бы вы вычислили это так же, как и любую другую линейную систему, вы бы увидели, что решениями являются все векторы на диагональной линии, натянутые на отрицательные 1, 1.",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Это соответствует тому факту, что неизмененная матрица 3, 0, 1, 2 приводит к растягиванию всех этих векторов в 2 раза.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Теперь двумерное преобразование не обязательно должно иметь собственные векторы.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Например, рассмотрим поворот на 90 градусов.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "У него нет собственных векторов, поскольку он вращает каждый вектор за пределы своего диапазона.",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Если вы действительно попытаетесь вычислить собственные значения вращения таким образом, обратите внимание, что произойдет.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Его матрица имеет столбцы 0, 1 и отрицательные 1, 0.",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Вычтите лямбду из диагональных элементов и найдите, когда определитель равен нулю.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "В этом случае вы получаете квадрат полинома лямбда плюс 1.",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Единственными корнями этого многочлена являются мнимые числа i и отрицательное i.",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Тот факт, что нет решений в действительных числах, указывает на отсутствие собственных векторов.",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Еще один довольно интересный пример, который стоит запомнить, — это ножницы.",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Это фиксирует i-шляпу на месте и перемещает j-шляпу 1, так что ее матрица имеет столбцы 1, 0 и 1, 1.",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Все векторы на оси x являются собственными векторами с собственным значением 1, поскольку они остаются фиксированными на месте.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "Фактически это единственные собственные векторы.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Когда вы вычитаете лямбду из диагоналей и вычисляете определитель, вы получаете 1 минус лямбда в квадрате.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "И единственный корень этого выражения — лямбда, равная 1.",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Это согласуется с тем, что мы видим геометрически: все собственные векторы имеют собственное значение 1.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Однако имейте в виду, что также возможно иметь только одно собственное значение, но с несколькими линиями, заполненными собственными векторами.",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Простой пример — матрица, которая масштабирует все на 2.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "Единственное собственное значение — 2, но каждый вектор на плоскости становится собственным вектором с этим собственным значением.",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Сейчас еще один хороший момент, чтобы сделать паузу и поразмыслить над этим, прежде чем я перейду к последней теме.",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Я хочу закончить здесь идеей собственного базиса, которая во многом опирается на идеи из последнего видео.",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Посмотрите, что произойдет, если наши базисные векторы окажутся собственными векторами.",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Например, возможно, i-hat масштабируется на минус 1, а j-hat масштабируется на 2.",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Записывая их новые координаты в виде столбцов матрицы, обратите внимание, что эти скалярные кратные, отрицательные 1 и 2, которые являются собственными значениями i-hat и j-hat, располагаются на диагонали нашей матрицы, а каждая вторая запись равна 0. .",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Каждый раз, когда матрица имеет нули везде, кроме диагонали, ее вполне обоснованно называют диагональной матрицей.",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "И это можно интерпретировать так: все базисные векторы являются собственными векторами, а диагональные элементы этой матрицы являются их собственными значениями.",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Есть много вещей, которые делают работу с диагональными матрицами намного приятнее.",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Одна из самых важных проблем заключается в том, что легче вычислить, что произойдет, если вы умножите эту матрицу саму на себя несколько раз.",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Поскольку все, что делает одна из этих матриц, — это масштабирует каждый базисный вектор на некоторое собственное значение, применение этой матрицы много раз, скажем, 100 раз, будет просто соответствовать масштабированию каждого базисного вектора в 100-й степени соответствующего собственного значения.",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Напротив, попробуйте вычислить 100-ю степень недиагональной матрицы.",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Действительно, попробуйте на минутку.",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Это кошмар.",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Конечно, вам редко повезет, чтобы ваши базисные векторы были также собственными векторами.",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Но если у вашего преобразования много собственных векторов, как в начале этого видео, достаточно, чтобы вы могли выбрать набор, охватывающий все пространство, тогда вы можете изменить свою систему координат так, чтобы эти собственные векторы были вашими базисными векторами.",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Я говорил об изменении основы в прошлом видео, но здесь я очень быстро напомню, как выразить преобразование, записанное в настоящее время в нашей системе координат, в другую систему.",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Возьмите координаты векторов, которые вы хотите использовать в качестве новой основы, что в данном случае означает два наших собственных вектора, затем сделайте эти координаты столбцами матрицы, что называется матрицей изменения базиса.",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Когда вы объединяете исходное преобразование, помещая изменение базовой матрицы справа и обратную матрицу изменения базиса слева, результатом будет матрица, представляющая то же самое преобразование, но с точки зрения координаты новых базисных векторов. система.",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Весь смысл этого с собственными векторами заключается в том, что эта новая матрица гарантированно будет диагональной с соответствующими собственными значениями по этой диагонали.",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Это связано с тем, что это представляет собой работу в системе координат, где с базисными векторами происходит масштабирование во время преобразования.",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Набор базисных векторов, которые также являются собственными векторами, опять-таки достаточно разумно называется собственным базисом.",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Так что, если, например, вам нужно вычислить 100-ю степень этой матрицы, было бы гораздо проще перейти к собственному базису, вычислить 100-ю степень в этой системе, а затем преобразовать обратно в нашу стандартную систему.",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Вы не можете сделать это со всеми преобразованиями.",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Например, сдвиг не имеет достаточного количества собственных векторов, чтобы охватить все пространство.",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Но если вы сможете найти собственный базис, это сделает матричные операции по-настоящему прекрасными.",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Для тех из вас, кто хочет разгадать довольно изящную головоломку, чтобы увидеть, как она выглядит в действии и как ее можно использовать для получения неожиданных результатов, я оставлю подсказку здесь, на экране.",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Это потребует некоторой работы, но я думаю, вам понравится.",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Следующее и последнее видео из этой серии будет посвящено абстрактным векторным пространствам.",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]