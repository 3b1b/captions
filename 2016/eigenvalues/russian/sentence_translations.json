[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Собственные векторы и собственные значения — одна из тех тем, которые многие студенты находят особенно неинтуитивными.",
  "from_community_srt": "Собственные векторы и собственные значения - это темы, которые многие учащиеся считают немного неинтуитивными.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Такие вопросы, как «почему мы это делаем и что это на самом деле означает», слишком часто просто уплывают в море вычислений без ответа.",
  "from_community_srt": "Вопросы вида \"зачем мы занимается этим\" и \"что они означают на самом деле\" возникают слишком часто и повисают в воздухе без ответа.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "И когда я выпустил видео из этой серии, многие из вас высказались о том, что с нетерпением ждут возможности визуализировать именно эту тему.",
  "from_community_srt": "Чем дальше я публиковал видео в этой серии тем больше комментариев содержали просьбу визуализировать в том числе эту тему",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Я подозреваю, что причина этого не столько в том, что собственные вещи особенно сложны или плохо объяснены.",
  "from_community_srt": "Подозреваю, что причиной тому является не то, что \"собственные\" штуки в чем-то сложны или плохо объясняются - на деле они относительно просты -",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "На самом деле, это сравнительно просто, и я думаю, что большинство книг прекрасно объясняют это.",
  "from_community_srt": "и, я считаю,",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Проблема в том, что это действительно имеет смысл только в том случае, если у вас есть четкое визуальное понимание многих тем, которые ему предшествуют.",
  "from_community_srt": "в большинстве книг их хорошо описывают Проблема в том, что они имеют смысл только если вы можете визуализировать их работу для последующих тем.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Самое важное здесь то, что вы умеете воспринимать матрицы как линейные преобразования, но вам также необходимо хорошо разбираться в таких вещах, как определители, линейные системы уравнений и смена базиса.",
  "from_community_srt": "Самое главное, вы знаете как рассматривать матрицы как линейные преобразования, но не менее важно знать такие элементы как: определитель,",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Путаница в отношении собственных веществ обычно больше связана с шатким фундаментом одной из этих тем, чем с самими собственными векторами и собственными значениями.",
  "from_community_srt": "система линейных алгебраических уравнений и смену базиса Путаница с \"собственными\" штуками обычно связана с непрочными знаниями в перечисленных темах чем непосредственно с собственными векторами и значениями.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Для начала рассмотрим некоторое линейное преобразование в двух измерениях, подобное тому, которое показано здесь.",
  "from_community_srt": "Для начала, пусть дано линейное преобразование в двумерном прострастве, как представленное здесь.",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Он перемещает базисный вектор i-hat в координаты 3, 0 и j-hat в координаты 1, 2.",
  "from_community_srt": "Оно перемещает базисный вектор i на координаты (3, 0), а вектор j - на (1,",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Таким образом, он представлен матрицей, столбцы которой равны 3, 0 и 1, 2.",
  "from_community_srt": "2), его можно представить в виде матрицы с колонками (3, 0) и (1,",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Сосредоточьтесь на том, что он делает с одним конкретным вектором, и подумайте о длине этого вектора, о линии, проходящей через его начало и кончик.",
  "from_community_srt": "2) Посмотрите, что преобразование сделает с выбранным вектором, и подумайте о прямой, проходящей через этот вектор.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "Большинство векторов будут выбиты из своего диапазона во время преобразования.",
  "from_community_srt": "Большинство векторов будут сдвинуты со своих осей во время преобразования, в смысле,",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Я имею в виду, что было бы довольно случайно, если бы место, где приземлился вектор, также оказалось где-то на этой линии.",
  "from_community_srt": "казалось очевидным, что конечная точка вектора будет лежать также на той же оси...",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Но некоторые специальные векторы остаются в своем собственном диапазоне, а это означает, что эффект, который матрица оказывает на такой вектор, заключается в его простом растягивании или сжатии, как скаляр.",
  "from_community_srt": "Однако некоторые особые векторы действительно остаются на своей оси, и все действие матрицы сводится к его растяжению или сжатию,",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "В этом конкретном примере базисный вектор i-hat является одним из таких специальных векторов.",
  "from_community_srt": "как будто это скаляр. Для представленного примера вектор базиса i является одним из таких особых векторов.",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "Диапазон i-hat — это ось X, и из первого столбца матрицы мы видим, что i-hat перемещается в 3 раза, все еще на этой оси X.",
  "from_community_srt": "Ось вектора i - это ось X, и исходя из первой колонки матрицы, мы можем увидеть, что вектор i движется на 3 длины самого себя,",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Более того, из-за того, как работают линейные преобразования, любой другой вектор на оси X также просто растягивается в 3 раза и, следовательно, остается на своем собственном интервале.",
  "from_community_srt": "но остается на оси X Вдобавок, из-за принципа работы линейного преобразования, любой другой вектор на оси X также будет растянут втрое и, следовательно, останется на своей оси.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Немного более хитрый вектор, который во время этого преобразования остается в своем диапазоне, имеет отрицательное значение 1, 1.",
  "from_community_srt": "Чуть более хитрый вектор, который остается на своей оси - это (-1,",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "В итоге он растягивается в 2 раза.",
  "from_community_srt": "1) - в конце-концов он будет растянут вдвое.",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "И опять же, линейность будет означать, что любой другой вектор на диагональной линии, охватываемой этим парнем, просто растянется в 2 раза.",
  "from_community_srt": "И вновь линейность подразумевает, что любой иной вектор на этой диагонали, проходящей через этого парня, будет просто растянут вдвое своей длины.",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "И для этого преобразования это все векторы с особым свойством оставаться в пределах своего диапазона.",
  "from_community_srt": "Для этого преобразования это все векторы с особым свойством - оставаться на своих осях.",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Те, что по оси X, растягиваются в 3 раза, а те, что на этой диагональной линии, растягиваются в 2 раза.",
  "from_community_srt": "Те, что на оси X будут растянуты втрое, те, что на диагонали, будут растянуты вдвое.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Любой другой вектор будет несколько повернут во время преобразования, сбитый с линии, которую он охватывает.",
  "from_community_srt": "Любой иной вектор будет повернут в какую-нибудь сторону во время преобразования, смещаясь со своей оси.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Как вы уже могли догадаться, эти специальные векторы называются собственными векторами преобразования, и с каждым собственным вектором связано так называемое собственное значение, которое является фактором, на который он растягивается или сжимается во время преобразования.",
  "from_community_srt": "Как вы могли догадаться к этому моменту, эти особые векторы называются \"собственными векторами\" преобразования, и каждый собственный вектор имеет связанное с ним число, которое называется \"собственным значением\", которое просто означает множитель, на который умножается вектор во время преобразования.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Конечно, нет ничего особенного в растяжении и сжатии или в том факте, что эти собственные значения оказываются положительными.",
  "from_community_srt": "Конечно, нет ничего особенного между растяжением или сжатием, или тем перед тем фактом, что собственные значения - положительные числа.",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "В другом примере у вас может быть собственный вектор с отрицательным собственным значением в 1 половину, что означает, что вектор переворачивается и сжимается в 1 половину.",
  "from_community_srt": "В другом примере, у вас может быть собственный вектор со значением -1/2, что означает, что вектор будет перевернут и сжат вдвое.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Но важной частью здесь является то, что он остается на линии, по которой проходит, не сворачивая с нее.",
  "from_community_srt": "Главное - он остается на той же линии, и не сходит с неё.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Чтобы понять, почему об этом может быть полезно подумать, рассмотрим некоторое трехмерное вращение.",
  "from_community_srt": "Почему это может быть полезным, представьте вращение трехмерного объекта.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Если вы можете найти собственный вектор для этого вращения, вектор, который остается на своем собственном участке, то вы нашли ось вращения.",
  "from_community_srt": "Если вы найдете собственный вектор этого вращения - вектор, который остается на своей прямой - то, значит, вы нашли ось вращения объекта.",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "И гораздо проще думать о трехмерном вращении с точки зрения некоторой оси вращения и угла, на который оно вращается, а не думать о полной матрице 3x3, связанной с этим преобразованием.",
  "from_community_srt": "И куда проще рассматривать трехмерное вращение, зная угол под которым оно производится, чем выяснять это по полной 3x3 матрице вращения, связанной с этим преобразованием.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "В этом случае, кстати, соответствующее собственное значение должно быть равно 1, поскольку вращение никогда ничего не растягивает и не сжимает, поэтому длина вектора останется прежней.",
  "from_community_srt": "Кстати, в данном случае, соответствующее собственное значение должно быть равно 1, раз вращение ничего не растягивает и не сжимает, а длина вектора остается одинаковой.",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Эта закономерность часто встречается в линейной алгебре.",
  "from_community_srt": "Это поведение проясняет многое в линейной алгебре.",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "При любом линейном преобразовании, описываемом матрицей, вы можете понять, что оно делает, прочитав столбцы этой матрицы как точки приземления базисных векторов.",
  "from_community_srt": "Для любого линейного преобразования, представленного в виде матрицы, вы можете понять, что оно делает проанализировав колонки этой матрицы как точки приземления для базисных векторов.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Но зачастую лучший способ понять суть того, что на самом деле делает линейное преобразование, менее зависимый от вашей конкретной системы координат, — это найти собственные векторы и собственные значения.",
  "from_community_srt": "Однако часто лучше добраться до центра того, что делает линейное преобразование на самом деле, и стать менее зависимым от выбранной системы координат, вычислив собственные векторы и собственные значения.",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Я не буду здесь подробно описывать методы вычисления собственных векторов и собственных значений, но попытаюсь дать обзор вычислительных идей, которые наиболее важны для концептуального понимания.",
  "from_community_srt": "Я не буду детально описывать способы вычисления собственных векторов и значений здесь, на постараюсь дать обзор идей для них, что очень важно для понимания самой концепции.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Вот как символически выглядит идея собственного вектора.",
  "from_community_srt": "Здесь дана символическая запись того, что из себя представляет собственный вектор.",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A — это матрица, представляющая некоторое преобразование, с v в качестве собственного вектора, а лямбда — это число, а именно соответствующее собственное значение.",
  "from_community_srt": "А - это матрица, обозначающая некоторое преобразование, v - это собственный вектор. и λ - это число, точнее, собственное значение соответствующего вектора.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Это выражение говорит о том, что произведение матрицы на вектор A, умноженное на v, дает тот же результат, что и простое масштабирование собственного вектора v на некоторое значение лямбда.",
  "from_community_srt": "Это выражение говорит нам, что произведение матрицы на вектор дает нам тот же результат, как если бы мы умножили собственный вектор на некоторое число λ.",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Таким образом, поиск собственных векторов и их собственных значений матрицы A сводится к поиску значений v и лямбда, которые делают это выражение истинным.",
  "from_community_srt": "Так что поиск собственных векторов и значений матрицы А сводится к нахождению таких значений v и λ, что это выражение превращается в верное тождество.",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "Поначалу с ним немного неудобно работать, потому что левая часть представляет собой умножение матрицы на вектор, а правая часть здесь — умножение на скалярный вектор.",
  "from_community_srt": "Поначалу странно работать с этим, поскольку левая часть представляет собой умножение матрицы на вектор, а правая часть - умножение вектора на число.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Итак, давайте начнем с того, что перепишем эту правую часть как своего рода умножение матрицы на вектор, используя матрицу, которая масштабирует любой вектор с коэффициентом лямбда.",
  "from_community_srt": "Так что давайте начнем с переписывается правой части в подобие умножения матрицы на вектор, с использованием матрицы, которая дает такой же эффект, как умножение вектора на число λ.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Столбцы такой матрицы будут представлять то, что происходит с каждым базисным вектором, и каждый базисный вектор просто умножается на лямбда, поэтому эта матрица будет иметь число лямбда по диагонали, с нулями повсюду.",
  "from_community_srt": "Колонки такой матрицы обозначают, что случится с каждым базисным вектором, а каждый из них просто умножается на λ. Так что матрица будет иметь λ на своей диагонали и 0 в остальных местах.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "Обычный способ написать этого парня — вынести эту лямбду на множитель и записать ее как лямбда, умноженную на i, где i — единичная матрица с единицами по диагонали.",
  "from_community_srt": "Общепринятый способ описать эту конструкцию - вынести λ за скобки и написать это как λ умножить на I, где I - это единичная матрица с 1 по главной диагонали. (E в русской литературе, прим.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Поскольку обе части выглядят как умножение матрицы на вектор, мы можем вычесть эту правую часть и исключить v.",
  "from_community_srt": "перев.) Теперь, когда обе части выглядят как умножение матрицы на вектор, мы можем перенести правую часть и вынести v за скобки.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Итак, теперь у нас есть новая матрица: A минус лямбда, умноженная на единицу, и мы ищем вектор v такой, чтобы эта новая матрица, умноженная на v, давала нулевой вектор.",
  "from_community_srt": "Теперь у нас есть новая матрица - A минус λ, умноженная на единичную матрицу, в то время как мы ищем вектор v такой, что эта новая матрица, умноженная на v дает нулевой вектор.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Это всегда будет верно, если v само по себе является нулевым вектором, но это скучно.",
  "from_community_srt": "Это всегда верно если v сам по себе нулевой вектор, но это тривиально.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Нам нужен ненулевой собственный вектор.",
  "from_community_srt": "Мы хотим ненулевой собственный вектор.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "И если вы посмотрите главы 5 и 6, вы поймете, что единственный способ, которым произведение матрицы с ненулевым вектором может стать нулевым, — это если преобразование, связанное с этой матрицей, сжимает пространство в более низкое измерение.",
  "from_community_srt": "Если вы просмотрели главу 5 и 6, то вы знаете, что единственный способ для произведения матрицы с ненулевым вектором стать равным нулю - это найти преобразование, которое сжимает пространство с меньшим числом измерений.",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "И это сжатие соответствует нулевому определителю матрицы.",
  "from_community_srt": "И это сжатие соответствует нулевому определителю матрицы.",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Чтобы быть конкретнее, предположим, что ваша матрица A имеет столбцы 2, 1 и 2, 3, и подумайте о вычитании переменной величины, лямбды, из каждой диагональной записи.",
  "from_community_srt": "Конкретно, пусть ваша матрица имеет колонки (2,1) и (2, 3), и вычтете некоторое значение, λ, из каждого элемента диагонали.",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Теперь представьте, что вы настраиваете лямбду, поворачивая ручку, чтобы изменить ее значение.",
  "from_community_srt": "Теперь попытайтесь изменить значение λ.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "По мере изменения этого значения лямбды меняется сама матрица, и, следовательно, меняется определитель матрицы.",
  "from_community_srt": "С изменением λ меняется вся матрица, как и её определитель.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Цель здесь — найти значение лямбды, при котором этот определитель будет равен нулю, а это означает, что измененное преобразование сжимает пространство в более низкое измерение.",
  "from_community_srt": "Цель здесь - найти такое значение λ, при котором определитель равен 0, а, значит, показанное преобразование сжимает пространство в более низкое измерение.",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "В этом случае золотая середина наступает, когда лямбда равна 1.",
  "from_community_srt": "В данном случае это происходит, когда λ равна 1.",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Конечно, если бы мы выбрали какую-то другую матрицу, собственное значение не обязательно было бы равно 1.",
  "from_community_srt": "Конечно, если бы мы выбрали другую матрицу, собственное значение необязательно будет равно 1,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Золотая середина может быть достигнута при каком-то другом значении лямбды.",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Так что это довольно много, но давайте разберемся, о чем идет речь.",
  "from_community_srt": "и сжатие пространства произойдёт при другой λ Мы уже прошли предостаточно, давайте подытожим, о чем сказали.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Когда лямбда равна 1, матрица A минус лямбда, умноженная на единицу, сжимает пространство в строке.",
  "from_community_srt": "Когда λ равна 1, матрица А минус λ, умноженная на единичную матрицу, схлопывает пространство в линию.",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Это означает, что существует ненулевой вектор v такой, что A минус лямбда, умноженное на единицу, умноженное на v, равно нулевому вектору.",
  "from_community_srt": "Это значит, что существует такой ненулевой вектор v, что разница А и λ, умноженной на единичную матрицу, умноженная на v равна нулевому вектору.",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "И помните, причина, по которой нас это волнует, заключается в том, что это означает, что A, умноженное на v, равно лямбда, умноженному на v, что вы можете прочитать как утверждение, что вектор v является собственным вектором A, оставаясь на своем собственном интервале во время преобразования A.",
  "from_community_srt": "И помните, мы следим за этим фактом, потому что A*v = λ*v, а это значит то же, что вектор v - это собственный вектор матрицы А, и он остается на своей оси во время преобразования, вызванного А.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "В этом примере соответствующее собственное значение равно 1, поэтому v фактически просто останется на месте.",
  "from_community_srt": "В данном примере соответствующее собственное значение равно 1, так что v будет просто стоять на месте.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Сделайте паузу и подумайте, нужно ли вам убедиться, что эта линия рассуждений вам нравится.",
  "from_community_srt": "Остановитесь и удостоверьтесь, что вышеописанное не вызывает вопросов.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Это то, о чем я упоминал во введении.",
  "from_community_srt": "Вот то,",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Если бы вы не имели четкого представления об определителях и о том, почему они связаны с линейными системами уравнений, имеющими ненулевые решения, такое выражение выглядело бы совершенно неожиданным.",
  "from_community_srt": "что я упоминал во вступлении: если у вас нет четкого понимания определителей и почему они связаны с системами линейных алгебраических уравнений с ненулевым числом решений, то это выражение будет совершенно не понятно.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Чтобы увидеть это в действии, давайте вернемся к примеру с самого начала, с матрицей, столбцы которой равны 3, 0 и 1, 2.",
  "from_community_srt": "Чтобы понять, давайте повторно рассмотрим пример из начала с матрицей с колонками (3, 0) и (1,",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Чтобы определить, является ли значение лямбда собственным значением, вычтите его из диагоналей этой матрицы и вычислите определитель.",
  "from_community_srt": "2), определим, что λ это собственное значение, вычтенное из диагоналей матрицы, и вычислим определитель.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Сделав это, мы получим некий квадратичный многочлен по лямбде, 3 минус лямбда, умноженный на 2 минус лямбда.",
  "from_community_srt": "Сделав это, мы получаем многочлен 2 степени:",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Поскольку лямбда может быть собственным значением только в том случае, если этот определитель равен нулю, вы можете заключить, что единственными возможными собственными значениями являются лямбда, равная 2, и лямбда, равная 3.",
  "from_community_srt": "(3-λ)*(2-λ) Поскольку λ может быть собственным значением, только если определитель равен нулю, вы можете сделать вывод, что единственно возможные значения λ - это 2 и 3.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Чтобы выяснить, какие собственные векторы на самом деле имеют одно из этих собственных значений, скажем, лямбда равна 2, подставьте это значение лямбда в матрицу, а затем решите, для каких векторов эта диагонально измененная матрица обнуляет.",
  "from_community_srt": "Чтобы выяснить, что собственные векторы соответствуют именно этим собственным значениям, пусть λ равно 2. Подставьте это значение λ в матрицу и затем решите, для каких векторов эта измененная по диагонали матрица обращается в 0.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Если бы вы вычислили это так же, как и любую другую линейную систему, вы бы увидели, что решениями являются все векторы на диагональной линии, натянутые на отрицательные 1, 1.",
  "from_community_srt": "Если вы решите это уравнение так же, как любую иную систему линейных уравнений, вы увидите, что решениями являются все векторы на диагонали, проходящей из начала координат к точке (-1,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Это соответствует тому факту, что неизмененная матрица 3, 0, 1, 2 приводит к растягиванию всех этих векторов в 2 раза.",
  "from_community_srt": "1) Это согласуется с тем фактом, что начальная матрица [(3, 0), (1, 2)] обладает свойством растягивать эти векторы вдвое.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Теперь двумерное преобразование не обязательно должно иметь собственные векторы.",
  "from_community_srt": "И да, двумерное преобразование необязательно имеет собственные векторы.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Например, рассмотрим поворот на 90 градусов.",
  "from_community_srt": "Например, представьте себе поворот на 90 градусов.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "У него нет собственных векторов, поскольку он вращает каждый вектор за пределы своего диапазона.",
  "from_community_srt": "У него вообще нет собственных векторов, раз он сдвигает каждый вектор со своей оси.",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Если вы действительно попытаетесь вычислить собственные значения вращения таким образом, обратите внимание, что произойдет.",
  "from_community_srt": "Если вы решите вычислить собственные значения поворота, подобного этому, обратите внимание, что происходит.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Его матрица имеет столбцы 0, 1 и отрицательные 1, 0.",
  "from_community_srt": "Эта матрица имеет колонки (0, 1) и (-1,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Вычтите лямбду из диагональных элементов и найдите, когда определитель равен нулю.",
  "from_community_srt": "0) Вычтем λ из диагональных элементов и рассмотрим случай, когда определитель равен 0.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "В этом случае вы получаете квадрат полинома лямбда плюс 1.",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Единственными корнями этого многочлена являются мнимые числа i и отрицательное i.",
  "from_community_srt": "В этом случае вы получаете многочлен λ^2+1 Корнями этого многочлена являются мнимые числа i и -i",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Тот факт, что нет решений в действительных числах, указывает на отсутствие собственных векторов.",
  "from_community_srt": "Факт того, что у многочлена нет вещественных решений обозначает, что у матрицы нет собственных векторов.",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Еще один довольно интересный пример, который стоит запомнить, — это ножницы.",
  "from_community_srt": "Интересно, хотя факт,что умножение на i на комплексной плоскости выглядит как вращение на 90 градусов, связан с фактом, что i  - собственное значение этого двумерного преобразования векторов. Это немного выходит за рамки того, что я вам хотел бы сегодня рассказать, но обратите внимание, что комплексные собственные значения, обычно соответствуют какому-то вращению при преобразовании. Ещё один интересный пример, который стоит запомнить,",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Это фиксирует i-шляпу на месте и перемещает j-шляпу 1, так что ее матрица имеет столбцы 1, 0 и 1, 1.",
  "from_community_srt": "это сдвиг Здесь вектор i остается на месте, а вектор j нависает над ним, так что матрица преобразования имеет колонки (1, 0) и (1,",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Все векторы на оси x являются собственными векторами с собственным значением 1, поскольку они остаются фиксированными на месте.",
  "from_community_srt": "1). Все векторы на оси X являются собственными векторами, так как они остаются на месте.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "Фактически это единственные собственные векторы.",
  "from_community_srt": "По сути, это все собственные векторы здесь.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Когда вы вычитаете лямбду из диагоналей и вычисляете определитель, вы получаете 1 минус лямбда в квадрате.",
  "from_community_srt": "Когда вы вычитаете λ из диагоналей и вычисляете определитель, вы получаете (1-λ)^2,",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "И единственный корень этого выражения — лямбда, равная 1.",
  "from_community_srt": "где единственный корень — λ=1 Это согласуется с тем,",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Это согласуется с тем, что мы видим геометрически: все собственные векторы имеют собственное значение 1.",
  "from_community_srt": "что мы видим на графике, что все собственные векторы имеют собственное значение 1.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Однако имейте в виду, что также возможно иметь только одно собственное значение, но с несколькими линиями, заполненными собственными векторами.",
  "from_community_srt": "Помните, однако, что также возможно, что существует только одно собственное значение, но ему соответствует не одна прямая, полная собственных векторов.",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Простой пример — матрица, которая масштабирует все на 2.",
  "from_community_srt": "Простой пример: матрица, которая увеличивает все вдвое.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "Единственное собственное значение — 2, но каждый вектор на плоскости становится собственным вектором с этим собственным значением.",
  "from_community_srt": "Единственное собственное значение - 2, но каждый вектор на плоскости является собственным вектором с этим собственным значением.",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Сейчас еще один хороший момент, чтобы сделать паузу и поразмыслить над этим, прежде чем я перейду к последней теме.",
  "from_community_srt": "Сейчас хороший момент, чтобы сделать паузу и подумать над всем этим, прежде чем мы перейдем к последней теме.",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Я хочу закончить здесь идеей собственного базиса, которая во многом опирается на идеи из последнего видео.",
  "from_community_srt": "Я хочу закончить здесь, описав собственный базис, который сильно связан с идеями из предыдущего видео.",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Посмотрите, что произойдет, если наши базисные векторы окажутся собственными векторами.",
  "from_community_srt": "Посмотрите, что бывает, если наши базисные векторы по чистой случайности стали собственными векторами.",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Например, возможно, i-hat масштабируется на минус 1, а j-hat масштабируется на 2.",
  "from_community_srt": "Например, вектор i умножается на -1, а вектор j - на 2.",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Записывая их новые координаты в виде столбцов матрицы, обратите внимание, что эти скалярные кратные, отрицательные 1 и 2, которые являются собственными значениями i-hat и j-hat, располагаются на диагонали нашей матрицы, а каждая вторая запись равна 0. .",
  "from_community_srt": "Записав их новые координаты как колонки матрицы, обратите внимание, что эти множители: -1 и 2 (которые являются собственными значениями векторов i и j) лежат на диагонали нашей матрицы, а все остальные её элементы равны 0.",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Каждый раз, когда матрица имеет нули везде, кроме диагонали, ее вполне обоснованно называют диагональной матрицей.",
  "from_community_srt": "Матрица, у которой все элементы не на диагонали равны 0, называется диагональной (что разумно).",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "И это можно интерпретировать так: все базисные векторы являются собственными векторами, а диагональные элементы этой матрицы являются их собственными значениями.",
  "from_community_srt": "Это можно истолковать, что все базисные векторы являются собственными векторами, а диагональные элементы матрицы - их собственные значения.",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Есть много вещей, которые делают работу с диагональными матрицами намного приятнее.",
  "from_community_srt": "Существует множество причин, почему с диагональными матрицами проще работать.",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Одна из самых важных проблем заключается в том, что легче вычислить, что произойдет, если вы умножите эту матрицу саму на себя несколько раз.",
  "from_community_srt": "Одна из них - куда легче рассчитать, что будет, если вы умножите такую матрицу на саму себя кучу раз.",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Поскольку все, что делает одна из этих матриц, — это масштабирует каждый базисный вектор на некоторое собственное значение, применение этой матрицы много раз, скажем, 100 раз, будет просто соответствовать масштабированию каждого базисного вектора в 100-й степени соответствующего собственного значения.",
  "from_community_srt": "Поскольку все, что делает эта матрица, - это умножает каждый базисный вектор на собственное значение, использовав эту матрицу много, допустим 100, раз, вы просто умножите каждый базисный вектор на сотую степень соответствующего собственного значения.",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Напротив, попробуйте вычислить 100-ю степень недиагональной матрицы.",
  "from_community_srt": "Для сравнения, попробуйте возвести в 100 степень недиагональную матрицу.",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Действительно, попробуйте на минутку.",
  "from_community_srt": "Попробуйте чуть-чуть,",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Это кошмар.",
  "from_community_srt": "это просто кошмар.",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Конечно, вам редко повезет, чтобы ваши базисные векторы были также собственными векторами.",
  "from_community_srt": "Конечно, вам редко будет везти так,",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Но если у вашего преобразования много собственных векторов, как в начале этого видео, достаточно, чтобы вы могли выбрать набор, охватывающий все пространство, тогда вы можете изменить свою систему координат так, чтобы эти собственные векторы были вашими базисными векторами.",
  "from_community_srt": "что ваши базисные векторы будут вдобавок собственными, но если у вашего преобразования много собственных векторов, как, например, у того, что было в начале видео, и достаточно, чтобы полностью задать пространство, то вы можете сменить систему координат на ту, где эти собственные векторы являются базисными.",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Я говорил об изменении основы в прошлом видео, но здесь я очень быстро напомню, как выразить преобразование, записанное в настоящее время в нашей системе координат, в другую систему.",
  "from_community_srt": "О смене базиса я говорил в последнем видео, но я быстро пройдусь, как записать в одной системе координат преобразование,",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Возьмите координаты векторов, которые вы хотите использовать в качестве новой основы, что в данном случае означает два наших собственных вектора, затем сделайте эти координаты столбцами матрицы, что называется матрицей изменения базиса.",
  "from_community_srt": "написанное в другой Возьмите координаты векторов, которые вы выбрали как новый базис, в данном случае это два собственных вектора, и сформируйте из этих координат колонки матрицы, известной как матрица смены базиса.",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Когда вы объединяете исходное преобразование, помещая изменение базовой матрицы справа и обратную матрицу изменения базиса слева, результатом будет матрица, представляющая то же самое преобразование, но с точки зрения координаты новых базисных векторов. система.",
  "from_community_srt": "Вставив изначальное преобразование между матрицей смены базиса справа и обращенной матрицей смены базиса слева, в качестве результата вы получите матрицу, обозначающую то же преобразование, но с точки зрения новой системы координат с новым базисом.",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Весь смысл этого с собственными векторами заключается в том, что эта новая матрица гарантированно будет диагональной с соответствующими собственными значениями по этой диагонали.",
  "from_community_srt": "Основная причина совершать эти операции с собственными векторами заключается в том, что новая матрица гарантированно будет диагональной, с соответствующими собственными значениями на главной диагонали.",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Это связано с тем, что это представляет собой работу в системе координат, где с базисными векторами происходит масштабирование во время преобразования.",
  "from_community_srt": "Это потому что она обозначает систему координат, где базисные векторы лишь умножаются на число во время преобразования",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Набор базисных векторов, которые также являются собственными векторами, опять-таки достаточно разумно называется собственным базисом.",
  "from_community_srt": "Множество базисных векторов, которые одновременно являются собственными, называется собственным базисом (что вновь довольно разумно).",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Так что, если, например, вам нужно вычислить 100-ю степень этой матрицы, было бы гораздо проще перейти к собственному базису, вычислить 100-ю степень в этой системе, а затем преобразовать обратно в нашу стандартную систему.",
  "from_community_srt": "Таким образом, если вам, например, нужно вычислить сотую степень этой матрицы, куда проще будет привести её к собственному базису, а затем вычислить 100 степень этой системы, после чего вернуться обратно в стандартную систему.",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Вы не можете сделать это со всеми преобразованиями.",
  "from_community_srt": "Вы не можете совершить это со всеми преобразованиями.",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Например, сдвиг не имеет достаточного количества собственных векторов, чтобы охватить все пространство.",
  "from_community_srt": "У сдвига, например, недостаточно собственных векторов для описания пространства.",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Но если вы сможете найти собственный базис, это сделает матричные операции по-настоящему прекрасными.",
  "from_community_srt": "Но если вы сумели найти собственный базис, операции над матрицами становятся проще простого.",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Для тех из вас, кто хочет разгадать довольно изящную головоломку, чтобы увидеть, как она выглядит в действии и как ее можно использовать для получения неожиданных результатов, я оставлю подсказку здесь, на экране.",
  "from_community_srt": "Для тех, кто хочет решить сложную задачу и увидеть все в действии и к каким результатам может приводить, я оставлю подсказку на экране.",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Это потребует некоторой работы, но я думаю, вам понравится.",
  "from_community_srt": "Потребуется немного поработать, но, я думаю, вам понравится.",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Следующее и последнее видео из этой серии будет посвящено абстрактным векторным пространствам.",
  "from_community_srt": "Следующее и последнее видео этой серии будет посвящено абстрактным векторым пространствам.",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]