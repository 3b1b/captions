[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Власні вектори та власні значення є однією з тих тем, які багато студентів вважають особливо неінтуїтивними.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Things like, why are we doing this, and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Такі речі, як, навіщо ми це робимо, і що це насправді означає, занадто часто залишаються просто спливати в морі обчислень без відповіді.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "І коли я публікував відео з цієї серії, багато з вас прокоментували, що з нетерпінням чекають візуалізації цієї теми, зокрема.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Я підозрюю, що причина цього полягає не стільки в тому, що власні речі є особливо складними або погано поясненими.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "Насправді це відносно просто, і я думаю, що більшість книжок чудово це пояснюють.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "What I want to do is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Те, що я хочу зробити, так це те, що це справді має сенс, лише якщо ви добре візуально розумієте багато тем, які йому передують.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Найважливішим тут є те, що ви знаєте, як думати про матриці як про лінійні перетворення, але вам також потрібно знати такі речі, як визначники, лінійні системи рівнянь і зміна базису.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Плутанина щодо власних елементів зазвичай більше пов’язана з хиткою основою однієї з цих тем, ніж із самими власними векторами та власними значеннями.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Для початку розглянемо лінійне перетворення у двох вимірах, як показано тут.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Він переміщує базисний вектор i-hat до координат 3, 0, а j-hat до 1, 2.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Отже, це представлено матрицею, стовпці якої 3, 0 і 1, 2.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Зосередьтеся на тому, що він робить з одним конкретним вектором, і подумайте про розмах цього вектора, лінію, що проходить через його початок і вершину.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "Більшість векторів будуть збиті зі свого діапазону під час трансформації.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Я маю на увазі, що це здавалося б досить випадковим, якби місце, де приземлився вектор, також виявилося десь на цій лінії.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Але деякі спеціальні вектори залишаються на своєму власному діапазоні, тобто ефект, який матриця справляє на такий вектор, полягає в тому, що він просто розтягується або здавлюється, як скаляр.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Для цього конкретного прикладу базисний вектор i-hat є одним із таких спеціальних векторів.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "Діапазон i-hat є віссю x, і з першого стовпця матриці ми бачимо, що i-hat переміщується в 3 рази, все ще на цій осі x.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Більше того, через те, як працюють лінійні перетворення, будь-який інший вектор на осі x також просто розтягується в 3 рази, а отже, залишається на своєму власному діапазоні.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Трохи скромніший вектор, який залишається на власному діапазоні під час цього перетворення, є від’ємним 1, 1.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Зрештою, він розтягується в 2 рази.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "І знову ж таки, лінійність означатиме, що будь-який інший вектор на діагональній лінії, яку охоплює цей хлопець, просто розтягнеться у 2 рази.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "І для цього перетворення це всі вектори з цією особливою властивістю залишатися на своєму діапазоні.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Ті, що знаходяться на осі х, розтягуються у 3 рази, а ті, що знаходяться на цій діагональній лінії, розтягуються у 2 рази.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Будь-який інший вектор дещо повернеться під час перетворення, збитий з лінії, яку він охоплює.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Як ви вже могли здогадатися, ці спеціальні вектори називаються власними векторами перетворення, і кожен власний вектор має пов’язане з ним те, що називається власним значенням, яке є просто фактором, на який він розтягується або здавлюється під час перетворення.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Звичайно, немає нічого особливого в розтягуванні проти хлюпання або факті, що ці власні значення є позитивними.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "В іншому прикладі ви можете мати власний вектор із власним значенням, що дорівнює 1 половині, що означає, що вектор перевертається та стискається в 1 раз.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Але важливою частиною тут є те, що він залишається на лінії, яку охоплює, не повертаючись від неї.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Щоб зрозуміти, чому це може бути корисним для роздумів, розглянемо тривимірне обертання.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Якщо ви можете знайти власний вектор для цього обертання, вектор, який залишається на власному діапазоні, ви знайшли вісь обертання.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3 by 3 matrix associated with that transformation.",
  "translatedText": "Набагато легше думати про 3D-обертання в термінах деякої осі обертання та кута, на який воно обертається, ніж думати про повну матрицю 3 на 3, пов’язану з цим перетворенням.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "У цьому випадку, до речі, відповідне власне значення мало б дорівнювати 1, оскільки обертання ніколи нічого не розтягує і не хлюпає, тому довжина вектора залишиться незмінною.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Ця закономірність часто проявляється в лінійній алгебрі.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "З будь-яким лінійним перетворенням, яке описується матрицею, ви можете зрозуміти, що воно робить, прочитавши стовпці цієї матриці як точки посадки для базисних векторів.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Але часто кращим способом зрозуміти суть того, що насправді робить лінійне перетворення, менш залежним від вашої конкретної системи координат, є знаходження власних векторів і власних значень.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Я не буду описувати всі деталі методів обчислення власних векторів і власних значень тут, але я спробую дати огляд обчислювальних ідей, які є найважливішими для концептуального розуміння.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Символічно, ось як виглядає ідея власного вектора.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A — це матриця, що представляє деяке перетворення, з v як власним вектором, а лямбда — числом, а саме відповідним власним значенням.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Цей вираз говорить про те, що добуток матриці на вектор, A помножене на v, дає той самий результат, що й просте масштабування власного вектора v на деяке значення лямбда.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Отже, пошук власних векторів та їхніх власних значень матриці A зводиться до пошуку значень v і лямбда, які роблять цей вираз істинним.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It's a little awkward to work with at first because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "Спочатку працювати з ним трохи незручно, тому що ліва частина представляє множення матриці на вектор, а права частина тут — множення на скалярний вектор.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Отже, давайте почнемо з того, що перепишемо цю праву частину як певне множення матриці на вектор, використовуючи матрицю, яка має ефект масштабування будь-якого вектора на коефіцієнт лямбда.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Стовпці такої матриці відображатимуть те, що відбувається з кожним базисним вектором, і кожен базисний вектор просто множиться на лямбда, тому ця матриця матиме число лямбда вниз по діагоналі з нулями всюди.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with ones down the diagonal.",
  "translatedText": "Звичайний спосіб записати цього хлопця полягає в тому, щоб винести це лямбда на множники та записати його як лямбда, помножене на i, де i є одиничною матрицею з одиницями по діагоналі.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Оскільки обидві сторони виглядають як множення матриці-вектора, ми можемо відняти цю праву частину та розкласти v.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix, times v, gives the zero vector.",
  "translatedText": "Тепер ми маємо нову матрицю, помножену на одиницю мінус лямбда, і ми шукаємо такий вектор v, щоб ця нова матриця, помножена на v, давала нульовий вектор.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Тепер це завжди буде вірним, якщо сама v є нульовим вектором, але це нудно.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Нам потрібен ненульовий власний вектор.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "І якщо ви подивіться розділи 5 і 6, ви зрозумієте, що єдиний спосіб, яким добуток матриці з ненульовим вектором може стати нулем, це якщо перетворення, пов’язане з цією матрицею, зміщує простір у нижчий вимір.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "І це стиснення відповідає нульовому визначнику для матриці.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Якщо бути конкретним, припустімо, що ваша матриця A має стовпці 2, 1 і 2, 3, і подумайте про віднімання змінної величини, лямбда, від кожного діагонального запису.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Тепер уявіть, що ви налаштовуєте лямбду, повертаючи ручку, щоб змінити її значення.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "Зі зміною цього значення лямбда змінюється сама матриця, а отже, змінюється і визначник матриці.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Мета тут полягає в тому, щоб знайти значення лямбда, яке зробить цей визначник нульовим, що означає, що налаштоване перетворення тисне простір у нижчий вимір.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "У цьому випадку найкраща точка настає, коли лямбда дорівнює 1.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Звичайно, якби ми вибрали якусь іншу матрицю, власне значення могло б не обов’язково дорівнювати 1.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Найкраще значення може бути досягнуто за якогось іншого значення лямбда.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Отже, це багато, але давайте розгадаємо, про що це говорить.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Коли лямбда дорівнює 1, матриця A мінус лямбда, помножена на тотожність, тисне простір на лінію.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Це означає, що є ненульовий вектор v такий, що A мінус лямбда, помножене на тотожність, помножене на v, дорівнює нульовому вектору.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "І запам’ятайте, причина, чому ми про це дбаємо, полягає в тому, що це означає, що A, помножене на v, дорівнює лямбда, помножене на v, що можна прочитати як те, що вектор v є власним вектором A, залишаючись у своєму власному інтервалі під час перетворення A.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "У цьому прикладі відповідне власне значення дорівнює 1, тому v насправді просто залишатиметься на місці.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Зупиніться та подумайте, якщо вам потрібно переконатися, що ця лінія міркувань приємна.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Це те, що я згадав у вступі.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Якби ви не мали чіткого розуміння визначників і того, чому вони пов’язані з лінійними системами рівнянь, які мають відмінні від нуля розв’язки, такий вираз виглядав би абсолютно несподіваним.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Щоб побачити це в дії, давайте переглянемо приклад із самого початку з матрицею, стовпці якої 3, 0 і 1, 2.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Щоб визначити, чи є значення лямбда власним значенням, відніміть його від діагоналей цієї матриці та обчисліть визначник.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Роблячи це, ми отримуємо певний квадратичний поліном від лямбда, 3 мінус лямбда помножити на 2 мінус лямбда.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Оскільки лямбда може бути власним значенням, лише якщо цей визначник дорівнює нулю, ви можете зробити висновок, що єдиними можливими власними значеннями є лямбда, що дорівнює 2, і лямбда, що дорівнює 3.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Щоб з’ясувати, які власні вектори насправді мають одне з цих власних значень, скажімо, лямбда дорівнює 2, підключіть це значення лямбда до матриці, а потім визначте, для яких векторів ця діагонально змінена матриця посилає на нуль.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Якщо ви обчислите це так само, як і будь-яку іншу лінійну систему, ви побачите, що розв’язками є всі вектори на діагональній лінії, натягнутій на мінус 1, 1.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Це відповідає тому факту, що незмінена матриця 3, 0, 1, 2 має ефект розтягування всіх цих векторів у 2 рази.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Тепер двовимірне перетворення не обов’язково має мати власні вектори.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Наприклад, розглянемо поворот на 90 градусів.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Це не має власних векторів, оскільки він обертає кожен вектор поза межами свого діапазону.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Якщо ви дійсно спробуєте обчислити власні значення обертання, як це, зауважте, що станеться.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Його матриця має стовпці 0, 1 і мінус 1, 0.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Відніміть лямбда від діагональних елементів і знайдіть, коли визначник дорівнює нулю.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "У цьому випадку ви отримаєте поліном лямбда в квадраті плюс 1.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Єдиними коренями цього многочлена є уявні числа i та від’ємне i.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Той факт, що немає дійсних чисел, вказує на відсутність власних векторів.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Ще один досить цікавий приклад, про який варто пам’ятати, – це стрижка.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Це фіксує i-hat на місці та переміщує j-hat 1, тому його матриця має стовпці 1, 0 і 1, 1.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Усі вектори на осі x є власними векторами з власним значенням 1, оскільки вони залишаються фіксованими на місці.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "Насправді це єдині власні вектори.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Коли ви віднімаєте лямбда від діагоналей і обчислюєте визначник, ви отримуєте 1 мінус лямбда в квадраті.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "І єдиний корінь цього виразу лямбда дорівнює 1.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Це узгоджується з тим, що ми бачимо геометрично, що всі власні вектори мають власне значення 1.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Майте на увазі, що також можливо мати лише одне власне значення, але з більш ніж просто лінією, повною власних векторів.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Простим прикладом є матриця, яка масштабує все на 2.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "Єдине власне значення дорівнює 2, але кожен вектор на площині стає власним вектором із цим власним значенням.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Зараз ще один гарний час, щоб зупинитись і поміркувати над цим, перш ніж я перейду до останньої теми.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Я хочу завершити тут ідеєю власної основи, яка значною мірою спирається на ідеї з останнього відео.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Подивіться, що станеться, якщо наші базисні вектори просто так випадуть власними векторами.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1, and j-hat is scaled by 2.",
  "translatedText": "Наприклад, можливо, i-hat має масштаб мінус 1, а j-hat має масштаб 2.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Записуючи їхні нові координати як стовпці матриці, зауважте, що ці скалярні кратні, від’ємні 1 і 2, які є власними значеннями i-hat і j-hat, розташовані на діагоналі нашої матриці, а кожен інший запис є 0.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Any time a matrix has 0s everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix, and the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "Кожного разу, коли матриця має 0 скрізь, окрім діагоналі, її називають, досить розумно, діагональною матрицею, і спосіб інтерпретації цього полягає в тому, що всі базисні вектори є власними векторами, а діагональні елементи цієї матриці є їхніми власними значеннями.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Є багато речей, які роблять діагональні матриці набагато зручнішими для роботи.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Одна велика полягає в тому, що легше обчислити, що станеться, якщо ви помножите цю матрицю саму на себе кілька разів.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Оскільки все, що робить одна з цих матриць, це масштабування кожного базисного вектора за деяким власним значенням, застосування цієї матриці багато разів, скажімо, 100 разів, просто відповідатиме масштабуванню кожного базисного вектора за 100-м степенем відповідного власного значення.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Навпаки, спробуйте обчислити 100-й ступінь недіагональної матриці.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Дійсно, спробуйте на мить.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Це кошмар.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Звичайно, вам рідко пощастить, щоб ваші базисні вектори також були власними векторами.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Але якщо ваше перетворення має багато власних векторів, як на початку цього відео, достатньо, щоб ви могли вибрати набір, який охоплює весь простір, тоді ви можете змінити свою систему координат так, щоб ці власні вектори були вашими базисними векторами.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "У минулому відео я говорив про зміну базису, але тут я коротко нагадаю, як виразити трансформацію, яка зараз записана в нашій системі координат, в іншу систему.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Візьміть координати векторів, які ви хочете використовувати як новий базис, що в даному випадку означає наші два власні вектори, а потім зробіть ці координати стовпцями матриці, відомої як матриця зміни базису.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Коли ви розміщуєте початкове перетворення, поміщаючи зміну базисної матриці праворуч, а зворотну до зміни базисної матриці ліворуч, результатом буде матриця, що представляє те саме перетворення, але з точки зору координат нових базисних векторів система.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Весь сенс робити це з власними векторами полягає в тому, що ця нова матриця гарантовано буде діагональною з її відповідними власними значеннями вниз по цій діагоналі.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Це тому, що він представляє роботу в системі координат, де те, що відбувається з базисними векторами, полягає в тому, що вони масштабуються під час перетворення.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Набір базисних векторів, які також є власними векторами, знову ж таки досить розумно називають власним базисом.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Отже, якщо, наприклад, вам потрібно обчислити 100-й ступінь цієї матриці, було б набагато простіше перейти до власної основи, обчислити 100-й ступінь у цій системі, а потім перетворити назад до нашої стандартної системи.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Ви не можете зробити це з усіма перетвореннями.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Зсув, наприклад, не має достатньо власних векторів, щоб охопити весь простір.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Але якщо ви можете знайти власну базу, це робить матричні операції справді чудовими.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Для тих із вас, хто бажає розібратися з гарною головоломкою, щоб побачити, як це виглядає в дії та як це можна використати для отримання дивовижних результатів, я залишу підказку тут, на екрані.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Це потребує трохи роботи, але я думаю, вам це сподобається.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Наступне й останнє відео цієї серії буде присвячено абстрактним векторним просторам.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "See you then!",
  "translatedText": "Побачимось!",
  "model": "google_nmt",
  "n_reviews": 0
 }
]