[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Autovettori e autovalori sono uno di quegli argomenti che molti studenti trovano particolarmente poco intuitivi.",
  "model": "google_nmt",
  "from_community_srt": "\"L'ultima volta, ho chiesto: 'Che cos'è la matematica secondo voi?', e alcune persone hanno risposto: 'La manipolazione dei numeri, la manipolazione delle strutture.' E se avessi chiesto cos'è la musica, avreste risposto: \"La manipolazione delle note?\" \" Serge Lang [NOTA: \"span\", \"chiusura lineare\" e \"copertura lineare\" sono sinonimi] \"Autovettori ed autovalori\" sono tra quegli argomenti che molti studenti trovano particolarmente poco intuitivi.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Domande come: perché lo stiamo facendo e cosa significa realmente, troppo spesso vengono lasciate fluttuare in un mare di calcoli senza risposta.",
  "model": "google_nmt",
  "from_community_srt": "Domande come \"perché lo stiamo facendo\" e \"cosa significa in realtà\" troppo spesso vengono lasciate in sospeso su un mare senza risposta di calcoli.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "E mentre pubblicavo i video di questa serie, molti di voi hanno commentato di non vedere l'ora di visualizzare questo argomento in particolare.",
  "model": "google_nmt",
  "from_community_srt": "E mentre caricavo i video di questa serie, molti di voi commentavano di non aspettare altro che vedere trattato questo argomento in particolare.",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Ho il sospetto che la ragione di ciò non sia tanto il fatto che gli effetti sono particolarmente complicati o scarsamente spiegati.",
  "model": "google_nmt",
  "from_community_srt": "Sospetto che la ragione di questo non sia dovuta tanto al fatto che gli \"auto-oggetti\" siano particolarmente complicati o mal spiegati.",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "In effetti, è relativamente semplice e penso che la maggior parte dei libri svolga un ottimo lavoro nello spiegarlo.",
  "model": "google_nmt",
  "from_community_srt": "In effetti, sono concetti relativamente semplici e penso che molti libri ne diano un'ottima spiegazione.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Il problema è che ha davvero senso solo se hai una solida conoscenza visiva di molti degli argomenti che lo precedono.",
  "model": "google_nmt",
  "from_community_srt": "Il problema è piuttosto questo: tutto assume davvero senso solo se si possiede una solida comprensione visiva di molti degli argomenti che li precedono.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "La cosa più importante qui è che tu sappia pensare alle matrici come trasformazioni lineari, ma devi anche sentirti a tuo agio con cose come determinanti, sistemi lineari di equazioni e cambio di base.",
  "model": "google_nmt",
  "from_community_srt": "È molto importante infatti saper pensare alle matrici come a delle trasformazioni lineari, ma bisogna anche sentirsi a proprio agio con i determinanti, i sistemi lineari di equazioni ed il cambiamento di base.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "La confusione sugli autovalori di solito ha più a che fare con le fondamenta traballanti di uno di questi argomenti che con gli autovettori e gli autovalori stessi.",
  "model": "google_nmt",
  "from_community_srt": "La confusione sugli \"auto-oggetti\" di solito ha più a che fare con le incertezze in merito a questi argomenti piuttosto che con gli autovettori e gli autovalori in sé.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Per iniziare, considera una trasformazione lineare in due dimensioni, come quella mostrata qui.",
  "model": "google_nmt",
  "from_community_srt": "Per cominciare, consideriamo una trasformazione lineare in due dimensioni, come quella mostrata qui.",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Sposta il vettore base i-hat sulle coordinate 3, 0 e j-hat su 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "Essa trasforma il vettore della base î nel vettore di coordinate (3, 0) e ĵ nel vettore (1,",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Quindi è rappresentato con una matrice le cui colonne sono 3, 0 e 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "2), quindi è rappresentata dalla matrice le cui colonne sono (3, 0) e (1,",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Concentrati su ciò che fa a un particolare vettore e pensa all'estensione di quel vettore, alla linea che passa attraverso la sua origine e la sua punta.",
  "model": "google_nmt",
  "from_community_srt": "2). Concentriamoci ora sulla trasformazione di un determinato vettore e, contemporaneamente, consideriamo anche lo span di tale vettore, cioè la retta passante per la sua origine e la sua punta.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "La maggior parte dei vettori verrà eliminata dal proprio intervallo durante la trasformazione.",
  "model": "google_nmt",
  "from_community_srt": "La maggior parte dei vettori verranno \"spinti\" ad fuori della loro copertura lineare durante la trasformazione.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Voglio dire, sembrerebbe piuttosto una coincidenza se anche il luogo in cui è atterrato il vettore fosse da qualche parte su quella linea.",
  "model": "google_nmt",
  "from_community_srt": "Voglio dire, sarebbe davvero un caso curioso se anche dopo la trasformazione il vettore si trovasse da qualche parte su quella retta.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Ma alcuni vettori speciali rimangono nel proprio arco, il che significa che l'effetto che la matrice ha su tale vettore è semplicemente quello di allungarlo o schiacciarlo, come uno scalare.",
  "model": "google_nmt",
  "from_community_srt": "Ma in effetti ci sono alcuni speciali vettori che rimangono sul proprio span dopo la trasformazione. e ciò significa che l'effetto che la matrice ha su di loro è semplicemente quello di stirarli o di comprimerli, proprio come farebbe uno scalare.",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Per questo esempio specifico, il vettore base i-hat è uno di questi vettori speciali.",
  "model": "google_nmt",
  "from_community_srt": "Nel nostro esempio specifico, il vettore della base î è uno di tali speciali vettori.",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "L'intervallo di i-hat è l'asse x e dalla prima colonna della matrice possiamo vedere che i-hat si sposta fino a 3 volte se stesso, sempre su quell'asse x.",
  "model": "google_nmt",
  "from_community_srt": "La chiusura lineare di î è l'asse delle x e, dalla prima colonna della matrice, possiamo vedere che î viene stirato fino a triplicare la sua lunghezza, rimanendo comunque sull'asse delle x.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Inoltre, a causa del modo in cui funzionano le trasformazioni lineari, anche qualsiasi altro vettore sull'asse x viene allungato di un fattore 3 e quindi rimane nella propria estensione.",
  "model": "google_nmt",
  "from_community_srt": "Inoltre, grazie al modo in cui funzionano le trasformazioni lineari, anche qualsiasi altro vettore sull'asse x viene semplicemente allungato di un fattore 3 rimanendo quindi sulla propria copertura lineare.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Un vettore leggermente più subdolo che rimane sul proprio arco durante questa trasformazione è negativo 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Un altro vettore, leggermente meno evidente, che rimane sul suo span durante questa trasformazione è (-1,",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Finisce per allungarsi di un fattore 2.",
  "model": "google_nmt",
  "from_community_srt": "1), il quale, infatti, finisce per essere allungato di un fattore pari a 2.",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "E ancora, la linearità implica che qualsiasi altro vettore sulla linea diagonale attraversata da questo ragazzo verrà allungato di un fattore 2.",
  "model": "google_nmt",
  "from_community_srt": "E di nuovo, la linearità della trasformazione implica che qualsiasi altro vettore sulla retta diagonale individuata da (-1, 1) si allungherà di un fattore 2.",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "E per questa trasformazione, questi sono tutti i vettori con questa proprietà speciale di restare nella loro portata.",
  "model": "google_nmt",
  "from_community_srt": "E per questa trasformazione d'esempio, quelli qui rappresentati sono tutti i vettori con questa speciale proprietà di rimanere sulla loro chiusura lineare.",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Quelli sull'asse x vengono allungati di un fattore 3, e quelli su questa linea diagonale vengono allungati di un fattore 2.",
  "model": "google_nmt",
  "from_community_srt": "Ovvero quelli giacenti sull'asse delle x che vengono stirati di un fattore 3 e quelli su questa retta diagonale che vengono allungati di un fattore 2.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Qualsiasi altro vettore verrà ruotato in qualche modo durante la trasformazione, eliminato dalla linea su cui si estende.",
  "model": "google_nmt",
  "from_community_srt": "Qualsiasi altro vettore verrà leggermente ruotato durante la trasformazione, \"fuoriuscendo\" così dalla retta che individua il suo span.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Come avrai ormai intuito, questi vettori speciali sono chiamati autovettori della trasformazione e a ciascun autovettore è associato quello che viene chiamato autovalore, che è proprio il fattore in base al quale viene allungato o schiacciato durante la trasformazione.",
  "model": "google_nmt",
  "from_community_srt": "Come potresti aver intuito ormai, questi vettori speciali sono chiamati \"autovettori\" della trasformazione, e ad ogni autovettore è associato quello che viene chiamato un \"autovalore\", che è semplicemente il fattore con cui il vettore si è allungato o accorciato durante la trasformazione.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Naturalmente, non c'è niente di speciale nello stretching rispetto allo schiacciamento, o nel fatto che questi autovalori siano positivi.",
  "model": "google_nmt",
  "from_community_srt": "Naturalmente, non solo gli autovalori possono assumere valori maggiori di 1 (allungamento) o compresi tra 0 e 1 (accorciamento) ma si possono anche avere autovalori negativi invece che positivi.",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "In un altro esempio, potresti avere un autovettore con autovalore negativo 1 metà, il che significa che il vettore viene capovolto e schiacciato di un fattore pari a 1 metà.",
  "model": "google_nmt",
  "from_community_srt": "Facendo un esempio diverso, potremmo avere un autovettore con autovalore pari a -1/2, nel qual caso il vettore verrà prima capovolto (poiché l'autovalore è negativo)",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Ma la parte importante qui è che rimanga sulla linea che si estende senza essere ruotato fuori da essa.",
  "model": "google_nmt",
  "from_community_srt": "e poi accorciato di un fattore pari a 1/2. Ma la cosa importante è, di nuovo, il fatto che l'autovettore rimanga sulla retta del suo span senza \"fuoriuscirne\"  a seguito della trasformazione",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Per avere un'idea del motivo per cui questa potrebbe essere una cosa utile a cui pensare, considera una rotazione tridimensionale.",
  "model": "google_nmt",
  "from_community_srt": "Per avere un'idea del perché questi oggetti si rivelino estremamente utili, consideriamo una rotazione tridimensionale.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Se riesci a trovare un autovettore per quella rotazione, un vettore che rimane sul proprio arco, quello che hai trovato è l'asse di rotazione.",
  "model": "google_nmt",
  "from_community_srt": "Se riusciamo a trovare un autovettore per tale rotazione, cioè un vettore che rimane sul suo span, quello che avremo trovato sarà in effetti l'asse di rotazione.",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "Ed è molto più semplice pensare ad una rotazione 3D in termini di un asse di rotazione e di un angolo di rotazione, piuttosto che pensare all'intera matrice 3x3 associata a quella trasformazione.",
  "model": "google_nmt",
  "from_community_srt": "Ed è molto più facile pensare a una rotazione 3D in termini di un angolo e di alcuni assi di rotazione, piuttosto che pensare all'intera matrice 3 x 3 associata a quella trasformazione.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "In questo caso, comunque, l'autovalore corrispondente dovrebbe essere 1, poiché le rotazioni non allungano o schiacciano mai nulla, quindi la lunghezza del vettore rimarrebbe la stessa.",
  "model": "google_nmt",
  "from_community_srt": "In questo caso, comunque, l'autovalore corrispondente dovrebbe essere 1, poiché le rotazioni non allungano né accorciano nulla, e quindi la lunghezza del vettore rimarrebbe la stessa.",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Questo modello si manifesta molto nell'algebra lineare.",
  "model": "google_nmt",
  "from_community_srt": "Questo approccio si adopera molto spesso nell'algebra lineare.",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Con qualsiasi trasformazione lineare descritta da una matrice, puoi capire cosa sta facendo leggendo le colonne di questa matrice come punti di atterraggio per i vettori di base.",
  "model": "google_nmt",
  "from_community_srt": "Data una qualsiasi trasformazione lineare descritta da una matrice, possiamo capire cosa sta facendo interpretando le colonne di questa matrice come i punti di \"atterraggio\" per i vettori della base.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Ma spesso, un modo migliore per arrivare al nocciolo di ciò che fa effettivamente la trasformazione lineare, meno dipendente dal tuo particolare sistema di coordinate, è trovare gli autovettori e gli autovalori.",
  "model": "google_nmt",
  "from_community_srt": "Ma spesso un modo migliore per andare al cuore di ciò che fa effettivamente la trasformazione lineare, peraltro meno dipendente dal particolare sistema di coordinate scelto, è quello di trovare gli autovettori e gli autovalori.",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Non tratterò qui tutti i dettagli sui metodi per calcolare autovettori e autovalori, ma cercherò di fornire una panoramica delle idee computazionali che sono più importanti per una comprensione concettuale.",
  "model": "google_nmt",
  "from_community_srt": "Non tratteremo qui in dettaglio tutti i metodi per il calcolo degli autovettori e degli autovalori, ma proveremo a dare una panoramica delle idee algebriche che sono più importanti per una chiara comprensione dei concetti.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Simbolicamente, ecco come appare l'idea di un autovettore.",
  "model": "google_nmt",
  "from_community_srt": "Algebricamente, ecco come si presenta l'idea di un autovettore.",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A è la matrice che rappresenta una trasformazione, con v come autovettore, e lambda è un numero, cioè l'autovalore corrispondente.",
  "model": "google_nmt",
  "from_community_srt": "La matrice A rappresenta una trasformazione cha ha in v⃗ un autovettore, e λ è un numero, vale a dire l'autovalore corrispondente.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Ciò che questa espressione sta dicendo è che il prodotto matrice-vettore, A per v, dà lo stesso risultato semplicemente scalando l'autovettore v di un valore lambda.",
  "model": "google_nmt",
  "from_community_srt": "Ciò che questa espressione sta dicendo è che il prodotto matrice-vettore \"A volte v⃗ \" dà lo stesso risultato del semplice ridimensionamento dell'autovettore v⃗ per un fattore λ.",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Quindi trovare gli autovettori e i loro autovalori di una matrice A si riduce a trovare i valori di v e lambda che rendono vera questa espressione.",
  "model": "google_nmt",
  "from_community_srt": "Quindi trovare gli autovettori e i loro autovalori per una matrice A si riduce a trovare i vettori v⃗ e i valori λ che rendono vera questa espressione.",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "All'inizio è un po' complicato lavorarci, perché il lato sinistro rappresenta la moltiplicazione di matrice-vettore, ma il lato destro qui è la moltiplicazione di vettore scalare.",
  "model": "google_nmt",
  "from_community_srt": "All'inizio è un po' strano lavorarci su perché il termine a sinistra rappresenta una moltiplicazione matrice-vettore, mentre il termine a destra è il prodotto di uno scalare per un vettoriale.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Quindi iniziamo riscrivendo il lato destro come una sorta di moltiplicazione matrice-vettore, utilizzando una matrice che ha l'effetto di ridimensionare qualsiasi vettore di un fattore lambda.",
  "model": "google_nmt",
  "from_community_srt": "Quindi cominciamo riscrivendo il termine a destra come una sorta di prodotto matrice-vettore, usando una matrice, che abbia l'effetto di scalare qualsiasi vettore di un fattore di λ.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Le colonne di tale matrice rappresenteranno ciò che accade a ciascun vettore di base, e ciascun vettore di base viene semplicemente moltiplicato per lambda, quindi questa matrice avrà il numero lambda lungo la diagonale, con zeri ovunque.",
  "model": "google_nmt",
  "from_community_srt": "Le colonne di tale matrice devono rappresentare ciò che accade a ciascun vettore della base, ma ogni vettore della base viene semplicemente moltiplicato per λ, quindi questa matrice avrà il numero λ lungo la diagonale principale e 0 in ogni altro termine.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "Il modo comune per scrivere questo tipo è fattorizzare quel lambda e scriverlo come lambda per i, dove i è la matrice identità con 1 lungo la diagonale.",
  "model": "google_nmt",
  "from_community_srt": "Il modo comune di scrivere questa matrice è di fattorizzare λ e scriverla come \"λ volte I\", dove I è la matrice identità (quella che ha solo 1 sulla diagonale principale).",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Dato che entrambi i lati assomigliano ad una moltiplicazione matrice-vettore, possiamo sottrarre il lato destro e fattorizzare v.",
  "model": "google_nmt",
  "from_community_srt": "Ora che entrambi i termini dell'equazione hanno la forma di un prodotto matrice-vettore, possiamo portare a sinistra il termine destro e raccogliere a fattor comune v⃗.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Quindi quello che abbiamo ora è una nuova matrice, A meno lambda moltiplicato per l'identità, e stiamo cercando un vettore v tale che questa nuova matrice moltiplicata per v dia il vettore zero.",
  "model": "google_nmt",
  "from_community_srt": "Quindi quello che abbiamo ora è una nuova matrice \"A meno λ volte l'identità\", e stiamo cercando un vettore v⃗, tale che questa nuova matrice moltiplicata per v⃗ fornisca il vettore nullo.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Ora, questo sarà sempre vero se v stesso è il vettore zero, ma è noioso.",
  "model": "google_nmt",
  "from_community_srt": "Naturalmente ciò sarà sempre verificato nel caso banale in cui v⃗ è il vettore nullo, Quindi ciò che cerchiamo è un autovettore diverso dal vettore nullo.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Ciò che vogliamo è un autovettore diverso da zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "E se guardi i capitoli 5 e 6, saprai che l'unico modo in cui è possibile che il prodotto di una matrice con un vettore diverso da zero diventi zero è se la trasformazione associata a quella matrice schiaccia lo spazio in una dimensione inferiore.",
  "model": "google_nmt",
  "from_community_srt": "Alla luce dei capitoli 5 e 6, sappiamo che l'unico caso in cui il prodotto tra una matrice ed un vettore NON nullo è uguale a 0⃗ è quando la trasformazione associata a quella matrice \"fa collassare\" lo spazio in una dimensione inferiore,",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "E questo schiacciamento corrisponde a un determinante zero per la matrice.",
  "model": "google_nmt",
  "from_community_srt": "e sappiamo che questo \"collasso\" corrisponde a un determinante della matrice pari a zero.",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Per essere concreti, supponiamo che la tua matrice A abbia le colonne 2, 1 e 2, 3, e pensa a sottrarre una quantità variabile, lambda, da ciascuna voce diagonale.",
  "model": "google_nmt",
  "from_community_srt": "Per essere concreti, supponiamo che la nostra matrice A abbia colonne (2, 1) e (2, 3), e immagiamo di sottrarre una quantità variabile λ da ciascun valore della diagonale principale.",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Ora immagina di modificare lambda, ruotando una manopola per modificarne il valore.",
  "model": "google_nmt",
  "from_community_srt": "Ora immagiamo di far variare λ, come se potessimo ruotare una manopola per modificarne il valore.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "Quando il valore di lambda cambia, cambia la matrice stessa, e quindi cambia il determinante della matrice.",
  "model": "google_nmt",
  "from_community_srt": "Quando il valore di λ cambia, la matrice stessa cambia e quindi cambia anche il determinante della matrice.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "L'obiettivo qui è trovare un valore di lambda che renderà questo determinante pari a zero, il che significa che la trasformazione ottimizzata schiaccia lo spazio in una dimensione inferiore.",
  "model": "google_nmt",
  "from_community_srt": "Il nostro fine allora è trovare un valore di λ tale per cui il determinante sia zero, vale a dire tale per cui la trasformazione \"faccia collassare\" lo spazio in una dimensione inferiore.",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "In questo caso, il punto debole arriva quando lambda è uguale a 1.",
  "model": "google_nmt",
  "from_community_srt": "In questo esempio, ciò accade quando λ è uguale a 1.",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Naturalmente, se avessimo scelto un'altra matrice, l'autovalore potrebbe non essere necessariamente 1.",
  "model": "google_nmt",
  "from_community_srt": "Certo, se avessimo scelto un'altra matrice, l'autovalore non necessariamente sarebbe stato 1,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Il punto debole potrebbe essere raggiunto con qualche altro valore di lambda.",
  "model": "google_nmt",
  "from_community_srt": "ma il valore di λ cercato sarebbe potuto essere un altro.",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Quindi questo è un po' tanto, ma sveliamo cosa sta dicendo.",
  "model": "google_nmt",
  "from_community_srt": "Quanto abbiamo visto finora può bastare,",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Quando lambda è uguale a 1, la matrice A meno lambda moltiplicata per l'identità schiaccia lo spazio su una linea.",
  "model": "google_nmt",
  "from_community_srt": "ma cerchiamo di chiarire per bene quello che stiamo dicendo: quando λ è uguale a 1, la matrice \"A meno λ volte l'identità\" fa collassare lo spazio su una retta.",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Ciò significa che esiste un vettore v diverso da zero tale che A meno lambda per l'identità per v è uguale al vettore zero.",
  "model": "google_nmt",
  "from_community_srt": "Ciò significa che esiste un vettore v⃗ NON nullo tale che il prodotto della matrice \"A meno λ volte l'identità\" per il vettore v⃗ è uguale al vettor nullo.",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "E ricorda, il motivo per cui ci preoccupiamo è perché significa A per v uguale lambda per v, che puoi leggere come se dicesse che il vettore v è un autovettore di A, rimanendo nel suo intervallo durante la trasformazione A.",
  "model": "google_nmt",
  "from_community_srt": "Ribadiamo, il motivo per cui questo risultato ci interessa è perché equivale a dire che il prodotto della matrice A per il vettore v⃗ è uguale a λ volte v⃗, cosa che possiamo esprimere dicendo che il vettore v⃗ è un autovettore di A, cioè che v⃗ rimane sulla sua chiusura lineare al termine della trasformazione apportata da A.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "In questo esempio, l'autovalore corrispondente è 1, quindi v rimarrebbe effettivamente fisso sul posto.",
  "model": "google_nmt",
  "from_community_srt": "In questo esempio, l'autovalore corrispondente è 1, quindi v⃗ rimane fisso esattamente  al suo posto dopo la trasformazione.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Fai una pausa e rifletti se devi assicurarti che quella linea di ragionamento ti faccia sentire bene.",
  "model": "google_nmt",
  "from_community_srt": "Metti in pausa e riflettici su se hai dubbi sulla bontà del ragionamento.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Questo è il genere di cose che ho menzionato nell'introduzione.",
  "model": "google_nmt",
  "from_community_srt": "Questo è proprio il genere di dubbi che sorgono per le ragioni che abbiamo menzionato nell'introduzione.",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Se non avessi una solida conoscenza dei determinanti e del motivo per cui si riferiscono a sistemi lineari di equazioni con soluzioni diverse da zero, un'espressione come questa sembrerebbe completamente inaspettata.",
  "model": "google_nmt",
  "from_community_srt": "Se, infatti, non si possiede una solida comprensione del significato dei determinanti né del perché essi descrivano sistemi lineari di equazioni con soluzioni non nulle, un'espressione come questa potrebbe apparire semplicemente uscita dal nulla.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Per vederlo in azione, rivisitiamo l'esempio dall'inizio, con una matrice le cui colonne sono 3, 0 e 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "Per vedere tutto ciò in azione, ripercorriamo dall'inizio l'esempio stavolta con la matrice le cui colonne sono (3,",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Per scoprire se un valore lambda è un autovalore, sottrailo dalle diagonali di questa matrice e calcola il determinante.",
  "model": "google_nmt",
  "from_community_srt": "0) e (1, 2). Per scoprire se un valore λ è un autovalore, sottraiamolo dalla diagonale principale di questa matrice e calcoliamo il determinante.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "In questo modo otteniamo un certo polinomio quadratico in lambda, 3 meno lambda per 2 meno lambda.",
  "model": "google_nmt",
  "from_community_srt": "In questo modo, otteniamo un certo polinomio quadratico in λ, cioè (3-λ)(2-λ).",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Poiché lambda può essere un autovalore solo se questo determinante è zero, puoi concludere che gli unici autovalori possibili sono lambda uguale a 2 e lambda uguale a 3.",
  "model": "google_nmt",
  "from_community_srt": "Poiché λ può essere un autovalore solo se questo determinante risulta nullo, possiamo concludere che gli unici autovalori possibili sono rispettivamente λ uguale a 2 e λ uguale a 3.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Per capire quali sono gli autovettori che hanno effettivamente uno di questi autovalori, diciamo che lambda è uguale a 2, collega quel valore di lambda alla matrice e quindi risolvi per quali vettori questa matrice alterata diagonalmente invia a zero.",
  "model": "google_nmt",
  "from_community_srt": "Per capire quali sono gli autovettori che hanno effettivamente uno di questi come autovalori, ad esempio λ uguale a 2, sostituiamo quel valore a λ nella matrice e quindi verifichiamo per quali vettori questa matrice modificata sulla diagonale principale porta al vettore nullo.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Se lo calcolassi come faresti con qualsiasi altro sistema lineare, vedresti che le soluzioni sono tutti i vettori sulla linea diagonale attraversata da -1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Effettuando i conti come faremmo con qualsiasi altro sistema lineare, osserviamo che le soluzioni sono tutti i vettori sulla retta obliqua individuata da (-1,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Ciò corrisponde al fatto che la matrice inalterata, 3, 0, 1, 2, ha l'effetto di allungare tutti quei vettori di un fattore 2.",
  "model": "google_nmt",
  "from_community_srt": "1). Ciò corrisponde al fatto che la matrice di partenza A=[(3, 0), (1, 2)] ha l'effetto di allungare tutti quei vettori sulla retta di un fattore pari a 2.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Ora, una trasformazione 2D non deve avere autovettori.",
  "model": "google_nmt",
  "from_community_srt": "Ora, una trasformazione 2D non deve avere necessariamente autovettori.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Consideriamo ad esempio una rotazione di 90 gradi.",
  "model": "google_nmt",
  "from_community_srt": "Ad esempio, consideriamo una rotazione di 90 gradi.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Questo non ha autovettori poiché ruota ogni vettore fuori dal proprio intervallo.",
  "model": "google_nmt",
  "from_community_srt": "Questa non ha autovettori, poiché ruota ogni vettore al di fuori del proprio span.",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Se provi effettivamente a calcolare gli autovalori di una rotazione come questa, nota cosa succede.",
  "model": "google_nmt",
  "from_community_srt": "Se effettivamente proviamo a calcolare gli autovalori di una rotazione come questa,",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "La sua matrice ha colonne 0, 1 e negativo 1, 0.",
  "model": "google_nmt",
  "from_community_srt": "guardiamo cosa succede: la sua matrice associata ha colonne (0, 1) e (-1,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Sottrai lambda dagli elementi diagonali e cerca quando il determinante è zero.",
  "model": "google_nmt",
  "from_community_srt": "0), sottraiamo λ dagli elementi della diagonale principale e cerchiamo quando il determinante è 0.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "In questo caso ottieni il polinomio lambda al quadrato più 1.",
  "model": "google_nmt",
  "from_community_srt": "In questo caso,",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Le uniche radici di quel polinomio sono i numeri immaginari, i e i negativo.",
  "model": "google_nmt",
  "from_community_srt": "otteniamo il polinomio (λ^2) + 1, e le uniche radici di questo polinomio sono i numeri immaginari i e -i.",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Il fatto che non esistano soluzioni di numeri reali indica che non esistono autovettori.",
  "model": "google_nmt",
  "from_community_srt": "Il fatto che non ci siano soluzioni con numeri reali indica che non ci sono autovettori.",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Un altro esempio piuttosto interessante che vale la pena tenere a mente è una cesoia.",
  "model": "google_nmt",
  "from_community_srt": "È interessante, tuttavia, notare una sorta di analogia tra le rotazioni nel piano complesso e quelle nel piano reale: da una parte, nel piano complesso, come ben sappiamo, il prodotto per l'unità immaginaria determina una rotazione di 90 gradi, dall'altra la rotazione di 90° di vettori reali 2D è legata invece all'avere nell'unità immaginaria i un autovalore della trasformazione. I dettagli del perché questo accada vanno un po' oltre ciò di cui vogliamo parlare in questa lezione, ma è importante ricordare che gli autovalori, quando sono numeri complessi, descrivono generalmente un qualche tipo rotazione nella trasformazione. Un altro esempio piuttosto interessante che merita di essere tenuto a mente è la \"trasformazione a forbice\".",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Questo fissa i-hat in posizione e sposta j-hat 1, quindi la sua matrice ha le colonne 1, 0 e 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Questa trasformazione lascia il versore î al suo posto ma inclina ĵ di un'unità pertanto la matrice associata ha come colonne (1,",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Tutti i vettori sull'asse x sono autovettori con autovalore 1 poiché rimangono fissi sul posto.",
  "model": "google_nmt",
  "from_community_srt": "0) e (1, 1). Tutti i vettori sull'asse delle x sono autovettori con autovalore 1, poiché rimangono fissi dove sono.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "In realtà, questi sono gli unici autovettori.",
  "model": "google_nmt",
  "from_community_srt": "In realtà, questi sono gli unici autovettori.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Quando sottrai lambda dalle diagonali e calcoli il determinante, ottieni 1 meno lambda al quadrato.",
  "model": "google_nmt",
  "from_community_srt": "Se sottraiamo λ dalla diagonale principale e calcoliamo il determinante, infatti, otteniamo  (1 - λ)^2,",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "E l'unica radice di questa espressione è lambda uguale a 1.",
  "model": "google_nmt",
  "from_community_srt": "e l'unica radice di questa espressione è λ=1.",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Ciò è in linea con ciò che vediamo geometricamente, ovvero che tutti gli autovettori hanno autovalore 1.",
  "model": "google_nmt",
  "from_community_srt": "Ciò è coerente con ciò che emerge geometricamente, cioè che tutti gli autovettori hanno per autovalore 1.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Tieni presente, tuttavia, che è anche possibile avere un solo autovalore, ma con più di una semplice riga piena di autovettori.",
  "model": "google_nmt",
  "from_community_srt": "Osserviamo, però, che è anche possibile vi sia un unico autovalore ma associato a più di una retta piena di autovettori.",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Un semplice esempio è una matrice che ridimensiona tutto di 2.",
  "model": "google_nmt",
  "from_community_srt": "Un semplice esempio è dato dalla matrice che raddoppia le dimensioni dell'intero spazio.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "L'unico autovalore è 2, ma ogni vettore nel piano diventa un autovettore con quell'autovalore.",
  "model": "google_nmt",
  "from_community_srt": "l'unico autovalore è 2, ma ogni vettore nel piano è in effetti un autovettore scalato da tale autovalore.",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Questo è un altro buon momento per fermarci e riflettere su alcuni di questi prima di passare all’ultimo argomento.",
  "model": "google_nmt",
  "from_community_srt": "Questo è un altro buon momento per mettere il video in pausa e meditare su quanto detto,",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Voglio concludere qui con l'idea di un'autobase, che si basa fortemente sulle idee dell'ultimo video.",
  "model": "google_nmt",
  "from_community_srt": "prima di passare all'ultimo argomento. Concludiamo con l'idea di \"autobase\", una nozione che attinge ampiamente a quanto abbiamo visto nel video precedente.",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Dai un'occhiata a cosa succede se i nostri vettori base sono autovettori.",
  "model": "google_nmt",
  "from_community_srt": "Diamo un'occhiata a cosa accade ai vettori della base quando questi sono tutti autovettori.",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Ad esempio, forse i-hat viene ridimensionato di meno 1 e j-hat viene ridimensionato di 2.",
  "model": "google_nmt",
  "from_community_srt": "Ad esempio, supponiamo che î sia ridimensionato di -1 e che ĵ sia ridimensionato di un fattore pari a 2.",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Scrivendo le loro nuove coordinate come colonne di una matrice, notiamo che quei multipli scalari, negativi 1 e 2, che sono gli autovalori di i-hat e j-hat, si trovano sulla diagonale della nostra matrice e ogni altra voce è uno 0 .",
  "model": "google_nmt",
  "from_community_srt": "Scrivendo le loro nuove coordinate come le colonne di una matrice, notiamo che quei multipli scalari -1 e 2, che sono gli autovalori di î e ĵ, stanno sulla diagonale principale della nostra matrice e che ogni altro elemento è 0.",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Ogni volta che una matrice ha zeri ovunque tranne che nella diagonale, viene chiamata, abbastanza ragionevolmente, matrice diagonale.",
  "model": "google_nmt",
  "from_community_srt": "Ogni volta che una matrice presenta ovunque 0 tranne che sulla sua diagonale principale, è detta, abbastanza ragionevolmente, matrice diagonale.",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "E il modo di interpretarlo è che tutti i vettori di base sono autovettori, con gli elementi diagonali di questa matrice che sono i loro autovalori.",
  "model": "google_nmt",
  "from_community_srt": "E il modo di interpretare ciò è che tutti i vettori della sua base sono autovettori, con i termini sulla diagonale principale di questa matrice corrispondenti agli autovalori.",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Ci sono molte cose che rendono molto più piacevole lavorare con le matrici diagonali.",
  "model": "google_nmt",
  "from_community_srt": "Vi sono molte ragioni che rendono le matrici diagonali le più belle con cui lavorare.",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Uno dei più importanti è che è più semplice calcolare cosa accadrà se moltiplichi questa matrice per se stessa un sacco di volte.",
  "model": "google_nmt",
  "from_community_srt": "La prima ragione è che è più facile calcolare cosa accadrà quando si moltiplia questa matrice per se stessa più volte.",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Poiché tutto ciò che una di queste matrici fa è ridimensionare ciascun vettore di base di un certo autovalore, applicare quella matrice molte volte, diciamo 100 volte, corrisponderà semplicemente a ridimensionare ciascun vettore di base della centesima potenza dell'autovalore corrispondente.",
  "model": "google_nmt",
  "from_community_srt": "Poiché tutte queste matrici non fanno altro che ridimensionare ogni vettore della base di un fattore dato da un qualche autovalore, applicare quella matrice molte volte, diciamo 100 volte, corrisponderà semplicemente al ridimensionamento di ciascun vettore della base della centesima potenza dell'autovalore corrispondente.",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Al contrario, prova a calcolare la centesima potenza di una matrice non diagonale.",
  "model": "google_nmt",
  "from_community_srt": "Al contrario, provate a calcolare la 100esima potenza di una matrice non diagonale...",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Davvero, provalo per un momento.",
  "model": "google_nmt",
  "from_community_srt": "Davvero,",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "È un incubo.",
  "model": "google_nmt",
  "from_community_srt": "provate a farlo per un momento...",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Naturalmente, raramente sarai così fortunato da avere anche i tuoi vettori di base come autovettori.",
  "model": "google_nmt",
  "from_community_srt": "è un incubo! Naturalmente, di rado saremo così fortunati da avere per autovettori proprio i vettori della base, ma se,",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Ma se la tua trasformazione ha molti autovettori, come quello all'inizio di questo video, abbastanza da poter scegliere un insieme che copra l'intero spazio, allora potresti cambiare il tuo sistema di coordinate in modo che questi autovettori siano i tuoi vettori di base.",
  "model": "google_nmt",
  "from_community_srt": "come accedeva nell'esempio all'inizio di questo video, la trasformazione ha molte rette di autovettori, abbastanza da poter scegliere tra essi un insieme di autovettori il cui span copra l'intero spazio, allora potremmo cambiare il nostro sistema di coordinate in modo che questi autovettori siano i vettori di una nuova base.",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Ho parlato del cambio di base nell'ultimo video, ma qui farò un rapido promemoria su come esprimere una trasformazione attualmente scritta nel nostro sistema di coordinate in un sistema diverso.",
  "model": "google_nmt",
  "from_community_srt": "Abbiamo già parlato del cambiamento di base nell'ultimo video, ma riproponiamo ora un rapidissimo sunto di come esprimere una trasformazione scritta nel nostro sistema di coordinate rispetto a un sistema diverso.",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Prendi le coordinate dei vettori che vuoi utilizzare come nuova base, che in questo caso significa i nostri due autovettori, quindi trasforma quelle coordinate nelle colonne di una matrice, nota come matrice di cambio di base.",
  "model": "google_nmt",
  "from_community_srt": "Prendiamo le coordinate dei vettori che intendiamo utilizzare come nuova base, in questo caso quelle di due autovettori, quindi poniamo tali coordinate nelle colonne di una matrice, nota come la matrice del cambiamento di base.",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Quando si inserisce la trasformazione originale, inserendo la matrice di cambio di base alla sua destra e l'inverso della matrice di cambio di base alla sua sinistra, il risultato sarà una matrice che rappresenta la stessa trasformazione, ma dal punto di vista delle nuove coordinate dei vettori di base sistema.",
  "model": "google_nmt",
  "from_community_srt": "Quando \"interponiamo\" la trasformazione originale tra la matrice del cambiamento di base (a destra) e l'inversa della matrice del cambiamento di base (a sinistra), il risultato che otterremo sarà una matrice che rappresenta quella stessa trasformazione, ma dal punto di vista del nuovo sistema di coordinate.",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Il punto centrale di farlo con gli autovettori è che è garantito che questa nuova matrice sia diagonale con i suoi autovalori corrispondenti lungo quella diagonale.",
  "model": "google_nmt",
  "from_community_srt": "Il punto di farlo con gli autovettori è che la nuova matrice associata alla trasformazione sarà di certo diagonale e i termini sulla sua diagonale principale saranno proprio i suoi autovalori.",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Questo perché rappresenta il lavoro in un sistema di coordinate in cui ciò che accade ai vettori base è che vengono ridimensionati durante la trasformazione.",
  "model": "google_nmt",
  "from_community_srt": "Questo perché essa agisce a partire da un sistema di coordiante dove ciò che accade ai vettori della base (poiché autovettori) è che vengono semplicemente ridimensionati durante la trasformazione.",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Un insieme di vettori base che sono anche autovettori è chiamato, ancora una volta, abbastanza ragionevolmente, autobasi.",
  "model": "google_nmt",
  "from_community_srt": "Una base di vettori che sono anche autovettori è chiamata, ancora una volta abbastanza ragionevolmente,",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Quindi, se, ad esempio, avessi bisogno di calcolare la centesima potenza di questa matrice, sarebbe molto più semplice passare a un'autobasi, calcolare la centesima potenza in quel sistema, quindi riconvertirla al nostro sistema standard.",
  "model": "google_nmt",
  "from_community_srt": "una \"autobase\". Quindi, se, ad esempio, avessimo bisogno di calcolare la 100esima potenza della matrice di partenza, sarà molto più facile passare a un'autobase, calcolare la centesima potenza nel nuovo sistema di autovettori, e infine riconvertire quanto ottenuto rispetto al sistema di coordinate inziale.",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Non è possibile farlo con tutte le trasformazioni.",
  "model": "google_nmt",
  "from_community_srt": "Questo però non è possibile farlo con tutte le trasformazioni, (una \"forbice\",",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Un taglio, ad esempio, non ha abbastanza autovettori per coprire l'intero spazio.",
  "model": "google_nmt",
  "from_community_srt": "ad esempio, non ha un numero di autovettori sufficiente per coprire l'intero spazio 2D) ma,",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Ma se riesci a trovare un'autobase, le operazioni con le matrici diventano davvero adorabili.",
  "model": "google_nmt",
  "from_community_srt": "in generale, se è possibile a trovare un'autobase, questa rende le operazioni con le matrici molto più facili.",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Per quelli di voi che sono disposti a risolvere un puzzle piuttosto accurato per vedere come appare in azione e come può essere utilizzato per produrre risultati sorprendenti, lascerò un messaggio qui sullo schermo.",
  "model": "google_nmt",
  "from_community_srt": "Per quelli tra voi disposti a lavorare su un interessante esercizio, per vedere come tutto ciò appare in azione e come ciò può essere usato per produrre risultati sorprendenti, lascerò un messaggio qui sullo schermo.",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Ci vuole un po' di impegno, ma penso che ti divertirai.",
  "model": "google_nmt",
  "from_community_srt": "Richiede un po' di lavoro, ma penso che ne valga la pena.",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Il prossimo e ultimo video di questa serie riguarderà gli spazi vettoriali astratti.",
  "model": "google_nmt",
  "from_community_srt": "Il prossimo e ultimo video di questa serie riguarderà gli spazi vettoriali astratti.",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]