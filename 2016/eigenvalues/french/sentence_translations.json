[
 {
  "translatedText": "Les vecteurs propres et les valeurs propres font partie de ces sujets que beaucoup d'étudiants trouvent particulièrement peu intuitifs.",
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "time_range": [
   19.92,
   25.76
  ]
 },
 {
  "translatedText": "Des questions telles que « pourquoi faisons-nous cela et qu'est-ce que cela signifie réellement » restent trop souvent flottantes dans une mer de calculs sans réponse.",
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "time_range": [
   25.76,
   33.26
  ]
 },
 {
  "translatedText": "Et au fur et à mesure que j'ai publié les vidéos de cette série, beaucoup d'entre vous ont exprimé leur impatience de visualiser ce sujet en particulier.",
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "time_range": [
   33.92,
   40.06
  ]
 },
 {
  "translatedText": "Je soupçonne que la raison en est pas tant que les choses soient particulièrement compliquées ou mal expliquées.",
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "time_range": [
   40.68,
   46.36
  ]
 },
 {
  "translatedText": "En fait, c’est relativement simple, et je pense que la plupart des livres l’expliquent très bien.",
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "time_range": [
   46.86,
   51.18
  ]
 },
 {
  "translatedText": "Le problème est que cela n’a vraiment de sens que si vous avez une solide compréhension visuelle de la plupart des sujets qui le précèdent.",
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "time_range": [
   51.52,
   58.48
  ]
 },
 {
  "translatedText": "Le plus important ici est que vous sachiez considérer les matrices comme des transformations linéaires, mais vous devez également être à l'aise avec des éléments tels que les déterminants, les systèmes d'équations linéaires et le changement de base.",
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "time_range": [
   59.06,
   69.94
  ]
 },
 {
  "translatedText": "La confusion à propos des choses propres a généralement plus à voir avec des fondations fragiles dans l'un de ces sujets qu'avec les vecteurs propres et les valeurs propres elles-mêmes.",
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "time_range": [
   70.72,
   79.24
  ]
 },
 {
  "translatedText": "Pour commencer, considérons une transformation linéaire en deux dimensions, comme celle présentée ici.",
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "time_range": [
   79.98,
   84.84
  ]
 },
 {
  "translatedText": "Il déplace le vecteur de base i-hat vers les coordonnées 3, 0 et j-hat vers 1, 2.",
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "time_range": [
   85.46,
   91.04
  ]
 },
 {
  "translatedText": "Il est donc représenté par une matrice dont les colonnes sont 3, 0 et 1, 2.",
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "time_range": [
   91.78,
   95.64
  ]
 },
 {
  "translatedText": "Concentrez-vous sur ce qu'il fait à un vecteur particulier et pensez à l'étendue de ce vecteur, à la ligne passant par son origine et sa pointe.",
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "time_range": [
   96.6,
   104.16
  ]
 },
 {
  "translatedText": "La plupart des vecteurs vont perdre leur portée pendant la transformation.",
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "time_range": [
   104.92,
   108.38
  ]
 },
 {
  "translatedText": "Je veux dire, cela semblerait une coïncidence si l'endroit où le vecteur a atterri se trouvait également quelque part sur cette ligne.",
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "time_range": [
   108.78,
   115.32
  ]
 },
 {
  "translatedText": "Mais certains vecteurs spéciaux restent sur leur propre étendue, ce qui signifie que l'effet de la matrice sur un tel vecteur est simplement de l'étirer ou de l'écraser, comme un scalaire.",
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "time_range": [
   117.4,
   127.04
  ]
 },
 {
  "translatedText": "Pour cet exemple spécifique, le vecteur de base i-hat est l’un de ces vecteurs spéciaux.",
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "time_range": [
   129.46,
   134.1
  ]
 },
 {
  "translatedText": "L'étendue de i-hat est l'axe des x, et à partir de la première colonne de la matrice, nous pouvons voir que i-hat se déplace jusqu'à 3 fois lui-même, toujours sur cet axe des x.",
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "time_range": [
   134.64,
   144.12
  ]
 },
 {
  "translatedText": "De plus, en raison du fonctionnement des transformations linéaires, tout autre vecteur sur l'axe des x est également simplement étiré d'un facteur 3 et reste donc sur sa propre étendue.",
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "time_range": [
   146.32,
   156.48
  ]
 },
 {
  "translatedText": "Un vecteur légèrement plus sournois qui reste sur sa propre étendue pendant cette transformation est moins 1, 1.",
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "time_range": [
   158.5,
   164.04
  ]
 },
 {
  "translatedText": "Il finit par être étiré d'un facteur 2.",
  "input": "It ends up getting stretched by a factor of 2.",
  "time_range": [
   164.66,
   167.14
  ]
 },
 {
  "translatedText": "Et encore une fois, la linéarité impliquera que tout autre vecteur sur la diagonale parcourue par ce type sera simplement étiré d'un facteur 2.",
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "time_range": [
   169.0,
   178.22
  ]
 },
 {
  "translatedText": "Et pour cette transformation, ce sont tous les vecteurs qui ont cette propriété particulière de rester sur leur portée.",
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "time_range": [
   179.82,
   185.18
  ]
 },
 {
  "translatedText": "Ceux sur l'axe des x sont étirés d'un facteur 3, et ceux sur cette ligne diagonale sont étirés d'un facteur 2.",
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "time_range": [
   185.62,
   191.98
  ]
 },
 {
  "translatedText": "Tout autre vecteur va subir une légère rotation pendant la transformation, et être retiré de la ligne qu'il couvre.",
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "time_range": [
   192.76,
   198.08
  ]
 },
 {
  "translatedText": "Comme vous l'avez peut-être deviné maintenant, ces vecteurs spéciaux sont appelés vecteurs propres de la transformation, et chaque vecteur propre est associé à ce qu'on appelle une valeur propre, qui est simplement le facteur par lequel il est étiré ou écrasé pendant la transformation.",
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "time_range": [
   202.52,
   217.38
  ]
 },
 {
  "translatedText": "Bien sûr, il n'y a rien de spécial entre l'étirement et l'écrasement, ou le fait que ces valeurs propres se révèlent positives.",
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "time_range": [
   220.28,
   225.94
  ]
 },
 {
  "translatedText": "Dans un autre exemple, vous pourriez avoir un vecteur propre avec une valeur propre négative de 1 moitié, ce qui signifie que le vecteur est inversé et écrasé d'un facteur de 1 moitié.",
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "time_range": [
   226.38,
   235.12
  ]
 },
 {
  "translatedText": "Mais ce qui est important ici, c'est qu'il reste sur la ligne qu'il s'étend sans en sortir.",
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "time_range": [
   236.98,
   242.76
  ]
 },
 {
  "translatedText": "Pour avoir un aperçu de la raison pour laquelle cela pourrait être une chose utile à considérer, envisagez une rotation tridimensionnelle.",
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "time_range": [
   244.46,
   249.8
  ]
 },
 {
  "translatedText": "Si vous pouvez trouver un vecteur propre pour cette rotation, un vecteur qui reste sur sa propre étendue, ce que vous avez trouvé est l'axe de rotation.",
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "time_range": [
   251.66,
   260.5
  ]
 },
 {
  "translatedText": "Et il est beaucoup plus facile de penser à une rotation 3D en termes d'un axe de rotation et d'un angle de rotation, plutôt que de penser à la matrice 3x3 complète associée à cette transformation.",
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "time_range": [
   262.6,
   274.74
  ]
 },
 {
  "translatedText": "Dans ce cas, d'ailleurs, la valeur propre correspondante devrait être 1, puisque les rotations ne s'étirent ni n'écrasent jamais quoi que ce soit, donc la longueur du vecteur resterait la même.",
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "time_range": [
   277.0,
   285.86
  ]
 },
 {
  "translatedText": "Ce modèle apparaît souvent en algèbre linéaire.",
  "input": "This pattern shows up a lot in linear algebra.",
  "time_range": [
   288.08,
   290.02
  ]
 },
 {
  "translatedText": "Avec toute transformation linéaire décrite par une matrice, vous pouvez comprendre ce qu'elle fait en lisant les colonnes de cette matrice comme points d'atterrissage pour les vecteurs de base.",
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "time_range": [
   290.44,
   299.4
  ]
 },
 {
  "translatedText": "Mais souvent, une meilleure façon d'aller au cœur de ce que fait réellement la transformation linéaire, moins dépendante de votre système de coordonnées particulier, est de trouver les vecteurs propres et les valeurs propres.",
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "time_range": [
   300.02,
   310.82
  ]
 },
 {
  "translatedText": "Je ne couvrirai pas ici tous les détails sur les méthodes de calcul des vecteurs propres et des valeurs propres, mais je vais essayer de donner un aperçu des idées informatiques les plus importantes pour une compréhension conceptuelle.",
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "time_range": [
   315.46,
   326.02
  ]
 },
 {
  "translatedText": "Symboliquement, voici à quoi ressemble l'idée d'un vecteur propre.",
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "time_range": [
   327.18,
   330.48
  ]
 },
 {
  "translatedText": "A est la matrice représentant une transformation, avec v comme vecteur propre, et lambda est un nombre, à savoir la valeur propre correspondante.",
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "time_range": [
   331.04,
   339.74
  ]
 },
 {
  "translatedText": "Ce que dit cette expression, c'est que le produit matrice-vecteur, A fois v, donne le même résultat qu'une simple mise à l'échelle du vecteur propre v par une certaine valeur lambda.",
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "time_range": [
   340.68,
   349.9
  ]
 },
 {
  "translatedText": "Ainsi, trouver les vecteurs propres et leurs valeurs propres d'une matrice A revient à trouver les valeurs de v et lambda qui rendent cette expression vraie.",
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "time_range": [
   351.0,
   360.1
  ]
 },
 {
  "translatedText": "C'est un peu difficile à utiliser au début, car le côté gauche représente la multiplication matrice-vecteur, mais le côté droit ici est la multiplication scalaire-vecteur.",
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "time_range": [
   361.92,
   370.54
  ]
 },
 {
  "translatedText": "Commençons donc par réécrire ce membre de droite comme une sorte de multiplication matrice-vecteur, en utilisant une matrice qui a pour effet de mettre à l'échelle n'importe quel vecteur par un facteur lambda.",
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "time_range": [
   371.12,
   380.62
  ]
 },
 {
  "translatedText": "Les colonnes d'une telle matrice représenteront ce qui arrive à chaque vecteur de base, et chaque vecteur de base est simplement multiplié par lambda, donc cette matrice aura le nombre lambda sur la diagonale, avec des zéros partout ailleurs.",
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "time_range": [
   381.68,
   394.32
  ]
 },
 {
  "translatedText": "La façon courante d'écrire ce type est de prendre en compte ce lambda et de l'écrire sous la forme lambda fois i, où i est la matrice d'identité avec des 1 sur la diagonale.",
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "time_range": [
   396.18,
   404.86
  ]
 },
 {
  "translatedText": "Les deux côtés ressemblant à une multiplication matrice-vecteur, nous pouvons soustraire ce côté droit et factoriser le v.",
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "time_range": [
   405.86,
   411.86
  ]
 },
 {
  "translatedText": "Nous avons donc maintenant une nouvelle matrice, A moins lambda fois l'identité, et nous recherchons un vecteur v tel que cette nouvelle matrice multipliée par v donne le vecteur zéro.",
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "time_range": [
   414.16,
   424.92
  ]
 },
 {
  "translatedText": "Maintenant, cela sera toujours vrai si v lui-même est le vecteur zéro, mais c'est ennuyeux.",
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "time_range": [
   426.38,
   431.1
  ]
 },
 {
  "translatedText": "Ce que nous voulons, c'est un vecteur propre non nul.",
  "input": "What we want is a non-zero eigenvector.",
  "time_range": [
   431.34,
   433.64
  ]
 },
 {
  "translatedText": "Et si vous regardez les chapitres 5 et 6, vous saurez que la seule façon pour le produit d'une matrice avec un vecteur non nul de devenir nul est si la transformation associée à cette matrice écrase l'espace dans une dimension inférieure.",
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "time_range": [
   434.42,
   448.02
  ]
 },
 {
  "translatedText": "Et cette squishification correspond à un déterminant nul pour la matrice.",
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "time_range": [
   449.3,
   454.22
  ]
 },
 {
  "translatedText": "Pour être concret, disons que votre matrice A comporte les colonnes 2, 1 et 2, 3, et pensez à soustraire un montant variable, lambda, de chaque entrée diagonale.",
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "time_range": [
   455.48,
   465.52
  ]
 },
 {
  "translatedText": "Imaginez maintenant que vous modifiez lambda, en tournant un bouton pour modifier sa valeur.",
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "time_range": [
   466.48,
   470.28
  ]
 },
 {
  "translatedText": "À mesure que cette valeur de lambda change, la matrice elle-même change, et donc le déterminant de la matrice change.",
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "time_range": [
   470.94,
   477.24
  ]
 },
 {
  "translatedText": "Le but ici est de trouver une valeur de lambda qui rendra ce déterminant nul, ce qui signifie que la transformation modifiée écrase l'espace dans une dimension inférieure.",
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "time_range": [
   478.22,
   487.24
  ]
 },
 {
  "translatedText": "Dans ce cas, le point idéal survient lorsque lambda est égal à 1.",
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "time_range": [
   488.16,
   491.16
  ]
 },
 {
  "translatedText": "Bien entendu, si nous avions choisi une autre matrice, la valeur propre ne serait pas nécessairement 1.",
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "time_range": [
   492.18,
   496.12
  ]
 },
 {
  "translatedText": "Le point idéal pourrait être atteint à une autre valeur de lambda.",
  "input": "The sweet spot might be hit at some other value of lambda.",
  "time_range": [
   496.24,
   498.6
  ]
 },
 {
  "translatedText": "C'est donc beaucoup, mais voyons ce que cela veut dire.",
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "time_range": [
   500.08,
   502.96
  ]
 },
 {
  "translatedText": "Lorsque lambda est égal à 1, la matrice A moins lambda multipliée par l'identité écrase l'espace sur une ligne.",
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "time_range": [
   502.96,
   509.56
  ]
 },
 {
  "translatedText": "Cela signifie qu'il existe un vecteur v non nul tel que A moins lambda fois l'identité fois v est égal au vecteur zéro.",
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "time_range": [
   510.44,
   518.56
  ]
 },
 {
  "translatedText": "Et rappelez-vous, la raison pour laquelle nous nous soucions de cela est que cela signifie que A fois v est égal à lambda fois v, ce que vous pouvez lire comme disant que le vecteur v est un vecteur propre de A, restant sur sa propre étendue pendant la transformation A.",
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "time_range": [
   520.48,
   537.28
  ]
 },
 {
  "translatedText": "Dans cet exemple, la valeur propre correspondante est 1, donc v resterait simplement fixe en place.",
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "time_range": [
   538.32,
   544.02
  ]
 },
 {
  "translatedText": "Faites une pause et réfléchissez si vous devez vous assurer que ce raisonnement vous convient.",
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "time_range": [
   546.22,
   549.5
  ]
 },
 {
  "translatedText": "C'est le genre de chose que j'ai mentionné dans l'introduction.",
  "input": "This is the kind of thing I mentioned in the introduction.",
  "time_range": [
   553.38,
   555.64
  ]
 },
 {
  "translatedText": "Si vous n'aviez pas une solide compréhension des déterminants et de la raison pour laquelle ils se rapportent à des systèmes d'équations linéaires ayant des solutions non nulles, une expression comme celle-ci semblerait complètement inattendue.",
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "time_range": [
   556.22,
   566.3
  ]
 },
 {
  "translatedText": "Pour voir cela en action, reprenons l'exemple du début, avec une matrice dont les colonnes sont 3, 0 et 1, 2.",
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "time_range": [
   568.32,
   574.54
  ]
 },
 {
  "translatedText": "Pour savoir si une valeur lambda est une valeur propre, soustrayez-la des diagonales de cette matrice et calculez le déterminant.",
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "time_range": [
   575.35,
   583.4
  ]
 },
 {
  "translatedText": "En faisant cela, nous obtenons un certain polynôme quadratique en lambda, 3 moins lambda fois 2 moins lambda.",
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "time_range": [
   590.58,
   596.72
  ]
 },
 {
  "translatedText": "Puisque lambda ne peut être une valeur propre que si ce déterminant est nul, vous pouvez conclure que les seules valeurs propres possibles sont lambda égale à 2 et lambda égale à 3.",
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "time_range": [
   597.8,
   608.84
  ]
 },
 {
  "translatedText": "Pour déterminer quels sont les vecteurs propres qui ont réellement l'une de ces valeurs propres, disons que lambda est égal à 2, branchez cette valeur de lambda à la matrice, puis déterminez pour quels vecteurs cette matrice modifiée en diagonale envoie à zéro.",
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "time_range": [
   609.64,
   623.9
  ]
 },
 {
  "translatedText": "Si vous calculiez cela comme vous le feriez avec n'importe quel autre système linéaire, vous verriez que les solutions sont tous les vecteurs sur la diagonale engendrée par moins 1, 1.",
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "time_range": [
   624.94,
   634.3
  ]
 },
 {
  "translatedText": "Cela correspond au fait que la matrice inchangée, 3, 0, 1, 2, a pour effet d'étirer tous ces vecteurs d'un facteur 2.",
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "time_range": [
   635.22,
   643.46
  ]
 },
 {
  "translatedText": "Désormais, une transformation 2D n'a pas besoin d'avoir de vecteurs propres.",
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "time_range": [
   646.32,
   650.2
  ]
 },
 {
  "translatedText": "Par exemple, considérons une rotation de 90 degrés.",
  "input": "For example, consider a rotation by 90 degrees.",
  "time_range": [
   650.72,
   653.4
  ]
 },
 {
  "translatedText": "Cela n'a pas de vecteurs propres puisqu'il fait pivoter chaque vecteur hors de sa propre étendue.",
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "time_range": [
   653.66,
   658.2
  ]
 },
 {
  "translatedText": "Si vous essayez réellement de calculer les valeurs propres d’une rotation comme celle-ci, remarquez ce qui se passe.",
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "time_range": [
   660.8,
   665.56
  ]
 },
 {
  "translatedText": "Sa matrice comporte les colonnes 0, 1 et moins 1, 0.",
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "time_range": [
   666.3,
   670.14
  ]
 },
 {
  "translatedText": "Soustrayez lambda des éléments diagonaux et recherchez quand le déterminant est zéro.",
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "time_range": [
   671.1,
   675.8
  ]
 },
 {
  "translatedText": "Dans ce cas, vous obtenez le polynôme lambda au carré plus 1.",
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "time_range": [
   678.14,
   681.94
  ]
 },
 {
  "translatedText": "Les seules racines de ce polynôme sont les nombres imaginaires i et i négatif.",
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "time_range": [
   682.68,
   687.92
  ]
 },
 {
  "translatedText": "Le fait qu’il n’y ait pas de solutions numériques réelles indique qu’il n’y a pas de vecteurs propres.",
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "time_range": [
   688.84,
   693.6
  ]
 },
 {
  "translatedText": "Un autre exemple assez intéressant qui mérite d’être gardé à l’esprit est une cisaille.",
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "time_range": [
   695.54,
   699.82
  ]
 },
 {
  "translatedText": "Cela fixe i-hat en place et déplace j-hat 1, de sorte que sa matrice a les colonnes 1, 0 et 1, 1.",
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "time_range": [
   700.56,
   707.84
  ]
 },
 {
  "translatedText": "Tous les vecteurs sur l'axe des x sont des vecteurs propres de valeur propre 1 puisqu'ils restent fixes.",
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "time_range": [
   708.74,
   714.54
  ]
 },
 {
  "translatedText": "En fait, ce sont les seuls vecteurs propres.",
  "input": "In fact, these are the only eigenvectors.",
  "time_range": [
   715.68,
   717.82
  ]
 },
 {
  "translatedText": "Lorsque vous soustrayez lambda des diagonales et calculez le déterminant, vous obtenez 1 moins lambda au carré.",
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "time_range": [
   718.76,
   726.54
  ]
 },
 {
  "translatedText": "Et la seule racine de cette expression est lambda égal à 1.",
  "input": "And the only root of this expression is lambda equals 1.",
  "time_range": [
   729.32,
   732.86
  ]
 },
 {
  "translatedText": "Cela correspond à ce que nous voyons géométriquement, à savoir que tous les vecteurs propres ont une valeur propre 1.",
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "time_range": [
   734.56,
   739.72
  ]
 },
 {
  "translatedText": "Gardez cependant à l’esprit qu’il est également possible d’avoir une seule valeur propre, mais avec plus qu’une simple ligne remplie de vecteurs propres.",
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "time_range": [
   741.08,
   748.02
  ]
 },
 {
  "translatedText": "Un exemple simple est une matrice qui met tout à l’échelle par 2.",
  "input": "A simple example is a matrix that scales everything by 2.",
  "time_range": [
   749.9,
   753.18
  ]
 },
 {
  "translatedText": "La seule valeur propre est 2, mais chaque vecteur du plan devient un vecteur propre avec cette valeur propre.",
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "time_range": [
   753.9,
   760.7
  ]
 },
 {
  "translatedText": "C’est maintenant un autre bon moment pour faire une pause et réfléchir à tout cela avant de passer au dernier sujet.",
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "time_range": [
   762.0,
   766.96
  ]
 },
 {
  "translatedText": "Je veux terminer ici avec l'idée d'une base propre, qui s'appuie fortement sur les idées de la dernière vidéo.",
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "time_range": [
   783.54,
   789.88
  ]
 },
 {
  "translatedText": "Jetez un œil à ce qui se passe si nos vecteurs de base se révèlent être des vecteurs propres.",
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "time_range": [
   791.48,
   796.38
  ]
 },
 {
  "translatedText": "Par exemple, peut-être que i-hat est mis à l'échelle de moins 1 et j-hat est mis à l'échelle de 2.",
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "time_range": [
   797.12,
   802.38
  ]
 },
 {
  "translatedText": "En écrivant leurs nouvelles coordonnées sous forme de colonnes d'une matrice, notez que ces multiples scalaires, négatifs 1 et 2, qui sont les valeurs propres de i-hat et j-hat, se trouvent sur la diagonale de notre matrice et que chaque autre entrée est un 0. .",
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "time_range": [
   803.42,
   817.18
  ]
 },
 {
  "translatedText": "Chaque fois qu'une matrice a des zéros partout ailleurs que sur la diagonale, on l'appelle, assez raisonnablement, une matrice diagonale.",
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "time_range": [
   818.88,
   825.42
  ]
 },
 {
  "translatedText": "Et la façon d'interpréter cela est que tous les vecteurs de base sont des vecteurs propres, les entrées diagonales de cette matrice étant leurs valeurs propres.",
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "time_range": [
   825.84,
   834.4
  ]
 },
 {
  "translatedText": "Il y a beaucoup de choses qui rendent les matrices diagonales beaucoup plus agréables à utiliser.",
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "time_range": [
   837.1,
   841.06
  ]
 },
 {
  "translatedText": "Le plus important est qu'il est plus facile de calculer ce qui se passera si vous multipliez cette matrice par elle-même plusieurs fois.",
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "time_range": [
   841.78,
   848.34
  ]
 },
 {
  "translatedText": "Puisque toutes ces matrices ne font que mettre à l'échelle chaque vecteur de base par une valeur propre, appliquer cette matrice plusieurs fois, disons 100 fois, va simplement correspondre à la mise à l'échelle de chaque vecteur de base par la puissance 100 de la valeur propre correspondante.",
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "time_range": [
   849.42,
   864.6
  ]
 },
 {
  "translatedText": "En revanche, essayez de calculer la puissance 100 d’une matrice non diagonale.",
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "time_range": [
   865.7,
   869.68
  ]
 },
 {
  "translatedText": "Vraiment, essayez-le un instant.",
  "input": "Really, try it for a moment.",
  "time_range": [
   869.68,
   871.32
  ]
 },
 {
  "translatedText": "C'est un cauchemar.",
  "input": "It's a nightmare.",
  "time_range": [
   871.74,
   872.44
  ]
 },
 {
  "translatedText": "Bien sûr, vous aurez rarement la chance que vos vecteurs de base soient également des vecteurs propres.",
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "time_range": [
   876.08,
   881.26
  ]
 },
 {
  "translatedText": "Mais si votre transformation comporte un grand nombre de vecteurs propres, comme celui du début de cette vidéo, suffisamment pour que vous puissiez choisir un ensemble qui s'étend sur tout l'espace, vous pouvez alors modifier votre système de coordonnées afin que ces vecteurs propres soient vos vecteurs de base.",
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "time_range": [
   882.04,
   896.54
  ]
 },
 {
  "translatedText": "J'ai parlé du changement de base dans la dernière vidéo, mais je vais faire ici un rappel très rapide de la façon d'exprimer une transformation actuellement écrite dans notre système de coordonnées dans un système différent.",
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "time_range": [
   897.14,
   907.04
  ]
 },
 {
  "translatedText": "Prenez les coordonnées des vecteurs que vous souhaitez utiliser comme nouvelle base, ce qui signifie dans ce cas nos deux vecteurs propres, puis faites de ces coordonnées les colonnes d'une matrice, connue sous le nom de matrice de changement de base.",
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "time_range": [
   908.44,
   919.44
  ]
 },
 {
  "translatedText": "Lorsque vous prenez en sandwich la transformation d'origine, en plaçant la matrice de changement de base à sa droite et l'inverse de la matrice de changement de base à sa gauche, le résultat sera une matrice représentant cette même transformation, mais du point de vue des nouvelles coordonnées des vecteurs de base. système.",
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "time_range": [
   920.18,
   936.5
  ]
 },
 {
  "translatedText": "L’intérêt de faire cela avec les vecteurs propres est que cette nouvelle matrice est garantie d’être diagonale avec ses valeurs propres correspondantes sur cette diagonale.",
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "time_range": [
   937.44,
   946.68
  ]
 },
 {
  "translatedText": "En effet, cela représente un travail dans un système de coordonnées où les vecteurs de base sont mis à l'échelle lors de la transformation.",
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "time_range": [
   946.86,
   954.32
  ]
 },
 {
  "translatedText": "Un ensemble de vecteurs de base qui sont également des vecteurs propres est appelé, encore une fois, assez raisonnablement, une base propre.",
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "time_range": [
   955.8,
   961.56
  ]
 },
 {
  "translatedText": "Ainsi, si, par exemple, vous deviez calculer la 100e puissance de cette matrice, il serait beaucoup plus facile de passer à une base propre, de calculer la 100e puissance dans ce système, puis de revenir à notre système standard.",
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "time_range": [
   962.34,
   975.68
  ]
 },
 {
  "translatedText": "Vous ne pouvez pas faire cela avec toutes les transformations.",
  "input": "You can't do this with all transformations.",
  "time_range": [
   976.62,
   978.32
  ]
 },
 {
  "translatedText": "Une cisaille, par exemple, n'a pas suffisamment de vecteurs propres pour couvrir tout l'espace.",
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "time_range": [
   978.32,
   982.98
  ]
 },
 {
  "translatedText": "Mais si vous pouvez trouver une base propre, cela rend les opérations matricielles vraiment agréables.",
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "time_range": [
   983.46,
   988.16
  ]
 },
 {
  "translatedText": "Pour ceux d'entre vous qui souhaitent résoudre un casse-tête assez soigné pour voir à quoi cela ressemble en action et comment il peut être utilisé pour produire des résultats surprenants, je vais laisser une invite ici à l'écran.",
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "time_range": [
   989.12,
   997.32
  ]
 },
 {
  "translatedText": "Cela demande un peu de travail, mais je pense que vous l'apprécierez.",
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "time_range": [
   997.6,
   1000.28
  ]
 },
 {
  "translatedText": "La prochaine et dernière vidéo de cette série portera sur les espaces vectoriels abstraits.",
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "time_range": [
   1000.84,
   1006.12
  ]
 }
]