[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "تعد المتجهات الذاتية والقيم الذاتية واحدة من تلك المواضيع التي يجدها الكثير من الطلاب غير بديهية بشكل خاص.",
  "model": "google_nmt",
  "from_community_srt": "\"Eigenvectors and eigenvalues\" هي واحدة من تلك المواضيع التي يجدها الكثير من الطلاب غير واضحة بشكل خاص.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "أسئلة مثل، لماذا نفعل هذا وماذا يعني هذا في الواقع، غالبًا ما تُترك لتطفو بعيدًا في بحر من الحسابات دون إجابة.",
  "model": "google_nmt",
  "from_community_srt": "أسئلة مثل \"لماذا نفعل هذا\" و \"ماذا يعني هذا في الواقع\" غالبًا ما يتم تركها في بحر من الحسابات دون إجابة.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "وبينما قمت بطرح مقاطع الفيديو الخاصة بهذه السلسلة، علق الكثير منكم حول التطلع إلى تصور هذا الموضوع على وجه الخصوص.",
  "model": "google_nmt",
  "from_community_srt": "ولأنني وضعت مقاطع الفيديو الخاصة بالسلسلة ، لقد علق الكثير منكم على التطلع إلى تصور هذا الموضوع على وجه الخصوص.",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "أظن أن السبب في ذلك لا يرجع إلى كون الأشياء الذاتية معقدة بشكل خاص أو سيئة التفسير.",
  "model": "google_nmt",
  "from_community_srt": "وأظن أن السبب في ذلك ليس كثيرا أن الأشياء eigen- معقدة أو سيئة بشكل خاص.",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "في الواقع، إنه أمر واضح ومباشر نسبيًا، وأعتقد أن معظم الكتب تقوم بعمل جيد في شرحه.",
  "model": "google_nmt",
  "from_community_srt": "في الواقع ، إنها بسيطة نسبيا وأعتقد أن معظم الكتب تقوم بعمل جيد لشرحها.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "تكمن المشكلة في أنه يكون الأمر منطقيًا فقط إذا كان لديك فهم بصري قوي للعديد من المواضيع التي تسبقه.",
  "model": "google_nmt",
  "from_community_srt": "القضية هي ذلك من المنطقي حقًا أن يكون لديك فهم مرئي راسخ للعديد من الموضوعات التي تسبقها.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "الأهم هنا هو أنك تعرف كيفية التفكير في المصفوفات كتحويلات خطية، ولكن عليك أيضًا أن تكون مرتاحًا لأشياء مثل المحددات وأنظمة المعادلات الخطية وتغيير الأساس.",
  "model": "google_nmt",
  "from_community_srt": "الأهم هنا هو أن تعرف كيف تفكر في المصفوفات كتحولات خطية ، ولكن عليك أيضًا أن تكون مرتاحًا مع الأشياء مثل المحددات والنظم الخطية للمعادلات وتغيير الأساس.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "عادةً ما يكون الارتباك حول الأشياء الذاتية مرتبطًا بأساس هش في أحد هذه المواضيع أكثر من ارتباطه بالمتجهات الذاتية والقيم الذاتية نفسها.",
  "model": "google_nmt",
  "from_community_srt": "الارتباك حول المواد eigen عادة ما يكون له علاقة مع أساس هش في واحدة من هذه المواضيع مما هي عليه في eigenvectors و eigenvalues ​​نفسها.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "للبدء، فكر في بعض التحولات الخطية في بعدين، مثل ذلك الموضح هنا.",
  "model": "google_nmt",
  "from_community_srt": "للبدء ، ضع في اعتبارك بعض التحولات الخطية في بعدين ، مثل الذي ظهر هنا.",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "يقوم بنقل المتجه الأساسي i-hat إلى الإحداثيات 3 و0 وj-hat إلى 1 و2.",
  "model": "google_nmt",
  "from_community_srt": "وهو يتحرك على أساس i-hat المتجه إلى الإحداثيات (3 ، 0) و j-hat إلى (1 ، 2) ، بحيث يتم تمثيلها بمصفوفة ، تكون أعمدةها (3 ، 0) و (1 ، 2).",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "لذلك يتم تمثيلها بمصفوفة أعمدتها هي 3، 0، و1، 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "ركز على ما يفعله بمتجه معين، وفكر في مدى هذا المتجه، أي الخط الذي يمر عبر نقطة الأصل وطرفه.",
  "model": "google_nmt",
  "from_community_srt": "التركيز على ما يفعله لمتجه واحد معين وفكر في مدى ذلك الناقل ، الخط الذي يمر من خلال أصله وطرفه.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "سيتم التخلص من معظم المتجهات خلال عملية التحول.",
  "model": "google_nmt",
  "from_community_srt": "معظم المتجهات سوف تخرج عن نطاقها خلال التحول.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "أعني أنه قد يبدو من قبيل الصدفة أن المكان الذي هبط فيه المتجه يقع أيضًا في مكان ما على هذا الخط.",
  "model": "google_nmt",
  "from_community_srt": "أعني ، قد يبدو الأمر من قبيل الصدفة إذا كان المكان الذي هبط فيه الناقل في مكان ما على هذا الخط.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "لكن بعض المتجهات الخاصة تظل في امتدادها الخاص، مما يعني أن تأثير المصفوفة على مثل هذا المتجه هو مجرد تمديده أو سحقه، مثل العددية.",
  "model": "google_nmt",
  "from_community_srt": "لكن بعض المتجهات الخاصة تبقى في نطاقها الخاص ، وهذا يعني أن تأثير المصفوفة على مثل هذا المتجه هو فقط لتمديده أو إسحقه ، مثل العدد القياسي.",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "في هذا المثال المحدد، يعتبر المتجه الأساسي i-hat أحد هذه المتجهات الخاصة.",
  "model": "google_nmt",
  "from_community_srt": "لهذا المثال بالتحديد ، فإن المتجه الأساسي i-hat هو أحد هذه المتجهات الخاصة.",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "مدى i-hat هو المحور السيني، ومن العمود الأول للمصفوفة، يمكننا أن نرى أن i-hat يتحرك إلى 3 أضعاف نفسه، ولا يزال على هذا المحور السيني.",
  "model": "google_nmt",
  "from_community_srt": "امتداد i-hat هو المحور السيني ، ومن العمود الأول للمصفوفة ، يمكننا أن نرى أن i-hat يتحرك إلى 3 أضعاف نفسه ، لا يزال على ذلك المحور x.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "علاوة على ذلك، نظرًا للطريقة التي تعمل بها التحويلات الخطية، فإن أي متجه آخر على المحور السيني يتم تمديده أيضًا بعامل 3، وبالتالي يبقى في امتداده الخاص.",
  "model": "google_nmt",
  "from_community_srt": "ما هو أكثر من ذلك ، بسبب الطريقة التي تعمل بها التحولات الخطية ، أي متجه آخر على المحور السيني هو أيضا امتدت من قبل عامل 3 وبالتالي لا يزال على المدى الخاص به.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "المتجه الأكثر تسللًا والذي يبقى على امتداده الخاص أثناء هذا التحويل هو سالب 1، 1.",
  "model": "google_nmt",
  "from_community_srt": "ناقل أضيق قليلاً الذي يبقى على امتداده خلال هذا التحول هو (-1 ، 1) ، ينتهي الأمر الحصول على تمدد بواسطة عامل 2.",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "وينتهي الأمر بالتمدد بعامل 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "ومرة أخرى، الخطية ستعني ضمنًا أن أي متجه آخر على الخط القطري الممتد بواسطة هذا الشخص سوف يتم تمديده بعامل قدره 2.",
  "model": "google_nmt",
  "from_community_srt": "ومرة أخرى ، فإن الخطية تعني ذلك أي متجه آخر على الخط القطري الذي يمتد إليه هذا الشخص هو مجرد الحصول على امتدت بها عامل 2.",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "وبالنسبة لهذا التحويل، هذه هي جميع المتجهات التي تتمتع بهذه الخاصية الخاصة وهي البقاء على امتدادها.",
  "model": "google_nmt",
  "from_community_srt": "ولهذا التحول ، هذه هي جميع المتجهات مع هذه الخاصية الخاصة للبقاء على امتدادها.",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "تلك الموجودة على المحور السيني تتمدد بعامل 3، وتلك الموجودة على هذا الخط القطري تتمدد بعامل 2.",
  "model": "google_nmt",
  "from_community_srt": "تلك التي على محور اكس تمد من قبل عامل 3 وتنتشر تلك الموجودة على هذا الخط القطري بواسطة عامل 2.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "سيتم تدوير أي متجه آخر إلى حد ما أثناء التحويل، مما يؤدي إلى إزالته من الخط الذي يمتد عليه.",
  "model": "google_nmt",
  "from_community_srt": "أي ناقل آخر سيتم تدويره نوعًا ما أثناء التحويل ، خرج من الخط الذي يمتد.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "كما كنت قد خمنت الآن، تسمى هذه المتجهات الخاصة بالمتجهات الذاتية للتحويل، ويرتبط كل ناقل ذاتي به بما يسمى القيمة الذاتية، وهو مجرد العامل الذي يتم من خلاله تمديده أو سحقه أثناء التحويل.",
  "model": "google_nmt",
  "from_community_srt": "كما قد تفكر الآن ، تسمى هذه النواقل الخاصة \"المتجهات الذاتية\" للتحول ، وقد ارتبط كل من eigenvector بها ، ما يسمى \"قيمة الذاتية\" ، وهو العامل الوحيد الذي امتد من خلاله أو تم سحقه أثناء التحول.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "بالطبع، لا يوجد شيء مميز في التمدد مقابل السحق، أو حقيقة أن هذه القيم الذاتية تكون إيجابية.",
  "model": "google_nmt",
  "from_community_srt": "بالطبع ، لا يوجد شيء خاص حول التمدد مقابل السحق أو حقيقة أن قيم eigenvalues ​​هذه كانت إيجابية.",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "في مثال آخر، يمكن أن يكون لديك متجه ذاتي قيمته الذاتية سالب 1 نصف، مما يعني أنه يتم قلب المتجه وسحقه بعامل قدره النصف.",
  "model": "google_nmt",
  "from_community_srt": "في مثال آخر ، يمكن أن يكون لديك متجاوب مع eigenvalue -1/2 ، وهذا يعني أن ناقلات يحصل انقلبت ويسحقها عامل 1/2.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "لكن الجزء المهم هنا هو أن يظل على الخط الذي يمتد إليه دون أن يدور خارجًا عنه.",
  "model": "google_nmt",
  "from_community_srt": "لكن الجزء المهم هنا هو أنه يبقى على الخط الذي يمتد من دون أن يدور حوله.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "للحصول على لمحة عن السبب الذي يجعل هذا أمرًا مفيدًا للتفكير فيه، فكر في بعض التدوير ثلاثي الأبعاد.",
  "model": "google_nmt",
  "from_community_srt": "وللمحافظة على السبب ، قد يكون هذا أمرًا مفيدًا للتفكير فيه ، النظر في بعض دوران ثلاثي الأبعاد.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "إذا تمكنت من العثور على متجه ذاتي لهذا الدوران، أي متجه يظل في امتداده الخاص، فإن ما وجدته هو محور الدوران.",
  "model": "google_nmt",
  "from_community_srt": "إذا كان بإمكانك العثور على مُخترِع لهذا التناوب ، ناقل يمتد على المدى الخاص به ، ما وجدته هو محور الدوران.",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "ومن الأسهل التفكير في دوران ثلاثي الأبعاد من حيث بعض محاور الدوران والزاوية التي يدور بها، بدلاً من التفكير في المصفوفة الكاملة 3x3 المرتبطة بهذا التحويل.",
  "model": "google_nmt",
  "from_community_srt": "ومن الأسهل التفكير في الدوران ثلاثي الأبعاد من حيث بعض محور الدوران وزاوية الدوران ، بدلاً من التفكير في المصفوفة الكاملة 3-بواسطة 3 المرتبطة بهذا التحول.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "في هذه الحالة، بالمناسبة، يجب أن تكون القيمة الذاتية المقابلة 1، نظرًا لأن الدورات لا تمد أو تسحق أي شيء أبدًا، وبالتالي فإن طول المتجه سيظل كما هو.",
  "model": "google_nmt",
  "from_community_srt": "في هذه الحالة ، بالمناسبة ، يجب أن تكون قيمة eigenvalue المقابلة 1 ، بما أن الدورات لا تمتد أو تسحق أي شيء ، لذا فإن طول المتجه سيبقى كما هو.",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "يظهر هذا النمط كثيرًا في الجبر الخطي.",
  "model": "google_nmt",
  "from_community_srt": "يظهر هذا النمط كثيرًا في الجبر الخطي.",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "مع أي تحويل خطي تصفه المصفوفة، يمكنك فهم ما تفعله من خلال قراءة أعمدة هذه المصفوفة كنقاط هبوط للمتجهات الأساسية.",
  "model": "google_nmt",
  "from_community_srt": "مع أي تحول خطي موصوف في المصفوفة ، يمكنك فهم ما تفعله من خلال قراءة أعمدة هذه المصفوفة مثل بقع الهبوط للمتجهات الأساسية.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "لكن في كثير من الأحيان، الطريقة الأفضل للوصول إلى قلب ما يفعله التحويل الخطي فعليًا، بشكل أقل اعتمادًا على نظام الإحداثيات الخاص بك، هي العثور على المتجهات الذاتية والقيم الذاتية.",
  "model": "google_nmt",
  "from_community_srt": "لكن في الغالب طريقة أفضل للحصول على قلب ما يحدثه التحول الخطي ، أقل اعتمادا على نظام الإحداثيات الخاص بك ، هو ايجاد eigenvectors و eigenvalues.",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "لن أغطي التفاصيل الكاملة حول طرق حساب المتجهات الذاتية والقيم الذاتية هنا، لكنني سأحاول تقديم نظرة عامة على الأفكار الحسابية الأكثر أهمية للفهم المفاهيمي.",
  "model": "google_nmt",
  "from_community_srt": "لن أغطي التفاصيل الكاملة حول طرق حساب eigenvectors وقيم eigenvalues ​​هنا ، ولكن سأحاول تقديم نظرة عامة على الأفكار الحسابية التي هي الأهم لفهم مفاهيمي.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "رمزيًا، إليك ما تبدو عليه فكرة المتجهات الذاتية.",
  "model": "google_nmt",
  "from_community_srt": "رمزيا ، وهنا ما تبدو فكرة eigenvector.",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A هي المصفوفة التي تمثل بعض التحولات، مع v كمتجه ذاتي، ولامدا رقم، أي القيمة الذاتية المقابلة.",
  "model": "google_nmt",
  "from_community_srt": "A هي المصفوفة التي تمثل بعض التحول ، مع v كالمخترع ، و λ هو رقم ، أي القيمة الذاتية المقابلة.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "ما يقوله هذا التعبير هو أن حاصل ضرب المصفوفة والمتجه، A في v، يعطي نفس النتيجة مثل مجرد قياس المتجه الذاتي v ببعض قيمة لامدا.",
  "model": "google_nmt",
  "from_community_srt": "ما يقوله هذا التعبير هو أن المنتج المتجه المصفوفة - A مرة v يعطي نفس النتيجة بمجرد تحجيم eigenvector v ببعض القيمة λ.",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "لذا فإن العثور على المتجهات الذاتية وقيمها الذاتية للمصفوفة A يعود إلى إيجاد قيم v وlamda التي تجعل هذا التعبير صحيحًا.",
  "model": "google_nmt",
  "from_community_srt": "حتى إيجاد eigenvectors وقيمها الذاتية من مصفوفة A ينزل لإيجاد قيم v و λ التي تجعل هذا التعبير صحيحًا.",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "قد يكون العمل به صعبًا بعض الشيء في البداية، لأن الجانب الأيسر يمثل الضرب بمتجه المصفوفة، لكن الجانب الأيمن هنا يمثل الضرب بالمتجه العددي.",
  "model": "google_nmt",
  "from_community_srt": "من الصعب أن تعمل معه في البداية ، لأن هذا الجانب الأيسر يمثل مضاعفة متجه المصفوفة ، لكن الجانب الأيمن هنا هو مضاعفة العددية.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "لذلك دعونا نبدأ بإعادة كتابة الجانب الأيمن كنوع من ضرب المصفوفة والمتجه، باستخدام مصفوفة لها تأثير قياس أي متجه بمعامل لامدا.",
  "model": "google_nmt",
  "from_community_srt": "لذلك دعونا نبدأ بإعادة كتابة ذلك الجانب الأيمن كنوع من مضاعفة متجهات المصفوفة ، باستخدام مصفوفة ، والتي لها تأثير تحجيم أي ناقل بواسطة عامل λ.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "ستمثل أعمدة هذه المصفوفة ما يحدث لكل متجه أساسي، ويتم ببساطة ضرب كل متجه أساسي في لامدا، لذلك سيكون لهذه المصفوفة رقم لامدا أسفل القطر، مع وجود أصفار في كل مكان آخر.",
  "model": "google_nmt",
  "from_community_srt": "سوف تمثل أعمدة مثل هذه المصفوفة ما يحدث لكل ناقل أساس ، وكل ناقل أساس هو ببساطة λ ، لذا فإن هذا المصفوفة سيكون لها الرقم λ أسفل القطر مع الصفر في كل مكان آخر.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "الطريقة الشائعة لكتابة هذا الرجل هي تحليل لامدا وكتابتها كـ لامدا في i، حيث i هي مصفوفة الهوية مع 1s أسفل القطر.",
  "model": "google_nmt",
  "from_community_srt": "الطريقة الشائعة لكتابة هذا الشخص هي أن تدرج ذلك وتكتبه كـ λ الأوقات I ، أين أنا مصفوفة الهوية مع 1 أسفل القطري.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "بما أن كلا الطرفين يشبهان ضرب المصفوفة والمتجه، فيمكننا طرح الجانب الأيمن وإخراج عامل v.",
  "model": "google_nmt",
  "from_community_srt": "مع النظر إلى كلا الجانبين مثل مضاعفة متجهات المصفوفة ، يمكننا طرح هذا الجانب الأيمن وعامل v.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "إذًا ما لدينا الآن هو مصفوفة جديدة، A ناقص لامدا مضروبًا في الهوية، ونحن نبحث عن متجه v بحيث تعطي هذه المصفوفة الجديدة مضروبًا في v المتجه صفرًا.",
  "model": "google_nmt",
  "from_community_srt": "إذاً ، ما لدينا الآن هو مصفوفة جديدة - A ناقص the أضعاف الهوية ، ونحن نبحث عن v المتجه ، بحيث تعطي هذه المصفوفة الجديدة v المتجه صفر.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "الآن، سيكون هذا صحيحًا دائمًا إذا كان v نفسه هو المتجه الصفري، لكن هذا ممل.",
  "model": "google_nmt",
  "from_community_srt": "الآن سيظل هذا صحيحًا دائمًا إذا كانت v هي المتجه صفر ، لكن هذا ممل.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "ما نريده هو متجه ذاتي غير صفري.",
  "model": "google_nmt",
  "from_community_srt": "ما نريده هو aigenvector غير الصفر.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "وإذا شاهدت الفصلين 5 و6، ستعرف أن الطريقة الوحيدة التي يمكن أن يصبح بها حاصل ضرب مصفوفة ذات متجه غير صفري صفرًا هي أن يؤدي التحويل المرتبط بتلك المصفوفة إلى سحق الفضاء إلى بُعد أقل.",
  "model": "google_nmt",
  "from_community_srt": "وإذا كنت تشاهد الفصلين الخامس والسادس ، ستعرف أن الطريقة الوحيدة الممكنة لمنتج مصفوفة ذات ناقل غير صفري لتصبح صفرًا إذا كان التحويل المرتبط بهذه المصفوفة يسحق الفضاء في بُعد أقل.",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "وهذا السحق يتوافق مع المحدد الصفري للمصفوفة.",
  "model": "google_nmt",
  "from_community_srt": "وهذا الاستنزال يقابله محدد صفري للمصفوفة.",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "لنكون واقعيين، لنفترض أن المصفوفة A تحتوي على أعمدة 2، 1 و2، 3، وفكر في طرح مبلغ متغير، لامدا، من كل مدخل قطري.",
  "model": "google_nmt",
  "from_community_srt": "لكي تكون ملموسًا ، لنفترض أن المصفوفة تحتوي على أعمدة (2 ، 1) و (2 ، 3) ، والتفكير في طرح كمية متغيرة λ من كل إدخال قطري.",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "الآن تخيل التغيير والتبديل في لامدا، وتدوير المقبض لتغيير قيمته.",
  "model": "google_nmt",
  "from_community_srt": "الآن تخيل التغيير والتبديل ، وتحويل مقبض لتغيير قيمته.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "ومع تغير قيمة لامدا، تتغير المصفوفة نفسها، وبالتالي يتغير محدد المصفوفة.",
  "model": "google_nmt",
  "from_community_srt": "حيث أن قيمة التغييرات، المصفوفة نفسها تتغير ، وبالتالي فإن محدد المصفوفة يتغير.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "الهدف هنا هو العثور على قيمة لامدا التي تجعل هذا المحدد صفرًا، مما يعني أن التحويل المعدل يسحق الفضاء إلى بُعد أقل.",
  "model": "google_nmt",
  "from_community_srt": "الهدف هنا هو إيجاد قيمة λ التي ستجعل هذا العزم صفر ، مما يعني أن التحويل المعدّل يسحق الفضاء في بُعد أقل.",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "في هذه الحالة، تأتي النقطة المثالية عندما تساوي لامدا 1.",
  "model": "google_nmt",
  "from_community_srt": "في هذه الحالة ، تأتي النقطة الحلوة عندما يساوي λ 1.",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "بالطبع، إذا اخترنا مصفوفة أخرى، فقد لا تكون القيمة الذاتية بالضرورة 1.",
  "model": "google_nmt",
  "from_community_srt": "بالطبع ، إذا اخترنا بعض المصفوفات الأخرى ، قد لا تكون قيمة eigenvalue بالضرورة 1 ، فقد يتم ضرب البقعة الحلوة بعض القيمة الأخرى لـ λ.",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "قد يتم ضرب النقطة الحلوة بقيمة أخرى من لامدا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "إذن هذا كثير نوعًا ما، لكن دعونا نكشف ما يقوله هذا.",
  "model": "google_nmt",
  "from_community_srt": "إذن هذا نوع كثير ، لكن دعنا نفكك ما يقوله هذا.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "عندما تساوي لامدا 1، فإن المصفوفة A ناقص لامدا مضروبة في الهوية تسحق المساحة على الخط.",
  "model": "google_nmt",
  "from_community_srt": "عندما تساوي λ 1 ، فإن المصفوفة A ناقص the تضغط الهوية على الفضاء على خط.",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "هذا يعني أن هناك متجهًا غير صفري v بحيث يكون A ناقص lambda مضروبًا في الهوية مضروبًا في v يساوي المتجه الصفري.",
  "model": "google_nmt",
  "from_community_srt": "هذا يعني أن هناك متجه غير صفري v ، بحيث يكون ناقص the أضعاف مرات الهوية v مساوية للناقل صفر.",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "وتذكر أن سبب اهتمامنا بهذا هو أنه يعني A في v يساوي lambda في v، وهو ما يمكنك قراءته كقول إن المتجه v هو متجه ذاتي لـ A، ويظل في امتداده الخاص أثناء التحويل A.",
  "model": "google_nmt",
  "from_community_srt": "وتذكر أن سبب اهتمامنا بذلك هو أنه يعني أن الأوقات v تساوي λ المرات v ، والتي يمكنك قراءتها كقول أن المتجه v هو عامل ملاحي من A ، البقاء على المدى الخاص بها أثناء التحول A.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "في هذا المثال، القيمة الذاتية المقابلة هي 1، لذا فإن v ستبقى ثابتة في مكانها.",
  "model": "google_nmt",
  "from_community_srt": "في هذا المثال ، تكون قيمة eigenvalue المقابلة هي 1 ، لذا فإن v سيكون في الواقع ثابتًا في مكانه.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "توقف مؤقتًا وتأمل إذا كنت تريد التأكد من أن هذا النوع من التفكير يبدو جيدًا.",
  "model": "google_nmt",
  "from_community_srt": "وقفة والتأمل إذا كنت بحاجة للتأكد من أن خط التفكير يبدو جيدا.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "وهذا هو النوع الذي ذكرته في المقدمة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "إذا لم يكن لديك فهم قوي للمحددات وسبب ارتباطها بأنظمة المعادلات الخطية التي لها حلول غير صفرية، فإن تعبيرًا مثل هذا سيبدو غريبًا تمامًا.",
  "model": "google_nmt",
  "from_community_srt": "هذا هو الشيء الذي ذكرته في المقدمة ، إذا لم يكن لديك فهم قوي للمحددات ولماذا تتعلق بنظم خطية من المعادلات التي لها حلول غير صفرية ، مثل هذا التعبير من شأنه أن يشعر تماما من فراغ.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "لرؤية ذلك عمليًا، دعنا نعيد النظر في المثال من البداية، مع مصفوفة أعمدتها هي 3، 0 و1، 2.",
  "model": "google_nmt",
  "from_community_srt": "لرؤية هذا في العمل ، دعنا نرجع إلى المثال من البداية مع المصفوفة التي تكون أعمدةها (3 ، 0) و (1 ، 2).",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "لمعرفة ما إذا كانت قيمة لامدا هي قيمة ذاتية، اطرحها من أقطار هذه المصفوفة واحسب المحدد.",
  "model": "google_nmt",
  "from_community_srt": "لمعرفة ما إذا كانت القيمة λ هي قيمة ذاتية ، طرح من الأقطار من هذه المصفوفة وحساب المحدد.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "من خلال القيام بذلك، نحصل على كثيرة حدود تربيعية معينة في لامدا، 3 ناقص لامدا في 2 ناقص لامدا.",
  "model": "google_nmt",
  "from_community_srt": "عند القيام بذلك ، نحصل على بعض الحدود المتعددة التربيعية في λ ، (3-λ) (2-λ).",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "نظرًا لأن لامدا لا يمكن أن تكون قيمة ذاتية إلا إذا كان هذا المحدد صفرًا، فيمكنك استنتاج أن القيم الذاتية الوحيدة الممكنة هي لامدا تساوي 2 ولامدا تساوي 3.",
  "model": "google_nmt",
  "from_community_srt": "بما أن λ لا يمكن أن تكون سوى قيمة ذاتية إذا كان هذا المحدد هو صفر ، يمكنك أن تستنتج أن القيم الذاتية الوحيدة الممكنة هي λ يساوي 2 و λ تساوي 3.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "لمعرفة المتجهات الذاتية التي لها بالفعل إحدى هذه القيم الذاتية، لنفترض أن لامدا تساوي 2، قم بتوصيل قيمة لامدا هذه إلى المصفوفة ثم حدد المتجهات التي ترسلها هذه المصفوفة المعدلة قطريًا إلى الصفر.",
  "model": "google_nmt",
  "from_community_srt": "لمعرفة ما هي العوامل الذاتية التي لديها في الواقع واحدة من هذه القيم الذاتية ، يقول λ يساوي 2 ، قم بتوصيل تلك القيمة بـ λ إلى المصفوفة ثم حل لأي نواقل ترسل هذه المصفوفة المتغيرة مائلًا إلى 0.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "إذا قمت بحساب ذلك بنفس الطريقة التي تقوم بها بأي نظام خطي آخر، فسترى أن الحلول هي جميع المتجهات على الخط القطري الممتد بسالب 1، 1.",
  "model": "google_nmt",
  "from_community_srt": "إذا قمت بحساب هذه الطريقة التي تريد بها أي نظام خطي آخر ، سترى أن الحلول هي جميع المتجهات على خط قطري يمتد من (-1 ، 1).",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "وهذا يتوافق مع حقيقة أن المصفوفة غير المعدلة، 3، 0، 1، 2، لها تأثير على تمديد كل تلك المتجهات بعامل 2.",
  "model": "google_nmt",
  "from_community_srt": "وهذا يتوافق مع حقيقة أن المصفوفة غير المعدلة [(3 ، 0) ، (1 ، 2)] لديه تأثير لتمديد كل تلك المتجهات بعامل 2.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "الآن، ليس من الضروري أن يحتوي التحويل ثنائي الأبعاد على متجهات ذاتية.",
  "model": "google_nmt",
  "from_community_srt": "الآن ، لا يجب أن يكون للتحول ثنائي البعد متواجدة.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "على سبيل المثال، النظر في دوران بمقدار 90 درجة.",
  "model": "google_nmt",
  "from_community_srt": "على سبيل المثال ، ضع في الاعتبار دوران بمقدار 90 درجة.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "لا يحتوي هذا على أي متجهات ذاتية لأنه يقوم بتدوير كل متجه خارج نطاقه.",
  "model": "google_nmt",
  "from_community_srt": "هذا لا يملك أي متابعين ، لأنه يدور كل ناقل من نطاقه الخاص.",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "إذا حاولت بالفعل حساب القيم الذاتية لدورة كهذه، لاحظ ما يحدث.",
  "model": "google_nmt",
  "from_community_srt": "إذا كنت تحاول بالفعل حساب قيم eigenvalues ​​لدوران كهذا ، فلاحظ ما يحدث.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "تحتوي المصفوفة على أعمدة 0، 1 وسالب 1، 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "اطرح لامدا من العناصر القطرية وابحث عن الوقت الذي يكون فيه المحدد صفرًا.",
  "model": "google_nmt",
  "from_community_srt": "تحتوي المصفوفة على أعمدة (0 ، 1) و (-1 ، 0) ، طرح λ من العناصر القطرية والبحث عن عندما يكون المحدد 0.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "في هذه الحالة، تحصل على كثيرة الحدود لامدا تربيع زائد 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "الجذور الوحيدة لذلك كثير الحدود هي الأعداد التخيلية، i والسالب i.",
  "model": "google_nmt",
  "from_community_srt": "في هذه الحالة ، تحصل على كثير الحدود λ ^ 2 + 1 ، والجذور الوحيدة لهذا متعدد الحدود هي الأرقام الخيالية i و i.",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "تشير حقيقة عدم وجود حلول للأعداد الحقيقية إلى عدم وجود متجهات ذاتية.",
  "model": "google_nmt",
  "from_community_srt": "تشير حقيقة عدم وجود حلول رقمية حقيقية إلى عدم وجود أي حركات ذاتية.",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "مثال آخر مثير للاهتمام يستحق الاحتفاظ به في الجزء الخلفي من عقلك هو القص.",
  "model": "google_nmt",
  "from_community_srt": "مثال آخر مثير للاهتمام يستحق الإمساك في الجزء الخلفي من عقلك هو القص.",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "يؤدي هذا إلى تثبيت i-hat في مكانه وتحريك j-hat 1، بحيث تحتوي المصفوفة على أعمدة 1 و0 و1 و1.",
  "model": "google_nmt",
  "from_community_srt": "يعمل هذا على إصلاح قبعة i-hat في مكانها وتحريك j-hat one over over، لذلك لها المصفوفة أعمدة (1 ، 0) و (1 ، 1).",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "جميع المتجهات الموجودة على المحور السيني هي متجهات ذاتية ذات قيمة ذاتية 1 لأنها تظل ثابتة في مكانها.",
  "model": "google_nmt",
  "from_community_srt": "جميع المتجهات على المحور السيني عبارة عن متجهات ذاتية ذات قيمة ذاتية 1 ، حيث تظل ثابتة في مكانها.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "في الواقع، هذه هي المتجهات الذاتية الوحيدة.",
  "model": "google_nmt",
  "from_community_srt": "في الواقع ، هذه هي eigenvectors الوحيدة.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "عندما تطرح لامدا من الأقطار وتحسب المحدد، فإن ما تحصل عليه هو 1 ناقص لامدا تربيع.",
  "model": "google_nmt",
  "from_community_srt": "عندما تقوم بطرح λ من الأقطار وحساب المحدد ، ما تحصل عليه هو (1-λ) ^ 2 ، والجذر الوحيد لهذا التعبير هو λ يساوي 1.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "والجذر الوحيد لهذا التعبير هو لامدا يساوي 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "وهذا يتماشى مع ما نراه هندسيًا، وهو أن جميع المتجهات الذاتية لها قيمة ذاتية 1.",
  "model": "google_nmt",
  "from_community_srt": "يتطابق هذا مع ما نراه هندسيًا من أن جميع المتجهات الذاتية لها قيمة ذاتية 1.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "مع ذلك، ضع في اعتبارك أنه من الممكن أيضًا أن يكون لديك قيمة ذاتية واحدة فقط، ولكن مع أكثر من مجرد سطر مليء بالمتجهات الذاتية.",
  "model": "google_nmt",
  "from_community_srt": "ضع في اعتبارك رغم ذلك ، من الممكن أيضًا أن تكون لديك قيمة ذاتية واحدة فقط ، ولكن مع أكثر من مجرد خط مليء بالمفاهيم الذاتية.",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "مثال بسيط هو المصفوفة التي تقيس كل شيء بمقدار 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "القيمة الذاتية الوحيدة هي 2، لكن كل متجه في المستوى يجب أن يكون متجهًا ذاتيًا بهذه القيمة الذاتية.",
  "model": "google_nmt",
  "from_community_srt": "مثال بسيط هو مصفوفة تقاس كل شيء بمقدار 2 ، قيمة eigenvalue الوحيدة هي 2 ، لكن كل متجه في المستوي يصبح متجاوراً ذا قيمة eigenvalue.",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "الآن هو وقت مناسب آخر للتوقف والتأمل في بعض هذه الأمور قبل أن أنتقل إلى الموضوع الأخير.",
  "model": "google_nmt",
  "from_community_srt": "الآن هو وقت آخر جيد للتوقف والتأمل في بعض من هذا قبل الانتقال إلى الموضوع الأخير.",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "أريد أن أنهي هنا بفكرة الأساس الذاتي، الذي يعتمد بشكل كبير على أفكار من الفيديو الأخير.",
  "model": "google_nmt",
  "from_community_srt": "أريد أن أنتهي هنا بفكرة وجود eigenbasis ، التي تعتمد بشكل كبير على أفكار من الفيديو الأخير.",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "ألقِ نظرة على ما يحدث إذا كانت المتجهات الأساسية هي متجهات ذاتية.",
  "model": "google_nmt",
  "from_community_srt": "ألقِ نظرة على ما يحدث إذا حدث أن تمكَّن لنا المتجهات الأساسية من أن تكون مجرد متجهات ذاتية.",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "على سبيل المثال، ربما يتم تحجيم i-hat بمقدار سالب 1 ويتم تحجيم j-hat بمقدار 2.",
  "model": "google_nmt",
  "from_community_srt": "على سبيل المثال ، ربما يتم تحجيم i-hat بمقدار -1 ويتم تحجيم j-hat بمقدار 2.",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "عند كتابة إحداثياتهم الجديدة كأعمدة مصفوفة، لاحظ أن تلك المضاعفات العددية، سالب 1 و2، وهي القيم الذاتية لـ i-hat وj-hat، تقع على قطري المصفوفة، وكل إدخال آخر هو 0 .",
  "model": "google_nmt",
  "from_community_srt": "كتابة إحداثياتهم الجديدة كأعمدة مصفوفة ، لاحظ أن هذه مضاعفات العددية -1 و 2 ، والتي هي القيم الذاتية من i-hat و j-hat ، الجلوس على قطري لدينا المصفوفة وكل إدخال آخر هو 0.",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "في أي وقت تحتوي المصفوفة على أصفار في كل مكان غير القطر، يطلق عليها، بشكل معقول، مصفوفة قطرية.",
  "model": "google_nmt",
  "from_community_srt": "في أي وقت ، تحتوي المصفوفة على صفر في كل مكان بخلاف القطر ، يطلق عليه ، بشكل معقول بما فيه الكفاية ، مصفوفة قطرية.",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "وطريقة تفسير ذلك هي أن جميع المتجهات الأساسية هي متجهات ذاتية، والمدخلات القطرية لهذه المصفوفة هي قيمها الذاتية.",
  "model": "google_nmt",
  "from_community_srt": "والطريقة لتفسير هذا هو أن جميع المتجهات الأساسية هي متجهات ذاتية ، مع الإدخالات القطرية لهذه المصفوفة كونها قيمها الذاتية.",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "هناك الكثير من الأشياء التي تجعل التعامل مع المصفوفات القطرية أفضل بكثير.",
  "model": "google_nmt",
  "from_community_srt": "هناك الكثير من الأشياء التي تجعل المصفوفات المائلة أكثر جاذبية للعمل بها.",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "أحد أهمها هو أنه من الأسهل حساب ما سيحدث إذا قمت بضرب هذه المصفوفة في نفسها عدة مرات.",
  "model": "google_nmt",
  "from_community_srt": "واحد كبير هو ذلك من الأسهل حساب ما سيحدث إذا ضربت هذه المصفوفة بذاتها مجموعة من المرات.",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "نظرًا لأن كل ما تفعله إحدى هذه المصفوفات هو قياس كل متجه أساسي بواسطة بعض القيمة الذاتية، فإن تطبيق هذه المصفوفة عدة مرات، على سبيل المثال 100 مرة، سيتوافق فقط مع قياس كل متجه أساسي بمقدار الأس 100 من القيمة الذاتية المقابلة.",
  "model": "google_nmt",
  "from_community_srt": "بما أن كل واحدة من هذه المصفوفات هي مقياس كل ناقل أساس بواسطة بعض القيم الذاتية ، تطبيق ذلك المصفوفة عدة مرات ، ولنقل 100 مرة ، سوف يتطابق فقط مع توسيع نطاق كل ناقل أساس بواسطة القوة 100 في القيمة الذاتية المقابلة.",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "في المقابل، حاول حساب القوة المائة لمصفوفة غير قطرية.",
  "model": "google_nmt",
  "from_community_srt": "على النقيض من ذلك ، جرب استخدام القوة المئة عشر لمصفوفة غير قطرية.",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "حقا، حاول ذلك للحظة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "انه كابوس.",
  "model": "google_nmt",
  "from_community_srt": "حقا ، جربها للحظة ، إنه كابوس.",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "بالطبع، نادرًا ما تكون محظوظًا لأن تكون المتجهات الأساسية الخاصة بك هي أيضًا متجهات ذاتية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "لكن إذا كان تحويلك يحتوي على الكثير من المتجهات الذاتية، مثل تلك الموجودة في بداية هذا الفيديو، بما يكفي بحيث يمكنك اختيار مجموعة تغطي المساحة الكاملة، فيمكنك تغيير نظام الإحداثيات الخاص بك بحيث تكون هذه المتجهات الذاتية هي المتجهات الأساسية.",
  "model": "google_nmt",
  "from_community_srt": "وبالطبع ، نادرًا ما تكون محظوظًا جدًا لأن تكون متجهاتك الأساسية هي أيضًا متجهات ذاتية ، ولكن إذا كان للتحول لديك الكثير من العوامل الذاتية ، مثل ذلك الذي يحدث في بداية هذا الفيديو ، بما فيه الكفاية بحيث يمكنك اختيار مجموعة تغطي المساحة الكاملة ، ثم يمكنك تغيير نظام الإحداثيات الخاص بك بحيث تكون هذه الموجات الذاتية هي المتجهات الأساسية الخاصة بك.",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "لقد تحدثت عن تغيير الأساس في الفيديو الأخير، لكنني سأقوم بتذكير سريع جدًا هنا بكيفية التعبير عن التحويل المكتوب حاليًا في نظامنا الإحداثي إلى نظام مختلف.",
  "model": "google_nmt",
  "from_community_srt": "تحدثت عن تغيير الفيديو الأخير أساس ، لكنني سأذهب من خلال تذكير سريع جدًا هنا كيفية التعبير عن تحويل مكتوب حاليًا في نظام الإحداثيات الخاص بنا إلى نظام مختلف.",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "خذ إحداثيات المتجهات التي تريد استخدامها كأساس جديد، وهو ما يعني في هذه الحالة المتجهات الذاتية لدينا، ثم اجعل تلك الإحداثيات أعمدة مصفوفة، تُعرف باسم مصفوفة تغيير الأساس.",
  "model": "google_nmt",
  "from_community_srt": "خذ إحداثيات المتجهات التي تريد استخدامها كأساس جديد ، والتي ، في هذه الحالة ، تعني اثنين من المتجهات الذاتية ، التي تجعل هذه الإحداثيات أعمدة مصفوفة ، والمعروفة باسم تغيير مصفوفة الأساس.",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "عندما تقوم بحصر التحويل الأصلي، واضعًا تغيير مصفوفة الأساس على يمينه ومعكوس تغيير مصفوفة الأساس على يساره، ستكون النتيجة مصفوفة تمثل نفس التحويل، ولكن من منظور متجهات الأساس الجديدة تنسق نظام.",
  "model": "google_nmt",
  "from_community_srt": "عند شطيرة التحول الأصلي وضع تغيير مصفوفة الأساس على حق وعكس تغيير مصفوفة الأساس على يساره ، ستكون النتيجة مصفوفة تمثل هذا التحول نفسه ، ولكن من منظور نظام تنسيق ناقلات أساس جديد.",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "بيت القصيد من القيام بذلك مع المتجهات الذاتية هو أن هذه المصفوفة الجديدة مضمونة أن تكون قطرية مع قيمها الذاتية المقابلة أسفل هذا القطر.",
  "model": "google_nmt",
  "from_community_srt": "بيت القصيد من القيام بذلك مع eigenvectors هو ذلك مضمونة هذه المصفوفة الجديدة لتكون قطري مع قيم eigen المقابلة لها أسفل هذا القطري.",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "وذلك لأنه يمثل العمل في نظام إحداثي حيث ما يحدث للمتجهات الأساسية هو أنه يتم قياسها أثناء التحويل.",
  "model": "google_nmt",
  "from_community_srt": "هذا لأنه يمثل العمل في نظام الإحداثيات حيث يحدث ما يحدث للمتجهات الأساسية التي يتم تحجيمها أثناء التحويل.",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "مجموعة من المتجهات الأساسية والتي هي أيضًا متجهات ذاتية تسمى، مرة أخرى، بشكل معقول، الأساس الذاتي.",
  "model": "google_nmt",
  "from_community_srt": "مجموعة من المتجهات الأساسية ، والتي هي أيضا ناقلات ، يسمى ، مرة أخرى ، بشكل معقول بما فيه الكفاية ، \"eigenbasis\".",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "لذا، على سبيل المثال، إذا كنت بحاجة إلى حساب القوة رقم 100 لهذه المصفوفة، فسيكون من الأسهل كثيرًا التحويل إلى الأساس الذاتي، وحساب القوة رقم 100 في هذا النظام، ثم التحويل مرة أخرى إلى نظامنا القياسي.",
  "model": "google_nmt",
  "from_community_srt": "إذا ، على سبيل المثال ، إذا كنت بحاجة إلى حساب القوة المائة عشر لهذه المصفوفة ، سيكون من الأسهل بكثير أن تتغير إلى أيغبار ، حساب القوة ال 100 في هذا النظام ، ثم تحويل إلى نظامنا القياسي.",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "لا يمكنك القيام بذلك مع كل التحولات.",
  "model": "google_nmt",
  "from_community_srt": "لا يمكنك فعل هذا مع كل التحولات.",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "القص، على سبيل المثال، لا يحتوي على ما يكفي من المتجهات الذاتية لتغطية المساحة الكاملة.",
  "model": "google_nmt",
  "from_community_srt": "على سبيل المثال ، لا يحتوي القص على ما يكفي من العناصر الذاتية لإمتداد المساحة الكاملة.",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "ولكن إذا تمكنت من العثور على الأساس الذاتي، فهذا يجعل عمليات المصفوفة رائعة حقًا.",
  "model": "google_nmt",
  "from_community_srt": "ولكن إذا كنت تستطيع إيجاد eigenbasis ، فإنه يجعل عمليات المصفوفة جميلة حقا.",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "لأولئك منكم الذين يرغبون في حل لغز أنيق جدًا لمعرفة كيف يبدو هذا أثناء العمل وكيف يمكن استخدامه لتحقيق بعض النتائج المدهشة، سأترك مطالبة هنا على الشاشة.",
  "model": "google_nmt",
  "from_community_srt": "لأولئك منكم على استعداد للعمل من خلال لغز أنيق جدا لنرى كيف يبدو هذا في العمل وكيف يمكن استخدامه لإنتاج بعض النتائج المفاجئة ، سأترك موجه هنا على الشاشة.",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "يستغرق الأمر القليل من العمل، ولكن أعتقد أنك ستستمتع به.",
  "model": "google_nmt",
  "from_community_srt": "يتطلب الأمر بعضًا من العمل ، ولكنني أعتقد أنك ستستمتع به.",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "الفيديو التالي والأخير من هذه السلسلة سيكون عن المساحات المتجهة المجردة.",
  "model": "google_nmt",
  "from_community_srt": "سيكون الفيديو التالي والأخير من هذه السلسلة على مساحات ناقلات مجردة.",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]