[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive. ",
  "translatedText": "特征向量和特征值是许多学生 认为特别不直观的主题之一。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Things like, why are we doing this, and what does this actually mean, are too often left just floating away in an unanswered sea of computations. ",
  "translatedText": "诸如我们为什么要这样做以及这实际上意味着什 么之类的事情常常被留在未解答的计算海洋中。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular. ",
  "translatedText": "当我发布这个系列的视频时，你们中的很 多人都表示特别期待可视化这个主题。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained. ",
  "translatedText": "我怀疑其原因并不在于本 征特别复杂或解释不清。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it. ",
  "translatedText": "事实上，它相对简单，我认为大 多数书籍都很好地解释了它。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "What I want to do is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it. ",
  "translatedText": "我想做的是，只有当您对前面的许多主题 有扎实的视觉理解时，它才真正有意义。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis. ",
  "translatedText": "这里最重要的是，您知道如何将矩阵 视为线性变换，但您还需要熟悉行列 式、线性方程组和基础变化等内容。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves. ",
  "translatedText": "对特征值的混淆通常更多地与这些主题之一的基础不 稳定有关，而不是与特征向量和特征值本身有关。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here. ",
  "translatedText": "首先，考虑一些二维线性变换，如下所示。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2. ",
  "translatedText": "它将基向量 i-hat 移动到坐标 3、0，将 j-hat 移动到坐标 1、2。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2. ",
  "translatedText": "所以它用一个列为 3, 0 和 1, 2 的矩阵来表示。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip. ",
  "translatedText": "专注于它对一个特定向量的作用，并考虑该 向量的跨度，即穿过其原点和尖端的线。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation. ",
  "translatedText": "大多数向量在转换过程中都会失去其跨度。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line. ",
  "translatedText": "我的意思是，如果矢量降落的地方也恰好在 那条线上的某个地方，那看起来很巧合。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar. ",
  "translatedText": "但一些特殊向量确实保留在它们自己的跨度上，这意味着矩 阵对此类向量的影响只是拉伸或压缩它，就像标量一样。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector. ",
  "translatedText": "对于这个具体示例，基向量 i-hat 就是这样一个特殊向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis. ",
  "translatedText": "i-hat 的跨度是 x 轴，从矩阵的第一列，我们可以 看到 i-hat 移动了 3 倍，仍然在该 x 轴上。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span. ",
  "translatedText": "更重要的是，由于线性变换的工作方式，x 轴上的任何其他 向量也只是拉伸了 3 倍，因此保持在其自己的跨度上。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1. ",
  "translatedText": "在此转换过程中，一个稍微狡猾的向量保 持在其自己的跨度上，即负 1, 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2. ",
  "translatedText": "它最终会被拉伸 2 倍。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2. ",
  "translatedText": "再说一次，线性意味着这个家伙跨越的对角 线上的任何其他向量都会被拉伸 2 倍。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span. ",
  "translatedText": "对于这种变换，这些都是具有保持 在其跨度上的特殊属性的向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2. ",
  "translatedText": "x 轴上的那些被拉伸了 3 倍，而 该对角线上的那些被拉伸了 2 倍。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans. ",
  "translatedText": "任何其他向量都会在变换过程中发生一定 程度的旋转，从它所跨越的直线上移开。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation. ",
  "translatedText": "正如您现在可能已经猜到的那样，这些特殊向量称为 变换的特征向量，每个特征向量都与所谓的特征值相 关联，特征值只是在变换过程中拉伸或压缩的因素。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing or the fact that these eigenvalues happen to be positive. ",
  "translatedText": "当然，拉伸与压缩或者这些特征值恰 好为正的事实并没有什么特别的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half. ",
  "translatedText": "在另一个示例中，您可能有一个特征值负 1 一半的特 征向量，这意味着该向量被翻转并压缩了 1 一半。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it. ",
  "translatedText": "但这里重要的部分是它保持在它所 延伸的线上而不会旋转离开它。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation. ",
  "translatedText": "为了了解为什么这可能是一个值得思考 的有用的事情，请考虑一些三维旋转。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation. ",
  "translatedText": "如果您可以找到该旋转的特征向量（一个保持在其 自身跨度上的向量），那么您找到的就是旋转轴。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3 by 3 matrix associated with that transformation. ",
  "translatedText": "从某个旋转轴和旋转角度来考虑 3D 旋转要容易得多，而不是考虑与该变 换相关的完整 3 x 3 矩阵。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same. ",
  "translatedText": "顺便说一句，在这种情况下，相应的特征值必须为 1，因为旋 转永远不会拉伸或挤压任何东西，因此向量的长度将保持不变。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra. ",
  "translatedText": "这种模式在线性代数中经常出现。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors. ",
  "translatedText": "对于矩阵描述的任何线性变换，您可以通过读取该 矩阵的列作为基向量的着陆点来理解它在做什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues. ",
  "translatedText": "但通常，了解线性变换实际作用的核心（较少依赖于 特定坐标系）的更好方法是找到特征向量和特征值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding. ",
  "translatedText": "我不会在这里介绍计算特征向量和特征 值的方法的完整细节，但我将尝试概 述对于概念理解最重要的计算思想。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like. ",
  "translatedText": "象征性地，这就是特征向量的概念。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue. ",
  "translatedText": "A是表示某种变换的矩阵，以v为特征向量， lambda是一个数字，即对应的特征值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda. ",
  "translatedText": "该表达式的意思是，矩阵向量乘积 A 乘以 v 所得到的结果与仅将 特征向量 v 按某个值 lambda 进行缩放得到的结果相同。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true. ",
  "translatedText": "因此，查找矩阵 A 的特征向量及其特征值可以归结为 查找使该表达式成立的 v 和 lambda 的值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication. ",
  "translatedText": "一开始使用起来有点尴尬，因为左边代表矩阵 向量乘法，但这里的右边是标量向量乘法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda. ",
  "translatedText": "因此，让我们首先将右侧重写为某种矩阵向量乘法，使用 具有将任何向量缩放 lambda 倍的效果的矩阵。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else. ",
  "translatedText": "这样的矩阵的列将表示每个基向量发生的情况，并且每 个基向量只需乘以 lambda，因此该矩阵将在 对角线上具有数字 lambda，其他位置为零。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with ones down the diagonal. ",
  "translatedText": "写这个家伙的常见方法是将 lambda 分解出来，并将其写为 l ambda 乘以 i，其中 i 是单位矩阵，对角线下方有 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v. ",
  "translatedText": "由于两边看起来都像矩阵向量乘法， 我们可以减去右边并分解出 v。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix, times v, gives the zero vector. ",
  "translatedText": "所以我们现在有了一个新矩阵，A 减去 lambda 乘以恒等式， 我们正在寻找一个向量 v，使得这个新矩阵乘以 v 给出零向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring. ",
  "translatedText": "现在，如果 v 本身是零向量，这总是正确的，但这很无聊。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector. ",
  "translatedText": "我们想要的是一个非零特征向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension. ",
  "translatedText": "如果你看了第 5 章和第 6 章，你就会知道 ，矩阵与非零向量的乘积可能变为零的唯一方法是 与该矩阵相关的变换将空间压缩到较低的维度。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix. ",
  "translatedText": "这种压缩对应于矩阵的零行列式。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry. ",
  "translatedText": "具体来说，假设您的矩阵 A 具有第 2、1 列和第 2、 3 列，并考虑从每个对角线条目中减去变量 lambda。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value. ",
  "translatedText": "现在想象一下调整 lambda，转动旋钮来改变它的值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes. ",
  "translatedText": "当 lambda 值发生变化时，矩阵本身也 会发生变化，因此矩阵的行列式也会发生变化。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension. ",
  "translatedText": "这里的目标是找到一个 lambda 值，使行列式为 零，这意味着调整后的变换将空间压缩到较低的维度。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1. ",
  "translatedText": "在这种情况下，当 lambda 等于 1 时，最佳点就出现了。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1. ",
  "translatedText": "当然，如果我们选择了其他矩阵，特征值可能不一定是1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda. ",
  "translatedText": "最佳点可能会在其他 lambda 值处达到。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying. ",
  "translatedText": "内容有点多，但让我们来解释一下这到底在说什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line. ",
  "translatedText": "当 lambda 等于 1 时，矩阵 A 减去 lambda 乘以恒等式会将空间压缩到一条线上。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector. ",
  "translatedText": "这意味着存在一个非零向量 v，使得 A 减去 lambda 乘以恒等式乘以 v 等于零向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A. ",
  "translatedText": "请记住，我们关心这一点的原因是因为它意味着 A 乘以 v 等于 lambda 乘以 v，您可以将其读作表示向量 v 是 A 的特征向量，在变换 A 期间保持其自己的跨度。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place. ",
  "translatedText": "在此示例中，相应的特征值为 1， 因此 v 实际上会保持固定不变。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good. ",
  "translatedText": "停下来思考一下你是否需要确保这种推理方式感觉良好。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction. ",
  "translatedText": "这就是我在引言中提到的事情。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue. ",
  "translatedText": "如果您没有充分掌握行列式以及为什么它们与具有非零解的线性 方程组相关，那么像这样的表达式会让人感觉完全出乎意料。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2. ",
  "translatedText": "为了查看其实际效果，让我们从头开始回顾一下示例 ，其中的矩阵的列为 3, 0 和 1, 2。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant. ",
  "translatedText": "要确定值 lambda 是否为特征值，请 从该矩阵的对角线中减去它并计算行列式。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda. ",
  "translatedText": "这样做，我们得到了 lambda 的某个二次多项式，即 3 减去 lambda 乘以 2 减去 lambda。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3. ",
  "translatedText": "由于只有当该行列式恰好为零时 lambda 才可能是特征值，因此您可以得出结 论，唯一可能的特征值是 lambda 等于 2 和 lambda 等于 3。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero. ",
  "translatedText": "要找出实际上具有这些特征值之一的特征向量，假设 l ambda 等于 2，请将 lambda 值代入 矩阵，然后求解该对角线更改的矩阵发送到零的向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1. ",
  "translatedText": "如果您按照任何其他线性系统的方式进行计算，您会发 现解是对角线上由负 1, 1 跨越的所有向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2. ",
  "translatedText": "这对应于以下事实：未改变的矩阵 3, 0, 1 , 2 具有将所有这些向量拉伸 2 倍的效果。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors. ",
  "translatedText": "现在，二维变换不必具有特征向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees. ",
  "translatedText": "例如，考虑旋转 90 度。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span. ",
  "translatedText": "它没有任何特征向量，因为它将每个向量旋转出自己的跨度。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens. ",
  "translatedText": "如果您实际上尝试像这样计算旋转的特征值，请注意会发生什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0. ",
  "translatedText": "它的矩阵有列 0、1 和负列 1、0。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero. ",
  "translatedText": "从对角线元素中减去 lambda 并查找行列式何时为零。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1. ",
  "translatedText": "在这种情况下，您将得到多项式 lambda 平方加 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i. ",
  "translatedText": "该多项式的唯一根是虚数 i 和负 i。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors. ",
  "translatedText": "没有实数解的事实表明不存在特征向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear. ",
  "translatedText": "另一个值得记住的非常有趣的例子是剪刀。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1. ",
  "translatedText": "这会将 i-hat 固定到位，并将 j-hat 1 移过去，因此其矩阵具有列 1, 0 和 1, 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place. ",
  "translatedText": "x 轴上的所有向量都是特征值为 1 的特征向量，因为它们保持固定位置。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors. ",
  "translatedText": "事实上，这些是唯一的特征向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared. ",
  "translatedText": "当您从对角线中减去 lambda 并计算行列式时 ，您得到的是 1 减去 lambda 的平方。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1. ",
  "translatedText": "该表达式的唯一根是 lambda 等于 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1. ",
  "translatedText": "这与我们在几何上看到的一致，即 所有特征向量都具有特征值 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors. ",
  "translatedText": "但请记住，也可以只有一个特征值， 但不仅仅是一行完整的特征向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2. ",
  "translatedText": "一个简单的例子是一个将所有内容缩放 2 的矩阵。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue. ",
  "translatedText": "唯一的特征值是 2，但平面中的每个 向量都是具有该特征值的特征向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic. ",
  "translatedText": "在我继续最后一个主题之前，现在是另 一个暂停并思考一些问题的好时机。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video. ",
  "translatedText": "我想以特征基的想法结束这里，它在很 大程度上依赖于上一个视频的想法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors. ",
  "translatedText": "看看如果我们的基向量恰好是特征向量会发生什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1, and j-hat is scaled by 2. ",
  "translatedText": "例如，也许 i-hat 按负 1 缩放，而 j-hat 按 2 缩放。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0. ",
  "translatedText": "将它们的新坐标写为矩阵的列，请注意那些标量倍数，负 1 和 2，它们是 i-hat 和 j-hat 的特征 值，位于矩阵的对角线上，并且每个其他条目都是 0 。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has 0s everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix, and the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues. ",
  "translatedText": "任何时候，一个矩阵除了对角线以外的所有位置都为 0 时，就足够合理地称为对角矩阵，解释这一点的方法是所有 基向量都是特征向量，该矩阵的对角项就是它们的特征值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with. ",
  "translatedText": "有很多东西可以让对角矩阵更容易使用。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times. ",
  "translatedText": "一个重要的问题是，如果将此矩阵与其自身相 乘很多次，则可以更轻松地计算会发生什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue. ",
  "translatedText": "由于所有这些矩阵的作用都是按某个特征值缩放每个基 向量，因此多次应用该矩阵（例如 100 次）就相 当于按相应特征值的 100 次方缩放每个基向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix. ",
  "translatedText": "相反，尝试计算非对角矩阵的 100 次方。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment. ",
  "translatedText": "真的，尝试一下。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare. ",
  "translatedText": "这是一场噩梦。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors. ",
  "translatedText": "当然，你很少会幸运地让你的基向量也是特征向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors. ",
  "translatedText": "但是，如果您的变换有很多特征向量（例如本视频开头的特征 向量），足以让您可以选择一组跨越整个空间的特征向量，那 么您可以更改坐标系，使这些特征向量成为您的基础向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system. ",
  "translatedText": "我在上一个视频中谈到了基础的变化，但 我将在这里快速提醒如何将当前在我们的 坐标系中编写的转换表达为不同的系统。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix. ",
  "translatedText": "将要用作新基的向量的坐标（在本例中指 的是我们的两个特征向量），然后将这些 坐标作为矩阵的列，称为基矩阵的变化。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system. ",
  "translatedText": "当您将原始变换夹在中间时，将基矩阵的 变化放在其右侧，将基矩阵的变化的逆放 在其左侧，结果将是表示相同变换的矩阵 ，但从新基向量坐标的角度来看系统。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal. ",
  "translatedText": "使用特征向量执行此操作的全部要点是，保证这个新 矩阵是对角线，其相应的特征值位于该对角线下方。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation. ",
  "translatedText": "这是因为它代表在坐标系中工作，其中基向 量发生的情况是它们在变换过程中被缩放。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis. ",
  "translatedText": "一组同时也是特征向量的基向量被合理地称为特征基。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system. ",
  "translatedText": "因此，例如，如果您需要计算该矩阵的 100 次方，则更改为特征基、计算该系统中的 100 次方，然后转换回我们的标准系统会容易得多。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations. ",
  "translatedText": "您无法对所有转换都做到这一点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space. ",
  "translatedText": "例如，剪切力没有足够的特征向量来跨越整个空间。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely. ",
  "translatedText": "但如果你能找到一个特征基，那么矩阵运算就会变得非常可爱。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen. ",
  "translatedText": "对于那些愿意完成一个相当简洁的谜题来看看它在 实际中是什么样子以及如何使用它来产生一些令人 惊讶的结果的人，我将在屏幕上留下一个提示。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it. ",
  "translatedText": "这需要一些工作，但我想你会喜欢它。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces. ",
  "translatedText": "本系列的下一个也是最后一个视频将介绍抽象向量空间。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1005.38
 },
 {
  "input": "See you then! ",
  "translatedText": "回头见！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.9,
  "end": 1006.12
 }
]