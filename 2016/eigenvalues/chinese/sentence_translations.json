[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive. ",
  "translatedText": "特征向量和特征值是许多学生 认为特别不直观的主题之一。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Things like, why are we doing this, and what does this actually mean, are too often left just floating away in an unanswered sea of computations. ",
  "translatedText": "诸如我们为什么要这样做以及这实际上意味着什 么之类的事情常常被留在未解答的计算海洋中。",
  "model": "google_nmt",
  "from_community_srt": "“特征向量与特征值”是许多学生认为非常不直观的一个话题 “为什么要这么做”以及“它究竟意味着什么”之类的问题 通常都淹没在计算的海洋中无人问津 在我放出这个视频系列之后 有很多人都在评论，",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular. ",
  "translatedText": "当我发布这个系列的视频时，你们中的很 多人都表示特别期待可视化这个主题。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained. ",
  "translatedText": "我怀疑其原因并不在于本 征特别复杂或解释不清。",
  "model": "google_nmt",
  "from_community_srt": "说特别期待这一部分的形象解释 我怀疑 原因并不在于特征的东西特别复杂或是缺乏说明 实际上，",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it. ",
  "translatedText": "事实上，它相对简单，我认为大 多数书籍都很好地解释了它。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "What I want to do is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it. ",
  "translatedText": "我想做的是，只有当您对前面的许多主题 有扎实的视觉理解时，它才真正有意义。",
  "model": "google_nmt",
  "from_community_srt": "相对而言它更加直接 而且我认为大部分的书也提供了良好的解释 问题在于 只有对之前讲的内容有充分的几何直观，",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis. ",
  "translatedText": "这里最重要的是，您知道如何将矩阵 视为线性变换，但您还需要熟悉行列 式、线性方程组和基础变化等内容。",
  "model": "google_nmt",
  "from_community_srt": "你才能真正理解它 这里最重要的部分是， 你需要了解如何将矩阵看作线性变换 但你也需要熟悉其他的内容 例如行列式、线性方程组和基变换 通常而言，",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves. ",
  "translatedText": "对特征值的混淆通常更多地与这些主题之一的基础不 稳定有关，而不是与特征向量和特征值本身有关。",
  "model": "google_nmt",
  "from_community_srt": "对特征的东西感到疑惑， 更多的是因为以上内容的薄弱基础 而不是在于特征向量与特征值本身 首先，",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here. ",
  "translatedText": "首先，考虑一些二维线性变换，如下所示。",
  "model": "google_nmt",
  "from_community_srt": "考虑二维空间中的某个线性变换 比如现在展示的这个 它将基向量i帽变换到坐标(3,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2. ",
  "translatedText": "它将基向量 i-hat 移动到坐标 3、0，将 j-hat 移动到坐标 1、2。",
  "model": "google_nmt",
  "from_community_srt": "0)， j帽变换到坐标(1,",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2. ",
  "translatedText": "所以它用一个列为 3, 0 和 1, 2 的矩阵来表示。",
  "model": "google_nmt",
  "from_community_srt": "2) 所以如果用矩阵来表达， 它的列就是(3, 0)和(1,",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip. ",
  "translatedText": "专注于它对一个特定向量的作用，并考虑该 向量的跨度，即穿过其原点和尖端的线。",
  "model": "google_nmt",
  "from_community_srt": "2) 我们关注它对一个特定向量的作用 并且考虑这个向量张成的空间，",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation. ",
  "translatedText": "大多数向量在转换过程中都会失去其跨度。",
  "model": "google_nmt",
  "from_community_srt": "也就是通过原点和向量尖端的直线 大部分向量在变换中都离开了其张成的空间 我的意思是，",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line. ",
  "translatedText": "我的意思是，如果矢量降落的地方也恰好在 那条线上的某个地方，那看起来很巧合。",
  "model": "google_nmt",
  "from_community_srt": "如果向量正好落在这条直线上，",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar. ",
  "translatedText": "但一些特殊向量确实保留在它们自己的跨度上，这意味着矩 阵对此类向量的影响只是拉伸或压缩它，就像标量一样。",
  "model": "google_nmt",
  "from_community_srt": "感觉更像是巧合 不过， 某些特殊向量的确留在它们张成的空间里 意味着矩阵对它的作用仅仅是拉伸或者压缩而已，",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector. ",
  "translatedText": "对于这个具体示例，基向量 i-hat 就是这样一个特殊向量。",
  "model": "google_nmt",
  "from_community_srt": "如同一个标量 在这个例子中，",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis. ",
  "translatedText": "i-hat 的跨度是 x 轴，从矩阵的第一列，我们可以 看到 i-hat 移动了 3 倍，仍然在该 x 轴上。",
  "model": "google_nmt",
  "from_community_srt": "基向量i帽就是这样一个特殊向量 i帽张成的空间是x轴 而且从矩阵的第一列可以看出 i帽变成了原来的3倍，",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span. ",
  "translatedText": "更重要的是，由于线性变换的工作方式，x 轴上的任何其他 向量也只是拉伸了 3 倍，因此保持在其自己的跨度上。",
  "model": "google_nmt",
  "from_community_srt": "仍然留在x轴上 此外， 因为线性变换的性质 x轴上的任何其他向量都只是被拉伸为原来的3倍 因此也就留在它们张成的空间里 有一个略显隐蔽的向量(-1,",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1. ",
  "translatedText": "在此转换过程中，一个稍微狡猾的向量保 持在其自己的跨度上，即负 1, 1。",
  "model": "google_nmt",
  "from_community_srt": "1)，",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2. ",
  "translatedText": "它最终会被拉伸 2 倍。",
  "model": "google_nmt",
  "from_community_srt": "它在变换中也留在自己张成的空间里 最终被拉伸为原来的2倍 同上，",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2. ",
  "translatedText": "再说一次，线性意味着这个家伙跨越的对角 线上的任何其他向量都会被拉伸 2 倍。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span. ",
  "translatedText": "对于这种变换，这些都是具有保持 在其跨度上的特殊属性的向量。",
  "model": "google_nmt",
  "from_community_srt": "线性性质暗示着一点 处在它所张成的对角线上的其他任何一个向量 也仅仅被拉伸为原来的2倍 对这个变换而言 以上就是所有拥有这一特殊性质（留在它们张成的空间里）的向量 x轴上的向量被拉伸为原来的3倍",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2. ",
  "translatedText": "x 轴上的那些被拉伸了 3 倍，而 该对角线上的那些被拉伸了 2 倍。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans. ",
  "translatedText": "任何其他向量都会在变换过程中发生一定 程度的旋转，从它所跨越的直线上移开。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation. ",
  "translatedText": "正如您现在可能已经猜到的那样，这些特殊向量称为 变换的特征向量，每个特征向量都与所谓的特征值相 关联，特征值只是在变换过程中拉伸或压缩的因素。",
  "model": "google_nmt",
  "from_community_srt": "而这条对角线上的向量被拉伸为原来的2倍 任何其他向量在变换中都有或多或少的旋转 从而离开它张成的直线 估计你现在已经猜到了 这些特殊向量就被称为变换的“特征向量” 每个特征向量都有一个所属的值， 被称为“特征值” 即衡量特征向量在变换中拉伸或压缩比例的因子 当然，",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing or the fact that these eigenvalues happen to be positive. ",
  "translatedText": "当然，拉伸与压缩或者这些特征值恰 好为正的事实并没有什么特别的。",
  "model": "google_nmt",
  "from_community_srt": "拉伸和压缩或者特征值恰好为正， 并没有什么特殊的地方 换个例子，",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half. ",
  "translatedText": "在另一个示例中，您可能有一个特征值负 1 一半的特 征向量，这意味着该向量被翻转并压缩了 1 一半。",
  "model": "google_nmt",
  "from_community_srt": "你可以有一个属于特征值-1/2的特征向量 意味着这个向量被反向，",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it. ",
  "translatedText": "但这里重要的部分是它保持在它所 延伸的线上而不会旋转离开它。",
  "model": "google_nmt",
  "from_community_srt": "并且被压缩为原来的1/2 但是重点在于， 它停留在它张成的直线上，",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation. ",
  "translatedText": "为了了解为什么这可能是一个值得思考 的有用的事情，请考虑一些三维旋转。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation. ",
  "translatedText": "如果您可以找到该旋转的特征向量（一个保持在其 自身跨度上的向量），那么您找到的就是旋转轴。",
  "model": "google_nmt",
  "from_community_srt": "并未发生旋转 若想知道为什么它有用途并且值得细究 那就考虑一个三维空间中的旋转 如果你能找到这个旋转的特征向量 也就是留在它张成的空间里的向量 那么你找到的就是旋转轴 而且把一个三维旋转看成绕某个轴旋转一定角度，",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3 by 3 matrix associated with that transformation. ",
  "translatedText": "从某个旋转轴和旋转角度来考虑 3D 旋转要容易得多，而不是考虑与该变 换相关的完整 3 x 3 矩阵。",
  "model": "google_nmt",
  "from_community_srt": "要比考虑相应的3×3矩阵直观得多 顺便一提，",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same. ",
  "translatedText": "顺便说一句，在这种情况下，相应的特征值必须为 1，因为旋 转永远不会拉伸或挤压任何东西，因此向量的长度将保持不变。",
  "model": "google_nmt",
  "from_community_srt": "在这种情况下，",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra. ",
  "translatedText": "这种模式在线性代数中经常出现。",
  "model": "google_nmt",
  "from_community_srt": "相应的特征值必须为1 因为旋转并不缩放任何一个向量 所以向量的长度保持不变 这种规律在线性代数中经常出现 对于任一矩阵描述的线性变换 你可以通过将矩阵的列看作变换后的基向量来理解它 但是，",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors. ",
  "translatedText": "对于矩阵描述的任何线性变换，您可以通过读取该 矩阵的列作为基向量的着陆点来理解它在做什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues. ",
  "translatedText": "但通常，了解线性变换实际作用的核心（较少依赖于 特定坐标系）的更好方法是找到特征向量和特征值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding. ",
  "translatedText": "我不会在这里介绍计算特征向量和特征 值的方法的完整细节，但我将尝试概 述对于概念理解最重要的计算思想。",
  "model": "google_nmt",
  "from_community_srt": "理解线性变换作用的关键往往较少依赖于你的特定坐标系 更好的方法是求出它的特征向量和特征值 我并不会涉及计算特征向量和特征值的具体细节 但是我会尽量概述一下计算思想 这对概念上的理解至关重要",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like. ",
  "translatedText": "象征性地，这就是特征向量的概念。",
  "model": "google_nmt",
  "from_community_srt": "用符号表示的话，",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue. ",
  "translatedText": "A是表示某种变换的矩阵，以v为特征向量， lambda是一个数字，即对应的特征值。",
  "model": "google_nmt",
  "from_community_srt": "以下就是特征向量的概念 A是代表某个变换的矩阵 v是特征向量 λ是一个数，",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda. ",
  "translatedText": "该表达式的意思是，矩阵向量乘积 A 乘以 v 所得到的结果与仅将 特征向量 v 按某个值 lambda 进行缩放得到的结果相同。",
  "model": "google_nmt",
  "from_community_srt": "也就是对应的特征值 这个等式是说， 矩阵向量乘积，",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true. ",
  "translatedText": "因此，查找矩阵 A 的特征向量及其特征值可以归结为 查找使该表达式成立的 v 和 lambda 的值。",
  "model": "google_nmt",
  "from_community_srt": "也就是A乘以v 等于特征向量v乘以某个数λ 因此求解矩阵A的特征向量和特征值 实际上就是求解使得这个等式成立的向量v和数λ 乍一看，",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication. ",
  "translatedText": "一开始使用起来有点尴尬，因为左边代表矩阵 向量乘法，但这里的右边是标量向量乘法。",
  "model": "google_nmt",
  "from_community_srt": "求解这个等式有些棘手 因为等号左侧代表的是矩阵向量乘积 但是等号右侧代表的是向量数乘 所以我们首先将等号右侧重写为某个矩阵向量乘积 其中，",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda. ",
  "translatedText": "因此，让我们首先将右侧重写为某种矩阵向量乘法，使用 具有将任何向量缩放 lambda 倍的效果的矩阵。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else. ",
  "translatedText": "这样的矩阵的列将表示每个基向量发生的情况，并且每 个基向量只需乘以 lambda，因此该矩阵将在 对角线上具有数字 lambda，其他位置为零。",
  "model": "google_nmt",
  "from_community_srt": "矩阵的作用效果是将任一向量乘以λ 这个矩阵的列代表着变换后的基向量 而每个基向量仅仅与λ相乘 所以这个矩阵的对角元均为λ，",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with ones down the diagonal. ",
  "translatedText": "写这个家伙的常见方法是将 lambda 分解出来，并将其写为 l ambda 乘以 i，其中 i 是单位矩阵，对角线下方有 1。",
  "model": "google_nmt",
  "from_community_srt": "其余位置都是0 通常的书写方法是提出因子λ， 写作λ乘以 I 这里的 I 就是单位矩阵，",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v. ",
  "translatedText": "由于两边看起来都像矩阵向量乘法， 我们可以减去右边并分解出 v。",
  "model": "google_nmt",
  "from_community_srt": "对角元均为1 现在两侧都是矩阵向量乘积的形式 我们就能将等号右侧的东西移到左侧，",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix, times v, gives the zero vector. ",
  "translatedText": "所以我们现在有了一个新矩阵，A 减去 lambda 乘以恒等式， 我们正在寻找一个向量 v，使得这个新矩阵乘以 v 给出零向量。",
  "model": "google_nmt",
  "from_community_srt": "然后提出因子v 现在我们得到的是一个新的矩阵 - A减去λ乘以单位阵 我们就寻找一个向量v，",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring. ",
  "translatedText": "现在，如果 v 本身是零向量，这总是正确的，但这很无聊。",
  "model": "google_nmt",
  "from_community_srt": "使得这个新矩阵与v相乘结果为零向量 如果v本身就是零向量的话，",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector. ",
  "translatedText": "我们想要的是一个非零特征向量。",
  "model": "google_nmt",
  "from_community_srt": "这个等式恒成立 但是这没什么意思 我们想要的是一个非零特征向量 如果你看过第五章和第六章视频的话 你就知道，",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension. ",
  "translatedText": "如果你看了第 5 章和第 6 章，你就会知道 ，矩阵与非零向量的乘积可能变为零的唯一方法是 与该矩阵相关的变换将空间压缩到较低的维度。",
  "model": "google_nmt",
  "from_community_srt": "当且仅当矩阵代表的变换将空间压缩到更低的维度时 才会存在一个非零向量，",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix. ",
  "translatedText": "这种压缩对应于矩阵的零行列式。",
  "model": "google_nmt",
  "from_community_srt": "使得矩阵和它的乘积为零向量 而空间压缩对应的就是矩阵的行列式为零 举个具体的例子，",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry. ",
  "translatedText": "具体来说，假设您的矩阵 A 具有第 2、1 列和第 2、 3 列，并考虑从每个对角线条目中减去变量 lambda。",
  "model": "google_nmt",
  "from_community_srt": "假设你有一个矩阵， 列为(2, 1)和(2, 3) 考虑每个对角元都减去某个变量λ 现在想象一下，",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value. ",
  "translatedText": "现在想象一下调整 lambda，转动旋钮来改变它的值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes. ",
  "translatedText": "当 lambda 值发生变化时，矩阵本身也 会发生变化，因此矩阵的行列式也会发生变化。",
  "model": "google_nmt",
  "from_community_srt": "逐渐调整λ的值 当λ的值改变时 矩阵本身发生改变，",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension. ",
  "translatedText": "这里的目标是找到一个 lambda 值，使行列式为 零，这意味着调整后的变换将空间压缩到较低的维度。",
  "model": "google_nmt",
  "from_community_srt": "因此行列式也在改变 目标在于找到一个λ使得这个行列式为零 也就是说调整后的变换将空间压缩到一个更低的维度上 在这个例子中，",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1. ",
  "translatedText": "在这种情况下，当 lambda 等于 1 时，最佳点就出现了。",
  "model": "google_nmt",
  "from_community_srt": "λ等于1时恰到好处 当然，",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1. ",
  "translatedText": "当然，如果我们选择了其他矩阵，特征值可能不一定是1。",
  "model": "google_nmt",
  "from_community_srt": "如果我选择其他的矩阵 特征值不一定是1，",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda. ",
  "translatedText": "最佳点可能会在其他 lambda 值处达到。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying. ",
  "translatedText": "内容有点多，但让我们来解释一下这到底在说什么。",
  "model": "google_nmt",
  "from_community_srt": "λ取其他值时才能使行列式为零 这看起来有点复杂，",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line. ",
  "translatedText": "当 lambda 等于 1 时，矩阵 A 减去 lambda 乘以恒等式会将空间压缩到一条线上。",
  "model": "google_nmt",
  "from_community_srt": "不过我们来解释这个过程的意思 当λ等于1时，",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector. ",
  "translatedText": "这意味着存在一个非零向量 v，使得 A 减去 lambda 乘以恒等式乘以 v 等于零向量。",
  "model": "google_nmt",
  "from_community_srt": "A减去λ乘以单位阵将空间压缩到一条直线上 这意味着存在一个非零向量v 使得A减去λ乘以单位阵的结果乘以v等于零向量 记住一点，",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A. ",
  "translatedText": "请记住，我们关心这一点的原因是因为它意味着 A 乘以 v 等于 lambda 乘以 v，您可以将其读作表示向量 v 是 A 的特征向量，在变换 A 期间保持其自己的跨度。",
  "model": "google_nmt",
  "from_community_srt": "我们关注它， 是因为它意味着A乘以v等于λ乘以v 也就是说向量v是A的一个特征向量 在变换中停留在它张成的空间里 在这个例子中，",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place. ",
  "translatedText": "在此示例中，相应的特征值为 1， 因此 v 实际上会保持固定不变。",
  "model": "google_nmt",
  "from_community_srt": "v对应的特征值是1，",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good. ",
  "translatedText": "停下来思考一下你是否需要确保这种推理方式感觉良好。",
  "model": "google_nmt",
  "from_community_srt": "所以它实际上保持不变 如果你觉得理解还不够透彻，",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction. ",
  "translatedText": "这就是我在引言中提到的事情。",
  "model": "google_nmt",
  "from_community_srt": "那就暂停思考一下 这就是我在绪论中提到的内容 如果你没有充分理解行列式，",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue. ",
  "translatedText": "如果您没有充分掌握行列式以及为什么它们与具有非零解的线性 方程组相关，那么像这样的表达式会让人感觉完全出乎意料。",
  "model": "google_nmt",
  "from_community_srt": "以及它为什么与线性方程组存在非零解有所联系 这个等式会让你感到出乎意料 为了了解这个过程，",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2. ",
  "translatedText": "为了查看其实际效果，让我们从头开始回顾一下示例 ，其中的矩阵的列为 3, 0 和 1, 2。",
  "model": "google_nmt",
  "from_community_srt": "我们重温一下视频开头的例子 这个矩阵的列是(3, 0)和(1,",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant. ",
  "translatedText": "要确定值 lambda 是否为特征值，请 从该矩阵的对角线中减去它并计算行列式。",
  "model": "google_nmt",
  "from_community_srt": "2) 为了求解特征值λ 将矩阵的对角元减去λ，",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda. ",
  "translatedText": "这样做，我们得到了 lambda 的某个二次多项式，即 3 减去 lambda 乘以 2 减去 lambda。",
  "model": "google_nmt",
  "from_community_srt": "然后计算行列式 这样我们就得到了一个关于λ的二次多项式(3-λ)(2-λ) 因为只有当这个行列式为零时，",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3. ",
  "translatedText": "由于只有当该行列式恰好为零时 lambda 才可能是特征值，因此您可以得出结 论，唯一可能的特征值是 lambda 等于 2 和 lambda 等于 3。",
  "model": "google_nmt",
  "from_community_srt": "λ才会是特征值 你就能推断出，",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero. ",
  "translatedText": "要找出实际上具有这些特征值之一的特征向量，假设 l ambda 等于 2，请将 lambda 值代入 矩阵，然后求解该对角线更改的矩阵发送到零的向量。",
  "model": "google_nmt",
  "from_community_srt": "所有可能的特征值是λ等于2和λ等于3 为了求出属于某个特征值的特征向量， 比如λ等于2 将λ的值代入矩阵当中 然后求解出在这个对角线变化的矩阵变换后成为零的向量 如果进行计算，",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1. ",
  "translatedText": "如果您按照任何其他线性系统的方式进行计算，您会发 现解是对角线上由负 1, 1 跨越的所有向量。",
  "model": "google_nmt",
  "from_community_srt": "如同求解其他线性方程组一样 你会发现所有的解全部落在由向量(-1,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2. ",
  "translatedText": "这对应于以下事实：未改变的矩阵 3, 0, 1 , 2 具有将所有这些向量拉伸 2 倍的效果。",
  "model": "google_nmt",
  "from_community_srt": "1)张成的对角线上 与之对应的， 就是原始的矩阵[(3, 0), (1,",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors. ",
  "translatedText": "现在，二维变换不必具有特征向量。",
  "model": "google_nmt",
  "from_community_srt": "2)]将这些向量拉伸为原来的2倍 不过， 二维线性变换不一定有特征向量 举个例子，",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees. ",
  "translatedText": "例如，考虑旋转 90 度。",
  "model": "google_nmt",
  "from_community_srt": "考虑这样一个90度的旋转 它并没有特征向量，",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span. ",
  "translatedText": "它没有任何特征向量，因为它将每个向量旋转出自己的跨度。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens. ",
  "translatedText": "如果您实际上尝试像这样计算旋转的特征值，请注意会发生什么。",
  "model": "google_nmt",
  "from_community_srt": "因为每一个向量都发生了旋转并离开了其张成的空间 如果你真的尝试去计算它的特征值，",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0. ",
  "translatedText": "它的矩阵有列 0、1 和负列 1、0。",
  "model": "google_nmt",
  "from_community_srt": "注意一下会发生什么 矩阵的列为(0, 1)和(-1,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero. ",
  "translatedText": "从对角线元素中减去 lambda 并查找行列式何时为零。",
  "model": "google_nmt",
  "from_community_srt": "0) 对角元减去λ后，",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1. ",
  "translatedText": "在这种情况下，您将得到多项式 lambda 平方加 1。",
  "model": "google_nmt",
  "from_community_srt": "然后寻找行列式为0的情形 在这个例子里，",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i. ",
  "translatedText": "该多项式的唯一根是虚数 i 和负 i。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors. ",
  "translatedText": "没有实数解的事实表明不存在特征向量。",
  "model": "google_nmt",
  "from_community_srt": "你会得到多项式λ^2+1 这个多项式的根只能是虚数i与-i 没有实数解表明它没有特征向量 另一个很有意思并且值得思考的例子是剪切变换 它保持i帽不变，",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear. ",
  "translatedText": "另一个值得记住的非常有趣的例子是剪刀。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1. ",
  "translatedText": "这会将 i-hat 固定到位，并将 j-hat 1 移过去，因此其矩阵具有列 1, 0 和 1, 1。",
  "model": "google_nmt",
  "from_community_srt": "将j帽向右移动一个单位 所以矩阵的列为(1, 0)和(1,",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place. ",
  "translatedText": "x 轴上的所有向量都是特征值为 1 的特征向量，因为它们保持固定位置。",
  "model": "google_nmt",
  "from_community_srt": "1) 所有x轴上的向量都是属于特征值1的特征向量，",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors. ",
  "translatedText": "事实上，这些是唯一的特征向量。",
  "model": "google_nmt",
  "from_community_srt": "因为它们都保持不变 实际上，",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared. ",
  "translatedText": "当您从对角线中减去 lambda 并计算行列式时 ，您得到的是 1 减去 lambda 的平方。",
  "model": "google_nmt",
  "from_community_srt": "这些就是所有的特征向量 当你将对角元减去λ，",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1. ",
  "translatedText": "该表达式的唯一根是 lambda 等于 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1. ",
  "translatedText": "这与我们在几何上看到的一致，即 所有特征向量都具有特征值 1。",
  "model": "google_nmt",
  "from_community_srt": "然后计算行列式 你得到的是(1-λ)^2 这个多项式唯一的根是λ等于1 这与几何上得到的“所有特征向量均属于特征值1”的结果一致 不过注意一点 可能会出现只有一个特征值，",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors. ",
  "translatedText": "但请记住，也可以只有一个特征值， 但不仅仅是一行完整的特征向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2. ",
  "translatedText": "一个简单的例子是一个将所有内容缩放 2 的矩阵。",
  "model": "google_nmt",
  "from_community_srt": "但是特征向量不止在一条直线上的情况 一个简单的例子是将所有向量变为两倍的矩阵 唯一的特征值是2，",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue. ",
  "translatedText": "唯一的特征值是 2，但平面中的每个 向量都是具有该特征值的特征向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic. ",
  "translatedText": "在我继续最后一个主题之前，现在是另 一个暂停并思考一些问题的好时机。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video. ",
  "translatedText": "我想以特征基的想法结束这里，它在很 大程度上依赖于上一个视频的想法。",
  "model": "google_nmt",
  "from_community_srt": "但是平面内每一个向量都是属于这个特征值的特征向量 现在是暂停思考这部分的绝好时机 因为接下来我会继续最后一个话题 我想以特征基的概念结束这期视频 而它在很大程度上依赖于上期视频的思想",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors. ",
  "translatedText": "看看如果我们的基向量恰好是特征向量会发生什么。",
  "model": "google_nmt",
  "from_community_srt": "如果我们的基向量恰好是特征向量，",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1, and j-hat is scaled by 2. ",
  "translatedText": "例如，也许 i-hat 按负 1 缩放，而 j-hat 按 2 缩放。",
  "model": "google_nmt",
  "from_community_srt": "来看看会发生什么 比如说， 可能i帽变为原来的(-1)倍，",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0. ",
  "translatedText": "将它们的新坐标写为矩阵的列，请注意那些标量倍数，负 1 和 2，它们是 i-hat 和 j-hat 的特征 值，位于矩阵的对角线上，并且每个其他条目都是 0 。",
  "model": "google_nmt",
  "from_community_srt": "j帽变为原来的2倍 将它们的新坐标作为矩阵的列 注意， 它们的倍数-1和2， 也就是i帽和j帽所属的特征值 位于矩阵的对角线上，",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has 0s everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix, and the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues. ",
  "translatedText": "任何时候，一个矩阵除了对角线以外的所有位置都为 0 时，就足够合理地称为对角矩阵，解释这一点的方法是所有 基向量都是特征向量，该矩阵的对角项就是它们的特征值。",
  "model": "google_nmt",
  "from_community_srt": "而其他元素均为0 除了对角元以外其他元素均为0的矩阵被称为对角矩阵， 这非常合理 解读它的方法是，",
  "n_reviews": 0,
  "start": 818.88,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with. ",
  "translatedText": "有很多东西可以让对角矩阵更容易使用。",
  "model": "google_nmt",
  "from_community_srt": "所有基向量都是特征向量 矩阵的对角元是它们所属的特征值 对角矩阵在很多方面都更容易处理 其中一个重要的方面是，",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times. ",
  "translatedText": "一个重要的问题是，如果将此矩阵与其自身相 乘很多次，则可以更轻松地计算会发生什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue. ",
  "translatedText": "由于所有这些矩阵的作用都是按某个特征值缩放每个基 向量，因此多次应用该矩阵（例如 100 次）就相 当于按相应特征值的 100 次方缩放每个基向量。",
  "model": "google_nmt",
  "from_community_srt": "矩阵与自己多次相乘的结果更容易计算 因为对角矩阵仅仅让基向量与某个特征值相乘 所以多次应用矩阵乘法， 比如100次 也只是将每个基向量与对应特征值的100次幂相乘 相比之下，",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix. ",
  "translatedText": "相反，尝试计算非对角矩阵的 100 次方。",
  "model": "google_nmt",
  "from_community_srt": "尝试计算一个非对角矩阵的100次幂 真的去试试看，",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment. ",
  "translatedText": "真的，尝试一下。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare. ",
  "translatedText": "这是一场噩梦。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors. ",
  "translatedText": "当然，你很少会幸运地让你的基向量也是特征向量。",
  "model": "google_nmt",
  "from_community_srt": "这就是场噩梦 当然对于基向量同时也是特征向量的情况，",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors. ",
  "translatedText": "但是，如果您的变换有很多特征向量（例如本视频开头的特征 向量），足以让您可以选择一组跨越整个空间的特征向量，那 么您可以更改坐标系，使这些特征向量成为您的基础向量。",
  "model": "google_nmt",
  "from_community_srt": "你很可能不像它那么幸运 但是如果你的变换有许多特征向量， 就像视频开始时的例子一样 多到你能选出一个张成全空间的集合 那么你就能变换你的坐标系，",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system. ",
  "translatedText": "我在上一个视频中谈到了基础的变化，但 我将在这里快速提醒如何将当前在我们的 坐标系中编写的转换表达为不同的系统。",
  "model": "google_nmt",
  "from_community_srt": "使得这些特征向量就是基向量 我在上期视频中已经讨论过基变换了 不过我在这里再做一次超快的回顾 说说如何在另一个坐标系中表达当前坐标系所描述的变换 取出你想用作新基的向量的坐标 在这里指的是两个特征向量",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix. ",
  "translatedText": "将要用作新基的向量的坐标（在本例中指 的是我们的两个特征向量），然后将这些 坐标作为矩阵的列，称为基矩阵的变化。",
  "model": "google_nmt",
  "from_community_srt": "然后将坐标作为一个矩阵的列，",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system. ",
  "translatedText": "当您将原始变换夹在中间时，将基矩阵的 变化放在其右侧，将基矩阵的变化的逆放 在其左侧，结果将是表示相同变换的矩阵 ，但从新基向量坐标的角度来看系统。",
  "model": "google_nmt",
  "from_community_srt": "这个矩阵就是基变换矩阵 在右侧写下基变换矩阵 在左侧写下基变换矩阵的逆 当你将原始的变换夹在两个矩阵中间时 所得的矩阵代表的是同一个变换 不过是从新基向量所构成的坐标系的角度来看的 用特征向量来完成这件事的意义在于",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal. ",
  "translatedText": "使用特征向量执行此操作的全部要点是，保证这个新 矩阵是对角线，其相应的特征值位于该对角线下方。",
  "model": "google_nmt",
  "from_community_srt": "这个新矩阵必然是对角的， 并且对角元为对应的特征值 这是因为，",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation. ",
  "translatedText": "这是因为它代表在坐标系中工作，其中基向 量发生的情况是它们在变换过程中被缩放。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis. ",
  "translatedText": "一组同时也是特征向量的基向量被合理地称为特征基。",
  "model": "google_nmt",
  "from_community_srt": "它所处的坐标系的基向量在变换中只进行了缩放 一组基向量（同样是特征向量）构成的集合被称为一组“特征基”，",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system. ",
  "translatedText": "因此，例如，如果您需要计算该矩阵的 100 次方，则更改为特征基、计算该系统中的 100 次方，然后转换回我们的标准系统会容易得多。",
  "model": "google_nmt",
  "from_community_srt": "这也非常合理 所以说， 如果你要计算这个矩阵的100次幂 一种更容易的做法是先变换到特征基 在那个坐标系中计算100次幂 然后转换回标准坐标系 不是所有变换都能进行这一过程 比如说剪切变换，",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations. ",
  "translatedText": "您无法对所有转换都做到这一点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space. ",
  "translatedText": "例如，剪切力没有足够的特征向量来跨越整个空间。",
  "model": "google_nmt",
  "from_community_srt": "它的特征向量不够多，",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely. ",
  "translatedText": "但如果你能找到一个特征基，那么矩阵运算就会变得非常可爱。",
  "model": "google_nmt",
  "from_community_srt": "并不能张成全空间 但是如果你能找到一组特征基，",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen. ",
  "translatedText": "对于那些愿意完成一个相当简洁的谜题来看看它在 实际中是什么样子以及如何使用它来产生一些令人 惊讶的结果的人，我将在屏幕上留下一个提示。",
  "model": "google_nmt",
  "from_community_srt": "矩阵运算就会变得非常轻松 对于那些愿意动笔计算的人， 我在屏幕上留了一道练习 帮助你们理解矩阵幂次计算的实际过程，",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it. ",
  "translatedText": "这需要一些工作，但我想你会喜欢它。",
  "model": "google_nmt",
  "from_community_srt": "以及它所能带来的惊人结论 这需要下一点功夫， 不过我觉得你会喜欢它的 下期，",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces. ",
  "translatedText": "本系列的下一个也是最后一个视频将介绍抽象向量空间。",
  "model": "google_nmt",
  "from_community_srt": "也就是最后一期视频， 是关于抽象向量空间的内容 到时候再见！",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1005.38
 },
 {
  "input": "See you then! ",
  "translatedText": "回头见！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.9,
  "end": 1006.12
 }
]