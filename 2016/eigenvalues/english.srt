1
00:00:19,282 --> 00:00:24,760
Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly

2
00:00:24,760 --> 00:00:26,460
unintuitive.

3
00:00:26,460 --> 00:00:30,320
Things like, why are we doing this, and what does this actually mean, are too often left

4
00:00:30,320 --> 00:00:34,020
just floating away in an unanswered sea of computations.

5
00:00:34,020 --> 00:00:37,340
And as I've put out the videos of this series, a lot of you have commented about looking

6
00:00:37,340 --> 00:00:40,700
forward to visualizing this topic in particular.

7
00:00:40,700 --> 00:00:44,700
I suspect that the reason for this is not so much that eigenthings are particularly

8
00:00:44,700 --> 00:00:46,460
complicated or poorly explained.

9
00:00:46,460 --> 00:00:51,020
In fact, it's comparatively straightforward, and I think most books do a fine job explaining

10
00:00:51,020 --> 00:00:52,020
it.

11
00:00:52,020 --> 00:00:56,500
What I want to do is that it only really makes sense if you have a solid visual understanding

12
00:00:56,500 --> 00:00:59,220
for many of the topics that precede it.

13
00:00:59,220 --> 00:01:04,460
Most important here is that you know how to think about matrices as linear transformations,

14
00:01:04,460 --> 00:01:09,140
but you also need to be comfortable with things like determinants, linear systems of equations,

15
00:01:09,140 --> 00:01:10,780
and change of basis.

16
00:01:10,780 --> 00:01:15,580
Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these

17
00:01:15,580 --> 00:01:20,420
topics than it does with eigenvectors and eigenvalues themselves.

18
00:01:20,420 --> 00:01:25,500
To start, consider some linear transformation in two dimensions, like the one shown here.

19
00:01:25,500 --> 00:01:31,860
It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.

20
00:01:31,860 --> 00:01:36,860
So it's represented with a matrix whose columns are 3, 0, and 1, 2.

21
00:01:36,860 --> 00:01:42,020
Focus in on what it does to one particular vector, and think about the span of that vector,

22
00:01:42,020 --> 00:01:45,220
the line passing through its origin and its tip.

23
00:01:45,220 --> 00:01:48,460
Most vectors are going to get knocked off their span during the transformation.

24
00:01:48,500 --> 00:01:53,140
I mean, it would seem pretty coincidental if the place where the vector landed also

25
00:01:53,140 --> 00:01:57,500
happened to be somewhere on that line.

26
00:01:57,500 --> 00:02:02,380
But some special vectors do remain on their own span, meaning the effect that the matrix

27
00:02:02,380 --> 00:02:09,660
has on such a vector is just to stretch it or squish it, like a scalar.

28
00:02:09,660 --> 00:02:15,100
For this specific example, the basis vector i-hat is one such special vector.

29
00:02:15,100 --> 00:02:19,940
The span of i-hat is the x-axis, and from the first column of the matrix, we can see

30
00:02:19,940 --> 00:02:26,500
that i-hat moves over to 3 times itself, still on that x-axis.

31
00:02:26,500 --> 00:02:32,540
What's more, because of the way linear transformations work, any other vector on the x-axis is also

32
00:02:32,540 --> 00:02:38,580
just stretched by a factor of 3, and hence remains on its own span.

33
00:02:38,580 --> 00:02:42,760
A slightly sneakier vector that remains on its own span during this transformation is

34
00:02:42,760 --> 00:02:44,880
negative 1, 1.

35
00:02:44,880 --> 00:02:49,120
It ends up getting stretched by a factor of 2.

36
00:02:49,120 --> 00:02:54,760
And again, linearity is going to imply that any other vector on the diagonal line spanned

37
00:02:54,760 --> 00:03:00,040
by this guy is just going to get stretched out by a factor of 2.

38
00:03:00,040 --> 00:03:04,200
And for this transformation, those are all the vectors with this special property of

39
00:03:04,200 --> 00:03:05,860
staying on their span.

40
00:03:05,860 --> 00:03:10,000
Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal

41
00:03:10,000 --> 00:03:12,940
line getting stretched by a factor of 2.

42
00:03:12,940 --> 00:03:16,600
Any other vector is going to get rotated somewhat during the transformation, knocked

43
00:03:16,600 --> 00:03:22,700
off the line that it spans.

44
00:03:22,700 --> 00:03:28,140
As you might have guessed by now, these special vectors are called the eigenvectors of the

45
00:03:28,140 --> 00:03:33,460
transformation, and each eigenvector has associated with it what's called an eigenvalue, which

46
00:03:33,460 --> 00:03:40,620
is just the factor by which it's stretched or squished during the transformation.

47
00:03:40,620 --> 00:03:44,220
Of course, there's nothing special about stretching versus squishing or the fact that

48
00:03:44,220 --> 00:03:46,580
these eigenvalues happen to be positive.

49
00:03:46,580 --> 00:03:51,820
In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that

50
00:03:51,820 --> 00:03:57,460
the vector gets flipped and squished by a factor of 1 half.

51
00:03:57,460 --> 00:04:01,580
But the important part here is that it stays on the line that it spans out without getting

52
00:04:01,580 --> 00:04:04,660
rotated off of it.

53
00:04:04,660 --> 00:04:09,780
For a glimpse of why this might be a useful thing to think about, consider some three-dimensional

54
00:04:09,780 --> 00:04:11,940
rotation.

55
00:04:11,940 --> 00:04:17,780
If you can find an eigenvector for that rotation, a vector that remains on its own span, what

56
00:04:17,780 --> 00:04:23,020
you have found is the axis of rotation.

57
00:04:23,020 --> 00:04:28,540
And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle

58
00:04:28,540 --> 00:04:33,880
by which it's rotating, rather than thinking about the full 3 by 3 matrix associated with

59
00:04:33,880 --> 00:04:37,140
that transformation.

60
00:04:37,140 --> 00:04:42,080
In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations

61
00:04:42,080 --> 00:04:48,180
never stretch or squish anything, so the length of the vector would remain the same.

62
00:04:48,180 --> 00:04:50,580
This pattern shows up a lot in linear algebra.

63
00:04:50,580 --> 00:04:55,420
With any linear transformation described by a matrix, you could understand what it's doing

64
00:04:55,420 --> 00:05:00,120
by reading off the columns of this matrix as the landing spots for basis vectors.

65
00:05:00,120 --> 00:05:04,180
But often, a better way to get at the heart of what the linear transformation actually

66
00:05:04,220 --> 00:05:15,780
does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.

67
00:05:15,780 --> 00:05:19,980
I won't cover the full details on methods for computing eigenvectors and eigenvalues

68
00:05:19,980 --> 00:05:24,600
here, but I'll try to give an overview of the computational ideas that are most important

69
00:05:24,600 --> 00:05:26,820
for a conceptual understanding.

70
00:05:26,820 --> 00:05:30,980
Symbolically, here's what the idea of an eigenvector looks like.

71
00:05:30,980 --> 00:05:37,220
A is the matrix representing some transformation, with v as the eigenvector, and lambda is a

72
00:05:37,220 --> 00:05:40,800
number, namely the corresponding eigenvalue.

73
00:05:40,800 --> 00:05:45,500
What this expression is saying is that the matrix-vector product, A times v, gives the

74
00:05:45,500 --> 00:05:51,520
same result as just scaling the eigenvector v by some value lambda.

75
00:05:51,520 --> 00:05:56,900
So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values

76
00:05:56,900 --> 00:06:02,420
of v and lambda that make this expression true.

77
00:06:02,420 --> 00:06:06,340
It's a little awkward to work with at first because that left-hand side represents matrix-vector

78
00:06:06,340 --> 00:06:11,220
multiplication, but the right-hand side here is scalar-vector multiplication.

79
00:06:11,220 --> 00:06:16,540
So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication,

80
00:06:16,540 --> 00:06:21,740
using a matrix which has the effect of scaling any vector by a factor of lambda.

81
00:06:21,740 --> 00:06:26,260
The columns of such a matrix will represent what happens to each basis vector, and each

82
00:06:26,260 --> 00:06:31,580
basis vector is simply multiplied by lambda, so this matrix will have the number lambda

83
00:06:31,580 --> 00:06:36,360
down the diagonal, with zeros everywhere else.

84
00:06:36,360 --> 00:06:40,980
The common way to write this guy is to factor that lambda out and write it as lambda times

85
00:06:40,980 --> 00:06:45,980
i, where i is the identity matrix with ones down the diagonal.

86
00:06:45,980 --> 00:06:50,260
With both sides looking like matrix-vector multiplication, we can subtract off that right-hand

87
00:06:50,260 --> 00:06:54,340
side and factor out the v.

88
00:06:54,420 --> 00:06:59,340
So what we now have is a new matrix, A minus lambda times the identity, and we're looking

89
00:06:59,340 --> 00:07:05,860
for a vector v such that this new matrix, times v, gives the zero vector.

90
00:07:05,860 --> 00:07:11,420
Now, this will always be true if v itself is the zero vector, but that's boring.

91
00:07:11,420 --> 00:07:14,540
What we want is a non-zero eigenvector.

92
00:07:14,540 --> 00:07:18,900
And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product

93
00:07:18,900 --> 00:07:24,940
of a matrix with a non-zero vector to become zero is if the transformation associated with

94
00:07:24,940 --> 00:07:29,940
that matrix squishes space into a lower dimension.

95
00:07:29,940 --> 00:07:35,560
And that squishification corresponds to a zero determinant for the matrix.

96
00:07:35,560 --> 00:07:41,700
To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting

97
00:07:41,700 --> 00:07:46,600
off a variable amount, lambda, from each diagonal entry.

98
00:07:46,600 --> 00:07:51,160
Now imagine tweaking lambda, turning a knob to change its value.

99
00:07:51,160 --> 00:07:56,320
As that value of lambda changes, the matrix itself changes, and so the determinant of

100
00:07:56,320 --> 00:07:58,240
the matrix changes.

101
00:07:58,240 --> 00:08:03,720
The goal here is to find a value of lambda that will make this determinant zero, meaning

102
00:08:03,720 --> 00:08:08,240
the tweaked transformation squishes space into a lower dimension.

103
00:08:08,240 --> 00:08:12,240
In this case, the sweet spot comes when lambda equals 1.

104
00:08:12,240 --> 00:08:16,480
Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.

105
00:08:16,480 --> 00:08:20,280
The sweet spot might be hit at some other value of lambda.

106
00:08:20,280 --> 00:08:23,620
So this is kind of a lot, but let's unravel what this is saying.

107
00:08:23,620 --> 00:08:30,620
When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.

108
00:08:30,620 --> 00:08:36,440
That means there's a non-zero vector v such that A minus lambda times the identity times

109
00:08:36,440 --> 00:08:40,680
v equals the zero vector.

110
00:08:40,680 --> 00:08:46,180
And remember, the reason we care about that is because it means A times v equals lambda

111
00:08:46,180 --> 00:08:54,040
times v, which you can read off as saying that the vector v is an eigenvector of A,

112
00:08:54,040 --> 00:08:58,580
staying on its own span during the transformation A.

113
00:08:58,580 --> 00:09:03,440
In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed

114
00:09:03,440 --> 00:09:06,200
in place.

115
00:09:06,240 --> 00:09:13,840
Pause and ponder if you need to make sure that that line of reasoning feels good.

116
00:09:13,840 --> 00:09:16,280
This is the kind of thing I mentioned in the introduction.

117
00:09:16,280 --> 00:09:21,320
If you didn't have a solid grasp of determinants and why they relate to linear systems of equations

118
00:09:21,320 --> 00:09:28,460
having non-zero solutions, an expression like this would feel completely out of the blue.

119
00:09:28,460 --> 00:09:32,400
To see this in action, let's revisit the example from the start, with a matrix whose

120
00:09:32,400 --> 00:09:35,640
columns are 3, 0 and 1, 2.

121
00:09:35,640 --> 00:09:41,600
To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix

122
00:09:41,600 --> 00:09:51,240
and compute the determinant.

123
00:09:51,240 --> 00:09:57,920
Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.

124
00:09:57,920 --> 00:10:03,000
Since lambda can only be an eigenvalue if this determinant happens to be zero, you can

125
00:10:03,000 --> 00:10:10,120
conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.

126
00:10:10,120 --> 00:10:14,340
To figure out what the eigenvectors are that actually have one of these eigenvalues, say

127
00:10:14,340 --> 00:10:20,840
lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors

128
00:10:20,840 --> 00:10:25,300
this diagonally altered matrix sends to zero.

129
00:10:25,300 --> 00:10:29,800
If you computed this the way you would any other linear system, you'd see that the solutions

130
00:10:29,800 --> 00:10:35,480
are all the vectors on the diagonal line spanned by negative 1, 1.

131
00:10:35,480 --> 00:10:41,200
This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching

132
00:10:41,200 --> 00:10:44,680
all those vectors by a factor of 2.

133
00:10:44,680 --> 00:10:50,880
Now, a 2D transformation doesn't have to have eigenvectors.

134
00:10:50,880 --> 00:10:53,960
For example, consider a rotation by 90 degrees.

135
00:10:53,960 --> 00:11:01,200
This doesn't have any eigenvectors since it rotates every vector off of its own span.

136
00:11:01,200 --> 00:11:06,400
If you actually try computing the eigenvalues of a rotation like this, notice what happens.

137
00:11:06,400 --> 00:11:11,120
Its matrix has columns 0, 1 and negative 1, 0.

138
00:11:11,120 --> 00:11:18,440
Subtract off lambda from the diagonal elements and look for when the determinant is zero.

139
00:11:18,440 --> 00:11:22,960
In this case, you get the polynomial lambda squared plus 1.

140
00:11:22,960 --> 00:11:29,000
The only roots of that polynomial are the imaginary numbers, i and negative i.

141
00:11:29,000 --> 00:11:36,120
The fact that there are no real number solutions indicates that there are no eigenvectors.

142
00:11:36,120 --> 00:11:40,640
Another pretty interesting example worth holding in the back of your mind is a shear.

143
00:11:40,640 --> 00:11:47,460
This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and

144
00:11:47,460 --> 00:11:49,000
1, 1.

145
00:11:49,040 --> 00:11:54,060
All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed

146
00:11:54,060 --> 00:11:55,060
in place.

147
00:11:55,060 --> 00:11:58,880
In fact, these are the only eigenvectors.

148
00:11:58,880 --> 00:12:04,400
When you subtract off lambda from the diagonals and compute the determinant, what you get

149
00:12:04,400 --> 00:12:09,640
is 1 minus lambda squared.

150
00:12:09,640 --> 00:12:15,080
And the only root of this expression is lambda equals 1.

151
00:12:15,080 --> 00:12:19,640
This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue

152
00:12:19,640 --> 00:12:21,200
1.

153
00:12:21,200 --> 00:12:26,280
Keep in mind though, it's also possible to have just one eigenvalue, but with more than

154
00:12:26,280 --> 00:12:30,000
just a line full of eigenvectors.

155
00:12:30,000 --> 00:12:34,040
A simple example is a matrix that scales everything by 2.

156
00:12:34,040 --> 00:12:39,680
The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with

157
00:12:39,680 --> 00:12:42,380
that eigenvalue.

158
00:12:42,380 --> 00:12:46,020
Now is another good time to pause and ponder some of this before I move on to the last

159
00:12:46,020 --> 00:12:46,900
topic.

160
00:13:03,900 --> 00:13:08,940
I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas

161
00:13:08,940 --> 00:13:11,720
from the last video.

162
00:13:11,720 --> 00:13:17,220
Take a look at what happens if our basis vectors just so happen to be eigenvectors.

163
00:13:17,220 --> 00:13:23,760
For example, maybe i-hat is scaled by negative 1, and j-hat is scaled by 2.

164
00:13:23,760 --> 00:13:28,800
Writing their new coordinates as the columns of a matrix, notice that those scalar multiples,

165
00:13:28,800 --> 00:13:34,500
negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of

166
00:13:34,500 --> 00:13:39,060
our matrix, and every other entry is a 0.

167
00:13:39,060 --> 00:13:43,940
Any time a matrix has 0s everywhere other than the diagonal, it's called, reasonably

168
00:13:43,940 --> 00:13:48,940
enough, a diagonal matrix, and the way to interpret this is that all the basis vectors

169
00:13:48,940 --> 00:13:57,380
are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.

170
00:13:57,380 --> 00:14:02,060
There are a lot of things that make diagonal matrices much nicer to work with.

171
00:14:02,060 --> 00:14:06,380
One big one is that it's easier to compute what will happen if you multiply this matrix

172
00:14:06,380 --> 00:14:09,500
by itself a whole bunch of times.

173
00:14:09,500 --> 00:14:15,140
Since all one of these matrices does is scale each basis vector by some eigenvalue, applying

174
00:14:15,140 --> 00:14:20,900
that matrix many times, say 100 times, is just going to correspond to scaling each basis

175
00:14:20,900 --> 00:14:25,880
vector by the 100th power of the corresponding eigenvalue.

176
00:14:25,880 --> 00:14:29,940
In contrast, try computing the 100th power of a non-diagonal matrix.

177
00:14:29,940 --> 00:14:31,940
Really, try it for a moment.

178
00:14:31,940 --> 00:14:32,580
It's a nightmare.

179
00:14:36,500 --> 00:14:42,220
Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.

180
00:14:42,220 --> 00:14:46,900
But if your transformation has a lot of eigenvectors, like the one from the start of this video,

181
00:14:46,900 --> 00:14:52,160
enough so that you can choose a set that spans the full space, then you could change your

182
00:14:52,160 --> 00:14:56,940
coordinate system so that these eigenvectors are your basis vectors.

183
00:14:56,940 --> 00:15:01,140
I talked about change of basis last video, but I'll go through a super quick reminder

184
00:15:01,140 --> 00:15:06,180
here of how to express a transformation currently written in our coordinate system into a

185
00:15:06,180 --> 00:15:08,540
different system.

186
00:15:08,540 --> 00:15:12,420
Take the coordinates of the vectors that you want to use as a new basis, which in this

187
00:15:12,420 --> 00:15:17,540
case means our two eigenvectors, then make those coordinates the columns of a matrix,

188
00:15:17,540 --> 00:15:20,380
known as the change of basis matrix.

189
00:15:20,380 --> 00:15:24,460
When you sandwich the original transformation, putting the change of basis matrix on its

190
00:15:24,460 --> 00:15:30,560
right and the inverse of the change of basis matrix on its left, the result will be a matrix

191
00:15:30,560 --> 00:15:35,520
representing that same transformation, but from the perspective of the new basis vectors

192
00:15:35,520 --> 00:15:37,640
coordinate system.

193
00:15:37,640 --> 00:15:42,640
The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be

194
00:15:42,640 --> 00:15:47,300
diagonal with its corresponding eigenvalues down that diagonal.

195
00:15:47,300 --> 00:15:51,080
This is because it represents working in a coordinate system where what happens to the

196
00:15:51,080 --> 00:15:55,740
basis vectors is that they get scaled during the transformation.

197
00:15:55,740 --> 00:16:02,400
A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.

198
00:16:02,400 --> 00:16:07,620
So if, for example, you needed to compute the 100th power of this matrix, it would be

199
00:16:07,620 --> 00:16:14,060
much easier to change to an eigenbasis, compute the 100th power in that system, then convert

200
00:16:14,060 --> 00:16:16,760
back to our standard system.

201
00:16:16,760 --> 00:16:18,460
You can't do this with all transformations.

202
00:16:18,460 --> 00:16:23,800
A shear, for example, doesn't have enough eigenvectors to span the full space.

203
00:16:23,800 --> 00:16:29,180
But if you can find an eigenbasis, it makes matrix operations really lovely.

204
00:16:29,180 --> 00:16:32,060
For those of you willing to work through a pretty neat puzzle to see what this looks

205
00:16:32,060 --> 00:16:35,880
like in action and how it can be used to produce some surprising results, I'll leave

206
00:16:35,880 --> 00:16:37,960
up a prompt here on the screen.

207
00:16:37,960 --> 00:16:40,960
It takes a bit of work, but I think you'll enjoy it.

208
00:16:40,960 --> 00:16:46,000
The next and final video of this series is going to be on abstract vector spaces.

209
00:16:46,000 --> 00:16:46,500
See you then!

