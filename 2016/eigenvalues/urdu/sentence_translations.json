[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive. ",
  "translatedText": "Eigenvectors اور eigenvalues ان عنوانات میں سے ایک ہے جو بہت سارے طلباء کو خاص طور پر غیر فہم معلوم ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Things like, why are we doing this, and what does this actually mean, are too often left just floating away in an unanswered sea of computations. ",
  "translatedText": "جیسی چیزیں، ہم ایسا کیوں کر رہے ہیں، اور اس کا اصل مطلب کیا ہے، اکثر حسابات کے بے جواب سمندر میں تیرتے ہی رہ جاتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular. ",
  "translatedText": "اور جیسا کہ میں نے اس سیریز کی ویڈیوز پیش کی ہیں، آپ میں سے بہت سے لوگوں نے خاص طور پر اس موضوع کو دیکھنے کے منتظر ہونے کے بارے میں تبصرہ کیا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained. ",
  "translatedText": "مجھے شبہ ہے کہ اس کی وجہ اتنی زیادہ نہیں ہے کہ چیزیں خاص طور پر پیچیدہ یا ناقص طور پر بیان کی گئی ہوں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it. ",
  "translatedText": "درحقیقت، یہ نسبتاً سیدھا ہے، اور میرے خیال میں زیادہ تر کتابیں اس کی وضاحت کرنے میں اچھا کام کرتی ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "What I want to do is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it. ",
  "translatedText": "میں جو کرنا چاہتا ہوں وہ یہ ہے کہ یہ صرف اس صورت میں معنی رکھتا ہے جب آپ کو اس سے پہلے کے بہت سے موضوعات کے بارے میں ٹھوس بصری سمجھ ہو۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis. ",
  "translatedText": "یہاں سب سے اہم یہ ہے کہ آپ میٹرکس کے بارے میں لکیری تبدیلیوں کے طور پر سوچنا جانتے ہیں، لیکن آپ کو تعین کرنے والے، مساوات کے خطی نظام، اور بنیاد کی تبدیلی جیسی چیزوں سے بھی راحت محسوس کرنے کی ضرورت ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves. ",
  "translatedText": "eigenstuffs کے بارے میں الجھن کا عام طور پر ان موضوعات میں سے کسی ایک کی متزلزل بنیاد کے ساتھ زیادہ تعلق ہوتا ہے جتنا کہ یہ خود eigenvectors اور eigenvalues کے ساتھ ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here. ",
  "translatedText": "شروع کرنے کے لیے، دو جہتوں میں کچھ لکیری تبدیلی پر غور کریں، جیسا کہ یہاں دکھایا گیا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2. ",
  "translatedText": "یہ بنیادی ویکٹر i-hat کو کوآرڈینیٹ 3، 0، اور j-hat کو 1، 2 پر منتقل کرتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2. ",
  "translatedText": "تو اس کی نمائندگی ایک میٹرکس کے ساتھ کی جاتی ہے جس کے کالم 3، 0، اور 1، 2 ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip. ",
  "translatedText": "اس پر توجہ مرکوز کریں کہ یہ ایک مخصوص ویکٹر کے ساتھ کیا کرتا ہے، اور اس ویکٹر کے دورانیے، اس کی اصل اور اس کے سرے سے گزرنے والی لکیر کے بارے میں سوچیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation. ",
  "translatedText": "تبدیلی کے دوران زیادہ تر ویکٹر اپنی مدت سے دستک حاصل کرنے والے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line. ",
  "translatedText": "میرا مطلب ہے، یہ کافی اتفاقی لگے گا اگر وہ جگہ جہاں ویکٹر اترا وہ بھی اس لائن پر کہیں ہو۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar. ",
  "translatedText": "لیکن کچھ خاص ویکٹر اپنے اپنے دورانیے پر قائم رہتے ہیں، یعنی میٹرکس کا ایسے ویکٹر پر جو اثر پڑتا ہے وہ صرف اس کو پھیلانا یا اسکویش کرنا ہے، جیسے اسکیلر۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector. ",
  "translatedText": "اس مخصوص مثال کے لیے، بنیاد ویکٹر i-hat ایک ایسا ہی خاص ویکٹر ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis. ",
  "translatedText": "i-ہیٹ کا دورانیہ x-axis ہے، اور میٹرکس کے پہلے کالم سے، ہم دیکھ سکتے ہیں کہ i-hat خود سے 3 گنا بڑھتا ہے، اب بھی اس x-axis پر ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span. ",
  "translatedText": "مزید کیا ہے، لکیری تبدیلیوں کے کام کرنے کے طریقے کی وجہ سے، x-axis پر کوئی دوسرا ویکٹر بھی صرف 3 کے فیکٹر سے پھیلا ہوا ہے، اور اس وجہ سے وہ اپنے اسپین پر رہتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1. ",
  "translatedText": "اس تبدیلی کے دوران ایک قدرے چپکے والا ویکٹر جو اپنی مدت پر رہتا ہے منفی 1، 1 ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2. ",
  "translatedText": "یہ 2 کے فیکٹر کے ذریعے پھیلا ہوا ختم ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2. ",
  "translatedText": "اور ایک بار پھر، لکیرییت کا مطلب یہ ہے کہ اس آدمی کی طرف سے پھیلی اخترن لکیر پر کوئی دوسرا ویکٹر صرف 2 کے فیکٹر سے پھیلا ہوا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span. ",
  "translatedText": "اور اس تبدیلی کے لیے، یہ وہ تمام ویکٹر ہیں جو اپنے دورانیے پر رہنے کی اس خاص خاصیت کے ساتھ ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2. ",
  "translatedText": "جو ایکس محور پر ہیں وہ 3 کے عنصر سے پھیلے ہوئے ہیں، اور جو اس ترچھی لکیر پر ہیں وہ 2 کے عنصر سے پھیلے ہوئے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans. ",
  "translatedText": "کوئی بھی دوسرا ویکٹر تبدیلی کے دوران کسی حد تک گھمایا جائے گا، اس لائن کو بند کر دیا گیا ہے جس پر یہ پھیلا ہوا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation. ",
  "translatedText": "جیسا کہ آپ اب تک اندازہ لگا چکے ہوں گے، ان خصوصی ویکٹرز کو تبدیلی کے eigenvectors کہا جاتا ہے، اور ہر eigenvector اس کے ساتھ منسلک ہوتا ہے جسے eigenvalue کہا جاتا ہے، یہ صرف وہ عنصر ہے جس کے ذریعے تبدیلی کے دوران اسے پھیلا یا جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing or the fact that these eigenvalues happen to be positive. ",
  "translatedText": "بلاشبہ، سٹریچنگ بمقابلہ squishing یا اس حقیقت کے بارے میں کوئی خاص بات نہیں ہے کہ یہ eigenvalues مثبت ہوتی ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half. ",
  "translatedText": "ایک اور مثال میں، آپ کے پاس eigenvalue منفی 1 نصف کے ساتھ ایک eigenvector ہو سکتا ہے، اس کا مطلب ہے کہ ویکٹر 1 نصف کے فیکٹر سے پلٹ جاتا ہے اور سکوئش ہو جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it. ",
  "translatedText": "لیکن یہاں اہم حصہ یہ ہے کہ یہ اس لائن پر رہتا ہے جس سے یہ گھمائے بغیر پھیل جاتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation. ",
  "translatedText": "اس کے بارے میں سوچنے کے لیے یہ ایک مفید چیز کیوں ہو سکتی ہے اس کی ایک جھلک کے لیے، کچھ تین جہتی گردش پر غور کریں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation. ",
  "translatedText": "اگر آپ اس گردش کے لیے ایک ایجین ویکٹر تلاش کر سکتے ہیں، ایک ایسا ویکٹر جو اپنے اسپین پر رہتا ہے، جو آپ نے پایا ہے وہ گردش کا محور ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3 by 3 matrix associated with that transformation. ",
  "translatedText": "اور اس تبدیلی سے وابستہ مکمل 3 بائی 3 میٹرکس کے بارے میں سوچنے کے بجائے گردش کے کچھ محور اور ایک زاویہ جس کے ذریعہ یہ گھوم رہا ہے کے لحاظ سے 3D گردش کے بارے میں سوچنا بہت آسان ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same. ",
  "translatedText": "اس صورت میں، ویسے، متعلقہ eigenvalue 1 ہونا ضروری ہے، کیونکہ گردشیں کبھی بھی کسی چیز کو نہیں کھینچتی ہیں اور نہ ہی اسکویش کرتی ہیں، اس لیے ویکٹر کی لمبائی وہی رہے گی۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra. ",
  "translatedText": "یہ نمونہ لکیری الجبرا میں بہت زیادہ ظاہر ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors. ",
  "translatedText": "میٹرکس کے ذریعہ بیان کردہ کسی بھی لکیری تبدیلی کے ساتھ، آپ اس میٹرکس کے کالموں کو بنیاد ویکٹر کے لیے لینڈنگ اسپاٹس کے طور پر پڑھ کر سمجھ سکتے ہیں کہ یہ کیا کر رہا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues. ",
  "translatedText": "لیکن اکثر، آپ کے مخصوص کوآرڈینیٹ سسٹم پر کم انحصار کرتے ہوئے، لکیری تبدیلی دراصل کیا کرتی ہے، اس کے دل میں جانے کا ایک بہتر طریقہ یہ ہے کہ eigenvectors اور eigenvalues کو تلاش کیا جائے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding. ",
  "translatedText": "میں یہاں eigenvectors اور eigenvalues کی کمپیوٹنگ کے طریقوں پر مکمل تفصیلات کا احاطہ نہیں کروں گا، لیکن میں ان کمپیوٹیشنل آئیڈیاز کا ایک جائزہ پیش کرنے کی کوشش کروں گا جو تصوراتی تفہیم کے لیے سب سے اہم ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like. ",
  "translatedText": "علامتی طور پر، یہاں یہ ہے کہ ایک eigenvector کا خیال کیسا لگتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue. ",
  "translatedText": "A میٹرکس ہے جو کچھ تبدیلی کی نمائندگی کرتا ہے، جس میں v بطور eigenvector ہے، اور lambda ایک عدد ہے، یعنی متعلقہ eigenvalue۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda. ",
  "translatedText": "یہ ایکسپریشن جو کہہ رہا ہے وہ یہ ہے کہ میٹرکس ویکٹر پروڈکٹ، A times v، وہی نتیجہ دیتا ہے جو صرف eigenvector v کو کچھ ویلیو لیمبڈا کے ذریعے اسکیل کرنا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true. ",
  "translatedText": "لہذا میٹرکس A کے eigenvectors اور ان کی eigenvalues کو تلاش کرنا v اور lambda کی قدروں کو تلاش کرنے کے لیے نیچے آتا ہے جو اس اظہار کو درست بناتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication. ",
  "translatedText": "شروع میں اس کے ساتھ کام کرنا تھوڑا سا عجیب ہے کیونکہ بائیں ہاتھ کی طرف میٹرکس ویکٹر ضرب کی نمائندگی کرتا ہے، لیکن یہاں دائیں طرف اسکیلر ویکٹر ضرب ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda. ",
  "translatedText": "تو آئیے اس میٹرکس کا استعمال کرتے ہوئے دائیں ہاتھ کی طرف کو کسی قسم کے میٹرکس ویکٹر ضرب کے طور پر دوبارہ لکھ کر شروع کرتے ہیں جس میں لیمبڈا کے عنصر کے ذریعہ کسی بھی ویکٹر کو اسکیل کرنے کا اثر ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else. ",
  "translatedText": "اس طرح کے میٹرکس کے کالم اس بات کی نمائندگی کریں گے کہ ہر بنیاد ویکٹر کے ساتھ کیا ہوتا ہے، اور ہر بنیاد ویکٹر کو صرف لیمبڈا سے ضرب دیا جاتا ہے، اس لیے اس میٹرکس میں ہر جگہ صفر کے ساتھ، اخترن کے نیچے لیمبڈا کا نمبر ہوگا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with ones down the diagonal. ",
  "translatedText": "اس آدمی کو لکھنے کا عام طریقہ یہ ہے کہ لیمبڈا کو فیکٹر کریں اور اسے لیمبڈا ٹائمز i کے طور پر لکھیں، جہاں i شناختی میٹرکس ہے جس میں ڈائیگنل نیچے والے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v. ",
  "translatedText": "دونوں اطراف میٹرکس ویکٹر ضرب کی طرح نظر آنے کے ساتھ، ہم اس دائیں طرف کو گھٹا سکتے ہیں اور v کو فیکٹر آؤٹ کر سکتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix, times v, gives the zero vector. ",
  "translatedText": "تو اب ہمارے پاس ایک نیا میٹرکس ہے، A مائنس لیمبڈا اوقات شناخت، اور ہم ایک ویکٹر v کی تلاش کر رہے ہیں کہ یہ نیا میٹرکس، اوقات v، صفر ویکٹر دیتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring. ",
  "translatedText": "اب، یہ ہمیشہ سچ ہوگا اگر v خود صفر ویکٹر ہے، لیکن یہ بورنگ ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector. ",
  "translatedText": "ہم جو چاہتے ہیں وہ ایک غیر صفر ایگن ویکٹر ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension. ",
  "translatedText": "اور اگر آپ باب 5 اور 6 دیکھتے ہیں، تو آپ کو معلوم ہو جائے گا کہ غیر صفر ویکٹر والے میٹرکس کی پیداوار کے صفر بننے کا واحد طریقہ یہ ہے کہ اس میٹرکس سے وابستہ تبدیلی خلا کو کم جہت میں نچوڑ دے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix. ",
  "translatedText": "اور یہ squishification میٹرکس کے لیے صفر کے تعین کنندہ کے مساوی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry. ",
  "translatedText": "ٹھوس ہونے کے لیے، فرض کریں کہ آپ کے میٹرکس A میں کالم 2، 1 اور 2، 3 ہیں، اور ہر ترچھی اندراج سے متغیر رقم، لیمبڈا کو گھٹانے کے بارے میں سوچیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value. ",
  "translatedText": "اب لیمبڈا کو ٹویک کرنے کا تصور کریں، اس کی قدر کو تبدیل کرنے کے لیے دستک کو موڑیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes. ",
  "translatedText": "جیسے جیسے لیمبڈا کی قدر بدلتی ہے، میٹرکس خود بدل جاتا ہے، اور اسی طرح میٹرکس کا تعین کنندہ بدل جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension. ",
  "translatedText": "یہاں مقصد لیمبڈا کی ایک قدر تلاش کرنا ہے جو اس فیصلہ کن کو صفر کردے گا، یعنی تبدیلی کی گئی تبدیلی خلا کو کم جہت میں نچوڑ دیتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1. ",
  "translatedText": "اس معاملے میں، میٹھی جگہ آتی ہے جب لیمبڈا 1 کے برابر ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1. ",
  "translatedText": "بلاشبہ، اگر ہم نے کوئی دوسرا میٹرکس منتخب کیا تھا، تو ضروری نہیں کہ eigenvalue 1 ہو۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda. ",
  "translatedText": "میٹھی جگہ لیمبڈا کی کسی اور قیمت پر پڑ سکتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying. ",
  "translatedText": "تو یہ بہت کچھ ہے، لیکن آئیے کھولتے ہیں کہ یہ کیا کہہ رہا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line. ",
  "translatedText": "جب لیمبڈا 1 کے برابر ہوتا ہے، تو میٹرکس A مائنس لیمبڈا گنا شناخت کی جگہ کو ایک لائن پر نچوڑ دیتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector. ",
  "translatedText": "اس کا مطلب ہے کہ ایک غیر صفر ویکٹر v ہے اس طرح کہ A مائنس لیمبڈا گنا شناختی اوقات v صفر ویکٹر کے برابر ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A. ",
  "translatedText": "اور یاد رکھیں، ہمیں اس کی پرواہ کی وجہ یہ ہے کہ اس کا مطلب ہے A times v برابر lambda times v، جسے آپ یہ کہہ کر پڑھ سکتے ہیں کہ ویکٹر v A کا ایک eigenvector ہے، تبدیلی A کے دوران اپنی مدت پر رہتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place. ",
  "translatedText": "اس مثال میں، متعلقہ eigenvalue 1 ہے، لہذا v اصل میں اپنی جگہ پر قائم رہے گا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good. ",
  "translatedText": "توقف کریں اور غور کریں اگر آپ کو یہ یقینی بنانا ہے کہ استدلال کی وہ لائن اچھی لگے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction. ",
  "translatedText": "یہ وہ قسم ہے جس کا میں نے تعارف میں ذکر کیا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue. ",
  "translatedText": "اگر آپ کے پاس تعین کنندگان کی ٹھوس گرفت نہیں تھی اور وہ غیر صفر حل والے مساوات کے لکیری نظاموں سے کیوں تعلق رکھتے ہیں، تو اس طرح کا اظہار بالکل نیلے رنگ سے باہر محسوس ہوگا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2. ",
  "translatedText": "اس کو عملی شکل میں دیکھنے کے لیے، آئیے شروع سے مثال کو دوبارہ دیکھیں، ایک میٹرکس کے ساتھ جس کے کالم 3، 0 اور 1، 2 ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant. ",
  "translatedText": "یہ معلوم کرنے کے لیے کہ آیا کوئی ویلیو لیمبڈا ایک ایگن ویلیو ہے، اسے اس میٹرکس کے اخترن سے منہا کریں اور تعین کنندہ کی گنتی کریں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda. ",
  "translatedText": "ایسا کرنے سے، ہمیں لیمبڈا میں ایک خاص چوکور کثیر الثانی حاصل ہوتا ہے، 3 مائنس لیمبڈا گنا 2 مائنس لیمبڈا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3. ",
  "translatedText": "چونکہ lambda صرف ایک eigenvalue ہو سکتا ہے اگر یہ تعین کنندہ صفر ہو، آپ یہ نتیجہ اخذ کر سکتے ہیں کہ صرف ممکنہ eigenvalues ہیں lambda equals 2 اور lambda equals 3۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero. ",
  "translatedText": "یہ معلوم کرنے کے لیے کہ اصل میں ان ایگن ویلیو میں سے ایک ایگین ویکٹرز کیا ہیں، کہتے ہیں کہ لیمبڈا برابر 2 ہے، لیمبڈا کی اس قدر کو میٹرکس میں لگائیں اور پھر حل کریں کہ یہ ترچھی تبدیل شدہ میٹرکس کن ویکٹرز کے لیے صفر پر بھیجتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1. ",
  "translatedText": "اگر آپ اس طرح سے کسی دوسرے لکیری نظام کی گنتی کرتے ہیں، تو آپ دیکھیں گے کہ حل منفی 1، 1 تک پھیلی اخترن لائن پر موجود تمام ویکٹر ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2. ",
  "translatedText": "یہ اس حقیقت سے مطابقت رکھتا ہے کہ غیر تبدیل شدہ میٹرکس، 3، 0، 1، 2، ان تمام ویکٹروں کو 2 کے عنصر سے پھیلانے کا اثر رکھتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors. ",
  "translatedText": "اب، 2D تبدیلی میں eigenvectors کا ہونا ضروری نہیں ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees. ",
  "translatedText": "مثال کے طور پر، 90 ڈگری کی گردش پر غور کریں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span. ",
  "translatedText": "اس میں کوئی ایجین ویکٹر نہیں ہے کیونکہ یہ ہر ویکٹر کو اپنے اسپین سے دور گھماتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens. ",
  "translatedText": "اگر آپ واقعی اس طرح کی گردش کی eigenvalues کو کمپیوٹنگ کرنے کی کوشش کرتے ہیں تو دیکھیں کہ کیا ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0. ",
  "translatedText": "اس کے میٹرکس میں کالم 0، 1 اور منفی 1، 0 ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero. ",
  "translatedText": "اخترن عناصر سے لیمبڈا کو گھٹائیں اور دیکھیں کہ جب تعین کنندہ صفر ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1. ",
  "translatedText": "اس صورت میں، آپ کو کثیر الثانی لیمبڈا مربع جمع 1 ملتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i. ",
  "translatedText": "اس کثیر الجہتی کی واحد جڑیں خیالی اعداد ہیں، i اور منفی i۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors. ",
  "translatedText": "حقیقت یہ ہے کہ کوئی حقیقی نمبر حل نہیں ہے اس بات کی نشاندہی کرتا ہے کہ کوئی eigenvectors نہیں ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear. ",
  "translatedText": "آپ کے دماغ کے پیچھے رکھنے کے قابل ایک اور خوبصورت دلچسپ مثال ایک قینچ ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1. ",
  "translatedText": "یہ i-hat کو جگہ پر ٹھیک کرتا ہے اور j-hat 1 کو اوپر لے جاتا ہے، لہذا اس کے میٹرکس میں کالم 1، 0 اور 1، 1 ہوتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place. ",
  "translatedText": "x-axis پر موجود تمام ویکٹرز eigenvectors ہیں eigenvalue 1 کے ساتھ کیونکہ وہ اپنی جگہ پر قائم رہتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors. ",
  "translatedText": "اصل میں، یہ صرف eigenvectors ہیں. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared. ",
  "translatedText": "جب آپ لیمبڈا کو اخترن سے گھٹاتے ہیں اور تعین کنندہ کی گنتی کرتے ہیں، تو آپ کو 1 مائنس لیمبڈا مربع ملتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1. ",
  "translatedText": "اور اس اظہار کی واحد جڑ لیمبڈا برابر 1 ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1. ",
  "translatedText": "یہ اس بات کے مطابق ہے جو ہم ہندسی طور پر دیکھتے ہیں، کہ تمام eigenvectors کی eigenvalue 1 ہوتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors. ",
  "translatedText": "اگرچہ ذہن میں رکھیں، یہ بھی ممکن ہے کہ صرف ایک eigenvalue ہو، لیکن eigenvectors سے بھری لائن سے زیادہ کے ساتھ۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2. ",
  "translatedText": "ایک سادہ مثال ایک میٹرکس ہے جو ہر چیز کو 2 سے پیمانہ کرتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue. ",
  "translatedText": "صرف eigenvalue 2 ہے، لیکن جہاز میں ہر ویکٹر اس eigenvalue کے ساتھ ایک eigenvector بنتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic. ",
  "translatedText": "آخری موضوع پر جانے سے پہلے اس میں سے کچھ توقف اور غور کرنے کا اب ایک اور اچھا وقت ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video. ",
  "translatedText": "میں یہاں ایک eigenbasis کے خیال کو ختم کرنا چاہتا ہوں، جو آخری ویڈیو کے آئیڈیاز پر بہت زیادہ انحصار کرتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors. ",
  "translatedText": "اس پر ایک نظر ڈالیں کہ کیا ہوتا ہے اگر ہمارے بنیادی ویکٹر بالکل اسی طرح eigenvectors ہوتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1, and j-hat is scaled by 2. ",
  "translatedText": "مثال کے طور پر، ہو سکتا ہے کہ i-hat کو منفی 1 سے چھوٹا کیا جائے، اور j-hat کو 2 سے چھوٹا کیا جائے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0. ",
  "translatedText": "میٹرکس کے کالم کے طور پر ان کے نئے نقاط کو لکھتے ہوئے، نوٹ کریں کہ وہ اسکیلر ملٹیلز، منفی 1 اور 2، جو i-hat اور j-hat کی ایگن ویلیوز ہیں، ہمارے میٹرکس کے اخترن پر بیٹھتے ہیں، اور ہر دوسرا اندراج 0 ہوتا ہے۔ . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has 0s everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix, and the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues. ",
  "translatedText": "کسی بھی وقت جب میٹرکس میں اخترن کے علاوہ ہر جگہ 0s ہوتا ہے، اسے کہا جاتا ہے، معقول حد تک، ایک اخترن میٹرکس، اور اس کی تشریح کرنے کا طریقہ یہ ہے کہ تمام بنیاد ویکٹر ایجین ویکٹر ہیں، اس میٹرکس کے اخترن اندراجات کے ساتھ ان کی eigenvalues ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with. ",
  "translatedText": "ایسی بہت سی چیزیں ہیں جو اخترن میٹرکس کو کام کرنے کے لیے بہت اچھی بناتی ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times. ",
  "translatedText": "ایک بڑی بات یہ ہے کہ یہ حساب لگانا آسان ہے کہ اگر آپ اس میٹرکس کو خود سے کئی گنا ضرب دیں گے تو کیا ہوگا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue. ",
  "translatedText": "چونکہ ان تمام میٹرکس میں سے ہر ایک بنیادی ویکٹر کو کسی نہ کسی eigenvalue سے پیمانہ کرنا ہوتا ہے، اس میٹرکس کو کئی بار لاگو کرنا، 100 بار کہنا، ہر بیس ویکٹر کو متعلقہ eigenvalue کی 100ویں طاقت سے اسکیل کرنے کے مساوی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix. ",
  "translatedText": "اس کے برعکس، غیر اخترن میٹرکس کی 100 ویں طاقت کو کمپیوٹنگ کرنے کی کوشش کریں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment. ",
  "translatedText": "واقعی، ایک لمحے کے لیے اسے آزمائیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare. ",
  "translatedText": "یہ ایک ڈراؤنا خواب ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors. ",
  "translatedText": "بلاشبہ، آپ شاذ و نادر ہی اتنے خوش قسمت ہوں گے کہ آپ کے بنیادی ویکٹر بھی eigenvectors ہوں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors. ",
  "translatedText": "لیکن اگر آپ کی تبدیلی میں بہت سارے eigenvectors ہیں، جیسا کہ اس ویڈیو کے آغاز سے، کافی ہے تاکہ آپ ایک سیٹ کا انتخاب کر سکیں جو پوری جگہ پر محیط ہو، پھر آپ اپنے کوآرڈینیٹ سسٹم کو تبدیل کر سکتے ہیں تاکہ یہ eigenvectors آپ کے بنیادی ویکٹر ہوں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system. ",
  "translatedText": "میں نے پچھلی ویڈیو کی بنیاد کی تبدیلی کے بارے میں بات کی تھی، لیکن میں یہاں ایک انتہائی تیز یاد دہانی کے ذریعے جاؤں گا کہ اس وقت ہمارے کوآرڈینیٹ سسٹم میں لکھی گئی تبدیلی کو مختلف نظام میں کیسے ظاہر کیا جائے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix. ",
  "translatedText": "ان ویکٹرز کے نقاط کو لیں جنہیں آپ ایک نئی بنیاد کے طور پر استعمال کرنا چاہتے ہیں، جس کا مطلب اس صورت میں ہمارے دو ایجین ویکٹر ہیں، پھر ان نقاط کو میٹرکس کے کالم بنائیں، جسے بنیاد میٹرکس کی تبدیلی کے نام سے جانا جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system. ",
  "translatedText": "جب آپ اصل تبدیلی کو سینڈویچ کرتے ہیں، بیس میٹرکس کی تبدیلی کو اس کے دائیں طرف اور اس کے بائیں طرف بیس میٹرکس کی تبدیلی کے الٹا لگاتے ہیں، تو نتیجہ ایک میٹرکس ہوگا جو اسی تبدیلی کی نمائندگی کرتا ہے، لیکن نئے بنیادوں کے ویکٹرز کوآرڈینیٹ کے نقطہ نظر سے نظام ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal. ",
  "translatedText": "eigenvectors کے ساتھ ایسا کرنے کا پورا نکتہ یہ ہے کہ یہ نیا میٹرکس اس اخترن کے نیچے اس کی متعلقہ eigenvalues کے ساتھ اخترن ہونے کی ضمانت ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation. ",
  "translatedText": "اس کی وجہ یہ ہے کہ یہ ایک کوآرڈینیٹ سسٹم میں کام کرنے کی نمائندگی کرتا ہے جہاں بنیادی ویکٹر کے ساتھ کیا ہوتا ہے کہ وہ تبدیلی کے دوران پیمانہ ہوجاتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis. ",
  "translatedText": "بنیاد ویکٹرز کا ایک مجموعہ جو eigenvectors بھی ہیں، ایک بار پھر، معقول حد تک، ایک eigenbasis کہلاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system. ",
  "translatedText": "لہذا اگر، مثال کے طور پر، آپ کو اس میٹرکس کی 100ویں طاقت کی گنتی کرنے کی ضرورت ہے، تو اسے eigenbasis میں تبدیل کرنا، اس نظام میں 100ویں طاقت کا حساب لگانا، پھر ہمارے معیاری نظام میں تبدیل کرنا بہت آسان ہوگا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations. ",
  "translatedText": "آپ یہ تمام تبدیلیوں کے ساتھ نہیں کر سکتے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space. ",
  "translatedText": "مثال کے طور پر، ایک قینچ میں پوری جگہ کو پھیلانے کے لیے کافی eigenvectors نہیں ہوتے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely. ",
  "translatedText": "لیکن اگر آپ کو ایک eigenbasis مل سکتا ہے، تو یہ میٹرکس آپریشنز کو واقعی خوبصورت بنا دیتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen. ",
  "translatedText": "آپ میں سے ان لوگوں کے لیے جو یہ دیکھنے کے لیے ایک صاف ستھرا پہیلی کے ذریعے کام کرنا چاہتے ہیں کہ یہ عمل میں کیسا لگتا ہے اور اسے کچھ حیران کن نتائج پیدا کرنے کے لیے کیسے استعمال کیا جا سکتا ہے، میں یہاں اسکرین پر ایک اشارہ چھوڑوں گا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it. ",
  "translatedText": "اس میں تھوڑا سا کام لگتا ہے، لیکن مجھے لگتا ہے کہ آپ اس سے لطف اندوز ہوں گے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces. ",
  "translatedText": "اس سیریز کی اگلی اور آخری ویڈیو خلاصہ ویکٹر اسپیس پر ہونے جا رہی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1005.38
 },
 {
  "input": "See you then! ",
  "translatedText": "پھر آپ دیکھیں! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.9,
  "end": 1006.12
 }
]