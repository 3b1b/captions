[
 {
  "translatedText": "Los vectores propios y los valores propios es uno de esos temas que muchos estudiantes encuentran particularmente poco intuitivo.",
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "time_range": [
   19.92,
   25.76
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Preguntas como por qué hacemos esto y qué significa realmente, con demasiada frecuencia quedan flotando en un mar de cálculos sin respuesta.",
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "time_range": [
   25.76,
   33.26
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Y a medida que publiqué los videos de esta serie, muchos de ustedes han comentado que esperan visualizar este tema en particular.",
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "time_range": [
   33.92,
   40.06
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Sospecho que la razón de esto no es tanto que las cosas propias sean particularmente complicadas o estén mal explicadas.",
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "time_range": [
   40.68,
   46.36
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "De hecho, es comparativamente sencillo y creo que la mayoría de los libros lo explican muy bien.",
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "time_range": [
   46.86,
   51.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "El problema es que sólo tiene sentido si tienes una comprensión visual sólida de muchos de los temas que lo preceden.",
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "time_range": [
   51.52,
   58.48
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Lo más importante aquí es que sepas pensar en las matrices como transformaciones lineales, pero también debes sentirte cómodo con cosas como determinantes, sistemas lineales de ecuaciones y cambios de base.",
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "time_range": [
   59.06,
   69.94
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "La confusión sobre las cosas propias suele tener más que ver con una base inestable en uno de estos temas que con los vectores propios y los valores propios.",
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "time_range": [
   70.72,
   79.24
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Para empezar, considere alguna transformación lineal en dos dimensiones, como la que se muestra aquí.",
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "time_range": [
   79.98,
   84.84
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Mueve el vector base i-hat a las coordenadas 3, 0 y j-hat a 1, 2.",
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "time_range": [
   85.46,
   91.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Entonces se representa con una matriz cuyas columnas son 3, 0 y 1, 2.",
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "time_range": [
   91.78,
   95.64
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Concéntrese en lo que le hace a un vector en particular y piense en el alcance de ese vector, la línea que pasa por su origen y su punta.",
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "time_range": [
   96.6,
   104.16
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "La mayoría de los vectores quedarán fuera de su alcance durante la transformación.",
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "time_range": [
   104.92,
   108.38
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Quiero decir, parecería bastante coincidente si el lugar donde aterrizó el vector también estuviera en algún lugar de esa línea.",
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "time_range": [
   108.78,
   115.32
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Pero algunos vectores especiales permanecen en su propio lapso, lo que significa que el efecto que tiene la matriz sobre dicho vector es simplemente estirarlo o aplastarlo, como un escalar.",
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "time_range": [
   117.4,
   127.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Para este ejemplo específico, el vector base i-hat es uno de esos vectores especiales.",
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "time_range": [
   129.46,
   134.1
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "El intervalo de i-hat es el eje x, y desde la primera columna de la matriz, podemos ver que i-hat se mueve 3 veces él mismo, todavía en ese eje x.",
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "time_range": [
   134.64,
   144.12
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Es más, debido a la forma en que funcionan las transformaciones lineales, cualquier otro vector en el eje x también se estira en un factor de 3 y, por lo tanto, permanece en su propio tramo.",
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "time_range": [
   146.32,
   156.48
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Un vector un poco más astuto que permanece en su propio lapso durante esta transformación es negativo 1, 1.",
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "time_range": [
   158.5,
   164.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Termina estirándose por un factor de 2.",
  "input": "It ends up getting stretched by a factor of 2.",
  "time_range": [
   164.66,
   167.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Y nuevamente, la linealidad implicará que cualquier otro vector en la línea diagonal trazada por este tipo simplemente se estirará en un factor de 2.",
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "time_range": [
   169.0,
   178.22
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Y para esta transformación, esos son todos los vectores con esta propiedad especial de permanecer en su tramo.",
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "time_range": [
   179.82,
   185.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Los que están en el eje x se estiran en un factor de 3 y los que están en esta línea diagonal se estiran en un factor de 2.",
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "time_range": [
   185.62,
   191.98
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Cualquier otro vector será rotado un poco durante la transformación, eliminado de la línea que abarca.",
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "time_range": [
   192.76,
   198.08
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Como ya habrás adivinado, estos vectores especiales se llaman vectores propios de la transformación, y cada vector propio tiene asociado lo que se llama un valor propio, que es simplemente el factor por el cual se estira o se aplasta durante la transformación.",
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "time_range": [
   202.52,
   217.38
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Por supuesto, no hay nada especial en estirar versus aplastar, o en el hecho de que estos valores propios resulten ser positivos.",
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "time_range": [
   220.28,
   225.94
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "En otro ejemplo, podría tener un vector propio con valor propio negativo 1 mitad, lo que significa que el vector se voltea y se aplasta en un factor de 1 mitad.",
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "time_range": [
   226.38,
   235.12
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Pero la parte importante aquí es que permanece en la línea que se extiende sin salirse de ella.",
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "time_range": [
   236.98,
   242.76
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Para tener una idea de por qué podría ser útil pensar en esto, considere una rotación tridimensional.",
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "time_range": [
   244.46,
   249.8
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Si puedes encontrar un vector propio para esa rotación, un vector que permanece en su propio lapso, lo que has encontrado es el eje de rotación.",
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "time_range": [
   251.66,
   260.5
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Y es mucho más fácil pensar en una rotación 3D en términos de algún eje de rotación y un ángulo según el cual gira, en lugar de pensar en la matriz completa de 3x3 asociada con esa transformación.",
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "time_range": [
   262.6,
   274.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "En este caso, por cierto, el valor propio correspondiente tendría que ser 1, ya que las rotaciones nunca estiran ni aplastan nada, por lo que la longitud del vector seguiría siendo la misma.",
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "time_range": [
   277.0,
   285.86
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Este patrón aparece mucho en álgebra lineal.",
  "input": "This pattern shows up a lot in linear algebra.",
  "time_range": [
   288.08,
   290.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Con cualquier transformación lineal descrita por una matriz, se puede entender lo que hace leyendo las columnas de esta matriz como puntos de aterrizaje para los vectores base.",
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "time_range": [
   290.44,
   299.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Pero a menudo, una mejor manera de llegar al núcleo de lo que realmente hace la transformación lineal, menos dependiente de su sistema de coordenadas particular, es encontrar los vectores propios y los valores propios.",
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "time_range": [
   300.02,
   310.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "No cubriré todos los detalles sobre los métodos para calcular vectores propios y valores propios aquí, pero intentaré dar una visión general de las ideas computacionales que son más importantes para una comprensión conceptual.",
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "time_range": [
   315.46,
   326.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Simbólicamente, así es como se ve la idea de un vector propio.",
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "time_range": [
   327.18,
   330.48
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "A es la matriz que representa alguna transformación, con v como vector propio y lambda es un número, es decir, el valor propio correspondiente.",
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "time_range": [
   331.04,
   339.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Lo que esta expresión dice es que el producto matriz-vector, A multiplicado por v, da el mismo resultado que simplemente escalar el vector propio v por algún valor lambda.",
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "time_range": [
   340.68,
   349.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Entonces, encontrar los vectores propios y sus valores propios de una matriz A se reduce a encontrar los valores de v y lambda que hacen que esta expresión sea verdadera.",
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "time_range": [
   351.0,
   360.1
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Es un poco incómodo trabajar con él al principio, porque el lado izquierdo representa la multiplicación de matriz-vector, pero el lado derecho aquí es la multiplicación de vector escalar.",
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "time_range": [
   361.92,
   370.54
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Entonces, comencemos reescribiendo ese lado derecho como una especie de multiplicación matriz-vector, usando una matriz que tiene el efecto de escalar cualquier vector por un factor lambda.",
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "time_range": [
   371.12,
   380.62
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Las columnas de dicha matriz representarán lo que sucede con cada vector base, y cada vector base simplemente se multiplica por lambda, por lo que esta matriz tendrá el número lambda en la diagonal, con ceros en el resto.",
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "time_range": [
   381.68,
   394.32
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "La forma común de escribir este tipo es factorizar esa lambda y escribirla como lambda multiplicada por i, donde i es la matriz de identidad con 1 en la diagonal.",
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "time_range": [
   396.18,
   404.86
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Como ambos lados parecen una multiplicación de matriz-vector, podemos restar ese lado derecho y factorizar v.",
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "time_range": [
   405.86,
   411.86
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Entonces, lo que ahora tenemos es una nueva matriz, A menos lambda multiplicada por la identidad, y estamos buscando un vector v tal que esta nueva matriz multiplicada por v dé el vector cero.",
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "time_range": [
   414.16,
   424.92
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ahora bien, esto siempre será cierto si v mismo es el vector cero, pero eso es aburrido.",
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "time_range": [
   426.38,
   431.1
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Lo que queremos es un vector propio distinto de cero.",
  "input": "What we want is a non-zero eigenvector.",
  "time_range": [
   431.34,
   433.64
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Y si miras los capítulos 5 y 6, sabrás que la única forma en que es posible que el producto de una matriz con un vector distinto de cero se convierta en cero es si la transformación asociada con esa matriz reduce el espacio a una dimensión inferior.",
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "time_range": [
   434.42,
   448.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Y esa compresión corresponde a un determinante cero para la matriz.",
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "time_range": [
   449.3,
   454.22
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Para ser concretos, digamos que su matriz A tiene las columnas 2, 1 y 2, 3, y piense en restar una cantidad variable, lambda, de cada entrada diagonal.",
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "time_range": [
   455.48,
   465.52
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ahora imagina ajustar lambda, girando una perilla para cambiar su valor.",
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "time_range": [
   466.48,
   470.28
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "A medida que cambia ese valor de lambda, la matriz misma cambia y, por lo tanto, cambia el determinante de la matriz.",
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "time_range": [
   470.94,
   477.24
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "El objetivo aquí es encontrar un valor de lambda que haga que este determinante sea cero, lo que significa que la transformación modificada aplasta el espacio a una dimensión inferior.",
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "time_range": [
   478.22,
   487.24
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "En este caso, el punto óptimo se produce cuando lambda es igual a 1.",
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "time_range": [
   488.16,
   491.16
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Por supuesto, si hubiéramos elegido otra matriz, el valor propio no necesariamente sería 1.",
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "time_range": [
   492.18,
   496.12
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "El punto óptimo podría alcanzarse con algún otro valor de lambda.",
  "input": "The sweet spot might be hit at some other value of lambda.",
  "time_range": [
   496.24,
   498.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Esto es mucho, pero analicemos lo que dice.",
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "time_range": [
   500.08,
   502.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Cuando lambda es igual a 1, la matriz A menos lambda multiplicada por la identidad aplasta el espacio en una línea.",
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "time_range": [
   502.96,
   509.56
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Eso significa que hay un vector v distinto de cero tal que A menos lambda multiplicado por la identidad multiplicado por v es igual al vector cero.",
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "time_range": [
   510.44,
   518.56
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Y recuerde, la razón por la que esto nos importa es porque significa que A multiplicado por v es igual a lambda multiplicado por v, lo que se puede interpretar como que el vector v es un vector propio de A, que permanece en su propio lapso durante la transformación A.",
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "time_range": [
   520.48,
   537.28
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "En este ejemplo, el valor propio correspondiente es 1, por lo que v en realidad permanecería fijo en su lugar.",
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "time_range": [
   538.32,
   544.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Haga una pausa y reflexione si necesita asegurarse de que esa línea de razonamiento se sienta bien.",
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "time_range": [
   546.22,
   549.5
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Este es el tipo de cosas que mencioné en la introducción.",
  "input": "This is the kind of thing I mentioned in the introduction.",
  "time_range": [
   553.38,
   555.64
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Si no tuvieras una comprensión sólida de los determinantes y de por qué se relacionan con sistemas lineales de ecuaciones que tienen soluciones distintas de cero, una expresión como esta parecería completamente inesperada.",
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "time_range": [
   556.22,
   566.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Para ver esto en acción, volvamos al ejemplo desde el principio, con una matriz cuyas columnas son 3, 0 y 1, 2.",
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "time_range": [
   568.32,
   574.54
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Para encontrar si un valor lambda es un valor propio, réstalo de las diagonales de esta matriz y calcula el determinante.",
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "time_range": [
   575.35,
   583.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Al hacer esto, obtenemos un determinado polinomio cuadrático en lambda, 3 menos lambda por 2 menos lambda.",
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "time_range": [
   590.58,
   596.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Dado que lambda solo puede ser un valor propio si este determinante es cero, se puede concluir que los únicos valores propios posibles son lambda igual a 2 y lambda igual a 3.",
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "time_range": [
   597.8,
   608.84
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Para descubrir cuáles son los vectores propios que realmente tienen uno de estos valores propios, digamos que lambda es igual a 2, conecte ese valor de lambda a la matriz y luego resuelva qué vectores esta matriz alterada diagonalmente envía a cero.",
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "time_range": [
   609.64,
   623.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Si calculas esto de la misma manera que lo harías con cualquier otro sistema lineal, verás que las soluciones son todos los vectores en la línea diagonal atravesada por menos 1, 1.",
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "time_range": [
   624.94,
   634.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Esto corresponde al hecho de que la matriz inalterada, 3, 0, 1, 2, tiene el efecto de estirar todos esos vectores por un factor de 2.",
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "time_range": [
   635.22,
   643.46
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ahora bien, una transformación 2D no tiene por qué tener vectores propios.",
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "time_range": [
   646.32,
   650.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Por ejemplo, considere una rotación de 90 grados.",
  "input": "For example, consider a rotation by 90 degrees.",
  "time_range": [
   650.72,
   653.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Esto no tiene vectores propios ya que rota cada vector fuera de su propio intervalo.",
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "time_range": [
   653.66,
   658.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Si realmente intentas calcular los valores propios de una rotación como esta, observa lo que sucede.",
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "time_range": [
   660.8,
   665.56
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Su matriz tiene columnas 0, 1 y negativo 1, 0.",
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "time_range": [
   666.3,
   670.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Resta lambda de los elementos de la diagonal y busca cuándo el determinante es cero.",
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "time_range": [
   671.1,
   675.8
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "En este caso, obtienes el polinomio lambda al cuadrado más 1.",
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "time_range": [
   678.14,
   681.94
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Las únicas raíces de ese polinomio son los números imaginarios, i y i negativo.",
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "time_range": [
   682.68,
   687.92
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "El hecho de que no haya soluciones de números reales indica que no hay vectores propios.",
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "time_range": [
   688.84,
   693.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Otro ejemplo bastante interesante que vale la pena tener en cuenta es una cizalla.",
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "time_range": [
   695.54,
   699.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Esto fija i-hat en su lugar y mueve j-hat 1, por lo que su matriz tiene las columnas 1, 0 y 1, 1.",
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "time_range": [
   700.56,
   707.84
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Todos los vectores en el eje x son vectores propios con valor propio 1 ya que permanecen fijos en su lugar.",
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "time_range": [
   708.74,
   714.54
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "De hecho, estos son los únicos vectores propios.",
  "input": "In fact, these are the only eigenvectors.",
  "time_range": [
   715.68,
   717.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Cuando restas lambda de las diagonales y calculas el determinante, lo que obtienes es 1 menos lambda al cuadrado.",
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "time_range": [
   718.76,
   726.54
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Y la única raíz de esta expresión es lambda igual a 1.",
  "input": "And the only root of this expression is lambda equals 1.",
  "time_range": [
   729.32,
   732.86
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Esto se alinea con lo que vemos geométricamente, que todos los vectores propios tienen valor propio 1.",
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "time_range": [
   734.56,
   739.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Sin embargo, tenga en cuenta que también es posible tener un solo valor propio, pero con más que una simple línea llena de vectores propios.",
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "time_range": [
   741.08,
   748.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Un ejemplo sencillo es una matriz que escala todo en 2.",
  "input": "A simple example is a matrix that scales everything by 2.",
  "time_range": [
   749.9,
   753.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "El único valor propio es 2, pero cada vector en el plano llega a ser un vector propio con ese valor propio.",
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "time_range": [
   753.9,
   760.7
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ahora es otro buen momento para hacer una pausa y reflexionar sobre algo de esto antes de pasar al último tema.",
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "time_range": [
   762.0,
   766.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Quiero terminar aquí con la idea de una base propia, que se basa en gran medida en las ideas del último vídeo.",
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "time_range": [
   783.54,
   789.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Eche un vistazo a lo que sucede si nuestros vectores base resultan ser vectores propios.",
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "time_range": [
   791.48,
   796.38
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Por ejemplo, tal vez i-hat esté escalado por 1 negativo y j-hat esté escalado por 2.",
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "time_range": [
   797.12,
   802.38
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Al escribir sus nuevas coordenadas como las columnas de una matriz, observe que esos múltiplos escalares, negativos 1 y 2, que son los valores propios de i-hat y j-hat, se ubican en la diagonal de nuestra matriz, y todas las demás entradas son 0. .",
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "time_range": [
   803.42,
   817.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Cada vez que una matriz tiene ceros en todas partes excepto en la diagonal, se llama, razonablemente, matriz diagonal.",
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "time_range": [
   818.88,
   825.42
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Y la forma de interpretar esto es que todos los vectores base son vectores propios, siendo las entradas diagonales de esta matriz sus valores propios.",
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "time_range": [
   825.84,
   834.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Hay muchas cosas que hacen que sea mucho más agradable trabajar con matrices diagonales.",
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "time_range": [
   837.1,
   841.06
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Uno de los más importantes es que es más fácil calcular lo que sucederá si multiplicas esta matriz por sí misma muchas veces.",
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "time_range": [
   841.78,
   848.34
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Dado que lo único que hace una de estas matrices es escalar cada vector base según algún valor propio, aplicar esa matriz muchas veces, digamos 100 veces, corresponderá a escalar cada vector base según la centésima potencia del valor propio correspondiente.",
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "time_range": [
   849.42,
   864.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Por el contrario, intente calcular la potencia número 100 de una matriz no diagonal.",
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "time_range": [
   865.7,
   869.68
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "De verdad, pruébalo por un momento.",
  "input": "Really, try it for a moment.",
  "time_range": [
   869.68,
   871.32
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Es una pesadilla.",
  "input": "It's a nightmare.",
  "time_range": [
   871.74,
   872.44
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Por supuesto, rara vez tendrás tanta suerte como para que tus vectores base también sean vectores propios.",
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "time_range": [
   876.08,
   881.26
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Pero si tu transformación tiene muchos vectores propios, como el del inicio de este vídeo, suficientes para que puedas elegir un conjunto que abarque todo el espacio, entonces podrías cambiar tu sistema de coordenadas para que estos vectores propios sean tus vectores base.",
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "time_range": [
   882.04,
   896.54
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Hablé sobre el cambio de base en el último video, pero aquí haré un recordatorio súper rápido de cómo expresar una transformación actualmente escrita en nuestro sistema de coordenadas en un sistema diferente.",
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "time_range": [
   897.14,
   907.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Tome las coordenadas de los vectores que desea usar como una nueva base, que en este caso significa nuestros dos vectores propios, luego convierta esas coordenadas en las columnas de una matriz, conocida como matriz de cambio de base.",
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "time_range": [
   908.44,
   919.44
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Cuando intercala la transformación original, poniendo la matriz de cambio de base a su derecha y la inversa de la matriz de cambio de base a su izquierda, el resultado será una matriz que representa esa misma transformación, pero desde la perspectiva de las nuevas coordenadas de los vectores base. sistema.",
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "time_range": [
   920.18,
   936.5
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "El objetivo de hacer esto con vectores propios es que se garantiza que esta nueva matriz será diagonal con sus valores propios correspondientes en esa diagonal.",
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "time_range": [
   937.44,
   946.68
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Esto se debe a que representa trabajar en un sistema de coordenadas donde lo que sucede con los vectores base es que se escalan durante la transformación.",
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "time_range": [
   946.86,
   954.32
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Un conjunto de vectores base que también son vectores propios se denomina, nuevamente, razonablemente, base propia.",
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "time_range": [
   955.8,
   961.56
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Entonces, si, por ejemplo, necesitara calcular la potencia número 100 de esta matriz, sería mucho más fácil cambiar a una base propia, calcular la potencia número 100 en ese sistema y luego volver a convertir a nuestro sistema estándar.",
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "time_range": [
   962.34,
   975.68
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "No puedes hacer esto con todas las transformaciones.",
  "input": "You can't do this with all transformations.",
  "time_range": [
   976.62,
   978.32
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Un corte, por ejemplo, no tiene suficientes vectores propios para abarcar todo el espacio.",
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "time_range": [
   978.32,
   982.98
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Pero si puedes encontrar una base propia, las operaciones matriciales son realmente hermosas.",
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "time_range": [
   983.46,
   988.16
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Para aquellos de ustedes que estén dispuestos a resolver un rompecabezas bastante interesante para ver cómo se ve en acción y cómo se puede usar para producir resultados sorprendentes, dejaré un mensaje aquí en la pantalla.",
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "time_range": [
   989.12,
   997.32
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Requiere un poco de trabajo, pero creo que lo disfrutarás.",
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "time_range": [
   997.6,
   1000.28
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "El siguiente y último vídeo de esta serie tratará sobre espacios vectoriales abstractos.",
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "time_range": [
   1000.84,
   1006.12
  ],
  "n_reviews": 0
 }
]