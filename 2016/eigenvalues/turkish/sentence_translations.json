[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Özvektörler ve özdeğerler birçok öğrencinin özellikle sezgisel bulmadığı konulardan biridir.",
  "model": "google_nmt",
  "from_community_srt": "\"Özvektörler ve özdeğerler\", birçok öğrencinin özellikle sezgisel bulmayacağı konulardan biridir.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Bunu neden yapıyoruz ve bunun gerçekte ne anlama geldiği gibi sorular çoğu zaman cevapsız bir hesaplama denizinde yüzüyor.",
  "model": "google_nmt",
  "from_community_srt": "\"Bunu neden yapıyoruz\" ve \"bu aslında ne anlama geliyor?\" Gibi sorular çok sıkça cevapsız bir hesaplamalar denizinde yüzmeye devam ediyorlar.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "Ve ben bu serinin videolarını yayınladığımda, çoğunuz özellikle bu konuyu görselleştirmeyi sabırsızlıkla beklediğiniz yönünde yorum yaptınız.",
  "model": "google_nmt",
  "from_community_srt": "Ve serinin videolarını ortaya koyarken, birçoğunuz bu konuyu özellikle görselleştirmeyi dört gözle beklediğiniz konusunda yorum yaptınız.",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Bunun nedeninin özşeylerin özellikle karmaşık olması veya yeterince açıklanmaması olmadığını düşünüyorum.",
  "model": "google_nmt",
  "from_community_srt": "ondan şüphelendim Bunun nedeni öz-şeylerin özellikle karmaşık veya kötü bir şekilde açıklandığı kadar değildir.",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "Aslında nispeten basittir ve çoğu kitabın bunu açıklamakta iyi iş çıkardığını düşünüyorum.",
  "model": "google_nmt",
  "from_community_srt": "Aslında, nispeten basit ve çoğu kitabın bunu açıklamak için iyi bir iş çıkardığını düşünüyorum.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Sorun şu ki, yalnızca kendisinden önceki konuların çoğu için sağlam bir görsel anlayışa sahipseniz gerçekten anlamlı olur.",
  "model": "google_nmt",
  "from_community_srt": "Sorun şu ki bu yalnızca ondan önce gelen birçok konu için sağlam bir görsel anlayışınız varsa anlamlıdır.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Burada en önemlisi, matrisleri doğrusal dönüşümler olarak nasıl düşüneceğinizi bilmenizdir, ancak aynı zamanda determinantlar, doğrusal denklem sistemleri ve taban değişimi gibi konularda da rahat olmanız gerekir.",
  "model": "google_nmt",
  "from_community_srt": "Buradaki en önemlisi, matrisler hakkında doğrusal dönüşümler olarak nasıl düşünüleceğini bilmeniz, ama aynı zamanda bazı şeylerde rahat olmalısın belirleyiciler, lineer denklem sistemleri ve temel değişimi gibi.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Öz maddelerle ilgili kafa karışıklığının, özvektörler ve özdeğerlerin kendilerinden çok, bu konulardan birindeki zayıf temelle ilgisi vardır.",
  "model": "google_nmt",
  "from_community_srt": "Özdeğerlerle ilgili kafa karışıklığının genellikle bu konulardan birinin titrek bir temeli ile ilgisi vardır. özvektörlerle ve özdeğerlerin kendisiyle yaptıklarından.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Başlamak için, burada gösterilene benzer, iki boyutlu bir doğrusal dönüşümü düşünün.",
  "model": "google_nmt",
  "from_community_srt": "Başlamak için iki boyutlu bazı doğrusal dönüşümleri düşünün, Burada gösterilen gibi.",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Temel vektör i-hat'ı 3, 0 koordinatlarına ve j-hat'ı 1, 2 koordinatlarına taşır.",
  "model": "google_nmt",
  "from_community_srt": "İ-hat vektörünü koordinatlara (3, 0), j-hat'a (1,",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Yani sütunları 3, 0 ve 1, 2 olan bir matrisle temsil edilir.",
  "model": "google_nmt",
  "from_community_srt": "2) hareket eder, bu yüzden sütunları (3, 0) ve (1, 2) olan bir matris ile temsil edilir.",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Belirli bir vektöre ne yaptığına odaklanın ve o vektörün yayılımını, orijininden ve ucundan geçen çizgiyi düşünün.",
  "model": "google_nmt",
  "from_community_srt": "Belirli bir vektöre ne yaptığına odaklan ve bu vektörün kapsamını, kaynağını ve ucunu geçen çizgiyi düşünün.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "Dönüşüm sırasında çoğu vektörün açıklığı kaybolacak.",
  "model": "google_nmt",
  "from_community_srt": "Dönüşüm sırasında çoğu vektör yayılmalarını azaltacak.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Demek istediğim, vektörün düştüğü yerin de bu çizgi üzerinde bir yerde olması oldukça tesadüfi görünebilir.",
  "model": "google_nmt",
  "from_community_srt": "Demek istediğim, tesadüf gibi görünüyor vektörün indiği yer de bu çizgide bir yerdeyse.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Ancak bazı özel vektörler kendi açıklıklarında kalır; bu, matrisin böyle bir vektör üzerindeki etkisinin, tıpkı bir skaler gibi, onu sadece uzatmak veya ezmek olduğu anlamına gelir.",
  "model": "google_nmt",
  "from_community_srt": "Ancak bazı özel vektörler kendi aralarında kalırlar, yani matrisin böyle bir vektör üzerindeki etkisi, bir skalar gibi sadece onu uzatmak veya ezmek anlamına gelir.",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Bu spesifik örnek için temel vektör i-hat böyle özel bir vektördür.",
  "model": "google_nmt",
  "from_community_srt": "Bu özel örnek için, temel vektör i-hat böyle özel bir vektördür.",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "i-hat'ın açıklığı x eksenidir ve matrisin ilk sütunundan i-hat'in kendisinin 3 katına kadar hareket ettiğini görebiliriz, hâlâ o x ekseni üzerindedir.",
  "model": "google_nmt",
  "from_community_srt": "İ-şapka'nın aralığı x eksenidir, ve matrisin ilk sütunundan, i-hat'ın yine de x ekseninde 3 kat daha fazla hareket ettiğini görebiliyoruz.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Dahası, doğrusal dönüşümlerin çalışma şekli nedeniyle, x ekseni üzerindeki herhangi bir vektör de 3 katı kadar uzar ve dolayısıyla kendi açıklığında kalır.",
  "model": "google_nmt",
  "from_community_srt": "Dahası, doğrusal dönüşümlerin çalışma şekli nedeniyle, X ekseni üzerindeki diğer herhangi bir vektör de sadece 3 faktörü ile gerilir ve bu yüzden kendi sahasında kalır.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Bu dönüşüm sırasında kendi açıklığında kalan biraz daha sinsi bir vektör negatif 1, 1'dir.",
  "model": "google_nmt",
  "from_community_srt": "Bu dönüşüm sırasında kendi sahasında kalan hafifçe sinsi bir vektör (-1,",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Sonunda 2 kat uzar.",
  "model": "google_nmt",
  "from_community_srt": "1), 2 faktörü ile gerilir.",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "Ve yine doğrusallık, bu adamın çapraz çizgi üzerindeki herhangi bir vektörün 2 katı kadar uzayacağını ima edecek.",
  "model": "google_nmt",
  "from_community_srt": "Ve yine, doğrusallık bunu ima edecek diyagonal hatta bu adam tarafından yayılan başka bir vektör sadece 2 kat gerilmiş olacak.",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "Ve bu dönüşüm için, bunların hepsi kendi açıklıkları üzerinde kalma özel özelliğine sahip vektörlerdir.",
  "model": "google_nmt",
  "from_community_srt": "Ve bu dönüşüm için bunların hepsi, bu özelliğin açıklıkta kalma özelliğine sahip vektörlerdir.",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "X eksenindekiler 3 kat, bu çapraz çizgidekiler ise 2 kat uzuyor.",
  "model": "google_nmt",
  "from_community_srt": "X ekseni üzerinde olanlar 3 kat gerilir ve bu çapraz çizgideki olanlar 2 kat gerilir.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Başka herhangi bir vektör, dönüşüm sırasında bir miktar döndürülecek ve yayıldığı çizgiden kopacaktır.",
  "model": "google_nmt",
  "from_community_srt": "Dönüşüm sırasında başka herhangi bir vektör bir miktar döndürülecek, yayıldığı çizgiyi düşürdü.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Şimdiye kadar tahmin etmiş olabileceğiniz gibi, bu özel vektörlere dönüşümün özvektörleri denir ve her bir özvektör onunla özdeğer adı verilen bir şeyle ilişkilendirilir, bu da tam da dönüşüm sırasında gerilmesini veya ezilmesini sağlayan faktördür.",
  "model": "google_nmt",
  "from_community_srt": "Şu ana kadar tahmin etmiş olabileceğiniz gibi bu özel vektörlere dönüşümün \"özvektörleri\" denir, ve her özvektör onunla özdeşleşmiştir, buna “özdeğer” denir, bu sadece dönüşüm sırasında gerilme veya ezilme faktörüdür.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Elbette, esnemeye karşı ezilmenin ya da bu özdeğerlerin pozitif olmasının özel bir tarafı yok.",
  "model": "google_nmt",
  "from_community_srt": "Elbette, esneme ve püskürme konusunda özel bir şey yok. veya bu özdeğerlerin olumlu olduğu gerçeği.",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "Başka bir örnekte, özdeğeri negatif 1 yarı olan bir özvektöre sahip olabilirsiniz; bu, vektörün 1 yarıya kadar ters çevrileceği ve ezileceği anlamına gelir.",
  "model": "google_nmt",
  "from_community_srt": "Başka bir örnekte, özdeğer -1 / 2 olan bir özvektöre sahip olabilirsiniz, bu, vektörün 1/2 kat döndürüldüğü ve ezildiği anlamına gelir.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Ancak burada önemli olan, uzandığı çizginin dışına çıkmadan, doğru üzerinde kalmasıdır.",
  "model": "google_nmt",
  "from_community_srt": "Ancak buradaki önemli kısım, dışına alınmadan yayıldığı çizgide kalmasıdır.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Bunun neden yararlı olabileceğine dair bir fikir edinmek için üç boyutlu döndürmeyi düşünün.",
  "model": "google_nmt",
  "from_community_srt": "Bunun neden düşünülmesi yararlı bir şey olabileceğine bir bakış için, bazı üç boyutlu rotasyon düşünün.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Eğer bu dönme için bir özvektör bulabilirseniz, kendi açıklığında kalan bir vektör, bulduğunuz şey dönme eksenidir.",
  "model": "google_nmt",
  "from_community_srt": "Bu rotasyon için bir özvektör bulabilirseniz, kendi sahasında kalan bir vektör, Bulduğunuz şey dönme eksenidir.",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "Ve bir 3 boyutlu dönüşü, bir dönme ekseni ve döndüğü açı açısından düşünmek, bu dönüşümle ilişkili tam 3x3 matrisi düşünmek yerine çok daha kolaydır.",
  "model": "google_nmt",
  "from_community_srt": "Ve bir 3-D dönüşü hakkında, bazı dönüş eksenleri ve dönme açıları açısından düşünmek çok daha kolaydır. Bu dönüşümle ilgili tam 3'e 3 matris hakkında düşünmek yerine.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "Bu durumda, bu arada, karşılık gelen özdeğerin 1 olması gerekir, çünkü dönüşler hiçbir şeyi germez veya ezmez, dolayısıyla vektörün uzunluğu aynı kalır.",
  "model": "google_nmt",
  "from_community_srt": "Bu durumda, karşılık gelen özdeğer 1 olmalıdır rotasyonlar asla bir şeyi germez ya da ezmez, bu yüzden vektörün uzunluğu aynı kalır.",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Bu model doğrusal cebirde çokça karşımıza çıkar.",
  "model": "google_nmt",
  "from_community_srt": "Bu desen lineer cebirde çok şey gösterir.",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Bir matris tarafından tanımlanan herhangi bir doğrusal dönüşümde, bu matrisin sütunlarını temel vektörlerin iniş noktaları olarak okuyarak ne yaptığını anlayabilirsiniz.",
  "model": "google_nmt",
  "from_community_srt": "Bir matris tarafından tanımlanan herhangi bir doğrusal dönüşümle, ne yaptığını anlayabilirsiniz Temel vektörler için iniş noktaları olarak bu matrisin sütunlarını okuyarak.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Ancak sıklıkla, doğrusal dönüşümün gerçekte ne yaptığını anlamanın daha iyi bir yolu, özel koordinat sisteminize daha az bağlı olarak özvektörleri ve özdeğerleri bulmaktır.",
  "model": "google_nmt",
  "from_community_srt": "Fakat çoğu zaman, doğrusal dönüşümün gerçekte yaptığı şeyin kalbine ulaşmanın daha iyi bir yolu, Özel koordinat sisteminize daha az bağımlı olmak, özvektörleri ve özdeğerleri bulmaktır.",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Burada özvektörleri ve özdeğerleri hesaplama yöntemlerinin tüm ayrıntılarını ele almayacağım, ancak kavramsal bir anlayış için en önemli olan hesaplamalı fikirlere genel bir bakış sunmaya çalışacağım.",
  "model": "google_nmt",
  "from_community_srt": "Burada özvektörleri ve özdeğerleri hesaplamak için kullanılan yöntemlerin tüm ayrıntılarını burada ele almayacağım. ama hesaplama fikirlerine genel bir bakış vermeye çalışacağım kavramsal bir anlayış için en önemli olan budur.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Sembolik olarak özvektör fikri şu şekilde görünür.",
  "model": "google_nmt",
  "from_community_srt": "Sembolik olarak, işte bir özvektör fikri nasıl görünüyor? A,",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A, v'nin özvektör olduğu bir dönüşümü temsil eden matristir ve lambda bir sayıdır, yani karşılık gelen özdeğerdir.",
  "model": "google_nmt",
  "from_community_srt": "bazı dönüşümleri temsil eden matristir, özvektör olarak v ile ve a, bir sayıdır, yani karşılık gelen özdeğerdir.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Bu ifadenin söylemek istediği, matris-vektör çarpımı A çarpı v'nin, özvektör v'yi bir lambda değeriyle ölçeklendirmekle aynı sonucu verdiğidir.",
  "model": "google_nmt",
  "from_community_srt": "Bu ifadenin demek istediği, matris-vektör çarpımı - A çarpı v sadece özvektörün v bir miktar λ ile ölçeklendirilmesiyle aynı sonucu verir.",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Yani bir A matrisinin özvektörlerini ve bunların özdeğerlerini bulmak, bu ifadeyi doğru yapan v ve lambda değerlerini bulmaktan geçer.",
  "model": "google_nmt",
  "from_community_srt": "Böylece özvektörleri ve A matrisinin özdeğerlerini bulma Bu ifadeyi gerçeğe dönüştüren v ve λ değerlerini bulmaya gelir.",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "İlk başta bununla çalışmak biraz garip çünkü sol taraf matris vektör çarpımını temsil ediyor, ancak buradaki sağ taraf skaler vektör çarpımını temsil ediyor.",
  "model": "google_nmt",
  "from_community_srt": "İlk başta çalışmak biraz garip. Çünkü bu sol taraf matris-vektör çarpımını temsil ediyor. ama burada sağ taraf skaler-vektör çarpımıdır.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "O halde, herhangi bir vektörü lambda faktörü ile ölçeklendirme etkisine sahip bir matris kullanarak sağ tarafı bir tür matris-vektör çarpımı olarak yeniden yazmaya başlayalım.",
  "model": "google_nmt",
  "from_community_srt": "Şimdi sağ tarafını bir tür matris-vektör çarpımı olarak yeniden yazarak başlayalım, Herhangi bir vektörü λ faktörü ile ölçekleme etkisine sahip bir matris kullanarak.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Böyle bir matrisin sütunları, her bir temel vektöre ne olduğunu temsil edecektir ve her bir temel vektör basitçe lambda ile çarpılır, dolayısıyla bu matris köşegende lambda sayısına sahip olacak ve diğer her yerde sıfırlar olacaktır.",
  "model": "google_nmt",
  "from_community_srt": "Böyle bir matrisin sütunları, her temel vektöre ne olacağını temsil edecektir, ve her temel vektör basitçe çarpı λ, bu yüzden bu matris, köşegen aşağı doğru λ sayısını her yerde 0'larla olacak.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "Bu adamı yazmanın genel yolu, lambda'yı çarpanlara ayırmak ve bunu lambda çarpı i olarak yazmaktır; burada i, köşegeninde 1'ler bulunan birim matristir.",
  "model": "google_nmt",
  "from_community_srt": "Bu adamı yazmanın en yaygın yolu λ’yı çıkarıp λ çarpı olarak yazmaktır. Burada kimlik matrisi olduğumda 1 ile köşegen aşağı iner.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Her iki taraf da matris-vektör çarpımına benzediğinden, sağ tarafı çıkarıp v'yi çarpanlara ayırabiliriz.",
  "model": "google_nmt",
  "from_community_srt": "Her iki taraf da matris-vektör çarpımına benzer. o sağ taraftan çıkartabilir ve v'yi kaldırabiliriz.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Şimdi elimizde yeni bir matris var, A eksi lambda çarpı birim ve bu yeni matris çarpı v'nin sıfır vektörünü vereceği bir v vektörü arıyoruz.",
  "model": "google_nmt",
  "from_community_srt": "Öyleyse şimdi sahip olduğumuz yeni bir matris - Bir eksi λ kimliğin, ve v vektörünü arıyoruz, öyle ki bu yeni matris çarpı v sıfır vektörünü verir.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Şimdi, v'nin kendisi sıfır vektörüyse bu her zaman doğru olacaktır, ama bu çok sıkıcı.",
  "model": "google_nmt",
  "from_community_srt": "Şimdi, eğer v'nin kendisi sıfır vektör ise, bu her zaman doğru olacaktır. ama bu çok sıkıcı.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Bizim istediğimiz sıfır olmayan bir özvektördür.",
  "model": "google_nmt",
  "from_community_srt": "İstediğimiz şey sıfır olmayan bir özvektör.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "Ve eğer 5. ve 6. bölümleri izlerseniz, sıfır olmayan bir vektöre sahip bir matrisin çarpımının sıfır olmasının mümkün olmasının tek yolunun, o matrisle ilişkili dönüşümün uzayı daha düşük bir boyuta sıkıştırması olduğunu bileceksiniz.",
  "model": "google_nmt",
  "from_community_srt": "Ve 5. ve 6. Bölümleri izlediyseniz, sıfır olmayan bir vektöre sahip bir matrisin sıfıra dönüşmesinin tek yolunun mümkün olduğunu bilirsiniz eğer bu matris ile ilişkili dönüşüm, alanı daha düşük bir boyuta sazlarsa.",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "Ve bu ezme matris için sıfır determinantına karşılık gelir.",
  "model": "google_nmt",
  "from_community_srt": "Ve bu squishification matris için sıfır belirleyici karşılık gelir.",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Daha somut olmak gerekirse, A matrisinizin 2, 1 ve 2, 3 numaralı sütunları olduğunu varsayalım ve her çapraz girişten değişken bir miktar olan lambda'yı çıkarmayı düşünün.",
  "model": "google_nmt",
  "from_community_srt": "Somut olmak gerekirse, matrisinizde a'nın (2, 1) ve (2, 3) sütunlarına sahip olduğunu varsayalım. ve her diyagonal girişten değişken bir miktar λ çıkarmayı düşünün.",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Şimdi lambda'da ince ayar yaptığınızı, değerini değiştirmek için bir düğmeyi çevirdiğinizi hayal edin.",
  "model": "google_nmt",
  "from_community_srt": "Şimdi λ ayarını, değerini değiştirmek için bir topuzu çevirdiğini hayal edin.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "Lambda'nın değeri değiştikçe matrisin kendisi de değişir ve dolayısıyla matrisin determinantı da değişir.",
  "model": "google_nmt",
  "from_community_srt": "Λ'nin değeri değiştikçe, matrisin kendisi değişir ve bu nedenle matrisin determinantı değişir.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Buradaki amaç, bu determinantı sıfır yapacak bir lambda değeri bulmaktır; bu, ince ayarlı dönüşümün alanı daha düşük bir boyuta sıkıştırması anlamına gelir.",
  "model": "google_nmt",
  "from_community_srt": "Buradaki amaç, bu determinantı sıfır yapacak λ değerini bulmak, yani bükülmüş dönüşüm, mekanı daha düşük bir boyuta sarar.",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "Bu durumda tatlı nokta lambda 1'e eşit olduğunda gelir.",
  "model": "google_nmt",
  "from_community_srt": "Bu durumda, tatlı nokta λ 1 olduğunda gelir.",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Elbette başka bir matris seçmiş olsaydık özdeğerin 1 olması gerekmeyebilirdi.",
  "model": "google_nmt",
  "from_community_srt": "Elbette, başka bir matris seçmiş olsaydık, özdeğerin mutlaka 1 olması gerekmeyebilir,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Tatlı nokta lambdanın başka bir değerinde vurulabilir.",
  "model": "google_nmt",
  "from_community_srt": "tatlı nokta diğer λ değerlerine çarpmış olabilir.",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Yani bu çok fazla, ama hadi bunun ne söylediğini çözelim.",
  "model": "google_nmt",
  "from_community_srt": "Yani bu çok fazla, ama ne dediğini çözelim.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Lambda 1'e eşit olduğunda, A eksi lambda çarpı özdeşlik matrisi uzayı bir doğruya sıkıştırır.",
  "model": "google_nmt",
  "from_community_srt": "Λ 1'e eşit olduğunda, matris A eksi λ çarpımı kimliğin bir çizgiye taşmasına neden olur.",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Bu, A eksi lambda çarpı birim çarpı v eşittir sıfır vektörüne eşit olan sıfır olmayan bir v vektörünün olduğu anlamına gelir.",
  "model": "google_nmt",
  "from_community_srt": "Bu sıfır olmayan bir vektör v olduğu anlamına gelir. Öyle ki A eksi çarpı çarpı çarpı çarpı v, sıfır vektöre eşittir.",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "Ve unutmayın, bunu önemsememizin sebebi bunun A çarpı v eşittir lambda çarpı v anlamına gelmesidir, bunu v vektörünün A'nın bir özvektörü olduğunu ve A dönüşümü sırasında kendi açıklığında kaldığını söyleyerek okuyabilirsiniz.",
  "model": "google_nmt",
  "from_community_srt": "Ve unutmayın, bunu umursamamızın nedeni, A çarpı v λ çarpı v demektir. v vektörünün A'nın bir özvektörü olduğunu söyleyerek okuyabileceğiniz A dönüşümü sırasında kendi sahasında kalmak.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "Bu örnekte karşılık gelen özdeğer 1'dir, yani v aslında yerinde sabit kalacaktır.",
  "model": "google_nmt",
  "from_community_srt": "Bu örnekte, karşılık gelen özdeğer 1, yani v aslında sadece sabit bir yerinde olacaktır.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Bu mantık tarzının iyi hissettirdiğinden emin olmanız gerekiyorsa duraklayın ve düşünün.",
  "model": "google_nmt",
  "from_community_srt": "Bu mantık çizgisinin iyi geldiğinden emin olmanız gerekiyorsa, duraklatın ve düşünün.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Girişte bahsettiğim türden bir şey bu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Eğer determinantlar ve bunların neden sıfırdan farklı çözümleri olan doğrusal denklem sistemleriyle ilgili olduğu konusunda sağlam bir kavrayışa sahip değilseniz, bunun gibi bir ifade tamamen birdenbire gibi görünecektir.",
  "model": "google_nmt",
  "from_community_srt": "Bu girişte bahsettiğim türden bir şey, Sağlam bir determinantı anlamadıysanız ve neden sıfır olmayan çözümleri olan doğrusal denklem sistemleri ile ilgili oldukları, Bunun gibi bir ifade tamamen maviden mahrum kalır.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Bunu çalışırken görmek için, sütunları 3, 0 ve 1, 2 olan bir matris ile örneği en baştan tekrar ele alalım.",
  "model": "google_nmt",
  "from_community_srt": "Bunu çalışırken görmek için örneği en baştan tekrar gözden geçirelim. sütunları (3, 0) ve (1, 2) matrisiyle birlikte.",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Lambda değerinin bir özdeğer olup olmadığını bulmak için onu bu matrisin köşegenlerinden çıkarın ve determinantı hesaplayın.",
  "model": "google_nmt",
  "from_community_srt": "Λ değerinin özdeğer olup olmadığını bulmak için, bu matrisin köşegenlerinden çıkarıldı ve determinantı hesapladı.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Bunu yaparak lambda'da ikinci dereceden belirli bir polinom elde ederiz, 3 eksi lambda çarpı 2 eksi lambda.",
  "model": "google_nmt",
  "from_community_srt": "Bunu yaparak, λ, (3-λ) (2-λ) 'da belirli bir ikinci dereceden polinom elde ediyoruz.",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Lambda yalnızca bu determinantın sıfır olması durumunda bir özdeğer olabileceğinden, mümkün olan tek özdeğerin lambda'nın 2'ye ve lambda'nın 3'e eşit olduğu sonucuna varabilirsiniz.",
  "model": "google_nmt",
  "from_community_srt": "Λ sadece bu determinant sıfır olduğunda bir özdeğer olabileceğinden, Sadece olası özdeğerlerin λ 2, λ 3 olduğu sonucuna varabilirsiniz.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Bu özdeğerlerden birine sahip olan özvektörlerin ne olduğunu bulmak için, örneğin lambda eşittir 2, lambda değerini matrise yerleştirin ve sonra bu çapraz olarak değiştirilmiş matrisin hangi vektörler için sıfıra göndereceğini çözün.",
  "model": "google_nmt",
  "from_community_srt": "Özdeğerlerin gerçekte bu özdeğerlerden birine sahip olduğunu anlamak için λ 2'ye eşittir, bu λ değerini matrise takın ve sonra bu çapraz olarak değiştirilmiş matrisin hangi vektörler için 0'a gönderildiğini çözün.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Bunu herhangi bir diğer doğrusal sistem gibi hesapladıysanız, çözümlerin tümünün negatif 1, 1 ile kesişen çapraz çizgi üzerindeki vektörler olduğunu görürsünüz.",
  "model": "google_nmt",
  "from_community_srt": "Bunu, başka bir lineer sistemde yaptığınız gibi hesaplarsanız, Çözümlerin, (-1, 1) yayılan çapraz çizgideki tüm vektörler olduğunu göreceksiniz.",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Bu, değiştirilmemiş 3, 0, 1, 2 matrisinin tüm bu vektörleri 2 kat uzatma etkisine sahip olduğu gerçeğine karşılık gelir.",
  "model": "google_nmt",
  "from_community_srt": "Bu, değiştirilmemiş matrisin [(3, 0), (1, 2)] gerçeğine karşılık gelir. tüm bu vektörleri 2 kat gerdirme etkisine sahiptir.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Artık bir 2 boyutlu dönüşümün özvektörlere sahip olması gerekmiyor.",
  "model": "google_nmt",
  "from_community_srt": "Şimdi, bir 2-D dönüşümü özvektörlere sahip olmak zorunda değildir.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Örneğin 90 derecelik bir döndürmeyi düşünün.",
  "model": "google_nmt",
  "from_community_srt": "Örneğin, 90 derece döndürmeyi düşünün.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Her vektörü kendi açıklığının dışında döndürdüğü için bunun herhangi bir özvektörü yoktur.",
  "model": "google_nmt",
  "from_community_srt": "Bu, herhangi bir özvektöre sahip değildir, çünkü her vektörü kendi yayılma alanından döndürür.",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Eğer gerçekten böyle bir döndürmenin özdeğerlerini hesaplamayı denerseniz, ne olduğuna dikkat edin.",
  "model": "google_nmt",
  "from_community_srt": "Aslında böyle bir rotasyonun özdeğerlerini hesaplamayı denerseniz, ne olduğuna dikkat edin.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Matrisin sütunları 0, 1 ve negatif 1, 0'dır.",
  "model": "google_nmt",
  "from_community_srt": "Matrisinde sütunlar (0, 1) ve (-1,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Lambda'yı köşegen elemanlardan çıkarın ve determinantın ne zaman sıfır olduğuna bakın.",
  "model": "google_nmt",
  "from_community_srt": "0), λ'yı çapraz elemanlardan çıkartın ve determinantın 0 olduğunu görün.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "Bu durumda lambda kare artı 1 polinomunu elde edersiniz.",
  "model": "google_nmt",
  "from_community_srt": "Bu durumda λ ^ 2 + 1 polinomunu alırsınız, Bu polinomun tek kökleri,",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Bu polinomun yegâne kökleri sanal sayılardır, i ve negatif i.",
  "model": "google_nmt",
  "from_community_srt": "ben ve -i gibi hayali sayılardır.",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Gerçek sayı çözümlerinin olmaması özvektörlerin olmadığını gösterir.",
  "model": "google_nmt",
  "from_community_srt": "Gerçek sayı çözümü olmadığı gerçeği, özvektörlerin olmadığını gösterir.",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Aklınızda bulundurmaya değer bir başka ilginç örnek de makaslamadır.",
  "model": "google_nmt",
  "from_community_srt": "Aklınızın arkasında durmaya değer başka bir ilginç örnek bir makas.",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Bu, i-hat'ı yerinde sabitler ve j-hat 1'i hareket ettirir, böylece matrisinin 1, 0 ve 1, 1 sütunları olur.",
  "model": "google_nmt",
  "from_community_srt": "Bu, i-hat yerine yerleştirilir ve j-hat'ı bir üst üste taşır, bu yüzden matrisi sütunlarında (1, 0) ve (1, 1) bulunur.",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "X eksenindeki vektörlerin tümü, yerinde sabit kaldıkları için öz değeri 1 olan özvektörlerdir.",
  "model": "google_nmt",
  "from_community_srt": "X ekseni üzerindeki tüm vektörler, yerinde sabit kaldıklarından, özdeğer 1 olan özvektörlerdir.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "Aslında bunlar tek özvektörlerdir.",
  "model": "google_nmt",
  "from_community_srt": "Aslında, bunlar sadece özvektörlerdir.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Köşegenlerden lambda'yı çıkarıp determinantı hesapladığınızda elde ettiğiniz sonuç 1 eksi lambda karedir.",
  "model": "google_nmt",
  "from_community_srt": "Λ'yı köşegenlerden çıkardığınızda ve determinantı hesapladığınızda, ne elde edersen (1-λ) ^ 2,",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "Ve bu ifadenin tek kökü lambda eşittir 1'dir.",
  "model": "google_nmt",
  "from_community_srt": "ve bu ifadenin tek kökü λ 1 eşittir.",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Bu, geometrik olarak gördüğümüz şeyle, tüm özvektörlerin özdeğer 1'e sahip olmasıyla örtüşmektedir.",
  "model": "google_nmt",
  "from_community_srt": "Bu, tüm özvektörlerin özdeğer 1'e sahip olduğunu geometrik olarak gördüğümüze göre sıralanır.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Ancak unutmayın, yalnızca bir özdeğere sahip olmanın da mümkün olduğunu, ancak özvektörlerle dolu bir çizgiden daha fazlasının mümkün olduğunu unutmayın.",
  "model": "google_nmt",
  "from_community_srt": "Yine de aklınızda bulundurun Aynı zamanda sadece bir özdeğere sahip olmak da mümkündür, fakat sadece özvektörlerle dolu bir çizgiden daha fazlasıdır.",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Basit bir örnek, her şeyi 2'ye kadar ölçeklendiren bir matristir.",
  "model": "google_nmt",
  "from_community_srt": "Basit bir örnek,",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "Tek özdeğer 2'dir, ancak düzlemdeki her vektör bu özdeğere sahip bir özvektör olur.",
  "model": "google_nmt",
  "from_community_srt": "her şeyi 2'ye ölçeklendiren bir matristir, sadece özdeğer 2'dir, fakat düzlemdeki her vektör bu özdeğerle bir özvektör olur.",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Şimdi son konuya geçmeden önce biraz durup düşünmek için iyi bir zaman.",
  "model": "google_nmt",
  "from_community_srt": "Şimdi bunlardan bazılarını duraklatmak ve düşünmek için başka bir iyi zaman son konuya geçmeden önce.",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Burada, ağırlıklı olarak son videodaki fikirlere dayanan öz temel fikriyle bitirmek istiyorum.",
  "model": "google_nmt",
  "from_community_srt": "Burada özbilgi fikri ile bitirmek istiyorum. Son videodaki fikirlere büyük ölçüde güveniyor.",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Temel vektörlerimiz özvektör olursa ne olacağına bir bakın.",
  "model": "google_nmt",
  "from_community_srt": "Eğer temel vektörlerimiz özvektörler olarak ortaya çıkarsa neler olduğuna bir bakın.",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Örneğin, i-hat negatif 1 ile, j-hat ise 2 ile ölçeklendirilebilir.",
  "model": "google_nmt",
  "from_community_srt": "Örneğin, belki i-şapka -1 ile ölçeklenir ve j-şapka 2 ile ölçeklenir.",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Yeni koordinatlarını bir matrisin sütunları olarak yazarken, i-hat ve j-hat'ın özdeğerleri olan negatif 1 ve 2 skaler katlarının matrisimizin köşegeninde oturduğuna ve diğer tüm girdilerin 0 olduğuna dikkat edin. .",
  "model": "google_nmt",
  "from_community_srt": "Yeni koordinatlarını bir matrisin sütunları olarak yazmak, i-hat ve j-hat'ın özdeğerleri olan skaler katlar -1 ve 2'nin, matrisimizin köşegenine oturun ve diğer bütün girişler 0'dır.",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Bir matrisin köşegen dışındaki her yerinde sıfır varsa, buna makul olarak köşegen matris denir.",
  "model": "google_nmt",
  "from_community_srt": "Her zaman bir matris, köşegen dışında her yerde 0'lar vardır, Buna yeterince yeterli bir çapraz matris denir.",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "Ve bunu yorumlamanın yolu, tüm temel vektörlerin özvektörler olduğu ve bu matrisin köşegen girişlerinin özdeğerleri olduğudur.",
  "model": "google_nmt",
  "from_community_srt": "Ve bunu yorumlamanın yolu, tüm temel vektörlerin özvektörler olduğudur. bu matrisin köşegen girdileri, özdeğerleridir.",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Köşegen matrislerle çalışmayı daha güzel hale getiren pek çok şey var.",
  "model": "google_nmt",
  "from_community_srt": "Diyagonal matrisleri çalışmak için daha güzel yapan birçok şey var.",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Bunlardan en büyüğü, bu matrisi kendisiyle birçok kez çarptığınızda ne olacağını hesaplamanın daha kolay olmasıdır.",
  "model": "google_nmt",
  "from_community_srt": "Bir büyük olan bu Bu matrisi bir çok kez kendi kendinize çarptığınızda ne olacağını hesaplamak daha kolaydır.",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Bu matrislerin tümü, her temel vektörü bir özdeğere göre ölçeklendirdiğinden, bu matrisi birçok kez, örneğin 100 kez uygulamak, her temel vektörü karşılık gelen özdeğerin 100'üncü kuvvetiyle ölçeklendirmeye karşılık gelecektir.",
  "model": "google_nmt",
  "from_community_srt": "Bu matrislerin hepsinin yaptığı, her bir temel vektörü özdeğerle ölçeklendirmek olduğundan, bu matrisi birçok kez uygulayarak, 100 kere söyleyin, sadece her bir temel vektörün karşılık gelen özdeğerin 100. gücü ile ölçeklendirilmesine tekabül edecektir.",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Bunun tersine, köşegen olmayan bir matrisin 100'üncü kuvvetini hesaplamayı deneyin.",
  "model": "google_nmt",
  "from_community_srt": "Buna karşılık, çapraz olmayan bir matrisin 100. gücünü hesaplamayı deneyin.",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Gerçekten, bir anlığına dene.",
  "model": "google_nmt",
  "from_community_srt": "Gerçekten,",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Bu bir kabus.",
  "model": "google_nmt",
  "from_community_srt": "bir an için dene, bu bir kabus.",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Tabii ki, taban vektörlerinizin aynı zamanda özvektör olması kadar şanslı olmanız çok nadirdir.",
  "model": "google_nmt",
  "from_community_srt": "Elbette, temel vektörlerinizin özvektörler olması nedeniyle nadiren çok şanslı olacaksınız.",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Ancak dönüşümünüzde, bu videonun başlangıcındaki gibi, tüm uzayı kapsayan bir küme seçebilmenize yetecek kadar çok özvektör varsa, o zaman koordinat sisteminizi, bu özvektörlerin temel vektörleriniz olacağı şekilde değiştirebilirsiniz.",
  "model": "google_nmt",
  "from_community_srt": "ancak dönüşümünüzün çok özvektörleri varsa, bu videonun başlangıcındaki gibi Tüm alanı kaplayan bir set seçebilecek kadar o zaman koordinat sisteminizi, bu özvektörlerin sizin temel vektörleriniz olacağı şekilde değiştirebilirsiniz.",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Geçen videoda taban değişiminden bahsetmiştim ama burada koordinat sistemimizde halihazırda yazılı olan bir dönüşümü farklı bir sisteme nasıl ifade edebileceğimizi çok hızlı bir şekilde hatırlatacağım.",
  "model": "google_nmt",
  "from_community_srt": "Son videonun temel değişikliğinden bahsettim. ama burada süper hızlı bir hatırlatmadan geçeceğim Şu anda koordinat sistemimizde yazılmış bir dönüşümün farklı bir sisteme nasıl ifade edileceği.",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Yeni bir temel olarak kullanmak istediğiniz vektörlerin koordinatlarını alın, bu durumda iki özvektörümüz anlamına gelir, daha sonra bu koordinatları, temel matrisin değişimi olarak bilinen bir matrisin sütunları haline getirin.",
  "model": "google_nmt",
  "from_community_srt": "Yeni bir temel olarak kullanmak istediğiniz vektörlerin koordinatlarını alın, bu durumda, iki özvektör olan araçlar, Bu, temel matrisin değişimi olarak bilinen bir matrisin sütunlarını koordine eden şeydir.",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Temel matris değişimini sağına ve temel matris değişiminin tersini soluna koyarak orijinal dönüşümü sandviçlediğinizde, sonuç aynı dönüşümü temsil eden bir matris olacaktır, ancak yeni temel vektörlerin koordinatları açısından sistem.",
  "model": "google_nmt",
  "from_community_srt": "Orijinal dönüşümü yaptığınızda Temel matris değişikliğini doğru yapmak ve solundaki temel matris değişikliğinin tersini, sonuç aynı dönüşümü temsil eden bir matris olacak, ancak yeni temel vektörler perspektifinden sistemi koordine eder.",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Bunu özvektörlerle yapmanın asıl amacı, bu yeni matrisin köşegen olmasının ve bu köşegenin aşağısında karşılık gelen özdeğerlerinin garanti edilmesidir.",
  "model": "google_nmt",
  "from_community_srt": "Bunu özvektörlerle yapmanın tek amacı; bu yeni matrisin, köşegen aşağı doğru özdeğerleri ile köşegen olması garanti edilir.",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Bunun nedeni, temel vektörlerin dönüşüm sırasında ölçeklendiği bir koordinat sisteminde çalışmayı temsil etmesidir.",
  "model": "google_nmt",
  "from_community_srt": "Bunun nedeni bir koordinat sisteminde çalışmayı temsil etmesidir. Temel vektörlere olan şey, dönüşüm sırasında ölçeklendikleridir.",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Aynı zamanda özvektör olan bir dizi temel vektöre yine yeterince makul bir şekilde öztaban adı verilir.",
  "model": "google_nmt",
  "from_community_srt": "Aynı zamanda özvektör olan bir dizi vektör yine, yeterince makul bir şekilde “özdeğer” olarak adlandırılır.",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Yani, örneğin, bu matrisin 100'üncü kuvvetini hesaplamanız gerekiyorsa, öztabanı değiştirmek, bu sistemdeki 100'üncü kuvveti hesaplamak ve ardından standart sistemimize geri dönüştürmek çok daha kolay olacaktır.",
  "model": "google_nmt",
  "from_community_srt": "Örneğin, bu matrisin 100. gücünü hesaplamanız gerekiyorsa, özbaziye geçmek daha kolay olurdu, bu sistemdeki 100. gücü hesaplayabilir, sonra standart sistemimize dönelim.",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Bunu tüm dönüşümlerle yapamazsınız.",
  "model": "google_nmt",
  "from_community_srt": "Bunu tüm dönüşümlerle yapamazsınız.",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Örneğin bir kesmenin tüm uzayı kaplamaya yetecek kadar özvektörü yoktur.",
  "model": "google_nmt",
  "from_community_srt": "Örneğin bir makas, tüm alanı kaplayacak kadar özvektöre sahip değildir.",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Ama eğer bir öztaban bulabilirseniz, bu matris işlemlerini gerçekten güzelleştirir.",
  "model": "google_nmt",
  "from_community_srt": "Fakat eğer bir özbasi bulabilirseniz, matris işlemlerini gerçekten çok güzel yapar.",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Bunun eylem halinde neye benzediğini ve bazı şaşırtıcı sonuçlar üretmek için nasıl kullanılabileceğini görmek için oldukça güzel bir bulmaca üzerinde çalışmaya istekli olanlar için, buraya, ekrana bir bilgi bırakacağım.",
  "model": "google_nmt",
  "from_community_srt": "Bunun nasıl göründüğünü görmek için oldukça düzgün bir bilmeceyle çalışmaya istekli olanlar için ve bazı şaşırtıcı sonuçlar üretmek için nasıl kullanılabildiğini, ekranda bir bilgi istemi bırakacağım.",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Biraz uğraş gerektiriyor ama keyif alacağınızı düşünüyorum.",
  "model": "google_nmt",
  "from_community_srt": "Biraz iş gerektiriyor ama bence hoşunuza gidecek.",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Bu serinin bir sonraki ve son videosu soyut vektör uzayları üzerine olacak.",
  "model": "google_nmt",
  "from_community_srt": "Bu serinin bir sonraki ve son videosu soyut vektör uzaylarında olacak.",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]