1
00:00:19,920 --> 00:00:22,812
Eigenvectors and eigenvalues is one of those topics 

2
00:00:22,812 --> 00:00:25,760
that a lot of students find particularly unintuitive.

3
00:00:25,760 --> 00:00:29,433
Questions like, why are we doing this and what does this actually mean, 

4
00:00:29,433 --> 00:00:33,260
are too often left just floating away in an unanswered sea of computations.

5
00:00:33,920 --> 00:00:36,026
And as I've put out the videos of this series, 

6
00:00:36,026 --> 00:00:40,060
a lot of you have commented about looking forward to visualizing this topic in particular.

7
00:00:40,680 --> 00:00:43,373
I suspect that the reason for this is not so much that 

8
00:00:43,373 --> 00:00:46,360
eigenthings are particularly complicated or poorly explained.

9
00:00:46,860 --> 00:00:49,065
In fact, it's comparatively straightforward, and 

10
00:00:49,065 --> 00:00:51,180
I think most books do a fine job explaining it.

11
00:00:51,520 --> 00:00:54,805
The issue is that it only really makes sense if you have a 

12
00:00:54,805 --> 00:00:58,480
solid visual understanding for many of the topics that precede it.

13
00:00:59,060 --> 00:01:02,616
Most important here is that you know how to think about matrices as 

14
00:01:02,616 --> 00:01:06,383
linear transformations, but you also need to be comfortable with things 

15
00:01:06,383 --> 00:01:09,940
like determinants, linear systems of equations, and change of basis.

16
00:01:10,720 --> 00:01:14,979
Confusion about eigenstuffs usually has more to do with a shaky foundation in 

17
00:01:14,979 --> 00:01:19,240
one of these topics than it does with eigenvectors and eigenvalues themselves.

18
00:01:19,980 --> 00:01:24,840
To start, consider some linear transformation in two dimensions, like the one shown here.

19
00:01:25,460 --> 00:01:31,040
It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.

20
00:01:31,780 --> 00:01:35,640
So it's represented with a matrix whose columns are 3, 0, and 1, 2.

21
00:01:36,600 --> 00:01:39,353
Focus in on what it does to one particular vector, 

22
00:01:39,353 --> 00:01:44,160
and think about the span of that vector, the line passing through its origin and its tip.

23
00:01:44,920 --> 00:01:48,380
Most vectors are going to get knocked off their span during the transformation.

24
00:01:48,780 --> 00:01:52,050
I mean, it would seem pretty coincidental if the place where 

25
00:01:52,050 --> 00:01:55,320
the vector landed also happened to be somewhere on that line.

26
00:01:57,400 --> 00:02:00,653
But some special vectors do remain on their own span, 

27
00:02:00,653 --> 00:02:05,533
meaning the effect that the matrix has on such a vector is just to stretch it or 

28
00:02:05,533 --> 00:02:07,040
squish it, like a scalar.

29
00:02:09,460 --> 00:02:14,100
For this specific example, the basis vector i-hat is one such special vector.

30
00:02:14,640 --> 00:02:19,412
The span of i-hat is the x-axis, and from the first column of the matrix, 

31
00:02:19,412 --> 00:02:24,120
we can see that i-hat moves over to 3 times itself, still on that x-axis.

32
00:02:26,320 --> 00:02:30,031
What's more, because of the way linear transformations work, 

33
00:02:30,031 --> 00:02:34,411
any other vector on the x-axis is also just stretched by a factor of 3, 

34
00:02:34,411 --> 00:02:36,480
and hence remains on its own span.

35
00:02:38,500 --> 00:02:41,325
A slightly sneakier vector that remains on its own 

36
00:02:41,325 --> 00:02:44,040
span during this transformation is negative 1, 1.

37
00:02:44,660 --> 00:02:47,140
It ends up getting stretched by a factor of 2.

38
00:02:49,000 --> 00:02:53,610
And again, linearity is going to imply that any other vector on the diagonal 

39
00:02:53,610 --> 00:02:58,220
line spanned by this guy is just going to get stretched out by a factor of 2.

40
00:02:59,820 --> 00:03:02,575
And for this transformation, those are all the vectors 

41
00:03:02,575 --> 00:03:05,180
with this special property of staying on their span.

42
00:03:05,620 --> 00:03:08,624
Those on the x-axis getting stretched out by a factor of 3, 

43
00:03:08,624 --> 00:03:11,980
and those on this diagonal line getting stretched by a factor of 2.

44
00:03:12,760 --> 00:03:16,417
Any other vector is going to get rotated somewhat during the transformation, 

45
00:03:16,417 --> 00:03:18,080
knocked off the line that it spans.

46
00:03:22,520 --> 00:03:27,362
As you might have guessed by now, these special vectors are called the eigenvectors of 

47
00:03:27,362 --> 00:03:31,870
the transformation, and each eigenvector has associated with it what's called an 

48
00:03:31,870 --> 00:03:36,545
eigenvalue, which is just the factor by which it's stretched or squished during the 

49
00:03:36,545 --> 00:03:37,380
transformation.

50
00:03:40,280 --> 00:03:43,399
Of course, there's nothing special about stretching versus squishing, 

51
00:03:43,399 --> 00:03:45,940
or the fact that these eigenvalues happen to be positive.

52
00:03:46,380 --> 00:03:51,060
In another example, you could have an eigenvector with eigenvalue negative 1 half, 

53
00:03:51,060 --> 00:03:55,120
meaning that the vector gets flipped and squished by a factor of 1 half.

54
00:03:56,980 --> 00:03:59,737
But the important part here is that it stays on the 

55
00:03:59,737 --> 00:04:02,760
line that it spans out without getting rotated off of it.

56
00:04:04,460 --> 00:04:07,753
For a glimpse of why this might be a useful thing to think about, 

57
00:04:07,753 --> 00:04:09,800
consider some three-dimensional rotation.

58
00:04:11,660 --> 00:04:14,983
If you can find an eigenvector for that rotation, 

59
00:04:14,983 --> 00:04:20,500
a vector that remains on its own span, what you have found is the axis of rotation.

60
00:04:22,600 --> 00:04:26,587
And it's much easier to think about a 3D rotation in terms of some 

61
00:04:26,587 --> 00:04:29,800
axis of rotation and an angle by which it's rotating, 

62
00:04:29,800 --> 00:04:34,740
rather than thinking about the full 3x3 matrix associated with that transformation.

63
00:04:37,000 --> 00:04:40,797
In this case, by the way, the corresponding eigenvalue would have to be 1, 

64
00:04:40,797 --> 00:04:43,328
since rotations never stretch or squish anything, 

65
00:04:43,328 --> 00:04:45,860
so the length of the vector would remain the same.

66
00:04:48,080 --> 00:04:50,020
This pattern shows up a lot in linear algebra.

67
00:04:50,440 --> 00:04:53,253
With any linear transformation described by a matrix, 

68
00:04:53,253 --> 00:04:57,733
you could understand what it's doing by reading off the columns of this matrix as the 

69
00:04:57,733 --> 00:04:59,400
landing spots for basis vectors.

70
00:05:00,020 --> 00:05:03,601
But often, a better way to get at the heart of what the linear 

71
00:05:03,601 --> 00:05:08,318
transformation actually does, less dependent on your particular coordinate system, 

72
00:05:08,318 --> 00:05:10,820
is to find the eigenvectors and eigenvalues.

73
00:05:15,460 --> 00:05:18,997
I won't cover the full details on methods for computing eigenvectors 

74
00:05:18,997 --> 00:05:22,175
and eigenvalues here, but I'll try to give an overview of the 

75
00:05:22,175 --> 00:05:26,020
computational ideas that are most important for a conceptual understanding.

76
00:05:27,180 --> 00:05:30,480
Symbolically, here's what the idea of an eigenvector looks like.

77
00:05:31,040 --> 00:05:35,929
A is the matrix representing some transformation, with v as the eigenvector, 

78
00:05:35,929 --> 00:05:39,740
and lambda is a number, namely the corresponding eigenvalue.

79
00:05:40,680 --> 00:05:45,289
What this expression is saying is that the matrix-vector product, A times v, 

80
00:05:45,289 --> 00:05:49,900
gives the same result as just scaling the eigenvector v by some value lambda.

81
00:05:51,000 --> 00:05:55,423
So finding the eigenvectors and their eigenvalues of a matrix A comes 

82
00:05:55,423 --> 00:06:00,100
down to finding the values of v and lambda that make this expression true.

83
00:06:01,920 --> 00:06:06,057
It's a little awkward to work with at first, because that left-hand side represents 

84
00:06:06,057 --> 00:06:09,801
matrix-vector multiplication, but the right-hand side here is scalar-vector 

85
00:06:09,801 --> 00:06:10,540
multiplication.

86
00:06:11,120 --> 00:06:15,408
So let's start by rewriting that right-hand side as some kind of matrix-vector 

87
00:06:15,408 --> 00:06:20,077
multiplication, using a matrix which has the effect of scaling any vector by a factor 

88
00:06:20,077 --> 00:06:20,620
of lambda.

89
00:06:21,680 --> 00:06:26,178
The columns of such a matrix will represent what happens to each basis vector, 

90
00:06:26,178 --> 00:06:29,252
and each basis vector is simply multiplied by lambda, 

91
00:06:29,252 --> 00:06:34,320
so this matrix will have the number lambda down the diagonal, with zeros everywhere else.

92
00:06:36,180 --> 00:06:40,491
The common way to write this guy is to factor that lambda out and write it 

93
00:06:40,491 --> 00:06:44,860
as lambda times i, where i is the identity matrix with 1s down the diagonal.

94
00:06:45,860 --> 00:06:48,785
With both sides looking like matrix-vector multiplication, 

95
00:06:48,785 --> 00:06:51,860
we can subtract off that right-hand side and factor out the v.

96
00:06:54,160 --> 00:06:58,971
So what we now have is a new matrix, A minus lambda times the identity, 

97
00:06:58,971 --> 00:07:04,920
and we're looking for a vector v such that this new matrix times v gives the zero vector.

98
00:07:06,380 --> 00:07:11,100
Now, this will always be true if v itself is the zero vector, but that's boring.

99
00:07:11,340 --> 00:07:13,640
What we want is a non-zero eigenvector.

100
00:07:14,420 --> 00:07:18,934
And if you watch chapter 5 and 6, you'll know that the only way it's possible 

101
00:07:18,934 --> 00:07:23,332
for the product of a matrix with a non-zero vector to become zero is if the 

102
00:07:23,332 --> 00:07:28,020
transformation associated with that matrix squishes space into a lower dimension.

103
00:07:29,300 --> 00:07:34,220
And that squishification corresponds to a zero determinant for the matrix.

104
00:07:35,480 --> 00:07:39,934
To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, 

105
00:07:39,934 --> 00:07:45,520
and think about subtracting off a variable amount, lambda, from each diagonal entry.

106
00:07:46,480 --> 00:07:50,280
Now imagine tweaking lambda, turning a knob to change its value.

107
00:07:50,940 --> 00:07:54,539
As that value of lambda changes, the matrix itself changes, 

108
00:07:54,539 --> 00:07:57,240
and so the determinant of the matrix changes.

109
00:07:58,220 --> 00:08:02,964
The goal here is to find a value of lambda that will make this determinant zero, 

110
00:08:02,964 --> 00:08:07,240
meaning the tweaked transformation squishes space into a lower dimension.

111
00:08:08,160 --> 00:08:11,160
In this case, the sweet spot comes when lambda equals 1.

112
00:08:12,180 --> 00:08:16,120
Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.

113
00:08:16,240 --> 00:08:18,600
The sweet spot might be hit at some other value of lambda.

114
00:08:20,080 --> 00:08:22,960
So this is kind of a lot, but let's unravel what this is saying.

115
00:08:22,960 --> 00:08:26,330
When lambda equals 1, the matrix A minus lambda 

116
00:08:26,330 --> 00:08:29,560
times the identity squishes space onto a line.

117
00:08:30,440 --> 00:08:34,500
That means there's a non-zero vector v such that A minus 

118
00:08:34,500 --> 00:08:38,559
lambda times the identity times v equals the zero vector.

119
00:08:40,480 --> 00:08:46,030
And remember, the reason we care about that is because it means A times v 

120
00:08:46,030 --> 00:08:51,579
equals lambda times v, which you can read off as saying that the vector v 

121
00:08:51,579 --> 00:08:57,280
is an eigenvector of A, staying on its own span during the transformation A.

122
00:08:58,320 --> 00:09:01,375
In this example, the corresponding eigenvalue is 1, 

123
00:09:01,375 --> 00:09:04,020
so v would actually just stay fixed in place.

124
00:09:06,220 --> 00:09:09,500
Pause and ponder if you need to make sure that that line of reasoning feels good.

125
00:09:13,380 --> 00:09:15,640
This is the kind of thing I mentioned in the introduction.

126
00:09:16,220 --> 00:09:19,526
If you didn't have a solid grasp of determinants and why they 

127
00:09:19,526 --> 00:09:22,993
relate to linear systems of equations having non-zero solutions, 

128
00:09:22,993 --> 00:09:26,300
an expression like this would feel completely out of the blue.

129
00:09:28,320 --> 00:09:31,962
To see this in action, let's revisit the example from the start, 

130
00:09:31,962 --> 00:09:34,540
with a matrix whose columns are 3, 0 and 1, 2.

131
00:09:35,350 --> 00:09:39,511
To find if a value lambda is an eigenvalue, subtract it from 

132
00:09:39,511 --> 00:09:43,400
the diagonals of this matrix and compute the determinant.

133
00:09:50,580 --> 00:09:54,441
Doing this, we get a certain quadratic polynomial in lambda, 

134
00:09:54,441 --> 00:09:56,720
3 minus lambda times 2 minus lambda.

135
00:09:57,800 --> 00:10:02,900
Since lambda can only be an eigenvalue if this determinant happens to be zero, 

136
00:10:02,900 --> 00:10:08,258
you can conclude that the only possible eigenvalues are lambda equals 2 and lambda 

137
00:10:08,258 --> 00:10:08,840
equals 3.

138
00:10:09,640 --> 00:10:14,979
To figure out what the eigenvectors are that actually have one of these eigenvalues, 

139
00:10:14,979 --> 00:10:19,565
say lambda equals 2, plug in that value of lambda to the matrix and then 

140
00:10:19,565 --> 00:10:23,900
solve for which vectors this diagonally altered matrix sends to zero.

141
00:10:24,940 --> 00:10:28,707
If you computed this the way you would any other linear system, 

142
00:10:28,707 --> 00:10:33,299
you'd see that the solutions are all the vectors on the diagonal line spanned 

143
00:10:33,299 --> 00:10:34,300
by negative 1, 1.

144
00:10:35,220 --> 00:10:39,277
This corresponds to the fact that the unaltered matrix, 3, 0, 1, 

145
00:10:39,277 --> 00:10:43,460
2, has the effect of stretching all those vectors by a factor of 2.

146
00:10:46,320 --> 00:10:50,200
Now, a 2D transformation doesn't have to have eigenvectors.

147
00:10:50,720 --> 00:10:53,400
For example, consider a rotation by 90 degrees.

148
00:10:53,660 --> 00:10:58,200
This doesn't have any eigenvectors since it rotates every vector off of its own span.

149
00:11:00,800 --> 00:11:04,513
If you actually try computing the eigenvalues of a rotation like this, 

150
00:11:04,513 --> 00:11:05,560
notice what happens.

151
00:11:06,300 --> 00:11:10,140
Its matrix has columns 0, 1 and negative 1, 0.

152
00:11:11,100 --> 00:11:15,800
Subtract off lambda from the diagonal elements and look for when the determinant is zero.

153
00:11:18,140 --> 00:11:21,940
In this case, you get the polynomial lambda squared plus 1.

154
00:11:22,680 --> 00:11:27,920
The only roots of that polynomial are the imaginary numbers, i and negative i.

155
00:11:28,840 --> 00:11:33,600
The fact that there are no real number solutions indicates that there are no eigenvectors.

156
00:11:35,540 --> 00:11:39,820
Another pretty interesting example worth holding in the back of your mind is a shear.

157
00:11:40,560 --> 00:11:47,840
This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.

158
00:11:48,740 --> 00:11:51,611
All of the vectors on the x-axis are eigenvectors 

159
00:11:51,611 --> 00:11:54,540
with eigenvalue 1 since they remain fixed in place.

160
00:11:55,680 --> 00:11:57,820
In fact, these are the only eigenvectors.

161
00:11:58,760 --> 00:12:03,924
When you subtract off lambda from the diagonals and compute the determinant, 

162
00:12:03,924 --> 00:12:06,540
what you get is 1 minus lambda squared.

163
00:12:09,320 --> 00:12:12,860
And the only root of this expression is lambda equals 1.

164
00:12:14,560 --> 00:12:17,112
This lines up with what we see geometrically, 

165
00:12:17,112 --> 00:12:19,720
that all of the eigenvectors have eigenvalue 1.

166
00:12:21,080 --> 00:12:25,037
Keep in mind though, it's also possible to have just one eigenvalue, 

167
00:12:25,037 --> 00:12:28,020
but with more than just a line full of eigenvectors.

168
00:12:29,900 --> 00:12:33,180
A simple example is a matrix that scales everything by 2.

169
00:12:33,900 --> 00:12:37,200
The only eigenvalue is 2, but every vector in the 

170
00:12:37,200 --> 00:12:40,700
plane gets to be an eigenvector with that eigenvalue.

171
00:12:42,000 --> 00:12:44,666
Now is another good time to pause and ponder some 

172
00:12:44,666 --> 00:12:46,960
of this before I move on to the last topic.

173
00:13:03,540 --> 00:13:06,944
I want to finish off here with the idea of an eigenbasis, 

174
00:13:06,944 --> 00:13:09,880
which relies heavily on ideas from the last video.

175
00:13:11,480 --> 00:13:16,380
Take a look at what happens if our basis vectors just so happen to be eigenvectors.

176
00:13:17,120 --> 00:13:22,380
For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.

177
00:13:23,420 --> 00:13:27,014
Writing their new coordinates as the columns of a matrix, 

178
00:13:27,014 --> 00:13:30,361
notice that those scalar multiples, negative 1 and 2, 

179
00:13:30,361 --> 00:13:35,382
which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, 

180
00:13:35,382 --> 00:13:37,180
and every other entry is a 0.

181
00:13:38,880 --> 00:13:42,551
Any time a matrix has zeros everywhere other than the diagonal, 

182
00:13:42,551 --> 00:13:45,420
it's called, reasonably enough, a diagonal matrix.

183
00:13:45,840 --> 00:13:50,509
And the way to interpret this is that all the basis vectors are eigenvectors, 

184
00:13:50,509 --> 00:13:54,400
with the diagonal entries of this matrix being their eigenvalues.

185
00:13:57,100 --> 00:14:01,060
There are a lot of things that make diagonal matrices much nicer to work with.

186
00:14:01,780 --> 00:14:05,032
One big one is that it's easier to compute what will happen 

187
00:14:05,032 --> 00:14:08,340
if you multiply this matrix by itself a whole bunch of times.

188
00:14:09,420 --> 00:14:14,733
Since all one of these matrices does is scale each basis vector by some eigenvalue, 

189
00:14:14,733 --> 00:14:17,769
applying that matrix many times, say 100 times, 

190
00:14:17,769 --> 00:14:22,765
is just going to correspond to scaling each basis vector by the 100th power of 

191
00:14:22,765 --> 00:14:24,600
the corresponding eigenvalue.

192
00:14:25,700 --> 00:14:29,680
In contrast, try computing the 100th power of a non-diagonal matrix.

193
00:14:29,680 --> 00:14:31,320
Really, try it for a moment.

194
00:14:31,740 --> 00:14:32,440
It's a nightmare.

195
00:14:36,080 --> 00:14:41,260
Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.

196
00:14:42,040 --> 00:14:45,110
But if your transformation has a lot of eigenvectors, 

197
00:14:45,110 --> 00:14:49,887
like the one from the start of this video, enough so that you can choose a set that 

198
00:14:49,887 --> 00:14:54,492
spans the full space, then you could change your coordinate system so that these 

199
00:14:54,492 --> 00:14:56,540
eigenvectors are your basis vectors.

200
00:14:57,140 --> 00:15:00,371
I talked about change of basis last video, but I'll go through 

201
00:15:00,371 --> 00:15:03,603
a super quick reminder here of how to express a transformation 

202
00:15:03,603 --> 00:15:07,040
currently written in our coordinate system into a different system.

203
00:15:08,440 --> 00:15:12,282
Take the coordinates of the vectors that you want to use as a new basis, 

204
00:15:12,282 --> 00:15:14,755
which in this case means our two eigenvectors, 

205
00:15:14,755 --> 00:15:19,440
then make those coordinates the columns of a matrix, known as the change of basis matrix.

206
00:15:20,180 --> 00:15:22,834
When you sandwich the original transformation, 

207
00:15:22,834 --> 00:15:26,843
putting the change of basis matrix on its right and the inverse of the 

208
00:15:26,843 --> 00:15:31,191
change of basis matrix on its left, the result will be a matrix representing 

209
00:15:31,191 --> 00:15:35,031
that same transformation, but from the perspective of the new basis 

210
00:15:35,031 --> 00:15:36,500
vectors coordinate system.

211
00:15:37,440 --> 00:15:41,910
The whole point of doing this with eigenvectors is that this new matrix is 

212
00:15:41,910 --> 00:15:46,680
guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.

213
00:15:46,860 --> 00:15:50,417
This is because it represents working in a coordinate system where what 

214
00:15:50,417 --> 00:15:54,320
happens to the basis vectors is that they get scaled during the transformation.

215
00:15:55,800 --> 00:15:59,301
A set of basis vectors which are also eigenvectors is called, 

216
00:15:59,301 --> 00:16:01,560
again, reasonably enough, an eigenbasis.

217
00:16:02,340 --> 00:16:07,108
So if, for example, you needed to compute the 100th power of this matrix, 

218
00:16:07,108 --> 00:16:10,460
it would be much easier to change to an eigenbasis, 

219
00:16:10,460 --> 00:16:15,680
compute the 100th power in that system, then convert back to our standard system.

220
00:16:16,620 --> 00:16:18,320
You can't do this with all transformations.

221
00:16:18,320 --> 00:16:22,980
A shear, for example, doesn't have enough eigenvectors to span the full space.

222
00:16:23,460 --> 00:16:28,160
But if you can find an eigenbasis, it makes matrix operations really lovely.

223
00:16:29,120 --> 00:16:31,771
For those of you willing to work through a pretty neat puzzle to 

224
00:16:31,771 --> 00:16:34,586
see what this looks like in action and how it can be used to produce 

225
00:16:34,586 --> 00:16:37,320
some surprising results, I'll leave up a prompt here on the screen.

226
00:16:37,600 --> 00:16:40,280
It takes a bit of work, but I think you'll enjoy it.

227
00:16:40,840 --> 00:16:46,120
The next and final video of this series is going to be on abstract vector spaces.

