[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Autovetores e autovalores é um daqueles tópicos que muitos estudantes consideram particularmente pouco intuitivo.",
  "model": "google_nmt",
  "from_community_srt": "\"Outra vez eu perguntei: 'O que a matemática significa para você?', e algumas pessoas responderam: 'A manipulação de números, a manipulação de estruturas.'  Se eu tivesse perguntado, 'E o que a música significa para você?', você teria respondido 'A manipulação de notas'?\" -- Serge Lang Autovetores e autovalores é um daqueles temas que um monte de alunos acham particularmente não intuitiva.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Perguntas como por que estamos fazendo isso e o que isso realmente significa são muitas vezes deixadas flutuando em um mar sem resposta de cálculos.",
  "model": "google_nmt",
  "from_community_srt": "Perguntas como “Por que estamos fazendo isso?” e “O que isso realmente significa?” muitas vezes são deixados apenas flutuando para longe",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "E enquanto eu lançava os vídeos desta série, muitos de vocês comentaram sobre o desejo de visualizar este tópico em particular.",
  "model": "google_nmt",
  "from_community_srt": "num mar sem resposta de computações e à medida que eu disponibilizava vídeos desta série, muitos de vocês têm comentado sobre estarem ansiosos para visualizar este tema em particular.",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Suspeito que a razão para isso não seja tanto o fato de as coisas serem particularmente complicadas ou mal explicadas.",
  "model": "google_nmt",
  "from_community_srt": "Eu suspeito que a razão para isso não é tanto que auto-coisas são particularmente complicadas ou mal explicadas.",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "Na verdade, é comparativamente simples, e acho que a maioria dos livros faz um bom trabalho ao explicá-lo.",
  "model": "google_nmt",
  "from_community_srt": "Na verdade, é relativamente simples e eu acho que a maioria dos livros fazem um bom trabalho de explicar isso.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "A questão é que isso só faz sentido se você tiver um conhecimento visual sólido de muitos dos tópicos que o precedem.",
  "model": "google_nmt",
  "from_community_srt": "A questão é que ele realmente só faz sentido se você tiver uma compreensão visual sólida para muitos dos tópicos que o precedem.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "O mais importante aqui é que você saiba pensar em matrizes como transformações lineares, mas também precisa estar confortável com coisas como determinantes, sistemas lineares de equações e mudança de base.",
  "model": "google_nmt",
  "from_community_srt": "O mais importante aqui é que você sabe como pensar sobre matrizes como transformações lineares mas você também precisa estar confortável com coisas como determinantes, sistemas lineares de equações e mudança de base.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "A confusão sobre materiais próprios geralmente tem mais a ver com uma base instável em um desses tópicos do que com os próprios vetores e valores próprios.",
  "model": "google_nmt",
  "from_community_srt": "A confusão sobre auto-coisas geralmente tem mais a ver com uma base instável em um desses tópicos do que com os próprios autovetores e autovalores.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Para começar, considere alguma transformação linear em duas dimensões, como a mostrada aqui.",
  "model": "google_nmt",
  "from_community_srt": "Para começar, considere certa transformação linear em duas dimensões como a mostrada aqui.",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Ele move o vetor base i-hat para as coordenadas 3, 0 e j-hat para 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "Ela move o vetor de base î para as coordenadas [3, 0] e ĵ para [1,",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Portanto, é representado por uma matriz cujas colunas são 3, 0 e 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "2] por isso é representada com uma matriz cujas colunas são [3,",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Concentre-se no que ele faz com um vetor específico e pense na extensão desse vetor, na reta que passa por sua origem e sua ponta.",
  "model": "google_nmt",
  "from_community_srt": "0] e [1, 2]. Concentre-se no que ela faz para um determinado vetor e pensar sobre a reta gerada por esse vetor, a linha que passa por sua origem e sua ponta.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "A maioria dos vetores será eliminada durante a transformação.",
  "model": "google_nmt",
  "from_community_srt": "A maioria dos vetores vão ser retirados de sua reta durante a transformação.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Quero dizer, pareceria bastante coincidência se o local onde o vetor pousou também estivesse em algum lugar nessa linha.",
  "model": "google_nmt",
  "from_community_srt": "Quer dizer, seria muita coincidência se o lugar onde o vetor aterrissou também fosse em algum lugar nessa linha.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Mas alguns vetores especiais permanecem em sua própria extensão, o que significa que o efeito que a matriz tem sobre tal vetor é apenas esticá-lo ou comprimi-lo, como um escalar.",
  "model": "google_nmt",
  "from_community_srt": "Mas alguns vetores especiais realmente permanecem em sua reta, significando que o efeito que a matriz tem em tal vetor é apenas para esticá-lo ou esmagá-lo como um escalar.",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Para este exemplo específico, o vetor base i-hat é um desses vetores especiais.",
  "model": "google_nmt",
  "from_community_srt": "Para este exemplo específico, o vetor de base î é um desses vetores especiais",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "A extensão de i-hat é o eixo x, e a partir da primeira coluna da matriz, podemos ver que i-hat se move 3 vezes, ainda nesse eixo x.",
  "model": "google_nmt",
  "from_community_srt": "o espaço de î é o eixo dos x e a partir da primeira coluna da matriz podemos ver que î se move sobre si mesmo 3 vezes,",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Além do mais, devido à forma como as transformações lineares funcionam, qualquer outro vetor no eixo x também é esticado por um fator de 3 e, portanto, permanece em seu próprio vão.",
  "model": "google_nmt",
  "from_community_srt": "ainda nesse eixo x. Além do mais, por causa da maneira transformações lineares trabalham qualquer outro vetor no eixo dos x também é simplesmente esticada por um fator de três e, portanto,",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Um vetor um pouco mais sorrateiro que permanece em sua própria extensão durante esta transformação é negativo 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "permanece na sua própria reta. Um vetor ligeiramente mais escondido que permanece em sua própria reta durante esta transformação é [-1,",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Acaba sendo esticado por um fator de 2.",
  "model": "google_nmt",
  "from_community_srt": "1]. Ele acaba sendo esticado por um fator de 2.",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "E, novamente, a linearidade implicará que qualquer outro vetor na reta diagonal gerada por esse cara será esticado por um fator de 2.",
  "model": "google_nmt",
  "from_community_srt": "E novamente linearidade vai implicar que qualquer outro vetor na linha diagonal gerado por esse cara só será estendido por um fator de 2.",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "E para esta transformação, estes são todos os vetores com esta propriedade especial de permanecer no seu vão.",
  "model": "google_nmt",
  "from_community_srt": "E para esta transformação esses são todos os vetores com esta propriedade especial de permanecer na sua reta.",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Aqueles no eixo x sendo esticados por um fator de 3, e aqueles nesta linha diagonal sendo esticados por um fator de 2.",
  "model": "google_nmt",
  "from_community_srt": "Aqueles no eixo x, ficando estendidos por um fator de 3 e aqueles nesta linha diagonal, ficando esticadas por um fator de 2.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Qualquer outro vetor será girado um pouco durante a transformação, saindo da linha que ele abrange.",
  "model": "google_nmt",
  "from_community_srt": "Qualquer outro vetor vai ser rodado um pouco durante a transformação, derrubado de sua própria linha.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Como você já deve ter adivinhado, esses vetores especiais são chamados de autovetores da transformação, e cada autovetor tem associado a ele o que é chamado de autovalor, que é apenas o fator pelo qual ele é esticado ou comprimido durante a transformação.",
  "model": "google_nmt",
  "from_community_srt": "Como você deve ter adivinhado por agora, esses vetores especiais são chamados de “autovetores” da transformação e cada autovetor tem associado com ele o que é chamado um \"autovalor\" ou “valor próprio”, que é apenas o fator pelo qual o vetor é esticado ou comprimido durante a transformação.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "É claro que não há nada de especial em esticar versus esmagar, ou no fato de que esses autovalores são positivos.",
  "model": "google_nmt",
  "from_community_srt": "Claro, não há nada de especial sobre alongar ou esmagar ou o fato de que esses autovalores são positivos.",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "Em outro exemplo, você poderia ter um autovetor com autovalor negativo 1 metade, o que significa que o vetor é invertido e comprimido por um fator de 1 metade.",
  "model": "google_nmt",
  "from_community_srt": "Em outro exemplo, você poderia ter um autovetor com autovalor -1/2 o que significa que o vetor foi virado e esmagado por um fator de 1/2.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Mas a parte importante aqui é que ele permanece na linha que se estende sem ser girado para fora dela.",
  "model": "google_nmt",
  "from_community_srt": "Mas a parte importante aqui é que ele permanece na linha que ele mesmo gera, sem ser rodado para fora dela.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Para entender por que isso pode ser útil para se pensar, considere alguma rotação tridimensional.",
  "model": "google_nmt",
  "from_community_srt": "Para um vislumbre de por que isso pode ser uma coisa útil sobre a qual se pensar, Considere certa rotação tridimensional.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Se você puder encontrar um autovetor para essa rotação, um vetor que permaneça em seu próprio vão, o que você encontrará é o eixo de rotação.",
  "model": "google_nmt",
  "from_community_srt": "Se você puder encontrar um autovetor para essa rotação, um vetor que permanece na sua própria reta,",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "E é muito mais fácil pensar em uma rotação 3D em termos de algum eixo de rotação e um ângulo pelo qual ela gira, em vez de pensar na matriz 3x3 completa associada a essa transformação.",
  "model": "google_nmt",
  "from_community_srt": "o que você encontrou é o eixo de rotação E é muito mais fácil pensar em uma rotação 3D em termos de algum eixo de rotação e um ângulo pelo qual ele está girando ao invés de pensar sobre a matriz 3x3 integral associada com essa transformação.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "Nesse caso, aliás, o autovalor correspondente teria que ser 1, já que as rotações nunca esticam ou comprimem nada, então o comprimento do vetor permaneceria o mesmo.",
  "model": "google_nmt",
  "from_community_srt": "Neste caso, a propósito O valor próprio correspondente teria de ser um uma vez que rotações não esticam nem esmagam nada, de modo que o comprimento do vetor permaneceria o mesmo.",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Esse padrão aparece muito na álgebra linear.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Com qualquer transformação linear descrita por uma matriz, você pode entender o que ela está fazendo lendo as colunas dessa matriz como pontos de aterrissagem para vetores de base.",
  "model": "google_nmt",
  "from_community_srt": "Este padrão mostra-se muito em Álgebra Linear Com qualquer transformação linear descrita por uma matriz você poderia entender o que está fazendo através da leitura das colunas desta matriz como os pontos de pouso para vetores de base.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Mas muitas vezes, a melhor maneira de chegar ao cerne do que a transformação linear realmente faz, menos dependente do seu sistema de coordenadas específico, é encontrar os autovetores e autovalores.",
  "model": "google_nmt",
  "from_community_srt": "Mas muitas vezes a melhor maneira de chegar ao coração do que a transformação linear realmente faz menos dependente do seu sistema de coordenadas particular, é encontrar os autovetores e autovalores.",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Não cobrirei todos os detalhes sobre métodos para calcular autovetores e autovalores aqui, mas tentarei fornecer uma visão geral das ideias computacionais que são mais importantes para uma compreensão conceitual.",
  "model": "google_nmt",
  "from_community_srt": "Eu não cobrirá os detalhes completos sobre os métodos para computar autovetores e autovalores aqui, mas vou tentar dar uma visão geral das idéias computacionais que são mais importantes para uma compreensão conceitual.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Simbolicamente, esta é a aparência da ideia de um autovetor.",
  "model": "google_nmt",
  "from_community_srt": "Simbolicamente, aqui está o que a ideia de um autovetor se parece.",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A é a matriz que representa alguma transformação, com v como autovetor, e lambda é um número, ou seja, o autovalor correspondente.",
  "model": "google_nmt",
  "from_community_srt": "A é a matriz que representa certa transformação com 'v' como o autovetor e λ é um número, ou seja, o autovalor correspondente.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "O que esta expressão está dizendo é que o produto matriz-vetor, A vezes v, dá o mesmo resultado que apenas dimensionar o autovetor v por algum valor lambda.",
  "model": "google_nmt",
  "from_community_srt": "O que esta expressão está dizendo é que o produto matriz-vector, 'Av' dá o mesmo resultado que apenas escalar o autovetor 'v' por algum valor λ.",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Portanto, encontrar os autovetores e seus autovalores de uma matriz A se resume a encontrar os valores de v e lambda que tornam essa expressão verdadeira.",
  "model": "google_nmt",
  "from_community_srt": "Assim, encontrar os autovetores e seus autovalores da matriz A se resume a encontrar os valores de v e λ que fazem esta expressão verdadeira.",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "É um pouco estranho trabalhar com isso no início, porque o lado esquerdo representa a multiplicação de vetores de matrizes, mas o lado direito aqui é a multiplicação de vetores escalares.",
  "model": "google_nmt",
  "from_community_srt": "É um pouco estranho para trabalhar com isto de início, pois o lado esquerdo representa multiplicação matriz-vetor, mas o lado direito aqui é multiplicação escalar-vetor.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Então, vamos começar reescrevendo o lado direito como algum tipo de multiplicação matriz-vetor, usando uma matriz que tem o efeito de escalonar qualquer vetor por um fator lambda.",
  "model": "google_nmt",
  "from_community_srt": "Então, vamos começar por reescrever o membro direito como algum tipo de multiplicação matriz-vetor usando uma matriz que tem o efeito de escalonar qualquer vetor por um factor de λ.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "As colunas dessa matriz representarão o que acontece com cada vetor de base, e cada vetor de base é simplesmente multiplicado por lambda, então essa matriz terá o número lambda na diagonal, com zeros em todos os outros lugares.",
  "model": "google_nmt",
  "from_community_srt": "As colunas de uma tal matriz irão representar o que acontece com cada vetor de base e cada vetor de base é simplesmente multiplicado por λ, logo, esta matriz terá o número λ ao longo da diagonal, com zeros em qualquer outro lugar.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "A maneira comum de escrever esse cara é fatorar esse lambda e escrevê-lo como lambda vezes i, onde i é a matriz identidade com 1s na diagonal.",
  "model": "google_nmt",
  "from_community_srt": "A maneira comum de escrever esse cara é fatorar fora aquele λ e escrevê-lo como λI, onde I é a matriz identidade com 1's ao longo da diagonal.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Com ambos os lados parecendo uma multiplicação de matrizes e vetores, podemos subtrair o lado direito e fatorar v.",
  "model": "google_nmt",
  "from_community_srt": "Com ambos os lados parecendo multiplicação matriz-vector podemos subtrair fora esse lado direito e fatorar o 'v'.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Então o que temos agora é uma nova matriz, A menos lambda vezes a identidade, e estamos procurando um vetor v tal que esta nova matriz vezes v dê o vetor zero.",
  "model": "google_nmt",
  "from_community_srt": "Então, o que temos agora é uma nova matriz, A-λI E nós estamos à procura de um vetor 'v' de tal forma que esta nova matriz vezes 'v' dá o vetor nulo.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Agora, isso sempre será verdade se v for o vetor zero, mas isso é chato.",
  "model": "google_nmt",
  "from_community_srt": "Agora, isto será sempre verdade se o próprio 'v' é o vetor nulo, mas isso é chato.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "O que queremos é um autovetor diferente de zero.",
  "model": "google_nmt",
  "from_community_srt": "O que nós queremos é um autovetor não-nulo.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "E se você assistir aos capítulos 5 e 6, saberá que a única maneira de o produto de uma matriz com um vetor diferente de zero se tornar zero é se a transformação associada a essa matriz comprimir o espaço em uma dimensão inferior.",
  "model": "google_nmt",
  "from_community_srt": "E se você assistiu os capítulos 5 e 6 você vai saber que a única maneira é possível para o produto de uma matriz com um vetor diferente de zero para se tornar 0, é se a transformação associada com a matriz esmagar o espaço para uma dimensão inferior.",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "E esse esmagamento corresponde a um determinante zero para a matriz.",
  "model": "google_nmt",
  "from_community_srt": "E que esse esmagamento corresponde a um determinante nulo para a matriz.",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Para ser concreto, digamos que sua matriz A tenha colunas 2, 1 e 2, 3, e pense em subtrair um valor variável, lambda, de cada entrada diagonal.",
  "model": "google_nmt",
  "from_community_srt": "Para ser concreto, digamos que a sua matriz A tenha colunas [2, 1] e [2, 3], e pense sobre subtrair uma quantidade variável λ, a partir de cada entrada diagonal.",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Agora imagine ajustar o lambda, girando um botão para alterar seu valor.",
  "model": "google_nmt",
  "from_community_srt": "Agora imagine ajustar λ, rodando um botão para mudar o seu valor",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "À medida que o valor de lambda muda, a própria matriz muda e, portanto, o determinante da matriz muda.",
  "model": "google_nmt",
  "from_community_srt": "à medida que o valor de λ muda, a própria matriz muda e também muda o determinante da matriz.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "O objetivo aqui é encontrar um valor de lambda que torne esse determinante zero, o que significa que a transformação ajustada comprime o espaço em uma dimensão inferior.",
  "model": "google_nmt",
  "from_community_srt": "O objetivo aqui é encontrar um valor de λ que vai fazer esse determinante 0 ou seja, a transformação ajustada esmaga o espaço para uma dimensão inferior.",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "Nesse caso, o ponto ideal ocorre quando lambda é igual a 1.",
  "model": "google_nmt",
  "from_community_srt": "Neste caso, o ponto ideal vem quando λ = 1.",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "É claro que, se tivéssemos escolhido alguma outra matriz, o autovalor poderia não ser necessariamente 1.",
  "model": "google_nmt",
  "from_community_srt": "Claro, se nós escolhêssemos alguma outra matriz o valor próprio pode não ser necessariamente 1.",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "O ponto ideal pode ser atingido por algum outro valor de lambda.",
  "model": "google_nmt",
  "from_community_srt": "O ponto ideal pode ser atingido algum outro valor de λ.",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Então isso é bastante, mas vamos desvendar o que isso quer dizer.",
  "model": "google_nmt",
  "from_community_srt": "Portanto, é muita informação, mas vamos desvendar o que isto está dizendo.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Quando lambda é igual a 1, a matriz A menos lambda vezes a identidade comprime o espaço em uma linha.",
  "model": "google_nmt",
  "from_community_srt": "Quando λ = 1, a matriz A-λI comprime o espaço para uma linha.",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Isso significa que existe um vetor diferente de zero v tal que A menos lambda vezes a identidade vezes v é igual ao vetor zero.",
  "model": "google_nmt",
  "from_community_srt": "Isso significa que há um vetor diferente de zero, 'v', de tal modo que (A-λI) v é igual ao vetor nulo.",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "E lembre-se, a razão pela qual nos preocupamos com isso é porque significa A vezes v é igual a lambda vezes v, o que você pode interpretar como dizendo que o vetor v é um autovetor de A, permanecendo em seu próprio vão durante a transformação A.",
  "model": "google_nmt",
  "from_community_srt": "E lembre-se, a razão pela qual nos preocupamos com isto é, porque isto significa Av = λv, que você pode ler como dizendo que o vetor 'v' é um autovetor de A ficando na sua própria reta durante a transformação A.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "Neste exemplo, o autovalor correspondente é 1, então v permaneceria fixo no lugar.",
  "model": "google_nmt",
  "from_community_srt": "Neste exemplo, o valor próprio correspondente é 1. Então 'v' apenas ficaria fixo no lugar.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Faça uma pausa e pondere se você precisa ter certeza de que essa linha de raciocínio é boa.",
  "model": "google_nmt",
  "from_community_srt": "Pare e pense se você precisa se certificar de que essa linha de raciocínio faz sentido.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Esse é o tipo de coisa que mencionei na introdução.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Se você não tivesse uma compreensão sólida dos determinantes e por que eles se relacionam com sistemas lineares de equações com soluções diferentes de zero, uma expressão como essa pareceria completamente inesperada.",
  "model": "google_nmt",
  "from_community_srt": "Este é o tipo de coisa que eu mencionei na introdução, se você não tem uma sólida compreensão dos determinantes e por que eles se relacionam com sistemas lineares de equações Tendo em soluções diferentes de zero, uma expressão como esta iria se parecer completamente vinda do nada.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Para ver isso em ação, vamos revisitar o exemplo desde o início, com uma matriz cujas colunas são 3, 0 e 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "Para ver isso em ação, vamos revisitar o exemplo do início. Com a matriz cujas colunas são [3,",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Para descobrir se um valor lambda é um autovalor, subtraia-o das diagonais desta matriz e calcule o determinante.",
  "model": "google_nmt",
  "from_community_srt": "0] e [1, 2], para descobrir se um valor λ é um autovalor subtraia o mesmo a partir da diagonal desta matriz e calcule o determinante.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Fazendo isso, obtemos um certo polinômio quadrático em lambda, 3 menos lambda vezes 2 menos lambda.",
  "model": "google_nmt",
  "from_community_srt": "Fazendo isso, temos um certo polinomial quadrático em λ, (3-λ) (2-λ)",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Como lambda só pode ser um autovalor se esse determinante for zero, você pode concluir que os únicos autovalores possíveis são lambda igual a 2 e lambda igual a 3.",
  "model": "google_nmt",
  "from_community_srt": "Dado que λ ​​só pode ser um autovalor se este determinante passa a ser 0, você pode concluir que os únicos valores próprios possíveis são λ = 2 e λ = 3.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Para descobrir quais são os autovetores que realmente possuem um desses autovalores, digamos que lambda é igual a 2, insira esse valor de lambda na matriz e, em seguida, resolva quais vetores essa matriz alterada diagonalmente envia para zero.",
  "model": "google_nmt",
  "from_community_srt": "Para descobrir quais são os autovetores que realmente têm um desses autovalores, digamos, λ = 2, coloque este valor de λ na matriz e, em seguida, resolva os vetores que esta matriz alterada envia ao vetor nulo.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Se você calculasse isso da mesma forma que faria com qualquer outro sistema linear, veria que as soluções são todos os vetores na linha diagonal medido por menos 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Se você fez a conta, da mesma forma que você faria com qualquer outro sistema linear, você veria que as soluções são todos os vetores na linha diagonal gerada por [-1,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Isto corresponde ao fato de que a matriz inalterada, 3, 0, 1, 2, tem o efeito de esticar todos esses vetores por um fator de 2.",
  "model": "google_nmt",
  "from_community_srt": "1]. Isto corresponde ao fato da matriz inalterada 3 1 0 2 tem o efeito de alongar todos esses vetores por um fator de 2.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Agora, uma transformação 2D não precisa ter vetores próprios.",
  "model": "google_nmt",
  "from_community_srt": "Agora, uma transformação 2D não tem que ter autovetores.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Por exemplo, considere uma rotação de 90 graus.",
  "model": "google_nmt",
  "from_community_srt": "Por exemplo,",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Isso não possui vetores próprios, pois gira cada vetor fora de seu próprio intervalo.",
  "model": "google_nmt",
  "from_community_srt": "considere uma rotação de 90°, esta não tem nenhum autovetor, uma vez que gira cada vetor fora de sua própria reta.",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Se você realmente tentar calcular os autovalores de uma rotação como essa, observe o que acontece.",
  "model": "google_nmt",
  "from_community_srt": "Se você realmente tente calcular os valores próprios de uma rotação como esta, observe o que acontece:",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Sua matriz possui colunas 0, 1 e negativo 1, 0.",
  "model": "google_nmt",
  "from_community_srt": "a sua matriz tem colunas [0, 1] e [-1,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Subtraia lambda dos elementos diagonais e procure quando o determinante é zero.",
  "model": "google_nmt",
  "from_community_srt": "0] subtraia λ a partir dos elementos da diagonal, e procure por quando o determinante é 0.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "Nesse caso, você obtém o polinômio lambda ao quadrado mais 1.",
  "model": "google_nmt",
  "from_community_srt": "Neste caso, você tem o polinômio a λ^2 + 1; as únicas raízes desse polinômio",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "As únicas raízes desse polinômio são os números imaginários, i e negativo i.",
  "model": "google_nmt",
  "from_community_srt": "são os números imaginários i e -i; o fato de que não existem soluções reais",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "O fato de não existirem soluções de números reais indica que não existem autovetores.",
  "model": "google_nmt",
  "from_community_srt": "indica que não existem autovetores.",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Outro exemplo bastante interessante que vale a pena manter em mente é uma tesoura.",
  "model": "google_nmt",
  "from_community_srt": "Outro exemplo bastante interessante, que vale a pena se lembrar, é um cisalhamento.",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Isso fixa o i-hat no lugar e move o j-hat 1, de modo que sua matriz tenha as colunas 1, 0 e 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Isso mantém î no lugar, e move ĵ chapéu para î+ĵ, Portanto, a sua matriz tem colunas [1,",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Todos os vetores no eixo x são autovetores com autovalor 1, pois permanecem fixos no lugar.",
  "model": "google_nmt",
  "from_community_srt": "0] e [1, 1] Todos os vetores no eixo x são autovetores com autovalor 1, uma vez que eles permanecem fixos no lugar.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "Na verdade, esses são os únicos autovetores.",
  "model": "google_nmt",
  "from_community_srt": "Na verdade,",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Quando você subtrai lambda das diagonais e calcula o determinante, o que você obtém é 1 menos lambda ao quadrado.",
  "model": "google_nmt",
  "from_community_srt": "estes são os únicos autovetores quando você subtrai λ das diagonais e calcula o determinante.",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "E a única raiz desta expressão é lambda igual a 1.",
  "model": "google_nmt",
  "from_community_srt": "O que você recebe é (1-λ)^2, e a única raiz desta expressão é λ = 1.",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Isso está de acordo com o que vemos geometricamente, que todos os autovetores têm autovalor 1.",
  "model": "google_nmt",
  "from_community_srt": "Isto se alinha com o que vemos geometricamente, que todos os autovetores têm valor próprio 1.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Tenha em mente, porém, que também é possível ter apenas um autovalor, mas com mais do que apenas uma linha cheia de autovetores.",
  "model": "google_nmt",
  "from_community_srt": "Tenha em mente, no entanto, também é possível ter apenas um valor próprio mas com mais do que apenas uma linha completa de autovetores.",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Um exemplo simples é uma matriz que dimensiona tudo por 2.",
  "model": "google_nmt",
  "from_community_srt": "Um exemplo simples é uma matriz que escala tudo por 2.",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "O único autovalor é 2, mas todo vetor no plano passa a ser um autovetor com esse autovalor.",
  "model": "google_nmt",
  "from_community_srt": "O único valor próprio é 2, mas cada vetor no plano consegue ser um autovetor associado a este autovalor.",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Agora é outro bom momento para fazer uma pausa e refletir sobre isso antes de passar para o último tópico.",
  "model": "google_nmt",
  "from_community_srt": "Agora é outro bom momento para fazer uma pausa e refletir sobre algumas dessas antes de passar para o último tópico.",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Quero terminar aqui com a ideia de uma base própria, que se baseia muito nas ideias do último vídeo.",
  "model": "google_nmt",
  "from_community_srt": "Eu quero terminar aqui com a idéia de uma \"autobase\", que depende fortemente de idéias do último vídeo.",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Dê uma olhada no que acontece se nossos vetores de base forem autovetores.",
  "model": "google_nmt",
  "from_community_srt": "Dê uma olhada no que acontece se os nossos vetores de base apenas calham de ser autovetores.",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Por exemplo, talvez i-hat seja dimensionado em menos 1 e j-hat seja dimensionado em 2.",
  "model": "google_nmt",
  "from_community_srt": "Por exemplo, talvez î é dimensionado por -1 e ĵ é dimensionado por 2.",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Escrevendo suas novas coordenadas como as colunas de uma matriz, observe que esses múltiplos escalares, menos 1 e 2, que são os autovalores de i-hat e j-hat, ficam na diagonal de nossa matriz, e todas as outras entradas são 0 .",
  "model": "google_nmt",
  "from_community_srt": "Escrevendo suas novas coordenadas como as colunas de uma matriz, note que estes múltiplos escalares -1 e 2 que são os valores próprios de î e ĵ, sentam-se na diagonal da nossa matriz e todas as outras entradas são 0.",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Sempre que uma matriz tem zeros em todos os lugares, exceto na diagonal, ela é chamada, razoavelmente, de matriz diagonal.",
  "model": "google_nmt",
  "from_community_srt": "Sempre que uma matriz tem zeros em todos os lugares com exceção da diagonal, ele é chamada, muito razoavelmente,",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "E a maneira de interpretar isto é que todos os vetores de base são autovetores, sendo as entradas diagonais desta matriz os seus autovalores.",
  "model": "google_nmt",
  "from_community_srt": "de “matriz diagonal” e a maneira de se interpretar isso é que todos os vetores de base são autovetores com os elementos da diagonal desta matriz sendo os seus valores próprios.",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Há muitas coisas que tornam as matrizes diagonais muito mais agradáveis de trabalhar.",
  "model": "google_nmt",
  "from_community_srt": "Há um monte de coisas que tornam matrizes diagonais muito mais agradáveis de se trabalhar, uma grande motivo é que é mais fácil de calcular o que vai acontecer",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Um grande problema é que é mais fácil calcular o que acontecerá se você multiplicar essa matriz por ela mesma várias vezes.",
  "model": "google_nmt",
  "from_community_srt": "se você multiplicar essa matriz por si só um monte de vezes uma vez que tudo que uma dessas matrizes faz",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Como tudo o que uma dessas matrizes faz é dimensionar cada vetor de base por algum autovalor, aplicar essa matriz muitas vezes, digamos 100 vezes, corresponderá apenas a dimensionar cada vetor de base pela 100ª potência do autovalor correspondente.",
  "model": "google_nmt",
  "from_community_srt": "é dimensionar cada vetor da base por algum autovalor. Aplicando essa matriz muitas vezes, digamos, 100 vezes, é só ir para corresponder a escalar cada vetor da base pela 100-ésima potência do valor próprio correspondente.",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Em contraste, tente calcular a centésima potência de uma matriz não diagonal.",
  "model": "google_nmt",
  "from_community_srt": "Em contraste, tente computar a centésima potência de uma matriz não-diagonal.",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Sério, experimente por um momento.",
  "model": "google_nmt",
  "from_community_srt": "Sério,",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "É um pesadelo.",
  "model": "google_nmt",
  "from_community_srt": "experimente por um momento, é um pesadelo!",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "É claro que você raramente terá a sorte de ter seus vetores de base também como autovetores.",
  "model": "google_nmt",
  "from_community_srt": "Claro, você raramente vai ter tanta sorte como ter seus vetores de base como  autovetores.",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Mas se a sua transformação tiver muitos autovetores, como o do início deste vídeo, o suficiente para que você possa escolher um conjunto que abranja todo o espaço, então você poderá alterar seu sistema de coordenadas para que esses autovetores sejam seus vetores de base.",
  "model": "google_nmt",
  "from_community_srt": "Mas se a sua transformação tem um monte de autovetores, como aquela no início deste vídeo, o suficiente para que você possa escolher um conjunto que gera todo o espaço, então você pode mudar o seu sistema de coordenadas de modo que estes autovetores são seus vetores de base.",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Falei sobre mudança de base no vídeo passado, mas vou fazer um lembrete super rápido aqui de como expressar uma transformação atualmente escrita em nosso sistema de coordenadas em um sistema diferente.",
  "model": "google_nmt",
  "from_community_srt": "Eu falei sobre mudança de base no último vídeo, mas eu vou passar um lembrete super rápido aqui de como expressar a transformação atualmente escrito em nosso sistema de coordenadas,",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Pegue as coordenadas dos vetores que deseja usar como uma nova base, que neste caso significa nossos dois autovetores, e depois transforme essas coordenadas nas colunas de uma matriz, conhecida como matriz de mudança de base.",
  "model": "google_nmt",
  "from_community_srt": "em um sistema diferente. Tome as coordenadas dos vetores que você deseja usar como uma nova base que, neste caso, são os dois autovetores, em seguida, faça essas coordenadas como as colunas de uma matriz conhecido como a matriz de mudança de base.",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Quando você imprensa a transformação original, colocando a matriz de mudança de base à sua direita e o inverso da matriz de mudança de base à sua esquerda, o resultado será uma matriz representando essa mesma transformação, mas da perspectiva da nova coordenada dos vetores de base sistema.",
  "model": "google_nmt",
  "from_community_srt": "Quando você faz um sanduíche com transformação original colocando a mudança da matriz de base à direita, e o inverso da matriz de mudança de  base à esquerda, o resultado será uma matriz representando aquela mesma transformação, mas a partir da perspectiva do sistema de coordenadas dos novos vetores de base.",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "O objetivo de fazer isso com autovetores é que essa nova matriz tem a garantia de ser diagonal com seus autovalores correspondentes nessa diagonal.",
  "model": "google_nmt",
  "from_community_srt": "Todo a ponto de fazer isso com autovetores é que esta nova matriz é garantida para ser diagonal com os seus autovalores correspondentes ao longo da diagonal.",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Isso ocorre porque representa trabalhar em um sistema de coordenadas onde o que acontece com os vetores de base é que eles são escalonados durante a transformação.",
  "model": "google_nmt",
  "from_community_srt": "Isso é porque ele representa trabalhar em um sistema de coordenadas onde o que acontece com os vetores de base é que eles são escalados durante a transformação.",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Um conjunto de vetores de base que também são autovetores é chamado, novamente, razoavelmente, de autobase.",
  "model": "google_nmt",
  "from_community_srt": "Um conjunto de vetores de base, que são também autovetores é chamado, novamente com muita razão, uma “autobase” [base de autovetores].",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Portanto, se, por exemplo, você precisasse calcular a centésima potência desta matriz, seria muito mais fácil mudar para uma base própria, calcular a centésima potência nesse sistema e depois converter novamente para o nosso sistema padrão.",
  "model": "google_nmt",
  "from_community_srt": "Assim, se, por exemplo, que você precisava calcular a 100-ésima potência desta matriz, seria muito mais fácil mudar para uma autobase calcular o poder 100º nesse sistema em seguida, converter de volta para o nosso sistema padrão.",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Você não pode fazer isso com todas as transformações.",
  "model": "google_nmt",
  "from_community_srt": "Você não pode fazer isso com todas as transformações.",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Um cisalhamento, por exemplo, não possui vetores próprios suficientes para abranger todo o espaço.",
  "model": "google_nmt",
  "from_community_srt": "Um cisalhamento, por exemplo, não tem autovetores suficientes para se estender por todo o espaço.",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Mas se você puder encontrar uma base própria, isso tornará as operações matriciais realmente adoráveis.",
  "model": "google_nmt",
  "from_community_srt": "Mas se você puder encontrar uma autobase, ela faz as operações matriciais realmente adoráveis.",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Para aqueles que desejam resolver um quebra-cabeça bem bacana para ver como ele fica em ação e como pode ser usado para produzir alguns resultados surpreendentes, deixarei um aviso aqui na tela.",
  "model": "google_nmt",
  "from_community_srt": "Para aqueles dispostos a trabalhar em um quebra-cabeça muito legal para ver o que esta parece em ação e como ele pode ser usado para produzir alguns resultados surpreendentes. Vou deixar um prompt aqui na tela.",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Dá um pouco de trabalho, mas acho que você vai gostar.",
  "model": "google_nmt",
  "from_community_srt": "É preciso um pouco de trabalho mas acho que você vai gostar.",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "O próximo e último vídeo desta série será sobre espaços vetoriais abstratos.",
  "model": "google_nmt",
  "from_community_srt": "O próximo e último vídeo desta série vai ser em “espaços vetoriais abstratos”.",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]