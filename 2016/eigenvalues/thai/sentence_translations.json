[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive. ",
  "translatedText": "ค่าลักษณะเฉพาะและค่าลักษณะเฉพาะเป็นหนึ่งในหัวข้อเหล่านั้นที่นักเรียนจำนวนมากพบว่าไม่เป็นไปตามสัญชาตญาณเป็นพิเศษ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Things like, why are we doing this, and what does this actually mean, are too often left just floating away in an unanswered sea of computations. ",
  "translatedText": "สิ่งต่างๆ เช่น ทำไมเราถึงทำเช่นนี้ และสิ่งนี้หมายความว่าอย่างไร จริงๆ แล้วมักถูกปล่อยทิ้งไว้ในทะเลแห่งการคำนวณที่ยังไม่มีคำตอบ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular. ",
  "translatedText": "และในขณะที่ฉันได้เผยแพร่วิดีโอของซีรีส์นี้ หลายๆ คนได้แสดงความคิดเห็นเกี่ยวกับการรอคอยที่จะได้เห็นหัวข้อนี้โดยเฉพาะ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained. ",
  "translatedText": "ฉันสงสัยว่าเหตุผลของเรื่องนี้ไม่ได้มากจนเกินไปจนทำให้ eigenthings มีความซับซ้อนเป็นพิเศษหรืออธิบายได้ไม่ดีนัก ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it. ",
  "translatedText": "ที่จริงแล้ว มันค่อนข้างตรงไปตรงมา และฉันคิดว่าหนังสือส่วนใหญ่ก็อธิบายเรื่องนี้ได้ดี ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "What I want to do is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it. ",
  "translatedText": "สิ่งที่ฉันต้องการทำคือมันจะสมเหตุสมผลถ้าคุณมีความเข้าใจที่ชัดเจนสำหรับหลายๆ หัวข้อที่อยู่ข้างหน้า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis. ",
  "translatedText": "สิ่งสำคัญที่สุดคือคุณรู้วิธีคิดเกี่ยวกับเมทริกซ์ว่าเป็นการแปลงเชิงเส้น แต่คุณต้องคุ้นเคยกับสิ่งต่างๆ เช่น ดีเทอร์มิแนนต์ ระบบสมการเชิงเส้น และการเปลี่ยนแปลงพื้นฐานด้วย ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves. ",
  "translatedText": "ความสับสนเกี่ยวกับลักษณะเฉพาะมักจะเกี่ยวข้องกับรากฐานที่สั่นคลอนในหัวข้อใดหัวข้อหนึ่งเหล่านี้มากกว่าเกี่ยวข้องกับค่าลักษณะเฉพาะและค่าลักษณะเฉพาะเอง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here. ",
  "translatedText": "ในการเริ่มต้น ให้พิจารณาการแปลงเชิงเส้นในสองมิติ ดังที่แสดงไว้ที่นี่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2. ",
  "translatedText": "มันย้ายเวกเตอร์ฐาน i-hat ไปยังพิกัด 3, 0 และ j-hat ไปที่ 1, 2 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2. ",
  "translatedText": "มันเลยแสดงด้วยเมทริกซ์ซึ่งมีคอลัมน์เป็น 3, 0 และ 1, 2 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip. ",
  "translatedText": "มุ่งเน้นไปที่สิ่งที่มันทำกับเวกเตอร์ตัวใดตัวหนึ่ง และคิดถึงสแปนของเวกเตอร์นั้น เส้นตรงที่ลากผ่านจุดกำเนิดและส่วนปลายของมัน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation. ",
  "translatedText": "เวกเตอร์ส่วนใหญ่จะหลุดจากสแปนระหว่างการแปลง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line. ",
  "translatedText": "ฉันหมายความว่า มันดูค่อนข้างบังเอิญถ้าสถานที่ที่เวกเตอร์ตกลงไปนั้นบังเอิญอยู่ที่ไหนสักแห่งบนเส้นนั้นด้วย ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar. ",
  "translatedText": "แต่เวกเตอร์พิเศษบางตัวยังคงอยู่ในสแปนของมันเอง ซึ่งหมายความว่าผลกระทบที่เมทริกซ์มีต่อเวกเตอร์นั้นก็แค่ยืดหรือบีบมัน เหมือนสเกลาร์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector. ",
  "translatedText": "สำหรับตัวอย่างเฉพาะนี้ เวกเตอร์พื้นฐาน i-hat คือเวกเตอร์พิเศษตัวหนึ่ง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis. ",
  "translatedText": "สแปนของ i-hat คือแกน x และจากคอลัมน์แรกของเมทริกซ์ เราจะเห็นว่า i-hat เคลื่อนที่ไป 3 เท่าของตัวมันเอง โดยยังอยู่บนแกน x นั้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span. ",
  "translatedText": "ยิ่งกว่านั้น เนื่องจากวิธีการทำงานของการแปลงเชิงเส้น เวกเตอร์อื่นๆ บนแกน x ก็ถูกยืดออกด้วยตัวคูณ 3 เช่นกัน และด้วยเหตุนี้จึงยังคงอยู่ในสแปนของมันเอง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1. ",
  "translatedText": "เวกเตอร์ส่อเสียดกว่าเล็กน้อยที่ยังคงอยู่ในสแปนของมันเองระหว่างการแปลงนี้คือลบ 1, 1 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2. ",
  "translatedText": "มันจบลงด้วยการยืดออกด้วย 2 เท่า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2. ",
  "translatedText": "อีกครั้ง ความเป็นเส้นตรงจะบอกเป็นนัยว่าเวกเตอร์อื่นๆ บนเส้นทแยงมุมที่เจ้านี่สแปน จะยืดออกด้วยตัวคูณ 2 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span. ",
  "translatedText": "และสำหรับการแปลงนี้, พวกนั้นคือเวกเตอร์ทั้งหมดที่มีคุณสมบัติพิเศษว่าคงอยู่ในสแปนของมัน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2. ",
  "translatedText": "สิ่งที่อยู่บนแกน x จะยืดออกด้วย 3 และสิ่งที่อยู่บนเส้นทแยงมุมนี้จะถูกยืดออกด้วย 2 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans. ",
  "translatedText": "เวกเตอร์อื่นๆ จะถูกหมุนไปบ้างระหว่างการแปลง ทำให้หลุดเส้นตรงที่มันสแปน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation. ",
  "translatedText": "ดังที่คุณอาจเดาได้ในตอนนี้ เวกเตอร์พิเศษเหล่านี้เรียกว่าเวกเตอร์ลักษณะเฉพาะของการแปลง และเวกเตอร์ลักษณะเฉพาะแต่ละตัวเชื่อมโยงกันด้วยสิ่งที่เรียกว่าค่าลักษณะเฉพาะ ซึ่งเป็นเพียงปัจจัยที่มันยืดหรือบีบระหว่างการแปลง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing or the fact that these eigenvalues happen to be positive. ",
  "translatedText": "แน่นอนว่าไม่มีอะไรพิเศษเกี่ยวกับการยืดหรือบีบหรือความจริงที่ว่าค่าลักษณะเฉพาะเหล่านี้เกิดขึ้นในเชิงบวก ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half. ",
  "translatedText": "ในอีกตัวอย่างหนึ่ง คุณอาจมีเวกเตอร์ลักษณะเฉพาะที่มีค่าลักษณะเฉพาะเป็นลบ 1 ครึ่ง ซึ่งหมายความว่าเวกเตอร์จะกลับด้านและถูกบีบไป 1 เท่าครึ่ง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it. ",
  "translatedText": "แต่ส่วนสำคัญตรงนี้คือ มันอยู่บนเส้นตรงที่ขยายออกโดยไม่ถูกหมุนออกไป ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation. ",
  "translatedText": "หากต้องการทราบว่าเหตุใดจึงอาจเป็นประโยชน์ให้พิจารณาการหมุนสามมิติ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation. ",
  "translatedText": "หากคุณสามารถหาเวกเตอร์ลักษณะเฉพาะสำหรับการหมุนนั้น ซึ่งเป็นเวกเตอร์ที่คงอยู่ในช่วงของมันเอง สิ่งที่คุณพบคือแกนของการหมุน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3 by 3 matrix associated with that transformation. ",
  "translatedText": "และมันง่ายกว่ามากที่จะคิดถึงการหมุน 3 มิติ ในแง่ของแกนการหมุนบางแกน และมุมที่ใช้หมุน แทนที่จะคิดถึงเมทริกซ์ขนาด 3 คูณ 3 เต็มที่เกี่ยวข้องกับการแปลงนั้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same. ",
  "translatedText": "อย่างไรก็ตาม ในกรณีนี้ ค่าลักษณะเฉพาะที่สอดคล้องกันจะต้องเป็น 1 เนื่องจากการหมุนไม่เคยยืดหรือบีบสิ่งใดๆ เลย ดังนั้นความยาวของเวกเตอร์จะยังคงเท่าเดิม ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra. ",
  "translatedText": "รูปแบบนี้แสดงให้เห็นมากในพีชคณิตเชิงเส้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors. ",
  "translatedText": "ด้วยการแปลงเชิงเส้นใดๆ ที่อธิบายโดยเมทริกซ์ คุณสามารถเข้าใจได้ว่ามันกำลังทำอะไรโดยการอ่านคอลัมน์ของเมทริกซ์นี้เป็นจุดลงสู่เวกเตอร์ฐาน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues. ",
  "translatedText": "แต่บ่อยครั้ง วิธีที่ดีกว่าในการเข้าใจถึงสิ่งที่การแปลงเชิงเส้นทำได้จริง โดยไม่ต้องขึ้นอยู่กับระบบพิกัดเฉพาะของคุณ ก็คือการค้นหาเวกเตอร์ลักษณะเฉพาะและค่าลักษณะเฉพาะ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding. ",
  "translatedText": "ฉันจะไม่กล่าวถึงรายละเอียดทั้งหมดเกี่ยวกับวิธีการคำนวณเวกเตอร์ลักษณะเฉพาะและค่าลักษณะเฉพาะที่นี่ แต่ฉันจะพยายามให้ภาพรวมของแนวคิดการคำนวณที่สำคัญที่สุดสำหรับการทำความเข้าใจแนวความคิด ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like. ",
  "translatedText": "ในเชิงสัญลักษณ์ นี่คือลักษณะของแนวคิดของเวกเตอร์ลักษณะเฉพาะ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue. ",
  "translatedText": "A คือเมทริกซ์ที่แสดงถึงการเปลี่ยนแปลงบางอย่าง โดยมี v เป็นเวกเตอร์ลักษณะเฉพาะ และแลมบ์ดาคือตัวเลข ซึ่งก็คือค่าลักษณะเฉพาะที่สอดคล้องกัน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda. ",
  "translatedText": "สิ่งที่พจน์นี้บอกคือผลคูณเมทริกซ์-เวกเตอร์, A คูณ v, ให้ผลลัพธ์เหมือนกับแค่ขยายเวกเตอร์ลักษณะเฉพาะ v ด้วยค่าแลมดาบางค่า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true. ",
  "translatedText": "ดังนั้นการค้นหาเวกเตอร์ลักษณะเฉพาะและค่าลักษณะเฉพาะของเมทริกซ์ A ลงมาเพื่อหาค่าของ v และแลมด้าที่ทำให้พจน์นี้เป็นจริง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication. ",
  "translatedText": "มันแปลกๆ นิดหน่อยที่จะจัดการทีแรก เพราะทางซ้ายมือนั่นแทนการคูณเมทริกซ์กับเวกเตอร์ แต่ทางขวามือตรงนี้คือการคูณสเกลาร์กับเวกเตอร์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda. ",
  "translatedText": "ลองเริ่มด้วยการเขียนทางขวามือนั้นใหม่เป็นการคูณเมทริกซ์-เวกเตอร์ โดยใช้เมทริกซ์ซึ่งมีผลของการขยายเวกเตอร์ใดๆ ด้วยแฟคเตอร์ของแลมดา ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else. ",
  "translatedText": "คอลัมน์ของเมทริกซ์ดังกล่าวจะแสดงสิ่งที่เกิดขึ้นกับเวกเตอร์พื้นฐานแต่ละตัว และเวกเตอร์พื้นฐานแต่ละตัวก็คูณด้วยแลมบ์ดา ดังนั้นเมทริกซ์นี้จะมีตัวเลขแลมดาอยู่ในแนวทแยง โดยมีศูนย์อยู่ที่อื่น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with ones down the diagonal. ",
  "translatedText": "วิธีทั่วไปในการเขียนเจ้านี่ คือแยกตัวประกอบแลมดานั้นออกมา แล้วเขียนมันเป็นแลมด้าคูณ i โดยที่ i คือเมทริกซ์เอกลักษณ์ที่มีตัวอยู่ในแนวทแยง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v. ",
  "translatedText": "เมื่อทั้งสองด้านดูเหมือนการคูณเมทริกซ์-เวกเตอร์ เราสามารถลบทางด้านขวามือนั้นออก แล้วแยกตัวประกอบ v ได้ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix, times v, gives the zero vector. ",
  "translatedText": "แล้วสิ่งที่เราได้ตอนนี้คือเมทริกซ์ใหม่ A ลบแลมด้าคูณเอกลักษณ์ และเรากำลังมองหาเวกเตอร์ v โดยที่เมทริกซ์ใหม่นี้ คูณ v ให้เวกเตอร์เป็นศูนย์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring. ",
  "translatedText": "ทีนี้, นี่จะเป็นจริงเสมอถ้า v เป็นเวกเตอร์ศูนย์ แต่มันน่าเบื่อ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector. ",
  "translatedText": "สิ่งที่เราต้องการคือเวกเตอร์ลักษณะเฉพาะที่ไม่เป็นศูนย์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension. ",
  "translatedText": "และถ้าคุณดูบทที่ 5 และ 6 คุณจะรู้ว่าวิธีเดียวที่เป็นไปได้ที่ผลคูณของเมทริกซ์ที่มีเวกเตอร์ที่ไม่ใช่ศูนย์จะกลายเป็น 0 คือถ้าการแปลงที่เกี่ยวข้องกับเมทริกซ์นั้นบีบพื้นที่ลงในมิติที่ต่ำกว่า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix. ",
  "translatedText": "และการบีบตัวนั้นสอดคล้องกับค่าศูนย์ของเมทริกซ์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry. ",
  "translatedText": "เพื่อให้เป็นรูปธรรม สมมติว่าเมทริกซ์ A มีคอลัมน์ 2, 1 และ 2, 3 และคิดถึงการลบจำนวนตัวแปร lambda ออกจากค่าในแนวทแยงแต่ละรายการ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value. ",
  "translatedText": "ตอนนี้ลองจินตนาการถึงการปรับแต่งแลมบ์ดาโดยหมุนปุ่มเพื่อเปลี่ยนค่า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes. ",
  "translatedText": "เมื่อค่าของแลมบ์ดาเปลี่ยนแปลง เมทริกซ์เองก็เปลี่ยน และดีเทอร์มิแนนต์ของเมทริกซ์ก็เปลี่ยนไปด้วย ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension. ",
  "translatedText": "เป้าหมายที่นี่คือการค้นหาค่าแลมบ์ดาที่จะทำให้ดีเทอร์มิแนนต์นี้เป็นศูนย์ ซึ่งหมายความว่าการแปลงที่ได้รับการปรับแต่งจะบีบพื้นที่ลงในมิติที่ต่ำกว่า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1. ",
  "translatedText": "ในกรณีนี้ จุดที่น่าสนใจจะเกิดขึ้นเมื่อแลมบ์ดาเท่ากับ 1 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1. ",
  "translatedText": "แน่นอน หากเราเลือกเมทริกซ์อื่น ค่าลักษณะเฉพาะอาจไม่จำเป็นต้องเป็น 1 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda. ",
  "translatedText": "จุดที่น่าสนใจอาจถูกกระทบด้วยค่าอื่นของแลมบ์ดา ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying. ",
  "translatedText": "นี่เป็นเรื่องค่อนข้างมาก แต่มาไขคำตอบกันดีกว่า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line. ",
  "translatedText": "เมื่อแลมบ์ดาเท่ากับ 1 เมทริกซ์ A ลบแลมบ์ดาคูณเอกลักษณ์จะบีบพื้นที่ลงบนเส้นตรง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector. ",
  "translatedText": "นั่นหมายความว่ามีเวกเตอร์ที่ไม่ใช่ศูนย์ โดยที่ A ลบ แลมด้า คูณเอกลักษณ์ คูณ v เท่ากับเวกเตอร์ศูนย์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A. ",
  "translatedText": "และจำไว้, เหตุผลที่เราสนใจเรื่องนั้นก็เพราะมันหมายความว่า A คูณ v เท่ากับแลมดาคูณ v, ซึ่งคุณอ่านออกได้ว่าเวกเตอร์ v เป็นเวกเตอร์ลักษณะเฉพาะของ A, คงอยู่ในช่วงสแปนของมันเองระหว่างการแปลง A ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place. ",
  "translatedText": "ในตัวอย่างนี้ ค่าลักษณะเฉพาะที่สอดคล้องกันคือ 1 ดังนั้น v จะคงที่อยู่กับที่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good. ",
  "translatedText": "หยุดและไตร่ตรองหากคุณต้องการให้แน่ใจว่าการใช้เหตุผลแบบนั้นรู้สึกดีหรือไม่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction. ",
  "translatedText": "นี่คือสิ่งที่ฉันกล่าวถึงในบทนำ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue. ",
  "translatedText": "หากคุณไม่เข้าใจปัจจัยกำหนดอย่างถ่องแท้ และทำไมมันถึงเกี่ยวข้องกับระบบสมการเชิงเส้นที่มีคำตอบที่ไม่เป็นศูนย์ นิพจน์แบบนี้จะให้ความรู้สึกผิดเพี้ยนไปโดยสิ้นเชิง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2. ",
  "translatedText": "หากต้องการดูการดำเนินการนี้ เรามาทบทวนตัวอย่างตั้งแต่เริ่มต้น โดยมีเมทริกซ์ซึ่งมีคอลัมน์เป็น 3, 0 และ 1, 2 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant. ",
  "translatedText": "หากต้องการค้นหาว่าค่าแลมบ์ดาเป็นค่าลักษณะเฉพาะหรือไม่ ให้ลบออกจากเส้นทแยงมุมของเมทริกซ์แล้วคำนวณดีเทอร์มิแนนต์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda. ",
  "translatedText": "ทำแบบนี้ เราจะได้พหุนามกำลังสองในแลมด้า 3 ลบแลมด้า คูณ 2 ลบแลมด้า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3. ",
  "translatedText": "เนื่องจากแลมบ์ดาสามารถเป็นค่าลักษณะเฉพาะได้ก็ต่อเมื่อปัจจัยกำหนดนี้มีค่าเป็นศูนย์ คุณจึงสามารถสรุปได้ว่าค่าลักษณะเฉพาะที่เป็นไปได้เพียงค่าเดียวคือแลมบ์ดาเท่ากับ 2 และแลมบ์ดาเท่ากับ 3 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero. ",
  "translatedText": "หากต้องการทราบว่าเวกเตอร์ลักษณะเฉพาะมีค่าใดค่าหนึ่งเหล่านี้ สมมติว่าแลมบ์ดาเท่ากับ 2 แทนค่าแลมบ์ดานั้นเข้ากับเมทริกซ์ แล้วแก้โจทย์ว่าเวกเตอร์ใดที่เมทริกซ์ที่ถูกแก้ไขในแนวทแยงนี้ส่งไปที่ศูนย์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1. ",
  "translatedText": "หากคุณคำนวณนี่เหมือนกับระบบเชิงเส้นอื่นๆ คุณจะเห็นว่าคำตอบคือเวกเตอร์ทั้งหมดบนเส้นทแยงมุมที่สแปนด้วยลบ 1, 1 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2. ",
  "translatedText": "สิ่งนี้สอดคล้องกับความจริงที่ว่าเมทริกซ์ที่ไม่เปลี่ยนแปลง 3, 0, 1, 2 มีผลในการยืดเวกเตอร์ทั้งหมดเหล่านั้นด้วย 2 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors. ",
  "translatedText": "ทีนี้ การแปลง 2 มิติไม่จำเป็นต้องมีเวกเตอร์ลักษณะเฉพาะ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees. ",
  "translatedText": "เช่น ลองพิจารณาการหมุน 90 องศา ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span. ",
  "translatedText": "นี่ไม่มีเวกเตอร์ลักษณะเฉพาะใดๆ เนื่องจากมันหมุนเวกเตอร์ทุกตัวออกจากสแปนของมันเอง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens. ",
  "translatedText": "หากคุณลองคำนวณค่าลักษณะเฉพาะของการหมุนแบบนี้ ให้สังเกตว่าเกิดอะไรขึ้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0. ",
  "translatedText": "เมทริกซ์มีคอลัมน์ 0, 1 และลบ 1, 0 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero. ",
  "translatedText": "ลบแลมบ์ดาออกจากองค์ประกอบในแนวทแยง และดูว่าเมื่อใดดีเทอร์มิแนนต์เป็นศูนย์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1. ",
  "translatedText": "ในกรณีนี้ คุณจะได้แลมดาพหุนามกำลังสองบวก 1 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i. ",
  "translatedText": "รากเดียวของพหุนามนั้นคือจำนวนจินตภาพ i และลบ i ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors. ",
  "translatedText": "ความจริงที่ว่าไม่มีคำตอบจำนวนจริงแสดงว่าไม่มีเวกเตอร์ลักษณะเฉพาะ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear. ",
  "translatedText": "อีกตัวอย่างที่น่าสนใจที่ควรค่าแก่การจดจำก็คือแรงเฉือน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1. ",
  "translatedText": "วิธีนี้จะแก้ไข i-hat ให้เข้าที่และย้าย j-hat 1 ไปด้านบน ดังนั้นเมทริกซ์จึงมีคอลัมน์ 1, 0 และ 1, 1 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place. ",
  "translatedText": "เวกเตอร์ทั้งหมดบนแกน x เป็นเวกเตอร์ลักษณะเฉพาะที่มีค่าลักษณะเฉพาะ 1 เนื่องจากพวกมันยังคงอยู่กับที่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors. ",
  "translatedText": "อันที่จริง สิ่งเหล่านี้เป็นเพียงเวกเตอร์ลักษณะเฉพาะเท่านั้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared. ",
  "translatedText": "เมื่อคุณลบแลมดาออกจากเส้นทแยงมุม แล้วคำนวณดีเทอร์มีแนนต์ สิ่งที่คุณจะได้คือ 1 ลบแลมดากำลังสอง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1. ",
  "translatedText": "และรากเดียวของพจน์นี้คือแลมดาเท่ากับ 1 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1. ",
  "translatedText": "สิ่งนี้สอดคล้องกับสิ่งที่เราเห็นในเชิงเรขาคณิต ว่าเวกเตอร์ลักษณะเฉพาะทั้งหมดมีค่าลักษณะเฉพาะ 1 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors. ",
  "translatedText": "โปรดจำไว้ว่า เป็นไปได้ที่จะมีค่าลักษณะเฉพาะเพียงค่าเดียว แต่มีมากกว่าแค่บรรทัดที่เต็มไปด้วยเวกเตอร์ลักษณะเฉพาะ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2. ",
  "translatedText": "ตัวอย่างง่ายๆ คือเมทริกซ์ที่ปรับขนาดทุกอย่างเป็น 2 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue. ",
  "translatedText": "ค่าลักษณะเฉพาะเพียงอย่างเดียวคือ 2 แต่เวกเตอร์ทุกตัวในระนาบจะเป็นเวกเตอร์ลักษณะเฉพาะที่มีค่าลักษณะเฉพาะนั้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic. ",
  "translatedText": "ตอนนี้เป็นอีกช่วงเวลาหนึ่งที่ดีที่จะหยุดและไตร่ตรองบางอย่างก่อนที่ฉันจะไปยังหัวข้อสุดท้าย ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video. ",
  "translatedText": "ฉันอยากจะจบที่นี่ด้วยแนวคิดเรื่องลักษณะเฉพาะ ซึ่งอาศัยแนวคิดจากวิดีโอที่แล้วเป็นอย่างมาก ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors. ",
  "translatedText": "ลองดูว่าจะเกิดอะไรขึ้นถ้าเวกเตอร์ฐานของเรา กลายเป็นเวกเตอร์ลักษณะเฉพาะ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1, and j-hat is scaled by 2. ",
  "translatedText": "ตัวอย่างเช่น บางที i-hat อาจถูกปรับขนาดเป็นลบ 1 และ j-hat อาจถูกปรับขนาดเป็น 2 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0. ",
  "translatedText": "เขียนพิกัดใหม่เป็นคอลัมน์ของเมทริกซ์ สังเกตว่าผลคูณสเกลาร์ ลบ 1 และ 2 ซึ่งเป็นค่าลักษณะเฉพาะของ i-hat และ j-hat อยู่บนเส้นทแยงมุมของเมทริกซ์ของเรา และค่าอื่นๆ ทุกค่าเป็น 0 . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has 0s everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix, and the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues. ",
  "translatedText": "เมื่อใดก็ตามที่เมทริกซ์มี 0 ทุกที่ที่ไม่ใช่เส้นทแยงมุม มันจะเรียกว่าเมทริกซ์แนวทแยง ซึ่งสมเหตุสมผลพอสมควร และวิธีตีความก็คือ เวกเตอร์พื้นฐานทั้งหมดเป็นเวกเตอร์ลักษณะเฉพาะ โดยค่าในแนวทแยงของเมทริกซ์นี้เป็นค่าลักษณะเฉพาะของพวกมัน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with. ",
  "translatedText": "มีหลายสิ่งที่ทำให้เมทริกซ์แนวทแยงทำงานได้ดีขึ้นมาก ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times. ",
  "translatedText": "เรื่องสำคัญประการหนึ่งคือ มันง่ายกว่าที่จะคำนวณสิ่งที่จะเกิดขึ้น หากคุณคูณเมทริกซ์นี้ด้วยตัวเองหลายๆ ครั้ง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue. ",
  "translatedText": "เนื่องจากเมทริกซ์ตัวใดตัวหนึ่งเหล่านี้ขยายขนาดเวกเตอร์พื้นฐานแต่ละตัวด้วยค่าลักษณะเฉพาะ การใช้เมทริกซ์นั้นหลายครั้ง เช่น 100 ครั้ง จะสอดคล้องกับการขยายเวกเตอร์ฐานแต่ละตัวด้วยกำลัง 100 ของค่าลักษณะเฉพาะที่สอดคล้องกัน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix. ",
  "translatedText": "ในทางตรงกันข้าม ให้ลองคำนวณกำลัง 100 ของเมทริกซ์ที่ไม่ใช่เส้นทแยงมุม ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment. ",
  "translatedText": "จริงครับ ลองสักนิด ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare. ",
  "translatedText": "มันเป็นฝันร้าย ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors. ",
  "translatedText": "แน่นอนว่า คุณจะไม่ค่อยโชคดีนักที่มีเวกเตอร์พื้นฐานเป็นเวกเตอร์ลักษณะเฉพาะด้วย ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors. ",
  "translatedText": "แต่หากการแปลงของคุณมีเวกเตอร์ลักษณะเฉพาะจำนวนมาก เหมือนกับอันตั้งแต่เริ่มวิดีโอนี้ เพียงพอให้คุณเลือกเซตที่ครอบคลุมพื้นที่ทั้งหมด คุณก็สามารถ เปลี่ยนระบบพิกัดของคุณให้เวกเตอร์ลักษณะเฉพาะเหล่านี้เป็นเวกเตอร์พื้นฐานได้ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system. ",
  "translatedText": "ผมพูดถึงการเปลี่ยนแปลงฐานในวิดีโอที่แล้ว แต่ผมจะพูดถึงวิธีแสดงการแปลงที่เขียนอยู่ในระบบพิกัดของเราไปเป็นระบบอื่น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix. ",
  "translatedText": "นำพิกัดของเวกเตอร์ที่คุณต้องการใช้เป็นฐานใหม่ ซึ่งในกรณีนี้หมายถึงเวกเตอร์ลักษณะเฉพาะสองตัวของเรา จากนั้นให้พิกัดเหล่านั้นเป็นคอลัมน์ของเมทริกซ์ หรือที่เรียกว่าการเปลี่ยนแปลงของเมทริกซ์พื้นฐาน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system. ",
  "translatedText": "เมื่อคุณประกบการแปลงดั้งเดิม โดยใส่การเปลี่ยนแปลงของเมทริกซ์พื้นฐานทางด้านขวาและค่าผกผันของการเปลี่ยนแปลงของเมทริกซ์พื้นฐานทางด้านซ้าย ผลลัพธ์จะเป็นเมทริกซ์ที่แสดงถึงการแปลงเดียวกันนั้น แต่จากมุมมองของพิกัดเวกเตอร์พื้นฐานใหม่ ระบบ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal. ",
  "translatedText": "จุดรวมของการทำสิ่งนี้กับเวกเตอร์ลักษณะเฉพาะคือเมทริกซ์ใหม่นี้รับประกันได้ว่าจะเป็นเส้นทแยงมุมโดยมีค่าลักษณะเฉพาะที่สอดคล้องกันในแนวทแยงนั้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation. ",
  "translatedText": "เนื่องจากมันแสดงถึงการทำงานในระบบพิกัด โดยที่สิ่งที่เกิดขึ้นกับเวกเตอร์พื้นฐานก็คือพวกมันจะถูกปรับขนาดระหว่างการแปลง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis. ",
  "translatedText": "ชุดของเวกเตอร์พื้นฐานซึ่งเป็นเวกเตอร์ลักษณะเฉพาะก็ถูกเรียกว่า eigenbasis อีกครั้งอย่างสมเหตุสมผล ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system. ",
  "translatedText": "ตัวอย่างเช่น หากคุณต้องการคำนวณกำลัง 100 ของเมทริกซ์นี้ มันจะง่ายกว่ามากที่จะเปลี่ยนเป็นค่าลักษณะเฉพาะ คำนวณกำลัง 100 ในระบบนั้น แล้วแปลงกลับเป็นระบบมาตรฐานของเรา ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations. ",
  "translatedText": "คุณไม่สามารถทำเช่นนี้กับการแปลงทั้งหมดได้ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space. ",
  "translatedText": "ตัวอย่างเช่น แรงเฉือนไม่มีเวกเตอร์ลักษณะเฉพาะเพียงพอที่จะขยายพื้นที่ทั้งหมด ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely. ",
  "translatedText": "แต่หากคุณสามารถหาค่าลักษณะเฉพาะได้ มันจะทำให้การดำเนินการของเมทริกซ์น่ารักมาก ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen. ",
  "translatedText": "สำหรับผู้ที่เต็มใจที่จะไขปริศนาที่เรียบร้อยเพื่อดูว่ามันดูเหมือนจริงอย่างไร และจะใช้มันเพื่อสร้างผลลัพธ์ที่น่าประหลาดใจได้อย่างไร ฉันจะฝากข้อความไว้บนหน้าจอ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it. ",
  "translatedText": "ต้องใช้เวลาสักหน่อย แต่ฉันคิดว่าคุณจะสนุกไปกับมัน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces. ",
  "translatedText": "วิดีโอถัดไปและเป็นวิดีโอสุดท้ายของชุดนี้ จะเป็นเกี่ยวกับปริภูมิเวกเตอร์เชิงนามธรรม ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1005.38
 },
 {
  "input": "See you then! ",
  "translatedText": "งั้นไว้เจอกันใหม่! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.9,
  "end": 1006.12
 }
]