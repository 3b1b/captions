1
00:00:19,870 --> 00:00:25,660
"Eigenvektoren und Eigenwerte" ist eines der Themen , das viele Studenten besonders unintuitiv finden.

2
00:00:26,100 --> 00:00:29,630
Fragen wie "warum machen wir das" und "was heißt das überhaupt"

3
00:00:29,630 --> 00:00:33,440
gehen oft in einem Meer aus Berechnungen unter.

4
00:00:33,950 --> 00:00:35,710
Und als ich die Videos dieser Serie herausbrachte,

5
00:00:35,810 --> 00:00:40,300
haben viele von Euch kommentiert, sie würden sich auf die Visualisierung dieses Themas im Speziellen freuen.

6
00:00:40,710 --> 00:00:41,370
Ich nehme an, dass

7
00:00:41,370 --> 00:00:46,560
der Grund dafür nicht darin liegt, dass "Eigen"-Dinge besonders kompliziert wären oder schlecht erklärt würden.

8
00:00:46,890 --> 00:00:48,970
Tatsächlich sind sie vergleichbar einfach

9
00:00:48,970 --> 00:00:51,170
und ich denke, dass die meisten Bücher es gut erklären.

10
00:00:51,720 --> 00:00:52,650
Das Problem ist, dass

11
00:00:52,650 --> 00:00:58,400
es nur wirklich Sinn ergibt, wenn man ein solides visuelles Verständnis für vorhergehende Themen hat.

12
00:00:59,050 --> 00:01:04,010
Am wichtigsten ist hier, dass man Matrizen als lineare Transformationen ansieht,

13
00:01:04,360 --> 00:01:06,110
aber man sollte auch mit Dingen

14
00:01:06,110 --> 00:01:10,160
wie Determinanten, linearen Gleichungssystemen und Basiswechsel vertraut sein.

15
00:01:10,690 --> 00:01:16,410
Verwirrung über "Eigen"-Dinge hat normalerweise mehr mit einem unsicheren Grundwissen in einem jener Themen zu tun,

16
00:01:16,410 --> 00:01:19,470
als mit Eigenvektoren und Eigenwerten selbst.

17
00:01:20,370 --> 00:01:23,790
Zu Anfang sei eine lineare Transformation in zwei Dimensionen,

18
00:01:23,790 --> 00:01:24,930
wie die hier gezeigte.

19
00:01:25,390 --> 00:01:31,230
Sie bewegt den Basisvektor î zu den Koordinaten (3 | 0) und ĵ zu (1 | 2),

20
00:01:31,770 --> 00:01:35,760
und wird daher durch eine Matrix mit den Spalten (3 | 0) und (1 | 2) repräsentiert.

21
00:01:36,620 --> 00:01:39,380
Konzentrier Dich darauf, was sie mit genau diesem Vektor macht

22
00:01:39,680 --> 00:01:44,340
und denk über den Spann dieses Vektors nach, die Gerade die durch die Spitze des Vektors und den Ursprung verläuft.

23
00:01:45,010 --> 00:01:48,670
Während der Transformation werden die meisten Vektoren aus ihrem Spann herausbewegt.

24
00:01:48,970 --> 00:01:51,010
Es schiene doch sehr zufällig,

25
00:01:51,010 --> 00:01:55,390
wenn die Stelle, an der der Vektor landet, sich auch auf dieser Geraden befindet.

26
00:01:57,410 --> 00:02:00,530
Aber ein paar spezielle Vektoren verbleiben auf ihrem Spann,

27
00:02:00,920 --> 00:02:07,090
bedeutet, dass die Matrix so einen Vektor nur streckt oder staucht, wie ein Skalar.

28
00:02:09,630 --> 00:02:14,280
Für dieses spezifische Beispiel, der Basisvektor î ist so ein spezieller Vektor.

29
00:02:15,060 --> 00:02:17,570
Der Spann von î ist die x-Achse,

30
00:02:17,780 --> 00:02:19,630
und der ersten Spalte der Matrix können wir entnehmen,

31
00:02:19,630 --> 00:02:24,290
dass î sich um das Dreifache auf der x-Achse verlängert.

32
00:02:26,370 --> 00:02:29,610
Zudem wird aufgrund der Weise, wie Transformationen funktionieren,

33
00:02:29,880 --> 00:02:34,540
jeder andere Vektor auf der x-Achse um das Dreifache gestreckt wird

34
00:02:34,540 --> 00:02:36,650
und daher auf seinem Spann bleibt.

35
00:02:38,600 --> 00:02:44,290
Ein etwas unauffälligerer Vektor, der auf seinem Spann bleibt ist (-1 | 1),

36
00:02:44,800 --> 00:02:47,280
wird um einen Faktor von zwei gestreckt.

37
00:02:49,000 --> 00:02:51,670
Und erneut impliziert die Linearität, dass

38
00:02:51,670 --> 00:02:55,770
jeder andere Vektor auf der diagonalen Gerade, die von dem Kerl aufgespannt wird,

39
00:02:55,770 --> 00:02:58,320
nur um einen Faktor von zwei gestreckt wird.

40
00:02:59,940 --> 00:03:01,140
Und für diese Transformation

41
00:03:01,370 --> 00:03:05,370
sind das alle Vektoren mit der speziellen Eigenschaft, dass sie auf ihrem Spann bleiben.

42
00:03:05,700 --> 00:03:08,500
Diejenigen, die auf der x-Achse um einen Faktor von drei gestreckt werden,

43
00:03:08,800 --> 00:03:12,170
und die auf dieser diagonalen Geraden, die auf das Doppelte gestreckt werden.

44
00:03:12,780 --> 00:03:16,050
Jeder andere Vektor wird während der Transformation irgendwie gedreht,

45
00:03:16,240 --> 00:03:18,250
weg von der Geraden die er aufspannt.

46
00:03:22,610 --> 00:03:23,780
Wie Ihr vielleicht schon erraten habt,

47
00:03:24,030 --> 00:03:28,360
diese speziellen Vektoren werden "Eigenvektoren" der Transformation genannt,

48
00:03:28,770 --> 00:03:33,150
und jeder Eigenvektor wird mit einem sogenannten "Eigenwert" assoziiert,

49
00:03:33,340 --> 00:03:37,630
der einfach dem Faktor entspricht, mit dem er während der Transformation gestreckt oder gestaucht wird.

50
00:03:40,540 --> 00:03:43,550
Natürlich ist das "Strecken" und "Stauchen" nichts besonderes

51
00:03:43,550 --> 00:03:46,190
oder, dass diese Eigenwerte positiv sind.

52
00:03:46,490 --> 00:03:50,950
In einem anderen Beispiel, könnte man einen Eigenvektor mit einem Eigenwert von -1/2 haben,

53
00:03:51,350 --> 00:03:55,170
was bedeute, dass der Vektor umgedreht und um den Faktor zwei gestaucht wird.

54
00:03:57,390 --> 00:04:02,660
Hauptsache er bleibt auf der Geraden, die er aufspannt, ohne weggedreht zu werden.

55
00:04:04,570 --> 00:04:07,500
Um einen Einblick zu bekommen, warum es sinnvoll wäre darüber nachzudenken,

56
00:04:07,760 --> 00:04:10,050
sollte man sich eine dreidimensionale Rotation vorstellen.

57
00:04:11,880 --> 00:04:15,050
Findet man einen Eigenvektor dieser Rotation,

58
00:04:15,270 --> 00:04:17,240
einen Vektor der auf seinem Spann bleibt,

59
00:04:17,640 --> 00:04:20,620
hat man die Rotationsachse gefunden.

60
00:04:22,890 --> 00:04:30,080
Und es ist viel einfacher sich eine 3D- Drehbewegung als Rotationsachse und -winkel vorzustellen,

61
00:04:30,080 --> 00:04:35,010
als als volle 3x3-Matrix, die diese Transformation beschreibt.

62
00:04:37,100 --> 00:04:40,880
In diesem Fall müsste der entsprechende Eigenwert 1 betragen,

63
00:04:41,020 --> 00:04:43,790
nachdem Rotationen nichts strecken oder stauchen

64
00:04:43,950 --> 00:04:46,010
und so die Länge des Vektors dieselbe bleibt.

65
00:04:48,120 --> 00:04:50,220
Dieses Muster taucht oft in linearer Algebra auf.

66
00:04:50,520 --> 00:04:55,500
Man kann verstehen was jede lineare Transformation, die durch eine Matrix beschrieben wird, tut,

67
00:04:55,500 --> 00:04:59,640
wenn man die Spalten jener Matrix als Landepunkte für die Eigenvektoren liest.

68
00:05:00,040 --> 00:05:05,050
Aber um zu verstehen, was eine lineare Transformation macht,

69
00:05:05,050 --> 00:05:07,710
unabhängiger vom eigenen Koordinatensystem,

70
00:05:08,060 --> 00:05:11,070
ist es oft besser, die Eigenvektoren und Eigenwerte zu finden.

71
00:05:15,790 --> 00:05:20,240
I werde hier nicht alle Details zur Berechnung von Eigenvektoren und Eigenwerten abdecken,

72
00:05:20,430 --> 00:05:23,440
aber ich werde versuchen einen Überblick über die rechnerischen Grundideen zu bieten,

73
00:05:23,630 --> 00:05:26,270
die am wichtigsten für das Konzeptverständnis sind.

74
00:05:27,140 --> 00:05:30,510
Symbolisch, hier ist, wie die Idee eines Eigenvektors aussieht.

75
00:05:30,990 --> 00:05:33,810
A sei eine Matrix, die eine gewisse Transformation darstellt,

76
00:05:34,010 --> 00:05:35,940
mit v als Eigenvektor,

77
00:05:36,290 --> 00:05:40,040
und Lambda als Entsprechenden Eigenwert.

78
00:05:40,680 --> 00:05:44,910
Was dieser Ausdruck besagt ist, dass das Matrix-Vektor-Produkt A ⋅ v

79
00:05:45,220 --> 00:05:50,140
das gleiche Ergebnis liefert, wie wenn man den Eigenvektor v um einen Wert Lambda skaliert.

80
00:05:51,430 --> 00:05:55,350
Also findet man im Endeffekt die Eigenvektoren und Eigenwerte der Matrix A,

81
00:05:55,620 --> 00:06:00,260
wenn man die Werte für v und Lambda findet, die diesen Ausdruck wahr machen.

82
00:06:02,320 --> 00:06:03,910
Es ist erstmals etwas unangenehm damit zu arbeiten,

83
00:06:03,910 --> 00:06:07,460
weil die linke Seite eine Matrix-Vektor-Multiplikation,

84
00:06:07,460 --> 00:06:10,820
aber die rechte Seine ein Skalarprodukt beinhaltet.

85
00:06:11,120 --> 00:06:16,160
Also beginnen wir damit, dass wir die Rechte Seite als eine Art Matrix-Vektor-Multiplikation scheiben,

86
00:06:16,430 --> 00:06:20,840
indem wir eine Matrix verwenden, die den Effekt einer Skalierung um den Faktor Lambda hat.

87
00:06:21,690 --> 00:06:25,530
Die Spalten dieser Matrix repräsentieren, was mit den jeweiligen Basisvektoren passiert,

88
00:06:25,770 --> 00:06:28,940
und da jeder Basisvektor einfach mit Lambda multipliziert wird,

89
00:06:29,320 --> 00:06:34,430
hat diese Matrix in der Diagonalen der Wert Lambda und 0 überall sonst.

90
00:06:36,310 --> 00:06:41,530
Üblicherweise wird das Lambda herausgehoben und es wird als Lambda mal I geschrieben,

91
00:06:41,530 --> 00:06:45,100
mit I als Einheitsmatrix mit Einsern hinunter in der Diagonalen.

92
00:06:45,890 --> 00:06:48,610
Wenn beide Seiten, wie eine Vektor-Matrix-Multiplikation aussehen,

93
00:06:48,880 --> 00:06:52,090
kann man die rechte Seite subtrahieren und v ausklammern.

94
00:06:54,250 --> 00:06:58,330
Jetzt haben wir eine neue Matrix A minus Lambda mal der Einheitsmatrix,

95
00:06:58,760 --> 00:07:05,150
und jetzt suchen wir einen Vektor v, sodass die neue Matrix mal v den Nullvektor ergibt.

96
00:07:06,710 --> 00:07:10,220
Jetzt wird das auch immer wahr sein, wenn  v der Nullvektor ist,

97
00:07:10,220 --> 00:07:11,330
aber das ist uninteressant.

98
00:07:11,330 --> 00:07:13,920
Was wir wollen ist ein Eigenvektor v ≠ 0.

99
00:07:14,430 --> 00:07:16,320
Wenn Du Kapitel 5 und 6 gesehen hat,

100
00:07:16,320 --> 00:07:22,290
wirst du wissen, dass die einzige Möglichkeit, wie das Produkt einer Matrix mit einem Vektor v ≠ 0, null ergeben kann,

101
00:07:22,710 --> 00:07:28,250
ist, wenn die mit der Matrix assoziierte Transformation den Raum in eine niedrigere Dimension quetscht.

102
00:07:29,840 --> 00:07:34,450
Und diese Quetschung entspricht einer Null-Determinante für die Matrix.

103
00:07:35,540 --> 00:07:40,210
Konkret sei A eine Matrix mit den Spalten (2 | 1) und (2 | 3),

104
00:07:40,590 --> 00:07:45,720
und Lambda ein variabler Wert, der von jedem diagonalen Eintrag subtrahiert wird.

105
00:07:46,510 --> 00:07:50,460
Man stelle sich vor, dass man Lambda wie 
mit einem Drehregler verändert.

106
00:07:51,090 --> 00:07:53,080
Und wenn sich der Wert von Lambda ändert,

107
00:07:53,080 --> 00:07:57,540
verändert sich die Matrix selbst, und somit ihre Determinante.

108
00:07:58,190 --> 00:08:03,140
Das Ziel hier ist es einen Wert für Lambda zu finden, bei dem die Determinante der Matrix 0 ist,

109
00:08:03,350 --> 00:08:07,500
was bedeutet, dass die angepasste Transformation den Raum in eine niedrigere Dimension quetscht.

110
00:08:08,210 --> 00:08:11,370
In diesem Fall, dieser Sweetspot wird erreicht, wenn Lambda 1 ist.

111
00:08:12,190 --> 00:08:14,140
Wenn wir eine andere Matrix gewählt hätten,

112
00:08:14,140 --> 00:08:18,820
müsste der Eigenwert natürlich nicht unbedingt 1 entsprechen, der Sweetspot könnte durch einen anderen Wert von Lambda erreicht werden.

113
00:08:20,190 --> 00:08:23,150
Das ist irgendwie viel, aber lasst uns den Sinn davon entwirren.

114
00:08:23,490 --> 00:08:29,750
Wenn Lambda 1 entspricht, quetscht die Matrix A - I ⋅ λ den Raum in eine Gerade.

115
00:08:30,380 --> 00:08:33,250
Das bedeutet, es gibt einen Vektor v ≠ 0,

116
00:08:33,250 --> 00:08:38,730
sodass A minus Lambda mal der Einheitsmatrix mal v dem Nullvektor entspricht.

117
00:08:40,580 --> 00:08:46,980
Zur Erinnerung, der Grund dafür, dass uns das interessiert, ist, dass es bedeutet, dass A ⋅ v = λ ⋅ v,

118
00:08:49,490 --> 00:08:53,580
was heißt, dass v ein Eigenvektor von A ist,

119
00:08:53,790 --> 00:08:57,470
und während der Transformation auf seinem Spann bleibt.

120
00:08:58,480 --> 00:09:04,100
In diesem Beispiel ist der entsprechende Eigenwert 1, also wäre v nur ein fixer Punkt.

121
00:09:06,180 --> 00:09:09,610
Pausiert und spult zurück um diesen Argumentationsweg nachzuvollziehe.

122
00:09:13,760 --> 00:09:15,890
Das habe ich in der Einleitung gemeint,

123
00:09:16,210 --> 00:09:18,750
wenn man keinen Dunst von Determinanten hat,

124
00:09:18,750 --> 00:09:22,960
und was sie mit linearen Gleichungssystemen mit Lösungen ≠ 0 zu tun haben,

125
00:09:23,270 --> 00:09:26,400
scheint ein Ausdruck wie dieser aus der Luft gegriffen.

126
00:09:28,400 --> 00:09:31,320
Um das in Aktion zu sehen, kehren wir zum Beispiel vom Anfang zurück

127
00:09:31,680 --> 00:09:34,730
mit einer Matrix, deren Spalten (3 | 0) und (1 | 2) sind.

128
00:09:35,560 --> 00:09:38,610
Um herauszufinden ob Lambda ein Eigenwert ist,

129
00:09:38,910 --> 00:09:42,900
subtrahiert man es von der Diagonale der Matrix und berechnet die Determinante.

130
00:09:51,060 --> 00:09:56,920
Dadurch erhält man eine quadratisches Polynom in Lambda, (3-λ)(2-λ).

131
00:09:57,800 --> 00:10:02,440
Da Lambda nur ein Eigenwert sein kann, wenn die Determinante 0 ist,

132
00:10:02,780 --> 00:10:08,980
kann man schlussfolgern, dass die einzigen Eigenwerte λ = 2 und λ = 3 sind.

133
00:10:10,040 --> 00:10:15,380
Um die Eigenvektoren herauszufinden, die einen dieser Eigenwerte haben, sei λ = 2,

134
00:10:15,900 --> 00:10:18,440
setzt man den Wert von Lambda in die Matrix ein

135
00:10:19,070 --> 00:10:24,100
und dann löst man für welche Vektoren diese diagonal veränderte Matrix 0 zu einem Ergebnis von 0 führt.

136
00:10:25,180 --> 00:10:28,180
Würde man es so wie jedes andere Gleichungssystem berechnen,

137
00:10:28,470 --> 00:10:34,550
sähe man, dass die Lösungen alle Vektoren sind, die auf der Diagonalen liegen, die von (-1 | 1) aufgespannt wird.

138
00:10:35,390 --> 00:10:39,890
Das ist darauf zurückzuführen, dass die unveränderte Matrix [(3 | 0), (1 | 2)]

139
00:10:39,890 --> 00:10:43,610
die Eigenschaft hat, jene Vektoren um den Faktor 2 zu strecken.

140
00:10:46,600 --> 00:10:50,400
Nun, eine 2D-Transformation muss keine Eigenvektoren haben.

141
00:10:50,860 --> 00:10:53,580
Man nehme beispielsweise eine 90°-Rotation.

142
00:10:53,890 --> 00:10:58,330
Sie hat keine Eigenvektoren, da sie jeden Vektor von seinem Spann wegdreht.

143
00:11:01,120 --> 00:11:05,700
Sieh, was passiert, wenn man versucht die Eigenwerte von so einer Rotation zu ermitteln.

144
00:11:06,290 --> 00:11:10,320
Die Matrix dieser Rotation hat die Spalten (0 | 1) und (-1 | 0),

145
00:11:11,040 --> 00:11:15,970
man subtrahiere Lambda von den diagonalen Elementen und, suche den Wert, wo die Determinante 0 ist.

146
00:11:18,370 --> 00:11:22,130
In diesem Fall erhält man das Polynom λ^2+1,

147
00:11:22,900 --> 00:11:28,060
und die einzigen Nullstellen von diesem Polynom sind die imaginären Zahlen i und -i.

148
00:11:28,970 --> 00:11:33,900
Die Tatsache, dass es keine reelle Lösung gibt, weist darauf hin, dass es keine Eigenvektoren gibt.

149
00:11:35,880 --> 00:11:40,020
Ein anderes sehr interessantes Beispiel, das es wert ist, im Gedächtnis zu behalten, ist ein "Shear".

150
00:11:40,520 --> 00:11:44,500
Das hält î am Platz und bewegt und ĵ um eins nach rechts,

151
00:11:44,800 --> 00:11:48,040
also hat seine Matrix die Spalten (1 | 0) und (1 | 1).

152
00:11:48,910 --> 00:11:54,730
Alle Vektoren auf der x-Achse sind Eigenvektoren mit Eigenwert 1, da sie am Platz bleiben.

153
00:11:55,630 --> 00:11:58,010
Tatsächlich sind sie die einzigen Eigenvektoren.

154
00:11:58,780 --> 00:12:02,770
Wenn man Lambda von den Diagonalen subtrahiert und die Determinante berechnet,

155
00:12:03,450 --> 00:12:06,720
erhält man (1-λ)²,

156
00:12:09,540 --> 00:12:13,070
und die einzige Nullstelle dieses Ausdrucks ist 1.

157
00:12:14,980 --> 00:12:19,890
Das stimmt mit dem überein, was man geometrisch sieht, nämlich, dass alle Eigenvektoren Eigenwerte von 1 haben.

158
00:12:21,070 --> 00:12:21,930
Man muss aber beachten,

159
00:12:21,930 --> 00:12:28,200
dass es auch möglich ist nur einen Eigenwert zu haben, aber mehr als eine Gerade an Eigenvektoren.

160
00:12:29,980 --> 00:12:33,390
Ein simples Beispiel wäre eine Matrix, die alles um den Faktor 2 skaliert,

161
00:12:33,970 --> 00:12:40,960
der einzige Eigenwert ist 2, aber jeder Vektor in der Ebene ist ein Eigenvektor mit diesem Eigenwert.

162
00:12:42,230 --> 00:12:44,840
Jetzt ist wieder ein guter Zeitpunkt zum Pausieren und Zurückspulen,

163
00:12:44,840 --> 00:12:46,570
bevor ich mit dem letzen Thema fortfahre.

164
00:13:03,900 --> 00:13:07,230
Ich möchte hier mit einer Idee von einer Eigenbasis abschließen,

165
00:13:07,420 --> 00:13:10,040
die sehr auf den Ideen des letzten Videos beruht.

166
00:13:11,580 --> 00:13:16,600
Seht, was passiert, wenn gerade die Basisvektoren Eigenvektoren sind.

167
00:13:17,180 --> 00:13:22,480
Zum Beispiel werde  î um -1 skaliert und ĵ um 2.

168
00:13:23,490 --> 00:13:26,200
Schreibt man ihre neuen Koordinaten als die Spalten einer Matrix,

169
00:13:26,560 --> 00:13:32,480
sieht man, dass die Skalarfaktoren -1 und 2, die die Eigenwerte von î und ĵ sind,

170
00:13:32,890 --> 00:13:37,340
sich auf der diagonalen der Matrix befinden und jeder andere Eintrag 0 ist.

171
00:13:38,890 --> 00:13:42,820
Immer, wenn eine Matrix bis auf die Diagonale nur aus Nullen besteht,

172
00:13:42,820 --> 00:13:45,610
wird sie, aus gutem Grund, diagonale Matrix genannt.

173
00:13:45,900 --> 00:13:50,000
Und das ist so zu interpretieren, dass alle Basisvektoren Eigenvektoren,

174
00:13:50,210 --> 00:13:54,760
und die diagonalen Werte deren Eigenwerte sind.

175
00:13:57,270 --> 00:14:01,090
Es gibt viele Szenarien, in denen es viel einfacher ist mit diagonalen Matrizen zu arbeiten.

176
00:14:01,910 --> 00:14:03,040
Ein entscheidendes ist, dass

177
00:14:03,040 --> 00:14:08,520
es einfacher ist zu berechnen, was passiert, wenn man diese Matrix sehr oft mit sich selbst multipliziert.

178
00:14:09,400 --> 00:14:14,330
Das all diese Matrizen nur die einzelnen Eigenvektoren um einen bestimmen Eigenwert skalieren,

179
00:14:14,720 --> 00:14:17,960
ist es dasselbe, die Matrix sehr viele Male, sagen wir hundertmal, anzuwenden,

180
00:14:18,240 --> 00:14:24,830
wie jeden Basisvektor einfach nur um die hundertste Potenz des jeweiligen Eigenwerts zu skalieren.

181
00:14:25,800 --> 00:14:29,900
Zum Vergleich, versuch mal die hundertste Potenz einer nicht-diagonalen Matrix zu berechnen.

182
00:14:30,140 --> 00:14:32,650
Wirklich jetzt, versuch es mal! Es ist ein Albtraum.

183
00:14:36,470 --> 00:14:41,470
Natürlich hat man selten so Glück, dass die Basisvektoren auch Eigenvektoren sind,

184
00:14:42,120 --> 00:14:46,520
aber wenn die Transformation viele Eigenvektoren hat, wie die vom Beginn des Videos,

185
00:14:46,860 --> 00:14:50,620
genug, dass man eine Menge wählen kann, die den ganzen Raum aufspannt,

186
00:14:51,240 --> 00:14:56,830
dann kann man das Koordinatensystem so verändern, sodass diese Eigenvektoren die Basisvektoren sind.

187
00:14:57,560 --> 00:14:59,560
Ich habe bereits letztes Video über Basiswechsel gesprochen,

188
00:14:59,560 --> 00:15:01,870
aber ich werde hier noch einmal sehr schnell wiederholen,

189
00:15:01,870 --> 00:15:07,180
wie man eine Transformation in unserem Koordinatensystem mit einem anderem System ausdrückt.

190
00:15:08,460 --> 00:15:11,930
Man nehme die Koordinaten der Vektoren, die man als neue Basis verwenden möchte,

191
00:15:11,930 --> 00:15:14,270
was, in diesem Fall, bedeutet, dass es zwei Eigenvektoren sind,

192
00:15:14,610 --> 00:15:19,640
die deren Koordinaten zu den Spalten einer Matrix machen, die sogenannte Basiswechsel-Matrix.

193
00:15:20,280 --> 00:15:22,680
Wenn man die ursprüngliche Transformation in die Mitte,

194
00:15:22,850 --> 00:15:24,830
die Basiswechsel-Matrix auf die rechte Seite,

195
00:15:25,150 --> 00:15:28,500
und die umgekehrte Basiswechsel-Matrix auf die linke Seite stellt,

196
00:15:28,890 --> 00:15:32,730
ist das Ergebnis eine Matrix, die dieselbe Transformation darstellt,

197
00:15:32,940 --> 00:15:36,730
aber von der Perspektive eines Koordinatensystem mit der neuen Basis.

198
00:15:37,620 --> 00:15:40,380
Die Intention, das zu tun, ist, dass

199
00:15:40,380 --> 00:15:46,880
diese neue Matrix garantiert diagonal ist mit den entsprechenden Eigenwerten in dieser Diagonale.

200
00:15:47,250 --> 00:15:50,270
Das funktioniert, da sie die Arbeit in einem Koordinatensystem repräsentiert,

201
00:15:50,270 --> 00:15:54,590
wo die Eigenvektoren durch diese Transformation nur skaliert werden.

202
00:15:55,770 --> 00:15:58,590
Eine Menge von Basisvektoren, die auch Eigenvektoren sind

203
00:15:58,590 --> 00:16:01,800
wird, wieder aus gutem Grund, "Eigenbasis" genannt.

204
00:16:02,330 --> 00:16:07,020
Wenn man also die hundertste Potenz dieser Matrix berechnen muss,

205
00:16:07,370 --> 00:16:10,550
ist es viel einfacher zu einer Eigenbasis zu wechseln,

206
00:16:10,870 --> 00:16:13,310
die hundertste Potenz in diesem System zu berechnen

207
00:16:13,630 --> 00:16:15,870
und dann zurück zu unserem Standardsystem zu konvertieren.

208
00:16:16,720 --> 00:16:18,600
Man kann das allerdings nicht mit allen Transformationen machen.

209
00:16:18,780 --> 00:16:23,120
Ein "Shear" beispielsweise hat nicht genug Eigenvektoren um den ganzen Raum aufzuspannen.

210
00:16:23,740 --> 00:16:28,380
Aber wenn man eine Eigenbasis findet, macht es Matrixoperationen wirklich schön.

211
00:16:29,160 --> 00:16:32,980
Für diejenigen, die sich durch ein ordentliches Rätsel arbeiten wollen, um zu sehen, wie das in Aktion aussieht

212
00:16:32,980 --> 00:16:37,570
und wie man es benutzen kann um überraschende Ergebnisse zu erzielen, werde ich eine Angabe auf dem Bildschirm einblenden.

213
00:16:37,890 --> 00:16:40,260
Es ist ein gutes Stück Arbeit, aber ich denke Ihr werdet es mögen.

214
00:16:40,910 --> 00:16:45,610
Das nächste und finale Video dieser Serie wird über abstrakte Vektorräume sein.

215
00:16:45,900 --> 00:16:46,520
Wir sehen und dann!

