[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "וקטורים עצמיים וערכים עצמיים הם אחד מאותם נושאים שהרבה תלמידים מוצאים שהם לא אינטואיטיביים במיוחד.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Things like, why are we doing this, and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "דברים כמו, למה אנחנו עושים את זה, ומה זה בעצם אומר, נותרים לעתים קרובות מדי פשוט מרחפים בים ללא מענה של חישובים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "וכשהוצאתי את הסרטונים של הסדרה הזו, הרבה מכם הגיבו על כך שאתם מצפים לדמיין את הנושא הזה במיוחד.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "אני חושד שהסיבה לכך היא לא כל כך שדברים עצמיים מסובכים במיוחד או מוסברים בצורה גרועה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "למעשה, זה פשוט יחסית, ואני חושב שרוב הספרים מצליחים להסביר את זה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "What I want to do is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "מה שאני רוצה לעשות זה שזה באמת הגיוני רק אם יש לך הבנה ויזואלית מוצקה עבור רבים מהנושאים הקודמים לזה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "הכי חשוב כאן הוא שתדע לחשוב על מטריצות כעל טרנספורמציות ליניאריות, אבל אתה גם צריך להיות נוח עם דברים כמו דטרמיננטים, מערכות משוואות ליניאריות ושינוי בסיס.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "הבלבול לגבי חומרים עצמיים קשור בדרך כלל יותר לבסיס רעוע באחד מהנושאים הללו מאשר עם וקטורים עצמיים וערכים עצמיים עצמם.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "כדי להתחיל, שקול קצת טרנספורמציה ליניארית בשני ממדים, כמו זו שמוצגת כאן.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "הוא מעביר את וקטור הבסיס i-hat לקואורדינטות 3, 0, ו-j-hat ל-1, 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "אז הוא מיוצג באמצעות מטריצה שהעמודות שלה הן 3, 0 ו-1, 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "התמקדו במה שהוא עושה לווקטור מסוים אחד, וחשבו על תוחלת הווקטור, הקו העובר דרך המקור שלו והקצה שלו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "רוב הווקטורים עומדים להיפטר מהטווח שלהם במהלך הטרנספורמציה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "כלומר, זה ייראה די מקרי אם המקום בו הווקטור נחת במקרה גם היה איפשהו על הקו הזה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "אבל כמה וקטורים מיוחדים נשארים בטווח שלהם, כלומר ההשפעה שיש למטריצה על וקטור כזה היא רק למתוח אותו או למעוך אותו, כמו סקלר.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "עבור הדוגמה הספציפית הזו, וקטור הבסיס i-hat הוא וקטור מיוחד כזה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "טווח ה-i-hat הוא ציר ה-x, ומהעמודה הראשונה של המטריצה, אנו יכולים לראות שה-i-hat עובר עד פי 3 מעצמו, עדיין על ציר ה-X הזה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "יתרה מכך, בגלל האופן שבו פועלות טרנספורמציות ליניאריות, כל וקטור אחר על ציר ה-x נמתח גם הוא רק בפקטור 3, ומכאן נשאר בטווח שלו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "וקטור מעט יותר ערמומי שנשאר בטווח שלו במהלך השינוי הזה הוא שלילי 1, 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "בסופו של דבר זה נמתח בפקטור של 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "ושוב, לינאריות הולכת לרמוז שכל וקטור אחר על הקו האלכסוני שמשתרע על ידי הבחור הזה פשוט הולך להימתח בפקטור של 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "ולטרנספורמציה הזו, אלה כל הווקטורים עם התכונה המיוחדת הזו של להישאר בטווח שלהם.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "אלה על ציר ה-x נמתחים בפקטור 3, ואלו בקו האלכסוני הזה נמתחים בפקטור של 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "כל וקטור אחר הולך להסתובב במקצת במהלך הטרנספורמציה, מודח מהקו שהוא משתרע.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "כפי שאפשר לנחש עד עכשיו, הוקטורים המיוחדים הללו נקראים הווקטורים העצמיים של הטרנספורמציה, וכל וקטור עצמי שייך אליו מה שנקרא ערך עצמי, שהוא רק הגורם שבאמצעותו הוא נמתח או נמעך במהלך הטרנספורמציה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing or the fact that these eigenvalues happen to be positive.",
  "translatedText": "כמובן, אין שום דבר מיוחד במתיחה מול מעיכה או בעובדה שהערכים העצמיים האלה הם חיוביים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "בדוגמה אחרת, יכול להיות לך וקטור עצמי עם ערך עצמי שלילי 1 חצי, כלומר הווקטור מתהפך ונמעך בפקטור של 1 חצי.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "אבל החלק החשוב כאן הוא שהוא נשאר על הקו שהוא משתרע החוצה מבלי להסתובב ממנו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "לקבלת הצצה למה זה עשוי להיות דבר שימושי לחשוב עליו, שקול סיבוב תלת מימדי.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "אם אתה יכול למצוא וקטור עצמי לאותו סיבוב, וקטור שנשאר בטווח שלו, מה שמצאת הוא ציר הסיבוב.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3 by 3 matrix associated with that transformation.",
  "translatedText": "והרבה יותר קל לחשוב על סיבוב תלת מימדי במונחים של ציר סיבוב כלשהו וזווית שבה הוא מסתובב, במקום לחשוב על המטריצה המלאה של 3 על 3 הקשורה לטרנספורמציה הזו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "במקרה זה, אגב, הערך העצמי המתאים יהיה חייב להיות 1, מכיוון שסיבובים לעולם אינם מותחים או מעוכים דבר, כך שאורך הווקטור יישאר זהה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "דפוס זה מופיע הרבה באלגברה לינארית.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "עם כל טרנספורמציה ליניארית המתוארת על ידי מטריצה, תוכל להבין מה היא עושה על ידי קריאת העמודות של המטריצה הזו כנקודות הנחיתה של וקטורי הבסיס.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "אבל לעתים קרובות, דרך טובה יותר להגיע ללב מה שהטרנספורמציה הליניארית עושה בפועל, פחות תלויה במערכת הקואורדינטות הספציפית שלך, היא למצוא את הווקטורים העצמיים והערכים העצמיים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "לא אכסה כאן את הפרטים המלאים על שיטות לחישוב וקטורים עצמיים וערכים עצמיים, אבל אנסה לתת סקירה כללית של הרעיונות החישוביים החשובים ביותר להבנה מושגית.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "באופן סמלי, הנה איך נראה הרעיון של וקטור עצמי.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A היא המטריצה המייצגת טרנספורמציה כלשהי, כאשר v כווקטור העצמי, ולמבדה היא מספר, כלומר הערך העצמי המתאים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "מה שהביטוי הזה אומר הוא שמכפלת וקטור המטריצה, A כפול v, נותן את אותה תוצאה כמו קנה מידה של הווקטור העצמי v לפי ערך למבדה כלשהו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "אז מציאת הווקטורים העצמיים והערכים העצמיים שלהם של מטריצה A מסתכמת במציאת הערכים של v ולמבדה שהופכים את הביטוי הזה לנכון.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "זה קצת מביך לעבוד איתו בהתחלה כי הצד השמאלי הזה מייצג כפל וקטור מטריקס, אבל הצד הימני כאן הוא כפל וקטור סקלרי.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "אז בואו נתחיל בשכתוב את הצד הימני הזה כסוג של כפל מטריצה-וקטור, באמצעות מטריצה שיש לה השפעה של קנה מידה של כל וקטור לפי גורם של למבדה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "העמודות של מטריצה כזו ייצגו את מה שקורה לכל וקטור בסיס, וכל וקטור בסיס פשוט מוכפל בלמבדה, אז למטריצה הזו תהיה המספר למבדה במורד האלכסון, עם אפסים בכל מקום אחר.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with ones down the diagonal.",
  "translatedText": "הדרך הנפוצה לכתוב את הבחור הזה היא לחלק את הלמבדה הזו ולכתוב אותה כ-למבדה כפול i, כאשר i היא מטריצת הזהות עם אלה למטה באלכסון.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "כששני הצדדים נראים כמו כפל מטריצה-וקטור, נוכל להחסיר את הצד הימני הזה ולפרק את ה-v.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix, times v, gives the zero vector.",
  "translatedText": "אז מה שיש לנו עכשיו הוא מטריצה חדשה, A מינוס למבדה כפול הזהות, ואנחנו מחפשים וקטור v כך שהמטריקס החדש הזה, כפול v, נותן את הווקטור האפס.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "עכשיו, זה תמיד יהיה נכון אם v עצמו הוא וקטור האפס, אבל זה משעמם.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "מה שאנחנו רוצים זה וקטור עצמי שאינו אפס.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "ואם תצפו בפרקים 5 ו-6, תדעו שהדרך היחידה שבה ניתן למכפלה של מטריצה עם וקטור לא-אפס להפוך לאפס היא אם הטרנספורמציה הקשורה למטריצה הזו מוחצת את החלל לממד נמוך יותר.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "וההתכווצות הזו תואמת אפס דטרמיננטה עבור המטריצה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "כדי להיות קונקרטיים, נניח למטריצה A שלך יש עמודות 2, 1 ו-2, 3, ונחשוב על הפחתת כמות משתנה, למבדה, מכל כניסה אלכסונית.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "כעת דמיינו לעצמכם התאמה של למבדה, מסובבת כפתור כדי לשנות את ערכה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "ככל שהערך הזה של למבדה משתנה, המטריצה עצמה משתנה, וכך הקובע של המטריצה משתנה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "המטרה כאן היא למצוא ערך של למבדה שיהפוך את הקובע הזה לאפס, כלומר הטרנספורמציה המצוינת מוחצת את החלל למימד נמוך יותר.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "במקרה זה, הנקודה המתוקה מגיעה כאשר הלמבדה שווה ל-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "כמובן, אם היינו בוחרים במטריצה אחרת, ייתכן שהערך העצמי לא בהכרח יהיה 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "הנקודה המתוקה עשויה להיפגע בערך אחר של הלמבדה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "אז זה די הרבה, אבל בואו נפרם מה זה אומר.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "כאשר למבדה שווה ל-1, המטריצה A מינוס למבדה כפול הזהות מוחצת את הרווח על קו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "זה אומר שיש וקטור v שאינו אפס, כך ש-A מינוס למבדה כפול הזהות כפול v שווה לוקטור האפס.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "ותזכרו, הסיבה שאכפת לנו מכך היא כי זה אומר ש-A כפול v שווה למבדה כפול v, מה שאפשר לקרוא כאומר שהווקטור v הוא וקטור עצמי של A, שנשאר בטווח שלו במהלך הטרנספורמציה A.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "בדוגמה זו, הערך העצמי המתאים הוא 1, כך ש-v למעשה פשוט יישאר קבוע במקום.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "עצור ותחשוב אם אתה צריך לוודא שקו ההיגיון הזה מרגיש טוב.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "זה מסוג הדברים שהזכרתי בהקדמה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "אם לא הייתה לך הבנה מוצקה של דטרמיננטים ומדוע הם קשורים למערכות ליניאריות של משוואות שיש להן פתרונות שאינם אפס, ביטוי כזה היה מרגיש לגמרי מחוץ לכחול.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "כדי לראות את זה בפעולה, בואו נחזור על הדוגמה מההתחלה, עם מטריצה שהעמודות שלה הן 3, 0 ו-1, 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "כדי לגלות אם ערך למבדה הוא ערך עצמי, החסר אותו מהאלכסונים של המטריצה הזו וחשב את הקובע.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "אם עושים זאת, נקבל פולינום ריבועי מסוים בלמבדה, 3 מינוס למבדה כפול 2 מינוס למבדה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "מכיוון שלמבדה יכולה להיות ערך עצמי רק אם הקובע הזה במקרה אפס, אתה יכול להסיק שהערכים העצמיים היחידים האפשריים הם למבדה שווה ל-2 ולמבדה שווה ל-3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "כדי להבין מהם הווקטורים העצמיים שלמעשה יש להם אחד מהערכים העצמיים האלה, נניח שלמבדה שווה ל-2, חבר את הערך הזה של למבדה למטריצה ואז פתור עבור אילו וקטורים המטריצה שהשתנה באלכסון שולחת לאפס.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "אם חישבת זאת כמו כל מערכת לינארית אחרת, היית רואה שהפתרונות הם כל הוקטורים על הקו האלכסוני המתפרשים על ידי 1, 1 שלילי.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "זה מתאים לעובדה שלמטריקס ללא שינוי, 3, 0, 1, 2, יש את ההשפעה של מתיחת כל אותם וקטורים בפקטור 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "כעת, טרנספורמציה דו-ממדית לא חייבת להיות בעלת וקטורים עצמיים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "לדוגמה, שקול סיבוב ב-90 מעלות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "אין לזה וקטורים עצמיים מכיוון שהוא מסובב כל וקטור מחוץ לטווח שלו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "אם אתה באמת מנסה לחשב את הערכים העצמיים של סיבוב כזה, שימו לב מה קורה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "למטריצה שלו יש עמודות 0, 1 ושליליות 1, 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "הורידו את הלמבדה מהאלמנטים האלכסוניים וחפשו מתי הקובע הוא אפס.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "במקרה זה, תקבל את הפולינום למבדה בריבוע פלוס 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "השורשים היחידים של אותו פולינום הם המספרים המדומים, i ו-i שלילי.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "העובדה שאין פתרונות מספרים ממשיים מעידה על כך שאין וקטורים עצמיים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "עוד דוגמה די מעניינת שכדאי להחזיק בעורף היא גזירה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "זה מתקן את i-hat במקום ומזיז את j-hat 1 מעל, כך שהמטריקס שלו כולל עמודות 1, 0 ו-1, 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "כל הוקטורים על ציר ה-x הם וקטורים עצמיים עם ערך עצמי 1 מכיוון שהם נשארים קבועים במקומם.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "למעשה, אלו הם הווקטורים העצמיים היחידים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "כאשר אתה מוריד את למבדה מהאלכסונים ומחשב את הקובע, מה שאתה מקבל הוא 1 פחות למבדה בריבוע.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "והשורש היחיד של הביטוי הזה הוא למבדה שווה 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "זה תואם את מה שאנו רואים מבחינה גיאומטרית, שלכל הווקטורים העצמיים יש ערך עצמי 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "זכור עם זאת, אפשר גם לקבל רק ערך עצמי אחד, אבל עם יותר מסתם קו מלא בוקטורים עצמיים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "דוגמה פשוטה היא מטריצה שמשנה הכל לפי 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "הערך העצמי היחיד הוא 2, אבל כל וקטור במישור הופך להיות וקטור עצמי עם הערך העצמי הזה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "עכשיו זה עוד זמן טוב לעצור ולהרהר קצת מזה לפני שאעבור לנושא האחרון.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "אני רוצה לסיים כאן ברעיון של בסיס עצמי, שמסתמך במידה רבה על רעיונות מהסרטון האחרון.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "תסתכל מה קורה אם וקטורי הבסיס שלנו הם במקרה וקטורים עצמיים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1, and j-hat is scaled by 2.",
  "translatedText": "לדוגמה, אולי קנה המידה של i-hat ב-1 שלילי, ו-j-hat בקנה מידה של 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "כתיבת הקואורדינטות החדשות שלהם כעמודות של מטריצה, שימו לב שהכפולות הסקלריות האלה, שליליות 1 ו-2, שהן הערכים העצמיים של i-hat ו-j-hat, יושבות על האלכסון של המטריצה שלנו, וכל ערך אחר הוא 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has 0s everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix, and the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "בכל פעם שלמטריקס יש 0s בכל מקום מלבד האלכסון, היא נקראת, באופן סביר, מטריצה אלכסונית, והדרך לפרש זאת היא שכל וקטורי הבסיס הם וקטורים עצמיים, כאשר הערכים האלכסוניים של המטריצה הזו הם הערכים העצמיים שלהם.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "יש הרבה דברים שהופכים מטריצות אלכסוניות להרבה יותר נחמדות לעבודה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "אחד גדול הוא שקל יותר לחשב מה יקרה אם תכפיל את המטריצה הזו בעצמה חבורה שלמה של פעמים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "מכיוון שכל מה שאחת מהמטריצות האלה עושה הוא לשנות קנה מידה של כל וקטור בסיס לפי ערך עצמי כלשהו, יישום המטריצה הזו פעמים רבות, נניח פי 100, רק יתאים לקנה מידה של כל וקטור בסיס בחזקת המאה של הערך העצמי המתאים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "לעומת זאת, נסה לחשב את החזקה ה-100 של מטריצה לא אלכסונית.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "באמת, נסה את זה לרגע.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "זה סיוט.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "כמובן, רק לעתים רחוקות יהיה לך כל כך מזל שווקטורי הבסיס שלך יהיו גם וקטורים עצמיים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "אבל אם לטרנספורמציה שלך יש הרבה וקטורים עצמיים, כמו זה מתחילת הסרטון הזה, מספיק כדי שתוכל לבחור קבוצה המשתרעת על פני כל המרחב, אז אתה יכול לשנות את מערכת הקואורדינטות שלך כך שהווקטורים העצמיים האלה יהיו וקטורי הבסיס שלך.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "דיברתי על שינוי בסיס הסרטון האחרון, אבל אעבור כאן על תזכורת סופר מהירה כיצד לבטא טרנספורמציה שנכתבה כעת במערכת הקואורדינטות שלנו למערכת אחרת.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "קח את הקואורדינטות של הוקטורים שאתה רוצה להשתמש בהם כבסיס חדש, שבמקרה זה אומר שני הווקטורים העצמיים שלנו, ואז הפוך את הקואורדינטות האלה לעמודות של מטריצה, המכונה שינוי מטריצת הבסיס.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "כאשר אתה מכניס את השינוי של מטריצת הבסיס בצד ימין שלה ואת היפוך של שינוי מטריצת הבסיס בצד שמאל שלה, התוצאה תהיה מטריצה המייצגת את אותה טרנספורמציה, אבל מנקודת המבט של קואורדינטות וקטור הבסיס החדש מערכת.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "כל העניין לעשות את זה עם וקטורים עצמיים היא שמטריצה חדשה זו מובטחת להיות אלכסונית עם הערכים העצמיים המתאימים לה במורד האלכסון.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "הסיבה לכך היא שהיא מייצגת עבודה במערכת קואורדינטות שבה מה שקורה לוקטורי הבסיס הוא שהם מקבלים קנה מידה במהלך הטרנספורמציה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "קבוצה של וקטורי בסיס שהם גם וקטורים עצמיים נקראת, שוב, באופן סביר, בסיס עצמי.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "אז אם, למשל, אתה צריך לחשב את החזקה ה-100 של המטריצה הזו, יהיה הרבה יותר קל לשנות לבסיס עצמי, לחשב את החזקה ה-100 במערכת הזו, ואז להמיר בחזרה למערכת הסטנדרטית שלנו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "אתה לא יכול לעשות את זה עם כל השינויים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "לגזירה, למשל, אין מספיק וקטורים עצמיים כדי להשתרע על כל החלל.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "אבל אם אתה יכול למצוא בסיס עצמי, זה הופך את פעולות המטריצה למקסימות באמת.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "לאלו מכם שמוכנים לעבוד על פאזל די מסודר כדי לראות איך זה נראה בפעולה וכיצד ניתן להשתמש בו כדי להפיק כמה תוצאות מפתיעות, אשאיר כאן הנחיה על המסך.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "זה דורש קצת עבודה, אבל אני חושב שאתה תהנה מזה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "הסרטון הבא והאחרון בסדרה זו הולך להיות על חללים וקטוריים מופשטים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1005.38
 },
 {
  "input": "See you then!",
  "translatedText": "נתראה!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.9,
  "end": 1006.12
 }
]