1
00:00:19,920 --> 00:00:22,840
Vektor eigen dan nilai eigen adalah salah satu 

2
00:00:22,840 --> 00:00:25,760
topik yang menurut banyak siswa tidak intuitif.

3
00:00:25,760 --> 00:00:29,733
Hal-hal seperti, mengapa kita melakukan hal ini, dan apa maksud sebenarnya dari hal ini, 

4
00:00:29,733 --> 00:00:33,260
sering kali dibiarkan begitu saja dalam lautan perhitungan yang tidak terjawab.

5
00:00:33,920 --> 00:00:37,133
Dan saat saya merilis video seri ini, banyak dari Anda berkomentar 

6
00:00:37,133 --> 00:00:40,060
tentang menantikan memvisualisasikan topik ini secara khusus.

7
00:00:40,680 --> 00:00:43,519
Saya menduga alasannya bukan karena eigenthings 

8
00:00:43,519 --> 00:00:46,360
menjadi rumit atau tidak dijelaskan dengan baik.

9
00:00:46,860 --> 00:00:48,709
Faktanya, ini relatif mudah, dan menurut saya 

10
00:00:48,709 --> 00:00:50,840
sebagian besar buku mampu menjelaskannya dengan baik.

11
00:00:50,840 --> 00:00:54,772
Apa yang ingin saya lakukan adalah hal ini hanya masuk akal jika Anda 

12
00:00:54,772 --> 00:00:58,480
memiliki pemahaman visual yang kuat untuk banyak topik sebelumnya.

13
00:00:59,060 --> 00:01:02,776
Yang terpenting di sini adalah Anda mengetahui cara berpikir matriks 

14
00:01:02,776 --> 00:01:07,516
sebagai transformasi linier, namun Anda juga harus memahami hal-hal seperti determinan, 

15
00:01:07,516 --> 00:01:09,940
sistem persamaan linier, dan perubahan basis.

16
00:01:10,720 --> 00:01:14,855
Kebingungan mengenai eigenstuff biasanya lebih berkaitan dengan fondasi yang goyah 

17
00:01:14,855 --> 00:01:19,240
dalam salah satu topik ini dibandingkan dengan vektor eigen dan nilai eigen itu sendiri.

18
00:01:19,979 --> 00:01:23,382
Untuk memulai, pertimbangkan beberapa transformasi linier dalam dua dimensi, 

19
00:01:23,382 --> 00:01:24,840
seperti yang ditunjukkan di sini.

20
00:01:25,460 --> 00:01:31,040
Ini memindahkan vektor basis i-hat ke koordinat 3, 0, dan j-hat ke 1, 2.

21
00:01:31,780 --> 00:01:35,640
Jadi direpresentasikan dengan matriks yang kolomnya 3, 0, dan 1, 2.

22
00:01:36,600 --> 00:01:39,733
Fokuslah pada apa yang dilakukannya pada satu vektor tertentu, 

23
00:01:39,733 --> 00:01:44,160
dan pikirkan tentang rentang vektor tersebut, garis yang melalui titik asal dan ujungnya.

24
00:01:44,920 --> 00:01:48,380
Kebanyakan vektor akan kehilangan rentangnya selama transformasi.

25
00:01:48,780 --> 00:01:51,832
Maksud saya, akan terlihat sangat kebetulan jika tempat 

26
00:01:51,832 --> 00:01:55,320
pendaratan vektor juga berada di suatu tempat di garis tersebut.

27
00:01:57,400 --> 00:02:00,929
Namun beberapa vektor khusus tetap berada pada rentangnya sendiri, 

28
00:02:00,929 --> 00:02:04,142
yang berarti bahwa pengaruh matriks terhadap vektor tersebut 

29
00:02:04,142 --> 00:02:07,040
hanyalah merenggangkan atau menekannya, seperti skalar.

30
00:02:09,460 --> 00:02:14,100
Untuk contoh spesifik ini, vektor basis i-hat adalah salah satu vektor khusus tersebut.

31
00:02:14,640 --> 00:02:19,483
Rentang i-hat adalah sumbu x, dan dari kolom pertama matriks terlihat 

32
00:02:19,483 --> 00:02:24,120
bahwa i-hat berpindah sebanyak 3 kali, masih pada sumbu x tersebut.

33
00:02:26,320 --> 00:02:29,547
Terlebih lagi, karena cara kerja transformasi linier, 

34
00:02:29,547 --> 00:02:33,432
vektor lain pada sumbu x juga hanya diregangkan sebanyak 3 kali, 

35
00:02:33,432 --> 00:02:36,480
dan karenanya tetap berada pada rentangnya sendiri.

36
00:02:38,500 --> 00:02:41,082
Vektor yang sedikit lebih licik yang tetap berada pada 

37
00:02:41,082 --> 00:02:44,040
rentangnya sendiri selama transformasi ini adalah negatif 1, 1.

38
00:02:44,660 --> 00:02:47,140
Itu akhirnya diregangkan dengan faktor 2.

39
00:02:49,000 --> 00:02:53,444
Dan lagi, linearitas akan menyiratkan bahwa vektor lain pada garis 

40
00:02:53,444 --> 00:02:58,220
diagonal yang direntang oleh orang ini akan diregangkan sebanyak 2 kali.

41
00:02:59,820 --> 00:03:02,424
Dan untuk transformasi ini, itu semua adalah vektor 

42
00:03:02,424 --> 00:03:05,180
dengan sifat khusus untuk tetap berada pada rentangnya.

43
00:03:05,620 --> 00:03:09,089
Garis yang berada pada sumbu x diregangkan sebanyak 3 kali, 

44
00:03:09,089 --> 00:03:11,980
dan garis diagonalnya diregangkan sebanyak 2 kali.

45
00:03:12,760 --> 00:03:15,340
Vektor lainnya akan diputar selama transformasi, 

46
00:03:15,340 --> 00:03:18,080
sehingga menyimpang dari garis yang dibentangkannya.

47
00:03:22,520 --> 00:03:24,987
Seperti yang mungkin sudah Anda duga sekarang, 

48
00:03:24,987 --> 00:03:28,400
vektor-vektor khusus ini disebut vektor eigen dari transformasi, 

49
00:03:28,400 --> 00:03:32,654
dan setiap vektor eigen dikaitkan dengannya dengan apa yang disebut nilai eigen, 

50
00:03:32,654 --> 00:03:37,380
yang merupakan faktor yang merenggangkan atau menekan vektor tersebut selama transformasi.

51
00:03:40,280 --> 00:03:43,010
Tentu saja, tidak ada yang istimewa tentang peregangan 

52
00:03:43,010 --> 00:03:45,940
versus pemampatan atau fakta bahwa nilai eigen ini positif.

53
00:03:46,380 --> 00:03:50,695
Dalam contoh lain, Anda dapat memiliki vektor eigen dengan nilai eigen negatif 

54
00:03:50,695 --> 00:03:55,120
1 setengah, artinya vektor tersebut dibalik dan diperas dengan faktor 1 setengah.

55
00:03:56,980 --> 00:03:59,948
Namun bagian yang penting di sini adalah ia tetap berada 

56
00:03:59,948 --> 00:04:02,760
pada garis yang dibentangkannya tanpa terputar keluar.

57
00:04:04,460 --> 00:04:07,654
Untuk melihat sekilas mengapa hal ini berguna untuk dipikirkan, 

58
00:04:07,654 --> 00:04:09,800
pertimbangkan beberapa rotasi tiga dimensi.

59
00:04:11,660 --> 00:04:15,195
Jika Anda dapat menemukan vektor eigen untuk rotasi tersebut, 

60
00:04:15,195 --> 00:04:18,047
sebuah vektor yang tetap pada rentangnya sendiri, 

61
00:04:18,047 --> 00:04:20,500
maka yang Anda temukan adalah sumbu rotasi.

62
00:04:22,600 --> 00:04:26,504
Dan jauh lebih mudah untuk memikirkan rotasi 3D dalam kaitannya 

63
00:04:26,504 --> 00:04:29,554
dengan beberapa sumbu rotasi dan sudut rotasinya, 

64
00:04:29,554 --> 00:04:34,740
daripada memikirkan matriks penuh 3 kali 3 yang terkait dengan transformasi tersebut.

65
00:04:37,000 --> 00:04:39,915
Dalam hal ini, nilai eigen yang sesuai haruslah 1, 

66
00:04:39,915 --> 00:04:43,402
karena rotasi tidak pernah meregangkan atau menekan apa pun, 

67
00:04:43,402 --> 00:04:45,860
sehingga panjang vektornya akan tetap sama.

68
00:04:48,080 --> 00:04:50,020
Pola ini banyak muncul dalam aljabar linier.

69
00:04:50,440 --> 00:04:54,287
Dengan transformasi linier apa pun yang dideskripsikan oleh sebuah matriks, 

70
00:04:54,287 --> 00:04:58,741
Anda dapat memahami fungsinya dengan membaca kolom-kolom matriks ini sebagai titik awal 

71
00:04:58,741 --> 00:04:59,400
vektor basis.

72
00:05:00,020 --> 00:05:03,477
Namun seringkali, cara yang lebih baik untuk memahami inti dari apa yang 

73
00:05:03,477 --> 00:05:07,125
sebenarnya dilakukan transformasi linier, yang tidak terlalu bergantung pada 

74
00:05:07,125 --> 00:05:10,820
sistem koordinat tertentu, adalah dengan mencari vektor eigen dan nilai eigen.

75
00:05:15,460 --> 00:05:18,995
Saya tidak akan membahas detail lengkap tentang metode penghitungan vektor 

76
00:05:18,995 --> 00:05:22,531
eigen dan nilai eigen di sini, namun saya akan mencoba memberikan gambaran 

77
00:05:22,531 --> 00:05:26,020
umum tentang ide komputasi yang paling penting untuk pemahaman konseptual.

78
00:05:27,180 --> 00:05:30,480
Secara simbolis, seperti inilah gagasan tentang vektor eigen.

79
00:05:31,040 --> 00:05:35,960
A adalah matriks yang mewakili suatu transformasi, dengan v sebagai vektor eigen, 

80
00:05:35,960 --> 00:05:39,740
dan lambda adalah bilangan, yaitu nilai eigen yang bersesuaian.

81
00:05:40,680 --> 00:05:44,669
Maksud dari ungkapan ini adalah bahwa perkalian vektor-matriks, A kali v, 

82
00:05:44,669 --> 00:05:49,199
memberikan hasil yang sama seperti hanya menskalakan vektor eigen v dengan beberapa 

83
00:05:49,199 --> 00:05:49,900
nilai lambda.

84
00:05:51,000 --> 00:05:55,299
Jadi mencari vektor eigen dan nilai eigennya dari matriks A 

85
00:05:55,299 --> 00:06:00,100
berarti mencari nilai v dan lambda yang membuat ungkapan ini benar.

86
00:06:01,920 --> 00:06:06,008
Agak canggung untuk mengerjakannya pada awalnya karena ruas kiri mewakili 

87
00:06:06,008 --> 00:06:10,540
perkalian matriks-vektor, namun ruas kanan di sini adalah perkalian skalar-vektor.

88
00:06:11,120 --> 00:06:14,203
Jadi mari kita mulai dengan menulis ulang ruas kanan tersebut 

89
00:06:14,203 --> 00:06:17,287
sebagai semacam perkalian matriks-vektor, menggunakan matriks 

90
00:06:17,287 --> 00:06:20,620
yang memiliki efek menskalakan vektor apa pun dengan faktor lambda.

91
00:06:21,680 --> 00:06:25,706
Kolom-kolom matriks tersebut akan mewakili apa yang terjadi pada masing-masing 

92
00:06:25,706 --> 00:06:29,223
vektor basis, dan setiap vektor basis hanya dikalikan dengan lambda, 

93
00:06:29,223 --> 00:06:32,994
sehingga matriks ini akan mempunyai bilangan lambda di bawah diagonalnya, 

94
00:06:32,994 --> 00:06:34,320
dengan nol di tempat lain.

95
00:06:36,180 --> 00:06:38,966
Cara umum untuk menulis orang ini adalah dengan memfaktorkan 

96
00:06:38,966 --> 00:06:41,799
lambda tersebut dan menuliskannya sebagai lambda dikalikan i, 

97
00:06:41,799 --> 00:06:44,860
dengan i adalah matriks identitas dengan matriks di bawah diagonal.

98
00:06:45,860 --> 00:06:48,766
Karena kedua ruasnya tampak seperti perkalian matriks-vektor, 

99
00:06:48,766 --> 00:06:51,860
kita dapat mengurangkan ruas kanan tersebut dan memfaktorkan vnya.

100
00:06:54,160 --> 00:06:59,570
Jadi yang kita punya sekarang adalah matriks baru, A dikurangi lambda dikali identitas, 

101
00:06:59,570 --> 00:07:04,920
dan kita mencari vektor v sehingga matriks baru ini, dikali v, menghasilkan vektor nol.

102
00:07:06,380 --> 00:07:10,010
Sekarang, ini akan selalu benar jika v itu sendiri adalah vektor nol, 

103
00:07:10,010 --> 00:07:11,100
tapi itu membosankan.

104
00:07:11,340 --> 00:07:13,640
Yang kami inginkan adalah vektor eigen bukan nol.

105
00:07:14,420 --> 00:07:18,953
Dan jika Anda menonton bab 5 dan 6, Anda akan tahu bahwa satu-satunya cara agar 

106
00:07:18,953 --> 00:07:23,486
hasil kali matriks dengan vektor bukan nol menjadi nol adalah jika transformasi 

107
00:07:23,486 --> 00:07:28,020
yang terkait dengan matriks tersebut menekan ruang ke dimensi yang lebih rendah.

108
00:07:29,300 --> 00:07:34,220
Dan pemerasan itu sesuai dengan determinan nol untuk matriks tersebut.

109
00:07:35,480 --> 00:07:40,255
Untuk lebih konkretnya, misalkan matriks A Anda memiliki kolom 2, 1 dan 2, 3, 

110
00:07:40,255 --> 00:07:45,520
dan pikirkan tentang mengurangkan jumlah variabel, lambda, dari setiap entri diagonal.

111
00:07:46,480 --> 00:07:50,280
Sekarang bayangkan mengutak-atik lambda, memutar kenop untuk mengubah nilainya.

112
00:07:50,940 --> 00:07:54,869
Ketika nilai lambda berubah, matriks itu sendiri berubah, 

113
00:07:54,869 --> 00:07:57,240
dan determinan matriks pun berubah.

114
00:07:58,220 --> 00:08:01,242
Tujuannya di sini adalah untuk menemukan nilai lambda yang akan 

115
00:08:01,242 --> 00:08:04,170
membuat determinan ini menjadi nol, yang berarti transformasi 

116
00:08:04,170 --> 00:08:07,240
yang disesuaikan akan menekan ruang ke dimensi yang lebih rendah.

117
00:08:08,160 --> 00:08:11,160
Dalam hal ini, sweet spot muncul ketika lambda sama dengan 1.

118
00:08:12,180 --> 00:08:16,120
Tentu saja, jika kita memilih matriks lain, nilai eigennya belum tentu 1.

119
00:08:16,240 --> 00:08:18,600
Titik manisnya mungkin terkena nilai lambda lainnya.

120
00:08:20,080 --> 00:08:22,960
Jadi ini agak banyak, tapi mari kita uraikan maksudnya.

121
00:08:22,960 --> 00:08:26,260
Ketika lambda sama dengan 1, matriks A dikurangi lambda 

122
00:08:26,260 --> 00:08:29,560
dikalikan identitas menekan ruang ke dalam sebuah garis.

123
00:08:30,440 --> 00:08:34,349
Artinya ada vektor bukan nol v sehingga A dikurangi 

124
00:08:34,349 --> 00:08:38,559
lambda dikali identitas dikali v sama dengan vektor nol.

125
00:08:40,480 --> 00:08:45,901
Dan ingat, alasan kita mempedulikannya adalah karena ini berarti A dikali v sama 

126
00:08:45,901 --> 00:08:51,590
dengan lambda dikali v, yang dapat Anda baca dengan menyatakan bahwa vektor v adalah 

127
00:08:51,590 --> 00:08:57,280
vektor eigen dari A, yang tetap berada pada rentangnya sendiri selama transformasi A.

128
00:08:58,320 --> 00:09:01,500
Dalam contoh ini, nilai eigen yang terkait adalah 1, 

129
00:09:01,500 --> 00:09:04,020
jadi v sebenarnya akan tetap di tempatnya.

130
00:09:06,220 --> 00:09:07,829
Berhentilah sejenak dan renungkan apakah Anda perlu 

131
00:09:07,829 --> 00:09:09,500
memastikan bahwa alur pemikiran tersebut terasa tepat.

132
00:09:13,380 --> 00:09:15,640
Ini adalah hal yang saya sebutkan di pendahuluan.

133
00:09:16,220 --> 00:09:19,422
Jika Anda tidak memiliki pemahaman yang kuat tentang determinan dan 

134
00:09:19,422 --> 00:09:22,767
mengapa determinan tersebut berhubungan dengan sistem persamaan linier 

135
00:09:22,767 --> 00:09:26,300
yang memiliki solusi bukan nol, ekspresi seperti ini akan terasa tiba-tiba.

136
00:09:28,320 --> 00:09:31,969
Untuk melihat cara kerjanya, mari kita lihat kembali contoh dari awal, 

137
00:09:31,969 --> 00:09:34,540
dengan matriks yang kolomnya adalah 3, 0 dan 1, 2.

138
00:09:35,350 --> 00:09:39,777
Untuk mengetahui apakah suatu nilai lambda merupakan nilai eigen, 

139
00:09:39,777 --> 00:09:43,400
kurangi diagonal matriks ini dan hitung determinannya.

140
00:09:50,580 --> 00:09:54,473
Dengan melakukan ini, kita mendapatkan polinomial kuadrat tertentu di lambda, 

141
00:09:54,473 --> 00:09:56,720
3 dikurangi lambda dikali 2 dikurangi lambda.

142
00:09:57,800 --> 00:10:02,259
Karena lambda hanya dapat menjadi nilai eigen jika determinannya sama dengan nol, 

143
00:10:02,259 --> 00:10:05,957
Anda dapat menyimpulkan bahwa satu-satunya nilai eigen yang mungkin 

144
00:10:05,957 --> 00:10:08,840
adalah lambda sama dengan 2 dan lambda sama dengan 3.

145
00:10:09,640 --> 00:10:14,486
Untuk mengetahui vektor eigen apa yang sebenarnya memiliki salah satu nilai eigen ini, 

146
00:10:14,486 --> 00:10:18,719
katakanlah lambda sama dengan 2, masukkan nilai lambda tersebut ke matriks, 

147
00:10:18,719 --> 00:10:23,677
lalu selesaikan vektor mana yang dikirim ke nol oleh matriks yang diubah secara diagonal 

148
00:10:23,677 --> 00:10:23,900
ini.

149
00:10:24,940 --> 00:10:28,886
Jika Anda menghitungnya seperti yang Anda lakukan pada sistem linier lainnya, 

150
00:10:28,886 --> 00:10:31,871
Anda akan melihat bahwa solusinya adalah semua vektor pada 

151
00:10:31,871 --> 00:10:34,300
garis diagonal yang direntang oleh negatif 1, 1.

152
00:10:35,220 --> 00:10:39,399
Hal ini sesuai dengan fakta bahwa matriks yang tidak diubah, 3, 0, 1, 

153
00:10:39,399 --> 00:10:43,460
2, mempunyai efek meregangkan semua vektor tersebut dengan faktor 2.

154
00:10:46,320 --> 00:10:50,200
Sekarang, transformasi 2D tidak harus memiliki vektor eigen.

155
00:10:50,720 --> 00:10:53,400
Misalnya, bayangkan rotasi sebesar 90 derajat.

156
00:10:53,660 --> 00:10:55,882
Ini tidak memiliki vektor eigen apa pun karena 

157
00:10:55,882 --> 00:10:58,200
ia memutar setiap vektor dari rentangnya sendiri.

158
00:11:00,800 --> 00:11:04,302
Jika Anda benar-benar mencoba menghitung nilai eigen dari rotasi seperti ini, 

159
00:11:04,302 --> 00:11:05,560
perhatikan apa yang terjadi.

160
00:11:06,300 --> 00:11:10,140
Matriksnya memiliki kolom 0, 1 dan negatif 1, 0.

161
00:11:11,100 --> 00:11:15,800
Kurangi lambda dari elemen diagonal dan cari determinannya nol.

162
00:11:18,140 --> 00:11:21,940
Dalam hal ini, Anda mendapatkan lambda polinomial kuadrat ditambah 1.

163
00:11:22,680 --> 00:11:27,920
Akar polinomial tersebut hanyalah bilangan imajiner, i dan negatif i.

164
00:11:28,840 --> 00:11:33,600
Fakta bahwa tidak ada solusi bilangan real menunjukkan bahwa tidak ada vektor eigen.

165
00:11:35,540 --> 00:11:39,820
Contoh lain yang cukup menarik yang patut diingat adalah sebuah guntingan.

166
00:11:40,560 --> 00:11:44,791
Ini memperbaiki i-hat di tempatnya dan memindahkan j-hat 1 ke atas, 

167
00:11:44,791 --> 00:11:47,840
sehingga matriksnya memiliki kolom 1, 0 dan 1, 1.

168
00:11:48,739 --> 00:11:51,711
Semua vektor pada sumbu x merupakan vektor eigen dengan nilai 

169
00:11:51,711 --> 00:11:54,540
eigen 1 karena vektor-vektor tersebut tetap pada tempatnya.

170
00:11:55,680 --> 00:11:57,820
Faktanya, ini adalah satu-satunya vektor eigen.

171
00:11:58,760 --> 00:12:03,241
Saat Anda mengurangi lambda dari diagonal dan menghitung determinannya, 

172
00:12:03,241 --> 00:12:06,540
yang Anda dapatkan adalah 1 dikurangi lambda kuadrat.

173
00:12:09,319 --> 00:12:12,860
Dan satu-satunya akar dari ungkapan ini adalah lambda sama dengan 1.

174
00:12:14,560 --> 00:12:17,447
Hal ini sejalan dengan apa yang kita lihat secara geometris, 

175
00:12:17,447 --> 00:12:19,720
bahwa semua vektor eigen memiliki nilai eigen 1.

176
00:12:21,080 --> 00:12:24,762
Namun perlu diingat, dimungkinkan juga untuk hanya memiliki satu nilai eigen, 

177
00:12:24,762 --> 00:12:28,020
namun dengan lebih dari sekedar garis yang penuh dengan vektor eigen.

178
00:12:29,900 --> 00:12:33,180
Contoh sederhananya adalah matriks yang menskalakan semuanya dengan 2.

179
00:12:33,900 --> 00:12:37,429
Satu-satunya nilai eigen adalah 2, tetapi setiap vektor pada bidang 

180
00:12:37,429 --> 00:12:40,700
tersebut akan menjadi vektor eigen dengan nilai eigen tersebut.

181
00:12:42,000 --> 00:12:44,233
Sekarang adalah saat yang tepat untuk berhenti sejenak dan 

182
00:12:44,233 --> 00:12:46,960
merenungkan beberapa hal ini sebelum saya melanjutkan ke topik terakhir.

183
00:13:03,540 --> 00:13:06,945
Saya ingin mengakhiri di sini dengan gagasan tentang eigenbasis, 

184
00:13:06,945 --> 00:13:09,880
yang sangat bergantung pada gagasan dari video terakhir.

185
00:13:11,480 --> 00:13:16,380
Lihatlah apa yang terjadi jika vektor basis kita kebetulan merupakan vektor eigen.

186
00:13:17,120 --> 00:13:22,380
Misalnya, mungkin i-hat berskala negatif 1, dan j-hat berskala 2.

187
00:13:23,420 --> 00:13:26,517
Menuliskan koordinat barunya sebagai kolom matriks, 

188
00:13:26,517 --> 00:13:30,151
perhatikan bahwa kelipatan skalar tersebut, negatif 1 dan 2, 

189
00:13:30,151 --> 00:13:35,154
yang merupakan nilai eigen dari i-hat dan j-hat, berada pada diagonal matriks kita, 

190
00:13:35,154 --> 00:13:37,180
dan setiap entri lainnya adalah 0.

191
00:13:38,880 --> 00:13:43,139
Setiap kali suatu matriks mempunyai angka 0 di mana pun selain diagonal, 

192
00:13:43,139 --> 00:13:45,939
maka matriks tersebut disebut matriks diagonal, 

193
00:13:45,939 --> 00:13:50,782
dan cara untuk menafsirkannya adalah bahwa semua vektor basis adalah vektor eigen, 

194
00:13:50,782 --> 00:13:54,400
dengan entri diagonal dari matriks ini menjadi nilai eigennya.

195
00:13:57,100 --> 00:14:01,060
Ada banyak hal yang membuat matriks diagonal lebih baik untuk dikerjakan.

196
00:14:01,780 --> 00:14:05,103
Salah satu kelebihannya adalah lebih mudah menghitung apa yang akan terjadi 

197
00:14:05,103 --> 00:14:08,340
jika Anda mengalikan matriks ini dengan matriks itu sendiri beberapa kali.

198
00:14:09,420 --> 00:14:13,242
Karena yang dilakukan matriks ini hanyalah menskalakan setiap vektor 

199
00:14:13,242 --> 00:14:17,508
basis dengan beberapa nilai eigen, menerapkan matriks tersebut berkali-kali, 

200
00:14:17,508 --> 00:14:21,275
katakanlah 100 kali, hanya akan menyamakan penskalaan setiap vektor 

201
00:14:21,275 --> 00:14:24,600
basis dengan pangkat 100 dari nilai eigen yang bersangkutan.

202
00:14:25,700 --> 00:14:29,680
Sebaliknya, coba hitung pangkat 100 dari matriks non-diagonal.

203
00:14:29,680 --> 00:14:31,320
Sungguh, cobalah sejenak.

204
00:14:31,740 --> 00:14:32,440
Ini mimpi buruk.

205
00:14:36,080 --> 00:14:38,725
Tentu saja, Anda jarang sekali beruntung karena 

206
00:14:38,725 --> 00:14:41,260
vektor basis Anda juga merupakan vektor eigen.

207
00:14:42,040 --> 00:14:45,330
Namun jika transformasi Anda memiliki banyak vektor eigen, 

208
00:14:45,330 --> 00:14:50,182
seperti yang ada di awal video ini, sehingga Anda dapat memilih himpunan yang mencakup 

209
00:14:50,182 --> 00:14:55,089
seluruh ruang, maka Anda dapat mengubah sistem koordinat sehingga vektor eigen tersebut 

210
00:14:55,089 --> 00:14:56,540
menjadi vektor basis Anda.

211
00:14:57,140 --> 00:14:59,604
Saya berbicara tentang perubahan basis di video terakhir, 

212
00:14:59,604 --> 00:15:03,130
tapi saya akan membahas pengingat super cepat di sini tentang cara mengekspresikan 

213
00:15:03,130 --> 00:15:06,487
transformasi yang saat ini ditulis dalam sistem koordinat kita ke dalam sistem 

214
00:15:06,487 --> 00:15:07,040
yang berbeda.

215
00:15:08,440 --> 00:15:11,533
Ambil koordinat vektor-vektor yang ingin dijadikan basis baru, 

216
00:15:11,533 --> 00:15:13,989
yang dalam hal ini berarti dua vektor eigen kita, 

217
00:15:13,989 --> 00:15:17,279
lalu jadikan koordinat tersebut sebagai kolom-kolom suatu matriks, 

218
00:15:17,279 --> 00:15:19,440
yang disebut dengan matriks perubahan basis.

219
00:15:20,180 --> 00:15:24,159
Ketika Anda mengapit transformasi asli, meletakkan matriks perubahan 

220
00:15:24,159 --> 00:15:29,291
basis di sebelah kanannya dan kebalikan dari matriks perubahan basis di sebelah kirinya, 

221
00:15:29,291 --> 00:15:33,155
hasilnya akan berupa matriks yang mewakili transformasi yang sama, 

222
00:15:33,155 --> 00:15:36,500
tetapi dari perspektif koordinat vektor basis baru sistem.

223
00:15:37,440 --> 00:15:41,861
Inti dari melakukan hal ini dengan vektor eigen adalah bahwa matriks baru ini 

224
00:15:41,861 --> 00:15:46,680
dijamin berbentuk diagonal dengan nilai eigen yang sesuai di bawah diagonal tersebut.

225
00:15:46,860 --> 00:15:50,386
Hal ini karena ini mewakili pekerjaan dalam sistem koordinat di mana apa yang 

226
00:15:50,386 --> 00:15:54,320
terjadi pada vektor basis adalah vektor-vektor tersebut diskalakan selama transformasi.

227
00:15:55,800 --> 00:16:01,560
Sekumpulan vektor basis yang juga merupakan vektor eigen disebut dengan basis eigen.

228
00:16:02,340 --> 00:16:06,806
Jadi jika, misalnya, Anda perlu menghitung pangkat ke-100 dari matriks ini, 

229
00:16:06,806 --> 00:16:09,803
akan lebih mudah untuk mengubahnya ke basis eigen, 

230
00:16:09,803 --> 00:16:12,682
menghitung pangkat ke-100 dalam sistem tersebut, 

231
00:16:12,682 --> 00:16:15,680
lalu mengonversinya kembali ke sistem standar kita.

232
00:16:16,620 --> 00:16:18,320
Anda tidak dapat melakukan ini dengan semua transformasi.

233
00:16:18,320 --> 00:16:20,600
Sebuah geser, misalnya, tidak memiliki vektor 

234
00:16:20,600 --> 00:16:22,980
eigen yang cukup untuk menjangkau seluruh ruang.

235
00:16:23,460 --> 00:16:25,763
Tetapi jika Anda dapat menemukan basis eigen, itu 

236
00:16:25,763 --> 00:16:28,160
membuat operasi matriks menjadi sangat menyenangkan.

237
00:16:29,120 --> 00:16:31,748
Bagi Anda yang ingin menyelesaikan teka-teki yang cukup rapi untuk melihat 

238
00:16:31,748 --> 00:16:34,446
seperti apa aksinya dan bagaimana hal ini dapat digunakan untuk menghasilkan 

239
00:16:34,446 --> 00:16:37,320
beberapa hasil yang mengejutkan, saya akan meninggalkan petunjuk di sini di layar.

240
00:16:37,600 --> 00:16:40,280
Memang butuh sedikit usaha, tapi menurut saya Anda akan menikmatinya.

241
00:16:40,840 --> 00:16:45,380
Video berikutnya dan terakhir dari seri ini akan membahas tentang ruang vektor abstrak.

242
00:16:45,900 --> 00:16:46,120
Sampai jumpa lagi!

