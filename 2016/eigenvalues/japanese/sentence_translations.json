[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive. ",
  "translatedText": "固有ベクトルと固有値は、多くの学生が特に直感 的ではないと感じるトピックの 1 つです。",
  "model": "nmt",
  "time_range": [
   19.920000000000005,
   25.76
  ]
 },
 {
  "input": "Things like, why are we doing this, and what does this actually mean, are too often left just floating away in an unanswered sea of computations. ",
  "translatedText": "なぜこれを行うのか、これは実際には何を意味するのかといったことは、答 えの出ない計算の海の中でただ漂ってしまうことがあまりにも多いのです。",
  "model": "nmt",
  "time_range": [
   25.76,
   33.26
  ]
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular. ",
  "translatedText": "このシリーズのビデオを公開したところ、特にこのトピックの映 像化を楽しみにしているというコメントを多くいただきました。",
  "model": "nmt",
  "time_range": [
   33.92,
   40.06
  ]
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained. ",
  "translatedText": "私は、その理由は、固有なものが特に複雑である、あるいは 説明が不十分であるということではないと考えています。",
  "model": "nmt",
  "time_range": [
   40.68,
   46.36
  ]
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it. ",
  "translatedText": "実際、それは比較的簡単で、ほとんどの 本がうまく説明していると思います。",
  "model": "nmt",
  "time_range": [
   46.86,
   50.84
  ]
 },
 {
  "input": "What I want to do is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it. ",
  "translatedText": "私がやりたいのは、それに先行するトピックの多くを視覚的に しっかりと理解していて初めて意味があるということです。",
  "model": "nmt",
  "time_range": [
   50.84,
   58.48
  ]
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis. ",
  "translatedText": "ここで最も重要なのは、行列を線形変換として考える 方法を知っていることですが、行列式、線形方程式系 、基底の変更などにも慣れておく必要もあります。",
  "model": "nmt",
  "time_range": [
   59.06,
   69.94
  ]
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves. ",
  "translatedText": "固有値に関する混乱は、通常、固有ベクトルや固有値そのものよりも、こ れらのトピックのいずれかの基礎が不安定であることに関係しています。",
  "model": "nmt",
  "time_range": [
   70.72,
   79.24
  ]
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here. ",
  "translatedText": "まず、ここに示すような 2 次元での線形変換を考えてみましょう。",
  "model": "nmt",
  "time_range": [
   79.97999999999999,
   84.84
  ]
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2. ",
  "translatedText": "基底ベクトル i-hat を座標 3、0 に移動し、j-hat を 1、2 に移動します。",
  "model": "nmt",
  "time_range": [
   85.46,
   91.04
  ]
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2. ",
  "translatedText": "したがって、列が 3、0、および 1、2 である行列で表されます。",
  "model": "nmt",
  "time_range": [
   91.78,
   95.64
  ]
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip. ",
  "translatedText": "特定の 1 つのベクトルにどのような影響を与えるかに焦点を当て、そ のベクトルの範囲、つまり原点と先端を通る線について考えてください。",
  "model": "nmt",
  "time_range": [
   96.6,
   104.16
  ]
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation. ",
  "translatedText": "ほとんどのベクトルは、変換中にそのスパンから外れてしまいます。",
  "model": "nmt",
  "time_range": [
   104.92,
   108.38
  ]
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line. ",
  "translatedText": "つまり、ベクトルが着地した場所もたまたまその線上のど こかにあったとしたら、かなりの偶然のように思えます。",
  "model": "nmt",
  "time_range": [
   108.78,
   115.32
  ]
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar. ",
  "translatedText": "ただし、一部の特殊なベクトルは独自のスパンに残ります。つまり、行列がそのようなベクトル に与える影響は、スカラーのように、単にそれを引き伸ばしたり押しつぶしたりするだけです。",
  "model": "nmt",
  "time_range": [
   117.4,
   127.04
  ]
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector. ",
  "translatedText": "この特定の例では、基底ベクトル i-hat がそのような特別なベクトルの 1 つです。",
  "model": "nmt",
  "time_range": [
   129.46,
   134.1
  ]
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis. ",
  "translatedText": "i-hat のスパンは x 軸であり、行列の最初の列から、i-hat が その x 軸上にあるまま、それ自体の 3 倍に移動することがわかります。",
  "model": "nmt",
  "time_range": [
   134.64,
   144.12
  ]
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span. ",
  "translatedText": "さらに、線形変換の仕組みにより、X 軸上の他のベクトルも 3 倍に引き伸ばされるだけなので、独自のスパンに留まります。",
  "model": "nmt",
  "time_range": [
   146.32,
   156.48
  ]
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1. ",
  "translatedText": "この変換中に独自のスパンに留まる少し卑 劣なベクトルは、負の 1, 1 です。",
  "model": "nmt",
  "time_range": [
   158.5,
   164.04
  ]
 },
 {
  "input": "It ends up getting stretched by a factor of 2. ",
  "translatedText": "最終的には 2 倍に引き伸ばされてしまいます。",
  "model": "nmt",
  "time_range": [
   164.66,
   167.14
  ]
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2. ",
  "translatedText": "そして、繰り返しになりますが、線形性は、この男がまたがる対角線 上の他のベクトルが 2 倍に引き伸ばされることを意味します。",
  "model": "nmt",
  "time_range": [
   169.0,
   178.22
  ]
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span. ",
  "translatedText": "そして、この変換では、これらはすべて、そのスパ ンに留まるという特別な特性を持つベクトルです。",
  "model": "nmt",
  "time_range": [
   179.82,
   185.18
  ]
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2. ",
  "translatedText": "X 軸上のものは 3 倍に引き伸ばされ、この 対角線上のものは 2 倍に引き伸ばされます。",
  "model": "nmt",
  "time_range": [
   185.62,
   191.98
  ]
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans. ",
  "translatedText": "他のベクトルは変換中にいくらか回転され、その ベクトルがまたがる線から外れてしまいます。",
  "model": "nmt",
  "time_range": [
   192.76,
   198.08
  ]
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation. ",
  "translatedText": "もうお気づきかもしれませんが、これらの特殊なベクトルは変換の固有ベクトル と呼ばれ、各固有ベクトルには固有値と呼ばれるものが関連付けられています。固有値は、変換中に引き伸ばされたり押しつぶされたりする係数にすぎません。",
  "model": "nmt",
  "time_range": [
   202.52,
   217.38
  ]
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing or the fact that these eigenvalues happen to be positive. ",
  "translatedText": "もちろん、伸ばすことと潰すこと、またはこれらの固有値がたま たま正であるという事実について特別なことは何もありません。",
  "model": "nmt",
  "time_range": [
   220.28,
   225.94
  ]
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half. ",
  "translatedText": "別の例では、固有値がマイナスの 1/2 である固有ベクトルを持つことができます 。これは、ベクトルが 1/2 の係数で反転され押しつぶされることを意味します。",
  "model": "nmt",
  "time_range": [
   226.38,
   235.12
  ]
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it. ",
  "translatedText": "ただし、ここで重要なのは、回転して外れること なく、広がるライン上に留まるということです。",
  "model": "nmt",
  "time_range": [
   236.98,
   242.76
  ]
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation. ",
  "translatedText": "これがなぜ考えるのに役立つのかを垣間見るた めに、3 次元の回転を考えてみましょう。",
  "model": "nmt",
  "time_range": [
   244.46,
   249.8
  ]
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation. ",
  "translatedText": "その回転の固有ベクトル、つまり独自のスパン上に残るベクトル を見つけることができれば、見つけたものが回転軸になります。",
  "model": "nmt",
  "time_range": [
   251.66,
   260.5
  ]
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3 by 3 matrix associated with that transformation. ",
  "translatedText": "また、3D 回転については、その変換に関連付けられた 3 × 3 の行列全体について考えるよりも、回転軸 とその回転角度の観点から考える方がはるかに簡単です。",
  "model": "nmt",
  "time_range": [
   262.6,
   274.74
  ]
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same. ",
  "translatedText": "ちなみに、この場合、対応する固有値は 1 でなければなりません。これは、回転によって 何も伸びたり潰されたりすることがないため、ベクトルの長さは同じままであるためです。",
  "model": "nmt",
  "time_range": [
   277.0,
   285.86
  ]
 },
 {
  "input": "This pattern shows up a lot in linear algebra. ",
  "translatedText": "このパターンは線形代数でよく現れます。",
  "model": "nmt",
  "time_range": [
   288.08,
   290.02
  ]
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors. ",
  "translatedText": "行列で記述される線形変換では、この行列の列を基底ベクトルの着地点 として読み取ることで、その変換が何を行っているかを理解できます。",
  "model": "nmt",
  "time_range": [
   290.44,
   299.4
  ]
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues. ",
  "translatedText": "しかし多くの場合、特定の座標系にあまり依存せず、線形変換が実際に行うこ との核心に迫るより良い方法は、固有ベクトルと固有値を見つけることです。",
  "model": "nmt",
  "time_range": [
   300.02,
   310.82
  ]
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding. ",
  "translatedText": "ここでは、固有ベクトルと固有値を計算する方法の 詳細については説明しませんが、概念的な理解に 最も重要な計算上の考え方の概要を説明します。",
  "model": "nmt",
  "time_range": [
   315.46,
   326.02
  ]
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like. ",
  "translatedText": "象徴的に、固有ベクトルの考え方は次のようになります。",
  "model": "nmt",
  "time_range": [
   327.18,
   330.48
  ]
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue. ",
  "translatedText": "A は、固有ベクトルとして v を持つ何らかの変換を表 す行列であり、ラムダは数値、つまり対応する固有値です。",
  "model": "nmt",
  "time_range": [
   331.04,
   339.74
  ]
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda. ",
  "translatedText": "この式が言いたいのは、行列とベクトルの積、A と v の積が、固有ベクトル v をある値ラムダでスケーリングしただけと同じ結果が得られるということです。",
  "model": "nmt",
  "time_range": [
   340.68,
   349.9
  ]
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true. ",
  "translatedText": "したがって、行列 A の固有ベクトルとその固有値を見つけることは、 結局、この式を真にする v とラムダの値を見つけることになります。",
  "model": "nmt",
  "time_range": [
   351.0,
   360.1
  ]
 },
 {
  "input": "It's a little awkward to work with at first because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication. ",
  "translatedText": "左側は行列とベクトルの乗算を表しているため、最初は扱いに くいですが、ここでの右側はスカラー ベクトルの乗算です。",
  "model": "nmt",
  "time_range": [
   361.92,
   370.54
  ]
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda. ",
  "translatedText": "そこで、ラムダ係数でベクトルをスケーリングする効果のある行列を使用して、そ の右辺をある種の行列とベクトルの乗算として書き直すことから始めましょう。",
  "model": "nmt",
  "time_range": [
   371.12,
   380.62
  ]
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else. ",
  "translatedText": "このような行列の列は、各基底ベクトルに何が起こるかを表し、 各基底ベクトルは単純にラムダを乗算するため、この行列の対角 線には数値ラムダが含まれ、その他の部分はゼロになります。",
  "model": "nmt",
  "time_range": [
   381.68,
   394.32
  ]
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with ones down the diagonal. ",
  "translatedText": "この関数を記述する一般的な方法は、ラムダを因数分解してラムダ x i と して記述することです。ここで、i は対角に 1 を加えた単位行列です。",
  "model": "nmt",
  "time_range": [
   396.18,
   404.86
  ]
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v. ",
  "translatedText": "両辺が行列とベクトルの乗算のように見えるので 、右側を減算して v を因数分解できます。",
  "model": "nmt",
  "time_range": [
   405.86,
   411.86
  ]
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix, times v, gives the zero vector. ",
  "translatedText": "したがって、今あるものは新しい行列、A からラムダを掛けた恒等式であり、この新しい 行列に v を掛けたものがゼロ ベクトルとなるようなベクトル v を探しています。",
  "model": "nmt",
  "time_range": [
   414.16,
   424.92
  ]
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring. ",
  "translatedText": "v 自体がゼロ ベクトルの場合、これは常に true になりますが、それは退屈です。",
  "model": "nmt",
  "time_range": [
   426.38,
   431.1
  ]
 },
 {
  "input": "What we want is a non-zero eigenvector. ",
  "translatedText": "私たちが必要とするのは、ゼロ以外の固有ベクトルです。",
  "model": "nmt",
  "time_range": [
   431.34,
   433.64
  ]
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension. ",
  "translatedText": "そして、第 5 章と第 6 章を見れば、行列と非ゼロ ベクトル の積がゼロになる唯一の方法は、その行列に関連付けられた変換によ って空間が低次元に押しつぶされる場合であることがわかります。",
  "model": "nmt",
  "time_range": [
   434.42,
   448.02
  ]
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix. ",
  "translatedText": "そして、その潰れは行列のゼロ行列式に対応します。",
  "model": "nmt",
  "time_range": [
   449.3,
   454.22
  ]
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry. ",
  "translatedText": "具体的には、行列 A に列 2、1 と列 2、3 があり、各 対角要素から可変量ラムダを減算することを考えてみましょう。",
  "model": "nmt",
  "time_range": [
   455.48,
   465.52
  ]
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value. ",
  "translatedText": "ここで、ラムダを微調整し、ノブを回して値を変更することを想像してください。",
  "model": "nmt",
  "time_range": [
   466.48,
   470.28
  ]
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes. ",
  "translatedText": "ラムダの値が変化すると、行列自体が変化 するため、行列の行列式も変化します。",
  "model": "nmt",
  "time_range": [
   470.94,
   477.24
  ]
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension. ",
  "translatedText": "ここでの目標は、この行列式をゼロにするラムダの値を見つけることです。これは、 微調整された変換によって空間がより低い次元に押しつぶされることを意味します。",
  "model": "nmt",
  "time_range": [
   478.22,
   487.24
  ]
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1. ",
  "translatedText": "この場合、スイート スポットはラムダが 1 に等しいときに発生します。",
  "model": "nmt",
  "time_range": [
   488.16,
   491.16
  ]
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1. ",
  "translatedText": "もちろん、他の行列を選択した場合、固有値は必ずしも 1 になるとは限りません。",
  "model": "nmt",
  "time_range": [
   492.18,
   496.12
  ]
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda. ",
  "translatedText": "スイート スポットは、ラムダの他の値でヒットする可能性があります。",
  "model": "nmt",
  "time_range": [
   496.24,
   498.6
  ]
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying. ",
  "translatedText": "かなりの量になりますが、これが何を言っているのかを紐解いてみましょう。",
  "model": "nmt",
  "time_range": [
   500.08,
   502.96
  ]
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line. ",
  "translatedText": "ラムダが 1 の場合、行列 A からラムダを引いた恒等式が、スペースを直線上に押し込みます。",
  "model": "nmt",
  "time_range": [
   502.96,
   509.56
  ]
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector. ",
  "translatedText": "これは、A からラムダを引いたものと単位積を乗じた値がゼロ ベクトル に等しいような、ゼロ以外のベクトル v が存在することを意味します。",
  "model": "nmt",
  "time_range": [
   510.44,
   518.56
  ]
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A. ",
  "translatedText": "覚えておいてください、これを気にする理由は、A と v の積がラムダと v の積に等しいことを意味するためです。これは、ベクトル v が A の固有 ベクトルであり、変換 A 中に独自のスパンに留まると解釈できるからです。",
  "model": "nmt",
  "time_range": [
   520.48,
   537.28
  ]
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place. ",
  "translatedText": "この例では、対応する固有値は 1 であるため、v は実際にはその場に固定されたままになります。",
  "model": "nmt",
  "time_range": [
   538.32,
   544.02
  ]
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good. ",
  "translatedText": "その推論が適切であるかどうかを確認する必要があるかどうか、立ち止まって熟考してください。",
  "model": "nmt",
  "time_range": [
   546.22,
   549.5
  ]
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction. ",
  "translatedText": "冒頭で述べたような内容です。",
  "model": "nmt",
  "time_range": [
   553.38,
   555.64
  ]
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue. ",
  "translatedText": "行列式と、行列式がゼロ以外の解を持つ線形方程式系に関連する理由をしっかりと理 解していなかった場合、このような式は全くの青天の霹靂に感じられるでしょう。",
  "model": "nmt",
  "time_range": [
   556.22,
   566.3
  ]
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2. ",
  "translatedText": "これを実際に確認するために、列が 3、0 および 1、2 である行列を使用して例を最初から見直してみましょう。",
  "model": "nmt",
  "time_range": [
   568.32,
   574.54
  ]
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant. ",
  "translatedText": "値ラムダが固有値であるかどうかを確認するには、この 行列の対角からそれを減算し、行列式を計算します。",
  "model": "nmt",
  "time_range": [
   575.35,
   583.4
  ]
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda. ",
  "translatedText": "これを行うと、ラムダで特定の 2 次多項式 (3 マイナス ラムダ × 2 マイナス ラムダ) が得られます。",
  "model": "nmt",
  "time_range": [
   590.58,
   596.72
  ]
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3. ",
  "translatedText": "ラムダは、この行列式がたまたまゼロである場合にのみ固有値になり得るため、可能な固有値はラム ダが 2 に等しい場合とラムダが 3 に等しい場合のみであると結論付けることができます。",
  "model": "nmt",
  "time_range": [
   597.8,
   608.84
  ]
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero. ",
  "translatedText": "実際にこれらの固有値の 1 つを持つ固有ベクトルが何であるかを把握 するには、ラムダが 2 に等しい場合、そのラムダの値を行列に代入し 、この対角的に変更された行列がゼロに送信するベクトルを解きます。",
  "model": "nmt",
  "time_range": [
   609.64,
   623.9
  ]
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1. ",
  "translatedText": "これを他の線形システムと同じ方法で計算すると、解は負の 1, 1 で囲まれた対角線上のすべてのベクトルであることがわかります。",
  "model": "nmt",
  "time_range": [
   624.94,
   634.3
  ]
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2. ",
  "translatedText": "これは、変更されていない行列 3、0、1、2 がこれらすべてのベ クトルを 2 倍に引き伸ばす効果があるという事実に対応します。",
  "model": "nmt",
  "time_range": [
   635.22,
   643.46
  ]
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors. ",
  "translatedText": "現在、2D 変換には固有ベクトルが必要ではありません。",
  "model": "nmt",
  "time_range": [
   646.32,
   650.2
  ]
 },
 {
  "input": "For example, consider a rotation by 90 degrees. ",
  "translatedText": "たとえば、90 度の回転を考えてみましょう。",
  "model": "nmt",
  "time_range": [
   650.72,
   653.4
  ]
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span. ",
  "translatedText": "これは、すべてのベクトルを独自のスパンから回転させるため、固有ベクトルを持ちません。",
  "model": "nmt",
  "time_range": [
   653.66,
   658.2
  ]
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens. ",
  "translatedText": "実際にこのように回転の固有値を計算してみると、何が起こるかに注目してください。",
  "model": "nmt",
  "time_range": [
   660.8,
   665.56
  ]
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0. ",
  "translatedText": "その行列には列 0、1 と負の 1、0 があります。",
  "model": "nmt",
  "time_range": [
   666.3,
   670.14
  ]
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero. ",
  "translatedText": "対角要素からラムダを減算し、行列式がゼロになるときを探します。",
  "model": "nmt",
  "time_range": [
   671.1,
   675.8
  ]
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1. ",
  "translatedText": "この場合、多項式ラムダの 2 乗プラス 1 が得られます。",
  "model": "nmt",
  "time_range": [
   678.14,
   681.94
  ]
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i. ",
  "translatedText": "その多項式の根は虚数 i と負の i だけです。",
  "model": "nmt",
  "time_range": [
   682.6800000000001,
   687.92
  ]
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors. ",
  "translatedText": "実数解が存在しないという事実は、固有ベクトルが存在しないことを示しています。",
  "model": "nmt",
  "time_range": [
   688.84,
   693.6
  ]
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear. ",
  "translatedText": "頭の片隅に留めておく価値のあるもう 1 つの非常に興味深い例は、ハサミです。",
  "model": "nmt",
  "time_range": [
   695.54,
   699.82
  ]
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1. ",
  "translatedText": "これにより、i-hat が所定の位置に固定され、j-hat 1 が上に 移動されるため、その行列には列 1、0 および 1、1 が含まれます。",
  "model": "nmt",
  "time_range": [
   700.56,
   707.84
  ]
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place. ",
  "translatedText": "X 軸上のベクトルはすべて、所定の位置に固定さ れているため、固有値 1 の固有ベクトルです。",
  "model": "nmt",
  "time_range": [
   708.7399999999999,
   714.54
  ]
 },
 {
  "input": "In fact, these are the only eigenvectors. ",
  "translatedText": "実際、固有ベクトルはこれらだけです。",
  "model": "nmt",
  "time_range": [
   715.68,
   717.82
  ]
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared. ",
  "translatedText": "対角線からラムダを減算して行列式を計算すると、1 からラムダの 2 乗を引いたものが得られます。",
  "model": "nmt",
  "time_range": [
   718.76,
   726.54
  ]
 },
 {
  "input": "And the only root of this expression is lambda equals 1. ",
  "translatedText": "そして、この式の唯一のルートはラムダが 1 に等しいということです。",
  "model": "nmt",
  "time_range": [
   729.3199999999999,
   732.86
  ]
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1. ",
  "translatedText": "これは、すべての固有ベクトルが固有値 1 を 持つという幾何学的にわかることと一致します。",
  "model": "nmt",
  "time_range": [
   734.5600000000001,
   739.72
  ]
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors. ",
  "translatedText": "ただし、固有値を 1 つだけ持つことも可能ですが、固有ベク トルで満たされた行だけではないことにも注意してください。",
  "model": "nmt",
  "time_range": [
   741.08,
   748.02
  ]
 },
 {
  "input": "A simple example is a matrix that scales everything by 2. ",
  "translatedText": "簡単な例は、すべてを 2 でスケールする行列です。",
  "model": "nmt",
  "time_range": [
   749.9,
   753.18
  ]
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue. ",
  "translatedText": "唯一の固有値は 2 ですが、平面内のすべてのベク トルはその固有値を持つ固有ベクトルになります。",
  "model": "nmt",
  "time_range": [
   753.9,
   760.7
  ]
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic. ",
  "translatedText": "最後のトピックに移る前に、ここで少し立ち止ま って、これについてじっくり考えてみましょう。",
  "model": "nmt",
  "time_range": [
   762.0,
   766.96
  ]
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video. ",
  "translatedText": "ここで、最後のビデオのアイデアに大きく依存す る固有基底のアイデアで終わりたいと思います。",
  "model": "nmt",
  "time_range": [
   783.54,
   789.88
  ]
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors. ",
  "translatedText": "基底ベクトルがたまたま固有ベクトルだった場合に何が起こるかを見てみましょう。",
  "model": "nmt",
  "time_range": [
   791.48,
   796.38
  ]
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1, and j-hat is scaled by 2. ",
  "translatedText": "たとえば、i-hat はマイナス 1 でスケーリングされ、j-hat は 2 でスケーリングされる可能性があります。",
  "model": "nmt",
  "time_range": [
   797.12,
   802.38
  ]
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0. ",
  "translatedText": "新しい座標を行列の列として書き込むと、i-hat と j-hat の固有値であるスカラー倍数、負の 1 と 2 が行列の対角線上にあ り、他のすべてのエントリが 0 であることに注意してください。。",
  "model": "nmt",
  "time_range": [
   803.42,
   817.18
  ]
 },
 {
  "input": "Any time a matrix has 0s everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix, and the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues. ",
  "translatedText": "行列の対角以外の場所に 0 がある場合、それは当然のことながら対角 行列と呼ばれます。これを解釈する方法は、すべての基底ベクトルが固有 ベクトルであり、この行列の対角要素が固有値であるということです。",
  "model": "nmt",
  "time_range": [
   818.88,
   834.4
  ]
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with. ",
  "translatedText": "対角行列をより使いやすくするものはたくさんあります。",
  "model": "nmt",
  "time_range": [
   837.1,
   841.06
  ]
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times. ",
  "translatedText": "大きな点の 1 つは、この行列を何回も乗算すると 何が起こるかを計算するのが簡単になることです。",
  "model": "nmt",
  "time_range": [
   841.7800000000001,
   848.34
  ]
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue. ",
  "translatedText": "これらの行列は各基底ベクトルを何らかの固有値でスケーリングすることだけな ので、その行列を何度も (たとえば 100 回) 適用すると、各基底ベ クトルを対応する固有値の 100 乗でスケーリングすることになります。",
  "model": "nmt",
  "time_range": [
   849.42,
   864.6
  ]
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix. ",
  "translatedText": "対照的に、非対角行列の 100 乗を計算してみます。",
  "model": "nmt",
  "time_range": [
   865.7,
   869.68
  ]
 },
 {
  "input": "Really, try it for a moment. ",
  "translatedText": "本当に、ちょっと試してみてください。",
  "model": "nmt",
  "time_range": [
   869.68,
   871.32
  ]
 },
 {
  "input": "It's a nightmare. ",
  "translatedText": "それは悪夢だ。",
  "model": "nmt",
  "time_range": [
   871.74,
   872.44
  ]
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors. ",
  "translatedText": "もちろん、基底ベクトルも固有ベクトルになるほど幸運なことはめったにありません。",
  "model": "nmt",
  "time_range": [
   876.08,
   881.26
  ]
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors. ",
  "translatedText": "ただし、このビデオの冒頭のもののように、変換に多数の固有ベクト ルがあり、空間全体にわたるセットを選択できる場合は、これらの 固有ベクトルが基底ベクトルになるように座標系を変更できます。",
  "model": "nmt",
  "time_range": [
   882.04,
   896.54
  ]
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system. ",
  "translatedText": "前回のビデオで基底の変更について話しましたが、ここで は、現在座標系で書かれている変換を別の系に表現する方 法について、非常に簡単に思い出させていただきます。",
  "model": "nmt",
  "time_range": [
   897.14,
   907.04
  ]
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix. ",
  "translatedText": "新しい基底として使用するベクトルの座標 (この場合は 2 つの固有ベクトルを意味します) を取得し、それ らの座標を基底行列の変更と呼ばれる行列の列にします。",
  "model": "nmt",
  "time_range": [
   908.44,
   919.44
  ]
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system. ",
  "translatedText": "元の変換を挟んで、基底行列の変化を右側に、 基底行列の変化の逆行列を左側に置くと、結果 は同じ変換を表す行列になりますが、新しい基 底ベクトル座標の観点から見ると、システム。",
  "model": "nmt",
  "time_range": [
   920.18,
   936.5
  ]
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal. ",
  "translatedText": "固有ベクトルを使用してこれを行うことの要点は、この新しい行列がその 対角方向にある対応する固有値と対角であることが保証されることです。",
  "model": "nmt",
  "time_range": [
   937.44,
   946.68
  ]
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation. ",
  "translatedText": "これは、変換中に基底ベクトルに何が起こる かという座標系での作業を表すためです。",
  "model": "nmt",
  "time_range": [
   946.86,
   954.32
  ]
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis. ",
  "translatedText": "固有ベクトルでもある基底ベクトルのセットは、やはり当然のことながら固有基底と呼ばれます。",
  "model": "nmt",
  "time_range": [
   955.8,
   961.56
  ]
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system. ",
  "translatedText": "したがって、たとえば、この行列の 100 乗を計算する必要が ある場合は、固有基底に変更し、そのシステムで 100 乗を計 算してから、標準システムに変換し直す方がはるかに簡単です。",
  "model": "nmt",
  "time_range": [
   962.34,
   975.68
  ]
 },
 {
  "input": "You can't do this with all transformations. ",
  "translatedText": "すべての変換でこれを行うことはできません。",
  "model": "nmt",
  "time_range": [
   976.62,
   978.32
  ]
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space. ",
  "translatedText": "たとえば、ハサミには空間全体に広がるのに十分な固有ベクトルがありません。",
  "model": "nmt",
  "time_range": [
   978.32,
   982.98
  ]
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely. ",
  "translatedText": "しかし、固有基底を見つけることができれば、行列演算が非常に楽しくなります。",
  "model": "nmt",
  "time_range": [
   983.46,
   988.16
  ]
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen. ",
  "translatedText": "これが実際にどのようなものになるのか、そしてそれをどのように使用して 驚くべき結果が得られるのかを確認するために、かなり巧妙なパズルに取り 組んでみたいという方のために、画面上にプロンプトを残しておきます。",
  "model": "nmt",
  "time_range": [
   989.12,
   997.32
  ]
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it. ",
  "translatedText": "少し手間はかかりますが、楽しんでいただけると思います。",
  "model": "nmt",
  "time_range": [
   997.6,
   1000.28
  ]
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces. ",
  "translatedText": "このシリーズの次の最後のビデオは、抽象的なベクトル空間に関するものになります。",
  "model": "nmt",
  "time_range": [
   1000.84,
   1005.38
  ]
 },
 {
  "input": "See you then! ",
  "translatedText": "それではまた！",
  "model": "nmt",
  "time_range": [
   1005.9,
   1006.12
  ]
 }
]