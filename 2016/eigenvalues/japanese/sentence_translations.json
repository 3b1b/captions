[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive. ",
  "translatedText": "固有ベクトルと固有値は、多くの学生が特に直感 的ではないと感じるトピックの 1 つです。",
  "model": "google_nmt",
  "from_community_srt": "「前回『あなたにとっての数学とは?』と尋ねました。 ある人はこう答えました。 『数字をいじること。 構造をいじること。 』 では『あなたにとっての音楽とは?』との問いにあなたは『音符をいじること』と答えますか?」 －サージ・ラング 「固有ベクトルと固有値」は、多くの学生が感覚をつかみにくいトピックの1つです",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Things like, why are we doing this, and what does this actually mean, are too often left just floating away in an unanswered sea of computations. ",
  "translatedText": "なぜこれを行うのか、これは実際には何を意味するのかといったことは、答 えの出ない計算の海の中でただ漂ってしまうことがあまりにも多いのです。",
  "model": "google_nmt",
  "from_community_srt": "「学ぶ理由は?」とか「その本義は？",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular. ",
  "translatedText": "このシリーズのビデオを公開したところ、特にこのトピックの映 像化を楽しみにしているというコメントを多くいただきました。",
  "model": "google_nmt",
  "from_community_srt": "」といった疑問が 未解答の計算問題の海を漂うみたいに毎回のように残されます このシリーズの動画をあげた時も みなさん特にこのトピックをビジュアライズしてほしいとコメントしてくださりました",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained. ",
  "translatedText": "私は、その理由は、固有なものが特に複雑である、あるいは 説明が不十分であるということではないと考えています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it. ",
  "translatedText": "実際、それは比較的簡単で、ほとんどの 本がうまく説明していると思います。",
  "model": "google_nmt",
  "from_community_srt": "思うに 固有何とかに関してとりわけ複雑だとか説明不足だという訳ではなさそうです 確かにこの分野は比較的直截的で たいていの本でもうまく説明されていると思います 問題なのは このトピックに至るまでの諸分野で視覚的にはっきりイメージできないと理解しにくいということです",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "What I want to do is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it. ",
  "translatedText": "私がやりたいのは、それに先行するトピックの多くを視覚的に しっかりと理解していて初めて意味があるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis. ",
  "translatedText": "ここで最も重要なのは、行列を線形変換として考える 方法を知っていることですが、行列式、線形方程式系 、基底の変更などにも慣れておく必要もあります。",
  "model": "google_nmt",
  "from_community_srt": "ここで大事なのは君たちは線形変換としての行列の考え方を学んだということ だが以下の内容も受け入れてほしい 行列式、線型方程式系、基底変換といった内容です 固有何とかへの混乱は固有ベクトル、固有値そのものに関するよりも多くのことに",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves. ",
  "translatedText": "固有値に関する混乱は、通常、固有ベクトルや固有値そのものよりも、こ れらのトピックのいずれかの基礎が不安定であることに関係しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here. ",
  "translatedText": "まず、ここに示すような 2 次元での線形変換を考えてみましょう。",
  "model": "google_nmt",
  "from_community_srt": "その基礎のぐらつきにあることが普通です 始めに、2次元での線形変換について考えてみよう 画面の通りに 変換により基底ベクトルiハットは座標 (3,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2. ",
  "translatedText": "基底ベクトル i-hat を座標 3、0 に移動し、j-hat を 1、2 に移動します。",
  "model": "google_nmt",
  "from_community_srt": "0) へ、jハットは(1,",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2. ",
  "translatedText": "したがって、列が 3、0、および 1、2 である行列で表されます。",
  "model": "google_nmt",
  "from_community_srt": "2)へ移動します これを行列で表すとその列は (3, 0) と (1,",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip. ",
  "translatedText": "特定の 1 つのベクトルにどのような影響を与えるかに焦点を当て、そ のベクトルの範囲、つまり原点と先端を通る線について考えてください。",
  "model": "google_nmt",
  "from_community_srt": "2)になります ある1つのベクトルに注目して見てみよう このベクトルのスパン、ベクトルの起点と終点を通る直線に着目する ほとんどのベクトルは変換中にスパンから外れていくだろう つまりベクトルの行き着く先が",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation. ",
  "translatedText": "ほとんどのベクトルは、変換中にそのスパンから外れてしまいます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line. ",
  "translatedText": "つまり、ベクトルが着地した場所もたまたまその線上のど こかにあったとしたら、かなりの偶然のように思えます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar. ",
  "translatedText": "ただし、一部の特殊なベクトルは独自のスパンに残ります。つまり、行列がそのようなベクトル に与える影響は、スカラーのように、単にそれを引き伸ばしたり押しつぶしたりするだけです。",
  "model": "google_nmt",
  "from_community_srt": "たまたまこのライン上にあったとするならこれは偶然の一致とみなせます しかしある特別なベクトルはスパン上にとどまる つまりそのようなベクトルに対して行列はスカラーのように伸ばしたり縮めたりする効果をもたらします",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector. ",
  "translatedText": "この特定の例では、基底ベクトル i-hat がそのような特別なベクトルの 1 つです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis. ",
  "translatedText": "i-hat のスパンは x 軸であり、行列の最初の列から、i-hat が その x 軸上にあるまま、それ自体の 3 倍に移動することがわかります。",
  "model": "google_nmt",
  "from_community_srt": "今回の例で言えば基底ベクトル i ハットがこの特別なベクトルにあたります i ハットのスパンはx軸であり 行列の1列目から i ハットは自身を3倍したところに動きx軸上に残る様子を見ることができます",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span. ",
  "translatedText": "さらに、線形変換の仕組みにより、X 軸上の他のベクトルも 3 倍に引き伸ばされるだけなので、独自のスパンに留まります。",
  "model": "google_nmt",
  "from_community_srt": "更には線形変換の働き方によって x軸上の任意のベクトルもまた3倍に引き伸ばされます 故にスパン上に残ります またこの変換で自身のスパンにとどまるベクトルが(-1,",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1. ",
  "translatedText": "この変換中に独自のスパンに留まる少し卑 劣なベクトルは、負の 1, 1 です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2. ",
  "translatedText": "最終的には 2 倍に引き伸ばされてしまいます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2. ",
  "translatedText": "そして、繰り返しになりますが、線形性は、この男がまたがる対角線 上の他のベクトルが 2 倍に引き伸ばされることを意味します。",
  "model": "google_nmt",
  "from_community_srt": "1)に隠れていました これは2倍に引き伸ばされます ここで再び線形性から言えることから このベクトルのスパンである対角ライン上の任意のベクトルもまた 2倍に引き伸ばされるでしょう この変換については",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span. ",
  "translatedText": "そして、この変換では、これらはすべて、そのスパ ンに留まるという特別な特性を持つベクトルです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2. ",
  "translatedText": "X 軸上のものは 3 倍に引き伸ばされ、この 対角線上のものは 2 倍に引き伸ばされます。",
  "model": "google_nmt",
  "from_community_srt": "これらがスパン上にとどまるという特性を持ったベクトルのすべてです x軸上にあるものは3倍に引き伸ばされ 対角線上にあるものは2倍に引き伸ばされます その他のベクトルは変換によりいくらか回転され",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans. ",
  "translatedText": "他のベクトルは変換中にいくらか回転され、その ベクトルがまたがる線から外れてしまいます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation. ",
  "translatedText": "もうお気づきかもしれませんが、これらの特殊なベクトルは変換の固有ベクトル と呼ばれ、各固有ベクトルには固有値と呼ばれるものが関連付けられています。固有値は、変換中に引き伸ばされたり押しつぶされたりする係数にすぎません。",
  "model": "google_nmt",
  "from_community_srt": "スパンから落とされます お気づきでしょうが これら特殊なベクトルを今の変換における「固有ベクトル」と呼び 各固有ベクトルは「固有値」と呼ばれるものと関連付けられます これは変換における引き伸ばしまたは縮みの倍率です",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing or the fact that these eigenvalues happen to be positive. ",
  "translatedText": "もちろん、伸ばすことと潰すこと、またはこれらの固有値がたま たま正であるという事実について特別なことは何もありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half. ",
  "translatedText": "別の例では、固有値がマイナスの 1/2 である固有ベクトルを持つことができます 。これは、ベクトルが 1/2 の係数で反転され押しつぶされることを意味します。",
  "model": "google_nmt",
  "from_community_srt": "もちろん、引き伸ばしか押し縮かの対立だとか たまたま固有値が正の数でも特別なことはありません 他の例では固有値-1/2の固有ベクトルがあったりします これは固有ベクトルが反転し1/2に縮むことを意味します",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it. ",
  "translatedText": "ただし、ここで重要なのは、回転して外れること なく、広がるライン上に留まるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation. ",
  "translatedText": "これがなぜ考えるのに役立つのかを垣間見るた めに、3 次元の回転を考えてみましょう。",
  "model": "google_nmt",
  "from_community_srt": "ここで大事なところはベクトルのスパンから外れることなく直線上に残ることです どうしてこの考えが役に立つか覗いてみよう 3次元回転を考えます この回転での固有ベクトルを見つけたとするなら",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation. ",
  "translatedText": "その回転の固有ベクトル、つまり独自のスパン上に残るベクトル を見つけることができれば、見つけたものが回転軸になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3 by 3 matrix associated with that transformation. ",
  "translatedText": "また、3D 回転については、その変換に関連付けられた 3 × 3 の行列全体について考えるよりも、回転軸 とその回転角度の観点から考える方がはるかに簡単です。",
  "model": "google_nmt",
  "from_community_srt": "自身のスパンにとどまるベクトルで 回転の軸を発見したことになります ３次元回転を考えるにあたりある回転軸と回転の角度を考えるほうが 変換に対応する３×３行列を考えるよりずっと簡単です",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same. ",
  "translatedText": "ちなみに、この場合、対応する固有値は 1 でなければなりません。これは、回転によって 何も伸びたり潰されたりすることがないため、ベクトルの長さは同じままであるためです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra. ",
  "translatedText": "このパターンは線形代数でよく現れます。",
  "model": "google_nmt",
  "from_community_srt": "ところでこの場合では、回転で伸長も収縮も起こりえないために 固有値が１であるべきことから ベクトルの長さは同じままです このパターンは線形代数でよく出てきます 任意の線形変換を行列で書き表すことによって、行列の縦列を基底ベクトルの",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors. ",
  "translatedText": "行列で記述される線形変換では、この行列の列を基底ベクトルの着地点 として読み取ることで、その変換が何を行っているかを理解できます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues. ",
  "translatedText": "しかし多くの場合、特定の座標系にあまり依存せず、線形変換が実際に行うこ との核心に迫るより良い方法は、固有ベクトルと固有値を見つけることです。",
  "model": "google_nmt",
  "from_community_srt": "行き着く先と読み解くことでその挙動を理解できるでしょう しかし大抵は線形変換の実際の動きに関する核心に迫るもっといい方法があり 特定の座標系への依存を少なくできる それは固有ベクトルと固有値を見つけることです",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding. ",
  "translatedText": "ここでは、固有ベクトルと固有値を計算する方法の 詳細については説明しませんが、概念的な理解に 最も重要な計算上の考え方の概要を説明します。",
  "model": "google_nmt",
  "from_community_srt": "固有ベクトルと固有値の計算メソッドの詳細については言及しませんが 計算のアイデアをざっくりと見せたいと思います 概念の理解で一番大事なことです 記号の説明から、これが固有ベクトルのアイデアです",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like. ",
  "translatedText": "象徴的に、固有ベクトルの考え方は次のようになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue. ",
  "translatedText": "A は、固有ベクトルとして v を持つ何らかの変換を表 す行列であり、ラムダは数値、つまり対応する固有値です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda. ",
  "translatedText": "この式が言いたいのは、行列とベクトルの積、A と v の積が、固有ベクトル v をある値ラムダでスケーリングしただけと同じ結果が得られるということです。",
  "model": "google_nmt",
  "from_community_srt": "Aは変換を表す行列 vが固有ベクトル そしてλが対応する固有値と呼ばれる数です この数式からいえることは行列とベクトルの積 A×v が ある値λによって固有ベクトルvをスケーリングしただけと同じ結果を与えることです",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true. ",
  "translatedText": "したがって、行列 A の固有ベクトルとその固有値を見つけることは、 結局、この式を真にする v とラムダの値を見つけることになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication. ",
  "translatedText": "左側は行列とベクトルの乗算を表しているため、最初は扱いに くいですが、ここでの右側はスカラー ベクトルの乗算です。",
  "model": "google_nmt",
  "from_community_srt": "なので行列Aに対する固有ベクトルとその固有値を探すことは この数式を成立させるvとλの値を探すことに帰着します 今の段階では少々厄介です なぜなら左辺は行列とベクトルの積を表し 一方右辺はスカラーとベクトルの積を表すからです",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda. ",
  "translatedText": "そこで、ラムダ係数でベクトルをスケーリングする効果のある行列を使用して、そ の右辺をある種の行列とベクトルの乗算として書き直すことから始めましょう。",
  "model": "google_nmt",
  "from_community_srt": "そこで右辺を行列とベクトルの積の形に書き直すところから始めましょう ここで任意のベクトルをλ倍にする効果を持つ行列を使用します そのような行列は縦列が各基底ベクトルへの作用を表し 今の場合単にλ倍するため",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else. ",
  "translatedText": "このような行列の列は、各基底ベクトルに何が起こるかを表し、 各基底ベクトルは単純にラムダを乗算するため、この行列の対角 線には数値ラムダが含まれ、その他の部分はゼロになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with ones down the diagonal. ",
  "translatedText": "この関数を記述する一般的な方法は、ラムダを因数分解してラムダ x i と して記述することです。ここで、i は対角に 1 を加えた単位行列です。",
  "model": "google_nmt",
  "from_community_srt": "この行列は対角成分に数λが入り他の成分は０になります 一般的にはこの行列をλでくくりλ×Iと書きます ここでIは単位行列で対角成分が１になります 両辺が行列とベクトルの積のようになって",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v. ",
  "translatedText": "両辺が行列とベクトルの乗算のように見えるので 、右側を減算して v を因数分解できます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix, times v, gives the zero vector. ",
  "translatedText": "したがって、今あるものは新しい行列、A からラムダを掛けた恒等式であり、この新しい 行列に v を掛けたものがゼロ ベクトルとなるようなベクトル v を探しています。",
  "model": "google_nmt",
  "from_community_srt": "右辺で両辺を引きvで因数を引き出せます すると今の式で新しい行列、A - λI と 求めようとするベクトルvが新しい行列とvの積がゼロベクトルになるようになります さてvがゼロベクトルならば常に正しいですが",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring. ",
  "translatedText": "v 自体がゼロ ベクトルの場合、これは常に true になりますが、それは退屈です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector. ",
  "translatedText": "私たちが必要とするのは、ゼロ以外の固有ベクトルです。",
  "model": "google_nmt",
  "from_community_srt": "それではつまらないです 求めているのはノンゼロベクトルです 第5，",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension. ",
  "translatedText": "そして、第 5 章と第 6 章を見れば、行列と非ゼロ ベクトル の積がゼロになる唯一の方法は、その行列に関連付けられた変換によ って空間が低次元に押しつぶされる場合であることがわかります。",
  "model": "google_nmt",
  "from_community_srt": "6章を見ていれば 行列とノンゼロベクトルの積がゼロになりうるただ1つの場合は 行列に対応する変換が空間を低次元につぶす場合だとわかるでしょう そしてこの圧縮が起こるのは行列式がゼロになる時です",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix. ",
  "translatedText": "そして、その潰れは行列のゼロ行列式に対応します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry. ",
  "translatedText": "具体的には、行列 A に列 2、1 と列 2、3 があり、各 対角要素から可変量ラムダを減算することを考えてみましょう。",
  "model": "google_nmt",
  "from_community_srt": "具体例として列が(2, 1)と(2,",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value. ",
  "translatedText": "ここで、ラムダを微調整し、ノブを回して値を変更することを想像してください。",
  "model": "google_nmt",
  "from_community_srt": "3)の行列があるとしよう 各対角成分を変数λで引き算したと考えよう さてλを微調整しようとつまみを回して値を変えるところを想像しよう λの値が変わるにつれて 行列そのものも変わって故に行列式も変わっていきます",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes. ",
  "translatedText": "ラムダの値が変化すると、行列自体が変化 するため、行列の行列式も変化します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension. ",
  "translatedText": "ここでの目標は、この行列式をゼロにするラムダの値を見つけることです。これは、 微調整された変換によって空間がより低い次元に押しつぶされることを意味します。",
  "model": "google_nmt",
  "from_community_srt": "今回の目的はその行列式を0にするλの値を探すことで つまり変換を微調整してより低い次元に空間がつぶれるようにします 今の場合λが1に等しい時がジャストミートです もちろんほかの行列を選んだときは",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1. ",
  "translatedText": "この場合、スイート スポットはラムダが 1 に等しいときに発生します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1. ",
  "translatedText": "もちろん、他の行列を選択した場合、固有値は必ずしも 1 になるとは限りません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda. ",
  "translatedText": "スイート スポットは、ラムダの他の値でヒットする可能性があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying. ",
  "translatedText": "かなりの量になりますが、これが何を言っているのかを紐解いてみましょう。",
  "model": "google_nmt",
  "from_community_srt": "固有値は1である必要はなくジャストポイントはラムダが他の値になるときでしょう 話が長くなりましたがその主張を明かしましょう λが1に等しい時（A引くλかける単位行列）が空間を1直線に圧縮します",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line. ",
  "translatedText": "ラムダが 1 の場合、行列 A からラムダを引いた恒等式が、スペースを直線上に押し込みます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector. ",
  "translatedText": "これは、A からラムダを引いたものと単位積を乗じた値がゼロ ベクトル に等しいような、ゼロ以外のベクトル v が存在することを意味します。",
  "model": "google_nmt",
  "from_community_srt": "つまりはあるノンゼロベクトルvがあり （A引くλかける単位行列）かけるvがゼロベクトルに等しくなります 思い出しましょう。",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A. ",
  "translatedText": "覚えておいてください、これを気にする理由は、A と v の積がラムダと v の積に等しいことを意味するためです。これは、ベクトル v が A の固有 ベクトルであり、変換 A 中に独自のスパンに留まると解釈できるからです。",
  "model": "google_nmt",
  "from_community_srt": "これを見てきた理由はA×v = λ×vに等しいからで この式からベクトルvがAの固有ベクトルで Aの変換でそのスパンは変わらないことが読み取れます この例では固有値が1であることからvはその場でしっかり固定されているでしょう",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place. ",
  "translatedText": "この例では、対応する固有値は 1 であるため、v は実際にはその場に固定されたままになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good. ",
  "translatedText": "その推論が適切であるかどうかを確認する必要があるかどうか、立ち止まって熟考してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction. ",
  "translatedText": "冒頭で述べたような内容です。",
  "model": "google_nmt",
  "from_community_srt": "必要に応じて一連の流れが確かか立ち止まって考えよう ある意味冒頭で示唆したことです 行列式のこととそれが線型方程式系と 非ゼロ解を持つこととの関連性についてちゃんとわかっていないなら",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue. ",
  "translatedText": "行列式と、行列式がゼロ以外の解を持つ線形方程式系に関連する理由をしっかりと理 解していなかった場合、このような式は全くの青天の霹靂に感じられるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2. ",
  "translatedText": "これを実際に確認するために、列が 3、0 および 1、2 である行列を使用して例を最初から見直してみましょう。",
  "model": "google_nmt",
  "from_community_srt": "このような式は青天の霹靂のように感じるでしょう 実際に動かしてみるために最初に例に挙げた 列が(3, 0)と(1,",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant. ",
  "translatedText": "値ラムダが固有値であるかどうかを確認するには、この 行列の対角からそれを減算し、行列式を計算します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda. ",
  "translatedText": "これを行うと、ラムダで特定の 2 次多項式 (3 マイナス ラムダ × 2 マイナス ラムダ) が得られます。",
  "model": "google_nmt",
  "from_community_srt": "2)の行列に立ち戻ろう λの値が固有値か確かめるために 行列の対角成分を引いて行列式を計算してみます そうすると (3-λ)(2-λ) というλの二次多項式が得られます λが固有値になるのは行列式が0になるときだけであることから",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3. ",
  "translatedText": "ラムダは、この行列式がたまたまゼロである場合にのみ固有値になり得るため、可能な固有値はラム ダが 2 に等しい場合とラムダが 3 に等しい場合のみであると結論付けることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero. ",
  "translatedText": "実際にこれらの固有値の 1 つを持つ固有ベクトルが何であるかを把握 するには、ラムダが 2 に等しい場合、そのラムダの値を行列に代入し 、この対角的に変更された行列がゼロに送信するベクトルを解きます。",
  "model": "google_nmt",
  "from_community_srt": "可能な固有値は λ=2、または λ=3 のときだけと結論付けられます 実際にその固有値の1つを持つ固有ベクトルが何か発見するために λ=2 として 行列内のλにその値を代入して 対角要素が変更された行列により0に移されるベクトルをはじき出します",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1. ",
  "translatedText": "これを他の線形システムと同じ方法で計算すると、解は負の 1, 1 で囲まれた対角線上のすべてのベクトルであることがわかります。",
  "model": "google_nmt",
  "from_community_srt": "この方法で他の線形系を計算したとき 答えは(-1, 1)を延長した対角線上のすべてのベクトルになることがわかります これは変形前の行列[(3,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2. ",
  "translatedText": "これは、変更されていない行列 3、0、1、2 がこれらすべてのベ クトルを 2 倍に引き伸ばす効果があるという事実に対応します。",
  "model": "google_nmt",
  "from_community_srt": "0), (1,",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors. ",
  "translatedText": "現在、2D 変換には固有ベクトルが必要ではありません。",
  "model": "google_nmt",
  "from_community_srt": "2)]によって これらすべてのベクトルが2倍に伸ばされることに対応します ところで2次元変換が固有ベクトルを持たなくてもいいです 例えば90度回転を考えると 全ベクトルが自身のスパンを離れて回転されるため固有ベクトルはありません",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees. ",
  "translatedText": "たとえば、90 度の回転を考えてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span. ",
  "translatedText": "これは、すべてのベクトルを独自のスパンから回転させるため、固有ベクトルを持ちません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens. ",
  "translatedText": "実際にこのように回転の固有値を計算してみると、何が起こるかに注目してください。",
  "model": "google_nmt",
  "from_community_srt": "実際にこのような回転の固有ベクトルを計算しようとしたときあることに気づきます 行列は列として(0,",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0. ",
  "translatedText": "その行列には列 0、1 と負の 1、0 があります。",
  "model": "google_nmt",
  "from_community_srt": "1)と(-1,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero. ",
  "translatedText": "対角要素からラムダを減算し、行列式がゼロになるときを探します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1. ",
  "translatedText": "この場合、多項式ラムダの 2 乗プラス 1 が得られます。",
  "model": "google_nmt",
  "from_community_srt": "0)を持ちます 対角成分をλで引き行列式が0になる時を探します この場合多項式 λ^2+1 が得られます その多項式の根は虚数 i と -i だけです 実数解が存在しない事実から固有ベクトルが存在しないことが示されます",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i. ",
  "translatedText": "その多項式の根は虚数 i と負の i だけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors. ",
  "translatedText": "実数解が存在しないという事実は、固有ベクトルが存在しないことを示しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear. ",
  "translatedText": "頭の片隅に留めておく価値のあるもう 1 つの非常に興味深い例は、ハサミです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1. ",
  "translatedText": "これにより、i-hat が所定の位置に固定され、j-hat 1 が上に 移動されるため、その行列には列 1、0 および 1、1 が含まれます。",
  "model": "google_nmt",
  "from_community_srt": "もう1つ心の片隅においてほしい興味深い例がせん断です iハットはその場にとどまりjハットは1つずれます なのでその行列は列が(1, 0)と(1,",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place. ",
  "translatedText": "X 軸上のベクトルはすべて、所定の位置に固定さ れているため、固有値 1 の固有ベクトルです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors. ",
  "translatedText": "実際、固有ベクトルはこれらだけです。",
  "model": "google_nmt",
  "from_community_srt": "1)になります x軸上にあるベクトルのすべてがその場から動かずに残るため固有値1の固有ベクトルです 実は固有ベクトルはこれだけです 対角成分からλを引いて行列式を計算したとき (1-λ)^2が得られます",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared. ",
  "translatedText": "対角線からラムダを減算して行列式を計算すると、1 からラムダの 2 乗を引いたものが得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1. ",
  "translatedText": "そして、この式の唯一のルートはラムダが 1 に等しいということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1. ",
  "translatedText": "これは、すべての固有ベクトルが固有値 1 を 持つという幾何学的にわかることと一致します。",
  "model": "google_nmt",
  "from_community_srt": "そしてこの式の根はλ=1ただ1つです 幾何学的にみてもすべての固有ベクトルが固有値1を持つことと整合します これもあります ただ1つの固有値で複数直線上の固有ベクトルをとることも可能です",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors. ",
  "translatedText": "ただし、固有値を 1 つだけ持つことも可能ですが、固有ベク トルで満たされた行だけではないことにも注意してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2. ",
  "translatedText": "簡単な例は、すべてを 2 でスケールする行列です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue. ",
  "translatedText": "唯一の固有値は 2 ですが、平面内のすべてのベク トルはその固有値を持つ固有ベクトルになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic. ",
  "translatedText": "最後のトピックに移る前に、ここで少し立ち止ま って、これについてじっくり考えてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video. ",
  "translatedText": "ここで、最後のビデオのアイデアに大きく依存す る固有基底のアイデアで終わりたいと思います。",
  "model": "google_nmt",
  "from_community_srt": "単純な例に何でも2倍にする行列があります 固有値は2だけですが平面状のどのベクトルもこの固有値を持つ固有ベクトルになります 最後のトピックに移る前の今が 振り返りのタイミングです 今から固有基底というアイデアで終わりにしたいと思います",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors. ",
  "translatedText": "基底ベクトルがたまたま固有ベクトルだった場合に何が起こるかを見てみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1, and j-hat is scaled by 2. ",
  "translatedText": "たとえば、i-hat はマイナス 1 でスケーリングされ、j-hat は 2 でスケーリングされる可能性があります。",
  "model": "google_nmt",
  "from_community_srt": "次動画の考えに強く関係しています 基底ベクトルが偶然固有ベクトルだったらどうなるか目を向けよう 例にiハットが‐1倍に、jハットが2倍になるかもしれない その新ベクトルを列とする行列を書けば",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0. ",
  "translatedText": "新しい座標を行列の列として書き込むと、i-hat と j-hat の固有値であるスカラー倍数、負の 1 と 2 が行列の対角線上にあ り、他のすべてのエントリが 0 であることに注意してください。。",
  "model": "google_nmt",
  "from_community_srt": "iハットとjハットの固有値にもなっているスカラー倍-1と2が 行列の対角成分にあり他に0が入ることに気づきます 行列の対角成分以外がどこも0である場合 当然のことながら対角行列と呼ばれます",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has 0s everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix, and the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues. ",
  "translatedText": "行列の対角以外の場所に 0 がある場合、それは当然のことながら対角 行列と呼ばれます。これを解釈する方法は、すべての基底ベクトルが固有 ベクトルであり、この行列の対角要素が固有値であるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with. ",
  "translatedText": "対角行列をより使いやすくするものはたくさんあります。",
  "model": "google_nmt",
  "from_community_srt": "これを別の言葉で表現するなら全基底ベクトルが固有ベクトルになり その行列の対角成分が固有値になります 対角行列が非常に役に立つことはたくさんありますが その中でも特に 行列をそれ自身で何度も積をとればどうなるかを簡単に計算できます",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times. ",
  "translatedText": "大きな点の 1 つは、この行列を何回も乗算すると 何が起こるかを計算するのが簡単になることです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue. ",
  "translatedText": "これらの行列は各基底ベクトルを何らかの固有値でスケーリングすることだけな ので、その行列を何度も (たとえば 100 回) 適用すると、各基底ベ クトルを対応する固有値の 100 乗でスケーリングすることになります。",
  "model": "google_nmt",
  "from_community_srt": "どの対角行列でも基底ベクトルをある固有値でスカラー倍することから 100倍でも何倍でも行列をかけようと思えば 各基底ベクトルを対応する固有値の100乗倍するだけのことになります 対して非対角行列の100乗を計算しようとします",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix. ",
  "translatedText": "対照的に、非対角行列の 100 乗を計算してみます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment. ",
  "translatedText": "本当に、ちょっと試してみてください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare. ",
  "translatedText": "それは悪夢だ。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors. ",
  "translatedText": "もちろん、基底ベクトルも固有ベクトルになるほど幸運なことはめったにありません。",
  "model": "google_nmt",
  "from_community_srt": "少しやってみても、まさに悪夢です もちろん基底ベクトルが固有ベクトルにもなっているような幸運はめったにありませんが この動画の最初の例のようにいくつもの固有ベクトルを持つ変換であるなら",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors. ",
  "translatedText": "ただし、このビデオの冒頭のもののように、変換に多数の固有ベクト ルがあり、空間全体にわたるセットを選択できる場合は、これらの 固有ベクトルが基底ベクトルになるように座標系を変更できます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system. ",
  "translatedText": "前回のビデオで基底の変更について話しましたが、ここで は、現在座標系で書かれている変換を別の系に表現する方 法について、非常に簡単に思い出させていただきます。",
  "model": "google_nmt",
  "from_community_srt": "全空間に拡張する組を選べれば十分で 座標系を固有ベクトルが基底ベクトルになるように変更できるでしょう 基底変換は次の動画で話しますが 手っ取り早いリマインダーを流します 現在の座標系で書かれた変換を別の座標系で表現する方法です",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix. ",
  "translatedText": "新しい基底として使用するベクトルの座標 (この場合は 2 つの固有ベクトルを意味します) を取得し、それ らの座標を基底行列の変更と呼ばれる行列の列にします。",
  "model": "google_nmt",
  "from_community_srt": "新しく基底として使用したいベクトルの座標を持ってきます この場合2つの固有ベクトルです その座標を列とする行列を作ります これが基底変換行列です 元の変換行列を 右側に基底変換行列 左側に基底変換行列の逆行列でサンドしたとき",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system. ",
  "translatedText": "元の変換を挟んで、基底行列の変化を右側に、 基底行列の変化の逆行列を左側に置くと、結果 は同じ変換を表す行列になりますが、新しい基 底ベクトル座標の観点から見ると、システム。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal. ",
  "translatedText": "固有ベクトルを使用してこれを行うことの要点は、この新しい行列がその 対角方向にある対応する固有値と対角であることが保証されることです。",
  "model": "google_nmt",
  "from_community_srt": "結果は元と同じ変換を表現する行列になりますが 新規の基底ベクトルでの座標系からの視点になります 固有ベクトルを使ったやり方の大事なところは この新行列が対角行列でその対角成分が固有値に対応することが保証されていることです",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation. ",
  "translatedText": "これは、変換中に基底ベクトルに何が起こる かという座標系での作業を表すためです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis. ",
  "translatedText": "固有ベクトルでもある基底ベクトルのセットは、やはり当然のことながら固有基底と呼ばれます。",
  "model": "google_nmt",
  "from_community_srt": "これは基底ベクトルが変換中に起こることが スカラー倍することになる座標系で動作することを表すからです 基底ベクトル、もしくは固有ベクトルの組は 当然のことながら「固有基底」と呼ばれます",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system. ",
  "translatedText": "したがって、たとえば、この行列の 100 乗を計算する必要が ある場合は、固有基底に変更し、そのシステムで 100 乗を計 算してから、標準システムに変換し直す方がはるかに簡単です。",
  "model": "google_nmt",
  "from_community_srt": "なのでたとえばこの行列の100乗を計算する必要があるとするならば 固有基底に変換して その座標系で100乗を計算して 元の座標系に復元すれば簡単です そうできない変換もあります せん断といった例は全空間に拡張するに十分な固有ベクトルを持ちません",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations. ",
  "translatedText": "すべての変換でこれを行うことはできません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space. ",
  "translatedText": "たとえば、ハサミには空間全体に広がるのに十分な固有ベクトルがありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely. ",
  "translatedText": "しかし、固有基底を見つけることができれば、行列演算が非常に楽しくなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen. ",
  "translatedText": "これが実際にどのようなものになるのか、そしてそれをどのように使用して 驚くべき結果が得られるのかを確認するために、かなり巧妙なパズルに取り 組んでみたいという方のために、画面上にプロンプトを残しておきます。",
  "model": "google_nmt",
  "from_community_srt": "しかし固有基底が見つかれば行列作用素に愛着が持てるようになります 実際の動きはどうなのか、どのようにして驚くべき結果に導かれるのかが見れる とてもよくできたパズルに意欲的に取り掛かりたい人向けに画面上に問題を残します",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it. ",
  "translatedText": "少し手間はかかりますが、楽しんでいただけると思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces. ",
  "translatedText": "このシリーズの次の最後のビデオは、抽象的なベクトル空間に関するものになります。",
  "model": "google_nmt",
  "from_community_srt": "少々骨が折れますが楽しんでくれると思います 次回の、そしてシリーズ最後の動画は抽象ベクトル空間になります それでは！",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1005.38
 },
 {
  "input": "See you then! ",
  "translatedText": "それではまた！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.9,
  "end": 1006.12
 }
]