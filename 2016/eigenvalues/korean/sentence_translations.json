[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "",
  "from_community_srt": "고유벡터와 고유값은 많은 학생들이 특히 직관적이지 않다고 느끼는 주제 중 하나입니다.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "",
  "from_community_srt": "\"왜 우리가 이걸 하고 있지? 혹은 \"이게 정말 의미하는게 뭐야?\" 와 같은 질문들은 너무 많은 계산들 사이로 그냥 흘러가 버립니다",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "",
  "from_community_srt": "그리고 제가 이 비디오 시리즈들을 게시하자 많은 분들이 특히 이 주제를 시각화 하는데에 기대를 가지고 코멘트를 남겼습니다",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "",
  "from_community_srt": "저는 이것에 대한 이유가 고유- 무언가들이 특별히 복잡하거나 나쁘게 설명되어서가 아니라고 생각합니다",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "",
  "from_community_srt": "사실, 이것들은 직접적인 편으로 대부분의 책들에 설명이 나름 잘 되어있습니다",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "",
  "from_community_srt": "문제는 당신이 선행되는 여러 주제에 대한 탄탄한 시각적 이해가 있어야지만 정말로 이해가 된다는 것입니다.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "",
  "from_community_srt": "여기서 가장 중요한 것은 당신이 행렬을 선형변형(linear  transformations)으로 생각하는 방법을 아는 것입니다 그리고 행렬식(determinants), 선형  방정식계(linear systems of equations), 기저 변환(change of basis)들에 익숙해야 할 필요도 있습니다",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "",
  "from_community_srt": "고유- 무언가들에 대한 혼란은 보통 이 주제들 중 하나를 확실하게 알고 있지 않는 탓이 크지 고유벡터와 고유값 자체에 있지 않습니다",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "",
  "from_community_srt": "시작하자면, 여기 보이는 것과 같은 이차원에서의 선형변환에 대해 생각해 보십시오",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "",
  "from_community_srt": "이 변환은 기저 벡터인 i-hat을 좌표 [3, 0]으로, j-hat을 [1, 2]로 옮겨서",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "",
  "from_community_srt": "열이 [3, 0] 과 [1, 2]인 행렬로 나타냅니다",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "",
  "from_community_srt": "한 특정한 벡터에 무엇이 일어나는지와 벡터의 종점과 시점을 지나는 선인 그 벡터의 스팬(span)에 대해서 생각해 봅시다",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "",
  "from_community_srt": "대부분의 벡터는 변환의 과정에서 자신의 스팬을 벗어 날 것입니다",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "",
  "from_community_srt": "제 말은, 만약 벡터가 위치하는 장소가 그 선 위의 어딘가가 된다면 상당히 우연처럼 보일 것입니다",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "",
  "from_community_srt": "하지만 몇몇 특별한 벡터들은 고유한 스팬에 남아있습니다 이것은 행렬이 이러한 벡터들이 마치 스칼라인 것처럼 늘이고 줄이는 것  밖에 하지 않는다는 뜻입니다",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "",
  "from_community_srt": "이 특정한 예시에서는 기저 벡터인 i-hat은 특별한 벡터입니다",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "",
  "from_community_srt": "i-hat의 스팬은 x축이고 행렬의 첫 열에서 나온 것입니다 우리는 i-hat이 자신의 3배가 되게 움직여도",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "",
  "from_community_srt": "아직 x축 위에 있는 것을 볼 수 있습니다 또, 선형변환이 이루어지는 방식 때문에 x축 위의 다른 벡터도 3배로 늘어나고 자신의 span에 남아있습니다",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "",
  "from_community_srt": "이 변환 과정에 span이 변하지 않는 또다른 벡터는 바로 [-1,",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "1]입니다 이 벡터는 두 배로 늘어나게 됩니다",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "그리고 선형성(linearity)에 의해 이 대각선 위에 있는 다른 어떤 벡터도 2배로 늘어나게 될 것이라는 것을 알 수 있습니다",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "",
  "from_community_srt": "그리고 이 변형에서 이것들은 span을 유지하는 특별한 성질을 가진 모든 벡터입니다",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "x축 상의 벡터들은 세 배로 늘어나고 대각선 상의 벡터들은 두 배로 늘어납니다",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "",
  "from_community_srt": "변환 도중에 다른 벡터들은 어떤 식으로든 회전이 되어",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "",
  "from_community_srt": "스팬하는 선 위를 벗어날 것입니다 지금쯤 당신이 짐작하고 있는 대로 이 특별한 벡터들은 변환의 \"고유벡터(eigenvectors)\"라고 불립니다 그리고 각 고유벡터들은 \"고유값(eigenvalue)\"라고 불리는 값들을 가지고 있습니다 이것은 변환 도중 늘어나고 줄어드는 정도의 배수에 불과합니다",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "",
  "from_community_srt": "당연하게도, 늘어나는 것 vs. 줄어드는 것에 특별한 것은 없습니다 고유값이 양수가 된다는 사실도 마찬가지입니다",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "",
  "from_community_srt": "다른 예시에서는, 고유값 -1/2를 가진 고유벡터가 주어질 수 있습니다 이는 벡터가 (음의 부호로 인해) 뒤집어지고 1/2 만큼 줄어든다는 것을 의미합니다",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "",
  "from_community_srt": "하지만 여기에서 중요한 것은 벡터가 스팬하는 선 위에 머무르고 회전해서 벗어나는 일이 없다는 것입니다",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "",
  "from_community_srt": "왜 이것이 중요한지 생각해 보기 위해 삼차원에서의 회전을 생각해 봅시다",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "",
  "from_community_srt": "만약 당신이 이 회전에서의 고유벡터, 자신의 span에 남아있는 벡터를 찾을 수 있다면 그것이 바로 회전축입니다",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "",
  "from_community_srt": "그리고 3D회전을 생각할 때는 몇 개의 회전축과 회전하게 되는 각을 가지고 생각하는 것이 훨씬 편합니다 이 변환과 관련된 3x3 행렬 전체를 생각하는 것보단 말이죠",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "",
  "from_community_srt": "이 경우에는 회전이 그 무엇도 늘이거나 줄이지 않으니 고유값은 1이 될 것입니다 벡터의 길이가 변하지 않는다는 것이죠",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "",
  "from_community_srt": "이 유형은 행렬로 묘사된",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "",
  "from_community_srt": "선형변환에서 많이 등장합니다 이 행렬의 열을 기저 벡터의 위치로 읽어내릴 수 있다면 무엇이 일어나는지 알 수 있습니다",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "",
  "from_community_srt": "하지만 선형변환 중 일어나는 일에 대해 확실하게 느낌을 받으려면 특정 좌표계에 덜 관계있는 \"고유벡터(eigenvectors)\"와 \"고유값(eigenvalues)\"를 찾아야 합니다",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "",
  "from_community_srt": "고유벡터와 고유값을 계산하는 방법에 대해 자세히 설명하지는 않을  것이지만 개념의 이해에 중요한 부분만 훑어 보려고 합니다",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "",
  "from_community_srt": "상징적으로, \"고유벡터\"의 개념은 이렇게 생겼습니다",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "",
  "from_community_srt": "A는 임의의 변환을 나타내는 행렬이며 v는 고유벡터, λ는 고유값인 상수입니다",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "",
  "from_community_srt": "이 표현이 나타내는 것은 행렬-벡터 곱셈인 Av가 고유벡터 v를 임의의 상수 λ로 스케일링 한 결과와 같다는 것입니다",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "",
  "from_community_srt": "따라서 행렬 A의 고유벡터와 고유값을 찾는 것은 이 표현을 참으로 만드는 v와 λ의 값을 찾는 것이 됩니다",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "",
  "from_community_srt": "왼쪽이 행렬-벡터 곱셈인 반면에 오른쪽은 스칼라-벡터 곱셈이여서 계산하기가 이상할 것입니다",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "",
  "from_community_srt": "그러니까 오른쪽을 임의의 벡터를 λ배로 스케일링 하는 행렬-벡터 곱으로 다시 쓰는 것부터 시작합시다",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "",
  "from_community_srt": "이러한 행렬의 열은 각 기저 벡터에 일어나는 변화를 나타내는데 여기선 단순히 λ배를 하므로 이 행렬은 대각선 아래로 상수 λ를 가지고 다른 곳은 전부 0이 들어갑니다",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "",
  "from_community_srt": "좀 더 보편적인 방법은 λ를 묶어내서 λI로 쓰는 것입니다 I는 대각선에 1이 있는 항등 행렬입니다",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "",
  "from_community_srt": "이제 양변이 모두 행렬-벡터 곱으로 나타내졌으니 양변을 빼서 v라는 인수로 묶어낼 수 있습니다",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "",
  "from_community_srt": "이제 새로운 행렬인 A-λI을 얻었으니 이 행렬과 곱해서 영벡터를 만드는 벡터 v를 찾아야 합니다",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "",
  "from_community_srt": "물론 v가 영벡터가 된다면 이 식은 항상 성립하겠지만, 재미가 없습니다",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "",
  "from_community_srt": "영이 아닌 고유벡터를 얻고 싶으니까요",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "",
  "from_community_srt": "만약 챕터 5와 6을 보셨다면 영벡터가 아닌 벡터와 행렬을 곱해서 0이 되게 만드는 방법은 행렬에 일어나는 변환이 낮은 차원으로 내리는 것이면 됩니다",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "",
  "from_community_srt": "이 축소는 행렬이 영 행렬식(zero determinant)이 되게 합니다",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "",
  "from_community_srt": "정확히 하기 위해, 어떤 행렬 A가 [2, 1]과 [2, 3]이라는 열을 가지고 변수 λ를 각 대각선의 값에서 빼는 경우를 생각해 봅시다",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "",
  "from_community_srt": "이제 λ의 값이 이리저리 바뀐다고 생각해 봅시다",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "",
  "from_community_srt": "λ의 값이 바뀌면, 행렬이 바뀌고 행렬식도 바뀌게 됩니다",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "",
  "from_community_srt": "이 행렬식을 0으로 만드는 λ값을 찾는 것은 이 공간의 차원을 낮추는 변환을 찾는 것입니다",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "",
  "from_community_srt": "이 경우에는 λ=1이 됩니다",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "",
  "from_community_srt": "당연히 다른 행렬이였다면 고유값이 1이 될 필요는 없습니다",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "",
  "from_community_srt": "아마 λ는 다른 값을 가졌을 겁니다",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "",
  "from_community_srt": "약간 로또같긴 하지만 이것이 갖는 의미를 해석해 봅시다",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "",
  "from_community_srt": "λ=1일 때, 행렬 A-λI는 선이 됩니다",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "",
  "from_community_srt": "(A-λI)을 영벡터로 만드는 영벡터가 아닌 v벡터가 있다는 겁니다",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "",
  "from_community_srt": "지금 우리가 이걸 하고 있는 것은 Av=λv로 만드는 v 벡터가 변환 A에서 span이 변하지 않는 고유벡터 v이기 때문에 찾고 있다는 것을 기억해 두십시요",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "",
  "from_community_srt": "이 예시에서 고유값은 1이 되어 v는 그대로 있습니다",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "",
  "from_community_srt": "잘 이해가 가지 않는다면 잠깐 멈춰서 생각해 보십시요",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "",
  "from_community_srt": "초반에 말했던 것처럼, 행렬식(determinants)과",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "",
  "from_community_srt": "영이 아닌 해를 가지는 선형 방정식계(linear systems of equations)가 어떤 관계가 있는지를 정확히 알지 못한다면 이러한 표현이 전혀 이해가 되지 않을 것입니다",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "",
  "from_community_srt": "영상으로 보기 위해 처음부터 예시를 다시 봅시다 [3, 0]와 [1, 2]의 열을 가지는 행렬과",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "",
  "from_community_srt": "고유값이 되는 λ의 값을 찾기 위해 이 행렬의 대각선에서 빼서 행렬식을 계산합니다",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "",
  "from_community_srt": "이렇게 하면, λ에 대한 이차식인 (3-λ)(2-λ)을 얻습니다",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "",
  "from_community_srt": "행렬식을 0으로 만드는 λ만이 고유값이 될 수 있으므로 가능한 고유값은 λ= 2와 λ=3 밖에 없다는 결론을 얻습니다",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "",
  "from_community_srt": "이 중 하나의 고유값을 가지는 고유벡터를 알아내기 위해 λ=2라고 가정합시다 λ의 값을 행렬에 집어넣어서 대각선이 바뀐 이 행렬이 [2, 0]를 어떻게 변환하는지 봅시다",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "",
  "from_community_srt": "다른 선형계에서 하듯이 이걸 계산하면 해가 [-1, 1]이 span하는 대각선 위에 있는 모든 벡터라는 사실을 알 수 있습니다",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "",
  "from_community_srt": "이건 원래 행렬 3, 0, 1, 2가 이 벡터들을 두 배로 늘린다는 사실과 일치합니다",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "",
  "from_community_srt": "2D 변환은 고유벡터가 무조건 존재하지 않습니다",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "",
  "from_community_srt": "예를 들어, 고유벡터가 없는",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "",
  "from_community_srt": "90°회전의 경우에는 모든 벡터가 자신의 span을 벗어납니다",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "",
  "from_community_srt": "이런 회전에서 고유벡터를 계산하려고 시도한다면 어떤 일이 일어나는지 봅시다",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "",
  "from_community_srt": "행렬의 열이 [0, 1]과 [-1,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "",
  "from_community_srt": "0]이고 대각선 성분에서 λ를 빼고 행렬식이 0이 되는 경우를 찾아보면",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "",
  "from_community_srt": "다항식 λ^2+1을 얻게 되는데",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "",
  "from_community_srt": "이 다항식의 해는 허수 i와 -i 뿐입니다",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "",
  "from_community_srt": "실수해가 없다는 것은 고유벡터가 없다는 것입니다",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "",
  "from_community_srt": "생각해 볼 만한 다른 재밌는 예시는 미는 것(shear)입니다",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "",
  "from_community_srt": "i-hat은 그대로 두고 j-hat은 1만큼 움직이면 행렬의 열이 [1, 0]과 [1,",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "",
  "from_community_srt": "1]이 됩니다 x축 위에 있는 모든 벡터들은 그 자리에 그대로 있기 때문에 고유값 1을 가지는 고유벡터가 됩니다",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "",
  "from_community_srt": "사실, 이 벡터들은 대각선에서 λ를 빼고",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "",
  "from_community_srt": "행렬식을 계산 했을 때 얻는 유일한 고유벡터입니다 1-λ^2이라는 식이 나오는데",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "",
  "from_community_srt": "이 식의 해는 λ=1 뿐입니다",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "",
  "from_community_srt": "우리가 기하학적으로 알아낸 모든 고유벡터는 고유값 1을 가진다는 것을 알아낸 사실과 일치합니다",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "",
  "from_community_srt": "하나의 고유값을 가지면서도 다양한 고유벡터들을 가질 수 있다는 것도 염두에 두십시요",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "",
  "from_community_srt": "간단한 예는 모든 것을 두배로 스케일하는 행렬인데,",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "",
  "from_community_srt": "고유값은 2 뿐이지만 평면 상의 모든 벡터는 고유벡터가 됩니다",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "",
  "from_community_srt": "마지막 주제로 넘어가기 전에 멈춰서 생각할 시간을 가질 좋은 타이밍입니다",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "",
  "from_community_srt": "저번 영상의 개념들과 깊은 관계가 있는 개념인 \"고유기저(eigenbasis)\"로 마무리짓고 싶습니다",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "",
  "from_community_srt": "만약 기저벡터들이 고유벡터이기도 했다면 어떤 일이 일어나는지 봅시다",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "",
  "from_community_srt": "예를 들어, i-hat은 1만큼, j-hat은 2만큼 스케일 되었다고 합시다",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "",
  "from_community_srt": "새로운 좌표로 행렬의 열을 써내면 i-hat과 j-hat의 고유값인 스칼라 곱 -1 과 2가 행렬의 대각선에 있고 다른 값들은 모두 0입니다",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "",
  "from_community_srt": "대각선 외에 모두 0인 행렬은 뻔하지만, \"대각선 행렬(diagonal matrix)\"라고 불립니다",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "",
  "from_community_srt": "이걸 해석하면 모든 기저벡터는 고유벡터이고 대각선의 값들은 고유값이 됩니다",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "",
  "from_community_srt": "대각선 행렬이 다루기 좋은 이유 중 하나는",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "",
  "from_community_srt": "만약 이 행렬을 아주 많이 스스로 곱한다면 무엇이 일어날지 계산하기 쉽다는 것입니다",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "",
  "from_community_srt": "이 행렬이 하는 것은 각 기저 벡터를 특정한 고유값으로 스케일 하는 것이니까 말입니다 이 행렬을 100번정도 적용하는 것은 기저 벡터들을 고유값의 100제곱으로 스케일링 하는 것과 동일합니다",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "",
  "from_community_srt": "대조적으로, 대각선 행렬이 아닌 것을 100번 곱한 것을 계산하는 걸 시도해 보십시오",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "",
  "from_community_srt": "진심으로, 한번 해 보세요",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "",
  "from_community_srt": "악몽입니다! 당연히, 기저 벡터가 고유벡터가 되는 상황은 아주 운이 좋은 상황일 것입니다",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "",
  "from_community_srt": "하지만 만약 변환이 앞서 했던 것과 같이 전체 공간을 span하는 세트를 고를 수 있을 만큼 고유벡터들을 많이 가지고 있다면 고유벡터가 기저벡터가 되도록 좌표계를 바꿀 수 있습니다",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "",
  "from_community_srt": "저번 영상에서 기저를 바꾸는 것을 설명했지만 쓰고 있는 좌표계를 바꿔 기술할 때 변환을 어떻게 표현하는 지에 대한 아주 빠른 복습을 해봅시다",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "",
  "from_community_srt": "새로운 기저로 쓰고 싶은 벡터의 좌표를 가져와 (여기선 두 고유벡터가 됩니다) 행렬의 열을 구성합니다 이게 기저 행렬을 바꾸는 것입니다",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "",
  "from_community_srt": "바뀐 기저 행렬을 오른쪽에 두고 왼쪽에 바뀐 기저 행렬의 역을 두고 원래 변환을 사이에 끼면 같은 변환을 나타내지만 다른 기저 벡터 좌표계를 쓰는 행렬을 얻습니다",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "",
  "from_community_srt": "이걸 고유벡터로 하는 이유는 새로운 행렬이 대각선에 고유값들을 가지는 대각선 행렬이 되게 하기 위해서입니다",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "",
  "from_community_srt": "이제 기저 벡터가 변환될 때 스케일 되기만 하는 좌표계에서 작업할 수 있습니다",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "",
  "from_community_srt": "기저벡터이기도 한 고유벡터의 쌍은 \"고유 기저(eigenbasis)\"라고 불립니다",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "",
  "from_community_srt": "만약 이 행렬의 100제곱을 계산해야 한다면 고유 기저로 바꾼 후 100제곱을 계산하고 다시 원래의 계로 전환하는 것이 훨씬 쉬울 것입니다",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "",
  "from_community_srt": "모든 변환에서 이게 되진 않습니다",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "",
  "from_community_srt": "미는 것(shear)은 모든 공간을 span하는 고유벡터가 없습니다",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "",
  "from_community_srt": "하지만 고유기저를 찾을 수 있다면 행렬 계산이 아주 쉬워집니다",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "",
  "from_community_srt": "이걸 활용해서 놀라운 결과를 내는 일을 직접 해보고 싶은 사람들을 위해 괜찮은 퀴즈를 준비했습니다 문제를 화면에 띄워 놓겠습니다",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "",
  "from_community_srt": "약간 힘들 수 있지만 재밌을 거라고 생각합니다",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "",
  "from_community_srt": "이 시리즈의 다음이자 마지막 영상은 \"추상 벡터 공간(abstract vector spaces)\"에 대한 것입니다",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]