[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "고유벡터와 고유값은 많은 학생들이 특히 직관적이지 않다고 생각하는 주제 중 하나입니다.",
  "model": "google_nmt",
  "from_community_srt": "고유벡터와 고유값은 많은 학생들이 특히 직관적이지 않다고 느끼는 주제 중 하나입니다.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "우리가 이것을 하는 이유와 이것이 실제로 무엇을 의미하는지와 같은 질문은 답이 없는 계산의 바다에 떠다니는 경우가 너무 많습니다.",
  "model": "google_nmt",
  "from_community_srt": "\"왜 우리가 이걸 하고 있지?",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "그리고 제가 이 시리즈의 영상을 공개하면서 많은 분들이 이 주제를 특히 시각화할 수 있기를 기대한다는 의견을 많이 주셨습니다.",
  "model": "google_nmt",
  "from_community_srt": "혹은 \"이게 정말 의미하는게 뭐야?\" 와 같은 질문들은 너무 많은 계산들 사이로 그냥 흘러가 버립니다 그리고 제가 이 비디오 시리즈들을 게시하자 많은 분들이 특히 이 주제를 시각화 하는데에 기대를 가지고 코멘트를 남겼습니다",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "나는 그 이유가 고유 사물이 특별히 복잡하거나 제대로 설명되지 않았기 때문이 아니라고 생각합니다.",
  "model": "google_nmt",
  "from_community_srt": "저는 이것에 대한 이유가 고유- 무언가들이 특별히 복잡하거나 나쁘게 설명되어서가 아니라고 생각합니다 사실,",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "사실, 그것은 비교적 간단하고, 대부분의 책이 그것을 잘 설명하고 있다고 생각합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "문제는 앞에 나오는 많은 주제에 대해 확실한 시각적 이해가 있는 경우에만 실제로 의미가 있다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "이것들은 직접적인 편으로 대부분의 책들에 설명이 나름 잘 되어있습니다 문제는 당신이 선행되는 여러 주제에 대한 탄탄한 시각적 이해가 있어야지만 정말로 이해가 된다는 것입니다.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "여기서 가장 중요한 것은 행렬을 선형 변환으로 생각하는 방법을 아는 것입니다. 하지만 행렬식, 방정식의 선형 시스템 및 기저 변경과 같은 사항에도 익숙해야 합니다.",
  "model": "google_nmt",
  "from_community_srt": "여기서 가장 중요한 것은 당신이 행렬을 선형변형(linear  transformations)으로 생각하는 방법을 아는 것입니다 그리고 행렬식(determinants), 선형  방정식계(linear systems of equations),",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "고유량에 대한 혼란은 일반적으로 고유벡터 및 고유값 자체보다는 이러한 주제 중 하나의 불안정한 기초와 더 관련이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "기저 변환(change of basis)들에 익숙해야 할 필요도 있습니다 고유- 무언가들에 대한 혼란은 보통 이 주제들 중 하나를 확실하게 알고 있지 않는 탓이 크지 고유벡터와 고유값 자체에 있지 않습니다 시작하자면,",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "시작하려면 여기에 표시된 것과 같은 2차원의 선형 변환을 고려해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "여기 보이는 것과 같은 이차원에서의 선형변환에 대해 생각해 보십시오 이 변환은 기저 벡터인 i-hat을 좌표 [3,",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "기본 벡터 i-hat을 좌표 3, 0으로 이동하고 j-hat을 1, 2로 이동합니다.",
  "model": "google_nmt",
  "from_community_srt": "0]으로, j-hat을 [1,",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "따라서 열이 3, 0, 1, 2인 행렬로 표현됩니다.",
  "model": "google_nmt",
  "from_community_srt": "2]로 옮겨서 열이 [3, 0] 과 [1,",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "하나의 특정 벡터에 어떤 역할을 하는지에 집중하고 해당 벡터의 범위, 원점과 끝을 통과하는 선에 대해 생각해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "2]인 행렬로 나타냅니다 한 특정한 벡터에 무엇이 일어나는지와 벡터의 종점과 시점을 지나는 선인 그 벡터의 스팬(span)에 대해서 생각해 봅시다 대부분의 벡터는 변환의 과정에서 자신의 스팬을 벗어 날 것입니다",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "대부분의 벡터는 변환 중에 해당 범위를 벗어나게 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "내 말은, 벡터가 착륙한 장소도 우연히 그 선 어딘가에 있었다면 그것은 꽤 우연의 일치처럼 보일 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "제 말은,",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "그러나 일부 특수 벡터는 자체 범위에 남아 있습니다. 즉, 행렬이 그러한 벡터에 미치는 영향은 스칼라처럼 단순히 늘리거나 찌그러뜨리는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "만약 벡터가 위치하는 장소가 그 선 위의 어딘가가 된다면 상당히 우연처럼 보일 것입니다 하지만 몇몇 특별한 벡터들은 고유한 스팬에 남아있습니다 이것은 행렬이 이러한 벡터들이 마치 스칼라인 것처럼",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "이 특정 예에서 기본 벡터 i-hat은 그러한 특수 벡터 중 하나입니다.",
  "model": "google_nmt",
  "from_community_srt": "늘이고 줄이는 것  밖에 하지 않는다는 뜻입니다 이 특정한 예시에서는 기저 벡터인 i-hat은 특별한 벡터입니다 i-hat의 스팬은 x축이고 행렬의 첫 열에서 나온 것입니다",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "i-hat의 범위는 x축이고, 행렬의 첫 번째 열에서 i-hat이 여전히 x축에 있는 3배로 이동하는 것을 볼 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "우리는 i-hat이 자신의 3배가 되게 움직여도 아직 x축 위에 있는 것을 볼 수 있습니다 또,",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "게다가 선형 변환이 작동하는 방식으로 인해 x축의 다른 벡터도 3배만큼 늘어나서 자체 범위를 유지합니다.",
  "model": "google_nmt",
  "from_community_srt": "선형변환이 이루어지는 방식 때문에 x축 위의 다른 벡터도 3배로 늘어나고 자신의 span에 남아있습니다 이 변환 과정에 span이 변하지 않는 또다른 벡터는 바로 [-1,",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "이 변환 중에 자체 범위에 남아 있는 약간 더 교묘한 벡터는 음수 1, 1입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "결국 2배로 늘어나게 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "그리고 다시 선형성은 이 사람이 가로지르는 대각선의 다른 벡터가 2배만큼 늘어나게 된다는 것을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "1]입니다 이 벡터는 두 배로 늘어나게 됩니다 그리고 선형성(linearity)에 의해 이 대각선 위에 있는 다른 어떤 벡터도 2배로 늘어나게 될 것이라는 것을 알 수 있습니다",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "그리고 이 변환의 경우, 그것들은 범위를 유지하는 특별한 속성을 가진 모든 벡터입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "x축에 있는 것들은 3배로 늘어나고, 이 대각선에 있는 것들은 2배로 늘어납니다.",
  "model": "google_nmt",
  "from_community_srt": "그리고 이 변형에서 이것들은 span을 유지하는 특별한 성질을 가진 모든 벡터입니다 x축 상의 벡터들은 세 배로 늘어나고 대각선 상의 벡터들은 두 배로 늘어납니다 변환 도중에 다른 벡터들은 어떤 식으로든 회전이 되어",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "다른 모든 벡터는 변환 중에 어느 정도 회전하여 해당 선을 벗어나게 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "지금쯤 추측할 수 있듯이 이러한 특수 벡터를 변환의 고유 벡터라고 하며 각 고유 벡터는 고유값이라고 불리는 것과 연관되어 있습니다. 이는 변환 중에 늘어나거나 찌그러지는 요소일 뿐입니다.",
  "model": "google_nmt",
  "from_community_srt": "스팬하는 선 위를 벗어날 것입니다 지금쯤 당신이 짐작하고 있는 대로 이 특별한 벡터들은 변환의 \"고유벡터(eigenvectors)\"라고 불립니다 그리고 각 고유벡터들은 \"고유값(eigenvalue)\"라고 불리는 값들을 가지고 있습니다 이것은 변환 도중 늘어나고 줄어드는 정도의 배수에 불과합니다 당연하게도,",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "물론, 스트레칭과 스퀴싱에 대해 특별한 것은 없으며 이러한 고유값이 양수라는 사실도 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "늘어나는 것 vs. 줄어드는 것에 특별한 것은 없습니다 고유값이 양수가 된다는 사실도 마찬가지입니다 다른 예시에서는,",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "또 다른 예로, 고유값이 1/2인 고유벡터가 있을 수 있습니다. 이는 벡터가 1/2만큼 뒤집히고 찌그러진다는 의미입니다.",
  "model": "google_nmt",
  "from_community_srt": "고유값 -1/2를 가진 고유벡터가 주어질 수 있습니다 이는 벡터가 (음의 부호로 인해) 뒤집어지고 1/2 만큼 줄어든다는 것을 의미합니다 하지만 여기에서 중요한 것은",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "하지만 여기서 중요한 점은 회전하지 않고 뻗어나가는 선에 머물러 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "이것이 생각하기에 유용한 이유를 엿볼 수 있도록 3차원 회전을 고려해보세요.",
  "model": "google_nmt",
  "from_community_srt": "벡터가 스팬하는 선 위에 머무르고 회전해서 벗어나는 일이 없다는 것입니다 왜 이것이 중요한지 생각해 보기 위해 삼차원에서의 회전을 생각해 봅시다 만약 당신이 이 회전에서의 고유벡터,",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "해당 회전에 대한 고유벡터(자체 범위에 남아 있는 벡터)를 찾을 수 있다면 회전축을 찾은 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "그리고 해당 변환과 관련된 전체 3x3 행렬에 대해 생각하는 것보다 일부 회전 축과 회전 각도 측면에서 3D 회전을 생각하는 것이 훨씬 쉽습니다.",
  "model": "google_nmt",
  "from_community_srt": "자신의 span에 남아있는 벡터를 찾을 수 있다면 그것이 바로 회전축입니다 그리고 3D회전을 생각할 때는 몇 개의 회전축과 회전하게 되는 각을 가지고 생각하는 것이 훨씬 편합니다",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "그런데 이 경우 해당 고유값은 1이어야 합니다. 왜냐하면 회전은 아무것도 늘어나거나 찌그러지지 않으므로 벡터의 길이는 동일하게 유지되기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 변환과 관련된 3x3 행렬 전체를 생각하는 것보단 말이죠 이 경우에는 회전이 그 무엇도 늘이거나 줄이지 않으니 고유값은 1이 될 것입니다 벡터의 길이가 변하지 않는다는 것이죠",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "이 패턴은 선형대수학에서 많이 나타납니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "행렬로 설명되는 모든 선형 변환을 사용하면 이 행렬의 열을 기저 벡터의 착지 지점으로 읽어서 무엇을 하는지 이해할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 유형은 행렬로 묘사된 선형변환에서 많이 등장합니다 이 행렬의 열을 기저 벡터의 위치로 읽어내릴 수 있다면 무엇이 일어나는지 알 수 있습니다 하지만 선형변환 중 일어나는 일에 대해",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "그러나 특정 좌표계에 덜 의존하면서 선형 변환이 실제로 수행하는 작업의 핵심을 파악하는 더 좋은 방법은 고유벡터와 고유값을 찾는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "확실하게 느낌을 받으려면 특정 좌표계에 덜 관계있는 \"고유벡터(eigenvectors)\"와 \"고유값(eigenvalues)\"를 찾아야 합니다 고유벡터와 고유값을 계산하는 방법에 대해",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "여기서는 고유벡터와 고유값을 계산하는 방법에 대해 자세히 다루지는 않지만 개념적 이해에 가장 중요한 계산 아이디어에 대한 개요를 제공하려고 노력할 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "자세히 설명하지는 않을  것이지만 개념의 이해에 중요한 부분만 훑어 보려고 합니다 상징적으로,",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "상징적으로 고유벡터의 아이디어는 다음과 같습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A는 v가 고유벡터인 일부 변환을 나타내는 행렬이고, 람다는 숫자, 즉 해당 고유값입니다.",
  "model": "google_nmt",
  "from_community_srt": "\"고유벡터\"의 개념은 이렇게 생겼습니다 A는 임의의 변환을 나타내는 행렬이며 v는 고유벡터,",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "이 표현식이 말하는 것은 행렬-벡터 곱 A 곱하기 v가 고유벡터 v를 일부 값 람다로 스케일링하는 것과 동일한 결과를 제공한다는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "λ는 고유값인 상수입니다 이 표현이 나타내는 것은 행렬-벡터 곱셈인 Av가 고유벡터 v를 임의의 상수 λ로 스케일링 한 결과와 같다는 것입니다 따라서 행렬 A의 고유벡터와 고유값을 찾는 것은",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "따라서 행렬 A의 고유벡터와 고유값을 찾는 것은 이 표현식을 참으로 만드는 v와 람다의 값을 찾는 것으로 귀결됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "처음에는 작업하기가 약간 어색합니다. 왜냐하면 왼쪽은 행렬-벡터 곱셈을 나타내지만 오른쪽은 스칼라-벡터 곱셈을 나타내기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 표현을 참으로 만드는 v와 λ의 값을 찾는 것이 됩니다 왼쪽이 행렬-벡터 곱셈인 반면에 오른쪽은 스칼라-벡터 곱셈이여서 계산하기가 이상할 것입니다 그러니까 오른쪽을",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "그럼 우변을 일종의 행렬-벡터 곱셈으로 다시 작성하는 것부터 시작하겠습니다. 행렬을 사용하면 모든 벡터를 람다 배율로 스케일링하는 효과가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "그러한 행렬의 열은 각 기저 벡터에 어떤 일이 일어나는지 나타내며, 각 기저 벡터는 단순히 람다와 곱해집니다. 따라서 이 행렬의 대각선 아래 숫자는 람다이고 다른 곳은 모두 0입니다.",
  "model": "google_nmt",
  "from_community_srt": "임의의 벡터를 λ배로 스케일링 하는 행렬-벡터 곱으로 다시 쓰는 것부터 시작합시다 이러한 행렬의 열은 각 기저 벡터에 일어나는 변화를 나타내는데 여기선 단순히 λ배를 하므로",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "이 함수를 작성하는 일반적인 방법은 람다를 인수분해하여 람다 곱하기 i로 작성하는 것입니다. 여기서 i는 대각선 아래에 1이 있는 단위 행렬입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 행렬은 대각선 아래로 상수 λ를 가지고 다른 곳은 전부 0이 들어갑니다 좀 더 보편적인 방법은 λ를 묶어내서 λI로 쓰는 것입니다 I는 대각선에 1이 있는 항등 행렬입니다",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "양쪽 변이 행렬-벡터 곱셈처럼 보이면 우변을 빼고 v를 인수분해할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "이제 우리가 가진 것은 새로운 행렬입니다. A - 람다에 항등식을 곱한 것입니다. 그리고 우리는 이 새로운 행렬에 v를 곱하여 0 벡터를 제공하는 벡터 v를 찾고 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이제 양변이 모두 행렬-벡터 곱으로 나타내졌으니 양변을 빼서 v라는 인수로 묶어낼 수 있습니다 이제 새로운 행렬인 A-λI을 얻었으니 이 행렬과 곱해서 영벡터를 만드는 벡터 v를 찾아야 합니다",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "자, v 자체가 0 벡터라면 이것은 항상 참이 될 것입니다. 그러나 그것은 지루합니다.",
  "model": "google_nmt",
  "from_community_srt": "물론 v가 영벡터가 된다면 이 식은 항상 성립하겠지만,",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "우리가 원하는 것은 0이 아닌 고유벡터입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "그리고 5장과 6장을 보면 0이 아닌 벡터를 가진 행렬의 곱이 0이 되는 유일한 방법은 해당 행렬과 관련된 변환이 공간을 더 낮은 차원으로 압축하는 것이라는 것을 알게 될 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "재미가 없습니다 영이 아닌 고유벡터를 얻고 싶으니까요 만약 챕터 5와 6을 보셨다면 영벡터가 아닌 벡터와 행렬을 곱해서 0이 되게 만드는 방법은 행렬에 일어나는 변환이 낮은 차원으로 내리는 것이면 됩니다",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "그리고 그 찌그러짐은 행렬의 행렬식 0에 해당합니다.",
  "model": "google_nmt",
  "from_community_srt": "이 축소는 행렬이 영 행렬식(zero determinant)이 되게 합니다 정확히 하기 위해,",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "구체적으로 행렬 A에 열 2, 1과 2, 3이 있다고 가정하고 각 대각선 항목에서 가변 양인 람다를 빼는 것을 생각해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "어떤 행렬 A가 [2, 1]과 [2,",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "이제 람다를 조정하고 손잡이를 돌려 값을 변경하는 것을 상상해 보십시오.",
  "model": "google_nmt",
  "from_community_srt": "3]이라는 열을 가지고 변수 λ를 각 대각선의 값에서 빼는 경우를 생각해 봅시다 이제 λ의 값이 이리저리 바뀐다고 생각해 봅시다 λ의 값이 바뀌면,",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "람다 값이 변경되면 행렬 자체도 변경되므로 행렬의 행렬식도 변경됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "여기서 목표는 이 행렬식을 0으로 만드는 람다 값을 찾는 것입니다. 즉, 조정된 변환이 공간을 더 낮은 차원으로 압축한다는 의미입니다.",
  "model": "google_nmt",
  "from_community_srt": "행렬이 바뀌고 행렬식도 바뀌게 됩니다 이 행렬식을 0으로 만드는 λ값을 찾는 것은 이 공간의 차원을 낮추는 변환을 찾는 것입니다 이 경우에는 λ=1이 됩니다 당연히 다른 행렬이였다면",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "이 경우 최적 지점은 람다가 1일 때 발생합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "물론, 다른 행렬을 선택했다면 고유값이 반드시 1이 아닐 수도 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "최적의 지점은 람다의 다른 값에 도달할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "내용이 좀 많지만 이것이 무엇을 말하는지 풀어보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "고유값이 1이 될 필요는 없습니다 아마 λ는 다른 값을 가졌을 겁니다 약간 로또같긴 하지만 이것이 갖는 의미를 해석해 봅시다 λ=1일 때, 행렬 A-λI는 선이 됩니다",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "람다가 1이면 행렬 A에서 람다를 곱하고 항등식을 곱하여 공간을 선으로 압축합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "이는 A 마이너스 람다 곱하기 항등 시간 v가 0 벡터와 같은 0이 아닌 벡터 v가 있다는 것을 의미합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "그리고 우리가 그것에 관심을 갖는 이유는 A 곱하기 v가 람다 곱하기 v와 같다는 것을 의미하기 때문이라는 것을 기억하세요. 이를 통해 벡터 v는 A의 고유 벡터이며 변환 A 동안 자체 범위를 유지한다고 읽을 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "(A-λI)을 영벡터로 만드는 영벡터가 아닌 v벡터가 있다는 겁니다 지금 우리가 이걸 하고 있는 것은 Av=λv로 만드는 v 벡터가 변환 A에서 span이 변하지 않는",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "이 예에서 해당 고유값은 1이므로 v는 실제로 고정된 상태로 유지됩니다.",
  "model": "google_nmt",
  "from_community_srt": "고유벡터 v이기 때문에 찾고 있다는 것을 기억해 두십시요 이 예시에서 고유값은 1이 되어 v는 그대로 있습니다 잘 이해가 가지 않는다면 잠깐 멈춰서 생각해 보십시요",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "해당 추론 방식이 좋은지 확인해야 하는지 잠시 멈추고 숙고해 보세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "서문에서 언급한 내용이 바로 이런 내용입니다.",
  "model": "google_nmt",
  "from_community_srt": "초반에 말했던 것처럼,",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "행렬식을 확실히 이해하지 못하고 왜 행렬식이 0이 아닌 해를 갖는 선형 방정식 시스템과 관련되어 있는지 알지 못한다면 이와 같은 표현은 전혀 예상치 못한 일처럼 느껴질 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "행렬식(determinants)과 영이 아닌 해를 가지는 선형 방정식계(linear systems of equations)가 어떤 관계가 있는지를 정확히 알지 못한다면",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "이것이 실제로 작동하는 모습을 보려면 열이 3, 0 및 1, 2인 행렬을 사용하여 처음부터 예제를 다시 살펴보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "이러한 표현이 전혀 이해가 되지 않을 것입니다 영상으로 보기 위해 처음부터 예시를 다시 봅시다 [3, 0]와 [1,",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "람다 값이 고유값인지 확인하려면 이 행렬의 대각선에서 이를 빼고 행렬식을 계산하세요.",
  "model": "google_nmt",
  "from_community_srt": "2]의 열을 가지는 행렬과 고유값이 되는 λ의 값을 찾기 위해 이 행렬의 대각선에서 빼서 행렬식을 계산합니다 이렇게 하면,",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "이렇게 하면 우리는 람다에서 3 - 람다 곱하기 2 - 람다라는 특정 이차 다항식을 얻습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "람다는 이 행렬식이 0인 경우에만 고유값이 될 수 있으므로 가능한 고유값은 람다가 2이고 람다가 3이라는 결론을 내릴 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "λ에 대한 이차식인 (3-λ)(2-λ)을 얻습니다 행렬식을 0으로 만드는 λ만이 고유값이 될 수 있으므로 가능한 고유값은 λ= 2와 λ=3 밖에 없다는 결론을 얻습니다",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "실제로 이러한 고유값 중 하나(예: 람다가 2)를 갖는 고유벡터가 무엇인지 알아내기 위해 해당 람다 값을 행렬에 연결한 다음 대각선으로 변경된 행렬이 0으로 보내는 벡터를 해결합니다.",
  "model": "google_nmt",
  "from_community_srt": "이 중 하나의 고유값을 가지는 고유벡터를 알아내기 위해 λ=2라고 가정합시다 λ의 값을 행렬에 집어넣어서 대각선이 바뀐 이 행렬이 [2,",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "다른 선형 시스템과 같은 방식으로 이를 계산하면 해는 -1, 1 범위의 대각선에 있는 모든 벡터라는 것을 알 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "0]를 어떻게 변환하는지 봅시다 다른 선형계에서 하듯이 이걸 계산하면 해가 [-1, 1]이 span하는 대각선 위에 있는 모든 벡터라는 사실을 알 수 있습니다 이건 원래 행렬 3,",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "이는 변경되지 않은 행렬 3, 0, 1, 2가 모든 벡터를 2배로 늘리는 효과가 있다는 사실에 해당합니다.",
  "model": "google_nmt",
  "from_community_srt": "0, 1,",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "이제 2D 변환에는 고유벡터가 필요하지 않습니다.",
  "model": "google_nmt",
  "from_community_srt": "2가 이 벡터들을 두 배로 늘린다는 사실과 일치합니다 2D 변환은 고유벡터가 무조건 존재하지 않습니다 예를 들어,",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "예를 들어 90도 회전을 가정해 보겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "이것은 자체 범위에서 모든 벡터를 회전시키기 때문에 고유 벡터가 없습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "실제로 이와 같이 회전의 고유값을 계산해 보면 어떤 일이 발생하는지 확인하세요.",
  "model": "google_nmt",
  "from_community_srt": "고유벡터가 없는 90°회전의 경우에는 모든 벡터가 자신의 span을 벗어납니다 이런 회전에서 고유벡터를 계산하려고 시도한다면 어떤 일이 일어나는지 봅시다 행렬의 열이 [0,",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "해당 행렬에는 열 0, 1과 음수 1, 0이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "1]과 [-1,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "대각선 요소에서 람다를 빼고 행렬식이 0이 되는 시점을 찾습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "이 경우 다항식 람다 제곱에 1을 더한 값을 얻습니다.",
  "model": "google_nmt",
  "from_community_srt": "0]이고 대각선 성분에서 λ를 빼고 행렬식이 0이 되는 경우를 찾아보면 다항식 λ^2+1을 얻게 되는데 이 다항식의 해는 허수 i와 -i 뿐입니다 실수해가 없다는 것은",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "해당 다항식의 유일한 근은 허수 i와 음수 i입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "실수 해가 없다는 사실은 고유벡터가 없다는 것을 나타냅니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "마음 속에 간직할 가치가 있는 또 다른 매우 흥미로운 예는 가위입니다.",
  "model": "google_nmt",
  "from_community_srt": "고유벡터가 없다는 것입니다 생각해 볼 만한 다른 재밌는 예시는 미는 것(shear)입니다 i-hat은 그대로 두고 j-hat은 1만큼 움직이면 행렬의 열이 [1,",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "그러면 i-hat이 제자리에 고정되고 j-hat 1이 위로 이동하므로 해당 행렬에는 열 1, 0과 1, 1이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "0]과 [1,",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "x축의 모든 벡터는 제자리에 고정되어 있으므로 고유값 1을 갖는 고유벡터입니다.",
  "model": "google_nmt",
  "from_community_srt": "1]이 됩니다 x축 위에 있는 모든 벡터들은 그 자리에 그대로 있기 때문에 고유값 1을 가지는 고유벡터가 됩니다 사실,",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "사실, 이것들은 유일한 고유벡터입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "대각선에서 람다를 빼고 행렬식을 계산하면 1 빼기 람다 제곱이 나옵니다.",
  "model": "google_nmt",
  "from_community_srt": "이 벡터들은 대각선에서 λ를 빼고 행렬식을 계산 했을 때 얻는 유일한 고유벡터입니다 1-λ^2이라는 식이 나오는데 이 식의 해는 λ=1 뿐입니다 우리가 기하학적으로 알아낸",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "그리고 이 표현식의 유일한 근은 람다가 1과 같다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "이는 우리가 기하학적으로 보는 것과 일치합니다. 모든 고유벡터는 고유값 1을 갖습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "그러나 고유값은 하나만 가질 수도 있지만 고유벡터로 가득 찬 선 이상을 가질 수도 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "모든 고유벡터는 고유값 1을 가진다는 것을 알아낸 사실과 일치합니다 하나의 고유값을 가지면서도 다양한 고유벡터들을 가질 수 있다는 것도 염두에 두십시요 간단한 예는 모든 것을 두배로 스케일하는 행렬인데,",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "간단한 예는 모든 것을 2로 확장하는 행렬입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "유일한 고유값은 2이지만 평면의 모든 벡터는 해당 고유값을 갖는 고유벡터가 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "이제 마지막 주제로 넘어가기 전에 잠시 멈춰서 이에 대해 생각해 볼 좋은 시간입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "지난 비디오의 아이디어에 크게 의존하는 고유기초 아이디어로 여기서 마무리하고 싶습니다.",
  "model": "google_nmt",
  "from_community_srt": "고유값은 2 뿐이지만 평면 상의 모든 벡터는 고유벡터가 됩니다 마지막 주제로 넘어가기 전에 멈춰서 생각할 시간을 가질 좋은 타이밍입니다 저번 영상의 개념들과 깊은 관계가 있는 개념인",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "우리의 기저 벡터가 우연히 고유 벡터가 된다면 무슨 일이 일어나는지 살펴보세요.",
  "model": "google_nmt",
  "from_community_srt": "\"고유기저(eigenbasis)\"로 마무리짓고 싶습니다 만약 기저벡터들이 고유벡터이기도 했다면 어떤 일이 일어나는지 봅시다 예를 들어,",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "예를 들어, i-hat은 -1로 스케일링되고 j-hat은 2로 스케일링될 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "i-hat은 1만큼,",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "새로운 좌표를 행렬의 열로 작성하면 i-hat과 j-hat의 고유값인 음수 1과 2의 스칼라 배수가 행렬의 대각선에 있고 다른 모든 항목은 0이라는 점에 유의하세요. .",
  "model": "google_nmt",
  "from_community_srt": "j-hat은 2만큼 스케일 되었다고 합시다 새로운 좌표로 행렬의 열을 써내면 i-hat과 j-hat의 고유값인 스칼라 곱 -1 과 2가 행렬의 대각선에 있고 다른 값들은 모두 0입니다",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "행렬의 대각선 이외의 모든 부분에서 0이 있을 때마다 이를 대각 행렬이라고 부르는 것이 합리적입니다.",
  "model": "google_nmt",
  "from_community_srt": "대각선 외에 모두 0인 행렬은 뻔하지만, \"대각선 행렬(diagonal matrix)\"라고 불립니다",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "그리고 이것을 해석하는 방법은 모든 기본 벡터가 고유 벡터이고 이 행렬의 대각선 항목이 고유값이라는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "대각 행렬을 작업하기 훨씬 더 좋게 만드는 많은 것들이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이걸 해석하면 모든 기저벡터는 고유벡터이고 대각선의 값들은 고유값이 됩니다 대각선 행렬이 다루기 좋은 이유 중 하나는 만약 이 행렬을 아주 많이 스스로 곱한다면 무엇이 일어날지 계산하기 쉽다는 것입니다",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "한 가지 큰 점은 이 행렬 자체를 여러 번 곱하면 어떤 일이 일어날지 계산하는 것이 더 쉽다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "이러한 행렬 중 하나는 각 기본 벡터를 일부 고유값만큼 스케일링하므로 해당 행렬을 여러 번 적용하는 것(가령 100번)은 각 기본 벡터를 해당 고유값의 100승으로 스케일링하는 것과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 행렬이 하는 것은 각 기저 벡터를 특정한 고유값으로 스케일 하는 것이니까 말입니다 이 행렬을 100번정도 적용하는 것은 기저 벡터들을 고유값의 100제곱으로 스케일링 하는 것과 동일합니다",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "이와 대조적으로, 비대각선 행렬의 100제곱을 계산해 보십시오.",
  "model": "google_nmt",
  "from_community_srt": "대조적으로, 대각선 행렬이 아닌 것을 100번 곱한 것을 계산하는 걸 시도해 보십시오 진심으로,",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "정말로, 한번 시도해 보세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "악몽이다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "물론, 기본 벡터가 고유벡터가 될 정도로 운이 좋은 경우는 거의 없습니다.",
  "model": "google_nmt",
  "from_community_srt": "한번 해 보세요 악몽입니다! 당연히,",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "그러나 변환에 이 비디오의 시작 부분과 같이 전체 공간에 걸쳐 있는 집합을 선택할 수 있을 만큼 고유벡터가 많은 경우 이러한 고유벡터가 기본 벡터가 되도록 좌표계를 변경할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "기저 벡터가 고유벡터가 되는 상황은 아주 운이 좋은 상황일 것입니다 하지만 만약 변환이 앞서 했던 것과 같이 전체 공간을 span하는 세트를 고를 수 있을 만큼 고유벡터들을 많이 가지고 있다면",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "지난 영상에서 기저 변경에 대해 이야기했지만, 여기서는 현재 좌표계에 쓰여진 변환을 다른 시스템으로 표현하는 방법에 대해 매우 빠르게 설명하겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "고유벡터가 기저벡터가 되도록 좌표계를 바꿀 수 있습니다 저번 영상에서 기저를 바꾸는 것을 설명했지만 쓰고 있는 좌표계를 바꿔 기술할 때 변환을 어떻게 표현하는 지에 대한",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "새 기저로 사용하려는 벡터의 좌표(이 경우에는 두 개의 고유 벡터를 의미)를 선택한 다음 해당 좌표를 기저 행렬의 변경이라고 알려진 행렬의 열로 만듭니다.",
  "model": "google_nmt",
  "from_community_srt": "아주 빠른 복습을 해봅시다 새로운 기저로 쓰고 싶은 벡터의 좌표를 가져와 (여기선 두 고유벡터가 됩니다) 행렬의 열을 구성합니다 이게 기저 행렬을 바꾸는 것입니다 바뀐 기저 행렬을 오른쪽에 두고",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "원래 변환을 끼우고 기본 행렬의 변경 사항을 오른쪽에 배치하고 기본 행렬 변경의 역수를 왼쪽에 배치하면 결과는 동일한 변환을 나타내는 행렬이 되지만 새 기본 벡터 좌표의 관점에서 보면 체계.",
  "model": "google_nmt",
  "from_community_srt": "왼쪽에 바뀐 기저 행렬의 역을 두고 원래 변환을 사이에 끼면 같은 변환을 나타내지만 다른 기저 벡터 좌표계를 쓰는 행렬을 얻습니다 이걸 고유벡터로 하는 이유는 새로운 행렬이 대각선에 고유값들을 가지는",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "고유벡터를 사용하여 이 작업을 수행하는 요점은 이 새로운 행렬이 해당 대각선 아래에 해당하는 고유값과 함께 대각선이 되도록 보장된다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "이는 기본 벡터에 발생하는 일이 변환 중에 크기가 조정되는 좌표계에서의 작업을 나타내기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "대각선 행렬이 되게 하기 위해서입니다 이제 기저 벡터가 변환될 때 스케일 되기만 하는 좌표계에서 작업할 수 있습니다 기저벡터이기도 한 고유벡터의 쌍은 \"고유 기저(eigenbasis)\"라고 불립니다",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "고유벡터이기도 한 기저 벡터 세트를 다시 고유기저라고 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "따라서 예를 들어 이 행렬의 100제곱을 계산해야 하는 경우 고유기저로 변경하고 해당 시스템에서 100제곱을 계산한 다음 표준 시스템으로 다시 변환하는 것이 훨씬 쉬울 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "만약 이 행렬의 100제곱을 계산해야 한다면 고유 기저로 바꾼 후 100제곱을 계산하고 다시 원래의 계로 전환하는 것이 훨씬 쉬울 것입니다 모든 변환에서 이게 되진 않습니다",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "모든 변환에 대해 이 작업을 수행할 수는 없습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "예를 들어 전단에는 전체 공간을 포괄할 만큼 고유벡터가 충분하지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "그러나 고유기저를 찾을 수 있다면 행렬 연산이 정말 멋질 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "미는 것(shear)은 모든 공간을 span하는 고유벡터가 없습니다 하지만 고유기저를 찾을 수 있다면 행렬 계산이 아주 쉬워집니다 이걸 활용해서 놀라운 결과를 내는 일을",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "이것이 실제로 어떻게 보이는지, 그리고 이것이 놀라운 결과를 생성하는 데 어떻게 사용될 수 있는지 알아보기 위해 매우 깔끔한 퍼즐을 풀고자 하는 분들을 위해 여기 화면에 프롬프트를 남겨 두겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "직접 해보고 싶은 사람들을 위해 괜찮은 퀴즈를 준비했습니다 문제를 화면에 띄워 놓겠습니다 약간 힘들 수 있지만 재밌을 거라고 생각합니다 이 시리즈의 다음이자 마지막 영상은",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "약간의 노력이 필요하지만, 즐기시면 될 것 같아요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "이 시리즈의 다음이자 마지막 비디오는 추상적인 벡터 공간에 관한 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "\"추상 벡터 공간(abstract vector spaces)\"에 대한 것입니다 다음에 봐요!",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]