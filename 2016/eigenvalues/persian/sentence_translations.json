[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive. ",
  "translatedText": "بردارهای ویژه و مقادیر ویژه یکی از آن موضوعاتی است که بسیاری از دانش آموزان آن را به طور خاص غیر شهودی می دانند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Things like, why are we doing this, and what does this actually mean, are too often left just floating away in an unanswered sea of computations. ",
  "translatedText": "چیزهایی مانند، چرا ما این کار را انجام می دهیم، و این در واقع چه معنایی دارد، اغلب در دریایی از محاسبات بی پاسخ مانده اند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular. ",
  "translatedText": "و همانطور که من ویدیوهای این مجموعه را منتشر کرده ام، بسیاری از شما در مورد اینکه مشتاقانه منتظر تجسم این موضوع هستید نظر داده اید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained. ",
  "translatedText": "من گمان می کنم که دلیل این امر آنقدرها این نیست که چیزهای خاص پیچیده یا ضعیف توضیح داده شده باشند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it. ",
  "translatedText": "در واقع، این نسبتاً ساده است، و من فکر می‌کنم که اکثر کتاب‌ها در توضیح آن به خوبی کار می‌کنند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "What I want to do is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it. ",
  "translatedText": "کاری که من می‌خواهم انجام دهم این است که تنها زمانی واقعاً منطقی به نظر می‌رسد که درک بصری محکمی برای بسیاری از موضوعات قبل از آن داشته باشید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis. ",
  "translatedText": "مهم‌تر از همه در اینجا این است که می‌دانید چگونه به ماتریس‌ها به‌عنوان تبدیل‌های خطی فکر کنید، اما همچنین باید با چیزهایی مانند تعیین‌کننده‌ها، سیستم‌های معادلات خطی و تغییر مبنا راحت باشید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves. ",
  "translatedText": "سردرگمی در مورد مواد ویژه معمولاً بیشتر به یک پایه متزلزل در یکی از این موضوعات مربوط می شود تا با خود بردارها و مقادیر ویژه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here. ",
  "translatedText": "برای شروع، مقداری تبدیل خطی را در دو بعد در نظر بگیرید، مانند آنچه در اینجا نشان داده شده است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2. ",
  "translatedText": "بردار پایه i-hat را به مختصات 3، 0 و j-hat را به 1، 2 منتقل می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2. ",
  "translatedText": "بنابراین با ماتریسی نشان داده می شود که ستون های آن 3، 0، و 1، 2 هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip. ",
  "translatedText": "روی کاری که با یک بردار خاص انجام می دهد تمرکز کنید و در مورد گستره آن بردار، خطی که از مبدا و نوک آن می گذرد فکر کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation. ",
  "translatedText": "بیشتر بردارها در طول تبدیل از دهانه خود حذف می شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line. ",
  "translatedText": "منظورم این است که اگر مکانی که بردار در آن فرود آمده نیز جایی در آن خط باشد، کاملاً تصادفی به نظر می رسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar. ",
  "translatedText": "اما برخی از بردارهای خاص در گستره خود باقی می مانند، به این معنی که اثری که ماتریس بر روی چنین برداری می گذارد، فقط کشش یا له کردن آن است، مانند یک اسکالر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector. ",
  "translatedText": "برای این مثال خاص، بردار پایه i-hat یکی از این بردارهای خاص است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis. ",
  "translatedText": "دهانه i-hat محور x است، و از ستون اول ماتریس، می‌توانیم ببینیم که i-hat به 3 برابر خودش حرکت می‌کند، همچنان روی آن محور x. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span. ",
  "translatedText": "علاوه بر این، به دلیل نحوه عملکرد تبدیل های خطی، هر بردار دیگری در محور x نیز فقط با ضریب 3 کشیده می شود و از این رو در دهانه خود باقی می ماند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1. ",
  "translatedText": "یک بردار کمی زیرک تر که در طول این تبدیل در دهانه خود باقی می ماند، منفی 1، 1 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2. ",
  "translatedText": "در نهایت با ضریب 2 کشیده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2. ",
  "translatedText": "و دوباره، خطی بودن به این معنی است که هر بردار دیگری در خط مورب که توسط این شخص پوشیده شده است فقط با ضریب 2 کشیده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span. ",
  "translatedText": "و برای این تبدیل، همه آن ها بردارهایی هستند که دارای این خاصیت ویژه ماندن در دهانه خود هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2. ",
  "translatedText": "آنهایی که در محور x قرار دارند با ضریب 3 کشیده می شوند و آنهایی که در این خط مورب هستند با ضریب 2 کشیده می شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans. ",
  "translatedText": "هر بردار دیگری قرار است در طول تبدیل تا حدودی بچرخد و خطی که در آن قرار دارد حذف شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation. ",
  "translatedText": "همانطور که تا به حال حدس زده اید، این بردارهای ویژه بردارهای ویژه تبدیل نامیده می شوند و هر بردار ویژه با آن چیزی که یک مقدار ویژه نامیده می شود، مرتبط کرده است، که فقط عاملی است که توسط آن در طول تبدیل کشیده یا فشرده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing or the fact that these eigenvalues happen to be positive. ",
  "translatedText": "البته، چیز خاصی در مورد کشش در مقابل له کردن یا مثبت بودن این مقادیر ویژه وجود ندارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half. ",
  "translatedText": "در مثالی دیگر، می‌توانید یک بردار ویژه با مقدار ویژه منفی 1 نصف داشته باشید، به این معنی که بردار با ضریب 1 نصف برگردانده و فشرده می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it. ",
  "translatedText": "اما بخش مهم در اینجا این است که روی خطی می ماند که از آن خارج می شود بدون اینکه از آن بچرخد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation. ",
  "translatedText": "برای درک اجمالی از اینکه چرا این ممکن است یک چیز مفید برای فکر کردن باشد، چرخش سه بعدی را در نظر بگیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation. ",
  "translatedText": "اگر بتوانید یک بردار ویژه برای آن چرخش بیابید، برداری که در گستره خودش باقی می ماند، چیزی که پیدا کردید محور چرخش است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3 by 3 matrix associated with that transformation. ",
  "translatedText": "و بسیار ساده تر است که در مورد چرخش سه بعدی از نظر برخی از محورهای چرخش و زاویه ای که در آن می چرخد فکر کنیم، به جای اینکه به ماتریس کامل 3 در 3 مرتبط با آن تبدیل فکر کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same. ",
  "translatedText": "در این مورد، به هر حال، مقدار ویژه مربوطه باید 1 باشد، زیرا چرخش ها هرگز چیزی را کش نمی دهند یا له نمی کنند، بنابراین طول بردار ثابت می ماند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra. ",
  "translatedText": "این الگو در جبر خطی بسیار خود را نشان می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors. ",
  "translatedText": "با هر تبدیل خطی توصیف شده توسط یک ماتریس، می‌توانید با خواندن ستون‌های این ماتریس به عنوان نقاط فرود برای بردارهای پایه، بفهمید که چه کاری انجام می‌دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues. ",
  "translatedText": "اما اغلب، یک راه بهتر برای دستیابی به قلب آنچه که تبدیل خطی در واقع انجام می دهد، کمتر به سیستم مختصات خاص شما وابسته است، یافتن بردارهای ویژه و مقادیر ویژه است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding. ",
  "translatedText": "من جزئیات کامل در مورد روش‌های محاسبه بردارهای ویژه و مقادیر ویژه را در اینجا پوشش نمی‌دهم، اما سعی می‌کنم یک نمای کلی از ایده‌های محاسباتی که برای درک مفهومی مهم هستند ارائه دهم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like. ",
  "translatedText": "به طور نمادین، در اینجا ایده یک بردار ویژه به نظر می رسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue. ",
  "translatedText": "A ماتریسی است که مقداری تبدیل را نشان می دهد، با v به عنوان بردار ویژه، و لامبدا یک عدد است، یعنی مقدار ویژه مربوطه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda. ",
  "translatedText": "چیزی که این عبارت می گوید این است که حاصلضرب ماتریس-بردار، A ضربدر v، همان نتیجه ای را به دست می دهد که فقط بردار ویژه v را با مقداری لامبدا مقیاس بندی می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true. ",
  "translatedText": "بنابراین، یافتن بردارهای ویژه و مقادیر ویژه آنها در ماتریس A به یافتن مقادیر v و lambda که این عبارت را درست می‌کنند، ختم می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication. ",
  "translatedText": "در ابتدا کار با آن کمی ناخوشایند است زیرا آن سمت چپ نشان دهنده ضرب ماتریس-بردار است، اما سمت راست در اینجا ضرب اسکالر-بردار است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda. ",
  "translatedText": "پس بیایید با بازنویسی آن سمت راست به عنوان نوعی ضرب ماتریس-بردار، با استفاده از ماتریسی شروع کنیم که اثر مقیاس گذاری هر بردار را با ضریب لامبدا دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else. ",
  "translatedText": "ستون‌های چنین ماتریسی نشان‌دهنده اتفاقی است که برای هر بردار پایه می‌افتد، و هر بردار پایه به سادگی در لامبدا ضرب می‌شود، بنابراین این ماتریس دارای عدد لامبدا در پایین مورب خواهد بود و در هر جای دیگر صفر خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with ones down the diagonal. ",
  "translatedText": "روش متداول برای نوشتن این مرد این است که آن لامبدا را فاکتور کنید و آن را به صورت لامبدا ضربدر i بنویسید، جایی که i ماتریس هویت است با مواردی که در مورب پایین هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v. ",
  "translatedText": "وقتی هر دو طرف شبیه ضرب ماتریس-بردار هستند، می‌توانیم آن سمت راست را کم کنیم و v را فاکتور کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix, times v, gives the zero vector. ",
  "translatedText": "بنابراین آنچه که اکنون داریم یک ماتریس جدید است، A منهای لامبدا ضربدر هویت، و ما به دنبال بردار v هستیم، به طوری که این ماتریس جدید، ضربدر v، بردار صفر را به دست دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring. ",
  "translatedText": "اکنون، اگر v خود بردار صفر باشد، این همیشه درست خواهد بود، اما این خسته کننده است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector. ",
  "translatedText": "آنچه ما می خواهیم یک بردار ویژه غیر صفر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension. ",
  "translatedText": "و اگر فصل 5 و 6 را تماشا کنید، می‌دانید که تنها راهی که می‌توان حاصل ضرب یک ماتریس با بردار غیرصفر صفر شود، این است که تبدیل مرتبط با آن ماتریس، فضا را به بعد پایین‌تر منقبض کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix. ",
  "translatedText": "و این انقباض با یک تعیین کننده صفر برای ماتریس مطابقت دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry. ",
  "translatedText": "برای مشخص بودن، فرض کنید ماتریس A شما دارای ستون‌های 2، 1 و 2، 3 است و به این فکر کنید که مقدار متغیر لامبدا را از هر ورودی مورب کم کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value. ",
  "translatedText": "حالا تصور کنید لامبدا را تغییر دهید، یک دستگیره را بچرخانید تا مقدار آن را تغییر دهید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes. ",
  "translatedText": "با تغییر مقدار لامبدا، خود ماتریس تغییر می کند و بنابراین تعیین کننده ماتریس تغییر می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension. ",
  "translatedText": "هدف در اینجا یافتن مقداری از لامبدا است که این تعیین کننده را صفر می کند، به این معنی که تبدیل بهینه شده فضا را به بعد پایین تر می کشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1. ",
  "translatedText": "در این مورد، نقطه شیرین زمانی می آید که لامبدا برابر با 1 باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1. ",
  "translatedText": "البته، اگر ماتریس دیگری را انتخاب کرده بودیم، مقدار ویژه ممکن است لزوماً 1 نباشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda. ",
  "translatedText": "نقطه شیرین ممکن است به مقدار دیگری از لامبدا برخورد کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying. ",
  "translatedText": "بنابراین این مقدار زیادی است، اما بیایید بفهمیم که این چه می گوید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line. ",
  "translatedText": "وقتی لامبدا برابر با 1 است، ماتریس A منهای لامبدا ضربدر هویت، فضا را روی یک خط فشار می‌دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector. ",
  "translatedText": "این بدان معناست که یک بردار غیرصفر v وجود دارد به طوری که A منهای لامبدا ضربدر هویت ضربدر v برابر با بردار صفر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A. ",
  "translatedText": "و به یاد داشته باشید، دلیل اهمیت دادن ما به آن این است که به این معنی است که A ضربدر v برابر با لامبدا ضربدر v است، که می‌توانید آن را به این صورت بخوانید که بردار v بردار ویژه A است و در طول تبدیل A در بازه خود باقی می‌ماند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place. ",
  "translatedText": "در این مثال، مقدار ویژه مربوطه 1 است، بنابراین v در واقع در جای خود ثابت می ماند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good. ",
  "translatedText": "اگر لازم است مطمئن شوید که آن خط استدلال احساس خوبی دارد، مکث کنید و فکر کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction. ",
  "translatedText": "این همان چیزی است که در مقدمه به آن اشاره کردم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue. ",
  "translatedText": "اگر درک کاملی از تعیین کننده ها و دلیل ارتباط آنها با سیستم های معادلات خطی با راه حل های غیر صفر نداشتید، عبارتی مانند این کاملاً غیرمعمول به نظر می رسید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2. ",
  "translatedText": "برای مشاهده عملی این، بیایید مثال را از ابتدا با ماتریسی که ستون های آن 3، 0 و 1، 2 هستند، مرور کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant. ",
  "translatedText": "برای اینکه بفهمید یک مقدار لامبدا یک مقدار ویژه است یا خیر، آن را از قطرهای این ماتریس کم کنید و تعیین کننده را محاسبه کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda. ",
  "translatedText": "با انجام این کار، یک چند جمله ای درجه دوم معین در لامبدا به دست می آوریم، 3 منهای لامبدا ضربدر 2 منهای لامبدا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3. ",
  "translatedText": "از آنجایی که لامبدا فقط زمانی می تواند یک مقدار ویژه باشد که این تعیین کننده صفر باشد، می توانید نتیجه بگیرید که تنها مقادیر ویژه ممکن لامبدا برابر 2 و لامبدا برابر با 3 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero. ",
  "translatedText": "برای اینکه بفهمید بردارهای ویژه کدامند که در واقع یکی از این مقادیر ویژه را دارند، مثلاً لامبدا برابر با 2 است، آن مقدار لامبدا را به ماتریس متصل کنید و سپس حل کنید که این ماتریس تغییر یافته مورب برای کدام بردارها به صفر می‌فرستد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1. ",
  "translatedText": "اگر این را به روشی که هر سیستم خطی دیگری محاسبه می‌کنید، محاسبه می‌کنید، می‌بینید که جواب‌ها همه بردارهای روی خط مورب هستند که در امتداد منفی 1، 1 قرار دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2. ",
  "translatedText": "این با این واقعیت مطابقت دارد که ماتریس بدون تغییر، 3، 0، 1، 2، اثر کشش همه آن بردارها را با ضریب 2 دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors. ",
  "translatedText": "اکنون، یک تبدیل دوبعدی نیازی به داشتن بردارهای ویژه ندارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees. ",
  "translatedText": "به عنوان مثال، چرخش 90 درجه را در نظر بگیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span. ",
  "translatedText": "این هیچ بردار ویژه ای ندارد زیرا هر بردار را خارج از دهانه خود می چرخاند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens. ",
  "translatedText": "اگر واقعاً سعی می کنید مقادیر ویژه یک چرخش مانند این را محاسبه کنید، توجه کنید که چه اتفاقی می افتد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0. ",
  "translatedText": "ماتریس آن دارای ستون های 0، 1 و منفی 1، 0 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero. ",
  "translatedText": "لامبدا را از عناصر مورب کم کنید و به دنبال زمانی باشید که تعیین کننده صفر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1. ",
  "translatedText": "در این حالت، چند جمله ای لامبدا به اضافه 1 را دریافت می کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i. ",
  "translatedText": "تنها ریشه های آن چند جمله ای اعداد خیالی i و منفی i هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors. ",
  "translatedText": "این واقعیت که جواب اعداد واقعی وجود ندارد نشان می دهد که بردار ویژه وجود ندارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear. ",
  "translatedText": "مثال جالب دیگری که ارزش نگه داشتن در پشت ذهن شما را دارد، برش است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1. ",
  "translatedText": "این i-hat را در جای خود ثابت می کند و j-hat 1 را به سمت بالا می برد، بنابراین ماتریس آن دارای ستون های 1، 0 و 1، 1 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place. ",
  "translatedText": "همه بردارهای روی محور x بردارهای ویژه با مقدار ویژه 1 هستند زیرا در جای خود ثابت می مانند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors. ",
  "translatedText": "در واقع، اینها تنها بردارهای ویژه هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared. ",
  "translatedText": "وقتی لامبدا را از قطرها کم می کنید و تعیین کننده را محاسبه می کنید، مقدار 1 منهای لامبدا به دست می آید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1. ",
  "translatedText": "و تنها ریشه این عبارت لامبدا برابر با 1 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1. ",
  "translatedText": "این با آنچه از نظر هندسی می بینیم مطابقت دارد، که همه بردارهای ویژه دارای مقدار ویژه 1 هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors. ",
  "translatedText": "البته به خاطر داشته باشید، همچنین ممکن است فقط یک مقدار ویژه داشته باشید، اما با بیش از یک خط پر از بردارهای ویژه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2. ",
  "translatedText": "یک مثال ساده ماتریسی است که همه چیز را 2 مقیاس می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue. ",
  "translatedText": "تنها مقدار ویژه 2 است، اما هر بردار در صفحه یک بردار ویژه با آن مقدار ویژه است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic. ",
  "translatedText": "اکنون زمان مناسب دیگری است که قبل از اینکه به موضوع آخر بروم، کمی مکث و تامل کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video. ",
  "translatedText": "من می خواهم اینجا را با ایده یک eigenbasis به پایان برسانم، که به شدت بر ایده های ویدیوی آخر متکی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors. ",
  "translatedText": "به این نگاه کنید که اگر بردارهای پایه ما بردارهای ویژه باشند چه اتفاقی می افتد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1, and j-hat is scaled by 2. ",
  "translatedText": "به عنوان مثال، ممکن است i-hat با منفی 1 و j-hat با 2 مقیاس شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0. ",
  "translatedText": "با نوشتن مختصات جدید آنها به عنوان ستون های یک ماتریس، توجه کنید که آن مضرب های اسکالر، منفی 1 و 2، که مقادیر ویژه i-hat و j-hat هستند، روی قطر ماتریس ما قرار دارند و هر ورودی دیگر 0 است. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has 0s everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix, and the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues. ",
  "translatedText": "هر زمانی که یک ماتریس در هر جایی غیر از قطر 0s داشته باشد، به اندازه کافی منطقی، ماتریس مورب نامیده می شود، و روش تفسیر این است که همه بردارهای پایه بردارهای ویژه هستند، با ورودی های قطری این ماتریس مقادیر ویژه آنها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 818.88,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with. ",
  "translatedText": "چیزهای زیادی وجود دارد که کار با ماتریس های مورب را بسیار زیباتر می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times. ",
  "translatedText": "یکی از موارد مهم این است که محاسبه اینکه اگر این ماتریس را چندین بار در خودش ضرب کنید آسانتر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue. ",
  "translatedText": "از آنجایی که تنها کاری که این ماتریس ها انجام می دهند این است که هر بردار پایه را با مقدار ویژه ای مقیاس می کند، اعمال آن ماتریس چندین بار، مثلاً 100 بار، فقط با مقیاس گذاری هر بردار پایه با توان 100 مقدار ویژه مربوطه مطابقت دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix. ",
  "translatedText": "در مقابل، سعی کنید صدمین توان یک ماتریس غیر قطری را محاسبه کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment. ",
  "translatedText": "راستی یه لحظه امتحان کن ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare. ",
  "translatedText": "این یک کابوس است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors. ",
  "translatedText": "البته، به ندرت آنقدر خوش شانس خواهید بود که بردارهای پایه شما نیز بردارهای ویژه باشند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors. ",
  "translatedText": "اما اگر تبدیل شما دارای بردارهای ویژه زیادی است، مانند آنچه در ابتدای این ویدیو بود، به اندازه ای که بتوانید مجموعه ای را انتخاب کنید که فضای کامل را بپوشاند، می توانید سیستم مختصات خود را طوری تغییر دهید که این بردارهای ویژه بردارهای پایه شما باشند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system. ",
  "translatedText": "من در مورد تغییر مبنا ویدیوی گذشته صحبت کردم، اما در اینجا یک یادآوری فوق العاده سریع درباره نحوه بیان تبدیلی که در حال حاضر در سیستم مختصات ما نوشته شده است را به یک سیستم متفاوت بیان می کنم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix. ",
  "translatedText": "مختصات بردارهایی را که می خواهید به عنوان پایه جدید استفاده کنید، که در این مورد به معنای دو بردار ویژه ما است، در نظر بگیرید، سپس آن مختصات را به ستون های یک ماتریس تبدیل کنید که به عنوان ماتریس تغییر پایه شناخته می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system. ",
  "translatedText": "وقتی تبدیل اصلی را ساندویچ می‌کنید، تغییر ماتریس پایه را در سمت راست و معکوس تغییر ماتریس پایه را در سمت چپ آن قرار می‌دهید، نتیجه ماتریسی خواهد بود که همان تبدیل را نشان می‌دهد، اما از منظر بردارهای پایه جدید مختصات سیستم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal. ",
  "translatedText": "تمام هدف انجام این کار با بردارهای ویژه این است که این ماتریس جدید تضمین شده است که مورب با مقادیر ویژه متناظر آن در پایین آن قطر باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation. ",
  "translatedText": "این به این دلیل است که نشان‌دهنده کار در یک سیستم مختصات است که در آن اتفاقی که برای بردارهای پایه می‌افتد این است که در طول تبدیل مقیاس می‌شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis. ",
  "translatedText": "مجموعه ای از بردارهای پایه که بردارهای ویژه نیز هستند، باز هم به اندازه کافی منطقی، یک پایه ویژه نامیده می شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system. ",
  "translatedText": "بنابراین، برای مثال، اگر شما نیاز به محاسبه توان 100 این ماتریس داشته باشید، تغییر به یک پایه ویژه، محاسبه توان 100 در آن سیستم، و سپس تبدیل مجدد به سیستم استاندارد ما بسیار آسان تر خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations. ",
  "translatedText": "با همه دگرگونی ها نمی توانید این کار را انجام دهید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space. ",
  "translatedText": "برای مثال، یک برش، بردارهای ویژه کافی برای گسترش فضای کامل را ندارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely. ",
  "translatedText": "اما اگر بتوانید یک eigenbasis پیدا کنید، عملیات ماتریس را واقعا دوست‌داشتنی می‌کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen. ",
  "translatedText": "برای کسانی از شما که مایلند روی یک پازل زیبا کار کنند تا ببینند این در عمل چگونه به نظر می‌رسد و چگونه می‌توان از آن برای تولید نتایج شگفت‌انگیز استفاده کرد، من یک درخواست را اینجا روی صفحه می‌گذارم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it. ",
  "translatedText": "کمی کار می خواهد، اما فکر می کنم از آن لذت خواهید برد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces. ",
  "translatedText": "ویدیوی بعدی و نهایی این مجموعه قرار است بر روی فضاهای برداری انتزاعی باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1005.38
 },
 {
  "input": "See you then! ",
  "translatedText": "بعدا می بینمت! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.9,
  "end": 1006.12
 }
]