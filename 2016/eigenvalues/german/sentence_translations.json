[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Eigenvektoren und Eigenwerte gehören zu den Themen, die für viele Schüler besonders unintuitiv sind.",
  "model": "DeepL",
  "from_community_srt": "\"Eigenvektoren und Eigenwerte\" ist eines der Themen , das viele Studenten besonders unintuitiv finden.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Fragen wie \"Warum tun wir das?\" und \"Was bedeutet das eigentlich?\" bleiben allzu oft in einem Meer von unbeantworteten Berechnungen unbeantwortet.",
  "model": "DeepL",
  "from_community_srt": "Fragen wie \"warum machen wir das\" und \"was heißt das überhaupt\" gehen oft in einem Meer aus Berechnungen unter.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "Und während ich die Videos dieser Serie veröffentlicht habe, haben viele von euch gesagt, dass sie sich besonders auf die Visualisierung dieses Themas freuen.",
  "model": "DeepL",
  "from_community_srt": "Und als ich die Videos dieser Serie herausbrachte, haben viele von Euch kommentiert, sie würden sich auf die Visualisierung dieses Themas im Speziellen freuen.",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Ich vermute, dass der Grund dafür nicht so sehr darin liegt, dass Eigenthings besonders kompliziert oder schlecht erklärt sind.",
  "model": "DeepL",
  "from_community_srt": "Ich nehme an, dass der Grund dafür nicht darin liegt, dass \"Eigen\"-Dinge besonders kompliziert wären oder schlecht erklärt würden.",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "Eigentlich ist es vergleichsweise einfach, und ich denke, die meisten Bücher erklären es gut.",
  "model": "DeepL",
  "from_community_srt": "Tatsächlich sind sie vergleichbar einfach und ich denke, dass die meisten Bücher es gut erklären.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Das Problem ist, dass es nur dann wirklich Sinn macht, wenn du ein solides visuelles Verständnis für viele der vorangegangenen Themen hast.",
  "model": "DeepL",
  "from_community_srt": "Das Problem ist, dass es nur wirklich Sinn ergibt, wenn man ein solides visuelles Verständnis für vorhergehende Themen hat.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Am wichtigsten ist, dass du weißt, wie man Matrizen als lineare Transformationen betrachtet, aber du musst auch mit Dingen wie Determinanten, linearen Gleichungssystemen und Basiswechsel vertraut sein.",
  "model": "DeepL",
  "from_community_srt": "Am wichtigsten ist hier, dass man Matrizen als lineare Transformationen ansieht, aber man sollte auch mit Dingen wie Determinanten, linearen Gleichungssystemen und Basiswechsel vertraut sein.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Die Verwirrung über die Eigenwerte hat meist mehr mit einem wackeligen Fundament in einem dieser Themen zu tun als mit den Eigenvektoren und Eigenwerten selbst.",
  "model": "DeepL",
  "from_community_srt": "Verwirrung über \"Eigen\"-Dinge hat normalerweise mehr mit einem unsicheren Grundwissen in einem jener Themen zu tun, als mit Eigenvektoren und Eigenwerten selbst.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Betrachte zunächst eine lineare Transformation in zwei Dimensionen, wie sie hier gezeigt wird.",
  "model": "DeepL",
  "from_community_srt": "Zu Anfang sei eine lineare Transformation in zwei Dimensionen, wie die hier gezeigte.",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Sie verschiebt den Basisvektor i-hat auf die Koordinaten 3, 0 und j-hat auf 1, 2.",
  "model": "DeepL",
  "from_community_srt": "Sie bewegt den Basisvektor î zu den Koordinaten (3 | 0) und ĵ zu (1 | 2), und wird daher durch eine Matrix mit den Spalten (3 | 0) und (1 | 2) repräsentiert.",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Sie wird also mit einer Matrix dargestellt, deren Spalten 3, 0 und 1, 2 sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Konzentriere dich darauf, was es mit einem bestimmten Vektor macht, und denke über die Spannweite dieses Vektors nach, die Linie, die durch seinen Ursprung und seine Spitze verläuft.",
  "model": "DeepL",
  "from_community_srt": "Konzentrier Dich darauf, was sie mit genau diesem Vektor macht und denk über den Spann dieses Vektors nach, die Gerade die durch die Spitze des Vektors und den Ursprung verläuft.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "Die meisten Vektoren werden bei der Umwandlung aus ihrer Spanne gerissen.",
  "model": "DeepL",
  "from_community_srt": "Während der Transformation werden die meisten Vektoren aus ihrem Spann herausbewegt.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Ich meine, es wäre ein ziemlicher Zufall, wenn der Ort, an dem der Vektor gelandet ist, auch irgendwo auf dieser Linie liegen würde.",
  "model": "DeepL",
  "from_community_srt": "Es schiene doch sehr zufällig, wenn die Stelle, an der der Vektor landet, sich auch auf dieser Geraden befindet.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Einige spezielle Vektoren bleiben jedoch in ihrer eigenen Spannweite, d.h. die Wirkung der Matrix auf einen solchen Vektor besteht lediglich darin, ihn zu strecken oder zu stauchen, wie ein Skalar.",
  "model": "DeepL",
  "from_community_srt": "Aber ein paar spezielle Vektoren verbleiben auf ihrem Spann, bedeutet, dass die Matrix so einen Vektor nur streckt oder staucht, wie ein Skalar.",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Für dieses spezielle Beispiel ist der Basisvektor i-hat ein solcher spezieller Vektor.",
  "model": "DeepL",
  "from_community_srt": "Für dieses spezifische Beispiel, der Basisvektor î ist so ein spezieller Vektor.",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "Die Spannweite von i-hat ist die x-Achse, und aus der ersten Spalte der Matrix können wir ersehen, dass sich i-hat bis zum Dreifachen seiner selbst bewegt, immer noch auf der x-Achse.",
  "model": "DeepL",
  "from_community_srt": "Der Spann von î ist die x-Achse, und der ersten Spalte der Matrix können wir entnehmen, dass î sich um das Dreifache auf der x-Achse verlängert.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Außerdem wird jeder andere Vektor auf der x-Achse aufgrund der linearen Transformationen einfach um den Faktor 3 gestreckt und bleibt somit auf seiner eigenen Strecke.",
  "model": "DeepL",
  "from_community_srt": "Zudem wird aufgrund der Weise, wie Transformationen funktionieren, jeder andere Vektor auf der x-Achse um das Dreifache gestreckt wird und daher auf seinem Spann bleibt.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Ein etwas raffinierterer Vektor, der bei dieser Transformation auf seiner eigenen Spanne bleibt, ist negativ 1, 1.",
  "model": "DeepL",
  "from_community_srt": "Ein etwas unauffälligerer Vektor,",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Am Ende wird er um den Faktor 2 gestreckt.",
  "model": "DeepL",
  "from_community_srt": "der auf seinem Spann bleibt ist (-1 | 1), wird um einen Faktor von zwei gestreckt.",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "Und wieder bedeutet die Linearität, dass jeder andere Vektor auf der Diagonalen, die dieser Typ aufspannt, einfach um den Faktor 2 gestreckt wird.",
  "model": "DeepL",
  "from_community_srt": "Und erneut impliziert die Linearität, dass jeder andere Vektor auf der diagonalen Gerade, die von dem Kerl aufgespannt wird, nur um einen Faktor von zwei gestreckt wird.",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "Und für diese Transformation sind das alle Vektoren, die die besondere Eigenschaft haben, auf ihrer Spannweite zu bleiben.",
  "model": "DeepL",
  "from_community_srt": "Und für diese Transformation sind das alle Vektoren mit der speziellen Eigenschaft, dass sie auf ihrem Spann bleiben.",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Die auf der X-Achse werden um den Faktor 3 gestreckt und die auf dieser diagonalen Linie um den Faktor 2 gestreckt.",
  "model": "DeepL",
  "from_community_srt": "Diejenigen, die auf der x-Achse um einen Faktor von drei gestreckt werden, und die auf dieser diagonalen Geraden, die auf das Doppelte gestreckt werden.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Jeder andere Vektor wird während der Transformation etwas gedreht und von der Linie, die er überspannt, abgeschlagen.",
  "model": "DeepL",
  "from_community_srt": "Jeder andere Vektor wird während der Transformation irgendwie gedreht, weg von der Geraden die er aufspannt.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Wie du vielleicht schon vermutet hast, werden diese speziellen Vektoren als Eigenvektoren der Transformation bezeichnet, und jedem Eigenvektor ist ein sogenannter Eigenwert zugeordnet, der einfach der Faktor ist, um den er während der Transformation gestreckt oder gestaucht wird.",
  "model": "DeepL",
  "from_community_srt": "Wie Ihr vielleicht schon erraten habt, diese speziellen Vektoren werden \"Eigenvektoren\" der Transformation genannt, und jeder Eigenvektor wird mit einem sogenannten \"Eigenwert\" assoziiert, der einfach dem Faktor entspricht, mit dem er während der Transformation gestreckt oder gestaucht wird.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Natürlich ist es nichts Besonderes, dass diese Eigenwerte positiv sind, oder dass sie gestreckt oder gequetscht werden.",
  "model": "DeepL",
  "from_community_srt": "Natürlich ist das \"Strecken\" und \"Stauchen\" nichts besonderes oder, dass diese Eigenwerte positiv sind.",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "In einem anderen Beispiel könntest du einen Eigenvektor mit dem Eigenwert negativ 1 halb haben, was bedeutet, dass der Vektor um den Faktor 1 halb gespiegelt und quadriert wird.",
  "model": "DeepL",
  "from_community_srt": "In einem anderen Beispiel, könnte man einen Eigenvektor mit einem Eigenwert von -1/2 haben, was bedeute, dass der Vektor umgedreht und um den Faktor zwei gestaucht wird.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Wichtig dabei ist, dass er auf der Linie bleibt, die er überspannt, ohne dass er von ihr weggedreht wird.",
  "model": "DeepL",
  "from_community_srt": "Hauptsache er bleibt auf der Geraden, die er aufspannt, ohne weggedreht zu werden.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Um einen Eindruck davon zu bekommen, warum es sinnvoll sein könnte, darüber nachzudenken, betrachte eine dreidimensionale Drehung.",
  "model": "DeepL",
  "from_community_srt": "Um einen Einblick zu bekommen, warum es sinnvoll wäre darüber nachzudenken, sollte man sich eine dreidimensionale Rotation vorstellen.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Wenn du einen Eigenvektor für diese Drehung finden kannst, also einen Vektor, der auf seiner eigenen Spannweite bleibt, hast du die Drehachse gefunden.",
  "model": "DeepL",
  "from_community_srt": "Findet man einen Eigenvektor dieser Rotation, einen Vektor der auf seinem Spann bleibt, hat man die Rotationsachse gefunden.",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "Und es ist viel einfacher, über eine 3D-Drehung in Form einer Drehachse und eines Winkels nachzudenken, um den sie sich dreht, als über die gesamte 3x3-Matrix, die mit dieser Transformation verbunden ist.",
  "model": "DeepL",
  "from_community_srt": "Und es ist viel einfacher sich eine 3D- Drehbewegung als Rotationsachse und -winkel vorzustellen, als als volle 3x3-Matrix, die diese Transformation beschreibt.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "In diesem Fall müsste der entsprechende Eigenwert übrigens 1 sein, da Rotationen nie etwas strecken oder stauchen, sodass die Länge des Vektors gleich bleibt.",
  "model": "DeepL",
  "from_community_srt": "In diesem Fall müsste der entsprechende Eigenwert 1 betragen, nachdem Rotationen nichts strecken oder stauchen und so die Länge des Vektors dieselbe bleibt.",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Dieses Muster taucht häufig in der linearen Algebra auf.",
  "model": "DeepL",
  "from_community_srt": "Dieses Muster taucht oft in linearer Algebra auf.",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Bei jeder linearen Transformation, die durch eine Matrix beschrieben wird, kannst du verstehen, was sie tut, indem du die Spalten dieser Matrix als Landeplätze für Basisvektoren abliest.",
  "model": "DeepL",
  "from_community_srt": "Man kann verstehen was jede lineare Transformation, die durch eine Matrix beschrieben wird, tut, wenn man die Spalten jener Matrix als Landepunkte für die Eigenvektoren liest.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Aber oft ist es besser, die Eigenvektoren und Eigenwerte zu finden, um herauszufinden, was die lineare Transformation tatsächlich bewirkt, und weniger von deinem speziellen Koordinatensystem abhängig zu sein.",
  "model": "DeepL",
  "from_community_srt": "Aber um zu verstehen, was eine lineare Transformation macht, unabhängiger vom eigenen Koordinatensystem, ist es oft besser, die Eigenvektoren und Eigenwerte zu finden.",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Ich werde hier nicht auf alle Details der Methoden zur Berechnung von Eigenvektoren und Eigenwerten eingehen, aber ich werde versuchen, einen Überblick über die Berechnungsideen zu geben, die für ein konzeptionelles Verständnis am wichtigsten sind.",
  "model": "DeepL",
  "from_community_srt": "I werde hier nicht alle Details zur Berechnung von Eigenvektoren und Eigenwerten abdecken, aber ich werde versuchen einen Überblick über die rechnerischen Grundideen zu bieten, die am wichtigsten für das Konzeptverständnis sind.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Symbolisch sieht die Idee eines Eigenvektors folgendermaßen aus.",
  "model": "DeepL",
  "from_community_srt": "Symbolisch, hier ist, wie die Idee eines Eigenvektors aussieht.",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A ist die Matrix, die eine Transformation darstellt, mit v als Eigenvektor, und lambda ist eine Zahl, nämlich der entsprechende Eigenwert.",
  "model": "DeepL",
  "from_community_srt": "A sei eine Matrix, die eine gewisse Transformation darstellt, mit v als Eigenvektor, und Lambda als Entsprechenden Eigenwert.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Dieser Ausdruck besagt, dass das Matrix-Vektor-Produkt, A mal v, dasselbe Ergebnis liefert wie die Skalierung des Eigenvektors v mit einem Wert lambda.",
  "model": "DeepL",
  "from_community_srt": "Was dieser Ausdruck besagt ist, dass das Matrix-Vektor-Produkt A ⋅ v das gleiche Ergebnis liefert, wie wenn man den Eigenvektor v um einen Wert Lambda skaliert.",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Um die Eigenvektoren und ihre Eigenwerte einer Matrix A zu finden, musst du also die Werte von v und lambda finden, die diesen Ausdruck wahr machen.",
  "model": "DeepL",
  "from_community_srt": "Also findet man im Endeffekt die Eigenvektoren und Eigenwerte der Matrix A, wenn man die Werte für v und Lambda findet, die diesen Ausdruck wahr machen.",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "Die linke Seite steht für die Matrix-Vektor-Multiplikation, aber die rechte Seite ist die Skalar-Vektor-Multiplikation.",
  "model": "DeepL",
  "from_community_srt": "Es ist erstmals etwas unangenehm damit zu arbeiten, weil die linke Seite eine Matrix-Vektor-Multiplikation, aber die rechte Seine ein Skalarprodukt beinhaltet.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Beginnen wir also damit, die rechte Seite als eine Art Matrix-Vektor-Multiplikation umzuschreiben, indem wir eine Matrix verwenden, die jeden Vektor um einen Faktor von Lambda skaliert.",
  "model": "DeepL",
  "from_community_srt": "Also beginnen wir damit, dass wir die Rechte Seite als eine Art Matrix-Vektor-Multiplikation scheiben, indem wir eine Matrix verwenden, die den Effekt einer Skalierung um den Faktor Lambda hat.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Die Spalten einer solchen Matrix stellen dar, was mit den einzelnen Basisvektoren passiert. Jeder Basisvektor wird einfach mit Lambda multipliziert, sodass diese Matrix auf der Diagonalen die Zahl Lambda und überall sonst Nullen hat.",
  "model": "DeepL",
  "from_community_srt": "Die Spalten dieser Matrix repräsentieren, was mit den jeweiligen Basisvektoren passiert, und da jeder Basisvektor einfach mit Lambda multipliziert wird, hat diese Matrix in der Diagonalen der Wert Lambda und 0 überall sonst.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "Die übliche Art, diesen Typ zu schreiben, ist, das Lambda herauszufaktorisieren und es als Lambda mal i zu schreiben, wobei i die Identitätsmatrix mit 1en auf der Diagonale ist.",
  "model": "DeepL",
  "from_community_srt": "Üblicherweise wird das Lambda herausgehoben und es wird als Lambda mal I geschrieben, mit I als Einheitsmatrix mit Einsern hinunter in der Diagonalen.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Da beide Seiten wie eine Matrix-Vektor-Multiplikation aussehen, können wir die rechte Seite subtrahieren und das v herausrechnen.",
  "model": "DeepL",
  "from_community_srt": "Wenn beide Seiten, wie eine Vektor-Matrix-Multiplikation aussehen, kann man die rechte Seite subtrahieren und v ausklammern.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Wir haben jetzt also eine neue Matrix, A minus Lambda mal die Identität, und suchen nach einem Vektor v, bei dem diese neue Matrix mal v den Nullvektor ergibt.",
  "model": "DeepL",
  "from_community_srt": "Jetzt haben wir eine neue Matrix A minus Lambda mal der Einheitsmatrix, und jetzt suchen wir einen Vektor v, sodass die neue Matrix mal v den Nullvektor ergibt.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Das ist immer dann der Fall, wenn v selbst der Nullvektor ist, aber das ist langweilig.",
  "model": "DeepL",
  "from_community_srt": "Jetzt wird das auch immer wahr sein, wenn  v der Nullvektor ist, aber das ist uninteressant.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Was wir wollen, ist ein Eigenvektor ungleich Null.",
  "model": "DeepL",
  "from_community_srt": "Was wir wollen ist ein Eigenvektor v ≠ 0.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "Und wenn du dir Kapitel 5 und 6 ansiehst, wirst du wissen, dass das Produkt einer Matrix mit einem Nicht-Null-Vektor nur dann Null werden kann, wenn die Transformation, die mit dieser Matrix verbunden ist, den Raum in eine niedrigere Dimension quetscht.",
  "model": "DeepL",
  "from_community_srt": "Wenn Du Kapitel 5 und 6 gesehen hat, wirst du wissen, dass die einzige Möglichkeit, wie das Produkt einer Matrix mit einem Vektor v ≠ 0, null ergeben kann, ist, wenn die mit der Matrix assoziierte Transformation den Raum in eine niedrigere Dimension quetscht.",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "Und diese Squishification entspricht einer Null-Determinante für die Matrix.",
  "model": "DeepL",
  "from_community_srt": "Und diese Quetschung entspricht einer Null-Determinante für die Matrix.",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Nehmen wir an, deine Matrix A hat die Spalten 2, 1 und 2, 3. Überlege dir, wie du einen variablen Betrag, Lambda, von jedem Diagonaleintrag abziehen kannst.",
  "model": "DeepL",
  "from_community_srt": "Konkret sei A eine Matrix mit den Spalten (2 | 1) und (2 | 3), und Lambda ein variabler Wert, der von jedem diagonalen Eintrag subtrahiert wird.",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Jetzt stell dir vor, du drehst an einem Knopf, um den Wert von Lambda zu ändern.",
  "model": "DeepL",
  "from_community_srt": "Man stelle sich vor, dass man Lambda wie mit einem Drehregler verändert.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "Wenn sich der Wert von Lambda ändert, ändert sich auch die Matrix selbst und damit auch die Determinante der Matrix.",
  "model": "DeepL",
  "from_community_srt": "Und wenn sich der Wert von Lambda ändert, verändert sich die Matrix selbst, und somit ihre Determinante.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Das Ziel ist es, einen Lambda-Wert zu finden, bei dem die Determinante Null ist, was bedeutet, dass die geänderte Transformation den Raum in eine niedrigere Dimension drückt.",
  "model": "DeepL",
  "from_community_srt": "Das Ziel hier ist es einen Wert für Lambda zu finden, bei dem die Determinante der Matrix 0 ist, was bedeutet, dass die angepasste Transformation den Raum in eine niedrigere Dimension quetscht.",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "In diesem Fall ist der Sweet Spot erreicht, wenn lambda gleich 1 ist.",
  "model": "DeepL",
  "from_community_srt": "In diesem Fall, dieser Sweetspot wird erreicht, wenn Lambda 1 ist.",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Hätten wir eine andere Matrix gewählt, müsste der Eigenwert natürlich nicht unbedingt 1 sein.",
  "model": "DeepL",
  "from_community_srt": "Wenn wir eine andere Matrix gewählt hätten, müsste der Eigenwert natürlich nicht unbedingt 1 entsprechen,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Der Sweet Spot könnte auch bei einem anderen Lambda-Wert erreicht werden.",
  "model": "DeepL",
  "from_community_srt": "der Sweetspot könnte durch einen anderen Wert von Lambda erreicht werden.",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Das ist ganz schön viel, aber lass uns enträtseln, was das bedeutet.",
  "model": "DeepL",
  "from_community_srt": "Das ist irgendwie viel, aber lasst uns den Sinn davon entwirren.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Wenn lambda gleich 1 ist, quetscht die Matrix A minus lambda mal die Identität den Raum auf eine Linie.",
  "model": "DeepL",
  "from_community_srt": "Wenn Lambda 1 entspricht, quetscht die Matrix A - I ⋅ λ den Raum in eine Gerade.",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Das bedeutet, dass es einen Nicht-Null-Vektor v gibt, so dass A minus Lambda mal die Identität mal v gleich dem Null-Vektor ist.",
  "model": "DeepL",
  "from_community_srt": "Das bedeutet, es gibt einen Vektor v ≠ 0, sodass A minus Lambda mal der Einheitsmatrix mal v dem Nullvektor entspricht.",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "Der Grund, warum uns das interessiert, ist, dass es bedeutet, dass A mal v gleich Lambda mal v ist, was bedeutet, dass der Vektor v ein Eigenvektor von A ist und während der Transformation A auf seiner eigenen Spanne bleibt.",
  "model": "DeepL",
  "from_community_srt": "Zur Erinnerung, der Grund dafür, dass uns das interessiert, ist, dass es bedeutet, dass A ⋅ v = λ ⋅ v, was heißt, dass v ein Eigenvektor von A ist, und während der Transformation auf seinem Spann bleibt.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "In diesem Beispiel ist der entsprechende Eigenwert 1, so dass v eigentlich an seinem Platz bleiben würde.",
  "model": "DeepL",
  "from_community_srt": "In diesem Beispiel ist der entsprechende Eigenwert 1, also wäre v nur ein fixer Punkt.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Halte inne und überlege, ob du sicherstellen musst, dass sich diese Argumentation gut anfühlt.",
  "model": "DeepL",
  "from_community_srt": "Pausiert und spult zurück um diesen Argumentationsweg nachzuvollziehe.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "Das ist die Art von Dingen, die ich in der Einleitung erwähnt habe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Wenn du kein solides Verständnis von Determinanten hättest und wüsstest, warum sie sich auf lineare Gleichungssysteme mit Lösungen ungleich Null beziehen, würde dir ein Ausdruck wie dieser völlig fremd vorkommen.",
  "model": "DeepL",
  "from_community_srt": "Das habe ich in der Einleitung gemeint, wenn man keinen Dunst von Determinanten hat, und was sie mit linearen Gleichungssystemen mit Lösungen ≠ 0 zu tun haben, scheint ein Ausdruck wie dieser aus der Luft gegriffen.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Um dies in Aktion zu sehen, lass uns das Beispiel vom Anfang wiederholen, mit einer Matrix, deren Spalten 3, 0 und 1, 2 sind.",
  "model": "DeepL",
  "from_community_srt": "Um das in Aktion zu sehen, kehren wir zum Beispiel vom Anfang zurück mit einer Matrix, deren Spalten (3 | 0) und (1 | 2) sind.",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Um herauszufinden, ob ein Wert lambda ein Eigenwert ist, ziehst du ihn von den Diagonalen dieser Matrix ab und berechnest die Determinante.",
  "model": "DeepL",
  "from_community_srt": "Um herauszufinden ob Lambda ein Eigenwert ist, subtrahiert man es von der Diagonale der Matrix und berechnet die Determinante.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Auf diese Weise erhalten wir ein bestimmtes quadratisches Polynom in Lambda, 3 minus Lambda mal 2 minus Lambda.",
  "model": "DeepL",
  "from_community_srt": "Dadurch erhält man eine quadratisches Polynom in Lambda, (3-λ)(2-λ).",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Da lambda nur dann ein Eigenwert sein kann, wenn diese Determinante Null ist, kannst du daraus schließen, dass die einzigen möglichen Eigenwerte lambda gleich 2 und lambda gleich 3 sind.",
  "model": "DeepL",
  "from_community_srt": "Da Lambda nur ein Eigenwert sein kann, wenn die Determinante 0 ist, kann man schlussfolgern, dass die einzigen Eigenwerte λ = 2 und λ = 3 sind.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Um herauszufinden, welche Eigenvektoren tatsächlich einen dieser Eigenwerte haben, z. B. Lambda gleich 2, fügst du diesen Wert von Lambda in die Matrix ein und löst dann, welche Vektoren diese diagonal veränderte Matrix zu Null macht.",
  "model": "DeepL",
  "from_community_srt": "Um die Eigenvektoren herauszufinden, die einen dieser Eigenwerte haben, sei λ = 2, setzt man den Wert von Lambda in die Matrix ein und dann löst man für welche Vektoren diese diagonal veränderte Matrix 0 zu einem Ergebnis von 0 führt.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Wenn du dieses System wie jedes andere lineare System berechnen würdest, würdest du sehen, dass die Lösungen alle Vektoren auf der Diagonalen sind, die von der negativen 1, 1 aufgespannt wird.",
  "model": "DeepL",
  "from_community_srt": "Würde man es so wie jedes andere Gleichungssystem berechnen, sähe man, dass die Lösungen alle Vektoren sind, die auf der Diagonalen liegen, die von (-1 | 1) aufgespannt wird.",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Dies entspricht der Tatsache, dass die unveränderte Matrix 3, 0, 1, 2 all diese Vektoren um den Faktor 2 streckt.",
  "model": "DeepL",
  "from_community_srt": "Das ist darauf zurückzuführen, dass die unveränderte Matrix [(3 | 0), (1 | 2)] die Eigenschaft hat, jene Vektoren um den Faktor 2 zu strecken.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Eine 2D-Transformation muss also keine Eigenvektoren haben.",
  "model": "DeepL",
  "from_community_srt": "Nun, eine 2D-Transformation muss keine Eigenvektoren haben.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Betrachte zum Beispiel eine Drehung um 90 Grad.",
  "model": "DeepL",
  "from_community_srt": "Man nehme beispielsweise eine 90°-Rotation.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Diese hat keine Eigenvektoren, da sie jeden Vektor aus seiner eigenen Spanne herausdreht.",
  "model": "DeepL",
  "from_community_srt": "Sie hat keine Eigenvektoren, da sie jeden Vektor von seinem Spann wegdreht.",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Wenn du versuchst, die Eigenwerte einer solchen Drehung zu berechnen, merkst du, was passiert.",
  "model": "DeepL",
  "from_community_srt": "Sieh, was passiert, wenn man versucht die Eigenwerte von so einer Rotation zu ermitteln.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Seine Matrix hat die Spalten 0, 1 und negativ 1, 0.",
  "model": "DeepL",
  "from_community_srt": "Die Matrix dieser Rotation hat die Spalten (0 | 1) und (-1 | 0), man subtrahiere Lambda von den diagonalen Elementen und,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Subtrahiere lambda von den Diagonalelementen und suche, wann die Determinante Null ist.",
  "model": "DeepL",
  "from_community_srt": "suche den Wert, wo die Determinante 0 ist.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "In diesem Fall erhältst du das Polynom lambda Quadrat plus 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Die einzigen Wurzeln dieses Polynoms sind die imaginären Zahlen, i und negativ i.",
  "model": "DeepL",
  "from_community_srt": "In diesem Fall erhält man das Polynom λ^2+1, und die einzigen Nullstellen von diesem Polynom sind die imaginären Zahlen i und -i.",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Die Tatsache, dass es keine Lösungen mit reellen Zahlen gibt, bedeutet, dass es keine Eigenvektoren gibt.",
  "model": "DeepL",
  "from_community_srt": "Die Tatsache, dass es keine reelle Lösung gibt, weist darauf hin, dass es keine Eigenvektoren gibt.",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Ein weiteres interessantes Beispiel, das du im Hinterkopf behalten solltest, ist eine Schere.",
  "model": "DeepL",
  "from_community_srt": "Ein anderes sehr interessantes Beispiel, das es wert ist, im Gedächtnis zu behalten, ist ein \"Shear\".",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Dadurch wird i-hat an seinem Platz fixiert und j-hat um 1 verschoben, sodass seine Matrix die Spalten 1, 0 und 1, 1 hat.",
  "model": "DeepL",
  "from_community_srt": "Das hält î am Platz und bewegt und ĵ um eins nach rechts, also hat seine Matrix die Spalten (1 | 0) und (1 | 1).",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Alle Vektoren auf der x-Achse sind Eigenvektoren mit dem Eigenwert 1, da sie an ihrem Platz bleiben.",
  "model": "DeepL",
  "from_community_srt": "Alle Vektoren auf der x-Achse sind Eigenvektoren mit Eigenwert 1, da sie am Platz bleiben.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "Tatsächlich sind dies die einzigen Eigenvektoren.",
  "model": "DeepL",
  "from_community_srt": "Tatsächlich sind sie die einzigen Eigenvektoren.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Wenn du lambda von den Diagonalen abziehst und die Determinante berechnest, erhältst du 1 minus lambda zum Quadrat.",
  "model": "DeepL",
  "from_community_srt": "Wenn man Lambda von den Diagonalen subtrahiert und die Determinante berechnet, erhält man (1-λ)²,",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "Und die einzige Wurzel dieses Ausdrucks ist lambda gleich 1.",
  "model": "DeepL",
  "from_community_srt": "und die einzige Nullstelle dieses Ausdrucks ist 1.",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Dies entspricht dem, was wir geometrisch sehen, nämlich dass alle Eigenvektoren den Eigenwert 1 haben.",
  "model": "DeepL",
  "from_community_srt": "Das stimmt mit dem überein, was man geometrisch sieht, nämlich, dass alle Eigenvektoren Eigenwerte von 1 haben.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Bedenke aber, dass es auch möglich ist, nur einen Eigenwert zu haben, aber mehr als nur eine Linie voller Eigenvektoren.",
  "model": "DeepL",
  "from_community_srt": "Man muss aber beachten, dass es auch möglich ist nur einen Eigenwert zu haben, aber mehr als eine Gerade an Eigenvektoren.",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Ein einfaches Beispiel ist eine Matrix, die alles mit 2 skaliert.",
  "model": "DeepL",
  "from_community_srt": "Ein simples Beispiel wäre eine Matrix,",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "Der einzige Eigenwert ist 2, aber jeder Vektor in der Ebene wird zu einem Eigenvektor mit diesem Eigenwert.",
  "model": "DeepL",
  "from_community_srt": "die alles um den Faktor 2 skaliert, der einzige Eigenwert ist 2, aber jeder Vektor in der Ebene ist ein Eigenvektor mit diesem Eigenwert.",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Jetzt ist ein weiterer guter Zeitpunkt, um innezuhalten und über einiges nachzudenken, bevor ich zum letzten Thema übergehe.",
  "model": "DeepL",
  "from_community_srt": "Jetzt ist wieder ein guter Zeitpunkt zum Pausieren und Zurückspulen, bevor ich mit dem letzen Thema fortfahre.",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Ich möchte hier mit der Idee einer Eigenbasis abschließen, die sich stark auf die Ideen aus dem letzten Video stützt.",
  "model": "DeepL",
  "from_community_srt": "Ich möchte hier mit einer Idee von einer Eigenbasis abschließen, die sehr auf den Ideen des letzten Videos beruht.",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Sieh dir an, was passiert, wenn unsere Basisvektoren zufällig Eigenvektoren sind.",
  "model": "DeepL",
  "from_community_srt": "Seht, was passiert, wenn gerade die Basisvektoren Eigenvektoren sind.",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Zum Beispiel könnte i-hat eine negative Skalierung von 1 und j-hat eine Skalierung von 2 haben.",
  "model": "DeepL",
  "from_community_srt": "Zum Beispiel werde  î um -1 skaliert und ĵ um 2.",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Wenn du ihre neuen Koordinaten als Spalten einer Matrix schreibst, merkst du, dass die skalaren Vielfachen, negative 1 und 2, die die Eigenwerte von i-hat und j-hat sind, auf der Diagonale unserer Matrix stehen und jeder andere Eintrag eine 0 ist.",
  "model": "DeepL",
  "from_community_srt": "Schreibt man ihre neuen Koordinaten als die Spalten einer Matrix, sieht man, dass die Skalarfaktoren -1 und 2, die die Eigenwerte von î und ĵ sind, sich auf der diagonalen der Matrix befinden und jeder andere Eintrag 0 ist.",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Wenn eine Matrix überall Nullen hat, außer auf der Diagonalen, nennt man sie vernünftigerweise eine Diagonalmatrix.",
  "model": "DeepL",
  "from_community_srt": "Immer, wenn eine Matrix bis auf die Diagonale nur aus Nullen besteht, wird sie, aus gutem Grund, diagonale Matrix genannt.",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "Das bedeutet, dass alle Basisvektoren Eigenvektoren sind und die Diagonaleinträge dieser Matrix ihre Eigenwerte sind.",
  "model": "DeepL",
  "from_community_srt": "Und das ist so zu interpretieren, dass alle Basisvektoren Eigenvektoren, und die diagonalen Werte deren Eigenwerte sind.",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Es gibt eine Menge Dinge, die die Arbeit mit diagonalen Matrizen viel angenehmer machen.",
  "model": "DeepL",
  "from_community_srt": "Es gibt viele Szenarien, in denen es viel einfacher ist mit diagonalen Matrizen zu arbeiten.",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Eine davon ist, dass es einfacher zu berechnen ist, was passiert, wenn du diese Matrix ein paar Mal mit sich selbst multiplizierst.",
  "model": "DeepL",
  "from_community_srt": "Ein entscheidendes ist, dass es einfacher ist zu berechnen, was passiert, wenn man diese Matrix sehr oft mit sich selbst multipliziert.",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Da eine dieser Matrizen jeden Basisvektor nur um einen Eigenwert skaliert, entspricht die Anwendung dieser Matrix viele Male, z. B. 100 Mal, der Skalierung jedes Basisvektors um die 100-te Potenz des entsprechenden Eigenwertes.",
  "model": "DeepL",
  "from_community_srt": "Das all diese Matrizen nur die einzelnen Eigenvektoren um einen bestimmen Eigenwert skalieren, ist es dasselbe, die Matrix sehr viele Male, sagen wir hundertmal, anzuwenden, wie jeden Basisvektor einfach nur um die hundertste Potenz des jeweiligen Eigenwerts zu skalieren.",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Versuche dagegen, die 100. Potenz einer nicht diagonalen Matrix zu berechnen.",
  "model": "DeepL",
  "from_community_srt": "Zum Vergleich, versuch mal die hundertste Potenz einer nicht-diagonalen Matrix zu berechnen.",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Probiere es doch mal aus.",
  "model": "DeepL",
  "from_community_srt": "Wirklich jetzt, versuch es mal!",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Es ist ein Alptraum.",
  "model": "DeepL",
  "from_community_srt": "Es ist ein Albtraum.",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Natürlich wirst du selten das Glück haben, dass deine Basisvektoren auch Eigenvektoren sind.",
  "model": "DeepL",
  "from_community_srt": "Natürlich hat man selten so Glück,",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Wenn deine Transformation aber viele Eigenvektoren hat, wie die vom Anfang dieses Videos, so dass du eine Menge auswählen kannst, die den gesamten Raum abdeckt, dann kannst du dein Koordinatensystem so ändern, dass diese Eigenvektoren deine Basisvektoren sind.",
  "model": "DeepL",
  "from_community_srt": "dass die Basisvektoren auch Eigenvektoren sind, aber wenn die Transformation viele Eigenvektoren hat, wie die vom Beginn des Videos, genug, dass man eine Menge wählen kann, die den ganzen Raum aufspannt, dann kann man das Koordinatensystem so verändern, sodass diese Eigenvektoren die Basisvektoren sind.",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Im letzten Video habe ich über den Wechsel der Basis gesprochen, aber ich werde hier noch einmal kurz erklären, wie man eine Transformation, die in unserem Koordinatensystem geschrieben wurde, in einem anderen System ausdrückt.",
  "model": "DeepL",
  "from_community_srt": "Ich habe bereits letztes Video über Basiswechsel gesprochen, aber ich werde hier noch einmal sehr schnell wiederholen, wie man eine Transformation in unserem Koordinatensystem mit einem anderem System ausdrückt.",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Nimm die Koordinaten der Vektoren, die du als neue Basis verwenden willst, also in diesem Fall unsere beiden Eigenvektoren, und mache diese Koordinaten zu den Spalten einer Matrix, der sogenannten Basisänderungsmatrix.",
  "model": "DeepL",
  "from_community_srt": "Man nehme die Koordinaten der Vektoren, die man als neue Basis verwenden möchte, was, in diesem Fall, bedeutet, dass es zwei Eigenvektoren sind, die deren Koordinaten zu den Spalten einer Matrix machen, die sogenannte Basiswechsel-Matrix.",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Wenn du die ursprüngliche Umwandlung in eine Sandwich-Matrix umwandelst, indem du die Basisänderungsmatrix auf die rechte Seite und die Umkehrung der Basisänderungsmatrix auf die linke Seite legst, ist das Ergebnis eine Matrix, die dieselbe Umwandlung darstellt, aber aus der Perspektive des Koordinatensystems der neuen Basisvektoren.",
  "model": "DeepL",
  "from_community_srt": "Wenn man die ursprüngliche Transformation in die Mitte, die Basiswechsel-Matrix auf die rechte Seite, und die umgekehrte Basiswechsel-Matrix auf die linke Seite stellt, ist das Ergebnis eine Matrix, die dieselbe Transformation darstellt, aber von der Perspektive eines Koordinatensystem mit der neuen Basis.",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Der Sinn dieser Methode mit Eigenvektoren ist, dass die neue Matrix garantiert diagonal ist und die entsprechenden Eigenwerte auf der Diagonalen liegen.",
  "model": "DeepL",
  "from_community_srt": "Die Intention, das zu tun, ist, dass diese neue Matrix garantiert diagonal ist mit den entsprechenden Eigenwerten in dieser Diagonale.",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Das liegt daran, dass es sich um ein Koordinatensystem handelt, in dem die Basisvektoren bei der Transformation skaliert werden.",
  "model": "DeepL",
  "from_community_srt": "Das funktioniert, da sie die Arbeit in einem Koordinatensystem repräsentiert, wo die Eigenvektoren durch diese Transformation nur skaliert werden.",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Eine Menge von Basisvektoren, die auch Eigenvektoren sind, nennt man vernünftigerweise eine Eigenbasis.",
  "model": "DeepL",
  "from_community_srt": "Eine Menge von Basisvektoren, die auch Eigenvektoren sind wird, wieder aus gutem Grund, \"Eigenbasis\" genannt.",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Wenn du also zum Beispiel die 100. Potenz dieser Matrix berechnen müsstest, wäre es viel einfacher, zu einer Eigenbasis zu wechseln, die 100. Potenz in diesem System zu berechnen und dann zu unserem Standardsystem zurückzukehren.",
  "model": "DeepL",
  "from_community_srt": "Wenn man also die hundertste Potenz dieser Matrix berechnen muss, ist es viel einfacher zu einer Eigenbasis zu wechseln, die hundertste Potenz in diesem System zu berechnen und dann zurück zu unserem Standardsystem zu konvertieren.",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Du kannst das nicht mit allen Transformationen machen.",
  "model": "DeepL",
  "from_community_srt": "Man kann das allerdings nicht mit allen Transformationen machen.",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Eine Scherung zum Beispiel hat nicht genug Eigenvektoren, um den gesamten Raum zu erfassen.",
  "model": "DeepL",
  "from_community_srt": "Ein \"Shear\" beispielsweise hat nicht genug Eigenvektoren um den ganzen Raum aufzuspannen.",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Aber wenn du eine Eigenbasis finden kannst, sind Matrixoperationen wirklich schön.",
  "model": "DeepL",
  "from_community_srt": "Aber wenn man eine Eigenbasis findet, macht es Matrixoperationen wirklich schön.",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Für diejenigen unter euch, die bereit sind, sich durch ein hübsches Rätsel zu arbeiten, um zu sehen, wie das in Aktion aussieht und wie man damit überraschende Ergebnisse erzielen kann, lasse ich hier eine Aufforderung auf dem Bildschirm.",
  "model": "DeepL",
  "from_community_srt": "Für diejenigen, die sich durch ein ordentliches Rätsel arbeiten wollen, um zu sehen, wie das in Aktion aussieht und wie man es benutzen kann um überraschende Ergebnisse zu erzielen, werde ich eine Angabe auf dem Bildschirm einblenden.",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Es ist ein bisschen Arbeit, aber ich glaube, es wird dir Spaß machen.",
  "model": "DeepL",
  "from_community_srt": "Es ist ein gutes Stück Arbeit, aber ich denke Ihr werdet es mögen.",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Das nächste und letzte Video dieser Reihe wird sich mit abstrakten Vektorräumen beschäftigen.",
  "model": "DeepL",
  "from_community_srt": "Das nächste und finale Video dieser Serie wird über abstrakte Vektorräume sein.",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]