[
 {
  "input": "Eigenvectors and eigenvalues is one of those topics that a lot of students find particularly unintuitive.",
  "translatedText": "Eigenvektoren und Eigenwerte gehören zu den Themen, die viele Studierende als besonders unintuitiv empfinden.",
  "model": "google_nmt",
  "from_community_srt": "\"Eigenvektoren und Eigenwerte\" ist eines der Themen , das viele Studenten besonders unintuitiv finden.",
  "n_reviews": 0,
  "start": 19.92,
  "end": 25.76
 },
 {
  "input": "Questions like, why are we doing this and what does this actually mean, are too often left just floating away in an unanswered sea of computations.",
  "translatedText": "Fragen wie „Warum tun wir das und was bedeutet das eigentlich?“ bleiben allzu oft in einem Meer unbeantworteter Berechnungen unbeantwortet.",
  "model": "google_nmt",
  "from_community_srt": "Fragen wie \"warum machen wir das\" und \"was heißt das überhaupt\" gehen oft in einem Meer aus Berechnungen unter.",
  "n_reviews": 0,
  "start": 25.76,
  "end": 33.26
 },
 {
  "input": "And as I've put out the videos of this series, a lot of you have commented about looking forward to visualizing this topic in particular.",
  "translatedText": "Und während ich die Videos dieser Serie veröffentlicht habe, haben viele von Ihnen kommentiert, dass sie sich insbesondere auf die Visualisierung dieses Themas freuen.",
  "model": "google_nmt",
  "from_community_srt": "Und als ich die Videos dieser Serie herausbrachte, haben viele von Euch kommentiert, sie würden sich auf die Visualisierung dieses Themas im Speziellen freuen.",
  "n_reviews": 0,
  "start": 33.92,
  "end": 40.06
 },
 {
  "input": "I suspect that the reason for this is not so much that eigenthings are particularly complicated or poorly explained.",
  "translatedText": "Ich vermute, dass der Grund dafür nicht so sehr darin liegt, dass Eigendinge besonders kompliziert oder schlecht erklärt sind.",
  "model": "google_nmt",
  "from_community_srt": "Ich nehme an, dass der Grund dafür nicht darin liegt, dass \"Eigen\"-Dinge besonders kompliziert wären oder schlecht erklärt würden.",
  "n_reviews": 0,
  "start": 40.68,
  "end": 46.36
 },
 {
  "input": "In fact, it's comparatively straightforward, and I think most books do a fine job explaining it.",
  "translatedText": "Tatsächlich ist es vergleichsweise einfach und ich denke, dass die meisten Bücher es gut erklären.",
  "model": "google_nmt",
  "from_community_srt": "Tatsächlich sind sie vergleichbar einfach und ich denke, dass die meisten Bücher es gut erklären.",
  "n_reviews": 0,
  "start": 46.86,
  "end": 51.18
 },
 {
  "input": "The issue is that it only really makes sense if you have a solid visual understanding for many of the topics that precede it.",
  "translatedText": "Das Problem ist, dass es nur dann wirklich Sinn macht, wenn man ein solides visuelles Verständnis für viele der vorangehenden Themen hat.",
  "model": "google_nmt",
  "from_community_srt": "Das Problem ist, dass es nur wirklich Sinn ergibt, wenn man ein solides visuelles Verständnis für vorhergehende Themen hat.",
  "n_reviews": 0,
  "start": 51.52,
  "end": 58.48
 },
 {
  "input": "Most important here is that you know how to think about matrices as linear transformations, but you also need to be comfortable with things like determinants, linear systems of equations, and change of basis.",
  "translatedText": "Am wichtigsten ist hier, dass Sie wissen, wie man sich Matrizen als lineare Transformationen vorstellt, aber Sie müssen sich auch mit Dingen wie Determinanten, linearen Gleichungssystemen und Basiswechseln auskennen.",
  "model": "google_nmt",
  "from_community_srt": "Am wichtigsten ist hier, dass man Matrizen als lineare Transformationen ansieht, aber man sollte auch mit Dingen wie Determinanten, linearen Gleichungssystemen und Basiswechsel vertraut sein.",
  "n_reviews": 0,
  "start": 59.06,
  "end": 69.94
 },
 {
  "input": "Confusion about eigenstuffs usually has more to do with a shaky foundation in one of these topics than it does with eigenvectors and eigenvalues themselves.",
  "translatedText": "Verwirrung über Eigenstoffe hat in der Regel mehr mit einer unsicheren Grundlage in einem dieser Themen zu tun als mit Eigenvektoren und Eigenwerten selbst.",
  "model": "google_nmt",
  "from_community_srt": "Verwirrung über \"Eigen\"-Dinge hat normalerweise mehr mit einem unsicheren Grundwissen in einem jener Themen zu tun, als mit Eigenvektoren und Eigenwerten selbst.",
  "n_reviews": 0,
  "start": 70.72,
  "end": 79.24
 },
 {
  "input": "To start, consider some linear transformation in two dimensions, like the one shown here.",
  "translatedText": "Betrachten Sie zunächst eine lineare Transformation in zwei Dimensionen, wie die hier gezeigte.",
  "model": "google_nmt",
  "from_community_srt": "Zu Anfang sei eine lineare Transformation in zwei Dimensionen, wie die hier gezeigte.",
  "n_reviews": 0,
  "start": 79.98,
  "end": 84.84
 },
 {
  "input": "It moves the basis vector i-hat to the coordinates 3, 0, and j-hat to 1, 2.",
  "translatedText": "Es verschiebt den Basisvektor i-hat zu den Koordinaten 3, 0 und j-hat zu 1, 2.",
  "model": "google_nmt",
  "from_community_srt": "Sie bewegt den Basisvektor î zu den Koordinaten (3 | 0) und ĵ zu (1 | 2), und wird daher durch eine Matrix mit den Spalten (3 | 0) und (1 | 2) repräsentiert.",
  "n_reviews": 0,
  "start": 85.46,
  "end": 91.04
 },
 {
  "input": "So it's represented with a matrix whose columns are 3, 0, and 1, 2.",
  "translatedText": "Es wird also durch eine Matrix dargestellt, deren Spalten 3, 0 und 1, 2 sind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.78,
  "end": 95.64
 },
 {
  "input": "Focus in on what it does to one particular vector, and think about the span of that vector, the line passing through its origin and its tip.",
  "translatedText": "Konzentrieren Sie sich darauf, was es mit einem bestimmten Vektor macht, und denken Sie an die Spanne dieses Vektors, die Linie, die durch seinen Ursprung und seine Spitze verläuft.",
  "model": "google_nmt",
  "from_community_srt": "Konzentrier Dich darauf, was sie mit genau diesem Vektor macht und denk über den Spann dieses Vektors nach, die Gerade die durch die Spitze des Vektors und den Ursprung verläuft.",
  "n_reviews": 0,
  "start": 96.6,
  "end": 104.16
 },
 {
  "input": "Most vectors are going to get knocked off their span during the transformation.",
  "translatedText": "Die meisten Vektoren werden während der Transformation aus ihrer Spanne gerissen.",
  "model": "google_nmt",
  "from_community_srt": "Während der Transformation werden die meisten Vektoren aus ihrem Spann herausbewegt.",
  "n_reviews": 0,
  "start": 104.92,
  "end": 108.38
 },
 {
  "input": "I mean, it would seem pretty coincidental if the place where the vector landed also happened to be somewhere on that line.",
  "translatedText": "Ich meine, es wäre ziemlich zufällig, wenn der Ort, an dem der Vektor gelandet ist, zufällig auch irgendwo auf dieser Linie liegen würde.",
  "model": "google_nmt",
  "from_community_srt": "Es schiene doch sehr zufällig, wenn die Stelle, an der der Vektor landet, sich auch auf dieser Geraden befindet.",
  "n_reviews": 0,
  "start": 108.78,
  "end": 115.32
 },
 {
  "input": "But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it, like a scalar.",
  "translatedText": "Einige spezielle Vektoren bleiben jedoch in ihrer eigenen Spanne, was bedeutet, dass die Wirkung der Matrix auf einen solchen Vektor lediglich darin besteht, ihn zu strecken oder zu stauchen, wie bei einem Skalar.",
  "model": "google_nmt",
  "from_community_srt": "Aber ein paar spezielle Vektoren verbleiben auf ihrem Spann, bedeutet, dass die Matrix so einen Vektor nur streckt oder staucht, wie ein Skalar.",
  "n_reviews": 0,
  "start": 117.4,
  "end": 127.04
 },
 {
  "input": "For this specific example, the basis vector i-hat is one such special vector.",
  "translatedText": "Für dieses spezielle Beispiel ist der Basisvektor i-hat ein solcher spezieller Vektor.",
  "model": "google_nmt",
  "from_community_srt": "Für dieses spezifische Beispiel, der Basisvektor î ist so ein spezieller Vektor.",
  "n_reviews": 0,
  "start": 129.46,
  "end": 134.1
 },
 {
  "input": "The span of i-hat is the x-axis, and from the first column of the matrix, we can see that i-hat moves over to 3 times itself, still on that x-axis.",
  "translatedText": "Die Spanne von i-hat ist die x-Achse, und aus der ersten Spalte der Matrix können wir sehen, dass sich i-hat auf das Dreifache seiner selbst bewegt, immer noch auf dieser x-Achse.",
  "model": "google_nmt",
  "from_community_srt": "Der Spann von î ist die x-Achse, und der ersten Spalte der Matrix können wir entnehmen, dass î sich um das Dreifache auf der x-Achse verlängert.",
  "n_reviews": 0,
  "start": 134.64,
  "end": 144.12
 },
 {
  "input": "What's more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3, and hence remains on its own span.",
  "translatedText": "Darüber hinaus wird aufgrund der Funktionsweise linearer Transformationen auch jeder andere Vektor auf der x-Achse nur um den Faktor 3 gestreckt und bleibt somit auf seiner eigenen Spanne.",
  "model": "google_nmt",
  "from_community_srt": "Zudem wird aufgrund der Weise, wie Transformationen funktionieren, jeder andere Vektor auf der x-Achse um das Dreifache gestreckt wird und daher auf seinem Spann bleibt.",
  "n_reviews": 0,
  "start": 146.32,
  "end": 156.48
 },
 {
  "input": "A slightly sneakier vector that remains on its own span during this transformation is negative 1, 1.",
  "translatedText": "Ein etwas hinterhältigerer Vektor, der während dieser Transformation auf seiner eigenen Spanne bleibt, ist negativ 1, 1.",
  "model": "google_nmt",
  "from_community_srt": "Ein etwas unauffälligerer Vektor,",
  "n_reviews": 0,
  "start": 158.5,
  "end": 164.04
 },
 {
  "input": "It ends up getting stretched by a factor of 2.",
  "translatedText": "Am Ende wird es um den Faktor 2 gedehnt.",
  "model": "google_nmt",
  "from_community_srt": "der auf seinem Spann bleibt ist (-1 | 1), wird um einen Faktor von zwei gestreckt.",
  "n_reviews": 0,
  "start": 164.66,
  "end": 167.14
 },
 {
  "input": "And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2.",
  "translatedText": "Und wiederum bedeutet Linearität, dass jeder andere Vektor auf der von diesem Kerl aufgespannten Diagonalen einfach um den Faktor 2 gedehnt wird.",
  "model": "google_nmt",
  "from_community_srt": "Und erneut impliziert die Linearität, dass jeder andere Vektor auf der diagonalen Gerade, die von dem Kerl aufgespannt wird, nur um einen Faktor von zwei gestreckt wird.",
  "n_reviews": 0,
  "start": 169.0,
  "end": 178.22
 },
 {
  "input": "And for this transformation, those are all the vectors with this special property of staying on their span.",
  "translatedText": "Und für diese Transformation sind das alles Vektoren mit der besonderen Eigenschaft, auf ihrer Spanne zu bleiben.",
  "model": "google_nmt",
  "from_community_srt": "Und für diese Transformation sind das alle Vektoren mit der speziellen Eigenschaft, dass sie auf ihrem Spann bleiben.",
  "n_reviews": 0,
  "start": 179.82,
  "end": 185.18
 },
 {
  "input": "Those on the x-axis getting stretched out by a factor of 3, and those on this diagonal line getting stretched by a factor of 2.",
  "translatedText": "Die auf der X-Achse werden um den Faktor 3 gestreckt, die auf dieser Diagonalen um den Faktor 2.",
  "model": "google_nmt",
  "from_community_srt": "Diejenigen, die auf der x-Achse um einen Faktor von drei gestreckt werden, und die auf dieser diagonalen Geraden, die auf das Doppelte gestreckt werden.",
  "n_reviews": 0,
  "start": 185.62,
  "end": 191.98
 },
 {
  "input": "Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans.",
  "translatedText": "Jeder andere Vektor wird während der Transformation etwas gedreht und von der Linie entfernt, die er aufspannt.",
  "model": "google_nmt",
  "from_community_srt": "Jeder andere Vektor wird während der Transformation irgendwie gedreht, weg von der Geraden die er aufspannt.",
  "n_reviews": 0,
  "start": 192.76,
  "end": 198.08
 },
 {
  "input": "As you might have guessed by now, these special vectors are called the eigenvectors of the transformation, and each eigenvector has associated with it what's called an eigenvalue, which is just the factor by which it's stretched or squished during the transformation.",
  "translatedText": "Wie Sie vielleicht schon erraten haben, werden diese speziellen Vektoren Eigenvektoren der Transformation genannt, und jedem Eigenvektor ist ein sogenannter Eigenwert zugeordnet, bei dem es sich lediglich um den Faktor handelt, um den er während der Transformation gedehnt oder gestaucht wird.",
  "model": "google_nmt",
  "from_community_srt": "Wie Ihr vielleicht schon erraten habt, diese speziellen Vektoren werden \"Eigenvektoren\" der Transformation genannt, und jeder Eigenvektor wird mit einem sogenannten \"Eigenwert\" assoziiert, der einfach dem Faktor entspricht, mit dem er während der Transformation gestreckt oder gestaucht wird.",
  "n_reviews": 0,
  "start": 202.52,
  "end": 217.38
 },
 {
  "input": "Of course, there's nothing special about stretching versus squishing, or the fact that these eigenvalues happen to be positive.",
  "translatedText": "Natürlich ist das Dehnen im Vergleich zum Quetschen nichts Besonderes oder die Tatsache, dass diese Eigenwerte zufällig positiv sind.",
  "model": "google_nmt",
  "from_community_srt": "Natürlich ist das \"Strecken\" und \"Stauchen\" nichts besonderes oder, dass diese Eigenwerte positiv sind.",
  "n_reviews": 0,
  "start": 220.28,
  "end": 225.94
 },
 {
  "input": "In another example, you could have an eigenvector with eigenvalue negative 1 half, meaning that the vector gets flipped and squished by a factor of 1 half.",
  "translatedText": "In einem anderen Beispiel könnten Sie einen Eigenvektor mit einem Eigenwert von minus 1 Hälfte haben, was bedeutet, dass der Vektor um den Faktor 1 Hälfte gespiegelt und gestaucht wird.",
  "model": "google_nmt",
  "from_community_srt": "In einem anderen Beispiel, könnte man einen Eigenvektor mit einem Eigenwert von -1/2 haben, was bedeute, dass der Vektor umgedreht und um den Faktor zwei gestaucht wird.",
  "n_reviews": 0,
  "start": 226.38,
  "end": 235.12
 },
 {
  "input": "But the important part here is that it stays on the line that it spans out without getting rotated off of it.",
  "translatedText": "Aber der wichtige Teil hier ist, dass es auf der Linie bleibt, die es überspannt, ohne dass es von dieser abgedreht wird.",
  "model": "google_nmt",
  "from_community_srt": "Hauptsache er bleibt auf der Geraden, die er aufspannt, ohne weggedreht zu werden.",
  "n_reviews": 0,
  "start": 236.98,
  "end": 242.76
 },
 {
  "input": "For a glimpse of why this might be a useful thing to think about, consider some three-dimensional rotation.",
  "translatedText": "Um einen Eindruck davon zu bekommen, warum es sinnvoll sein könnte, darüber nachzudenken, betrachten Sie eine dreidimensionale Rotation.",
  "model": "google_nmt",
  "from_community_srt": "Um einen Einblick zu bekommen, warum es sinnvoll wäre darüber nachzudenken, sollte man sich eine dreidimensionale Rotation vorstellen.",
  "n_reviews": 0,
  "start": 244.46,
  "end": 249.8
 },
 {
  "input": "If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the axis of rotation.",
  "translatedText": "Wenn Sie einen Eigenvektor für diese Rotation finden können, einen Vektor, der auf seiner eigenen Spanne bleibt, haben Sie die Rotationsachse gefunden.",
  "model": "google_nmt",
  "from_community_srt": "Findet man einen Eigenvektor dieser Rotation, einen Vektor der auf seinem Spann bleibt, hat man die Rotationsachse gefunden.",
  "n_reviews": 0,
  "start": 251.66,
  "end": 260.5
 },
 {
  "input": "And it's much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which it's rotating, rather than thinking about the full 3x3 matrix associated with that transformation.",
  "translatedText": "Und es ist viel einfacher, sich eine 3D-Rotation im Hinblick auf eine Rotationsachse und einen Winkel vorzustellen, um den sie sich dreht, als über die vollständige 3x3-Matrix nachzudenken, die mit dieser Transformation verbunden ist.",
  "model": "google_nmt",
  "from_community_srt": "Und es ist viel einfacher sich eine 3D- Drehbewegung als Rotationsachse und -winkel vorzustellen, als als volle 3x3-Matrix, die diese Transformation beschreibt.",
  "n_reviews": 0,
  "start": 262.6,
  "end": 274.74
 },
 {
  "input": "In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never stretch or squish anything, so the length of the vector would remain the same.",
  "translatedText": "In diesem Fall müsste der entsprechende Eigenwert übrigens 1 sein, da Rotationen nie etwas strecken oder stauchen, die Länge des Vektors also gleich bleiben würde.",
  "model": "google_nmt",
  "from_community_srt": "In diesem Fall müsste der entsprechende Eigenwert 1 betragen, nachdem Rotationen nichts strecken oder stauchen und so die Länge des Vektors dieselbe bleibt.",
  "n_reviews": 0,
  "start": 277.0,
  "end": 285.86
 },
 {
  "input": "This pattern shows up a lot in linear algebra.",
  "translatedText": "Dieses Muster kommt in der linearen Algebra häufig vor.",
  "model": "google_nmt",
  "from_community_srt": "Dieses Muster taucht oft in linearer Algebra auf.",
  "n_reviews": 0,
  "start": 288.08,
  "end": 290.02
 },
 {
  "input": "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors.",
  "translatedText": "Bei jeder linearen Transformation, die durch eine Matrix beschrieben wird, können Sie verstehen, was sie bewirkt, indem Sie die Spalten dieser Matrix als Landepunkte für Basisvektoren ablesen.",
  "model": "google_nmt",
  "from_community_srt": "Man kann verstehen was jede lineare Transformation, die durch eine Matrix beschrieben wird, tut, wenn man die Spalten jener Matrix als Landepunkte für die Eigenvektoren liest.",
  "n_reviews": 0,
  "start": 290.44,
  "end": 299.4
 },
 {
  "input": "But often, a better way to get at the heart of what the linear transformation actually does, less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues.",
  "translatedText": "Eine bessere Möglichkeit, den Kern dessen zu verstehen, was die lineare Transformation tatsächlich bewirkt, und die weniger von Ihrem speziellen Koordinatensystem abhängt, besteht jedoch häufig darin, die Eigenvektoren und Eigenwerte zu ermitteln.",
  "model": "google_nmt",
  "from_community_srt": "Aber um zu verstehen, was eine lineare Transformation macht, unabhängiger vom eigenen Koordinatensystem, ist es oft besser, die Eigenvektoren und Eigenwerte zu finden.",
  "n_reviews": 0,
  "start": 300.02,
  "end": 310.82
 },
 {
  "input": "I won't cover the full details on methods for computing eigenvectors and eigenvalues here, but I'll try to give an overview of the computational ideas that are most important for a conceptual understanding.",
  "translatedText": "Ich werde hier nicht alle Details zu Methoden zur Berechnung von Eigenvektoren und Eigenwerten behandeln, aber ich werde versuchen, einen Überblick über die Berechnungsideen zu geben, die für ein konzeptionelles Verständnis am wichtigsten sind.",
  "model": "google_nmt",
  "from_community_srt": "I werde hier nicht alle Details zur Berechnung von Eigenvektoren und Eigenwerten abdecken, aber ich werde versuchen einen Überblick über die rechnerischen Grundideen zu bieten, die am wichtigsten für das Konzeptverständnis sind.",
  "n_reviews": 0,
  "start": 315.46,
  "end": 326.02
 },
 {
  "input": "Symbolically, here's what the idea of an eigenvector looks like.",
  "translatedText": "Symbolisch gesehen sieht die Idee eines Eigenvektors wie folgt aus.",
  "model": "google_nmt",
  "from_community_srt": "Symbolisch, hier ist, wie die Idee eines Eigenvektors aussieht.",
  "n_reviews": 0,
  "start": 327.18,
  "end": 330.48
 },
 {
  "input": "A is the matrix representing some transformation, with v as the eigenvector, and lambda is a number, namely the corresponding eigenvalue.",
  "translatedText": "A ist die Matrix, die eine Transformation darstellt, mit v als Eigenvektor und Lambda ist eine Zahl, nämlich der entsprechende Eigenwert.",
  "model": "google_nmt",
  "from_community_srt": "A sei eine Matrix, die eine gewisse Transformation darstellt, mit v als Eigenvektor, und Lambda als Entsprechenden Eigenwert.",
  "n_reviews": 0,
  "start": 331.04,
  "end": 339.74
 },
 {
  "input": "What this expression is saying is that the matrix-vector product, A times v, gives the same result as just scaling the eigenvector v by some value lambda.",
  "translatedText": "Dieser Ausdruck besagt, dass das Matrix-Vektor-Produkt A mal v das gleiche Ergebnis liefert wie die einfache Skalierung des Eigenvektors v um einen Lambda-Wert.",
  "model": "google_nmt",
  "from_community_srt": "Was dieser Ausdruck besagt ist, dass das Matrix-Vektor-Produkt A ⋅ v das gleiche Ergebnis liefert, wie wenn man den Eigenvektor v um einen Wert Lambda skaliert.",
  "n_reviews": 0,
  "start": 340.68,
  "end": 349.9
 },
 {
  "input": "So finding the eigenvectors and their eigenvalues of a matrix A comes down to finding the values of v and lambda that make this expression true.",
  "translatedText": "Um die Eigenvektoren und ihre Eigenwerte einer Matrix A zu finden, kommt es also darauf an, die Werte von v und Lambda zu finden, die diesen Ausdruck wahr machen.",
  "model": "google_nmt",
  "from_community_srt": "Also findet man im Endeffekt die Eigenvektoren und Eigenwerte der Matrix A, wenn man die Werte für v und Lambda findet, die diesen Ausdruck wahr machen.",
  "n_reviews": 0,
  "start": 351.0,
  "end": 360.1
 },
 {
  "input": "It's a little awkward to work with at first, because that left-hand side represents matrix-vector multiplication, but the right-hand side here is scalar-vector multiplication.",
  "translatedText": "Es ist zunächst etwas umständlich, damit zu arbeiten, da die linke Seite die Matrix-Vektor-Multiplikation darstellt, die rechte Seite hier jedoch die Skalar-Vektor-Multiplikation.",
  "model": "google_nmt",
  "from_community_srt": "Es ist erstmals etwas unangenehm damit zu arbeiten, weil die linke Seite eine Matrix-Vektor-Multiplikation, aber die rechte Seine ein Skalarprodukt beinhaltet.",
  "n_reviews": 0,
  "start": 361.92,
  "end": 370.54
 },
 {
  "input": "So let's start by rewriting that right-hand side as some kind of matrix-vector multiplication, using a matrix which has the effect of scaling any vector by a factor of lambda.",
  "translatedText": "Beginnen wir also damit, die rechte Seite als eine Art Matrix-Vektor-Multiplikation umzuschreiben und dabei eine Matrix zu verwenden, die den Effekt hat, jeden Vektor um einen Lambda-Faktor zu skalieren.",
  "model": "google_nmt",
  "from_community_srt": "Also beginnen wir damit, dass wir die Rechte Seite als eine Art Matrix-Vektor-Multiplikation scheiben, indem wir eine Matrix verwenden, die den Effekt einer Skalierung um den Faktor Lambda hat.",
  "n_reviews": 0,
  "start": 371.12,
  "end": 380.62
 },
 {
  "input": "The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply multiplied by lambda, so this matrix will have the number lambda down the diagonal, with zeros everywhere else.",
  "translatedText": "Die Spalten einer solchen Matrix stellen dar, was mit jedem Basisvektor passiert, und jeder Basisvektor wird einfach mit Lambda multipliziert, sodass diese Matrix entlang der Diagonale die Zahl Lambda hat und an allen anderen Stellen Nullen.",
  "model": "google_nmt",
  "from_community_srt": "Die Spalten dieser Matrix repräsentieren, was mit den jeweiligen Basisvektoren passiert, und da jeder Basisvektor einfach mit Lambda multipliziert wird, hat diese Matrix in der Diagonalen der Wert Lambda und 0 überall sonst.",
  "n_reviews": 0,
  "start": 381.68,
  "end": 394.32
 },
 {
  "input": "The common way to write this guy is to factor that lambda out and write it as lambda times i, where i is the identity matrix with 1s down the diagonal.",
  "translatedText": "Die übliche Art, diesen Kerl zu schreiben, besteht darin, das Lambda herauszurechnen und es als Lambda mal i zu schreiben, wobei i die Identitätsmatrix mit Einsen entlang der Diagonale ist.",
  "model": "google_nmt",
  "from_community_srt": "Üblicherweise wird das Lambda herausgehoben und es wird als Lambda mal I geschrieben, mit I als Einheitsmatrix mit Einsern hinunter in der Diagonalen.",
  "n_reviews": 0,
  "start": 396.18,
  "end": 404.86
 },
 {
  "input": "With both sides looking like matrix-vector multiplication, we can subtract off that right-hand side and factor out the v.",
  "translatedText": "Da beide Seiten wie eine Matrix-Vektor-Multiplikation aussehen, können wir diese rechte Seite subtrahieren und v herausrechnen.",
  "model": "google_nmt",
  "from_community_srt": "Wenn beide Seiten, wie eine Vektor-Matrix-Multiplikation aussehen, kann man die rechte Seite subtrahieren und v ausklammern.",
  "n_reviews": 0,
  "start": 405.86,
  "end": 411.86
 },
 {
  "input": "So what we now have is a new matrix, A minus lambda times the identity, and we're looking for a vector v such that this new matrix times v gives the zero vector.",
  "translatedText": "Was wir jetzt haben, ist eine neue Matrix, A minus Lambda multipliziert mit der Identität, und wir suchen nach einem Vektor v, so dass diese neue Matrix multipliziert mit v den Nullvektor ergibt.",
  "model": "google_nmt",
  "from_community_srt": "Jetzt haben wir eine neue Matrix A minus Lambda mal der Einheitsmatrix, und jetzt suchen wir einen Vektor v, sodass die neue Matrix mal v den Nullvektor ergibt.",
  "n_reviews": 0,
  "start": 414.16,
  "end": 424.92
 },
 {
  "input": "Now, this will always be true if v itself is the zero vector, but that's boring.",
  "translatedText": "Das gilt zwar immer, wenn v selbst der Nullvektor ist, aber das ist langweilig.",
  "model": "google_nmt",
  "from_community_srt": "Jetzt wird das auch immer wahr sein, wenn  v der Nullvektor ist, aber das ist uninteressant.",
  "n_reviews": 0,
  "start": 426.38,
  "end": 431.1
 },
 {
  "input": "What we want is a non-zero eigenvector.",
  "translatedText": "Was wir wollen, ist ein Eigenvektor ungleich Null.",
  "model": "google_nmt",
  "from_community_srt": "Was wir wollen ist ein Eigenvektor v ≠ 0.",
  "n_reviews": 0,
  "start": 431.34,
  "end": 433.64
 },
 {
  "input": "And if you watch chapter 5 and 6, you'll know that the only way it's possible for the product of a matrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.",
  "translatedText": "Und wenn Sie sich Kapitel 5 und 6 ansehen, wissen Sie, dass das Produkt einer Matrix mit einem Vektor ungleich Null nur dann zu Null werden kann, wenn die mit dieser Matrix verbundene Transformation den Raum in eine niedrigere Dimension quetscht.",
  "model": "google_nmt",
  "from_community_srt": "Wenn Du Kapitel 5 und 6 gesehen hat, wirst du wissen, dass die einzige Möglichkeit, wie das Produkt einer Matrix mit einem Vektor v ≠ 0, null ergeben kann, ist, wenn die mit der Matrix assoziierte Transformation den Raum in eine niedrigere Dimension quetscht.",
  "n_reviews": 0,
  "start": 434.42,
  "end": 448.02
 },
 {
  "input": "And that squishification corresponds to a zero determinant for the matrix.",
  "translatedText": "Und diese Quetschung entspricht einer Nulldeterminante für die Matrix.",
  "model": "google_nmt",
  "from_community_srt": "Und diese Quetschung entspricht einer Null-Determinante für die Matrix.",
  "n_reviews": 0,
  "start": 449.3,
  "end": 454.22
 },
 {
  "input": "To be concrete, let's say your matrix A has columns 2, 1 and 2, 3, and think about subtracting off a variable amount, lambda, from each diagonal entry.",
  "translatedText": "Um konkret zu sein: Nehmen wir an, Ihre Matrix A hat die Spalten 2, 1 und 2, 3 und überlegen Sie, von jedem diagonalen Eintrag einen variablen Betrag, Lambda, abzuziehen.",
  "model": "google_nmt",
  "from_community_srt": "Konkret sei A eine Matrix mit den Spalten (2 | 1) und (2 | 3), und Lambda ein variabler Wert, der von jedem diagonalen Eintrag subtrahiert wird.",
  "n_reviews": 0,
  "start": 455.48,
  "end": 465.52
 },
 {
  "input": "Now imagine tweaking lambda, turning a knob to change its value.",
  "translatedText": "Stellen Sie sich nun vor, Sie optimieren Lambda und drehen einen Knopf, um seinen Wert zu ändern.",
  "model": "google_nmt",
  "from_community_srt": "Man stelle sich vor, dass man Lambda wie mit einem Drehregler verändert.",
  "n_reviews": 0,
  "start": 466.48,
  "end": 470.28
 },
 {
  "input": "As that value of lambda changes, the matrix itself changes, and so the determinant of the matrix changes.",
  "translatedText": "Wenn sich dieser Lambda-Wert ändert, ändert sich auch die Matrix selbst und damit auch die Determinante der Matrix.",
  "model": "google_nmt",
  "from_community_srt": "Und wenn sich der Wert von Lambda ändert, verändert sich die Matrix selbst, und somit ihre Determinante.",
  "n_reviews": 0,
  "start": 470.94,
  "end": 477.24
 },
 {
  "input": "The goal here is to find a value of lambda that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension.",
  "translatedText": "Das Ziel hier besteht darin, einen Lambda-Wert zu finden, der diese Determinante auf Null setzt, was bedeutet, dass die optimierte Transformation den Raum in eine niedrigere Dimension zerquetscht.",
  "model": "google_nmt",
  "from_community_srt": "Das Ziel hier ist es einen Wert für Lambda zu finden, bei dem die Determinante der Matrix 0 ist, was bedeutet, dass die angepasste Transformation den Raum in eine niedrigere Dimension quetscht.",
  "n_reviews": 0,
  "start": 478.22,
  "end": 487.24
 },
 {
  "input": "In this case, the sweet spot comes when lambda equals 1.",
  "translatedText": "In diesem Fall entsteht der Sweet Spot, wenn Lambda gleich 1 ist.",
  "model": "google_nmt",
  "from_community_srt": "In diesem Fall, dieser Sweetspot wird erreicht, wenn Lambda 1 ist.",
  "n_reviews": 0,
  "start": 488.16,
  "end": 491.16
 },
 {
  "input": "Of course, if we had chosen some other matrix, the eigenvalue might not necessarily be 1.",
  "translatedText": "Wenn wir eine andere Matrix gewählt hätten, wäre der Eigenwert natürlich nicht unbedingt 1.",
  "model": "google_nmt",
  "from_community_srt": "Wenn wir eine andere Matrix gewählt hätten, müsste der Eigenwert natürlich nicht unbedingt 1 entsprechen,",
  "n_reviews": 0,
  "start": 492.18,
  "end": 496.12
 },
 {
  "input": "The sweet spot might be hit at some other value of lambda.",
  "translatedText": "Der Sweet Spot könnte bei einem anderen Lambda-Wert erreicht werden.",
  "model": "google_nmt",
  "from_community_srt": "der Sweetspot könnte durch einen anderen Wert von Lambda erreicht werden.",
  "n_reviews": 0,
  "start": 496.24,
  "end": 498.6
 },
 {
  "input": "So this is kind of a lot, but let's unravel what this is saying.",
  "translatedText": "Das ist also ziemlich viel, aber lassen Sie uns herausfinden, was damit gemeint ist.",
  "model": "google_nmt",
  "from_community_srt": "Das ist irgendwie viel, aber lasst uns den Sinn davon entwirren.",
  "n_reviews": 0,
  "start": 500.08,
  "end": 502.96
 },
 {
  "input": "When lambda equals 1, the matrix A minus lambda times the identity squishes space onto a line.",
  "translatedText": "Wenn Lambda gleich 1 ist, quetscht die Matrix A minus Lambda multipliziert mit der Identität Platz auf eine Linie.",
  "model": "google_nmt",
  "from_community_srt": "Wenn Lambda 1 entspricht, quetscht die Matrix A - I ⋅ λ den Raum in eine Gerade.",
  "n_reviews": 0,
  "start": 502.96,
  "end": 509.56
 },
 {
  "input": "That means there's a non-zero vector v such that A minus lambda times the identity times v equals the zero vector.",
  "translatedText": "Das bedeutet, dass es einen Vektor v ungleich Null gibt, sodass A minus Lambda mal Identität mal v gleich dem Nullvektor ist.",
  "model": "google_nmt",
  "from_community_srt": "Das bedeutet, es gibt einen Vektor v ≠ 0, sodass A minus Lambda mal der Einheitsmatrix mal v dem Nullvektor entspricht.",
  "n_reviews": 0,
  "start": 510.44,
  "end": 518.56
 },
 {
  "input": "And remember, the reason we care about that is because it means A times v equals lambda times v, which you can read off as saying that the vector v is an eigenvector of A, staying on its own span during the transformation A.",
  "translatedText": "Und denken Sie daran, der Grund, warum uns das interessiert, ist, dass es bedeutet, dass A mal v gleich Lambda mal v ist, was man so interpretieren kann, dass der Vektor v ein Eigenvektor von A ist und während der Transformation A auf seiner eigenen Spanne bleibt.",
  "model": "google_nmt",
  "from_community_srt": "Zur Erinnerung, der Grund dafür, dass uns das interessiert, ist, dass es bedeutet, dass A ⋅ v = λ ⋅ v, was heißt, dass v ein Eigenvektor von A ist, und während der Transformation auf seinem Spann bleibt.",
  "n_reviews": 0,
  "start": 520.48,
  "end": 537.28
 },
 {
  "input": "In this example, the corresponding eigenvalue is 1, so v would actually just stay fixed in place.",
  "translatedText": "In diesem Beispiel ist der entsprechende Eigenwert 1, sodass v eigentlich einfach an seinem Platz bleiben würde.",
  "model": "google_nmt",
  "from_community_srt": "In diesem Beispiel ist der entsprechende Eigenwert 1, also wäre v nur ein fixer Punkt.",
  "n_reviews": 0,
  "start": 538.32,
  "end": 544.02
 },
 {
  "input": "Pause and ponder if you need to make sure that that line of reasoning feels good.",
  "translatedText": "Halten Sie inne und überlegen Sie, ob Sie sicherstellen müssen, dass sich diese Argumentation gut anfühlt.",
  "model": "google_nmt",
  "from_community_srt": "Pausiert und spult zurück um diesen Argumentationsweg nachzuvollziehe.",
  "n_reviews": 0,
  "start": 546.22,
  "end": 549.5
 },
 {
  "input": "This is the kind of thing I mentioned in the introduction.",
  "translatedText": "So etwas habe ich in der Einleitung erwähnt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.38,
  "end": 555.64
 },
 {
  "input": "If you didn't have a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions, an expression like this would feel completely out of the blue.",
  "translatedText": "Wenn Sie kein solides Verständnis für Determinanten hätten und wissen würden, warum sie sich auf lineare Gleichungssysteme mit Lösungen ungleich Null beziehen, würde ein Ausdruck wie dieser völlig aus heiterem Himmel erscheinen.",
  "model": "google_nmt",
  "from_community_srt": "Das habe ich in der Einleitung gemeint, wenn man keinen Dunst von Determinanten hat, und was sie mit linearen Gleichungssystemen mit Lösungen ≠ 0 zu tun haben, scheint ein Ausdruck wie dieser aus der Luft gegriffen.",
  "n_reviews": 0,
  "start": 556.22,
  "end": 566.3
 },
 {
  "input": "To see this in action, let's revisit the example from the start, with a matrix whose columns are 3, 0 and 1, 2.",
  "translatedText": "Um dies in Aktion zu sehen, schauen wir uns das Beispiel noch einmal von Anfang an an, mit einer Matrix, deren Spalten 3, 0 und 1, 2 sind.",
  "model": "google_nmt",
  "from_community_srt": "Um das in Aktion zu sehen, kehren wir zum Beispiel vom Anfang zurück mit einer Matrix, deren Spalten (3 | 0) und (1 | 2) sind.",
  "n_reviews": 0,
  "start": 568.32,
  "end": 574.54
 },
 {
  "input": "To find if a value lambda is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant.",
  "translatedText": "Um herauszufinden, ob ein Wert Lambda ein Eigenwert ist, subtrahieren Sie ihn von den Diagonalen dieser Matrix und berechnen Sie die Determinante.",
  "model": "google_nmt",
  "from_community_srt": "Um herauszufinden ob Lambda ein Eigenwert ist, subtrahiert man es von der Diagonale der Matrix und berechnet die Determinante.",
  "n_reviews": 0,
  "start": 575.35,
  "end": 583.4
 },
 {
  "input": "Doing this, we get a certain quadratic polynomial in lambda, 3 minus lambda times 2 minus lambda.",
  "translatedText": "Dadurch erhalten wir ein bestimmtes quadratisches Polynom in Lambda, 3 minus Lambda mal 2 minus Lambda.",
  "model": "google_nmt",
  "from_community_srt": "Dadurch erhält man eine quadratisches Polynom in Lambda, (3-λ)(2-λ).",
  "n_reviews": 0,
  "start": 590.58,
  "end": 596.72
 },
 {
  "input": "Since lambda can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are lambda equals 2 and lambda equals 3.",
  "translatedText": "Da Lambda nur dann ein Eigenwert sein kann, wenn diese Determinante zufällig Null ist, können Sie daraus schließen, dass die einzig möglichen Eigenwerte Lambda gleich 2 und Lambda gleich 3 sind.",
  "model": "google_nmt",
  "from_community_srt": "Da Lambda nur ein Eigenwert sein kann, wenn die Determinante 0 ist, kann man schlussfolgern, dass die einzigen Eigenwerte λ = 2 und λ = 3 sind.",
  "n_reviews": 0,
  "start": 597.8,
  "end": 608.84
 },
 {
  "input": "To figure out what the eigenvectors are that actually have one of these eigenvalues, say lambda equals 2, plug in that value of lambda to the matrix and then solve for which vectors this diagonally altered matrix sends to zero.",
  "translatedText": "Um herauszufinden, welche Eigenvektoren tatsächlich einen dieser Eigenwerte haben, sagen wir, Lambda ist gleich 2, fügen Sie diesen Lambda-Wert in die Matrix ein und lösen Sie dann, welche Vektoren diese diagonal veränderte Matrix auf Null sendet.",
  "model": "google_nmt",
  "from_community_srt": "Um die Eigenvektoren herauszufinden, die einen dieser Eigenwerte haben, sei λ = 2, setzt man den Wert von Lambda in die Matrix ein und dann löst man für welche Vektoren diese diagonal veränderte Matrix 0 zu einem Ergebnis von 0 führt.",
  "n_reviews": 0,
  "start": 609.64,
  "end": 623.9
 },
 {
  "input": "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by negative 1, 1.",
  "translatedText": "Wenn Sie dies wie jedes andere lineare System berechnen würden, würden Sie sehen, dass die Lösungen alle Vektoren auf der diagonalen Linie sind, die durch minus 1, 1 aufgespannt wird.",
  "model": "google_nmt",
  "from_community_srt": "Würde man es so wie jedes andere Gleichungssystem berechnen, sähe man, dass die Lösungen alle Vektoren sind, die auf der Diagonalen liegen, die von (-1 | 1) aufgespannt wird.",
  "n_reviews": 0,
  "start": 624.94,
  "end": 634.3
 },
 {
  "input": "This corresponds to the fact that the unaltered matrix, 3, 0, 1, 2, has the effect of stretching all those vectors by a factor of 2.",
  "translatedText": "Dies entspricht der Tatsache, dass die unveränderte Matrix 3, 0, 1, 2 den Effekt hat, alle diese Vektoren um den Faktor 2 zu strecken.",
  "model": "google_nmt",
  "from_community_srt": "Das ist darauf zurückzuführen, dass die unveränderte Matrix [(3 | 0), (1 | 2)] die Eigenschaft hat, jene Vektoren um den Faktor 2 zu strecken.",
  "n_reviews": 0,
  "start": 635.22,
  "end": 643.46
 },
 {
  "input": "Now, a 2D transformation doesn't have to have eigenvectors.",
  "translatedText": "Nun muss eine 2D-Transformation keine Eigenvektoren haben.",
  "model": "google_nmt",
  "from_community_srt": "Nun, eine 2D-Transformation muss keine Eigenvektoren haben.",
  "n_reviews": 0,
  "start": 646.32,
  "end": 650.2
 },
 {
  "input": "For example, consider a rotation by 90 degrees.",
  "translatedText": "Betrachten Sie beispielsweise eine Drehung um 90 Grad.",
  "model": "google_nmt",
  "from_community_srt": "Man nehme beispielsweise eine 90°-Rotation.",
  "n_reviews": 0,
  "start": 650.72,
  "end": 653.4
 },
 {
  "input": "This doesn't have any eigenvectors since it rotates every vector off of its own span.",
  "translatedText": "Dies hat keine Eigenvektoren, da es jeden Vektor aus seiner eigenen Spanne dreht.",
  "model": "google_nmt",
  "from_community_srt": "Sie hat keine Eigenvektoren, da sie jeden Vektor von seinem Spann wegdreht.",
  "n_reviews": 0,
  "start": 653.66,
  "end": 658.2
 },
 {
  "input": "If you actually try computing the eigenvalues of a rotation like this, notice what happens.",
  "translatedText": "Wenn Sie tatsächlich versuchen, die Eigenwerte einer solchen Drehung zu berechnen, beachten Sie, was passiert.",
  "model": "google_nmt",
  "from_community_srt": "Sieh, was passiert, wenn man versucht die Eigenwerte von so einer Rotation zu ermitteln.",
  "n_reviews": 0,
  "start": 660.8,
  "end": 665.56
 },
 {
  "input": "Its matrix has columns 0, 1 and negative 1, 0.",
  "translatedText": "Seine Matrix hat die Spalten 0, 1 und negativ 1, 0.",
  "model": "google_nmt",
  "from_community_srt": "Die Matrix dieser Rotation hat die Spalten (0 | 1) und (-1 | 0), man subtrahiere Lambda von den diagonalen Elementen und,",
  "n_reviews": 0,
  "start": 666.3,
  "end": 670.14
 },
 {
  "input": "Subtract off lambda from the diagonal elements and look for when the determinant is zero.",
  "translatedText": "Subtrahieren Sie Lambda von den Diagonalelementen und suchen Sie, wann die Determinante Null ist.",
  "model": "google_nmt",
  "from_community_srt": "suche den Wert, wo die Determinante 0 ist.",
  "n_reviews": 0,
  "start": 671.1,
  "end": 675.8
 },
 {
  "input": "In this case, you get the polynomial lambda squared plus 1.",
  "translatedText": "In diesem Fall erhält man das Polynom Lambda zum Quadrat plus 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 678.14,
  "end": 681.94
 },
 {
  "input": "The only roots of that polynomial are the imaginary numbers, i and negative i.",
  "translatedText": "Die einzigen Wurzeln dieses Polynoms sind die imaginären Zahlen i und negativ i.",
  "model": "google_nmt",
  "from_community_srt": "In diesem Fall erhält man das Polynom λ^2+1, und die einzigen Nullstellen von diesem Polynom sind die imaginären Zahlen i und -i.",
  "n_reviews": 0,
  "start": 682.68,
  "end": 687.92
 },
 {
  "input": "The fact that there are no real number solutions indicates that there are no eigenvectors.",
  "translatedText": "Die Tatsache, dass es keine reellen Zahlenlösungen gibt, weist darauf hin, dass es keine Eigenvektoren gibt.",
  "model": "google_nmt",
  "from_community_srt": "Die Tatsache, dass es keine reelle Lösung gibt, weist darauf hin, dass es keine Eigenvektoren gibt.",
  "n_reviews": 0,
  "start": 688.84,
  "end": 693.6
 },
 {
  "input": "Another pretty interesting example worth holding in the back of your mind is a shear.",
  "translatedText": "Ein weiteres ziemlich interessantes Beispiel, das es wert ist, im Hinterkopf zu behalten, ist eine Schere.",
  "model": "google_nmt",
  "from_community_srt": "Ein anderes sehr interessantes Beispiel, das es wert ist, im Gedächtnis zu behalten, ist ein \"Shear\".",
  "n_reviews": 0,
  "start": 695.54,
  "end": 699.82
 },
 {
  "input": "This fixes i-hat in place and moves j-hat 1 over, so its matrix has columns 1, 0 and 1, 1.",
  "translatedText": "Dadurch wird i-hat an seinem Platz fixiert und j-hat 1 verschoben, sodass seine Matrix die Spalten 1, 0 und 1, 1 hat.",
  "model": "google_nmt",
  "from_community_srt": "Das hält î am Platz und bewegt und ĵ um eins nach rechts, also hat seine Matrix die Spalten (1 | 0) und (1 | 1).",
  "n_reviews": 0,
  "start": 700.56,
  "end": 707.84
 },
 {
  "input": "All of the vectors on the x-axis are eigenvectors with eigenvalue 1 since they remain fixed in place.",
  "translatedText": "Alle Vektoren auf der x-Achse sind Eigenvektoren mit Eigenwert 1, da sie ortsfest bleiben.",
  "model": "google_nmt",
  "from_community_srt": "Alle Vektoren auf der x-Achse sind Eigenvektoren mit Eigenwert 1, da sie am Platz bleiben.",
  "n_reviews": 0,
  "start": 708.74,
  "end": 714.54
 },
 {
  "input": "In fact, these are the only eigenvectors.",
  "translatedText": "Tatsächlich sind dies die einzigen Eigenvektoren.",
  "model": "google_nmt",
  "from_community_srt": "Tatsächlich sind sie die einzigen Eigenvektoren.",
  "n_reviews": 0,
  "start": 715.68,
  "end": 717.82
 },
 {
  "input": "When you subtract off lambda from the diagonals and compute the determinant, what you get is 1 minus lambda squared.",
  "translatedText": "Wenn Sie Lambda von den Diagonalen subtrahieren und die Determinante berechnen, erhalten Sie 1 minus Lambda im Quadrat.",
  "model": "google_nmt",
  "from_community_srt": "Wenn man Lambda von den Diagonalen subtrahiert und die Determinante berechnet, erhält man (1-λ)²,",
  "n_reviews": 0,
  "start": 718.76,
  "end": 726.54
 },
 {
  "input": "And the only root of this expression is lambda equals 1.",
  "translatedText": "Und die einzige Wurzel dieses Ausdrucks ist Lambda gleich 1.",
  "model": "google_nmt",
  "from_community_srt": "und die einzige Nullstelle dieses Ausdrucks ist 1.",
  "n_reviews": 0,
  "start": 729.32,
  "end": 732.86
 },
 {
  "input": "This lines up with what we see geometrically, that all of the eigenvectors have eigenvalue 1.",
  "translatedText": "Dies steht im Einklang mit dem, was wir geometrisch sehen, dass alle Eigenvektoren den Eigenwert 1 haben.",
  "model": "google_nmt",
  "from_community_srt": "Das stimmt mit dem überein, was man geometrisch sieht, nämlich, dass alle Eigenvektoren Eigenwerte von 1 haben.",
  "n_reviews": 0,
  "start": 734.56,
  "end": 739.72
 },
 {
  "input": "Keep in mind though, it's also possible to have just one eigenvalue, but with more than just a line full of eigenvectors.",
  "translatedText": "Beachten Sie jedoch, dass es auch möglich ist, nur einen Eigenwert zu haben, jedoch mit mehr als nur einer Linie voller Eigenvektoren.",
  "model": "google_nmt",
  "from_community_srt": "Man muss aber beachten, dass es auch möglich ist nur einen Eigenwert zu haben, aber mehr als eine Gerade an Eigenvektoren.",
  "n_reviews": 0,
  "start": 741.08,
  "end": 748.02
 },
 {
  "input": "A simple example is a matrix that scales everything by 2.",
  "translatedText": "Ein einfaches Beispiel ist eine Matrix, die alles um 2 skaliert.",
  "model": "google_nmt",
  "from_community_srt": "Ein simples Beispiel wäre eine Matrix,",
  "n_reviews": 0,
  "start": 749.9,
  "end": 753.18
 },
 {
  "input": "The only eigenvalue is 2, but every vector in the plane gets to be an eigenvector with that eigenvalue.",
  "translatedText": "Der einzige Eigenwert ist 2, aber jeder Vektor in der Ebene wird ein Eigenvektor mit diesem Eigenwert.",
  "model": "google_nmt",
  "from_community_srt": "die alles um den Faktor 2 skaliert, der einzige Eigenwert ist 2, aber jeder Vektor in der Ebene ist ein Eigenvektor mit diesem Eigenwert.",
  "n_reviews": 0,
  "start": 753.9,
  "end": 760.7
 },
 {
  "input": "Now is another good time to pause and ponder some of this before I move on to the last topic.",
  "translatedText": "Jetzt ist ein weiterer guter Zeitpunkt, innezuhalten und darüber nachzudenken, bevor ich zum letzten Thema übergehe.",
  "model": "google_nmt",
  "from_community_srt": "Jetzt ist wieder ein guter Zeitpunkt zum Pausieren und Zurückspulen, bevor ich mit dem letzen Thema fortfahre.",
  "n_reviews": 0,
  "start": 762.0,
  "end": 766.96
 },
 {
  "input": "I want to finish off here with the idea of an eigenbasis, which relies heavily on ideas from the last video.",
  "translatedText": "Ich möchte hier mit der Idee einer Eigenbasis abschließen, die stark auf Ideen aus dem letzten Video basiert.",
  "model": "google_nmt",
  "from_community_srt": "Ich möchte hier mit einer Idee von einer Eigenbasis abschließen, die sehr auf den Ideen des letzten Videos beruht.",
  "n_reviews": 0,
  "start": 783.54,
  "end": 789.88
 },
 {
  "input": "Take a look at what happens if our basis vectors just so happen to be eigenvectors.",
  "translatedText": "Schauen Sie sich an, was passiert, wenn unsere Basisvektoren zufällig Eigenvektoren sind.",
  "model": "google_nmt",
  "from_community_srt": "Seht, was passiert, wenn gerade die Basisvektoren Eigenvektoren sind.",
  "n_reviews": 0,
  "start": 791.48,
  "end": 796.38
 },
 {
  "input": "For example, maybe i-hat is scaled by negative 1 and j-hat is scaled by 2.",
  "translatedText": "Zum Beispiel könnte i-hat mit minus 1 skaliert werden und j-hat mit minus 2.",
  "model": "google_nmt",
  "from_community_srt": "Zum Beispiel werde  î um -1 skaliert und ĵ um 2.",
  "n_reviews": 0,
  "start": 797.12,
  "end": 802.38
 },
 {
  "input": "Writing their new coordinates as the columns of a matrix, notice that those scalar multiples, negative 1 and 2, which are the eigenvalues of i-hat and j-hat, sit on the diagonal of our matrix, and every other entry is a 0.",
  "translatedText": "Wenn Sie ihre neuen Koordinaten als Spalten einer Matrix schreiben, beachten Sie, dass diese skalaren Vielfachen, negativ 1 und 2, die die Eigenwerte von i-hat und j-hat sind, auf der Diagonale unserer Matrix liegen und jeder andere Eintrag eine 0 ist .",
  "model": "google_nmt",
  "from_community_srt": "Schreibt man ihre neuen Koordinaten als die Spalten einer Matrix, sieht man, dass die Skalarfaktoren -1 und 2, die die Eigenwerte von î und ĵ sind, sich auf der diagonalen der Matrix befinden und jeder andere Eintrag 0 ist.",
  "n_reviews": 0,
  "start": 803.42,
  "end": 817.18
 },
 {
  "input": "Any time a matrix has zeros everywhere other than the diagonal, it's called, reasonably enough, a diagonal matrix.",
  "translatedText": "Immer wenn eine Matrix überall außer der Diagonale Nullen hat, wird sie vernünftigerweise Diagonalmatrix genannt.",
  "model": "google_nmt",
  "from_community_srt": "Immer, wenn eine Matrix bis auf die Diagonale nur aus Nullen besteht, wird sie, aus gutem Grund, diagonale Matrix genannt.",
  "n_reviews": 0,
  "start": 818.88,
  "end": 825.42
 },
 {
  "input": "And the way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues.",
  "translatedText": "Dies lässt sich so interpretieren, dass alle Basisvektoren Eigenvektoren sind und die diagonalen Einträge dieser Matrix ihre Eigenwerte sind.",
  "model": "google_nmt",
  "from_community_srt": "Und das ist so zu interpretieren, dass alle Basisvektoren Eigenvektoren, und die diagonalen Werte deren Eigenwerte sind.",
  "n_reviews": 0,
  "start": 825.84,
  "end": 834.4
 },
 {
  "input": "There are a lot of things that make diagonal matrices much nicer to work with.",
  "translatedText": "Es gibt viele Dinge, die die Arbeit mit Diagonalmatrizen viel angenehmer machen.",
  "model": "google_nmt",
  "from_community_srt": "Es gibt viele Szenarien, in denen es viel einfacher ist mit diagonalen Matrizen zu arbeiten.",
  "n_reviews": 0,
  "start": 837.1,
  "end": 841.06
 },
 {
  "input": "One big one is that it's easier to compute what will happen if you multiply this matrix by itself a whole bunch of times.",
  "translatedText": "Ein großer Nachteil ist, dass es einfacher ist zu berechnen, was passiert, wenn man diese Matrix mehrmals mit sich selbst multipliziert.",
  "model": "google_nmt",
  "from_community_srt": "Ein entscheidendes ist, dass es einfacher ist zu berechnen, was passiert, wenn man diese Matrix sehr oft mit sich selbst multipliziert.",
  "n_reviews": 0,
  "start": 841.78,
  "end": 848.34
 },
 {
  "input": "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to correspond to scaling each basis vector by the 100th power of the corresponding eigenvalue.",
  "translatedText": "Da eine dieser Matrizen lediglich jeden Basisvektor um einen Eigenwert skaliert, entspricht die mehrmalige Anwendung dieser Matrix, beispielsweise 100 Mal, lediglich einer Skalierung jedes Basisvektors um die 100. Potenz des entsprechenden Eigenwerts.",
  "model": "google_nmt",
  "from_community_srt": "Das all diese Matrizen nur die einzelnen Eigenvektoren um einen bestimmen Eigenwert skalieren, ist es dasselbe, die Matrix sehr viele Male, sagen wir hundertmal, anzuwenden, wie jeden Basisvektor einfach nur um die hundertste Potenz des jeweiligen Eigenwerts zu skalieren.",
  "n_reviews": 0,
  "start": 849.42,
  "end": 864.6
 },
 {
  "input": "In contrast, try computing the 100th power of a non-diagonal matrix.",
  "translatedText": "Versuchen Sie im Gegensatz dazu, die 100. Potenz einer nichtdiagonalen Matrix zu berechnen.",
  "model": "google_nmt",
  "from_community_srt": "Zum Vergleich, versuch mal die hundertste Potenz einer nicht-diagonalen Matrix zu berechnen.",
  "n_reviews": 0,
  "start": 865.7,
  "end": 869.68
 },
 {
  "input": "Really, try it for a moment.",
  "translatedText": "Probieren Sie es wirklich einmal aus.",
  "model": "google_nmt",
  "from_community_srt": "Wirklich jetzt, versuch es mal!",
  "n_reviews": 0,
  "start": 869.68,
  "end": 871.32
 },
 {
  "input": "It's a nightmare.",
  "translatedText": "Es ist ein Albtraum.",
  "model": "google_nmt",
  "from_community_srt": "Es ist ein Albtraum.",
  "n_reviews": 0,
  "start": 871.74,
  "end": 872.44
 },
 {
  "input": "Of course, you'll rarely be so lucky as to have your basis vectors also be eigenvectors.",
  "translatedText": "Natürlich werden Sie selten das Glück haben, dass Ihre Basisvektoren auch Eigenvektoren sind.",
  "model": "google_nmt",
  "from_community_srt": "Natürlich hat man selten so Glück,",
  "n_reviews": 0,
  "start": 876.08,
  "end": 881.26
 },
 {
  "input": "But if your transformation has a lot of eigenvectors, like the one from the start of this video, enough so that you can choose a set that spans the full space, then you could change your coordinate system so that these eigenvectors are your basis vectors.",
  "translatedText": "Wenn Ihre Transformation jedoch viele Eigenvektoren hat, wie die vom Anfang dieses Videos, so viele, dass Sie einen Satz auswählen können, der den gesamten Raum abdeckt, können Sie Ihr Koordinatensystem so ändern, dass diese Eigenvektoren Ihre Basisvektoren sind.",
  "model": "google_nmt",
  "from_community_srt": "dass die Basisvektoren auch Eigenvektoren sind, aber wenn die Transformation viele Eigenvektoren hat, wie die vom Beginn des Videos, genug, dass man eine Menge wählen kann, die den ganzen Raum aufspannt, dann kann man das Koordinatensystem so verändern, sodass diese Eigenvektoren die Basisvektoren sind.",
  "n_reviews": 0,
  "start": 882.04,
  "end": 896.54
 },
 {
  "input": "I talked about change of basis last video, but I'll go through a super quick reminder here of how to express a transformation currently written in our coordinate system into a different system.",
  "translatedText": "Ich habe im letzten Video über den Basiswechsel gesprochen, aber ich werde hier ganz kurz daran erinnern, wie man eine Transformation, die derzeit in unserem Koordinatensystem geschrieben ist, in ein anderes System ausdrückt.",
  "model": "google_nmt",
  "from_community_srt": "Ich habe bereits letztes Video über Basiswechsel gesprochen, aber ich werde hier noch einmal sehr schnell wiederholen, wie man eine Transformation in unserem Koordinatensystem mit einem anderem System ausdrückt.",
  "n_reviews": 0,
  "start": 897.14,
  "end": 907.04
 },
 {
  "input": "Take the coordinates of the vectors that you want to use as a new basis, which in this case means our two eigenvectors, then make those coordinates the columns of a matrix, known as the change of basis matrix.",
  "translatedText": "Nehmen Sie die Koordinaten der Vektoren, die Sie als neue Basis verwenden möchten, was in diesem Fall unsere beiden Eigenvektoren bedeutet, und machen Sie diese Koordinaten dann zu den Spalten einer Matrix, die als Basisänderungsmatrix bezeichnet wird.",
  "model": "google_nmt",
  "from_community_srt": "Man nehme die Koordinaten der Vektoren, die man als neue Basis verwenden möchte, was, in diesem Fall, bedeutet, dass es zwei Eigenvektoren sind, die deren Koordinaten zu den Spalten einer Matrix machen, die sogenannte Basiswechsel-Matrix.",
  "n_reviews": 0,
  "start": 908.44,
  "end": 919.44
 },
 {
  "input": "When you sandwich the original transformation, putting the change of basis matrix on its right and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system.",
  "translatedText": "Wenn Sie die ursprüngliche Transformation einordnen und die Änderung der Basismatrix auf der rechten Seite und die Umkehrung der Änderung der Basismatrix auf der linken Seite platzieren, ist das Ergebnis eine Matrix, die dieselbe Transformation darstellt, jedoch aus der Perspektive der neuen Basisvektorkoordinaten System.",
  "model": "google_nmt",
  "from_community_srt": "Wenn man die ursprüngliche Transformation in die Mitte, die Basiswechsel-Matrix auf die rechte Seite, und die umgekehrte Basiswechsel-Matrix auf die linke Seite stellt, ist das Ergebnis eine Matrix, die dieselbe Transformation darstellt, aber von der Perspektive eines Koordinatensystem mit der neuen Basis.",
  "n_reviews": 0,
  "start": 920.18,
  "end": 936.5
 },
 {
  "input": "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its corresponding eigenvalues down that diagonal.",
  "translatedText": "Der Sinn dieser Vorgehensweise mit Eigenvektoren besteht darin, dass diese neue Matrix mit ihren entsprechenden Eigenwerten garantiert diagonal verläuft.",
  "model": "google_nmt",
  "from_community_srt": "Die Intention, das zu tun, ist, dass diese neue Matrix garantiert diagonal ist mit den entsprechenden Eigenwerten in dieser Diagonale.",
  "n_reviews": 0,
  "start": 937.44,
  "end": 946.68
 },
 {
  "input": "This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation.",
  "translatedText": "Dies liegt daran, dass es sich um die Arbeit in einem Koordinatensystem handelt, bei dem die Basisvektoren während der Transformation skaliert werden.",
  "model": "google_nmt",
  "from_community_srt": "Das funktioniert, da sie die Arbeit in einem Koordinatensystem repräsentiert, wo die Eigenvektoren durch diese Transformation nur skaliert werden.",
  "n_reviews": 0,
  "start": 946.86,
  "end": 954.32
 },
 {
  "input": "A set of basis vectors which are also eigenvectors is called, again, reasonably enough, an eigenbasis.",
  "translatedText": "Eine Menge von Basisvektoren, die auch Eigenvektoren sind, wird wiederum sinnvollerweise Eigenbasis genannt.",
  "model": "google_nmt",
  "from_community_srt": "Eine Menge von Basisvektoren, die auch Eigenvektoren sind wird, wieder aus gutem Grund, \"Eigenbasis\" genannt.",
  "n_reviews": 0,
  "start": 955.8,
  "end": 961.56
 },
 {
  "input": "So if, for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system.",
  "translatedText": "Wenn Sie beispielsweise die 100. Potenz dieser Matrix berechnen müssten, wäre es viel einfacher, zu einer Eigenbasis zu wechseln, die 100. Potenz in diesem System zu berechnen und dann wieder in unser Standardsystem umzuwandeln.",
  "model": "google_nmt",
  "from_community_srt": "Wenn man also die hundertste Potenz dieser Matrix berechnen muss, ist es viel einfacher zu einer Eigenbasis zu wechseln, die hundertste Potenz in diesem System zu berechnen und dann zurück zu unserem Standardsystem zu konvertieren.",
  "n_reviews": 0,
  "start": 962.34,
  "end": 975.68
 },
 {
  "input": "You can't do this with all transformations.",
  "translatedText": "Das ist nicht bei allen Transformationen möglich.",
  "model": "google_nmt",
  "from_community_srt": "Man kann das allerdings nicht mit allen Transformationen machen.",
  "n_reviews": 0,
  "start": 976.62,
  "end": 978.32
 },
 {
  "input": "A shear, for example, doesn't have enough eigenvectors to span the full space.",
  "translatedText": "Eine Scherung hat beispielsweise nicht genügend Eigenvektoren, um den gesamten Raum abzudecken.",
  "model": "google_nmt",
  "from_community_srt": "Ein \"Shear\" beispielsweise hat nicht genug Eigenvektoren um den ganzen Raum aufzuspannen.",
  "n_reviews": 0,
  "start": 978.32,
  "end": 982.98
 },
 {
  "input": "But if you can find an eigenbasis, it makes matrix operations really lovely.",
  "translatedText": "Aber wenn Sie eine Eigenbasis finden können, macht das Matrixoperationen wirklich schön.",
  "model": "google_nmt",
  "from_community_srt": "Aber wenn man eine Eigenbasis findet, macht es Matrixoperationen wirklich schön.",
  "n_reviews": 0,
  "start": 983.46,
  "end": 988.16
 },
 {
  "input": "For those of you willing to work through a pretty neat puzzle to see what this looks like in action and how it can be used to produce some surprising results, I'll leave up a prompt here on the screen.",
  "translatedText": "Für diejenigen unter Ihnen, die bereit sind, ein ziemlich nettes Puzzle durchzuarbeiten, um zu sehen, wie das in Aktion aussieht und wie es zu überraschenden Ergebnissen führen kann, lasse ich hier auf dem Bildschirm eine Eingabeaufforderung.",
  "model": "google_nmt",
  "from_community_srt": "Für diejenigen, die sich durch ein ordentliches Rätsel arbeiten wollen, um zu sehen, wie das in Aktion aussieht und wie man es benutzen kann um überraschende Ergebnisse zu erzielen, werde ich eine Angabe auf dem Bildschirm einblenden.",
  "n_reviews": 0,
  "start": 989.12,
  "end": 997.32
 },
 {
  "input": "It takes a bit of work, but I think you'll enjoy it.",
  "translatedText": "Es erfordert ein wenig Arbeit, aber ich denke, es wird Ihnen Spaß machen.",
  "model": "google_nmt",
  "from_community_srt": "Es ist ein gutes Stück Arbeit, aber ich denke Ihr werdet es mögen.",
  "n_reviews": 0,
  "start": 997.6,
  "end": 1000.28
 },
 {
  "input": "The next and final video of this series is going to be on abstract vector spaces.",
  "translatedText": "Das nächste und letzte Video dieser Serie wird sich mit abstrakten Vektorräumen befassen.",
  "model": "google_nmt",
  "from_community_srt": "Das nächste und finale Video dieser Serie wird über abstrakte Vektorräume sein.",
  "n_reviews": 0,
  "start": 1000.84,
  "end": 1006.12
 }
]