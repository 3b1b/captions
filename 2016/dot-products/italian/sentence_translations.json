[
 {
  "input": "[Music] Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[Musica] Tradizionalmente, i prodotti scalari sono qualcosa che viene introdotto molto presto in un corso di algebra lineare, in genere proprio all'inizio.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "Quindi potrebbe sembrare strano che li abbia spinti indietro così lontano nella serie.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "L'ho fatto perché esiste un modo standard per introdurre l'argomento, che non richiede altro che una conoscenza di base dei vettori, ma una comprensione più completa del ruolo che i prodotti scalari svolgono in matematica può essere trovata solo alla luce delle trasformazioni lineari.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "Prima di ciò, però, permettetemi di illustrare brevemente il modo standard in cui vengono introdotti i prodotti punto, che presumo sia almeno parzialmente rivisto per un certo numero di spettatori.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "Numericamente, se hai due vettori della stessa dimensione, due elenchi di numeri con la stessa lunghezza, prendere il loro prodotto scalare significa accoppiare tutte le coordinate, moltiplicare quelle coppie insieme e sommare il risultato.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "Quindi il vettore 1, 2 punteggiato da 3, 4 sarebbe 1 per 3 più 2 per 4.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "Il vettore 6, 2, 8, 3 punteggiato da 1, 8, 5, 3 sarebbe 6 per 1 più 2 per 8 più 8 per 5 più 3 per 3.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "Fortunatamente, questo calcolo ha un’interpretazione geometrica davvero interessante.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "Per pensare al prodotto scalare tra due vettori, v e w, immagina di proiettare w sulla linea che passa attraverso l'origine e la punta di v.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "Moltiplicando la lunghezza di questa proiezione per la lunghezza di v, ottieni il prodotto scalare v punto w.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "Tranne quando questa proiezione di w punta nella direzione opposta a v, quel prodotto scalare sarà effettivamente negativo.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "Quindi, quando due vettori puntano generalmente nella stessa direzione, il loro prodotto scalare è positivo.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "Quando sono perpendicolari, ovvero la proiezione dell'uno sull'altro è il vettore zero, il loro prodotto scalare è zero.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "E se puntano generalmente nella direzione opposta, il loro prodotto scalare è negativo.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "Ora, questa interpretazione è stranamente asimmetrica.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "Tratta i due vettori in modo molto diverso.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "Quindi, quando l'ho imparato per la prima volta, sono rimasto sorpreso dal fatto che l'ordine non abbia importanza.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "Potresti invece proiettare v su w, moltiplicare la lunghezza della v proiettata per la lunghezza di w e ottenere lo stesso risultato.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "Voglio dire, non sembra un processo davvero diverso?",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "Ecco l'intuizione del perché l'ordine non ha importanza.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "Se v e w avessero la stessa lunghezza, potremmo sfruttare una certa simmetria.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "Poiché proiettare w su v, quindi moltiplicare la lunghezza di quella proiezione per la lunghezza di v, è un'immagine speculare completa della proiezione di v su w, quindi moltiplicare la lunghezza di quella proiezione per la lunghezza di w.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "Ora, se ne ridimensioni uno, diciamo v, di una costante come 2, in modo che non abbiano la stessa lunghezza, la simmetria viene rotta.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "Ma pensiamo a come interpretare il prodotto scalare tra questo nuovo vettore, 2 volte v e w.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "Se pensi che w venga proiettato su v, allora il prodotto scalare 2v punto w sarà esattamente il doppio del prodotto scalare v punto w.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "Questo perché quando ridimensioni v di 2, non cambia la lunghezza della proiezione di w, ma raddoppia la lunghezza del vettore su cui stai proiettando.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "Ma d'altra parte, diciamo che stavi pensando che v venisse proiettato su w.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "Beh, in questo caso, la lunghezza della proiezione è la cosa che viene ridimensionata quando moltiplichiamo v per 2, ma la lunghezza del vettore su cui stai proiettando rimane costante.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "Quindi l'effetto complessivo è ancora quello di raddoppiare il prodotto scalare.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "Quindi, anche se in questo caso la simmetria è rotta, l'effetto che questo ridimensionamento ha sul valore del prodotto scalare è lo stesso in entrambe le interpretazioni.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "C'è anche un'altra grande domanda che mi ha confuso quando ho imparato queste cose per la prima volta.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "Perché mai questo processo numerico di corrispondenza delle coordinate, moltiplicazione di coppie e somma delle stesse ha qualcosa a che fare con la proiezione?",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "Ebbene, per dare una risposta soddisfacente, e anche per rendere piena giustizia al significato del prodotto scalare, dobbiamo portare alla luce qualcosa di un po' più profondo che sta accadendo qui, che spesso va sotto il nome di dualità.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "Ma prima di approfondire l'argomento, devo spendere un po' di tempo parlando delle trasformazioni lineari da più dimensioni a una dimensione, che è proprio la linea numerica.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "These are functions that take in a 2d vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2d input and a 1d output.",
  "translatedText": "Queste sono funzioni che accettano un vettore 2D e producono un certo numero, ma le trasformazioni lineari sono ovviamente molto più limitate rispetto alla normale funzione con un input 2D e un output 1D.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "Come per le trasformazioni nelle dimensioni superiori, come quelle di cui ho parlato nel capitolo 3, ci sono alcune proprietà formali che rendono queste funzioni lineari, ma qui le ignorerò di proposito per non distrarre dal nostro obiettivo finale, e invece concentrarsi su una certa proprietà visiva che è equivalente a tutte le cose formali.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "Se prendi una linea di punti equidistanti e applichi una trasformazione, una trasformazione lineare manterrà quei punti equidistanti una volta che si fermano nello spazio di output, che è la linea numerica.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "Altrimenti, se c'è una linea di punti che non è distanziata in modo uniforme, la trasformazione non sarà lineare.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "Come nei casi che abbiamo visto prima, una di queste trasformazioni lineari è completamente determinata da dove prendono i-hat e j-hat, ma questa volta ognuno di questi vettori di base si ferma su un numero, quindi quando registriamo dove atterrano come colonne di una matrice, ciascuna di quelle colonne ha solo un singolo numero.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "Questa è una matrice 1x2.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "Esaminiamo un esempio di cosa significa applicare una di queste trasformazioni a un vettore.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "Supponiamo che tu abbia una trasformazione lineare che porta i-hat a 1 e j-hat a meno 2.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "Per seguire dove finisce un vettore con coordinate, diciamo 4, 3, pensa di suddividere questo vettore come 4 volte i-hat più 3 volte j-hat.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "Una conseguenza della linearità è che dopo la trasformazione, il vettore sarà 4 volte il punto in cui si ferma i-hat, 1, più 3 volte il punto in cui si ferma j-hat, negativo 2, che in questo caso implica che si ferma su negativo 2.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "Quando esegui questo calcolo in modo puramente numerico, è una moltiplicazione di vettori di matrice.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "Ora, questa operazione numerica di moltiplicare una matrice 1x2 per un vettore sembra proprio come prendere il prodotto scalare di due vettori.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "Quella matrice 1x2 non sembra proprio un vettore che abbiamo inclinato su un lato?",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "In effetti, potremmo dire subito che esiste una bella associazione tra matrici 1x2 e vettori 2D, definita inclinando la rappresentazione numerica di un vettore su un lato per ottenere la matrice associata, o inclinando la matrice verso l'alto per ottenere il vettore associato.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "Dato che al momento stiamo esaminando solo le espressioni numeriche, andare avanti e indietro tra vettori e matrici 1x2 potrebbe sembrare una cosa sciocca da fare.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "Ma questo suggerisce qualcosa di veramente fantastico dal punto di vista geometrico.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "Esiste una sorta di connessione tra le trasformazioni lineari che portano i vettori in numeri e i vettori stessi.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "Lasciatemi mostrare un esempio che chiarisce il significato e che, guarda caso, risponde anche al puzzle del prodotto scalare di prima.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "Disimpara ciò che hai imparato e immagina di non sapere già che il prodotto scalare si riferisce alla proiezione.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "Quello che farò qui sarà prendere una copia della linea numerica e posizionarla in qualche modo diagonalmente nello spazio, con il numero 0 all'origine.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now think of the two-dimensional unit vector, whose tip sits where the number 1 on the number line is.",
  "translatedText": "Ora pensa al vettore unitario bidimensionale, la cui punta si trova dove si trova il numero 1 sulla linea numerica.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I want to give that guy a name, U-hat.",
  "translatedText": "Voglio dare un nome a quel ragazzo, U-hat.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "Questo piccoletto gioca un ruolo importante in ciò che sta per accadere, quindi tienilo in un angolo della tua mente.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If we project 2D vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2D vectors to numbers.",
  "translatedText": "Se proiettiamo i vettori 2D direttamente su questa linea numerica diagonale, in effetti, abbiamo appena definito una funzione che trasforma i vettori 2D in numeri.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "Inoltre, questa funzione è in realtà lineare, poiché supera il nostro test visivo secondo cui qualsiasi linea di punti equidistanti rimane uniformemente distanziata una volta che si ferma sulla linea numerica.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2D space like this, the outputs of the function are numbers, not 2D vectors.",
  "translatedText": "Giusto per essere chiari, anche se ho incorporato la linea numerica nello spazio 2D in questo modo, gli output della funzione sono numeri, non vettori 2D.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "Dovresti pensare a una funzione che accetta due coordinate e restituisce una singola coordinata.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But that vector U-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "Ma quel vettore U-hat è un vettore bidimensionale, che vive nello spazio di input.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "È semplicemente situato in modo tale da sovrapporsi all'incorporamento della linea numerica.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2D vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "Con questa proiezione, abbiamo appena definito una trasformazione lineare da vettori 2D a numeri, quindi saremo in grado di trovare una sorta di matrice 1x2 che descriva quella trasformazione.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where I-hat and J-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "Per trovare la matrice 1x2, ingrandiamo questa configurazione della linea numerica diagonale e pensiamo a dove si trovano i cappelli I e J, poiché quei punti di atterraggio saranno le colonne della matrice.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This part's super cool.",
  "translatedText": "Questa parte è fantastica.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "Possiamo ragionarci sopra con un pezzo di simmetria davvero elegante.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Since I-hat and U-hat are both unit vectors, projecting I-hat onto the line passing through U-hat looks totally symmetric to projecting U-hat onto the x-axis.",
  "translatedText": "Poiché I-hat e U-hat sono entrambi vettori unitari, la proiezione di I-hat sulla linea che passa attraverso U-hat sembra totalmente simmetrica rispetto alla proiezione di U-hat sull'asse x.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So when we ask what number does I-hat land on when it gets projected, the answer is going to be the same as whatever U-hat lands on when it's projected onto the x-axis.",
  "translatedText": "Quindi quando chiediamo su quale numero si ferma l'I-hat quando viene proiettato, la risposta sarà la stessa di qualunque numero su cui si ferma l'U-hat quando viene proiettato sull'asse x.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But projecting U-hat onto the x-axis just means taking the x-coordinate of U-hat.",
  "translatedText": "Ma proiettare U-hat sull'asse x significa semplicemente prendere la coordinata x di U-hat.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So by symmetry, the number where I-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of U-hat.",
  "translatedText": "Quindi per simmetria, il numero dove I-hat si ferma quando viene proiettato sulla linea numerica diagonale sarà la coordinata x di U-hat.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "Non è bello?",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The reasoning is almost identical for the J-hat case.",
  "translatedText": "Il ragionamento è quasi identico per il caso J-hat.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "Pensateci per un momento.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For all the same reasons, the y-coordinate of U-hat gives us the number where J-hat lands when it's projected onto the number line copy.",
  "translatedText": "Per tutti gli stessi motivi, la coordinata y di U-hat ci fornisce il numero in cui J-hat si ferma quando viene proiettato sulla copia della linea numerica.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "Fermatevi e riflettete per un momento.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "Penso solo che sia davvero fantastico.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of U-hat.",
  "translatedText": "Quindi le voci della matrice 1x2 che descrivono la trasformazione della proiezione saranno le coordinate di U-hat.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with U-hat.",
  "translatedText": "E calcolare questa trasformazione della proiezione per vettori arbitrari nello spazio, che richiede la moltiplicazione di quella matrice per quei vettori, è computazionalmente identico a prendere un prodotto scalare con U-hat.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "Questo è il motivo per cui prendere il prodotto scalare con un vettore unitario può essere interpretato come proiettare un vettore sull'intervallo di quel vettore unitario e prenderne la lunghezza.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "E che dire dei vettori non unitari?",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For example, let's say we take that unit vector U-hat, but we scale it up by a factor of 3.",
  "translatedText": "Ad esempio, supponiamo di prendere il vettore unitario U-hat, ma di ingrandirlo di un fattore 3.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "Numericamente, ciascuno dei suoi componenti viene moltiplicato per 3.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes I-hat and J-hat to three times the values where they landed before.",
  "translatedText": "Quindi, guardando la matrice associata a quel vettore, ci vogliono I-hat e J-hat tre volte i valori in cui si trovavano prima.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "Poiché tutto questo è lineare, implica più in generale che la nuova matrice può essere interpretata come se proiettasse qualsiasi vettore sulla copia della linea numerica e moltiplicasse il punto in cui si ferma per 3.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "Questo è il motivo per cui il prodotto scalare con un vettore non unitario può essere interpretato come se si proiettasse prima su quel vettore, quindi si aumentasse la lunghezza di quella proiezione in base alla lunghezza del vettore.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "Prenditi un momento per pensare a quello che è successo qui.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "Abbiamo avuto una trasformazione lineare dallo spazio 2D alla linea numerica, che non è stata definita in termini di vettori numerici o prodotti scalari numerici, è stata semplicemente definita proiettando lo spazio su una copia diagonale della linea numerica.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "Ma poiché la trasformazione è lineare, è stata necessariamente descritta da una matrice 1x2.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "E poiché moltiplicare una matrice 1x2 per un vettore 2D equivale a girare la matrice su un lato e prendere un prodotto scalare, questa trasformazione era inevitabilmente correlata a un vettore 2D.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "La lezione qui è che ogni volta che si ha una di queste trasformazioni lineari il cui spazio di output è la linea numerica, non importa come sia stata definita, ci sarà un vettore univoco v corrispondente a quella trasformazione, nel senso che l'applicazione della trasformazione è la stessa cosa che prendere un prodotto scalare con quel vettore.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "Per me questo è assolutamente bellissimo.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "È un esempio di qualcosa in matematica chiamato dualità.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "La dualità si manifesta in molti modi e forme diverse in tutta la matematica, ed è molto difficile definirla effettivamente.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "In parole povere, si riferisce a situazioni in cui si ha una corrispondenza naturale ma sorprendente tra due tipi di cose matematiche.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "Per il caso dell'algebra lineare che hai appena imparato, diresti che il duale di un vettore è la trasformazione lineare che codifica, e il duale di una trasformazione lineare da uno spazio a una dimensione è un certo vettore in quello spazio.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "Quindi, per riassumere, in superficie, il prodotto scalare è uno strumento geometrico molto utile per comprendere le proiezioni e per verificare se i vettori tendono o meno a puntare nella stessa direzione.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "E questa è probabilmente la cosa più importante da ricordare riguardo al prodotto scalare.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "Ma a un livello più profondo, mettere insieme due vettori è un modo per tradurre uno di essi nel mondo delle trasformazioni.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "Ancora una volta, numericamente, questo potrebbe sembrare un punto sciocco da sottolineare.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It's just too computationally.",
  "translatedText": "È semplicemente troppo computazionale.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "Ma il motivo per cui lo trovo così importante è che in matematica, quando hai a che fare con un vettore, una volta che conosci veramente la sua personalità, a volte ti rendi conto che è più facile comprenderlo non come una freccia nello spazio, ma come il incarnazione fisica di una trasformazione lineare.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space.",
  "translatedText": "È come se il vettore fosse in realtà solo una scorciatoia concettuale per una certa trasformazione, dal momento che è più facile per noi pensare alle frecce nello spazio piuttosto che spostare tutto quello spazio.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action as I talk about the cross product.",
  "translatedText": "Nel prossimo video vedrai un altro esempio davvero interessante di questa dualità in azione mentre parlo del prodotto incrociato.",
  "model": "google_nmt",
  "n_reviews": 0
 }
]