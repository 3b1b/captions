[
 {
  "input": "Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[&quot;אודה לשמחה&quot;, מאת בטהובן, מנגן עד קצה הפסנתר.] באופן מסורתי, מוצרי נקודות הם משהו שמוצג ממש מוקדם בקורס אלגברה ליניארי, בדרך כלל ממש בהתחלה.",
  "model": "google_nmt",
  "from_community_srt": "-קאלווין: \"אתה יודע, אני לא חושב שמתמטיקה זה מדע, אני חושב שזו דת.\" -הובס: \"דת?\" -קאלווין: \" כן. כל המשוואות הללו הן כמו נס. אתה לוקח שני מספרים וכשאתה מחבר אותם, הם באופן פלאי הופכים למספר אחד חדש! אף אחד לא יכול להגיד איך זה יכול לקרות. זה או שאתה מאמין לזה או שלא\" באופן מסורתי, מכפלה סקלרית היא דבר שמציגים בדרך כלל מאוד מוקדם בקורס של אלגברה לינארית. בדרך כלל,",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "אז זה אולי נראה מוזר שדחפתי אותם כל כך רחוק בסדרה.",
  "model": "google_nmt",
  "from_community_srt": "ממש בהתחלה. אז זה אולי יראה מוזר שאני דחפתי אותם כל כך רחוק בסידרה הזו(של הסירטונים).",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "עשיתי זאת כי יש דרך סטנדרטית להציג את הנושא, שדורשת לא יותר מאשר הבנה בסיסית של וקטורים, אבל הבנה מלאה יותר של התפקיד שממלאים תוצרי נקודות במתמטיקה אפשר למצוא באמת רק באור של טרנספורמציות ליניאריות.",
  "model": "google_nmt",
  "from_community_srt": "עשיתי זאת בגלל שיש דרך סטנדרטית להציג את הנושא שהוא דורש לא יותר מהבנה בסיסית של וקטורים, אבל הבנה מלאה של התפקיד אשר משחקת מכפלה סקלרית במתמטיקה, אפשרי רק למצוא תחת האור של טרנספורמציות לינאריות.",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "עם זאת, לפני כן, הרשו לי רק לכסות בקצרה את הדרך הסטנדרטית בה מציגים מוצרי נקודה, שאני מניח שהיא לפחות סקירה חלקית עבור מספר צופים.",
  "model": "google_nmt",
  "from_community_srt": "לפני כן, תנו לי לכסות בקצרה את הדרך הרגילה בה מכפלה סקאלרית מוצגת. כאשר אני מניח שלפחות חלק מכם, הצופים,",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "מבחינה מספרית, אם יש לך שני וקטורים מאותו מימד, שתי רשימות של מספרים בעלות אותם אורכים, לקיחת מכפלת הנקודות שלהם פירושה זיווג של כל הקואורדינטות, הכפלת הזוגות הללו והוספת התוצאה.",
  "model": "google_nmt",
  "from_community_srt": "חוו זאת. באופן מספרי, אם יש לך שני וקטורים מאותו מימד; שתי רשימות של מספרים עם אותו אורך, לקחת את מכפלתם הסקאלרית אומרת כי צריך לצוות יחדיו את כל הקואורדינטות, להכפיל אותן ביחד(אחת בשניה)",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "אז הווקטור 1, 2 המנוקד ב-3, 4 יהיה 1 כפול 3 ועוד 2 כפול 4.",
  "model": "google_nmt",
  "from_community_srt": "ולחבר את התוצאה המתקבלת. אז המכפלה הסקאלרית של הוקטור [1,2] עם [3,4], תהיה 1x3 ועוד 2x4.",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "הווקטור 6, 2, 8, 3 המנוקד ב-1, 8, 5, 3 יהיה 6 כפול 1 ועוד 2 כפול 8 ועוד 8 כפול 5 ועוד 3 כפול 3.",
  "model": "google_nmt",
  "from_community_srt": "המכפלה הסקלרית של הוקטור [3, 8, 2, 6] עם הוקטור [3, 5, 8, 1] תהיה: 6x1 +2x8 + 8x5 +3x3",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "למרבה המזל, לחישוב הזה יש פרשנות גיאומטרית ממש נחמדה.",
  "model": "google_nmt",
  "from_community_srt": "למזלנו, לחישוב הזה יש פירוש גיאומטרי ממש יפה.",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "כדי לחשוב על מכפלת הנקודה בין שני וקטורים, v ו-w, דמיינו את ההשלכה של w על הקו שעובר דרך המקור וקצה v.",
  "model": "google_nmt",
  "from_community_srt": "לחשוב על מכפלה סקאלרית בין שני וקטורים v ו-w, תדמיין היטל של w על הקו שעובר דרך הראשית והקצה של v.",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "מכפילים את אורך ההקרנה באורך v, יש לך את מכפלת הנקודה v dot w.",
  "model": "google_nmt",
  "from_community_srt": "ע\"י הכפלת אורך ההיטל הזה באורך של v, אתה תקבל את המכפלה הסקלרית: v*w.",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "למעט כאשר השלכה זו של w מצביעה בכיוון ההפוך מ-v, מכפלת הנקודה הזו תהיה למעשה שלילית.",
  "model": "google_nmt",
  "from_community_srt": "למעט המקרה שההיטל של w מצביע בכיוון הנגדי לזה של v, המכפלה הסקאלרית תהיה הפעם שלילית.",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "אז כששני וקטורים בדרך כלל מצביעים לאותו כיוון, תוצר הנקודות שלהם חיובי.",
  "model": "google_nmt",
  "from_community_srt": "אז בדרך, כאשר שני הוקטורים מצביעים על אותו הכיוון, המכפלה הסקאלרית שלהם תהיה חיובית.",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "כאשר הם מאונכים, כלומר ההשלכה של אחד על השני היא וקטור האפס, מכפלת הנקודות שלהם היא אפס.",
  "model": "google_nmt",
  "from_community_srt": "כאשר הם מאונכים זה לזה, הכוונה היא ההיטל של וקטור אחד על השני הוא וקטור האפס, אזי המכפלה הסקאלרית היא 0.",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "ואם הם מצביעים בדרך כלל לכיוון ההפוך, תוצר הנקודות שלהם שלילי.",
  "model": "google_nmt",
  "from_community_srt": "אם הם בדרך כלל מצביעים כל אחד מהם בכיוון הנגדי,",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "עכשיו, הפרשנות הזו אסימטרית בצורה מוזרה.",
  "model": "google_nmt",
  "from_community_srt": "המכפלה הסקאלרית שלהם תהיה שלילית. עכשיו, באופן ממש מוזר הפירוש הזה לא סימטרי.",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "זה מתייחס לשני הוקטורים בצורה שונה מאוד.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "אז כשנודע לי על זה לראשונה, הופתעתי שהסדר לא משנה.",
  "model": "google_nmt",
  "from_community_srt": "זה מטפל בשני הוקטורים בצורה מאוד שונה, אז כשאני למדתי לראשונה זאת,",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "אתה יכול במקום זאת להקרין v על w, להכפיל את אורך ה-v המוקרן באורך w, ולקבל את אותה תוצאה.",
  "model": "google_nmt",
  "from_community_srt": "אני הופתעתי שהסדר לא משנה. אתה באותו אופן יכול לעשות את ההיטל של v על w. תכפיל את אורך ההיטל של v באורך של w ותקבל את אותה התוצאה.",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "כלומר, זה לא מרגיש כמו תהליך שונה באמת?",
  "model": "google_nmt",
  "from_community_srt": "אני מתכוון,",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "הנה האינטואיציה מדוע הסדר לא משנה.",
  "model": "google_nmt",
  "from_community_srt": "האם זה לא מרגיש כמו תהליך ממש שונה? הנה האינטואיציה למה הסדר לא משנה:",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "אם ל-v ול-w במקרה היה אותו אורך, נוכל למנף קצת סימטריה.",
  "model": "google_nmt",
  "from_community_srt": "אם v ו-w במקרה באותו האורך, אנחנו יכולים להשיג קצת סימטריה.",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "מכיוון שהקרנת w על v, אז הכפלת אורך ההקרנה באורך v, היא תמונת מראה שלמה של הקרנת v על w, ואז הכפלת אורך ההקרנה באורך w.",
  "model": "google_nmt",
  "from_community_srt": "מכיוון שההיטל של w על v ואז הכפלת האורך של אותו היטל באורך של v, הוא תמונת מראה מלאה של ההיטל של v על w ואז הכפלת אותו אורך של ההיטל באורך של w.",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "עכשיו, אם אתה משנה את קנה המידה של אחד מהם, נניח v, לפי קבוע כלשהו כמו 2, כדי שלא יהיה להם אורך שווה, הסימטריה נשברת.",
  "model": "google_nmt",
  "from_community_srt": "עכשיו, אם תכפיל בסקלר אחד מהם, למשל נכפיל את v בקבוע שהוא 2, כך שלא יהיה להם אורך זהה, אז אין לנו כאן יותר סימטריה.",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "אבל בואו נחשוב איך לפרש את תוצר הנקודה בין הווקטור החדש הזה, 2 פעמים v, ו-w.",
  "model": "google_nmt",
  "from_community_srt": "אבל בוא נחשוב איך לפרש את המכפלה סקלרית בין הוקטור החדש הזה 2v לבין w.",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "אם אתה חושב ש-w מוקרן על v, אז תוצר הנקודה 2v נקודה w יהיה בדיוק פי שניים ממוצר הנקודה v נקודה w.",
  "model": "google_nmt",
  "from_community_srt": "אם אתה חושב שאתה הולך להשים את ההיטל של w על v אז המכפלה הסקאלרית  2v*w תהיה בדיוק פעמיים המכפלה הסקלרית של v*w.",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "הסיבה לכך היא שכאשר אתה משנה את קנה המידה של v ב-2, זה לא משנה את אורך ההקרנה של w, אלא מכפיל את אורך הווקטור שאתה מקרין עליו.",
  "model": "google_nmt",
  "from_community_srt": "זה בגלל שהכפלת את v בסקלר של 2, זה לא משנה את אורך ההיטל של w אבל זה מכפיל את אורך הוקטור שעליו אתה שם את ההיטל.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "אבל מצד שני, נניח שחשבת על v להקרין על w.",
  "model": "google_nmt",
  "from_community_srt": "אבל, מצד שני, בוא נניח שאתה חושב להשים את ההיטל של v על w.",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "ובכן, במקרה כזה, אורך ההשלכה הוא הדבר שמקבל קנה מידה כשאנחנו מכפילים את v ב-2, אבל אורך הווקטור שאתה מקרין עליו נשאר קבוע.",
  "model": "google_nmt",
  "from_community_srt": "ובכן, במקרה הזה, האורך של ההיטל הוא הדבר אותו אנחנו מכפילים בסקלר כאשר אנחנו מכפילים את v ב-2. אורך הוקטור שאתה שם עליו את ההיטל נשאר קבוע.",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "אז ההשפעה הכוללת היא עדיין רק להכפיל את מוצר הנקודה.",
  "model": "google_nmt",
  "from_community_srt": "כך שההשפעה הכוללת היא עדיין המכפלה הסקלרית כפול 2.",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "אז למרות שהסימטריה נשברת במקרה זה, ההשפעה שיש לקנה מידה זה על הערך של תוצר הנקודה זהה בשתי הפירושים.",
  "model": "google_nmt",
  "from_community_srt": "אז, אפילו כשאין לנו סימטריה במקרה הזה, ההשפעה על הערך של אותה \"מכפלה סקאלרית\" ,",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "יש גם שאלה אחת גדולה שבלבלה אותי כאשר למדתי את הדברים האלה לראשונה.",
  "model": "google_nmt",
  "from_community_srt": "היא זהה עבור אותם שני פירושים. יש אפילו שאלה אחת גדולה יותר שנותרה.",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "מדוע לכל הרוחות יש קשר כלשהו לתהליך המספרי הזה של התאמת קואורדינטות, הכפלת זוגות וחיבורם יחד עם הקרנה?",
  "model": "google_nmt",
  "from_community_srt": "היא בלבלה אותי כשלראשונה למדתי את זה: למה לעזאזל התהליך המספרים הזה של התאמת הקואורדינטות, הכפלת זוגות של מספרים ו- חיבורם יחדיו, יש קשר כלשהו לאותו היטל?",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "ובכן, כדי לתת תשובה מספקת, וגם לעשות צדק מלא עם המשמעות של מוצר הנקודה, אנחנו צריכים לחשוף משהו קצת יותר עמוק שקורה כאן, שלעיתים קרובות נקרא דואליות.",
  "model": "google_nmt",
  "from_community_srt": "ובכן, אתן לך תשובה משביעת רצון, וכדי לעשות צדק מלא למשמעות של מכפלה סקאלרית, אנחנו צריכים לגלות משהו יותר עמוק שהולך כאן שלעיתים קרובות הולך לפי השם \"דואליות\".",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "אבל לפני שנכנס לזה, אני צריך להקדיש זמן לדבר על טרנספורמציות ליניאריות מממדים מרובים למימד אחד, שהוא רק קו המספרים.",
  "model": "google_nmt",
  "from_community_srt": "אבל, לפני שנכנס לתוך זה, אני צריך לבזבז קצת זמן לדבר על טרנספורמציות לינאריות ממספר מימדים למימד אחד שהוא קו המספרים.",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "אלו הן פונקציות שמקבלות וקטור דו-ממדי ויורקות מספר כלשהו, אבל טרנספורמציות ליניאריות מוגבלות כמובן הרבה יותר מפונקציית ה-run-of-the-mill שלך עם קלט דו-ממדי ופלט דו-ממדי.",
  "model": "google_nmt",
  "from_community_srt": "אלו הן פונקציות שלוקחות לתוכן וקטור מדו-מימד ויורקות החוצה מספר כלשהו. אבל טרנספורמציות לינאריות, כמובן, יותר מוגבלות מסתם תהליך של לקבל  קלט של וקטור בדו-מימד ולירוק פלט בחד-מימד.",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "כמו בטרנספורמציות בממדים גבוהים יותר, כמו אלה שדיברתי עליהם בפרק 3, יש כמה מאפיינים פורמליים שהופכים את הפונקציות הללו ללינאריות, אבל אני מתכוון להתעלם מאלה כאן כדי לא להסיח את הדעת מהמטרה הסופית שלנו, ובמקום זאת. התמקדו במאפיין חזותי מסוים ששווה ערך לכל הדברים הרשמיים.",
  "model": "google_nmt",
  "from_community_srt": "כמו שעם הטרנספורמציות ממימד גבוהה יותר, כמו אותן טרנספורמציות שדיברתי עליהן בפרק השלישי, ישנן תכונות פורמליות שהופכות את הפונקציות הללו ללינאריות. אבל כאן אני הולך בכוונה להתעלם מזה כאן, כך שזה לא יסיח את דעתך מהמטרה הסופית, ובמקום זאת להתמקד על תכונה ויזואלית ששווה לכל הדברים הפורמלים.",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "אם אתה לוקח קו של נקודות ברווח שווה ומחיל טרנספורמציה, טרנספורמציה ליניארית תשמור על רווחים שווה של הנקודות האלה ברגע שהן נוחתות במרחב הפלט, שהוא קו המספרים.",
  "model": "google_nmt",
  "from_community_srt": "אם אתה לוקח קו שיש בו נקודות במרחק שווה זו מזו ומבצע טרנספורמציה, הטרנספורמציה הלינארית תשמור על הנקודות הללו במרחק שווה זו מזו, ברגע שהן נחתן במרחב, שהוא קו המספרים.",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "אחרת, אם יש איזו שורה של נקודות שמרווחת בצורה לא אחידה, אז השינוי שלך אינו ליניארי.",
  "model": "google_nmt",
  "from_community_srt": "אחרת, אם קיים קו עם נקודות שמרחקן לא שווה זו מזו אז הטרנספורמציה שלך אינה לינארית.",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "כמו במקרים שראינו בעבר, אחת מהטרנספורמציות הליניאריות הללו נקבעת לחלוטין לפי המקום שבו היא לוקחת i-hat ו-j-hat, אבל הפעם כל אחד מהווקטורים הבסיסיים האלה פשוט נוחת על מספר, אז כשאנחנו רושמים איפה הם נוחתים כעמודות של מטריצה, לכל אחת מהעמודות האלה יש רק מספר בודד.",
  "model": "google_nmt",
  "from_community_srt": "וכמו במקרים אחרים שראינו קודם, אחת מהטרנספורמציות הלינאריות הללו היא לגמרי נקבעת ע\"י כך שלאיפה היא לוקחת את  i כובע ו-j כובע אבל במקרה הזה, כל אחד מאותם וקטורי הבסיס פשוט נוחת על מספר. אז, כשאנחנו שומרים איפה הם נחתו, בתור עמודות של מטריצה כל אחד מהעמודות הללו יש מספר אחד בלבד.",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "זוהי מטריצה 1x2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "בואו נעבור על דוגמה למשמעות של יישום אחת מהטרנספורמציות הללו על וקטור.",
  "model": "google_nmt",
  "from_community_srt": "זאתי מטריצה 1 על 2. בוא נמשיך עם הדוגמא של מה המשמעות להפעיל אחת מהטרנספורמציות הלינאריות הללו על וקטור.",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "נניח שיש לך טרנספורמציה ליניארית שלוקחת את i-hat ל-1 ואת j-hat לשלילי 2.",
  "model": "google_nmt",
  "from_community_srt": "בוא נניח שיש לך טרנספורמציה לינארית שלוקחת את i-כובע ל-1 ו-j כובע למינוס 2.",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "כדי לעקוב אחר המקום שבו מגיע וקטור עם קואורדינטות, נניח, 4, 3, חשבו על פירוק הווקטור הזה כ-4 פעמים i-hat ועוד 3 פעמים j-hat.",
  "model": "google_nmt",
  "from_community_srt": "כדי שנוכל לעקוב לאיפה הוקטור, למשל, עם הקואורדינטות [4,3] נוחת בסוף, תחשוב על פירוק הוקטור הזה כ-4 כפול i כובע ועוד 3 כפול j כובע.",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "תוצאה של ליניאריות היא שאחרי הטרנספורמציה, הווקטור יהיה פי 4 מהמקום בו נוחת i-hat, 1, ועוד פי 3 מהמקום בו נוחת j-hat, שלילי 2, מה שבמקרה זה מרמז שהוא נוחת על שלילי 2.",
  "model": "google_nmt",
  "from_community_srt": "ההשלכה של לינאריות, היא, שלאחר טרנספורמציה הוקטור יהיה 4 פעמים איפה הוקטור i-כובע נוחת, 1, ועוד 3 פעמים איפה הוקטור j-כובע נוחת, 2-. מה שאומר,",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "כאשר אתה עושה את החישוב הזה באופן מספרי בלבד, זה מכפל וקטור מטריצה.",
  "model": "google_nmt",
  "from_community_srt": "שבמקרה הזה הוא נוחת במינוס 2. כשאתה עושה חושב מסוג זה בצורה לגמרי מספרית,",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "כעת, הפעולה המספרית הזו של הכפלת מטריצה של 1x2 בוקטור מרגישה בדיוק כמו לקיחת מכפלת הנקודות של שני וקטורים.",
  "model": "google_nmt",
  "from_community_srt": "זו הכפלה של וקטור במטריצה. עכשיו, התהליך המספרי הזה של הכפלת מטריצה 1 על 2 בוקטור, מרגיש באופן דומה כאילו לקחנו מכפלה סקאלרית של שני וקטורים.",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "האם המטריצה הזו בגודל 1x2 לא נראית כמו וקטור שהטנו על צדו?",
  "model": "google_nmt",
  "from_community_srt": "האם לא מטריצה 1x2 נראית כמו וקטור שהפכנו אותו על צידו? למעשה,",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "למעשה, אנחנו יכולים לומר כרגע שיש קשר יפה בין מטריצות 1x2 לוקטורים דו-ממדיים, המוגדרים על ידי הטיית הייצוג המספרי של וקטור בצדו כדי לקבל את המטריצה המשויכת, או להטות את המטריצה לאחור כדי לקבל את הווקטור המשויך. .",
  "model": "google_nmt",
  "from_community_srt": "אנחנו יכולים להגיד כבר עכשיו שיש קשר בין מטריצות 1x2 לוקטורים בעולם הדו-מימדי, מוגדר ע\"י הטיה של התצוגה המספרית של אותו וקטור כדי שיהיה לו קשר למטריצה, או להטות את המטריצה חזרה כדי לקבל את הוקטור הקשור אליה.",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "מכיוון שאנחנו רק מסתכלים על ביטויים מספריים עכשיו, מעבר הלוך ושוב בין וקטורים ומטריצות 1x2 עשוי להרגיש כמו דבר טיפשי לעשות.",
  "model": "google_nmt",
  "from_community_srt": "מכיוון שאנו מסתכלים על ביטויים מספריים כרגע, הולכים הלוך ושוב בין הוקטורים ומטריצות 1x2 ירגיש אולי קצת כמו משהו טיפשי",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "אבל זה מרמז על משהו שהוא באמת מדהים מהנוף הגיאומטרי.",
  "model": "google_nmt",
  "from_community_srt": "לעשותו.",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "יש איזשהו קשר בין טרנספורמציות ליניאריות שלוקחות וקטורים למספרים ולווקטורים עצמם.",
  "model": "google_nmt",
  "from_community_srt": "אבל זה רומז על משהו ממש מגניב מנקודת מבט גיאומטרית: ישנו סוג של חיבור בין טרנספורמציות לינאריות שלוקחות וקטורים למספרים",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "הרשו לי להראות דוגמה שמבהירה את המשמעות, ושבמקרה גם עונה על חידת המוצר הנקודה מקודם.",
  "model": "google_nmt",
  "from_community_srt": "ווקטורים לעצמם. תנו לי לתת לכם דוגמא כדי להבהיר לכם את החשיבות לכך אשר במקרה גם עונה לנו על החידה של מכפלה סקאלרית ממקודם.",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "הסר את מה שלמדת, ודמיין שאתה עדיין לא יודע שמוצר הנקודה קשור להקרנה.",
  "model": "google_nmt",
  "from_community_srt": "תעמיד פנים שלא למדת כלום עד כה ותדמיין שאתה כבר לא יודע שמכפלה סקלרית מתקשר להיטל.",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "מה שאני הולך לעשות כאן זה לקחת עותק של שורת המספרים ולמקם אותו באלכסון במרחב איכשהו, כשהמספר 0 יושב במקור.",
  "model": "google_nmt",
  "from_community_srt": "מה שאני הולך לעשות כאן, הוא לקחת עותק של קו המספרים והלניח אותו במרחב, באלכסון, בצורה כלשהי כך שהמספר 0 יושב בראשית שלו.",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "עכשיו חשבו על וקטור היחידה הדו-ממדית שקצהו יושב במקום שבו נמצא המספר 1 במספר.",
  "model": "google_nmt",
  "from_community_srt": "עכשיו תחשוב על וקטור היחידה מהעולם הדו-מימדי, כשהקצוות שלו יושבות על המספר 1 שבקו המספרים.",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "אני רוצה לתת לבחור הזה שם, כובע.",
  "model": "google_nmt",
  "from_community_srt": "אני רוצה לתת לך מישהו בשם u-כובע.",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "הבחור הקטן הזה ממלא תפקיד חשוב במה שעומד לקרות, אז פשוט שמור אותו בראשך.",
  "model": "google_nmt",
  "from_community_srt": "הבחור הזה משחק תפקיד חשוב במה שהולך לקרות לנו כאן, אז ככה שתשמור זאת בחלק האחורי של המוח שלך.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "אם נקרין וקטורים דו-ממדיים ישר על קו המספרים האלכסוני הזה, למעשה, הגדרנו זה עתה פונקציה שלוקחת וקטורים דו-ממדיים למספרים.",
  "model": "google_nmt",
  "from_community_srt": "אם אנחנו נזרוק וקטורים דו-מימדיים ישר לתוך קו המספרים האלכסוני, במובן שבדיוק עכשיו הגדרנו פונקציה שלוקחת וקטורים דו-מימדיים למספרים.",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "יתרה מכך, הפונקציה הזו היא למעשה ליניארית, מכיוון שהיא עוברת את המבחן החזותי שלנו שכל שורה של נקודות ברווח שווה נשארת מרווחת ברגע שהיא נוחתת על קו המספרים.",
  "model": "google_nmt",
  "from_community_srt": "מה שעוד, הפונקציה הזאת היא למעשה לינארית מכיוון שהיא עברה את המבחן הויזואלי שלנו כך שכל קו עם נקודות במרחק שווה זו מזו נשארות כך גם ברגע שהן נוחתות",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "רק כדי להיות ברור, למרות שהטבעתי את קו המספרים במרחב דו מימדי כך, הפלטים של הפונקציה הם מספרים, לא וקטורים דו מימדיים.",
  "model": "google_nmt",
  "from_community_srt": "על קו המספרים. רק כדי להיות ברור כאן, למרות ששיבצתי את קו המספרים במרחב הדו-מימדי ככה הפלט של הפונקציה הם מספרים, לא וקטורים דו-מימדיים.",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "כדאי לחשוב על פונקציה שמקבלת שתי קואורדינטות ומוציאה קואורדינטה אחת.",
  "model": "google_nmt",
  "from_community_srt": "אתה צריך לחשוב על פונקציה שלוקחת מספר קואורדינטות ומוציאה קואורדינטה אחת ויחידה.",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "אבל ה-u-hat הווקטור הזה הוא וקטור דו מימדי, החי במרחב הקלט.",
  "model": "google_nmt",
  "from_community_srt": "אבל הוקטור הזה - u-כובע, הוא וקטור דו-מימדי שחי במרחב של הפלט(של הפונקציה).",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "זה פשוט ממוקם בצורה כזו שחופפת עם הטבעה של שורת המספרים.",
  "model": "google_nmt",
  "from_community_srt": "הוא רק מונח בצורה כזאת שהוא מדלג על השיבוץ של המספרים על קו המספרים.",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "עם הקרנה זו, הגדרנו טרנספורמציה ליניארית מוקטורים דו-מימדיים למספרים, אז נוכל למצוא איזושהי מטריצה של 1x2 שמתארת את הטרנספורמציה הזו.",
  "model": "google_nmt",
  "from_community_srt": "עם ההיטל הזה, אנחנו עכשיו הגדרנו טרנספורמציה לינארית מוקטורים דו-מימדיים למספרים, אז אנחנו יכולים למצוא סוג של מטריצה 1x2 שמתארת טרנספורמציה.",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "כדי למצוא את המטריצה הזו בגודל 1x2, הבה נתקרב למערך קו המספרים האלכסוני הזה ונחשוב היכן i-hat ו-j-hat כל אחד נוחת, מכיוון שנקודות הנחיתה הללו יהיו העמודות של המטריצה.",
  "model": "google_nmt",
  "from_community_srt": "כדי למצוא מטריצה כזו, בוא נתקרב לצורה בה קו המספרים האלכסוני מונח ונחשוב איפה אמורים i-כובע ו-j כובע לנחות, מכיוון שנקודות הנחיתה הללו הולכות להיות עמודות המטריצה",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "החלק הזה סופר מגניב.",
  "model": "google_nmt",
  "from_community_srt": "החלק הזה ממש מגניב,",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "אנחנו יכולים לחשוב דרך זה עם פיסת סימטריה אלגנטית באמת.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "מכיוון ש-i-hat ו-u-hat הם שניהם וקטורים של יחידות, הקרנת i-hat על הקו העובר דרך u-hat נראית סימטרית לחלוטין להקרנת u-hat על ציר ה-x.",
  "model": "google_nmt",
  "from_community_srt": "אנחנו יכולים לנמק זאת בעזרת חתיכה אלגנטית של סימטריה: מכיוון שi-כובע ו-u כובע שניהם וקטורי יחידה, ההיטל של i-כובע של הקו העובר דרך u-כובע נראה לגמרי סימטרי להשים את ההיטל של u-כבע על ציר ה-x.",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "אז כשאנחנו שואלים על איזה מספר נוחת i-hat כשהוא מוקרן, התשובה תהיה זהה לכל כובע u שנוחת עליו כשהוא מוקרן על ציר ה-x.",
  "model": "google_nmt",
  "from_community_srt": "כך כשאנו נשאלים על איזה מספר i כובע נוחת, כאשר מניחים עליו היטל התשובה הולכת להיות זהה לא משנה איפה u-כובע נוחת כשמניחים את ההיטל שלו על",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "אבל השלכת כובע u על ציר ה-x פירושה רק לקחת את קואורדינטת ה-x של כובע u.",
  "model": "google_nmt",
  "from_community_srt": "ציר ה-x. אבל להניח את ההיטל של u-כובע על ציר ה-x משמעותה שלקחנו קואורדינטת x של u-כובע.",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "אז לפי סימטריה, המספר שבו i-hat נוחת כאשר הוא מוקרן על אותו קו מספרים אלכסוני הולך להיות קואורדינטת ה-x של u-hat.",
  "model": "google_nmt",
  "from_community_srt": "אז, מסימטריה, המספר איפה ש-i-כובע נוחת כאשר מניחים את ההיטל שלו על קו מספרים אלכסוני הולך להיות הקואורדינטה x של u-כובע.",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "זה לא מגניב?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "הנימוק כמעט זהה למקרה j-hat.",
  "model": "google_nmt",
  "from_community_srt": "האם זה לא מגניב? ההיגיון הוא כמעט זהה עבור המקרה של j-כובע.",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "תחשוב על זה לרגע.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "מכל אותן סיבות, קואורדינטת ה-y של u-hat נותנת לנו את המספר שבו נוחת ה-j-hat כאשר הוא מוקרן על עותק קו המספרים.",
  "model": "google_nmt",
  "from_community_srt": "תחשוב על זה לרגע. עבור כל אותן סיבות, הקואורדינטה y של u-כובע נותנת לנו את המספר איפה ש-j-כובע נוחת כאשר מניחים את היטלו על העתק של ציר המספרים.",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "עצור ותחשוב על זה לרגע.",
  "model": "google_nmt",
  "from_community_srt": "תפסיק לרגע את הסירטון ותהרהר לרגע;",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "אני פשוט חושב שזה ממש מגניב.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "אז הערכים של המטריצה 1x2 המתארים את טרנספורמציה ההשלכה יהיו הקואורדינטות של u-hat.",
  "model": "google_nmt",
  "from_community_srt": "אני פשוט חושב שזה ממש מגניב. כך שהערכים של מטריצה 1x2 המתארים את הטרנספורמציה של ההטלה הולכים להיות הקואורדינטות של u-כובע.",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "וחישוב טרנספורציית ההקרנה הזו עבור וקטורים שרירותיים במרחב, המחייבת הכפלת המטריצה הזו בוקטורים האלה, זהה מבחינה חישובית לנטילת מכפלת נקודה עם כובע u-hat.",
  "model": "google_nmt",
  "from_community_srt": "וחישוב אותה טרנספורמציה של ההיטל, עבור וקטורים שרירותיים במרחב, אשר דורש הכפלת אותה מטריצה בוקטורים הללו, היא זהה לחלוטין מבחינה חישובית כך שאם נבצע מכפלה סקאלרית על הוקטור u-כובע.",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "זו הסיבה שלקיחת מכפלת הנקודה עם וקטור יחידה יכולה להתפרש כהשלכת וקטור על הטווח של אותו וקטור יחידה ולקיחת האורך.",
  "model": "google_nmt",
  "from_community_srt": "זה למה לקחת מכפלה סקאלרית עם וקטור היחידה, אפשר לפרשה כך שמטילים את הוקטור על מרחב הפרישה של אותו וקטור יחידה",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "אז מה לגבי וקטורים שאינם יחידות?",
  "model": "google_nmt",
  "from_community_srt": "ולוקחים את האורך.",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "לדוגמה, נניח שניקח את הכובע הווקטורי של יחידה, אבל נגדיל אותו לפי גורם של 3.",
  "model": "google_nmt",
  "from_community_srt": "אז מה הם וקטורים שאינן וקטורי יחידה? לדוגמא, בוא נגיד שאנחנו לוקחים וקטור יחידה u-כובע, אבל אנחנו מכפילים אותו סקלרית בפקטור 3.",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "מבחינה מספרית, כל אחד מהמרכיבים שלו מוכפל ב-3.",
  "model": "google_nmt",
  "from_community_srt": "מבחינה מספרית,",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "אז כשמסתכלים על המטריצה הקשורה לווקטור הזה, נדרשות i-hat ו-j-hat עד פי שלושה מהערכים שבהם הם נחתו קודם לכן.",
  "model": "google_nmt",
  "from_community_srt": "כל אחד מרכיבי הוקטור מוכפל ב-3, אז אם מסתכלים על המטריצה הקשורה לאותו וקטור, היא לוקחת את i-כובע ו-j-כובע ל-3 פעמים הערך בו הם נחתו קודם.",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "מכיוון שהכל ליניארי, זה מרמז באופן כללי יותר שניתן לפרש את המטריצה החדשה כהשלכת כל וקטור על עותק קו המספרים והכפלה של המקום שבו הוא נוחת ב-3.",
  "model": "google_nmt",
  "from_community_srt": "מכיוון שכל זה לינארית, זה רומז בצורה יותר כללית, שאפשר לפרש את המטריצה החדשה כהטלת כל וקטור על העתק של קו המספרים ולהכפלת המקום בו הוא נוחת ב-3.",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "זו הסיבה שמכפלת הנקודה עם וקטור שאינו יחידה יכול להתפרש כהקרנה תחילה על אותו וקטור, ולאחר מכן להגדיל את אורך ההקרנה באורך הווקטור.",
  "model": "google_nmt",
  "from_community_srt": "זה למה מכפלה סקאלרית של וקטור שהוא לא וקטור יחידה אפשר לפרש זאת קודם כהיטל על אותו הוקטור ואז לעלות למעלה על אותו היטל ע\"י אורך הוקטור.",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "קחו רגע לחשוב על מה שקרה כאן.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "הייתה לנו טרנספורמציה ליניארית ממרחב דו מימדי לקו המספרים, שלא הוגדרה במונחים של וקטורים מספריים או תוצרי נקודות מספריים, היא פשוט הוגדרה על ידי הקרנת מרחב על עותק אלכסוני של קו המספרים.",
  "model": "google_nmt",
  "from_community_srt": "תיקח לעצמך רגע לחשוב מה קרה כאן, הייתה לנו טרנספורמציה לינארית מהמרחב הדו-מימדי לקו המספרים. אשר לא הייתה מוגדרת במונחים של וקטורים מספריים או ערכים מספריים מכפלה סקאלרית, זה היה פשוט מוגדר ע\"י הטלת המרחב על ההעתק של קו המספרים האלכסוני.",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "אבל מכיוון שהטרנספורמציה היא ליניארית, היא תוארה בהכרח על ידי איזו מטריצה 1x2.",
  "model": "google_nmt",
  "from_community_srt": "אבל בגלל שהטרנספורמציה היא לינארית, היא בהכרח מתוארת ע\"י מטריצה 1 על 2,",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "ומכיוון שהכפלת מטריצה 1x2 בוקטור דו-ממדית זהה להפיכת המטריצה הזו על צדה ולקחת מכפלת נקודות, הטרנספורמציה הזו הייתה קשורה באופן בלתי נמנע לוקטור דו-ממדי כלשהו.",
  "model": "google_nmt",
  "from_community_srt": "ומכיוון שהכפלת מטריצה 1x2 ע\"י וקטור דו-מימדי הוא כמו להטות את המטריצה הזאת על צידה ולאחר מכן להכפילה בסקלר, הטרנספורמציה הזאת הייתה קשורה, בצורה שלא ניתן להפרידה,",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "הלקח כאן הוא שבכל פעם שיש לך אחת מהטרנספורמציות הליניאריות האלה שמרחב הפלט שלה הוא קו המספרים, לא משנה איך הוא הוגדר, יהיה איזה וקטור ייחודי V המתאים לאותה הטרנספורמציה, במובן שיישום הטרנספורמציה הוא אותו דבר כמו לקחת תוצר נקודה עם הווקטור הזה.",
  "model": "google_nmt",
  "from_community_srt": "לוקטור דו-מימדי כלשהו. השיעור הנלמד מכאן הוא, שבכל רגע שיש לך את אחת מהטרנספורמציות הלינאריות הללו, כאשר הפלט שלה במרחב הוא קו מספרים, לא משנה איך היא הייתה מוגדרת, תמיד יהיה וקטור מיוחד, הוקטור v המתאים לטרנספורמציה הזאת, במובן שלהפעיל את הטרנספורמציה,",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "בעיני זה יפה לגמרי.",
  "model": "google_nmt",
  "from_community_srt": "זה כמו לקחת לבצע מכפלה סקאלרית עם אותו וקטור. בשבילי, זה לחלוטין יפה.",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "זו דוגמה למשהו במתמטיקה שנקרא דואליות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "הדואליות מופיעה בדרכים ובצורות רבות ושונות במהלך המתמטיקה, וקשה מאוד להגדיר אותה.",
  "model": "google_nmt",
  "from_community_srt": "זו דוגמא למשהו במתמטיקה שנקראית \"דואליות\", ה\"דואליות\" צצה בכל מיני צורות ודרכים לכל אורכה של המתמטיקה וממש קשה להגדירה.",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "באופן רופף, זה מתייחס למצבים שבהם יש לך התכתבות טבעית אך מפתיעה בין שני סוגים של דברים מתמטיים.",
  "model": "google_nmt",
  "from_community_srt": "אם נדבר בצורה חופשית, זה מתייחס למצבים שיש לך התאמה מפתיעה, אך טבעית בין שני סוגים של משהו במתמטיקה.",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "עבור מקרה האלגברה הליניארית שעליו למדת זה עתה, היית אומר שהדואלי של וקטור הוא הטרנספורמציה הליניארית שהוא מקודד, והדואלי של טרנספורמציה ליניארית ממרחב כלשהו למימד אחד הוא וקטור מסוים במרחב הזה.",
  "model": "google_nmt",
  "from_community_srt": "עבור המקרה של אלגברה לינארית שהרגע למדת עליו, אתה תאמר שה\"דואליות\" של וקטור הוא הטרנספורמציה הלינארית שהוא מצפין בתוכו. והדואליות של טרנספורמציה לינארית ממרחב כלשהו למימד חד-מימדי, הוא וקטור מסוים במרחב הזה.",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "אז לסיכום, על פני השטח, תוצר הנקודה הוא כלי גיאומטרי שימושי מאוד להבנת תחזיות ולבדיקה האם וקטורים נוטים להצביע לאותו כיוון או לא.",
  "model": "google_nmt",
  "from_community_srt": "אז, לסיכום, על פני השטח, המכפלה הסקאלרית היא כלי מאוד שימושי מבחינה גיאומטרית כדי להבין היטלים וכדי לבדוק האם כן או לא הוקטורים כן נוטים להצביע באותו כיוון.",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "וזה כנראה הדבר הכי חשוב לך לזכור לגבי מוצר הנקודה.",
  "model": "google_nmt",
  "from_community_srt": "וזה בהכרח הדבר הכי חשוב בשבילך שתזכור לגבי המכפלה סקאלרית.",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "אבל ברמה עמוקה יותר, ניקוד שני וקטורים יחד הוא דרך לתרגם אחד מהם לעולם של טרנספורמציות.",
  "model": "google_nmt",
  "from_community_srt": "אבל ברמה עמוקה יותר, לבצע מכפלה סקאלרית על שני וקטורים היא דרך לתרגם אחד מהם לתוך העולם של טרנספורמציות: שוב,",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "שוב, מבחינה מספרית, זה עשוי להרגיש כמו נקודה מטופשת להדגיש.",
  "model": "google_nmt",
  "from_community_srt": "מבחינה מספרית,",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "זה רק שני חישובים שנראים דומים במקרה.",
  "model": "google_nmt",
  "from_community_srt": "זה אולי מרגיש כמו משהו טיפשי להדגיש כאן, זה פשוט שני חישובים שבמקרה נראים זהים.",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "אבל הסיבה שאני מוצא את זה כל כך חשוב היא שבמהלך המתמטיקה, כשאתה מתמודד עם וקטור, ברגע שאתה באמת מכיר את האישיות שלו, לפעמים אתה מבין שקל יותר להבין אותו לא כחץ במרחב, אלא בתור התגלמות פיזית של טרנספורמציה ליניארית.",
  "model": "google_nmt",
  "from_community_srt": "אבל הסיבה שאני חושב שזה כה חשוב, היא כי במהלך המתמטיקה, כשאתה מתעסק עם וקטור, ברגע שאתה מתחיל לדעת את האישיות שלו לפעמים אתה מבין שזה קל יותר להבין את זה, לא כל כחץ במרחב, אלא כהתגלמות פיזית של טרנספורמציה לינארית.",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "זה כאילו הווקטור הוא בעצם רק קיצור רעיוני לטרנספורמציה מסוימת, מכיוון שקל לנו יותר לחשוב על חיצים במרחב במקום להעביר את כל הרווח הזה לקו המספרים.",
  "model": "google_nmt",
  "from_community_srt": "זה כאילו שהוקטור הוא קיצור רעיוני לטרנספורמציה כלשהי, מכיוון שעבורינו יותר קל לחשוב על חצים במרחב יותר מאשר להזיז את כל המרחב לכדי קו מספרים.",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.97
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "בסרטון הבא, תראו עוד דוגמה ממש מגניבה של הדואליות הזו בפעולה, כשאני מדבר על מוצר הצלב.",
  "model": "google_nmt",
  "from_community_srt": "בסירטון הבא, אתה תראה עוד דוגמא מגניבה של אותו \"דואליות\" בפעולה כשאני מדבר על מכפלה וקטורית.",
  "n_reviews": 0,
  "start": 822.61,
  "end": 829.19
 }
]