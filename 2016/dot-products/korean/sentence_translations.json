[
 {
  "input": "[\"Ode to Joy\", by Beethoven, plays to the end of the piano.] Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "",
  "from_community_srt": "전통적으로, 내적(dot product) 같은 것은 선형대수 강의에서 상당히 앞쪽에 나와. 보통 시작 부근에 있지.",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "",
  "from_community_srt": "그래서 내가 이 시리즈에서 뒷쪽에 놓는 게 좀 이상하게 보일수도 있어.",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "",
  "from_community_srt": "내가 이렇게 한 이유는 기존 방법대로 소개하면 벡터의 기초지식만 있어도 되긴한데, 하지만 수학에서 내적의 역할에 대한 제대로된 이해를 하려면 선형변환이라는 이해가 반드시 있어야만 해.",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "",
  "from_community_srt": "하지만, 그전에 간단하게 소개할게. 기존 방법이 내적을 어떻게 소개하는지를. 내가 가정하는 건 많은 사람들에게 적어도 부분적으로라도 검토받았어.",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "",
  "from_community_srt": "수치적으로, 같은 차원의 두 벡터가 있다고 해볼게. 같은 갯수의 숫자를 가지고 있을 것이고, 내적(dot product) 구한다는 것은, 말하자면, 같은 좌표값으로 짝을 지어 곱하고 모두 더하면 돼.",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "",
  "from_community_srt": "그래서 [1,2] 벡터와 [3,4] 벡터의 내적은 1 x 3 + 2 x 4 가 되지.",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "",
  "from_community_srt": "[6, 2, 8, 3] 벡터와 [1, 8, 5, 3] 벡터의 내적은 6 x 1   +   2 x 8   +   8 x 5   +   3 x 3 다행히도,",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "",
  "from_community_srt": "이 계산에 잘 맞는 기하학적 해석방법이 있어.",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "",
  "from_community_srt": "두 벡터 v, w 의 내적(dot product)을 살펴보자. w 벡터를 투영(project)할건데, v 벡터와 원점을 지나는 선 위로 할거야.",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "",
  "from_community_srt": "이 투영된 w 벡터의 길이에 벡터v 길이를 곱하는 것 이것이 v・w 내적이야.",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "",
  "from_community_srt": "w 벡터 투사체가 v 벡터 방향과 반대이면,",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "",
  "from_community_srt": "그럼 내적(dot product)은 음수가 돼. 그리고 두 벡터가 같은 방향을 가리키면,",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "",
  "from_community_srt": "내적은 양수(positive number)가 돼. 그리고 직각을 이루는 경우, 즉, 한 벡터가 다른벡터로 투영 하면 0벡터가 되는 경우, 내적은 0 이야.",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "",
  "from_community_srt": "반대방향을 가리킬 경우,",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "",
  "from_community_srt": "내적은 음수가 돼. 근데, 이런 해석방법은 뭔가 좀 비대칭적이야.",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "",
  "from_community_srt": "이 방법은 두 벡터를 매우 다르게 다뤄.",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "",
  "from_community_srt": "그래서 난 처음 이걸 배울때,",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "",
  "from_community_srt": "순서가 중요하지 않다는 것에 놀랐었어. 반대로 v 를 w 로 투사(project) 하는 것, 즉, v 의 투사체에 w 벡터길이를 곱해도 같은 값을 얻게 된다니!",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "",
  "from_community_srt": "두 방법이 전혀 다른 계산 같지 않아?",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "",
  "from_community_srt": "그럼 왜 순서가 중요하지 않은지에 대해 직관적 설명을 해볼게.",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "",
  "from_community_srt": "v 와 w 가 같은 길이를 가진다면,",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "",
  "from_community_srt": "여기서는 둘의 대칭성을 사용할 수 있어. 그럼, w 를 v 쪽으로 투사해서 투사체 길이에 v 길이를 곱하는 것이 반대방향으로 투사하는 것과 완전한 대칭 거울상이야. v 를 w 로 투사해서 그 길이에 w 길이를 곱하는 것과 말야.",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "",
  "from_community_srt": "이제, 벡터들 중 하나를 \"스케일(scale)\" 해서, v 벡터를 2배만큼 늘려보자. 그래서 두 벡터의 길이를 다르게 만들어. 이제 대칭성이 깨졌어.",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "",
  "from_community_srt": "2v 와 w 의 내적을 어떻게 해석해야할지 생각해보자.",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "",
  "from_community_srt": "만약 w 가 v 로 투사하는 경우라면, 2v・w 내적값은 정확히 v・w 내적값의 두배가 될거야.",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "",
  "from_community_srt": "(v 길이만 두배가 됐기때문에) 왜냐하면 v 를 2로 \"스케일(scale)\" 했기 때문이야. 투사된 w 길이는 그대로야. 하지만 투사받는 쪽 벡터 길이는 두배가 됬지.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "",
  "from_community_srt": "이번엔 반대로, v 에서 w 로 투사하는 경우를 생각해보자.",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "",
  "from_community_srt": "이 경우에는, \"스케일\" 된 v 벡터의 투사체 길이가 2 배야. 투사받는 쪽인 벡터 길이는 그대로야.",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "",
  "from_community_srt": "그래서 결과적으로 이번에도 내적이 두배가 돼.",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "",
  "from_community_srt": "이렇게 대칭성이 깨진 경우라도, 내적값에 영향을 주는 \"스케일링\" 효과는 어느쪽 설명으로보나 똑같아.",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "",
  "from_community_srt": "이제 또 다른 커다란 질문이 있어. 처음 배울때 나를 혼란스럽게 했던 거야.",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "",
  "from_community_srt": "도대체 숫자상으로 좌표값을 매칭하고 곱한다음 더하는 거랑, 투사(projection,",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "",
  "from_community_srt": "투영)과는 무슨 관계인거야? 글쎄, 만족스러운 대답을 얻으려면, 또, 내적에 중요성에 대한 제대로된 정의를 해야한다면, 좀 더 파고 내려가야 될 거야. 그리고 거기서 \"이중성(duality)\" 이라는 것을 만나게 돼.",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "",
  "from_community_srt": "근데, 바로 설명 들어가기 전에, 선형변환에 대해 먼저 얘기해야할 게 있어. 다차원에서 1차원으로의 선형변환에 관한 거야. 즉,",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "",
  "from_community_srt": "결과 차원이 그냥 1차원 수선이 되는 변환이지. 이 변환들은 일종의 함수로서, 2차원 벡터를 입력받아서 숫자 하나를 내놓고 있어. 물론, 선형변환은 2차원 입력에서 1차원 출력으로 가는 다른 흔한 함수들보다는 훨씬 제한적이야.",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "",
  "from_community_srt": "더 높은 차원으로 가는 변환도 마찬가지로, 챕터 3에서 내가 말했던 것이기도 한데, 이 함수가 선형(linear)이 되려면 어떤 공식적인 속성이 있어야만 하지. (역주: 평행, 균등간격, 원점고정) 하지만 여기서는 자세한 것들을 무시할게. 이런 것들은 주의만 산만해질 뿐이야. 대신 모든 공식적 속성을 동등한 시각적 속성을 통해 살펴보자.",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "",
  "from_community_srt": "선에 같은 간격으로 점이 있고, 이 선에 변환을 적용하면, 선형변환이라면 점들사이 균등간격이 유지될거야. 결과 공간, 즉 수선(number line) 의 점들로 바뀐 이후에도 그렇겠지.",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "",
  "from_community_srt": "반면에, 선의 점 간격이 균등하지 않게 바뀌면,",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "",
  "from_community_srt": "이런 변환은 비선형적(not linear) 이야. 우리가 앞서 봤던 예제들 처럼, 선형변환은 i-hat 과 j-hat 의 도착위치에 의해 완전히 결정돼. 하지만 이번에는, 기저벡터들의 도착지가 수선의 숫자일뿐이지. 기저벡터의 도착지가 행렬의 열이기 때문에 각 열은 하나의 숫자로만 이루어지게 돼.",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "",
  "from_community_srt": "x 2",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "",
  "from_community_srt": "이것이 1 x 2 행렬이야. 예를 통해서 좀 더 살펴보자. 이 변환(행렬)을 벡터에 적용한다는 것은 무슨 의미일까?",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "",
  "from_community_srt": "어떤 선형변환이 있다고 해보자. 변환후 i-hat 은 1, j-hat 은 -2 인 경우야.",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "",
  "from_community_srt": "좌표값 [4, 3]인 벡터가 어디로 이동하는지 알기 위해서는 벡터를 4 * i-hat + 3 * j-hat 으로 분해해서 생각해야해.",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "",
  "from_community_srt": "선형성(linearity)에 따르면, 변환 후에는 같은 비율이여야 하기 때문에 변환된 i-hat 의 4배에 변환된 j-hat 의 3배를 더한것과 같아. 이 경우 결과적으로 -2 에 도착하지.",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "",
  "from_community_srt": "이 계산을 순전히 수치적으로 보면,",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "",
  "from_community_srt": "이게바로 행렬-벡터 곱셈이 되지. 이제, 이 1x2 행렬에 벡터를 곱하는 수치연산은 두 벡터의 내적(dot product)과 똑같게 느껴질거야.",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "",
  "from_community_srt": "1x2 행렬이 벡터를 그냥 옆으로 뉘여놓은 것 같지 않아?",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "",
  "from_community_srt": "사실, 이제는 말할때가 된 것 같아. 1x2 행렬과 2차원 벡터사이에는 멋진 관련성이 있어. 벡터의 숫자 표현을 옆으로 기울여서 연관 행렬을 얻거나  또는 행렬을 세워서 연관 벡터를 얻거나 하는 관련성이지.",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "",
  "from_community_srt": "당장 수치 표현만을 살펴볼것이기 때문에 벡터와 1 × 2 행렬 사이를 바꾸는 것이 뭔가 단순한 바보 짓처럼 느껴질수도 있어.",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "",
  "from_community_srt": "하지만, 이건 기하학적 관점에서 보면 뭔가 멋진 어떤 것을 제공해줘.",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "",
  "from_community_srt": "어떤 연결성 같은 건데, 입력이 벡터고 출력이 숫자인 선형변환과 벡터 그 자신과의 관계야.",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "",
  "from_community_srt": "그 중요성을 명확히 보여주는 예를 하나 들어볼게. 그리고 앞에나온 내적 문제에 관한 답변과도 관련있어.",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "",
  "from_community_srt": "기존에 배웠던 것들은 잊어버리고 내적과 투영이 관계있다는 것을 아직 모른다고 생각하고 들어봐.",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "",
  "from_community_srt": "난 여기 수선의 복사본을 만들어서 대각선 방향으로 비스듬히 놓을 건데, 숫자 0 이 원점과 겹치게 둘거야.",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "",
  "from_community_srt": "이제 2차원 형태의 수선의 단위벡터를 떠올려봐. 수선의 숫자 1을 가리키고 있을거야.",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "",
  "from_community_srt": "이 벡터에 u-hat 이라고 이름을 붙일거야.",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "",
  "from_community_srt": "이 작은 벡터가 앞으로 일어날 일에서 중요한 역할을 할거야. 마음속에 잘 기억해둬.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "",
  "from_community_srt": "2차원 벡터를 이 대각선방향의 수선에 투영(project) 하게 되면 이건 사실상, 우리가 2차원 벡터를 입력받아 숫자를 내놓는 함수를 정의한게 돼.",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "",
  "from_community_srt": "무엇보다도, 이 함수는 진짜 선형적이야. 눈으로 보면 일단 맞아. 균등 간격의 점을 가진 선이라면 수선으로 투사한 이후에도 균등 간격이야.",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "",
  "from_community_srt": "다만 이해를 도우려고, 2차원 공간에 수선을 넣어놨을 뿐, 이 함수의 출력은 (수선의) 숫자이지, 2차원 평면의 벡터로 보면 안돼.",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "",
  "from_community_srt": "이 함수를 마치 두개의 좌표값을 입력받아 하나의 좌표값을 출력하는 것처럼 생각할 수도 있어.",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "",
  "from_community_srt": "근데 벡터 u-hat 은 2차원 벡터야. 입력과 같은 공간에 존재하지.",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "",
  "from_community_srt": "단지 삽입된 수선과 겹쳐진 상황이지.",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "",
  "from_community_srt": "이렇게 투영을 통해, 우리는 2차원벡터에서 숫자로 가는 선형변환 하나를 정의했어. 이 변환은 1x2 행렬로 나타낼 수 있지.",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "",
  "from_community_srt": "이 1 × 2 행렬의 값을 알아내기 위해서, 아까 그려놓은 수선을 확대해서 i-hat 과 j-hat 이 어디로 움직이는 살펴보자. 기저벡터의 도착지가 행렬의 열인 것은 이미 알거야.",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "",
  "from_community_srt": "진짜 우아한 대칭성을",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "",
  "from_community_srt": "이 부분이 굉장히 멋진 부분인데, 우리는 진짜 우아한 대칭성을 사용해서 알아낼 수 있어.",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "",
  "from_community_srt": "i-hat 과 u-hat 모두 단위벡터(길이 1)여서 i-hat 을 u-hat 을 통과하는 선으로 투영하는 것은 u-hat 을 x 축(i-hat) 에 투영하는 것과 완전히 대칭이야. (역자:",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "",
  "from_community_srt": "앞의 내적 설명에서 나왔죠.) 그래서 i-hat 이 투영 위치를 구할때 u-hat 을 x 축(i-hat) 에 투영된 위치를 구하는 것과 똑같아.",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "",
  "from_community_srt": "그런데 u-hat 을 x 축에 투영하는 것은 그냥 u-hat 의 x 좌표값을 구하는 거랑 같아.",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "",
  "from_community_srt": "그래서, 대칭성에 의해, i-hat 에서 수선으로 투영후 위치는  u-hat 의 x 좌표값이 될거야.",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "",
  "from_community_srt": "잠깐만",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "",
  "from_community_srt": "멋지지 않아? j-hat 의 경우에도 완전이 같아.",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "",
  "from_community_srt": "y",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "",
  "from_community_srt": "잠깐만 생각하면 나올거야. 같은 이유로, u-hat 의 y 좌표값은 j-hat 을 수선으로 투영한 위치와 같을거야.",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "",
  "from_community_srt": "잠깐 여기서 잠시 생각해봐야 돼.",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "",
  "from_community_srt": "난 이부분이 정말로 굉장한 것 같아.",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "",
  "from_community_srt": "u-hat 의 좌표가 되는거야.(!) 임의 벡터의 투영은 이 행렬에 임의벡터를",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "",
  "from_community_srt": "투영 변환을 나타내는 1x2 행렬은 그냥 u-hat 의 좌표가 되는거야.(!) 임의 벡터의 투영은 이 행렬에 임의벡터를 곱하는 것이고, 이건 계산적으로 u-hat 과의 내적(dot product)과 똑같아.",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "",
  "from_community_srt": "이것이 어째서 단위벡터와의 내적과 , 벡터를 단위벡터로 투영한 길이를 구하는 것과 같아지는지를 설명해줘.",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "",
  "from_community_srt": "그럼 비-단위(non-unit) 벡터의 경우는 어떨까?",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "",
  "from_community_srt": "예를 들어, u-hat 벡터를 가져다가 3배로 \"스케일\" 해보는 거야.",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "",
  "from_community_srt": "수치적으로는, 각 구성요소가 3배가 될거야.",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "",
  "from_community_srt": "그리고 그 벡터와 연관된 행렬을 찾으려면 이전에 i-hat 과 j-hat 의 변환된 위치에 3배를 하면 돼.",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "",
  "from_community_srt": "모두 선형(linear) 이기 때문이야. 이것이 암시하는 것은, 새로운 행렬을 해석하기를, 어떤 벡터를 수선(number line) 원본 (3배하기 전)에 투영한다음  투영 후 위치에서 3을 곱한거야.",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "",
  "from_community_srt": "이것이 왜 비-단위(non-unit) 벡터의 내적(dot product)을 해석할때, 그 벡터 위로 투영한 후, 벡터의 길이만큼 투사체 길이를 늘리는 것이라고 보는 이유야.",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "",
  "from_community_srt": "잠시 멈춰서 이게 무슨 말인지 생각해보자.",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "",
  "from_community_srt": "우리는 2차원 공간에서 1차원 수선으로 가는 선형변환을 가지고 있었어. 수치적 벡터나 수치적 내적이라는 용어로 정의하기 전이였지. 단지 대각방향의 수선 원본에 투영만을 정의했을 뿐이야.",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "",
  "from_community_srt": "하지만, 변환이 선형적(linear) 이기 때문에 이건 필연적으로 1x2 행렬로 표현할 수 있었어. (역자:",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "",
  "from_community_srt": "선형변환은 모두 행렬로 표현할 수 있는 듯) 그리고 1x2 행렬에 2차원 벡터를 곱하는 것은 그 행렬을 옆으로 눕혀서 내적을 구한 것과 같기 때문에 이런 변환은 불가피하게 2차원 벡터와 관련이 있을 수밖에 없어.",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "",
  "from_community_srt": "여기서 배울 것은, 언제든 이런 선형 변환 중 하나를 가지고 있다면, 즉, 결과 공간이 수선(number line)인 선형변환을 가지고 있다면, 어떻게 정의하든지 간에, 어떤 유일한 벡터가 그 변환에 대응되고 있을거야. 변환의 적용은 벡터의 내적을 구하는 것과 같음을 알 수 있어.",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "",
  "from_community_srt": "나에게 있어, 이건 정말 아름답게 느껴져.",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "",
  "from_community_srt": "이것이 수학에서 말하는 \"이중성(duality)\" 에 관한 경우야.",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "",
  "from_community_srt": "\"이중성\" 은 수학 전반을 통해 많은 방법과 형태로 등장해. 그리고 실제로 정의 내용은 굉장히 까다로워.",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "",
  "from_community_srt": "쉽게 말하자면, 자연스러우면서도 놀라운 대응관계가 두 개의 수학적 대상물 사이에서 나타나는 거야.",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "",
  "from_community_srt": "방금 배운 선형 대수의 경우에는, 벡터에서 \"이중(dual)\"이라는 것은 그 벡터가 가진 선형변한 성질을 말해. 그리고 1차원으로 변환시키는 선형변환에서 이중(dual)이란, 공간상의 특정 벡터를 말해.",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "",
  "from_community_srt": "요약하면, 내적은 투영을 이해하는데 매우 유용한 기하학적 도구야. 벡터가 같은 방향을 가리키는지를 알아내는데도 유용한 도구지.",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "",
  "from_community_srt": "이것이 아마도 너가 내적에서 기억해야 할 가장 중요한 점일 거야.",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "",
  "from_community_srt": "좀 더 깊게 보자면, 두 벡터를 함께 내적하는 것(dotting)은 두 벡터 중 하나를 변환인자로 보는 거야.",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "",
  "from_community_srt": "다시말해, 수치적으로 볼 때, 이건 강조하는게 좀 웃길수도 있는데,",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "",
  "from_community_srt": "두 계산이 유사하다는 거야.",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "",
  "from_community_srt": "하지만 이것이 중요한 이유는, 수학 전반에 걸쳐서, 너가 벡터로 다룰때라든지 할때, 너가 제대로 이해했다면, 언젠가 느껴질때가 있을거야. 공간속의 화살표보다는 선형변환의 물리적 실체 이해하는 편이 더 쉬울때가 있을거야.",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "",
  "from_community_srt": "벡터를 마치 어떤 변환의 개념적 단축 표현으로 생각해. 우리는 공간상의 화살표를 생각하는 것이 공간에서 수선으로 투영을 생각하는 것보다 쉽기 때문이야.",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.97
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "",
  "from_community_srt": "다음 동영상에서는 \"이중성(duality)\" 의 또다른 멋진 예제를 보여줄거야.",
  "n_reviews": 0,
  "start": 822.61,
  "end": 829.19
 }
]