[
 {
  "input": "Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[베토벤의 &quot;환희의 송가&quot;는 피아노 끝까지 연주됩니다.] 전통적으로 내적은 선형 대수 과정의 초기 단계, 일반적으로 시작 부분에 소개되는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "전통적으로, 내적(dot product) 같은 것은 선형대수 강의에서 상당히 앞쪽에 나와. 보통 시작 부근에 있지.",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "그래서 제가 시리즈에서 그것들을 여기까지 뒤로 밀었다는 것이 이상하게 보일 수도 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서 내가 이 시리즈에서 뒷쪽에 놓는 게 좀 이상하게 보일수도 있어.",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "제가 이렇게 한 이유는 주제를 소개하는 표준 방법이 있기 때문입니다. 이를 위해서는 벡터에 대한 기본적인 이해만 필요하지만 수학에서 내적의 역할에 대한 완전한 이해는 실제로 선형 변환을 통해서만 찾을 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "내가 이렇게 한 이유는 기존 방법대로 소개하면 벡터의 기초지식만 있어도 되긴한데, 하지만 수학에서 내적의 역할에 대한 제대로된 이해를 하려면 선형변환이라는 이해가 반드시 있어야만 해.",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "하지만 그 전에 내적(dot product)이 도입되는 표준 방식에 대해 간략하게 설명하겠습니다. 이는 적어도 많은 시청자를 위해 부분적으로 검토된 것으로 가정합니다.",
  "model": "google_nmt",
  "from_community_srt": "하지만, 그전에 간단하게 소개할게. 기존 방법이 내적을 어떻게 소개하는지를. 내가 가정하는 건 많은 사람들에게 적어도 부분적으로라도 검토받았어.",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "수치적으로, 동일한 차원의 두 벡터, 동일한 길이의 두 숫자 목록이 있는 경우 내적을 취한다는 것은 모든 좌표를 쌍으로 만들고 해당 쌍을 곱한 다음 결과를 더하는 것을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "수치적으로, 같은 차원의 두 벡터가 있다고 해볼게. 같은 갯수의 숫자를 가지고 있을 것이고, 내적(dot product) 구한다는 것은, 말하자면, 같은 좌표값으로 짝을 지어 곱하고 모두 더하면 돼.",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "따라서 3, 4가 점으로 표시된 벡터 1, 2는 1 곱하기 3 더하기 2 곱하기 4가 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서 [1,2] 벡터와 [3,4] 벡터의 내적은 1 x 3 + 2 x 4 가 되지.",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "1, 8, 5, 3이 점으로 표시된 벡터 6, 2, 8, 3은 6 곱하기 1 더하기 2 곱하기 8 더하기 8 곱하기 5 더하기 3 곱하기 3이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "[6, 2, 8, 3] 벡터와 [1, 8, 5, 3] 벡터의 내적은 6 x 1   +   2 x 8   +   8 x 5   +   3 x 3 다행히도,",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "운 좋게도 이 계산에는 정말 좋은 기하학적 해석이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 계산에 잘 맞는 기하학적 해석방법이 있어.",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "두 벡터 v와 w 사이의 내적을 생각하려면 v의 원점과 끝을 통과하는 선에 w를 투영한다고 상상해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "두 벡터 v, w 의 내적(dot product)을 살펴보자. w 벡터를 투영(project)할건데, v 벡터와 원점을 지나는 선 위로 할거야.",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "이 투영의 길이에 v의 길이를 곱하면 내적 v dot w가 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "이 투영된 w 벡터의 길이에 벡터v 길이를 곱하는 것 이것이 v・w 내적이야.",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "w의 투영이 v와 반대 방향을 가리키는 경우를 제외하고 내적은 실제로 음수가 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "w 벡터 투사체가 v 벡터 방향과 반대이면, 그럼 내적(dot product)은 음수가 돼.",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "따라서 두 벡터가 일반적으로 같은 방향을 가리키면 내적은 양수입니다.",
  "model": "google_nmt",
  "from_community_srt": "그리고 두 벡터가 같은 방향을 가리키면, 내적은 양수(positive number)가 돼.",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "수직인 경우, 즉 하나를 다른 하나에 투영하는 것이 0 벡터임을 의미하며 내적은 0입니다.",
  "model": "google_nmt",
  "from_community_srt": "그리고 직각을 이루는 경우, 즉, 한 벡터가 다른벡터로 투영 하면 0벡터가 되는 경우, 내적은 0 이야.",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "그리고 일반적으로 반대 방향을 가리키면 내적은 음수입니다.",
  "model": "google_nmt",
  "from_community_srt": "반대방향을 가리킬 경우,",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "그런데 이 해석은 이상하게도 비대칭적입니다.",
  "model": "google_nmt",
  "from_community_srt": "내적은 음수가 돼. 근데, 이런 해석방법은 뭔가 좀 비대칭적이야.",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "두 벡터를 매우 다르게 처리합니다.",
  "model": "google_nmt",
  "from_community_srt": "이 방법은 두 벡터를 매우 다르게 다뤄.",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "그래서 처음 이것을 배웠을 때 순서가 중요하지 않다는 사실에 놀랐습니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서 난 처음 이걸 배울때,",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "대신 v를 w에 투영하고 투영된 v의 길이에 w의 길이를 곱하여 동일한 결과를 얻을 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "순서가 중요하지 않다는 것에 놀랐었어. 반대로 v 를 w 로 투사(project) 하는 것, 즉,",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "제 말은, 그건 정말 다른 과정처럼 느껴지지 않나요?",
  "model": "google_nmt",
  "from_community_srt": "v 의 투사체에 w 벡터길이를 곱해도 같은 값을 얻게 된다니! 두 방법이 전혀 다른 계산 같지 않아?",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "순서가 중요하지 않은 이유에 대한 직관은 다음과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "그럼 왜 순서가 중요하지 않은지에 대해 직관적 설명을 해볼게.",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "v와 w의 길이가 같다면 대칭성을 활용할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "v 와 w 가 같은 길이를 가진다면, 여기서는 둘의 대칭성을 사용할 수 있어.",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "w를 v에 투영한 다음 해당 투영의 길이에 v의 길이를 곱하면 v를 w에 투영한 다음 해당 투영의 길이에 w의 길이를 곱하는 완전한 거울상이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "그럼, w 를 v 쪽으로 투사해서 투사체 길이에 v 길이를 곱하는 것이 반대방향으로 투사하는 것과 완전한 대칭 거울상이야. v 를 w 로 투사해서 그 길이에 w 길이를 곱하는 것과 말야.",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "이제 v 중 하나를 2와 같은 상수로 확장하여 길이가 동일하지 않게 하면 대칭이 깨집니다.",
  "model": "google_nmt",
  "from_community_srt": "이제, 벡터들 중 하나를 \"스케일(scale)\" 해서, v 벡터를 2배만큼 늘려보자. 그래서 두 벡터의 길이를 다르게 만들어. 이제 대칭성이 깨졌어.",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "하지만 이 새로운 벡터인 2 곱하기 v와 w 사이의 내적을 어떻게 해석할지 생각해 봅시다.",
  "model": "google_nmt",
  "from_community_srt": "2v 와 w 의 내적을 어떻게 해석해야할지 생각해보자.",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "w가 v에 투영되는 것으로 생각하면 내적 2v dot w는 내적 v dot w의 정확히 두 배가 될 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "만약 w 가 v 로 투사하는 경우라면, 2v・w 내적값은 정확히 v・w 내적값의 두배가 될거야.",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "이는 v를 2로 스케일링하면 w의 투영 길이는 변경되지 않지만 투영하는 벡터의 길이는 두 배가 되기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "(v 길이만 두배가 됐기때문에) 왜냐하면 v 를 2로 \"스케일(scale)\" 했기 때문이야. 투사된 w 길이는 그대로야. 하지만 투사받는 쪽 벡터 길이는 두배가 됬지.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "하지만 다른 한편으로, v가 w에 투영되는 것에 대해 생각하고 있다고 가정해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "이번엔 반대로, v 에서 w 로 투사하는 경우를 생각해보자.",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "이 경우 투영의 길이는 v에 2를 곱할 때 크기가 조정되지만 투영하는 벡터의 길이는 일정하게 유지됩니다.",
  "model": "google_nmt",
  "from_community_srt": "이 경우에는, \"스케일\" 된 v 벡터의 투사체 길이가 2 배야. 투사받는 쪽인 벡터 길이는 그대로야.",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "따라서 전체적인 효과는 여전히 내적을 두 배로 늘리는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서 결과적으로 이번에도 내적이 두배가 돼.",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "따라서 이 경우 대칭이 깨졌더라도 이 스케일링이 내적 값에 미치는 영향은 두 해석 모두에서 동일합니다.",
  "model": "google_nmt",
  "from_community_srt": "이렇게 대칭성이 깨진 경우라도, 내적값에 영향을 주는 \"스케일링\" 효과는",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "제가 이 내용을 처음 배웠을 때 저를 혼란스럽게 했던 또 하나의 큰 질문이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "어느쪽 설명으로보나 똑같아. 이제 또 다른 커다란 질문이 있어. 처음 배울때 나를 혼란스럽게 했던 거야.",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "좌표를 일치시키고, 쌍을 곱하고, 더하는 이 수치적 과정이 도대체 투영과 관련이 있는 이유는 무엇입니까?",
  "model": "google_nmt",
  "from_community_srt": "도대체 숫자상으로 좌표값을 매칭하고 곱한다음 더하는 거랑, 투사(projection,",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "글쎄, 만족스러운 대답을 제공하고 내적의 중요성을 완전히 정의하려면 여기에서 진행되는 좀 더 깊은 내용을 찾아야 합니다. 이는 종종 이중성이라는 이름으로 사용됩니다.",
  "model": "google_nmt",
  "from_community_srt": "투영)과는 무슨 관계인거야? 글쎄, 만족스러운 대답을 얻으려면, 또, 내적에 중요성에 대한 제대로된 정의를 해야한다면, 좀 더 파고 내려가야 될 거야. 그리고 거기서 \"이중성(duality)\" 이라는 것을 만나게 돼.",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "하지만 이에 대해 알아보기 전에 다차원에서 수직선인 1차원으로의 선형 변환에 대해 이야기하는 데 시간을 좀 할애해야 합니다.",
  "model": "google_nmt",
  "from_community_srt": "근데, 바로 설명 들어가기 전에, 선형변환에 대해 먼저 얘기해야할 게 있어. 다차원에서 1차원으로의 선형변환에 관한 거야. 즉,",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "이는 2D 벡터를 받아들이고 일부 숫자를 뱉어내는 함수이지만 선형 변환은 물론 2D 입력 및 1D 출력을 사용하는 일반적인 함수보다 훨씬 더 제한됩니다.",
  "model": "google_nmt",
  "from_community_srt": "결과 차원이 그냥 1차원 수선이 되는 변환이지. 이 변환들은 일종의 함수로서, 2차원 벡터를 입력받아서 숫자 하나를 내놓고 있어. 물론, 선형변환은 2차원 입력에서 1차원 출력으로 가는 다른 흔한 함수들보다는 훨씬 제한적이야.",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "3장에서 이야기한 것과 같은 더 높은 차원의 변환과 마찬가지로 이러한 함수를 선형으로 만드는 몇 가지 형식적 속성이 있지만 여기서는 최종 목표에 방해가 되지 않도록 의도적으로 이러한 속성을 무시하겠습니다. 모든 형식적인 내용과 동등한 특정 시각적 속성에 중점을 둡니다.",
  "model": "google_nmt",
  "from_community_srt": "더 높은 차원으로 가는 변환도 마찬가지로, 챕터 3에서 내가 말했던 것이기도 한데, 이 함수가 선형(linear)이 되려면 어떤 공식적인 속성이 있어야만 하지. (역주: 평행, 균등간격, 원점고정) 하지만 여기서는 자세한 것들을 무시할게. 이런 것들은 주의만 산만해질 뿐이야. 대신 모든 공식적 속성을 동등한 시각적 속성을 통해 살펴보자.",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "균일한 간격의 점선을 선택하고 변환을 적용하면 선형 변환은 해당 점들이 수직선인 출력 공간에 도달한 후 균일한 간격을 유지합니다.",
  "model": "google_nmt",
  "from_community_srt": "선에 같은 간격으로 점이 있고, 이 선에 변환을 적용하면, 선형변환이라면 점들사이 균등간격이 유지될거야. 결과 공간, 즉 수선(number line) 의 점들로 바뀐 이후에도 그렇겠지.",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "그렇지 않고 간격이 고르지 않은 점선이 있으면 변환이 선형이 아닙니다.",
  "model": "google_nmt",
  "from_community_srt": "반면에, 선의 점 간격이 균등하지 않게 바뀌면, 이런 변환은 비선형적(not linear) 이야.",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "이전에 본 사례와 마찬가지로 이러한 선형 변환 중 하나는 i-hat 및 j-hat이 사용되는 위치에 따라 완전히 결정되지만 이번에는 해당 기본 벡터 각각이 숫자에 도달하므로 위치를 기록할 때 그들은 행렬의 열로 착륙하며, 각 열에는 단일 숫자만 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "우리가 앞서 봤던 예제들 처럼, 선형변환은 i-hat 과 j-hat 의 도착위치에 의해 완전히 결정돼. 하지만 이번에는, 기저벡터들의 도착지가 수선의 숫자일뿐이지. 기저벡터의 도착지가 행렬의 열이기 때문에 각 열은 하나의 숫자로만 이루어지게 돼.",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "이것은 1x2 행렬입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "이러한 변환 중 하나를 벡터에 적용한다는 것이 무엇을 의미하는지 예를 통해 살펴보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "이것이 1 x 2 행렬이야. 예를 통해서 좀 더 살펴보자.",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "i-hat을 1로, j-hat을 -2로 변환하는 선형 변환이 있다고 가정해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 변환(행렬)을 벡터에 적용한다는 것은 무슨 의미일까? 어떤 선형변환이 있다고 해보자. 변환후 i-hat 은 1, j-hat 은 -2 인 경우야.",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "좌표가 있는 벡터(가령 4, 3)가 끝나는 위치를 추적하려면 이 벡터를 i-hat의 4배 + j-hat의 3배로 나누는 것을 생각해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "좌표값 [4, 3]인 벡터가 어디로 이동하는지 알기 위해서는 벡터를 4 * i-hat + 3 * j-hat 으로 분해해서 생각해야해.",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "선형성의 결과로 변환 후 벡터는 i-hat이 착지한 위치의 4배인 1에 더해 j-hat이 착지한 장소의 3배인 -2가 됩니다. 이 경우 이는 음수에 착지했음을 의미합니다. 2.",
  "model": "google_nmt",
  "from_community_srt": "선형성(linearity)에 따르면, 변환 후에는 같은 비율이여야 하기 때문에 변환된 i-hat 의 4배에 변환된 j-hat 의 3배를 더한것과 같아. 이 경우 결과적으로 -2 에 도착하지.",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "이 계산을 순전히 수치적으로 수행하면 행렬 벡터 곱셈이 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "이 계산을 순전히 수치적으로 보면,",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "이제 1x2 행렬에 벡터를 곱하는 수치 연산은 두 벡터의 내적을 구하는 것과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "이게바로 행렬-벡터 곱셈이 되지. 이제, 이 1x2 행렬에 벡터를 곱하는 수치연산은 두 벡터의 내적(dot product)과 똑같게 느껴질거야.",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "저 1x2 행렬은 우리가 옆으로 기울인 벡터처럼 보이지 않나요?",
  "model": "google_nmt",
  "from_community_srt": "1x2 행렬이 벡터를 그냥 옆으로 뉘여놓은 것 같지 않아? 사실,",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "사실, 우리는 지금 당장 1x2 행렬과 2D 벡터 사이에 좋은 연관성이 있다고 말할 수 있습니다. 이는 벡터의 수치 표현을 옆으로 기울여 관련 행렬을 얻거나 행렬을 위로 기울여 관련 벡터를 얻는 방식으로 정의됩니다. .",
  "model": "google_nmt",
  "from_community_srt": "이제는 말할때가 된 것 같아. 1x2 행렬과 2차원 벡터사이에는 멋진 관련성이 있어. 벡터의 숫자 표현을 옆으로 기울여서 연관 행렬을 얻거나 또는 행렬을 세워서 연관 벡터를 얻거나 하는 관련성이지.",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "지금은 수치 표현식만 보고 있기 때문에 벡터와 1x2 행렬 사이를 오가는 것이 어리석은 일처럼 느껴질 수도 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "당장 수치 표현만을 살펴볼것이기 때문에 벡터와 1 × 2 행렬 사이를 바꾸는 것이 뭔가 단순한 바보 짓처럼 느껴질수도 있어.",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "그러나 이것은 기하학적 관점에서 볼 때 정말 놀라운 것을 시사합니다.",
  "model": "google_nmt",
  "from_community_srt": "하지만, 이건 기하학적 관점에서 보면 뭔가 멋진 어떤 것을 제공해줘.",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "벡터를 숫자로 변환하는 선형 변환과 벡터 자체 사이에는 일종의 연관성이 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "어떤 연결성 같은 건데, 입력이 벡터고 출력이 숫자인 선형변환과 벡터 그 자신과의 관계야.",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "그 의미를 명확하게 하고 이전의 내적 퍼즐에 답하는 예를 보여 드리겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "그 중요성을 명확히 보여주는 예를 하나 들어볼게. 그리고 앞에나온 내적 문제에 관한 답변과도 관련있어.",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "배운 내용을 잊어버리고 내적이 투영과 관련되어 있다는 사실을 아직 모른다고 상상해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "기존에 배웠던 것들은 잊어버리고 내적과 투영이 관계있다는 것을 아직 모른다고 생각하고 들어봐.",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "여기서 제가 하려는 일은 수직선의 복사본을 가져와 어떻게든 공간에 대각선으로 배치하고 숫자 0을 원점에 두는 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "난 여기 수선의 복사본을 만들어서 대각선 방향으로 비스듬히 놓을 건데, 숫자 0 이 원점과 겹치게 둘거야.",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "이제 숫자의 숫자 1이 있는 곳에 팁이 있는 2차원 단위 벡터를 생각해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "이제 2차원 형태의 수선의 단위벡터를 떠올려봐. 수선의 숫자 1을 가리키고 있을거야.",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "그 사람에게 이름을 지어주고 싶어요. u-hat.",
  "model": "google_nmt",
  "from_community_srt": "이 벡터에 u-hat 이라고 이름을 붙일거야.",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "이 작은 사람은 앞으로 일어날 일에서 중요한 역할을 하므로 그를 마음 속에 간직하십시오.",
  "model": "google_nmt",
  "from_community_srt": "이 작은 벡터가 앞으로 일어날 일에서 중요한 역할을 할거야. 마음속에 잘 기억해둬.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "2차원 벡터를 이 대각선 수직선에 직접 투영하면 사실상 2차원 벡터를 숫자로 변환하는 함수를 정의한 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "2차원 벡터를 이 대각선방향의 수선에 투영(project) 하게 되면 이건 사실상, 우리가 2차원 벡터를 입력받아 숫자를 내놓는 함수를 정의한게 돼.",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "게다가 이 함수는 실제로 선형입니다. 왜냐하면 균일하게 간격을 둔 점의 선이 일단 수직선에 도달하면 균일한 간격으로 유지된다는 시각적 테스트를 통과했기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "무엇보다도, 이 함수는 진짜 선형적이야. 눈으로 보면 일단 맞아. 균등 간격의 점을 가진 선이라면 수선으로 투사한 이후에도 균등 간격이야.",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "분명히 말하자면, 이와 같이 2차원 공간에 수직선을 삽입했더라도 함수의 출력은 2차원 벡터가 아니라 숫자입니다.",
  "model": "google_nmt",
  "from_community_srt": "다만 이해를 도우려고, 2차원 공간에 수선을 넣어놨을 뿐, 이 함수의 출력은 (수선의) 숫자이지, 2차원 평면의 벡터로 보면 안돼.",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "두 개의 좌표를 받아 하나의 좌표를 출력하는 함수를 생각해야 합니다.",
  "model": "google_nmt",
  "from_community_srt": "이 함수를 마치 두개의 좌표값을 입력받아 하나의 좌표값을 출력하는 것처럼 생각할 수도 있어.",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "하지만 그 벡터 u-hat은 입력 공간에 존재하는 2차원 벡터입니다.",
  "model": "google_nmt",
  "from_community_srt": "근데 벡터 u-hat 은 2차원 벡터야. 입력과 같은 공간에 존재하지.",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "그것은 단지 수직선의 삽입과 겹치는 방식으로 위치할 뿐입니다.",
  "model": "google_nmt",
  "from_community_srt": "단지 삽입된 수선과 겹쳐진 상황이지.",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "이 투영을 통해 우리는 2차원 벡터에서 숫자로의 선형 변환을 정의했으므로 해당 변환을 설명하는 일종의 1x2 행렬을 찾을 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이렇게 투영을 통해, 우리는 2차원벡터에서 숫자로 가는 선형변환 하나를 정의했어. 이 변환은 1x2 행렬로 나타낼 수 있지.",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "1x2 행렬을 찾기 위해 이 대각선 숫자선 설정을 확대하고 i-hat과 j-hat이 각각 어디에 착지하는지 생각해 봅시다. 착지 지점이 행렬의 열이 될 것이기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "이 1 × 2 행렬의 값을 알아내기 위해서, 아까 그려놓은 수선을 확대해서 i-hat 과 j-hat 이 어디로 움직이는 살펴보자. 기저벡터의 도착지가 행렬의 열인 것은 이미 알거야.",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "이 부분은 정말 멋지네요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "우리는 정말 우아한 대칭 조각으로 그것을 추론할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "이 부분이 굉장히 멋진 부분인데, 우리는 진짜 우아한 대칭성을 사용해서 알아낼 수 있어.",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "i-hat과 u-hat은 모두 단위 벡터이므로 u-hat을 통과하는 선에 i-hat을 투영하는 것은 u-hat을 x축에 투영하는 것과 완전히 대칭으로 보입니다.",
  "model": "google_nmt",
  "from_community_srt": "i-hat 과 u-hat 모두 단위벡터(길이 1)여서 i-hat 을 u-hat 을 통과하는 선으로 투영하는 것은 u-hat 을 x 축(i-hat) 에 투영하는 것과 완전히 대칭이야. (역자:",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "따라서 i-hat이 투영될 때 어떤 숫자에 착지하는지 묻는다면 대답은 x축에 투영될 때 u-hat이 착지하는 숫자와 동일할 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "앞의 내적 설명에서 나왔죠.) 그래서 i-hat 이 투영 위치를 구할때 u-hat 을 x 축(i-hat) 에 투영된 위치를 구하는 것과 똑같아.",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "그러나 u-hat을 x축에 투영한다는 것은 u-hat의 x 좌표를 취하는 것을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "그런데 u-hat 을 x 축에 투영하는 것은 그냥 u-hat 의 x 좌표값을 구하는 거랑 같아.",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "따라서 대칭에 따라 i-hat이 대각선 수직선에 투영될 때 착지하는 숫자는 u-hat의 x 좌표가 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "그래서, 대칭성에 의해, i-hat 에서 수선으로 투영후 위치는 u-hat 의 x 좌표값이 될거야.",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "멋지지 않나요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "j-hat 사건의 추론은 거의 동일합니다.",
  "model": "google_nmt",
  "from_community_srt": "멋지지 않아? j-hat 의 경우에도 완전이 같아.",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "잠시 생각해 보십시오.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "같은 이유로 u-hat의 y좌표는 수직선 사본에 투영될 때 j-hat이 도달하는 숫자를 제공합니다.",
  "model": "google_nmt",
  "from_community_srt": "잠깐만 생각하면 나올거야. 같은 이유로, u-hat 의 y 좌표값은 j-hat 을 수선으로 투영한 위치와 같을거야.",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "잠시 멈춰서 생각해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "잠깐 여기서 잠시 생각해봐야 돼.",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "정말 멋지다고 생각해요.",
  "model": "google_nmt",
  "from_community_srt": "난 이부분이 정말로 굉장한 것 같아.",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "따라서 투영 변환을 설명하는 1x2 행렬의 항목은 u-hat의 좌표가 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "그리고 해당 행렬에 해당 벡터를 곱해야 하는 공간의 임의 벡터에 대한 이 투영 변환을 계산하는 것은 u-hat과 내적을 구하는 것과 계산상 동일합니다.",
  "model": "google_nmt",
  "from_community_srt": "투영 변환을 나타내는 1x2 행렬은 그냥 u-hat 의 좌표가 되는거야.(!) 임의 벡터의 투영은 이 행렬에 임의벡터를 곱하는 것이고, 이건 계산적으로 u-hat 과의 내적(dot product)과 똑같아.",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "이것이 단위 벡터와 내적을 취하는 것이 벡터를 해당 단위 벡터의 범위에 투영하고 길이를 취하는 것으로 해석될 수 있는 이유입니다.",
  "model": "google_nmt",
  "from_community_srt": "이것이 어째서 단위벡터와의 내적과 , 벡터를 단위벡터로 투영한 길이를 구하는 것과 같아지는지를 설명해줘.",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "그렇다면 단위가 아닌 벡터는 어떨까요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "예를 들어, 단위 벡터 u-hat을 사용하고 이를 3배로 확장한다고 가정해 보겠습니다.",
  "model": "google_nmt",
  "from_community_srt": "그럼 비-단위(non-unit) 벡터의 경우는 어떨까? 예를 들어, u-hat 벡터를 가져다가 3배로 \"스케일\" 해보는 거야.",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "수치적으로 각 구성 요소에는 3이 곱해집니다.",
  "model": "google_nmt",
  "from_community_srt": "수치적으로는, 각 구성요소가 3배가 될거야.",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "따라서 해당 벡터와 관련된 행렬을 보면 i-hat과 j-hat이 이전에 도달한 값의 3배가 됩니다.",
  "model": "google_nmt",
  "from_community_srt": "그리고 그 벡터와 연관된 행렬을 찾으려면 이전에 i-hat 과 j-hat 의 변환된 위치에 3배를 하면 돼.",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "이것은 모두 선형이기 때문에 더 일반적으로 새로운 행렬은 모든 벡터를 수직선 복사본에 투영하고 해당 위치에 3을 곱하는 것으로 해석될 수 있음을 의미합니다.",
  "model": "google_nmt",
  "from_community_srt": "모두 선형(linear) 이기 때문이야. 이것이 암시하는 것은, 새로운 행렬을 해석하기를, 어떤 벡터를 수선(number line) 원본 (3배하기 전)에 투영한다음 투영 후 위치에서 3을 곱한거야.",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "이것이 단위가 아닌 벡터의 내적이 먼저 해당 벡터에 투영된 다음 해당 투영의 길이를 벡터의 길이로 확장하는 것으로 해석될 수 있는 이유입니다.",
  "model": "google_nmt",
  "from_community_srt": "이것이 왜 비-단위(non-unit) 벡터의 내적(dot product)을 해석할때, 그 벡터 위로 투영한 후, 벡터의 길이만큼 투사체 길이를 늘리는 것이라고 보는 이유야.",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "여기서 무슨 일이 일어났는지 잠시 생각해 보세요.",
  "model": "google_nmt",
  "from_community_srt": "잠시 멈춰서 이게 무슨 말인지 생각해보자.",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "우리는 2D 공간에서 수직선으로의 선형 변환을 수행했는데, 이는 수치 벡터나 수치 내적의 관점에서 정의되지 않고 수직선의 대각선 복사본에 공간을 투영하여 정의되었습니다.",
  "model": "google_nmt",
  "from_community_srt": "우리는 2차원 공간에서 1차원 수선으로 가는 선형변환을 가지고 있었어. 수치적 벡터나 수치적 내적이라는 용어로 정의하기 전이였지. 단지 대각방향의 수선 원본에 투영만을 정의했을 뿐이야.",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "그러나 변환이 선형이기 때문에 필연적으로 일부 1x2 행렬로 설명되었습니다.",
  "model": "google_nmt",
  "from_community_srt": "하지만, 변환이 선형적(linear) 이기 때문에 이건 필연적으로 1x2 행렬로 표현할 수 있었어. (역자:",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "그리고 1x2 행렬에 2D 벡터를 곱하는 것은 해당 행렬을 옆으로 돌려 내적을 취하는 것과 같기 때문에 이 변환은 불가피하게 일부 2D 벡터와 관련이 있었습니다.",
  "model": "google_nmt",
  "from_community_srt": "선형변환은 모두 행렬로 표현할 수 있는 듯) 그리고 1x2 행렬에 2차원 벡터를 곱하는 것은 그 행렬을 옆으로 눕혀서 내적을 구한 것과 같기 때문에 이런 변환은 불가피하게 2차원 벡터와 관련이 있을 수밖에 없어.",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "여기서 얻을 수 있는 교훈은 출력 공간이 수직선인 이러한 선형 변환 중 하나가 있을 때마다 그것이 어떻게 정의되었는지에 관계없이 변환을 적용한다는 의미에서 해당 변환에 해당하는 고유한 벡터 v가 있을 것이라는 점입니다. 해당 벡터로 내적을 취하는 것과 같습니다.",
  "model": "google_nmt",
  "from_community_srt": "여기서 배울 것은, 언제든 이런 선형 변환 중 하나를 가지고 있다면, 즉, 결과 공간이 수선(number line)인 선형변환을 가지고 있다면, 어떻게 정의하든지 간에, 어떤 유일한 벡터가 그 변환에 대응되고 있을거야. 변환의 적용은 벡터의 내적을 구하는 것과 같음을 알 수 있어.",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "나에게 이것은 정말 아름답습니다.",
  "model": "google_nmt",
  "from_community_srt": "나에게 있어, 이건 정말 아름답게 느껴져.",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "이것은 수학에서 이중성이라고 불리는 것의 예입니다.",
  "model": "google_nmt",
  "from_community_srt": "이것이 수학에서 말하는 \"이중성(duality)\" 에 관한 경우야.",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "이중성은 수학 전반에 걸쳐 다양한 방식과 형태로 나타나며 실제로 정의하는 것은 매우 까다롭습니다.",
  "model": "google_nmt",
  "from_community_srt": "\"이중성\" 은 수학 전반을 통해 많은 방법과 형태로 등장해. 그리고 실제로 정의 내용은 굉장히 까다로워.",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "느슨하게 말하면, 두 가지 유형의 수학적 사물 사이에 자연스러우면서도 놀라운 대응 관계가 있는 상황을 말합니다.",
  "model": "google_nmt",
  "from_community_srt": "쉽게 말하자면, 자연스러우면서도 놀라운 대응관계가 두 개의 수학적 대상물 사이에서 나타나는 거야.",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "방금 배운 선형 대수학의 경우, 벡터의 쌍대(dual)는 그것이 인코딩하는 선형 변환이고, 일부 공간에서 한 차원으로의 선형 변환의 쌍대(dual)는 해당 공간의 특정 벡터라고 말할 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "방금 배운 선형 대수의 경우에는, 벡터에서 \"이중(dual)\"이라는 것은 그 벡터가 가진 선형변한 성질을 말해. 그리고 1차원으로 변환시키는 선형변환에서 이중(dual)이란, 공간상의 특정 벡터를 말해.",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "요약하자면, 내적은 투영을 이해하고 벡터가 같은 방향을 가리키는 경향이 있는지 여부를 테스트하는 데 매우 유용한 기하학적 도구입니다.",
  "model": "google_nmt",
  "from_community_srt": "요약하면, 내적은 투영을 이해하는데 매우 유용한 기하학적 도구야. 벡터가 같은 방향을 가리키는지를 알아내는데도 유용한 도구지.",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "그리고 그것은 아마도 내적에 관해 여러분이 기억해야 할 가장 중요한 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "이것이 아마도 너가 내적에서 기억해야 할 가장 중요한 점일 거야.",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "그러나 더 깊은 수준에서 두 벡터를 함께 점으로 찍는 것은 그 중 하나를 변환의 세계로 변환하는 방법입니다.",
  "model": "google_nmt",
  "from_community_srt": "좀 더 깊게 보자면, 두 벡터를 함께 내적하는 것(dotting)은 두 벡터 중 하나를 변환인자로 보는 거야.",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "다시 말하지만, 이는 수치적으로 강조하기에는 어리석은 점처럼 느껴질 수 있습니다.",
  "model": "google_nmt",
  "from_community_srt": "다시말해, 수치적으로 볼 때,",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "유사해 보이는 것은 단지 두 가지 계산일 뿐입니다.",
  "model": "google_nmt",
  "from_community_srt": "이건 강조하는게 좀 웃길수도 있는데, 두 계산이 유사하다는 거야.",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "하지만 제가 이것이 중요하다고 생각하는 이유는 수학 전반에 걸쳐 벡터를 다룰 때 벡터의 성격을 실제로 알게 되면 때로는 벡터를 공간의 화살표가 아니라 벡터로 이해하는 것이 더 쉽다는 것을 깨닫기 때문입니다. 선형 변환의 물리적 구현.",
  "model": "google_nmt",
  "from_community_srt": "하지만 이것이 중요한 이유는, 수학 전반에 걸쳐서, 너가 벡터로 다룰때라든지 할때, 너가 제대로 이해했다면, 언젠가 느껴질때가 있을거야. 공간속의 화살표보다는 선형변환의 물리적 실체 이해하는 편이 더 쉬울때가 있을거야.",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "이는 마치 벡터가 실제로 특정 변환에 대한 개념적 약칭인 것처럼 보입니다. 공간 전체를 수직선으로 이동하는 것보다 공간의 화살표를 생각하는 것이 더 쉽기 때문입니다.",
  "model": "google_nmt",
  "from_community_srt": "벡터를 마치 어떤 변환의 개념적 단축 표현으로 생각해. 우리는 공간상의 화살표를 생각하는 것이 공간에서 수선으로 투영을 생각하는 것보다 쉽기 때문이야.",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.97
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "다음 비디오에서는 교차곱에 대해 이야기하면서 이 이중성이 실제로 작동하는 또 다른 정말 멋진 예를 보게 될 것입니다.",
  "model": "google_nmt",
  "from_community_srt": "다음 동영상에서는 \"이중성(duality)\" 의 또다른 멋진 예제를 보여줄거야.",
  "n_reviews": 0,
  "start": 822.61,
  "end": 829.19
 }
]