[
 {
  "input": "Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[موسیقی] به‌طور سنتی، محصولات نقطه‌ای چیزی هستند که در اوایل دوره جبر خطی، معمولاً در ابتدا معرفی می‌شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "بنابراین ممکن است عجیب به نظر برسد که من آنها را تا این حد در سریال عقب انداخته ام. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "من این کار را به این دلیل انجام دادم که یک روش استاندارد برای معرفی موضوع وجود دارد که به چیزی بیش از درک پایه از بردارها نیاز ندارد، اما درک کامل‌تر از نقشی که محصولات نقطه‌ای در ریاضی بازی می‌کنند را می‌توان تنها در زیر نور تبدیل‌های خطی یافت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "با این حال، قبل از آن، اجازه دهید به طور خلاصه به روش استاندارد معرفی محصولات نقطه‌ای بپردازم، که فرض می‌کنم حداقل تا حدی برای تعدادی از بینندگان بررسی می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "از نظر عددی، اگر دو بردار با ابعاد یکسان، دو لیست از اعداد با طول های یکسان داشته باشید، در نظر گرفتن حاصل ضرب نقطه ای آنها به معنای جفت کردن همه مختصات، ضرب آن جفت ها در یکدیگر و جمع کردن نتیجه است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "بنابراین بردار 1، 2 نقطه گذاری شده با 3، 4، 1 ضربدر 3 به علاوه 2 ضربدر 4 خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "بردار 6، 2، 8، 3 که با 1، 8، 5، 3 نقطه چین شده است، 6 ضرب در 1 به علاوه 2 ضربدر 8 به علاوه 8 ضربدر 5 به علاوه 3 ضربدر 3 خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "خوشبختانه، این محاسبات تفسیر هندسی بسیار خوبی دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "برای فکر کردن در مورد حاصل ضرب نقطه بین دو بردار، v و w، تصور کنید که w را بر روی خطی که از مبدا و نوک v می گذرد، نمایش دهید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "با ضرب طول این طرح در طول v، حاصل ضرب نقطه ای v نقطه w را خواهید داشت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "به جز زمانی که این طرح w در جهت مخالف v باشد، آن حاصلضرب نقطه در واقع منفی خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "بنابراین وقتی دو بردار به طور کلی در یک جهت هستند، حاصلضرب نقطه آنها مثبت است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "هنگامی که آنها عمود هستند، به این معنی که طرح یکی بر دیگری بردار صفر است، حاصل ضرب نقطه آنها صفر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "و اگر آنها به طور کلی در جهت مخالف اشاره کنند، حاصلضرب نقطه آنها منفی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "اکنون، این تعبیر به طرز عجیبی نامتقارن است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "با دو بردار بسیار متفاوت رفتار می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "بنابراین وقتی برای اولین بار این را یاد گرفتم، از اینکه ترتیب مهم نیست تعجب کردم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "در عوض می‌توانید v را روی w قرار دهید، طول v پیش‌بینی‌شده را در طول w ضرب کنید و همان نتیجه را بگیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "منظورم این است که آیا این فرآیند واقعاً متفاوتی به نظر نمی رسد؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "در اینجا شهودی وجود دارد که چرا نظم مهم نیست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "اگر طول v و w یکسان بود، می‌توانیم از تقارن استفاده کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "از آنجایی که پرتاب کردن w بر روی v، سپس ضرب طول آن برجستگی در طول v، یک تصویر آینه‌ای کامل از پرتاب کردن v بر روی w است، سپس طول آن برآمدگی را در طول w ضرب می‌کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "حال، اگر یکی از آنها، مثلاً v را با مقداری ثابت مانند 2 مقیاس کنید، به طوری که طول آنها مساوی نباشد، تقارن شکسته می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "اما بیایید به نحوه تفسیر نقطه بین این بردار جدید، 2 برابر v و w فکر کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "اگر فکر می‌کنید که w روی v پیش‌بینی می‌شود، آنگاه حاصل ضرب نقطه‌ای 2v نقطه w دقیقاً دو برابر حاصلضرب نقطه‌ای v نقطه w خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "این به این دلیل است که وقتی v را با 2 مقیاس می‌دهید، طول طرح w را تغییر نمی‌دهد، اما طول برداری را که روی آن پرتاب می‌کنید دو برابر می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "اما از سوی دیگر، فرض کنید شما به این فکر می‌کردید که v روی w پیش‌بینی شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "خوب، در این حالت، طول طرح چیزی است که وقتی v را در 2 ضرب می‌کنیم، مقیاس می‌شود، اما طول برداری که شما روی آن نمایش می‌دهید ثابت می‌ماند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "بنابراین اثر کلی هنوز دوبرابر کردن محصول نقطه است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "بنابراین حتی اگر تقارن در این مورد شکسته شود، تأثیری که این مقیاس‌گذاری بر مقدار حاصلضرب نقطه‌ای دارد، در هر دو تفسیر یکسان است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "همچنین یک سوال بزرگ دیگر وجود دارد که وقتی برای اولین بار این چیزها را یاد گرفتم، من را گیج کرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "چرا روی زمین این فرآیند عددی تطبیق مختصات، ضرب جفت ها و جمع آنها با هم ربطی به فرافکنی دارد؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "خوب، برای دادن یک پاسخ رضایت‌بخش، و همچنین عدالت کامل در مورد اهمیت محصول نقطه‌ای، باید چیزی کمی عمیق‌تر در اینجا کشف کنیم، که اغلب به نام دوگانگی شناخته می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "اما قبل از پرداختن به آن، باید مدتی را صرف صحبت در مورد تبدیل‌های خطی از چند بعد به یک بعد کنم که فقط خط عددی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "اینها توابعی هستند که یک بردار 2 بعدی را می گیرند و مقداری را بیرون می اندازند، اما تبدیل های خطی البته بسیار محدودتر از تابع معمولی شما با ورودی 2 بعدی و خروجی 1 بعدی هستند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "مانند تغییرات در ابعاد بالاتر، مانند مواردی که در فصل 3 در مورد آنها صحبت کردم، برخی از ویژگی های رسمی وجود دارد که این توابع را خطی می کند، اما من به طور هدفمند آنها را نادیده می گیرم تا از هدف نهایی خود منحرف نشویم، و در عوض روی یک ویژگی بصری خاص تمرکز کنید که معادل همه چیزهای رسمی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "اگر خطی از نقاط با فواصل مساوی را انتخاب کنید و تبدیلی را اعمال کنید، یک تبدیل خطی زمانی که در فضای خروجی که همان خط اعداد است، آن نقاط را در فاصله مساوی نگه می دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "در غیر این صورت، اگر خطی از نقاط وجود داشته باشد که به طور ناموزون فاصله داشته باشد، تبدیل شما خطی نیست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "مانند مواردی که قبلاً دیده‌ایم، یکی از این تبدیل‌های خطی کاملاً توسط i-hat و j-hat تعیین می‌شود، اما این بار هر یک از آن بردارهای پایه فقط بر روی یک عدد قرار می‌گیرند، بنابراین وقتی ثبت می‌کنیم کجا آنها به عنوان ستون های یک ماتریس قرار می گیرند، هر یک از آن ستون ها فقط یک عدد دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "این یک ماتریس 1x2 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "بیایید از طریق مثالی از معنای اعمال یکی از این تبدیل ها در یک بردار را مرور کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "فرض کنید یک تبدیل خطی دارید که i-hat را به 1 و j-hat را به منفی 2 می رساند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "برای دنبال کردن جایی که یک بردار با مختصات، مثلاً 4، 3 به پایان می رسد، در نظر بگیرید که این بردار را به صورت 4 برابر i-hat به اضافه 3 برابر j-hat تجزیه کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "نتیجه خطی بودن این است که پس از تبدیل، بردار 4 برابر مکان فرود i-hat، 1، به علاوه 3 برابر مکان فرود j-hat، منفی 2 خواهد بود، که در این مورد نشان می دهد که بر روی منفی قرار می گیرد. 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "وقتی این محاسبه را صرفاً به صورت عددی انجام می دهید، ضرب برداری ماتریسی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "اکنون، این عملیات عددی ضرب یک ماتریس 1x2 در یک بردار، درست مانند گرفتن حاصل ضرب نقطه‌ای دو بردار است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "آیا آن ماتریس 1x2 فقط شبیه برداری نیست که به سمت آن منحرف شده ایم؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "در واقع، همین الان می‌توانیم بگوییم که ارتباط خوبی بین ماتریس‌های 1x2 و بردارهای 2 بعدی وجود دارد، که با کج کردن نمایش عددی یک بردار در سمت آن برای به دست آوردن ماتریس مرتبط، یا بازگرداندن ماتریس به سمت بالا برای دریافت بردار مرتبط تعریف می‌شود. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "از آنجایی که در حال حاضر فقط به عبارات عددی نگاه می کنیم، رفت و برگشت بین بردارها و ماتریس های 1x2 ممکن است کار احمقانه ای به نظر برسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "اما این چیزی را نشان می دهد که از نظر هندسی واقعاً عالی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "نوعی ارتباط بین تبدیل های خطی وجود دارد که بردارها را به اعداد و خود بردارها می برد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "اجازه دهید مثالی را نشان دهم که اهمیت را روشن می کند و اتفاقاً به معمای محصول نقطه ای قبلی نیز پاسخ می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "چیزهایی را که یاد گرفته‌اید فراموش نکنید و تصور کنید که از قبل نمی‌دانید که محصول نقطه‌ای به فرافکنی مربوط می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "کاری که من در اینجا انجام می‌دهم این است که یک کپی از خط اعداد را بگیرم و آن را به شکلی مورب در فضا قرار دهیم و عدد 0 در مبدا قرار گیرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "حال به بردار واحد دوبعدی فکر کنید که نوک آن در جایی است که عدد 1 روی خط اعداد قرار دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "من می خواهم به آن مرد یک نام بگذارم، یو-هت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "این پسر کوچک نقش مهمی در اتفاقی که قرار است بیفتد بازی می کند، پس فقط او را در پشت ذهن خود نگه دارید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "اگر بردارهای دوبعدی را مستقیماً روی این خط اعداد مورب طراحی کنیم، در واقع تابعی را تعریف کرده‌ایم که بردارهای دو بعدی را به اعداد می‌برد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "علاوه بر این، این تابع در واقع خطی است، زیرا از آزمون بصری ما عبور می کند که هر خطی از نقاط با فواصل مساوی پس از قرار گرفتن روی خط اعداد به طور مساوی باقی می ماند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "فقط برای واضح بودن، حتی اگر من خط اعداد را در فضای دو بعدی به این شکل جاسازی کرده ام، خروجی های تابع اعداد هستند، نه بردارهای دو بعدی. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "شما باید تابعی را در نظر بگیرید که دو مختصات را بگیرد و یک مختصات را خروجی دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "اما آن بردار U-hat یک بردار دو بعدی است که در فضای ورودی زندگی می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "فقط به گونه ای قرار گرفته است که با جاسازی خط عدد همپوشانی دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "با این طرح، ما فقط یک تبدیل خطی از بردارهای دو بعدی به اعداد تعریف کردیم، بنابراین می‌توانیم نوعی ماتریس 1x2 را پیدا کنیم که آن تبدیل را توصیف می‌کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "برای یافتن آن ماتریس 1x2، بیایید روی این تنظیم خط اعداد مورب زوم کنیم و به این فکر کنیم که I-hat و J-hat هر کدام کجا فرود می آیند، زیرا آن نقاط فرود ستون های ماتریس خواهند بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "این قسمت فوق العاده باحاله ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "ما می توانیم از طریق آن با یک قطعه تقارن واقعاً ظریف استدلال کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "از آنجایی که I-hat و U-hat هر دو بردار واحد هستند، نمایش I-hat بر روی خطی که از U-hat می گذرد کاملاً متقارن به نظر می رسد که U-hat روی محور x قرار دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "بنابراین وقتی می‌پرسیم که I-hat روی چه عددی فرود می‌آید، پاسخ همان خواهد بود که هر عدد U-hat وقتی روی محور x فرود می‌آید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "اما پرتاب کردن U-hat بر روی محور x فقط به معنای گرفتن مختصات x از U-hat است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "بنابراین با تقارن، عددی که در آن I-hat فرود می آید وقتی روی آن خط عددی مورب پیش بینی می شود، مختصات x از U-hat خواهد بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "این باحال نیست؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "استدلال برای مورد J-hat تقریباً یکسان است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "یک لحظه درباره آن فکر کن. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "به همین دلایل، مختصات y از U-hat عددی را به ما می دهد که در آن جا J-hat بر روی کپی خط عددی نمایش داده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "یک لحظه مکث کنید و در آن فکر کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "من فقط فکر می کنم که واقعا عالی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "بنابراین ورودی های ماتریس 1x2 که تبدیل طرح ریزی را توصیف می کند، مختصات U-hat خواهند بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "و محاسبه این تبدیل پیش‌بینی برای بردارهای دلخواه در فضا، که مستلزم ضرب آن ماتریس در آن بردارها است، از نظر محاسباتی با گرفتن یک حاصل ضرب نقطه‌ای با U-hat یکسان است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "به همین دلیل است که گرفتن حاصلضرب نقطه با بردار واحد را می توان به عنوان طرح ریزی یک بردار بر روی دهانه آن بردار واحد و گرفتن طول تفسیر کرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "پس بردارهای غیر واحدی چطور؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "برای مثال، فرض کنید آن بردار واحد U-hat را می گیریم، اما آن را با ضریب 3 بزرگ می کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "از نظر عددی هر یک از اجزای آن در 3 ضرب می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "بنابراین با نگاهی به ماتریس مرتبط با آن بردار، مقادیر I-hat و J-hat سه برابر مقادیری هستند که قبلاً در آن قرار داشتند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "از آنجایی که این همه خطی است، به طور کلی‌تر نشان می‌دهد که ماتریس جدید را می‌توان به گونه‌ای تفسیر کرد که هر بردار را بر روی کپی خط عددی نمایش می‌دهد و در جایی که فرود می‌آید در 3 ضرب می‌کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "به همین دلیل است که حاصلضرب نقطه‌ای با بردار غیر واحدی را می‌توان به این صورت تفسیر کرد که ابتدا بر روی آن بردار قرار می‌گیرد، سپس طول آن برجستگی را با طول بردار افزایش می‌دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "لحظه ای به آنچه اینجا اتفاق افتاده فکر کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "ما یک تبدیل خطی از فضای دو بعدی به خط اعداد داشتیم که بر حسب بردارهای عددی یا حاصل ضرب نقطه‌های عددی تعریف نشده بود، فقط با پیش‌بینی فضا بر روی یک کپی مورب از خط عددی تعریف شد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "اما از آنجایی که تبدیل خطی است، لزوماً توسط ماتریس 1x2 توصیف شده است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "و از آنجایی که ضرب یک ماتریس 1x2 در یک بردار دوبعدی مانند چرخاندن آن ماتریس در سمت آن و گرفتن یک ضرب نقطه ای است، این تبدیل به طور اجتناب ناپذیری به برخی از بردارهای دو بعدی مرتبط بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "درسی که در اینجا وجود دارد این است که هر زمان که یکی از این تبدیل های خطی را داشته باشید که فضای خروجی آن خط عددی است، مهم نیست که چگونه تعریف شده است، بردار v منحصر به فرد مربوط به آن تبدیل وجود خواهد داشت، به این معنا که اعمال تبدیل همان چیزی است که یک محصول نقطه ای را با آن بردار بگیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "برای من، این کاملا زیبا است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "این نمونه ای از چیزی در ریاضیات به نام دوگانگی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "دوگانگی به روش‌ها و اشکال مختلف در سراسر ریاضیات ظاهر می‌شود، و واقعاً تعریف کردن آن بسیار دشوار است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "به زبان ساده، به موقعیت هایی اشاره دارد که در آن شما یک مطابقت طبیعی اما شگفت انگیز بین دو نوع چیز ریاضی دارید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "برای مورد جبر خطی که به تازگی در مورد آن یاد گرفتید، می‌توانید بگویید که دوتایی یک بردار تبدیل خطی است که آن را رمزگذاری می‌کند، و دوتایی یک تبدیل خطی از یک فضا به یک بعد، بردار خاصی در آن فضا است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "بنابراین به طور خلاصه، در سطح، حاصلضرب نقطه یک ابزار هندسی بسیار مفید برای درک پیش بینی ها و برای آزمایش اینکه آیا بردارها تمایل دارند به یک جهت اشاره کنند یا خیر است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "و این احتمالاً مهمترین چیزی است که باید در مورد محصول نقطه ای به خاطر بسپارید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "اما در یک سطح عمیق تر، نقطه گذاری دو بردار با هم راهی برای ترجمه یکی از آنها به دنیای تبدیل ها است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "باز هم، از نظر عددی، ممکن است این یک نکته احمقانه برای تأکید به نظر برسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "فقط خیلی محاسباتی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "اما دلیل اینکه من این را بسیار مهم می‌دانم این است که در ریاضیات، وقتی با یک بردار سروکار دارید، وقتی واقعاً شخصیت آن را بشناسید، گاهی اوقات متوجه می‌شوید که درک آن نه به عنوان یک فلش در فضا، بلکه به عنوان یک بردار آسان‌تر است. تجسم فیزیکی یک تبدیل خطی ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space t",
  "translatedText": "انگار که بردار واقعاً فقط یک مختصر مفهومی برای یک تبدیل خاص است، زیرا فکر کردن به فلش‌ها در فضا برای ما راحت‌تر است تا جابجایی تمام آن فضا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.31
 },
 {
  "input": "o the number line. In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "در ویدیوی بعدی، یک نمونه واقعا جالب دیگر از این دوگانگی را در عمل خواهید دید که من در مورد محصول متقابل صحبت می کنم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 820.31,
  "end": 829.19
 }
]