[
 {
  "input": "[\"Ode to Joy\", by Beethoven, plays to the end of the piano.] Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[&quot;Oda a la alegría&quot;, de Beethoven, se toca hasta el final del piano.] Tradicionalmente, los productos escalares son algo que se introduce muy temprano en un curso de álgebra lineal, normalmente desde el principio.",
  "n_reviews": 0
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "Así que puede parecer extraño que los haya hecho retroceder hasta este punto en la serie.",
  "n_reviews": 0
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "Hice esto porque hay una forma estándar de presentar el tema, que no requiere más que una comprensión básica de los vectores, pero una comprensión más completa del papel que desempeñan los productos escalares en matemáticas solo se puede encontrar a la luz de las transformaciones lineales.",
  "n_reviews": 0
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "Antes de eso, sin embargo, permítanme cubrir brevemente la forma estándar en que se presentan los productos punto, que supongo que es al menos parcialmente una revisión para varios espectadores.",
  "n_reviews": 0
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "Numéricamente, si tienes dos vectores de la misma dimensión, dos listas de números con las mismas longitudes, tomar su producto escalar significa emparejar todas las coordenadas, multiplicar esos pares y sumar el resultado.",
  "n_reviews": 0
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "Entonces, el vector 1, 2 punteado con 3, 4 sería 1 por 3 más 2 por 4.",
  "n_reviews": 0
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "El vector 6, 2, 8, 3 punteado con 1, 8, 5, 3 sería 6 por 1 más 2 por 8 más 8 por 5 más 3 por 3.",
  "n_reviews": 0
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "Afortunadamente, este cálculo tiene una interpretación geométrica realmente agradable.",
  "n_reviews": 0
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "Para pensar en el producto escalar entre dos vectores, v y w, imagina proyectar w sobre la línea que pasa por el origen y la punta de v.",
  "n_reviews": 0
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "Multiplicando la longitud de esta proyección por la longitud de v, se obtiene el producto escalar v punto w.",
  "n_reviews": 0
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "Excepto cuando esta proyección de w apunta en la dirección opuesta a v, ese producto escalar en realidad será negativo.",
  "n_reviews": 0
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "Entonces, cuando dos vectores generalmente apuntan en la misma dirección, su producto escalar es positivo.",
  "n_reviews": 0
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "Cuando son perpendiculares, es decir, la proyección de uno sobre el otro es el vector cero, su producto escalar es cero.",
  "n_reviews": 0
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "Y si apuntan generalmente en la dirección opuesta, su producto escalar es negativo.",
  "n_reviews": 0
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "Ahora bien, esta interpretación es extrañamente asimétrica.",
  "n_reviews": 0
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "Trata a los dos vectores de manera muy diferente.",
  "n_reviews": 0
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "Entonces, cuando supe esto por primera vez, me sorprendió que el orden no importara.",
  "n_reviews": 0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "En su lugar, podría proyectar v sobre w, multiplicar la longitud de la v proyectada por la longitud de w y obtener el mismo resultado.",
  "n_reviews": 0
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "Quiero decir, ¿no parece un proceso realmente diferente?",
  "n_reviews": 0
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "Aquí está la intuición de por qué el orden no importa.",
  "n_reviews": 0
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "Si v y w tuvieran la misma longitud, podríamos aprovechar cierta simetría.",
  "n_reviews": 0
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "Dado que proyectar w sobre v, luego multiplicar la longitud de esa proyección por la longitud de v, es una imagen especular completa de proyectar v sobre w, luego multiplicar la longitud de esa proyección por la longitud de w.",
  "n_reviews": 0
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "Ahora, si escalas uno de ellos, digamos v, por alguna constante como 2, de modo que no tengan la misma longitud, la simetría se rompe.",
  "n_reviews": 0
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "Pero pensemos en cómo interpretar el producto escalar entre este nuevo vector, 2 por v, y w.",
  "n_reviews": 0
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "Si piensa que w se proyecta sobre v, entonces el producto escalar 2v punto w será exactamente el doble del producto escalar v punto w.",
  "n_reviews": 0
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "Esto se debe a que cuando escalas v en 2, no cambia la longitud de la proyección de w, pero duplica la longitud del vector sobre el que estás proyectando.",
  "n_reviews": 0
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "Pero, por otro lado, digamos que estás pensando en proyectar v sobre w.",
  "n_reviews": 0
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "Bueno, en ese caso, la longitud de la proyección es lo que se escala cuando multiplicamos v por 2, pero la longitud del vector sobre el que estás proyectando se mantiene constante.",
  "n_reviews": 0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "Entonces, el efecto general sigue siendo simplemente duplicar el producto escalar.",
  "n_reviews": 0
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "Entonces, aunque en este caso se rompe la simetría, el efecto que tiene esta escala sobre el valor del producto escalar es el mismo en ambas interpretaciones.",
  "n_reviews": 0
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "También hay otra gran pregunta que me confundió cuando aprendí esto por primera vez.",
  "n_reviews": 0
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "¿Por qué diablos este proceso numérico de hacer coincidir coordenadas, multiplicar pares y sumarlos tiene algo que ver con la proyección?",
  "n_reviews": 0
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "Bueno, para dar una respuesta satisfactoria, y también para hacer plena justicia a la importancia del producto escalar, necesitamos descubrir algo un poco más profundo que está sucediendo aquí, que a menudo recibe el nombre de dualidad.",
  "n_reviews": 0
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "Pero antes de entrar en eso, necesito dedicar algo de tiempo a hablar sobre transformaciones lineales de múltiples dimensiones a una dimensión, que es solo la recta numérica.",
  "n_reviews": 0
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "Estas son funciones que toman un vector 2D y escupen algún número, pero las transformaciones lineales son, por supuesto, mucho más restringidas que la función común y corriente con una entrada 2D y una salida 1D.",
  "n_reviews": 0
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "Al igual que con las transformaciones en dimensiones superiores, como las que hablé en el capítulo 3, hay algunas propiedades formales que hacen que estas funciones sean lineales, pero voy a ignorarlas aquí a propósito para no distraernos de nuestro objetivo final, y en su lugar centrarse en una determinada propiedad visual que sea equivalente a todo el material formal.",
  "n_reviews": 0
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "Si toma una línea de puntos espaciados uniformemente y aplica una transformación, una transformación lineal mantendrá esos puntos espaciados uniformemente una vez que lleguen al espacio de salida, que es la recta numérica.",
  "n_reviews": 0
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "De lo contrario, si hay alguna línea de puntos que se espacia de manera desigual, entonces su transformación no es lineal.",
  "n_reviews": 0
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "Como en los casos que hemos visto antes, una de estas transformaciones lineales está completamente determinada por dónde toma i-hat y j-hat, pero esta vez cada uno de esos vectores base simplemente aterriza en un número, así que cuando registramos dónde aterrizan como las columnas de una matriz, cada una de esas columnas solo tiene un número.",
  "n_reviews": 0
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "Esta es una matriz de 1x2.",
  "n_reviews": 0
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "Veamos un ejemplo de lo que significa aplicar una de estas transformaciones a un vector.",
  "n_reviews": 0
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "Digamos que tienes una transformación lineal que lleva i-hat a 1 y j-hat a menos 2.",
  "n_reviews": 0
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "Para seguir dónde termina un vector con coordenadas, digamos, 4, 3, piense en dividir este vector como 4 veces i-hat más 3 veces j-hat.",
  "n_reviews": 0
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "Una consecuencia de la linealidad es que después de la transformación, el vector será 4 veces el lugar donde aterriza i-hat, 1, más 3 veces el lugar donde aterriza j-hat, menos 2, lo que en este caso implica que aterriza en negativo 2.",
  "n_reviews": 0
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "Cuando haces este cálculo puramente numérico, es una multiplicación de vectores matriciales.",
  "n_reviews": 0
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "Ahora bien, esta operación numérica de multiplicar una matriz de 1x2 por un vector es como tomar el producto escalar de dos vectores.",
  "n_reviews": 0
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "¿No parece esa matriz de 1x2 simplemente un vector que inclinamos de lado?",
  "n_reviews": 0
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "De hecho, podríamos decir ahora mismo que existe una buena asociación entre matrices 1x2 y vectores 2D, definida inclinando la representación numérica de un vector de lado para obtener la matriz asociada, o inclinando la matriz hacia arriba para obtener el vector asociado. .",
  "n_reviews": 0
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "Dado que ahora solo estamos viendo expresiones numéricas, ir y venir entre vectores y matrices de 1x2 puede parecer una tontería.",
  "n_reviews": 0
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "Pero esto sugiere algo que es realmente asombroso desde el punto de vista geométrico.",
  "n_reviews": 0
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "Existe algún tipo de conexión entre las transformaciones lineales que convierten los vectores en números y los propios vectores.",
  "n_reviews": 0
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "Permítanme mostrarles un ejemplo que aclara el significado y que resulta que también responde al enigma del producto escalar de antes.",
  "n_reviews": 0
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "Desaprende lo que has aprendido e imagina que aún no sabes que el producto escalar se relaciona con la proyección.",
  "n_reviews": 0
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "Lo que voy a hacer aquí es tomar una copia de la recta numérica y colocarla diagonalmente en el espacio de alguna manera, con el número 0 en el origen.",
  "n_reviews": 0
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "Ahora piense en el vector unitario bidimensional cuya punta se encuentra donde está el número 1 del número.",
  "n_reviews": 0
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "Quiero darle un nombre a ese tipo, u-sombrero.",
  "n_reviews": 0
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "Este pequeño juega un papel importante en lo que está por suceder, así que mantenlo en el fondo de tu mente.",
  "n_reviews": 0
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "Si proyectamos vectores 2d directamente sobre esta recta numérica diagonal, en efecto, acabamos de definir una función que convierte vectores 2d en números.",
  "n_reviews": 0
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "Es más, esta función es en realidad lineal, ya que pasa nuestra prueba visual de que cualquier línea de puntos espaciados uniformemente permanece igualmente espaciada una vez que llega a la recta numérica.",
  "n_reviews": 0
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "Para que quede claro, aunque he incrustado la recta numérica en un espacio 2D como este, las salidas de la función son números, no vectores 2D.",
  "n_reviews": 0
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "Deberías pensar en una función que tome dos coordenadas y genere una sola coordenada.",
  "n_reviews": 0
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "Pero ese vector u-hat es un vector bidimensional que vive en el espacio de entrada.",
  "n_reviews": 0
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "Simplemente está situado de tal manera que se superpone con la incrustación de la recta numérica.",
  "n_reviews": 0
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "Con esta proyección, acabamos de definir una transformación lineal de vectores 2d a números, por lo que podremos encontrar algún tipo de matriz de 1x2 que describa esa transformación.",
  "n_reviews": 0
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "Para encontrar esa matriz de 1x2, acerquémonos a esta configuración de línea numérica diagonal y pensemos en dónde aterrizan i-hat y j-hat, ya que esos puntos de aterrizaje serán las columnas de la matriz.",
  "n_reviews": 0
 },
 {
  "input": "This part's super cool.",
  "translatedText": "Esta parte es genial.",
  "n_reviews": 0
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "Podemos razonarlo con una pieza de simetría realmente elegante.",
  "n_reviews": 0
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "Dado que i-hat y u-hat son vectores unitarios, proyectar i-hat sobre la línea que pasa por u-hat parece totalmente simétrico a proyectar u-hat sobre el eje x.",
  "n_reviews": 0
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "Entonces, cuando preguntamos en qué número aterriza i-hat cuando se proyecta, la respuesta será la misma que cualquier número en el que aterriza u-hat cuando se proyecta sobre el eje x.",
  "n_reviews": 0
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "Pero proyectar u-hat sobre el eje x solo significa tomar la coordenada x de u-hat.",
  "n_reviews": 0
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "Entonces, por simetría, el número donde aterriza i-hat cuando se proyecta sobre esa recta numérica diagonal será la coordenada x de u-hat.",
  "n_reviews": 0
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "¿No es genial?",
  "n_reviews": 0
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "El razonamiento es casi idéntico para el caso j-hat.",
  "n_reviews": 0
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "Piensa un momento en ello.",
  "n_reviews": 0
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "Por las mismas razones, la coordenada y de u-hat nos da el número donde aterriza j-hat cuando se proyecta en la copia de la recta numérica.",
  "n_reviews": 0
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "Haga una pausa y reflexione sobre eso por un momento.",
  "n_reviews": 0
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "Creo que eso es realmente genial.",
  "n_reviews": 0
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "Entonces, las entradas de la matriz 1x2 que describe la transformación de proyección serán las coordenadas de u-hat.",
  "n_reviews": 0
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "Y calcular esta transformación de proyección para vectores arbitrarios en el espacio, que requiere multiplicar esa matriz por esos vectores, es computacionalmente idéntico a tomar un producto escalar con u-hat.",
  "n_reviews": 0
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "Es por eso que tomar el producto escalar con un vector unitario puede interpretarse como proyectar un vector en el tramo de ese vector unitario y tomar la longitud.",
  "n_reviews": 0
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "Entonces, ¿qué pasa con los vectores no unitarios?",
  "n_reviews": 0
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "Por ejemplo, digamos que tomamos ese vector unitario u-hat, pero lo ampliamos en un factor de 3.",
  "n_reviews": 0
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "Numéricamente, cada uno de sus componentes se multiplica por 3.",
  "n_reviews": 0
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "Entonces, al observar la matriz asociada con ese vector, i-hat y j-hat alcanzan tres veces los valores donde aterrizaron antes.",
  "n_reviews": 0
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "Dado que todo esto es lineal, implica de manera más general que la nueva matriz puede interpretarse como proyectar cualquier vector en la copia de la recta numérica y multiplicar donde aterriza por 3.",
  "n_reviews": 0
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "Esta es la razón por la que el producto escalar con un vector no unitario se puede interpretar como una proyección primero sobre ese vector y luego un aumento de la longitud de esa proyección según la longitud del vector.",
  "n_reviews": 0
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "Tómate un momento para pensar en lo que pasó aquí.",
  "n_reviews": 0
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "Tuvimos una transformación lineal del espacio 2D a la recta numérica, que no se definió en términos de vectores numéricos o productos escalares numéricos, simplemente se definió proyectando el espacio sobre una copia diagonal de la recta numérica.",
  "n_reviews": 0
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "Pero como la transformación es lineal, necesariamente se describió mediante alguna matriz de 1x2.",
  "n_reviews": 0
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "Y dado que multiplicar una matriz de 1x2 por un vector 2D es lo mismo que girar esa matriz de lado y tomar un producto escalar, esta transformación estaba ineludiblemente relacionada con algún vector 2D.",
  "n_reviews": 0
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "La lección aquí es que cada vez que tienes una de estas transformaciones lineales cuyo espacio de salida es la recta numérica, sin importar cómo se definió, habrá algún vector único v correspondiente a esa transformación, en el sentido de que aplicar la transformación es lo mismo que tomar un producto escalar con ese vector.",
  "n_reviews": 0
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "Para mí, esto es absolutamente hermoso.",
  "n_reviews": 0
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "Es un ejemplo de algo en matemáticas llamado dualidad.",
  "n_reviews": 0
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "La dualidad aparece de muchas maneras y formas diferentes en las matemáticas, y es muy complicado definirla.",
  "n_reviews": 0
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "En términos generales, se refiere a situaciones en las que existe una correspondencia natural pero sorprendente entre dos tipos de elementos matemáticos.",
  "n_reviews": 0
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "Para el caso de álgebra lineal que acabas de aprender, dirías que el dual de un vector es la transformación lineal que codifica, y el dual de una transformación lineal de algún espacio a una dimensión es un determinado vector en ese espacio.",
  "n_reviews": 0
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "Entonces, para resumir, en la superficie, el producto escalar es una herramienta geométrica muy útil para comprender las proyecciones y para probar si los vectores tienden o no a apuntar en la misma dirección.",
  "n_reviews": 0
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "Y eso es probablemente lo más importante que debes recordar sobre el producto escalar.",
  "n_reviews": 0
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "Pero a un nivel más profundo, unir dos vectores es una forma de traducir uno de ellos al mundo de las transformaciones.",
  "n_reviews": 0
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "Una vez más, numéricamente, esto podría parecer un punto tonto que hay que enfatizar.",
  "n_reviews": 0
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "Son sólo dos cálculos que parecen similares.",
  "n_reviews": 0
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "Pero la razón por la que encuentro esto tan importante es que en matemáticas, cuando trabajas con un vector, una vez que realmente conoces su personalidad, a veces te das cuenta de que es más fácil entenderlo no como una flecha en el espacio, sino como el realización física de una transformación lineal.",
  "n_reviews": 0
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "Es como si el vector fuera en realidad sólo una abreviatura conceptual de una determinada transformación, ya que es más fácil para nosotros pensar en flechas en el espacio en lugar de mover todo ese espacio a la recta numérica.",
  "n_reviews": 0
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "En el siguiente video, verás otro ejemplo realmente interesante de esta dualidad en acción, mientras hablo sobre el producto cruzado.",
  "n_reviews": 0
 }
]