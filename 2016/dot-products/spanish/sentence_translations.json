[
 {
  "translatedText": "[&quot;Oda a la alegría&quot;, de Beethoven, se toca hasta el final del piano.] Tradicionalmente, los productos escalares son algo que se introduce muy temprano en un curso de álgebra lineal, normalmente desde el principio.",
  "input": "[\"Ode to Joy\", by Beethoven, plays to the end of the piano.] Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "time_range": [
   16.58,
   26.3
  ]
 },
 {
  "translatedText": "Así que puede parecer extraño que los haya hecho retroceder hasta este punto en la serie.",
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "time_range": [
   26.64,
   29.58
  ]
 },
 {
  "translatedText": "Hice esto porque hay una forma estándar de presentar el tema, que no requiere más que una comprensión básica de los vectores, pero una comprensión más completa del papel que desempeñan los productos escalares en matemáticas solo se puede encontrar a la luz de las transformaciones lineales.",
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "time_range": [
   29.58,
   42.44
  ]
 },
 {
  "translatedText": "Antes de eso, sin embargo, permítanme cubrir brevemente la forma estándar en que se presentan los productos punto, que supongo que es al menos parcialmente una revisión para varios espectadores.",
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "time_range": [
   43.48,
   50.62
  ]
 },
 {
  "translatedText": "Numéricamente, si tienes dos vectores de la misma dimensión, dos listas de números con las mismas longitudes, tomar su producto escalar significa emparejar todas las coordenadas, multiplicar esos pares y sumar el resultado.",
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "time_range": [
   51.44,
   64.98
  ]
 },
 {
  "translatedText": "Entonces, el vector 1, 2 punteado con 3, 4 sería 1 por 3 más 2 por 4.",
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "time_range": [
   66.86,
   73.18
  ]
 },
 {
  "translatedText": "El vector 6, 2, 8, 3 punteado con 1, 8, 5, 3 sería 6 por 1 más 2 por 8 más 8 por 5 más 3 por 3.",
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "time_range": [
   74.58,
   83.72
  ]
 },
 {
  "translatedText": "Afortunadamente, este cálculo tiene una interpretación geométrica realmente agradable.",
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "time_range": [
   84.74,
   88.66
  ]
 },
 {
  "translatedText": "Para pensar en el producto escalar entre dos vectores, v y w, imagina proyectar w sobre la línea que pasa por el origen y la punta de v.",
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "time_range": [
   89.34,
   97.98
  ]
 },
 {
  "translatedText": "Multiplicando la longitud de esta proyección por la longitud de v, se obtiene el producto escalar v punto w.",
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "time_range": [
   98.78,
   104.46
  ]
 },
 {
  "translatedText": "Excepto cuando esta proyección de w apunta en la dirección opuesta a v, ese producto escalar en realidad será negativo.",
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "time_range": [
   106.42,
   112.16
  ]
 },
 {
  "translatedText": "Entonces, cuando dos vectores generalmente apuntan en la misma dirección, su producto escalar es positivo.",
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "time_range": [
   113.72,
   117.86
  ]
 },
 {
  "translatedText": "Cuando son perpendiculares, es decir, la proyección de uno sobre el otro es el vector cero, su producto escalar es cero.",
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "time_range": [
   119.24,
   125.56
  ]
 },
 {
  "translatedText": "Y si apuntan generalmente en la dirección opuesta, su producto escalar es negativo.",
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "time_range": [
   125.98,
   129.6
  ]
 },
 {
  "translatedText": "Ahora bien, esta interpretación es extrañamente asimétrica.",
  "input": "Now, this interpretation is weirdly asymmetric.",
  "time_range": [
   131.62,
   134.56
  ]
 },
 {
  "translatedText": "Trata a los dos vectores de manera muy diferente.",
  "input": "It treats the two vectors very differently.",
  "time_range": [
   134.8,
   136.5
  ]
 },
 {
  "translatedText": "Entonces, cuando supe esto por primera vez, me sorprendió que el orden no importara.",
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "time_range": [
   136.88,
   140.0
  ]
 },
 {
  "translatedText": "En su lugar, podría proyectar v sobre w, multiplicar la longitud de la v proyectada por la longitud de w y obtener el mismo resultado.",
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "time_range": [
   140.96,
   148.22
  ]
 },
 {
  "translatedText": "Quiero decir, ¿no parece un proceso realmente diferente?",
  "input": "I mean, doesn't that feel like a really different process?",
  "time_range": [
   150.4,
   152.84
  ]
 },
 {
  "translatedText": "Aquí está la intuición de por qué el orden no importa.",
  "input": "Here's the intuition for why order doesn't matter.",
  "time_range": [
   155.32,
   157.76
  ]
 },
 {
  "translatedText": "Si v y w tuvieran la misma longitud, podríamos aprovechar cierta simetría.",
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "time_range": [
   158.44,
   162.18
  ]
 },
 {
  "translatedText": "Dado que proyectar w sobre v, luego multiplicar la longitud de esa proyección por la longitud de v, es una imagen especular completa de proyectar v sobre w, luego multiplicar la longitud de esa proyección por la longitud de w.",
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "time_range": [
   163.08,
   175.24
  ]
 },
 {
  "translatedText": "Ahora, si escalas uno de ellos, digamos v, por alguna constante como 2, de modo que no tengan la misma longitud, la simetría se rompe.",
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "time_range": [
   177.28,
   184.36
  ]
 },
 {
  "translatedText": "Pero pensemos en cómo interpretar el producto escalar entre este nuevo vector, 2 por v, y w.",
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "time_range": [
   185.02,
   190.04
  ]
 },
 {
  "translatedText": "Si piensa que w se proyecta sobre v, entonces el producto escalar 2v punto w será exactamente el doble del producto escalar v punto w.",
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "time_range": [
   190.88,
   199.72
  ]
 },
 {
  "translatedText": "Esto se debe a que cuando escalas v en 2, no cambia la longitud de la proyección de w, pero duplica la longitud del vector sobre el que estás proyectando.",
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "time_range": [
   200.46,
   209.52
  ]
 },
 {
  "translatedText": "Pero, por otro lado, digamos que estás pensando en proyectar v sobre w.",
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "time_range": [
   210.46,
   214.2
  ]
 },
 {
  "translatedText": "Bueno, en ese caso, la longitud de la proyección es lo que se escala cuando multiplicamos v por 2, pero la longitud del vector sobre el que estás proyectando se mantiene constante.",
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "time_range": [
   214.9,
   223.0
  ]
 },
 {
  "translatedText": "Entonces, el efecto general sigue siendo simplemente duplicar el producto escalar.",
  "input": "So the overall effect is still to just double the dot product.",
  "time_range": [
   223.0,
   226.66
  ]
 },
 {
  "translatedText": "Entonces, aunque en este caso se rompe la simetría, el efecto que tiene esta escala sobre el valor del producto escalar es el mismo en ambas interpretaciones.",
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "time_range": [
   227.28,
   234.86
  ]
 },
 {
  "translatedText": "También hay otra gran pregunta que me confundió cuando aprendí esto por primera vez.",
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "time_range": [
   236.64,
   240.34
  ]
 },
 {
  "translatedText": "¿Por qué diablos este proceso numérico de hacer coincidir coordenadas, multiplicar pares y sumarlos tiene algo que ver con la proyección?",
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "time_range": [
   240.84,
   248.74
  ]
 },
 {
  "translatedText": "Bueno, para dar una respuesta satisfactoria, y también para hacer plena justicia a la importancia del producto escalar, necesitamos descubrir algo un poco más profundo que está sucediendo aquí, que a menudo recibe el nombre de dualidad.",
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "time_range": [
   250.64,
   261.4
  ]
 },
 {
  "translatedText": "Pero antes de entrar en eso, necesito dedicar algo de tiempo a hablar sobre transformaciones lineales de múltiples dimensiones a una dimensión, que es solo la recta numérica.",
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "time_range": [
   262.14,
   270.04
  ]
 },
 {
  "translatedText": "Estas son funciones que toman un vector 2D y escupen algún número, pero las transformaciones lineales son, por supuesto, mucho más restringidas que la función común y corriente con una entrada 2D y una salida 1D.",
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "time_range": [
   272.42,
   282.3
  ]
 },
 {
  "translatedText": "Al igual que con las transformaciones en dimensiones superiores, como las que hablé en el capítulo 3, hay algunas propiedades formales que hacen que estas funciones sean lineales, pero voy a ignorarlas aquí a propósito para no distraernos de nuestro objetivo final, y en su lugar centrarse en una determinada propiedad visual que sea equivalente a todo el material formal.",
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "time_range": [
   283.02,
   298.26
  ]
 },
 {
  "translatedText": "Si toma una línea de puntos espaciados uniformemente y aplica una transformación, una transformación lineal mantendrá esos puntos espaciados uniformemente una vez que lleguen al espacio de salida, que es la recta numérica.",
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "time_range": [
   299.04,
   311.28
  ]
 },
 {
  "translatedText": "De lo contrario, si hay alguna línea de puntos que se espacia de manera desigual, entonces su transformación no es lineal.",
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "time_range": [
   312.42,
   317.14
  ]
 },
 {
  "translatedText": "Como en los casos que hemos visto antes, una de estas transformaciones lineales está completamente determinada por dónde toma i-hat y j-hat, pero esta vez cada uno de esos vectores base simplemente aterriza en un número, así que cuando registramos dónde aterrizan como las columnas de una matriz, cada una de esas columnas solo tiene un número.",
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "time_range": [
   319.22,
   336.82
  ]
 },
 {
  "translatedText": "Esta es una matriz de 1x2.",
  "input": "This is a 1x2 matrix.",
  "time_range": [
   338.46,
   339.84
  ]
 },
 {
  "translatedText": "Veamos un ejemplo de lo que significa aplicar una de estas transformaciones a un vector.",
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "time_range": [
   341.86,
   345.66
  ]
 },
 {
  "translatedText": "Digamos que tienes una transformación lineal que lleva i-hat a 1 y j-hat a menos 2.",
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "time_range": [
   346.38,
   351.68
  ]
 },
 {
  "translatedText": "Para seguir dónde termina un vector con coordenadas, digamos, 4, 3, piense en dividir este vector como 4 veces i-hat más 3 veces j-hat.",
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "time_range": [
   352.42,
   361.02
  ]
 },
 {
  "translatedText": "Una consecuencia de la linealidad es que después de la transformación, el vector será 4 veces el lugar donde aterriza i-hat, 1, más 3 veces el lugar donde aterriza j-hat, menos 2, lo que en este caso implica que aterriza en negativo 2.",
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "time_range": [
   361.84,
   375.78
  ]
 },
 {
  "translatedText": "Cuando haces este cálculo puramente numérico, es una multiplicación de vectores matriciales.",
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "time_range": [
   378.02,
   382.36
  ]
 },
 {
  "translatedText": "Ahora bien, esta operación numérica de multiplicar una matriz de 1x2 por un vector es como tomar el producto escalar de dos vectores.",
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "time_range": [
   385.7,
   392.86
  ]
 },
 {
  "translatedText": "¿No parece esa matriz de 1x2 simplemente un vector que inclinamos de lado?",
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "time_range": [
   393.46,
   396.8
  ]
 },
 {
  "translatedText": "De hecho, podríamos decir ahora mismo que existe una buena asociación entre matrices 1x2 y vectores 2D, definida inclinando la representación numérica de un vector de lado para obtener la matriz asociada, o inclinando la matriz hacia arriba para obtener el vector asociado. .",
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "time_range": [
   397.96,
   412.58
  ]
 },
 {
  "translatedText": "Dado que ahora solo estamos viendo expresiones numéricas, ir y venir entre vectores y matrices de 1x2 puede parecer una tontería.",
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "time_range": [
   413.56,
   420.86
  ]
 },
 {
  "translatedText": "Pero esto sugiere algo que es realmente asombroso desde el punto de vista geométrico.",
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "time_range": [
   421.46,
   425.12
  ]
 },
 {
  "translatedText": "Existe algún tipo de conexión entre las transformaciones lineales que convierten los vectores en números y los propios vectores.",
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "time_range": [
   425.38,
   431.72
  ]
 },
 {
  "translatedText": "Permítanme mostrarles un ejemplo que aclara el significado y que resulta que también responde al enigma del producto escalar de antes.",
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "time_range": [
   434.78,
   441.38
  ]
 },
 {
  "translatedText": "Desaprende lo que has aprendido e imagina que aún no sabes que el producto escalar se relaciona con la proyección.",
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "time_range": [
   442.14,
   447.18
  ]
 },
 {
  "translatedText": "Lo que voy a hacer aquí es tomar una copia de la recta numérica y colocarla diagonalmente en el espacio de alguna manera, con el número 0 en el origen.",
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "time_range": [
   448.86,
   456.06
  ]
 },
 {
  "translatedText": "Ahora piense en el vector unitario bidimensional cuya punta se encuentra donde está el número 1 del número.",
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "time_range": [
   456.9,
   461.92
  ]
 },
 {
  "translatedText": "Quiero darle un nombre a ese tipo, u-sombrero.",
  "input": "I want to give that guy a name, u-hat.",
  "time_range": [
   462.4,
   464.56
  ]
 },
 {
  "translatedText": "Este pequeño juega un papel importante en lo que está por suceder, así que mantenlo en el fondo de tu mente.",
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "time_range": [
   465.62,
   470.02
  ]
 },
 {
  "translatedText": "Si proyectamos vectores 2d directamente sobre esta recta numérica diagonal, en efecto, acabamos de definir una función que convierte vectores 2d en números.",
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "time_range": [
   470.74,
   478.96
  ]
 },
 {
  "translatedText": "Es más, esta función es en realidad lineal, ya que pasa nuestra prueba visual de que cualquier línea de puntos espaciados uniformemente permanece igualmente espaciada una vez que llega a la recta numérica.",
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "time_range": [
   479.66,
   488.96
  ]
 },
 {
  "translatedText": "Para que quede claro, aunque he incrustado la recta numérica en un espacio 2D como este, las salidas de la función son números, no vectores 2D.",
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "time_range": [
   491.64,
   499.28
  ]
 },
 {
  "translatedText": "Deberías pensar en una función que tome dos coordenadas y genere una sola coordenada.",
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "time_range": [
   499.96,
   503.68
  ]
 },
 {
  "translatedText": "Pero ese vector u-hat es un vector bidimensional que vive en el espacio de entrada.",
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "time_range": [
   505.06,
   509.02
  ]
 },
 {
  "translatedText": "Simplemente está situado de tal manera que se superpone con la incrustación de la recta numérica.",
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "time_range": [
   509.44,
   513.22
  ]
 },
 {
  "translatedText": "Con esta proyección, acabamos de definir una transformación lineal de vectores 2d a números, por lo que podremos encontrar algún tipo de matriz de 1x2 que describa esa transformación.",
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "time_range": [
   514.6,
   524.6
  ]
 },
 {
  "translatedText": "Para encontrar esa matriz de 1x2, acerquémonos a esta configuración de línea numérica diagonal y pensemos en dónde aterrizan i-hat y j-hat, ya que esos puntos de aterrizaje serán las columnas de la matriz.",
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "time_range": [
   525.54,
   536.46
  ]
 },
 {
  "translatedText": "Esta parte es genial.",
  "input": "This part's super cool.",
  "time_range": [
   538.48,
   539.44
  ]
 },
 {
  "translatedText": "Podemos razonarlo con una pieza de simetría realmente elegante.",
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "time_range": [
   539.7,
   542.42
  ]
 },
 {
  "translatedText": "Dado que i-hat y u-hat son vectores unitarios, proyectar i-hat sobre la línea que pasa por u-hat parece totalmente simétrico a proyectar u-hat sobre el eje x.",
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "time_range": [
   543.02,
   553.16
  ]
 },
 {
  "translatedText": "Entonces, cuando preguntamos en qué número aterriza i-hat cuando se proyecta, la respuesta será la misma que cualquier número en el que aterriza u-hat cuando se proyecta sobre el eje x.",
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "time_range": [
   553.84,
   562.32
  ]
 },
 {
  "translatedText": "Pero proyectar u-hat sobre el eje x solo significa tomar la coordenada x de u-hat.",
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "time_range": [
   562.92,
   568.6
  ]
 },
 {
  "translatedText": "Entonces, por simetría, el número donde aterriza i-hat cuando se proyecta sobre esa recta numérica diagonal será la coordenada x de u-hat.",
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "time_range": [
   569.02,
   576.62
  ]
 },
 {
  "translatedText": "¿No es genial?",
  "input": "Isn't that cool?",
  "time_range": [
   577.16,
   577.66
  ]
 },
 {
  "translatedText": "El razonamiento es casi idéntico para el caso j-hat.",
  "input": "The reasoning is almost identical for the j-hat case.",
  "time_range": [
   579.2,
   581.8
  ]
 },
 {
  "translatedText": "Piensa un momento en ello.",
  "input": "Think about it for a moment.",
  "time_range": [
   582.18,
   583.26
  ]
 },
 {
  "translatedText": "Por las mismas razones, la coordenada y de u-hat nos da el número donde aterriza j-hat cuando se proyecta en la copia de la recta numérica.",
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "time_range": [
   589.12,
   596.6
  ]
 },
 {
  "translatedText": "Haga una pausa y reflexione sobre eso por un momento.",
  "input": "Pause and ponder that for a moment.",
  "time_range": [
   597.58,
   598.72
  ]
 },
 {
  "translatedText": "Creo que eso es realmente genial.",
  "input": "I just think that's really cool.",
  "time_range": [
   598.78,
   600.2
  ]
 },
 {
  "translatedText": "Entonces, las entradas de la matriz 1x2 que describe la transformación de proyección serán las coordenadas de u-hat.",
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "time_range": [
   600.92,
   607.26
  ]
 },
 {
  "translatedText": "Y calcular esta transformación de proyección para vectores arbitrarios en el espacio, que requiere multiplicar esa matriz por esos vectores, es computacionalmente idéntico a tomar un producto escalar con u-hat.",
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "time_range": [
   608.04,
   618.88
  ]
 },
 {
  "translatedText": "Es por eso que tomar el producto escalar con un vector unitario puede interpretarse como proyectar un vector en el tramo de ese vector unitario y tomar la longitud.",
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "time_range": [
   621.46,
   630.59
  ]
 },
 {
  "translatedText": "Entonces, ¿qué pasa con los vectores no unitarios?",
  "input": "So what about non-unit vectors?",
  "time_range": [
   634.03,
   635.79
  ]
 },
 {
  "translatedText": "Por ejemplo, digamos que tomamos ese vector unitario u-hat, pero lo ampliamos en un factor de 3.",
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "time_range": [
   636.31,
   640.63
  ]
 },
 {
  "translatedText": "Numéricamente, cada uno de sus componentes se multiplica por 3.",
  "input": "Numerically, each of its components gets multiplied by 3.",
  "time_range": [
   641.35,
   644.39
  ]
 },
 {
  "translatedText": "Entonces, al observar la matriz asociada con ese vector, i-hat y j-hat alcanzan tres veces los valores donde aterrizaron antes.",
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "time_range": [
   644.81,
   652.39
  ]
 },
 {
  "translatedText": "Dado que todo esto es lineal, implica de manera más general que la nueva matriz puede interpretarse como proyectar cualquier vector en la copia de la recta numérica y multiplicar donde aterriza por 3.",
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "time_range": [
   655.23,
   664.65
  ]
 },
 {
  "translatedText": "Esta es la razón por la que el producto escalar con un vector no unitario se puede interpretar como una proyección primero sobre ese vector y luego un aumento de la longitud de esa proyección según la longitud del vector.",
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "time_range": [
   665.47,
   674.95
  ]
 },
 {
  "translatedText": "Tómate un momento para pensar en lo que pasó aquí.",
  "input": "Take a moment to think about what happened here.",
  "time_range": [
   677.59,
   679.55
  ]
 },
 {
  "translatedText": "Tuvimos una transformación lineal del espacio 2D a la recta numérica, que no se definió en términos de vectores numéricos o productos escalares numéricos, simplemente se definió proyectando el espacio sobre una copia diagonal de la recta numérica.",
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "time_range": [
   679.89,
   690.89
  ]
 },
 {
  "translatedText": "Pero como la transformación es lineal, necesariamente se describió mediante alguna matriz de 1x2.",
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "time_range": [
   691.67,
   696.83
  ]
 },
 {
  "translatedText": "Y dado que multiplicar una matriz de 1x2 por un vector 2D es lo mismo que girar esa matriz de lado y tomar un producto escalar, esta transformación estaba ineludiblemente relacionada con algún vector 2D.",
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "time_range": [
   697.33,
   707.91
  ]
 },
 {
  "translatedText": "La lección aquí es que cada vez que tienes una de estas transformaciones lineales cuyo espacio de salida es la recta numérica, sin importar cómo se definió, habrá algún vector único v correspondiente a esa transformación, en el sentido de que aplicar la transformación es lo mismo que tomar un producto escalar con ese vector.",
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "time_range": [
   709.41,
   726.35
  ]
 },
 {
  "translatedText": "Para mí, esto es absolutamente hermoso.",
  "input": "To me, this is utterly beautiful.",
  "time_range": [
   729.93,
   732.03
  ]
 },
 {
  "translatedText": "Es un ejemplo de algo en matemáticas llamado dualidad.",
  "input": "It's an example of something in math called duality.",
  "time_range": [
   732.73,
   735.39
  ]
 },
 {
  "translatedText": "La dualidad aparece de muchas maneras y formas diferentes en las matemáticas, y es muy complicado definirla.",
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "time_range": [
   736.27,
   741.93
  ]
 },
 {
  "translatedText": "En términos generales, se refiere a situaciones en las que existe una correspondencia natural pero sorprendente entre dos tipos de elementos matemáticos.",
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "time_range": [
   742.67,
   750.23
  ]
 },
 {
  "translatedText": "Para el caso de álgebra lineal que acabas de aprender, dirías que el dual de un vector es la transformación lineal que codifica, y el dual de una transformación lineal de algún espacio a una dimensión es un determinado vector en ese espacio.",
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "time_range": [
   751.01,
   764.65
  ]
 },
 {
  "translatedText": "Entonces, para resumir, en la superficie, el producto escalar es una herramienta geométrica muy útil para comprender las proyecciones y para probar si los vectores tienden o no a apuntar en la misma dirección.",
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "time_range": [
   766.73,
   776.31
  ]
 },
 {
  "translatedText": "Y eso es probablemente lo más importante que debes recordar sobre el producto escalar.",
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "time_range": [
   776.97,
   780.79
  ]
 },
 {
  "translatedText": "Pero a un nivel más profundo, unir dos vectores es una forma de traducir uno de ellos al mundo de las transformaciones.",
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "time_range": [
   781.27,
   787.73
  ]
 },
 {
  "translatedText": "Una vez más, numéricamente, esto podría parecer un punto tonto que hay que enfatizar.",
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "time_range": [
   788.67,
   791.55
  ]
 },
 {
  "translatedText": "Son sólo dos cálculos que parecen similares.",
  "input": "It's just two computations that happen to look similar.",
  "time_range": [
   791.67,
   794.49
  ]
 },
 {
  "translatedText": "Pero la razón por la que encuentro esto tan importante es que en matemáticas, cuando trabajas con un vector, una vez que realmente conoces su personalidad, a veces te das cuenta de que es más fácil entenderlo no como una flecha en el espacio, sino como el realización física de una transformación lineal.",
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "time_range": [
   794.49,
   810.09
  ]
 },
 {
  "translatedText": "Es como si el vector fuera en realidad sólo una abreviatura conceptual de una determinada transformación, ya que es más fácil para nosotros pensar en flechas en el espacio en lugar de mover todo ese espacio a la recta numérica.",
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "time_range": [
   810.73,
   820.97
  ]
 },
 {
  "translatedText": "En el siguiente video, verás otro ejemplo realmente interesante de esta dualidad en acción, mientras hablo sobre el producto cruzado.",
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "time_range": [
   822.61,
   829.19
  ]
 }
]