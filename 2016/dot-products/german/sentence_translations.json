[
 {
  "input": "Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "Traditionell werden Punktprodukte schon sehr früh in einem Kurs über lineare Algebra eingeführt, normalerweise gleich zu Beginn.",
  "model": "DeepL",
  "from_community_srt": "klassischerweise, Skalarprodukt oder irgendwas, dass sehr früher in lineare Algebra eingeführt wurde kurs in der Regel gleich zu Beginn.",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "Es mag also seltsam erscheinen, dass ich sie in der Serie so weit zurückgeschoben habe.",
  "model": "DeepL",
  "from_community_srt": "Es mag also seltsam erscheinen, dass ich sie in der Serie so weit zurückschiebe.",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "Ich habe das getan, weil es einen Standardweg gibt, um das Thema einzuführen, der nicht mehr als ein grundlegendes Verständnis von Vektoren erfordert, aber ein umfassenderes Verständnis der Rolle, die Punktprodukte in der Mathematik spielen, kann nur unter dem Licht der linearen Transformationen gefunden werden.",
  "model": "DeepL",
  "from_community_srt": "Ich habe dies getan, weil es einen Standardweg gibt, um das Thema einzuführen, welches erfordert nichts weiter als ein grundlegendes Verständnis der Vektoren, aber ein umfassenderes Verständnis der Rolle, die die Skalarprodukte in der Mathematik spielen, kann wirklich nur unter dem Licht der  linearen Transformationen gefunden werden.",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "Zuvor möchte ich jedoch kurz auf die übliche Art und Weise eingehen, in der Punktprodukte eingeführt werden, und ich gehe davon aus, dass viele Betrachterinnen und Betrachter dies zumindest teilweise nachvollziehen können.",
  "model": "DeepL",
  "from_community_srt": "Lassen Sie mich jedoch vorher kurz darauf eingehen die Standardmethode für die Einführung von Produkten Welche ich annehme, ist zumindest teilweise eine Überprüfung für eine Reihe von Zuschauern.",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "Wenn du zwei Vektoren der gleichen Dimension hast, also zwei Listen von Zahlen mit der gleichen Länge, bedeutet das Punktprodukt, dass du alle Koordinatenpaare miteinander multiplizierst und das Ergebnis addierst.",
  "model": "DeepL",
  "from_community_srt": "Numerisch, wenn Sie zwei Vektoren derselben Dimension haben; zur Liste von Zahlen mit der gleichen Länge, ihr Skalarprodukt zu nehmen bedeutet alle Koordinaten Paarweise koppeln, Multiplizieren dieser Paare zusammen,",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "Der Vektor 1, 2 gepunktet mit 3, 4 wäre also 1 mal 3 plus 2 mal 4.",
  "model": "DeepL",
  "from_community_srt": "und deren Ergebnisse zusammen addieren so der Vektor [1,2] gepunktet mit [3,4] wäre 1 x 3 + 2 x 4.",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "Der Vektor 6, 2, 8, 3 gepunktet mit 1, 8, 5, 3 wäre 6 mal 1 plus 2 mal 8 plus 8 mal 5 plus 3 mal 3.",
  "model": "DeepL",
  "from_community_srt": "der Vektor [6, 2, 8, 3] gepunktet mit [1, 8, 5, 3] wäre dann: 6 x 1 + 2 x 8 + 8 x 5 + 3 x 3",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "Zum Glück gibt es für diese Berechnung eine sehr schöne geometrische Interpretation.",
  "model": "DeepL",
  "from_community_srt": "Glücklicherweise hat diese Berechnung eine wirklich schöne geometrische Interpretation.",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "Um über das Punktprodukt zwischen zwei Vektoren, v und w, nachzudenken, stelle dir vor, dass du w auf die Linie projizierst, die durch den Ursprung und die Spitze von v verläuft.",
  "model": "DeepL",
  "from_community_srt": "Um über das Skalarprodukt zwischen zwei Vektoren v und w nachzudenken, Stellen Sie sich vor, Sie projizieren w auf die Linie, die durch den Ursprung und die Spitze von v verläuft.",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "Multiplizierst du die Länge dieser Projektion mit der Länge von v, erhältst du das Punktprodukt v Punkt w.",
  "model": "DeepL",
  "from_community_srt": "Wenn Sie die Länge dieser Projektion mit der Länge von v multiplizieren, erhalten Sie das Skalarprodukt v・w.",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "Nur wenn diese Projektion von w in die entgegengesetzte Richtung von v zeigt, ist das Punktprodukt negativ.",
  "model": "DeepL",
  "from_community_srt": "Außer wenn diese Projektion von w in die entgegengesetzte Richtung von v zeigt, Dieses Skalarprodukt ist tatsächlich negativ.",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "Wenn also zwei Vektoren generell in dieselbe Richtung zeigen, ist ihr Punktprodukt positiv.",
  "model": "DeepL",
  "from_community_srt": "Wenn also zwei Vektoren im Allgemeinen in die gleiche Richtung zeigen, ihr Skalarprodukt ist positiv.",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "Wenn sie senkrecht zueinander stehen, d.h. die Projektion des einen Vektors auf den anderen der Nullvektor ist, ist ihr Punktprodukt gleich Null.",
  "model": "DeepL",
  "from_community_srt": "Wenn sie senkrecht sind, bedeutet das die Projektion von einem auf das andere ist der 0-Vektor, das Skalarprodukt ist 0",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "Und wenn sie generell in die entgegengesetzte Richtung zeigen, ist ihr Punktprodukt negativ.",
  "model": "DeepL",
  "from_community_srt": "Und wenn sie im Allgemeinen in die entgegengesetzte Richtung zeigen,",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "Diese Interpretation ist seltsam asymmetrisch.",
  "model": "DeepL",
  "from_community_srt": "ist ihr Skalarprodukt negativ. Nun ist diese Interpretation seltsam asymmetrisch, es behandelt die beiden Vektoren sehr unterschiedlich,",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "Es behandelt die beiden Vektoren sehr unterschiedlich.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "Als ich das zum ersten Mal erfuhr, war ich überrascht, dass die Reihenfolge keine Rolle spielt.",
  "model": "DeepL",
  "from_community_srt": "Als ich das zum ersten Mal erfuhr, war ich überrascht,",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "Du könntest stattdessen v auf w projizieren, die Länge des projizierten v mit der Länge von w multiplizieren und würdest das gleiche Ergebnis erhalten.",
  "model": "DeepL",
  "from_community_srt": "dass die Reihenfolge keine Rolle spielt. Sie könnten stattdessen v auf w projizieren; Sie könnten stattdessen v auf w projizieren; und erhalten Sie das gleiche Ergebnis.",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "Ich meine, fühlt sich das nicht wie ein ganz anderer Prozess an?",
  "model": "DeepL",
  "from_community_srt": "Ich meine,",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "Hier ist die Erklärung, warum die Reihenfolge keine Rolle spielt.",
  "model": "DeepL",
  "from_community_srt": "fühlt sich das nicht nach einem wirklich anderen Prozess an? Hier ist die Intuition,",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "Wenn v und w zufällig die gleiche Länge haben, können wir eine gewisse Symmetrie nutzen.",
  "model": "DeepL",
  "from_community_srt": "warum Ordnung keine Rolle spielt: wenn v und w zufällig die gleiche Länge haben,",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "Da die Projektion von w auf v und die anschließende Multiplikation der Länge dieser Projektion mit der Länge von v ein vollständiges Spiegelbild der Projektion von v auf w und die anschließende Multiplikation der Länge dieser Projektion mit der Länge von w ist.",
  "model": "DeepL",
  "from_community_srt": "Wir könnten eine gewisse Symmetrie nutzen. Da w auf v projiziert wird dann multipliziere die Länge dieser Projektion mit der Länge von v, ist ein vollständiges Spiegelbild der Projektion von v auf w und der Multiplikation der Länge davon Projektion um die Länge von w.",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "Wenn du nun eine von ihnen, sagen wir v, um eine Konstante wie 2 skalierst, so dass sie nicht mehr gleich lang sind, wird die Symmetrie gebrochen.",
  "model": "DeepL",
  "from_community_srt": "Wenn Sie nun einen von den „skalieren“, sagen Sie v mit einer Konstanten wie 2, damit sie nicht gleich lang sind, Die Symmetrie ist gebrochen.",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "Aber lass uns darüber nachdenken, wie wir das Punktprodukt zwischen diesem neuen Vektor, 2 mal v, und w interpretieren.",
  "model": "DeepL",
  "from_community_srt": "Aber lassen Sie uns überlegen, wie das Skalarprodukt zwischen diesem neuen Vektor 2v und zu interpretieren ist w Wenn Sie an w denken,",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "Wenn du dir vorstellst, dass w auf v projiziert wird, dann ist das Punktprodukt 2v Punkt w genau doppelt so groß wie das Punktprodukt v Punkt w.",
  "model": "DeepL",
  "from_community_srt": "wird w auf v projiziert dann ist das Skalarprodukt 2v ・ w genau doppelt so groß wie das Skalarprodukt v ・ w.",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "Denn wenn du v um 2 skalierst, ändert sich die Länge der Projektion von w nicht, sondern verdoppelt sich die Länge des Vektors, auf den du projizierst.",
  "model": "DeepL",
  "from_community_srt": "Dies liegt daran, dass, wenn Sie v um 2 „skalieren“, es ändert nicht die Länge der Projektion von w Es verdoppelt jedoch die Länge des Vektors, auf den Sie projizieren.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "Aber nehmen wir mal an, du denkst daran, dass v auf w projiziert wird.",
  "model": "DeepL",
  "from_community_srt": "Nehmen wir andererseits an, Sie denken darüber nach, ob v auf w projiziert wird.",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "In diesem Fall ist die Länge der Projektion das, was skaliert wird, wenn wir v mit 2 multiplizieren, aber die Länge des Vektors, auf den du projizierst, bleibt konstant.",
  "model": "DeepL",
  "from_community_srt": "Nun, in diesem Fall ist die Länge der Projektion die Sache, die \"skaliert\" werden muss, wenn wir multiplizieren v um 2. Die Länge des Vektors, auf den Sie projizieren,",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "Der Gesamteffekt ist also immer noch die Verdoppelung des Punktprodukts.",
  "model": "DeepL",
  "from_community_srt": "bleibt konstant. Der Gesamteffekt besteht also immer noch darin, das Skalarprodukt nur zu verdoppeln.",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "Obwohl die Symmetrie in diesem Fall gebrochen ist, ist der Effekt, den diese Skalierung auf den Wert des Punktprodukts hat, bei beiden Interpretationen derselbe.",
  "model": "DeepL",
  "from_community_srt": "Also, obwohl die Symmetrie in diesem Fall gebrochen ist, Der Effekt, den diese „Skalierung“ auf den Wert des Skalarprodukt hat,",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "Es gibt noch eine weitere große Frage, die mich verwirrt hat, als ich das erste Mal davon erfuhr.",
  "model": "DeepL",
  "from_community_srt": "ist der gleiche unter beiden Interpretationen. Es gibt noch eine andere große Frage, die mich verwirrte,",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "Was um alles in der Welt hat dieser numerische Prozess des Abgleichs von Koordinaten, des Multiplizierens von Paaren und des Addierens irgendetwas mit Projektion zu tun?",
  "model": "DeepL",
  "from_community_srt": "als ich dieses Zeug zum ersten Mal lernte: Warum um alles in der Welt führt dieser numerische Prozess das Abgleichen von Koordinaten, das Multiplizieren von Paaren und addieren sie zusammen, Haben Sie etwas mit Projektion zu tun?",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "Nun, um eine zufriedenstellende Antwort zu geben und auch um der Bedeutung des Punktprodukts gerecht zu werden, müssen wir etwas Tiefergehendes ausgraben, das oft unter dem Namen Dualität bekannt ist.",
  "model": "DeepL",
  "from_community_srt": "Nun, um eine zufriedenstellende Antwort zu geben, und auch um der Bedeutung des Skalarprodukt voll gerecht zu werden, Wir müssen hier etwas Tieferes entdecken was oft unter dem Namen \"Dualität\" bekannt ist.",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "Aber bevor ich darauf eingehe, muss ich noch etwas über lineare Transformationen von mehreren Dimensionen in eine Dimension sprechen, nämlich die Zahlenreihe.",
  "model": "DeepL",
  "from_community_srt": "Aber bevor wir darauf eingehen, Ich muss einige Zeit damit verbringen, über lineare Transformationen zu sprechen von mehreren Dimensionen zu einer Dimension welche nur die Zahlenreihe ist.",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "Das sind Funktionen, die einen 2D-Vektor aufnehmen und eine Zahl ausgeben, aber lineare Transformationen sind natürlich viel eingeschränkter als eine gewöhnliche Funktion mit einer 2D-Eingabe und einer 1D-Ausgabe.",
  "model": "DeepL",
  "from_community_srt": "Dies sind Funktionen, die einen 2D-Vektor aufnehmen und eine Zahl ausspucken. Aber lineare Transformationen sind natürlich viel eingeschränkter als Ihre normale Funktion mit einem 2D-Eingabe und einem 1D-Ausgabe.",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "Wie bei den Transformationen in höheren Dimensionen, über die ich in Kapitel 3 gesprochen habe, gibt es einige formale Eigenschaften, die diese Funktionen linear machen, aber ich werde sie hier bewusst ignorieren, um nicht von unserem Ziel abzulenken.",
  "model": "DeepL",
  "from_community_srt": "Wie bei Transformationen in höheren Dimensionen, wie die, über die ich in Kapitel 3 gesprochen habe, Es gibt einige formale Eigenschaften, die diese Funktionen linear machen. Aber ich werde diese hier absichtlich ignorieren, um nicht von unserem Endziel abzulenken. und konzentrieren Sie sich stattdessen auf eine bestimmte visuelle Eigenschaft,",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "Wenn du eine Linie mit gleichmäßig verteilten Punkten nimmst und eine Transformation anwendest, sorgt eine lineare Transformation dafür, dass die Punkte gleichmäßig verteilt bleiben, sobald sie im Ausgabebereich, also der Zahlenlinie, landen.",
  "model": "DeepL",
  "from_community_srt": "die allen formalen Dingen entspricht. Wenn Sie eine Reihe gleichmäßig verteilter Punkte nehmen und eine Transformation anwenden, Eine lineare Transformation hält diese Punkte gleichmäßig verteilt. Sobald sie im Ausgabebereich landen, ist dies die Zahlenreihe.",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "Wenn es sonst eine Reihe von Punkten gibt, die ungleichmäßig verteilt sind, dann ist deine Transformation nicht linear.",
  "model": "DeepL",
  "from_community_srt": "Andernfalls, wenn eine Punktreihe ungleichmäßig verteilt ist dann ist deine Transformation nicht linear.",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "Wie in den Fällen, die wir zuvor gesehen haben, wird eine dieser linearen Transformationen vollständig dadurch bestimmt, wo sie i-hat und j-hat, aber dieses Mal landet jeder dieser Basisvektoren nur auf einer Zahl, so dass, wenn wir aufzeichnen, wo sie als Spalten einer Matrix landen, jede dieser Spalten nur eine einzige Zahl hat.",
  "model": "DeepL",
  "from_community_srt": "Wie bei den Fällen, die wir zuvor gesehen haben, eine dieser linearen Transformationen wird vollständig davon bestimmt, wohin es i-dach und j-dach nimmt Aber diesmal landet jeder dieser Basisvektoren nur auf einer Zahl. Jede dieser Spalten hat nur eine einzige Nummer.",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "Dies ist eine 1x2-Matrix.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "Gehen wir an einem Beispiel durch, was es bedeutet, eine dieser Transformationen auf einen Vektor anzuwenden.",
  "model": "DeepL",
  "from_community_srt": "Dies ist eine 1 x 2-Matrix. Lassen Sie uns ein Beispiel durchgehen, was es bedeutet, eine dieser Transformationen auf a anzuwenden Vektor.",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "Nehmen wir an, du hast eine lineare Transformation, die i-hat auf 1 und j-hat auf negative 2 bringt.",
  "model": "DeepL",
  "from_community_srt": "Angenommen, Sie haben eine lineare Transformation, die i-dach zu 1 und j-dach zu -2 führt.",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "Um nachzuvollziehen, wo ein Vektor mit den Koordinaten 4, 3 endet, stell dir vor, du zerlegst diesen Vektor in 4 mal i-hat plus 3 mal j-hat.",
  "model": "DeepL",
  "from_community_srt": "Um zu folgen, wo ein Vektor mit Koordinaten endet, sagen wir [4, 3], Stellen Sie sich vor, Sie zerlegen diesen Vektor als 4-mal i-dach + 3-mal j-dach.",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "Eine Konsequenz der Linearität ist, dass der Vektor nach der Transformation 4 mal die Stelle, an der der i-Hut landet, also 1, plus 3 mal die Stelle, an der der j-Hut landet, also negativ 2, ist, was in diesem Fall bedeutet, dass er auf negativ 2 landet.",
  "model": "DeepL",
  "from_community_srt": "Eine Folge der Linearität ist die nach der Transformation Der Vektor ist: 4 mal der Ort, an dem i-Dach landet, 1, plus 3 mal die Stelle, an der j-Dach landet, -2. was in diesem Fall impliziert,",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "Wenn du diese Berechnung rein numerisch durchführst, handelt es sich um eine Matrix-Vektor-Multiplikation.",
  "model": "DeepL",
  "from_community_srt": "dass es auf -2 landet. Wenn Sie diese Berechnung rein numerisch durchführen,",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "Diese numerische Operation, bei der eine 1x2-Matrix mit einem Vektor multipliziert wird, fühlt sich genauso an wie das Punktprodukt von zwei Vektoren.",
  "model": "DeepL",
  "from_community_srt": "handelt es sich um eine Matrix-Vektor-Multiplikation. Diese numerische Operation zum Multiplizieren einer 1 mit 2-Matrix mit einem Vektor fühlt sich an, als würde man das Skalarprodukt zweier Vektoren nehmen.",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "Sieht diese 1x2-Matrix nicht einfach wie ein Vektor aus, den wir auf die Seite gekippt haben?",
  "model": "DeepL",
  "from_community_srt": "Sieht diese 1 x 2-Matrix nicht wie ein Vektor aus,",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "Tatsächlich könnten wir jetzt sagen, dass es eine schöne Verbindung zwischen 1x2-Matrizen und 2D-Vektoren gibt, die dadurch definiert ist, dass man die numerische Darstellung eines Vektors auf die Seite kippt, um die zugehörige Matrix zu erhalten, oder die Matrix wieder nach oben kippt, um den zugehörigen Vektor zu erhalten.",
  "model": "DeepL",
  "from_community_srt": "den wir auf die Seite gekippt haben? Tatsächlich könnten wir jetzt sagen, dass es eine schöne Assoziation zwischen 1 x 2 Matrizen gibt und 2D-Vektoren, definiert durch Kippen der numerischen Darstellung eines Vektors auf seiner Seite, um die zugehörige zu erhalten matrix, oder um die Matrix wieder nach oben zu kippen,",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "Da wir uns im Moment nur mit numerischen Ausdrücken beschäftigen, erscheint es vielleicht albern, zwischen Vektoren und 1x2-Matrizen hin und her zu wechseln.",
  "model": "DeepL",
  "from_community_srt": "um den zugehörigen Vektor zu erhalten. Da wir uns gerade nur mit numerischen Ausdrücken befassen, Das Hin- und Hergehen zwischen Vektoren und 1 x 2 Matrizen könnte sich wie eine dumme Sache anfühlen",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "Aber das deutet auf etwas hin, das aus geometrischer Sicht wirklich fantastisch ist.",
  "model": "DeepL",
  "from_community_srt": "machen Dies deutet jedoch auf etwas hin,",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "Es gibt eine Art Verbindung zwischen linearen Transformationen, die Vektoren in Zahlen umwandeln, und Vektoren selbst.",
  "model": "DeepL",
  "from_community_srt": "das aus geometrischer Sicht wirklich beeindruckend ist: Es gibt eine Art Verbindung zwischen linearen Transformationen,",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "Ich zeige dir ein Beispiel, das die Bedeutung verdeutlicht und zufälligerweise auch das Punktprodukt-Rätsel von vorhin löst.",
  "model": "DeepL",
  "from_community_srt": "die Vektoren zu Zahlen führen und Vektoren selbst. Lassen Sie mich ein Beispiel zeigen, das die Bedeutung verdeutlicht und was zufällig auch das Skalarprodukt-Puzzle von früher beantwortet.",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "Vergiss, was du gelernt hast, und stell dir vor, dass du noch nicht weißt, dass das Punktprodukt mit der Projektion zusammenhängt.",
  "model": "DeepL",
  "from_community_srt": "Verlernen Sie, was Sie gelernt haben und stellen Sie sich vor, Sie wissen noch nicht,",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "Ich nehme eine Kopie der Zahlenreihe und platziere sie irgendwie diagonal im Raum, wobei die Zahl 0 im Ursprung sitzt.",
  "model": "DeepL",
  "from_community_srt": "dass sich das Skalarprodukt auf die Projektion bezieht. Was ich hier tun werde, ist eine Kopie der Zahlenreihe zu nehmen und platziere es diagonal und platziere es irgendwie mit der Zahl 0 am Ursprung.",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "Stell dir nun den zweidimensionalen Einheitsvektor vor, dessen Spitze dort sitzt, wo die Zahl 1 auf der Zahl ist.",
  "model": "DeepL",
  "from_community_srt": "Denken Sie nun an den zweidimensionalen Einheitsvektor, deren Spitzen sitzen dort,",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "Ich möchte dem Kerl einen Namen geben: U-Hut.",
  "model": "DeepL",
  "from_community_srt": "wo die Nummer 1 auf der Zahlenlinie ist Ich möchte diesem Kerl einen Namen geben.",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "Dieser kleine Kerl spielt eine wichtige Rolle bei dem, was passieren wird, also behalte ihn einfach im Hinterkopf.",
  "model": "DeepL",
  "from_community_srt": "Dieser kleine Kerl spielt eine wichtige Rolle in dem, was passieren wird. Behalte sie also einfach im Hinterkopf.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "Wenn wir 2D-Vektoren direkt auf diese diagonale Zahlenlinie projizieren, haben wir soeben eine Funktion definiert, die 2D-Vektoren in Zahlen umwandelt.",
  "model": "DeepL",
  "from_community_srt": "Wenn wir 2D-Vektoren direkt auf diese diagonale Zahlenlinie projizieren, Tatsächlich haben wir gerade eine Funktion definiert, die 2D-Vektoren zu Zahlen verarbeitet.",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "Außerdem ist diese Funktion tatsächlich linear, denn sie besteht unseren visuellen Test, dass jede Linie aus gleichmäßig verteilten Punkten gleichmäßig verteilt bleibt, sobald sie auf der Zahlenlinie landet.",
  "model": "DeepL",
  "from_community_srt": "Darüber hinaus ist diese Funktion tatsächlich linear da es unseren visuellen Test besteht dass jede Linie von gleichmäßig verteilten Punkten gleichmäßig verteilt bleibt,",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "Nur um das klarzustellen: Auch wenn ich die Zahlenreihe so in den 2D-Raum eingebettet habe, sind die Ausgaben der Funktion Zahlen und keine 2D-Vektoren.",
  "model": "DeepL",
  "from_community_srt": "sobald sie auf der Zahl landet Linie. Nur um das klar zu stellen, obwohl ich die Zahlenlinie so in den 2D-Raum eingebettet habe, Die Ausgabe der Funktion sind Zahlen,",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "Du solltest dir eine Funktion vorstellen, die zwei Koordinaten aufnimmt und eine einzige Koordinate ausgibt.",
  "model": "DeepL",
  "from_community_srt": "keine 2D-Vektoren. Sie sollten sich eine Funktion vorstellen, die Koordinaten aufnimmt und eine einzelne Koordinate ausgibt.",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "Aber dieser Vektor u-hat ist ein zweidimensionaler Vektor, der im Eingaberaum lebt.",
  "model": "DeepL",
  "from_community_srt": "Aber dieser Vektor u-hat ist ein zweidimensionaler Vektor im Eingaberaum leben.",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "Sie ist nur so angeordnet, dass sie sich mit der Einbettung der Zahlenreihe überschneidet.",
  "model": "DeepL",
  "from_community_srt": "Es ist nur so angeordnet, dass es sich mit der Einbettung der Zahlenlinie überschneidet.",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "Mit dieser Projektion haben wir gerade eine lineare Transformation von 2D-Vektoren in Zahlen definiert, also werden wir in der Lage sein, eine Art 1x2-Matrix zu finden, die diese Transformation beschreibt.",
  "model": "DeepL",
  "from_community_srt": "Mit dieser Projektion haben wir gerade eine lineare Transformation von 2D-Vektoren zu Zahlen definiert. Wir werden also in der Lage sein, eine Art 1 x 2-Matrix zu finden, die diese Transformation beschreibt.",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "Um diese 1x2-Matrix zu finden, zoomen wir auf die diagonale Zahlenreihe und überlegen uns, wo i-hat und j-hat jeweils landen, denn diese Landepunkte werden die Spalten der Matrix sein.",
  "model": "DeepL",
  "from_community_srt": "Um diese 1 x 2-Matrix zu finden, zoomen wir in diese diagonale Zahlenlinieneinstellung hinein und denke darüber nach, wo i-Dach und j-Dach jedes Land, da diese Landeplätze die Spalten der Matrix sein werden.",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "Dieser Teil ist super cool.",
  "model": "DeepL",
  "from_community_srt": "Dieser Teil ist super cool,",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "Wir können es mit einem wirklich eleganten Stück Symmetrie begründen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "Da i-hat und u-hat beide Einheitsvektoren sind, ist die Projektion von i-hat auf die Linie, die durch u-hat verläuft, völlig symmetrisch zur Projektion von u-hat auf die x-Achse.",
  "model": "DeepL",
  "from_community_srt": "wir können ihn mit einem wirklich eleganten Stück Symmetrie durchdenken: da i-Dach und u-Dach beide Einheitsvektoren sind, Projektion von i-hat auf die Linie durch u-hat sieht völlig symmetrisch aus, um den U-Dach auf der x-Achse zu schützen.",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "Wenn wir also fragen, auf welcher Zahl der i-Hut landet, wenn er auf die x-Achse projiziert wird, ist die Antwort die gleiche wie die, auf der der u-Hut landet, wenn er auf die x-Achse projiziert wird.",
  "model": "DeepL",
  "from_community_srt": "Als wir fragten, auf welcher Zahl landet i-Dach, wenn es projiziert wird Die Antwort wird die gleiche sein wie das, worauf U-Dach landet,",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "Die Projektion von u-hat auf die x-Achse bedeutet aber nur, dass du die x-Koordinate von u-hat nimmst.",
  "model": "DeepL",
  "from_community_srt": "wenn es auf das projiziert wird x-Achse aber U-Dach auf die x-Achse projizieren bedeutet nur, die x-Koordinate von u-Dach zu nehmen.",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "Durch die Symmetrie ist die Zahl, auf der i-hat landet, wenn man sie auf die diagonale Zahlenlinie projiziert, die x-Koordinate von u-hat.",
  "model": "DeepL",
  "from_community_srt": "Aus Symmetriegründen also die Zahl, bei der i-hat landet, wenn es auf diese diagonale Zahl projiziert wird Linie wird die x-Koordinate von u-Dach sein.",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "Ist das nicht cool?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "Die Argumentation ist im Fall des J-Hats fast identisch.",
  "model": "DeepL",
  "from_community_srt": "ist das nicht cool? Die Argumentation ist für den Fall j-hat fast identisch.",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "Denk mal einen Moment darüber nach.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "Aus denselben Gründen gibt die y-Koordinate von u-hat die Zahl an, auf der j-hat landet, wenn man sie auf die Zahlenreihe projiziert.",
  "model": "DeepL",
  "from_community_srt": "Denken Sie einen Moment darüber nach. Aus den gleichen Gründen ist die y-Koordinate von u-Dach gibt uns die Nummer an, an der j-Dach landet, wenn es auf die Zahlenzeilenkopie projiziert wird.",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "Halte inne und denke einen Moment darüber nach.",
  "model": "DeepL",
  "from_community_srt": "Halten Sie inne und denken Sie einen Moment darüber nach.",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "Ich finde das wirklich cool.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "Die Einträge der 1x2-Matrix, die die Projektionstransformation beschreibt, sind also die Koordinaten von u-hat.",
  "model": "DeepL",
  "from_community_srt": "Ich finde das einfach cool. Also die Einträge der 1 x 2 Matrix, die die Projektionstransformation beschreiben werden die Koordinaten von u-Dach sein.",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "Die Berechnung dieser Projektionstransformation für beliebige Vektoren im Raum, bei der diese Matrix mit den Vektoren multipliziert werden muss, ist rechnerisch identisch mit dem Punktprodukt mit u-hat.",
  "model": "DeepL",
  "from_community_srt": "Und Berechnung dieser Projektionstransformation für beliebige Vektoren im Raum, was erfordert, diese Matrix mit diesen Vektoren zu multiplizieren, ist rechnerisch identisch mit der Aufnahme eines Punktprodukts mit u-Dach.",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "Deshalb kann das Punktprodukt mit einem Einheitsvektor so interpretiert werden, dass man einen Vektor auf die Spannweite dieses Einheitsvektors projiziert und die Länge nimmt.",
  "model": "DeepL",
  "from_community_srt": "Aus diesem Grund wird das Punktprodukt mit einem Einheitsvektor genommen. kann so interpretiert werden, dass ein Vektor auf die Spanne dieses Einheitsvektors projiziert und genommen wird die Länge",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "Und was ist mit Nicht-Einheitsvektoren?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "Nehmen wir zum Beispiel den Einheitsvektor u-hat, aber wir skalieren ihn um den Faktor 3 hoch.",
  "model": "DeepL",
  "from_community_srt": "Was ist also mit Nicht-Einheitsvektoren? Zum Beispiel, Nehmen wir an, wir nehmen diesen Einheitsvektor-U-Dach. aber wir \"skalieren\" es um den Faktor 3.",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "Numerisch gesehen wird jede seiner Komponenten mit 3 multipliziert.",
  "model": "DeepL",
  "from_community_srt": "Numerisch wird jede seiner Komponenten mit 3 multipliziert.",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "Wenn du dir also die Matrix dieses Vektors ansiehst, steigen i-hat und j-hat auf das Dreifache der Werte, auf denen sie vorher gelandet sind.",
  "model": "DeepL",
  "from_community_srt": "Betrachten Sie also die mit diesem Vektor verknüpfte Matrix. i-Dach und j-Dach nehmen das 3-fache der Werte an,",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "Da dies alles linear ist, bedeutet es ganz allgemein, dass die neue Matrix so interpretiert werden kann, dass ein beliebiger Vektor auf die Kopie der Zahlenreihe projiziert und dort, wo er landet, mit 3 multipliziert wird.",
  "model": "DeepL",
  "from_community_srt": "bei denen sie zuvor gelandet sind. Da dies alles linear ist, es impliziert allgemeiner, dass die neue Matrix so interpretiert werden kann, dass jeder Vektor auf die Zahlenlinie projiziert wird Kopieren und multiplizieren,",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "Deshalb kann das Punktprodukt mit einem Nicht-Einheitsvektor so interpretiert werden, dass zuerst auf diesen Vektor projiziert und dann die Länge dieser Projektion mit der Länge des Vektors hochskaliert wird.",
  "model": "DeepL",
  "from_community_srt": "wo es landet, mit 3. Aus diesem Grund das Punktprodukt mit einem Nicht-Einheitsvektor kann als erste Projektion auf diesen Vektor interpretiert werden Skalieren Sie dann die Länge dieser Projektion um die Länge des Vektors.",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "Nimm dir einen Moment Zeit, um darüber nachzudenken, was hier passiert ist.",
  "model": "DeepL",
  "from_community_srt": "Nehmen Sie sich einen Moment Zeit, um darüber nachzudenken,",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "Wir hatten eine lineare Transformation vom 2D-Raum zur Zahlengeraden, die nicht durch numerische Vektoren oder numerische Punktprodukte definiert war, sondern einfach durch die Projektion des Raums auf eine diagonale Kopie der Zahlengeraden.",
  "model": "DeepL",
  "from_community_srt": "was hier passiert ist. Wir hatten eine lineare Transformation vom 2D-Raum zur Zahlenlinie. die nicht in Form von numerischen Vektoren oder numerischen Skalarprodukten definiert wurde. Es wurde nur durch Projizieren von Leerzeichen auf eine diagonale Kopie der Zahlenlinie definiert.",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "Da die Transformation aber linear ist, wurde sie notwendigerweise durch eine 1x2-Matrix beschrieben.",
  "model": "DeepL",
  "from_community_srt": "Aber weil die Transformation linear ist, es wurde notwendigerweise durch eine 1 x 2 Matrix beschrieben,",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "Und da die Multiplikation einer 1x2-Matrix mit einem 2D-Vektor dasselbe ist wie das Drehen dieser Matrix auf die Seite und die Bildung eines Punktprodukts, war diese Transformation unweigerlich mit einem 2D-Vektor verbunden.",
  "model": "DeepL",
  "from_community_srt": "und da eine 1 x 2-Matrix mit einem 2D-Vektor multipliziert wird ist das gleiche wie diese Matrix auf die Seite zu drehen und ein Skalarprodukt zu nehmen, Diese Transformation war unweigerlich mit einem 2D-Vektor verbunden.",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "Die Lektion hier ist, dass es bei jeder linearen Transformation, deren Ausgangsraum die Zahlengerade ist, egal wie sie definiert wurde, einen eindeutigen Vektor v gibt, der dieser Transformation entspricht, und zwar in dem Sinne, dass die Anwendung der Transformation dasselbe ist wie ein Punktprodukt mit diesem Vektor.",
  "model": "DeepL",
  "from_community_srt": "Die Lehre hier ist, dass Sie immer dann eine dieser linearen Transformationen haben dessen Ausgaberaum die Zahlenreihe ist, egal wie es definiert wurde, es wird einen eindeutigen Vektor v geben entsprechend dieser Transformation, in dem Sinne, dass das Anwenden der Transformation dasselbe ist wie das Nehmen eines Skalarprodukts mit diesem Vektor.",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "Für mich ist das wunderschön.",
  "model": "DeepL",
  "from_community_srt": "Für mich ist das absolut schön.",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "Das ist ein Beispiel für etwas, das man in der Mathematik Dualität nennt.",
  "model": "DeepL",
  "from_community_srt": "Es ist ein Beispiel für etwas in der Mathematik, das „Dualität“ genannt wird.",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "Die Dualität taucht in der Mathematik auf viele verschiedene Arten und Weisen auf, und es ist sehr schwierig, sie zu definieren.",
  "model": "DeepL",
  "from_community_srt": "\"Dualität\" zeigt sich in der gesamten Mathematik auf viele verschiedene Arten und Formen und es ist super schwierig, tatsächlich zu definieren.",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "Grob gesagt handelt es sich um Situationen, in denen es eine natürliche, aber überraschende Entsprechung zwischen zwei Arten von mathematischen Dingen gibt.",
  "model": "DeepL",
  "from_community_srt": "Im Grunde genommen bezieht es sich auf Situationen, in denen Sie eine natürliche, aber überraschende Entsprechung haben zwischen zwei Arten von mathematischen Dingen Für den Fall der linearen Algebra,",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "Im Fall der linearen Algebra, die du gerade kennengelernt hast, würdest du sagen, dass das Dual eines Vektors die lineare Transformation ist, die er kodiert, und das Dual einer linearen Transformation von einem Raum in eine Dimension ist ein bestimmter Vektor in diesem Raum.",
  "model": "DeepL",
  "from_community_srt": "den Sie gerade kennengelernt haben, Sie würden sagen, dass das „Dual“ eines Vektors die lineare Transformation ist, die er codiert. Und das Duale einer linearen Transformation vom Raum in eine Dimension, ist ein bestimmter Vektor in diesem Raum.",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "Zusammenfassend lässt sich sagen, dass das Punktprodukt oberflächlich betrachtet ein sehr nützliches geometrisches Werkzeug ist, um Projektionen zu verstehen und um zu prüfen, ob Vektoren in die gleiche Richtung zeigen oder nicht.",
  "model": "DeepL",
  "from_community_srt": "Zusammenfassend ist das Skalarprodukt also ein sehr nützliches geometrisches Werkzeug zum Verständnis Projektionen und zum Testen, ob Vektoren dazu neigen, in die gleiche Richtung zu zeigen oder nicht.",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "Und das ist wahrscheinlich das Wichtigste, was du dir über das Punktprodukt merken musst.",
  "model": "DeepL",
  "from_community_srt": "Und das ist wahrscheinlich das Wichtigste, an das Sie sich bei dem Skalarprodukt erinnern müssen.",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "Aber auf einer tieferen Ebene ist das Zusammenfügen von zwei Vektoren eine Möglichkeit, einen von ihnen in die Welt der Transformationen zu übertragen.",
  "model": "DeepL",
  "from_community_srt": "aber auf einer tieferen Ebene punktieren zwei Vektoren zusammen ist eine Möglichkeit,",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "Auch dies mag sich numerisch gesehen wie ein dummer Punkt anfühlen, den man betonen sollte.",
  "model": "DeepL",
  "from_community_srt": "einen von ihnen in die Welt der Transformationen zu übersetzen: Auch hier könnte sich dies numerisch wie ein dummer Punkt anfühlen,",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "Es sind nur zwei Berechnungen, die zufällig ähnlich aussehen.",
  "model": "DeepL",
  "from_community_srt": "den man hervorheben sollte: Es sind nur zwei Berechnungen, die ähnlich aussehen.",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "Der Grund, warum ich das so wichtig finde, ist, dass es in der gesamten Mathematik, wenn du mit einem Vektor zu tun hast, manchmal einfacher ist, ihn nicht als einen Pfeil im Raum zu verstehen, sondern als die physikalische Verkörperung einer linearen Transformation, wenn du seine Persönlichkeit kennenlernst.",
  "model": "DeepL",
  "from_community_srt": "Aber der Grund, warum ich das so wichtig finde, ist das während der gesamten Mathematik, wenn Sie mit einem Vektor zu tun haben, Sobald Sie seine Persönlichkeit wirklich Kennenlernen manchmal merkt man, dass es einfacher ist, es zu verstehen, nicht als Pfeil im Raum, sondern als physikalische Verkörperung einer linearen Transformation. Es ist,",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "Es ist, als ob der Vektor nur eine begriffliche Abkürzung für eine bestimmte Transformation ist, da es für uns einfacher ist, an Pfeile im Raum zu denken, als den gesamten Raum auf die Zahlenlinie zu übertragen.",
  "model": "DeepL",
  "from_community_srt": "als ob der Vektor wirklich nur eine konzeptionelle Abkürzung für eine bestimmte Transformation ist. da es für uns einfacher ist, über Pfeile und Leerzeichen nachzudenken anstatt den gesamten Raum auf die Zahlenlinie zu verschieben.",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.97
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "Im nächsten Video wirst du ein weiteres cooles Beispiel für diese Dualität in Aktion sehen, wenn ich über das Kreuzprodukt spreche.",
  "model": "DeepL",
  "from_community_srt": "Im nächsten Video sehen Sie ein weiteres wirklich cooles Beispiel für diese \"Dualität\" in Aktion",
  "n_reviews": 0,
  "start": 822.61,
  "end": 829.19
 }
]