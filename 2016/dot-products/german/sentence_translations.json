[
 {
  "input": "[\"Ode to Joy\", by Beethoven, plays to the end of the piano.] Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[„Ode an die Freude“ von Beethoven wird bis zum Ende des Klaviers gespielt.] Traditionell werden Skalarprodukte sehr früh in einem Kurs über lineare Algebra eingeführt, normalerweise gleich zu Beginn.",
  "model": "google_nmt",
  "from_community_srt": "klassischerweise, Skalarprodukt oder irgendwas, dass sehr früher in lineare Algebra eingeführt wurde kurs in der Regel gleich zu Beginn.",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "Daher mag es seltsam erscheinen, dass ich sie in der Serie so weit nach hinten verschoben habe.",
  "model": "google_nmt",
  "from_community_srt": "Es mag also seltsam erscheinen, dass ich sie in der Serie so weit zurückschiebe.",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "Ich habe dies getan, weil es eine Standardmethode zur Einführung in das Thema gibt, die lediglich ein grundlegendes Verständnis von Vektoren erfordert. Ein umfassenderes Verständnis der Rolle, die Skalarprodukte in der Mathematik spielen, kann jedoch nur im Licht linearer Transformationen wirklich gefunden werden.",
  "model": "google_nmt",
  "from_community_srt": "Ich habe dies getan, weil es einen Standardweg gibt, um das Thema einzuführen, welches erfordert nichts weiter als ein grundlegendes Verständnis der Vektoren, aber ein umfassenderes Verständnis der Rolle, die die Skalarprodukte in der Mathematik spielen, kann wirklich nur unter dem Licht der  linearen Transformationen gefunden werden.",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "Zuvor möchte ich jedoch kurz auf die übliche Art und Weise eingehen, wie Dot-Produkte eingeführt werden, was, wie ich annehme, für einige Zuschauer zumindest teilweise rezensiert ist.",
  "model": "google_nmt",
  "from_community_srt": "Lassen Sie mich jedoch vorher kurz darauf eingehen die Standardmethode für die Einführung von Produkten Welche ich annehme, ist zumindest teilweise eine Überprüfung für eine Reihe von Zuschauern.",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "Wenn Sie numerisch gesehen zwei Vektoren derselben Dimension und zwei Listen von Zahlen derselben Länge haben, bedeutet die Bildung ihres Skalarprodukts, alle Koordinaten zu paaren, diese Paare miteinander zu multiplizieren und das Ergebnis zu addieren.",
  "model": "google_nmt",
  "from_community_srt": "Numerisch, wenn Sie zwei Vektoren derselben Dimension haben; zur Liste von Zahlen mit der gleichen Länge, ihr Skalarprodukt zu nehmen bedeutet alle Koordinaten Paarweise koppeln, Multiplizieren dieser Paare zusammen,",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "Der Vektor 1, 2 gepunktet mit 3, 4 wäre also 1 mal 3 plus 2 mal 4.",
  "model": "google_nmt",
  "from_community_srt": "und deren Ergebnisse zusammen addieren so der Vektor [1,2] gepunktet mit [3,4] wäre 1 x 3 + 2 x 4.",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "Der Vektor 6, 2, 8, 3 gepunktet mit 1, 8, 5, 3 wäre 6 mal 1 plus 2 mal 8 plus 8 mal 5 plus 3 mal 3.",
  "model": "google_nmt",
  "from_community_srt": "der Vektor [6, 2, 8, 3] gepunktet mit [1, 8, 5, 3] wäre dann: 6 x 1 + 2 x 8 + 8 x 5 + 3 x 3",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "Glücklicherweise gibt es für diese Berechnung eine wirklich schöne geometrische Interpretation.",
  "model": "google_nmt",
  "from_community_srt": "Glücklicherweise hat diese Berechnung eine wirklich schöne geometrische Interpretation.",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "Um über das Skalarprodukt zwischen zwei Vektoren v und w nachzudenken, stellen Sie sich vor, w auf die Linie zu projizieren, die durch den Ursprung und die Spitze von v verläuft.",
  "model": "google_nmt",
  "from_community_srt": "Um über das Skalarprodukt zwischen zwei Vektoren v und w nachzudenken, Stellen Sie sich vor, Sie projizieren w auf die Linie, die durch den Ursprung und die Spitze von v verläuft.",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "Multipliziert man die Länge dieser Projektion mit der Länge von v, erhält man das Skalarprodukt v punkt w.",
  "model": "google_nmt",
  "from_community_srt": "Wenn Sie die Länge dieser Projektion mit der Länge von v multiplizieren, erhalten Sie das Skalarprodukt v・w.",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "Außer wenn diese Projektion von w in die entgegengesetzte Richtung von v zeigt, ist dieses Skalarprodukt tatsächlich negativ.",
  "model": "google_nmt",
  "from_community_srt": "Außer wenn diese Projektion von w in die entgegengesetzte Richtung von v zeigt, Dieses Skalarprodukt ist tatsächlich negativ.",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "Wenn also zwei Vektoren im Allgemeinen in die gleiche Richtung zeigen, ist ihr Skalarprodukt positiv.",
  "model": "google_nmt",
  "from_community_srt": "Wenn also zwei Vektoren im Allgemeinen in die gleiche Richtung zeigen, ihr Skalarprodukt ist positiv.",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "Wenn sie senkrecht stehen, was bedeutet, dass die Projektion des einen auf den anderen der Nullvektor ist, ist ihr Skalarprodukt Null.",
  "model": "google_nmt",
  "from_community_srt": "Wenn sie senkrecht sind, bedeutet das die Projektion von einem auf das andere ist der 0-Vektor, das Skalarprodukt ist 0",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "Und wenn sie generell in die entgegengesetzte Richtung zeigen, ist ihr Skalarprodukt negativ.",
  "model": "google_nmt",
  "from_community_srt": "Und wenn sie im Allgemeinen in die entgegengesetzte Richtung zeigen,",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "Nun, diese Interpretation ist seltsam asymmetrisch.",
  "model": "google_nmt",
  "from_community_srt": "ist ihr Skalarprodukt negativ. Nun ist diese Interpretation seltsam asymmetrisch, es behandelt die beiden Vektoren sehr unterschiedlich,",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "Es behandelt die beiden Vektoren sehr unterschiedlich.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "Als ich das zum ersten Mal erfuhr, war ich überrascht, dass die Reihenfolge keine Rolle spielt.",
  "model": "google_nmt",
  "from_community_srt": "Als ich das zum ersten Mal erfuhr, war ich überrascht,",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "Sie könnten stattdessen v auf w projizieren, die Länge des projizierten v mit der Länge von w multiplizieren und das gleiche Ergebnis erhalten.",
  "model": "google_nmt",
  "from_community_srt": "dass die Reihenfolge keine Rolle spielt. Sie könnten stattdessen v auf w projizieren; Sie könnten stattdessen v auf w projizieren; und erhalten Sie das gleiche Ergebnis.",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "Ich meine, fühlt sich das nicht wie ein ganz anderer Prozess an?",
  "model": "google_nmt",
  "from_community_srt": "Ich meine,",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "Hier erfahren Sie, warum Ordnung keine Rolle spielt.",
  "model": "google_nmt",
  "from_community_srt": "fühlt sich das nicht nach einem wirklich anderen Prozess an? Hier ist die Intuition,",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "Wenn v und w zufällig die gleiche Länge hätten, könnten wir eine gewisse Symmetrie nutzen.",
  "model": "google_nmt",
  "from_community_srt": "warum Ordnung keine Rolle spielt: wenn v und w zufällig die gleiche Länge haben,",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "Da die Projektion von w auf v und die anschließende Multiplikation der Länge dieser Projektion mit der Länge von v ein vollständiges Spiegelbild der Projektion von v auf w und der anschließenden Multiplikation der Länge dieser Projektion mit der Länge von w ist.",
  "model": "google_nmt",
  "from_community_srt": "Wir könnten eine gewisse Symmetrie nutzen. Da w auf v projiziert wird dann multipliziere die Länge dieser Projektion mit der Länge von v, ist ein vollständiges Spiegelbild der Projektion von v auf w und der Multiplikation der Länge davon Projektion um die Länge von w.",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "Wenn Sie nun eines davon, sagen wir v, um eine Konstante wie 2 skalieren, sodass sie nicht die gleiche Länge haben, wird die Symmetrie gebrochen.",
  "model": "google_nmt",
  "from_community_srt": "Wenn Sie nun einen von den „skalieren“, sagen Sie v mit einer Konstanten wie 2, damit sie nicht gleich lang sind, Die Symmetrie ist gebrochen.",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "Aber lassen Sie uns darüber nachdenken, wie das Skalarprodukt zwischen diesem neuen Vektor, 2 mal v, und w zu interpretieren ist.",
  "model": "google_nmt",
  "from_community_srt": "Aber lassen Sie uns überlegen, wie das Skalarprodukt zwischen diesem neuen Vektor 2v und zu interpretieren ist w Wenn Sie an w denken,",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "Wenn Sie sich vorstellen, dass w auf v projiziert wird, dann ist das Skalarprodukt 2v Punkt w genau das Doppelte des Skalarprodukts v Punkt w.",
  "model": "google_nmt",
  "from_community_srt": "wird w auf v projiziert dann ist das Skalarprodukt 2v ・ w genau doppelt so groß wie das Skalarprodukt v ・ w.",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "Dies liegt daran, dass sich bei der Skalierung von v um 2 nicht die Länge der Projektion von w ändert, sondern die Länge des Vektors, auf den Sie projizieren, verdoppelt wird.",
  "model": "google_nmt",
  "from_community_srt": "Dies liegt daran, dass, wenn Sie v um 2 „skalieren“, es ändert nicht die Länge der Projektion von w Es verdoppelt jedoch die Länge des Vektors, auf den Sie projizieren.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "Nehmen wir andererseits an, Sie denken darüber nach, v auf w zu projizieren.",
  "model": "google_nmt",
  "from_community_srt": "Nehmen wir andererseits an, Sie denken darüber nach, ob v auf w projiziert wird.",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "Nun, in diesem Fall wird die Länge der Projektion skaliert, wenn wir v mit 2 multiplizieren, aber die Länge des Vektors, auf den Sie projizieren, bleibt konstant.",
  "model": "google_nmt",
  "from_community_srt": "Nun, in diesem Fall ist die Länge der Projektion die Sache, die \"skaliert\" werden muss, wenn wir multiplizieren v um 2. Die Länge des Vektors, auf den Sie projizieren,",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "Der Gesamteffekt besteht also immer noch darin, dass sich das Skalarprodukt lediglich verdoppelt.",
  "model": "google_nmt",
  "from_community_srt": "bleibt konstant. Der Gesamteffekt besteht also immer noch darin, das Skalarprodukt nur zu verdoppeln.",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "Auch wenn in diesem Fall die Symmetrie gebrochen ist, ist der Effekt, den diese Skalierung auf den Wert des Skalarprodukts hat, bei beiden Interpretationen derselbe.",
  "model": "google_nmt",
  "from_community_srt": "Also, obwohl die Symmetrie in diesem Fall gebrochen ist, Der Effekt, den diese „Skalierung“ auf den Wert des Skalarprodukt hat,",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "Es gibt noch eine andere große Frage, die mich verwirrte, als ich das erste Mal davon erfuhr.",
  "model": "google_nmt",
  "from_community_srt": "ist der gleiche unter beiden Interpretationen. Es gibt noch eine andere große Frage, die mich verwirrte,",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "Warum zum Teufel hat dieser numerische Prozess, bei dem Koordinaten abgeglichen, Paare multipliziert und addiert werden, irgendetwas mit Projektion zu tun?",
  "model": "google_nmt",
  "from_community_srt": "als ich dieses Zeug zum ersten Mal lernte: Warum um alles in der Welt führt dieser numerische Prozess das Abgleichen von Koordinaten, das Multiplizieren von Paaren und addieren sie zusammen, Haben Sie etwas mit Projektion zu tun?",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "Nun, um eine zufriedenstellende Antwort zu geben und auch um der Bedeutung des Skalarprodukts voll und ganz gerecht zu werden, müssen wir hier etwas etwas tiefergehendes ans Licht bringen, das oft als Dualität bezeichnet wird.",
  "model": "google_nmt",
  "from_community_srt": "Nun, um eine zufriedenstellende Antwort zu geben, und auch um der Bedeutung des Skalarprodukt voll gerecht zu werden, Wir müssen hier etwas Tieferes entdecken was oft unter dem Namen \"Dualität\" bekannt ist.",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "Aber bevor ich darauf eingehe, muss ich etwas Zeit damit verbringen, über lineare Transformationen von mehreren Dimensionen in eine Dimension zu sprechen, die einfach die Zahlenlinie ist.",
  "model": "google_nmt",
  "from_community_srt": "Aber bevor wir darauf eingehen, Ich muss einige Zeit damit verbringen, über lineare Transformationen zu sprechen von mehreren Dimensionen zu einer Dimension welche nur die Zahlenreihe ist.",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "Dies sind Funktionen, die einen 2D-Vektor aufnehmen und eine bestimmte Zahl ausspucken, aber lineare Transformationen sind natürlich viel eingeschränkter als Ihre gewöhnliche Funktion mit einer 2D-Eingabe und einer 1D-Ausgabe.",
  "model": "google_nmt",
  "from_community_srt": "Dies sind Funktionen, die einen 2D-Vektor aufnehmen und eine Zahl ausspucken. Aber lineare Transformationen sind natürlich viel eingeschränkter als Ihre normale Funktion mit einem 2D-Eingabe und einem 1D-Ausgabe.",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "Wie bei Transformationen in höheren Dimensionen, wie denen, über die ich in Kapitel 3 gesprochen habe, gibt es einige formale Eigenschaften, die diese Funktionen linear machen, aber ich werde diese hier absichtlich ignorieren, um nicht von unserem Endziel abzulenken, und stattdessen Konzentrieren Sie sich auf eine bestimmte visuelle Eigenschaft, die allen formalen Dingen entspricht.",
  "model": "google_nmt",
  "from_community_srt": "Wie bei Transformationen in höheren Dimensionen, wie die, über die ich in Kapitel 3 gesprochen habe, Es gibt einige formale Eigenschaften, die diese Funktionen linear machen. Aber ich werde diese hier absichtlich ignorieren, um nicht von unserem Endziel abzulenken. und konzentrieren Sie sich stattdessen auf eine bestimmte visuelle Eigenschaft,",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "Wenn Sie eine Linie aus gleichmäßig verteilten Punkten nehmen und eine Transformation anwenden, sorgt eine lineare Transformation dafür, dass diese Punkte gleichmäßig verteilt bleiben, sobald sie im Ausgaberaum, der Zahlenlinie, landen.",
  "model": "google_nmt",
  "from_community_srt": "die allen formalen Dingen entspricht. Wenn Sie eine Reihe gleichmäßig verteilter Punkte nehmen und eine Transformation anwenden, Eine lineare Transformation hält diese Punkte gleichmäßig verteilt. Sobald sie im Ausgabebereich landen, ist dies die Zahlenreihe.",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "Andernfalls ist Ihre Transformation nicht linear, wenn es eine Reihe von Punkten gibt, die ungleichmäßig verteilt sind.",
  "model": "google_nmt",
  "from_community_srt": "Andernfalls, wenn eine Punktreihe ungleichmäßig verteilt ist dann ist deine Transformation nicht linear.",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "Wie bei den Fällen, die wir zuvor gesehen haben, wird eine dieser linearen Transformationen vollständig davon bestimmt, wo i-hat und j-hat ankommen, aber dieses Mal landet jeder dieser Basisvektoren einfach auf einer Zahl, wenn wir also aufzeichnen, wo Sie bilden die Spalten einer Matrix, wobei jede dieser Spalten nur eine einzelne Zahl hat.",
  "model": "google_nmt",
  "from_community_srt": "Wie bei den Fällen, die wir zuvor gesehen haben, eine dieser linearen Transformationen wird vollständig davon bestimmt, wohin es i-dach und j-dach nimmt Aber diesmal landet jeder dieser Basisvektoren nur auf einer Zahl. Jede dieser Spalten hat nur eine einzige Nummer.",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "Dies ist eine 1x2-Matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "Lassen Sie uns anhand eines Beispiels durchgehen, was es bedeutet, eine dieser Transformationen auf einen Vektor anzuwenden.",
  "model": "google_nmt",
  "from_community_srt": "Dies ist eine 1 x 2-Matrix. Lassen Sie uns ein Beispiel durchgehen, was es bedeutet, eine dieser Transformationen auf a anzuwenden Vektor.",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "Nehmen wir an, Sie haben eine lineare Transformation, die i-hat auf 1 und j-hat auf minus 2 bringt.",
  "model": "google_nmt",
  "from_community_srt": "Angenommen, Sie haben eine lineare Transformation, die i-dach zu 1 und j-dach zu -2 führt.",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "Um zu verfolgen, wo ein Vektor mit den Koordinaten, sagen wir, 4, 3 endet, stellen Sie sich vor, diesen Vektor in 4 mal i-hat plus 3 mal j-hat aufzuteilen.",
  "model": "google_nmt",
  "from_community_srt": "Um zu folgen, wo ein Vektor mit Koordinaten endet, sagen wir [4, 3], Stellen Sie sich vor, Sie zerlegen diesen Vektor als 4-mal i-dach + 3-mal j-dach.",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "Eine Folge der Linearität ist, dass der Vektor nach der Transformation viermal so groß ist wie die Stelle, an der i-hat landet, 1, plus dreimal so groß wie die Stelle, an der j-hat landet, negativ 2, was in diesem Fall bedeutet, dass er auf negativ landet 2.",
  "model": "google_nmt",
  "from_community_srt": "Eine Folge der Linearität ist die nach der Transformation Der Vektor ist: 4 mal der Ort, an dem i-Dach landet, 1, plus 3 mal die Stelle, an der j-Dach landet, -2. was in diesem Fall impliziert,",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "Wenn Sie diese Berechnung rein numerisch durchführen, handelt es sich um eine Matrixvektormultiplikation.",
  "model": "google_nmt",
  "from_community_srt": "dass es auf -2 landet. Wenn Sie diese Berechnung rein numerisch durchführen,",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "Nun fühlt sich diese numerische Operation der Multiplikation einer 1x2-Matrix mit einem Vektor an, als würde man das Skalarprodukt zweier Vektoren bilden.",
  "model": "google_nmt",
  "from_community_srt": "handelt es sich um eine Matrix-Vektor-Multiplikation. Diese numerische Operation zum Multiplizieren einer 1 mit 2-Matrix mit einem Vektor fühlt sich an, als würde man das Skalarprodukt zweier Vektoren nehmen.",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "Sieht diese 1x2-Matrix nicht einfach wie ein Vektor aus, den wir auf die Seite gedreht haben?",
  "model": "google_nmt",
  "from_community_srt": "Sieht diese 1 x 2-Matrix nicht wie ein Vektor aus,",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "Tatsächlich könnten wir jetzt sagen, dass es einen schönen Zusammenhang zwischen 1x2-Matrizen und 2D-Vektoren gibt, der dadurch definiert wird, dass man die numerische Darstellung eines Vektors auf die Seite neigt, um die zugehörige Matrix zu erhalten, oder indem man die Matrix wieder nach oben neigt, um den zugehörigen Vektor zu erhalten .",
  "model": "google_nmt",
  "from_community_srt": "den wir auf die Seite gekippt haben? Tatsächlich könnten wir jetzt sagen, dass es eine schöne Assoziation zwischen 1 x 2 Matrizen gibt und 2D-Vektoren, definiert durch Kippen der numerischen Darstellung eines Vektors auf seiner Seite, um die zugehörige zu erhalten matrix, oder um die Matrix wieder nach oben zu kippen,",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "Da wir uns im Moment nur mit numerischen Ausdrücken befassen, könnte es albern erscheinen, zwischen Vektoren und 1x2-Matrizen hin und her zu wechseln.",
  "model": "google_nmt",
  "from_community_srt": "um den zugehörigen Vektor zu erhalten. Da wir uns gerade nur mit numerischen Ausdrücken befassen, Das Hin- und Hergehen zwischen Vektoren und 1 x 2 Matrizen könnte sich wie eine dumme Sache anfühlen",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "Aber das deutet auf etwas hin, das aus geometrischer Sicht wirklich großartig ist.",
  "model": "google_nmt",
  "from_community_srt": "machen Dies deutet jedoch auf etwas hin,",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "Es gibt eine Art Zusammenhang zwischen linearen Transformationen, die Vektoren in Zahlen umwandeln, und den Vektoren selbst.",
  "model": "google_nmt",
  "from_community_srt": "das aus geometrischer Sicht wirklich beeindruckend ist: Es gibt eine Art Verbindung zwischen linearen Transformationen,",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "Lassen Sie mich ein Beispiel zeigen, das die Bedeutung verdeutlicht und zufällig auch das Skalarprodukt-Rätsel von vorhin löst.",
  "model": "google_nmt",
  "from_community_srt": "die Vektoren zu Zahlen führen und Vektoren selbst. Lassen Sie mich ein Beispiel zeigen, das die Bedeutung verdeutlicht und was zufällig auch das Skalarprodukt-Puzzle von früher beantwortet.",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "Verlernen Sie, was Sie gelernt haben, und stellen Sie sich vor, Sie wüssten noch nicht, dass sich das Punktprodukt auf die Projektion bezieht.",
  "model": "google_nmt",
  "from_community_srt": "Verlernen Sie, was Sie gelernt haben und stellen Sie sich vor, Sie wissen noch nicht,",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "Was ich hier tun werde, ist, eine Kopie der Zahlenlinie zu nehmen und sie irgendwie diagonal im Raum zu platzieren, wobei die Zahl 0 im Ursprung liegt.",
  "model": "google_nmt",
  "from_community_srt": "dass sich das Skalarprodukt auf die Projektion bezieht. Was ich hier tun werde, ist eine Kopie der Zahlenreihe zu nehmen und platziere es diagonal und platziere es irgendwie mit der Zahl 0 am Ursprung.",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "Stellen Sie sich nun den zweidimensionalen Einheitsvektor vor, dessen Spitze dort sitzt, wo die Zahl 1 auf der Zahl ist.",
  "model": "google_nmt",
  "from_community_srt": "Denken Sie nun an den zweidimensionalen Einheitsvektor, deren Spitzen sitzen dort,",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "Ich möchte diesem Kerl einen Namen geben, na ja.",
  "model": "google_nmt",
  "from_community_srt": "wo die Nummer 1 auf der Zahlenlinie ist Ich möchte diesem Kerl einen Namen geben.",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "Dieser kleine Kerl spielt eine wichtige Rolle in dem, was passieren wird, also behalten Sie ihn im Hinterkopf.",
  "model": "google_nmt",
  "from_community_srt": "Dieser kleine Kerl spielt eine wichtige Rolle in dem, was passieren wird. Behalte sie also einfach im Hinterkopf.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "Wenn wir 2D-Vektoren direkt auf diese diagonale Zahlenlinie projizieren, haben wir praktisch gerade eine Funktion definiert, die 2D-Vektoren in Zahlen umwandelt.",
  "model": "google_nmt",
  "from_community_srt": "Wenn wir 2D-Vektoren direkt auf diese diagonale Zahlenlinie projizieren, Tatsächlich haben wir gerade eine Funktion definiert, die 2D-Vektoren zu Zahlen verarbeitet.",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "Darüber hinaus ist diese Funktion tatsächlich linear, da sie unseren visuellen Test besteht, dass jede Linie aus gleichmäßig verteilten Punkten gleichmäßig verteilt bleibt, sobald sie auf der Zahlenlinie landet.",
  "model": "google_nmt",
  "from_community_srt": "Darüber hinaus ist diese Funktion tatsächlich linear da es unseren visuellen Test besteht dass jede Linie von gleichmäßig verteilten Punkten gleichmäßig verteilt bleibt,",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "Um es klarzustellen: Auch wenn ich die Zahlenlinie auf diese Weise in den 2D-Raum eingebettet habe, sind die Ausgaben der Funktion Zahlen und keine 2D-Vektoren.",
  "model": "google_nmt",
  "from_community_srt": "sobald sie auf der Zahl landet Linie. Nur um das klar zu stellen, obwohl ich die Zahlenlinie so in den 2D-Raum eingebettet habe, Die Ausgabe der Funktion sind Zahlen,",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "Sie sollten sich eine Funktion vorstellen, die zwei Koordinaten aufnimmt und eine einzelne Koordinate ausgibt.",
  "model": "google_nmt",
  "from_community_srt": "keine 2D-Vektoren. Sie sollten sich eine Funktion vorstellen, die Koordinaten aufnimmt und eine einzelne Koordinate ausgibt.",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "Aber dieser Vektor u-hat ist ein zweidimensionaler Vektor, der im Eingaberaum lebt.",
  "model": "google_nmt",
  "from_community_srt": "Aber dieser Vektor u-hat ist ein zweidimensionaler Vektor im Eingaberaum leben.",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "Es ist lediglich so angeordnet, dass es sich mit der Einbettung des Zahlenstrahls überschneidet.",
  "model": "google_nmt",
  "from_community_srt": "Es ist nur so angeordnet, dass es sich mit der Einbettung der Zahlenlinie überschneidet.",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "Mit dieser Projektion haben wir gerade eine lineare Transformation von 2D-Vektoren in Zahlen definiert, sodass wir eine Art 1x2-Matrix finden können, die diese Transformation beschreibt.",
  "model": "google_nmt",
  "from_community_srt": "Mit dieser Projektion haben wir gerade eine lineare Transformation von 2D-Vektoren zu Zahlen definiert. Wir werden also in der Lage sein, eine Art 1 x 2-Matrix zu finden, die diese Transformation beschreibt.",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "Um diese 1x2-Matrix zu finden, vergrößern wir den Aufbau dieser diagonalen Zahlenlinie und überlegen, wo i-hat und j-hat jeweils landen, da diese Landepunkte die Spalten der Matrix sein werden.",
  "model": "google_nmt",
  "from_community_srt": "Um diese 1 x 2-Matrix zu finden, zoomen wir in diese diagonale Zahlenlinieneinstellung hinein und denke darüber nach, wo i-Dach und j-Dach jedes Land, da diese Landeplätze die Spalten der Matrix sein werden.",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "Dieser Teil ist super cool.",
  "model": "google_nmt",
  "from_community_srt": "Dieser Teil ist super cool,",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "Wir können es mit einem wirklich eleganten Stück Symmetrie durchdenken.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "Da i-hat und u-hat beide Einheitsvektoren sind, sieht die Projektion von i-hat auf die durch u-hat verlaufende Linie völlig symmetrisch zur Projektion von u-hat auf die x-Achse aus.",
  "model": "google_nmt",
  "from_community_srt": "wir können ihn mit einem wirklich eleganten Stück Symmetrie durchdenken: da i-Dach und u-Dach beide Einheitsvektoren sind, Projektion von i-hat auf die Linie durch u-hat sieht völlig symmetrisch aus, um den U-Dach auf der x-Achse zu schützen.",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "Wenn wir also fragen, auf welcher Zahl der i-hat landet, wenn er projiziert wird, wird die Antwort dieselbe sein wie bei der Zahl, auf der der u-hat landet, wenn er auf die x-Achse projiziert wird.",
  "model": "google_nmt",
  "from_community_srt": "Als wir fragten, auf welcher Zahl landet i-Dach, wenn es projiziert wird Die Antwort wird die gleiche sein wie das, worauf U-Dach landet,",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "Aber das Projizieren von U-Hat auf die X-Achse bedeutet lediglich, dass man die X-Koordinate von U-Hat nimmt.",
  "model": "google_nmt",
  "from_community_srt": "wenn es auf das projiziert wird x-Achse aber U-Dach auf die x-Achse projizieren bedeutet nur, die x-Koordinate von u-Dach zu nehmen.",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "Aus Symmetriegründen ist die Zahl, auf der i-hat landet, wenn es auf die diagonale Zahlenlinie projiziert wird, die x-Koordinate von u-hat.",
  "model": "google_nmt",
  "from_community_srt": "Aus Symmetriegründen also die Zahl, bei der i-hat landet, wenn es auf diese diagonale Zahl projiziert wird Linie wird die x-Koordinate von u-Dach sein.",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "Ist das nicht cool?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "Die Argumentation ist für den J-Hat-Fall nahezu identisch.",
  "model": "google_nmt",
  "from_community_srt": "ist das nicht cool? Die Argumentation ist für den Fall j-hat fast identisch.",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "Denken Sie einen Moment darüber nach.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "Aus den gleichen Gründen gibt uns die y-Koordinate von u-hat die Zahl, wo j-hat landet, wenn es auf die Nummernlinienkopie projiziert wird.",
  "model": "google_nmt",
  "from_community_srt": "Denken Sie einen Moment darüber nach. Aus den gleichen Gründen ist die y-Koordinate von u-Dach gibt uns die Nummer an, an der j-Dach landet, wenn es auf die Zahlenzeilenkopie projiziert wird.",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "Halten Sie inne und denken Sie einen Moment darüber nach.",
  "model": "google_nmt",
  "from_community_srt": "Halten Sie inne und denken Sie einen Moment darüber nach.",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "Ich finde das einfach wirklich cool.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "Die Einträge der 1x2-Matrix, die die Projektionstransformation beschreibt, werden also die Koordinaten von u-hat sein.",
  "model": "google_nmt",
  "from_community_srt": "Ich finde das einfach cool. Also die Einträge der 1 x 2 Matrix, die die Projektionstransformation beschreiben werden die Koordinaten von u-Dach sein.",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "Und die Berechnung dieser Projektionstransformation für beliebige Vektoren im Raum, die die Multiplikation dieser Matrix mit diesen Vektoren erfordert, ist rechnerisch identisch mit der Berechnung eines Skalarprodukts mit U-Hat.",
  "model": "google_nmt",
  "from_community_srt": "Und Berechnung dieser Projektionstransformation für beliebige Vektoren im Raum, was erfordert, diese Matrix mit diesen Vektoren zu multiplizieren, ist rechnerisch identisch mit der Aufnahme eines Punktprodukts mit u-Dach.",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "Aus diesem Grund kann die Bildung des Skalarprodukts mit einem Einheitsvektor so interpretiert werden, als würde man einen Vektor auf die Spanne dieses Einheitsvektors projizieren und die Länge nehmen.",
  "model": "google_nmt",
  "from_community_srt": "Aus diesem Grund wird das Punktprodukt mit einem Einheitsvektor genommen. kann so interpretiert werden, dass ein Vektor auf die Spanne dieses Einheitsvektors projiziert und genommen wird die Länge",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "Was ist also mit Nicht-Einheitsvektoren?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "Nehmen wir zum Beispiel an, wir nehmen den Einheitsvektor U-Hat, skalieren ihn aber um den Faktor 3.",
  "model": "google_nmt",
  "from_community_srt": "Was ist also mit Nicht-Einheitsvektoren? Zum Beispiel, Nehmen wir an, wir nehmen diesen Einheitsvektor-U-Dach. aber wir \"skalieren\" es um den Faktor 3.",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "Numerisch wird jede seiner Komponenten mit 3 multipliziert.",
  "model": "google_nmt",
  "from_community_srt": "Numerisch wird jede seiner Komponenten mit 3 multipliziert.",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "Wenn man sich also die Matrix ansieht, die diesem Vektor zugeordnet ist, nimmt i-hat und j-hat das Dreifache der Werte an, die sie zuvor erreicht haben.",
  "model": "google_nmt",
  "from_community_srt": "Betrachten Sie also die mit diesem Vektor verknüpfte Matrix. i-Dach und j-Dach nehmen das 3-fache der Werte an,",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "Da dies alles linear ist, impliziert dies allgemeiner, dass die neue Matrix so interpretiert werden kann, dass sie einen beliebigen Vektor auf die Kopie der Zahlenlinie projiziert und dort, wo er landet, mit 3 multipliziert.",
  "model": "google_nmt",
  "from_community_srt": "bei denen sie zuvor gelandet sind. Da dies alles linear ist, es impliziert allgemeiner, dass die neue Matrix so interpretiert werden kann, dass jeder Vektor auf die Zahlenlinie projiziert wird Kopieren und multiplizieren,",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "Aus diesem Grund kann das Skalarprodukt mit einem Nicht-Einheitsvektor so interpretiert werden, dass es zunächst auf diesen Vektor projiziert und dann die Länge dieser Projektion um die Länge des Vektors vergrößert.",
  "model": "google_nmt",
  "from_community_srt": "wo es landet, mit 3. Aus diesem Grund das Punktprodukt mit einem Nicht-Einheitsvektor kann als erste Projektion auf diesen Vektor interpretiert werden Skalieren Sie dann die Länge dieser Projektion um die Länge des Vektors.",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "Nehmen Sie sich einen Moment Zeit, darüber nachzudenken, was hier passiert ist.",
  "model": "google_nmt",
  "from_community_srt": "Nehmen Sie sich einen Moment Zeit, um darüber nachzudenken,",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "Wir hatten eine lineare Transformation vom zweidimensionalen Raum zur Zahlenlinie, die nicht durch numerische Vektoren oder numerische Skalarprodukte definiert wurde, sondern lediglich durch die Projektion des Raums auf eine diagonale Kopie der Zahlenlinie.",
  "model": "google_nmt",
  "from_community_srt": "was hier passiert ist. Wir hatten eine lineare Transformation vom 2D-Raum zur Zahlenlinie. die nicht in Form von numerischen Vektoren oder numerischen Skalarprodukten definiert wurde. Es wurde nur durch Projizieren von Leerzeichen auf eine diagonale Kopie der Zahlenlinie definiert.",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "Da die Transformation jedoch linear ist, wurde sie notwendigerweise durch eine 1x2-Matrix beschrieben.",
  "model": "google_nmt",
  "from_community_srt": "Aber weil die Transformation linear ist, es wurde notwendigerweise durch eine 1 x 2 Matrix beschrieben,",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "Und da das Multiplizieren einer 1x2-Matrix mit einem 2D-Vektor dasselbe ist, als würde man diese Matrix auf die Seite drehen und ein Skalarprodukt bilden, hing diese Transformation unweigerlich mit einem 2D-Vektor zusammen.",
  "model": "google_nmt",
  "from_community_srt": "und da eine 1 x 2-Matrix mit einem 2D-Vektor multipliziert wird ist das gleiche wie diese Matrix auf die Seite zu drehen und ein Skalarprodukt zu nehmen, Diese Transformation war unweigerlich mit einem 2D-Vektor verbunden.",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "Die Lektion hier ist, dass es jedes Mal, wenn Sie eine dieser linearen Transformationen haben, deren Ausgaberaum die Zahlenlinie ist, unabhängig davon, wie sie definiert wurde, einen eindeutigen Vektor v geben wird, der dieser Transformation entspricht, in dem Sinne, wie es bei der Anwendung der Transformation der Fall ist das Gleiche, als würde man mit diesem Vektor ein Skalarprodukt bilden.",
  "model": "google_nmt",
  "from_community_srt": "Die Lehre hier ist, dass Sie immer dann eine dieser linearen Transformationen haben dessen Ausgaberaum die Zahlenreihe ist, egal wie es definiert wurde, es wird einen eindeutigen Vektor v geben entsprechend dieser Transformation, in dem Sinne, dass das Anwenden der Transformation dasselbe ist wie das Nehmen eines Skalarprodukts mit diesem Vektor.",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "Für mich ist das absolut schön.",
  "model": "google_nmt",
  "from_community_srt": "Für mich ist das absolut schön.",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "Es ist ein Beispiel für etwas in der Mathematik, das Dualität genannt wird.",
  "model": "google_nmt",
  "from_community_srt": "Es ist ein Beispiel für etwas in der Mathematik, das „Dualität“ genannt wird.",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "Dualität taucht in der Mathematik auf viele verschiedene Arten und Formen auf und es ist äußerst schwierig, sie tatsächlich zu definieren.",
  "model": "google_nmt",
  "from_community_srt": "\"Dualität\" zeigt sich in der gesamten Mathematik auf viele verschiedene Arten und Formen und es ist super schwierig, tatsächlich zu definieren.",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "Grob gesagt bezieht es sich auf Situationen, in denen es eine natürliche, aber überraschende Entsprechung zwischen zwei Arten mathematischer Dinge gibt.",
  "model": "google_nmt",
  "from_community_srt": "Im Grunde genommen bezieht es sich auf Situationen, in denen Sie eine natürliche, aber überraschende Entsprechung haben zwischen zwei Arten von mathematischen Dingen Für den Fall der linearen Algebra,",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "Für den Fall der linearen Algebra, den Sie gerade kennengelernt haben, würden Sie sagen, dass das Dual eines Vektors die lineare Transformation ist, die er kodiert, und dass das Dual einer linearen Transformation von einem Raum in eine Dimension ein bestimmter Vektor in diesem Raum ist.",
  "model": "google_nmt",
  "from_community_srt": "den Sie gerade kennengelernt haben, Sie würden sagen, dass das „Dual“ eines Vektors die lineare Transformation ist, die er codiert. Und das Duale einer linearen Transformation vom Raum in eine Dimension, ist ein bestimmter Vektor in diesem Raum.",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "Zusammenfassend lässt sich sagen, dass das Skalarprodukt oberflächlich betrachtet ein sehr nützliches geometrisches Werkzeug zum Verständnis von Projektionen und zum Testen ist, ob Vektoren dazu neigen, in die gleiche Richtung zu zeigen.",
  "model": "google_nmt",
  "from_community_srt": "Zusammenfassend ist das Skalarprodukt also ein sehr nützliches geometrisches Werkzeug zum Verständnis Projektionen und zum Testen, ob Vektoren dazu neigen, in die gleiche Richtung zu zeigen oder nicht.",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "Und das ist wahrscheinlich das Wichtigste, was Sie beim Skalarprodukt beachten sollten.",
  "model": "google_nmt",
  "from_community_srt": "Und das ist wahrscheinlich das Wichtigste, an das Sie sich bei dem Skalarprodukt erinnern müssen.",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "Aber auf einer tieferen Ebene ist die Punktierung zweier Vektoren eine Möglichkeit, einen von ihnen in die Welt der Transformationen zu übertragen.",
  "model": "google_nmt",
  "from_community_srt": "aber auf einer tieferen Ebene punktieren zwei Vektoren zusammen ist eine Möglichkeit,",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "Auch hier könnte es sich zahlenmäßig wie ein alberner Punkt anfühlen, den man hervorheben sollte.",
  "model": "google_nmt",
  "from_community_srt": "einen von ihnen in die Welt der Transformationen zu übersetzen: Auch hier könnte sich dies numerisch wie ein dummer Punkt anfühlen,",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "Es sind nur zwei Berechnungen, die zufällig ähnlich aussehen.",
  "model": "google_nmt",
  "from_community_srt": "den man hervorheben sollte: Es sind nur zwei Berechnungen, die ähnlich aussehen.",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "Aber der Grund, warum ich das so wichtig finde, ist, dass man in der gesamten Mathematik, wenn man mit einem Vektor zu tun hat und seine Persönlichkeit erst einmal wirklich kennengelernt hat, manchmal erkennt, dass es einfacher ist, ihn nicht als Pfeil im Raum, sondern als zu verstehen physikalische Verkörperung einer linearen Transformation.",
  "model": "google_nmt",
  "from_community_srt": "Aber der Grund, warum ich das so wichtig finde, ist das während der gesamten Mathematik, wenn Sie mit einem Vektor zu tun haben, Sobald Sie seine Persönlichkeit wirklich Kennenlernen manchmal merkt man, dass es einfacher ist, es zu verstehen, nicht als Pfeil im Raum, sondern als physikalische Verkörperung einer linearen Transformation. Es ist,",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "Es ist, als wäre der Vektor eigentlich nur eine konzeptionelle Abkürzung für eine bestimmte Transformation, da es für uns einfacher ist, an Pfeile im Raum zu denken, als den gesamten Raum auf die Zahlenlinie zu verschieben.",
  "model": "google_nmt",
  "from_community_srt": "als ob der Vektor wirklich nur eine konzeptionelle Abkürzung für eine bestimmte Transformation ist. da es für uns einfacher ist, über Pfeile und Leerzeichen nachzudenken anstatt den gesamten Raum auf die Zahlenlinie zu verschieben.",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.97
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "Im nächsten Video sehen Sie ein weiteres wirklich cooles Beispiel dieser Dualität in Aktion, wenn ich über das Kreuzprodukt spreche.",
  "model": "google_nmt",
  "from_community_srt": "Im nächsten Video sehen Sie ein weiteres wirklich cooles Beispiel für diese \"Dualität\" in Aktion",
  "n_reviews": 0,
  "start": 822.61,
  "end": 829.19
 }
]