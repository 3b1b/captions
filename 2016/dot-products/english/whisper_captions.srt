1
00:00:16,580 --> 00:00:17,460
["Ode to Joy", by Beethoven, plays to the end of the piano.]

2
00:00:20,920 --> 00:00:25,060
Traditionally, dot products are something that's introduced really early on in a linear algebra course,

3
00:00:25,260 --> 00:00:26,300
typically right at the start.

4
00:00:26,640 --> 00:00:29,580
So it might seem strange that I've pushed them back this far in the series.

5
00:00:29,580 --> 00:00:32,900
I did this because there's a standard way to introduce the topic,

6
00:00:32,940 --> 00:00:35,720
which requires nothing more than a basic understanding of vectors,

7
00:00:36,020 --> 00:00:39,200
but a fuller understanding of the role that dot products play in math

8
00:00:39,200 --> 00:00:42,440
can only really be found under the light of linear transformations.

9
00:00:43,480 --> 00:00:47,340
Before that, though, let me just briefly cover the standard way that dot products are introduced,

10
00:00:47,700 --> 00:00:50,620
which I'm assuming is at least partially review for a number of viewers.

11
00:00:51,440 --> 00:00:54,460
Numerically, if you have two vectors of the same dimension,

12
00:00:54,960 --> 00:00:56,880
two lists of numbers with the same lengths,

13
00:00:56,880 --> 00:01:01,020
taking their dot product means pairing up all of the coordinates,

14
00:01:01,760 --> 00:01:04,980
multiplying those pairs together, and adding the result.

15
00:01:06,860 --> 00:01:13,180
So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.

16
00:01:14,580 --> 00:01:21,220
The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8

17
00:01:21,220 --> 00:01:23,720
plus 8 times 5 plus 3 times 3.

18
00:01:24,740 --> 00:01:28,660
Luckily, this computation has a really nice geometric interpretation.

19
00:01:29,340 --> 00:01:32,420
To think about the dot product between two vectors, v and w,

20
00:01:32,980 --> 00:01:37,980
imagine projecting w onto the line that passes through the origin and the tip of v.

21
00:01:38,780 --> 00:01:44,460
Multiplying the length of this projection by the length of v, you have the dot product v dot w.

22
00:01:46,420 --> 00:01:50,020
Except when this projection of w is pointing in the opposite direction from v,

23
00:01:50,360 --> 00:01:52,160
that dot product will actually be negative.

24
00:01:53,720 --> 00:01:57,860
So when two vectors are generally pointing in the same direction, their dot product is positive.

25
00:01:59,240 --> 00:02:03,940
When they're perpendicular, meaning the projection of one onto the other is the zero vector,

26
00:02:04,240 --> 00:02:05,560
their dot product is zero.

27
00:02:05,980 --> 00:02:09,600
And if they point in generally the opposite direction, their dot product is negative.

28
00:02:11,620 --> 00:02:14,560
Now, this interpretation is weirdly asymmetric.

29
00:02:14,800 --> 00:02:16,500
It treats the two vectors very differently.

30
00:02:16,880 --> 00:02:20,000
So when I first learned this, I was surprised that order doesn't matter.

31
00:02:20,960 --> 00:02:23,580
You could instead project v onto w,

32
00:02:24,060 --> 00:02:28,220
multiply the length of the projected v by the length of w, and get the same result.

33
00:02:30,400 --> 00:02:32,840
I mean, doesn't that feel like a really different process?

34
00:02:35,320 --> 00:02:37,760
Here's the intuition for why order doesn't matter.

35
00:02:38,440 --> 00:02:42,180
If v and w happened to have the same length, we could leverage some symmetry.

36
00:02:43,080 --> 00:02:48,160
Since projecting w onto v, then multiplying the length of that projection by the length of v,

37
00:02:48,160 --> 00:02:51,880
is a complete mirror image of projecting v onto w,

38
00:02:51,880 --> 00:02:55,240
then multiplying the length of that projection by the length of w.

39
00:02:57,280 --> 00:03:00,920
Now, if you scale one of them, say v, by some constant like 2,

40
00:03:01,260 --> 00:03:04,360
so that they don't have equal length, the symmetry is broken.

41
00:03:05,020 --> 00:03:10,040
But let's think through how to interpret the dot product between this new vector, 2 times v, and w.

42
00:03:10,880 --> 00:03:16,040
If you think of w as getting projected onto v, then the dot product 2v dot w

43
00:03:16,040 --> 00:03:19,720
will be exactly twice the dot product v dot w.

44
00:03:20,460 --> 00:03:25,880
This is because when you scale v by 2, it doesn't change the length of the projection of w,

45
00:03:26,240 --> 00:03:29,520
but it doubles the length of the vector that you're projecting onto.

46
00:03:30,460 --> 00:03:34,200
But on the other hand, let's say you were thinking about v getting projected onto w.

47
00:03:34,900 --> 00:03:39,780
Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2,

48
00:03:40,120 --> 00:03:43,000
but the length of the vector that you're projecting onto stays constant.

49
00:03:43,000 --> 00:03:46,660
So the overall effect is still to just double the dot product.

50
00:03:47,280 --> 00:03:51,200
So even though symmetry is broken in this case, the effect that this scaling has

51
00:03:51,200 --> 00:03:54,860
on the value of the dot product is the same under both interpretations.

52
00:03:56,640 --> 00:04:00,340
There's also one other big question that confused me when I first learned this stuff.

53
00:04:00,840 --> 00:04:04,160
Why on earth does this numerical process of matching coordinates,

54
00:04:04,460 --> 00:04:08,740
multiplying pairs, and adding them together have anything to do with projection?

55
00:04:10,640 --> 00:04:16,020
Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product,

56
00:04:16,540 --> 00:04:19,000
we need to unearth something a little bit deeper going on here,

57
00:04:19,320 --> 00:04:21,400
which often goes by the name duality.

58
00:04:22,140 --> 00:04:26,100
But before getting into that, I need to spend some time talking about linear transformations

59
00:04:26,100 --> 00:04:30,040
from multiple dimensions to one dimension, which is just the number line.

60
00:04:32,420 --> 00:04:35,640
These are functions that take in a 2D vector and spit out some number,

61
00:04:35,640 --> 00:04:38,920
but linear transformations are of course much more restricted

62
00:04:38,920 --> 00:04:42,300
than your run-of-the-mill function with a 2D input and a 1D output.

63
00:04:43,020 --> 00:04:46,800
As with transformations in higher dimensions, like the ones I talked about in chapter 3,

64
00:04:47,220 --> 00:04:49,880
there are some formal properties that make these functions linear,

65
00:04:50,180 --> 00:04:53,780
but I'm going to purposefully ignore those here so as to not distract from our end goal,

66
00:04:54,120 --> 00:04:58,260
and instead focus on a certain visual property that's equivalent to all the formal stuff.

67
00:04:59,040 --> 00:05:03,420
If you take a line of evenly spaced dots and apply a transformation,

68
00:05:04,080 --> 00:05:08,020
a linear transformation will keep those dots evenly spaced

69
00:05:08,020 --> 00:05:11,280
once they land in the output space, which is the number line.

70
00:05:12,420 --> 00:05:15,380
Otherwise, if there's some line of dots that gets unevenly spaced,

71
00:05:15,600 --> 00:05:17,140
then your transformation is not linear.

72
00:05:19,220 --> 00:05:20,700
As with the cases we've seen before,

73
00:05:21,180 --> 00:05:26,460
one of these linear transformations is completely determined by where it takes i-hat and j-hat,

74
00:05:26,840 --> 00:05:30,160
but this time each one of those basis vectors just lands on a number,

75
00:05:30,160 --> 00:05:33,680
so when we record where they land as the columns of a matrix,

76
00:05:34,380 --> 00:05:36,820
each of those columns just has a single number.

77
00:05:38,460 --> 00:05:39,840
This is a 1x2 matrix.

78
00:05:41,860 --> 00:05:45,660
Let's walk through an example of what it means to apply one of these transformations to a vector.

79
00:05:46,380 --> 00:05:51,680
Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.

80
00:05:52,420 --> 00:05:56,400
To follow where a vector with coordinates, say, 4, 3 ends up,

81
00:05:56,400 --> 00:06:01,020
think of breaking up this vector as 4 times i-hat plus 3 times j-hat.

82
00:06:01,840 --> 00:06:04,860
A consequence of linearity is that after the transformation,

83
00:06:05,440 --> 00:06:08,740
the vector will be 4 times the place where i-hat lands, 1,

84
00:06:09,060 --> 00:06:12,360
plus 3 times the place where j-hat lands, negative 2,

85
00:06:12,840 --> 00:06:15,780
which in this case implies that it lands on negative 2.

86
00:06:18,020 --> 00:06:22,360
When you do this calculation purely numerically, it's matrix vector multiplication.

87
00:06:25,700 --> 00:06:30,140
Now, this numerical operation of multiplying a 1x2 matrix by a vector

88
00:06:30,140 --> 00:06:32,860
feels just like taking the dot product of two vectors.

89
00:06:33,460 --> 00:06:36,800
Doesn't that 1x2 matrix just look like a vector that we tipped on its side?

90
00:06:37,960 --> 00:06:43,260
In fact, we could say right now that there's a nice association between 1x2 matrices and 2D

91
00:06:43,260 --> 00:06:47,660
vectors, defined by tilting the numerical representation of a vector on its side

92
00:06:47,660 --> 00:06:52,580
to get the associated matrix, or to tip the matrix back up to get the associated vector.

93
00:06:53,560 --> 00:06:55,920
Since we're just looking at numerical expressions right now,

94
00:06:56,220 --> 00:07:00,860
going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.

95
00:07:01,460 --> 00:07:05,120
But this suggests something that's truly awesome from the geometric view.

96
00:07:05,380 --> 00:07:08,440
There's some kind of connection between linear transformations

97
00:07:08,440 --> 00:07:11,720
that take vectors to numbers and vectors themselves.

98
00:07:14,780 --> 00:07:19,300
Let me show an example that clarifies the significance, and which just so happens to

99
00:07:19,300 --> 00:07:21,380
also answer the dot product puzzle from earlier.

100
00:07:22,140 --> 00:07:26,100
Unlearn what you have learned, and imagine that you don't already know that the dot product

101
00:07:26,100 --> 00:07:27,180
relates to projection.

102
00:07:28,860 --> 00:07:33,300
What I'm going to do here is take a copy of the number line and place it diagonally in

103
00:07:33,300 --> 00:07:36,060
space somehow, with the number 0 sitting at the origin.

104
00:07:36,900 --> 00:07:41,540
Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number

105
00:07:41,540 --> 00:07:41,920
is.

106
00:07:42,400 --> 00:07:44,560
I want to give that guy a name, u-hat.

107
00:07:45,620 --> 00:07:48,240
This little guy plays an important role in what's about to happen,

108
00:07:48,280 --> 00:07:50,020
so just keep him in the back of your mind.

109
00:07:50,740 --> 00:07:54,660
If we project 2d vectors straight onto this diagonal number line,

110
00:07:54,920 --> 00:07:58,960
in effect, we've just defined a function that takes 2d vectors to numbers.

111
00:07:59,660 --> 00:08:03,620
What's more, this function is actually linear, since it passes our visual test

112
00:08:03,620 --> 00:08:08,960
that any line of evenly spaced dots remains evenly spaced once it lands on the number line.

113
00:08:11,640 --> 00:08:16,160
Just to be clear, even though I've embedded the number line in 2d space like this,

114
00:08:16,480 --> 00:08:19,280
the outputs of the function are numbers, not 2d vectors.

115
00:08:19,960 --> 00:08:23,680
You should think of a function that takes in two coordinates and outputs a single coordinate.

116
00:08:25,060 --> 00:08:29,020
But that vector u-hat is a two-dimensional vector, living in the input space.

117
00:08:29,440 --> 00:08:33,220
It's just situated in such a way that overlaps with the embedding of the number line.

118
00:08:34,600 --> 00:08:39,700
With this projection, we just defined a linear transformation from 2d vectors to numbers,

119
00:08:39,700 --> 00:08:44,600
so we're going to be able to find some kind of 1x2 matrix that describes that transformation.

120
00:08:45,540 --> 00:08:49,740
To find that 1x2 matrix, let's zoom in on this diagonal number line setup

121
00:08:49,740 --> 00:08:53,100
and think about where i-hat and j-hat each land,

122
00:08:53,320 --> 00:08:56,460
since those landing spots are going to be the columns of the matrix.

123
00:08:58,480 --> 00:08:59,440
This part's super cool.

124
00:08:59,700 --> 00:09:02,420
We can reason through it with a really elegant piece of symmetry.

125
00:09:03,020 --> 00:09:05,560
Since i-hat and u-hat are both unit vectors,

126
00:09:05,560 --> 00:09:08,840
projecting i-hat onto the line passing through u-hat

127
00:09:08,840 --> 00:09:13,160
looks totally symmetric to projecting u-hat onto the x-axis.

128
00:09:13,840 --> 00:09:16,940
So when we ask what number does i-hat land on when it gets projected,

129
00:09:17,420 --> 00:09:22,320
the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.

130
00:09:22,920 --> 00:09:28,600
But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.

131
00:09:29,020 --> 00:09:34,160
So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line

132
00:09:34,160 --> 00:09:36,620
is going to be the x-coordinate of u-hat.

133
00:09:37,160 --> 00:09:37,660
Isn't that cool?

134
00:09:39,200 --> 00:09:41,800
The reasoning is almost identical for the j-hat case.

135
00:09:42,180 --> 00:09:43,260
Think about it for a moment.

136
00:09:49,120 --> 00:09:54,360
For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands

137
00:09:54,360 --> 00:09:56,600
when it's projected onto the number line copy.

138
00:09:57,580 --> 00:09:58,720
Pause and ponder that for a moment.

139
00:09:58,780 --> 00:10:00,200
I just think that's really cool.

140
00:10:00,920 --> 00:10:04,660
So the entries of the 1x2 matrix describing the projection transformation

141
00:10:04,660 --> 00:10:07,260
are going to be the coordinates of u-hat.

142
00:10:08,040 --> 00:10:11,640
And computing this projection transformation for arbitrary vectors in space,

143
00:10:11,960 --> 00:10:14,760
which requires multiplying that matrix by those vectors,

144
00:10:15,080 --> 00:10:18,880
is computationally identical to taking a dot product with u-hat.

145
00:10:21,460 --> 00:10:25,990
This is why taking the dot product with a unit vector can be interpreted as

146
00:10:25,990 --> 00:10:30,590
projecting a vector onto the span of that unit vector and taking the length.

147
00:10:34,030 --> 00:10:35,790
So what about non-unit vectors?

148
00:10:36,310 --> 00:10:38,550
For example, let's say we take that unit vector u-hat,

149
00:10:38,790 --> 00:10:40,630
but we scale it up by a factor of 3.

150
00:10:41,350 --> 00:10:44,390
Numerically, each of its components gets multiplied by 3.

151
00:10:44,810 --> 00:10:47,530
So looking at the matrix associated with that vector,

152
00:10:47,990 --> 00:10:52,390
it takes i-hat and j-hat to three times the values where they landed before.

153
00:10:55,230 --> 00:11:00,230
Since this is all linear, it implies more generally that the new matrix can be interpreted

154
00:11:00,230 --> 00:11:04,650
as projecting any vector onto the number line copy and multiplying where it lands by 3.

155
00:11:05,470 --> 00:11:10,390
This is why the dot product with a non-unit vector can be interpreted as first projecting

156
00:11:10,390 --> 00:11:14,950
onto that vector, then scaling up the length of that projection by the length of the vector.

157
00:11:17,590 --> 00:11:19,550
Take a moment to think about what happened here.

158
00:11:19,890 --> 00:11:22,370
We had a linear transformation from 2D space to the number line,

159
00:11:22,370 --> 00:11:26,790
which was not defined in terms of numerical vectors or numerical dot products,

160
00:11:27,090 --> 00:11:30,890
it was just defined by projecting space onto a diagonal copy of the number line.

161
00:11:31,670 --> 00:11:36,830
But because the transformation is linear, it was necessarily described by some 1x2 matrix.

162
00:11:37,330 --> 00:11:42,410
And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side

163
00:11:42,410 --> 00:11:47,910
and taking a dot product, this transformation was inescapably related to some 2D vector.

164
00:11:49,410 --> 00:11:52,880
The lesson here is that any time you have one of these linear transformations

165
00:11:53,370 --> 00:11:57,070
whose output space is the number line, no matter how it was defined,

166
00:11:57,410 --> 00:12:00,780
there's going to be some unique vector v corresponding to that transformation,

167
00:12:01,490 --> 00:12:06,350
in the sense that applying the transformation is the same thing as taking a dot product with that vector.

168
00:12:09,930 --> 00:12:12,030
To me, this is utterly beautiful.

169
00:12:12,730 --> 00:12:15,390
It's an example of something in math called duality.

170
00:12:16,270 --> 00:12:19,670
Duality shows up in many different ways and forms throughout math,

171
00:12:19,710 --> 00:12:21,930
and it's super tricky to actually define.

172
00:12:22,670 --> 00:12:27,610
Loosely speaking, it refers to situations where you have a natural but surprising correspondence

173
00:12:27,610 --> 00:12:30,230
between two types of mathematical thing.

174
00:12:31,010 --> 00:12:33,090
For the linear algebra case that you just learned about,

175
00:12:33,370 --> 00:12:37,750
you'd say that the dual of a vector is the linear transformation that it encodes,

176
00:12:38,410 --> 00:12:42,210
and the dual of a linear transformation from some space to one dimension

177
00:12:42,210 --> 00:12:44,650
is a certain vector in that space.

178
00:12:46,730 --> 00:12:51,430
So to sum up, on the surface, the dot product is a very useful geometric tool

179
00:12:51,430 --> 00:12:56,310
for understanding projections and for testing whether or not vectors tend to point in the same direction.

180
00:12:56,970 --> 00:13:00,790
And that's probably the most important thing for you to remember about the dot product.

181
00:13:01,270 --> 00:13:04,130
But at a deeper level, dotting two vectors together

182
00:13:04,130 --> 00:13:07,730
is a way to translate one of them into the world of transformations.

183
00:13:08,670 --> 00:13:11,550
Again, numerically, this might feel like a silly point to emphasize.

184
00:13:11,670 --> 00:13:14,490
It's just two computations that happen to look similar.

185
00:13:14,490 --> 00:13:17,850
But the reason I find this so important is that throughout math,

186
00:13:18,070 --> 00:13:21,810
when you're dealing with a vector, once you really get to know its personality,

187
00:13:22,290 --> 00:13:26,330
sometimes you realize that it's easier to understand it not as an arrow in space,

188
00:13:26,750 --> 00:13:30,090
but as the physical embodiment of a linear transformation.

189
00:13:30,730 --> 00:13:35,050
It's as if the vector is really just a conceptual shorthand for a certain transformation,

190
00:13:35,470 --> 00:13:38,070
since it's easier for us to think about arrows in space

191
00:13:38,070 --> 00:13:40,970
rather than moving all of that space to the number line.

192
00:13:42,610 --> 00:13:47,250
In the next video, you'll see another really cool example of this duality in action,

193
00:13:47,570 --> 00:13:49,190
as I talk about the cross product.

194
00:14:10,970 --> 00:14:11,030
you

