1
00:00:16,580 --> 00:00:21,476
Traditionally, dot products are something that's introduced really 

2
00:00:21,476 --> 00:00:26,300
early on in a linear algebra course, typically right at the start.

3
00:00:26,640 --> 00:00:29,580
So it might seem strange that I've pushed them back this far in the series.

4
00:00:29,580 --> 00:00:32,723
I did this because there's a standard way to introduce the topic, 

5
00:00:32,723 --> 00:00:35,914
which requires nothing more than a basic understanding of vectors, 

6
00:00:35,914 --> 00:00:40,153
but a fuller understanding of the role that dot products play in math can only really be 

7
00:00:40,153 --> 00:00:42,440
found under the light of linear transformations.

8
00:00:43,480 --> 00:00:47,092
Before that, though, let me just briefly cover the standard way that dot products are 

9
00:00:47,092 --> 00:00:50,620
introduced, which I'm assuming is at least partially review for a number of viewers.

10
00:00:51,440 --> 00:00:55,034
Numerically, if you have two vectors of the same dimension, 

11
00:00:55,034 --> 00:00:59,528
two lists of numbers with the same lengths, taking their dot product means 

12
00:00:59,528 --> 00:01:03,661
pairing up all of the coordinates, multiplying those pairs together, 

13
00:01:03,661 --> 00:01:04,980
and adding the result.

14
00:01:06,860 --> 00:01:13,180
So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.

15
00:01:14,580 --> 00:01:19,108
The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 

16
00:01:19,108 --> 00:01:23,720
6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.

17
00:01:24,740 --> 00:01:28,660
Luckily, this computation has a really nice geometric interpretation.

18
00:01:29,340 --> 00:01:33,000
To think about the dot product between two vectors, v and w, 

19
00:01:33,000 --> 00:01:37,980
imagine projecting w onto the line that passes through the origin and the tip of v.

20
00:01:38,780 --> 00:01:42,486
Multiplying the length of this projection by the length of v, 

21
00:01:42,486 --> 00:01:44,460
you have the dot product v dot w.

22
00:01:46,420 --> 00:01:50,136
Except when this projection of w is pointing in the opposite direction from v, 

23
00:01:50,136 --> 00:01:52,160
that dot product will actually be negative.

24
00:01:53,720 --> 00:01:56,566
So when two vectors are generally pointing in the same direction, 

25
00:01:56,566 --> 00:01:57,860
their dot product is positive.

26
00:01:59,240 --> 00:02:02,320
When they're perpendicular, meaning the projection of one 

27
00:02:02,320 --> 00:02:05,560
onto the other is the zero vector, their dot product is zero.

28
00:02:05,980 --> 00:02:09,600
And if they point in generally the opposite direction, their dot product is negative.

29
00:02:11,620 --> 00:02:14,560
Now, this interpretation is weirdly asymmetric.

30
00:02:14,800 --> 00:02:16,500
It treats the two vectors very differently.

31
00:02:16,880 --> 00:02:20,000
So when I first learned this, I was surprised that order doesn't matter.

32
00:02:20,960 --> 00:02:24,559
You could instead project v onto w, multiply the length of 

33
00:02:24,559 --> 00:02:28,220
the projected v by the length of w, and get the same result.

34
00:02:30,400 --> 00:02:32,840
I mean, doesn't that feel like a really different process?

35
00:02:35,320 --> 00:02:37,760
Here's the intuition for why order doesn't matter.

36
00:02:38,440 --> 00:02:42,180
If v and w happened to have the same length, we could leverage some symmetry.

37
00:02:43,080 --> 00:02:47,344
Since projecting w onto v, then multiplying the length of that projection 

38
00:02:47,344 --> 00:02:51,436
by the length of v, is a complete mirror image of projecting v onto w, 

39
00:02:51,436 --> 00:02:55,240
then multiplying the length of that projection by the length of w.

40
00:02:57,280 --> 00:03:00,877
Now, if you scale one of them, say v, by some constant like 2, 

41
00:03:00,877 --> 00:03:04,360
so that they don't have equal length, the symmetry is broken.

42
00:03:05,020 --> 00:03:09,177
But let's think through how to interpret the dot product between this new vector, 

43
00:03:09,177 --> 00:03:10,040
2 times v, and w.

44
00:03:10,880 --> 00:03:14,257
If you think of w as getting projected onto v, 

45
00:03:14,257 --> 00:03:19,720
then the dot product 2v dot w will be exactly twice the dot product v dot w.

46
00:03:20,460 --> 00:03:24,706
This is because when you scale v by 2, it doesn't change the length of the 

47
00:03:24,706 --> 00:03:29,520
projection of w, but it doubles the length of the vector that you're projecting onto.

48
00:03:30,460 --> 00:03:34,200
But on the other hand, let's say you were thinking about v getting projected onto w.

49
00:03:34,900 --> 00:03:38,903
Well, in that case, the length of the projection is the thing that gets scaled when we 

50
00:03:38,903 --> 00:03:43,000
multiply v by 2, but the length of the vector that you're projecting onto stays constant.

51
00:03:43,000 --> 00:03:46,660
So the overall effect is still to just double the dot product.

52
00:03:47,280 --> 00:03:49,673
So even though symmetry is broken in this case, 

53
00:03:49,673 --> 00:03:53,513
the effect that this scaling has on the value of the dot product is the same 

54
00:03:53,513 --> 00:03:54,860
under both interpretations.

55
00:03:56,640 --> 00:04:00,340
There's also one other big question that confused me when I first learned this stuff.

56
00:04:00,840 --> 00:04:04,411
Why on earth does this numerical process of matching coordinates, 

57
00:04:04,411 --> 00:04:08,740
multiplying pairs, and adding them together have anything to do with projection?

58
00:04:10,640 --> 00:04:14,191
Well, to give a satisfactory answer, and also to do full justice to 

59
00:04:14,191 --> 00:04:17,743
the significance of the dot product, we need to unearth something a 

60
00:04:17,743 --> 00:04:21,399
little bit deeper going on here, which often goes by the name duality.

61
00:04:22,140 --> 00:04:25,804
But before getting into that, I need to spend some time talking about linear 

62
00:04:25,804 --> 00:04:30,040
transformations from multiple dimensions to one dimension, which is just the number line.

63
00:04:32,420 --> 00:04:35,927
These are functions that take in a 2D vector and spit out some number, 

64
00:04:35,927 --> 00:04:39,237
but linear transformations are of course much more restricted than 

65
00:04:39,237 --> 00:04:42,300
your run-of-the-mill function with a 2D input and a 1D output.

66
00:04:43,020 --> 00:04:47,080
As with transformations in higher dimensions, like the ones I talked about in chapter 3, 

67
00:04:47,080 --> 00:04:50,138
there are some formal properties that make these functions linear, 

68
00:04:50,138 --> 00:04:54,199
but I'm going to purposefully ignore those here so as to not distract from our end goal, 

69
00:04:54,199 --> 00:04:58,260
and instead focus on a certain visual property that's equivalent to all the formal stuff.

70
00:04:59,040 --> 00:05:03,508
If you take a line of evenly spaced dots and apply a transformation, 

71
00:05:03,508 --> 00:05:07,653
a linear transformation will keep those dots evenly spaced once 

72
00:05:07,653 --> 00:05:11,280
they land in the output space, which is the number line.

73
00:05:12,420 --> 00:05:15,403
Otherwise, if there's some line of dots that gets unevenly spaced, 

74
00:05:15,403 --> 00:05:17,140
then your transformation is not linear.

75
00:05:19,220 --> 00:05:23,507
As with the cases we've seen before, one of these linear transformations is 

76
00:05:23,507 --> 00:05:26,722
completely determined by where it takes i-hat and j-hat, 

77
00:05:26,722 --> 00:05:30,671
but this time each one of those basis vectors just lands on a number, 

78
00:05:30,671 --> 00:05:34,168
so when we record where they land as the columns of a matrix, 

79
00:05:34,168 --> 00:05:36,820
each of those columns just has a single number.

80
00:05:38,460 --> 00:05:39,840
This is a 1x2 matrix.

81
00:05:41,860 --> 00:05:43,701
Let's walk through an example of what it means 

82
00:05:43,701 --> 00:05:45,660
to apply one of these transformations to a vector.

83
00:05:46,380 --> 00:05:51,680
Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.

84
00:05:52,420 --> 00:05:56,490
To follow where a vector with coordinates, say, 4, 3 ends up, 

85
00:05:56,490 --> 00:06:01,020
think of breaking up this vector as 4 times i-hat plus 3 times j-hat.

86
00:06:01,840 --> 00:06:05,553
A consequence of linearity is that after the transformation, 

87
00:06:05,553 --> 00:06:09,144
the vector will be 4 times the place where i-hat lands, 1, 

88
00:06:09,144 --> 00:06:12,431
plus 3 times the place where j-hat lands, negative 2, 

89
00:06:12,431 --> 00:06:15,780
which in this case implies that it lands on negative 2.

90
00:06:18,020 --> 00:06:22,360
When you do this calculation purely numerically, it's matrix vector multiplication.

91
00:06:25,700 --> 00:06:29,222
Now, this numerical operation of multiplying a 1x2 matrix by 

92
00:06:29,222 --> 00:06:32,860
a vector feels just like taking the dot product of two vectors.

93
00:06:33,460 --> 00:06:36,800
Doesn't that 1x2 matrix just look like a vector that we tipped on its side?

94
00:06:37,960 --> 00:06:42,721
In fact, we could say right now that there's a nice association between 1x2 matrices 

95
00:06:42,721 --> 00:06:47,650
and 2D vectors, defined by tilting the numerical representation of a vector on its side 

96
00:06:47,650 --> 00:06:52,580
to get the associated matrix, or to tip the matrix back up to get the associated vector.

97
00:06:53,560 --> 00:06:56,509
Since we're just looking at numerical expressions right now, 

98
00:06:56,509 --> 00:07:00,860
going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.

99
00:07:01,460 --> 00:07:05,120
But this suggests something that's truly awesome from the geometric view.

100
00:07:05,380 --> 00:07:08,853
There's some kind of connection between linear transformations 

101
00:07:08,853 --> 00:07:11,720
that take vectors to numbers and vectors themselves.

102
00:07:14,780 --> 00:07:17,558
Let me show an example that clarifies the significance, 

103
00:07:17,558 --> 00:07:21,380
and which just so happens to also answer the dot product puzzle from earlier.

104
00:07:22,140 --> 00:07:24,704
Unlearn what you have learned, and imagine that you don't 

105
00:07:24,704 --> 00:07:27,180
already know that the dot product relates to projection.

106
00:07:28,860 --> 00:07:32,409
What I'm going to do here is take a copy of the number line and place 

107
00:07:32,409 --> 00:07:36,060
it diagonally in space somehow, with the number 0 sitting at the origin.

108
00:07:36,900 --> 00:07:39,566
Now think of the two-dimensional unit vector whose 

109
00:07:39,566 --> 00:07:41,920
tip sits where the number 1 on the number is.

110
00:07:42,400 --> 00:07:44,560
I want to give that guy a name, u-hat.

111
00:07:45,620 --> 00:07:48,324
This little guy plays an important role in what's about to happen, 

112
00:07:48,324 --> 00:07:50,020
so just keep him in the back of your mind.

113
00:07:50,740 --> 00:07:54,615
If we project 2d vectors straight onto this diagonal number line, 

114
00:07:54,615 --> 00:07:58,960
in effect, we've just defined a function that takes 2d vectors to numbers.

115
00:07:59,660 --> 00:08:02,231
What's more, this function is actually linear, 

116
00:08:02,231 --> 00:08:06,771
since it passes our visual test that any line of evenly spaced dots remains evenly 

117
00:08:06,771 --> 00:08:08,960
spaced once it lands on the number line.

118
00:08:11,640 --> 00:08:16,202
Just to be clear, even though I've embedded the number line in 2d space like this, 

119
00:08:16,202 --> 00:08:19,280
the outputs of the function are numbers, not 2d vectors.

120
00:08:19,960 --> 00:08:21,919
You should think of a function that takes in two 

121
00:08:21,919 --> 00:08:23,680
coordinates and outputs a single coordinate.

122
00:08:25,060 --> 00:08:29,020
But that vector u-hat is a two-dimensional vector, living in the input space.

123
00:08:29,440 --> 00:08:33,220
It's just situated in such a way that overlaps with the embedding of the number line.

124
00:08:34,600 --> 00:08:39,518
With this projection, we just defined a linear transformation from 2d vectors to numbers, 

125
00:08:39,518 --> 00:08:42,960
so we're going to be able to find some kind of 1x2 matrix that 

126
00:08:42,960 --> 00:08:44,600
describes that transformation.

127
00:08:45,540 --> 00:08:49,141
To find that 1x2 matrix, let's zoom in on this diagonal number 

128
00:08:49,141 --> 00:08:52,572
line setup and think about where i-hat and j-hat each land, 

129
00:08:52,572 --> 00:08:56,460
since those landing spots are going to be the columns of the matrix.

130
00:08:58,480 --> 00:08:59,440
This part's super cool.

131
00:08:59,700 --> 00:09:02,420
We can reason through it with a really elegant piece of symmetry.

132
00:09:03,020 --> 00:09:07,897
Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line 

133
00:09:07,897 --> 00:09:13,160
passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.

134
00:09:13,840 --> 00:09:17,373
So when we ask what number does i-hat land on when it gets projected, 

135
00:09:17,373 --> 00:09:21,512
the answer is going to be the same as whatever u-hat lands on when it's projected 

136
00:09:21,512 --> 00:09:22,320
onto the x-axis.

137
00:09:22,920 --> 00:09:28,600
But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.

138
00:09:29,020 --> 00:09:32,903
So by symmetry, the number where i-hat lands when it's projected onto 

139
00:09:32,903 --> 00:09:36,620
that diagonal number line is going to be the x-coordinate of u-hat.

140
00:09:37,160 --> 00:09:37,660
Isn't that cool?

141
00:09:39,200 --> 00:09:41,800
The reasoning is almost identical for the j-hat case.

142
00:09:42,180 --> 00:09:43,260
Think about it for a moment.

143
00:09:49,120 --> 00:09:52,694
For all the same reasons, the y-coordinate of u-hat gives us the 

144
00:09:52,694 --> 00:09:56,600
number where j-hat lands when it's projected onto the number line copy.

145
00:09:57,580 --> 00:09:58,720
Pause and ponder that for a moment.

146
00:09:58,780 --> 00:10:00,200
I just think that's really cool.

147
00:10:00,920 --> 00:10:04,172
So the entries of the 1x2 matrix describing the projection 

148
00:10:04,172 --> 00:10:07,260
transformation are going to be the coordinates of u-hat.

149
00:10:08,040 --> 00:10:12,255
And computing this projection transformation for arbitrary vectors in space, 

150
00:10:12,255 --> 00:10:15,376
which requires multiplying that matrix by those vectors, 

151
00:10:15,376 --> 00:10:18,880
is computationally identical to taking a dot product with u-hat.

152
00:10:21,460 --> 00:10:26,025
This is why taking the dot product with a unit vector can be interpreted as 

153
00:10:26,025 --> 00:10:30,590
projecting a vector onto the span of that unit vector and taking the length.

154
00:10:34,030 --> 00:10:35,790
So what about non-unit vectors?

155
00:10:36,310 --> 00:10:38,920
For example, let's say we take that unit vector u-hat, 

156
00:10:38,920 --> 00:10:40,630
but we scale it up by a factor of 3.

157
00:10:41,350 --> 00:10:44,390
Numerically, each of its components gets multiplied by 3.

158
00:10:44,810 --> 00:10:47,958
So looking at the matrix associated with that vector, 

159
00:10:47,958 --> 00:10:52,390
it takes i-hat and j-hat to three times the values where they landed before.

160
00:10:55,230 --> 00:10:59,410
Since this is all linear, it implies more generally that the new matrix can be 

161
00:10:59,410 --> 00:11:04,067
interpreted as projecting any vector onto the number line copy and multiplying where it 

162
00:11:04,067 --> 00:11:04,650
lands by 3.

163
00:11:05,470 --> 00:11:08,491
This is why the dot product with a non-unit vector can be 

164
00:11:08,491 --> 00:11:11,095
interpreted as first projecting onto that vector, 

165
00:11:11,095 --> 00:11:14,950
then scaling up the length of that projection by the length of the vector.

166
00:11:17,590 --> 00:11:19,550
Take a moment to think about what happened here.

167
00:11:19,890 --> 00:11:23,081
We had a linear transformation from 2D space to the number line, 

168
00:11:23,081 --> 00:11:26,961
which was not defined in terms of numerical vectors or numerical dot products, 

169
00:11:26,961 --> 00:11:30,890
it was just defined by projecting space onto a diagonal copy of the number line.

170
00:11:31,670 --> 00:11:36,830
But because the transformation is linear, it was necessarily described by some 1x2 matrix.

171
00:11:37,330 --> 00:11:40,875
And since multiplying a 1x2 matrix by a 2D vector is the same 

172
00:11:40,875 --> 00:11:44,364
as turning that matrix on its side and taking a dot product, 

173
00:11:44,364 --> 00:11:47,910
this transformation was inescapably related to some 2D vector.

174
00:11:49,410 --> 00:11:53,708
The lesson here is that any time you have one of these linear transformations whose 

175
00:11:53,708 --> 00:11:56,933
output space is the number line, no matter how it was defined, 

176
00:11:56,933 --> 00:12:00,976
there's going to be some unique vector v corresponding to that transformation, 

177
00:12:00,976 --> 00:12:05,070
in the sense that applying the transformation is the same thing as taking a dot 

178
00:12:05,070 --> 00:12:06,350
product with that vector.

179
00:12:09,930 --> 00:12:12,030
To me, this is utterly beautiful.

180
00:12:12,730 --> 00:12:15,390
It's an example of something in math called duality.

181
00:12:16,270 --> 00:12:19,781
Duality shows up in many different ways and forms throughout math, 

182
00:12:19,781 --> 00:12:21,930
and it's super tricky to actually define.

183
00:12:22,670 --> 00:12:26,367
Loosely speaking, it refers to situations where you have a natural 

184
00:12:26,367 --> 00:12:30,230
but surprising correspondence between two types of mathematical thing.

185
00:12:31,010 --> 00:12:34,170
For the linear algebra case that you just learned about, 

186
00:12:34,170 --> 00:12:38,717
you'd say that the dual of a vector is the linear transformation that it encodes, 

187
00:12:38,717 --> 00:12:43,042
and the dual of a linear transformation from some space to one dimension is a 

188
00:12:43,042 --> 00:12:44,650
certain vector in that space.

189
00:12:46,730 --> 00:12:50,028
So to sum up, on the surface, the dot product is a very useful 

190
00:12:50,028 --> 00:12:53,221
geometric tool for understanding projections and for testing 

191
00:12:53,221 --> 00:12:56,310
whether or not vectors tend to point in the same direction.

192
00:12:56,970 --> 00:13:00,790
And that's probably the most important thing for you to remember about the dot product.

193
00:13:01,270 --> 00:13:04,553
But at a deeper level, dotting two vectors together is a way 

194
00:13:04,553 --> 00:13:07,730
to translate one of them into the world of transformations.

195
00:13:08,670 --> 00:13:11,550
Again, numerically, this might feel like a silly point to emphasize.

196
00:13:11,670 --> 00:13:14,490
It's just two computations that happen to look similar.

197
00:13:14,490 --> 00:13:18,047
But the reason I find this so important is that throughout math, 

198
00:13:18,047 --> 00:13:22,426
when you're dealing with a vector, once you really get to know its personality, 

199
00:13:22,426 --> 00:13:26,915
sometimes you realize that it's easier to understand it not as an arrow in space, 

200
00:13:26,915 --> 00:13:30,090
but as the physical embodiment of a linear transformation.

201
00:13:30,730 --> 00:13:35,292
It's as if the vector is really just a conceptual shorthand for a certain transformation, 

202
00:13:35,292 --> 00:13:38,739
since it's easier for us to think about arrows in space rather than 

203
00:13:38,739 --> 00:13:40,970
moving all of that space to the number line.

204
00:13:42,610 --> 00:13:47,310
In the next video, you'll see another really cool example of this duality in action, 

205
00:13:47,310 --> 00:13:49,190
as I talk about the cross product.

