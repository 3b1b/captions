1
00:00:00,000 --> 00:00:20,800
[Music]

2
00:00:20,800 --> 00:00:25,120
Traditionally, dot products are something that's introduced really early on in a linear algebra course,

3
00:00:25,120 --> 00:00:26,640
typically right at the start.

4
00:00:26,640 --> 00:00:29,960
So it might seem strange that I've pushed them back this far in the series.

5
00:00:30,120 --> 00:00:32,920
I did this because there's a standard way to introduce the topic,

6
00:00:32,920 --> 00:00:36,120
which requires nothing more than a basic understanding of vectors,

7
00:00:36,120 --> 00:00:39,480
but a fuller understanding of the role that dot products play in math

8
00:00:39,480 --> 00:00:42,760
can only really be found under the light of linear transformations.

9
00:00:43,320 --> 00:00:47,560
Before that, though, let me just briefly cover the standard way that dot products are introduced,

10
00:00:47,560 --> 00:00:50,840
which I'm assuming is at least partially review for a number of viewers.

11
00:00:51,240 --> 00:00:54,840
Numerically, if you have two vectors of the same dimension,

12
00:00:54,840 --> 00:00:57,320
two lists of numbers with the same lengths,

13
00:00:57,320 --> 00:01:01,000
taking their dot product means pairing up all of the coordinates,

14
00:01:01,640 --> 00:01:04,920
multiplying those pairs together, and adding the result.

15
00:01:06,680 --> 00:01:13,080
So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.

16
00:01:14,520 --> 00:01:21,240
The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8

17
00:01:21,240 --> 00:01:23,640
plus 8 times 5 plus 3 times 3.

18
00:01:24,520 --> 00:01:28,840
Luckily, this computation has a really nice geometric interpretation.

19
00:01:28,840 --> 00:01:32,520
To think about the dot product between two vectors, v and w,

20
00:01:32,520 --> 00:01:37,800
imagine projecting w onto the line that passes through the origin and the tip of v.

21
00:01:38,280 --> 00:01:44,360
Multiplying the length of this projection by the length of v, you have the dot product v dot w.

22
00:01:46,040 --> 00:01:49,880
Except when this projection of w is pointing in the opposite direction from v,

23
00:01:49,880 --> 00:01:52,120
that dot product will actually be negative.

24
00:01:53,800 --> 00:01:57,800
So when two vectors are generally pointing in the same direction, their dot product is positive.

25
00:01:59,400 --> 00:02:04,040
When they're perpendicular, meaning the projection of one onto the other is the zero vector,

26
00:02:04,040 --> 00:02:05,880
their dot product is zero.

27
00:02:05,880 --> 00:02:09,560
And if they point in generally the opposite direction, their dot product is negative.

28
00:02:11,640 --> 00:02:14,680
Now, this interpretation is weirdly asymmetric.

29
00:02:14,680 --> 00:02:16,760
It treats the two vectors very differently.

30
00:02:16,760 --> 00:02:19,880
So when I first learned this, I was surprised that order doesn't matter.

31
00:02:20,280 --> 00:02:23,000
You could instead project v onto w,

32
00:02:23,000 --> 00:02:27,400
multiply the length of the projected v by the length of w, and get the same result.

33
00:02:29,400 --> 00:02:32,120
I mean, doesn't that feel like a really different process?

34
00:02:34,600 --> 00:02:36,840
Here's the intuition for why order doesn't matter.

35
00:02:37,640 --> 00:02:41,400
If v and w happened to have the same length, we could leverage some symmetry.

36
00:02:42,200 --> 00:02:47,560
Since projecting w onto v, then multiplying the length of that projection by the length of v,

37
00:02:48,440 --> 00:02:52,040
is a complete mirror image of projecting v onto w,

38
00:02:52,040 --> 00:02:55,080
then multiplying the length of that projection by the length of w.

39
00:02:57,160 --> 00:03:01,080
Now, if you scale one of them, say v, by some constant like 2,

40
00:03:01,080 --> 00:03:04,840
so that they don't have equal length, the symmetry is broken.

41
00:03:04,840 --> 00:03:07,240
But let's think through how to interpret the dot product

42
00:03:07,240 --> 00:03:09,960
between this new vector, 2 times v, and w.

43
00:03:10,840 --> 00:03:13,480
If you think of w as getting projected onto v,

44
00:03:13,480 --> 00:03:19,720
then the dot product 2v dot w will be exactly twice the dot product v dot w.

45
00:03:20,280 --> 00:03:26,120
This is because when you scale v by 2, it doesn't change the length of the projection of w,

46
00:03:26,120 --> 00:03:29,560
but it doubles the length of the vector that you're projecting onto.

47
00:03:30,200 --> 00:03:34,120
But on the other hand, let's say you were thinking about v getting projected onto w.

48
00:03:34,760 --> 00:03:39,960
Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2,

49
00:03:39,960 --> 00:03:43,320
but the length of the vector that you're projecting onto stays constant.

50
00:03:43,320 --> 00:03:47,000
So the overall effect is still to just double the dot product.

51
00:03:47,000 --> 00:03:49,400
So even though symmetry is broken in this case,

52
00:03:49,400 --> 00:03:52,920
the effect that this scaling has on the value of the dot product

53
00:03:52,920 --> 00:03:54,920
is the same under both interpretations.

54
00:03:56,760 --> 00:04:00,120
There's also one other big question that confused me when I first learned this stuff.

55
00:04:00,760 --> 00:04:04,280
Why on earth does this numerical process of matching coordinates,

56
00:04:04,280 --> 00:04:08,760
multiplying pairs, and adding them together have anything to do with projection?

57
00:04:08,760 --> 00:04:16,280
Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product,

58
00:04:16,280 --> 00:04:19,160
we need to unearth something a little bit deeper going on here,

59
00:04:19,160 --> 00:04:21,320
which often goes by the name duality.

60
00:04:21,880 --> 00:04:26,280
But before getting into that, I need to spend some time talking about linear transformations

61
00:04:26,280 --> 00:04:29,880
from multiple dimensions to one dimension, which is just the number line.

62
00:04:32,520 --> 00:04:35,960
These are functions that take in a 2d vector and spit out some number,

63
00:04:35,960 --> 00:04:38,840
but linear transformations are of course much more restricted

64
00:04:38,840 --> 00:04:42,200
than your run-of-the-mill function with a 2d input and a 1d output.

65
00:04:42,760 --> 00:04:47,080
As with transformations in higher dimensions, like the ones I talked about in chapter 3,

66
00:04:47,080 --> 00:04:50,040
there are some formal properties that make these functions linear,

67
00:04:50,040 --> 00:04:53,960
but I'm going to purposefully ignore those here so as to not distract from our end goal,

68
00:04:53,960 --> 00:04:58,040
and instead focus on a certain visual property that's equivalent to all the formal stuff.

69
00:04:58,920 --> 00:05:03,320
If you take a line of evenly spaced dots and apply a transformation,

70
00:05:04,280 --> 00:05:08,120
a linear transformation will keep those dots evenly spaced

71
00:05:08,120 --> 00:05:11,000
once they land in the output space, which is the number line.

72
00:05:12,200 --> 00:05:15,320
Otherwise, if there's some line of dots that gets unevenly spaced,

73
00:05:15,320 --> 00:05:17,080
then your transformation is not linear.

74
00:05:19,160 --> 00:05:23,000
As with the cases we've seen before, one of these linear transformations

75
00:05:23,000 --> 00:05:26,760
is completely determined by where it takes i-hat and j-hat,

76
00:05:26,760 --> 00:05:30,440
but this time each one of those basis vectors just lands on a number,

77
00:05:30,440 --> 00:05:34,120
so when we record where they land as the columns of a matrix,

78
00:05:34,120 --> 00:05:36,680
each of those columns just has a single number.

79
00:05:38,280 --> 00:05:39,720
This is a 1x2 matrix.

80
00:05:41,640 --> 00:05:45,640
Let's walk through an example of what it means to apply one of these transformations to a vector.

81
00:05:46,200 --> 00:05:51,560
Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.

82
00:05:52,280 --> 00:05:56,600
To follow where a vector with coordinates, say, 4, 3 ends up,

83
00:05:56,600 --> 00:06:00,920
think of breaking up this vector as 4 times i-hat plus 3 times j-hat.

84
00:06:01,640 --> 00:06:05,160
A consequence of linearity is that after the transformation,

85
00:06:05,160 --> 00:06:09,000
the vector will be 4 times the place where i-hat lands, 1,

86
00:06:09,000 --> 00:06:12,680
plus 3 times the place where j-hat lands, negative 2,

87
00:06:12,680 --> 00:06:15,320
which in this case implies that it lands on negative 2.

88
00:06:17,960 --> 00:06:22,360
When you do this calculation purely numerically, it's matrix vector multiplication.

89
00:06:23,240 --> 00:06:30,440
Now, this numerical operation of multiplying a 1x2 matrix by a vector

90
00:06:30,440 --> 00:06:33,160
feels just like taking the dot product of two vectors.

91
00:06:33,160 --> 00:06:36,760
Doesn't that 1x2 matrix just look like a vector that we tipped on its side?

92
00:06:37,880 --> 00:06:43,160
In fact, we could say right now that there's a nice association between 1x2 matrices and 2D

93
00:06:43,160 --> 00:06:47,640
vectors, defined by tilting the numerical representation of a vector on its side

94
00:06:47,640 --> 00:06:52,520
to get the associated matrix, or to tip the matrix back up to get the associated vector.

95
00:06:53,400 --> 00:06:56,040
Since we're just looking at numerical expressions right now,

96
00:06:56,040 --> 00:07:00,600
going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.

97
00:07:01,560 --> 00:07:05,480
But this suggests something that's truly awesome from the geometric view.

98
00:07:05,480 --> 00:07:08,440
There's some kind of connection between linear transformations

99
00:07:08,440 --> 00:07:11,640
that take vectors to numbers and vectors themselves.

100
00:07:12,520 --> 00:07:17,880
Let me show an example that clarifies the significance,

101
00:07:17,880 --> 00:07:21,320
and which just so happens to also answer the dot product puzzle from earlier.

102
00:07:21,960 --> 00:07:23,320
Unlearn what you have learned,

103
00:07:23,320 --> 00:07:27,160
and imagine that you don't already know that the dot product relates to projection.

104
00:07:28,920 --> 00:07:33,480
What I'm going to do here is take a copy of the number line and place it diagonally in space

105
00:07:33,480 --> 00:07:39,000
somehow, with the number 0 sitting at the origin. Now think of the two-dimensional unit vector,

106
00:07:39,000 --> 00:07:44,520
whose tip sits where the number 1 on the number line is. I want to give that guy a name, U-hat.

107
00:07:45,560 --> 00:07:48,280
This little guy plays an important role in what's about to happen,

108
00:07:48,280 --> 00:07:49,960
so just keep him in the back of your mind.

109
00:07:50,920 --> 00:07:54,840
If we project 2D vectors straight onto this diagonal number line,

110
00:07:54,840 --> 00:07:58,920
in effect, we've just defined a function that takes 2D vectors to numbers.

111
00:07:59,480 --> 00:08:03,720
What's more, this function is actually linear, since it passes our visual test

112
00:08:03,720 --> 00:08:08,840
that any line of evenly spaced dots remains evenly spaced once it lands on the number line.

113
00:08:09,080 --> 00:08:16,280
Just to be clear, even though I've embedded the number line in 2D space like this,

114
00:08:16,280 --> 00:08:19,720
the outputs of the function are numbers, not 2D vectors.

115
00:08:19,720 --> 00:08:23,640
You should think of a function that takes in two coordinates and outputs a single coordinate.

116
00:08:24,920 --> 00:08:29,240
But that vector U-hat is a two-dimensional vector, living in the input space.

117
00:08:29,240 --> 00:08:33,160
It's just situated in such a way that overlaps with the embedding of the number line.

118
00:08:33,160 --> 00:08:39,960
With this projection, we just defined a linear transformation from 2D vectors to numbers,

119
00:08:39,960 --> 00:08:44,600
so we're going to be able to find some kind of 1x2 matrix that describes that transformation.

120
00:08:45,320 --> 00:08:49,960
To find that 1x2 matrix, let's zoom in on this diagonal number line setup

121
00:08:49,960 --> 00:08:53,240
and think about where I-hat and J-hat each land,

122
00:08:53,240 --> 00:08:56,360
since those landing spots are going to be the columns of the matrix.

123
00:08:58,360 --> 00:09:02,840
This part's super cool. We can reason through it with a really elegant piece of symmetry.

124
00:09:02,920 --> 00:09:05,800
Since I-hat and U-hat are both unit vectors,

125
00:09:05,800 --> 00:09:09,160
projecting I-hat onto the line passing through U-hat

126
00:09:09,160 --> 00:09:13,560
looks totally symmetric to projecting U-hat onto the x-axis.

127
00:09:13,560 --> 00:09:17,240
So when we ask what number does I-hat land on when it gets projected,

128
00:09:17,240 --> 00:09:22,680
the answer is going to be the same as whatever U-hat lands on when it's projected onto the x-axis.

129
00:09:22,680 --> 00:09:28,920
But projecting U-hat onto the x-axis just means taking the x-coordinate of U-hat.

130
00:09:28,920 --> 00:09:34,280
So by symmetry, the number where I-hat lands when it's projected onto that diagonal number line

131
00:09:34,280 --> 00:09:37,560
is going to be the x-coordinate of U-hat. Isn't that cool?

132
00:09:39,080 --> 00:09:43,000
The reasoning is almost identical for the J-hat case. Think about it for a moment.

133
00:09:49,240 --> 00:09:52,280
For all the same reasons, the y-coordinate of U-hat

134
00:09:52,280 --> 00:09:56,520
gives us the number where J-hat lands when it's projected onto the number line copy.

135
00:09:57,400 --> 00:10:00,040
Pause and ponder that for a moment. I just think that's really cool.

136
00:10:00,920 --> 00:10:05,000
So the entries of the 1x2 matrix describing the projection transformation

137
00:10:05,000 --> 00:10:07,160
are going to be the coordinates of U-hat.

138
00:10:07,800 --> 00:10:11,720
And computing this projection transformation for arbitrary vectors in space,

139
00:10:11,720 --> 00:10:15,080
which requires multiplying that matrix by those vectors,

140
00:10:15,080 --> 00:10:18,840
is computationally identical to taking a dot product with U-hat.

141
00:10:21,800 --> 00:10:24,760
This is why taking the dot product with a unit vector

142
00:10:24,840 --> 00:10:30,520
can be interpreted as projecting a vector onto the span of that unit vector and taking the length.

143
00:10:34,120 --> 00:10:36,200
So what about non-unit vectors?

144
00:10:36,200 --> 00:10:40,600
For example, let's say we take that unit vector U-hat, but we scale it up by a factor of 3.

145
00:10:41,240 --> 00:10:44,760
Numerically, each of its components gets multiplied by 3.

146
00:10:44,760 --> 00:10:47,880
So looking at the matrix associated with that vector,

147
00:10:47,880 --> 00:10:52,360
it takes I-hat and J-hat to three times the values where they landed before.

148
00:10:55,400 --> 00:11:00,280
Since this is all linear, it implies more generally that the new matrix can be interpreted

149
00:11:00,280 --> 00:11:04,600
as projecting any vector onto the number line copy and multiplying where it lands by 3.

150
00:11:05,320 --> 00:11:10,360
This is why the dot product with a non-unit vector can be interpreted as first projecting

151
00:11:10,360 --> 00:11:14,920
onto that vector, then scaling up the length of that projection by the length of the vector.

152
00:11:17,720 --> 00:11:19,800
Take a moment to think about what happened here.

153
00:11:19,800 --> 00:11:23,000
We had a linear transformation from 2D space to the number line,

154
00:11:23,000 --> 00:11:26,920
which was not defined in terms of numerical vectors or numerical dot products,

155
00:11:26,920 --> 00:11:30,760
it was just defined by projecting space onto a diagonal copy of the number line.

156
00:11:31,400 --> 00:11:37,080
But because the transformation is linear, it was necessarily described by some 1x2 matrix.

157
00:11:37,080 --> 00:11:42,360
And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side

158
00:11:42,360 --> 00:11:47,880
and taking a dot product, this transformation was inescapably related to some 2D vector.

159
00:11:48,680 --> 00:11:52,600
The lesson here is that any time you have one of these linear transformations

160
00:11:52,600 --> 00:11:56,280
whose output space is the number line, no matter how it was defined,

161
00:11:56,280 --> 00:12:00,440
there's going to be some unique vector v corresponding to that transformation,

162
00:12:00,440 --> 00:12:05,400
in the sense that applying the transformation is the same thing as taking a dot product with that vector.

163
00:12:08,840 --> 00:12:11,160
To me, this is utterly beautiful.

164
00:12:11,800 --> 00:12:14,360
It's an example of something in math called duality.

165
00:12:14,360 --> 00:12:18,040
Duality shows up in many different ways and forms throughout math,

166
00:12:18,040 --> 00:12:20,360
and it's super tricky to actually define.

167
00:12:20,360 --> 00:12:26,040
Loosely speaking, it refers to situations where you have a natural but surprising correspondence

168
00:12:26,040 --> 00:12:28,440
between two types of mathematical thing.

169
00:12:29,000 --> 00:12:31,400
For the linear algebra case that you just learned about,

170
00:12:31,400 --> 00:12:35,880
you'd say that the dual of a vector is the linear transformation that it encodes,

171
00:12:36,760 --> 00:12:40,600
and the dual of a linear transformation from some space to one dimension

172
00:12:40,600 --> 00:12:43,000
is a certain vector in that space.

173
00:12:43,240 --> 00:12:47,800
So to sum up, on the surface, the dot product is a very useful geometric tool

174
00:12:47,800 --> 00:12:52,920
for understanding projections and for testing whether or not vectors tend to point in the same direction.

175
00:12:52,920 --> 00:12:57,240
And that's probably the most important thing for you to remember about the dot product.

176
00:12:57,240 --> 00:13:02,120
But at a deeper level, dotting two vectors together is a way to translate one of them

177
00:13:02,120 --> 00:13:04,440
into the world of transformations.

178
00:13:04,440 --> 00:13:08,040
Again, numerically, this might feel like a silly point to emphasize.

179
00:13:08,040 --> 00:13:09,960
It's just too computationally.

180
00:13:10,040 --> 00:13:13,240
But the reason I find this so important is that throughout math,

181
00:13:13,240 --> 00:13:17,320
when you're dealing with a vector, once you really get to know its personality,

182
00:13:17,320 --> 00:13:21,720
sometimes you realize that it's easier to understand it not as an arrow in space,

183
00:13:21,720 --> 00:13:25,640
but as the physical embodiment of a linear transformation.

184
00:13:25,640 --> 00:13:30,440
It's as if the vector is really just a conceptual shorthand for a certain transformation,

185
00:13:30,440 --> 00:13:35,640
since it's easier for us to think about arrows in space rather than moving all of that space.

186
00:13:35,880 --> 00:13:40,440
In the next video, you'll see another really cool example of this duality in action

187
00:13:40,440 --> 00:13:42,440
as I talk about the cross product.

