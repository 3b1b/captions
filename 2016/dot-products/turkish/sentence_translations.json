[
 {
  "input": "[Music] Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[Beethoven'ın &quot;Ode to Joy&quot; adlı eseri piyanonun sonuna kadar çalıyor. ] Geleneksel olarak nokta çarpımlar, doğrusal cebir dersinin çok erken safhalarında, genellikle de en başında tanıtılan bir şeydir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "Bu yüzden onları dizide bu kadar geriye itmiş olmam garip görünebilir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "Bunu yaptım çünkü konuyu tanıtmanın standart bir yolu var, bu da vektörlere ilişkin temel bir anlayıştan başka bir şey gerektirmez, ancak nokta çarpımlarının matematikte oynadığı rolün daha kapsamlı anlaşılması yalnızca doğrusal dönüşümlerin ışığı altında bulunabilir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "Ancak bundan önce, nokta ürünlerinin piyasaya sürülmesinin standart yolunu kısaca ele almama izin verin; bunun en azından bazı izleyiciler için kısmen incelendiğini varsayıyorum.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "Sayısal olarak, eğer aynı boyutta iki vektörünüz varsa, aynı uzunlukta iki sayı listesi varsa, bunların nokta çarpımını almak, tüm koordinatları eşleştirmek, bu çiftleri birbiriyle çarpmak ve sonucu eklemek anlamına gelir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "Yani 3, 4 ile noktalanan 1, 2 vektörü 1 çarpı 3 artı 2 çarpı 4 olur.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "1, 8, 5, 3'ün noktalı vektörü 6, 2, 8, 3, 6 çarpı 1 artı 2 çarpı 8 artı 8 çarpı 5 artı 3 çarpı 3 olur.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "Şans eseri, bu hesaplamanın gerçekten güzel bir geometrik yorumu var.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "İki vektör (v ve w) arasındaki nokta çarpımı düşünmek için, w'nin v'nin başlangıç noktasından ve ucundan geçen çizgiye izdüşümünü hayal edin.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "Bu izdüşümün uzunluğunu v uzunluğuyla çarptığınızda v nokta w nokta çarpımını elde edersiniz.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "W'nin bu izdüşümünün v'nin ters yönünü göstermesi dışında, bu nokta çarpım aslında negatif olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "Yani iki vektör genellikle aynı yöne işaret ettiğinde bunların nokta çarpımı pozitiftir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "Dik olduklarında, yani birinin diğerine izdüşümünün sıfır vektörü olduğu anlamına gelir, nokta çarpımları sıfırdır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "Ve eğer genel olarak ters yöne işaret ediyorlarsa, nokta çarpımları negatif olur.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "Bu yorum garip bir şekilde asimetriktir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "İki vektöre çok farklı davranır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "Yani bunu ilk öğrendiğimde sıranın önemli olmadığına şaşırdım.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "Bunun yerine v'yi w'ye yansıtabilir, yansıtılan v'nin uzunluğunu w'nin uzunluğuyla çarpabilir ve aynı sonucu elde edebilirsiniz.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "Yani bu gerçekten farklı bir süreç gibi gelmiyor mu?",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "İşte sıranın neden önemli olmadığına dair sezgi.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "Eğer v ve w aynı uzunluğa sahip olsaydı, bir miktar simetriden yararlanabilirdik.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "W'yi v'ye yansıtmak, ardından bu projeksiyonun uzunluğunu v'nin uzunluğuyla çarpmak, v'yi w'ye yansıtmanın ve ardından bu projeksiyonun uzunluğunu w'nin uzunluğuyla çarpmanın tam bir ayna görüntüsüdür.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "Şimdi bunlardan birini, örneğin v'yi, eşit uzunluğa sahip olmayacak şekilde 2 gibi bir sabitle ölçeklendirirseniz, simetri bozulur.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "Ama gelin bu yeni vektör (2 çarpı v ve w) arasındaki nokta çarpımı nasıl yorumlayacağımızı düşünelim.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "W'nin v üzerine yansıtıldığını düşünüyorsanız, o zaman 2v nokta w nokta çarpımı v nokta w nokta çarpımının tam olarak iki katı olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "Bunun nedeni, v'yi 2'ye ölçeklendirdiğinizde, w'nin izdüşümünün uzunluğunu değiştirmemesi, ancak üzerine izdüşüm yaptığınız vektörün uzunluğunu iki katına çıkarmasıdır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "Ama diğer taraftan diyelim ki v'nin w'ye yansıtılmasını düşünüyorsunuz.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "Bu durumda izdüşümün uzunluğu, v'yi 2 ile çarptığımızda ölçeklenen şeydir, ancak üzerine izdüşümü yaptığınız vektörün uzunluğu sabit kalır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "Yani genel etki hala nokta çarpımının iki katına çıkmasıdır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "Yani bu durumda simetri bozulsa da bu ölçeklendirmenin nokta çarpımın değeri üzerindeki etkisi her iki yorumda da aynıdır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "Bu şeyleri ilk öğrendiğimde kafamı karıştıran bir büyük soru daha var.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "Koordinatları eşleştirme, çiftleri çarpma ve bunları bir araya toplama şeklindeki bu sayısal sürecin neden projeksiyonla bir ilgisi var?",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "Tatmin edici bir cevap vermek ve nokta çarpımın öneminin hakkını tam olarak vermek için, burada biraz daha derinlerde olan ve genellikle dualite adıyla anılan bir şeyi ortaya çıkarmamız gerekiyor.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "Ancak buna girmeden önce, birden fazla boyuttan tek boyuta, yani sadece sayı doğrusuna olan doğrusal dönüşümler hakkında konuşmaya biraz zaman ayırmam gerekiyor.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "These are functions that take in a 2d vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2d input and a 1d output.",
  "translatedText": "Bunlar 2 boyutlu bir vektör alan ve bir miktar sayı veren işlevlerdir, ancak doğrusal dönüşümler elbette 2 boyutlu girdi ve 1 boyutlu çıktılı sıradan işlevinizden çok daha sınırlıdır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "Yüksek boyutlardaki dönüşümlerde olduğu gibi, 3. Bölüm'de bahsettiğim gibi, bu fonksiyonları doğrusal hale getiren bazı biçimsel özellikler vardır, ancak nihai amacımızdan dikkatimizi dağıtmamak için burada bunları kasıtlı olarak görmezden geleceğim ve bunun yerine tüm resmi şeylere eşdeğer olan belirli bir görsel özelliğe odaklanın.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "Eşit aralıklı noktalardan oluşan bir çizgi alıp bir dönüşüm uygularsanız, doğrusal bir dönüşüm bu noktaları sayı doğrusu olan çıktı alanına indiklerinde eşit aralıklı tutacaktır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "Aksi takdirde, eşit olmayan şekilde aralıklı bir dizi nokta varsa dönüşümünüz doğrusal değildir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "Daha önce gördüğümüz durumlarda olduğu gibi, bu doğrusal dönüşümlerden biri tamamen i-hat ve j-hat'ı nereye götürdüğüne göre belirlenir, ancak bu sefer bu temel vektörlerin her biri sadece bir sayıya denk gelir, yani nereye kaydettiğimizde bir matrisin sütunları olarak inerler, bu sütunların her biri tek bir sayıya sahiptir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "Bu 1x2'lik bir matristir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "Bu dönüşümlerden birini bir vektöre uygulamanın ne anlama geldiğini gösteren bir örnek üzerinden gidelim.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "Diyelim ki i-hat'ı 1'e ve j-hat'ı negatif 2'ye götüren doğrusal bir dönüşümünüz var.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "Koordinatları 4, 3 olan bir vektörün nerede biteceğini takip etmek için, bu vektörü 4 çarpı i-hat artı 3 çarpı j-hat şeklinde bölmeyi düşünün.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "Doğrusallığın bir sonucu, dönüşümden sonra vektörün, i-hat'ın indiği yerin 4 katı, 1 artı j-hat'ın indiği yerin 3 katı, negatif 2 olması, bu durumda negatife indiği anlamına gelir. 2.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "Bu hesaplamayı tamamen sayısal olarak yaptığınızda, bu matris vektör çarpımıdır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "Şimdi, 1x2'lik bir matrisi bir vektörle çarpmaya ilişkin bu sayısal işlem, iki vektörün nokta çarpımını almak gibi hissettiriyor.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "Bu 1x2'lik matris, kendi tarafına eğdiğimiz bir vektöre benzemiyor mu?",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "Aslında, şu anda 1x2 matrisler ile 2 boyutlu vektörler arasında güzel bir ilişki olduğunu söyleyebiliriz; bu ilişki, ilgili matrisi elde etmek için bir vektörün sayısal gösterimini kendi tarafına eğerek veya ilişkili vektörü elde etmek için matrisi yukarı doğru eğerek tanımlanır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "Şu anda sadece sayısal ifadelere baktığımız için, vektörler ve 1x2 matrisler arasında ileri geri gitmek aptalca bir şey gibi gelebilir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "Ancak bu, geometrik açıdan gerçekten muhteşem bir şeyi akla getiriyor.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "Vektörleri sayılara götüren doğrusal dönüşümler ile vektörlerin kendisi arasında bir tür bağlantı vardır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "Önemini açıklığa kavuşturacak ve daha önceki nokta çarpım bulmacasına da cevap verecek bir örnek göstereyim.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "Öğrendiklerinizi unutun ve nokta çarpımın projeksiyonla ilgili olduğunu henüz bilmediğinizi hayal edin.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "Burada yapacağım şey sayı doğrusunun bir kopyasını alıp onu bir şekilde uzaya çapraz olarak, 0 sayısı orijinde olacak şekilde yerleştirmek.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Now think of the two-dimensional unit vector, whose tip sits where the number 1 on the number line is.",
  "translatedText": "Şimdi ucu sayı doğrusunda 1 sayısının olduğu yerde bulunan iki boyutlu birim vektörü düşünün.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I want to give that guy a name, U-hat.",
  "translatedText": "Bu adama bir isim vermek istiyorum, U-hat.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "Bu küçük adam olacaklarda önemli bir rol oynuyor, o yüzden onu aklınızın bir köşesinde tutun.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "If we project 2D vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2D vectors to numbers.",
  "translatedText": "2B vektörleri doğrudan bu çapraz sayı doğrusuna yansıtırsak, aslında, 2B vektörleri sayılara götüren bir fonksiyon tanımlamış oluruz.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "Dahası, bu fonksiyon aslında doğrusaldır, çünkü eşit aralıklı noktalardan oluşan herhangi bir çizginin sayı doğrusuna geldiğinde eşit aralıklı kaldığı görsel testimizi geçmiştir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2D space like this, the outputs of the function are numbers, not 2D vectors.",
  "translatedText": "Açık olmak gerekirse, sayı doğrusunu bu şekilde 2 boyutlu uzaya yerleştirmiş olsam da, fonksiyonun çıktıları 2 boyutlu vektörler değil sayılardır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "İki koordinat alan ve tek bir koordinat çıktısı veren bir fonksiyon düşünmelisiniz.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But that vector U-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "Ancak bu U-hat vektörü, girdi uzayında yaşayan iki boyutlu bir vektördür.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "Sadece sayı doğrusunun yerleşimi ile örtüşecek şekilde konumlandırılmıştır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2D vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "Bu projeksiyonla, 2 boyutlu vektörlerden sayılara doğrusal bir dönüşüm tanımladık, böylece bu dönüşümü tanımlayan bir çeşit 1x2 matris bulabileceğiz.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where I-hat and J-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "Bu 1x2'lik matrisi bulmak için, bu çapraz sayı doğrusu düzenini yakınlaştıralım ve I-hat ve J-hat'ın her birinin nereye düştüğünü düşünelim, çünkü bu iniş noktaları matrisin sütunları olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This part's super cool.",
  "translatedText": "Bu kısım süper harika.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "Gerçekten zarif bir simetri parçasıyla bunun üzerinden akıl yürütebiliriz.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Since I-hat and U-hat are both unit vectors, projecting I-hat onto the line passing through U-hat looks totally symmetric to projecting U-hat onto the x-axis.",
  "translatedText": "I-hat ve U-hat'ın her ikisi de birim vektörler olduğundan, I-hat'ın U-hat'tan geçen çizgiye izdüşümü, U-hat'ın x eksenine izdüşümüne tamamen simetrik görünür.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So when we ask what number does I-hat land on when it gets projected, the answer is going to be the same as whatever U-hat lands on when it's projected onto the x-axis.",
  "translatedText": "Yani I-hat'ın yansıtıldığında hangi sayıya indiğini sorduğumuzda cevap, U-hat'ın x eksenine yansıtıldığında hangi sayıya indiği ile aynı olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But projecting U-hat onto the x-axis just means taking the x-coordinate of U-hat.",
  "translatedText": "Ancak U-hat'ı x eksenine yansıtmak, U-hat'ın x koordinatını almak anlamına gelir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So by symmetry, the number where I-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of U-hat.",
  "translatedText": "Yani simetri gereği, I-hat'ın çapraz sayı doğrusuna izdüşümü yapıldığında düştüğü sayı, U-hat'ın x koordinatı olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "Çok hoş değil mi?",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The reasoning is almost identical for the J-hat case.",
  "translatedText": "J-şapka vakası için de mantık neredeyse aynı.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "Bir anlığına düşünün.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For all the same reasons, the y-coordinate of U-hat gives us the number where J-hat lands when it's projected onto the number line copy.",
  "translatedText": "Aynı nedenlerden dolayı, U-hat'ın y-koordinatı bize J-hat'ın sayı doğrusu kopyasına yansıtıldığında düştüğü yerin sayısını verir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "Bir an durup bunu düşünün.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "Bence bu gerçekten harika.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of U-hat.",
  "translatedText": "Yani izdüşüm dönüşümünü tanımlayan 1x2 matrisinin girdileri U-hat'ın koordinatları olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with U-hat.",
  "translatedText": "Ve uzaydaki rastgele vektörler için bu projeksiyon dönüşümünü hesaplamak, ki bu matrisin bu vektörlerle çarpılmasını gerektirir, hesaplama açısından U-hat ile bir nokta çarpımı almakla aynıdır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "Bu nedenle bir birim vektör ile iç çarpımın alınması, bir vektörün o birim vektörün açıklığına izdüşümünün alınması ve uzunluğunun alınması şeklinde yorumlanabilir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "Peki birim olmayan vektörler ne olacak?",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For example, let's say we take that unit vector U-hat, but we scale it up by a factor of 3.",
  "translatedText": "Örneğin, diyelim ki U-hat birim vektörünü aldık ama ölçeğini 3 katına çıkardık.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "Sayısal olarak bileşenlerinin her biri 3 ile çarpılır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes I-hat and J-hat to three times the values where they landed before.",
  "translatedText": "Yani bu vektörle ilişkili matrise baktığımızda, I-hat ve J-hat'ın daha önce geldikleri değerlerin üç katına çıktığını görüyoruz.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "Bunların hepsi doğrusal olduğundan, daha genel olarak yeni matrisin herhangi bir vektörü sayı doğrusu kopyasına yansıttığı ve geldiği yeri 3 ile çarptığı şeklinde yorumlanabileceği anlamına gelir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "Birim olmayan bir vektöre sahip nokta çarpımın, önce o vektöre izdüşümü, ardından bu izdüşümü uzunluğunun vektörün uzunluğu kadar büyütülmesi olarak yorumlanabilmesinin nedeni budur.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "Burada olanları düşünmek için bir dakikanızı ayırın.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "2 boyutlu uzaydan sayı doğrusuna doğru doğrusal bir dönüşüm yaşadık; bu, sayısal vektörler ya da sayısal nokta çarpımları ile tanımlanmamıştı, sadece uzayın sayı doğrusunun çapraz bir kopyasına yansıtılmasıyla tanımlanıyordu.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "Ancak dönüşüm doğrusal olduğu için zorunlu olarak 1x2'lik bir matris tarafından tanımlanıyor.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "Ve 1x2'lik bir matrisi 2 boyutlu bir vektörle çarpmak, o matrisi kendi tarafına çevirip nokta çarpımı almakla aynı şey olduğundan, bu dönüşümün kaçınılmaz olarak 2 boyutlu bir vektörle ilişkili olması kaçınılmazdı.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "Buradan alınacak ders şu; çıktı uzayı sayı doğrusu olan bu doğrusal dönüşümlerden birine sahip olduğunuzda, nasıl tanımlanırsa tanımlansın, bu dönüşüme karşılık gelen benzersiz bir v vektörü olacaktır, yani dönüşümün uygulanması şu anlama gelir: bu vektörle bir iç çarpım almakla aynı şeydir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "Bana göre bu son derece güzel.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "Bu matematikte dualite denilen bir şeyin örneğidir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "Dualite matematikte birçok farklı şekilde ve biçimde ortaya çıkar ve aslında tanımlanması çok zordur.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "Genel anlamda konuşursak, iki tür matematiksel şey arasında doğal ama şaşırtıcı bir yazışmanın olduğu durumları ifade eder.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "Az önce öğrendiğiniz lineer cebir durumu için, bir vektörün dualinin kodladığı lineer dönüşüm olduğunu ve bir uzaydan bir boyuta lineer dönüşümün dualinin o uzaydaki belirli bir vektör olduğunu söyleyebilirsiniz.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "Özetlemek gerekirse, yüzeyde nokta çarpımı, projeksiyonları anlamak ve vektörlerin aynı yöne işaret edip etmediğini test etmek için çok yararlı bir geometrik araçtır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "Ve bu muhtemelen sizin için nokta çarpım hakkında hatırlamanız gereken en önemli şey.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "Ancak daha derin bir düzeyde, iki vektörü bir araya getirmek, bunlardan birini dönüşümler dünyasına dönüştürmenin bir yoludur.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "Yine sayısal olarak bu, vurgulanması gereken saçma bir nokta gibi görünebilir.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It's just too computationally.",
  "translatedText": "Bu çok fazla hesaplamaya dayalı.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "Ama bunu bu kadar önemli bulmamın nedeni, matematik boyunca, bir vektörle uğraşırken, onun kişiliğini gerçekten tanıdığınızda, bazen onu uzaydaki bir ok olarak değil, bir ok olarak anlamanın daha kolay olduğunu fark etmenizdir. doğrusal bir dönüşümün fiziksel düzenlemesi.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space.",
  "translatedText": "Sanki vektör aslında belirli bir dönüşümün kavramsal bir kısaltmasıdır, çünkü uzaydaki okları düşünmek bizim için tüm uzayı hareket ettirmekten daha kolaydır.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action as I talk about the cross product.",
  "translatedText": "Bir sonraki videoda, ben çapraz çarpımdan bahsederken, bu dualitenin gerçekten harika bir örneğini daha göreceksiniz.",
  "model": "google_nmt",
  "n_reviews": 0
 }
]