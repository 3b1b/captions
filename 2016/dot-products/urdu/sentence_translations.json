[
 {
  "input": "Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[موسیقی] روایتی طور پر، ڈاٹ پروڈکٹس ایسی چیز ہوتی ہیں جو واقعی ابتدائی طور پر ایک لکیری الجبرا کورس میں متعارف کرائی جاتی ہیں، عام طور پر شروع میں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "تو یہ عجیب لگ سکتا ہے کہ میں نے انہیں سیریز میں اب تک پیچھے دھکیل دیا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "میں نے یہ اس لیے کیا کیونکہ موضوع کو متعارف کرانے کا ایک معیاری طریقہ ہے، جس کے لیے ویکٹرز کی بنیادی تفہیم کے علاوہ کچھ نہیں، لیکن اس کردار کی مکمل تفہیم جو ڈاٹ پروڈکٹس ریاضی میں ادا کرتے ہیں، صرف لکیری تبدیلیوں کی روشنی میں ہی پایا جا سکتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "اس سے پہلے، اگرچہ، میں مختصراً اس معیاری طریقے کا احاطہ کرتا ہوں جس میں ڈاٹ پروڈکٹس متعارف کرائے جاتے ہیں، جس کا میں فرض کر رہا ہوں کہ کم از کم متعدد ناظرین کے لیے جزوی طور پر جائزہ لیا گیا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "عددی طور پر، اگر آپ کے پاس ایک ہی جہت کے دو ویکٹر ہیں، ایک ہی لمبائی والے نمبروں کی دو فہرستیں، ان کے ڈاٹ پروڈکٹ کو لینے کا مطلب ہے تمام نقاط کو جوڑنا، ان جوڑوں کو ایک ساتھ ضرب دینا، اور نتیجہ شامل کرنا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "تو 3، 4 کے ساتھ ویکٹر 1، 2 ڈاٹڈ 1 ضرب 3 جمع 2 گنا 4 ہوگا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "1، 8، 5، 3 کے ساتھ ویکٹر 6، 2، 8، 3 ڈاٹڈ 6 گنا 1 جمع 2 گنا 8 جمع 8 گنا 5 جمع 3 گنا 3 ہوگا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "خوش قسمتی سے، اس کمپیوٹیشن میں واقعی ایک عمدہ ہندسی تشریح ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "دو ویکٹرز، v اور w کے درمیان ڈاٹ پروڈکٹ کے بارے میں سوچنے کے لیے، w کو اس لکیر پر پیش کرنے کا تصور کریں جو اصل اور v کے سرے سے گزرتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "اس پروجیکشن کی لمبائی کو v کی لمبائی سے ضرب کرنے سے، آپ کے پاس ڈاٹ پروڈکٹ v ڈاٹ ڈبلیو ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "سوائے اس کے کہ جب w کا یہ پروجیکشن v سے مخالف سمت میں اشارہ کر رہا ہو، وہ ڈاٹ پروڈکٹ دراصل منفی ہو گا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "لہذا جب دو ویکٹر عام طور پر ایک ہی سمت میں اشارہ کر رہے ہوتے ہیں، تو ان کا ڈاٹ پروڈکٹ مثبت ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "جب وہ کھڑے ہوتے ہیں، یعنی ایک کا دوسرے پر پروجیکشن صفر ویکٹر ہوتا ہے، تو ان کا ڈاٹ پروڈکٹ صفر ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "اور اگر وہ عام طور پر مخالف سمت کی طرف اشارہ کرتے ہیں، تو ان کا ڈاٹ پروڈکٹ منفی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "اب، یہ تشریح عجیب طور پر غیر متناسب ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "یہ دونوں ویکٹر کے ساتھ بہت مختلف سلوک کرتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "تو جب میں نے پہلی بار یہ سیکھا تو میں حیران رہ گیا کہ حکم سے کوئی فرق نہیں پڑتا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "اس کے بجائے آپ v کو w پر پروجیکٹ کر سکتے ہیں، متوقع v کی لمبائی کو w کی لمبائی سے ضرب دے سکتے ہیں، اور وہی نتیجہ حاصل کر سکتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "میرا مطلب ہے، کیا یہ واقعی ایک مختلف عمل کی طرح محسوس نہیں ہوتا؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "حکم سے کوئی فرق کیوں نہیں پڑتا اس کے لیے یہاں وجدان ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "اگر v اور w کی لمبائی ایک ہی ہے تو ہم کچھ ہم آہنگی کا فائدہ اٹھا سکتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "چونکہ w کو v پر پیش کرنا، پھر اس پروجیکشن کی لمبائی کو v کی لمبائی سے ضرب کرنا، v کو w پر پیش کرنے کا ایک مکمل عکس ہے، پھر اس پروجیکشن کی لمبائی کو w کی لمبائی سے ضرب دینا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "اب، اگر آپ ان میں سے کسی ایک کو پیمانہ کرتے ہیں، تو v کو کچھ مستقل 2 جیسے کہیے، تاکہ ان کی لمبائی برابر نہ ہو، توازن ٹوٹ جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "لیکن آئیے سوچتے ہیں کہ اس نئے ویکٹر، 2 بار v، اور w کے درمیان ڈاٹ پروڈکٹ کی تشریح کیسے کی جائے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "اگر آپ w کے بارے میں سوچتے ہیں کہ v پر پیش کیا جارہا ہے، تو ڈاٹ پروڈکٹ 2v ڈاٹ ڈبلیو ڈاٹ پروڈکٹ v ڈاٹ ڈبلیو سے بالکل دوگنا ہوگا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "اس کی وجہ یہ ہے کہ جب آپ v کو 2 سے اسکیل کرتے ہیں تو یہ w کے پروجیکشن کی لمبائی کو تبدیل نہیں کرتا ہے، لیکن یہ اس ویکٹر کی لمبائی کو دوگنا کر دیتا ہے جس پر آپ پروجیکٹ کر رہے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "لیکن دوسری طرف، ہم کہتے ہیں کہ آپ v کو w پر پیش کرنے کے بارے میں سوچ رہے تھے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "ٹھیک ہے، اس صورت میں، پروجیکشن کی لمبائی وہ چیز ہے جو اس وقت بڑھ جاتی ہے جب ہم v کو 2 سے ضرب دیتے ہیں، لیکن آپ جس ویکٹر پر پروجیکٹ کر رہے ہیں اس کی لمبائی مستقل رہتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "لہذا مجموعی اثر اب بھی صرف ڈاٹ پروڈکٹ کو دوگنا کرنا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "لہٰذا اگرچہ اس معاملے میں ہم آہنگی ٹوٹ گئی ہے، لیکن اس اسکیلنگ کا اثر ڈاٹ پروڈکٹ کی قدر پر پڑتا ہے دونوں تشریحات کے تحت ایک جیسا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "ایک اور بڑا سوال بھی ہے جس نے مجھے الجھن میں ڈال دیا جب میں نے پہلی بار یہ چیزیں سیکھیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "زمین پر نقاط کو ملانے، جوڑوں کو ضرب دینے اور ان کو ایک ساتھ جوڑنے کے اس عددی عمل کا پروجیکشن سے کوئی تعلق کیوں ہے؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "ٹھیک ہے، ایک تسلی بخش جواب دینے کے لیے، اور ڈاٹ پروڈکٹ کی اہمیت کے ساتھ پورا انصاف کرنے کے لیے، ہمیں یہاں کچھ گہرائی سے کچھ دریافت کرنے کی ضرورت ہے، جسے اکثر دوہری کا نام دیا جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "لیکن اس میں جانے سے پہلے، مجھے متعدد جہتوں سے ایک جہت میں لکیری تبدیلیوں کے بارے میں بات کرنے میں کچھ وقت گزارنا ہوگا، جو کہ صرف نمبر لائن ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "یہ وہ فنکشنز ہیں جو 2d ویکٹر میں لیتے ہیں اور کچھ نمبر نکالتے ہیں، لیکن لکیری تبدیلیاں یقیناً 2d ان پٹ اور 1d آؤٹ پٹ کے ساتھ آپ کے رن آف دی مل فنکشن سے کہیں زیادہ محدود ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "جیسا کہ اعلیٰ جہتوں میں ہونے والی تبدیلیوں کے ساتھ، جیسا کہ میں نے باب 3 میں بات کی تھی، کچھ رسمی خصوصیات ہیں جو ان افعال کو لکیری بناتی ہیں، لیکن میں یہاں ان کو جان بوجھ کر نظر انداز کرنے جا رہا ہوں تاکہ ہمارے آخری مقصد سے توجہ نہ ہٹا سکے، اور اس کے بجائے ایک مخصوص بصری خاصیت پر توجہ مرکوز کریں جو تمام رسمی چیزوں کے مساوی ہو۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "اگر آپ یکساں فاصلہ والے نقطوں کی لائن لیتے ہیں اور تبدیلی کا اطلاق کرتے ہیں، تو ایک لکیری تبدیلی ان نقطوں کو یکساں طور پر فاصلہ رکھے گی جب وہ آؤٹ پٹ اسپیس میں اتریں گے، جو کہ نمبر لائن ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "دوسری صورت میں، اگر نقطوں کی کچھ لکیریں ہیں جو غیر مساوی طور پر فاصلہ رکھتی ہیں، تو آپ کی تبدیلی لکیری نہیں ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "جیسا کہ ہم پہلے دیکھ چکے ہیں، ان لکیری تبدیلیوں میں سے ایک مکمل طور پر اس بات سے طے کی جاتی ہے کہ یہ i-hat اور j-hat کہاں لیتا ہے، لیکن اس بار ان میں سے ہر ایک بنیادی ویکٹر صرف ایک عدد پر اترتا ہے، لہذا جب ہم ریکارڈ کرتے ہیں کہ کہاں ہے وہ میٹرکس کے کالم کے طور پر اترتے ہیں، ان میں سے ہر ایک کالم میں صرف ایک نمبر ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "یہ 1x2 میٹرکس ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "آئیے ایک مثال کے ذریعے چلتے ہیں کہ ان تبدیلیوں میں سے کسی ایک کو ویکٹر پر لاگو کرنے کا کیا مطلب ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "فرض کریں کہ آپ کے پاس ایک لکیری تبدیلی ہے جو i-hat کو 1 اور j-hat کو منفی 2 پر لے جاتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "اس کی پیروی کرنے کے لیے جہاں نقاط کے ساتھ ایک ویکٹر، کہیے، 4، 3 ختم ہوتا ہے، اس ویکٹر کو 4 گنا i-hat جمع 3 گنا j-hat کے طور پر توڑنے کے بارے میں سوچیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "لکیریٹی کا نتیجہ یہ ہے کہ تبدیلی کے بعد، ویکٹر اس جگہ سے 4 گنا ہو گا جہاں i-hat اترتا ہے، 1، علاوہ 3 گنا اس جگہ سے جہاں j-hat اترتا ہے، منفی 2، جس کا مطلب ہے کہ یہ منفی پر اترتا ہے۔ 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "جب آپ یہ حساب خالص عددی طور پر کرتے ہیں، تو یہ میٹرکس ویکٹر ضرب ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "اب، 1x2 میٹرکس کو ویکٹر کے ذریعے ضرب دینے کا یہ عددی عمل بالکل ایسا ہی محسوس ہوتا ہے جیسے دو ویکٹروں کے ڈاٹ پروڈکٹ کو لیا جائے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "کیا وہ 1x2 میٹرکس صرف ایک ویکٹر کی طرح نظر نہیں آتا جسے ہم نے اس کی طرف ٹپ کیا ہے؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "درحقیقت، ہم ابھی کہہ سکتے ہیں کہ 1x2 میٹرکس اور 2D ویکٹرز کے درمیان ایک اچھی ایسوسی ایشن ہے، جس کی وضاحت متعلقہ میٹرکس حاصل کرنے کے لیے کسی ویکٹر کی عددی نمائندگی کو اس کی طرف جھکا کر، یا متعلقہ ویکٹر کو حاصل کرنے کے لیے میٹرکس کو بیک اپ کرنے کے لیے۔ . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "چونکہ ہم ابھی صرف عددی اظہار کو دیکھ رہے ہیں، اس لیے ویکٹر اور 1x2 میٹرکس کے درمیان آگے پیچھے جانا ایک احمقانہ چیز کی طرح محسوس ہو سکتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "لیکن یہ کچھ ایسی تجویز کرتا ہے جو جیومیٹرک نقطہ نظر سے واقعی بہت اچھا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "لکیری تبدیلیوں کے درمیان ایک قسم کا تعلق ہے جو ویکٹر کو نمبروں اور خود ویکٹر تک لے جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "میں ایک مثال دکھاتا ہوں جو اہمیت کو واضح کرتا ہے، اور جو پہلے سے ڈاٹ پروڈکٹ پہیلی کا جواب دینے کے لیے بھی ہوتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "آپ نے جو کچھ سیکھا ہے اس سے جان لیں، اور تصور کریں کہ آپ پہلے سے نہیں جانتے کہ ڈاٹ پروڈکٹ کا تعلق پروجیکشن سے ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "میں یہاں جو کرنے جا رہا ہوں وہ یہ ہے کہ نمبر لائن کی ایک کاپی لیں اور اسے کسی نہ کسی طرح خلاء میں ترچھی طور پر رکھیں، جس میں نمبر 0 اصل پر بیٹھا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "اب دو جہتی یونٹ ویکٹر کے بارے میں سوچیں، جس کی نوک وہیں بیٹھتی ہے جہاں نمبر لائن پر نمبر 1 ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "میں اس لڑکے کو ایک نام دینا چاہتا ہوں، یو ٹوپی۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "یہ چھوٹا لڑکا جو کچھ ہونے والا ہے اس میں اہم کردار ادا کرتا ہے، اس لیے اسے اپنے دماغ کے پیچھے رکھیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "اگر ہم 2D ویکٹر کو سیدھے اس اخترن نمبر لائن پر پیش کرتے ہیں، تو درحقیقت، ہم نے صرف ایک فنکشن کی وضاحت کی ہے جو 2D ویکٹر کو نمبروں تک لے جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "مزید یہ کہ یہ فنکشن دراصل لکیری ہے، کیونکہ یہ ہمارے بصری امتحان کو پاس کرتا ہے کہ یکساں فاصلہ والے نقطوں کی کوئی بھی لائن ایک بار نمبر لائن پر اترنے کے بعد یکساں فاصلہ پر رہتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "صرف واضح کرنے کے لیے، اگرچہ میں نے نمبر لائن کو 2D اسپیس میں اس طرح ایمبیڈ کیا ہے، فنکشن کے آؤٹ پٹ نمبرز ہیں، 2D ویکٹر نہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "آپ کو ایک ایسے فنکشن کے بارے میں سوچنا چاہیے جو دو کوآرڈینیٹ لے اور ایک سنگل کوآرڈینیٹ آؤٹ پٹ کرے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "لیکن وہ ویکٹر U-hat ایک دو جہتی ویکٹر ہے، جو ان پٹ اسپیس میں رہتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "یہ صرف اس طرح واقع ہے جو نمبر لائن کے سرایت کے ساتھ اوورلیپ کرتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "اس پروجیکشن کے ساتھ، ہم نے صرف 2D ویکٹر سے اعداد میں ایک لکیری تبدیلی کی وضاحت کی ہے، لہذا ہم کسی قسم کا 1x2 میٹرکس تلاش کرنے کے قابل ہو جائیں گے جو اس تبدیلی کو بیان کرتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "اس 1x2 میٹرکس کو تلاش کرنے کے لیے، آئیے اس ڈائیگنل نمبر لائن سیٹ اپ پر زوم ان کریں اور سوچیں کہ I-hat اور J-hat ہر ایک لینڈ کہاں ہے، کیونکہ وہ لینڈنگ اسپاٹس میٹرکس کے کالم بننے جا رہے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "یہ حصہ بہت اچھا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "ہم توازن کے واقعی خوبصورت ٹکڑے کے ساتھ اس کے ذریعے استدلال کرسکتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "چونکہ I-hat اور U-hat دونوں یونٹ ویکٹر ہیں، I-hat کو U-hat سے گزرنے والی لائن پر پیش کرنا U-hat کو x-axis پر پیش کرنے کے لیے مکمل طور پر ہم آہنگ نظر آتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "لہٰذا جب ہم پوچھتے ہیں کہ جب I-hat کس نمبر پر آتی ہے تو اس کا اندازہ لگایا جاتا ہے، تو جواب وہی ہوگا جو U-hat پر اترتا ہے جب اسے x-axis پر پیش کیا جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "لیکن U-ہیٹ کو x-axis پر پیش کرنے کا مطلب صرف U-hat کا x-coordinate لینا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "تو ہم آہنگی کے لحاظ سے، وہ نمبر جہاں I-hat اترتا ہے جب اسے اس اخترن نمبر لائن پر پیش کیا جاتا ہے U-hat کا x-coordinate ہو گا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "کیا یہ ٹھنڈا نہیں ہے؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "جے ٹوپی کیس کے لیے استدلال تقریباً یکساں ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "ایک لمحے کے لیے اس کے بارے میں سوچیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "انہی وجوہات کی بنا پر، U-hat کا y-Coordinate ہمیں وہ نمبر دیتا ہے جہاں J-hat اترتا ہے جب اسے نمبر لائن کاپی پر پیش کیا جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "ایک لمحے کے لیے رکیں اور اس پر غور کریں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "مجھے لگتا ہے کہ یہ واقعی بہت اچھا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "لہذا 1x2 میٹرکس کے اندراجات جو پروجیکشن ٹرانسفارمیشن کو بیان کرتے ہیں U-ہیٹ کے نقاط بننے جا رہے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "اور خلا میں صوابدیدی ویکٹرز کے لیے اس پروجیکشن ٹرانسفارمیشن کو کمپیوٹنگ کرنا، جس کے لیے اس میٹرکس کو ان ویکٹرز سے ضرب کرنے کی ضرورت ہوتی ہے، یو ہیٹ کے ساتھ ڈاٹ پروڈکٹ لینے کے حسابی طور پر مماثل ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "یہی وجہ ہے کہ ڈاٹ پروڈکٹ کو یونٹ ویکٹر کے ساتھ لینے کی تشریح اس یونٹ ویکٹر کے اسپین پر کسی ویکٹر کو پیش کرنے اور لمبائی لینے سے کی جا سکتی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "تو غیر یونٹ ویکٹر کے بارے میں کیا خیال ہے؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "مثال کے طور پر، ہم کہتے ہیں کہ ہم اس یونٹ ویکٹر U-hat کو لیتے ہیں، لیکن ہم اسے 3 کے فیکٹر سے پیمانہ کرتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "عددی طور پر، اس کے ہر اجزاء کو 3 سے ضرب دیا جاتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "تو اس ویکٹر سے وابستہ میٹرکس کو دیکھتے ہوئے، یہ I-hat اور J-hat کو تین گنا قدروں تک لے جاتا ہے جہاں وہ پہلے اترے تھے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "چونکہ یہ سب لکیری ہے، اس کا عام طور پر مطلب یہ ہے کہ نئے میٹرکس کو کسی بھی ویکٹر کو نمبر لائن کاپی پر پیش کرنے اور 3 سے ضرب کرنے کے طور پر سمجھا جا سکتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "یہی وجہ ہے کہ ایک نان یونٹ ویکٹر کے ساتھ ڈاٹ پروڈکٹ کو پہلے اس ویکٹر پر پروجیکٹ کرنے، پھر اس پروجیکشن کی لمبائی کو ویکٹر کی لمبائی سے پیمانہ کرنے سے سمجھا جا سکتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "یہاں کیا ہوا اس کے بارے میں سوچنے کے لئے ایک لمحہ نکالیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "ہمارے پاس 2D اسپیس سے نمبر لائن میں ایک لکیری تبدیلی تھی، جس کی وضاحت عددی ویکٹر یا عددی ڈاٹ مصنوعات کے لحاظ سے نہیں کی گئی تھی، اس کی وضاحت صرف نمبر لائن کی ترچھی کاپی پر جگہ کو پیش کرنے سے کی گئی تھی۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "لیکن چونکہ تبدیلی لکیری ہے، اس لیے ضروری طور پر کچھ 1x2 میٹرکس کے ذریعے بیان کیا گیا تھا۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "اور چونکہ 1x2 میٹرکس کو 2D ویکٹر سے ضرب دینا اس میٹرکس کو اس کی طرف موڑنے اور ڈاٹ پروڈکٹ لینے کے مترادف ہے، اس لیے یہ تبدیلی ناگزیر طور پر کچھ 2D ویکٹر سے متعلق تھی۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "یہاں سبق یہ ہے کہ جب بھی آپ کے پاس ان لکیری تبدیلیوں میں سے کوئی ایک ہے جس کی آؤٹ پٹ اسپیس نمبر لائن ہے، اس سے کوئی فرق نہیں پڑتا ہے کہ اس کی تعریف کیسے کی گئی ہے، اس تبدیلی کے مطابق کچھ منفرد ویکٹر v ہوگا، اس معنی میں کہ تبدیلی کو لاگو کرنا وہی چیز جو اس ویکٹر کے ساتھ ڈاٹ پروڈکٹ لے رہی ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "میرے نزدیک یہ بالکل خوبصورت ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "یہ ریاضی میں کسی چیز کی مثال ہے جسے ڈوئلٹی کہتے ہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "دوہرا ریاضی میں بہت سے مختلف طریقوں اور شکلوں میں ظاہر ہوتا ہے، اور حقیقت میں اس کی وضاحت کرنا بہت مشکل ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "ڈھیلے الفاظ میں، یہ ان حالات سے مراد ہے جہاں آپ کے پاس دو قسم کی ریاضیاتی چیزوں کے درمیان قدرتی لیکن حیران کن خط و کتابت ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "لکیری الجبرا کیس کے لیے جس کے بارے میں آپ نے ابھی سیکھا ہے، آپ کہیں گے کہ ایک ویکٹر کا دوہرا وہ لکیری تبدیلی ہے جسے یہ انکوڈ کرتا ہے، اور کسی جگہ سے ایک جہت تک لکیری تبدیلی کا دوہرا اس خلا میں ایک مخصوص ویکٹر ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "لہٰذا خلاصہ یہ ہے کہ سطح پر، ڈاٹ پروڈکٹ تخمینوں کو سمجھنے اور جانچنے کے لیے ایک بہت ہی مفید ہندسی ٹول ہے کہ آیا ویکٹر ایک ہی سمت میں اشارہ کرتے ہیں یا نہیں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "اور آپ کے لیے ڈاٹ پروڈکٹ کے بارے میں یاد رکھنا شاید سب سے اہم چیز ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "لیکن گہری سطح پر، دو ویکٹروں کو ایک ساتھ باندھنا ان میں سے ایک کو تبدیلیوں کی دنیا میں ترجمہ کرنے کا ایک طریقہ ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "ایک بار پھر، عددی طور پر، یہ زور دینے کے لیے ایک احمقانہ نقطہ کی طرح محسوس ہو سکتا ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "یہ صرف بہت کمپیوٹیشنل ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "لیکن جس وجہ سے مجھے یہ بہت اہم لگتا ہے وہ یہ ہے کہ پوری ریاضی میں، جب آپ کسی ویکٹر کے ساتھ کام کر رہے ہوتے ہیں، ایک بار جب آپ واقعی اس کی شخصیت کو جان لیتے ہیں، تو کبھی کبھی آپ کو احساس ہوتا ہے کہ اسے خلا میں تیر کے طور پر نہیں، بلکہ اس کو سمجھنا آسان ہے۔ لکیری تبدیلی کا جسمانی مجسم۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space t",
  "translatedText": "ایسا لگتا ہے کہ ویکٹر واقعی کسی خاص تبدیلی کے لیے محض ایک تصوراتی شارٹ ہینڈ ہے، کیونکہ ہمارے لیے اس ساری جگہ کو منتقل کرنے کے بجائے خلا میں تیر کے بارے میں سوچنا آسان ہے۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.31
 },
 {
  "input": "o the number line. In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "اگلی ویڈیو میں، آپ اس دوہرے پن کی ایک اور عمدہ مثال دیکھیں گے جب میں کراس پروڈکٹ کے بارے میں بات کرتا ہوں۔ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 820.31,
  "end": 829.19
 }
]