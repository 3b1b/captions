[
 {
  "input": "Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[ベートーベンの「歓喜の歌」がピアノの最後まで演奏されます。 ] 伝統的に、ドット積は線形代数コースの非常に早 い段階で、通常は開始直後に導入されるものです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "したがって、私がシリーズでここまで押し戻したのは奇妙に思われるかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "私がこのようにしたのは、このトピックを紹介する標準的な 方法があり、それにはベクトルの基本的な理解だけが必要 ですが、数学において内積が果たす役割をより完全に理解 するには、実際には線形変換を考慮する必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "ただし、その前に、点積が導入される標準的な方法について簡単に説明しておきます。 これは、多くの視聴者にとって少なくとも部分的には復習されていると思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "数値的には、同じ次元の 2 つのベクトル、つまり同じ 長さの数値の 2 つのリストがある場合、それらの内積 を求めることは、すべての座標をペアにし、それらのペア を掛け合わせて、その結果を加算することを意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "したがって、3、4 が点在するベクトル 1、2 は、1 掛ける 3 と 2 掛ける 4 になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "1、8、5、3 が点在するベクトル 6、2、8、3 は、6 掛ける 1 プラス 2 掛ける 8 プラス 8 掛ける 5 プラス 3 掛ける 3 になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "幸いなことに、この計算には非常に優れた幾何学的解釈が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "2 つのベクトル v と w の間の内積について考えるには、 v の原点と先端を通る直線に w を投影すると想像します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "この投影の長さに v の長さを乗算すると、内積 v dot w が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "この w の射影が v と反対の方向を向いてい る場合を除き、その内積は実際には負になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "したがって、2 つのベクトルが通常同じ方向を向いている場合、それらの内積は正になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "それらが垂直である場合、つまり一方のもう一方への投影がゼ ロ ベクトルである場合、それらの内積はゼロになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "そして、それらがほぼ逆の方向を向いている場合、それらの内積は負になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "さて、この解釈は奇妙なことに非対称です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "2 つのベクトルをまったく異なる方法で扱います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "なので、初めてこのことを知ったとき、順番は関係ないことに驚きました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "代わりに、v を w に投影し、投影された v の長 さに w の長さを乗算すると、同じ結果が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "つまり、それはまったく異なるプロセスのように感じませんか?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "順序が重要ではない理由についての直感は次のとおりです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "v と w がたまたま同じ長さであれば、対称性を活用することができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "w を v に投影し、その投影の長さに v の長さを 掛けることは、v を w に投影し、その投影の長さに w の長さを掛けることの完全な鏡像となるためです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "ここで、そのうちの 1 つ (たとえば v) を 2 などの定数 でスケールして、長さが等しくなるようにすると、対称性が崩れます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "しかし、この新しいベクトル、v の 2 倍と w の間の内積を解釈する方法をよく考えてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "w を v に投影すると考えると、内積 2v dot w は内積 v dot w のちょうど 2 倍になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "これは、v を 2 倍にスケールすると、w の投影の長さは変わ りませんが、投影先のベクトルの長さが 2 倍になるためです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "しかしその一方で、v が w に投影されることを考えていたとしましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "その場合、v を 2 で乗算すると投影の長さがスケーリング されますが、投影しているベクトルの長さは一定のままです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "したがって、全体的な効果は依然として内積を 2 倍にするだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "したがって、この場合は対称性が崩れてい ますが、このスケーリングが内積の値に与 える影響はどちらの解釈でも同じです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "私がこのことを初めて学んだとき、私を混乱させたもう 1 つの大きな疑問があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "一体なぜ、座標を一致させ、ペアを掛け合わせ、それらを加 算するという数値処理が投影と関係があるのでしょうか?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "そうですね、満足のいく答えを与えるために、また内積の重要性を完 全に正当に評価するには、ここで起こっていることをもう少し深く 掘り起こす必要があります。 これはしばしば二重性と呼ばれます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "しかし、本題に入る前に、複数の次元から 1 つの次元 (単 なる数直線) への線形変換について説明する必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "これらは 2D ベクトルを受け取り、何らかの数値を吐き出す関 数ですが、線形変換は、当然のことながら、2D 入力と 1D 出力を使用するありふれた関数よりもはるかに制限されています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "第 3 章で説明したような高次元での変換と同様、これらの関 数を線形にする形式的特性がいくつかありますが、最終目標から 逸れないように、ここでは意図的にそれらを無視します。 すべて の形式的なものと同等の特定の視覚的特性に焦点を当てます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "等間隔のドットの線を取得して変換を適用すると、線形 変換により、これらのドットが出力空間 (数直線) に到達すると、それらのドットは等間隔に保たれます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "それ以外の場合、不均等な間隔の点の線が ある場合、変換は線形ではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "これまでに見たケースと同様に、これらの線形変換の 1 つ は、i-hat と j-hat がどこにかかるかによって 完全に決定されますが、今回は、これらの基底ベクトルのそれ ぞれが数値に到達するだけです。 それらは行列の列として配置 され、それらの各列には 1 つの数値だけが含まれます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "これは 1x2 の行列です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "これらの変換の 1 つをベクトルに適用することが何を意味するのか、例を見てみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "i-hat を 1 に、j-hat をマイナス 2 にする線形変換があるとします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "たとえば 4, 3 などの座標を持つベクトルがどこに終わるかを追跡するには、このベクトル を i-hat の 4 倍と j-hat の 3 倍として分割することを考えてください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "線形性の結果、変換後のベクトルは、i-hat が着地 する場所の 4 倍 1、さらに j-hat が着地す る場所の 3 倍、マイナス 2 になります。 これは、 この場合、負の位置に着地することを意味します。 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "この計算を純粋に数値的に行う場合、それは行列ベクトルの乗算になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "さて、1x2 行列にベクトルを乗算するこの数値演算は、2 つのベクトルの内積を取るのと同じように感じられます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "その 1x2 行列は、単にベクトルを横に傾けたように見えませんか?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "実際、今のところ、1x2 行列と 2D ベクトルの間には適切な関連性があると言 えます。 これは、ベクトルの数値表現を横に傾けて関連する行列を取得するか、行列を 傾けて元の位置に戻して関連するベクトルを取得することによって定義されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "今は数値式を見ているだけなので、ベクトルと 1x2 行列の間を 行ったり来たりするのは愚かなことのように感じるかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "しかし、これは幾何学的な観点から見ると本当に素晴らしいことを示唆しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "ベクトルを数値に変換する線形変換とベクトル 自体の間には、ある種のつながりがあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "重要性を明確にする例を示しますが、これは偶然 にも、先ほどの内積パズルの答えにもなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "これまで学んだことを忘れて、内積が投影に関連し ていることをまだ知らないと想像してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "ここでやろうとしていることは、数直線のコピーを取り、それを何らかの形で空間に斜めに配 置し、数字の 0 を原点に置くことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "ここで、数直線上の数字 1 の位置に先端があ る 2 次元の単位ベクトルについて考えてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "あの男に名前を付けたい、ユーハット。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "この小さな男はこれから起こることにおいて重要な役割を 果たしているので、頭の片隅に置いておいてください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "2D ベクトルをこの対角の数直線上に真っ直ぐ投影すると、事実上 、2D ベクトルを数値に変換する関数を定義したことになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "さらに、この関数は実際には線形であり、等間隔の点の行が数直線上に 到達すると等間隔のままであるという視覚テストに合格するためです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "念のために言っておきますが、このように数直線を 2D 空間に埋め 込んだとしても、関数の出力は 2D ベクトルではなく数値です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "2 つの座標を受け取り、1 つの座標を出力する関数を考える必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "しかし、そのベクトル U ハットは入力空間に存在する 2 次元ベクトルです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "数直線の埋め込みと重なるように配置されているだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "この投影では、2D ベクトルから数値への線形変換を定義しただけなので、その 変換を記述するある種の 1x2 行列を見つけることができるようになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "1x2 行列を見つけるために、この対角線の数直線設定を拡大し て、I ハットと J ハットがそれぞれどこに着地するかを考え てみましょう。 それらの着地スポットは行列の列になるからです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "この部分は超クールです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "非常にエレガントな対称性の作品によって、それを推論することができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "I ハットと U ハットはどちらも単位ベクトルであるため 、I ハットを U ハットを通る線上に投影すると、U ハットを X 軸上に投影するのと完全に対称に見えます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "したがって、I ハットが投影されたときに着地する番号は何なのかと尋ねると、答 えは、U ハットが X 軸に投影されたときに着地するものと同じになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "ただし、U ハットを X 軸に投影するということは、U ハットの X 座標を取得することを意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "したがって、対称性により、I ハットが対角の数直線に投影されたときに着 地する数値が U ハットの x 座標になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "それはクールじゃないですか？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "推論は J-hat 事件でもほぼ同じです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "少し考えてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "同じ理由で、U ハットの y 座標は、J ハットが数直 線のコピーに投影されたときに着地する番号を示します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "立ち止まって、少し考えてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "それは本当に素晴らしいことだと思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "したがって、射影変換を記述する 1x2 行列 のエントリは、U ハットの座標になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "そして、空間内の任意のベクトルに対するこの射影変換を計算 することは、その行列にそれらのベクトルを乗算する必要があ り、計算的には U ハットで内積を求めることと同じです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "このため、単位ベクトルとの内積を求めることは、その単位ベクトル のスパンにベクトルを投影し、長さを求めることと解釈できます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "では、単位でないベクトルはどうなるでしょうか?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "たとえば、単位ベクトル U ハットを 3 倍に拡大するとします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "数値的には、その各コンポーネントは 3 倍されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "したがって、そのベクトルに関連付けられた行列を見ると、I ハ ットと J ハットは以前に到達した値の 3 倍かかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "これはすべて線形であるため、より一般的には、新しい行列は任意のベクトルを数直線の コピーに投影し、その到達点で 3 を乗じるものとして解釈できることを意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "これが、非単位ベクトルとの内積が、最初にそのベクトルに投影され、次にその 投影の長さをベクトルの長さだけスケールアップすると解釈できる理由です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "ここで何が起こったのか少し考えてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "2D 空間から数直線への線形変換がありましたが、これは 数値ベクトルや数値内積によって定義されず、空間を数直 線の対角コピーに投影することによって定義されました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "しかし、変換は線形であるため、必然的に何らかの 1x2 行列で記述されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "そして、1x2 行列に 2D ベクトルを乗算することは、その行列をひっくり返して内積を求める ことと同じであるため、この変換は必然的に何らかの 2D ベクトルに関連することになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "ここでの教訓は、出力空間が数直線であるこれらの線形変換の 1 つがある ときは常に、それがどのように定義されたかに関係なく、変換を適用すると いう意味で、その変換に対応する何らかの一意のベクトル v が存在する ことになるということです。 そのベクトルの内積を取るのと同じことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "私にとって、これはまったく美しいことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "これは数学における双対性と呼ばれるものの一例です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "双対性は数学全体でさまざまな方法や形で現れ ますが、実際に定義するのは非常に困難です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "大まかに言えば、これは 2 種類の数学的なものの間 に自然だが驚くべき対応関係がある状況を指します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "先ほど学んだ線形代数の場合、ベクトルの双対 はそれがエンコードする線形変換であり、ある 空間から 1 次元への線形変換の双対はその 空間内の特定のベクトルであると言えます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "要約すると、表面的には、内積は投影を理解し、ベクトルが同じ方向を向く 傾向があるかどうかをテストするための非常に便利な幾何学的ツールです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "そしてそれはおそらく、ドット積について覚えておくべき最も重要なことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "しかし、より深いレベルでは、2 つのベクトルを点在させるこ とは、そのうちの 1 つを変換の世界に変換する方法です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "繰り返しになりますが、数値的には、これを強調するのは愚かな点のように感じるかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "それはあまりにも計算的です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "しかし、私がこれが非常に重要であると考える理由は、数学全体を通 して、ベクトルを扱うとき、その性質を実際に理解すると、それを 空間の矢印としてではなく、ベクトルとして理解する方が簡単であ ることに気づくことがあるためです。 線形変換の物理的な具体化。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "空間全体を動かすよりも空間内の矢印について考えるほうが簡単なので、ベク トルが実際には特定の変換の単なる概念的な省略表現であるかのようです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.15
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "次のビデオでは、外積について説明しながら、この二重性が実際 に動作している別の本当に素晴らしい例をご覧いただけます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 820.15,
  "end": 829.19
 }
]