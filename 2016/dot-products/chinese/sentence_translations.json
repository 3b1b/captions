[
 {
  "input": "[Music] Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "model": "nmt",
  "translatedText": "[贝多芬的《欢乐颂》，钢琴演奏到最后。 ] 传统上，点积是在线性代数课程中很早 就引入的东西，通常是在开始时引入的。",
  "time_range": [
   16.580000000000005,
   23.14
  ],
  "n_reviews": 0
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "model": "nmt",
  "translatedText": "所以我在这个系列中把它们推迟到这么远可能看起来很奇怪。",
  "time_range": [
   23.14,
   27.32
  ],
  "n_reviews": 0
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "model": "nmt",
  "translatedText": "我这样做是因为有一个标准的方法来介绍这 个主题，它只需要对向量有基本的了解，但 对点积在数学中所扮演的角色的更全面的理 解只能在线性变换的指导下才能真正找到。",
  "time_range": [
   27.32,
   40.84
  ],
  "n_reviews": 0
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "model": "nmt",
  "translatedText": "不过，在此之前，让我简单介绍一下点积的引入标准 方式，我认为至少部分观众已经了解了这种方式。",
  "time_range": [
   40.84,
   49.96
  ],
  "n_reviews": 0
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "model": "nmt",
  "translatedText": "在数字上，如果有两个相同维度的向量 ，两个具有相同长度的数字列表，获 取它们的点积意味着将所有坐标配对 ，将这些对相乘，然后将结果相加。",
  "time_range": [
   49.96,
   64.98
  ],
  "n_reviews": 0
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "model": "nmt",
  "translatedText": "因此向量 1, 2 点缀着 3, 4 将是 1 乘以 3 加 2 乘以 4。",
  "time_range": [
   66.86,
   73.18
  ],
  "n_reviews": 0
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "model": "nmt",
  "translatedText": "向量 6, 2, 8, 3 点缀着 1, 8, 5, 3 将是 6 乘以 1 加 2 乘以 8 加 8 乘以 5 加 3 乘以 3。",
  "time_range": [
   74.58,
   83.72
  ],
  "n_reviews": 0
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "model": "nmt",
  "translatedText": "幸运的是，这个计算有一个非常好的几何解释。",
  "time_range": [
   84.74000000000001,
   88.66
  ],
  "n_reviews": 0
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "model": "nmt",
  "translatedText": "要考虑两个向量 v 和 w 之间的点积，请想象 将 w 投影到穿过原点和 v 尖端的直线上。",
  "time_range": [
   89.34,
   97.98
  ],
  "n_reviews": 0
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "model": "nmt",
  "translatedText": "将此投影的长度乘以 v 的长度，即可得到点积 v 点 w。",
  "time_range": [
   98.78,
   104.46
  ],
  "n_reviews": 0
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "model": "nmt",
  "translatedText": "除非 w 的投影指向 v 的相反 方向，否则该点积实际上将为负。",
  "time_range": [
   106.42,
   112.16
  ],
  "n_reviews": 0
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "model": "nmt",
  "translatedText": "因此，当两个向量通常指向同一方向时，它们的点积为正。",
  "time_range": [
   113.72,
   117.86
  ],
  "n_reviews": 0
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "model": "nmt",
  "translatedText": "当它们垂直时，意味着一个到另一个 的投影为零向量，它们的点积为零。",
  "time_range": [
   119.24,
   125.56
  ],
  "n_reviews": 0
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "model": "nmt",
  "translatedText": "如果它们通常指向相反的方向，则它们的点积为负。",
  "time_range": [
   125.98,
   129.6
  ],
  "n_reviews": 0
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "model": "nmt",
  "translatedText": "现在，这种解释奇怪地不对称。",
  "time_range": [
   131.62,
   134.56
  ],
  "n_reviews": 0
 },
 {
  "input": "It treats the two vectors very differently.",
  "model": "nmt",
  "translatedText": "它对待这两个向量的方式非常不同。",
  "time_range": [
   134.8,
   136.5
  ],
  "n_reviews": 0
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "model": "nmt",
  "translatedText": "所以当我第一次了解到这一点时，我很惊讶顺序并不重要。",
  "time_range": [
   136.88,
   140.0
  ],
  "n_reviews": 0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "model": "nmt",
  "translatedText": "您可以将 v 投影到 w 上，将投影 v 的长度乘以 w 的长度，并得到相同的结果。",
  "time_range": [
   140.96,
   148.22
  ],
  "n_reviews": 0
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "model": "nmt",
  "translatedText": "我的意思是，这难道不感觉是一个完全不同的过程吗？",
  "time_range": [
   150.4,
   152.84
  ],
  "n_reviews": 0
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "model": "nmt",
  "translatedText": "这是为什么顺序不重要的直觉。",
  "time_range": [
   155.32,
   157.76
  ],
  "n_reviews": 0
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "model": "nmt",
  "translatedText": "如果 v 和 w 碰巧具有相同的长度，我们可以利用一些对称性。",
  "time_range": [
   158.44,
   162.18
  ],
  "n_reviews": 0
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "model": "nmt",
  "translatedText": "由于将 w 投影到 v 上，然后将该投影的长度乘 以 v 的长度，因此是将 v 投影到 w 上， 然后将该投影的长度乘以 w 的长度的完整镜像。",
  "time_range": [
   163.08,
   175.24
  ],
  "n_reviews": 0
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "model": "nmt",
  "translatedText": "现在，如果您将其中一个（例如 v）缩放为某个常数（例如 2），从而使它们的长度不相等，则对称性就会被打破。",
  "time_range": [
   177.28,
   184.36
  ],
  "n_reviews": 0
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "model": "nmt",
  "translatedText": "但是让我们考虑一下如何解释这个新向量 （2 乘以 v）和 w 之间的点积。",
  "time_range": [
   185.02,
   190.04
  ],
  "n_reviews": 0
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "model": "nmt",
  "translatedText": "如果您将 w 视为投影到 v 上，则点积 2v d ot w 将恰好是点积 v dot w 的两倍。",
  "time_range": [
   190.88,
   199.72
  ],
  "n_reviews": 0
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "model": "nmt",
  "translatedText": "这是因为当您将 v 缩放 2 时，它不会改变 w 投影的长度，但会将您投影到的向量的长度加倍。",
  "time_range": [
   200.46,
   209.52
  ],
  "n_reviews": 0
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "model": "nmt",
  "translatedText": "但另一方面，假设您正在考虑将 v 投影到 w 上。",
  "time_range": [
   210.46,
   214.2
  ],
  "n_reviews": 0
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "model": "nmt",
  "translatedText": "好吧，在这种情况下，当我们将 v 乘以 2 时，投 影的长度就会缩放，但投影到的向量的长度保持不变。",
  "time_range": [
   214.9,
   223.0
  ],
  "n_reviews": 0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "model": "nmt",
  "translatedText": "所以总体效果仍然是点积的两倍。",
  "time_range": [
   223.0,
   226.66
  ],
  "n_reviews": 0
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "model": "nmt",
  "translatedText": "因此，即使在这种情况下对称性 被打破，这种缩放对点积值的影 响在两种解释下都是相同的。",
  "time_range": [
   227.28,
   234.86
  ],
  "n_reviews": 0
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "model": "nmt",
  "translatedText": "当我第一次学习这些东西时，还有一个让我困惑的大问题。",
  "time_range": [
   236.64,
   240.34
  ],
  "n_reviews": 0
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "model": "nmt",
  "translatedText": "到底为什么这种匹配坐标、相乘并将它 们加在一起的数值过程与投影有关？",
  "time_range": [
   240.84,
   248.74
  ],
  "n_reviews": 0
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "model": "nmt",
  "translatedText": "好吧，为了给出一个令人满意的答案，并且为 了充分理解点积的重要性，我们需要挖掘这里 发生的更深入的事情，这通常被称为对偶性。",
  "time_range": [
   250.64,
   261.4
  ],
  "n_reviews": 0
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "model": "nmt",
  "translatedText": "但在开始讨论之前，我需要花一些时间讨论 从多维到一维的线性变换，这只是数轴。",
  "time_range": [
   262.14,
   270.04
  ],
  "n_reviews": 0
 },
 {
  "input": "These are functions that take in a 2d vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2d input and a 1d output.",
  "model": "nmt",
  "translatedText": "这些函数接受 2d 向量并输出一些数字 ，但线性变换当然比具有 2d 输入和 1d 输出的普通函数受到更多限制。",
  "time_range": [
   272.42,
   282.3
  ],
  "n_reviews": 0
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "model": "nmt",
  "translatedText": "与更高维度的变换一样，就像我在第 3 章中讨 论的那样，有一些形式属性使这些函数成为线性， 但我将故意忽略这些，以免分散我们的最终目标， 相反专注于某种与所有正式事物等效的视觉属性。",
  "time_range": [
   283.02,
   298.26
  ],
  "n_reviews": 0
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "model": "nmt",
  "translatedText": "如果您采用一行均匀间隔的点并应用变换， 那么一旦这些点落在输出空间（即数轴） 中，线性变换将使这些点保持均匀间隔。",
  "time_range": [
   299.04,
   311.28
  ],
  "n_reviews": 0
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "model": "nmt",
  "translatedText": "否则，如果有一些点线的间距不均 匀，那么您的变换就不是线性的。",
  "time_range": [
   312.42,
   317.14
  ],
  "n_reviews": 0
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "model": "nmt",
  "translatedText": "与我们之前看到的情况一样，这些线性变换 之一完全由 i-hat 和 j-ha t 的位置决定，但这次每个基向量都落 在一个数字上，因此当我们记录它们以矩 阵的列形式出现，每一列只有一个数字。",
  "time_range": [
   319.22,
   336.82
  ],
  "n_reviews": 0
 },
 {
  "input": "This is a 1x2 matrix.",
  "model": "nmt",
  "translatedText": "这是一个 1x2 矩阵。",
  "time_range": [
   338.46,
   339.84
  ],
  "n_reviews": 0
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "model": "nmt",
  "translatedText": "让我们通过一个示例来了解将这些变换之一应用于向量的含义。",
  "time_range": [
   341.86,
   345.66
  ],
  "n_reviews": 0
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "model": "nmt",
  "translatedText": "假设您有一个线性变换，将 i-hat 变为 1，将 j-hat 变为负 2。",
  "time_range": [
   346.38,
   351.68
  ],
  "n_reviews": 0
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "model": "nmt",
  "translatedText": "要追踪坐标为 4、3 的向量的最终位置，请将此向量分解 为 4 乘以 i-hat 加上 3 乘以 j-hat。",
  "time_range": [
   352.42,
   361.02
  ],
  "n_reviews": 0
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "model": "nmt",
  "translatedText": "线性的结果是，变换后，向量将是 i-h at 落地位置的 4 倍，即 1，再加 上 j-hat 落地位置的 3 倍，即 负 2，在本例中意味着它落在负数上2.",
  "time_range": [
   361.84,
   375.78
  ],
  "n_reviews": 0
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "model": "nmt",
  "translatedText": "当您纯粹以数字方式进行此计算时，它是矩阵向量乘法。",
  "time_range": [
   378.02,
   382.36
  ],
  "n_reviews": 0
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "model": "nmt",
  "translatedText": "现在，将 1x2 矩阵乘以向量的数 值运算就像计算两个向量的点积一样。",
  "time_range": [
   385.7,
   392.86
  ],
  "n_reviews": 0
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "model": "nmt",
  "translatedText": "这个 1x2 矩阵看起来不像是我们倾斜的向量吗？",
  "time_range": [
   393.46,
   396.8
  ],
  "n_reviews": 0
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "model": "nmt",
  "translatedText": "事实上，我们现在可以说 1x2 矩阵和 2D 向量之间 存在很好的关联，通过将向量的数值表示倾斜到其一侧以获得 关联的矩阵，或者将矩阵向后倾斜以获得关联的向量来定义。",
  "time_range": [
   397.96,
   412.58
  ],
  "n_reviews": 0
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "model": "nmt",
  "translatedText": "由于我们现在只是在研究数值表达式，因此在向量和 1 x2 矩阵之间来回切换可能感觉像是一件愚蠢的事情。",
  "time_range": [
   413.56,
   420.86
  ],
  "n_reviews": 0
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "model": "nmt",
  "translatedText": "但这表明从几何角度来看确实很棒。",
  "time_range": [
   421.46,
   425.12
  ],
  "n_reviews": 0
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "model": "nmt",
  "translatedText": "将向量转化为数字的线性变换与 向量本身之间存在某种联系。",
  "time_range": [
   425.38,
   431.72
  ],
  "n_reviews": 0
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "model": "nmt",
  "translatedText": "让我举一个例子来阐明其重要性，并 且恰好也回答了之前的点积难题。",
  "time_range": [
   434.78,
   441.38
  ],
  "n_reviews": 0
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "model": "nmt",
  "translatedText": "忘掉你所学的知识，想象一下 你还不知道点积与投影有关。",
  "time_range": [
   442.14,
   447.18
  ],
  "n_reviews": 0
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "model": "nmt",
  "translatedText": "我要做的就是复制一份数轴，并以某种方式将其对角放置在空 间中，数字 0 位于原点。",
  "time_range": [
   448.86,
   456.06
  ],
  "n_reviews": 0
 },
 {
  "input": "Now think of the two-dimensional unit vector, whose tip sits where the number 1 on the number line is.",
  "model": "nmt",
  "translatedText": "现在考虑二维单位向量，其尖 端位于数轴上数字 1 的位置。",
  "time_range": [
   456.9,
   461.92
  ],
  "n_reviews": 0
 },
 {
  "input": "I want to give that guy a name, U-hat.",
  "model": "nmt",
  "translatedText": "我想给那个家伙起个名字，U-hat。",
  "time_range": [
   462.4,
   464.56
  ],
  "n_reviews": 0
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "model": "nmt",
  "translatedText": "这个小家伙在即将发生的事情中扮演着重 要的角色，所以把他放在你的脑海里吧。",
  "time_range": [
   465.62,
   470.02
  ],
  "n_reviews": 0
 },
 {
  "input": "If we project 2D vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2D vectors to numbers.",
  "model": "nmt",
  "translatedText": "如果我们将 2D 向量直接投影到这条对角数线上，实际上 ，我们刚刚定义了一个将 2D 向量转换为数字的函数。",
  "time_range": [
   470.74,
   478.96
  ],
  "n_reviews": 0
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "model": "nmt",
  "translatedText": "更重要的是，这个函数实际上是线性的，因为它通过了我们的视觉 测试，即任何均匀分布的点线一旦落在数轴上就保持均匀分布。",
  "time_range": [
   479.66,
   488.96
  ],
  "n_reviews": 0
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2D space like this, the outputs of the function are numbers, not 2D vectors.",
  "model": "nmt",
  "translatedText": "需要明确的是，即使我像这样将数轴嵌入到二维空 间中，函数的输出也是数字，而不是二维向量。",
  "time_range": [
   491.64,
   499.28
  ],
  "n_reviews": 0
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "model": "nmt",
  "translatedText": "您应该考虑一个接受两个坐标并输出一个坐标的函数。",
  "time_range": [
   499.96,
   503.68
  ],
  "n_reviews": 0
 },
 {
  "input": "But that vector U-hat is a two-dimensional vector, living in the input space.",
  "model": "nmt",
  "translatedText": "但向量 U-hat 是一个二维向量，存在于输入空间中。",
  "time_range": [
   505.06,
   509.02
  ],
  "n_reviews": 0
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "model": "nmt",
  "translatedText": "它只是以与数轴的嵌入重叠的方式定位。",
  "time_range": [
   509.44,
   513.22
  ],
  "n_reviews": 0
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2D vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "model": "nmt",
  "translatedText": "通过这个投影，我们刚刚定义了从 2D 向量到数字的线性变 换，因此我们将能够找到某种描述该变换的 1x2 矩阵。",
  "time_range": [
   514.6,
   524.6
  ],
  "n_reviews": 0
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where I-hat and J-hat each land, since those landing spots are going to be the columns of the matrix.",
  "model": "nmt",
  "translatedText": "为了找到 1x2 矩阵，让我们放大这个对角数 轴设置，并考虑 I-hat 和 J-hat 各自着陆的位置，因为这些着陆点将是矩阵的列。",
  "time_range": [
   525.54,
   536.46
  ],
  "n_reviews": 0
 },
 {
  "input": "This part's super cool.",
  "model": "nmt",
  "translatedText": "这部分超级酷。",
  "time_range": [
   538.48,
   539.44
  ],
  "n_reviews": 0
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "model": "nmt",
  "translatedText": "我们可以用一个非常优雅的对称性来推理它。",
  "time_range": [
   539.7,
   542.42
  ],
  "n_reviews": 0
 },
 {
  "input": "Since I-hat and U-hat are both unit vectors, projecting I-hat onto the line passing through U-hat looks totally symmetric to projecting U-hat onto the x-axis.",
  "model": "nmt",
  "translatedText": "由于 I-hat 和 U-hat 都是单位向量，因 此将 I-hat 投影到穿过 U-hat 的直线上 看起来与将 U-hat 投影到 x 轴上完全对称。",
  "time_range": [
   543.02,
   553.16
  ],
  "n_reviews": 0
 },
 {
  "input": "So when we ask what number does I-hat land on when it gets projected, the answer is going to be the same as whatever U-hat lands on when it's projected onto the x-axis.",
  "model": "nmt",
  "translatedText": "因此，当我们询问 I-hat 在投影时落在什么数字上时，答 案将与 U-hat 在投影到 x 轴上时落在的数字相同。",
  "time_range": [
   553.84,
   562.32
  ],
  "n_reviews": 0
 },
 {
  "input": "But projecting U-hat onto the x-axis just means taking the x-coordinate of U-hat.",
  "model": "nmt",
  "translatedText": "但将 U-hat 投影到 x 轴只是意味着获取 U-hat 的 x 坐标。",
  "time_range": [
   562.92,
   568.6
  ],
  "n_reviews": 0
 },
 {
  "input": "So by symmetry, the number where I-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of U-hat.",
  "model": "nmt",
  "translatedText": "因此，根据对称性，当 I-hat 投影到对角数轴上时，I-hat 落在的数字将是 U-hat 的 x 坐标。",
  "time_range": [
   569.02,
   576.62
  ],
  "n_reviews": 0
 },
 {
  "input": "Isn't that cool?",
  "model": "nmt",
  "translatedText": "这不是很酷吗？",
  "time_range": [
   577.16,
   577.66
  ],
  "n_reviews": 0
 },
 {
  "input": "The reasoning is almost identical for the J-hat case.",
  "model": "nmt",
  "translatedText": "J 帽案例的推理几乎相同。",
  "time_range": [
   579.2,
   581.8
  ],
  "n_reviews": 0
 },
 {
  "input": "Think about it for a moment.",
  "model": "nmt",
  "translatedText": "想一想。",
  "time_range": [
   582.18,
   583.26
  ],
  "n_reviews": 0
 },
 {
  "input": "For all the same reasons, the y-coordinate of U-hat gives us the number where J-hat lands when it's projected onto the number line copy.",
  "model": "nmt",
  "translatedText": "出于所有相同的原因，U-hat 的 y 坐标为我们 提供了 J-hat 投影到数轴副本上时落在的数字。",
  "time_range": [
   589.12,
   596.6
  ],
  "n_reviews": 0
 },
 {
  "input": "Pause and ponder that for a moment.",
  "model": "nmt",
  "translatedText": "暂停并思考一下。",
  "time_range": [
   597.58,
   598.72
  ],
  "n_reviews": 0
 },
 {
  "input": "I just think that's really cool.",
  "model": "nmt",
  "translatedText": "我只是觉得这真的很酷。",
  "time_range": [
   598.78,
   600.2
  ],
  "n_reviews": 0
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of U-hat.",
  "model": "nmt",
  "translatedText": "因此，描述投影变换的 1x2 矩阵 的条目将是 U-hat 的坐标。",
  "time_range": [
   600.9200000000001,
   607.26
  ],
  "n_reviews": 0
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with U-hat.",
  "model": "nmt",
  "translatedText": "计算空间中任意向量的投影变换，需要 将该矩阵乘以这些向量，在计算上与 使用 U-hat 进行点积相同。",
  "time_range": [
   608.04,
   618.88
  ],
  "n_reviews": 0
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "model": "nmt",
  "translatedText": "这就是为什么用单位向量求点积可以解释为将 向量投影到该单位向量的跨度上并获取长度。",
  "time_range": [
   621.46,
   630.59
  ],
  "n_reviews": 0
 },
 {
  "input": "So what about non-unit vectors?",
  "model": "nmt",
  "translatedText": "那么非单位向量呢？",
  "time_range": [
   634.03,
   635.79
  ],
  "n_reviews": 0
 },
 {
  "input": "For example, let's say we take that unit vector U-hat, but we scale it up by a factor of 3.",
  "model": "nmt",
  "translatedText": "例如，假设我们采用单位向量 U-hat，但我们将其放大 3 倍。",
  "time_range": [
   636.31,
   640.63
  ],
  "n_reviews": 0
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "model": "nmt",
  "translatedText": "从数字上看，它的每个分量都乘以 3。",
  "time_range": [
   641.35,
   644.39
  ],
  "n_reviews": 0
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes I-hat and J-hat to three times the values where they landed before.",
  "model": "nmt",
  "translatedText": "因此，查看与该向量关联的矩阵，I-hat 和 J-hat 的值是它们之前的值的三倍。",
  "time_range": [
   644.81,
   652.39
  ],
  "n_reviews": 0
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "model": "nmt",
  "translatedText": "由于这都是线性的，因此更一般地意味着新矩阵可以解释为 将任何向量投影到数轴副本上并将其所在位置乘以 3。",
  "time_range": [
   655.23,
   664.65
  ],
  "n_reviews": 0
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "model": "nmt",
  "translatedText": "这就是为什么具有非单位向量的点积可以解释为首先投影 到该向量上，然后将该投影的长度按向量的长度放大。",
  "time_range": [
   665.47,
   674.95
  ],
  "n_reviews": 0
 },
 {
  "input": "Take a moment to think about what happened here.",
  "model": "nmt",
  "translatedText": "花点时间想想这里发生了什么。",
  "time_range": [
   677.59,
   679.55
  ],
  "n_reviews": 0
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "model": "nmt",
  "translatedText": "我们有一个从二维空间到数轴的线性变换，它不 是用数值向量或数值点积来定义的，它只是通 过将空间投影到数轴的对角线副本上来定义。",
  "time_range": [
   679.89,
   690.89
  ],
  "n_reviews": 0
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "model": "nmt",
  "translatedText": "但由于变换是线性的，因此必须用某个 1x2 矩阵来描述。",
  "time_range": [
   691.67,
   696.83
  ],
  "n_reviews": 0
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "model": "nmt",
  "translatedText": "由于将 1x2 矩阵乘以 2D 向量与将该矩阵翻转并取点 积相同，因此这种变换不可避免地与某些 2D 向量相关。",
  "time_range": [
   697.33,
   707.91
  ],
  "n_reviews": 0
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "model": "nmt",
  "translatedText": "这里的教训是，任何时候你有这些线性变换之一 ，其输出空间是数轴，无论它是如何定义的，都 会有一些与该变换相对应的唯一向量 v，从某 种意义上说，应用变换是与该向量的点积相同。",
  "time_range": [
   709.41,
   726.35
  ],
  "n_reviews": 0
 },
 {
  "input": "To me, this is utterly beautiful.",
  "model": "nmt",
  "translatedText": "对我来说，这真是太美了。",
  "time_range": [
   729.93,
   732.03
  ],
  "n_reviews": 0
 },
 {
  "input": "It's an example of something in math called duality.",
  "model": "nmt",
  "translatedText": "这是数学中所谓的对偶性的一个例子。",
  "time_range": [
   732.73,
   735.39
  ],
  "n_reviews": 0
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "model": "nmt",
  "translatedText": "对偶性在数学中以多种不同的方式和形 式出现，并且实际定义起来非常棘手。",
  "time_range": [
   736.2700000000001,
   741.93
  ],
  "n_reviews": 0
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "model": "nmt",
  "translatedText": "宽泛地说，它指的是两种数学事物之间存 在自然但令人惊讶的对应关系的情况。",
  "time_range": [
   742.67,
   750.23
  ],
  "n_reviews": 0
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "model": "nmt",
  "translatedText": "对于您刚刚了解的线性代数情况，您 会说向量的对偶是它编码的线性变 换，而从某个空间到一维的线性变 换的对偶是该空间中的某个向量。",
  "time_range": [
   751.01,
   764.65
  ],
  "n_reviews": 0
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "model": "nmt",
  "translatedText": "总而言之，从表面上看，点积是一个非常有用的几何工具 ，用于理解投影和测试向量是否倾向于指向同一方向。",
  "time_range": [
   766.73,
   776.31
  ],
  "n_reviews": 0
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "model": "nmt",
  "translatedText": "这可能是您要记住的关于点积的最重要的事情。",
  "time_range": [
   776.97,
   780.79
  ],
  "n_reviews": 0
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "model": "nmt",
  "translatedText": "但在更深层次上，将两个向量点在一起是将 其中一个向量转化为变换世界的一种方法。",
  "time_range": [
   781.27,
   787.73
  ],
  "n_reviews": 0
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "model": "nmt",
  "translatedText": "再次强调，从数字上看，这可能感觉是一个愚蠢的强调点。",
  "time_range": [
   788.67,
   791.55
  ],
  "n_reviews": 0
 },
 {
  "input": "It's just too computationally.",
  "model": "nmt",
  "translatedText": "实在是太计算了。",
  "time_range": [
   791.67,
   794.09
  ],
  "n_reviews": 0
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "model": "nmt",
  "translatedText": "但我发现这一点如此重要的原因是，在整个数学过程 中，当你处理向量时，一旦你真正了解它的个性，有 时你会意识到，将它理解为空间中的箭头，而不是空 间中的箭头，会更容易理解。 线性变换的物理体现。",
  "time_range": [
   794.09,
   810.09
  ],
  "n_reviews": 0
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space.",
  "model": "nmt",
  "translatedText": "就好像向量实际上只是某种变换的概念简写，因为我 们更容易考虑空间中的箭头而不是移动整个空间。",
  "time_range": [
   810.73,
   820.97
  ],
  "n_reviews": 0
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action as I talk about the cross product.",
  "model": "nmt",
  "translatedText": "在下一个视频中，当我谈论叉积时，您 将看到另一个非常酷的二元性示例。",
  "time_range": [
   822.61,
   829.19
  ],
  "n_reviews": 0
 }
]