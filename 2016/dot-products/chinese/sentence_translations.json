[
 {
  "input": "Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[贝多芬的《欢乐颂》，钢琴演奏到最后。 ] 传统上，点积是在线性代数课程中很早 就引入的东西，通常是在开始时引入的。",
  "model": "google_nmt",
  "from_community_srt": "传统上， 点积是线性代数课程中很靠前的内容 一般就在最开始 我把它放得如此靠后，",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "所以我在这个系列中把它们推迟到这么远可能看起来很奇怪。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "我这样做是因为有一个标准的方法来介绍这 个主题，它只需要对向量有基本的了解，但 对点积在数学中所扮演的角色的更全面的理 解只能在线性变换的指导下才能真正找到。",
  "model": "google_nmt",
  "from_community_srt": "看起来似乎很奇怪 我这么做是有原因的， 虽然引入点积的标准方法只需要向量的基础认识即可 但是要更进一步理解点积所发挥的作用 只能从线性变换的角度才能完成 不过在此之前，",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "不过，在此之前，让我简单介绍一下点积的引入标准 方式，我认为至少部分观众已经了解了这种方式。",
  "model": "google_nmt",
  "from_community_srt": "我先简单介绍引入点积的标准方法 我想对于部分观众来说，",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "在数字上，如果有两个相同维度的向量 ，两个具有相同长度的数字列表，获 取它们的点积意味着将所有坐标配对 ，将这些对相乘，然后将结果相加。",
  "model": "google_nmt",
  "from_community_srt": "这算是复习了吧 如果你有两个维数相同的向量 或是两个长度相同的数组 求它们的点积， 就是将相应坐标配对 求出每一对坐标的乘积 然后将结果相加 所以向量(1,",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "因此向量 1, 2 点缀着 3, 4 将是 1 乘以 3 加 2 乘以 4。",
  "model": "google_nmt",
  "from_community_srt": "2)点乘向量(3, 4)的结果为1×3+2×4 向量(6,",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "向量 6, 2, 8, 3 点缀着 1, 8, 5, 3 将是 6 乘以 1 加 2 乘以 8 加 8 乘以 5 加 3 乘以 3。",
  "model": "google_nmt",
  "from_community_srt": "2, 8, 3)点乘向量(1, 8, 5, 3)的结果为6×1+2×8+8×5+3×3 幸运的是，",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "幸运的是，这个计算有一个非常好的几何解释。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "要考虑两个向量 v 和 w 之间的点积，请想象 将 w 投影到穿过原点和 v 尖端的直线上。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "将此投影的长度乘以 v 的长度，即可得到点积 v 点 w。",
  "model": "google_nmt",
  "from_community_srt": "这个计算有一个优美的几何解释 要求两个向量v和w的点积 想象将向量w朝着过原点和向量v终点的直线上投影 将投影的长度与向量v的长度相乘 你就得到了它们的点积 - v点乘w 除非w的投影与v的方向相反",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "除非 w 的投影指向 v 的相反 方向，否则该点积实际上将为负。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "因此，当两个向量通常指向同一方向时，它们的点积为正。",
  "model": "google_nmt",
  "from_community_srt": "这种情况下点积为负值 所以当两个向量的指向大致相同时 它们的点积为正 当它们相互垂直时，",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "当它们垂直时，意味着一个到另一个 的投影为零向量，它们的点积为零。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "如果它们通常指向相反的方向，则它们的点积为负。",
  "model": "google_nmt",
  "from_community_srt": "意味着一个向量在另一个向量上的投影为零向量 它们的点积为零 而当它们的指向基本相反时 它们的点积为负 现在看看，",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "现在，这种解释奇怪地不对称。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "它对待这两个向量的方式非常不同。",
  "model": "google_nmt",
  "from_community_srt": "这种解释异常地不对称 它对两个向量的处理方式完全不同 所以我初次学习时，",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "所以当我第一次了解到这一点时，我很惊讶顺序并不重要。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "您可以将 v 投影到 w 上，将投影 v 的长度乘以 w 的长度，并得到相同的结果。",
  "model": "google_nmt",
  "from_community_srt": "点积与顺序无关让我感到很惊讶 你可以将v投影到w上， 将v的投影长度与w的长度相乘 然后得到相同的结果 我说，",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "我的意思是，这难道不感觉是一个完全不同的过程吗？",
  "model": "google_nmt",
  "from_community_srt": "你不觉得这是个完全不同的过程吗？",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "这是为什么顺序不重要的直觉。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "如果 v 和 w 碰巧具有相同的长度，我们可以利用一些对称性。",
  "model": "google_nmt",
  "from_community_srt": "下面从直观上说说为什么点积与顺序无关 如果v和w的长度恰好相同，",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "由于将 w 投影到 v 上，然后将该投影的长度乘 以 v 的长度，因此是将 v 投影到 w 上， 然后将该投影的长度乘以 w 的长度的完整镜像。",
  "model": "google_nmt",
  "from_community_srt": "我们可以利用其中的对称性 因为w向v上投影， 并将w的投影长度与v的长度相乘 和v向w上投影， 并将v的投影长度与w的长度相乘互为镜像 现在如果你将其中一个缩放若干倍，",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "现在，如果您将其中一个（例如 v）缩放为某个常数（例如 2），从而使它们的长度不相等，则对称性就会被打破。",
  "model": "google_nmt",
  "from_community_srt": "比如将v变成两倍 使得它们的长度不同，",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "但是让我们考虑一下如何解释这个新向量 （2 乘以 v）和 w 之间的点积。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "如果您将 w 视为投影到 v 上，则点积 2v d ot w 将恰好是点积 v dot w 的两倍。",
  "model": "google_nmt",
  "from_community_srt": "对称性就被破坏了 但是我们可以这样解读新向量2v和w的点积 如果你认为w向v上投影 那么2v点乘w就应该恰好是v点乘w的两倍 这是因为，",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "这是因为当您将 v 缩放 2 时，它不会改变 w 投影的长度，但会将您投影到的向量的长度加倍。",
  "model": "google_nmt",
  "from_community_srt": "将v放大为原来的两倍并不改变w的投影长度 但是被投影的向量长度变为原来的两倍 另一方面，",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "但另一方面，假设您正在考虑将 v 投影到 w 上。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "好吧，在这种情况下，当我们将 v 乘以 2 时，投 影的长度就会缩放，但投影到的向量的长度保持不变。",
  "model": "google_nmt",
  "from_community_srt": "假设你想将v投影到w上 我们将v变为原来的两倍， 这次是投影的长度变为原来的两倍 但是被投影的向量长度保持不变 所以总体效果仍然是点积变为两倍 所以说，",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "所以总体效果仍然是点积的两倍。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "因此，即使在这种情况下对称性 被打破，这种缩放对点积值的影 响在两种解释下都是相同的。",
  "model": "google_nmt",
  "from_community_srt": "即便这种情况下对称性被破坏了 在两种理解方式下， 缩放向量对点积结果的影响是相同的 初次学习的时候，",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "当我第一次学习这些东西时，还有一个让我困惑的大问题。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "到底为什么这种匹配坐标、相乘并将它 们加在一起的数值过程与投影有关？",
  "model": "google_nmt",
  "from_community_srt": "我还遇到了另一个让我困惑的大问题 究竟为什么点积的这一运算过程 也就是对应坐标相乘并将结果相加， 和投影有所联系？",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "好吧，为了给出一个令人满意的答案，并且为 了充分理解点积的重要性，我们需要挖掘这里 发生的更深入的事情，这通常被称为对偶性。",
  "model": "google_nmt",
  "from_community_srt": "如果想要给出一个满意的答案， 并且正视点积的重要性 我们需要挖掘更深层次的东西，",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "但在开始讨论之前，我需要花一些时间讨论 从多维到一维的线性变换，这只是数轴。",
  "model": "google_nmt",
  "from_community_srt": "它通常被称为“对偶性” 不过在继续深入之前，",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "这些函数接受 2d 向量并输出一些数字 ，但线性变换当然比具有 2d 输入和 1d 输出的普通函数受到更多限制。",
  "model": "google_nmt",
  "from_community_srt": "我需要花点时间讨论 多维空间到一维空间（数轴）的线性变换 有不少函数能够接收二维向量并输出一个数 同样是二维输入和一维输出， 和一般的函数相比，",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "与更高维度的变换一样，就像我在第 3 章中讨 论的那样，有一些形式属性使这些函数成为线性， 但我将故意忽略这些，以免分散我们的最终目标， 相反专注于某种与所有正式事物等效的视觉属性。",
  "model": "google_nmt",
  "from_community_srt": "线性变换的要求更加严格 就像我在第三章中讨论的， 高维空间中的变换需要满足一些严格的性质才会具有线性 但是为了不偏离最终目标， 我特意忽略这些内容 而是聚焦于一种与之等价的直观特性 如果你有一系列等距分布于一条直线上的点，",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "如果您采用一行均匀间隔的点并应用变换， 那么一旦这些点落在输出空间（即数轴） 中，线性变换将使这些点保持均匀间隔。",
  "model": "google_nmt",
  "from_community_srt": "然后应用变换 线性变换会保持这些点等距分布在输出空间中， 也就是数轴上 否则，",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "否则，如果有一些点线的间距不均 匀，那么您的变换就不是线性的。",
  "model": "google_nmt",
  "from_community_srt": "如果这些点没有等距分布，",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "与我们之前看到的情况一样，这些线性变换 之一完全由 i-hat 和 j-ha t 的位置决定，但这次每个基向量都落 在一个数字上，因此当我们记录它们以矩 阵的列形式出现，每一列只有一个数字。",
  "model": "google_nmt",
  "from_community_srt": "那么这个变换就不是线性的 如同我们之前看到的例子一样 这些线性变换完全由它对i帽和j帽的变换决定 但是这一次， 这些基向量只落在一个数上 所以当我们将它们变换后的位置记录为矩阵的列时 矩阵的每列只是一个单独的数 这是一个1×2矩阵 我们来考察一个例子，",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "这是一个 1x2 矩阵。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "让我们通过一个示例来了解将这些变换之一应用于向量的含义。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "假设您有一个线性变换，将 i-hat 变为 1，将 j-hat 变为负 2。",
  "model": "google_nmt",
  "from_community_srt": "了解它对向量作用的含义 假设你有一个线性变换， 它将i帽和j帽分别变换至1和-2 要跟踪一个向量，",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "要追踪坐标为 4、3 的向量的最终位置，请将此向量分解 为 4 乘以 i-hat 加上 3 乘以 j-hat。",
  "model": "google_nmt",
  "from_community_srt": "比如向量(4, 3)， 在变换之后的去向 将这个向量分解为4乘以i帽加上3乘以j帽 由于线性性质，",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "线性的结果是，变换后，向量将是 i-h at 落地位置的 4 倍，即 1，再加 上 j-hat 落地位置的 3 倍，即 负 2，在本例中意味着它落在负数上2.",
  "model": "google_nmt",
  "from_community_srt": "在变换后 这个向量的位置是4乘以变换后的i帽， 也就是1 加上3乘以变换后的j帽，",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "当您纯粹以数字方式进行此计算时，它是矩阵向量乘法。",
  "model": "google_nmt",
  "from_community_srt": "也就是-2 结果说明它落在-2上 当你完全从数值角度进行计算时，",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "现在，将 1x2 矩阵乘以向量的数 值运算就像计算两个向量的点积一样。",
  "model": "google_nmt",
  "from_community_srt": "它就是矩阵向量乘法 1×2矩阵与向量相乘这一数值运算过程 感觉上就和两个向量的点积一样 那个1×2矩阵不正像是一个倾倒的向量吗？",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "这个 1x2 矩阵看起来不像是我们倾斜的向量吗？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "事实上，我们现在可以说 1x2 矩阵和 2D 向量之间 存在很好的关联，通过将向量的数值表示倾斜到其一侧以获得 关联的矩阵，或者将矩阵向后倾斜以获得关联的向量来定义。",
  "model": "google_nmt",
  "from_community_srt": "实际上， 我们现在可以说 1×2矩阵与二维向量之间有着微妙的联系 这种关系在于：将向量放倒， 从而得到与之相关的矩阵 或者将矩阵立直，",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "由于我们现在只是在研究数值表达式，因此在向量和 1 x2 矩阵之间来回切换可能感觉像是一件愚蠢的事情。",
  "model": "google_nmt",
  "from_community_srt": "从而得到与之相关的向量 因为我们现在只是从数值表达上来看这个联系 所以向量和1×2矩阵之间的来回转化看上去毫无意义 但是这暗示了一点，",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "但这表明从几何角度来看确实很棒。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "将向量转化为数字的线性变换与 向量本身之间存在某种联系。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "让我举一个例子来阐明其重要性，并 且恰好也回答了之前的点积难题。",
  "model": "google_nmt",
  "from_community_srt": "从几何角度可以看到一些美妙的事情 将向量转化为数的线性变换和这个向量本身有着某种关系 我来举个例子说明这种关系的重要性 而它恰恰回答了之前提到的点积的问题 忘记你所学过的 假设你还不知道点积与投影有关",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "忘掉你所学的知识，想象一下 你还不知道点积与投影有关。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "我要做的就是复制一份数轴，并以某种方式将其对角放置在空 间中，数字 0 位于原点。",
  "model": "google_nmt",
  "from_community_srt": "我现在将数轴复制一份 然后保持0在原点，",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "现在考虑二维单位向量，其尖 端位于数轴上数字 1 的位置。",
  "model": "google_nmt",
  "from_community_srt": "将它斜向放置在空间中 现在考虑这样一个二维向量， 它的终点落在这条数轴的1上 我给它起个名字，",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "我想给那个家伙起个名字，U-hat。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "这个小家伙在即将发生的事情中扮演着重 要的角色，所以把他放在你的脑海里吧。",
  "model": "google_nmt",
  "from_community_srt": "就叫“u帽” 这个向量在接下来的讲解中扮演着重要的角色 所以请你牢记它 如果将二维向量直接投影到这条数轴上 实际上，",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "如果我们将 2D 向量直接投影到这条对角数线上，实际上 ，我们刚刚定义了一个将 2D 向量转换为数字的函数。",
  "model": "google_nmt",
  "from_community_srt": "我们就这样定义了一个从二维向量到数的函数 更重要的是，",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "更重要的是，这个函数实际上是线性的，因为它通过了我们的视觉 测试，即任何均匀分布的点线一旦落在数轴上就保持均匀分布。",
  "model": "google_nmt",
  "from_community_srt": "这个函数是线性的， 因为它顺利通过了线性检验 即直线上等距分布的点在投影到数轴上后仍然等距分布 这里说明一点，",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "需要明确的是，即使我像这样将数轴嵌入到二维空 间中，函数的输出也是数字，而不是二维向量。",
  "model": "google_nmt",
  "from_community_srt": "即便我把这条数轴放在二维空间中 上述函数的输出结果还是数，",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "您应该考虑一个接受两个坐标并输出一个坐标的函数。",
  "model": "google_nmt",
  "from_community_srt": "而不是二维向量 你应该把它看作一个接收两个坐标并输出一个坐标的函数 不过，",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "但向量 U-hat 是一个二维向量，存在于输入空间中。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "它只是以与数轴的嵌入重叠的方式定位。",
  "model": "google_nmt",
  "from_community_srt": "u帽是二维空间中的一个向量 而它碰巧又落在这条数轴上 根据这个投影，",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "通过这个投影，我们刚刚定义了从 2D 向量到数字的线性变 换，因此我们将能够找到某种描述该变换的 1x2 矩阵。",
  "model": "google_nmt",
  "from_community_srt": "我们定义了一个从二维向量到数的线性变换 所以我们就能够找到描述这个变换的1×2矩阵 为了找到这个矩阵，",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "为了找到 1x2 矩阵，让我们放大这个对角数 轴设置，并考虑 I-hat 和 J-hat 各自着陆的位置，因为这些着陆点将是矩阵的列。",
  "model": "google_nmt",
  "from_community_srt": "我们把这条斜着的数轴放大来看 并且需要考虑变换后i帽和j帽的位置，",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "这部分超级酷。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "我们可以用一个非常优雅的对称性来推理它。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "由于 I-hat 和 U-hat 都是单位向量，因 此将 I-hat 投影到穿过 U-hat 的直线上 看起来与将 U-hat 投影到 x 轴上完全对称。",
  "model": "google_nmt",
  "from_community_srt": "因为它们就是矩阵的列 这一部分内容超级漂亮 我们可以通过精妙的对称性进行推理 因为i帽和u帽都是单位向量 将i帽向u帽所在的直线投影与u帽向x轴投影看上去完全对称 所以说，",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "因此，当我们询问 I-hat 在投影时落在什么数字上时，答 案将与 U-hat 在投影到 x 轴上时落在的数字相同。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "但将 U-hat 投影到 x 轴只是意味着获取 U-hat 的 x 坐标。",
  "model": "google_nmt",
  "from_community_srt": "如果要问i帽在投影之后落在哪个数上 答案就应该是u帽向x轴投影所得到的数 而u帽向x轴投影得到的数就是u帽的横坐标 因此根据对称性，",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "因此，根据对称性，当 I-hat 投影到对角数轴上时，I-hat 落在的数字将是 U-hat 的 x 坐标。",
  "model": "google_nmt",
  "from_community_srt": "将i帽向斜着的数轴上投影所得到的数 就是u帽的横坐标 是不是很酷？",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "这不是很酷吗？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "J 帽案例的推理几乎相同。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "想一想。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "出于所有相同的原因，U-hat 的 y 坐标为我们 提供了 J-hat 投影到数轴副本上时落在的数字。",
  "model": "google_nmt",
  "from_community_srt": "以上推理过程对j帽几乎一致 花点时间思考一下 与之前的原因相同 u帽的y坐标给出了j帽向斜着的数轴上投影所得到的数 暂停思考一会儿，",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "暂停并思考一下。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "我只是觉得这真的很酷。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "因此，描述投影变换的 1x2 矩阵 的条目将是 U-hat 的坐标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "计算空间中任意向量的投影变换，需要 将该矩阵乘以这些向量，在计算上与 使用 U-hat 进行点积相同。",
  "model": "google_nmt",
  "from_community_srt": "我觉得这部分非常漂亮 所以描述投影变换的1×2矩阵的两列 就分别是u帽的两个坐标 而空间中任意向量经过投影变换的结果 也就是投影矩阵与这个向量相乘 和这个向量与u帽的点积在计算上完全相同",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "这就是为什么用单位向量求点积可以解释为将 向量投影到该单位向量的跨度上并获取长度。",
  "model": "google_nmt",
  "from_community_srt": "这就是为什么与单位向量的点积可以解读为 将向量投影到单位向量所在的直线上所得到的投影长度 那对于非单位向量呢？",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "那么非单位向量呢？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "例如，假设我们采用单位向量 U-hat，但我们将其放大 3 倍。",
  "model": "google_nmt",
  "from_community_srt": "比如说， 还是这个单位向量u帽， 不过我们把它放大为原来的3倍 数值上说，",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "从数字上看，它的每个分量都乘以 3。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "因此，查看与该向量关联的矩阵，I-hat 和 J-hat 的值是它们之前的值的三倍。",
  "model": "google_nmt",
  "from_community_srt": "它的每个坐标都被放大为原来的3倍 所以要寻找与这个向量相关的投影矩阵 实际上就是之前i帽和j帽投影得到的值的3倍 更普遍地说，",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "由于这都是线性的，因此更一般地意味着新矩阵可以解释为 将任何向量投影到数轴副本上并将其所在位置乘以 3。",
  "model": "google_nmt",
  "from_community_srt": "因为这个变换是线性的， 意味着这个新矩阵可以看作 将任何向量朝斜着的数轴上投影，",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "这就是为什么具有非单位向量的点积可以解释为首先投影 到该向量上，然后将该投影的长度按向量的长度放大。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "花点时间想想这里发生了什么。",
  "model": "google_nmt",
  "from_community_srt": "然后将结果乘以3 这就是为什么向量与给定非单位向量的点积可以解读为 首先朝给定向量上投影 然后将投影的值与给定向量长度相乘 思考一下这个过程 我们有一个从二维空间到数轴的线性变换 它并不是由向量数值或点积运算定义得到的",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "我们有一个从二维空间到数轴的线性变换，它不 是用数值向量或数值点积来定义的，它只是通 过将空间投影到数轴的对角线副本上来定义。",
  "model": "google_nmt",
  "from_community_srt": "而只是通过将空间投影到给定数轴上来定义 但是因为这个变换是线性的，",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "但由于变换是线性的，因此必须用某个 1x2 矩阵来描述。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "由于将 1x2 矩阵乘以 2D 向量与将该矩阵翻转并取点 积相同，因此这种变换不可避免地与某些 2D 向量相关。",
  "model": "google_nmt",
  "from_community_srt": "所以它必然可以用某个1×2矩阵描述 又因为1×2矩阵与二维向量相乘的计算过程 和转置矩阵并求点积的计算过程相同 所以这个投影变换必然会与某个二维向量相关 这里给你的启发是，",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "这里的教训是，任何时候你有这些线性变换之一 ，其输出空间是数轴，无论它是如何定义的，都 会有一些与该变换相对应的唯一向量 v，从某 种意义上说，应用变换是与该向量的点积相同。",
  "model": "google_nmt",
  "from_community_srt": "你在任何时候看到一个线性变换 它的输出空间是一维数轴 无论它是如何定义的 空间中会存在唯一的向量v与之相关 就这一意义而言，",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "对我来说，这真是太美了。",
  "model": "google_nmt",
  "from_community_srt": "应用变换和与向量v做点积是一样的 对我来说，",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "这是数学中所谓的对偶性的一个例子。",
  "model": "google_nmt",
  "from_community_srt": "这个结果格外精彩 它是数学中“对偶性”的一个实例 对偶性贯穿数学始终，",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "对偶性在数学中以多种不同的方式和形 式出现，并且实际定义起来非常棘手。",
  "model": "google_nmt",
  "from_community_srt": "在多个方面均有体现 而实际定义它却是比较棘手的 粗略地说，",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "宽泛地说，它指的是两种数学事物之间存 在自然但令人惊讶的对应关系的情况。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "对于您刚刚了解的线性代数情况，您 会说向量的对偶是它编码的线性变 换，而从某个空间到一维的线性变 换的对偶是该空间中的某个向量。",
  "model": "google_nmt",
  "from_community_srt": "它指的是 两种数学事物之间自然而又出乎意料的对应关系 对于你刚学到的情况而言 你可以说一个向量的对偶是由它定义的线性变换 一个多维空间到一维空间的线性变换的对偶是多维空间中的某个特定向量",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "总而言之，从表面上看，点积是一个非常有用的几何工具 ，用于理解投影和测试向量是否倾向于指向同一方向。",
  "model": "google_nmt",
  "from_community_srt": "总结一下， 表面上看，",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "这可能是您要记住的关于点积的最重要的事情。",
  "model": "google_nmt",
  "from_community_srt": "点积是理解投影的有利几何工具 并且方便检验两个向量的指向是否相同 这大概也是你需要记住的点积中最重要的部分 不过更进一步讲 两个向量点乘，",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "但在更深层次上，将两个向量点在一起是将 其中一个向量转化为变换世界的一种方法。",
  "model": "google_nmt",
  "from_community_srt": "就是将其中一个向量转化为线性变换 同样，",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "再次强调，从数字上看，这可能感觉是一个愚蠢的强调点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "实在是太计算了。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "但我发现这一点如此重要的原因是，在整个数学过程 中，当你处理向量时，一旦你真正了解它的个性，有 时你会意识到，将它理解为空间中的箭头，而不是空 间中的箭头，会更容易理解。 线性变换的物理体现。",
  "model": "google_nmt",
  "from_community_srt": "在数值上强调它可能显得没有意义 因为只是两种看上去恰好相似的计算过程而已 但是我认为这一过程非常重要 因为从始至终你都在和向量打交道 一旦你真正了解了向量的“个性” 有时你就会意识到， 不把它看作空间中的箭头 而把它看作线性变换的物质载体，",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "就好像向量实际上只是某种变换的概念简写，因为我 们更容易考虑空间中的箭头而不是移动整个空间。",
  "model": "google_nmt",
  "from_community_srt": "会更容易理解向量 向量就仿佛是一个特定变换的概念性记号 因为对我们来说，",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.15
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "在下一个视频中，当我谈论叉积时，您 将看到另一个非常酷的二元性示例。",
  "model": "google_nmt",
  "from_community_srt": "想象空间中的向量比想象整个空间移动到数轴上更加容易 下期视频中 我将开始讨论叉积，",
  "n_reviews": 0,
  "start": 820.15,
  "end": 829.19
 }
]