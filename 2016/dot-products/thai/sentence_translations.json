[
 {
  "input": "Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[ดนตรี] ตามเนื้อผ้า ผลิตภัณฑ์ดอทเป็นสิ่งที่ถูกนำเสนอตั้งแต่เนิ่นๆ ในหลักสูตรพีชคณิตเชิงเส้น โดยทั่วไปจะเริ่มต้นตั้งแต่เริ่มต้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "มันอาจดูแปลกที่ฉันผลักพวกเขากลับไปไกลขนาดนี้ในซีรีส์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "ฉันทำสิ่งนี้เพราะมีวิธีมาตรฐานในการแนะนำหัวข้อ ซึ่งไม่ต้องการอะไรมากไปกว่าความเข้าใจพื้นฐานเกี่ยวกับเวกเตอร์ แต่ความเข้าใจที่สมบูรณ์ยิ่งขึ้นเกี่ยวกับบทบาทของดอทโปรดัคในคณิตศาสตร์นั้นสามารถพบได้จริงๆ ภายใต้แสงของการแปลงเชิงเส้นเท่านั้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "ก่อนหน้านั้น ผมขอกล่าวถึงวิธีมาตรฐานในการนำเสนอผลิตภัณฑ์ดอทโดยย่อ ซึ่งผมถือว่าอย่างน้อยก็เป็นเพียงการทบทวนบางส่วนสำหรับผู้ชมจำนวนหนึ่ง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "ในเชิงตัวเลข ถ้าคุณมีเวกเตอร์สองตัวที่มีมิติเท่ากัน รายการตัวเลขสองรายการที่มีความยาวเท่ากัน การใช้ดอทโปรดัคหมายถึงการจับคู่พิกัดทั้งหมด คูณคู่เหล่านั้นเข้าด้วยกัน แล้วบวกผลลัพธ์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "แล้วเวกเตอร์ 1, 2 ดอทกับ 3, 4 จะเป็น 1 คูณ 3 บวก 2 คูณ 4 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "เวกเตอร์ 6, 2, 8, 3 ดอทด้วย 1, 8, 5, 3 จะเป็น 6 คูณ 1 บวก 2 คูณ 8 บวก 8 คูณ 5 บวก 3 คูณ 3 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "โชคดีที่การคำนวณนี้มีการตีความทางเรขาคณิตที่ดีมาก ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "เมื่อต้องการคิดถึงดอทโปรดัคระหว่างเวกเตอร์สองตัว v และ w ลองจินตนาการถึงการฉาย w ลงบนเส้นตรงที่ผ่านจุดกำเนิดและส่วนปลายของ v ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "คูณความยาวของเส้นโครงนี้ด้วยความยาวของ v, คุณจะได้ดอทโปรดัค v ดอท w ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "ยกเว้นกรณีที่เส้นโครงของ w ชี้ไปในทิศทางตรงกันข้ามกับ v ผลคูณดอทนั้นจะเป็นลบจริงๆ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "ดังนั้นเมื่อเวกเตอร์สองตัวโดยทั่วไปชี้ไปในทิศทางเดียวกัน ผลคูณดอทของพวกมันจะเป็นบวก ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "เมื่อพวกมันตั้งฉากกัน หมายความว่าเส้นโครงของอันหนึ่งไปยังอีกอันหนึ่งเป็นเวกเตอร์ศูนย์ ผลคูณดอทของพวกมันจะเป็นศูนย์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "และหากโดยทั่วไปพวกมันชี้ไปในทิศทางตรงกันข้าม ผลคูณดอทจะเป็นลบ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "ทีนี้ การตีความนี้ไม่สมมาตรอย่างประหลาด ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "มันปฏิบัติต่อเวกเตอร์ทั้งสองต่างกันมาก ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "ดังนั้นเมื่อฉันเรียนรู้สิ่งนี้ครั้งแรก ฉันรู้สึกประหลาดใจที่ลำดับไม่สำคัญ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "คุณสามารถฉาย v ไปยัง w แทนได้ โดยคูณความยาวของ v ที่ฉายด้วยความยาวของ w แล้วได้ผลลัพธ์เดียวกัน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "ฉันหมายความว่านั่นไม่รู้สึกเหมือนเป็นกระบวนการที่แตกต่างออกไปจริงๆเหรอ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "นี่คือสัญชาตญาณว่าทำไมลำดับจึงไม่สำคัญ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "ถ้า v และ w มีความยาวเท่ากัน เราก็สามารถใช้ประโยชน์จากความสมมาตรได้ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "นับตั้งแต่ฉาย w ไปบน v แล้วคูณความยาวของเส้นโครงนั้นด้วยความยาวของ v จึงเป็นภาพสะท้อนที่สมบูรณ์ของการฉาย v ไปบน w แล้วคูณความยาวของเส้นโครงนั้นด้วยความยาวของ w ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "ทีนี้ ถ้าคุณปรับขนาดอันใดอันหนึ่ง เช่น v โดยค่าคงที่ประมาณ 2 เพื่อให้มันมีความยาวไม่เท่ากัน สมมาตรจะขาด ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "แต่ลองคิดดูว่าจะตีความดอทโปรดัคระหว่างเวกเตอร์ใหม่นี้ 2 คูณ v และ w ได้อย่างไร ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "หากคุณคิดว่า w ฉายลงบน v แล้วดอทโปรดัค 2v ดอท w จะเป็นสองเท่าของดอทโปรดัค v ดอท w พอดี ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "นี่เป็นเพราะเมื่อคุณปรับขนาด v ด้วย 2, มันจะไม่เปลี่ยนความยาวของโครงของ w แต่มันจะเพิ่มความยาวของเวกเตอร์ที่คุณฉายลงไปเป็นสองเท่า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "แต่ในทางกลับกัน, สมมุติว่าคุณกำลังคิดถึง v ที่จะฉายลงบน w ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "ในกรณีนี้ ความยาวของเส้นโครงคือสิ่งที่ถูกขยายเมื่อเราคูณ v ด้วย 2 แต่ความยาวของเวกเตอร์ที่คุณฉายจะคงที่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "ผลโดยรวมก็ยังเป็นดอทโปรดัคเป็นสองเท่า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "ดังนั้น แม้ว่าความสมมาตรจะขาดไปในกรณีนี้ แต่ผลกระทบที่มาตราส่วนนี้มีต่อค่าของผลคูณดอทจะเหมือนกันภายใต้การตีความทั้งสอง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "ยังมีคำถามใหญ่อีกคำถามหนึ่งที่ทำให้ฉันสับสนเมื่อเรียนรู้สิ่งนี้ครั้งแรก ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "เหตุใดกระบวนการทางตัวเลขของการจับคู่พิกัด การคูณคู่ และการบวกเข้าด้วยกัน จึงมีส่วนเกี่ยวข้องกับการฉายภาพ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "เพื่อให้คำตอบที่น่าพอใจ และเพื่อให้ความยุติธรรมกับความสำคัญของดอทโปรดัค เราจำเป็นต้องค้นพบบางสิ่งที่ลึกลงไปอีกหน่อย ที่เกิดขึ้นที่นี่ ซึ่งมักจะเป็นไปตามชื่อความเป็นคู่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "แต่ก่อนที่จะเข้าเรื่องนั้น ฉันต้องใช้เวลาพูดถึงการแปลงเชิงเส้นจากหลายมิติไปเป็นมิติเดียว ซึ่งก็คือเส้นจำนวน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "ฟังก์ชันเหล่านี้เป็นฟังก์ชันที่รับเวกเตอร์ 2d แล้วแยกตัวเลขออกมา แต่แน่นอนว่าการแปลงเชิงเส้นนั้นถูกจำกัดมากกว่าฟังก์ชันทั่วไปที่มีอินพุต 2d และเอาต์พุต 1d ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "เช่นเดียวกับการแปลงในมิติที่สูงกว่า เหมือนกับที่ผมพูดถึงในบทที่ 3 มีคุณสมบัติทางการบางอย่างที่ทำให้ฟังก์ชันเหล่านี้เป็นเส้นตรง แต่ผมจะตั้งใจเพิกเฉยต่อสิ่งเหล่านั้นในที่นี้ เพื่อไม่ให้เบี่ยงเบนความสนใจไปจากเป้าหมายสุดท้ายของเรา และแทน มุ่งเน้นไปที่คุณสมบัติทางสายตาบางอย่างที่เทียบเท่ากับสิ่งที่เป็นทางการทั้งหมด ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "หากคุณใช้เส้นจุดที่เว้นระยะเท่าๆ กันและใช้การแปลง การแปลงเชิงเส้นจะทำให้จุดเหล่านั้นมีระยะห่างเท่าๆ กันเมื่อจุดตกลงในพื้นที่เอาต์พุตซึ่งก็คือเส้นจำนวน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "ไม่เช่นนั้น หากมีเส้นจุดที่มีระยะห่างไม่เท่ากัน การแปลงของคุณจะไม่เป็นเส้นตรง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "เช่นเดียวกับกรณีที่เราเคยเห็นมาก่อน หนึ่งในการแปลงเชิงเส้นเหล่านี้ถูกกำหนดโดยสมบูรณ์โดยที่มันใช้ i-hat และ j-hat แต่คราวนี้ เวกเตอร์พื้นฐานแต่ละตัวเหล่านั้นตกลงบนตัวเลข ดังนั้นเมื่อเราบันทึกว่า พวกมันลงจอดเป็นคอลัมน์ของเมทริกซ์ แต่ละคอลัมน์มีเลขตัวเดียว ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "นี่คือเมทริกซ์ขนาด 1x2 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "เรามาดูตัวอย่างความหมายของการใช้การแปลงอย่างใดอย่างหนึ่งกับเวกเตอร์กัน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "สมมุติว่าคุณมีการแปลงเชิงเส้นที่เอา i-hat เป็น 1 และ j-hat เป็นลบ 2 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "เพื่อติดตามว่าเวกเตอร์ที่มีพิกัด เช่น 4, 3 จบลง ลองแยกเวกเตอร์นี้เป็น 4 คูณ i-hat บวก 3 คูณ j-hat ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "ผลที่ตามมาของความเป็นเส้นตรงคือหลังจากการแปลง เวกเตอร์จะเป็น 4 คูณตำแหน่งที่ i-hat ตกลง 1 บวก 3 คูณตำแหน่งที่ j-hat ตกลง ลบ 2 ซึ่งในกรณีนี้บอกเป็นนัยว่ามันตกลงบนค่าลบ 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "เมื่อคุณคำนวณโดยใช้ตัวเลขล้วนๆ, มันจะเป็นการคูณเมทริกซ์เวกเตอร์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "ทีนี้ การดำเนินการเชิงตัวเลขของการคูณเมทริกซ์ 1x2 ด้วยเวกเตอร์ ให้ความรู้สึกเหมือนกับการหาดอทโปรดัคของเวกเตอร์สองตัว ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "เมทริกซ์ 1x2 นั่นดูเหมือนเวกเตอร์ที่เราเอียงไปด้านข้างไม่ใช่เหรอ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "ในความเป็นจริง เราสามารถพูดได้ในขณะนี้ว่ามีความสัมพันธ์ที่ดีระหว่างเมทริกซ์ 1x2 และเวกเตอร์ 2D ซึ่งกำหนดโดยการเอียงการแสดงตัวเลขของเวกเตอร์ที่ด้านข้างเพื่อให้ได้เมทริกซ์ที่เกี่ยวข้อง หรือให้ทิปเมทริกซ์สำรองเพื่อให้ได้เวกเตอร์ที่เกี่ยวข้อง . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "เนื่องจากเราแค่ดูนิพจน์ตัวเลขตอนนี้ การกลับไปกลับมาระหว่างเวกเตอร์กับเมทริกซ์ 1x2 อาจดูเหมือนเป็นเรื่องไร้สาระ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "แต่นี่แสดงให้เห็นบางสิ่งที่ยอดเยี่ยมจริงๆ เมื่อมองจากมุมมองทางเรขาคณิต ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "มีความเชื่อมโยงบางอย่างระหว่างการแปลงเชิงเส้น ที่เปลี่ยนเวกเตอร์เป็นตัวเลข และเวกเตอร์ด้วยตัวมันเอง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "ผมขอแสดงตัวอย่างที่ให้ความกระจ่างถึงความสำคัญ และสิ่งที่เกิดขึ้นเพื่อตอบคำถามดอทโปรดัคจากก่อนหน้านี้ด้วย ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "ลืมสิ่งที่คุณได้เรียนรู้ และจินตนาการว่าคุณยังไม่รู้ว่าดอทโปรดัคเกี่ยวข้องกับการฉายภาพ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "สิ่งที่ฉันจะทำตรงนี้คือคัดลอกเส้นจำนวนมาวางในแนวทแยงในช่องว่าง โดยให้เลข 0 อยู่ที่จุดกำเนิด. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "ทีนี้ ลองคิดถึงเวกเตอร์หน่วยสองมิติ ซึ่งมีปลายอยู่ที่เลข 1 บนเส้นจำนวน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "ฉันอยากจะตั้งชื่อผู้ชายคนนั้นว่า ยูแฮต ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "เจ้าตัวน้อยคนนี้มีบทบาทสำคัญในสิ่งที่กำลังจะเกิดขึ้น ดังนั้นเพียงแค่เก็บเขาไว้ในใจของคุณ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "ถ้าเราฉายเวกเตอร์ 2D ลงบนเส้นจำนวนแนวทแยงนี้โดยตรง เราก็เพิ่งนิยามฟังก์ชันที่นำเวกเตอร์ 2D ไปเป็นตัวเลข ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "ยิ่งไปกว่านั้น ฟังก์ชันนี้เป็นฟังก์ชันเชิงเส้นจริง เนื่องจากผ่านการทดสอบการมองเห็นว่าเส้นใดๆ ของจุดที่เว้นระยะเท่าๆ กันยังคงมีระยะห่างเท่าๆ กันเมื่อตกลงบนเส้นจำนวน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "เพื่อให้ชัดเจน แม้ว่าฉันจะฝังเส้นจำนวนในพื้นที่ 2D เช่นนี้ แต่ผลลัพธ์ของฟังก์ชันจะเป็นตัวเลข ไม่ใช่เวกเตอร์ 2D ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "คุณควรคิดถึงฟังก์ชันที่รับพิกัดสองพิกัดและส่งออกพิกัดเดียว ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "แต่เวกเตอร์ U-hat นั้นเป็นเวกเตอร์สองมิติ อยู่ในสเปซอินพุต ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "มันตั้งอยู่ในลักษณะที่ซ้อนทับกับการฝังเส้นจำนวน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "ด้วยการฉายภาพนี้, เราเพิ่งนิยามการแปลงเชิงเส้นจากเวกเตอร์ 2D ไปเป็นตัวเลข เราก็จะสามารถหาเมทริกซ์ 1x2 สักแบบที่อธิบายการแปลงนั้นได้ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "หากต้องการหาเมทริกซ์ 1x2 ลองขยายการตั้งค่าเส้นจำนวนแนวทแยงนี้แล้วคิดว่าแต่ละแฮตและแฮตลงจอดตรงไหน เนื่องจากจุดลงจอดเหล่านั้นจะเป็นคอลัมน์ของเมทริกซ์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "ส่วนนี้เจ๋งสุดๆ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "เราสามารถให้เหตุผลผ่านมันด้วยความสมมาตรที่สวยงามจริงๆ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "เนื่องจาก I-hat และ U-hat เป็นเวกเตอร์หน่วยเดียวกัน การฉาย I-hat บนเส้นที่ผ่าน U-hat จึงดูสมมาตรโดยสิ้นเชิงเมื่อฉาย U-hat บนแกน x ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "ดังนั้นเมื่อเราถามว่าหมวก I ตกลงบนเลขอะไรตอนที่มันถูกฉายออกไป คำตอบจะเหมือนกับว่าหมวก U ตกลงบนเลขอะไรตอนที่ฉายลงบนแกน x ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "แต่การฉายหมวก U ไปบนแกน x หมายถึงการหาพิกัด x ของ U-hat ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "ตามสมมาตรแล้ว ตัวเลขที่หมวก I ตกลงเมื่อฉายบนเส้นจำนวนแนวทแยง จะเป็นพิกัด x ของ U-hat ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "ไม่ดีเหรอ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "การให้เหตุผลเกือบจะเหมือนกันกับกรณีของ J-hat ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "ลองคิดดูสักครู่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "ด้วยเหตุผลเดียวกันทั้งหมด พิกัด y ของ U-hat ให้หมายเลขที่ J-hat ตกลงเมื่อฉายลงบนสำเนาเส้นจำนวน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "หยุดและไตร่ตรองสักครู่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "ฉันแค่คิดว่ามันเจ๋งจริงๆ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "ดังนั้นค่าของเมทริกซ์ 1x2 ที่อธิบายการแปลงโปรเจ็กชัน จะเป็นพิกัดของยูแฮต ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "และการคำนวณการแปลงการฉายภาพสำหรับเวกเตอร์ใดๆ ในอวกาศ ซึ่งต้องคูณเมทริกซ์นั้นด้วยเวกเตอร์เหล่านั้น ในทางคำนวณก็เหมือนกับการหาดอทโปรดัคด้วย U-hat ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "นี่คือเหตุผลว่าทำไมการหาดอทโปรดัคด้วยเวกเตอร์หน่วยจึงสามารถตีความได้ว่าเป็นการฉายเวกเตอร์ไปยังสแปนของเวกเตอร์หน่วยนั้นและรับความยาว ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "แล้วเวกเตอร์ที่ไม่ใช่หน่วยล่ะ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "ตัวอย่างเช่น สมมุติว่าเราหาเวกเตอร์หน่วย U-hat แต่เราขยายมันขึ้นเป็น 3 เท่า ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "โดยตัวเลข แต่ละส่วนประกอบจะถูกคูณด้วย 3 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "เมื่อดูเมทริกซ์ที่เกี่ยวข้องกับเวกเตอร์นั้น, มันต้องใช้ I-hat และ J-hat เป็นสามเท่าของค่าที่มันตกลงมาก่อน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "เนื่องจากนี่เป็นเส้นตรงทั้งหมด จึงบอกเป็นนัยโดยทั่วไปว่าเมทริกซ์ใหม่สามารถตีความได้ว่าเป็นการฉายเวกเตอร์ใดๆ ลงบนสำเนาเส้นจำนวนแล้วคูณตำแหน่งที่มันตกลงด้วย 3 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "นี่คือเหตุผลว่าทำไมดอทโปรดัคที่มีเวกเตอร์ที่ไม่ใช่หน่วยจึงสามารถตีความได้ว่าเป็นการฉายภาพลงบนเวกเตอร์นั้นก่อน จากนั้นจึงขยายความยาวของเส้นฉายภาพนั้นตามความยาวของเวกเตอร์ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "ใช้เวลาสักครู่เพื่อคิดถึงสิ่งที่เกิดขึ้นที่นี่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "เรามีการแปลงเชิงเส้นจากปริภูมิ 2 มิติไปเป็นเส้นจำนวน ซึ่งไม่ได้กำหนดไว้เป็นเวกเตอร์ตัวเลขหรือผลคูณดอทตัวเลข แต่ถูกกำหนดโดยการฉายพื้นที่ลงบนสำเนาเส้นทแยงมุมของเส้นจำนวน ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "แต่เนื่องจากการแปลงเป็นแบบเชิงเส้น จึงจำเป็นต้องอธิบายด้วยเมทริกซ์ขนาด 1x2 บางตัว ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "และเนื่องจากการคูณเมทริกซ์ 1x2 ด้วยเวกเตอร์ 2 มิติ ก็เหมือนกับการหมุนเมทริกซ์นั้นไปข้าง ๆ แล้วหาดอทโปรดัค การแปลงนี้จึงเกี่ยวข้องกับเวกเตอร์ 2 มิติบางตัวอย่างหลีกเลี่ยงไม่ได้ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "บทเรียนตรงนี้ก็คือว่าทุกครั้งที่คุณมีการแปลงเชิงเส้นอันใดอันหนึ่ง โดยที่สเปซเอาท์พุตคือเส้นจำนวน ไม่ว่าจะนิยามไว้ยังไง ก็จะมีเวกเตอร์ v เฉพาะตัวที่สอดคล้องกับการแปลงนั้น ในความหมายว่าการใช้การแปลงนั้น ก็เหมือนกับการหาดอทโปรดัคกับเวกเตอร์นั้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "สำหรับฉันแล้ว นี่มันงดงามจริงๆ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "มันเป็นตัวอย่างของบางสิ่งในทางคณิตศาสตร์ที่เรียกว่าความเป็นคู่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "ความเป็นคู่แสดงออกมาในรูปแบบและรูปแบบต่างๆ มากมายตลอดทั้งคณิตศาสตร์ และเป็นเรื่องยากมากที่จะให้คำจำกัดความจริงๆ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "หากพูดแบบหลวมๆ มันหมายถึงสถานการณ์ที่คุณมีความสอดคล้องกันอย่างเป็นธรรมชาติแต่น่าประหลาดใจระหว่างสิ่งทางคณิตศาสตร์สองประเภท ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "สำหรับกรณีพีชคณิตเชิงเส้นที่คุณเพิ่งเรียนมา คุณจะบอกว่าคู่ของเวกเตอร์คือการแปลงเชิงเส้นที่มันเข้ารหัส และคู่ของการแปลงเชิงเส้นจากปริภูมิหนึ่งเป็นหนึ่งมิติคือเวกเตอร์ตัวใดตัวหนึ่งในปริภูมินั้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "หากมองจากภายนอกแล้ว ดอทโปรดัคเป็นเครื่องมือทางเรขาคณิตที่มีประโยชน์มากสำหรับการทำความเข้าใจการฉายภาพและสำหรับการทดสอบว่าเวกเตอร์มีแนวโน้มที่จะชี้ไปในทิศทางเดียวกันหรือไม่ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "และนั่นอาจเป็นสิ่งที่สำคัญที่สุดที่คุณต้องจำเกี่ยวกับดอทโปรดัค ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "แต่ในระดับที่ลึกกว่านั้น การดอทเวกเตอร์สองตัวเข้าด้วยกันเป็นวิธีหนึ่งในการแปลเวกเตอร์หนึ่งตัวเข้าสู่โลกแห่งการเปลี่ยนแปลง ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "ขอย้ำอีกครั้งว่านี่อาจดูเหมือนเป็นจุดไร้สาระที่ต้องเน้นย้ำ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "มันเป็นเพียงการคำนวณมากเกินไป ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "แต่เหตุผลที่ฉันพบว่าสิ่งนี้สำคัญมากก็คือ ตลอดทั้งคณิตศาสตร์ เมื่อคุณต้องจัดการกับเวกเตอร์ เมื่อคุณได้รู้จักบุคลิกภาพของมันแล้ว บางครั้งคุณก็ตระหนักว่า มันง่ายกว่าที่จะเข้าใจ ไม่ใช่เป็นลูกศรในอวกาศ แต่เป็น รูปลักษณ์ทางกายภาพของการแปลงเชิงเส้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space t",
  "translatedText": "เหมือนกับว่าเวกเตอร์เป็นเพียงการจดชวเลขแนวความคิดสำหรับการแปลงบางอย่าง เนื่องจากมันง่ายกว่าสำหรับเราที่จะคิดถึงลูกศรในอวกาศ แทนที่จะย้ายพื้นที่ทั้งหมดนั้น ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.31
 },
 {
  "input": "o the number line. In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "ในวิดีโอหน้า คุณจะเห็นตัวอย่างเจ๋งๆ อีกอย่างหนึ่งของความเป็นคู่นี้ ขณะที่ผมพูดถึงผลคูณไขว้ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 820.31,
  "end": 829.19
 }
]