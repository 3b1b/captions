[
 {
  "input": "[\"Ode to Joy\", by Beethoven, plays to the end of the piano.] Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "",
  "from_community_srt": "Calvin: \"Sabe, não acho que matemática seja uma ciência. Acho que é uma religião.\" Hobbes: \"Uma religião?\" Calvin: \"É. Todas aquelas equações são como milagres. Você pega dois números, e quando você os soma, eles magicamente viram um número NOVO! Calvin: \"E ninguém sabe dizer como isso acontece. Ou você acredita, ou não acredita.\" Tradicionalmente, o produto escalar é algo introduzido bem cedo num curso de Álgebra Linear, geralmente bem no começo.",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "",
  "from_community_srt": "Então pode parecer estranho que adiei  tanto o conceito nesta série.",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "",
  "from_community_srt": "Eu fiz isso pois existe uma maneira padrão de introduzir este tópico a qual requer nada mais que um entendimento básico de vetores, mas um entendimento profundo do papel que produtos escalares desempenham em matemática apenas pode ser obtido por meio das transformações lineares.",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "",
  "from_community_srt": "Antes disso, porém, deixe-me cobrir brevemente a maneira padrão com a qual produtos escalares são introduzidos. Que estou assumindo ser pelo menos uma revisão parcial para alguns espectadores.",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "",
  "from_community_srt": "Numericamente, se você tem dois vetores da mesma dimensão; duas listas de números com o mesmo tamanho, tomar o produto escalar entre eles, significa, colocar par a par todas as coordenadas, multiplicar estes pares,",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "",
  "from_community_srt": "e somar o resultado. Então o vetor [1, 2] escalar com [3, 4], seria 1 x 3 + 2 x 4.",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "",
  "from_community_srt": "O vetor [6, 2, 8, 3] escalar com [1, 8, 5, 3] seria: 6 x 1 + 2 x 8 + 8 x 5 + 3 x 3.",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "",
  "from_community_srt": "Por sorte, esta conta tem uma bela interpretação geométrica.",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "",
  "from_community_srt": "Pense sobre o produto escalar entre dois vetores 'v' e 'w', imagine a projeção de 'w' na linha que passa pela origem e pela ponta de 'v'.",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "",
  "from_community_srt": "Multiplicando o comprimento desta projeção pelo comprimento de 'v', temos o produto escalar",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "",
  "from_community_srt": "'v . w'. Exceto quando esta projeção de 'w' está apontando na direção oposta de 'v',",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "",
  "from_community_srt": "este produto escalar seria negativo. Então quando dois vetores estão apontando mais ou menos na mesma direção,",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "",
  "from_community_srt": "seu produto escalar é positivo. quando eles são perpendiculares, significando que a projeção de um sobre o outro é o vetor [0] o produto escalar é 0.",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "",
  "from_community_srt": "E se eles apontam em direções opostas,",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "",
  "from_community_srt": "seu produto escalar é negativo. Agora, esta interpretação é estranhamente assimétrica,",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "",
  "from_community_srt": "tratando os dois vetores de forma diferente,",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "",
  "from_community_srt": "então, quando aprendi isso pela primeira vez,",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "",
  "from_community_srt": "fiquei surpreso que a ordem não importava. Você poderia projetar 'v' em 'w'; multiplicar o comprimento do 'v' projetado e ainda sair com o mesmo resultado.",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "",
  "from_community_srt": "Digo, não parece ser um processo completamente diferente?",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "",
  "from_community_srt": "Aqui está a intuição para por que a ordem não importa:",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "",
  "from_community_srt": "se 'v' e 'w' tivessem o mesmo comprimento por sorte,",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "",
  "from_community_srt": "poderíamos aproveitar um pouco de simetria, pois projetar 'w' em 'v' e em seguida multiplicar o comprimento daquela projeção pelo comprimento de 'v', é uma imagem refletida da projeção de 'v' em 'w' e posterior multiplicação do comprimento daquela projeção por 'w'.",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "",
  "from_community_srt": "Agora, se você \"escala\" um deles, digamos, 'v', por uma constante, tipo 2, de modo que agora eles não têm o mesmo comprimento, a simetria foi quebrada.",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "",
  "from_community_srt": "Mas vamos pensar como interpretar o produto escalar entre este novo vetor '2v' e",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "",
  "from_community_srt": "'w'. Se você pensa em 'w' sendo projetado em 'v', então o produto escalar '2v . w' será exatamente o dobro do produto escalar 'v.w'.",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "",
  "from_community_srt": "Isto é porque quando você \"escala\" 'v' por 2, não muda o comprimento da projeção em 'w' mas dobra o comprimento do vetor no qual você está projetando.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "",
  "from_community_srt": "Por outro lado, vamos dizer que você está pensando em 'v' sendo projetado sobre 'w'.",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "",
  "from_community_srt": "Bem, neste caso, o comprimento da projeção é a coisa a ser \"escalada\" quando multiplicamos 'v' por 2. O comprimento do vetor em que você está projetando permanece constante.",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "",
  "from_community_srt": "Então, o efeito geral é o mesmo: dobrar o produto escalar.",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "",
  "from_community_srt": "Assim sendo, apesar da simetria estar quebrada neste caso, o efeito desta \"escalagem\" no valor do produto escalar é o mesmo",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "",
  "from_community_srt": "sobre ambas as interpretações. Também há outra grande pergunta que me confundia quando aprendi isso pela primeira vez:",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "",
  "from_community_srt": "\"O que raios esse processo numérico de multiplicar pares de coordenadas e somar os produtos, tem a ver com projeção?\"",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "",
  "from_community_srt": "Bem, para dar uma resposta satisfatória, e também para fazer justiça completa ao significado do produto escalar, precisamos escavar algo mais profundo acontecendo aqui, comumente chamado de \"dualidade\".",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "",
  "from_community_srt": "Mas antes de chegar lá, preciso passar um tempo falando sobre transformações lineares de múltiplas dimensões em uma dimensão,",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "",
  "from_community_srt": "que é apenas a reta numérica. Estas são funções que tomam vetores 2D e cospem números. Mas transformações lineares são, é claro, muito mais restritas que suas funções gerais de duas variáveis com uma saída.",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "",
  "from_community_srt": "Como em transformações em dimensões maiores, aquelas sobre as quais conversei no capítulo 3, há propriedades mais formais que fazem estas funções serem lineares. Mas, como vou ignorar de propósito essas propriedades aqui para não nos distrairmos, vamos focar em uma certa propriedade visual que é equivalente às coisas formais.",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "",
  "from_community_srt": "Se você toma uma linha de pontos igualmente espaçados, e aplica a transformação, uma transformação linear vai manter esses pontos espaçados quando que eles aterrissam no espaço de chegada, que é a reta numérica.",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "",
  "from_community_srt": "Do contrário, se há uma linha de pontos que não fica igualmente espaçada,",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "",
  "from_community_srt": "então a sua transformação não é linear. Como nos casos que vimos antes, qualquer uma dessas transformações lineares é completamente determinada por onde ela leva î e ĵ mas dessa vez, cada um desses vetores de base é levado em um número. Então, quando registramos onde vão parar como colunas de uma matriz, cada uma dessas colunas tem apenas um único número.",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "",
  "from_community_srt": "Trata-se de uma matriz 1 x 2. Vamos acompanhar um exemplo do que significa aplicar uma destas transformações",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "",
  "from_community_srt": "a um vetor. Digamos que você tenha uma transformação linear que leva î em 1 e ĵ em -2.",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "",
  "from_community_srt": "Para seguir onde um vetor com coordenadas, digamos, [4, 3] vai parar, quebre este vetor como 4 vezes î + 3 vezes ĵ.",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "",
  "from_community_srt": "Como consequência da linearidade, após a transformação o vetor será: 4 vezes o lugar onde Î foi parar, que é 1, mais 3 vezes o lugar onde ĵ foi parar, -2. Que, neste caso,",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "",
  "from_community_srt": "significa que o vetor foi parar em -2. Quando você faz essa conta de uma forma puramente numérica,",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "",
  "from_community_srt": "é uma multiplicação matriz-vetor. Essa operação de multiplicar uma matriz 1 x 2 por um vetor, parece com o produto escalar de dois vetores.",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "",
  "from_community_srt": "Aquela matriz 1 x 2 não se parece muito com um vetor virado de lado?",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "",
  "from_community_srt": "Na verdade, poderíamos dizer agora que há uma bela associação entre matrizes 1 x 2 e vetores 2D, definida pela inclinação lateral da representação numérica de um vetor, para conseguir a matriz associada, ou colocar a matriz de pé para obter o vetor associado.",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "",
  "from_community_srt": "Dado que estamos olhando por expressões numéricas agora, ficar indo e vindo entre vetores e matrizes pode parecer uma coisa boba",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "",
  "from_community_srt": "a se fazer. Mas isso sugere algo que é verdadeiramente interessante do ponto de vista geométrico:",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "",
  "from_community_srt": "há algum tipo de conexão entre transformações lineares que levam vetores a números, [funcionais lineares],",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "",
  "from_community_srt": "e os próprios vetores. Deixe-me mostrar um exemplo que esclarece esse significado, E que termina por responder a pergunta sobre o produto escalar de antes.",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "",
  "from_community_srt": "Desaprenda o que aprendeu e imagine que você não saiba que como o produto escalar se relaciona com a projeção.",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "",
  "from_community_srt": "O que vou fazer aqui é tomar uma cópia da reta numérica, e colocá-la diagonalmente no espaço de alguma forma,",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "",
  "from_community_srt": "com o número 0 na origem. Agora, pense no vetor unitário bidimensional cuja ponta fica onde o número 1 da reta está.",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "",
  "from_community_srt": "Vou dar um nome àquele cara:",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "",
  "from_community_srt": "û. Esse carinha cumpre um papel importante no que vai acontecer, então mantenha-o em mente.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "",
  "from_community_srt": "Se projetarmos vetores 2D direto nessa linha diagonal, acabamos de definir uma função que  leva vetores 2D em números.",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "",
  "from_community_srt": "Mais ainda, essa função é linear dado que passa no nosso teste visual segundo o qual qualquer linha de pontos igualmente espaçados deve se manter assim quando for",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "",
  "from_community_srt": "para a reta numérica. Só pra esclarecer, ainda que eu tenha mergulhado a reta numérica no espaço 2D assim, A saída da função é composta por números,",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "",
  "from_community_srt": "não vetores 2D. Você deveria pensar numa função que pega coordenadas e cospe uma coordenada simples.",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "",
  "from_community_srt": "Mas aquele vetor û é um vetor bidimensional, vivendo no espaço de entrada.",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "",
  "from_community_srt": "Ele apenas está situado de forma que se sobrepõe com a reta numérica imersa.",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "",
  "from_community_srt": "Com esta projeção, acabamos de definir uma transformação linear que leva de vetores 2D em números, enão vamos ser capazes de encontrar algum tipo de matriz 1 x 2 que descreve esta transformação.",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "",
  "from_community_srt": "Para encontrar essa matriz, vamos olhar nosso esquema da reta diagonal e pensar em onde î e ĵ vão parar, dado que esses pontos de aterrissagem serão as colunas da matriz.",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "",
  "from_community_srt": "Esta parte é super legal,",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "",
  "from_community_srt": "podemos pensar sobre isso com um pouco de simetria bem elegante:",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "",
  "from_community_srt": "uma vez que î e ĵ são ambos vetores unitários, projetar î na linha passando por û é totalmente simétrico a projetar û no eixo x.",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "",
  "from_community_srt": "Então, quando formos perguntados em que número o î vai parar quando projetado, a resposta será igual a onde û vai parar quando projetado",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "",
  "from_community_srt": "no eixo x. Mas projetar û no eixo x é o mesmo que tomar a coordenada x de û.",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "",
  "from_community_srt": "Então, por simetria, o número onde î vai parar quando projetado na linha diagonal vai ser a coordenada x de û.",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "",
  "from_community_srt": "Não é legal? O raciocínio é quase idêntico para o caso do ĵ.",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "",
  "from_community_srt": "Pense sobre ele um pouco. Pelas mesmas razões, a coordenada y de û nos dá o número onde ĵ vai parar quando projetado na cópia da linha numérica.",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "",
  "from_community_srt": "Pare e pense um pouco sobre isso;",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "",
  "from_community_srt": "eu simplesmente acho isso muito legal.",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "",
  "from_community_srt": "Então, as entradas da matriz 1 x 2 descrevendo a transformação de projeção vão ser as coordenadas de û.",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "",
  "from_community_srt": "Computar essa projeção para vetores arbitrários no espaço, que requer multiplicar aquela matriz por aqueles vetores, é computacionalmente idêntico a  tomar um produto escalar com û.",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "",
  "from_community_srt": "É por isso que fazer o produto escalar com um vetor unitário pode ser interpretado como projetar um vetor na reta gerada por aquele vetor unitário e",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "",
  "from_community_srt": "tomar o comprimento. E sobre vetores não unitários?",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "",
  "from_community_srt": "Por exemplo, digamos que tomamos aquele vetor unitário û, mas o \"escalamos\" por um fator de 3.",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "",
  "from_community_srt": "Numericamente, cada coordenada é multiplicada por 3,",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "",
  "from_community_srt": "então olhando a matriz associada com aquele vetor, ela leva î e ĵ aos valores iguais a 3 vezes os valores antigos.",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "",
  "from_community_srt": "Dado que tudo isso é linear, implica que, mais geralmente, a nova matriz pode ser interpretada como a que projeta qualquer vetor na cópia da reta numérica, e multipica a projeção por 3.",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "",
  "from_community_srt": "É por isso que o produto interno com um vetor não unitário pode ser interpretado como primeiro projetar no vetor, depois multiplicar o comprimento da projeção pelo comprimento do vetor.",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "",
  "from_community_srt": "Pare um pouco para pensar no que aconteceu aqui.",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "",
  "from_community_srt": "Tínhamos uma transformação linear do espaço 2D para a reta numérica, a qual não foi definida em termos de vetores ou produtos escalares numéricos. Ela foi definida apenas pela projeção do espaço em uma cópia diagonal da reta numérica.",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "",
  "from_community_srt": "Mas, dado que a transformação é linear, seria necessariamente descrita por alguma matriz 1 x 2,",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "",
  "from_community_srt": "e dado que multiplicar uma matriz 1 x 2 por um vetor 2D é o mesmo que virar a matriz de lado e tomar o produto interno, essa transformação era, inescapavelmente,",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "",
  "from_community_srt": "relacionada a algum vetor 2D. A lição aqui é que, a qualquer momento que você tiver uma dessas transformações lineares cujo espaço de saída é a reta numérica, não importa como foi definida, vai haver um vetor único 'v', que se corresponde à aquela transformação, no sentido de que aplicar a transformação é o mesmo que tomar o produto escalar",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "",
  "from_community_srt": "com aquele vetor. Para mim, isso é absolutamente lindo.",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "",
  "from_community_srt": "É um exemplo de algo em matemática chamado \"dualidade\".",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "",
  "from_community_srt": "\"Dualidade\" aparece em muitas formas diferentes através da matemática e é bem difícil de se definir,",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "",
  "from_community_srt": "na verdade. Vagamente, se refere a situações em que você tem uma correspondência natural mas surpreendente entre dois tipos de coisas matemáticas.",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "",
  "from_community_srt": "Para o caso de Álgebra Linear que você acabou de aprender, você diria que o \"dual\" de um vetor é a transformação linear que ele codifica. E o dual de uma transformação linear do espaço em uma dimensão é um certo valor naquele espaço.",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "",
  "from_community_srt": "Então, para resumir, o produto interno é uma ferramenta geométrica bem útil para entender projeções e para testar se vetores estão ou não na mesma direção geral.",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "",
  "from_community_srt": "E isso é provavelmente a coisa mais importante para você lembrar sobre o produto escalar,",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "",
  "from_community_srt": "mas, num nível mais profundo, o produto escalar de dois vetores é uma forma de traduzir um deles no mundo de transformações:",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "",
  "from_community_srt": "mais uma vez, numericamente, isso pode parecer bobo de se enfatizar,",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "",
  "from_community_srt": "são só dois cálculos bem parecidos por acaso,",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "",
  "from_community_srt": "mas a razão de achar isso tão importante é que em matemática, quando você lida com um vetor, uma vez que você conhece a sua personalidade, às vezes você percebe que é mais fácil entendê-lo não como uma flecha no espaço, mas como a materialização física de uma transformação linear.",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "",
  "from_community_srt": "É como se o vetor fosse só uma abreviação para uma dada transformação, pois é mais fácil pra gente pensar em flechas no espaço do que mover todo o espaço para a reta numérica.",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.97
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "",
  "from_community_srt": "No próximo vídeo, você vai ver outro exemplo muito legal desta \"dualidade\" em ação",
  "n_reviews": 0,
  "start": 822.61,
  "end": 829.19
 }
]