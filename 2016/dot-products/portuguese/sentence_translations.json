[
 {
  "input": "Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "[&quot;Ode to Joy&quot;, de Beethoven, toca até o final do piano.] Tradicionalmente, produtos escalares são algo introduzido bem no início de um curso de álgebra linear, normalmente logo no início.",
  "model": "google_nmt",
  "from_community_srt": "Calvin: \"Sabe, não acho que matemática seja uma ciência. Acho que é uma religião.\" Hobbes: \"Uma religião?\" Calvin: \"É. Todas aquelas equações são como milagres. Você pega dois números, e quando você os soma, eles magicamente viram um número NOVO! Calvin: \"E ninguém sabe dizer como isso acontece. Ou você acredita, ou não acredita.\" Tradicionalmente, o produto escalar é algo introduzido bem cedo num curso de Álgebra Linear, geralmente bem no começo.",
  "n_reviews": 0,
  "start": 16.58,
  "end": 26.3
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "Portanto, pode parecer estranho que eu os tenha empurrado tão longe na série.",
  "model": "google_nmt",
  "from_community_srt": "Então pode parecer estranho que adiei  tanto o conceito nesta série.",
  "n_reviews": 0,
  "start": 26.64,
  "end": 29.58
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "Fiz isso porque existe uma maneira padrão de introduzir o tópico, que requer nada mais do que uma compreensão básica de vetores, mas uma compreensão mais completa do papel que os produtos escalares desempenham na matemática só pode ser realmente encontrada sob a luz das transformações lineares.",
  "model": "google_nmt",
  "from_community_srt": "Eu fiz isso pois existe uma maneira padrão de introduzir este tópico a qual requer nada mais que um entendimento básico de vetores, mas um entendimento profundo do papel que produtos escalares desempenham em matemática apenas pode ser obtido por meio das transformações lineares.",
  "n_reviews": 0,
  "start": 29.58,
  "end": 42.44
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "Antes disso, porém, deixe-me abordar brevemente a forma padrão como os produtos escalares são introduzidos, o que presumo ser pelo menos parcialmente revisado para vários espectadores.",
  "model": "google_nmt",
  "from_community_srt": "Antes disso, porém, deixe-me cobrir brevemente a maneira padrão com a qual produtos escalares são introduzidos. Que estou assumindo ser pelo menos uma revisão parcial para alguns espectadores.",
  "n_reviews": 0,
  "start": 43.48,
  "end": 50.62
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "Numericamente, se você tiver dois vetores da mesma dimensão, duas listas de números com os mesmos comprimentos, calcular seu produto escalar significa emparelhar todas as coordenadas, multiplicar esses pares e somar o resultado.",
  "model": "google_nmt",
  "from_community_srt": "Numericamente, se você tem dois vetores da mesma dimensão; duas listas de números com o mesmo tamanho, tomar o produto escalar entre eles, significa, colocar par a par todas as coordenadas, multiplicar estes pares, e somar o resultado.",
  "n_reviews": 0,
  "start": 51.44,
  "end": 64.98
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "Portanto, o vetor 1, 2 pontilhado com 3, 4 seria 1 vezes 3 mais 2 vezes 4.",
  "model": "google_nmt",
  "from_community_srt": "Então o vetor [1, 2] escalar com [3, 4], seria 1 x 3 + 2 x 4.",
  "n_reviews": 0,
  "start": 66.86,
  "end": 73.18
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "O vetor 6, 2, 8, 3 pontilhado com 1, 8, 5, 3 seria 6 vezes 1 mais 2 vezes 8 mais 8 vezes 5 mais 3 vezes 3.",
  "model": "google_nmt",
  "from_community_srt": "O vetor [6, 2, 8, 3] escalar com [1, 8, 5, 3] seria: 6 x 1 + 2 x 8 + 8 x 5 + 3 x 3.",
  "n_reviews": 0,
  "start": 74.58,
  "end": 83.72
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "Felizmente, este cálculo tem uma interpretação geométrica muito boa.",
  "model": "google_nmt",
  "from_community_srt": "Por sorte, esta conta tem uma bela interpretação geométrica.",
  "n_reviews": 0,
  "start": 84.74,
  "end": 88.66
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "Para pensar no produto escalar entre dois vetores, v e w, imagine projetar w na reta que passa pela origem e pela ponta de v.",
  "model": "google_nmt",
  "from_community_srt": "Pense sobre o produto escalar entre dois vetores 'v' e 'w', imagine a projeção de 'w' na linha que passa pela origem e pela ponta de 'v'.",
  "n_reviews": 0,
  "start": 89.34,
  "end": 97.98
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "Multiplicando o comprimento desta projeção pelo comprimento de v, você tem o produto escalar v ponto w.",
  "model": "google_nmt",
  "from_community_srt": "Multiplicando o comprimento desta projeção pelo comprimento de 'v', temos o produto escalar 'v .",
  "n_reviews": 0,
  "start": 98.78,
  "end": 104.46
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "Exceto quando esta projeção de w estiver apontando na direção oposta de v, esse produto escalar será na verdade negativo.",
  "model": "google_nmt",
  "from_community_srt": "w'. Exceto quando esta projeção de 'w' está apontando na direção oposta de 'v', este produto escalar seria negativo.",
  "n_reviews": 0,
  "start": 106.42,
  "end": 112.16
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "Portanto, quando dois vetores geralmente apontam na mesma direção, seu produto escalar é positivo.",
  "model": "google_nmt",
  "from_community_srt": "Então quando dois vetores estão apontando mais ou menos na mesma direção, seu produto escalar é positivo.",
  "n_reviews": 0,
  "start": 113.72,
  "end": 117.86
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "Quando eles são perpendiculares, o que significa que a projeção de um sobre o outro é o vetor zero, seu produto escalar é zero.",
  "model": "google_nmt",
  "from_community_srt": "quando eles são perpendiculares, significando que a projeção de um sobre o outro é o vetor [0] o produto escalar é 0.",
  "n_reviews": 0,
  "start": 119.24,
  "end": 125.56
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "E se eles apontarem geralmente na direção oposta, seu produto escalar será negativo.",
  "model": "google_nmt",
  "from_community_srt": "E se eles apontam em direções opostas,",
  "n_reviews": 0,
  "start": 125.98,
  "end": 129.6
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "Agora, esta interpretação é estranhamente assimétrica.",
  "model": "google_nmt",
  "from_community_srt": "seu produto escalar é negativo. Agora, esta interpretação é estranhamente assimétrica, tratando os dois vetores de forma diferente,",
  "n_reviews": 0,
  "start": 131.62,
  "end": 134.56
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "Trata os dois vetores de maneira muito diferente.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.8,
  "end": 136.5
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "Então, quando aprendi isso pela primeira vez, fiquei surpreso ao ver que a ordem não importa.",
  "model": "google_nmt",
  "from_community_srt": "então, quando aprendi isso pela primeira vez,",
  "n_reviews": 0,
  "start": 136.88,
  "end": 140.0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "Em vez disso, você poderia projetar v em w, multiplicar o comprimento do v projetado pelo comprimento de w e obter o mesmo resultado.",
  "model": "google_nmt",
  "from_community_srt": "fiquei surpreso que a ordem não importava. Você poderia projetar 'v' em 'w'; multiplicar o comprimento do 'v' projetado e ainda sair com o mesmo resultado.",
  "n_reviews": 0,
  "start": 140.96,
  "end": 148.22
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "Quero dizer, isso não parece um processo realmente diferente?",
  "model": "google_nmt",
  "from_community_srt": "Digo,",
  "n_reviews": 0,
  "start": 150.4,
  "end": 152.84
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "Aqui está a intuição de por que a ordem não importa.",
  "model": "google_nmt",
  "from_community_srt": "não parece ser um processo completamente diferente? Aqui está a intuição para por que a ordem não importa:",
  "n_reviews": 0,
  "start": 155.32,
  "end": 157.76
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "Se v e w tivessem o mesmo comprimento, poderíamos aproveitar alguma simetria.",
  "model": "google_nmt",
  "from_community_srt": "se 'v' e 'w' tivessem o mesmo comprimento por sorte, poderíamos aproveitar um pouco de simetria,",
  "n_reviews": 0,
  "start": 158.44,
  "end": 162.18
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "Já que projetar w em v, então multiplicar o comprimento dessa projeção pelo comprimento de v, é uma imagem espelhada completa de projetar v em w, então multiplicar o comprimento dessa projeção pelo comprimento de w.",
  "model": "google_nmt",
  "from_community_srt": "pois projetar 'w' em 'v' e em seguida multiplicar o comprimento daquela projeção pelo comprimento de 'v', é uma imagem refletida da projeção de 'v' em 'w' e posterior multiplicação do comprimento daquela projeção por 'w'.",
  "n_reviews": 0,
  "start": 163.08,
  "end": 175.24
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "Agora, se você dimensionar um deles, digamos v, por alguma constante como 2, de modo que eles não tenham comprimento igual, a simetria será quebrada.",
  "model": "google_nmt",
  "from_community_srt": "Agora, se você \"escala\" um deles, digamos, 'v', por uma constante, tipo 2, de modo que agora eles não têm o mesmo comprimento, a simetria foi quebrada.",
  "n_reviews": 0,
  "start": 177.28,
  "end": 184.36
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "Mas vamos pensar em como interpretar o produto escalar entre esse novo vetor, 2 vezes v e w.",
  "model": "google_nmt",
  "from_community_srt": "Mas vamos pensar como interpretar o produto escalar entre este novo vetor '2v' e 'w'.",
  "n_reviews": 0,
  "start": 185.02,
  "end": 190.04
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "Se você pensar que estava sendo projetado em v, então o produto escalar 2v ponto w será exatamente o dobro do produto escalar v ponto w.",
  "model": "google_nmt",
  "from_community_srt": "Se você pensa em 'w' sendo projetado em 'v', então o produto escalar '2v . w' será exatamente o dobro do produto escalar 'v.w'.",
  "n_reviews": 0,
  "start": 190.88,
  "end": 199.72
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "Isso ocorre porque quando você dimensiona v por 2, isso não altera o comprimento da projeção de w, mas dobra o comprimento do vetor no qual você está projetando.",
  "model": "google_nmt",
  "from_community_srt": "Isto é porque quando você \"escala\" 'v' por 2, não muda o comprimento da projeção em 'w' mas dobra o comprimento do vetor no qual você está projetando.",
  "n_reviews": 0,
  "start": 200.46,
  "end": 209.52
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "Mas, por outro lado, digamos que você estava pensando em v ser projetado em w.",
  "model": "google_nmt",
  "from_community_srt": "Por outro lado, vamos dizer que você está pensando em 'v' sendo projetado sobre 'w'.",
  "n_reviews": 0,
  "start": 210.46,
  "end": 214.2
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "Bem, nesse caso, o comprimento da projeção é o que é dimensionado quando multiplicamos v por 2, mas o comprimento do vetor no qual você está projetando permanece constante.",
  "model": "google_nmt",
  "from_community_srt": "Bem, neste caso, o comprimento da projeção é a coisa a ser \"escalada\" quando multiplicamos 'v' por 2. O comprimento do vetor em que você está projetando permanece constante.",
  "n_reviews": 0,
  "start": 214.9,
  "end": 223.0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "Portanto, o efeito geral ainda é apenas dobrar o produto escalar.",
  "model": "google_nmt",
  "from_community_srt": "Então, o efeito geral é o mesmo: dobrar o produto escalar.",
  "n_reviews": 0,
  "start": 223.0,
  "end": 226.66
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "Portanto, mesmo que a simetria seja quebrada neste caso, o efeito que esta escala tem sobre o valor do produto escalar é o mesmo em ambas as interpretações.",
  "model": "google_nmt",
  "from_community_srt": "Assim sendo, apesar da simetria estar quebrada neste caso, o efeito desta \"escalagem\" no valor do produto escalar é o mesmo",
  "n_reviews": 0,
  "start": 227.28,
  "end": 234.86
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "Há também uma outra grande questão que me confundiu quando aprendi essas coisas pela primeira vez.",
  "model": "google_nmt",
  "from_community_srt": "sobre ambas as interpretações.",
  "n_reviews": 0,
  "start": 236.64,
  "end": 240.34
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "Por que diabos esse processo numérico de combinar coordenadas, multiplicar pares e adicioná-los tem algo a ver com projeção?",
  "model": "google_nmt",
  "from_community_srt": "Também há outra grande pergunta que me confundia quando aprendi isso pela primeira vez: \"O que raios esse processo numérico de multiplicar pares de coordenadas e somar os produtos, tem a ver com projeção?\"",
  "n_reviews": 0,
  "start": 240.84,
  "end": 248.74
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "Bem, para dar uma resposta satisfatória, e também para fazer plena justiça ao significado do produto escalar, precisamos desenterrar algo um pouco mais profundo acontecendo aqui, que muitas vezes é conhecido pelo nome de dualidade.",
  "model": "google_nmt",
  "from_community_srt": "Bem, para dar uma resposta satisfatória, e também para fazer justiça completa ao significado do produto escalar, precisamos escavar algo mais profundo acontecendo aqui, comumente chamado de \"dualidade\".",
  "n_reviews": 0,
  "start": 250.64,
  "end": 261.4
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "Mas antes de entrar nisso, preciso passar algum tempo falando sobre transformações lineares de múltiplas dimensões para uma dimensão, que é apenas a reta numérica.",
  "model": "google_nmt",
  "from_community_srt": "Mas antes de chegar lá, preciso passar um tempo falando sobre transformações lineares de múltiplas dimensões em uma dimensão, que é apenas a reta numérica.",
  "n_reviews": 0,
  "start": 262.14,
  "end": 270.04
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "Essas são funções que recebem um vetor 2D e geram algum número, mas as transformações lineares são, obviamente, muito mais restritas do que sua função comum com uma entrada 2D e uma saída 1D.",
  "model": "google_nmt",
  "from_community_srt": "Estas são funções que tomam vetores 2D e cospem números. Mas transformações lineares são, é claro, muito mais restritas que suas funções gerais de duas variáveis com uma saída.",
  "n_reviews": 0,
  "start": 272.42,
  "end": 282.3
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "Tal como acontece com as transformações em dimensões superiores, como aquelas de que falei no capítulo 3, existem algumas propriedades formais que tornam estas funções lineares, mas vou ignorá-las propositadamente aqui para não desviar a atenção do nosso objetivo final e, em vez disso, concentre-se em uma determinada propriedade visual que seja equivalente a todas as coisas formais.",
  "model": "google_nmt",
  "from_community_srt": "Como em transformações em dimensões maiores, aquelas sobre as quais conversei no capítulo 3, há propriedades mais formais que fazem estas funções serem lineares. Mas, como vou ignorar de propósito essas propriedades aqui para não nos distrairmos, vamos focar em uma certa propriedade visual que é equivalente às coisas formais.",
  "n_reviews": 0,
  "start": 283.02,
  "end": 298.26
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "Se você pegar uma linha de pontos espaçados uniformemente e aplicar uma transformação, uma transformação linear manterá esses pontos espaçados uniformemente quando pousarem no espaço de saída, que é a reta numérica.",
  "model": "google_nmt",
  "from_community_srt": "Se você toma uma linha de pontos igualmente espaçados, e aplica a transformação, uma transformação linear vai manter esses pontos espaçados quando que eles aterrissam no espaço de chegada, que é a reta numérica.",
  "n_reviews": 0,
  "start": 299.04,
  "end": 311.28
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "Caso contrário, se houver alguma linha de pontos com espaçamento desigual, sua transformação não será linear.",
  "model": "google_nmt",
  "from_community_srt": "Do contrário, se há uma linha de pontos que não fica igualmente espaçada, então a sua transformação não é linear.",
  "n_reviews": 0,
  "start": 312.42,
  "end": 317.14
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "Tal como acontece com os casos que vimos antes, uma dessas transformações lineares é completamente determinada por onde leva i-hat e j-hat, mas desta vez cada um desses vetores de base apenas pousa em um número, então quando registramos onde eles caem como colunas de uma matriz, cada uma dessas colunas tem apenas um único número.",
  "model": "google_nmt",
  "from_community_srt": "Como nos casos que vimos antes, qualquer uma dessas transformações lineares é completamente determinada por onde ela leva î e ĵ mas dessa vez, cada um desses vetores de base é levado em um número. Então, quando registramos onde vão parar como colunas de uma matriz, cada uma dessas colunas tem apenas um único número.",
  "n_reviews": 0,
  "start": 319.22,
  "end": 336.82
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "Esta é uma matriz 1x2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.46,
  "end": 339.84
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "Vejamos um exemplo do que significa aplicar uma dessas transformações a um vetor.",
  "model": "google_nmt",
  "from_community_srt": "Trata-se de uma matriz 1 x 2. Vamos acompanhar um exemplo do que significa aplicar uma destas transformações a um vetor.",
  "n_reviews": 0,
  "start": 341.86,
  "end": 345.66
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "Digamos que você tenha uma transformação linear que leva i-hat para 1 e j-hat para menos 2.",
  "model": "google_nmt",
  "from_community_srt": "Digamos que você tenha uma transformação linear que leva î em 1 e ĵ em -2.",
  "n_reviews": 0,
  "start": 346.38,
  "end": 351.68
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "Para saber onde termina um vetor com coordenadas, digamos, 4, 3, pense em dividir esse vetor como 4 vezes i-hat mais 3 vezes j-hat.",
  "model": "google_nmt",
  "from_community_srt": "Para seguir onde um vetor com coordenadas, digamos, [4, 3] vai parar, quebre este vetor como 4 vezes î + 3 vezes ĵ.",
  "n_reviews": 0,
  "start": 352.42,
  "end": 361.02
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "Uma consequência da linearidade é que após a transformação, o vetor será 4 vezes o local onde i-hat pousa, 1, mais 3 vezes o local onde j-hat pousa, menos 2, o que neste caso implica que ele pousa em negativo 2.",
  "model": "google_nmt",
  "from_community_srt": "Como consequência da linearidade, após a transformação o vetor será: 4 vezes o lugar onde Î foi parar, que é 1, mais 3 vezes o lugar onde ĵ foi parar, -2. Que, neste caso,",
  "n_reviews": 0,
  "start": 361.84,
  "end": 375.78
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "Quando você faz esse cálculo puramente numericamente, é uma multiplicação de vetores de matrizes.",
  "model": "google_nmt",
  "from_community_srt": "significa que o vetor foi parar em -2. Quando você faz essa conta de uma forma puramente numérica,",
  "n_reviews": 0,
  "start": 378.02,
  "end": 382.36
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "Agora, esta operação numérica de multiplicar uma matriz 1x2 por um vetor é como calcular o produto escalar de dois vetores.",
  "model": "google_nmt",
  "from_community_srt": "é uma multiplicação matriz-vetor. Essa operação de multiplicar uma matriz 1 x 2 por um vetor, parece com o produto escalar de dois vetores.",
  "n_reviews": 0,
  "start": 385.7,
  "end": 392.86
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "Essa matriz 1x2 não se parece apenas com um vetor que viramos de lado?",
  "model": "google_nmt",
  "from_community_srt": "Aquela matriz 1 x 2 não se parece muito com um vetor virado de lado? Na verdade,",
  "n_reviews": 0,
  "start": 393.46,
  "end": 396.8
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "Na verdade, poderíamos dizer agora que existe uma boa associação entre matrizes 1x2 e vetores 2D, definida inclinando a representação numérica de um vetor para o lado para obter a matriz associada, ou inclinando a matriz de volta para obter o vetor associado. .",
  "model": "google_nmt",
  "from_community_srt": "poderíamos dizer agora que há uma bela associação entre matrizes 1 x 2 e vetores 2D, definida pela inclinação lateral da representação numérica de um vetor, para conseguir a matriz associada, ou colocar a matriz de pé para obter o vetor associado.",
  "n_reviews": 0,
  "start": 397.96,
  "end": 412.58
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "Como estamos apenas olhando expressões numéricas agora, ir e voltar entre vetores e matrizes 1x2 pode parecer uma coisa boba de se fazer.",
  "model": "google_nmt",
  "from_community_srt": "Dado que estamos olhando por expressões numéricas agora, ficar indo e vindo entre vetores e matrizes pode parecer uma coisa boba",
  "n_reviews": 0,
  "start": 413.56,
  "end": 420.86
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "Mas isso sugere algo que é realmente incrível do ponto de vista geométrico.",
  "model": "google_nmt",
  "from_community_srt": "a se fazer.",
  "n_reviews": 0,
  "start": 421.46,
  "end": 425.12
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "Existe algum tipo de conexão entre as transformações lineares que transformam vetores em números e os próprios vetores.",
  "model": "google_nmt",
  "from_community_srt": "Mas isso sugere algo que é verdadeiramente interessante do ponto de vista geométrico: há algum tipo de conexão entre transformações lineares que levam vetores a números, [funcionais lineares],",
  "n_reviews": 0,
  "start": 425.38,
  "end": 431.72
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "Deixe-me mostrar um exemplo que esclarece o significado e que também responde ao quebra-cabeça do produto escalar anterior.",
  "model": "google_nmt",
  "from_community_srt": "e os próprios vetores. Deixe-me mostrar um exemplo que esclarece esse significado, E que termina por responder a pergunta sobre o produto escalar de antes.",
  "n_reviews": 0,
  "start": 434.78,
  "end": 441.38
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "Desaprenda o que aprendeu e imagine que ainda não sabe que o produto escalar está relacionado à projeção.",
  "model": "google_nmt",
  "from_community_srt": "Desaprenda o que aprendeu e imagine que você não saiba que como o produto escalar se relaciona com a projeção.",
  "n_reviews": 0,
  "start": 442.14,
  "end": 447.18
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "O que vou fazer aqui é pegar uma cópia da reta numérica e colocá-la de alguma forma na diagonal no espaço, com o número 0 na origem.",
  "model": "google_nmt",
  "from_community_srt": "O que vou fazer aqui é tomar uma cópia da reta numérica, e colocá-la diagonalmente no espaço de alguma forma,",
  "n_reviews": 0,
  "start": 448.86,
  "end": 456.06
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "Agora pense no vetor unitário bidimensional cuja ponta fica onde está o número 1 do número.",
  "model": "google_nmt",
  "from_community_srt": "com o número 0 na origem. Agora, pense no vetor unitário bidimensional cuja ponta fica onde o número 1 da reta está.",
  "n_reviews": 0,
  "start": 456.9,
  "end": 461.92
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "Eu quero dar um nome a esse cara, seu chapéu.",
  "model": "google_nmt",
  "from_community_srt": "Vou dar um nome àquele cara:",
  "n_reviews": 0,
  "start": 462.4,
  "end": 464.56
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "Esse carinha desempenha um papel importante no que está para acontecer, então mantenha-o em mente.",
  "model": "google_nmt",
  "from_community_srt": "û. Esse carinha cumpre um papel importante no que vai acontecer, então mantenha-o em mente.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 470.02
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "Se projetarmos vetores 2d diretamente nesta reta numérica diagonal, na verdade, acabamos de definir uma função que transforma vetores 2d em números.",
  "model": "google_nmt",
  "from_community_srt": "Se projetarmos vetores 2D direto nessa linha diagonal, acabamos de definir uma função que  leva vetores 2D em números.",
  "n_reviews": 0,
  "start": 470.74,
  "end": 478.96
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "Além do mais, esta função é na verdade linear, uma vez que passa no nosso teste visual de que qualquer linha de pontos espaçados uniformemente permanece espaçada uniformemente quando pousa na reta numérica.",
  "model": "google_nmt",
  "from_community_srt": "Mais ainda, essa função é linear dado que passa no nosso teste visual segundo o qual qualquer linha de pontos igualmente espaçados deve se manter assim quando for",
  "n_reviews": 0,
  "start": 479.66,
  "end": 488.96
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "Só para ficar claro, embora eu tenha incorporado a reta numérica no espaço 2D assim, as saídas da função são números, não vetores 2D.",
  "model": "google_nmt",
  "from_community_srt": "para a reta numérica. Só pra esclarecer, ainda que eu tenha mergulhado a reta numérica no espaço 2D assim, A saída da função é composta por números,",
  "n_reviews": 0,
  "start": 491.64,
  "end": 499.28
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "Você deve pensar em uma função que recebe duas coordenadas e gera uma única coordenada.",
  "model": "google_nmt",
  "from_community_srt": "não vetores 2D. Você deveria pensar numa função que pega coordenadas e cospe uma coordenada simples.",
  "n_reviews": 0,
  "start": 499.96,
  "end": 503.68
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "Mas esse vetor u-hat é um vetor bidimensional, que vive no espaço de entrada.",
  "model": "google_nmt",
  "from_community_srt": "Mas aquele vetor û é um vetor bidimensional, vivendo no espaço de entrada.",
  "n_reviews": 0,
  "start": 505.06,
  "end": 509.02
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "Ele está situado de tal forma que se sobrepõe à incorporação da reta numérica.",
  "model": "google_nmt",
  "from_community_srt": "Ele apenas está situado de forma que se sobrepõe com a reta numérica imersa.",
  "n_reviews": 0,
  "start": 509.44,
  "end": 513.22
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "Com esta projeção, acabamos de definir uma transformação linear de vetores 2d em números, então seremos capazes de encontrar algum tipo de matriz 1x2 que descreva essa transformação.",
  "model": "google_nmt",
  "from_community_srt": "Com esta projeção, acabamos de definir uma transformação linear que leva de vetores 2D em números, enão vamos ser capazes de encontrar algum tipo de matriz 1 x 2 que descreve esta transformação.",
  "n_reviews": 0,
  "start": 514.6,
  "end": 524.6
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "Para encontrar essa matriz 1x2, vamos ampliar essa configuração de reta numérica diagonal e pensar sobre onde i-hat e j-hat cada um pousa, já que esses pontos de aterrissagem serão as colunas da matriz.",
  "model": "google_nmt",
  "from_community_srt": "Para encontrar essa matriz, vamos olhar nosso esquema da reta diagonal e pensar em onde î e ĵ vão parar, dado que esses pontos de aterrissagem serão as colunas da matriz.",
  "n_reviews": 0,
  "start": 525.54,
  "end": 536.46
 },
 {
  "input": "This part's super cool.",
  "translatedText": "Essa parte é super legal.",
  "model": "google_nmt",
  "from_community_srt": "Esta parte é super legal,",
  "n_reviews": 0,
  "start": 538.48,
  "end": 539.44
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "Podemos raciocinar sobre isso com uma simetria realmente elegante.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.7,
  "end": 542.42
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "Como i-hat e u-hat são vetores unitários, projetar i-hat na linha que passa por u-hat parece totalmente simétrico a projetar u-hat no eixo x.",
  "model": "google_nmt",
  "from_community_srt": "podemos pensar sobre isso com um pouco de simetria bem elegante: uma vez que î e ĵ são ambos vetores unitários, projetar î na linha passando por û é totalmente simétrico a projetar û no eixo x.",
  "n_reviews": 0,
  "start": 543.02,
  "end": 553.16
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "Então, quando perguntamos em que número o i-hat pousa quando é projetado, a resposta será a mesma que qualquer número em que o u-hat pousa quando é projetado no eixo x.",
  "model": "google_nmt",
  "from_community_srt": "Então, quando formos perguntados em que número o î vai parar quando projetado, a resposta será igual a onde û vai parar quando projetado",
  "n_reviews": 0,
  "start": 553.84,
  "end": 562.32
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "Mas projetar o chapéu no eixo x significa apenas pegar a coordenada x do chapéu.",
  "model": "google_nmt",
  "from_community_srt": "no eixo x. Mas projetar û no eixo x é o mesmo que tomar a coordenada x de û.",
  "n_reviews": 0,
  "start": 562.92,
  "end": 568.6
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "Então, por simetria, o número onde i-hat cai quando é projetado naquela reta numérica diagonal será a coordenada x de u-hat.",
  "model": "google_nmt",
  "from_community_srt": "Então, por simetria, o número onde î vai parar quando projetado na linha diagonal vai ser a coordenada x de û.",
  "n_reviews": 0,
  "start": 569.02,
  "end": 576.62
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "Não é legal?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.16,
  "end": 577.66
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "O raciocínio é quase idêntico para o caso j-hat.",
  "model": "google_nmt",
  "from_community_srt": "Não é legal? O raciocínio é quase idêntico para o caso do ĵ.",
  "n_reviews": 0,
  "start": 579.2,
  "end": 581.8
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "Pense nisso por um momento.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 582.18,
  "end": 583.26
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "Pelas mesmas razões, a coordenada y de u-hat nos dá o número onde j-hat pousa quando é projetado na cópia da reta numérica.",
  "model": "google_nmt",
  "from_community_srt": "Pense sobre ele um pouco. Pelas mesmas razões, a coordenada y de û nos dá o número onde ĵ vai parar quando projetado na cópia da linha numérica.",
  "n_reviews": 0,
  "start": 589.12,
  "end": 596.6
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "Faça uma pausa e pondere sobre isso por um momento.",
  "model": "google_nmt",
  "from_community_srt": "Pare e pense um pouco sobre isso;",
  "n_reviews": 0,
  "start": 597.58,
  "end": 598.72
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "Eu simplesmente acho isso muito legal.",
  "model": "google_nmt",
  "from_community_srt": "eu simplesmente acho isso muito legal.",
  "n_reviews": 0,
  "start": 598.78,
  "end": 600.2
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "Portanto, as entradas da matriz 1x2 que descrevem a transformação da projeção serão as coordenadas de u-hat.",
  "model": "google_nmt",
  "from_community_srt": "Então, as entradas da matriz 1 x 2 descrevendo a transformação de projeção vão ser as coordenadas de û.",
  "n_reviews": 0,
  "start": 600.92,
  "end": 607.26
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "E calcular esta transformação de projeção para vetores arbitrários no espaço, que requer a multiplicação dessa matriz por esses vetores, é computacionalmente idêntico a calcular um produto escalar com u-hat.",
  "model": "google_nmt",
  "from_community_srt": "Computar essa projeção para vetores arbitrários no espaço, que requer multiplicar aquela matriz por aqueles vetores, é computacionalmente idêntico a  tomar um produto escalar com û.",
  "n_reviews": 0,
  "start": 608.04,
  "end": 618.88
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "É por isso que tomar o produto escalar com um vetor unitário pode ser interpretado como projetar um vetor na extensão desse vetor unitário e calcular o comprimento.",
  "model": "google_nmt",
  "from_community_srt": "É por isso que fazer o produto escalar com um vetor unitário pode ser interpretado como projetar um vetor na reta gerada por aquele vetor unitário e",
  "n_reviews": 0,
  "start": 621.46,
  "end": 630.59
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "Então, e os vetores não unitários?",
  "model": "google_nmt",
  "from_community_srt": "tomar o comprimento.",
  "n_reviews": 0,
  "start": 634.03,
  "end": 635.79
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "Por exemplo, digamos que pegamos aquele vetor unitário u-hat, mas o aumentamos por um fator de 3.",
  "model": "google_nmt",
  "from_community_srt": "E sobre vetores não unitários? Por exemplo, digamos que tomamos aquele vetor unitário û, mas o \"escalamos\" por um fator de 3.",
  "n_reviews": 0,
  "start": 636.31,
  "end": 640.63
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "Numericamente, cada um dos seus componentes é multiplicado por 3.",
  "model": "google_nmt",
  "from_community_srt": "Numericamente,",
  "n_reviews": 0,
  "start": 641.35,
  "end": 644.39
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "Portanto, olhando para a matriz associada a esse vetor, leva i-hat e j-hat a três vezes os valores onde pousaram antes.",
  "model": "google_nmt",
  "from_community_srt": "cada coordenada é multiplicada por 3, então olhando a matriz associada com aquele vetor, ela leva î e ĵ aos valores iguais a 3 vezes os valores antigos.",
  "n_reviews": 0,
  "start": 644.81,
  "end": 652.39
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "Como tudo isso é linear, implica de forma mais geral que a nova matriz pode ser interpretada como a projeção de qualquer vetor na cópia da reta numérica e a multiplicação de onde ele parar por 3.",
  "model": "google_nmt",
  "from_community_srt": "Dado que tudo isso é linear, implica que, mais geralmente, a nova matriz pode ser interpretada como a que projeta qualquer vetor na cópia da reta numérica, e multipica a projeção por 3.",
  "n_reviews": 0,
  "start": 655.23,
  "end": 664.65
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "É por isso que o produto escalar com um vetor não unitário pode ser interpretado como primeiro projetando-se nesse vetor e depois aumentando o comprimento dessa projeção pelo comprimento do vetor.",
  "model": "google_nmt",
  "from_community_srt": "É por isso que o produto interno com um vetor não unitário pode ser interpretado como primeiro projetar no vetor, depois multiplicar o comprimento da projeção pelo comprimento do vetor.",
  "n_reviews": 0,
  "start": 665.47,
  "end": 674.95
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "Pare um momento para pensar sobre o que aconteceu aqui.",
  "model": "google_nmt",
  "from_community_srt": "Pare um pouco para pensar no que aconteceu aqui.",
  "n_reviews": 0,
  "start": 677.59,
  "end": 679.55
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "Tivemos uma transformação linear do espaço 2D para a reta numérica, que não foi definida em termos de vetores numéricos ou produtos escalares numéricos, foi apenas definida pela projeção do espaço em uma cópia diagonal da reta numérica.",
  "model": "google_nmt",
  "from_community_srt": "Tínhamos uma transformação linear do espaço 2D para a reta numérica, a qual não foi definida em termos de vetores ou produtos escalares numéricos. Ela foi definida apenas pela projeção do espaço em uma cópia diagonal da reta numérica.",
  "n_reviews": 0,
  "start": 679.89,
  "end": 690.89
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "Mas como a transformação é linear, foi necessariamente descrita por alguma matriz 1x2.",
  "model": "google_nmt",
  "from_community_srt": "Mas, dado que a transformação é linear, seria necessariamente descrita por alguma matriz 1 x 2,",
  "n_reviews": 0,
  "start": 691.67,
  "end": 696.83
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "E como multiplicar uma matriz 1x2 por um vetor 2D é o mesmo que virar essa matriz de lado e obter um produto escalar, essa transformação estava inevitavelmente relacionada a algum vetor 2D.",
  "model": "google_nmt",
  "from_community_srt": "e dado que multiplicar uma matriz 1 x 2 por um vetor 2D é o mesmo que virar a matriz de lado e tomar o produto interno, essa transformação era, inescapavelmente,",
  "n_reviews": 0,
  "start": 697.33,
  "end": 707.91
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "A lição aqui é que sempre que você tiver uma dessas transformações lineares cujo espaço de saída é a reta numérica, não importa como ela foi definida, haverá algum vetor único v correspondente a essa transformação, no sentido de que aplicar a transformação é a mesma coisa que pegar um produto escalar com esse vetor.",
  "model": "google_nmt",
  "from_community_srt": "relacionada a algum vetor 2D. A lição aqui é que, a qualquer momento que você tiver uma dessas transformações lineares cujo espaço de saída é a reta numérica, não importa como foi definida, vai haver um vetor único 'v', que se corresponde à aquela transformação, no sentido de que aplicar a transformação é o mesmo que tomar o produto escalar com aquele vetor.",
  "n_reviews": 0,
  "start": 709.41,
  "end": 726.35
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "Para mim, isso é absolutamente lindo.",
  "model": "google_nmt",
  "from_community_srt": "Para mim, isso é absolutamente lindo.",
  "n_reviews": 0,
  "start": 729.93,
  "end": 732.03
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "É um exemplo de algo em matemática chamado dualidade.",
  "model": "google_nmt",
  "from_community_srt": "É um exemplo de algo em matemática chamado \"dualidade\".",
  "n_reviews": 0,
  "start": 732.73,
  "end": 735.39
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "A dualidade aparece de muitas maneiras e formas diferentes na matemática e é muito difícil de definir.",
  "model": "google_nmt",
  "from_community_srt": "\"Dualidade\" aparece em muitas formas diferentes através da matemática e é bem difícil de se definir,",
  "n_reviews": 0,
  "start": 736.27,
  "end": 741.93
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "Em termos gerais, refere-se a situações em que há uma correspondência natural, mas surpreendente, entre dois tipos de coisas matemáticas.",
  "model": "google_nmt",
  "from_community_srt": "na verdade. Vagamente, se refere a situações em que você tem uma correspondência natural mas surpreendente entre dois tipos de coisas matemáticas.",
  "n_reviews": 0,
  "start": 742.67,
  "end": 750.23
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "Para o caso de álgebra linear que você acabou de aprender, você diria que o dual de um vetor é a transformação linear que ele codifica, e o dual de uma transformação linear de algum espaço para uma dimensão é um determinado vetor nesse espaço.",
  "model": "google_nmt",
  "from_community_srt": "Para o caso de Álgebra Linear que você acabou de aprender, você diria que o \"dual\" de um vetor é a transformação linear que ele codifica. E o dual de uma transformação linear do espaço em uma dimensão é um certo valor naquele espaço.",
  "n_reviews": 0,
  "start": 751.01,
  "end": 764.65
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "Então, resumindo, superficialmente, o produto escalar é uma ferramenta geométrica muito útil para compreender projeções e para testar se os vetores tendem ou não a apontar na mesma direção.",
  "model": "google_nmt",
  "from_community_srt": "Então, para resumir, o produto interno é uma ferramenta geométrica bem útil para entender projeções e para testar se vetores estão ou não na mesma direção geral.",
  "n_reviews": 0,
  "start": 766.73,
  "end": 776.31
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "E essa é provavelmente a coisa mais importante que você deve lembrar sobre o produto escalar.",
  "model": "google_nmt",
  "from_community_srt": "E isso é provavelmente a coisa mais importante para você lembrar sobre o produto escalar, mas,",
  "n_reviews": 0,
  "start": 776.97,
  "end": 780.79
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "Mas, num nível mais profundo, juntar dois vetores é uma forma de traduzir um deles para o mundo das transformações.",
  "model": "google_nmt",
  "from_community_srt": "num nível mais profundo, o produto escalar de dois vetores é uma forma de traduzir um deles no mundo de transformações:",
  "n_reviews": 0,
  "start": 781.27,
  "end": 787.73
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "Novamente, numericamente, isso pode parecer um ponto bobo de enfatizar.",
  "model": "google_nmt",
  "from_community_srt": "mais uma vez, numericamente,",
  "n_reviews": 0,
  "start": 788.67,
  "end": 791.55
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "São apenas dois cálculos que parecem semelhantes.",
  "model": "google_nmt",
  "from_community_srt": "isso pode parecer bobo de se enfatizar, são só dois cálculos bem parecidos por acaso,",
  "n_reviews": 0,
  "start": 791.67,
  "end": 794.49
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "Mas a razão pela qual considero isso tão importante é que em toda a matemática, quando você lida com um vetor, quando você realmente conhece sua personalidade, às vezes você percebe que é mais fácil entendê-lo não como uma flecha no espaço, mas como o concretização física de uma transformação linear.",
  "model": "google_nmt",
  "from_community_srt": "mas a razão de achar isso tão importante é que em matemática, quando você lida com um vetor, uma vez que você conhece a sua personalidade, às vezes você percebe que é mais fácil entendê-lo não como uma flecha no espaço, mas como a materialização física de uma transformação linear.",
  "n_reviews": 0,
  "start": 794.49,
  "end": 810.09
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "É como se o vetor fosse apenas uma abreviatura conceitual para uma determinada transformação, já que é mais fácil pensarmos em setas no espaço em vez de mover todo esse espaço para a reta numérica.",
  "model": "google_nmt",
  "from_community_srt": "É como se o vetor fosse só uma abreviação para uma dada transformação, pois é mais fácil pra gente pensar em flechas no espaço do que mover todo o espaço para a reta numérica.",
  "n_reviews": 0,
  "start": 810.73,
  "end": 820.97
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "No próximo vídeo, você verá outro exemplo muito legal dessa dualidade em ação, enquanto falo sobre o produto vetorial.",
  "model": "google_nmt",
  "from_community_srt": "No próximo vídeo,",
  "n_reviews": 0,
  "start": 822.61,
  "end": 829.19
 }
]