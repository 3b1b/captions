[
 {
  "input": "[\"Ode to Joy\", by Beethoven, plays to the end of the piano.] Traditionally, dot products are something that's introduced really early on in a linear algebra course, typically right at the start.",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So it might seem strange that I've pushed them back this far in the series.",
  "translatedText": "Proto se může zdát divné, že jsem je v sérii posunul tak daleko.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "I did this because there's a standard way to introduce the topic, which requires nothing more than a basic understanding of vectors, but a fuller understanding of the role that dot products play in math can only really be found under the light of linear transformations.",
  "translatedText": "Udělal jsem to proto, že existuje standardní způsob, jak toto téma představit, který nevyžaduje nic víc než základní znalosti vektorů, ale plnější pochopení úlohy, kterou v matematice hrají bodové součinové součinů, lze skutečně nalézt pouze ve světle lineárních transformací.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Before that, though, let me just briefly cover the standard way that dot products are introduced, which I'm assuming is at least partially review for a number of viewers.",
  "translatedText": "Ještě předtím mi však dovolte, abych se krátce zmínil o standardním způsobu zavedení bodových součinů, o kterém předpokládám, že je pro řadu diváků alespoň částečně přehledný.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Numerically, if you have two vectors of the same dimension, two lists of numbers with the same lengths, taking their dot product means pairing up all of the coordinates, multiplying those pairs together, and adding the result.",
  "translatedText": "Pokud máme dva vektory stejného rozměru, dva seznamy čísel se stejnou délkou, jejich bodový součin znamená, že všechny souřadnice spárujeme, tyto dvojice vynásobíme a výsledek sečteme.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
  "translatedText": "Takže vektor 1, 2 s tečkou 3, 4 by byl 1 krát 3 plus 2 krát 4.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be 6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
  "translatedText": "Vektor 6, 2, 8, 3 s tečkami 1, 8, 5, 3 by byl 6 krát 1 plus 2 krát 8 plus 8 krát 5 plus 3 krát 3.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Luckily, this computation has a really nice geometric interpretation.",
  "translatedText": "Naštěstí má tento výpočet opravdu pěknou geometrickou interpretaci.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "To think about the dot product between two vectors, v and w, imagine projecting w onto the line that passes through the origin and the tip of v.",
  "translatedText": "Chcete-li uvažovat o tečkovém součinu dvou vektorů v a w, představte si, že promítnete w na přímku, která prochází počátkem a vrcholem v.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Multiplying the length of this projection by the length of v, you have the dot product v dot w.",
  "translatedText": "Vynásobením délky tohoto průmětu délkou v získáme tečkový součin v tečka w.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Except when this projection of w is pointing in the opposite direction from v, that dot product will actually be negative.",
  "translatedText": "Až na to, že pokud tato projekce w směřuje opačným směrem než v, bude tento bodový součin ve skutečnosti záporný.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So when two vectors are generally pointing in the same direction, their dot product is positive.",
  "translatedText": "Pokud tedy dva vektory směřují obecně stejným směrem, je jejich tečkový součin kladný.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "When they're perpendicular, meaning the projection of one onto the other is the zero vector, their dot product is zero.",
  "translatedText": "Pokud jsou kolmé, což znamená, že průmětem jednoho z nich do druhého je nulový vektor, je jejich tečkový součin roven nule.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "And if they point in generally the opposite direction, their dot product is negative.",
  "translatedText": "A pokud směřují obecně opačným směrem, je jejich bodový součin záporný.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Now, this interpretation is weirdly asymmetric.",
  "translatedText": "Tento výklad je podivně asymetrický.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "It treats the two vectors very differently.",
  "translatedText": "K oběma vektorům přistupuje velmi odlišně.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So when I first learned this, I was surprised that order doesn't matter.",
  "translatedText": "Když jsem se to dozvěděl poprvé, překvapilo mě, že na pořadí nezáleží.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "You could instead project v onto w, multiply the length of the projected v by the length of w, and get the same result.",
  "translatedText": "Místo toho můžete promítnout v na w, vynásobit délku promítnutého v délkou w a získat stejný výsledek.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "I mean, doesn't that feel like a really different process?",
  "translatedText": "Nepřipadá vám to jako úplně jiný proces?",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Here's the intuition for why order doesn't matter.",
  "translatedText": "Zde je intuice, proč na pořadí nezáleží.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "If v and w happened to have the same length, we could leverage some symmetry.",
  "translatedText": "Pokud by v a w měly stejnou délku, mohli bychom využít symetrie.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Since projecting w onto v, then multiplying the length of that projection by the length of v, is a complete mirror image of projecting v onto w, then multiplying the length of that projection by the length of w.",
  "translatedText": "Protože promítnutí w do v a následné vynásobení délky tohoto promítnutí délkou v je úplným zrcadlovým obrazem promítnutí v do w a následného vynásobení délky tohoto promítnutí délkou w.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Now, if you scale one of them, say v, by some constant like 2, so that they don't have equal length, the symmetry is broken.",
  "translatedText": "Pokud jednu z nich, řekněme v, zvětšíte o nějakou konstantu, například 2, takže nebudou mít stejnou délku, symetrie se naruší.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But let's think through how to interpret the dot product between this new vector, 2 times v, and w.",
  "translatedText": "Promysleme si však, jak interpretovat tečkový součin mezi tímto novým vektorem, 2 krát v, a w.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "If you think of w as getting projected onto v, then the dot product 2v dot w will be exactly twice the dot product v dot w.",
  "translatedText": "Pokud si představíte, že se w promítá na v, pak bodový součin 2v dot w bude přesně dvojnásobkem bodového součinu v dot w.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This is because when you scale v by 2, it doesn't change the length of the projection of w, but it doubles the length of the vector that you're projecting onto.",
  "translatedText": "Je to proto, že když měříte v o 2, nezmění se délka projekce w, ale zdvojnásobí se délka vektoru, do kterého se promítá.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But on the other hand, let's say you were thinking about v getting projected onto w.",
  "translatedText": "Ale na druhou stranu, řekněme, že jste uvažovali o tom, že se v promítne do w.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Well, in that case, the length of the projection is the thing that gets scaled when we multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
  "translatedText": "V tomto případě se délka projekce zmenší, když v vynásobíme 2, ale délka vektoru, na který promítáme, zůstává konstantní.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So the overall effect is still to just double the dot product.",
  "translatedText": "Celkový efekt je tedy stále jen zdvojnásobení bodového součinu.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So even though symmetry is broken in this case, the effect that this scaling has on the value of the dot product is the same under both interpretations.",
  "translatedText": "Přestože je tedy v tomto případě symetrie porušena, vliv tohoto škálování na hodnotu bodového součinu je při obou interpretacích stejný.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "There's also one other big question that confused me when I first learned this stuff.",
  "translatedText": "Je tu ještě jedna velká otázka, která mě zmátla, když jsem se o tom učil poprvé.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Why on earth does this numerical process of matching coordinates, multiplying pairs, and adding them together have anything to do with projection?",
  "translatedText": "Proč má tento numerický proces porovnávání souřadnic, násobení dvojic a jejich sčítání proboha něco společného s promítáním?",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Well, to give a satisfactory answer, and also to do full justice to the significance of the dot product, we need to unearth something a little bit deeper going on here, which often goes by the name duality.",
  "translatedText": "Abychom mohli uspokojivě odpovědět a také plně pochopit význam bodového součinu, musíme odhalit něco trochu hlubšího, co se zde děje a co se často nazývá dualita.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But before getting into that, I need to spend some time talking about linear transformations from multiple dimensions to one dimension, which is just the number line.",
  "translatedText": "Než se k tomu ale dostaneme, musím se chvíli věnovat lineárním transformacím z více rozměrů do jednoho rozměru, kterým je právě číselná přímka.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "These are functions that take in a 2D vector and spit out some number, but linear transformations are of course much more restricted than your run-of-the-mill function with a 2D input and a 1D output.",
  "translatedText": "Jedná se o funkce, které přijmou 2D vektor a vyplivnou nějaké číslo, ale lineární transformace jsou samozřejmě mnohem omezenější než běžné funkce s 2D vstupem a 1D výstupem.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, there are some formal properties that make these functions linear, but I'm going to purposefully ignore those here so as to not distract from our end goal, and instead focus on a certain visual property that's equivalent to all the formal stuff.",
  "translatedText": "Stejně jako u transformací ve vyšších dimenzích, o kterých jsem mluvil v kapitole 3, existují určité formální vlastnosti, které činí tyto funkce lineárními, ale ty zde budu záměrně ignorovat, abych neodváděl pozornost od našeho konečného cíle, a místo toho se zaměřím na určitou vizuální vlastnost, která je ekvivalentní všem formálním věcem.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "If you take a line of evenly spaced dots and apply a transformation, a linear transformation will keep those dots evenly spaced once they land in the output space, which is the number line.",
  "translatedText": "Pokud vezmete řadu rovnoměrně rozmístěných bodů a použijete transformaci, lineární transformace zachová tyto body rovnoměrně rozmístěné, jakmile se dostanou do výstupního prostoru, kterým je číselná řada.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Otherwise, if there's some line of dots that gets unevenly spaced, then your transformation is not linear.",
  "translatedText": "V opačném případě, pokud je nějaká řada bodů nerovnoměrně rozmístěna, není transformace lineární.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "As with the cases we've seen before, one of these linear transformations is completely determined by where it takes i-hat and j-hat, but this time each one of those basis vectors just lands on a number, so when we record where they land as the columns of a matrix, each of those columns just has a single number.",
  "translatedText": "Stejně jako v předchozích případech je jedna z těchto lineárních transformací zcela určena tím, kde se nachází i-hat a j-hat, ale tentokrát každý z těchto bázových vektorů prostě dopadá na číslo, takže když zaznamenáme, kde dopadají jako sloupce matice, každý z těchto sloupců má jen jedno číslo.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This is a 1x2 matrix.",
  "translatedText": "Jedná se o matici 1x2.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Let's walk through an example of what it means to apply one of these transformations to a vector.",
  "translatedText": "Ukážeme si na příkladu, co znamená použít jednu z těchto transformací na vektor.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
  "translatedText": "Řekněme, že máte lineární transformaci, která přenese i-hat na hodnotu 1 a j-hat na zápornou hodnotu 2.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "To follow where a vector with coordinates, say, 4, 3 ends up, think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
  "translatedText": "Chcete-li sledovat, kde skončí vektor se souřadnicemi například 4, 3, představte si, že tento vektor rozdělíte jako 4 krát i-hat plus 3 krát j-hat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "A consequence of linearity is that after the transformation, the vector will be 4 times the place where i-hat lands, 1, plus 3 times the place where j-hat lands, negative 2, which in this case implies that it lands on negative 2.",
  "translatedText": "Důsledkem linearity je, že po transformaci bude mít vektor 4násobek místa, kde dopadne i-čepice, tedy 1, plus 3násobek místa, kde dopadne j-čepice, tedy záporné 2, což v tomto případě znamená, že dopadne na záporné 2.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "When you do this calculation purely numerically, it's matrix vector multiplication.",
  "translatedText": "Pokud tento výpočet provedete čistě numericky, jedná se o maticové násobení vektorů.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Now, this numerical operation of multiplying a 1x2 matrix by a vector feels just like taking the dot product of two vectors.",
  "translatedText": "Tato numerická operace násobení matice 1x2 vektorem se nyní podobá tečkovému součinu dvou vektorů.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
  "translatedText": "Nevypadá ta matice 1x2 jako vektor, který jsme převrátili na bok?",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "In fact, we could say right now that there's a nice association between 1x2 matrices and 2D vectors, defined by tilting the numerical representation of a vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.",
  "translatedText": "Ve skutečnosti bychom mohli hned teď říci, že existuje pěkná asociace mezi maticemi 1x2 a 2D vektory, definovaná nakloněním číselné reprezentace vektoru na jeho stranu, abychom získali přidruženou matici, nebo nakloněním matice zpět nahoru, abychom získali přidružený vektor.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Since we're just looking at numerical expressions right now, going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
  "translatedText": "Vzhledem k tomu, že se nyní zabýváme pouze číselnými výrazy, může se nám zdát, že přecházet mezi vektory a maticemi 1x2 je hloupost.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But this suggests something that's truly awesome from the geometric view.",
  "translatedText": "To však naznačuje něco, co je z geometrického hlediska opravdu úžasné.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "There's some kind of connection between linear transformations that take vectors to numbers and vectors themselves.",
  "translatedText": "Mezi lineárními transformacemi, které převádějí vektory na čísla, a vektory samotnými existuje určitá souvislost.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Let me show an example that clarifies the significance, and which just so happens to also answer the dot product puzzle from earlier.",
  "translatedText": "Dovolte mi ukázat příklad, který objasňuje význam a který shodou okolností odpovídá i na hádanku o bodovém součinu z dřívějška.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Unlearn what you have learned, and imagine that you don't already know that the dot product relates to projection.",
  "translatedText": "Odnaučte se, co jste se naučili, a představte si, že ještě nevíte, že bodový součin souvisí s promítáním.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "What I'm going to do here is take a copy of the number line and place it diagonally in space somehow, with the number 0 sitting at the origin.",
  "translatedText": "Udělám to tak, že vezmu kopii číselné přímky a umístím ji nějak šikmo do prostoru, přičemž číslo 0 bude v počátku.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Now think of the two-dimensional unit vector whose tip sits where the number 1 on the number is.",
  "translatedText": "Nyní si představte dvourozměrný jednotkový vektor, jehož vrchol se nachází v místě, kde je na čísle 1.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "I want to give that guy a name, u-hat.",
  "translatedText": "Chtěl bych tomu chlápkovi dát jméno, u-hat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This little guy plays an important role in what's about to happen, so just keep him in the back of your mind.",
  "translatedText": "Tenhle človíček hraje důležitou roli v tom, co se bude dít, takže ho mějte na paměti.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "If we project 2d vectors straight onto this diagonal number line, in effect, we've just defined a function that takes 2d vectors to numbers.",
  "translatedText": "Promítneme-li 2d vektory přímo na tuto diagonální číselnou přímku, v podstatě jsme právě definovali funkci, která přenáší 2d vektory na čísla.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "What's more, this function is actually linear, since it passes our visual test that any line of evenly spaced dots remains evenly spaced once it lands on the number line.",
  "translatedText": "Navíc je tato funkce skutečně lineární, protože projde naším vizuálním testem, podle kterého zůstane jakákoli řada rovnoměrně rozmístěných bodů rovnoměrně rozmístěná, jakmile se ocitne na číselné řadě.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Just to be clear, even though I've embedded the number line in 2d space like this, the outputs of the function are numbers, not 2d vectors.",
  "translatedText": "Aby bylo jasno, i když jsem takto vložil číselnou řadu do 2d prostoru, výstupy funkce jsou čísla, nikoli 2d vektory.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "You should think of a function that takes in two coordinates and outputs a single coordinate.",
  "translatedText": "Měli byste si představit funkci, která přijme dvě souřadnice a vypíše jednu souřadnici.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But that vector u-hat is a two-dimensional vector, living in the input space.",
  "translatedText": "Vektor u-hat je však dvourozměrný vektor, který se nachází ve vstupním prostoru.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "It's just situated in such a way that overlaps with the embedding of the number line.",
  "translatedText": "Jen je umístěn tak, že se překrývá s vložením číselné řady.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "With this projection, we just defined a linear transformation from 2d vectors to numbers, so we're going to be able to find some kind of 1x2 matrix that describes that transformation.",
  "translatedText": "Pomocí této projekce jsme právě definovali lineární transformaci z 2d vektorů na čísla, takže budeme schopni najít nějakou matici 1x2, která tuto transformaci popisuje.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "To find that 1x2 matrix, let's zoom in on this diagonal number line setup and think about where i-hat and j-hat each land, since those landing spots are going to be the columns of the matrix.",
  "translatedText": "Abychom zjistili matici 1x2, přiblížíme si tuto diagonální číselnou řadu a zamyslíme se nad tím, kde přistane i-hat a j-hat, protože tato místa přistání budou sloupci matice.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This part's super cool.",
  "translatedText": "Tahle část je super.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "We can reason through it with a really elegant piece of symmetry.",
  "translatedText": "Můžeme to zdůvodnit opravdu elegantní symetrií.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
  "translatedText": "Protože i-hat a u-hat jsou oba jednotkové vektory, promítnutí i-hat na přímku procházející u-hat vypadá zcela symetricky k promítnutí u-hat na osu x.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So when we ask what number does i-hat land on when it gets projected, the answer is going to be the same as whatever u-hat lands on when it's projected onto the x-axis.",
  "translatedText": "Když se tedy zeptáme, na jakém čísle přistane i-klobouk, když se promítne, odpověď bude stejná jako u-klobouk, když se promítne na osu x.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
  "translatedText": "Promítnutí u-hat na osu x však znamená pouze vzít x-ovou souřadnici u-hat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So by symmetry, the number where i-hat lands when it's projected onto that diagonal number line is going to be the x-coordinate of u-hat.",
  "translatedText": "Takže podle symetrie bude číslo, na které se i-čepice promítne na diagonální číselnou přímku, x-ová souřadnice u-čepice.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Isn't that cool?",
  "translatedText": "Není to skvělé?",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The reasoning is almost identical for the j-hat case.",
  "translatedText": "Argumentace je téměř totožná pro případ j-hat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Think about it for a moment.",
  "translatedText": "Chvíli o tom přemýšlejte.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "For all the same reasons, the y-coordinate of u-hat gives us the number where j-hat lands when it's projected onto the number line copy.",
  "translatedText": "Ze stejných důvodů nám y-ová souřadnice u-čepice udává číslo, kam se j-čepice promítne na kopii číselné přímky.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Pause and ponder that for a moment.",
  "translatedText": "Na chvíli se zastavte a zamyslete se nad tím.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "I just think that's really cool.",
  "translatedText": "Myslím, že je to opravdu skvělé.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So the entries of the 1x2 matrix describing the projection transformation are going to be the coordinates of u-hat.",
  "translatedText": "Takže položky matice 1x2 popisující projekční transformaci budou souřadnice u-čepice.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat.",
  "translatedText": "A výpočet této projekční transformace pro libovolné vektory v prostoru, který vyžaduje vynásobení této matice těmito vektory, je výpočetně totožný s tečkovým součinem s u-hat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This is why taking the dot product with a unit vector can be interpreted as projecting a vector onto the span of that unit vector and taking the length.",
  "translatedText": "Proto lze tečkový součin s jednotkovým vektorem interpretovat jako promítnutí vektoru do rozpětí tohoto jednotkového vektoru a odečtení jeho délky.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So what about non-unit vectors?",
  "translatedText": "Jak je to tedy s nejednotkovými vektory?",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "For example, let's say we take that unit vector u-hat, but we scale it up by a factor of 3.",
  "translatedText": "Řekněme například, že vezmeme jednotkový vektor u-hat, ale zvětšíme ho o trojnásobek.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Numerically, each of its components gets multiplied by 3.",
  "translatedText": "Číselně se každá jeho složka vynásobí třemi.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So looking at the matrix associated with that vector, it takes i-hat and j-hat to three times the values where they landed before.",
  "translatedText": "Když se podíváme na matici spojenou s tímto vektorem, zjistíme, že i-hat a j-hat nabývají třikrát vyšších hodnot než předtím.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Since this is all linear, it implies more generally that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3.",
  "translatedText": "Protože je to všechno lineární, znamená to obecněji, že novou matici lze interpretovat jako promítnutí libovolného vektoru na kopii číselné přímky a vynásobení místa, kam dopadne, číslem 3.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector, then scaling up the length of that projection by the length of the vector.",
  "translatedText": "Proto lze tečkový součin s nejednotkovým vektorem interpretovat tak, že se nejprve promítne do tohoto vektoru a poté se délka tohoto promítnutí zvětší o délku vektoru.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Take a moment to think about what happened here.",
  "translatedText": "Chvíli přemýšlejte o tom, co se zde stalo.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "We had a linear transformation from 2D space to the number line, which was not defined in terms of numerical vectors or numerical dot products, it was just defined by projecting space onto a diagonal copy of the number line.",
  "translatedText": "Měli jsme lineární transformaci z 2D prostoru na číselnou přímku, která nebyla definována v termínech číselných vektorů nebo číselných bodových součinů, ale byla definována pouze promítnutím prostoru na diagonální kopii číselné přímky.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
  "translatedText": "Protože je však transformace lineární, byla nutně popsána nějakou maticí 1x2.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "And since multiplying a 1x2 matrix by a 2D vector is the same as turning that matrix on its side and taking a dot product, this transformation was inescapably related to some 2D vector.",
  "translatedText": "A protože násobení matice 1x2 2D vektorem je totéž jako otočení této matice na bok a provedení tečkového součinu, byla tato transformace nevyhnutelně spojena s nějakým 2D vektorem.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The lesson here is that any time you have one of these linear transformations whose output space is the number line, no matter how it was defined, there's going to be some unique vector v corresponding to that transformation, in the sense that applying the transformation is the same thing as taking a dot product with that vector.",
  "translatedText": "Z toho plyne ponaučení, že kdykoli máte jednu z těchto lineárních transformací, jejímž výstupním prostorem je číselná přímka, bez ohledu na to, jak byla definována, bude existovat nějaký jedinečný vektor v odpovídající této transformaci v tom smyslu, že použití transformace je totéž, jako když s tímto vektorem provedete tečkový součin.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "To me, this is utterly beautiful.",
  "translatedText": "Pro mě je to naprosto nádherné.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "It's an example of something in math called duality.",
  "translatedText": "Je to příklad něčeho, čemu se v matematice říká dualita.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Duality shows up in many different ways and forms throughout math, and it's super tricky to actually define.",
  "translatedText": "Dualita se v matematice projevuje mnoha různými způsoby a formami a je velmi složité ji definovat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Loosely speaking, it refers to situations where you have a natural but surprising correspondence between two types of mathematical thing.",
  "translatedText": "Volně řečeno se týká situací, kdy existuje přirozená, ale překvapivá shoda mezi dvěma typy matematických věcí.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "For the linear algebra case that you just learned about, you'd say that the dual of a vector is the linear transformation that it encodes, and the dual of a linear transformation from some space to one dimension is a certain vector in that space.",
  "translatedText": "V případě lineární algebry, o které jste se právě učili, byste řekli, že duál vektoru je lineární transformace, kterou kóduje, a duál lineární transformace z nějakého prostoru do jedné dimenze je určitý vektor v tomto prostoru.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "So to sum up, on the surface, the dot product is a very useful geometric tool for understanding projections and for testing whether or not vectors tend to point in the same direction.",
  "translatedText": "Na první pohled je tedy bodový součin velmi užitečným geometrickým nástrojem pro pochopení promítání a pro testování, zda vektory mají tendenci směřovat stejným směrem.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "And that's probably the most important thing for you to remember about the dot product.",
  "translatedText": "A to je pravděpodobně to nejdůležitější, co byste si měli o tečkovém součinu zapamatovat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But at a deeper level, dotting two vectors together is a way to translate one of them into the world of transformations.",
  "translatedText": "Na hlubší úrovni je však spojení dvou vektorů tečkou způsob, jak jeden z nich převést do světa transformací.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Again, numerically, this might feel like a silly point to emphasize.",
  "translatedText": "Z číselného hlediska se může zdát, že je to hloupé zdůrazňovat.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "It's just two computations that happen to look similar.",
  "translatedText": "Jsou to jen dva výpočty, které náhodou vypadají podobně.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "But the reason I find this so important is that throughout math, when you're dealing with a vector, once you really get to know its personality, sometimes you realize that it's easier to understand it not as an arrow in space, but as the physical embodiment of a linear transformation.",
  "translatedText": "Důvod, proč to považuji za tak důležité, je ten, že v matematice, když se zabýváte vektorem, jakmile skutečně poznáte jeho osobnost, někdy si uvědomíte, že je snazší chápat ho ne jako šipku v prostoru, ale jako fyzické ztělesnění lineární transformace.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "It's as if the vector is really just a conceptual shorthand for a certain transformation, since it's easier for us to think about arrows in space rather than moving all of that space to the number line.",
  "translatedText": "Jako by vektor byl ve skutečnosti jen pojmovou zkratkou pro určitou transformaci, protože je pro nás jednodušší přemýšlet o šipkách v prostoru, než přesouvat celý tento prostor na číselnou přímku.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "In the next video, you'll see another really cool example of this duality in action, as I talk about the cross product.",
  "translatedText": "V příštím videu uvidíte další skvělý příklad této duality v akci, když budu mluvit o křížovém součinu.",
  "model": "DeepL",
  "n_reviews": 0
 }
]