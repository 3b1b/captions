[
 {
  "input": "The goal is for you to come away from this video understanding one of the most important formulas in all of probability, Bayes' theorem.",
  "translatedText": "Цель состоит в том, чтобы после просмотра этого видео вы поняли одну из самых важных формул теории вероятностей — теорему Байеса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.84
 },
 {
  "input": "This formula is central to scientific discovery, it's a core tool in machine learning and AI, and it's even been used for treasure hunting, when in the 1980s a small team led by Tommy Thompson, and I'm not making up that name, used Bayesian search tactics to help uncover a ship that had sunk a century and a half earlier, and the ship was carrying what in today's terms amounts to $700 million worth of gold.",
  "translatedText": "Эта формула занимает центральное место в научных открытиях, является основным инструментом в машинном обучении и искусственном интеллекте, и ее даже использовали для поиска сокровищ, когда в 1980-х годах небольшая группа под руководством Томми Томпсона (я не придумываю это имя) использовала Байесовская поисковая тактика, помогающая обнаружить корабль, затонувший полтора века назад и перевозивший золото на сумму, по сегодняшним меркам, 700 миллионов долларов.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 7.48,
  "end": 30.74
 },
 {
  "input": "So it's a formula worth understanding, but of course there are multiple different levels of possible understanding.",
  "translatedText": "Так что эту формулу стоит понять, но, конечно, существует множество различных уровней понимания.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.34,
  "end": 37.04
 },
 {
  "input": "At the simplest there's just knowing what each one of the parts means, so that you can plug in numbers.",
  "translatedText": "В самом простом случае нужно просто знать, что означает каждая из частей, чтобы можно было подставлять цифры.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 37.6,
  "end": 42.04
 },
 {
  "input": "Then there's understanding why it's true, and later I'm going to show you a certain diagram that's helpful for rediscovering this formula on the fly as needed.",
  "translatedText": "Тогда приходит понимание, почему это правда, и позже я собираюсь показать вам определенную диаграмму, которая поможет заново открыть эту формулу на лету, если это необходимо.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.76,
  "end": 50.58
 },
 {
  "input": "But maybe the most important level is being able to recognize when you need to use it.",
  "translatedText": "Но, возможно, самый важный уровень — это способность распознавать, когда вам нужно его использовать.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.24,
  "end": 55.54
 },
 {
  "input": "And with the goal of gaining a deeper understanding, you and I are going to tackle these in reverse order.",
  "translatedText": "И с целью достижения более глубокого понимания мы с вами собираемся заняться этим в обратном порядке.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.54,
  "end": 60.56
 },
 {
  "input": "So before dissecting the formula or explaining the visual that makes it obvious, I'd like to tell you about a man named Steve.",
  "translatedText": "Итак, прежде чем анализировать формулу или объяснять наглядность, я хотел бы рассказать вам о человеке по имени Стив.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.02,
  "end": 66.86
 },
 {
  "input": "Listen carefully now.",
  "translatedText": "Слушайте внимательно сейчас.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.32,
  "end": 68.72
 },
 {
  "input": "Steve is very shy and withdrawn, invariably helpful but with very little interest in people or the world of reality.",
  "translatedText": "Стив очень застенчив и замкнут, всегда готов помочь, но очень мало интересуется людьми и миром реальности.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.74,
  "end": 79.16
 },
 {
  "input": "A meek and tidy soul, he has a need for order and structure, and a passion for detail.",
  "translatedText": "Кроткий и аккуратный человек, он любит порядок и структуру, а также страсть к деталям.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.74,
  "end": 84.1
 },
 {
  "input": "Which of the following do you find more likely?",
  "translatedText": "Что из перечисленного вы считаете более вероятным?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 84.62,
  "end": 86.78
 },
 {
  "input": "Steve is a librarian, or Steve is a farmer?",
  "translatedText": "Стив — библиотекарь или Стив — фермер?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.2,
  "end": 90.38
 },
 {
  "input": "Some of you may recognize this as an example from a study conducted by the two psychologists Daniel Kahneman and Amos Tversky.",
  "translatedText": "Некоторые из вас могут узнать это по примеру исследования, проведенного двумя психологами Дэниелом Канеманом и Амосом Тверски.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.4,
  "end": 97.44
 },
 {
  "input": "Their work was a big deal, it won a Nobel Prize, and it's been popularized many times over in books like Kahneman's Thinking Fast and Slow, or Michael Lewis's The Undoing Project.",
  "translatedText": "Их работа имела большое значение, она получила Нобелевскую премию и много раз популяризировалась в таких книгах, как «Думай быстро и медленно» Канемана или «Проект отмены» Майкла Льюиса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.2,
  "end": 106.56
 },
 {
  "input": "What they researched was human judgments, with a frequent focus on when these judgments irrationally contradict what the laws of probability suggest they should be.",
  "translatedText": "Они исследовали человеческие суждения, часто уделяя особое внимание тому, когда эти суждения иррационально противоречат тому, какими они должны быть, согласно законам вероятности.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.42,
  "end": 115.78
 },
 {
  "input": "The example with Steve, our maybe librarian, maybe farmer, illustrates one specific type of irrationality, or maybe I should say alleged irrationality, there are people who debate the conclusion here, but more on that later on.",
  "translatedText": "Пример со Стивом, нашим, может быть, библиотекарем, а может быть, фермером, иллюстрирует один конкретный тип иррациональности, или, может быть, я должен сказать предполагаемую иррациональность, здесь есть люди, которые обсуждают этот вывод, но об этом позже.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 116.34,
  "end": 129.62
 },
 {
  "input": "According to Kahneman and Tversky, after people are given this description of Steve as a meek and tidy soul, most say he's more likely to be a librarian.",
  "translatedText": "По словам Канемана и Тверски, после того, как людям дали описание Стива как кроткого и аккуратного человека, большинство из них сказали, что он, скорее всего, библиотекарь.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.98,
  "end": 138.0
 },
 {
  "input": "After all, these traits line up better with the stereotypical view of a librarian than a farmer.",
  "translatedText": "В конце концов, эти черты лучше соответствуют стереотипному представлению о библиотекаре, чем о фермере.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 138.0,
  "end": 143.46
 },
 {
  "input": "And according to Kahneman and Tversky, this is irrational.",
  "translatedText": "А по мнению Канемана и Тверски, это иррационально.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.2,
  "end": 146.88
 },
 {
  "input": "The point is not whether people hold correct or biased views about the personalities of librarians and farmers, it's that almost nobody thinks to incorporate information about the ratio of farmers to librarians in their judgments.",
  "translatedText": "Дело не в том, придерживаются ли люди правильных или предвзятых взглядов на личности библиотекарей и фермеров, а в том, что почти никто не думает включать в свои суждения информацию о соотношении фермеров и библиотекарей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.6,
  "end": 160.24
 },
 {
  "input": "In their paper, Kahneman and Tversky said that in the US, that ratio is about 20 to 1.",
  "translatedText": "В своей статье Канеман и Тверски заявили, что в США это соотношение составляет примерно 20 к 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 160.92,
  "end": 165.18
 },
 {
  "input": "The numbers I could find today put that much higher, but let's stick with the 20 to 1 number, since it's a little easier to illustrate and proves the point as well.",
  "translatedText": "Цифры, которые я смог найти сегодня, намного выше, но давайте придерживаться числа 20 к 1, поскольку его немного легче проиллюстрировать, а также это доказывает нашу точку зрения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.58,
  "end": 173.42
 },
 {
  "input": "To be clear, anyone who is asked this question is not expected to have perfect information about the actual statistics of farmers and librarians and their personality traits.",
  "translatedText": "Чтобы внести ясность: от любого, кому задают этот вопрос, не ожидается, что он будет обладать точной информацией о фактической статистике фермеров и библиотекарей и их личностных качествах.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.28,
  "end": 183.14
 },
 {
  "input": "But the question is whether people even think to consider that ratio enough to at least make a rough estimate.",
  "translatedText": "Но вопрос в том, думают ли люди вообще считать это соотношение достаточным, чтобы хотя бы сделать приблизительную оценку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.68,
  "end": 189.22
 },
 {
  "input": "Rationality is not about knowing facts, it's about recognizing which facts are relevant.",
  "translatedText": "Рациональность заключается не в знании фактов, а в признании того, какие факты релевантны.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.04,
  "end": 194.46
 },
 {
  "input": "Now if you do think to make that estimate, there's a pretty simple way to reason about the question, which, spoiler alert, involves all of the essential reasoning behind Bayes' theorem.",
  "translatedText": "Теперь, если вы действительно думаете сделать такую оценку, есть довольно простой способ рассуждать по этому вопросу, который, предупреждаю, спойлер, включает в себя все основные рассуждения, лежащие в основе теоремы Байеса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 195.88,
  "end": 203.9
 },
 {
  "input": "You might start by picturing a representative sample of farmers and librarians, say 200 farmers and 10 librarians.",
  "translatedText": "Вы можете начать с представления репрезентативной выборки фермеров и библиотекарей, скажем, 200 фермеров и 10 библиотекарей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.66,
  "end": 211.02
 },
 {
  "input": "Then when you hear of this meek and tidy soul description, let's say that your gut instinct is that 40% of librarians would fit that description, and that 10% of farmers would.",
  "translatedText": "Затем, когда вы слышите об этом описании кроткой и аккуратной души, предположим, что вы инстинктивно подсказывает, что 40% библиотекарей соответствуют этому описанию, а 10% фермеров - под него.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.74,
  "end": 221.36
 },
 {
  "input": "If those are your estimates, it would mean that from your sample you would expect about 4 librarians to fit the description, and about 20 farmers to fit that description.",
  "translatedText": "Если это ваши оценки, это будет означать, что из вашей выборки вы ожидаете, что около 4 библиотекарей будут соответствовать этому описанию и около 20 фермеров будут соответствовать этому описанию.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 222.02,
  "end": 230.24
 },
 {
  "input": "So the probability that a random person among those who fit this description is a librarian is 4 out of 24, or 16.7%.",
  "translatedText": "Таким образом, вероятность того, что случайный человек среди тех, кто подходит под это описание, окажется библиотекарем, равна 4 из 24, или 16.7%.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.02,
  "end": 240.1
 },
 {
  "input": "So even if you think that a librarian is 4 times as likely as a farmer to fit this description, that's not enough to overcome the fact that there are way more farmers.",
  "translatedText": "Таким образом, даже если вы думаете, что библиотекарь в 4 раза чаще соответствует этому описанию, чем фермер, этого недостаточно, чтобы преодолеть тот факт, что фермеров гораздо больше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.1,
  "end": 249.02
 },
 {
  "input": "The upshot, and this is the key mantra underlying Bayes' theorem, is that new evidence does not completely determine your beliefs in a vacuum, it should update prior beliefs.",
  "translatedText": "В результате (и это ключевая мантра, лежащая в основе теоремы Байеса) новые данные не полностью определяют ваши убеждения в вакууме, они должны обновлять предыдущие убеждения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 249.72,
  "end": 259.22
 },
 {
  "input": "If this line of reasoning makes sense to you, the way that seeing evidence restricts the space of possibilities and the ratio you need to consider after that, then congratulations, you understand the heart of Bayes' theorem.",
  "translatedText": "Если эта линия рассуждений имеет для вас смысл, то, как видимые доказательства ограничивают пространство возможностей и соотношение, которое вам нужно учитывать после этого, то поздравляю, вы поняли суть теоремы Байеса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.12,
  "end": 272.36
 },
 {
  "input": "Maybe the numbers you would estimate would be a little different, but what matters is how you fit the numbers together to update your beliefs based on evidence.",
  "translatedText": "Возможно, цифры, которые вы оценили, были бы немного другими, но важно то, как вы сопоставляете цифры, чтобы обновить свои убеждения на основе фактических данных.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.36,
  "end": 280.6
 },
 {
  "input": "Understanding one example is one thing, but see if you can take a minute to generalize everything we just did and write it all down as a formula.",
  "translatedText": "Понять один пример — это одно, но посмотрите, сможете ли вы потратить минуту на то, чтобы обобщить все, что мы только что сделали, и записать все это в виде формулы.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.08,
  "end": 289.74
 },
 {
  "input": "The general situation where Bayes' theorem is relevant is when you have some hypothesis, like Steve is a librarian, and you see some new evidence, say this verbal description of Steve as a meek and tidy soul.",
  "translatedText": "Общая ситуация, когда теорема Байеса актуальна, — это когда у вас есть какая-то гипотеза, например, Стив — библиотекарь, и вы видите какие-то новые доказательства, скажем, это словесное описание Стива как кроткой и аккуратной души.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 292.32,
  "end": 303.98
 },
 {
  "input": "You want to know the probability that your hypothesis holds given that the evidence is true.",
  "translatedText": "Вы хотите знать вероятность того, что ваша гипотеза имеет место при условии, что доказательства верны.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.38,
  "end": 309.64
 },
 {
  "input": "In the standard notation, this vertical bar means given that, as in we're restricting our view only to the possibilities where the evidence holds.",
  "translatedText": "В стандартных обозначениях эта вертикальная черта означает, что мы ограничиваем наш взгляд только теми возможностями, в которых имеются доказательства.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 310.44,
  "end": 318.96
 },
 {
  "input": "Remember the first relevant number we used, the probability that the hypothesis holds before considering any of that new evidence.",
  "translatedText": "Вспомните первое подходящее число, которое мы использовали, — вероятность того, что гипотеза справедлива до рассмотрения любого из этих новых доказательств.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.22,
  "end": 327.34
 },
 {
  "input": "In our example, that was 1 out of 21, and it came from considering the ratio of librarians to farmers in the general population.",
  "translatedText": "В нашем примере это был 1 из 21, и это было получено с учетом соотношения библиотекарей и фермеров среди населения в целом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.72,
  "end": 334.64
 },
 {
  "input": "This number is known as the prior.",
  "translatedText": "Это число известно как априорное.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 335.52,
  "end": 336.98
 },
 {
  "input": "After that, we need to consider the proportion of librarians that fit this description, the probability that we would see the evidence given that the hypothesis is true.",
  "translatedText": "После этого нам нужно рассмотреть долю библиотекарей, подходящих под это описание, вероятность того, что мы увидим доказательства при условии, что гипотеза верна.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.02,
  "end": 347.3
 },
 {
  "input": "Again, when you see this vertical bar, it means we're talking about some proportion of a limited part of the total space of possibilities.",
  "translatedText": "Опять же, когда вы видите эту вертикальную полосу, это означает, что мы говорим о некоторой доле ограниченной части общего пространства возможностей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 354.84
 },
 {
  "input": "In this case, that limited part is the left side, where the hypothesis holds.",
  "translatedText": "В данном случае этой ограниченной частью является левая часть, где гипотеза справедлива.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.32,
  "end": 359.3
 },
 {
  "input": "In the context of Bayes' theorem, this value also has a special name, it's called the likelihood.",
  "translatedText": "В контексте теоремы Байеса эта величина также имеет особое название — вероятность.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 359.96,
  "end": 364.64
 },
 {
  "input": "Similarly, you need to know how much of the other side of the space includes the evidence, the probability of seeing the evidence given that the hypothesis isn't true.",
  "translatedText": "Точно так же вам необходимо знать, какая часть другой стороны пространства включает доказательства, а также вероятность увидеть доказательства, учитывая, что гипотеза неверна.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.7,
  "end": 373.56
 },
 {
  "input": "This funny little elbow symbol is commonly used in probability to mean not.",
  "translatedText": "Этот забавный маленький символ локтя обычно используется в значении «нет».",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.34,
  "end": 378.42
 },
 {
  "input": "So, with the notation in place, remember what our final answer was.",
  "translatedText": "Итак, имея обозначения, запомните, каким был наш окончательный ответ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.86,
  "end": 383.02
 },
 {
  "input": "The probability that our librarian hypothesis is true given the evidence is the total number of librarians fitting the evidence, 4, divided by the total number of people fitting the evidence, 24.",
  "translatedText": "Вероятность того, что наша библиотечная гипотеза верна при наличии доказательств, равна общему числу библиотекарей, подходящих под доказательства, 4, деленному на общее количество людей, подходящих под доказательства, 24.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 383.36,
  "end": 394.88
 },
 {
  "input": "But where did that 4 come from?",
  "translatedText": "Но откуда взялась эта цифра 4?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.76,
  "end": 397.18
 },
 {
  "input": "Well, it's the total number of people, times the prior probability of being a librarian, giving us the 10 total librarians, times the probability that one of those fits the evidence.",
  "translatedText": "Ну, это общее количество людей, умноженное на априорную вероятность быть библиотекарем, что дает нам 10 библиотекарей, умноженное на вероятность того, что один из них соответствует доказательствам.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 397.84,
  "end": 408.42
 },
 {
  "input": "That same number shows up again in the denominator, but we need to add in the rest, the total number of people times the proportion who are not librarians, times the proportion of those who fit the evidence, which in our example gives 20.",
  "translatedText": "То же самое число снова появляется в знаменателе, но нам нужно добавить остальное: общее количество людей, умноженное на долю небиблиотекарей, умноженное на долю тех, кто соответствует доказательствам, что в нашем примере дает 20.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.22,
  "end": 422.14
 },
 {
  "input": "Now notice the total number of people here, 210, that gets cancelled out, and of course it should, that was just an arbitrary choice made for the sake of illustration.",
  "translatedText": "Теперь обратите внимание на общее количество людей здесь, 210, которое вычеркивается, и, конечно, так и должно быть, это был просто произвольный выбор, сделанный ради иллюстрации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.22,
  "end": 431.04
 },
 {
  "input": "This leaves us finally with a more abstract representation purely in terms of probabilities, and this, my friends, is Bayes' theorem.",
  "translatedText": "В конечном итоге это дает нам более абстрактное представление, чисто в терминах вероятностей, и это, друзья мои, и есть теорема Байеса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 439.22
 },
 {
  "input": "More often, you see this denominator written simply as P of E, the total probability of seeing the evidence, which in our example would be the 24 out of 210.",
  "translatedText": "Чаще всего вы видите этот знаменатель, записанный просто как P от E, общая вероятность увидеть доказательства, которая в нашем примере будет равна 24 из 210.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 440.42,
  "end": 450.46
 },
 {
  "input": "But in practice, to calculate it, you almost always have to break it down into the case where the hypothesis is true, and the one where it isn't.",
  "translatedText": "Но на практике, чтобы ее вычислить, почти всегда приходится разбивать ее на случай, когда гипотеза верна, и случай, когда она неверна.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 451.12,
  "end": 458.8
 },
 {
  "input": "Capping things off with one final bit of jargon, this answer is called the posterior, it's your belief about the hypothesis after seeing the evidence.",
  "translatedText": "Завершая все это еще одним жаргоном, этот ответ называется апостериорным: это ваше убеждение в отношении гипотезы после просмотра доказательств.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 460.06,
  "end": 468.6
 },
 {
  "input": "Writing it out abstractly might seem more complicated than just thinking through the example directly with a representative sample.",
  "translatedText": "Абстрактное изложение может показаться более сложным, чем простое обдумывание примера непосредственно на репрезентативной выборке.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 476.5
 },
 {
  "input": "And yeah, it is.",
  "translatedText": "И да, это так.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 476.92,
  "end": 478.78
 },
 {
  "input": "Keep in mind though, the value of a formula like this is that it lets you quantify and systematize the idea of changing beliefs.",
  "translatedText": "Однако имейте в виду, что ценность такой формулы заключается в том, что она позволяет вам количественно оценить и систематизировать идею изменения убеждений.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.2,
  "end": 486.26
 },
 {
  "input": "Scientists use this formula when they're analyzing the extent to which new data validates or invalidates their models.",
  "translatedText": "Ученые используют эту формулу, когда анализируют, в какой степени новые данные подтверждают или опровергают их модели.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.94,
  "end": 492.84
 },
 {
  "input": "Programmers will sometimes use it in building artificial intelligence, where at times you want to explicitly and numerically model a machine's belief.",
  "translatedText": "Программисты иногда используют его при создании искусственного интеллекта, когда иногда требуется явно и численно смоделировать убеждения машины.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.84,
  "end": 500.64
 },
 {
  "input": "And honestly, just for the way you view yourself and your own opinions and what it takes for your mind to change, Bayes' theorem has a way of reframing how you even think about thought itself.",
  "translatedText": "И, честно говоря, теорема Байеса позволяет переосмыслить то, как вы вообще думаете о самой мысли, именно в отношении того, как вы смотрите на себя и свои собственные мнения, а также на то, что нужно для изменения вашего разума.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 501.4,
  "end": 510.82
 },
 {
  "input": "Putting a formula to it can also be more important as the examples get more and more intricate.",
  "translatedText": "Добавление формулы также может оказаться более важным, поскольку примеры становятся все более и более сложными.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.3,
  "end": 516.34
 },
 {
  "input": "However you write it, I actually encourage you not to try memorizing the formula, but to instead draw out this diagram as needed.",
  "translatedText": "Как бы вы это ни написали, я на самом деле советую вам не пытаться запомнить формулу, а вместо этого рисовать эту диаграмму по мере необходимости.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.08,
  "end": 524.68
 },
 {
  "input": "It's sort of a distilled version of thinking with a representative sample, where we think with areas instead of counts, which is more flexible and easier to sketch on the fly.",
  "translatedText": "Это своего рода очищенная версия мышления с репрезентативной выборкой, где мы думаем площадями, а не подсчетами, что более гибко и легче рисовать на лету.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.26,
  "end": 533.62
 },
 {
  "input": "Rather than bringing to mind some specific number of examples, like 210, think of the space of all possibilities as a 1x1 square.",
  "translatedText": "Вместо того чтобы приводить в голову какое-то конкретное количество примеров, например 210, представьте себе пространство всех возможностей как квадрат 1х1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 534.26,
  "end": 541.38
 },
 {
  "input": "Then any event occupies some subset of this space, and the probability of that event can be thought about as the area of that subset.",
  "translatedText": "Тогда любое событие занимает некоторое подмножество этого пространства, и вероятность этого события можно рассматривать как площадь этого подмножества.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 542.12,
  "end": 550.94
 },
 {
  "input": "For example, I like to think of the hypothesis as living in the left part of the square with a width of p of h.",
  "translatedText": "Например, мне нравится думать о гипотезе как о живущей в левой части квадрата шириной p или h.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 551.54,
  "end": 557.66
 },
 {
  "input": "I recognize I'm being a bit repetitive, but when you see evidence, the space of possibilities gets restricted, and the crucial part is that restriction might not be even between the left and the right, so the new probability for the hypothesis is the proportion it occupies in this restricted wonky shape.",
  "translatedText": "Я признаю, что немного повторяюсь, но когда вы видите доказательства, пространство возможностей сужается, и самое главное заключается в том, что ограничение может отсутствовать даже между левыми и правыми, поэтому новая вероятность гипотезы равна пропорцию, которую он занимает в этой ограниченной шаткой форме.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 558.32,
  "end": 576.94
 },
 {
  "input": "Now if you think a farmer is just as likely to fit the evidence as a librarian, then the proportion doesn't change, which should make sense, right?",
  "translatedText": "Теперь, если вы думаете, что фермер с такой же вероятностью соответствует доказательствам, как и библиотекарь, тогда пропорция не изменится, что должно иметь смысл, не так ли?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.64,
  "end": 586.24
 },
 {
  "input": "And evidence doesn't change your beliefs.",
  "translatedText": "И доказательства не меняют ваших убеждений.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.26,
  "end": 588.32
 },
 {
  "input": "But when these likelihoods are very different from each other, that's when your belief changes a lot.",
  "translatedText": "Но когда эти вероятности сильно отличаются друг от друга, тогда ваше убеждение сильно меняется.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 588.9,
  "end": 593.48
 },
 {
  "input": "Bayes' theorem spells out what that proportion is, and if you want you can read it geometrically.",
  "translatedText": "Теорема Байеса объясняет, что представляет собой эта пропорция, и если хотите, вы можете прочитать ее геометрически.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.76,
  "end": 600.52
 },
 {
  "input": "Something like p of h times p of e given h, the probability of both the hypothesis and the evidence occurring together, is the width times the height of this little left rectangle, the area of that region.",
  "translatedText": "Что-то вроде p из h, умноженного на p из e при условии h, вероятность того, что гипотеза и свидетельства встречаются вместе, равна произведению ширины на высоту этого маленького левого прямоугольника, то есть площади этой области.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.9,
  "end": 613.08
 },
 {
  "input": "Alright, this is probably a good time to take a step back and consider a few of the broader takeaways about how to make probability more intuitive, beyond Bayes' theorem.",
  "translatedText": "Хорошо, возможно, сейчас самое время сделать шаг назад и рассмотреть несколько более общих выводов о том, как сделать вероятность более интуитивно понятной, помимо теоремы Байеса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 614.76,
  "end": 623.22
 },
 {
  "input": "First off, notice how the trick of thinking about a representative sample with some specific number of people, like our 210 librarians and farmers, was really helpful.",
  "translatedText": "Прежде всего, обратите внимание, насколько действительно полезным оказался трюк с представлением о репрезентативной выборке с некоторым конкретным количеством людей, таких как наши 210 библиотекарей и фермеров.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.78,
  "end": 632.4
 },
 {
  "input": "There's actually another Kahneman and Tversky result which is all about this, and it's interesting enough to interject here.",
  "translatedText": "На самом деле есть еще один результат Канемана и Тверски, который полностью посвящен этому, и он достаточно интересен, чтобы вставить его здесь.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.96,
  "end": 638.38
 },
 {
  "input": "They did this experiment that was similar to the one with Steve, but where people were given the following description of a fictitious woman named Linda.",
  "translatedText": "Они провели эксперимент, похожий на эксперимент со Стивом, но в котором людям давали следующее описание вымышленной женщины по имени Линда.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.52,
  "end": 645.72
 },
 {
  "input": "Linda is 31 years old, single, outspoken, and very bright.",
  "translatedText": "Линде 31 год, она одинока, откровенна и очень умна.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.4,
  "end": 650.62
 },
 {
  "input": "She majored in philosophy.",
  "translatedText": "Она специализировалась в области философии.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 651.14,
  "end": 652.16
 },
 {
  "input": "As a student she was deeply concerned with issues of discrimination and social justice, and also participated in the anti-nuclear demonstrations.",
  "translatedText": "Будучи студенткой, она была глубоко озабочена вопросами дискриминации и социальной справедливости, а также участвовала в антиядерных демонстрациях.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 652.64,
  "end": 659.54
 },
 {
  "input": "After seeing this people were asked what's more likely, 1.",
  "translatedText": "Увидев это, людей спросили, что более вероятно: 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 660.7,
  "end": 664.02
 },
 {
  "input": "That Linda is a bank teller, or 2.",
  "translatedText": "Что Линда работает кассиром в банке, или 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 664.34,
  "end": 666.46
 },
 {
  "input": "That Linda is a bank teller and is active in the feminist movement.",
  "translatedText": "Линда работает кассиром в банке и активно участвует в феминистском движении.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.92,
  "end": 669.9
 },
 {
  "input": "85%, 85% of participants said that the latter is more likely than the former, even though the set of bank tellers who are active in the feminist movement is a subset of the set of bank tellers.",
  "translatedText": "85%, 85% участников сказали, что последнее более вероятно, чем первое, даже несмотря на то, что совокупность банковских кассиров, активных в феминистском движении, является подгруппой банковских кассиров.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.22,
  "end": 683.32
 },
 {
  "input": "It has to be smaller.",
  "translatedText": "Он должен быть меньше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 683.56,
  "end": 684.68
 },
 {
  "input": "So that's interesting enough, but what's fascinating is that there's a simple way that you can rephrase the question that dropped this error from 85% to 0.",
  "translatedText": "Это достаточно интересно, но что самое интересное, так это то, что есть простой способ перефразировать вопрос, который снизил эту ошибку с 85% до 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.64,
  "end": 694.1
 },
 {
  "input": "Instead, if participants were told that there are 100 people who fit this description, and then asked to estimate how many of those 100 are bank tellers, and how many are bank tellers active in the feminist movement, nobody makes the error.",
  "translatedText": "Вместо этого, если участникам сказать, что есть 100 человек, которые подходят под это описание, а затем попросить оценить, сколько из этих 100 являются банковскими кассирами и сколько банковских кассиров активно участвуют в феминистском движении, никто не допустит ошибки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 694.96,
  "end": 708.5
 },
 {
  "input": "Everybody correctly assigns a higher number to the first option than to the second.",
  "translatedText": "Все правильно присваивают первому варианту большее число, чем второму.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.5,
  "end": 713.18
 },
 {
  "input": "It's weird, somehow phrases like 40 out of 100 kick our intuitions into gear much more effectively than 40%, much less 0.4, and much less abstractly referencing the idea of something being more or less likely.",
  "translatedText": "Странно, но каким-то образом фразы вроде 40 из 100 активизируют нашу интуицию гораздо эффективнее, чем 40%, а тем более 0.4, и гораздо менее абстрактно отсылает к идее о том, что что-то более или менее вероятно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 714.78,
  "end": 728.06
 },
 {
  "input": "That said, representative samples don't easily capture the continuous nature of probability.",
  "translatedText": "Тем не менее, репрезентативные выборки нелегко отразить непрерывную природу вероятности.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.4,
  "end": 734.1
 },
 {
  "input": "So turning to area is a nice alternative, not just because of the continuity, but also because it's way easier to sketch out when you're sitting there pencil and paper puzzling over some problem.",
  "translatedText": "Таким образом, обращение к пространству — хорошая альтернатива не только из-за непрерывности, но и потому, что гораздо легче делать наброски, когда вы сидите и ломаете голову над какой-то проблемой карандашом и бумагой.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.1,
  "end": 744.04
 },
 {
  "input": "People often think about probability as being the study of uncertainty, and that is of course how it's applied in science, but the actual math of probability, where all the formulas come from, is just the math of proportions, and in that context turning to geometry is exceedingly helpful.",
  "translatedText": "Люди часто думают о вероятности как об изучении неопределенности, и именно так она и применяется в науке, но реальная математика вероятностей, откуда берутся все формулы, — это всего лишь математика пропорций, и в этом контексте обращение к геометрия чрезвычайно полезна.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 745.22,
  "end": 761.02
 },
 {
  "input": "I mean, take a look at Bayes' theorem as a statement about proportions, whether that's proportions of people, of areas, whatever.",
  "translatedText": "Я имею в виду, взгляните на теорему Байеса как на утверждение о пропорциях, будь то пропорции людей, территорий и т. д.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 764.26,
  "end": 770.72
 },
 {
  "input": "Once you digest what it's saying, it's actually kind of obvious.",
  "translatedText": "Как только вы переварите то, что он говорит, это станет довольно очевидным.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 771.3,
  "end": 774.46
 },
 {
  "input": "Both sides tell you to look at the cases where the evidence is true, and then to consider the proportion of those cases where the hypothesis is also true.",
  "translatedText": "Обе стороны советуют вам рассмотреть случаи, когда доказательства верны, а затем рассмотреть долю тех случаев, когда гипотеза также верна.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 775.04,
  "end": 782.72
 },
 {
  "input": "That's it, that's all it's saying.",
  "translatedText": "Вот и все, что здесь говорится.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.24,
  "end": 784.64
 },
 {
  "input": "The right-hand side just spells out how to compute it.",
  "translatedText": "В правой части просто указано, как это вычислить.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 784.86,
  "end": 786.9
 },
 {
  "input": "What's noteworthy is that such a straightforward fact about proportions can become hugely significant for science, for artificial intelligence, and really any situation where you want to quantify belief.",
  "translatedText": "Примечательно то, что такой простой факт о пропорциях может стать чрезвычайно важным для науки, для искусственного интеллекта и вообще для любой ситуации, когда вы хотите количественно оценить убеждения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 787.54,
  "end": 797.92
 },
 {
  "input": "I hope to give you a better glimpse of that as we get into more examples.",
  "translatedText": "Я надеюсь дать вам лучшее представление об этом, когда мы перейдем к большему количеству примеров.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.54,
  "end": 801.42
 },
 {
  "input": "But before more examples, we have a little bit of unfinished business with Steve.",
  "translatedText": "Но прежде чем приводить новые примеры, у нас со Стивом осталось немного незаконченных дел.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.38,
  "end": 805.74
 },
 {
  "input": "As I mentioned, some psychologists debate Kahneman and Tversky's conclusion that the rational thing to do is to bring to mind the ratio of farmers to librarians.",
  "translatedText": "Как я уже упоминал, некоторые психологи оспаривают вывод Канемана и Тверски о том, что разумнее всего вспомнить соотношение фермеров и библиотекарей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.48,
  "end": 814.8
 },
 {
  "input": "They complain that the context is ambiguous.",
  "translatedText": "Они жалуются, что контекст неоднозначен.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 815.14,
  "end": 817.26
 },
 {
  "input": "I mean, who is Steve, exactly?",
  "translatedText": "Я имею в виду, кто такой Стив?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.92,
  "end": 819.84
 },
 {
  "input": "Should you expect that he's a randomly sampled American?",
  "translatedText": "Стоит ли ожидать, что он случайно выбранный американец?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 819.84,
  "end": 822.66
 },
 {
  "input": "Or would you be better to assume that he's a friend of the two psychologists interrogating you?",
  "translatedText": "Или вам лучше предположить, что он друг двух психологов, которые вас допрашивают?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 823.26,
  "end": 827.0
 },
 {
  "input": "Or maybe that he's someone you're personally likely to know?",
  "translatedText": "Или, может быть, вы, скорее всего, знаете его лично?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 827.22,
  "end": 829.74
 },
 {
  "input": "This assumption determines the prior.",
  "translatedText": "Это предположение определяет априор.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 830.42,
  "end": 832.4
 },
 {
  "input": "I for one run into way more librarians in a given month than I do farmers.",
  "translatedText": "Например, за месяц я встречаю гораздо больше библиотекарей, чем фермеров.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 832.96,
  "end": 836.68
 },
 {
  "input": "Needless to say, the probability of a librarian or a farmer fitting this description is highly open to interpretation.",
  "translatedText": "Излишне говорить, что вероятность того, что библиотекарь или фермер соответствует этому описанию, весьма открыта для интерпретации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.5,
  "end": 843.52
 },
 {
  "input": "For our purposes, understanding the math, what I want to emphasize is that any question worth debating here can be pictured in the context of the diagram.",
  "translatedText": "Для наших целей, понимания математики, я хочу подчеркнуть, что любой вопрос, достойный обсуждения здесь, можно изобразить в контексте диаграммы.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 844.44,
  "end": 852.3
 },
 {
  "input": "Questions about the context shift around the prior, and questions about the personalities and stereotypes shift around the relevant likelihoods.",
  "translatedText": "Вопросы о контексте меняются вокруг предшествующего, а вопросы о личностях и стереотипах — вокруг соответствующих вероятностей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 853.0,
  "end": 860.58
 },
 {
  "input": "All that said, whether or not you buy this particular experiment, the ultimate point that evidence should not determine beliefs, but update them, is worth tattooing in your brain.",
  "translatedText": "С учетом всего вышесказанного, независимо от того, верите ли вы в этот конкретный эксперимент или нет, конечный пункт о том, что доказательства должны не определять убеждения, а обновлять их, стоит вытатуировать в своем мозгу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.1,
  "end": 871.0
 },
 {
  "input": "I'm in no position to say whether this does or does not run against natural human instinct.",
  "translatedText": "Я не могу сказать, противоречит ли это естественным человеческим инстинктам или нет.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.8,
  "end": 876.5
 },
 {
  "input": "We'll leave that to the psychologists.",
  "translatedText": "Оставим это психологам.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.5,
  "end": 878.24
 },
 {
  "input": "What's more interesting to me is how we can reprogram our intuition to authentically reflect the implications of math, and bringing to mind the right image can often do just that.",
  "translatedText": "Что меня больше интересует, так это то, как мы можем перепрограммировать нашу интуицию, чтобы достоверно отражать математические выводы, и привлечение в голову правильного образа часто может помочь именно в этом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 878.92,
  "end": 888.06
 }
]