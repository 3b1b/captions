[
 {
  "input": "The goal is for you to come away from this video understanding one of the most important formulas in all of probability, Bayes' theorem. ",
  "translatedText": "লক্ষ্য হল আপনি এই ভিডিও থেকে বেরিয়ে আসা সমস্ত সম্ভাবনার অন্যতম গুরুত্বপূর্ণ সূত্র, বেইসের উপপাদ্য বোঝা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.84
 },
 {
  "input": "This formula is central to scientific discovery, it's a core tool in machine learning and AI, and it's even been used for treasure hunting, when in the 1980s a small team led by Tommy Thompson, and I'm not making up that name, used Bayesian search tactics to help uncover a ship that had sunk a century and a half earlier, and the ship was carrying what in today's terms amounts to $700 million worth of gold. ",
  "translatedText": "এই সূত্রটি বৈজ্ঞানিক আবিষ্কারের কেন্দ্রবিন্দু, এটি মেশিন লার্নিং এবং এআই-এর একটি মূল হাতিয়ার, এবং এটি গুপ্তধন শিকারের জন্যও ব্যবহার করা হয়েছে, যখন 1980-এর দশকে টমি থম্পসনের নেতৃত্বে একটি ছোট দল, এবং আমি সেই নামটি তৈরি করছি না, ব্যবহার করা হয়েছিল দেড় শতাব্দী আগে ডুবে যাওয়া একটি জাহাজকে উদ্ঘাটন করতে বায়েসিয়ান অনুসন্ধান কৌশল, এবং জাহাজটি আজকের পরিপ্রেক্ষিতে $700 মিলিয়ন মূল্যের সোনা বহন করছিল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 7.48,
  "end": 30.74
 },
 {
  "input": "So it's a formula worth understanding, but of course there are multiple different levels of possible understanding. ",
  "translatedText": "সুতরাং এটি বোঝার মতো একটি সূত্র, তবে অবশ্যই সম্ভাব্য বোঝার একাধিক বিভিন্ন স্তর রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.34,
  "end": 37.04
 },
 {
  "input": "At the simplest there's just knowing what each one of the parts means, so that you can plug in numbers. ",
  "translatedText": "সহজে, প্রতিটি অংশের অর্থ কী তা জানা আছে, যাতে আপনি নম্বরগুলি প্লাগ করতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 37.6,
  "end": 42.04
 },
 {
  "input": "Then there's understanding why it's true, and later I'm going to show you a certain diagram that's helpful for rediscovering this formula on the fly as needed. ",
  "translatedText": "তারপরে বোঝা যাবে কেন এটি সত্য, এবং পরে আমি আপনাকে একটি নির্দিষ্ট ডায়াগ্রাম দেখাব যা প্রয়োজন অনুসারে উড়তে এই সূত্রটি পুনরায় আবিষ্কার করার জন্য সহায়ক।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.76,
  "end": 50.58
 },
 {
  "input": "But maybe the most important level is being able to recognize when you need to use it. ",
  "translatedText": "কিন্তু সম্ভবত সবচেয়ে গুরুত্বপূর্ণ স্তর চিনতে সক্ষম হচ্ছে যখন আপনি এটি ব্যবহার করতে হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.24,
  "end": 55.54
 },
 {
  "input": "And with the goal of gaining a deeper understanding, you and I are going to tackle these in reverse order. ",
  "translatedText": "এবং একটি গভীর উপলব্ধি অর্জনের লক্ষ্যে, আপনি এবং আমি বিপরীত ক্রমে এগুলি মোকাবেলা করতে যাচ্ছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.54,
  "end": 60.56
 },
 {
  "input": "So before dissecting the formula or explaining the visual that makes it obvious, I'd like to tell you about a man named Steve. ",
  "translatedText": "সুতরাং সূত্রটি ব্যবচ্ছেদ করার আগে বা দৃশ্যটি ব্যাখ্যা করার আগে যা এটিকে স্পষ্ট করে তোলে, আমি আপনাকে স্টিভ নামে একজন ব্যক্তির সম্পর্কে বলতে চাই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.02,
  "end": 66.86
 },
 {
  "input": "Listen carefully now. ",
  "translatedText": "এখন মনোযোগ দিয়ে শুনুন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.32,
  "end": 68.72
 },
 {
  "input": "Steve is very shy and withdrawn, invariably helpful but with very little interest in people or the world of reality. ",
  "translatedText": "স্টিভ খুব লাজুক এবং প্রত্যাহার করে, সবসময় সহায়ক কিন্তু মানুষ বা বাস্তবতার জগতে খুব কম আগ্রহের সাথে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.74,
  "end": 79.16
 },
 {
  "input": "A meek and tidy soul, he has a need for order and structure, and a passion for detail. ",
  "translatedText": "একটি নম্র এবং পরিপাটি আত্মা, তার অর্ডার এবং কাঠামোর প্রয়োজন এবং বিশদ বিবরণের জন্য একটি আবেগ রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.74,
  "end": 84.1
 },
 {
  "input": "Which of the following do you find more likely? ",
  "translatedText": "নিচের কোনটি আপনি বেশি সম্ভাবনা খুঁজে পান? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 84.62,
  "end": 86.78
 },
 {
  "input": "Steve is a librarian, or Steve is a farmer? ",
  "translatedText": "স্টিভ একজন লাইব্রেরিয়ান, নাকি স্টিভ একজন কৃষক? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.2,
  "end": 90.38
 },
 {
  "input": "Some of you may recognize this as an example from a study conducted by the two psychologists Daniel Kahneman and Amos Tversky. ",
  "translatedText": "আপনার মধ্যে কেউ কেউ এটিকে দুই মনোবিজ্ঞানী ড্যানিয়েল কাহনেম্যান এবং আমোস টভারস্কি দ্বারা পরিচালিত একটি গবেষণা থেকে উদাহরণ হিসাবে চিনতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.4,
  "end": 97.44
 },
 {
  "input": "Their work was a big deal, it won a Nobel Prize, and it's been popularized many times over in books like Kahneman's Thinking Fast and Slow, or Michael Lewis's The Undoing Project. ",
  "translatedText": "তাদের কাজটি একটি বড় বিষয় ছিল, এটি একটি নোবেল পুরস্কার জিতেছে এবং এটি কাহনেম্যানের থিংকিং ফাস্ট অ্যান্ড স্লো বা মাইকেল লুইসের দ্য আনডুয়িং প্রজেক্টের মতো বইগুলিতে বহুবার জনপ্রিয় হয়েছে৷ তারা যা গবেষণা করেছিল তা ছিল মানুষের রায়, যখন এই রায়গুলি অযৌক্তিকভাবে বিরোধিতা করে যখন সম্ভাব্যতার নিয়মগুলি তাদের হওয়া উচিত বলে নির্দেশ করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.2,
  "end": 106.56
 },
 {
  "input": "What they researched was human judgments, with a frequent focus on when these judgments irrationally contradict what the laws of probability suggest they should be. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.42,
  "end": 115.78
 },
 {
  "input": "The example with Steve, our maybe librarian, maybe farmer, illustrates one specific type of irrationality, or maybe I should say alleged irrationality, there are people who debate the conclusion here, but more on that later on. ",
  "translatedText": "স্টিভের উদাহরণ, আমাদের হতে পারে লাইব্রেরিয়ান, হতে পারে কৃষক, একটি নির্দিষ্ট ধরনের অযৌক্তিকতাকে চিত্রিত করে, অথবা হয়তো আমার বলা উচিত অযৌক্তিকতা বলা উচিত, এখানে উপসংহার নিয়ে বিতর্ক করার মতো লোক আছে, কিন্তু পরবর্তীতে আরও বেশি কিছু।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 116.34,
  "end": 129.62
 },
 {
  "input": "According to Kahneman and Tversky, after people are given this description of Steve as a meek and tidy soul, most say he's more likely to be a librarian. ",
  "translatedText": "কাহনেম্যান এবং টোভারস্কির মতে, স্টিভকে একজন নম্র এবং পরিপাটি আত্মা হিসাবে এই বর্ণনা দেওয়ার পরে, বেশিরভাগই বলে যে তিনি একজন গ্রন্থাগারিক হওয়ার সম্ভাবনা বেশি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.98,
  "end": 138.0
 },
 {
  "input": "After all, these traits line up better with the stereotypical view of a librarian than a farmer. ",
  "translatedText": "সর্বোপরি, এই বৈশিষ্ট্যগুলি একজন কৃষকের চেয়ে একজন গ্রন্থাগারিকের স্টেরিওটাইপিক্যাল দৃষ্টিভঙ্গির সাথে আরও ভালভাবে সারিবদ্ধ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 138.0,
  "end": 143.46
 },
 {
  "input": "And according to Kahneman and Tversky, this is irrational. ",
  "translatedText": "এবং Kahneman এবং Tversky এর মতে, এটি অযৌক্তিক।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.2,
  "end": 146.88
 },
 {
  "input": "The point is not whether people hold correct or biased views about the personalities of librarians and farmers, it's that almost nobody thinks to incorporate information about the ratio of farmers to librarians in their judgments. ",
  "translatedText": "মূল বিষয় হল গ্রন্থাগারিক এবং কৃষকদের ব্যক্তিত্ব সম্পর্কে লোকেরা সঠিক বা পক্ষপাতদুষ্ট দৃষ্টিভঙ্গি রাখে কিনা, এটি হল যে প্রায় কেউই তাদের বিচারে গ্রন্থাগারিক এবং কৃষকদের অনুপাত সম্পর্কে তথ্য অন্তর্ভুক্ত করার কথা ভাবেন না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.6,
  "end": 160.24
 },
 {
  "input": "In their paper, Kahneman and Tversky said that in the US, that ratio is about 20 to 1. ",
  "translatedText": "তাদের গবেষণাপত্রে, কাহনেম্যান এবং টভারস্কি বলেছেন যে মার্কিন যুক্তরাষ্ট্রে সেই অনুপাত প্রায় 20 থেকে 1।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 160.92,
  "end": 165.18
 },
 {
  "input": "The numbers I could find today put that much higher, but let's stick with the 20 to 1 number, since it's a little easier to illustrate and proves the point as well. ",
  "translatedText": "আজ আমি যে সংখ্যাগুলি খুঁজে পেয়েছি তা অনেক বেশি, কিন্তু আসুন 20 থেকে 1 সংখ্যার সাথে লেগে থাকি, কারণ এটি ব্যাখ্যা করা একটু সহজ এবং বিন্দুটিকেও প্রমাণ করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.58,
  "end": 173.42
 },
 {
  "input": "To be clear, anyone who is asked this question is not expected to have perfect information about the actual statistics of farmers and librarians and their personality traits. ",
  "translatedText": "স্পষ্ট করে বলতে গেলে, যাকে এই প্রশ্নটি জিজ্ঞাসা করা হয় তার কাছে কৃষক এবং গ্রন্থাগারিকদের প্রকৃত পরিসংখ্যান এবং তাদের ব্যক্তিত্বের বৈশিষ্ট্য সম্পর্কে নিখুঁত তথ্য আশা করা যায় না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.28,
  "end": 183.14
 },
 {
  "input": "But the question is whether people even think to consider that ratio enough to at least make a rough estimate. ",
  "translatedText": "কিন্তু প্রশ্ন হল মানুষ কি সেই অনুপাতকে অন্তত একটি মোটামুটি অনুমান করার জন্য যথেষ্ট বিবেচনা করতে পারে কিনা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.68,
  "end": 189.22
 },
 {
  "input": "Rationality is not about knowing facts, it's about recognizing which facts are relevant. ",
  "translatedText": "যৌক্তিকতা তথ্য জানার বিষয়ে নয়, এটি কোন তথ্য প্রাসঙ্গিক তা সনাক্ত করা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.04,
  "end": 194.46
 },
 {
  "input": "Now if you do think to make that estimate, there's a pretty simple way to reason about the question, which, spoiler alert, involves all of the essential reasoning behind Bayes' theorem. ",
  "translatedText": "এখন আপনি যদি সেই অনুমানটি তৈরি করার কথা ভাবেন, প্রশ্নটি সম্পর্কে যুক্তি করার একটি সহজ উপায় রয়েছে, যা, স্পয়লার সতর্কতা, বেইসের উপপাদ্যের পিছনে সমস্ত প্রয়োজনীয় যুক্তি জড়িত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 195.88,
  "end": 203.9
 },
 {
  "input": "You might start by picturing a representative sample of farmers and librarians, say 200 farmers and 10 librarians. ",
  "translatedText": "আপনি কৃষক এবং গ্রন্থাগারিকদের একটি প্রতিনিধি নমুনা চিত্রিত করে শুরু করতে পারেন, বলুন 200 জন কৃষক এবং 10 জন গ্রন্থাগারিক।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.66,
  "end": 211.02
 },
 {
  "input": "Then when you hear of this meek and tidy soul description, let's say that your gut instinct is that 40% of librarians would fit that description, and that 10% of farmers would. ",
  "translatedText": "তারপরে আপনি যখন এই নম্র এবং পরিপাটি আত্মার বর্ণনা শুনবেন, তখন বলুন যে আপনার অন্ত্রের প্রবৃত্তি হল যে 40% গ্রন্থাগারিক সেই বর্ণনার সাথে মানানসই হবে এবং 10% কৃষকরা তা করবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.74,
  "end": 221.36
 },
 {
  "input": "If those are your estimates, it would mean that from your sample you would expect about 4 librarians to fit the description, and about 20 farmers to fit that description. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 222.02,
  "end": 230.24
 },
 {
  "input": "So the probability that a random person among those who fit this description is a librarian is 4 out of 24, or 16.7%. ",
  "translatedText": "যদি সেগুলি আপনার অনুমান হয়, তাহলে এর অর্থ হল যে আপনার নমুনা থেকে আপনি প্রায় 4 জন গ্রন্থাগারিক বর্ণনার সাথে মানানসই হবেন এবং প্রায় 20 জন কৃষক সেই বিবরণের সাথে মানানসই হবেন বলে আশা করবেন৷ সুতরাং যারা এই বর্ণনার সাথে মানানসই তাদের মধ্যে একজন লাইব্রেরিয়ান হওয়ার সম্ভাবনা 24 এর মধ্যে 4 বা 16।7%।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.02,
  "end": 240.1
 },
 {
  "input": "So even if you think that a librarian is 4 times as likely as a farmer to fit this description, that's not enough to overcome the fact that there are way more farmers. ",
  "translatedText": "সুতরাং আপনি যদি মনে করেন যে একজন লাইব্রেরিয়ান একজন কৃষকের তুলনায় 4 গুণ বেশি সম্ভাবনাময় এই বর্ণনাটি মানানসই, তবে এটি আরও বেশি কৃষক রয়েছে এই সত্যটি অতিক্রম করার জন্য যথেষ্ট নয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.1,
  "end": 249.02
 },
 {
  "input": "The upshot, and this is the key mantra underlying Bayes' theorem, is that new evidence does not completely determine your beliefs in a vacuum, it should update prior beliefs. ",
  "translatedText": "ফলাফল, এবং এটি হল বেয়েসের তত্ত্বের অন্তর্নিহিত মূল মন্ত্র, যে নতুন প্রমাণ আপনার বিশ্বাসকে শূন্যতায় সম্পূর্ণরূপে নির্ধারণ করে না, এটির পূর্বের বিশ্বাসগুলিকে আপডেট করা উচিত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 249.72,
  "end": 259.22
 },
 {
  "input": "If this line of reasoning makes sense to you, the way that seeing evidence restricts the space of possibilities and the ratio you need to consider after that, then congratulations, you understand the heart of Bayes' theorem. ",
  "translatedText": "যুক্তির এই লাইনটি যদি আপনার কাছে বোধগম্য হয়, প্রমাণ দেখার উপায়টি সম্ভাবনার স্থানকে সীমাবদ্ধ করে এবং এর পরে আপনাকে যে অনুপাতটি বিবেচনা করতে হবে, তাহলে অভিনন্দন, আপনি বেইসের উপপাদ্যটির হৃদয় বুঝতে পেরেছেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.12,
  "end": 272.36
 },
 {
  "input": "Maybe the numbers you would estimate would be a little different, but what matters is how you fit the numbers together to update your beliefs based on evidence. ",
  "translatedText": "হতে পারে আপনি যে সংখ্যাগুলি অনুমান করবেন তা একটু আলাদা হবে, তবে কী গুরুত্বপূর্ণ তা হল আপনি প্রমাণের ভিত্তিতে আপনার বিশ্বাসগুলি আপডেট করার জন্য সংখ্যাগুলিকে একসাথে কীভাবে ফিট করেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.36,
  "end": 280.6
 },
 {
  "input": "Understanding one example is one thing, but see if you can take a minute to generalize everything we just did and write it all down as a formula. ",
  "translatedText": "একটি উদাহরণ বোঝা একটি জিনিস, কিন্তু দেখুন আমরা যা করেছি সব কিছুকে সাধারণীকরণ করতে এবং এটিকে একটি সূত্র হিসাবে লিখতে আপনি এক মিনিট সময় নিতে পারেন কিনা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.08,
  "end": 289.74
 },
 {
  "input": "The general situation where Bayes' theorem is relevant is when you have some hypothesis, like Steve is a librarian, and you see some new evidence, say this verbal description of Steve as a meek and tidy soul. ",
  "translatedText": "সাধারণ পরিস্থিতি যেখানে বেয়েসের উপপাদ্য প্রাসঙ্গিক হয় যখন আপনার কাছে কিছু অনুমান থাকে, যেমন স্টিভ একজন গ্রন্থাগারিক, এবং আপনি কিছু নতুন প্রমাণ দেখতে পান, স্টিভের এই মৌখিক বর্ণনাটিকে একজন নম্র এবং পরিপাটি আত্মা হিসাবে বলুন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 292.32,
  "end": 303.98
 },
 {
  "input": "You want to know the probability that your hypothesis holds given that the evidence is true. ",
  "translatedText": "আপনি আপনার অনুমানের সম্ভাব্যতা জানতে চান যে প্রমাণটি সত্য।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.38,
  "end": 309.64
 },
 {
  "input": "In the standard notation, this vertical bar means given that, as in we're restricting our view only to the possibilities where the evidence holds. ",
  "translatedText": "স্ট্যান্ডার্ড স্বরলিপিতে, এই উল্লম্ব দণ্ডের অর্থ হল যে, আমরা আমাদের দৃষ্টিভঙ্গি শুধুমাত্র সেই সম্ভাবনার মধ্যে সীমাবদ্ধ করছি যেখানে প্রমাণ রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 310.44,
  "end": 318.96
 },
 {
  "input": "Remember the first relevant number we used, the probability that the hypothesis holds before considering any of that new evidence. ",
  "translatedText": "আমরা যে প্রথম প্রাসঙ্গিক সংখ্যাটি ব্যবহার করেছি তা মনে রাখবেন, সেই নতুন প্রমাণের কোনোটি বিবেচনা করার আগে হাইপোথিসিস ধারণ করার সম্ভাবনা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.22,
  "end": 327.34
 },
 {
  "input": "In our example, that was 1 out of 21, and it came from considering the ratio of librarians to farmers in the general population. ",
  "translatedText": "আমাদের উদাহরণে, এটি 21 টির মধ্যে 1 ছিল এবং এটি সাধারণ জনসংখ্যার কৃষকদের মধ্যে গ্রন্থাগারিকের অনুপাত বিবেচনা করে এসেছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.72,
  "end": 334.64
 },
 {
  "input": "This number is known as the prior. ",
  "translatedText": "এই সংখ্যাটি পূর্বের হিসাবে পরিচিত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 335.52,
  "end": 336.98
 },
 {
  "input": "After that, we need to consider the proportion of librarians that fit this description, the probability that we would see the evidence given that the hypothesis is true. ",
  "translatedText": "এর পরে, আমাদের এই বর্ণনার সাথে মানানসই গ্রন্থাগারিকদের অনুপাত বিবেচনা করতে হবে, সম্ভাব্যতা যে আমরা প্রদত্ত প্রমাণ দেখতে পাব যে অনুমানটি সত্য।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.02,
  "end": 347.3
 },
 {
  "input": "Again, when you see this vertical bar, it means we're talking about some proportion of a limited part of the total space of possibilities. ",
  "translatedText": "আবার, আপনি যখন এই উল্লম্ব বারটি দেখেন, তখন এর মানে হল আমরা সম্ভাবনার মোট স্থানের একটি সীমিত অংশের কিছু অনুপাতের কথা বলছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 354.84
 },
 {
  "input": "In this case, that limited part is the left side, where the hypothesis holds. ",
  "translatedText": "এই ক্ষেত্রে, সেই সীমিত অংশটি বাম দিকে, যেখানে অনুমানটি ধারণ করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.32,
  "end": 359.3
 },
 {
  "input": "In the context of Bayes' theorem, this value also has a special name, it's called the likelihood. ",
  "translatedText": "বেইসের উপপাদ্যের পরিপ্রেক্ষিতে, এই মানটিরও একটি বিশেষ নাম রয়েছে, একে সম্ভাবনা বলা হয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 359.96,
  "end": 364.64
 },
 {
  "input": "Similarly, you need to know how much of the other side of the space includes the evidence, the probability of seeing the evidence given that the hypothesis isn't true. ",
  "translatedText": "একইভাবে, আপনাকে জানতে হবে যে মহাকাশের অন্য দিকের কতটা প্রমাণ রয়েছে, প্রদত্ত প্রমাণ দেখার সম্ভাবনা যে অনুমানটি সত্য নয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.7,
  "end": 373.56
 },
 {
  "input": "This funny little elbow symbol is commonly used in probability to mean not. ",
  "translatedText": "এই মজার ছোট কনুই চিহ্নটি সাধারণত সম্ভাব্যতা না বোঝাতে ব্যবহৃত হয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.34,
  "end": 378.42
 },
 {
  "input": "So, with the notation in place, remember what our final answer was. ",
  "translatedText": "সুতরাং, স্বরলিপির জায়গায়, আমাদের চূড়ান্ত উত্তর কী ছিল তা মনে রাখবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.86,
  "end": 383.02
 },
 {
  "input": "The probability that our librarian hypothesis is true given the evidence is the total number of librarians fitting the evidence, 4, divided by the total number of people fitting the evidence, 24. ",
  "translatedText": "প্রমাণের ভিত্তিতে আমাদের গ্রন্থাগারিক অনুমান সত্য হওয়ার সম্ভাবনা হল লাইব্রেরিয়ানদের মোট সংখ্যা, 4, প্রমাণের উপযুক্ত লোকের মোট সংখ্যা দ্বারা ভাগ করা হয়, 24।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 383.36,
  "end": 394.88
 },
 {
  "input": "But where did that 4 come from? ",
  "translatedText": "কিন্তু যে 4 কোথা থেকে এসেছে? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.76,
  "end": 397.18
 },
 {
  "input": "Well, it's the total number of people, times the prior probability of being a librarian, giving us the 10 total librarians, times the probability that one of those fits the evidence. ",
  "translatedText": "ঠিক আছে, এটি মোট লোকের সংখ্যা, লাইব্রেরিয়ান হওয়ার পূর্ব সম্ভাবনার গুণ, আমাদের মোট 10 জন গ্রন্থাগারিক প্রদান করে, তাদের মধ্যে একজন প্রমাণের সাথে মানানসই হওয়ার সম্ভাবনার গুণ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 397.84,
  "end": 408.42
 },
 {
  "input": "That same number shows up again in the denominator, but we need to add in the rest, the total number of people times the proportion who are not librarians, times the proportion of those who fit the evidence, which in our example gives 20. ",
  "translatedText": "সেই একই সংখ্যাটি আবার হর-এ দেখা যাচ্ছে, কিন্তু আমাদের বাকি অংশে যোগ করতে হবে, লাইব্রেরিয়ান নন এমন লোকের সংখ্যার অনুপাতের গুণ, যারা প্রমাণের জন্য উপযুক্ত তাদের অনুপাতের গুণ, যা আমাদের উদাহরণে 20 দেয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.22,
  "end": 422.14
 },
 {
  "input": "Now notice the total number of people here, 210, that gets cancelled out, and of course it should, that was just an arbitrary choice made for the sake of illustration. ",
  "translatedText": "এখন এখানে মোট 210 জন লোকের সংখ্যা লক্ষ্য করুন, যেগুলি বাতিল হয়ে যায়, এবং অবশ্যই এটি হওয়া উচিত, এটি কেবল উদাহরণের খাতিরে করা একটি স্বেচ্ছাচারী পছন্দ ছিল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.22,
  "end": 431.04
 },
 {
  "input": "This leaves us finally with a more abstract representation purely in terms of probabilities, and this, my friends, is Bayes' theorem. ",
  "translatedText": "এটি পরিশেষে আমাদেরকে বিশুদ্ধভাবে সম্ভাবনার পরিপ্রেক্ষিতে আরও বিমূর্ত উপস্থাপনা দেয়, এবং এটি, আমার বন্ধুরা, বেইসের উপপাদ্য।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 439.22
 },
 {
  "input": "More often, you see this denominator written simply as P of E, the total probability of seeing the evidence, which in our example would be the 24 out of 210. ",
  "translatedText": "প্রায়শই, আপনি এই হরকে কেবল E-এর P হিসাবে লেখা দেখেন, প্রমাণ দেখার মোট সম্ভাবনা, যা আমাদের উদাহরণে 210-এর মধ্যে 24 হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 440.42,
  "end": 450.46
 },
 {
  "input": "But in practice, to calculate it, you almost always have to break it down into the case where the hypothesis is true, and the one where it isn't. ",
  "translatedText": "কিন্তু অনুশীলনে, এটি গণনা করার জন্য, আপনাকে প্রায় সর্বদা এটিকে ভেঙে ফেলতে হবে যেখানে হাইপোথিসিসটি সত্য, এবং যেখানে এটি নয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 451.12,
  "end": 458.8
 },
 {
  "input": "Capping things off with one final bit of jargon, this answer is called the posterior, it's your belief about the hypothesis after seeing the evidence. ",
  "translatedText": "একটি চূড়ান্ত বিট জার্গন দিয়ে জিনিসগুলি বন্ধ করে, এই উত্তরটিকে পোস্টেরিয়র বলা হয়, এটি প্রমাণ দেখার পরে হাইপোথিসিস সম্পর্কে আপনার বিশ্বাস।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 460.06,
  "end": 468.6
 },
 {
  "input": "Writing it out abstractly might seem more complicated than just thinking through the example directly with a representative sample. ",
  "translatedText": "একটি প্রতিনিধি নমুনা দিয়ে সরাসরি উদাহরণের মাধ্যমে চিন্তা করার চেয়ে বিমূর্তভাবে এটি লেখা আরও জটিল বলে মনে হতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 476.5
 },
 {
  "input": "And yeah, it is. ",
  "translatedText": "এবং হ্যাঁ, এটা. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 476.92,
  "end": 478.78
 },
 {
  "input": "Keep in mind though, the value of a formula like this is that it lets you quantify and systematize the idea of changing beliefs. ",
  "translatedText": "যদিও মনে রাখবেন, এর মতো একটি সূত্রের মান হল এটি আপনাকে বিশ্বাস পরিবর্তনের ধারণাকে পরিমাপ এবং পদ্ধতিগত করতে দেয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.2,
  "end": 486.26
 },
 {
  "input": "Scientists use this formula when they're analyzing the extent to which new data validates or invalidates their models. ",
  "translatedText": "বিজ্ঞানীরা এই সূত্রটি ব্যবহার করেন যখন তারা বিশ্লেষণ করছেন যে নতুন ডেটা তাদের মডেলগুলিকে বৈধ বা অকার্যকর করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.94,
  "end": 492.84
 },
 {
  "input": "Programmers will sometimes use it in building artificial intelligence, where at times you want to explicitly and numerically model a machine's belief. ",
  "translatedText": "প্রোগ্রামাররা কখনও কখনও এটিকে কৃত্রিম বুদ্ধিমত্তা তৈরিতে ব্যবহার করবে, যেখানে কখনও কখনও আপনি স্পষ্টভাবে এবং সংখ্যাগতভাবে একটি মেশিনের বিশ্বাসকে মডেল করতে চান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.84,
  "end": 500.64
 },
 {
  "input": "And honestly, just for the way you view yourself and your own opinions and what it takes for your mind to change, Bayes' theorem has a way of reframing how you even think about thought itself. ",
  "translatedText": "এবং সত্যই, আপনি নিজেকে এবং আপনার নিজের মতামতকে যেভাবে দেখেন এবং আপনার মন পরিবর্তন করতে কী লাগে তার জন্য, বেইসের উপপাদ্যের একটি উপায় রয়েছে যে আপনি কীভাবে চিন্তাভাবনা সম্পর্কেও চিন্তা করেন তা পুনর্বিন্যাস করার একটি উপায় রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 501.4,
  "end": 510.82
 },
 {
  "input": "Putting a formula to it can also be more important as the examples get more and more intricate. ",
  "translatedText": "উদাহরণগুলি আরও জটিল হওয়ার কারণে এটিতে একটি সূত্র রাখা আরও গুরুত্বপূর্ণ হতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.3,
  "end": 516.34
 },
 {
  "input": "However you write it, I actually encourage you not to try memorizing the formula, but to instead draw out this diagram as needed. ",
  "translatedText": "যাইহোক আপনি এটি লিখুন, আমি আসলে আপনাকে উত্সাহিত করি সূত্রটি মুখস্থ করার চেষ্টা না করে বরং প্রয়োজন অনুসারে এই চিত্রটি আঁকতে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.08,
  "end": 524.68
 },
 {
  "input": "It's sort of a distilled version of thinking with a representative sample, where we think with areas instead of counts, which is more flexible and easier to sketch on the fly. ",
  "translatedText": "এটি একটি প্রতিনিধি নমুনা সহ চিন্তার একটি পাতিত সংস্করণ, যেখানে আমরা গণনার পরিবর্তে অঞ্চলগুলি নিয়ে চিন্তা করি, যা উড়ে যাওয়ার সময় স্কেচ করা আরও নমনীয় এবং সহজ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.26,
  "end": 533.62
 },
 {
  "input": "Rather than bringing to mind some specific number of examples, like 210, think of the space of all possibilities as a 1x1 square. ",
  "translatedText": "210 এর মত কিছু নির্দিষ্ট সংখ্যক উদাহরণ মনে রাখার পরিবর্তে, সমস্ত সম্ভাবনার স্থানকে 1x1 বর্গ হিসাবে ভাবুন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 534.26,
  "end": 541.38
 },
 {
  "input": "Then any event occupies some subset of this space, and the probability of that event can be thought about as the area of that subset. ",
  "translatedText": "তারপর যে কোনো ঘটনা এই স্থানের কিছু উপসেট দখল করে, এবং সেই ঘটনার সম্ভাব্যতাকে সেই উপসেটের ক্ষেত্রফল হিসেবে ভাবা যেতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 542.12,
  "end": 550.94
 },
 {
  "input": "For example, I like to think of the hypothesis as living in the left part of the square with a width of p of h. ",
  "translatedText": "উদাহরণস্বরূপ, আমি h এর p এর প্রস্থ সহ বর্গক্ষেত্রের বাম অংশে বসবাসকারী অনুমানটিকে ভাবতে চাই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 551.54,
  "end": 557.66
 },
 {
  "input": "I recognize I'm being a bit repetitive, but when you see evidence, the space of possibilities gets restricted, and the crucial part is that restriction might not be even between the left and the right, so the new probability for the hypothesis is the proportion it occupies in this restricted wonky shape. ",
  "translatedText": "আমি বুঝতে পারি যে আমি কিছুটা পুনরাবৃত্তি করছি, কিন্তু যখন আপনি প্রমাণ দেখতে পান, তখন সম্ভাবনার স্থান সীমাবদ্ধ হয়ে যায় এবং গুরুত্বপূর্ণ অংশটি হল যে সীমাবদ্ধতা বাম এবং ডানের মধ্যেও নাও হতে পারে, তাই অনুমানের জন্য নতুন সম্ভাবনা হল অনুপাত এটি এই সীমাবদ্ধ অস্থির আকারে দখল করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 558.32,
  "end": 576.94
 },
 {
  "input": "Now if you think a farmer is just as likely to fit the evidence as a librarian, then the proportion doesn't change, which should make sense, right? ",
  "translatedText": "এখন আপনি যদি মনে করেন যে একজন কৃষক গ্রন্থাগারিকের মতো প্রমাণের সাথে মানানসই হওয়ার সম্ভাবনা রয়েছে, তাহলে অনুপাতটি পরিবর্তিত হয় না, যার অর্থ হওয়া উচিত, তাই না? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.64,
  "end": 586.24
 },
 {
  "input": "And evidence doesn't change your beliefs. ",
  "translatedText": "এবং প্রমাণ আপনার বিশ্বাস পরিবর্তন করে না. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.26,
  "end": 588.32
 },
 {
  "input": "But when these likelihoods are very different from each other, that's when your belief changes a lot. ",
  "translatedText": "কিন্তু যখন এই সম্ভাবনাগুলি একে অপরের থেকে খুব আলাদা হয়, তখনই আপনার বিশ্বাস অনেক বদলে যায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 588.9,
  "end": 593.48
 },
 {
  "input": "Bayes' theorem spells out what that proportion is, and if you want you can read it geometrically. ",
  "translatedText": "বেয়েসের উপপাদ্যটি সেই অনুপাতটি কী তা বানান করে এবং আপনি যদি চান আপনি এটি জ্যামিতিকভাবে পড়তে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.76,
  "end": 600.52
 },
 {
  "input": "Something like p of h times p of e given h, the probability of both the hypothesis and the evidence occurring together, is the width times the height of this little left rectangle, the area of that region. ",
  "translatedText": "প্রদত্ত h এর p এর h বার p এর মতো কিছু, অনুমান এবং প্রমাণ উভয়েরই সম্ভাব্যতা একত্রে ঘটছে, এই সামান্য বাম আয়তক্ষেত্রটির উচ্চতার প্রস্থ গুণ, সেই অঞ্চলের ক্ষেত্রফল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.9,
  "end": 613.08
 },
 {
  "input": "Alright, this is probably a good time to take a step back and consider a few of the broader takeaways about how to make probability more intuitive, beyond Bayes' theorem. ",
  "translatedText": "ঠিক আছে, বেইসের উপপাদ্যের বাইরে কীভাবে সম্ভাব্যতাকে আরও স্বজ্ঞাত করা যায় সে সম্পর্কে একটি পদক্ষেপ পিছিয়ে নেওয়ার এবং কয়েকটি বিস্তৃত উপায় বিবেচনা করার এটি সম্ভবত একটি ভাল সময়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 614.76,
  "end": 623.22
 },
 {
  "input": "First off, notice how the trick of thinking about a representative sample with some specific number of people, like our 210 librarians and farmers, was really helpful. ",
  "translatedText": "প্রথমত, লক্ষ্য করুন যে আমাদের 210 জন গ্রন্থাগারিক এবং কৃষকদের মতো কিছু নির্দিষ্ট সংখ্যক লোকের সাথে একটি প্রতিনিধি নমুনা সম্পর্কে চিন্তা করার কৌশলটি সত্যিই সহায়ক ছিল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.78,
  "end": 632.4
 },
 {
  "input": "There's actually another Kahneman and Tversky result which is all about this, and it's interesting enough to interject here. ",
  "translatedText": "প্রকৃতপক্ষে আরেকটি কাহনেম্যান এবং টোভারস্কির ফলাফল রয়েছে যা এই সম্পর্কে, এবং এটি এখানে ইন্টারজেক্ট করা যথেষ্ট আকর্ষণীয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.96,
  "end": 638.38
 },
 {
  "input": "They did this experiment that was similar to the one with Steve, but where people were given the following description of a fictitious woman named Linda. ",
  "translatedText": "তারা এই পরীক্ষাটি করেছিল যা স্টিভের মতোই ছিল, কিন্তু যেখানে লোকেদের লিন্ডা নামে একটি কাল্পনিক মহিলার নিম্নলিখিত বর্ণনা দেওয়া হয়েছিল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.52,
  "end": 645.72
 },
 {
  "input": "Linda is 31 years old, single, outspoken, and very bright. ",
  "translatedText": "লিন্ডা 31 বছর বয়সী, একক, স্পষ্টভাষী এবং খুব উজ্জ্বল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.4,
  "end": 650.62
 },
 {
  "input": "She majored in philosophy. ",
  "translatedText": "তিনি দর্শনে পড়াশোনা করেছেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 651.14,
  "end": 652.16
 },
 {
  "input": "As a student she was deeply concerned with issues of discrimination and social justice, and also participated in the anti-nuclear demonstrations. ",
  "translatedText": "একজন ছাত্র হিসাবে তিনি বৈষম্য এবং সামাজিক ন্যায়বিচারের বিষয়ে গভীরভাবে উদ্বিগ্ন ছিলেন এবং পারমাণবিক বিরোধী বিক্ষোভেও অংশ নিয়েছিলেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 652.64,
  "end": 659.54
 },
 {
  "input": "After seeing this people were asked what's more likely, 1. ",
  "translatedText": "এটা দেখার পর লোকজনকে জিজ্ঞেস করা হলো কিসের সম্ভাবনা বেশি, ১. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 660.7,
  "end": 664.02
 },
 {
  "input": "That Linda is a bank teller, or 2. ",
  "translatedText": "সেই লিন্ডা একজন ব্যাঙ্ক টেলার, বা ২. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 664.34,
  "end": 666.46
 },
 {
  "input": "That Linda is a bank teller and is active in the feminist movement. ",
  "translatedText": "যে লিন্ডা একজন ব্যাঙ্ক টেলার এবং নারীবাদী আন্দোলনে সক্রিয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.92,
  "end": 669.9
 },
 {
  "input": "85%, 85% of participants said that the latter is more likely than the former, even though the set of bank tellers who are active in the feminist movement is a subset of the set of bank tellers. ",
  "translatedText": "85%, 85% অংশগ্রহণকারীরা বলেছেন যে পরবর্তীটি প্রাক্তনের চেয়ে বেশি, যদিও ব্যাঙ্ক টেলারদের সেট যারা নারীবাদী আন্দোলনে সক্রিয় রয়েছে তারা ব্যাঙ্ক টেলারদের সেটের একটি উপসেট।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.22,
  "end": 683.32
 },
 {
  "input": "It has to be smaller. ",
  "translatedText": "এটা ছোট হতে হবে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 683.56,
  "end": 684.68
 },
 {
  "input": "So that's interesting enough, but what's fascinating is that there's a simple way that you can rephrase the question that dropped this error from 85% to 0. ",
  "translatedText": "তাই এটি যথেষ্ট আকর্ষণীয়, কিন্তু যা আকর্ষণীয় তা হল একটি সহজ উপায় যা আপনি প্রশ্নটিকে পুনরায় ব্যাখ্যা করতে পারেন যা এই ত্রুটিটিকে 85% থেকে 0 এ নামিয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.64,
  "end": 694.1
 },
 {
  "input": "Instead, if participants were told that there are 100 people who fit this description, and then asked to estimate how many of those 100 are bank tellers, and how many are bank tellers active in the feminist movement, nobody makes the error. ",
  "translatedText": "পরিবর্তে, যদি অংশগ্রহণকারীদের বলা হয় যে এই বর্ণনার সাথে মানানসই 100 জন লোক আছে, এবং তারপর অনুমান করতে বলা হয় যে এই 100 জনের মধ্যে কতজন ব্যাঙ্ক টেলার, এবং কতজন ব্যাঙ্ক টেলার নারীবাদী আন্দোলনে সক্রিয়, কেউ ভুল করে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 694.96,
  "end": 708.5
 },
 {
  "input": "Everybody correctly assigns a higher number to the first option than to the second. ",
  "translatedText": "প্রত্যেকে সঠিকভাবে দ্বিতীয় বিকল্পের চেয়ে প্রথম বিকল্পে একটি উচ্চ নম্বর বরাদ্দ করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.5,
  "end": 713.18
 },
 {
  "input": "It's weird, somehow phrases like 40 out of 100 kick our intuitions into gear much more effectively than 40%, much less 0.4, and much less abstractly referencing the idea of something being more or less likely. ",
  "translatedText": "এটা অদ্ভুত, 100টির মধ্যে 40টির মতো বাক্যাংশ আমাদের অন্তর্দৃষ্টিকে 40% এর চেয়ে অনেক বেশি কার্যকরভাবে গিয়ারে লাথি দেয়, অনেক কম 0।4, এবং অনেক কম বিমূর্তভাবে কিছু হওয়ার ধারণাটি কম বা কম হওয়ার সম্ভাবনা উল্লেখ করা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 714.78,
  "end": 728.06
 },
 {
  "input": "That said, representative samples don't easily capture the continuous nature of probability. ",
  "translatedText": "এটি বলেছে, প্রতিনিধি নমুনাগুলি সম্ভাব্যতার অবিচ্ছিন্ন প্রকৃতিকে সহজেই ক্যাপচার করে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.4,
  "end": 734.1
 },
 {
  "input": "So turning to area is a nice alternative, not just because of the continuity, but also because it's way easier to sketch out when you're sitting there pencil and paper puzzling over some problem. ",
  "translatedText": "তাই এলাকাতে যাওয়া একটি চমৎকার বিকল্প, শুধুমাত্র ধারাবাহিকতার কারণে নয়, বরং আপনি যখন সেখানে বসে পেন্সিল এবং কাগজে কিছু সমস্যা নিয়ে ধাঁধাঁ তুলছেন তখন স্কেচ করা সহজ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.1,
  "end": 744.04
 },
 {
  "input": "People often think about probability as being the study of uncertainty, and that is of course how it's applied in science, but the actual math of probability, where all the formulas come from, is just the math of proportions, and in that context turning to geometry is exceedingly helpful. ",
  "translatedText": "লোকেরা প্রায়শই সম্ভাব্যতাকে অনিশ্চয়তার অধ্যয়ন হিসাবে মনে করে, এবং এটি অবশ্যই বিজ্ঞানে কীভাবে প্রয়োগ করা হয়, তবে সম্ভাব্যতার প্রকৃত গণিত, যেখানে সমস্ত সূত্র আসে, কেবলমাত্র অনুপাতের গণিত, এবং সেই প্রসঙ্গে মোড় নেয় জ্যামিতি অত্যন্ত সহায়ক।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 745.22,
  "end": 761.02
 },
 {
  "input": "I mean, take a look at Bayes' theorem as a statement about proportions, whether that's proportions of people, of areas, whatever. ",
  "translatedText": "আমি বলতে চাচ্ছি, অনুপাত সম্পর্কে একটি বিবৃতি হিসাবে বেইসের উপপাদ্যটি দেখুন, তা মানুষের অনুপাত, এলাকার, যাই হোক না কেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 764.26,
  "end": 770.72
 },
 {
  "input": "Once you digest what it's saying, it's actually kind of obvious. ",
  "translatedText": "একবার আপনি এটি যা বলছে তা হজম করলে, এটি আসলে এক ধরণের সুস্পষ্ট।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 771.3,
  "end": 774.46
 },
 {
  "input": "Both sides tell you to look at the cases where the evidence is true, and then to consider the proportion of those cases where the hypothesis is also true. ",
  "translatedText": "উভয় পক্ষই আপনাকে বলে যে ক্ষেত্রে প্রমাণগুলি সত্য তা দেখতে, এবং তারপর সেই মামলাগুলির অনুপাত বিবেচনা করতে যেখানে অনুমানটিও সত্য।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 775.04,
  "end": 782.72
 },
 {
  "input": "That's it, that's all it's saying. ",
  "translatedText": "এটাই, এইটুকুই বলছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.24,
  "end": 784.64
 },
 {
  "input": "The right-hand side just spells out how to compute it. ",
  "translatedText": "ডান-হাতের দিকটি কেবল এটি কীভাবে গণনা করতে হয় তা বানান করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 784.86,
  "end": 786.9
 },
 {
  "input": "What's noteworthy is that such a straightforward fact about proportions can become hugely significant for science, for artificial intelligence, and really any situation where you want to quantify belief. ",
  "translatedText": "কি লক্ষণীয় যে অনুপাত সম্পর্কে এই ধরনের একটি সরল সত্য বিজ্ঞানের জন্য, কৃত্রিম বুদ্ধিমত্তার জন্য এবং সত্যিই যে কোনও পরিস্থিতি যেখানে আপনি বিশ্বাসের পরিমাণ নির্ধারণ করতে চান তার জন্য অত্যন্ত তাৎপর্যপূর্ণ হয়ে উঠতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 787.54,
  "end": 797.92
 },
 {
  "input": "I hope to give you a better glimpse of that as we get into more examples. ",
  "translatedText": "আমরা আরো উদাহরণ পেতে আপনি যে একটি ভাল আভাস দিতে আশা করি. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.54,
  "end": 801.42
 },
 {
  "input": "But before more examples, we have a little bit of unfinished business with Steve. ",
  "translatedText": "কিন্তু আরও উদাহরণের আগে, স্টিভের সাথে আমাদের কিছুটা অসমাপ্ত ব্যবসা আছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.38,
  "end": 805.74
 },
 {
  "input": "As I mentioned, some psychologists debate Kahneman and Tversky's conclusion that the rational thing to do is to bring to mind the ratio of farmers to librarians. ",
  "translatedText": "আমি যেমন উল্লেখ করেছি, কিছু মনোবিজ্ঞানী কাহনেম্যান এবং টোভারস্কির উপসংহার নিয়ে বিতর্ক করেছেন যে যৌক্তিক জিনিসটি হল লাইব্রেরিয়ানদের সাথে কৃষকদের অনুপাতকে মনে রাখা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.48,
  "end": 814.8
 },
 {
  "input": "They complain that the context is ambiguous. ",
  "translatedText": "তারা অভিযোগ করেন যে প্রসঙ্গটি অস্পষ্ট।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 815.14,
  "end": 817.26
 },
 {
  "input": "I mean, who is Steve, exactly? ",
  "translatedText": "মানে, স্টিভ কে, ঠিক? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.92,
  "end": 819.84
 },
 {
  "input": "Should you expect that he's a randomly sampled American? ",
  "translatedText": "আপনি কি আশা করা উচিত যে তিনি একজন এলোমেলোভাবে নমুনাযুক্ত আমেরিকান? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 819.84,
  "end": 822.66
 },
 {
  "input": "Or would you be better to assume that he's a friend of the two psychologists interrogating you? ",
  "translatedText": "অথবা আপনি কি ধরে নিবেন যে তিনি আপনাকে জিজ্ঞাসাবাদ করছেন এমন দুই মনোবিজ্ঞানীর বন্ধু? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 823.26,
  "end": 827.0
 },
 {
  "input": "Or maybe that he's someone you're personally likely to know? ",
  "translatedText": "অথবা হতে পারে যে তিনি এমন একজন যাকে আপনি ব্যক্তিগতভাবে জানেন? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 827.22,
  "end": 829.74
 },
 {
  "input": "This assumption determines the prior. ",
  "translatedText": "এই অনুমান পূর্ববর্তী নির্ধারণ করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 830.42,
  "end": 832.4
 },
 {
  "input": "I for one run into way more librarians in a given month than I do farmers. ",
  "translatedText": "একটি নির্দিষ্ট মাসে আমি কৃষকদের চেয়ে অনেক বেশি গ্রন্থাগারিকের কাছে দৌড়াচ্ছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 832.96,
  "end": 836.68
 },
 {
  "input": "Needless to say, the probability of a librarian or a farmer fitting this description is highly open to interpretation. ",
  "translatedText": "বলা বাহুল্য, একজন গ্রন্থাগারিক বা একজন কৃষকের এই বর্ণনাটি উপযুক্ত হওয়ার সম্ভাবনা ব্যাখ্যার জন্য অত্যন্ত উন্মুক্ত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.5,
  "end": 843.52
 },
 {
  "input": "For our purposes, understanding the math, what I want to emphasize is that any question worth debating here can be pictured in the context of the diagram. ",
  "translatedText": "আমাদের উদ্দেশ্যে, গণিত বোঝার জন্য, আমি যে বিষয়টির উপর জোর দিতে চাই তা হল এখানে বিতর্ক করার মতো যেকোন প্রশ্ন চিত্রের প্রেক্ষাপটে চিত্রিত করা যেতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 844.44,
  "end": 852.3
 },
 {
  "input": "Questions about the context shift around the prior, and questions about the personalities and stereotypes shift around the relevant likelihoods. ",
  "translatedText": "প্রসঙ্গ পরিবর্তন সম্পর্কে প্রশ্নগুলি পূর্বের চারপাশে এবং ব্যক্তিত্ব এবং স্টেরিওটাইপ সম্পর্কে প্রশ্নগুলি প্রাসঙ্গিক সম্ভাবনার চারপাশে স্থানান্তরিত হয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 853.0,
  "end": 860.58
 },
 {
  "input": "All that said, whether or not you buy this particular experiment, the ultimate point that evidence should not determine beliefs, but update them, is worth tattooing in your brain. ",
  "translatedText": "যা বলেছে, আপনি এই বিশেষ পরীক্ষাটি কিনুন বা না করুন, চূড়ান্ত বিন্দু যে প্রমাণগুলি বিশ্বাসগুলি নির্ধারণ করা উচিত নয়, তবে সেগুলিকে আপডেট করা উচিত, আপনার মস্তিষ্কে ট্যাটু করা মূল্যবান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.1,
  "end": 871.0
 },
 {
  "input": "I'm in no position to say whether this does or does not run against natural human instinct. ",
  "translatedText": "এটি প্রাকৃতিক মানবিক প্রবৃত্তির বিরুদ্ধে কাজ করে বা না করে তা বলার মতো কোনো অবস্থানে নই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.8,
  "end": 876.5
 },
 {
  "input": "We'll leave that to the psychologists. ",
  "translatedText": "আমরা এটি মনোবিজ্ঞানীদের উপর ছেড়ে দেব।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.5,
  "end": 878.24
 },
 {
  "input": "What's more interesting to me is how we can reprogram our intuition to authentically reflect the implications of math, and bringing to mind the right image can often do just that. ",
  "translatedText": "আমার কাছে আরও আকর্ষণীয় বিষয় হল কিভাবে আমরা গণিতের প্রভাবকে প্রামাণিকভাবে প্রতিফলিত করার জন্য আমাদের অন্তর্দৃষ্টিকে পুনরায় প্রোগ্রাম করতে পারি এবং সঠিক চিত্রটি মনে রাখা প্রায়শই এটি করতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 878.92,
  "end": 888.06
 }
]