[
 {
  "input": "The goal is for you to come away from this video understanding one of the most important formulas in all of probability, Bayes' theorem.",
  "translatedText": "목표는 여러분이 이 비디오를 통해 확률의 가장 중요한 공식 중 하나인 베이즈 정리를 이해하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.84
 },
 {
  "input": "This formula is central to scientific discovery, it's a core tool in machine learning and AI, and it's even been used for treasure hunting, when in the 1980s a small team led by Tommy Thompson, and I'm not making up that name, used Bayesian search tactics to help uncover a ship that had sunk a century and a half earlier, and the ship was carrying what in today's terms amounts to $700 million worth of gold.",
  "translatedText": "이 공식은 과학적 발견의 핵심이며 기계 학습과 AI의 핵심 도구이며 심지어 보물 찾기에도 사용되었습니다. 1980년대에 Tommy Thompson이 이끄는 소규모 팀이 사용했습니다. 150년 전에 침몰한 배를 발견하는 데 도움이 되는 베이지안 수색 전술은 오늘날의 가치로 7억 달러 상당의 금을 운반하고 있었습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 7.48,
  "end": 30.74
 },
 {
  "input": "So it's a formula worth understanding, but of course there are multiple different levels of possible understanding.",
  "translatedText": "따라서 이는 이해할 가치가 있는 공식이지만, 물론 가능한 이해 수준에는 여러 가지가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.34,
  "end": 37.04
 },
 {
  "input": "At the simplest there's just knowing what each one of the parts means, so that you can plug in numbers.",
  "translatedText": "가장 간단하게는 각 부품의 의미를 알고 숫자를 연결하면 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 37.6,
  "end": 42.04
 },
 {
  "input": "Then there's understanding why it's true, and later I'm going to show you a certain diagram that's helpful for rediscovering this formula on the fly as needed.",
  "translatedText": "그런 다음 그것이 왜 사실인지 이해하고 나중에 필요에 따라 이 공식을 즉석에서 재발견하는 데 도움이 되는 특정 다이어그램을 보여 드리겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.76,
  "end": 50.58
 },
 {
  "input": "But maybe the most important level is being able to recognize when you need to use it.",
  "translatedText": "하지만 아마도 가장 중요한 수준은 언제 사용해야 하는지를 인식하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.24,
  "end": 55.54
 },
 {
  "input": "And with the goal of gaining a deeper understanding, you and I are going to tackle these in reverse order.",
  "translatedText": "그리고 더 깊은 이해를 얻기 위해 당신과 나는 이 문제들을 역순으로 다룰 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.54,
  "end": 60.56
 },
 {
  "input": "So before dissecting the formula or explaining the visual that makes it obvious, I'd like to tell you about a man named Steve.",
  "translatedText": "따라서 공식을 분석하거나 공식을 명백하게 보여주는 시각적인 내용을 설명하기 전에 Steve라는 사람에 대해 말씀드리고 싶습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.02,
  "end": 66.86
 },
 {
  "input": "Listen carefully now.",
  "translatedText": "이제 잘 들어보세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.32,
  "end": 68.72
 },
 {
  "input": "Steve is very shy and withdrawn, invariably helpful but with very little interest in people or the world of reality.",
  "translatedText": "스티브는 매우 수줍어하고 내성적이며 변함없이 도움을 주지만 사람이나 현실 세계에는 거의 관심이 없습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.74,
  "end": 79.16
 },
 {
  "input": "A meek and tidy soul, he has a need for order and structure, and a passion for detail.",
  "translatedText": "온유하고 깔끔한 영혼을 지닌 그는 질서와 구조에 대한 욕구와 세부 사항에 대한 열정을 가지고 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.74,
  "end": 84.1
 },
 {
  "input": "Which of the following do you find more likely?",
  "translatedText": "다음 중 어느 것이 더 가능성이 있다고 생각하시나요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 84.62,
  "end": 86.78
 },
 {
  "input": "Steve is a librarian, or Steve is a farmer?",
  "translatedText": "스티브는 사서인가요, 아니면 스티브는 농부인가요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.2,
  "end": 90.38
 },
 {
  "input": "Some of you may recognize this as an example from a study conducted by the two psychologists Daniel Kahneman and Amos Tversky.",
  "translatedText": "여러분 중 일부는 이것을 심리학자 Daniel Kahneman과 Amos Tversky가 실시한 연구의 사례로 인식할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.4,
  "end": 97.44
 },
 {
  "input": "Their work was a big deal, it won a Nobel Prize, and it's been popularized many times over in books like Kahneman's Thinking Fast and Slow, or Michael Lewis's The Undoing Project.",
  "translatedText": "그들의 작업은 대단한 일이었으며 노벨상을 수상했으며 Kahneman의 Thinking Fast and Slow 또는 Michael Lewis의 The Undoing Project와 같은 책을 통해 여러 번 대중화되었습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.2,
  "end": 106.56
 },
 {
  "input": "What they researched was human judgments, with a frequent focus on when these judgments irrationally contradict what the laws of probability suggest they should be.",
  "translatedText": "그들이 연구한 것은 인간의 판단이었고, 이러한 판단이 확률의 법칙이 제안하는 것과 비합리적으로 모순되는 경우에 자주 초점을 맞췄습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.42,
  "end": 115.78
 },
 {
  "input": "The example with Steve, our maybe librarian, maybe farmer, illustrates one specific type of irrationality, or maybe I should say alleged irrationality, there are people who debate the conclusion here, but more on that later on.",
  "translatedText": "우리의 사서이자 농부일 수도 있는 스티브의 예는 특정 유형의 비합리성을 보여줍니다. 또는 비합리성이라고 주장해야 할 수도 있습니다. 여기서 결론에 대해 토론하는 사람들이 있지만 이에 대해서는 나중에 더 자세히 설명합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 116.34,
  "end": 129.62
 },
 {
  "input": "According to Kahneman and Tversky, after people are given this description of Steve as a meek and tidy soul, most say he's more likely to be a librarian.",
  "translatedText": "Kahneman과 Tversky에 따르면, 사람들이 Steve를 온유하고 깔끔한 영혼으로 묘사한 후에 대부분의 사람들은 그가 사서가 될 가능성이 더 높다고 말합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.98,
  "end": 138.0
 },
 {
  "input": "After all, these traits line up better with the stereotypical view of a librarian than a farmer.",
  "translatedText": "결국, 이러한 특성은 농부보다는 사서의 고정관념에 더 잘 부합합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 138.0,
  "end": 143.46
 },
 {
  "input": "And according to Kahneman and Tversky, this is irrational.",
  "translatedText": "그리고 Kahneman과 Tversky에 따르면 이것은 비합리적입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.2,
  "end": 146.88
 },
 {
  "input": "The point is not whether people hold correct or biased views about the personalities of librarians and farmers, it's that almost nobody thinks to incorporate information about the ratio of farmers to librarians in their judgments.",
  "translatedText": "중요한 것은 사람들이 사서와 농부의 성격에 대해 올바른 견해를 갖고 있는지 아니면 편향된 견해를 갖고 있는지가 아니라, 농부와 사서의 비율에 대한 정보를 판단에 포함시키려고 생각하는 사람이 거의 없다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.6,
  "end": 160.24
 },
 {
  "input": "In their paper, Kahneman and Tversky said that in the US, that ratio is about 20 to 1.",
  "translatedText": "Kahneman과 Tversky는 그들의 논문에서 미국의 경우 그 비율이 약 20:1이라고 말했습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 160.92,
  "end": 165.18
 },
 {
  "input": "The numbers I could find today put that much higher, but let's stick with the 20 to 1 number, since it's a little easier to illustrate and proves the point as well.",
  "translatedText": "오늘 제가 찾을 수 있는 숫자는 그보다 훨씬 높지만 20:1 숫자를 고수하겠습니다. 설명하기가 조금 더 쉽고 요점도 증명하기 때문입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.58,
  "end": 173.42
 },
 {
  "input": "To be clear, anyone who is asked this question is not expected to have perfect information about the actual statistics of farmers and librarians and their personality traits.",
  "translatedText": "분명히 말하면, 이 질문을 받는 사람은 누구나 농부와 사서의 실제 통계와 그들의 성격 특성에 대한 완벽한 정보를 가질 것으로 기대되지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.28,
  "end": 183.14
 },
 {
  "input": "But the question is whether people even think to consider that ratio enough to at least make a rough estimate.",
  "translatedText": "그러나 문제는 사람들이 최소한 대략적인 추정을 할 수 있을 만큼 그 비율을 고려한다고 생각하는지 여부입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.68,
  "end": 189.22
 },
 {
  "input": "Rationality is not about knowing facts, it's about recognizing which facts are relevant.",
  "translatedText": "합리성은 사실을 아는 것이 아니라 어떤 사실이 관련성이 있는지를 인식하는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.04,
  "end": 194.46
 },
 {
  "input": "Now if you do think to make that estimate, there's a pretty simple way to reason about the question, which, spoiler alert, involves all of the essential reasoning behind Bayes' theorem.",
  "translatedText": "이제 그러한 추정을 하려고 생각한다면 질문에 대해 추론하는 매우 간단한 방법이 있습니다. 스포일러 경고에는 베이즈 정리 뒤에 있는 모든 필수 추론이 포함됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 195.88,
  "end": 203.9
 },
 {
  "input": "You might start by picturing a representative sample of farmers and librarians, say 200 farmers and 10 librarians.",
  "translatedText": "농부와 사서의 대표 표본, 예를 들어 200명의 농부와 10명의 사서를 그리는 것부터 시작할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.66,
  "end": 211.02
 },
 {
  "input": "Then when you hear of this meek and tidy soul description, let's say that your gut instinct is that 40% of librarians would fit that description, and that 10% of farmers would.",
  "translatedText": "그러면 당신이 이 온유하고 깔끔한 영혼에 대한 설명을 들었을 때, 당신의 직감은 사서의 40%가 그 설명에 적합하고 농부의 10%가 그럴 것이라고 생각한다고 가정해 봅시다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.74,
  "end": 221.36
 },
 {
  "input": "If those are your estimates, it would mean that from your sample you would expect about 4 librarians to fit the description, and about 20 farmers to fit that description.",
  "translatedText": "이것이 귀하의 추정치라면 표본에서 약 4명의 사서가 해당 설명에 적합할 것으로 예상하고 약 20명의 농부가 해당 설명에 적합할 것으로 예상한다는 의미입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 222.02,
  "end": 230.24
 },
 {
  "input": "So the probability that a random person among those who fit this description is a librarian is 4 out of 24, or 16.7%.",
  "translatedText": "따라서 이 설명에 맞는 사람 중 임의의 사람이 사서일 확률은 24명 중 4명, 즉 16명입니다.7%.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.02,
  "end": 240.1
 },
 {
  "input": "So even if you think that a librarian is 4 times as likely as a farmer to fit this description, that's not enough to overcome the fact that there are way more farmers.",
  "translatedText": "따라서 사서가 농부에 비해 이 설명에 부합할 가능성이 4배 더 높다고 생각하더라도 농부가 훨씬 더 많다는 사실을 극복하기에는 충분하지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.1,
  "end": 249.02
 },
 {
  "input": "The upshot, and this is the key mantra underlying Bayes' theorem, is that new evidence does not completely determine your beliefs in a vacuum, it should update prior beliefs.",
  "translatedText": "결과, 그리고 이것이 베이즈 정리의 근간을 이루는 핵심 진언은 새로운 증거가 공백 상태에서 당신의 믿음을 완전히 결정하는 것이 아니라 이전 믿음을 업데이트해야 한다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 249.72,
  "end": 259.22
 },
 {
  "input": "If this line of reasoning makes sense to you, the way that seeing evidence restricts the space of possibilities and the ratio you need to consider after that, then congratulations, you understand the heart of Bayes' theorem.",
  "translatedText": "이러한 추론이 당신에게 이해가 된다면, 증거를 보는 것이 가능성의 공간과 그 후에 고려해야 할 비율을 제한하는 방식이라면, 축하합니다. 당신은 베이즈 정리의 핵심을 이해했습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.12,
  "end": 272.36
 },
 {
  "input": "Maybe the numbers you would estimate would be a little different, but what matters is how you fit the numbers together to update your beliefs based on evidence.",
  "translatedText": "어쩌면 추정할 숫자가 약간 다를 수도 있지만, 중요한 것은 증거를 기반으로 믿음을 업데이트하기 위해 숫자를 어떻게 맞추는가입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.36,
  "end": 280.6
 },
 {
  "input": "Understanding one example is one thing, but see if you can take a minute to generalize everything we just did and write it all down as a formula.",
  "translatedText": "한 가지 예를 이해하는 것도 중요하지만, 방금 수행한 모든 것을 일반화하고 공식으로 모두 적어 보는 데 잠시 시간을 할애할 수 있는지 확인하세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.08,
  "end": 289.74
 },
 {
  "input": "The general situation where Bayes' theorem is relevant is when you have some hypothesis, like Steve is a librarian, and you see some new evidence, say this verbal description of Steve as a meek and tidy soul.",
  "translatedText": "베이즈 정리가 관련되는 일반적인 상황은 Steve가 사서인 것처럼 가설이 있고 Steve를 온유하고 깔끔한 영혼으로 묘사하는 새로운 증거를 볼 때입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 292.32,
  "end": 303.98
 },
 {
  "input": "You want to know the probability that your hypothesis holds given that the evidence is true.",
  "translatedText": "증거가 참일 때 가설이 성립할 확률을 알고 싶습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.38,
  "end": 309.64
 },
 {
  "input": "In the standard notation, this vertical bar means given that, as in we're restricting our view only to the possibilities where the evidence holds.",
  "translatedText": "표준 표기법에서 이 수직 막대는 증거가 있는 가능성에만 우리의 견해를 제한한다는 의미입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 310.44,
  "end": 318.96
 },
 {
  "input": "Remember the first relevant number we used, the probability that the hypothesis holds before considering any of that new evidence.",
  "translatedText": "우리가 사용한 첫 번째 관련 숫자, 즉 새로운 증거를 고려하기 전에 가설이 유지될 확률을 기억하십시오.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.22,
  "end": 327.34
 },
 {
  "input": "In our example, that was 1 out of 21, and it came from considering the ratio of librarians to farmers in the general population.",
  "translatedText": "우리의 예에서 그것은 21명 중 1명이었고, 이는 일반 인구 중 사서와 농부의 비율을 고려한 결과였습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.72,
  "end": 334.64
 },
 {
  "input": "This number is known as the prior.",
  "translatedText": "이 숫자는 이전 숫자로 알려져 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 335.52,
  "end": 336.98
 },
 {
  "input": "After that, we need to consider the proportion of librarians that fit this description, the probability that we would see the evidence given that the hypothesis is true.",
  "translatedText": "그 후, 우리는 이 설명에 맞는 사서의 비율, 즉 가설이 참이라는 가정하에 증거를 볼 확률을 고려해야 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.02,
  "end": 347.3
 },
 {
  "input": "Again, when you see this vertical bar, it means we're talking about some proportion of a limited part of the total space of possibilities.",
  "translatedText": "다시 말하지만, 이 수직 막대를 보면 전체 가능성 공간 중 제한된 부분의 일부에 대해 이야기하고 있다는 의미입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 354.84
 },
 {
  "input": "In this case, that limited part is the left side, where the hypothesis holds.",
  "translatedText": "이 경우 제한된 부분은 가설이 성립하는 왼쪽입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.32,
  "end": 359.3
 },
 {
  "input": "In the context of Bayes' theorem, this value also has a special name, it's called the likelihood.",
  "translatedText": "베이즈 정리의 맥락에서 이 값은 우도라고 불리는 특별한 이름도 갖습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 359.96,
  "end": 364.64
 },
 {
  "input": "Similarly, you need to know how much of the other side of the space includes the evidence, the probability of seeing the evidence given that the hypothesis isn't true.",
  "translatedText": "마찬가지로 공간 반대편에 증거가 얼마나 포함되어 있는지, 즉 가설이 사실이 아닐 때 증거를 볼 확률을 알아야 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.7,
  "end": 373.56
 },
 {
  "input": "This funny little elbow symbol is commonly used in probability to mean not.",
  "translatedText": "이 재미있는 작은 팔꿈치 기호는 확률적으로 '아니요'를 의미하는 데 일반적으로 사용됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.34,
  "end": 378.42
 },
 {
  "input": "So, with the notation in place, remember what our final answer was.",
  "translatedText": "따라서 표기법을 적용한 후 최종 답변이 무엇인지 기억하세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.86,
  "end": 383.02
 },
 {
  "input": "The probability that our librarian hypothesis is true given the evidence is the total number of librarians fitting the evidence, 4, divided by the total number of people fitting the evidence, 24.",
  "translatedText": "주어진 증거에서 우리의 사서 가설이 참일 확률은 증거에 맞는 총 사서 수 4를 증거에 맞는 총 사람 수 24로 나눈 값입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 383.36,
  "end": 394.88
 },
 {
  "input": "But where did that 4 come from?",
  "translatedText": "그런데 그 4개는 어디서 나온 걸까요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.76,
  "end": 397.18
 },
 {
  "input": "Well, it's the total number of people, times the prior probability of being a librarian, giving us the 10 total librarians, times the probability that one of those fits the evidence.",
  "translatedText": "음, 총 사람 수에 사서가 될 사전 확률을 곱하면 총 10명의 사서가 되고 그 중 한 명이 증거에 부합할 확률을 곱하면 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 397.84,
  "end": 408.42
 },
 {
  "input": "That same number shows up again in the denominator, but we need to add in the rest, the total number of people times the proportion who are not librarians, times the proportion of those who fit the evidence, which in our example gives 20.",
  "translatedText": "동일한 숫자가 분모에 다시 표시되지만 나머지도 더해야 합니다. 총 사람 수에 사서가 아닌 비율을 곱하고 증거에 맞는 사람의 비율을 곱한 것입니다. 이 예에서는 20이 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.22,
  "end": 422.14
 },
 {
  "input": "Now notice the total number of people here, 210, that gets cancelled out, and of course it should, that was just an arbitrary choice made for the sake of illustration.",
  "translatedText": "이제 취소된 총 인원 수인 210명을 주목하세요. 물론 그래야 합니다. 이는 단지 설명을 위해 임의로 선택한 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.22,
  "end": 431.04
 },
 {
  "input": "This leaves us finally with a more abstract representation purely in terms of probabilities, and this, my friends, is Bayes' theorem.",
  "translatedText": "이로써 우리는 순전히 확률 측면에서 보다 추상적인 표현을 얻게 되었으며, 이것이 베이즈의 정리입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 439.22
 },
 {
  "input": "More often, you see this denominator written simply as P of E, the total probability of seeing the evidence, which in our example would be the 24 out of 210.",
  "translatedText": "더 자주, 이 분모는 P/E, 즉 증거를 볼 전체 확률로 간단히 표시되는 것을 볼 수 있으며, 이 예에서는 210개 중 24개입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 440.42,
  "end": 450.46
 },
 {
  "input": "But in practice, to calculate it, you almost always have to break it down into the case where the hypothesis is true, and the one where it isn't.",
  "translatedText": "그러나 실제로 이를 계산하려면 거의 항상 가설이 참인 경우와 그렇지 않은 경우로 나누어야 합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 451.12,
  "end": 458.8
 },
 {
  "input": "Capping things off with one final bit of jargon, this answer is called the posterior, it's your belief about the hypothesis after seeing the evidence.",
  "translatedText": "마지막으로 전문 용어를 사용하여 마무리하면 이 답변을 사후라고 합니다. 이는 증거를 본 후 가설에 대한 귀하의 믿음입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 460.06,
  "end": 468.6
 },
 {
  "input": "Writing it out abstractly might seem more complicated than just thinking through the example directly with a representative sample.",
  "translatedText": "추상적으로 작성하는 것은 대표 샘플을 사용하여 직접 예를 통해 생각하는 것보다 더 복잡해 보일 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 476.5
 },
 {
  "input": "And yeah, it is.",
  "translatedText": "그리고 그렇습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 476.92,
  "end": 478.78
 },
 {
  "input": "Keep in mind though, the value of a formula like this is that it lets you quantify and systematize the idea of changing beliefs.",
  "translatedText": "하지만 이와 같은 공식의 가치는 신념을 바꾸는 아이디어를 정량화하고 체계화할 수 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.2,
  "end": 486.26
 },
 {
  "input": "Scientists use this formula when they're analyzing the extent to which new data validates or invalidates their models.",
  "translatedText": "과학자들은 새로운 데이터가 모델을 검증하거나 무효화하는 정도를 분석할 때 이 공식을 사용합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.94,
  "end": 492.84
 },
 {
  "input": "Programmers will sometimes use it in building artificial intelligence, where at times you want to explicitly and numerically model a machine's belief.",
  "translatedText": "프로그래머는 때때로 기계의 믿음을 명시적이고 수치적으로 모델링하려는 인공 지능을 구축하는 데 이를 사용합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.84,
  "end": 500.64
 },
 {
  "input": "And honestly, just for the way you view yourself and your own opinions and what it takes for your mind to change, Bayes' theorem has a way of reframing how you even think about thought itself.",
  "translatedText": "그리고 솔직히 말해서, 당신이 자신과 자신의 의견을 보는 방식, 그리고 마음이 바뀌는 데 필요한 것이 무엇인지에 대해 베이즈의 정리는 당신이 생각 자체에 대해 생각하는 방식을 재구성하는 방법을 가지고 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 501.4,
  "end": 510.82
 },
 {
  "input": "Putting a formula to it can also be more important as the examples get more and more intricate.",
  "translatedText": "예제가 점점 더 복잡해짐에 따라 공식을 적용하는 것이 더 중요할 수도 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.3,
  "end": 516.34
 },
 {
  "input": "However you write it, I actually encourage you not to try memorizing the formula, but to instead draw out this diagram as needed.",
  "translatedText": "어떻게 작성하든 공식을 외우려고 하지 말고 대신 필요에 따라 이 다이어그램을 그려 보시기 바랍니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.08,
  "end": 524.68
 },
 {
  "input": "It's sort of a distilled version of thinking with a representative sample, where we think with areas instead of counts, which is more flexible and easier to sketch on the fly.",
  "translatedText": "이는 개수 대신 영역으로 생각하는 대표 샘플을 사용한 일종의 증류된 사고 방식으로, 즉석에서 더 유연하고 쉽게 스케치할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.26,
  "end": 533.62
 },
 {
  "input": "Rather than bringing to mind some specific number of examples, like 210, think of the space of all possibilities as a 1x1 square.",
  "translatedText": "210과 같은 특정 수의 예를 염두에 두기보다는 모든 가능성의 공간을 1x1 정사각형으로 생각하십시오.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 534.26,
  "end": 541.38
 },
 {
  "input": "Then any event occupies some subset of this space, and the probability of that event can be thought about as the area of that subset.",
  "translatedText": "그러면 모든 사건은 이 공간의 일부 부분 집합을 차지하고 해당 사건의 확률은 해당 부분 집합의 영역으로 생각할 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 542.12,
  "end": 550.94
 },
 {
  "input": "For example, I like to think of the hypothesis as living in the left part of the square with a width of p of h.",
  "translatedText": "예를 들어, 나는 가설이 너비가 p, h인 정사각형의 왼쪽 부분에 살고 있다고 생각하고 싶습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 551.54,
  "end": 557.66
 },
 {
  "input": "I recognize I'm being a bit repetitive, but when you see evidence, the space of possibilities gets restricted, and the crucial part is that restriction might not be even between the left and the right, so the new probability for the hypothesis is the proportion it occupies in this restricted wonky shape.",
  "translatedText": "제가 약간 반복적이라는 것을 알지만 증거를 보면 가능성의 공간이 제한되고 중요한 부분은 제한이 왼쪽과 오른쪽 사이에도 없을 수도 있다는 것입니다. 따라서 가설에 대한 새로운 확률은 다음과 같습니다. 이 제한된 불안정한 모양에서 차지하는 비율입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 558.32,
  "end": 576.94
 },
 {
  "input": "Now if you think a farmer is just as likely to fit the evidence as a librarian, then the proportion doesn't change, which should make sense, right?",
  "translatedText": "이제 농부가 사서와 마찬가지로 증거에 적합할 가능성이 높다고 생각한다면 그 비율은 변하지 않습니다. 그게 말이 되겠죠?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.64,
  "end": 586.24
 },
 {
  "input": "And evidence doesn't change your beliefs.",
  "translatedText": "그리고 증거는 당신의 믿음을 바꾸지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.26,
  "end": 588.32
 },
 {
  "input": "But when these likelihoods are very different from each other, that's when your belief changes a lot.",
  "translatedText": "그러나 이러한 가능성이 서로 매우 다를 때 믿음이 많이 변합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 588.9,
  "end": 593.48
 },
 {
  "input": "Bayes' theorem spells out what that proportion is, and if you want you can read it geometrically.",
  "translatedText": "베이즈 정리는 그 비율이 무엇인지 설명하며, 원하는 경우 기하학적으로 읽을 수 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.76,
  "end": 600.52
 },
 {
  "input": "Something like p of h times p of e given h, the probability of both the hypothesis and the evidence occurring together, is the width times the height of this little left rectangle, the area of that region.",
  "translatedText": "p의 h 곱하기 p의 e와 같은 h가 주어졌을 때 가설과 증거가 함께 나타날 확률은 너비에 이 작은 왼쪽 직사각형의 높이를 곱한 값, 즉 해당 영역의 면적입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.9,
  "end": 613.08
 },
 {
  "input": "Alright, this is probably a good time to take a step back and consider a few of the broader takeaways about how to make probability more intuitive, beyond Bayes' theorem.",
  "translatedText": "자, 지금은 아마도 한 걸음 물러서서 베이즈 정리를 넘어 확률을 보다 직관적으로 만드는 방법에 대한 몇 가지 더 광범위한 시사점을 고려해 볼 좋은 시기일 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 614.76,
  "end": 623.22
 },
 {
  "input": "First off, notice how the trick of thinking about a representative sample with some specific number of people, like our 210 librarians and farmers, was really helpful.",
  "translatedText": "우선, 210명의 사서와 농부 등 특정 수의 사람들을 대상으로 한 대표 표본을 생각하는 방법이 얼마나 도움이 되었는지 살펴보세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.78,
  "end": 632.4
 },
 {
  "input": "There's actually another Kahneman and Tversky result which is all about this, and it's interesting enough to interject here.",
  "translatedText": "실제로 이것에 관한 또 다른 Kahneman과 Tversky 결과가 있으며 여기에 삽입할 만큼 흥미롭습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.96,
  "end": 638.38
 },
 {
  "input": "They did this experiment that was similar to the one with Steve, but where people were given the following description of a fictitious woman named Linda.",
  "translatedText": "그들은 Steve가 했던 것과 비슷한 실험을 했습니다. 그러나 사람들에게 Linda라는 가상의 여성에 대한 다음과 같은 설명이 주어졌습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.52,
  "end": 645.72
 },
 {
  "input": "Linda is 31 years old, single, outspoken, and very bright.",
  "translatedText": "Linda는 31세이고 독신이며 솔직하고 매우 똑똑합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.4,
  "end": 650.62
 },
 {
  "input": "She majored in philosophy.",
  "translatedText": "그녀는 철학을 전공했습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 651.14,
  "end": 652.16
 },
 {
  "input": "As a student she was deeply concerned with issues of discrimination and social justice, and also participated in the anti-nuclear demonstrations.",
  "translatedText": "그녀는 학생 시절 차별과 사회 정의 문제에 깊은 관심을 갖고 반핵 시위에도 참여했습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 652.64,
  "end": 659.54
 },
 {
  "input": "After seeing this people were asked what's more likely, 1.",
  "translatedText": "이것을 본 후 사람들은 무엇이 더 가능성이 높은지 질문을 받았습니다. 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 660.7,
  "end": 664.02
 },
 {
  "input": "That Linda is a bank teller, or 2.",
  "translatedText": "Linda가 은행원이라는 것, 또는 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 664.34,
  "end": 666.46
 },
 {
  "input": "That Linda is a bank teller and is active in the feminist movement.",
  "translatedText": "Linda는 은행원이며 페미니스트 운동에 적극적으로 참여하고 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.92,
  "end": 669.9
 },
 {
  "input": "85%, 85% of participants said that the latter is more likely than the former, even though the set of bank tellers who are active in the feminist movement is a subset of the set of bank tellers.",
  "translatedText": "85%, 85%의 참가자는 페미니스트 운동에 적극적으로 참여하는 은행원 집합이 은행원 집합의 하위 집합임에도 불구하고 후자가 전자보다 가능성이 더 높다고 말했습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.22,
  "end": 683.32
 },
 {
  "input": "It has to be smaller.",
  "translatedText": "더 작아야합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 683.56,
  "end": 684.68
 },
 {
  "input": "So that's interesting enough, but what's fascinating is that there's a simple way that you can rephrase the question that dropped this error from 85% to 0.",
  "translatedText": "그것만으로도 충분히 흥미롭지만, 흥미로운 점은 이 오류를 85%에서 0으로 낮추는 질문을 간단히 바꿀 수 있는 방법이 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.64,
  "end": 694.1
 },
 {
  "input": "Instead, if participants were told that there are 100 people who fit this description, and then asked to estimate how many of those 100 are bank tellers, and how many are bank tellers active in the feminist movement, nobody makes the error.",
  "translatedText": "대신 참가자들에게 이 설명에 맞는 사람이 100명 있다는 말을 듣고 그 100명 중 은행원이 몇 명인지, 페미니스트 운동에 적극적으로 참여하는 은행원이 몇 명인지 추산해 보라고 하면 아무도 오류를 범하지 않습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 694.96,
  "end": 708.5
 },
 {
  "input": "Everybody correctly assigns a higher number to the first option than to the second.",
  "translatedText": "모든 사람은 두 번째 옵션보다 첫 번째 옵션에 더 높은 숫자를 정확하게 할당합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.5,
  "end": 713.18
 },
 {
  "input": "It's weird, somehow phrases like 40 out of 100 kick our intuitions into gear much more effectively than 40%, much less 0.4, and much less abstractly referencing the idea of something being more or less likely.",
  "translatedText": "이상하게도 100점 만점에 40점이라는 문구는 0보다 훨씬 적은 40%보다 훨씬 더 효과적으로 우리의 직관을 작동시킵니다.4, 그리고 어떤 것이 어느 정도 가능성이 있다는 생각을 추상적으로 언급하는 것은 훨씬 적습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 714.78,
  "end": 728.06
 },
 {
  "input": "That said, representative samples don't easily capture the continuous nature of probability.",
  "translatedText": "즉, 대표 표본은 확률의 연속적 특성을 쉽게 포착하지 못합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.4,
  "end": 734.1
 },
 {
  "input": "So turning to area is a nice alternative, not just because of the continuity, but also because it's way easier to sketch out when you're sitting there pencil and paper puzzling over some problem.",
  "translatedText": "따라서 영역으로 전환하는 것은 연속성 때문만이 아니라 연필과 종이에 앉아 문제를 풀 때 스케치하는 것이 훨씬 더 쉽기 때문에 좋은 대안입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.1,
  "end": 744.04
 },
 {
  "input": "People often think about probability as being the study of uncertainty, and that is of course how it's applied in science, but the actual math of probability, where all the formulas come from, is just the math of proportions, and in that context turning to geometry is exceedingly helpful.",
  "translatedText": "사람들은 확률을 불확실성에 대한 연구라고 생각하는 경우가 많습니다. 물론 이것이 과학에 적용되는 방식이지만, 모든 공식의 근원이 되는 실제 확률 수학은 단지 비율의 수학일 뿐이며 그런 맥락에서 다음과 같이 변합니다. 기하학은 매우 도움이 됩니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 745.22,
  "end": 761.02
 },
 {
  "input": "I mean, take a look at Bayes' theorem as a statement about proportions, whether that's proportions of people, of areas, whatever.",
  "translatedText": "즉, 사람의 비율이든, 면적의 비율이든 관계없이 비율에 대한 설명인 베이즈의 정리를 살펴보세요.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 764.26,
  "end": 770.72
 },
 {
  "input": "Once you digest what it's saying, it's actually kind of obvious.",
  "translatedText": "그것이 말하는 내용을 소화하고 나면 실제로는 분명합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 771.3,
  "end": 774.46
 },
 {
  "input": "Both sides tell you to look at the cases where the evidence is true, and then to consider the proportion of those cases where the hypothesis is also true.",
  "translatedText": "양측 모두 증거가 참인 경우를 살펴보고, 가설도 참인 경우의 비율을 고려하라고 말합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 775.04,
  "end": 782.72
 },
 {
  "input": "That's it, that's all it's saying.",
  "translatedText": "그게 다야, 그게 말하는 전부야.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.24,
  "end": 784.64
 },
 {
  "input": "The right-hand side just spells out how to compute it.",
  "translatedText": "오른쪽에는 계산 방법이 설명되어 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 784.86,
  "end": 786.9
 },
 {
  "input": "What's noteworthy is that such a straightforward fact about proportions can become hugely significant for science, for artificial intelligence, and really any situation where you want to quantify belief.",
  "translatedText": "주목할만한 점은 비율에 대한 이러한 간단한 사실이 과학, 인공 지능 및 실제로 믿음을 정량화하려는 모든 상황에서 매우 중요할 수 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 787.54,
  "end": 797.92
 },
 {
  "input": "I hope to give you a better glimpse of that as we get into more examples.",
  "translatedText": "더 많은 예제를 살펴보면서 이에 대해 더 잘 엿볼 수 있기를 바랍니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.54,
  "end": 801.42
 },
 {
  "input": "But before more examples, we have a little bit of unfinished business with Steve.",
  "translatedText": "그러나 더 많은 예를 보기 전에 Steve와의 아직 끝나지 않은 작업이 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.38,
  "end": 805.74
 },
 {
  "input": "As I mentioned, some psychologists debate Kahneman and Tversky's conclusion that the rational thing to do is to bring to mind the ratio of farmers to librarians.",
  "translatedText": "내가 언급한 것처럼 일부 심리학자들은 농부와 사서의 비율을 염두에 두는 것이 합리적이라는 Kahneman과 Tversky의 결론에 대해 논쟁을 벌이고 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.48,
  "end": 814.8
 },
 {
  "input": "They complain that the context is ambiguous.",
  "translatedText": "그들은 맥락이 모호하다고 불평한다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 815.14,
  "end": 817.26
 },
 {
  "input": "I mean, who is Steve, exactly?",
  "translatedText": "내 말은, 스티브가 정확히 누구죠?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.92,
  "end": 819.84
 },
 {
  "input": "Should you expect that he's a randomly sampled American?",
  "translatedText": "그가 무작위로 추출된 미국인이라고 예상해야 합니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 819.84,
  "end": 822.66
 },
 {
  "input": "Or would you be better to assume that he's a friend of the two psychologists interrogating you?",
  "translatedText": "아니면 그가 당신을 심문하는 두 심리학자의 친구라고 가정하는 것이 더 낫습니까?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 823.26,
  "end": 827.0
 },
 {
  "input": "Or maybe that he's someone you're personally likely to know?",
  "translatedText": "아니면 그 사람이 당신이 개인적으로 알 가능성이 있는 사람일까요?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 827.22,
  "end": 829.74
 },
 {
  "input": "This assumption determines the prior.",
  "translatedText": "이 가정이 사전을 결정합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 830.42,
  "end": 832.4
 },
 {
  "input": "I for one run into way more librarians in a given month than I do farmers.",
  "translatedText": "나는 특정 달에 농부보다 사서를 훨씬 더 많이 만난다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 832.96,
  "end": 836.68
 },
 {
  "input": "Needless to say, the probability of a librarian or a farmer fitting this description is highly open to interpretation.",
  "translatedText": "말할 필요도 없이, 사서나 농부가 이 설명에 적합할 확률은 해석의 여지가 매우 높습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.5,
  "end": 843.52
 },
 {
  "input": "For our purposes, understanding the math, what I want to emphasize is that any question worth debating here can be pictured in the context of the diagram.",
  "translatedText": "우리의 목적인 수학 이해를 위해 제가 강조하고 싶은 것은 여기에서 토론할 가치가 있는 모든 질문이 다이어그램의 맥락에서 그려질 수 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 844.44,
  "end": 852.3
 },
 {
  "input": "Questions about the context shift around the prior, and questions about the personalities and stereotypes shift around the relevant likelihoods.",
  "translatedText": "맥락에 대한 질문은 이전을 중심으로 바뀌고 성격과 고정관념에 대한 질문은 관련 가능성을 중심으로 이동합니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 853.0,
  "end": 860.58
 },
 {
  "input": "All that said, whether or not you buy this particular experiment, the ultimate point that evidence should not determine beliefs, but update them, is worth tattooing in your brain.",
  "translatedText": "즉, 당신이 이 특정 실험을 구매하든 안하든, 증거가 믿음을 결정하는 것이 아니라 업데이트해야 한다는 궁극적인 요점은 당신의 두뇌에 문신을 새길 가치가 있습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.1,
  "end": 871.0
 },
 {
  "input": "I'm in no position to say whether this does or does not run against natural human instinct.",
  "translatedText": "나는 이것이 인간의 자연스러운 본능에 어긋나는지 여부를 말할 수 있는 입장이 아닙니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.8,
  "end": 876.5
 },
 {
  "input": "We'll leave that to the psychologists.",
  "translatedText": "그건 심리학자에게 맡기겠습니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.5,
  "end": 878.24
 },
 {
  "input": "What's more interesting to me is how we can reprogram our intuition to authentically reflect the implications of math, and bringing to mind the right image can often do just that.",
  "translatedText": "나에게 더 흥미로운 점은 수학의 의미를 실제로 반영하도록 직관을 다시 프로그래밍하는 방법이며, 올바른 이미지를 떠올리는 것이 종종 그렇게 할 수 있다는 것입니다.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 878.92,
  "end": 888.06
 }
]