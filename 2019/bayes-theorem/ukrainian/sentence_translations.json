[
 {
  "input": "The goal is for you to come away from this video understanding one of the most important formulas in all of probability, Bayes' theorem.",
  "translatedText": "Мета полягає в тому, щоб ви вийшли з цього відео та зрозуміли одну з найважливіших формул у всій ймовірності, теорему Байєса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 6.84
 },
 {
  "input": "This formula is central to scientific discovery, it's a core tool in machine learning and AI, and it's even been used for treasure hunting, when in the 1980s a small team led by Tommy Thompson, and I'm not making up that name, used Bayesian search tactics to help uncover a ship that had sunk a century and a half earlier, and the ship was carrying what in today's terms amounts to $700 million worth of gold.",
  "translatedText": "Ця формула є центральною для наукових відкриттів, це основний інструмент у машинному навчанні та штучному інтелекті, і її навіть використовували для пошуку скарбів, коли у 1980-х роках невелика команда під керівництвом Томмі Томпсона, я не вигадую це ім’я, використовувала Байєсовська тактика пошуку, щоб допомогти розкрити корабель, який затонув півтора століття тому, і корабель перевозив золото на суму 700 мільйонів доларів США.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 7.48,
  "end": 30.74
 },
 {
  "input": "So it's a formula worth understanding, but of course there are multiple different levels of possible understanding.",
  "translatedText": "Отже, це формула, яку варто зрозуміти, але, звичайно, існує кілька різних рівнів можливого розуміння.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.34,
  "end": 37.04
 },
 {
  "input": "At the simplest there's just knowing what each one of the parts means, so that you can plug in numbers.",
  "translatedText": "У найпростішому – просто знати, що означає кожна з частин, щоб можна було вводити цифри.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 37.6,
  "end": 42.04
 },
 {
  "input": "Then there's understanding why it's true, and later I'm going to show you a certain diagram that's helpful for rediscovering this formula on the fly as needed.",
  "translatedText": "Потім з’являється розуміння, чому це правда, і пізніше я покажу вам певну діаграму, яка допоможе швидко відкрити цю формулу за потреби.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.76,
  "end": 50.58
 },
 {
  "input": "But maybe the most important level is being able to recognize when you need to use it.",
  "translatedText": "Але, можливо, найважливішим рівнем є здатність розпізнавати, коли вам потрібно його використовувати.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.24,
  "end": 55.54
 },
 {
  "input": "And with the goal of gaining a deeper understanding, you and I are going to tackle these in reverse order.",
  "translatedText": "І з метою глибшого розуміння ми з вами розглянемо це у зворотному порядку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.54,
  "end": 60.56
 },
 {
  "input": "So before dissecting the formula or explaining the visual that makes it obvious, I'd like to tell you about a man named Steve.",
  "translatedText": "Отже, перш ніж розбирати формулу чи пояснювати візуальне зображення, яке робить її очевидною, я хотів би розповісти вам про чоловіка на ім’я Стів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.02,
  "end": 66.86
 },
 {
  "input": "Listen carefully now.",
  "translatedText": "Тепер слухайте уважно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.32,
  "end": 68.72
 },
 {
  "input": "Steve is very shy and withdrawn, invariably helpful but with very little interest in people or the world of reality.",
  "translatedText": "Стів дуже сором'язливий і замкнутий, незмінно допомагає, але його дуже мало цікавлять люди чи світ реальності.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.74,
  "end": 79.16
 },
 {
  "input": "A meek and tidy soul, he has a need for order and structure, and a passion for detail.",
  "translatedText": "Лагідний і охайний, він має потребу в порядку та структурі та пристрасть до деталей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.74,
  "end": 84.1
 },
 {
  "input": "Which of the following do you find more likely?",
  "translatedText": "Що з наведеного нижче ви вважаєте більш імовірним?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 84.62,
  "end": 86.78
 },
 {
  "input": "Steve is a librarian, or Steve is a farmer?",
  "translatedText": "Стів бібліотекар чи Стів фермер?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.2,
  "end": 90.38
 },
 {
  "input": "Some of you may recognize this as an example from a study conducted by the two psychologists Daniel Kahneman and Amos Tversky.",
  "translatedText": "Дехто з вас може впізнати це як приклад із дослідження, проведеного двома психологами Даніелем Канеманом і Амосом Тверскі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.4,
  "end": 97.44
 },
 {
  "input": "Their work was a big deal, it won a Nobel Prize, and it's been popularized many times over in books like Kahneman's Thinking Fast and Slow, or Michael Lewis's The Undoing Project.",
  "translatedText": "Їхня робота була великою справою, вона отримала Нобелівську премію, і її багато разів популяризували в таких книгах, як «Мислення швидко і повільно» Канемана або «Проект руйнування» Майкла Льюїса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 98.2,
  "end": 106.56
 },
 {
  "input": "What they researched was human judgments, with a frequent focus on when these judgments irrationally contradict what the laws of probability suggest they should be.",
  "translatedText": "Вони досліджували людські судження, часто зосереджуючись на тому, коли ці судження ірраціонально суперечать тому, що вони мають бути згідно із законами ймовірності.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.42,
  "end": 115.78
 },
 {
  "input": "The example with Steve, our maybe librarian, maybe farmer, illustrates one specific type of irrationality, or maybe I should say alleged irrationality, there are people who debate the conclusion here, but more on that later on.",
  "translatedText": "Приклад зі Стівом, нашим, можливо, бібліотекарем, можливо, фермером, ілюструє один конкретний тип ірраціональності, або, можливо, я повинен сказати, нібито ірраціональність, є люди, які обговорюють цей висновок, але про це пізніше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 116.34,
  "end": 129.62
 },
 {
  "input": "According to Kahneman and Tversky, after people are given this description of Steve as a meek and tidy soul, most say he's more likely to be a librarian.",
  "translatedText": "За словами Канемана та Тверскі, після того, як люди охарактеризували Стіва як лагідну та охайну душу, більшість каже, що він швидше за все буде бібліотекарем.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 129.98,
  "end": 138.0
 },
 {
  "input": "After all, these traits line up better with the stereotypical view of a librarian than a farmer.",
  "translatedText": "Зрештою, ці риси краще узгоджуються зі стереотипним уявленням про бібліотекаря, аніж про фермера.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 138.0,
  "end": 143.46
 },
 {
  "input": "And according to Kahneman and Tversky, this is irrational.",
  "translatedText": "А на думку Канемана і Тверського, це нераціонально.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.2,
  "end": 146.88
 },
 {
  "input": "The point is not whether people hold correct or biased views about the personalities of librarians and farmers, it's that almost nobody thinks to incorporate information about the ratio of farmers to librarians in their judgments.",
  "translatedText": "Справа не в тому, чи дотримуються люди правильних чи упереджених поглядів на особистості бібліотекарів і фермерів, а в тому, що майже ніхто не думає включати інформацію про співвідношення фермерів і бібліотекарів у своїх судженнях.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.6,
  "end": 160.24
 },
 {
  "input": "In their paper, Kahneman and Tversky said that in the US, that ratio is about 20 to 1.",
  "translatedText": "У своїй статті Канеман і Тверскі сказали, що в США це співвідношення приблизно 20 до 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 160.92,
  "end": 165.18
 },
 {
  "input": "The numbers I could find today put that much higher, but let's stick with the 20 to 1 number, since it's a little easier to illustrate and proves the point as well.",
  "translatedText": "Числа, які я зміг знайти сьогодні, свідчать про те, що вони набагато вищі, але давайте залишимося з числом 20 до 1, оскільки це трохи легше проілюструвати, а також підтверджує тезу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 165.58,
  "end": 173.42
 },
 {
  "input": "To be clear, anyone who is asked this question is not expected to have perfect information about the actual statistics of farmers and librarians and their personality traits.",
  "translatedText": "Щоб було зрозуміло, не очікується, що будь-хто, кому задають це запитання, матиме досконалу інформацію про фактичну статистику фермерів і бібліотекарів та їхні риси особистості.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.28,
  "end": 183.14
 },
 {
  "input": "But the question is whether people even think to consider that ratio enough to at least make a rough estimate.",
  "translatedText": "Але питання полягає в тому, чи люди навіть думають вважати це співвідношення достатньо, щоб зробити хоча б приблизну оцінку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.68,
  "end": 189.22
 },
 {
  "input": "Rationality is not about knowing facts, it's about recognizing which facts are relevant.",
  "translatedText": "Раціональність — це не знання фактів, а визнання того, які факти є релевантними.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 190.04,
  "end": 194.46
 },
 {
  "input": "Now if you do think to make that estimate, there's a pretty simple way to reason about the question, which, spoiler alert, involves all of the essential reasoning behind Bayes' theorem.",
  "translatedText": "Тепер, якщо ви все-таки думаєте зробити таку оцінку, є досить простий спосіб міркувати над запитанням, який, спойлер, включає всі основні міркування, що стоять за теоремою Байєса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 195.88,
  "end": 203.9
 },
 {
  "input": "You might start by picturing a representative sample of farmers and librarians, say 200 farmers and 10 librarians.",
  "translatedText": "Ви можете почати з зображення репрезентативної вибірки фермерів і бібліотекарів, скажімо, 200 фермерів і 10 бібліотекарів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.66,
  "end": 211.02
 },
 {
  "input": "Then when you hear of this meek and tidy soul description, let's say that your gut instinct is that 40% of librarians would fit that description, and that 10% of farmers would.",
  "translatedText": "Потім, коли ви чуєте про цей опис лагідної та охайної душі, припустімо, що ваш інстинкт підказує, що 40% бібліотекарів підійдуть під цей опис, а 10% фермерів підійдуть.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.74,
  "end": 221.36
 },
 {
  "input": "If those are your estimates, it would mean that from your sample you would expect about 4 librarians to fit the description, and about 20 farmers to fit that description.",
  "translatedText": "Якщо це ваші оцінки, це означатиме, що з вашої вибірки ви очікуєте, що приблизно 4 бібліотекарі відповідатимуть цьому опису та приблизно 20 фермерів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 222.02,
  "end": 230.24
 },
 {
  "input": "So the probability that a random person among those who fit this description is a librarian is 4 out of 24, or 16.7%.",
  "translatedText": "Таким чином, ймовірність того, що випадкова людина серед тих, хто відповідає цьому опису, є бібліотекарем, становить 4 з 24, або 16.7%.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.02,
  "end": 240.1
 },
 {
  "input": "So even if you think that a librarian is 4 times as likely as a farmer to fit this description, that's not enough to overcome the fact that there are way more farmers.",
  "translatedText": "Отже, навіть якщо ви думаєте, що бібліотекар у 4 рази частіше відповідає цьому опису, ніж фермер, цього недостатньо, щоб подолати той факт, що фермерів набагато більше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.1,
  "end": 249.02
 },
 {
  "input": "The upshot, and this is the key mantra underlying Bayes' theorem, is that new evidence does not completely determine your beliefs in a vacuum, it should update prior beliefs.",
  "translatedText": "Підсумок, і це ключова мантра, яка лежить в основі теореми Байєса, полягає в тому, що нові докази не повністю визначають ваші переконання у вакуумі, вони повинні оновити попередні переконання.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 249.72,
  "end": 259.22
 },
 {
  "input": "If this line of reasoning makes sense to you, the way that seeing evidence restricts the space of possibilities and the ratio you need to consider after that, then congratulations, you understand the heart of Bayes' theorem.",
  "translatedText": "Якщо ця лінія міркувань має сенс для вас, як те, що бачення доказів обмежує простір можливостей і співвідношення, яке вам потрібно розглянути після цього, тоді вітаємо, ви розумієте суть теореми Байєса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.12,
  "end": 272.36
 },
 {
  "input": "Maybe the numbers you would estimate would be a little different, but what matters is how you fit the numbers together to update your beliefs based on evidence.",
  "translatedText": "Можливо, цифри, які ви б оцінили, будуть дещо іншими, але важливо те, як ви зіставляєте цифри разом, щоб оновити свої переконання на основі доказів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.36,
  "end": 280.6
 },
 {
  "input": "Understanding one example is one thing, but see if you can take a minute to generalize everything we just did and write it all down as a formula.",
  "translatedText": "Зрозуміти один приклад — це одне, але подивіться, чи можете ви витратити хвилину, щоб узагальнити все, що ми щойно зробили, і записати це як формулу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.08,
  "end": 289.74
 },
 {
  "input": "The general situation where Bayes' theorem is relevant is when you have some hypothesis, like Steve is a librarian, and you see some new evidence, say this verbal description of Steve as a meek and tidy soul.",
  "translatedText": "Загальна ситуація, коли теорема Байєса доречна, коли у вас є якась гіпотеза, наприклад, Стів бібліотекар, і ви бачите нові докази, скажімо, словесний опис Стіва як лагідної та охайної душі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 292.32,
  "end": 303.98
 },
 {
  "input": "You want to know the probability that your hypothesis holds given that the evidence is true.",
  "translatedText": "Ви хочете знати ймовірність того, що ваша гіпотеза справедлива за умови, що докази правдиві.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.38,
  "end": 309.64
 },
 {
  "input": "In the standard notation, this vertical bar means given that, as in we're restricting our view only to the possibilities where the evidence holds.",
  "translatedText": "У стандартній нотації ця вертикальна риска означає, що ми обмежуємо свій погляд лише тими можливостями, які є доказами.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 310.44,
  "end": 318.96
 },
 {
  "input": "Remember the first relevant number we used, the probability that the hypothesis holds before considering any of that new evidence.",
  "translatedText": "Запам'ятайте перше релевантне число, яке ми використали, ймовірність того, що гіпотеза справедлива, перш ніж розглядати будь-які з цих нових доказів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.22,
  "end": 327.34
 },
 {
  "input": "In our example, that was 1 out of 21, and it came from considering the ratio of librarians to farmers in the general population.",
  "translatedText": "У нашому прикладі це було 1 з 21, і це було отримано з розгляду співвідношення бібліотекарів і фермерів у загальній сукупності.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.72,
  "end": 334.64
 },
 {
  "input": "This number is known as the prior.",
  "translatedText": "Цей номер відомий як попередній.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 335.52,
  "end": 336.98
 },
 {
  "input": "After that, we need to consider the proportion of librarians that fit this description, the probability that we would see the evidence given that the hypothesis is true.",
  "translatedText": "Після цього нам потрібно розглянути частку бібліотекарів, які відповідають цьому опису, ймовірність того, що ми побачимо докази, якщо гіпотеза вірна.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.02,
  "end": 347.3
 },
 {
  "input": "Again, when you see this vertical bar, it means we're talking about some proportion of a limited part of the total space of possibilities.",
  "translatedText": "Знову ж таки, коли ви бачите цю вертикальну смужку, це означає, що ми говоримо про певну частку обмеженої частини загального простору можливостей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.1,
  "end": 354.84
 },
 {
  "input": "In this case, that limited part is the left side, where the hypothesis holds.",
  "translatedText": "У цьому випадку обмеженою частиною є ліва сторона, де виконується гіпотеза.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.32,
  "end": 359.3
 },
 {
  "input": "In the context of Bayes' theorem, this value also has a special name, it's called the likelihood.",
  "translatedText": "У контексті теореми Байєса це значення також має спеціальну назву, воно називається правдоподібністю.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 359.96,
  "end": 364.64
 },
 {
  "input": "Similarly, you need to know how much of the other side of the space includes the evidence, the probability of seeing the evidence given that the hypothesis isn't true.",
  "translatedText": "Так само вам потрібно знати, яка частина іншого боку простору містить докази, ймовірність побачити докази, якщо гіпотеза не відповідає дійсності.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 365.7,
  "end": 373.56
 },
 {
  "input": "This funny little elbow symbol is commonly used in probability to mean not.",
  "translatedText": "Цей кумедний маленький символ ліктя зазвичай використовується для ймовірності, що означає ні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.34,
  "end": 378.42
 },
 {
  "input": "So, with the notation in place, remember what our final answer was.",
  "translatedText": "Отже, з позначеннями на місці запам’ятайте, якою була наша остаточна відповідь.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.86,
  "end": 383.02
 },
 {
  "input": "The probability that our librarian hypothesis is true given the evidence is the total number of librarians fitting the evidence, 4, divided by the total number of people fitting the evidence, 24.",
  "translatedText": "Імовірність того, що наша бібліотекарська гіпотеза є вірною, враховуючи докази, дорівнює загальній кількості бібліотекарів, які підтверджують докази, 4, поділену на загальну кількість людей, які відповідають доказам, 24.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 383.36,
  "end": 394.88
 },
 {
  "input": "But where did that 4 come from?",
  "translatedText": "Але звідки ця 4?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 395.76,
  "end": 397.18
 },
 {
  "input": "Well, it's the total number of people, times the prior probability of being a librarian, giving us the 10 total librarians, times the probability that one of those fits the evidence.",
  "translatedText": "Ну, це загальна кількість людей, помножена на попередню ймовірність бути бібліотекарем, що дає нам 10 загальних бібліотекарів, помножену на ймовірність того, що один із них відповідає доказам.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 397.84,
  "end": 408.42
 },
 {
  "input": "That same number shows up again in the denominator, but we need to add in the rest, the total number of people times the proportion who are not librarians, times the proportion of those who fit the evidence, which in our example gives 20.",
  "translatedText": "Те саме число знову відображається в знаменнику, але нам потрібно додати решту, загальну кількість людей, помножену на частку тих, хто не є бібліотекарями, на частку тих, хто відповідає свідченням, що в нашому прикладі дає 20.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.22,
  "end": 422.14
 },
 {
  "input": "Now notice the total number of people here, 210, that gets cancelled out, and of course it should, that was just an arbitrary choice made for the sake of illustration.",
  "translatedText": "Тепер зверніть увагу на загальну кількість людей тут, 210, яка була скасована, і, звичайно, так і повинно бути, це був лише довільний вибір, зроблений для ілюстрації.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.22,
  "end": 431.04
 },
 {
  "input": "This leaves us finally with a more abstract representation purely in terms of probabilities, and this, my friends, is Bayes' theorem.",
  "translatedText": "Нарешті це залишає нам більш абстрактне представлення виключно в термінах ймовірностей, і це, мої друзі, теорема Байєса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 431.62,
  "end": 439.22
 },
 {
  "input": "More often, you see this denominator written simply as P of E, the total probability of seeing the evidence, which in our example would be the 24 out of 210.",
  "translatedText": "Частіше ви бачите цей знаменник, записаний просто як P від E, загальної ймовірності побачити докази, яка в нашому прикладі буде 24 із 210.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 440.42,
  "end": 450.46
 },
 {
  "input": "But in practice, to calculate it, you almost always have to break it down into the case where the hypothesis is true, and the one where it isn't.",
  "translatedText": "Але на практиці, щоб обчислити його, вам майже завжди доводиться розбивати його на випадки, коли гіпотеза вірна, і випадки, коли вона ні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 451.12,
  "end": 458.8
 },
 {
  "input": "Capping things off with one final bit of jargon, this answer is called the posterior, it's your belief about the hypothesis after seeing the evidence.",
  "translatedText": "Закінчуючи все останнім жаргоном, ця відповідь називається постериорною, це ваша віра в гіпотезу після перегляду доказів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 460.06,
  "end": 468.6
 },
 {
  "input": "Writing it out abstractly might seem more complicated than just thinking through the example directly with a representative sample.",
  "translatedText": "Написати це абстрактно може здатися складнішим, ніж просто продумати приклад безпосередньо з репрезентативною вибіркою.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 476.5
 },
 {
  "input": "And yeah, it is.",
  "translatedText": "І так, це так.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 476.92,
  "end": 478.78
 },
 {
  "input": "Keep in mind though, the value of a formula like this is that it lets you quantify and systematize the idea of changing beliefs.",
  "translatedText": "Однак майте на увазі, що цінність такої формули полягає в тому, що вона дозволяє кількісно визначити та систематизувати ідею зміни переконань.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.2,
  "end": 486.26
 },
 {
  "input": "Scientists use this formula when they're analyzing the extent to which new data validates or invalidates their models.",
  "translatedText": "Вчені використовують цю формулу, коли аналізують, наскільки нові дані підтверджують або скасовують їхні моделі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 486.94,
  "end": 492.84
 },
 {
  "input": "Programmers will sometimes use it in building artificial intelligence, where at times you want to explicitly and numerically model a machine's belief.",
  "translatedText": "Програмісти іноді використовують його для побудови штучного інтелекту, де іноді потрібно явно чисельно змоделювати переконання машини.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.84,
  "end": 500.64
 },
 {
  "input": "And honestly, just for the way you view yourself and your own opinions and what it takes for your mind to change, Bayes' theorem has a way of reframing how you even think about thought itself.",
  "translatedText": "І, чесно кажучи, лише для того, як ви бачите себе та свою думку, а також те, що потрібно, щоб ваш розум змінився, теорема Байєса має спосіб змінити те, як ви думаєте про саму думку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 501.4,
  "end": 510.82
 },
 {
  "input": "Putting a formula to it can also be more important as the examples get more and more intricate.",
  "translatedText": "Додавання до нього формули також може бути важливішим, оскільки приклади стають дедалі заплутанішими.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.3,
  "end": 516.34
 },
 {
  "input": "However you write it, I actually encourage you not to try memorizing the formula, but to instead draw out this diagram as needed.",
  "translatedText": "Як би ви це не писали, я насправді заохочую вас не намагатися запам’ятати формулу, а натомість намалювати цю діаграму за потреби.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.08,
  "end": 524.68
 },
 {
  "input": "It's sort of a distilled version of thinking with a representative sample, where we think with areas instead of counts, which is more flexible and easier to sketch on the fly.",
  "translatedText": "Це щось на кшталт дистильованої версії мислення з репрезентативною вибіркою, де ми думаємо площами замість підрахунків, що є більш гнучким і його легше малювати на льоту.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.26,
  "end": 533.62
 },
 {
  "input": "Rather than bringing to mind some specific number of examples, like 210, think of the space of all possibilities as a 1x1 square.",
  "translatedText": "Замість того, щоб згадувати якусь конкретну кількість прикладів, наприклад 210, подумайте про простір усіх можливостей як про квадрат 1x1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 534.26,
  "end": 541.38
 },
 {
  "input": "Then any event occupies some subset of this space, and the probability of that event can be thought about as the area of that subset.",
  "translatedText": "Тоді будь-яка подія займає деяку підмножину цього простору, і ймовірність цієї події можна розглядати як площу цієї підмножини.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 542.12,
  "end": 550.94
 },
 {
  "input": "For example, I like to think of the hypothesis as living in the left part of the square with a width of p of h.",
  "translatedText": "Наприклад, мені подобається думати про гіпотезу як про життя в лівій частині квадрата з шириною p = h.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 551.54,
  "end": 557.66
 },
 {
  "input": "I recognize I'm being a bit repetitive, but when you see evidence, the space of possibilities gets restricted, and the crucial part is that restriction might not be even between the left and the right, so the new probability for the hypothesis is the proportion it occupies in this restricted wonky shape.",
  "translatedText": "Я розумію, що я трохи повторююся, але коли ви бачите докази, простір можливостей стає обмеженим, і найважливішим є те, що обмеження може не бути навіть між лівим і правим, тому нова ймовірність для гіпотези полягає в тому, що пропорції, які він займає в цій обмеженій хиткій формі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 558.32,
  "end": 576.94
 },
 {
  "input": "Now if you think a farmer is just as likely to fit the evidence as a librarian, then the proportion doesn't change, which should make sense, right?",
  "translatedText": "Тепер, якщо ви думаєте, що фермер з такою ж імовірністю відповідатиме доказам, як і бібліотекар, то пропорція не зміниться, що мало б мати сенс, чи не так?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.64,
  "end": 586.24
 },
 {
  "input": "And evidence doesn't change your beliefs.",
  "translatedText": "І докази не змінюють ваших переконань.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.26,
  "end": 588.32
 },
 {
  "input": "But when these likelihoods are very different from each other, that's when your belief changes a lot.",
  "translatedText": "Але коли ці ймовірності сильно відрізняються одна від одної, саме тоді ваша віра сильно змінюється.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 588.9,
  "end": 593.48
 },
 {
  "input": "Bayes' theorem spells out what that proportion is, and if you want you can read it geometrically.",
  "translatedText": "Теорема Байєса пояснює, що це за пропорція, і якщо ви хочете, ви можете прочитати її геометрично.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 595.76,
  "end": 600.52
 },
 {
  "input": "Something like p of h times p of e given h, the probability of both the hypothesis and the evidence occurring together, is the width times the height of this little left rectangle, the area of that region.",
  "translatedText": "Щось на кшталт p з h, помноженого на p з e, враховуючи h, ймовірність того, що і гіпотеза, і доказ з’являться разом, є шириною, помноженою на висоту цього маленького лівого прямокутника, площі цієї області.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.9,
  "end": 613.08
 },
 {
  "input": "Alright, this is probably a good time to take a step back and consider a few of the broader takeaways about how to make probability more intuitive, beyond Bayes' theorem.",
  "translatedText": "Гаразд, це, мабуть, гарний час, щоб зробити крок назад і розглянути кілька ширших висновків про те, як зробити ймовірність більш інтуїтивною, крім теореми Байєса.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 614.76,
  "end": 623.22
 },
 {
  "input": "First off, notice how the trick of thinking about a representative sample with some specific number of people, like our 210 librarians and farmers, was really helpful.",
  "translatedText": "По-перше, зауважте, наскільки корисним був трюк подумати про репрезентативну вибірку з певною кількістю людей, наприклад наших 210 бібліотекарів і фермерів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.78,
  "end": 632.4
 },
 {
  "input": "There's actually another Kahneman and Tversky result which is all about this, and it's interesting enough to interject here.",
  "translatedText": "Насправді є ще один результат Канемана і Тверського, який стосується всього цього, і він досить цікавий, щоб вставити тут.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.96,
  "end": 638.38
 },
 {
  "input": "They did this experiment that was similar to the one with Steve, but where people were given the following description of a fictitious woman named Linda.",
  "translatedText": "Вони провели цей експеримент, який був схожий на експеримент зі Стівом, але де людям дали наступний опис вигаданої жінки на ім’я Лінда.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.52,
  "end": 645.72
 },
 {
  "input": "Linda is 31 years old, single, outspoken, and very bright.",
  "translatedText": "Лінді 31 рік, вона незаміжня, відверта та дуже яскрава.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.4,
  "end": 650.62
 },
 {
  "input": "She majored in philosophy.",
  "translatedText": "Здобула спеціальність філософія.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 651.14,
  "end": 652.16
 },
 {
  "input": "As a student she was deeply concerned with issues of discrimination and social justice, and also participated in the anti-nuclear demonstrations.",
  "translatedText": "У студентські роки вона була глибоко стурбована проблемами дискримінації та соціальної справедливості, а також брала участь у антиядерних демонстраціях.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 652.64,
  "end": 659.54
 },
 {
  "input": "After seeing this people were asked what's more likely, 1.",
  "translatedText": "Побачивши це, людей запитали, що більш імовірно, 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 660.7,
  "end": 664.02
 },
 {
  "input": "That Linda is a bank teller, or 2.",
  "translatedText": "Те, що Лінда касірка в банку, або 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 664.34,
  "end": 666.46
 },
 {
  "input": "That Linda is a bank teller and is active in the feminist movement.",
  "translatedText": "Те, що Лінда працює банківським касиром і бере активну участь у феміністичному русі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.92,
  "end": 669.9
 },
 {
  "input": "85%, 85% of participants said that the latter is more likely than the former, even though the set of bank tellers who are active in the feminist movement is a subset of the set of bank tellers.",
  "translatedText": "85 %, 85 % учасниць сказали, що останнє вірогідніше, ніж перше, навіть незважаючи на те, що набір банківських касирів, які беруть активну участь у феміністичному русі, є підмножиною набору банківських касірів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.22,
  "end": 683.32
 },
 {
  "input": "It has to be smaller.",
  "translatedText": "Він повинен бути меншим.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 683.56,
  "end": 684.68
 },
 {
  "input": "So that's interesting enough, but what's fascinating is that there's a simple way that you can rephrase the question that dropped this error from 85% to 0.",
  "translatedText": "Тож це досить цікаво, але захоплююче те, що є простий спосіб, за допомогою якого ви можете перефразувати запитання, завдяки якому ця помилка зменшилася з 85% до 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.64,
  "end": 694.1
 },
 {
  "input": "Instead, if participants were told that there are 100 people who fit this description, and then asked to estimate how many of those 100 are bank tellers, and how many are bank tellers active in the feminist movement, nobody makes the error.",
  "translatedText": "Натомість, якщо учасникам сказали, що є 100 осіб, які відповідають цьому опису, а потім попросили оцінити, скільки з цих 100 банківських касирів і скільки банківських касирів є активними у феміністичному русі, ніхто не припустився помилки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 694.96,
  "end": 708.5
 },
 {
  "input": "Everybody correctly assigns a higher number to the first option than to the second.",
  "translatedText": "Першому варіанту всі правильно присвоюють більше число, ніж другому.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.5,
  "end": 713.18
 },
 {
  "input": "It's weird, somehow phrases like 40 out of 100 kick our intuitions into gear much more effectively than 40%, much less 0.4, and much less abstractly referencing the idea of something being more or less likely.",
  "translatedText": "Дивно, але чомусь такі фрази, як 40 зі 100, активізують нашу інтуїцію набагато ефективніше, ніж 40%, а тим більше 0.4, і набагато менш абстрактне посилання на ідею того, що щось є більш-менш імовірним.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 714.78,
  "end": 728.06
 },
 {
  "input": "That said, representative samples don't easily capture the continuous nature of probability.",
  "translatedText": "Тим не менш, репрезентативні вибірки нелегко охопити безперервну природу ймовірності.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 729.4,
  "end": 734.1
 },
 {
  "input": "So turning to area is a nice alternative, not just because of the continuity, but also because it's way easier to sketch out when you're sitting there pencil and paper puzzling over some problem.",
  "translatedText": "Тож звернення до області є гарною альтернативою не лише через безперервність, а й тому, що набагато легше малювати, коли ти сидиш, ламаючи голову над якоюсь проблемою.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.1,
  "end": 744.04
 },
 {
  "input": "People often think about probability as being the study of uncertainty, and that is of course how it's applied in science, but the actual math of probability, where all the formulas come from, is just the math of proportions, and in that context turning to geometry is exceedingly helpful.",
  "translatedText": "Люди часто думають про ймовірність як про дослідження невизначеності, і, звичайно, це те, як це застосовується в науці, але фактична математика ймовірності, звідки походять усі формули, є просто математикою пропорцій, і в цьому контексті звертаючись до геометрія надзвичайно корисна.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 745.22,
  "end": 761.02
 },
 {
  "input": "I mean, take a look at Bayes' theorem as a statement about proportions, whether that's proportions of people, of areas, whatever.",
  "translatedText": "Я маю на увазі, подивіться на теорему Байєса як на твердження про пропорції, чи це пропорції людей, площ тощо.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 764.26,
  "end": 770.72
 },
 {
  "input": "Once you digest what it's saying, it's actually kind of obvious.",
  "translatedText": "Коли ви зрозумієте, що в ньому сказано, це стане очевидно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 771.3,
  "end": 774.46
 },
 {
  "input": "Both sides tell you to look at the cases where the evidence is true, and then to consider the proportion of those cases where the hypothesis is also true.",
  "translatedText": "Обидві сторони кажуть вам переглянути випадки, де докази правдиві, а потім розглянути частку тих випадків, де гіпотеза також вірна.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 775.04,
  "end": 782.72
 },
 {
  "input": "That's it, that's all it's saying.",
  "translatedText": "Ось і все, це все, що він говорить.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.24,
  "end": 784.64
 },
 {
  "input": "The right-hand side just spells out how to compute it.",
  "translatedText": "У правій частині лише вказано, як це обчислити.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 784.86,
  "end": 786.9
 },
 {
  "input": "What's noteworthy is that such a straightforward fact about proportions can become hugely significant for science, for artificial intelligence, and really any situation where you want to quantify belief.",
  "translatedText": "Варто відзначити, що такий простий факт про пропорції може стати надзвичайно важливим для науки, для штучного інтелекту та справді для будь-якої ситуації, коли ви хочете кількісно оцінити переконання.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 787.54,
  "end": 797.92
 },
 {
  "input": "I hope to give you a better glimpse of that as we get into more examples.",
  "translatedText": "Я сподіваюся дати вам краще уявлення про це, коли ми ознайомимося з іншими прикладами.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.54,
  "end": 801.42
 },
 {
  "input": "But before more examples, we have a little bit of unfinished business with Steve.",
  "translatedText": "Але перш ніж наводити більше прикладів, у нас є трохи незавершених справ зі Стівом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.38,
  "end": 805.74
 },
 {
  "input": "As I mentioned, some psychologists debate Kahneman and Tversky's conclusion that the rational thing to do is to bring to mind the ratio of farmers to librarians.",
  "translatedText": "Як я вже згадував, деякі психологи обговорюють висновок Канемана і Тверського про те, що раціонально згадати про співвідношення фермерів і бібліотекарів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.48,
  "end": 814.8
 },
 {
  "input": "They complain that the context is ambiguous.",
  "translatedText": "Скаржаться, що контекст неоднозначний.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 815.14,
  "end": 817.26
 },
 {
  "input": "I mean, who is Steve, exactly?",
  "translatedText": "Я маю на увазі, хто такий Стів, насправді?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.92,
  "end": 819.84
 },
 {
  "input": "Should you expect that he's a randomly sampled American?",
  "translatedText": "Чи варто очікувати, що він американець випадкової вибірки?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 819.84,
  "end": 822.66
 },
 {
  "input": "Or would you be better to assume that he's a friend of the two psychologists interrogating you?",
  "translatedText": "Або вам краще припустити, що він друг тих двох психологів, які вас допитують?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 823.26,
  "end": 827.0
 },
 {
  "input": "Or maybe that he's someone you're personally likely to know?",
  "translatedText": "Чи, можливо, це хтось, кого ви, швидше за все, знаєте особисто?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 827.22,
  "end": 829.74
 },
 {
  "input": "This assumption determines the prior.",
  "translatedText": "Це припущення визначає пріоритет.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 830.42,
  "end": 832.4
 },
 {
  "input": "I for one run into way more librarians in a given month than I do farmers.",
  "translatedText": "Я, наприклад, зустрічаю значно більше бібліотекарів за певний місяць, ніж фермерів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 832.96,
  "end": 836.68
 },
 {
  "input": "Needless to say, the probability of a librarian or a farmer fitting this description is highly open to interpretation.",
  "translatedText": "Зайве говорити, що ймовірність бібліотекаря чи фермера підходити під цей опис дуже відкрита для тлумачення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.5,
  "end": 843.52
 },
 {
  "input": "For our purposes, understanding the math, what I want to emphasize is that any question worth debating here can be pictured in the context of the diagram.",
  "translatedText": "Для наших цілей, розуміння математики, я хочу підкреслити, що будь-яке питання, яке варто обговорити тут, можна відобразити в контексті діаграми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 844.44,
  "end": 852.3
 },
 {
  "input": "Questions about the context shift around the prior, and questions about the personalities and stereotypes shift around the relevant likelihoods.",
  "translatedText": "Питання про контекст зміщуються навколо попереднього, а питання про особистості та стереотипи зміщуються навколо відповідних ймовірностей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 853.0,
  "end": 860.58
 },
 {
  "input": "All that said, whether or not you buy this particular experiment, the ultimate point that evidence should not determine beliefs, but update them, is worth tattooing in your brain.",
  "translatedText": "Все це сказане, незалежно від того, погоджуєтеся ви на цей конкретний експеримент чи ні, остаточний висновок про те, що докази не повинні визначати переконання, а оновлювати їх, варто витатуювати у вашому мозку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.1,
  "end": 871.0
 },
 {
  "input": "I'm in no position to say whether this does or does not run against natural human instinct.",
  "translatedText": "Я не в змозі сказати, суперечить це природним людським інстинктам чи ні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 871.8,
  "end": 876.5
 },
 {
  "input": "We'll leave that to the psychologists.",
  "translatedText": "Залишимо це психологам.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 876.5,
  "end": 878.24
 },
 {
  "input": "What's more interesting to me is how we can reprogram our intuition to authentically reflect the implications of math, and bringing to mind the right image can often do just that.",
  "translatedText": "Для мене більш цікавим є те, як ми можемо перепрограмувати нашу інтуїцію, щоб автентично відображати наслідки математики, і приведення в пам’ять правильного образу часто може зробити саме це.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 878.92,
  "end": 888.06
 }
]