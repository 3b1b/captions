[
 {
  "input": "In a previous video, I’ve talked about linear systems of equations, and I sort of brushed aside the discussion of actually computing solutions to these systems. ",
  "translatedText": "在之前的视频中，我讨论了线性方程组，并且我有 点忽略了对这些系统的实际计算解决方案的讨论。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.2,
  "end": 19.14
 },
 {
  "input": "And while it’s true that number-crunching is something we typically leave to the computers, digging into some of these computational methods is a good litmus test for whether or not you actually understand what’s going on, since this is really where the rubber meets the road. ",
  "translatedText": "虽然数字处理通常是我们留给计算机的事情，但深入研究其 中一些计算方法是检验您是否真正理解正在发生的事情的一 个很好的试金石，因为这确实是橡胶与道路相遇的地方。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.7,
  "end": 31.52
 },
 {
  "input": "Here I want to describe the geometry behind a certain method for computing solutions to these systems, known as Cramer’s rule. ",
  "translatedText": "在这里，我想描述计算这些系统的解决方案的某种方法背后的几何 结构，称为克莱默规则。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 32.12,
  "end": 38.88
 },
 {
  "input": "The relevant background needed here is an understanding of determinants, dot products, and of linear systems of equations, so be sure to watch the relevant videos on those topics if you’re unfamiliar or rusty. ",
  "translatedText": "这里所需的相关背景是对行列式、点积和线性方程组的理解，因此，如果您不熟悉或生疏，请务必观看有关这些主题的相关视频。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.68,
  "end": 50.42
 },
 {
  "input": "But first! ",
  "translatedText": "但首先！ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.02,
  "end": 51.44
 },
 {
  "input": "I should say up front that Cramer’s rule is not the best way for computing solutions to linear systems of equations. ",
  "translatedText": "我应该预先说明，克莱默规则并不是计算线性方程组解的最佳方法。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.44,
  "end": 57.26
 },
 {
  "input": "Gaussian elimination, for example, will always be faster. ",
  "translatedText": "例如，高斯消除法总是更快。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 58.14,
  "end": 61.26
 },
 {
  "input": "So why learn it? ",
  "translatedText": "那为什么要学呢？ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.98,
  "end": 63.52
 },
 {
  "input": "Think of this as a sort of cultural excursion; it’s a helpful exercise in deepening your knowledge of the theory of these systems. ",
  "translatedText": "将此视为一种文化之旅；这是加深您对这些系统理论知识的有用练习。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.78,
  "end": 70.46
 },
 {
  "input": "Wrapping your mind around this concept will help consolidate ideas from linear algebra, like the determinant and linear systems, by seeing how they relate to each other. ",
  "translatedText": "了解这个概念将有助于通过 了解线性代数（例如行列式和线性系统）之间的相互 关系来巩固它们的想法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.04,
  "end": 79.62
 },
 {
  "input": "Also, from a purely artistic standpoint, the ultimate result is just really pretty to think about, much more so that Gaussian elimination. ",
  "translatedText": "此外，从纯粹的艺术角度来看， 这里的最终结果确实非常漂亮，比高斯消去法要好得多。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.1,
  "end": 86.88
 },
 {
  "input": "Alright, so the setup here will be some linear system of equations, say with two unknowns, x and y, and two equations. ",
  "translatedText": "好吧，这里的设置将是一些线性方程组，比如有两个未知数 x 和 y，以及两个方程。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.68,
  "end": 95.62
 },
 {
  "input": "In principle, everything we’re talking about will work systems with a larger number of unknowns, and the same number of equations. ",
  "translatedText": "原则上，我们所讨论的一切都适用于具有大量未知数和相同数量方程的系统。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 96.3,
  "end": 101.94
 },
 {
  "input": "But for simplicity, a smaller example is nicer to hold in our heads. ",
  "translatedText": "但为了简单起见，一个较小的例子更容易记住。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.44,
  "end": 105.58
 },
 {
  "input": "So as I talked about in a previous video, you can think of this setup geometrically as a certain known matrix transforming an unknown vector, [x; y], where you know what the output is going to be, in this case [-4; -2]. ",
  "translatedText": "正如我在之前的视频中谈到的，您可以在几何上将此设置视为某个已知矩阵对未知向量进行变换，[x; y]，您知道输出是什么，在本例中为 [-4; -2]。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.32,
  "end": 120.04
 },
 {
  "input": "Remember, the columns of this matrix tell you how the matrix acts as a transform, each one telling you where the basis vectors of the input space land. ",
  "translatedText": "请记住，该矩阵的列告诉您矩阵如何充当变换，每一列都告诉您输入空间的基向量落在哪里。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.8,
  "end": 130.08
 },
 {
  "input": "So this is a sort of puzzle, what input [x; y], is going to give you this output [-4; -2]? ",
  "translatedText": "所以这是一个难题，输入什么[x; y]，将为您提供此输出 [-4; -2]？ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.86,
  "end": 137.6
 },
 {
  "input": "Remember, the type of answer you get here can depend on whether or not the transformation squishes all of space into a lower dimension. ",
  "translatedText": "请记住，您在这里得到的答案类型可能取决于变换是否将所有空间压缩到较低的维度。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 137.6,
  "end": 148.4
 },
 {
  "input": "That is if it has zero determinant. ",
  "translatedText": "也就是说，如果它的行列式为零。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 148.4,
  "end": 151.22
 },
 {
  "input": "In that case, either none of the inputs land on our given output or there are a whole bunch of inputs landing on that output. ",
  "translatedText": "在这种情况下，要么没有任何输入落在我们给定的输出上，要么有一大堆输入落在该输出上。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 151.22,
  "end": 156.22
 },
 {
  "input": "But for this video we’ll limit our view to the case of a non-zero determinant, meaning the output of this transformation still spans the full n-dimensional space it started in; every input lands on one and only one output and every output has one and only one input. ",
  "translatedText": "但对于这个视频，我们 将把我们的观点限制在非零行列式的情况，这意味着这个转换的输出 仍然跨越它开始的整个维度空间。每个输入都落在一个且仅有一个输出上， 并且每个输出都有一个且仅有一个输入。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 157.24,
  "end": 168.94
 },
 {
  "input": "One way to think about our puzzle is that we know the given output vector is some linear combination of the columns of the matrix; x*(the vector where i-hat lands) + y*(the vector where j-hat lands), but we wish to compute what exactly x and y are. ",
  "translatedText": "思考我们的难题的一种方法是，我们知道给定的输出向量是矩阵列的某种线性组合； x*（i-hat 落在的向量）+ y*（j-hat 落在的向量），但我们希望计算 x 和 y 到底是什么。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.94,
  "end": 186.72
 },
 {
  "input": "As a first pass, let me show an idea that is wrong, but in the right direction. ",
  "translatedText": "首先，让我展示一个错误但方向正确的想法。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 186.72,
  "end": 198.16
 },
 {
  "input": "The x-coordinate of this mystery input vector is what you get by taking its dot product with the first basis vector, [1; 0]. ",
  "translatedText": "这个神秘输入向量的 x 坐标是通过将其与第一个基向量 [1; 进行点积计算得到的。 0]。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.8,
  "end": 205.44
 },
 {
  "input": "Likewise, the y-coordinate is what you get by dotting it with the second basis vector, [0; 1]. ",
  "translatedText": "同样，y 坐标是用第二个基向 量 0, 1 点起来得到的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.16,
  "end": 211.4
 },
 {
  "input": "So maybe you hope that after the transformation, the dot products with the transformed version of the mystery vector with the transformed versions of the basis vectors will also be these coordinates x and y. ",
  "translatedText": "所以也许你希望在变换之后 ，神秘向量的变换版本与变换版本的点积也将是这 些坐标，x和y。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.9,
  "end": 223.24
 },
 {
  "input": "That’d be fantastic because we know the transformed versions of each of these vectors. ",
  "translatedText": "那太棒了，因为我们知道每个向量 的转换版本是什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.94,
  "end": 228.74
 },
 {
  "input": "There’s just one problem with this: it’s not at all true! ",
  "translatedText": "它只有一个问题，它根本不是真的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.18,
  "end": 234.2
 },
 {
  "input": "For most linear transformations, the dot product before and after the transformation will be very different. ",
  "translatedText": "对于大多数线性变换，变换前后的点积看起来会非常 不同。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 234.64,
  "end": 240.24
 },
 {
  "input": "For example, you could have two vectors generally pointing in the same direction, with a positive dot product, which get pulled away from each other during the transformation, in such a way that they then have a negative dot product. ",
  "translatedText": "例如，您可能有两个通常指向同一方向且具 有正点积的向量，这两个向量在转换过程中彼此分 开，最终导致它们具有负点积。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.8,
  "end": 251.52
 },
 {
  "input": "Likewise, if things start off perpendicular, with dot product zero, like the two basis vectors, there’s no guarantee that they will stay perpendicular after the transformation, preserving that zero dot product. ",
  "translatedText": "同样，从点积 0 开始垂直的事物（例如两个基向量）在变换后通常不会 保持彼此垂直，也就是说，它们不会保留 0 点积。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 252.22,
  "end": 263.48
 },
 {
  "input": "In the example we were looking at, dot products certainly aren’t preserved. ",
  "translatedText": "在我们看到的示例中，点积当然不会被保留。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 264.1,
  "end": 267.16
 },
 {
  "input": "They tend to get bigger since most vectors are getting stretched. ",
  "translatedText": "由于大多数向量都被拉伸，它们往往会变得更大。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.5,
  "end": 269.94
 },
 {
  "input": "In fact, transformations which do preserve dot products are special enough to have their own name: Orthonormal transformations. ",
  "translatedText": "事实上，保留点积的变换非常特殊，有自己的名字：正交变换。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.94,
  "end": 279.1
 },
 {
  "input": "These are the ones which leave all the basis vectors perpendicular to each other with unit lengths. ",
  "translatedText": "这些是使所有基本向量彼此 垂直并且仍然具有单位长度的向量。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 279.72,
  "end": 284.66
 },
 {
  "input": "You often think of these as rotation matrices. ",
  "translatedText": "您通常将它们视为旋转矩阵。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.74,
  "end": 287.88
 },
 {
  "input": "The correspond to rigid motion, with no stretching, squishing or morphing. ",
  "translatedText": "它们对应于刚性运动，没有拉伸、挤压或变形。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.42,
  "end": 292.2
 },
 {
  "input": "Solving a linear system with an orthonormal matrix is very easy: Since dot products are preserved, taking the dot product between the output vector and all the columns of your matrix will be the same as taking the dot products between the input vector and all the basis vectors, which is the same as finding the coordinates of the input vector. ",
  "translatedText": "求解具有正交矩阵的线性系统非常容易：由于保留了点积，因此在输出向量和矩阵的所有列之间求点积将与在输入向量和所有基之间求点积相同向量，这与查找输入向量的坐标相同。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.0,
  "end": 312.98
 },
 {
  "input": "So, in that very special case, x would be the dot product of the first column with the output vector, and y would be the dot product of the second column with the output vector. ",
  "translatedText": "因此， 在这种非常特殊的情况下，x 将是第一列与输出 向量的点积，y 将是第二列与输出向量的点积。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.68,
  "end": 323.76
 },
 {
  "input": "Now, even though this idea breaks down for most linear systems, it points us in the direction of something to look for: Is there an alternate geometric understanding for the coordinates of our input vector which remains unchanged after the transformation? ",
  "translatedText": "当这个想法对于几乎所有线性系统来说都行不通时，为什么我要提出这个问题呢？嗯，它为我们指明了寻找方向。对于变换后保持 不变的输入向量的坐标是否有替代的几何理 解？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.82,
  "end": 341.66
 },
 {
  "input": "If your mind has been mulling over determinants, you might think of this clever idea: Take the parallelogram defined by the first basis vector, i-hat, and the mystery input vector [x; y]. ",
  "translatedText": "如果您一直在思考决定因素，您可能会想到以下聪明的 想法。采用由第一个基向量 i-hat 和神秘输入向量 xy 定义的平行四 边形。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.36,
  "end": 353.76
 },
 {
  "input": "The area of this parallelogram is its base, 1, times the height perpendicular to that base, which is the y-coordinate of our input vector. ",
  "translatedText": "该平行四边形的面积是底边 1 乘以垂直于该底边的高 度，即该输入向量的 y 坐标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 354.44,
  "end": 362.96
 },
 {
  "input": "So, the area of this parallelogram is sort of a screwy roundabout way to describe the vector’s y-coordinate; it’s a wacky way to talk about coordinates, but run with me. ",
  "translatedText": "因此，平行四边形的面积是 描述向量 y 坐标的一种扭曲的迂回方式。这是一种奇怪的谈论坐 标的方式，但跟我一起跑吧。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 363.68,
  "end": 373.14
 },
 {
  "input": "Actually, to be more accurate, you should think of the signed area of this parallelogram, in the sense described by the determinant video. ",
  "translatedText": "实际上，为了更准确一点，您应该将其视为 该平行四边形的带符号区域，就像行列式视频中描述的那样。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 373.7,
  "end": 381.64
 },
 {
  "input": "That way, a vector with negative y-coordinate would correspond to a negative area for this parallelogram. ",
  "translatedText": "这样，具有负 y 坐标的向量将对应于该平行四边形的负面积， 至少如果您认为 i-hat 在某种意义上是定义平行四边形的 这两个向量中的第一个。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.2,
  "end": 388.58
 },
 {
  "input": "Symmetrically, if you look at the parallelogram spanned by the vector and the second basis vector, j-hat, its area will be the x-coordinate of the vector. ",
  "translatedText": "对称地，如果你看一下由我们的神秘输入向 量和第二个基 j-hat 组成的平行四边形，它的面积将是该神 秘向量的 x 坐标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 388.96,
  "end": 392.96
 },
 {
  "input": "Again, it’s a strange way to represent the x-coordinate, but you’ll see what it buys us in a moment. ",
  "translatedText": "同样，这是一种奇怪的表示 x 坐标的方式，但您很快 就会明白它给我们带来了什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 392.96,
  "end": 398.78
 },
 {
  "input": "Here’s what this would look like in three-dimensions: Ordinarily the way you might think of one of a vector’s coordinate, say its z-coordinate, would be to take its dot product with the third standard basis vector, k-hat. ",
  "translatedText": "为了确保清楚这如何概括，让我们 从三个维度来看。通常，您可能会考虑一个向量的坐标之一（例 如它的 z 坐标），将其与第三个标准基向量（通常称为 k- hat）进行点积。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.78,
  "end": 410.08
 },
 {
  "input": "But instead, consider the parallelepiped it creates with the other two basis vectors, i-hat and j-hat. ",
  "translatedText": "但另一种几何解释是考虑它与其他两个基本向量 i-hat 和 j-hat 创建的平行六面体。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 410.68,
  "end": 412.64
 },
 {
  "input": "If you think of the square with area 1 spanned by i-hat and j-hat as the base of this guy, its volume is the same its height, which is the third coordinate of our vector. ",
  "translatedText": "如果您将 i- hat 和 j-hat 所跨越的面积为 1 的正方形视为整个形状的基础， 那么它的体积与其高度相同，这是我们向量的第三个坐标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.74,
  "end": 417.14
 },
 {
  "input": "Likewise, the wacky way to think about any other coordinate of this vector is to form the parallelepiped between this vector an all the basis vectors other than the one you’re looking for, and get its volume. ",
  "translatedText": "同样，考虑向量的 其他坐标的一种古怪方法是使用该向量形成一个平行六面体， 然后使用除与您正在寻找的方向相对应的方向之外的所有基本 向量。然后这个体积就可以给你坐标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.14,
  "end": 429.72
 },
 {
  "input": "Or, rather, we should talk about the signed volume of these parallelepipeds, in the sense described in the determinant video, where the order in which you list the three vectors matters and you’re using the right-hand rule. ",
  "translatedText": "或者更确切地说，我们应该 讨论平行六面体的有符号体积，在行列式视频中使用右手定则描述的 意义上。因此，列出这三个向量的顺序很重要。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.72,
  "end": 442.38
 },
 {
  "input": "That way negative coordinates still make sense. ",
  "translatedText": "这样，负坐 标仍然有意义。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.24
 },
 {
  "input": "Okay, so why think of coordinates as areas and volumes like this? ",
  "translatedText": "好吧，那么为什么要把坐标看作这样的面积和体积呢？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.24,
  "end": 447.5
 },
 {
  "input": "As you apply some matrix transformation, the areas of the parallelograms don’t stay the same, they may get scaled up or down. ",
  "translatedText": "好吧，当您应用某种矩阵变换时，这些平行四边形的面积，它们不会 保持不变，它们可能会按比例放大或缩小。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 450.0
 },
 {
  "input": "But(!), and this is a key idea of determinants, all these areas get scaled by the same amount. ",
  "translatedText": "但是（！），这是决定因素的关键思想，所有这些区域都会按相同的量缩放。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.0,
  "end": 453.96
 },
 {
  "input": "Namely, the determinant of our transformation matrix. ",
  "translatedText": "即，我们的变换矩阵的行列式。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.96,
  "end": 457.9
 },
 {
  "input": "For example, if you look the parallelogram spanned by the vector where your first basis vector lands, which is the first column of the matrix, and the transformed version of [x; y], what is its area? ",
  "translatedText": "例如，如果您查看第一个基向量所在的向量（即矩阵的第一列）和 [x; 的变换版本] 所跨越的平行四边形。 y]，它的面积是多少？ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.44,
  "end": 470.72
 },
 {
  "input": "Well, this is the transformed version of that parallelogram we were looking at earlier, whose area was the y-coordinate of the mystery input vector. ",
  "translatedText": "嗯，这是我们之前看到的平行四边形的 变换版本，其面积是神秘输入向量的 y 坐标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.72,
  "end": 478.28
 },
 {
  "input": "So its area will be the determinant of the transformation multiplied by that value. ",
  "translatedText": "所以它的面积就是变换的决定因素乘以 y 坐标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 478.96,
  "end": 481.96
 },
 {
  "input": "So, the y-coordinate of our mystery input vector is the area of this parallelogram, spanned by the first column of the matrix and the output vector, divided by the determinant of the full transformation. ",
  "translatedText": "因此，我们神秘的输入向量的 y 坐标是这个平行四边形的面积，由矩阵的第一列和输出向量跨越，除以完整变换的行列式。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.96,
  "end": 492.16
 },
 {
  "input": "And how do you get this area? ",
  "translatedText": "那么如何获得这个区域呢？ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.16,
  "end": 494.88
 },
 {
  "input": "Well, we know the coordinates for where the mystery input vector lands, that’s the whole point of a linear system of equations. ",
  "translatedText": "好吧，我们知道神秘输入向量落地的坐标，这就是线性方程组的全部要点。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.88,
  "end": 499.88
 },
 {
  "input": "So, create a matrix whose first column is the same as that of our matrix, and whose second column is the output vector, and take its determinant. ",
  "translatedText": "因此，您可能要做的就是创建一个新矩阵 ，其第一列与我们的矩阵相同，但第二列是输出向量，然后获 取其行列式。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.88,
  "end": 512.88
 },
 {
  "input": "So look at that; just using data from the output of the transformation, namely the columns of the matrix and the coordinates of our output vector, we can recover the y-coordinate of our mystery input vector. ",
  "translatedText": "所以看一下，只需使用变换输出的数据， 即矩阵的列和输出向量的坐标，我们就可以恢复神 秘输入向量的 y 坐标，这就是解决系统的一半 。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.88,
  "end": 522.1
 },
 {
  "input": "Likewise, the same idea can get you the x-coordinate. ",
  "translatedText": "同样，同样的想法可以给我们 x 坐标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 522.1,
  "end": 523.5
 },
 {
  "input": "Look at that parallelogram we defined early which encodes the x-coordinate of the mystery input vector, spanned by the input vector and j-hat. ",
  "translatedText": "看看我们之前定义的平行四边 形，它编码了神秘输入向量的 x 坐标，由该向量和 j-hat 跨越 。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 523.5,
  "end": 537.14
 },
 {
  "input": "The transformed version of this guy is spanned by the output vector and the second column of the matrix, and its area will have been multiplied by the determinant of the matrix. ",
  "translatedText": "这个家伙的变换版本由输出向量和矩阵的第 二列组成，它的面积将乘以该矩阵的行列式。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 537.14,
  "end": 545.04
 },
 {
  "input": "So the x-coordinate of our mystery input vector is this area divided by the determinant of the transformation. ",
  "translatedText": "因此，要求解 x，您可以将这个新面积除以完整变换的行 列式。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 545.06,
  "end": 555.14
 },
 {
  "input": "Symmetric to what we did before, you can compute the area of that output parallelogram by creating a new matrix whose first column is the output vector, and whose second column is the same as the original matrix. ",
  "translatedText": "与我们之前所做的类似，您可以通过创建一个新矩阵 来计算输出平行四边形的面积，该新矩阵的第一列是输出向量 ，第二列与原始矩阵相同。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.6,
  "end": 563.42
 },
 {
  "input": "So again, just using data from the output space, the numbers we see in our original linear system, we can recover the x-coordinate of our mystery input vector. ",
  "translatedText": "同样，只需使用输出空间中的数据，即我 们在原始线性系统中看到的数字，我们就可以求解 x 必须是什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.42,
  "end": 573.42
 },
 {
  "input": "This formula for finding the solutions to a linear system of equations is known as Cramer’s rule. ",
  "translatedText": "这个求线性方程组解的公式称为克莱默法则。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.42,
  "end": 584.48
 },
 {
  "input": "Here, just to sanity check ourselves, plug in the numbers here. ",
  "translatedText": "在这里，只是为了检查一下我们自己的理智，在这里插入一些数字。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 584.48,
  "end": 585.34
 },
 {
  "input": "The determinant of that top altered matrix is 4+2, which is 6, and the bottom determinant is 2, so the x-coordinate should be 3. ",
  "translatedText": "顶部更改矩阵的行列式 是 4 加 2，即 6，底部行列式是 2，因此 x 坐标应为 3。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.34,
  "end": 592.94
 },
 {
  "input": "And indeed, looking back at that input vector we started with, it’s x-coordinate is 3. ",
  "translatedText": "事实上，回顾我们开始时的输入向量，x 坐标是 3。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.86,
  "end": 604.34
 },
 {
  "input": "Likewise, Cramer’s rule suggests the y-coordinate should be 4/2, or 2, and that is indeed the y-coordinate of the input vector we started with here. ",
  "translatedText": "同样，克莱默规则表明 y 坐标应为 4 除以 2 ，即 2，这就是我们开始时输入向量的 y 坐标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 604.34,
  "end": 607.72
 },
 {
  "input": "The case with three dimensions is similar, and I highly recommend you pause to think it through yourself. ",
  "translatedText": "3 维或更多维度的情况类似，我强烈建议您花点时间 自己思考一下。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 607.72,
  "end": 618.42
 },
 {
  "input": "Here, I’ll give you a little momentum. ",
  "translatedText": "在这里，我给大家一点动力。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 619.12,
  "end": 621.58
 },
 {
  "input": "We have this known transformation, given by a 3x3 matrix, and a known output vector, given by the right side of our linear system, and we want to know what input vector lands on this output vector. ",
  "translatedText": "我们拥有的是 由某个 3x3 矩阵给出的已知变换，以及由线性系统右侧给出的已 知输出向量，我们想知道什么输入落在该输出上。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.58,
  "end": 633.26
 },
 {
  "input": "If you think of, say, the z-coordinate of the input vector as the volume of this parallelepiped spanned by i-hat, j-hat, and the mystery input vector, what happens to the volume of this parallelepiped after the transformation? ",
  "translatedText": "例如，如果您将输入向量的 z 坐标视为由 i-hat、j-hat 和神秘输入向量跨越的平行六面体的体积，那么变换后该平行六面体的体积会发生什么？ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 633.26,
  "end": 644.64
 },
 {
  "input": "How can you compute that new volume? ",
  "translatedText": "你如何计算新的体积？ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.64,
  "end": 651.66
 },
 {
  "input": "Really, pause and take a moment to think through the details of generalizing this to higher dimensions; finding an expression for each coordinate of the solution to larger linear systems. ",
  "translatedText": "真的，停下来，花点时间思考一下将其推广到更高维度的细节；找到较大线性系统解的每个坐标的表达式。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 651.66,
  "end": 664.68
 },
 {
  "input": "Thinking through more general cases and convincing yourself that it works is where all the learning will happen, much more so than listening to some dude on YouTube walk through the reasoning again. ",
  "translatedText": "思考像这样的更一般的案例，并说服自己它是有效的以及为 什么它有效，这是所有学习真正发生的地方，比听 YouT ube 上的某个人再次引导你进行相同的推理要重要得多。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.1,
  "end": 708.5
 }
]