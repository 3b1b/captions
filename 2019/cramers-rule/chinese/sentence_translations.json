[
 {
  "input": "In a previous video I've talked about linear systems of equations, and I sort of brushed aside the discussion of actually computing solutions to these systems.",
  "translatedText": "在之前的视频中，我讨论了线性方程组，并且我有 点忽略了对这些系统的实际计算解决方案的讨论。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 11.2,
  "end": 19.14
 },
 {
  "input": "And while it's true that number crunching is typically something we leave to the computers, digging into some of these computational methods is a good litmus test for whether or not you actually understand what's going on, since that's really where the rubber meets the road.",
  "translatedText": "虽然数字处理通常是我们留给计算机的事情，但深入研究其 中一些计算方法是检验您是否真正理解正在发生的事情的一 个很好的试金石，因为这确实是橡胶与道路相遇的地方。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 19.7,
  "end": 31.52
 },
 {
  "input": "Here I want to describe the geometry behind a certain method for computing solutions to these systems, known as Cramer's rule.",
  "translatedText": "在这里，我想描述计算这些系统的解决方案的某种方法背后的几何 结构，称为克莱默规则。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 32.12,
  "end": 38.88
 },
 {
  "input": "The relevant background here is understanding determinants, a little bit of dot products, and of course linear systems of equations, so be sure to watch the relevant videos on those topics if you're unfamiliar or rusty.",
  "translatedText": "这里所需的相关背景是对行列式、点积和线性方程组的理解，因此，如果您不熟悉或生疏，请务必观看有关这些主题的相关视频。 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.68,
  "end": 50.42
 },
 {
  "input": "But first I should say up front that this Cramer's rule is not actually the best way for computing solutions to linear systems of equations, Gaussian elimination for example will always be faster.",
  "translatedText": "但首先我要说的是，克拉默法则实际上并不是计算线性方程组解的最佳方法，例如高斯消元总是更快。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.02,
  "end": 61.26
 },
 {
  "input": "So why learn it?",
  "translatedText": "那为什么要学呢？ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 61.98,
  "end": 63.52
 },
 {
  "input": "Well think of it as a sort of cultural excursion.",
  "translatedText": "就当是一次文化之旅吧。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.78,
  "end": 65.84
 },
 {
  "input": "It's a helpful exercise in deepening your knowledge of the theory behind these systems.",
  "translatedText": "这对加深你对这些系统背后理论的了解很有帮助。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.42,
  "end": 70.46
 },
 {
  "input": "Wrapping your mind around this concept is going to help consolidate ideas from linear algebra, like the determinant and linear systems, by seeing how they relate to each other.",
  "translatedText": "了解这个概念将有助于通过 了解线性代数（例如行列式和线性系统）之间的相互 关系来巩固它们的想法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 71.04,
  "end": 79.62
 },
 {
  "input": "Also from a purely artistic standpoint the ultimate result here is just really pretty to think about, way more so than Gaussian elimination.",
  "translatedText": "此外，从纯粹的艺术角度来看， 这里的最终结果确实非常漂亮，比高斯消去法要好得多。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.1,
  "end": 86.88
 },
 {
  "input": "Alright so the setup here will be some linear system of equations, say with two unknowns x and y and two equations.",
  "translatedText": "好吧，这里的设置将是一些线性方程组，比如有两个未知数 x 和 y，以及两个方程。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.68,
  "end": 95.62
 },
 {
  "input": "In principle everything we're talking about will also work for systems with larger number of unknowns and the same number of equations, but for simplicity a smaller example is just nicer to hold in our heads.",
  "translatedText": "原则上，我们所说的一切也适用于未知数较多、方程数相同的系统，但为了简单起见，我们还是举一个较小的例子比较好。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 96.3,
  "end": 105.58
 },
 {
  "input": "So as I talked about in a previous video you can think of this setup geometrically as a certain known matrix transforming an unknown vector x y where you know what the output is going to be, in this case negative 4 negative 2.",
  "translatedText": "正如我在之前的视频中谈到的，您可以在几何上将此设置视为某个已知矩阵对未知向量进行变换，[x; y]，您知道输出是什么，在本例中为 [-4; -2]。 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 106.32,
  "end": 120.04
 },
 {
  "input": "Remember the columns of this matrix are telling you how that matrix acts as a transform, each one telling you where the basis vectors of the input space land.",
  "translatedText": "请记住，该矩阵的列告诉您矩阵如何充当变换，每一列都告诉您输入空间的基向量落在哪里。 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.8,
  "end": 130.08
 },
 {
  "input": "So what we have is a sort of puzzle, which input vector x y is going to land on this output negative 4 negative 2.",
  "translatedText": "因此，我们面临的是一个谜题：哪个输入向量 x y 会指向负 4 负 2 的输出？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.86,
  "end": 138.6
 },
 {
  "input": "One way to think about our little puzzle here is that we know the given output vector is some linear combination of the columns of the matrix x times the vector where i hat lands plus y times the vector where j hat lands, but what we want is to figure out what exactly that linear combination should be.",
  "translatedText": "思考这个小难题的一种方法是，我们知道给定的输出矢量是矩阵 x 的列乘以 i 个帽子所在矢量加上 y 乘以 j 个帽子所在矢量的线性组合，但我们要做的是找出这个线性组合的具体内容。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 139.7,
  "end": 156.22
 },
 {
  "input": "Remember the type of answer you get here can depend on whether or not the transformation squishes all of space into a lower dimension, that is if it has a zero determinant.",
  "translatedText": "请记住，您在这里得到的答案类型可能取决于变换是否将所有空间挤压到一个较低的维度，也就是说，如果它的行列式为零。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 157.24,
  "end": 166.1
 },
 {
  "input": "In that case either none of the inputs land on our given output, or there's a whole bunch of inputs landing on that output.",
  "translatedText": "在这种情况下，要么没有任何输入落在给定的输出上，要么有一大堆输入落在输出上。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 166.1,
  "end": 173.9
 },
 {
  "input": "But for this video we'll limit our view to the case of a non-zero determinant, meaning the outputs of this transformation still span the full in-dimensional space that it started in.",
  "translatedText": "但在本视频中，我们将把视角限制在行列式不为零的情况下，这意味着这种变换的输出仍然跨越它开始时的整个内维空间。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 187.14
 },
 {
  "input": "Every input lands on one and only one output, and every output has one and only one input.",
  "translatedText": "每个输入只有一个输出，每个输出也只有一个输入。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 187.5,
  "end": 192.7
 },
 {
  "input": "As a first pass let me show you an idea that's wrong but in the right direction.",
  "translatedText": "首先，让我向大家展示一个错误但方向正确的想法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 194.18,
  "end": 198.16
 },
 {
  "input": "The x coordinate of this mystery input vector is what you get by taking its dot product with the first basis vector 1 0.",
  "translatedText": "这个神秘输入向量的 x 坐标是通过将其与第一个基向量 [1; 进行点积计算得到的。 0]。 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 198.8,
  "end": 205.44
 },
 {
  "input": "Likewise the y coordinate is what you get by dotting it with the second basis vector 0 1.",
  "translatedText": "同样，y 坐标是用第二个基向 量 0, 1 点起来得到的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 206.16,
  "end": 211.4
 },
 {
  "input": "So maybe you hope that after the transformation the dot products with the transformed version of the mystery vector with the transformed version of the basis vectors will also be these coordinates x and y.",
  "translatedText": "所以也许你希望在变换之后 ，神秘向量的变换版本与变换版本的点积也将是这 些坐标，x和y。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 211.9,
  "end": 223.24
 },
 {
  "input": "That'd be fantastic because we know what the transformed version of each of those vectors are.",
  "translatedText": "那太棒了，因为我们知道每个向量 的转换版本是什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.94,
  "end": 228.74
 },
 {
  "input": "There's just one problem with it, it's not at all true.",
  "translatedText": "它只有一个问题，它根本不是真的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.18,
  "end": 234.2
 },
 {
  "input": "For most linear transformations the dot product before and after the transformation will look very different.",
  "translatedText": "对于大多数线性变换，变换前后的点积看起来会非常 不同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 234.64,
  "end": 240.24
 },
 {
  "input": "For example, you could have two vectors generally pointing in the same direction with a positive dot product, which get pulled apart from each other during the transformation in such a way that they end up having a negative dot product.",
  "translatedText": "例如，您可能有两个通常指向同一方向且具 有正点积的向量，这两个向量在转换过程中彼此分 开，最终导致它们具有负点积。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 240.8,
  "end": 251.52
 },
 {
  "input": "Likewise things that start off perpendicular with dot product 0, like the two basis vectors, quite often don't stay perpendicular to each other after the transformation, that is they don't preserve that 0 dot product.",
  "translatedText": "同样，从点积 0 开始垂直的事物（例如两个基向量）在变换后通常不会 保持彼此垂直，也就是说，它们不会保留 0 点积。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.22,
  "end": 263.48
 },
 {
  "input": "And looking at the example I just showed dot products certainly aren't preserved, they tend to get bigger since most vectors are getting stretched out.",
  "translatedText": "再看我刚才展示的例子，点积肯定不会保持不变，它们往往会变大，因为大多数矢量都被拉长了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.1,
  "end": 270.3
 },
 {
  "input": "In fact, worthwhile side note here, transformations which do preserve dot products are special enough to have their own name, orthonormal transformations.",
  "translatedText": "事实上，值得注意的是，保留点积的变换非常特别，有自己的名字--正交变换。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 271.04,
  "end": 279.1
 },
 {
  "input": "These are the ones that leave all of the basis vectors perpendicular to each other and still with unit lengths.",
  "translatedText": "这些是使所有基本向量彼此 垂直并且仍然具有单位长度的向量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 279.72,
  "end": 284.66
 },
 {
  "input": "You often think of these as the rotation matrices, they correspond to rigid motion with no stretching or squishing or morphing.",
  "translatedText": "你通常会把这些矩阵看作旋转矩阵，它们对应的是没有拉伸、挤压或变形的刚性运动。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 285.74,
  "end": 292.2
 },
 {
  "input": "Solving a linear system with an orthonormal matrix is actually super easy.",
  "translatedText": "用正交矩阵求解线性系统其实超级简单。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.0,
  "end": 296.78
 },
 {
  "input": "Because dot products are preserved, taking the dot product between the output vector and all the columns of your matrix will be the same as taking the dot product between the mystery input vector and all of the basis vectors, which is the same as just finding the coordinates of that mystery input.",
  "translatedText": "由于点积是保留的，因此求输出向量与矩阵所有列之间的点积，就等同于求神秘输入向量与所有基向量之间的点积，也就等同于求神秘输入向量的坐标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 297.26,
  "end": 312.98
 },
 {
  "input": "So in that very special case, x would be the dot product of the first column with the output vector, and y would be the dot product of the second column with the output vector.",
  "translatedText": "因此， 在这种非常特殊的情况下，x 将是第一列与输出 向量的点积，y 将是第二列与输出向量的点积。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 323.76
 },
 {
  "input": "Why am I bringing this up when this idea breaks down for almost all linear systems?",
  "translatedText": "几乎所有的线性系统都会出现这种情况，我为什么还要提出这个问题呢？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 326.82,
  "end": 330.86
 },
 {
  "input": "Well, it points us in a direction of something to look for.",
  "translatedText": "好吧，它为我们指出了一个需要寻找的方向。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 331.42,
  "end": 334.08
 },
 {
  "input": "Is there an alternate geometric understanding for the coordinates of our input vector that remains unchanged after the transformation?",
  "translatedText": "输入矢量的坐标在变换后保持不变，是否有另一种几何理解？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 334.32,
  "end": 341.66
 },
 {
  "input": "If your mind has been mulling over determinants, you might think of the following clever idea.",
  "translatedText": "如果你一直在思考决定因素，你可能会想到下面这个聪明的想法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 342.36,
  "end": 346.8
 },
 {
  "input": "Take the parallelogram defined by the first basis vector i-hat and the mystery input vector xy.",
  "translatedText": "取第一个基础向量 i-hat 和神秘输入向量 xy 定义的平行四边形。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 347.36,
  "end": 353.76
 },
 {
  "input": "The area of this parallelogram is the base, 1, times the height perpendicular to that base, which is the y-coordinate of that input vector.",
  "translatedText": "该平行四边形的面积是底边 1 乘以垂直于该底边的高 度，即该输入向量的 y 坐标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 354.44,
  "end": 362.96
 },
 {
  "input": "So the area of that parallelogram is a sort of screwy roundabout way to describe the vector's y-coordinate.",
  "translatedText": "因此，平行四边形的面积是描述矢量 y 坐标的一种迂回曲折的方法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 363.68,
  "end": 369.96
 },
 {
  "input": "It's a wacky way to talk about coordinates, but run with me.",
  "translatedText": "用这种古怪的方式来谈论坐标，但请跟我来。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 370.42,
  "end": 373.14
 },
 {
  "input": "And actually, to be a little more accurate, you should think of this as the signed area of that parallelogram, in the sense described in the determinant video.",
  "translatedText": "实际上，为了更准确一点，您应该将其视为 该平行四边形的带符号区域，就像行列式视频中描述的那样。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 373.7,
  "end": 381.64
 },
 {
  "input": "That way, a vector with a negative y-coordinate would correspond to a negative area for this parallelogram, at least if you think of i-hat as in some sense being the first out of these two vectors defining the parallelogram.",
  "translatedText": "这样，Y 坐标为负的矢量将对应于这个平行四边形的负面积，至少从某种意义上说，i-帽子是这两个定义平行四边形的矢量中的第一个。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.2,
  "end": 394.5
 },
 {
  "input": "And symmetrically, if you look at the parallelogram spanned by our mystery input vector and the second basis, j-hat, its area is going to be the x-coordinate of that mystery vector.",
  "translatedText": "对称地看，如果你看一下神秘输入向量和第二个基础 j-hat 所跨的平行四边形，它的面积就是神秘向量的 x 坐标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.16,
  "end": 405.2
 },
 {
  "input": "Again, it's a strange way to represent the x-coordinate, but see what it buys us in a moment.",
  "translatedText": "同样，这种表示 x 坐标的方法也很奇怪，但稍后我们就会知道它能给我们带来什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 405.78,
  "end": 410.08
 },
 {
  "input": "And just to make sure it's clear how this might generalize, let's look in three dimensions.",
  "translatedText": "为了让大家清楚这一点，让我们从三个维度来看看。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.68,
  "end": 413.76
 },
 {
  "input": "Ordinarily, the way you might think about one of a vector's coordinates, say its z-coordinate, would be to take its dot product with the third standard basis vector, often called k-hat.",
  "translatedText": "通常情况下，你可能会考虑一个向量的坐标，比如它的 z 坐标，将其与第三个标准基向量（通常称为 k-hat）进行点积。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 414.3,
  "end": 423.56
 },
 {
  "input": "But an alternate geometric interpretation would be to consider the parallelepiped that it creates with the other two basis vectors, i-hat and j-hat.",
  "translatedText": "但另一种几何解释是，考虑它与另外两个基向量 i-hat 和 j-hat 形成的平行六面体。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 424.28,
  "end": 432.88
 },
 {
  "input": "If you think of the square with area 1 spanned by i-hat and j-hat as the base of this whole shape, then its volume is the same as its height, which is the third coordinate of our vector.",
  "translatedText": "如果把 i-hat 和 j-hat 所跨面积为 1 的正方形看作是整个图形的底面，那么它的体积就等于它的高度，也就是我们向量的第三个坐标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.42,
  "end": 443.58
 },
 {
  "input": "And likewise, the wacky way to think about the other coordinates of the vector would be to form a parallelepiped using the vector and then all of the basis vectors other than the one corresponding to the direction you're looking for.",
  "translatedText": "同样，考虑矢量其他坐标的古怪方法是，用矢量和所有基矢量（与你要找的方向对应的基矢量除外）组成一个平行六面体。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 444.34,
  "end": 455.44
 },
 {
  "input": "Then the volume of this gives you the coordinate.",
  "translatedText": "然后求出这个体积的坐标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 455.9,
  "end": 457.9
 },
 {
  "input": "Or rather, we should be talking about the signed volume of parallelepiped in the sense described in the determinant video using the right-hand rule.",
  "translatedText": "或者说，我们应该用右手定则来讨论行列式视频中描述的平行四边形的带符号体积。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.44,
  "end": 465.0
 },
 {
  "input": "So the order in which you list these three vectors matters.",
  "translatedText": "因此，列出这三个向量的顺序很重要。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.56,
  "end": 468.14
 },
 {
  "input": "That way, negative coordinates still make sense.",
  "translatedText": "这样，负坐标仍然有意义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 468.8,
  "end": 471.1
 },
 {
  "input": "Okay, so why think of coordinates as areas and volumes like this?",
  "translatedText": "好吧，为什么要这样把坐标看成面积和体积呢？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 472.04,
  "end": 475.24
 },
 {
  "input": "Well, as you apply some sort of matrix transformation, the areas of these parallelograms, well, they don't stay the same, they might get scaled up or down.",
  "translatedText": "嗯，当你应用某种矩阵变换时，这些平行四边形的面积，嗯，它们不会保持不变，可能会被放大或缩小。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.72,
  "end": 483.78
 },
 {
  "input": "But, and this is the key idea of determinants, all of the areas get scaled by the same amount, namely the determinant of our transformation matrix.",
  "translatedText": "但是，这也是行列式的关键所在，所有区域的比例都是相同的，即我们变换矩阵的行列式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.16,
  "end": 492.64
 },
 {
  "input": "For example, if you look at the parallelogram spanned by the vector where your first basis vector lands, which is the first column of the matrix, and the transformed version of xy, what is its area?",
  "translatedText": "例如，如果你观察第一个基向量所在的向量（即矩阵的第一列）和 xy 的变换版本所跨过的平行四边形，它的面积是多少？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 493.52,
  "end": 504.58
 },
 {
  "input": "Well, this is the transformed version of the parallelogram we were looking at earlier, the one whose area was the y-coordinate of the mystery input vector.",
  "translatedText": "这就是我们之前看到的平行四边形的转换版本，其面积就是神秘输入向量的 Y 坐标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 505.58,
  "end": 513.38
 },
 {
  "input": "So its area is just going to be the determinant of the transformation multiplied by that y-coordinate.",
  "translatedText": "因此，它的面积就是变换的行列式乘以 Y 坐标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 513.7,
  "end": 519.28
 },
 {
  "input": "So that means we can solve for y by taking the area of this new parallelogram in the output space divided by the determinant of the full transformation.",
  "translatedText": "因此，我们可以用输出空间中新平行四边形的面积除以完全变换的行列式来求解 y 的值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 520.18,
  "end": 529.88
 },
 {
  "input": "And how do you get that area?",
  "translatedText": "如何获得该区域？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 530.9,
  "end": 532.42
 },
 {
  "input": "Well, we know the coordinates for where the mystery input vector lands, that's the whole point of a linear system of equations.",
  "translatedText": "我们知道神秘输入向量的坐标，这就是线性方程组的意义所在。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 533.24,
  "end": 539.16
 },
 {
  "input": "So what you might do is create a new matrix whose first column is the same as that of our matrix, but whose second column is the output vector, and then you take its determinant.",
  "translatedText": "因此，你可以创建一个新矩阵，其第一列与我们的矩阵相同，但第二列是输出向量，然后求其行列式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.72,
  "end": 550.32
 },
 {
  "input": "So look at that, just using data from the output of the transformation, namely the columns of the matrix and the coordinates of our output vector, we can recover the y-coordinate of the mystery input vector, which is halfway to solving the system.",
  "translatedText": "你看，只需使用转换输出的数据，即矩阵的列和输出向量的坐标，我们就能求得神秘输入向量的 y 坐标，这就等于解决了系统的一半问题。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 551.26,
  "end": 564.4
 },
 {
  "input": "Likewise, the same idea can give us the x-coordinate.",
  "translatedText": "同样，同样的思路也能得出 x 坐标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 565.12,
  "end": 567.54
 },
 {
  "input": "Look at the parallelogram we defined earlier, which encodes the x-coordinate of the mystery input vector spanned by that vector and j-hat.",
  "translatedText": "看看我们之前定义的平行四边形，它编码了神秘输入向量的 x 坐标，该向量与 j-hat 相跨。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 568.0,
  "end": 575.74
 },
 {
  "input": "The transformed version of this guy is spanned by the output vector and the second column of the matrix, and its area will have been multiplied by the determinant of that matrix.",
  "translatedText": "这个家伙的转换版本由输出向量和矩阵的第二列跨过，其面积将乘以该矩阵的行列式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.4,
  "end": 586.92
 },
 {
  "input": "So to solve for x, you can take this new area divided by the determinant of the full transformation.",
  "translatedText": "因此，要求解 x，可以用这个新面积除以完全变换的行列式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.7,
  "end": 592.94
 },
 {
  "input": "And similar to what we did before, you can compute the area of that output parallelogram by creating a new matrix whose first column is the output vector and whose second column is the same as the original matrix.",
  "translatedText": "与我们之前所做的类似，你可以通过创建一个新矩阵来计算输出平行四边形的面积，该矩阵的第一列是输出向量，第二列与原始矩阵相同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 593.86,
  "end": 605.66
 },
 {
  "input": "So again, just using data from the output space, the numbers we see in our original linear system, we can solve for what x must be.",
  "translatedText": "因此，只要再次使用输出空间的数据，即我们在原始线性系统中看到的数字，我们就能求解 x 的值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.24,
  "end": 612.86
 },
 {
  "input": "This formula for finding the solutions to a linear system of equations is known as Cramer's rule.",
  "translatedText": "这个求线性方程组解的公式被称为克拉默法则。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.42,
  "end": 618.42
 },
 {
  "input": "Here, just to sanity check ourselves, plug in some numbers here.",
  "translatedText": "在这里，为了检验我们自己的理智，请输入一些数字。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 619.12,
  "end": 621.9
 },
 {
  "input": "The determinant of that top altered matrix is 4 plus 2, which is 6, and the bottom determinant is 2, so the x-coordinate should be 3.",
  "translatedText": "顶部改变矩阵的行列式为 4 加 2，即 6，底部行列式为 2，因此 x 坐标应为 3。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 622.26,
  "end": 630.82
 },
 {
  "input": "And indeed, looking back at the input vector we started with, the x-coordinate is 3.",
  "translatedText": "回过头来看输入向量，X 坐标确实是 3。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.44,
  "end": 635.46
 },
 {
  "input": "Likewise, Cramer's rule suggests that the y-coordinate should be 4 divided by 2, or 2, and that is in fact the y-coordinate of the input vector we were starting with.",
  "translatedText": "同样，根据克拉默法则，Y 坐标应该是 4 除以 2，即 2，而这实际上就是我们开始时输入向量的 Y 坐标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 636.32,
  "end": 646.5
 },
 {
  "input": "The case with three dimensions or more is similar, and I highly recommend you take a moment to pause and think through it yourself.",
  "translatedText": "三维或更多维的情况也类似，我强烈建议你花点时间停下来自己思考一下。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 647.38,
  "end": 653.48
 },
 {
  "input": "Here, I'll give you a little bit of momentum.",
  "translatedText": "在这里，我会给你一点动力。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.18,
  "end": 655.9
 },
 {
  "input": "What we have is a known transformation given by some 3x3 matrix and a known output vector given by the right side of our linear system, and we want to know what input lands on that output.",
  "translatedText": "我们有一个由某个 3x3 矩阵给出的已知变换，以及一个由线性系统右边给出的已知输出向量，我们想知道该输出的输入是什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.34,
  "end": 668.24
 },
 {
  "input": "And if you think of, say, the z-coordinate of that input vector as the volume of that special parallelepiped we were looking at earlier, spanned by i-hat, j-hat, and the mystery input vector, what happens to that volume after the transformation?",
  "translatedText": "如果把输入矢量的 z 坐标看作我们之前看到的那个特殊平行六面体的体积，即 i-hat、j-hat 和神秘输入矢量的跨度，那么变换之后，这个体积会发生什么变化呢？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.1,
  "end": 683.78
 },
 {
  "input": "And what are the various ways you can compute that volume?",
  "translatedText": "计算体积的方法有哪些？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 684.8,
  "end": 687.48
 },
 {
  "input": "Really, pause and take a moment to think through the details of generalizing this to higher dimensions, finding an expression for each coordinate of the solution to a larger linear system.",
  "translatedText": "真的，请暂停一下，花点时间思考一下将其推广到更高维度的细节，找到更大线性系统解的每个坐标的表达式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 688.34,
  "end": 697.42
 },
 {
  "input": "Thinking through more general cases like this and convincing yourself that it works and why it works is where all the learning really happens, much more so than listening to some dude on YouTube walk you through the same reasoning again.",
  "translatedText": "思考更多类似的一般案例，并说服自己它是可行的，以及为什么可行，这才是真正的学习，比听 YouTube 上的某个家伙再给你讲一遍同样的推理要强得多。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 698.06,
  "end": 708.5
 }
]