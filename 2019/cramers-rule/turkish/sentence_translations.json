[
 {
  "input": "In a previous video, I’ve talked about linear systems of equations, and I sort of brushed aside the discussion of actually computing solutions to these systems. ",
  "translatedText": "Önceki bir videoda doğrusal denklem sistemlerinden bahsetmiştim ve bu sistemlerin gerçek hesaplama çözümlerine ilişkin tartışmayı bir nevi bir kenara ittim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.2,
  "end": 19.14
 },
 {
  "input": "And while it’s true that number-crunching is something we typically leave to the computers, digging into some of these computational methods is a good litmus test for whether or not you actually understand what’s going on, since this is really where the rubber meets the road. ",
  "translatedText": "Sayıları hesaplamanın genellikle bilgisayarlara bıraktığımız bir şey olduğu doğru olsa da, bu hesaplamalı yöntemlerden bazılarını derinlemesine incelemek, gerçekte neler olup bittiğini anlayıp anlamadığınızı anlamak için iyi bir turnusol testidir, çünkü kauçuğun yolla buluştuğu yer burasıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.7,
  "end": 31.52
 },
 {
  "input": "Here I want to describe the geometry behind a certain method for computing solutions to these systems, known as Cramer’s rule. ",
  "translatedText": "Burada, bu sistemlerin çözümlerini hesaplamak için Cramer kuralı olarak bilinen belirli bir yöntemin arkasındaki geometriyi açıklamak istiyorum. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 32.12,
  "end": 38.88
 },
 {
  "input": "The relevant background needed here is an understanding of determinants, dot products, and of linear systems of equations, so be sure to watch the relevant videos on those topics if you’re unfamiliar or rusty. ",
  "translatedText": "Buradaki ilgili arka plan determinantları, biraz nokta çarpımları ve elbette doğrusal denklem sistemlerini anlamaktır; bu nedenle, yabancıysanız veya paslanmışsanız bu konularla ilgili videoları izlediğinizden emin olun. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.68,
  "end": 50.42
 },
 {
  "input": "But first! ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.02,
  "end": 51.44
 },
 {
  "input": "I should say up front that Cramer’s rule is not the best way for computing solutions to linear systems of equations. ",
  "translatedText": "Ancak öncelikle şunu söylemeliyim ki bu Cramer kuralı aslında doğrusal denklem sistemlerinin çözümlerini hesaplamak için en iyi yol değildir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.44,
  "end": 57.26
 },
 {
  "input": "Gaussian elimination, for example, will always be faster. ",
  "translatedText": "Örneğin Gauss eliminasyonu her zaman daha hızlı olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 58.14,
  "end": 61.26
 },
 {
  "input": "So why learn it? ",
  "translatedText": "Peki neden öğrenesiniz? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.98,
  "end": 63.52
 },
 {
  "input": "Think of this as a sort of cultural excursion; it’s a helpful exercise in deepening your knowledge of the theory of these systems. ",
  "translatedText": "Bunu bir tür kültürel gezi olarak düşünün. Bu sistemlerin ardındaki teoriye ilişkin bilginizi derinleştirmenize yardımcı olacak yararlı bir alıştırmadır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.78,
  "end": 70.46
 },
 {
  "input": "Wrapping your mind around this concept will help consolidate ideas from linear algebra, like the determinant and linear systems, by seeing how they relate to each other. ",
  "translatedText": "Zihninizi bu kavram etrafında toplamak, determinant ve doğrusal sistemler gibi doğrusal cebirdeki fikirlerin birbirleriyle nasıl ilişkili olduğunu görerek pekiştirmenize yardımcı olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.04,
  "end": 79.62
 },
 {
  "input": "Also, from a purely artistic standpoint, the ultimate result is just really pretty to think about, much more so that Gaussian elimination. ",
  "translatedText": "Ayrıca tamamen sanatsal bir bakış açısından bakıldığında, buradaki nihai sonucun düşünülmesi gerçekten çok hoş, Gauss elemesinden çok daha fazla. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.1,
  "end": 86.88
 },
 {
  "input": "Alright, so the setup here will be some linear system of equations, say with two unknowns, x and y, and two equations. ",
  "translatedText": "Tamam, buradaki düzen, örneğin iki bilinmeyenli, x ve y ve iki denklemli bir doğrusal denklem sistemi olacak. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.68,
  "end": 95.62
 },
 {
  "input": "In principle, everything we’re talking about will work systems with a larger number of unknowns, and the same number of equations. ",
  "translatedText": "Prensip olarak, bahsettiğimiz her şey daha fazla bilinmeyene ve aynı sayıda denkleme sahip sistemler için de işe yarayacaktır, ancak basitlik açısından daha küçük bir örneği aklımızda tutmak daha güzel olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 96.3,
  "end": 101.94
 },
 {
  "input": "But for simplicity, a smaller example is nicer to hold in our heads. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.44,
  "end": 105.58
 },
 {
  "input": "So as I talked about in a previous video, you can think of this setup geometrically as a certain known matrix transforming an unknown vector, [x; y], where you know what the output is going to be, in this case [-4; -2]. ",
  "translatedText": "Önceki bir videoda bahsettiğim gibi, bu düzeni geometrik olarak, bilinen bir matrisin bilinmeyen bir vektörü (x, y) dönüştürmesi olarak düşünebilirsiniz, burada çıktının ne olacağını bilirsiniz, bu durumda negatif 4, negatif 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.32,
  "end": 120.04
 },
 {
  "input": "Remember, the columns of this matrix tell you how the matrix acts as a transform, each one telling you where the basis vectors of the input space land. ",
  "translatedText": "Unutmayın, bu matrisin sütunları size bu matrisin nasıl bir dönüşüm gibi davrandığını anlatıyor; her biri size girdi uzayının temel vektörlerinin nereye düştüğünü söylüyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.8,
  "end": 130.08
 },
 {
  "input": "So this is a sort of puzzle, what input [x; y], is going to give you this output [-4; -2]? ",
  "translatedText": "Yani elimizde bir tür bulmaca var. Hangi girdi vektörü x, y bu çıktıya, eksi 4, eksi 2'ye inecek? Buradaki küçük bulmacamız hakkında düşünmenin bir yolu, verilen çıktı vektörünün matrisin sütunlarının doğrusal bir birleşimi olduğunu biliyoruz, x çarpı i-hat'ın indiği vektör artı y çarpı j-hat'ın indiği vektör, ama ne Amacımız bu doğrusal kombinasyonun tam olarak ne olması gerektiğini bulmak. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.86,
  "end": 137.6
 },
 {
  "input": "Remember, the type of answer you get here can depend on whether or not the transformation squishes all of space into a lower dimension. ",
  "translatedText": "Unutmayın, burada alacağınız yanıt türü, dönüşümün tüm uzayı daha düşük bir boyuta sıkıştırıp sıkıştırmamasına, yani sıfır determinantına sahip olup olmamasına bağlı olabilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 137.6,
  "end": 148.4
 },
 {
  "input": "That is if it has zero determinant. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 148.4,
  "end": 151.22
 },
 {
  "input": "In that case, either none of the inputs land on our given output or there are a whole bunch of inputs landing on that output. ",
  "translatedText": "Bu durumda, ya girdilerin hiçbiri verilen çıktıya ulaşmıyor ya da bu çıktıya inen bir sürü girdi var. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 151.22,
  "end": 156.22
 },
 {
  "input": "But for this video we’ll limit our view to the case of a non-zero determinant, meaning the output of this transformation still spans the full n-dimensional space it started in; every input lands on one and only one output and every output has one and only one input. ",
  "translatedText": "Ancak bu video için görüşümüzü sıfır olmayan bir determinant durumuyla sınırlayacağız, bu da bu dönüşümün çıktılarının hâlâ başladığı boyutsal uzayın tamamını kapsadığı anlamına geliyor. Her girdi bir ve yalnızca bir çıktıya ulaşır ve her çıktının bir ve yalnızca bir girdisi vardır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 157.24,
  "end": 168.94
 },
 {
  "input": "One way to think about our puzzle is that we know the given output vector is some linear combination of the columns of the matrix; x*(the vector where i-hat lands) + y*(the vector where j-hat lands), but we wish to compute what exactly x and y are. ",
  "translatedText": "İlk olarak size yanlış ama doğru yönde olan bir fikir göstereyim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.94,
  "end": 186.72
 },
 {
  "input": "As a first pass, let me show an idea that is wrong, but in the right direction. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 186.72,
  "end": 198.16
 },
 {
  "input": "The x-coordinate of this mystery input vector is what you get by taking its dot product with the first basis vector, [1; 0]. ",
  "translatedText": "Bu gizemli girdi vektörünün x koordinatı, birinci temel vektör olan 1, 0 ile nokta çarpımını alarak elde ettiğiniz şeydir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.8,
  "end": 205.44
 },
 {
  "input": "Likewise, the y-coordinate is what you get by dotting it with the second basis vector, [0; 1]. ",
  "translatedText": "Benzer şekilde, y koordinatı da onu ikinci temel vektör olan 0, 1 ile noktalayarak elde ettiğiniz şeydir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.16,
  "end": 211.4
 },
 {
  "input": "So maybe you hope that after the transformation, the dot products with the transformed version of the mystery vector with the transformed versions of the basis vectors will also be these coordinates x and y. ",
  "translatedText": "Belki de dönüşümden sonra gizemli vektörün dönüştürülmüş versiyonuyla nokta çarpımlarının dönüştürülmüş versiyonuyla bu koordinatlar, x ve y olacağını umuyorsunuz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.9,
  "end": 223.24
 },
 {
  "input": "That’d be fantastic because we know the transformed versions of each of these vectors. ",
  "translatedText": "Bu harika olurdu çünkü bu vektörlerin her birinin dönüştürülmüş versiyonunun ne olduğunu biliyoruz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.94,
  "end": 228.74
 },
 {
  "input": "There’s just one problem with this: it’s not at all true! ",
  "translatedText": "Bununla ilgili tek bir sorun var, bu kesinlikle doğru değil. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.18,
  "end": 234.2
 },
 {
  "input": "For most linear transformations, the dot product before and after the transformation will be very different. ",
  "translatedText": "Çoğu doğrusal dönüşüm için, dönüşümden önceki ve sonraki nokta çarpım çok farklı görünecektir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 234.64,
  "end": 240.24
 },
 {
  "input": "For example, you could have two vectors generally pointing in the same direction, with a positive dot product, which get pulled away from each other during the transformation, in such a way that they then have a negative dot product. ",
  "translatedText": "Örneğin, pozitif bir nokta çarpımla genellikle aynı yöne işaret eden ve dönüşüm sırasında birbirlerinden negatif bir nokta çarpım elde edecek şekilde ayrılan iki vektörünüz olabilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.8,
  "end": 251.52
 },
 {
  "input": "Likewise, if things start off perpendicular, with dot product zero, like the two basis vectors, there’s no guarantee that they will stay perpendicular after the transformation, preserving that zero dot product. ",
  "translatedText": "Benzer şekilde, iki temel vektör gibi, 0 nokta çarpımı ile dik olarak başlayan şeyler, dönüşümden sonra sıklıkla birbirlerine dik kalmazlar, yani 0 nokta çarpımını korumazlar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 252.22,
  "end": 263.48
 },
 {
  "input": "In the example we were looking at, dot products certainly aren’t preserved. ",
  "translatedText": "Ve az önce gösterdiğim örneğe baktığımızda, nokta çarpımları kesinlikle korunmaz, çoğu vektör uzadığı için büyüme eğilimindedirler. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 264.1,
  "end": 267.16
 },
 {
  "input": "They tend to get bigger since most vectors are getting stretched. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.5,
  "end": 269.94
 },
 {
  "input": "In fact, transformations which do preserve dot products are special enough to have their own name: Orthonormal transformations. ",
  "translatedText": "Aslında, burada önemli bir yan not, nokta çarpımlarını koruyan dönüşümler kendi adlarını alacak kadar özeldir, ortonormal dönüşümler. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.94,
  "end": 279.1
 },
 {
  "input": "These are the ones which leave all the basis vectors perpendicular to each other with unit lengths. ",
  "translatedText": "Bunlar, temel vektörlerin tamamını birbirine dik ve hala birim uzunluklarda bırakan vektörlerdir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 279.72,
  "end": 284.66
 },
 {
  "input": "You often think of these as rotation matrices. ",
  "translatedText": "Bunları genellikle dönme matrisleri olarak düşünürsünüz; bunlar, esneme, ezilme veya şekil değiştirmenin olmadığı katı harekete karşılık gelir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.74,
  "end": 287.88
 },
 {
  "input": "The correspond to rigid motion, with no stretching, squishing or morphing. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.42,
  "end": 292.2
 },
 {
  "input": "Solving a linear system with an orthonormal matrix is very easy: Since dot products are preserved, taking the dot product between the output vector and all the columns of your matrix will be the same as taking the dot products between the input vector and all the basis vectors, which is the same as finding the coordinates of the input vector. ",
  "translatedText": "Doğrusal bir sistemi ortonormal matrisle çözmek aslında çok kolaydır. Nokta çarpımlar korunduğu için, çıktı vektörü ile matrisinizin tüm sütunları arasındaki nokta çarpımı almak, gizemli girdi vektörü ile tüm temel vektörler arasındaki nokta çarpımı almakla aynı olacaktır. Bu gizemli girdinin koordinatları. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.0,
  "end": 312.98
 },
 {
  "input": "So, in that very special case, x would be the dot product of the first column with the output vector, and y would be the dot product of the second column with the output vector. ",
  "translatedText": "Yani bu çok özel durumda, x, birinci sütunun çıktı vektörü ile nokta çarpımı olacaktır ve y, ikinci sütunun çıktı vektörü ile nokta çarpımı olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.68,
  "end": 323.76
 },
 {
  "input": "Now, even though this idea breaks down for most linear systems, it points us in the direction of something to look for: Is there an alternate geometric understanding for the coordinates of our input vector which remains unchanged after the transformation? ",
  "translatedText": "Bu fikir neredeyse tüm lineer sistemler için geçerli değilken neden bunu gündeme getiriyorum? Bizi arayacağımız bir şeye yönlendiriyor. Giriş vektörümüzün koordinatlarının dönüşüm sonrasında değişmeden kalan alternatif bir geometrik anlayışı var mı? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.82,
  "end": 341.66
 },
 {
  "input": "If your mind has been mulling over determinants, you might think of this clever idea: Take the parallelogram defined by the first basis vector, i-hat, and the mystery input vector [x; y]. ",
  "translatedText": "Eğer zihniniz belirleyiciler üzerinde kafa yoruyorsa aşağıdaki akıllıca fikri düşünebilirsiniz. Birinci temel vektör i-hat ve gizemli girdi vektörü xy tarafından tanımlanan paralelkenarı alın. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.36,
  "end": 353.76
 },
 {
  "input": "The area of this parallelogram is its base, 1, times the height perpendicular to that base, which is the y-coordinate of our input vector. ",
  "translatedText": "Bu paralelkenarın alanı taban 1 çarpı bu tabana dik olan yüksekliktir, bu da giriş vektörünün y koordinatıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 354.44,
  "end": 362.96
 },
 {
  "input": "So, the area of this parallelogram is sort of a screwy roundabout way to describe the vector’s y-coordinate; it’s a wacky way to talk about coordinates, but run with me. ",
  "translatedText": "Yani bu paralelkenarın alanı, vektörün y koordinatını tanımlamanın bir çeşit çılgın dolambaçlı yoludur. Koordinatlar hakkında konuşmanın tuhaf bir yolu ama benimle koş. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 363.68,
  "end": 373.14
 },
 {
  "input": "Actually, to be more accurate, you should think of the signed area of this parallelogram, in the sense described by the determinant video. ",
  "translatedText": "Ve aslında, biraz daha doğru olmak gerekirse, bunu, determinant videoda anlatıldığı anlamda, paralelkenarın işaretli alanı olarak düşünmelisiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 373.7,
  "end": 381.64
 },
 {
  "input": "That way, a vector with negative y-coordinate would correspond to a negative area for this parallelogram. ",
  "translatedText": "Bu şekilde, negatif y koordinatına sahip bir vektör, bu paralelkenar için negatif bir alana karşılık gelecektir; en azından i-hat'ın bir anlamda paralelkenarı tanımlayan bu iki vektörden ilki olduğunu düşünüyorsanız. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.2,
  "end": 388.58
 },
 {
  "input": "Symmetrically, if you look at the parallelogram spanned by the vector and the second basis vector, j-hat, its area will be the x-coordinate of the vector. ",
  "translatedText": "Ve simetrik olarak, gizemli girdi vektörümüz ve ikinci taban j-hat tarafından yayılan paralelkenara bakarsanız, alanı o gizemli vektörün x koordinatı olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 388.96,
  "end": 392.96
 },
 {
  "input": "Again, it’s a strange way to represent the x-coordinate, but you’ll see what it buys us in a moment. ",
  "translatedText": "Yine, x koordinatını temsil etmenin tuhaf bir yolu ama birazdan bize ne kazandıracağını göreceksiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 392.96,
  "end": 398.78
 },
 {
  "input": "Here’s what this would look like in three-dimensions: Ordinarily the way you might think of one of a vector’s coordinate, say its z-coordinate, would be to take its dot product with the third standard basis vector, k-hat. ",
  "translatedText": "Bunun nasıl genelleştirilebileceğinin açık olduğundan emin olmak için üç boyutlu bakalım. Normalde, bir vektörün koordinatlarından biri, örneğin z koordinatı hakkında düşünme şekliniz, genellikle k-hat olarak adlandırılan üçüncü standart temel vektörle nokta çarpımını almak olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.78,
  "end": 410.08
 },
 {
  "input": "But instead, consider the parallelepiped it creates with the other two basis vectors, i-hat and j-hat. ",
  "translatedText": "Ancak alternatif bir geometrik yorum, diğer iki temel vektör olan i-hat ve j-hat ile oluşturduğu paralelyüzlüyü dikkate almak olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 410.68,
  "end": 412.64
 },
 {
  "input": "If you think of the square with area 1 spanned by i-hat and j-hat as the base of this guy, its volume is the same its height, which is the third coordinate of our vector. ",
  "translatedText": "Tüm bu şeklin tabanının i-hat ve j-hat tarafından kapsanan alanı 1 olan kareyi düşünürseniz, bu durumda hacmi, vektörümüzün üçüncü koordinatı olan yüksekliğiyle aynıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.74,
  "end": 417.14
 },
 {
  "input": "Likewise, the wacky way to think about any other coordinate of this vector is to form the parallelepiped between this vector an all the basis vectors other than the one you’re looking for, and get its volume. ",
  "translatedText": "Ve aynı şekilde, vektörün diğer koordinatları hakkında düşünmenin tuhaf yolu, vektörü ve ardından aradığınız yöne karşılık gelenin dışındaki tüm temel vektörleri kullanarak bir paralelyüz oluşturmak olacaktır. O zaman bunun hacmi size koordinatı verir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.14,
  "end": 429.72
 },
 {
  "input": "Or, rather, we should talk about the signed volume of these parallelepipeds, in the sense described in the determinant video, where the order in which you list the three vectors matters and you’re using the right-hand rule. ",
  "translatedText": "Daha doğrusu, determinant videoda sağ el kuralını kullanarak anlatılan anlamda paralelyüzlü işaretli hacimden bahsediyor olmalıyız. Dolayısıyla bu üç vektörü listelediğiniz sıra önemlidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.72,
  "end": 442.38
 },
 {
  "input": "That way negative coordinates still make sense. ",
  "translatedText": "Bu şekilde negatif koordinatlar hala anlamlıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.24
 },
 {
  "input": "Okay, so why think of coordinates as areas and volumes like this? ",
  "translatedText": "Peki neden koordinatları bunun gibi alanlar ve hacimler olarak düşünüyorsunuz? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.24,
  "end": 447.5
 },
 {
  "input": "As you apply some matrix transformation, the areas of the parallelograms don’t stay the same, they may get scaled up or down. ",
  "translatedText": "Bir çeşit matris dönüşümü uyguladığınızda, bu paralelkenarların alanları aynı kalmıyor, ölçekleri büyüyüp küçülebiliyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 450.0
 },
 {
  "input": "But(!), and this is a key idea of determinants, all these areas get scaled by the same amount. ",
  "translatedText": "Ancak determinantların ana fikri de budur, tüm alanlar aynı miktarda, yani dönüşüm matrisimizin determinantıyla ölçeklendirilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.0,
  "end": 453.96
 },
 {
  "input": "Namely, the determinant of our transformation matrix. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.96,
  "end": 457.9
 },
 {
  "input": "For example, if you look the parallelogram spanned by the vector where your first basis vector lands, which is the first column of the matrix, and the transformed version of [x; y], what is its area? ",
  "translatedText": "Örneğin, matrisin ilk sütunu olan ilk temel vektörünüzün bulunduğu vektör ile xy'nin dönüştürülmüş versiyonunun kapsadığı paralelkenara bakarsanız, alanı nedir? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.44,
  "end": 470.72
 },
 {
  "input": "Well, this is the transformed version of that parallelogram we were looking at earlier, whose area was the y-coordinate of the mystery input vector. ",
  "translatedText": "Bu, daha önce incelediğimiz paralelkenarın dönüştürülmüş versiyonu, alanı gizemli girdi vektörünün y koordinatı olan paralelkenar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.72,
  "end": 478.28
 },
 {
  "input": "So its area will be the determinant of the transformation multiplied by that value. ",
  "translatedText": "Yani alanı dönüşümün determinantı ile y koordinatının çarpımı olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 478.96,
  "end": 481.96
 },
 {
  "input": "So, the y-coordinate of our mystery input vector is the area of this parallelogram, spanned by the first column of the matrix and the output vector, divided by the determinant of the full transformation. ",
  "translatedText": "Yani bu, çıktı uzayındaki bu yeni paralelkenarın alanını tam dönüşümün determinantına bölerek y'yi çözebileceğimiz anlamına geliyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.96,
  "end": 492.16
 },
 {
  "input": "And how do you get this area? ",
  "translatedText": "Peki bu alanı nasıl elde edersiniz? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.16,
  "end": 494.88
 },
 {
  "input": "Well, we know the coordinates for where the mystery input vector lands, that’s the whole point of a linear system of equations. ",
  "translatedText": "Gizemli girdi vektörünün düştüğü yerin koordinatlarını biliyoruz, doğrusal denklem sisteminin tüm amacı budur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.88,
  "end": 499.88
 },
 {
  "input": "So, create a matrix whose first column is the same as that of our matrix, and whose second column is the output vector, and take its determinant. ",
  "translatedText": "Yapabileceğiniz şey, ilk sütunu bizim matrisimizinkiyle aynı olan ancak ikinci sütunu çıktı vektörü olan yeni bir matris yaratmak ve sonra onun determinantını almaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.88,
  "end": 512.88
 },
 {
  "input": "So look at that; just using data from the output of the transformation, namely the columns of the matrix and the coordinates of our output vector, we can recover the y-coordinate of our mystery input vector. ",
  "translatedText": "Şuna bakın, sadece dönüşümün çıktısından elde edilen verileri, yani matrisin sütunlarını ve çıktı vektörümüzün koordinatlarını kullanarak, sistemi çözmenin yarısı olan gizemli girdi vektörünün y-koordinatını kurtarabiliriz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.88,
  "end": 522.1
 },
 {
  "input": "Likewise, the same idea can get you the x-coordinate. ",
  "translatedText": "Aynı şekilde aynı fikir bize x koordinatını da verebilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 522.1,
  "end": 523.5
 },
 {
  "input": "Look at that parallelogram we defined early which encodes the x-coordinate of the mystery input vector, spanned by the input vector and j-hat. ",
  "translatedText": "Daha önce tanımladığımız, gizemli girdi vektörünün x koordinatını kodlayan, o vektör ve j-hat tarafından yayılan paralelkenara bakın. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 523.5,
  "end": 537.14
 },
 {
  "input": "The transformed version of this guy is spanned by the output vector and the second column of the matrix, and its area will have been multiplied by the determinant of the matrix. ",
  "translatedText": "Bu adamın dönüştürülmüş versiyonu çıktı vektörü ve matrisin ikinci sütunu tarafından kapsanır ve alanı bu matrisin determinantıyla çarpılmış olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 537.14,
  "end": 545.04
 },
 {
  "input": "So the x-coordinate of our mystery input vector is this area divided by the determinant of the transformation. ",
  "translatedText": "Yani x'i bulmak için bu yeni alanı tam dönüşümün determinantına bölebilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 545.06,
  "end": 555.14
 },
 {
  "input": "Symmetric to what we did before, you can compute the area of that output parallelogram by creating a new matrix whose first column is the output vector, and whose second column is the same as the original matrix. ",
  "translatedText": "Ve daha önce yaptığımıza benzer şekilde, ilk sütunu çıktı vektörü olan ve ikinci sütunu orijinal matrisle aynı olan yeni bir matris oluşturarak bu çıktı paralelkenarının alanını hesaplayabilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.6,
  "end": 563.42
 },
 {
  "input": "So again, just using data from the output space, the numbers we see in our original linear system, we can recover the x-coordinate of our mystery input vector. ",
  "translatedText": "Yani yine, sadece çıkış uzayındaki verileri, orijinal doğrusal sistemimizde gördüğümüz sayıları kullanarak x'in ne olması gerektiğini çözebiliriz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.42,
  "end": 573.42
 },
 {
  "input": "This formula for finding the solutions to a linear system of equations is known as Cramer’s rule. ",
  "translatedText": "Doğrusal bir denklem sisteminin çözümlerini bulmak için kullanılan bu formül Cramer kuralı olarak bilinir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.42,
  "end": 584.48
 },
 {
  "input": "Here, just to sanity check ourselves, plug in the numbers here. ",
  "translatedText": "Burada, akıl sağlığımızı kontrol etmek için buraya bazı sayıları girin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 584.48,
  "end": 585.34
 },
 {
  "input": "The determinant of that top altered matrix is 4+2, which is 6, and the bottom determinant is 2, so the x-coordinate should be 3. ",
  "translatedText": "Üstteki değiştirilmiş matrisin determinantı 4 artı 2 yani 6'dır ve alt determinantı 2'dir, dolayısıyla x koordinatı 3 olmalıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.34,
  "end": 592.94
 },
 {
  "input": "And indeed, looking back at that input vector we started with, it’s x-coordinate is 3. ",
  "translatedText": "Ve aslında, başladığımız girdi vektörüne baktığımızda x koordinatının 3 olduğunu görürüz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.86,
  "end": 604.34
 },
 {
  "input": "Likewise, Cramer’s rule suggests the y-coordinate should be 4/2, or 2, and that is indeed the y-coordinate of the input vector we started with here. ",
  "translatedText": "Benzer şekilde Cramer kuralı, y koordinatının 4 bölü 2 veya 2 olması gerektiğini ve bunun da başladığımız girdi vektörünün y koordinatı olduğunu öne sürer. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 604.34,
  "end": 607.72
 },
 {
  "input": "The case with three dimensions is similar, and I highly recommend you pause to think it through yourself. ",
  "translatedText": "3 veya daha fazla boyutlu durum da benzerdir ve biraz durup kendi başınıza düşünmenizi şiddetle tavsiye ederim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 607.72,
  "end": 618.42
 },
 {
  "input": "Here, I’ll give you a little momentum. ",
  "translatedText": "Burada size biraz ivme vereceğim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 619.12,
  "end": 621.58
 },
 {
  "input": "We have this known transformation, given by a 3x3 matrix, and a known output vector, given by the right side of our linear system, and we want to know what input vector lands on this output vector. ",
  "translatedText": "Elimizde 3x3'lük bir matris tarafından verilen bilinen bir dönüşüm ve doğrusal sistemimizin sağ tarafı tarafından verilen bilinen bir çıktı vektörü var ve biz bu çıktıya hangi girdinin geldiğini bilmek istiyoruz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.58,
  "end": 633.26
 },
 {
  "input": "If you think of, say, the z-coordinate of the input vector as the volume of this parallelepiped spanned by i-hat, j-hat, and the mystery input vector, what happens to the volume of this parallelepiped after the transformation? ",
  "translatedText": "Ve diyelim ki, bu giriş vektörünün z koordinatını, daha önce incelediğimiz özel paralelyüzün hacmi olarak düşünürseniz, i-hat, j-hat ve gizemli giriş vektörü tarafından kapsanır, bu hacme ne olur? dönüşümden sonra mı? Peki bu hacmi hesaplamanın çeşitli yolları nelerdir? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 633.26,
  "end": 644.64
 },
 {
  "input": "How can you compute that new volume? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.64,
  "end": 651.66
 },
 {
  "input": "Really, pause and take a moment to think through the details of generalizing this to higher dimensions; finding an expression for each coordinate of the solution to larger linear systems. ",
  "translatedText": "Gerçekten, durun ve bunu daha yüksek boyutlara genelleştirmenin ayrıntılarını düşünmek için bir dakikanızı ayırın, daha büyük bir doğrusal sistemin çözümünün her koordinatı için bir ifade bulun. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 651.66,
  "end": 664.68
 },
 {
  "input": "Thinking through more general cases and convincing yourself that it works is where all the learning will happen, much more so than listening to some dude on YouTube walk through the reasoning again. ",
  "translatedText": "Bunun gibi daha genel vakalar üzerinde düşünmek ve bunun işe yaradığına ve neden işe yaradığına kendinizi ikna etmek, tüm öğrenmenin gerçekte gerçekleştiği yerdir; YouTube'daki bir adamı dinlemek size aynı mantık yürütmeyi tekrar anlatmaktan çok daha fazlasıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.1,
  "end": 708.5
 }
]