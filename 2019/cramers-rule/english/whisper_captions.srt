1
00:00:11,200 --> 00:00:15,860
In a previous video I've talked about linear systems of equations, and I sort of brushed

2
00:00:15,860 --> 00:00:19,140
aside the discussion of actually computing solutions to these systems.

3
00:00:19,700 --> 00:00:23,080
And while it's true that number crunching is typically something we leave to the computers,

4
00:00:23,500 --> 00:00:27,420
digging into some of these computational methods is a good litmus test for whether or not you

5
00:00:27,420 --> 00:00:31,520
actually understand what's going on, since that's really where the rubber meets the road.

6
00:00:32,120 --> 00:00:36,420
Here I want to describe the geometry behind a certain method for computing solutions to

7
00:00:36,420 --> 00:00:42,480
these systems, known as Cramer's rule. The relevant background here is understanding determinants,

8
00:00:42,840 --> 00:00:47,260
a little bit of dot products, and of course linear systems of equations, so be sure to watch the

9
00:00:47,260 --> 00:00:52,740
relevant videos on those topics if you're unfamiliar or rusty. But first I should say up front that

10
00:00:52,740 --> 00:00:57,260
this Cramer's rule is not actually the best way for computing solutions to linear systems

11
00:00:58,140 --> 00:01:04,460
Gaussian elimination for example will always be faster. So why learn it? Well think of it as a

12
00:01:04,460 --> 00:01:08,760
sort of cultural excursion. It's a helpful exercise in deepening your knowledge of the

13
00:01:08,760 --> 00:01:13,780
theory behind these systems. Wrapping your mind around this concept is going to help consolidate

14
00:01:13,780 --> 00:01:19,160
ideas from linear algebra, like the determinant and linear systems, by seeing how they relate to

15
00:01:19,160 --> 00:01:24,600
each other. Also from a purely artistic standpoint the ultimate result here is just really pretty to

16
00:01:24,600 --> 00:01:30,920
think about, way more so than Gaussian elimination. Alright so the setup here will be some linear

17
00:01:30,920 --> 00:01:37,500
system of equations, say with two unknowns x and y and two equations. In principle everything we're

18
00:01:37,500 --> 00:01:41,380
talking about will also work for systems with larger number of unknowns and the same number

19
00:01:41,380 --> 00:01:45,580
of equations, but for simplicity a smaller example is just nicer to hold in our heads.

20
00:01:46,320 --> 00:01:51,380
So as I talked about in a previous video you can think of this setup geometrically as a certain

21
00:01:51,380 --> 00:01:57,860
known matrix transforming an unknown vector x y where you know what the output is going to be,

22
00:01:58,160 --> 00:02:03,920
in this case negative 4 negative 2. Remember the columns of this matrix are telling you how that

23
00:02:03,920 --> 00:02:10,080
matrix acts as a transform, each one telling you where the basis vectors of the input space land.

24
00:02:10,860 --> 00:02:17,600
So what we have is a sort of puzzle, which input vector x y is going to land on this output negative

25
00:02:17,600 --> 00:02:23,440
4 negative 2. One way to think about our little puzzle here is that we know the given output

26
00:02:23,440 --> 00:02:29,440
vector is some linear combination of the columns of the matrix x times the vector where i hat lands

27
00:02:29,440 --> 00:02:34,460
plus y times the vector where j hat lands, but what we want is to figure out what exactly that

28
00:02:34,460 --> 00:02:40,300
linear combination should be. Remember the type of answer you get here can depend on whether or not

29
00:02:40,300 --> 00:02:46,100
the transformation squishes all of space into a lower dimension, that is if it has a zero determinant.

30
00:02:46,100 --> 00:02:50,240
In that case either none of the inputs land on our given output,

31
00:02:50,580 --> 00:02:53,900
or there's a whole bunch of inputs landing on that output.

32
00:02:57,060 --> 00:03:02,760
But for this video we'll limit our view to the case of a non-zero determinant, meaning the outputs of

33
00:03:02,760 --> 00:03:07,140
this transformation still span the full in-dimensional space that it started in.

34
00:03:07,500 --> 00:03:12,700
Every input lands on one and only one output, and every output has one and only one input.

35
00:03:14,180 --> 00:03:19,360
As a first pass let me show you an idea that's wrong but in the right direction. The x coordinate

36
00:03:19,360 --> 00:03:24,620
of this mystery input vector is what you get by taking its dot product with the first basis vector

37
00:03:24,620 --> 00:03:31,400
1 0. Likewise the y coordinate is what you get by dotting it with the second basis vector 0 1.

38
00:03:31,900 --> 00:03:37,260
So maybe you hope that after the transformation the dot products with the transformed version

39
00:03:37,260 --> 00:03:42,040
of the mystery vector with the transformed version of the basis vectors will also be

40
00:03:42,040 --> 00:03:47,380
these coordinates x and y. That'd be fantastic because we know what the transformed version of

41
00:03:47,380 --> 00:03:55,280
each of those vectors are. There's just one problem with it, it's not at all true. For most linear

42
00:03:55,280 --> 00:04:00,940
transformations the dot product before and after the transformation will look very different. For

43
00:04:00,940 --> 00:04:05,000
example you could have two vectors generally pointing in the same direction with a positive

44
00:04:05,000 --> 00:04:09,800
dot product which get pulled apart from each other during the transformation in such a way that they

45
00:04:09,800 --> 00:04:15,380
have a negative dot product. Likewise things that start off perpendicular with dot product 0,

46
00:04:15,580 --> 00:04:20,100
like the two basis vectors, quite often don't stay perpendicular to each other after the

47
00:04:20,100 --> 00:04:25,320
transformation, that is they don't preserve that 0 dot product. And looking at the example I just

48
00:04:25,320 --> 00:04:29,420
showed dot products certainly aren't preserved, they tend to get bigger since most vectors are

49
00:04:29,420 --> 00:04:34,980
getting stretched out. In fact, worthwhile side note here, transformations which do preserve dot

50
00:04:34,980 --> 00:04:40,100
products are special enough to have their own name, orthonormal transformations. These are the

51
00:04:40,100 --> 00:04:44,240
ones that leave all of the basis vectors perpendicular to each other and still with unit

52
00:04:44,240 --> 00:04:50,480
lengths. You often think of these as the rotation matrices, they correspond to rigid motion with no

53
00:04:50,480 --> 00:04:55,600
stretching or squishing or morphing. Solving a linear system with an orthonormal matrix is

54
00:04:55,600 --> 00:05:01,140
actually super easy. Because dot products are preserved, taking the dot product between the

55
00:05:01,140 --> 00:05:06,580
vector and all the columns of your matrix will be the same as taking the dot product between the

56
00:05:06,580 --> 00:05:11,660
mystery input vector and all of the basis vectors, which is the same as just finding the coordinates

57
00:05:11,660 --> 00:05:17,980
of that mystery input. So in that very special case, x would be the dot product of the first

58
00:05:17,980 --> 00:05:23,380
column with the output vector, and y would be the dot product of the second column with the output

59
00:05:23,380 --> 00:05:30,860
vector. Why am I bringing this up when this idea breaks down for almost all linear systems?

60
00:05:31,420 --> 00:05:36,060
Well, it points us in a direction of something to look for. Is there an alternate geometric

61
00:05:36,060 --> 00:05:41,100
understanding for the coordinates of our input vector that remains unchanged after the

62
00:05:41,100 --> 00:05:46,060
transformation? If your mind has been mulling over determinants, you might think of the following

63
00:05:46,060 --> 00:05:52,760
clever idea. Take the parallelogram defined by the first basis vector i-hat and the mystery input

64
00:05:52,760 --> 00:05:59,860
vector xy. The area of this parallelogram is the base, 1, times the height perpendicular to that

65
00:05:59,860 --> 00:06:06,280
base, which is the y-coordinate of that input vector. So the area of that parallelogram is a

66
00:06:06,280 --> 00:06:11,340
sort of screwy roundabout way to describe the vector's y-coordinate. It's a wacky way to talk

67
00:06:11,340 --> 00:06:16,180
about coordinates, but run with me. And actually, to be a little more accurate, you should think of

68
00:06:16,180 --> 00:06:21,640
this as the signed area of that parallelogram, in the sense described in the determinant video.

69
00:06:22,200 --> 00:06:27,860
That way, a vector with a negative y-coordinate would correspond to a negative area for this

70
00:06:27,860 --> 00:06:32,600
parallelogram, at least if you think of i-hat as in some sense being the first out of these two

71
00:06:32,600 --> 00:06:37,360
vectors defining the parallelogram. And symmetrically, if you look at the parallelogram

72
00:06:37,360 --> 00:06:43,260
spanned by our mystery input vector and the second basis, j-hat, its area is going to be the

73
00:06:43,260 --> 00:06:48,640
x-coordinate of that mystery vector. Again, it's a strange way to represent the x-coordinate, but

74
00:06:48,640 --> 00:06:52,640
see what it buys us in a moment. And just to make sure it's clear how this might generalize,

75
00:06:52,740 --> 00:06:57,140
let's look in three dimensions. Ordinarily, the way you might think about one of a vector's

76
00:06:57,140 --> 00:07:01,780
coordinates, say its z-coordinate, would be to take its dot product with the third standard

77
00:07:01,780 --> 00:07:07,820
basis vector, often called k-hat. But an alternate geometric interpretation would be to consider the

78
00:07:08,320 --> 00:07:14,060
parallelepiped that it creates with the other two basis vectors, i-hat and j-hat. If you think of

79
00:07:14,060 --> 00:07:20,520
the square with area 1 spanned by i-hat and j-hat as the base of this whole shape, then its volume

80
00:07:20,520 --> 00:07:25,580
is the same as its height, which is the third coordinate of our vector. And likewise, the wacky

81
00:07:25,580 --> 00:07:29,780
way to think about the other coordinates of the vector would be to form a parallelepiped using

82
00:07:29,780 --> 00:07:34,880
the vector and then all of the basis vectors other than the one corresponding to the direction you're

83
00:07:34,880 --> 00:07:39,740
looking for. Then the volume of this gives you the coordinate. Or rather, we should be talking about

84
00:07:39,740 --> 00:07:44,360
the signed volume of parallelepiped in the sense described in the determinant video using the

85
00:07:44,360 --> 00:07:49,640
right-hand rule. So the order in which you list these three vectors matters. That way, negative

86
00:07:49,640 --> 00:07:55,240
coordinates still make sense. Okay, so why think of coordinates as areas and volumes like this?

87
00:07:55,720 --> 00:08:00,920
Well, as you apply some sort of matrix transformation, the areas of these parallelograms,

88
00:08:01,260 --> 00:08:05,880
well, they don't stay the same, they might get scaled up or down. But, and this is the key idea

89
00:08:05,880 --> 00:08:11,660
of determinants, all of the areas get scaled by the same amount, namely the determinant of our

90
00:08:11,660 --> 00:08:17,180
transformation matrix. For example, if you look at the parallelogram spanned by the vector where your

91
00:08:17,180 --> 00:08:22,540
first basis vector lands, which is the first column of the matrix, and the transformed version

92
00:08:22,540 --> 00:08:28,860
of xy, what is its area? Well, this is the transformed version of the parallelogram we were

93
00:08:28,860 --> 00:08:34,180
looking at earlier, the one whose area was the y-coordinate of the mystery input vector. So its

94
00:08:34,180 --> 00:08:39,280
area is just going to be the determinant of the transformation multiplied by that y-coordinate.

95
00:08:40,180 --> 00:08:46,320
So that means we can solve for y by taking the area of this new parallelogram in the output space

96
00:08:46,320 --> 00:08:52,420
divided by the determinant of the full transformation. And how do you get that area?

97
00:08:53,240 --> 00:08:57,440
Well, we know the coordinates for where the mystery input vector lands, that's the whole

98
00:08:57,440 --> 00:09:03,000
point of a linear system of equations. So what you might do is create a new matrix whose first column

99
00:09:03,000 --> 00:09:09,280
is the same as that of our matrix, but whose second column is the output vector, and then you

100
00:09:09,280 --> 00:09:15,900
take its determinant. So look at that, just using data from the output of the transformation, namely

101
00:09:15,900 --> 00:09:21,080
the columns of the matrix and the coordinates of our output vector, we can recover the y-coordinate

102
00:09:21,080 --> 00:09:26,480
of the mystery input vector, which is halfway to solving the system. Likewise, the same idea can

103
00:09:26,480 --> 00:09:31,920
give us the x-coordinate. Look at the parallelogram we defined earlier, which encodes the x-coordinate

104
00:09:31,920 --> 00:09:38,320
of the mystery input vector spanned by that vector and j-hat. The transformed version of this guy

105
00:09:38,320 --> 00:09:44,640
is spanned by the output vector and the second column of the matrix, and its area will have been

106
00:09:44,640 --> 00:09:50,340
multiplied by the determinant of that matrix. So to solve for x, you can take this new area

107
00:09:50,340 --> 00:09:55,100
divided by the determinant of the full transformation. And similar to what we did before,

108
00:09:55,340 --> 00:10:00,160
you can compute the area of that output parallelogram by creating a new matrix whose

109
00:10:00,160 --> 00:10:05,660
first column is the output vector and whose second column is the same as the original matrix.

110
00:10:06,240 --> 00:10:10,760
So again, just using data from the output space, the numbers we see in our original linear system,

111
00:10:11,120 --> 00:10:16,040
we can solve for what x must be. This formula for finding the solutions to a linear system

112
00:10:16,040 --> 00:10:21,580
of equations is known as Cramer's rule. Here, just to sanity check ourselves, plug in some numbers

113
00:10:21,580 --> 00:10:28,440
here. The determinant of that top altered matrix is 4 plus 2, which is 6, and the bottom determinant

114
00:10:28,440 --> 00:10:33,960
is 2, so the x-coordinate should be 3. And indeed, looking back at the input vector we started with,

115
00:10:34,140 --> 00:10:40,380
the x-coordinate is 3. Likewise, Cramer's rule suggests that the y-coordinate should be 4 divided

116
00:10:40,380 --> 00:10:46,500
by 2, or 2, and that is in fact the y-coordinate of the input vector we were starting with.

117
00:10:47,380 --> 00:10:52,260
The case with three dimensions or more is similar, and I highly recommend you take a moment to pause

118
00:10:52,260 --> 00:10:57,500
and think through it yourself. Here, I'll give you a little bit of momentum. What we have is a

119
00:10:57,500 --> 00:11:03,880
known transformation given by some 3x3 matrix and a known output vector given by the right side of

120
00:11:03,880 --> 00:11:10,160
our linear system, and we want to know what input lands on that output. And if you think of, say,

121
00:11:10,240 --> 00:11:15,500
the z-coordinate of that input vector as the volume of that special parallelepiped we were

122
00:11:15,500 --> 00:11:22,180
looking at earlier, spanned by i-hat, j-hat, and the mystery input vector, what happens to that volume

123
00:11:22,180 --> 00:11:27,480
after the transformation? And what are the various ways you can compute that volume?

124
00:11:28,340 --> 00:11:32,640
Really, pause and think through the details of generalizing this to higher dimensions,

125
00:11:33,020 --> 00:11:37,420
finding an expression for each coordinate of the solution to a larger linear system.

126
00:11:38,060 --> 00:11:42,820
Thinking through more general cases like this and convincing yourself that it works and why it works

127
00:11:42,820 --> 00:11:47,260
is where all the learning really happens, much more so than listening to some dude on YouTube

128
00:11:47,260 --> 00:11:49,200
walk you through the same reasoning again.

