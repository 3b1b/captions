1
00:00:11,200 --> 00:00:14,545
In a previous video I've talked about linear systems of equations, 

2
00:00:14,545 --> 00:00:18,440
and I sort of brushed aside the discussion of actually computing solutions to 

3
00:00:18,440 --> 00:00:19,140
these systems.

4
00:00:19,700 --> 00:00:22,622
And while it's true that number crunching is typically something we 

5
00:00:22,622 --> 00:00:25,416
leave to the computers, digging into some of these computational 

6
00:00:25,416 --> 00:00:29,284
methods is a good litmus test for whether or not you actually understand what's going on, 

7
00:00:29,284 --> 00:00:31,520
since that's really where the rubber meets the road.

8
00:00:32,119 --> 00:00:35,392
Here I want to describe the geometry behind a certain method 

9
00:00:35,392 --> 00:00:38,880
for computing solutions to these systems, known as Cramer's rule.

10
00:00:39,680 --> 00:00:42,622
The relevant background here is understanding determinants, 

11
00:00:42,622 --> 00:00:46,202
a little bit of dot products, and of course linear systems of equations, 

12
00:00:46,202 --> 00:00:50,420
so be sure to watch the relevant videos on those topics if you're unfamiliar or rusty.

13
00:00:51,020 --> 00:00:54,339
But first I should say up front that this Cramer's rule is 

14
00:00:54,339 --> 00:00:57,715
not actually the best way for computing solutions to linear 

15
00:00:57,715 --> 00:01:01,260
systems Gaussian elimination for example will always be faster.

16
00:01:01,980 --> 00:01:03,520
So why learn it?

17
00:01:03,780 --> 00:01:05,840
Well think of it as a sort of cultural excursion.

18
00:01:06,420 --> 00:01:10,460
It's a helpful exercise in deepening your knowledge of the theory behind these systems.

19
00:01:11,040 --> 00:01:15,232
Wrapping your mind around this concept is going to help consolidate ideas from linear 

20
00:01:15,232 --> 00:01:19,620
algebra, like the determinant and linear systems, by seeing how they relate to each other.

21
00:01:20,100 --> 00:01:23,586
Also from a purely artistic standpoint the ultimate result here is just 

22
00:01:23,586 --> 00:01:26,880
really pretty to think about, way more so than Gaussian elimination.

23
00:01:28,680 --> 00:01:32,723
Alright so the setup here will be some linear system of equations, 

24
00:01:32,723 --> 00:01:35,620
say with two unknowns x and y and two equations.

25
00:01:36,300 --> 00:01:39,467
In principle everything we're talking about will also work for systems 

26
00:01:39,467 --> 00:01:42,367
with larger number of unknowns and the same number of equations, 

27
00:01:42,367 --> 00:01:45,580
but for simplicity a smaller example is just nicer to hold in our heads.

28
00:01:46,320 --> 00:01:50,508
So as I talked about in a previous video you can think of this setup 

29
00:01:50,508 --> 00:01:55,061
geometrically as a certain known matrix transforming an unknown vector x y 

30
00:01:55,061 --> 00:02:00,040
where you know what the output is going to be, in this case negative 4 negative 2.

31
00:02:00,800 --> 00:02:06,027
Remember the columns of this matrix are telling you how that matrix acts as a transform, 

32
00:02:06,027 --> 00:02:10,080
each one telling you where the basis vectors of the input space land.

33
00:02:10,860 --> 00:02:14,662
So what we have is a sort of puzzle, which input vector 

34
00:02:14,662 --> 00:02:18,600
x y is going to land on this output negative 4 negative 2.

35
00:02:19,700 --> 00:02:23,625
One way to think about our little puzzle here is that we know the given 

36
00:02:23,625 --> 00:02:27,551
output vector is some linear combination of the columns of the matrix x 

37
00:02:27,551 --> 00:02:31,803
times the vector where i hat lands plus y times the vector where j hat lands, 

38
00:02:31,803 --> 00:02:36,220
but what we want is to figure out what exactly that linear combination should be.

39
00:02:37,240 --> 00:02:41,824
Remember the type of answer you get here can depend on whether or not the transformation 

40
00:02:41,824 --> 00:02:46,100
squishes all of space into a lower dimension, that is if it has a zero determinant.

41
00:02:46,100 --> 00:02:50,221
In that case either none of the inputs land on our given output, 

42
00:02:50,221 --> 00:02:53,900
or there's a whole bunch of inputs landing on that output.

43
00:02:57,060 --> 00:03:01,435
But for this video we'll limit our view to the case of a non-zero determinant, 

44
00:03:01,435 --> 00:03:04,647
meaning the outputs of this transformation still span the 

45
00:03:04,647 --> 00:03:07,140
full in-dimensional space that it started in.

46
00:03:07,500 --> 00:03:12,700
Every input lands on one and only one output, and every output has one and only one input.

47
00:03:14,180 --> 00:03:18,160
As a first pass let me show you an idea that's wrong but in the right direction.

48
00:03:18,800 --> 00:03:22,009
The x coordinate of this mystery input vector is what you 

49
00:03:22,009 --> 00:03:25,440
get by taking its dot product with the first basis vector 1 0.

50
00:03:26,160 --> 00:03:31,400
Likewise the y coordinate is what you get by dotting it with the second basis vector 0 1.

51
00:03:31,900 --> 00:03:35,772
So maybe you hope that after the transformation the dot products with 

52
00:03:35,772 --> 00:03:39,478
the transformed version of the mystery vector with the transformed 

53
00:03:39,478 --> 00:03:43,240
version of the basis vectors will also be these coordinates x and y.

54
00:03:43,940 --> 00:03:46,850
That'd be fantastic because we know what the transformed 

55
00:03:46,850 --> 00:03:48,740
version of each of those vectors are.

56
00:03:51,180 --> 00:03:54,200
There's just one problem with it, it's not at all true.

57
00:03:54,640 --> 00:03:57,465
For most linear transformations the dot product before 

58
00:03:57,465 --> 00:04:00,240
and after the transformation will look very different.

59
00:04:00,800 --> 00:04:04,135
For example you could have two vectors generally pointing in the same 

60
00:04:04,135 --> 00:04:07,803
direction with a positive dot product which get pulled apart from each other 

61
00:04:07,803 --> 00:04:11,520
during the transformation in such a way that they have a negative dot product.

62
00:04:12,220 --> 00:04:15,592
Likewise things that start off perpendicular with dot product 0, 

63
00:04:15,592 --> 00:04:19,380
like the two basis vectors, quite often don't stay perpendicular to each 

64
00:04:19,380 --> 00:04:23,480
other after the transformation, that is they don't preserve that 0 dot product.

65
00:04:24,100 --> 00:04:27,466
And looking at the example I just showed dot products certainly aren't preserved, 

66
00:04:27,466 --> 00:04:30,300
they tend to get bigger since most vectors are getting stretched out.

67
00:04:31,040 --> 00:04:34,912
In fact, worthwhile side note here, transformations which do preserve dot 

68
00:04:34,912 --> 00:04:39,100
products are special enough to have their own name, orthonormal transformations.

69
00:04:39,720 --> 00:04:42,167
These are the ones that leave all of the basis vectors 

70
00:04:42,167 --> 00:04:44,660
perpendicular to each other and still with unit lengths.

71
00:04:45,740 --> 00:04:48,334
You often think of these as the rotation matrices, 

72
00:04:48,334 --> 00:04:52,200
they correspond to rigid motion with no stretching or squishing or morphing.

73
00:04:53,000 --> 00:04:56,780
Solving a linear system with an orthonormal matrix is actually super easy.

74
00:04:57,260 --> 00:05:01,095
Because dot products are preserved, taking the dot product between the 

75
00:05:01,095 --> 00:05:05,038
vector and all the columns of your matrix will be the same as taking the 

76
00:05:05,038 --> 00:05:09,090
dot product between the mystery input vector and all of the basis vectors, 

77
00:05:09,090 --> 00:05:12,980
which is the same as just finding the coordinates of that mystery input.

78
00:05:13,680 --> 00:05:18,605
So in that very special case, x would be the dot product of the first column with the 

79
00:05:18,605 --> 00:05:23,760
output vector, and y would be the dot product of the second column with the output vector.

80
00:05:26,820 --> 00:05:30,860
Why am I bringing this up when this idea breaks down for almost all linear systems?

81
00:05:31,420 --> 00:05:34,080
Well, it points us in a direction of something to look for.

82
00:05:34,320 --> 00:05:37,935
Is there an alternate geometric understanding for the coordinates 

83
00:05:37,935 --> 00:05:41,660
of our input vector that remains unchanged after the transformation?

84
00:05:42,360 --> 00:05:44,674
If your mind has been mulling over determinants, 

85
00:05:44,674 --> 00:05:46,800
you might think of the following clever idea.

86
00:05:47,360 --> 00:05:50,728
Take the parallelogram defined by the first basis 

87
00:05:50,728 --> 00:05:53,760
vector i-hat and the mystery input vector xy.

88
00:05:54,440 --> 00:05:57,320
The area of this parallelogram is the base, 1, 

89
00:05:57,320 --> 00:06:01,672
times the height perpendicular to that base, which is the y-coordinate 

90
00:06:01,672 --> 00:06:02,960
of that input vector.

91
00:06:03,680 --> 00:06:06,849
So the area of that parallelogram is a sort of screwy 

92
00:06:06,849 --> 00:06:09,960
roundabout way to describe the vector's y-coordinate.

93
00:06:10,420 --> 00:06:13,140
It's a wacky way to talk about coordinates, but run with me.

94
00:06:13,700 --> 00:06:17,495
And actually, to be a little more accurate, you should think of this as the 

95
00:06:17,495 --> 00:06:21,640
signed area of that parallelogram, in the sense described in the determinant video.

96
00:06:22,200 --> 00:06:26,043
That way, a vector with a negative y-coordinate would correspond to a 

97
00:06:26,043 --> 00:06:30,162
negative area for this parallelogram, at least if you think of i-hat as in 

98
00:06:30,162 --> 00:06:34,500
some sense being the first out of these two vectors defining the parallelogram.

99
00:06:35,160 --> 00:06:38,488
And symmetrically, if you look at the parallelogram spanned 

100
00:06:38,488 --> 00:06:41,649
by our mystery input vector and the second basis, j-hat, 

101
00:06:41,649 --> 00:06:45,200
its area is going to be the x-coordinate of that mystery vector.

102
00:06:45,780 --> 00:06:48,415
Again, it's a strange way to represent the x-coordinate, 

103
00:06:48,415 --> 00:06:50,080
but see what it buys us in a moment.

104
00:06:50,680 --> 00:06:52,710
And just to make sure it's clear how this might generalize, 

105
00:06:52,710 --> 00:06:53,760
let's look in three dimensions.

106
00:06:54,300 --> 00:06:57,953
Ordinarily, the way you might think about one of a vector's coordinates, 

107
00:06:57,953 --> 00:07:00,957
say its z-coordinate, would be to take its dot product with 

108
00:07:00,957 --> 00:07:03,560
the third standard basis vector, often called k-hat.

109
00:07:04,280 --> 00:07:08,173
But an alternate geometric interpretation would be to consider the 

110
00:07:08,173 --> 00:07:12,880
parallelepiped that it creates with the other two basis vectors, i-hat and j-hat.

111
00:07:13,420 --> 00:07:16,697
If you think of the square with area 1 spanned by i-hat and 

112
00:07:16,697 --> 00:07:21,176
j-hat as the base of this whole shape, then its volume is the same as its height, 

113
00:07:21,176 --> 00:07:23,580
which is the third coordinate of our vector.

114
00:07:24,340 --> 00:07:28,103
And likewise, the wacky way to think about the other coordinates of the vector 

115
00:07:28,103 --> 00:07:31,771
would be to form a parallelepiped using the vector and then all of the basis 

116
00:07:31,771 --> 00:07:35,440
vectors other than the one corresponding to the direction you're looking for.

117
00:07:35,900 --> 00:07:37,900
Then the volume of this gives you the coordinate.

118
00:07:38,440 --> 00:07:41,720
Or rather, we should be talking about the signed volume of parallelepiped 

119
00:07:41,720 --> 00:07:45,000
in the sense described in the determinant video using the right-hand rule.

120
00:07:45,560 --> 00:07:48,140
So the order in which you list these three vectors matters.

121
00:07:48,800 --> 00:07:51,100
That way, negative coordinates still make sense.

122
00:07:52,040 --> 00:07:55,240
Okay, so why think of coordinates as areas and volumes like this?

123
00:07:55,720 --> 00:08:00,400
Well, as you apply some sort of matrix transformation, the areas of these parallelograms, 

124
00:08:00,400 --> 00:08:03,780
well, they don't stay the same, they might get scaled up or down.

125
00:08:04,160 --> 00:08:06,871
But, and this is the key idea of determinants, 

126
00:08:06,871 --> 00:08:09,640
all of the areas get scaled by the same amount, 

127
00:08:09,640 --> 00:08:12,640
namely the determinant of our transformation matrix.

128
00:08:13,520 --> 00:08:17,318
For example, if you look at the parallelogram spanned by the vector 

129
00:08:17,318 --> 00:08:21,675
where your first basis vector lands, which is the first column of the matrix, 

130
00:08:21,675 --> 00:08:24,580
and the transformed version of xy, what is its area?

131
00:08:25,580 --> 00:08:29,958
Well, this is the transformed version of the parallelogram we were looking at earlier, 

132
00:08:29,958 --> 00:08:33,380
the one whose area was the y-coordinate of the mystery input vector.

133
00:08:33,700 --> 00:08:36,490
So its area is just going to be the determinant of 

134
00:08:36,490 --> 00:08:39,280
the transformation multiplied by that y-coordinate.

135
00:08:40,179 --> 00:08:45,157
So that means we can solve for y by taking the area of this new parallelogram 

136
00:08:45,157 --> 00:08:49,880
in the output space divided by the determinant of the full transformation.

137
00:08:50,900 --> 00:08:52,420
And how do you get that area?

138
00:08:53,240 --> 00:08:56,596
Well, we know the coordinates for where the mystery input vector lands, 

139
00:08:56,596 --> 00:08:59,160
that's the whole point of a linear system of equations.

140
00:08:59,720 --> 00:09:05,079
So what you might do is create a new matrix whose first column is the same as that of our 

141
00:09:05,079 --> 00:09:10,320
matrix, but whose second column is the output vector, and then you take its determinant.

142
00:09:11,260 --> 00:09:15,090
So look at that, just using data from the output of the transformation, 

143
00:09:15,090 --> 00:09:19,080
namely the columns of the matrix and the coordinates of our output vector, 

144
00:09:19,080 --> 00:09:22,325
we can recover the y-coordinate of the mystery input vector, 

145
00:09:22,325 --> 00:09:24,400
which is halfway to solving the system.

146
00:09:25,120 --> 00:09:27,540
Likewise, the same idea can give us the x-coordinate.

147
00:09:28,000 --> 00:09:31,589
Look at the parallelogram we defined earlier, which encodes the 

148
00:09:31,589 --> 00:09:35,740
x-coordinate of the mystery input vector spanned by that vector and j-hat.

149
00:09:36,400 --> 00:09:41,719
The transformed version of this guy is spanned by the output vector and the second column 

150
00:09:41,719 --> 00:09:46,920
of the matrix, and its area will have been multiplied by the determinant of that matrix.

151
00:09:47,700 --> 00:09:50,110
So to solve for x, you can take this new area 

152
00:09:50,110 --> 00:09:52,940
divided by the determinant of the full transformation.

153
00:09:53,860 --> 00:09:57,627
And similar to what we did before, you can compute the area of that 

154
00:09:57,627 --> 00:10:01,615
output parallelogram by creating a new matrix whose first column is the 

155
00:10:01,615 --> 00:10:05,660
output vector and whose second column is the same as the original matrix.

156
00:10:06,240 --> 00:10:08,716
So again, just using data from the output space, 

157
00:10:08,716 --> 00:10:12,860
the numbers we see in our original linear system, we can solve for what x must be.

158
00:10:13,420 --> 00:10:16,048
This formula for finding the solutions to a linear 

159
00:10:16,048 --> 00:10:18,420
system of equations is known as Cramer's rule.

160
00:10:19,120 --> 00:10:21,900
Here, just to sanity check ourselves, plug in some numbers here.

161
00:10:22,260 --> 00:10:26,636
The determinant of that top altered matrix is 4 plus 2, which is 6, 

162
00:10:26,636 --> 00:10:30,820
and the bottom determinant is 2, so the x-coordinate should be 3.

163
00:10:31,440 --> 00:10:35,460
And indeed, looking back at the input vector we started with, the x-coordinate is 3.

164
00:10:36,320 --> 00:10:41,287
Likewise, Cramer's rule suggests that the y-coordinate should be 4 divided by 2, 

165
00:10:41,287 --> 00:10:46,500
or 2, and that is in fact the y-coordinate of the input vector we were starting with.

166
00:10:47,380 --> 00:10:49,754
The case with three dimensions or more is similar, 

167
00:10:49,754 --> 00:10:53,480
and I highly recommend you take a moment to pause and think through it yourself.

168
00:10:54,180 --> 00:10:55,900
Here, I'll give you a little bit of momentum.

169
00:10:56,340 --> 00:11:00,391
What we have is a known transformation given by some 3x3 matrix 

170
00:11:00,391 --> 00:11:04,948
and a known output vector given by the right side of our linear system, 

171
00:11:04,948 --> 00:11:08,240
and we want to know what input lands on that output.

172
00:11:09,100 --> 00:11:13,953
And if you think of, say, the z-coordinate of that input vector as the volume of 

173
00:11:13,953 --> 00:11:18,806
that special parallelepiped we were looking at earlier, spanned by i-hat, j-hat, 

174
00:11:18,806 --> 00:11:23,780
and the mystery input vector, what happens to that volume after the transformation?

175
00:11:24,800 --> 00:11:27,480
And what are the various ways you can compute that volume?

176
00:11:28,340 --> 00:11:32,959
Really, pause and think through the details of generalizing this to higher dimensions, 

177
00:11:32,959 --> 00:11:37,420
finding an expression for each coordinate of the solution to a larger linear system.

178
00:11:38,060 --> 00:11:41,495
Thinking through more general cases like this and convincing yourself that it 

179
00:11:41,495 --> 00:11:44,359
works and why it works is where all the learning really happens, 

180
00:11:44,359 --> 00:11:47,795
much more so than listening to some dude on YouTube walk you through the same 

181
00:11:47,795 --> 00:11:48,500
reasoning again.

