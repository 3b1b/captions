[
 {
  "input": "In a previous video I've talked about linear systems of equations, and I sort of brushed aside the discussion of actually computing solutions to these systems.",
  "translatedText": "이전 비디오에서 나는 선형 방정식 시스템에 대해 이야기했으며 이러한 시스템에 대한 실제로 컴퓨팅 솔루션에 대한 논의는 무시했습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 11.2,
  "end": 19.14
 },
 {
  "input": "And while it's true that number crunching is typically something we leave to the computers, digging into some of these computational methods is a good litmus test for whether or not you actually understand what's going on, since that's really where the rubber meets the road.",
  "translatedText": "그리고 숫자 분석이 일반적으로 컴퓨터에 맡기는 것이 사실이지만, 이러한 계산 방법 중 일부를 파헤치는 것은 실제로 무슨 일이 일어나고 있는지 이해하고 있는지 여부에 대한 좋은 리트머스 테스트입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 19.7,
  "end": 31.52
 },
 {
  "input": "Here I want to describe the geometry behind a certain method for computing solutions to these systems, known as Cramer's rule.",
  "translatedText": "여기서는 Cramer의 법칙으로 알려진 이러한 시스템에 대한 컴퓨팅 솔루션을 위한 특정 방법 뒤에 있는 기하학을 설명하고 싶습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 32.12,
  "end": 38.88
 },
 {
  "input": "The relevant background here is understanding determinants, a little bit of dot products, and of course linear systems of equations, so be sure to watch the relevant videos on those topics if you're unfamiliar or rusty.",
  "translatedText": "여기에 필요한 관련 배경 지식은 행렬식, 내적 및 방정식의 선형 시스템에 대한 이해이므로 익숙하지 않거나 녹슬지 않은 경우 해당 주제에 대한 관련 비디오를 시청하십시오. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.68,
  "end": 50.42
 },
 {
  "input": "But first I should say up front that this Cramer's rule is not actually the best way for computing solutions to linear systems of equations, Gaussian elimination for example will always be faster.",
  "translatedText": "하지만 먼저 이 크레이머의 법칙이 실제로 선형 방정식 시스템의 해를 계산하는 데 가장 좋은 방법은 아니며, 예를 들어 가우스 제거가 항상 더 빠르다는 점을 미리 말씀드리고 싶습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.02,
  "end": 61.26
 },
 {
  "input": "So why learn it?",
  "translatedText": "그렇다면 왜 배워야 할까요? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 61.98,
  "end": 63.52
 },
 {
  "input": "Well think of it as a sort of cultural excursion.",
  "translatedText": "일종의 문화 여행이라고 생각하시면 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.78,
  "end": 65.84
 },
 {
  "input": "It's a helpful exercise in deepening your knowledge of the theory behind these systems.",
  "translatedText": "이러한 시스템의 이론에 대한 지식을 심화시키는 데 도움이 되는 연습 문제입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.42,
  "end": 70.46
 },
 {
  "input": "Wrapping your mind around this concept is going to help consolidate ideas from linear algebra, like the determinant and linear systems, by seeing how they relate to each other.",
  "translatedText": "이 개념을 염두에 두는 것은 행렬식 및 선형 시스템과 같은 선형 대수학의 아이디어가 서로 어떻게 연관되어 있는지 확인함으로써 아이디어를 통합하는 데 도움이 될 것입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 71.04,
  "end": 79.62
 },
 {
  "input": "Also from a purely artistic standpoint the ultimate result here is just really pretty to think about, way more so than Gaussian elimination.",
  "translatedText": "또한 순전히 예술적인 관점에서 볼 때 여기서의 최종 결과는 가우스 제거보다 훨씬 더 생각하기 정말 아름답습니다. 좋습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.1,
  "end": 86.88
 },
 {
  "input": "Alright so the setup here will be some linear system of equations, say with two unknowns x and y and two equations.",
  "translatedText": "여기서 설정은 두 개의 미지수 x와 y와 두 개의 방정식을 사용하는 선형 방정식 시스템이 됩니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.68,
  "end": 95.62
 },
 {
  "input": "In principle everything we're talking about will also work for systems with larger number of unknowns and the same number of equations, but for simplicity a smaller example is just nicer to hold in our heads.",
  "translatedText": "원칙적으로 우리가 말하는 모든 것은 미지수의 수가 많고 방정식의 수가 같은 시스템에서도 작동하지만, 단순성을 위해 더 작은 예제를 머릿속에 떠올리는 것이 더 좋습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 96.3,
  "end": 105.58
 },
 {
  "input": "So as I talked about in a previous video you can think of this setup geometrically as a certain known matrix transforming an unknown vector x y where you know what the output is going to be, in this case negative 4 negative 2.",
  "translatedText": "이전 비디오에서 제가 이야기한 것처럼 이 설정은 기하학적으로 알 수 없는 벡터 [x; y], 여기서 출력이 어떻게 될지 알 수 있습니다. 이 경우에는 [-4; -2]. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 106.32,
  "end": 120.04
 },
 {
  "input": "Remember the columns of this matrix are telling you how that matrix acts as a transform, each one telling you where the basis vectors of the input space land.",
  "translatedText": "이 행렬의 열은 해당 행렬이 어떻게 변환으로 작동하는지 알려주며, 각 열은 입력 공간의 기본 벡터가 어디에 있는지 알려줍니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.8,
  "end": 130.08
 },
 {
  "input": "So what we have is a sort of puzzle, which input vector x y is going to land on this output negative 4 negative 2.",
  "translatedText": "따라서 우리가 가진 것은 일종의 퍼즐인데, 입력 벡터 x y가 이 출력 음수 4 음수 2에 도달하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.86,
  "end": 138.6
 },
 {
  "input": "One way to think about our little puzzle here is that we know the given output vector is some linear combination of the columns of the matrix x times the vector where i hat lands plus y times the vector where j hat lands, but what we want is to figure out what exactly that linear combination should be.",
  "translatedText": "여기서 작은 퍼즐을 생각하는 한 가지 방법은 주어진 출력 벡터가 행렬의 열에 i가 위치한 벡터의 x배와 j가 위치한 벡터의 y배를 더한 선형 조합이라는 것을 알고 있지만, 우리가 원하는 것은 그 선형 조합이 정확히 무엇이어야 하는지 알아내는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 139.7,
  "end": 156.22
 },
 {
  "input": "Remember the type of answer you get here can depend on whether or not the transformation squishes all of space into a lower dimension, that is if it has a zero determinant.",
  "translatedText": "여기서 얻을 수 있는 답의 유형은 변환이 모든 공간을 더 낮은 차원으로 쪼개는지 여부, 즉 결정자가 0인지 여부에 따라 달라질 수 있다는 점을 기억하세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 157.24,
  "end": 166.1
 },
 {
  "input": "In that case either none of the inputs land on our given output, or there's a whole bunch of inputs landing on that output.",
  "translatedText": "이 경우 입력 중 어느 것도 주어진 출력에 도달하지 않거나 입력이 모두 해당 출력에 도달합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 166.1,
  "end": 173.9
 },
 {
  "input": "But for this video we'll limit our view to the case of a non-zero determinant, meaning the outputs of this transformation still span the full in-dimensional space that it started in.",
  "translatedText": "하지만 이 동영상에서는 0이 아닌 결정자의 경우, 즉 이 변환의 출력이 여전히 변환이 시작된 전체 차원 공간에 걸쳐 있는 경우로 한정하여 살펴보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 187.14
 },
 {
  "input": "Every input lands on one and only one output, and every output has one and only one input.",
  "translatedText": "모든 입력은 하나의 출력으로 연결되고 모든 출력은 하나의 입력으로만 연결됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 187.5,
  "end": 192.7
 },
 {
  "input": "As a first pass let me show you an idea that's wrong but in the right direction.",
  "translatedText": "첫 번째 패스로 잘못되었지만 올바른 방향으로 나아가는 아이디어를 보여드리겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 194.18,
  "end": 198.16
 },
 {
  "input": "The x coordinate of this mystery input vector is what you get by taking its dot product with the first basis vector 1 0.",
  "translatedText": "이 미스터리 입력 벡터의 x 좌표는 첫 번째 기본 벡터 [1; 0]. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 198.8,
  "end": 205.44
 },
 {
  "input": "Likewise the y coordinate is what you get by dotting it with the second basis vector 0 1.",
  "translatedText": "마찬가지로 y 좌표는 두 번째 기본 벡터인 0, 1로 점을 찍어서 얻는 것입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 206.16,
  "end": 211.4
 },
 {
  "input": "So maybe you hope that after the transformation the dot products with the transformed version of the mystery vector with the transformed version of the basis vectors will also be these coordinates x and y.",
  "translatedText": "따라서 변환 후에 변환된 버전의 미스터리 벡터와 변환된 버전의 내적이 x 및 y 좌표가 되기를 바랄 수도 있습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 211.9,
  "end": 223.24
 },
 {
  "input": "That'd be fantastic because we know what the transformed version of each of those vectors are.",
  "translatedText": "우리는 각 벡터의 변환된 버전이 무엇인지 알고 있기 때문에 정말 환상적일 것입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.94,
  "end": 228.74
 },
 {
  "input": "There's just one problem with it, it's not at all true.",
  "translatedText": "다만 한 가지 문제가 있는데, 전혀 사실이 아닙니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.18,
  "end": 234.2
 },
 {
  "input": "For most linear transformations the dot product before and after the transformation will look very different.",
  "translatedText": "대부분의 선형 변환의 경우 변환 전후의 내적은 매우 다르게 보입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 234.64,
  "end": 240.24
 },
 {
  "input": "For example, you could have two vectors generally pointing in the same direction with a positive dot product, which get pulled apart from each other during the transformation in such a way that they end up having a negative dot product.",
  "translatedText": "예를 들어, 일반적으로 양의 내적을 사용하여 동일한 방향을 가리키는 두 개의 벡터가 있을 수 있습니다. 이 벡터는 변환 중에 서로 떨어져서 결국 음의 내적을 갖게 됩니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 240.8,
  "end": 251.52
 },
 {
  "input": "Likewise things that start off perpendicular with dot product 0, like the two basis vectors, quite often don't stay perpendicular to each other after the transformation, that is they don't preserve that 0 dot product.",
  "translatedText": "마찬가지로, 두 개의 기본 벡터처럼 내적 0과 수직으로 시작하는 것들은 변환 후에 서로 수직을 유지하지 않는 경우가 많습니다. 즉, 0 내적을 유지하지 않습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.22,
  "end": 263.48
 },
 {
  "input": "And looking at the example I just showed dot products certainly aren't preserved, they tend to get bigger since most vectors are getting stretched out.",
  "translatedText": "그리고 방금 보여드린 예시를 보면 도트 제품은 확실히 보존되지 않고 대부분의 벡터가 늘어나기 때문에 점점 커지는 경향이 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.1,
  "end": 270.3
 },
 {
  "input": "In fact, worthwhile side note here, transformations which do preserve dot products are special enough to have their own name, orthonormal transformations.",
  "translatedText": "여기서 주목할 점은 도트 프로덕트를 보존하는 변환은 오소노멀 변환이라는 고유한 이름을 가질 만큼 특별하다는 점입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 271.04,
  "end": 279.1
 },
 {
  "input": "These are the ones that leave all of the basis vectors perpendicular to each other and still with unit lengths.",
  "translatedText": "이는 모든 기본 벡터를 단위 길이로 서로 수직으로 유지하는 것입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 279.72,
  "end": 284.66
 },
 {
  "input": "You often think of these as the rotation matrices, they correspond to rigid motion with no stretching or squishing or morphing.",
  "translatedText": "흔히 회전 행렬이라고 생각하시는데, 이는 늘어나거나 찌그러지거나 변형되지 않는 단단한 동작에 해당합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 285.74,
  "end": 292.2
 },
 {
  "input": "Solving a linear system with an orthonormal matrix is actually super easy.",
  "translatedText": "직교 행렬로 선형 시스템을 푸는 것은 사실 매우 쉽습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.0,
  "end": 296.78
 },
 {
  "input": "Because dot products are preserved, taking the dot product between the output vector and all the columns of your matrix will be the same as taking the dot product between the mystery input vector and all of the basis vectors, which is the same as just finding the coordinates of that mystery input.",
  "translatedText": "도트 곱은 보존되므로 출력 벡터와 행렬의 모든 열 사이의 도트 곱을 구하는 것은 미스터리 입력 벡터와 모든 기저 벡터 사이의 도트 곱을 구하는 것과 동일하며, 이는 미스터리 입력의 좌표를 찾는 것과 동일합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 297.26,
  "end": 312.98
 },
 {
  "input": "So in that very special case, x would be the dot product of the first column with the output vector, and y would be the dot product of the second column with the output vector.",
  "translatedText": "따라서 아주 특별한 경우에 x는 출력 벡터와 첫 번째 열의 내적이고, y는 출력 벡터와 두 번째 열의 내적입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 323.76
 },
 {
  "input": "Why am I bringing this up when this idea breaks down for almost all linear systems?",
  "translatedText": "이 아이디어는 거의 모든 선형 시스템에서 실패하는데 왜 이 얘기를 꺼내는 걸까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 326.82,
  "end": 330.86
 },
 {
  "input": "Well, it points us in a direction of something to look for.",
  "translatedText": "우리가 찾아야 할 방향을 알려줍니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 331.42,
  "end": 334.08
 },
 {
  "input": "Is there an alternate geometric understanding for the coordinates of our input vector that remains unchanged after the transformation?",
  "translatedText": "변환 후에도 변하지 않는 입력 벡터의 좌표에 대한 대체 기하학적 이해가 있나요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 334.32,
  "end": 341.66
 },
 {
  "input": "If your mind has been mulling over determinants, you might think of the following clever idea.",
  "translatedText": "결정 요인에 대해 고민하고 있다면 다음과 같은 기발한 아이디어가 떠올랐을 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 342.36,
  "end": 346.8
 },
 {
  "input": "Take the parallelogram defined by the first basis vector i-hat and the mystery input vector xy.",
  "translatedText": "첫 번째 기저 벡터 i-hat와 미스터리 입력 벡터 xy로 정의된 평행 사변형을 취합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 347.36,
  "end": 353.76
 },
 {
  "input": "The area of this parallelogram is the base, 1, times the height perpendicular to that base, which is the y-coordinate of that input vector.",
  "translatedText": "이 평행사변형의 면적은 밑변에 수직인 높이(해당 입력 벡터의 y 좌표)에 밑변 1을 곱한 값입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 354.44,
  "end": 362.96
 },
 {
  "input": "So the area of that parallelogram is a sort of screwy roundabout way to describe the vector's y-coordinate.",
  "translatedText": "따라서 평행 사변형의 면적은 벡터의 y 좌표를 설명하는 일종의 회전식 로터리 방식입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 363.68,
  "end": 369.96
 },
 {
  "input": "It's a wacky way to talk about coordinates, but run with me.",
  "translatedText": "좌표에 대해 이야기하는 것은 엉뚱한 방법이지만 저와 함께 달려보세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 370.42,
  "end": 373.14
 },
 {
  "input": "And actually, to be a little more accurate, you should think of this as the signed area of that parallelogram, in the sense described in the determinant video.",
  "translatedText": "그리고 실제로 좀 더 정확하게 말하자면, 이것을 행렬식 비디오에서 설명한 의미에서 평행사변형의 부호 있는 영역으로 생각해야 합니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 373.7,
  "end": 381.64
 },
 {
  "input": "That way, a vector with a negative y-coordinate would correspond to a negative area for this parallelogram, at least if you think of i-hat as in some sense being the first out of these two vectors defining the parallelogram.",
  "translatedText": "이렇게 하면, 적어도 평행 사변형을 정의하는 두 벡터 중 첫 번째 벡터가 음수라고 생각하면 음의 Y 좌표를 가진 벡터는 이 평행 사변형의 음의 영역에 해당합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.2,
  "end": 394.5
 },
 {
  "input": "And symmetrically, if you look at the parallelogram spanned by our mystery input vector and the second basis, j-hat, its area is going to be the x-coordinate of that mystery vector.",
  "translatedText": "그리고 대칭적으로 미스터리 입력 벡터와 두 번째 기저인 j-모양으로 이루어진 평행사변형을 보면, 그 면적은 미스터리 벡터의 x 좌표가 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.16,
  "end": 405.2
 },
 {
  "input": "Again, it's a strange way to represent the x-coordinate, but see what it buys us in a moment.",
  "translatedText": "다시 말하지만, X 좌표를 표현하는 이상한 방법이지만 잠시 후에 이 방법이 우리에게 무엇을 가져다주는지 살펴보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 405.78,
  "end": 410.08
 },
 {
  "input": "And just to make sure it's clear how this might generalize, let's look in three dimensions.",
  "translatedText": "그리고 이것이 어떻게 일반화될 수 있는지 명확히 하기 위해 입체적으로 살펴봅시다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.68,
  "end": 413.76
 },
 {
  "input": "Ordinarily, the way you might think about one of a vector's coordinates, say its z-coordinate, would be to take its dot product with the third standard basis vector, often called k-hat.",
  "translatedText": "일반적으로 벡터의 좌표 중 하나, 예를 들어 z 좌표에 대해 생각할 수 있는 방법은 그 좌표의 점 곱을 세 번째 표준 기저 벡터, 흔히 K-모자라고 하는 벡터로 구하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 414.3,
  "end": 423.56
 },
 {
  "input": "But an alternate geometric interpretation would be to consider the parallelepiped that it creates with the other two basis vectors, i-hat and j-hat.",
  "translatedText": "그러나 다른 기하학적 해석은 다른 두 기저 벡터인 i-햇과 j-햇으로 생성되는 평행 육면체를 고려하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 424.28,
  "end": 432.88
 },
 {
  "input": "If you think of the square with area 1 spanned by i-hat and j-hat as the base of this whole shape, then its volume is the same as its height, which is the third coordinate of our vector.",
  "translatedText": "면적 1의 정사각형을 i모와 j모에 걸쳐 있는 정사각형을 이 전체 도형의 밑면이라고 생각하면, 그 부피는 벡터의 세 번째 좌표인 높이와 동일합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.42,
  "end": 443.58
 },
 {
  "input": "And likewise, the wacky way to think about the other coordinates of the vector would be to form a parallelepiped using the vector and then all of the basis vectors other than the one corresponding to the direction you're looking for.",
  "translatedText": "마찬가지로 벡터의 다른 좌표에 대해 생각하는 엉뚱한 방법은 벡터와 찾고 있는 방향에 해당하는 벡터를 제외한 모든 기저 벡터를 사용하여 평행 육면체를 형성하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 444.34,
  "end": 455.44
 },
 {
  "input": "Then the volume of this gives you the coordinate.",
  "translatedText": "그런 다음 이 볼륨을 통해 좌표를 얻을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 455.9,
  "end": 457.9
 },
 {
  "input": "Or rather, we should be talking about the signed volume of parallelepiped in the sense described in the determinant video using the right-hand rule.",
  "translatedText": "또는 오른손 규칙을 사용하여 결정자 비디오에서 설명한 의미에서 평행 육면체의 부호에 대해 이야기해야 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.44,
  "end": 465.0
 },
 {
  "input": "So the order in which you list these three vectors matters.",
  "translatedText": "따라서 이 세 가지 벡터를 나열하는 순서가 중요합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.56,
  "end": 468.14
 },
 {
  "input": "That way, negative coordinates still make sense.",
  "translatedText": "이렇게 하면 음수 좌표도 여전히 의미가 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 468.8,
  "end": 471.1
 },
 {
  "input": "Okay, so why think of coordinates as areas and volumes like this?",
  "translatedText": "그렇다면 좌표를 이렇게 면적과 부피로 생각하는 이유는 무엇일까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 472.04,
  "end": 475.24
 },
 {
  "input": "Well, as you apply some sort of matrix transformation, the areas of these parallelograms, well, they don't stay the same, they might get scaled up or down.",
  "translatedText": "일종의 행렬 변환을 적용하면 이 평행 사변형의 영역이 동일하게 유지되지 않고 확대되거나 축소될 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.72,
  "end": 483.78
 },
 {
  "input": "But, and this is the key idea of determinants, all of the areas get scaled by the same amount, namely the determinant of our transformation matrix.",
  "translatedText": "그러나 이것이 결정자의 핵심 개념이며, 모든 영역은 변환 행렬의 결정자, 즉 모든 영역이 같은 양만큼 스케일링됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.16,
  "end": 492.64
 },
 {
  "input": "For example, if you look at the parallelogram spanned by the vector where your first basis vector lands, which is the first column of the matrix, and the transformed version of xy, what is its area?",
  "translatedText": "예를 들어, 행렬의 첫 번째 기저 벡터가 놓인 벡터, 즉 행렬의 첫 번째 열과 변환된 버전의 xy가 걸쳐 있는 평행사변형을 보면 그 면적은 얼마인가요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 493.52,
  "end": 504.58
 },
 {
  "input": "Well, this is the transformed version of the parallelogram we were looking at earlier, the one whose area was the y-coordinate of the mystery input vector.",
  "translatedText": "이것은 앞서 살펴본 평행 사변형의 변형된 버전으로, 그 면적이 미스터리 입력 벡터의 y 좌표인 사변형입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 505.58,
  "end": 513.38
 },
 {
  "input": "So its area is just going to be the determinant of the transformation multiplied by that y-coordinate.",
  "translatedText": "따라서 그 면적은 변환의 결정 요소에 해당 Y 좌표를 곱한 값입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 513.7,
  "end": 519.28
 },
 {
  "input": "So that means we can solve for y by taking the area of this new parallelogram in the output space divided by the determinant of the full transformation.",
  "translatedText": "즉, 출력 공간에서 이 새로운 평행 사변형의 면적을 전체 변환의 결정식으로 나누어 y를 구할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 520.18,
  "end": 529.88
 },
 {
  "input": "And how do you get that area?",
  "translatedText": "그 영역은 어떻게 확보할 수 있을까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 530.9,
  "end": 532.42
 },
 {
  "input": "Well, we know the coordinates for where the mystery input vector lands, that's the whole point of a linear system of equations.",
  "translatedText": "미스터리 입력 벡터가 어디에 위치할지 좌표를 알면 선형 방정식의 핵심을 파악할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 533.24,
  "end": 539.16
 },
 {
  "input": "So what you might do is create a new matrix whose first column is the same as that of our matrix, but whose second column is the output vector, and then you take its determinant.",
  "translatedText": "따라서 첫 번째 열은 행렬의 열과 같지만 두 번째 열이 출력 벡터인 새 행렬을 만든 다음 행렬의 행렬식을 구할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.72,
  "end": 550.32
 },
 {
  "input": "So look at that, just using data from the output of the transformation, namely the columns of the matrix and the coordinates of our output vector, we can recover the y-coordinate of the mystery input vector, which is halfway to solving the system.",
  "translatedText": "변환의 출력 데이터, 즉 행렬의 열과 출력 벡터의 좌표를 사용하면 시스템을 푸는 데 절반이 되는 미스터리 입력 벡터의 Y 좌표를 복구할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 551.26,
  "end": 564.4
 },
 {
  "input": "Likewise, the same idea can give us the x-coordinate.",
  "translatedText": "마찬가지로 동일한 아이디어로 X 좌표를 얻을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 565.12,
  "end": 567.54
 },
 {
  "input": "Look at the parallelogram we defined earlier, which encodes the x-coordinate of the mystery input vector spanned by that vector and j-hat.",
  "translatedText": "앞서 정의한 평행사변형을 살펴보면, 해당 벡터와 j-모자에 걸쳐 있는 미스터리 입력 벡터의 x 좌표를 인코딩합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 568.0,
  "end": 575.74
 },
 {
  "input": "The transformed version of this guy is spanned by the output vector and the second column of the matrix, and its area will have been multiplied by the determinant of that matrix.",
  "translatedText": "이 녀석의 변환된 버전은 출력 벡터와 행렬의 두 번째 열에 걸쳐 있으며, 그 면적에 해당 행렬의 행렬식이 곱해집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.4,
  "end": 586.92
 },
 {
  "input": "So to solve for x, you can take this new area divided by the determinant of the full transformation.",
  "translatedText": "따라서 x를 풀려면 이 새로운 영역을 전체 변환의 결정식으로 나누면 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.7,
  "end": 592.94
 },
 {
  "input": "And similar to what we did before, you can compute the area of that output parallelogram by creating a new matrix whose first column is the output vector and whose second column is the same as the original matrix.",
  "translatedText": "그리고 이전에 했던 것과 유사하게 첫 번째 열이 출력 벡터이고 두 번째 열이 원래 행렬과 동일한 새 행렬을 만들어 출력 평행 사변형의 면적을 계산할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 593.86,
  "end": 605.66
 },
 {
  "input": "So again, just using data from the output space, the numbers we see in our original linear system, we can solve for what x must be.",
  "translatedText": "다시 말하지만, 출력 공간의 데이터, 즉 원래 선형 시스템에서 볼 수 있는 숫자를 사용하면 x가 무엇인지 풀 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.24,
  "end": 612.86
 },
 {
  "input": "This formula for finding the solutions to a linear system of equations is known as Cramer's rule.",
  "translatedText": "선형 방정식의 해를 구하는 이 공식을 크레이머의 법칙이라고 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.42,
  "end": 618.42
 },
 {
  "input": "Here, just to sanity check ourselves, plug in some numbers here.",
  "translatedText": "정신을 차리기 위해 여기에 몇 가지 숫자를 입력해 보세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 619.12,
  "end": 621.9
 },
 {
  "input": "The determinant of that top altered matrix is 4 plus 2, which is 6, and the bottom determinant is 2, so the x-coordinate should be 3.",
  "translatedText": "위쪽 변경 행렬의 행렬식은 4에 2를 더한 6이고 아래쪽 행렬식은 2이므로 X 좌표는 3이 되어야 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 622.26,
  "end": 630.82
 },
 {
  "input": "And indeed, looking back at the input vector we started with, the x-coordinate is 3.",
  "translatedText": "실제로 우리가 처음에 입력한 벡터를 다시 살펴보면 x 좌표는 3입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.44,
  "end": 635.46
 },
 {
  "input": "Likewise, Cramer's rule suggests that the y-coordinate should be 4 divided by 2, or 2, and that is in fact the y-coordinate of the input vector we were starting with.",
  "translatedText": "마찬가지로 크레이머의 규칙에 따르면 Y 좌표는 4를 2로 나눈 값, 즉 2가 되어야 하며, 실제로 이것이 우리가 시작했던 입력 벡터의 Y 좌표입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 636.32,
  "end": 646.5
 },
 {
  "input": "The case with three dimensions or more is similar, and I highly recommend you take a moment to pause and think through it yourself.",
  "translatedText": "3차원 이상의 경우도 비슷하며, 잠시 멈춰서 직접 생각해 보시기를 적극 권장합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 647.38,
  "end": 653.48
 },
 {
  "input": "Here, I'll give you a little bit of momentum.",
  "translatedText": "여기, 약간의 추진력을 드리겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.18,
  "end": 655.9
 },
 {
  "input": "What we have is a known transformation given by some 3x3 matrix and a known output vector given by the right side of our linear system, and we want to know what input lands on that output.",
  "translatedText": "우리가 가진 것은 3x3 행렬에 의해 주어진 알려진 변환과 선형 시스템의 오른쪽에 주어진 알려진 출력 벡터이며, 우리는 그 출력에 어떤 입력이 있는지 알고 싶습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.34,
  "end": 668.24
 },
 {
  "input": "And if you think of, say, the z-coordinate of that input vector as the volume of that special parallelepiped we were looking at earlier, spanned by i-hat, j-hat, and the mystery input vector, what happens to that volume after the transformation?",
  "translatedText": "예를 들어 입력 벡터의 z 좌표를 앞서 살펴본 특수 평행 육면체의 체적이라고 생각하면, i-모자, j-모자, 미스터리 입력 벡터에 걸쳐 있는 이 체적은 변환 후 어떻게 될까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.1,
  "end": 683.78
 },
 {
  "input": "And what are the various ways you can compute that volume?",
  "translatedText": "그렇다면 이 볼륨을 계산할 수 있는 다양한 방법에는 어떤 것이 있을까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 684.8,
  "end": 687.48
 },
 {
  "input": "Really, pause and take a moment to think through the details of generalizing this to higher dimensions, finding an expression for each coordinate of the solution to a larger linear system.",
  "translatedText": "잠시 멈춰서 이것을 더 높은 차원으로 일반화하여 더 큰 선형 시스템에 대한 해의 각 좌표에 대한 식을 찾는 세부 사항을 생각해 보시기 바랍니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 688.34,
  "end": 697.42
 },
 {
  "input": "Thinking through more general cases like this and convincing yourself that it works and why it works is where all the learning really happens, much more so than listening to some dude on YouTube walk you through the same reasoning again.",
  "translatedText": "이와 같은 일반적인 사례를 통해 그것이 효과가 있고 왜 효과가 있는지 스스로 납득하는 것이 유튜브에서 어떤 사람이 같은 논리를 다시 설명하는 것을 듣는 것보다 훨씬 더 많은 학습이 이루어지는 곳입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 698.06,
  "end": 708.5
 }
]