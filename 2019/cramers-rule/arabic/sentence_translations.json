[
 {
  "input": "In a previous video, I’ve talked about linear systems of equations, and I sort of brushed aside the discussion of actually computing solutions to these systems. ",
  "translatedText": "لقد تحدثت في مقطع فيديو سابق عن أنظمة المعادلات الخطية، وقمت بتجاهل مناقشة الحلول الحاسوبية الفعلية لهذه الأنظمة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.2,
  "end": 19.14
 },
 {
  "input": "And while it’s true that number-crunching is something we typically leave to the computers, digging into some of these computational methods is a good litmus test for whether or not you actually understand what’s going on, since this is really where the rubber meets the road. ",
  "translatedText": "وعلى الرغم من أنه من الصحيح أن معالجة الأرقام هي عادةً شيء نتركه لأجهزة الكمبيوتر، فإن البحث في بعض هذه الأساليب الحسابية يعد اختبارًا جيدًا لمعرفة ما إذا كنت تفهم بالفعل ما يحدث أم لا، نظرًا لأن هذا هو المكان الذي يلتقي فيه المطاط بالطريق. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 19.7,
  "end": 31.52
 },
 {
  "input": "Here I want to describe the geometry behind a certain method for computing solutions to these systems, known as Cramer’s rule. ",
  "translatedText": "أريد هنا أن أصف الهندسة وراء طريقة معينة لحلول الحوسبة لهذه الأنظمة، والمعروفة باسم قاعدة كرامر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 32.12,
  "end": 38.88
 },
 {
  "input": "The relevant background needed here is an understanding of determinants, dot products, and of linear systems of equations, so be sure to watch the relevant videos on those topics if you’re unfamiliar or rusty. ",
  "translatedText": "الخلفية ذات الصلة المطلوبة هنا هي فهم المحددات والمنتجات النقطية وأنظمة المعادلات الخطية، لذا تأكد من مشاهدة مقاطع الفيديو ذات الصلة بهذه المواضيع إذا كنت غير مألوف أو صدئ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.68,
  "end": 50.42
 },
 {
  "input": "But first! ",
  "translatedText": "لكن اولا! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.02,
  "end": 51.44
 },
 {
  "input": "I should say up front that Cramer’s rule is not the best way for computing solutions to linear systems of equations. ",
  "translatedText": "يجب أن أقول مقدمًا أن قاعدة كرامر ليست أفضل طريقة لحلول الحوسبة لأنظمة المعادلات الخطية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.44,
  "end": 57.26
 },
 {
  "input": "Gaussian elimination, for example, will always be faster. ",
  "translatedText": "على سبيل المثال، سيكون الحذف الغاوسي أسرع دائمًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 58.14,
  "end": 61.26
 },
 {
  "input": "So why learn it? ",
  "translatedText": "فلماذا تعلم ذلك؟ حسنًا، فكر في الأمر كنوع من الرحلة الثقافية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.98,
  "end": 63.52
 },
 {
  "input": "Think of this as a sort of cultural excursion; it’s a helpful exercise in deepening your knowledge of the theory of these systems. ",
  "translatedText": "إنه تمرين مفيد في تعميق معرفتك بالنظرية الكامنة وراء هذه الأنظمة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.78,
  "end": 70.46
 },
 {
  "input": "Wrapping your mind around this concept will help consolidate ideas from linear algebra, like the determinant and linear systems, by seeing how they relate to each other. ",
  "translatedText": "إن لف عقلك حول هذا المفهوم سيساعد في دمج الأفكار من الجبر الخطي، مثل المحددات والأنظمة الخطية، من خلال رؤية كيفية ارتباطها ببعضها البعض. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.04,
  "end": 79.62
 },
 {
  "input": "Also, from a purely artistic standpoint, the ultimate result is just really pretty to think about, much more so that Gaussian elimination. ",
  "translatedText": "أيضًا، من وجهة نظر فنية بحتة، فإن النتيجة النهائية هنا جميلة حقًا للتفكير فيها، أكثر بكثير من الإزالة الغوسية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.1,
  "end": 86.88
 },
 {
  "input": "Alright, so the setup here will be some linear system of equations, say with two unknowns, x and y, and two equations. ",
  "translatedText": "حسنًا، الإعداد هنا سيكون عبارة عن نظام خطي من المعادلات، مثلًا مع مجهولين، x وy، ومعادلتين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.68,
  "end": 95.62
 },
 {
  "input": "In principle, everything we’re talking about will work systems with a larger number of unknowns, and the same number of equations. ",
  "translatedText": "من حيث المبدأ، كل ما نتحدث عنه سيعمل بأنظمة تحتوي على عدد أكبر من المجهولات، ونفس عدد المعادلات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 96.3,
  "end": 101.94
 },
 {
  "input": "But for simplicity, a smaller example is nicer to hold in our heads. ",
  "translatedText": "لكن من أجل التبسيط، من الأفضل أن نحتفظ بمثال أصغر في رؤوسنا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 102.44,
  "end": 105.58
 },
 {
  "input": "So as I talked about in a previous video, you can think of this setup geometrically as a certain known matrix transforming an unknown vector, [x; y], where you know what the output is going to be, in this case [-4; -2]. ",
  "translatedText": "وكما تحدثت في مقطع فيديو سابق، يمكنك التفكير في هذا الإعداد هندسيًا باعتباره مصفوفة معروفة معينة تحول متجهًا غير معروف، [x; y]، حيث تعرف ما سيكون عليه الناتج، في هذه الحالة [-4؛ -2]. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.32,
  "end": 120.04
 },
 {
  "input": "Remember, the columns of this matrix tell you how the matrix acts as a transform, each one telling you where the basis vectors of the input space land. ",
  "translatedText": "تذكر أن أعمدة هذه المصفوفة تخبرك كيف تعمل هذه المصفوفة كتحويل، حيث يخبرك كل منها بمكان تواجد المتجهات الأساسية لمساحة الإدخال. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.8,
  "end": 130.08
 },
 {
  "input": "So this is a sort of puzzle, what input [x; y], is going to give you this output [-4; -2]? ",
  "translatedText": "إذن ما لدينا هو نوع من اللغز. ما متجه الدخل x، y، الذي سيصل إلى هذا الخرج، سالب 4، سالب 2؟ إحدى الطرق للتفكير في لغزنا الصغير هنا هي أننا نعرف أن متجه الإخراج المحدد هو مجموعة خطية من أعمدة المصفوفة، x مضروبًا في المتجه حيث يهبط i-hat بالإضافة إلى y مضروبًا في المتجه حيث يهبط j-hat، لكن ماذا نريد أن نعرف بالضبط ما يجب أن تكون عليه هذه التركيبة الخطية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.86,
  "end": 137.6
 },
 {
  "input": "Remember, the type of answer you get here can depend on whether or not the transformation squishes all of space into a lower dimension. ",
  "translatedText": "تذكر أن نوع الإجابة التي تحصل عليها هنا يمكن أن يعتمد على ما إذا كان التحويل يسحق كل المساحة إلى بُعد أقل أم لا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 137.6,
  "end": 148.4
 },
 {
  "input": "That is if it has zero determinant. ",
  "translatedText": "هذا إذا كان لديه محدد صفر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 148.4,
  "end": 151.22
 },
 {
  "input": "In that case, either none of the inputs land on our given output or there are a whole bunch of inputs landing on that output. ",
  "translatedText": "في هذه الحالة، إما أن لا تصل أي من المدخلات إلى المخرج المعطى أو أن هناك مجموعة كاملة من المدخلات تقع على هذا المخرج. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 151.22,
  "end": 156.22
 },
 {
  "input": "But for this video we’ll limit our view to the case of a non-zero determinant, meaning the output of this transformation still spans the full n-dimensional space it started in; every input lands on one and only one output and every output has one and only one input. ",
  "translatedText": "لكن في هذا الفيديو، سنقتصر وجهة نظرنا على حالة المحدد غير الصفري، مما يعني أن مخرجات هذا التحويل لا تزال تمتد عبر الفضاء الأبعادي الكامل الذي بدأ فيه. يقع كل مدخل على مخرج واحد فقط، وكل مخرج له مدخل واحد فقط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 157.24,
  "end": 168.94
 },
 {
  "input": "One way to think about our puzzle is that we know the given output vector is some linear combination of the columns of the matrix; x*(the vector where i-hat lands) + y*(the vector where j-hat lands), but we wish to compute what exactly x and y are. ",
  "translatedText": "إحدى طرق التفكير في هذا اللغز هي أننا نعرف أن متجه المخرجات المعطى هو عبارة عن مجموعة خطية من أعمدة المصفوفة؛ x*(المتجه حيث يهبط i-hat) + y*(المتجه حيث يهبط j-hat)، ولكننا نرغب في حساب ماهية x وy بالضبط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.94,
  "end": 186.72
 },
 {
  "input": "As a first pass, let me show an idea that is wrong, but in the right direction. ",
  "translatedText": "كنقطة أولى، اسمحوا لي أن أعرض فكرة خاطئة، ولكنها في الاتجاه الصحيح. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 186.72,
  "end": 198.16
 },
 {
  "input": "The x-coordinate of this mystery input vector is what you get by taking its dot product with the first basis vector, [1; 0]. ",
  "translatedText": "الإحداثي السيني لمتجه الإدخال الغامض هذا هو ما تحصل عليه من خلال أخذ حاصل الضرب النقطي مع المتجه الأساسي الأول، [1؛ 0]. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.8,
  "end": 205.44
 },
 {
  "input": "Likewise, the y-coordinate is what you get by dotting it with the second basis vector, [0; 1]. ",
  "translatedText": "وبالمثل، فإن الإحداثي y هو ما تحصل عليه عن طريق تنقيطه بالمتجه الأساسي الثاني، 0، 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.16,
  "end": 211.4
 },
 {
  "input": "So maybe you hope that after the transformation, the dot products with the transformed version of the mystery vector with the transformed versions of the basis vectors will also be these coordinates x and y. ",
  "translatedText": "لذا ربما تأمل أنه بعد التحويل، ستكون المنتجات النقطية ذات النسخة المحولة من المتجه الغامض مع النسخة المحولة أيضًا هذه الإحداثيات، x وy. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.9,
  "end": 223.24
 },
 {
  "input": "That’d be fantastic because we know the transformed versions of each of these vectors. ",
  "translatedText": "سيكون ذلك رائعًا، لأننا نعرف ما هي النسخة المحولة لكل من تلك المتجهات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.94,
  "end": 228.74
 },
 {
  "input": "There’s just one problem with this: it’s not at all true! ",
  "translatedText": "هناك مشكلة واحدة فقط في هذا الأمر، هذا ليس صحيحًا على الإطلاق. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.18,
  "end": 234.2
 },
 {
  "input": "For most linear transformations, the dot product before and after the transformation will be very different. ",
  "translatedText": "بالنسبة لمعظم التحويلات الخطية، سيبدو المنتج النقطي قبل التحويل وبعده مختلفًا تمامًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 234.64,
  "end": 240.24
 },
 {
  "input": "For example, you could have two vectors generally pointing in the same direction, with a positive dot product, which get pulled away from each other during the transformation, in such a way that they then have a negative dot product. ",
  "translatedText": "على سبيل المثال، يمكن أن يكون لديك متجهان يشيران عمومًا في نفس الاتجاه مع منتج نقطي موجب، ويتم فصلهما عن بعضهما البعض أثناء التحويل بطريقة تجعلهما في النهاية منتجًا نقطيًا سالبًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.8,
  "end": 251.52
 },
 {
  "input": "Likewise, if things start off perpendicular, with dot product zero, like the two basis vectors, there’s no guarantee that they will stay perpendicular after the transformation, preserving that zero dot product. ",
  "translatedText": "وبالمثل، فإن الأشياء التي تبدأ متعامدة مع المنتج النقطي 0، مثل المتجهين الأساسيين، في كثير من الأحيان لا تظل متعامدة مع بعضها البعض بعد التحويل، أي أنها لا تحافظ على المنتج النقطي 0. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 252.22,
  "end": 263.48
 },
 {
  "input": "In the example we were looking at, dot products certainly aren’t preserved. ",
  "translatedText": "في المثال الذي كنا ننظر إليه، بالتأكيد لا يتم حفظ المنتجات النقطية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 264.1,
  "end": 267.16
 },
 {
  "input": "They tend to get bigger since most vectors are getting stretched. ",
  "translatedText": "تميل إلى أن تصبح أكبر نظرًا لأن معظم النواقل تتمدد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.5,
  "end": 269.94
 },
 {
  "input": "In fact, transformations which do preserve dot products are special enough to have their own name: Orthonormal transformations. ",
  "translatedText": "في الواقع، التحويلات التي تحافظ على المنتجات النقطية هي خاصة بما يكفي لتكون لها اسم خاص بها: التحولات المتعامدة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.94,
  "end": 279.1
 },
 {
  "input": "These are the ones which leave all the basis vectors perpendicular to each other with unit lengths. ",
  "translatedText": "هذه هي المتجهات التي تترك جميع المتجهات الأساسية متعامدة مع بعضها البعض بأطوال الوحدة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 279.72,
  "end": 284.66
 },
 {
  "input": "You often think of these as rotation matrices. ",
  "translatedText": "غالبًا ما تفكر في هذه كمصفوفات دوران. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.74,
  "end": 287.88
 },
 {
  "input": "The correspond to rigid motion, with no stretching, squishing or morphing. ",
  "translatedText": "تتوافق مع الحركة الصلبة، دون أي تمدد أو سحق أو تحول. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.42,
  "end": 292.2
 },
 {
  "input": "Solving a linear system with an orthonormal matrix is very easy: Since dot products are preserved, taking the dot product between the output vector and all the columns of your matrix will be the same as taking the dot products between the input vector and all the basis vectors, which is the same as finding the coordinates of the input vector. ",
  "translatedText": "إن حل نظام خطي بمصفوفة متعامدة هو في الواقع أمر سهل للغاية. نظرًا لأنه يتم الحفاظ على المنتجات النقطية، فإن أخذ حاصل الضرب النقطي بين متجه الإخراج وجميع أعمدة المصفوفة سيكون هو نفسه أخذ حاصل الضرب النقطي بين متجه الإدخال الغامض وجميع المتجهات الأساسية، وهو نفس مجرد العثور على إحداثيات هذا الإدخال الغامض. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.0,
  "end": 312.98
 },
 {
  "input": "So, in that very special case, x would be the dot product of the first column with the output vector, and y would be the dot product of the second column with the output vector. ",
  "translatedText": "لذلك في هذه الحالة الخاصة جدًا، x سيكون المنتج النقطي للعمود الأول مع متجه الإخراج، وy سيكون المنتج النقطي للعمود الثاني مع متجه الإخراج. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 313.68,
  "end": 323.76
 },
 {
  "input": "Now, even though this idea breaks down for most linear systems, it points us in the direction of something to look for: Is there an alternate geometric understanding for the coordinates of our input vector which remains unchanged after the transformation? ",
  "translatedText": "لماذا أطرح هذا الأمر عندما تنهار هذه الفكرة بالنسبة لجميع الأنظمة الخطية تقريبًا؟ حسنًا، إنه يوجهنا نحو شيء ما لنبحث عنه. هل هناك فهم هندسي بديل لإحداثيات متجه الإدخال الذي يظل دون تغيير بعد التحويل؟ إذا كان عقلك يفكر في المحددات، فقد تفكر في الفكرة الذكية التالية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.82,
  "end": 341.66
 },
 {
  "input": "If your mind has been mulling over determinants, you might think of this clever idea: Take the parallelogram defined by the first basis vector, i-hat, and the mystery input vector [x; y]. ",
  "translatedText": "خذ متوازي الأضلاع المحدد بواسطة المتجه الأساسي الأول، i-hat، ومتجه الإدخال الغامض، xy. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 342.36,
  "end": 353.76
 },
 {
  "input": "The area of this parallelogram is its base, 1, times the height perpendicular to that base, which is the y-coordinate of our input vector. ",
  "translatedText": "مساحة متوازي الأضلاع هذا هي القاعدة، 1، مضروبة في الارتفاع المتعامد مع تلك القاعدة، وهو إحداثي y لمتجه الإدخال هذا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 354.44,
  "end": 362.96
 },
 {
  "input": "So, the area of this parallelogram is sort of a screwy roundabout way to describe the vector’s y-coordinate; it’s a wacky way to talk about coordinates, but run with me. ",
  "translatedText": "لذا فإن مساحة متوازي الأضلاع هي طريقة ملتوية لوصف إحداثي y للمتجه. إنها طريقة غريبة للحديث عن الإحداثيات، لكن اركض معي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 363.68,
  "end": 373.14
 },
 {
  "input": "Actually, to be more accurate, you should think of the signed area of this parallelogram, in the sense described by the determinant video. ",
  "translatedText": "وفي الواقع، لكي نكون أكثر دقة، يجب أن تفكر في هذه المنطقة باعتبارها المساحة المميزة لمتوازي الأضلاع، بالمعنى الموضح في مقطع الفيديو المحدد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 373.7,
  "end": 381.64
 },
 {
  "input": "That way, a vector with negative y-coordinate would correspond to a negative area for this parallelogram. ",
  "translatedText": "بهذه الطريقة، فإن المتجه ذو الإحداثي y السالب سيتوافق مع المساحة السالبة لمتوازي الأضلاع هذا، على الأقل إذا كنت تعتقد أن i-hat هو الأول من بين هذين المتجهين الذي يحدد متوازي الأضلاع. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.2,
  "end": 388.58
 },
 {
  "input": "Symmetrically, if you look at the parallelogram spanned by the vector and the second basis vector, j-hat, its area will be the x-coordinate of the vector. ",
  "translatedText": "وبشكل متماثل، إذا نظرت إلى متوازي الأضلاع الممتد بواسطة متجه الإدخال الغامض والأساس الثاني، j-hat، فإن مساحته ستكون إحداثي x لهذا المتجه الغامض. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 388.96,
  "end": 392.96
 },
 {
  "input": "Again, it’s a strange way to represent the x-coordinate, but you’ll see what it buys us in a moment. ",
  "translatedText": "مرة أخرى، إنها طريقة غريبة لتمثيل الإحداثي x، لكنك سترى ما ستشتريه لنا بعد قليل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 392.96,
  "end": 398.78
 },
 {
  "input": "Here’s what this would look like in three-dimensions: Ordinarily the way you might think of one of a vector’s coordinate, say its z-coordinate, would be to take its dot product with the third standard basis vector, k-hat. ",
  "translatedText": "وفقط للتأكد من أنه من الواضح كيف يمكن تعميم ذلك، دعونا ننظر في ثلاثة أبعاد. عادةً، الطريقة التي قد تفكر بها في أحد إحداثيات المتجه، مثل الإحداثي z، هي أخذ حاصل الضرب النقطي مع المتجه الأساسي القياسي الثالث، والذي يُسمى غالبًا k-hat. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 398.78,
  "end": 410.08
 },
 {
  "input": "But instead, consider the parallelepiped it creates with the other two basis vectors, i-hat and j-hat. ",
  "translatedText": "لكن التفسير الهندسي البديل هو أن نأخذ في الاعتبار متوازي السطوح الذي ينشئه مع المتجهين الأساسيين الآخرين، i-hat وj-hat. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 410.68,
  "end": 412.64
 },
 {
  "input": "If you think of the square with area 1 spanned by i-hat and j-hat as the base of this guy, its volume is the same its height, which is the third coordinate of our vector. ",
  "translatedText": "إذا كنت تعتقد أن المربع الذي مساحته 1 يمتد بين i-hat وj-hat هو قاعدة هذا الشكل بأكمله، فإن حجمه يساوي ارتفاعه، وهو الإحداثي الثالث للمتجه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.74,
  "end": 417.14
 },
 {
  "input": "Likewise, the wacky way to think about any other coordinate of this vector is to form the parallelepiped between this vector an all the basis vectors other than the one you’re looking for, and get its volume. ",
  "translatedText": "وبالمثل، فإن الطريقة الغريبة للتفكير في الإحداثيات الأخرى للمتجه هي تكوين خط متوازي باستخدام المتجه ثم جميع المتجهات الأساسية بخلاف تلك المقابلة للاتجاه الذي تبحث عنه. ثم حجم هذا يعطيك الإحداثيات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.14,
  "end": 429.72
 },
 {
  "input": "Or, rather, we should talk about the signed volume of these parallelepipeds, in the sense described in the determinant video, where the order in which you list the three vectors matters and you’re using the right-hand rule. ",
  "translatedText": "أو بالأحرى، يجب أن نتحدث عن الحجم الموصوف لمتوازي السطوح، بالمعنى الموضح في فيديو المحدد باستخدام قاعدة اليد اليمنى. لذا فإن الترتيب الذي تُدرج به هذه المتجهات الثلاثة مهم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.72,
  "end": 442.38
 },
 {
  "input": "That way negative coordinates still make sense. ",
  "translatedText": "بهذه الطريقة تظل الإحداثيات السلبية منطقية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.24
 },
 {
  "input": "Okay, so why think of coordinates as areas and volumes like this? ",
  "translatedText": "حسنًا، لماذا نفكر في الإحداثيات كمساحات وأحجام كهذه؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.24,
  "end": 447.5
 },
 {
  "input": "As you apply some matrix transformation, the areas of the parallelograms don’t stay the same, they may get scaled up or down. ",
  "translatedText": "عندما تقوم بتطبيق بعض تحويلات المصفوفة، فإن مناطق متوازي الأضلاع لا تبقى كما هي، بل قد يتم تكبيرها أو تقليلها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 450.0
 },
 {
  "input": "But(!), and this is a key idea of determinants, all these areas get scaled by the same amount. ",
  "translatedText": "لكن (!)، وهذه فكرة أساسية للمحددات، يتم قياس كل هذه المناطق بنفس المقدار. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.0,
  "end": 453.96
 },
 {
  "input": "Namely, the determinant of our transformation matrix. ",
  "translatedText": "وهي محدد مصفوفة التحويل لدينا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.96,
  "end": 457.9
 },
 {
  "input": "For example, if you look the parallelogram spanned by the vector where your first basis vector lands, which is the first column of the matrix, and the transformed version of [x; y], what is its area? ",
  "translatedText": "على سبيل المثال، إذا نظرت إلى متوازي الأضلاع الممتد عبر المتجه حيث يقع المتجه الأساسي الأول، وهو العمود الأول من المصفوفة، والنسخة المحولة من [x؛ ذ]، ما هي مساحتها؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.44,
  "end": 470.72
 },
 {
  "input": "Well, this is the transformed version of that parallelogram we were looking at earlier, whose area was the y-coordinate of the mystery input vector. ",
  "translatedText": "حسنًا، هذه هي النسخة المحولة من متوازي الأضلاع الذي كنا ننظر إليه سابقًا، والذي كانت مساحته هي الإحداثي y لمتجه الإدخال الغامض. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.72,
  "end": 478.28
 },
 {
  "input": "So its area will be the determinant of the transformation multiplied by that value. ",
  "translatedText": "إذن، ستكون مساحتها هي العامل المحدد للتحويل مضروبًا في تلك القيمة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 478.96,
  "end": 481.96
 },
 {
  "input": "So, the y-coordinate of our mystery input vector is the area of this parallelogram, spanned by the first column of the matrix and the output vector, divided by the determinant of the full transformation. ",
  "translatedText": "إذن، الإحداثي y لمتجه الإدخال الغامض هو مساحة متوازي الأضلاع هذا، ممتدًا عبر العمود الأول من المصفوفة ومتجه الإخراج، مقسومًا على محدد التحويل الكامل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.96,
  "end": 492.16
 },
 {
  "input": "And how do you get this area? ",
  "translatedText": "وكيف تحصل على هذه المنطقة؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.16,
  "end": 494.88
 },
 {
  "input": "Well, we know the coordinates for where the mystery input vector lands, that’s the whole point of a linear system of equations. ",
  "translatedText": "حسنًا، نحن نعرف إحداثيات المكان الذي يستقر فيه متجه الإدخال الغامض، وهذا هو الهدف الأساسي لنظام المعادلات الخطية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.88,
  "end": 499.88
 },
 {
  "input": "So, create a matrix whose first column is the same as that of our matrix, and whose second column is the output vector, and take its determinant. ",
  "translatedText": "إذن ما يمكنك فعله هو إنشاء مصفوفة جديدة عمودها الأول هو نفسه عمود المصفوفة، لكن عمودها الثاني هو متجه المخرجات، ثم تأخذ محددها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 499.88,
  "end": 512.88
 },
 {
  "input": "So look at that; just using data from the output of the transformation, namely the columns of the matrix and the coordinates of our output vector, we can recover the y-coordinate of our mystery input vector. ",
  "translatedText": "لذا انظر إلى ذلك، فقط باستخدام البيانات من مخرجات التحويل، أي أعمدة المصفوفة وإحداثيات متجه المخرجات، يمكننا استعادة إحداثي y لمتجه الإدخال الغامض، والذي هو في منتصف الطريق لحل النظام. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 512.88,
  "end": 522.1
 },
 {
  "input": "Likewise, the same idea can get you the x-coordinate. ",
  "translatedText": "وبالمثل، فإن نفس الفكرة يمكن أن تعطينا إحداثي x. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 522.1,
  "end": 523.5
 },
 {
  "input": "Look at that parallelogram we defined early which encodes the x-coordinate of the mystery input vector, spanned by the input vector and j-hat. ",
  "translatedText": "انظر إلى متوازي الأضلاع الذي عرفناه سابقًا، والذي يشفر إحداثي x لمتجه الإدخال الغامض، الممتد عبر هذا المتجه وقبعة j. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 523.5,
  "end": 537.14
 },
 {
  "input": "The transformed version of this guy is spanned by the output vector and the second column of the matrix, and its area will have been multiplied by the determinant of the matrix. ",
  "translatedText": "النسخة المحولة من هذا الرجل تمتد عبر متجه الإخراج والعمود الثاني من المصفوفة، وستكون مساحتها مضروبة في محدد تلك المصفوفة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 537.14,
  "end": 545.04
 },
 {
  "input": "So the x-coordinate of our mystery input vector is this area divided by the determinant of the transformation. ",
  "translatedText": "لذا، لحل مشكلة x، يمكنك قسمة هذه المساحة الجديدة على محدد التحويل الكامل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 545.06,
  "end": 555.14
 },
 {
  "input": "Symmetric to what we did before, you can compute the area of that output parallelogram by creating a new matrix whose first column is the output vector, and whose second column is the same as the original matrix. ",
  "translatedText": "وكما فعلنا من قبل، يمكنك حساب مساحة متوازي الأضلاع الناتج عن طريق إنشاء مصفوفة جديدة عمودها الأول هو متجه المخرجات وعمودها الثاني هو نفس المصفوفة الأصلية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.6,
  "end": 563.42
 },
 {
  "input": "So again, just using data from the output space, the numbers we see in our original linear system, we can recover the x-coordinate of our mystery input vector. ",
  "translatedText": "لذا مرة أخرى، فقط باستخدام البيانات من مساحة الإخراج، أي الأرقام التي نراها في نظامنا الخطي الأصلي، يمكننا إيجاد القيمة التي يجب أن تكون عليها x. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.42,
  "end": 573.42
 },
 {
  "input": "This formula for finding the solutions to a linear system of equations is known as Cramer’s rule. ",
  "translatedText": "تُعرف هذه الصيغة لإيجاد الحلول لنظام خطي من المعادلات بقاعدة كرامر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.42,
  "end": 584.48
 },
 {
  "input": "Here, just to sanity check ourselves, plug in the numbers here. ",
  "translatedText": "هنا، فقط للتحقق من سلامة أنفسنا، أدخل بعض الأرقام هنا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 584.48,
  "end": 585.34
 },
 {
  "input": "The determinant of that top altered matrix is 4+2, which is 6, and the bottom determinant is 2, so the x-coordinate should be 3. ",
  "translatedText": "محدد تلك المصفوفة العلوية المعدلة هو 4 زائد 2، وهو ما يساوي 6، والمحدد السفلي هو 2، لذا يجب أن يكون إحداثي x 3. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.34,
  "end": 592.94
 },
 {
  "input": "And indeed, looking back at that input vector we started with, it’s x-coordinate is 3. ",
  "translatedText": "وبالفعل، بالنظر إلى متجه الإدخال الذي بدأنا به، فإن إحداثي x هو 3. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.86,
  "end": 604.34
 },
 {
  "input": "Likewise, Cramer’s rule suggests the y-coordinate should be 4/2, or 2, and that is indeed the y-coordinate of the input vector we started with here. ",
  "translatedText": "وبالمثل، تقترح قاعدة كرامر أن إحداثي y يجب أن يكون 4 مقسومًا على 2 أو 2، وهذا هو إحداثي y لمتجه الإدخال الذي بدأنا به. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 604.34,
  "end": 607.72
 },
 {
  "input": "The case with three dimensions is similar, and I highly recommend you pause to think it through yourself. ",
  "translatedText": "الحالة ذات الأبعاد الثلاثة أو أكثر متشابهة، وأوصي بشدة أن تتوقف للحظة وتفكر فيها بنفسك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 607.72,
  "end": 618.42
 },
 {
  "input": "Here, I’ll give you a little momentum. ",
  "translatedText": "هنا، سأعطيك القليل من الزخم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 619.12,
  "end": 621.58
 },
 {
  "input": "We have this known transformation, given by a 3x3 matrix, and a known output vector, given by the right side of our linear system, and we want to know what input vector lands on this output vector. ",
  "translatedText": "ما لدينا هو تحويل معروف معطى من مصفوفة 3x3، ومتجه مخرجات معروف معطى من الجانب الأيمن من نظامنا الخطي، ونريد أن نعرف ما هي المدخلات التي تصل إلى هذا المخرج. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.58,
  "end": 633.26
 },
 {
  "input": "If you think of, say, the z-coordinate of the input vector as the volume of this parallelepiped spanned by i-hat, j-hat, and the mystery input vector, what happens to the volume of this parallelepiped after the transformation? ",
  "translatedText": "إذا فكرت، على سبيل المثال، في الإحداثي z لمتجه الإدخال باعتباره حجم متوازي السطوح هذا الممتد بين i-hat وj-hat ومتجه الإدخال الغامض، فماذا يحدث لحجم متوازي السطوح هذا بعد التحويل؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 633.26,
  "end": 644.64
 },
 {
  "input": "How can you compute that new volume? ",
  "translatedText": "كيف يمكنك حساب هذا الحجم الجديد؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.64,
  "end": 651.66
 },
 {
  "input": "Really, pause and take a moment to think through the details of generalizing this to higher dimensions; finding an expression for each coordinate of the solution to larger linear systems. ",
  "translatedText": "حقًا، توقف وتوقف لحظة للتفكير في تفاصيل تعميم ذلك على أبعاد أعلى؛ إيجاد تعبير لكل إحداثيات الحل للأنظمة الخطية الأكبر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 651.66,
  "end": 664.68
 },
 {
  "input": "Thinking through more general cases and convincing yourself that it works is where all the learning will happen, much more so than listening to some dude on YouTube walk through the reasoning again. ",
  "translatedText": "إن التفكير في حالات أكثر عمومية وإقناع نفسك بنجاحها هو المكان الذي سيحدث فيه كل التعلم، أكثر بكثير من الاستماع إلى بعض الأشخاص على YouTube وهم يشرحون المنطق مرة أخرى. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.1,
  "end": 708.5
 }
]