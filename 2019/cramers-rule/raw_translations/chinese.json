[
 {
  "input": "In a previous video, I’ve talked about linear 00:00:15,800 --> 00:00:19,690 3  is a good litmus test for whether or not you actually understand what’s going on, since 00:00:31,680 --> 00:00:32,680 00:00:32,680 --> 00:00:36,379 8 9  sure to watch the relevant videos on those topics if you’re unfamiliar or rusty.",
  "model": "nmt",
  "translatedText": "在之前的视频中，我讨论了线性方程组，并且我有 点忽略了对这些系统的实际计算解决方案的讨论。 虽然数字处理通常是我们留给计算机的事情，但深入研究其 中一些计算方法是检验您是否真正理解正在发生的事情的一 个很好的试金石，因为这确实是橡胶与道路相遇的地方。 在这里，我想描述计算这些系统的解决方案的某种方法背后的几何 结构，称为克莱默规则。 这里的相关背景是理解行列式、 一点点积，当然还有线性方程组，所以如果您不熟 悉或生疏，请务必观看有关这些主题的相关视频。 但首先我应该预先指出，克莱默规则实际 上并不是计算线性方程组解的最佳方法。 例如，高斯消除法总是更快。 那为什么要学呢？ 我们可以将其视为一种文化游览。 这是加深您对这些系统背后 的理论知识的有用练习。"
 },
 {
  "input": "But first!",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "I should say up front that Cramer’s rule 00:00:56,379 --> 00:00:58,010 00:00:58,010 --> 00:01:01,950 16 17   help consolidate ideas from linear algebra, like the determinant and linear systems, by 00:01:19,960 --> 00:01:24,619 22 23   will work systems with a larger number of unknowns, and the same number of equations.",
  "model": "nmt",
  "translatedText": "了解这个概念将有助于通过 了解线性代数（例如行列式和线性系统）之间的相互 关系来巩固它们的想法。 此外，从纯粹的艺术角度来看， 这里的最终结果确实非常漂亮，比高斯消去法要好得多。 好吧，这里的设置将是一些线性方程组，比如有两个未知数 x 和 y，以及两个方程。 原则上，我们所讨论的一 切也适用于具有大量未知数和相同数量方程的系 统，但为了简单起见，较小的示例更容易记住。 正如我在之前的视频中谈到的，您可以从几何角度考虑此设置，将其 视为转换未知向量 x, y 的某个已知矩阵，您知道输出将是什 么，在本例中为负 4，负 2. 请记住，该矩阵的列告诉您该矩 阵如何充当变换，每一列都告诉您输入空间的基向量落在哪 里。 所以我们面临的是一个难题。 哪个输入向量 x、y 将落在该输出上 （负 4、负 2）？思考这个小难题的一种方法是，我们知道给定的输 出向量是矩阵列的某种线性组合，x 乘以 i-hat 所在的向量 加上 y 乘以 j-hat 所在的向量，但是什么我们想要的是弄清 楚这个线性组合到底应该是什么。"
 },
 {
  "input": "But for simplicity, a smaller example is nicer 00:01:46,349 --> 00:01:50,599 29  -2].",
  "model": "nmt",
  "translatedText": "请记住，您在这里得到的答案 类型可能取决于变换是否将所有空间压缩到较低的维度，即 它是否具有零行列式。 在这种情况下，要么没有任何输入落在我们给 定的输出上，要么有一大堆输入落在该输出上。 但对于这个视频，我们 将把我们的观点限制在非零行列式的情况，这意味着这个转换的输出 仍然跨越它开始的整个维度空间。 每个输入都落在一个且仅有一个输出上， 并且每个输出都有一个且仅有一个输入。"
 },
 {
  "input": "Remember, the columns of this matrix tell 00:02:06,250 --> 00:02:10,899 33   type of answer you get here can depend on whether or not the transformation squishes 00:02:44,299 --> 00:02:46,080 00:02:46,080 --> 00:02:51,849 39 40  the full n-dimensional space it started in; every input lands on one and only one output 00:03:12,549 --> 00:03:14,840 44  compute what exactly x and y are.",
  "model": "nmt",
  "translatedText": "首先，让我向您展示一个错误但 方向正确的想法。 这个神秘输入向量的 x 坐标是通过将其与第一 个基向量 1, 0 进行点积得到的。 同样，y 坐标是用第二个基向 量 0, 1 点起来得到的。 所以也许你希望在变换之后 ，神秘向量的变换版本与变换版本的点积也将是这 些坐标，x和y。 那太棒了，因为我们知道每个向量 的转换版本是什么。 它只有一个问题，它根本不是真的。 对于大多数线性变换，变换前后的点积看起来会非常 不同。 例如，您可能有两个通常指向同一方向且具 有正点积的向量，这两个向量在转换过程中彼此分 开，最终导致它们具有负点积。 同样，从点积 0 开始垂直的事物（例如两个基向量）在变换后通常不会 保持彼此垂直，也就是说，它们不会保留 0 点积。 看看我刚刚展示的例子，点积肯定不会被保留，它 们往往会变得更大，因为大多数向量都被拉伸了。 事实上，这里值得注意的是，保留点积的变换足够特殊， 有自己的名字：正交变换。 这些是使所有基本向量彼此 垂直并且仍然具有单位长度的向量。"
 },
 {
  "input": "As a first pass, let me show an idea that 00:03:18,829 --> 00:03:23,340 48 49   the dot products with the transformed version of the mystery vector with the transformed 00:03:41,939 --> 00:03:43,590 00:03:43,590 --> 00:03:50,400 55  before and after the transformation will be very different.",
  "model": "nmt",
  "translatedText": "您通常将它们视为 旋转矩阵，它们对应于没有拉伸、挤压或变形的刚性运动。 求解具有正交矩阵的线性系统实际上非常简单。 由于点积被保 留，因此在输出向量和矩阵的所有列之间求点 积将与在神秘输入向量和所有基向量之间求点 积相同，这与仅查找该神秘输入的坐标。 因此， 在这种非常特殊的情况下，x 将是第一列与输出 向量的点积，y 将是第二列与输出向量的点积。 当这个想法对于几乎所有线性系统来说都行不通时，为什么我要提出这个问题呢？ 嗯，它为我们指明了寻找方向。 对于变换后保持 不变的输入向量的坐标是否有替代的几何理 解？如果您一直在思考决定因素，您可能会想到以下聪明的 想法。 采用由第一个基向量 i-hat 和神秘输入向量 xy 定义的平行四 边形。 该平行四边形的面积是底边 1 乘以垂直于该底边的高 度，即该输入向量的 y 坐标。"
 },
 {
  "input": "For example, you could have two vectors generally 00:04:04,959 --> 00:04:09,270 60 61  will stay perpendicular after the transformation, preserving that zero dot product.",
  "model": "nmt",
  "translatedText": "因此，平行四边形的面积是 描述向量 y 坐标的一种扭曲的迂回方式。 这是一种奇怪的谈论坐 标的方式，但跟我一起跑吧。 实际上，为了更准确一点，您应该将其视为 该平行四边形的带符号区域，就像行列式视频中描述的那样。 这样，具有负 y 坐标的向量将对应于该平行四边形的负面积， 至少如果您认为 i-hat 在某种意义上是定义平行四边形的 这两个向量中的第一个。 对称地，如果你看一下由我们的神秘输入向 量和第二个基 j-hat 组成的平行四边形，它的面积将是该神 秘向量的 x 坐标。"
 },
 {
  "input": "In the example we were looking at, dot products 00:04:27,140 --> 00:04:30,950 66   vectors perpendicular to each other with unit lengths.",
  "model": "nmt",
  "translatedText": "同样，这是一种奇怪的表示 x 坐标的方式，但您很快 就会明白它给我们带来了什么。 为了确保清楚这如何概括，让我们 从三个维度来看。 通常，您可能会考虑一个向量的坐标之一（例 如它的 z 坐标），将其与第三个标准基向量（通常称为 k- hat）进行点积。 但另一种几何解释是考虑它与其他两个基本向量 i-hat 和 j-hat 创建的平行六面体。 如果您将 i- hat 和 j-hat 所跨越的面积为 1 的正方形视为整个形状的基础， 那么它的体积与其高度相同，这是我们向量的第三个坐标。"
 },
 {
  "input": "You often think of these as rotation matrices.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "The correspond to rigid motion, with no stretching, 00:04:53,000 --> 00:04:58,920 73  products between the input vector and all the basis vectors, which is the same as finding 00:05:13,599 --> 00:05:18,690 77  most linear systems, it points us in the direction of something to look for: Is there an alternate 00:05:37,780 --> 00:05:42,780 81  vector, i-hat, and the mystery input vector [x; y].",
  "model": "nmt",
  "translatedText": "同样，考虑向量的 其他坐标的一种古怪方法是使用该向量形成一个平行六面体， 然后使用除与您正在寻找的方向相对应的方向之外的所有基本 向量。 然后这个体积就可以给你坐标。 或者更确切地说，我们应该 讨论平行六面体的有符号体积，在行列式视频中使用右手定则描述的 意义上。 因此，列出这三个向量的顺序很重要。 这样，负坐 标仍然有意义。 好吧，那么为什么要把坐标看作这样的面积和体积呢？ 好吧，当您应用某种矩阵变换时，这些平行四边形的面积，它们不会 保持不变，它们可能会按比例放大或缩小。 但是，这是行列式的 关键思想，所有区域都按相同的量缩放，即我们的变换矩阵 的行列式。 例如，如果您查看第一个基向量所在的向量（即矩阵 的第一列）和 xy 的变换版本所跨越的平行四边形，那么它 的面积是多少？嗯，这是我们之前看到的平行四边形的 变换版本，其面积是神秘输入向量的 y 坐标。 所以它的面积就是变换的决定因素乘以 y 坐标。 因此，这意味着我们可以通过输出空间中这个新平行四边形的面 积除以完整变换的行列式来求解 y。"
 },
 {
  "input": "The area of this parallelogram is its base, 00:05:59,990 --> 00:06:03,460 86  to talk about coordinates, but run with me.",
  "model": "nmt",
  "translatedText": "那么如何获得该区域呢？ 好吧，我们知道神秘输入向量落地的坐标，这就是线性方 程组的全部要点。 因此，您可能要做的就是创建一个新矩阵 ，其第一列与我们的矩阵相同，但第二列是输出向量，然后获 取其行列式。 所以看一下，只需使用变换输出的数据， 即矩阵的列和输出向量的坐标，我们就可以恢复神 秘输入向量的 y 坐标，这就是解决系统的一半。"
 },
 {
  "input": "Actually, to be more accurate, you should 00:06:19,690 --> 00:06:22,440 90   look at the parallelogram spanned by the vector and the second basis vector, j-hat, its area 00:06:45,099 --> 00:06:49,370 95 96  would be to take its dot product with the third standard basis vector, k-hat.",
  "model": "nmt",
  "translatedText": "同样，同样的想法可以给我们 x 坐标。 看看我们之前定义的平行四边 形，它编码了神秘输入向量的 x 坐标，由该向量和 j-hat 跨越。 这个家伙的变换版本由输出向量和矩阵的第 二列组成，它的面积将乘以该矩阵的行列式。 因此，要求解 x，您可以将这个新面积除以完整变换的行 列式。 与我们之前所做的类似，您可以通过创建一个新矩阵 来计算输出平行四边形的面积，该新矩阵的第一列是输出向量 ，第二列与原始矩阵相同。 同样，只需使用输出空间中的数据，即我 们在原始线性系统中看到的数字，我们就可以求解 x 必须是什么。 这个求线性方程组解的公式称为克莱默法则。 在这里，只是为了检查一下我们自己的理智，在这里插入一些数字。 顶部更改矩阵的行列式 是 4 加 2，即 6，底部行列式是 2，因此 x 坐标应为 3。"
 },
 {
  "input": "But instead, consider the parallelepiped it 00:07:11,870 --> 00:07:13,569 00:07:13,569 --> 00:07:20,030 102  other coordinate of this vector is to form the parallelepiped between this vector an 00:07:34,950 --> 00:07:37,900 00:07:37,900 --> 00:07:42,490 107  rule.",
  "model": "nmt",
  "translatedText": "事实上，回顾我们开始时的输入向量，x 坐标是 3。 同样，克莱默规则表明 y 坐标应为 4 除以 2 ，即 2，这就是我们开始时输入向量的 y 坐标。 3 维或更多维度的情况类似，我强烈建议您花点时间 自己思考一下。 在这里，我给大家一点动力。 我们拥有的是 由某个 3x3 矩阵给出的已知变换，以及由线性系统右侧给出的已 知输出向量，我们想知道什么输入落在该输出上。 如果您将该输入向 量的 z 坐标视为我们之前看到的特殊平行六面体的体积，由 i-hat、j-hat 和神秘输入向量跨越，那么该体积 会发生什么转型后？计算该体积的方法有哪些？ 实际上，停下来，花点时间思考一下将其推广到更高维 度的细节，找到更大线性系统解的每个坐标的表达式。"
 },
 {
  "input": "That way negative coordinates still make sense.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "Okay, so why think of coordinates as areas 00:07:56,000 --> 00:08:02,039 112 113  matrix.",
  "model": "nmt",
  "translatedText": "思考像这样的更一般的案例，并说服自己它是有效的以及为 什么它有效，这是所有学习真正发生的地方，比听 YouT ube 上的某个人再次引导你进行相同的推理要重要得多。"
 },
 {
  "input": "For example, if you look the parallelogram 00:08:17,850 --> 00:08:22,850 117 118  input vector.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "So its area will be the determinant of the 00:08:39,080 --> 00:08:44,590 122    mystery input vector lands, that’s the whole point of a linear system of equations.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "So, create a matrix whose first column is 00:09:05,670 --> 00:09:11,250 129  vector, we can recover the y-coordinate of our mystery input vector.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "Likewise, the same idea can get you the x-coordinate.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "Look at that parallelogram we defined early 00:09:32,580 --> 00:09:36,420 135  multiplied by the determinant of the matrix.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "So the x-coordinate of our mystery input vector 00:09:52,000 --> 00:09:53,980 00:09:53,980 --> 00:09:58,900 140   space, the numbers we see in our original linear system, we can recover the x-coordinate 00:10:13,600 --> 00:10:18,440 145 146  is 4+2, which is 6, and the bottom determinant is 2, so the x-coordinate should be 3.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "And indeed, looking back at that input vector 00:10:35,910 --> 00:10:43,850 151  and I highly recommend you pause to think it through yourself.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "Here, I’ll give you a little momentum.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "We have this known transformation, given by 00:11:02,780 --> 00:11:07,580 157 158  vector, what happens to the volume of this parallelepiped after the transformation?",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "How can you compute that new volume?",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "Really, pause and take a moment to think through 00:11:32,200 --> 00:11:37,330 164 165  some dude on YouTube walk through the reasoning again.",
  "model": "nmt",
  "translatedText": ""
 }
]