[
 {
  "input": "In a previous video, I’ve talked about linear 00:00:15,800 --> 00:00:19,690 3  is a good litmus test for whether or not you actually understand what’s going on, since 00:00:31,680 --> 00:00:32,680 00:00:32,680 --> 00:00:36,379 8 9  sure to watch the relevant videos on those topics if you’re unfamiliar or rusty.",
  "model": "nmt",
  "translatedText": "以前のビデオで私は線形方程式系について話しましたが、これらの系の 解を実際に計算することについての議論は脇に置いてしまいました。 そして、数値計算は一般的にコンピューターに任せているのは事実ですが、これらの計算手法のい くつかを掘り下げることは、何が起こっているのかを実際に理解しているかどうかを知るための良 いリトマス試験紙になります。 なぜなら、それがまさに現実の世界と出会うところだからです。 ここでは、これらのシステムの解を計算するための特定の方法 (クラマー則として知られる) の背後にある 幾何学について説明したいと思います。 ここでの関連する背景は、行列式、少しの内積、そし てもちろん線形連立方程式を理解することです。 そのため、慣れていない場合や慣れ ていない場合は、これらのトピックに関する関連ビデオを必ず視聴してください。 しかし、最初に最初に言っておきたいのは、このクラマーの法則は、実際に は線形方程式系の解を計算するための最良の方法ではないということです。 たとえば、ガウス消去法は常に高速になります。 では、なぜそれを学ぶのでしょうか？ まあ、それは一種の文化旅行だと考えてください。 これは、これらのシステムの背後にある理論に ついての知識を深めるのに役立つ演習です。"
 },
 {
  "input": "But first!",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "I should say up front that Cramer’s rule 00:00:56,379 --> 00:00:58,010 00:00:58,010 --> 00:01:01,950 16 17   help consolidate ideas from linear algebra, like the determinant and linear systems, by 00:01:19,960 --> 00:01:24,619 22 23   will work systems with a larger number of unknowns, and the same number of equations.",
  "model": "nmt",
  "translatedText": "この概念を頭に入れておくと、行列式や線形系な どの線形代数のアイデアが相互にどのように関係しているかを確認することができ、それ らのアイデアを統合するのに役立ちます。 また、純粋に芸術的な観点から見ると、ここで の最終的な結果は、ガウス消去法よりもはるかに美しく、考えるのが非常に美しいです。 さて、ここでの設定は、たとえば 2 つの未知数 x と y、および 2 つ の方程式を含む線形方程式系になります。 原理的には、私たちが話していることは すべて、より多くの未知数と同じ数の方程式を持つシステムにも当てはまりますが、 話を簡単にするために、より小さな例を頭に入れておくとわかりやすいでしょう。 前回のビデオでお話ししたように、この設定は、未知のベクトル x、y を変換する特定の既 知の行列として幾何学的に考えることができます。 出力がどのようになるかはわかっています (この場合はマイナス 4)。 マイナス2。 この行列の列は、その行列が変換としてどのように機 能するかを示しており、各列は入力空間の基底ベクトルがどこに到達するかを示していることに注意してく ださい。 つまり、私たちが持っているのは一種のパズルです。 どの入力ベクトル x、y がこの出力 (負の 4、負の 2) に到達するでしょうか? ここでの小さなパズルを考える 1 つの方法は、指定された出力ベクト ルが行列の列の線形結合 (i-hat が着地するベクトルの x 倍と、j-hat が着地する ベクトルの y 倍) であることがわかっているということです。 私たちが望んでいるのは、その線形 結合が正確に何であるべきかを理解することです。"
 },
 {
  "input": "But for simplicity, a smaller example is nicer 00:01:46,349 --> 00:01:50,599 29  -2].",
  "model": "nmt",
  "translatedText": "ここで得られる答えの種類は、変換によって 空間全体が低次元に押しつぶされるかどうか、つまり行列式がゼロかどうかによって決ま ることを覚えておいてください。 その場合、指定された出力に入力がまったく到達しない か、その出力に大量の入力が到達するかのどちらかです。 ただし、このビデオでは、ゼロ 以外の行列式の場合に視点を限定します。 つまり、この変換の出力は、開始時の完全な次 元空間にまだ広がっていることを意味します。 すべての入力は 1 つだけの出力に接続され、す べての出力には 1 つだけの入力があります。"
 },
 {
  "input": "Remember, the columns of this matrix tell 00:02:06,250 --> 00:02:10,899 33   type of answer you get here can depend on whether or not the transformation squishes 00:02:44,299 --> 00:02:46,080 00:02:46,080 --> 00:02:51,849 39 40  the full n-dimensional space it started in; every input lands on one and only one output 00:03:12,549 --> 00:03:14,840 44  compute what exactly x and y are.",
  "model": "nmt",
  "translatedText": "最初のパスとして、間違っているが正しい方向にあ るアイデアを示しましょう。 この謎の入力ベクトルの x 座標は、最初の基底ベクトル 1 , 0 との内積を取ることで得られるものです。 同様に、y 座標は、2 番目の基底ベクトル 0, 1 で点を打つことによって得られます。 したがって、変換後、ミステリー ベクト ルと変換後のバージョンの内積もこれらの座標 x と y になることを 期待しているかもしれません。 これらの各ベクトルの変換バージョンが何であるかがわかって いるので、それは素晴らしいことです。 ただ一つ問題があるのですが、それは全く真実ではありません。 ほとんどの線形変換では、変換前と変換後の内積は大きく異な ります。 たとえば、正の内積を持つ 2 つのベクトルが通常は同じ方 向を向いている場合、変換中にこれらのベクトルが互いに引き離されて 、最終的に負の内積になる場合があります。 同様に、2 つの基底ベク トルのように、内積 0 で垂直に始まるものは、変換後も互いに垂直のまま ではないことがよくあります。 つまり、その内積 0 が保持されません。 そして、先ほど示した例を見ると、内積は確かに保存されず、ほとんど のベクトルが引き伸ばされるため、内積は大きくなる傾向があります。 実際、ここで補足しておきますが、ドット積を保持する変換は、正規直交変換 という独自の名前を持つほど特殊です。 これらは、すべての基底ベクトルを互い に垂直にし、単位長を維持したままにするものです。"
 },
 {
  "input": "As a first pass, let me show an idea that 00:03:18,829 --> 00:03:23,340 48 49   the dot products with the transformed version of the mystery vector with the transformed 00:03:41,939 --> 00:03:43,590 00:03:43,590 --> 00:03:50,400 55  before and after the transformation will be very different.",
  "model": "nmt",
  "translatedText": "これらは回転マトリックスと 考えることが多く、伸びたり潰したりモーフィングを行わない剛体の動きに対応します。 正規直交行列を使用して線形システムを解くのは、実際には非常に簡単です。 ドット積は保持さ れるため、出力ベクトルと行列のすべての列の間のドット積を求め ることは、謎の入力ベクトルとすべての基底ベクトルの間のドット 積を求めることと同じになり、単にその謎の入力の座標。 したがっ て、この非常に特殊なケースでは、x は最初の列と出力ベクトルのドット 積になり、y は 2 番目の列と出力ベクトルのドット積になります。 この考えはほとんどすべての線形システムで当てはまらないのに、なぜこれを取り上げるのでしょうか? そう、それは私たちに、探すべきものの方向性を示してくれます。 変換後も変化しない、入 力ベクトルの座標に対する代替の幾何学的な理解はあるのでし ょうか? 決定要因について考えているなら、次のような賢いアイデアを思いつくかもしれ ません。 最初の基底ベクトル i-hat と謎の入力ベクトル xy によって定義される平行四辺形を 考えます。 この平行四辺形の面積は、底辺とその底辺に垂直な高さの 1 倍であり、 これが入力ベクトルの y 座標になります。"
 },
 {
  "input": "For example, you could have two vectors generally 00:04:04,959 --> 00:04:09,270 60 61  will stay perpendicular after the transformation, preserving that zero dot product.",
  "model": "nmt",
  "translatedText": "したがって、その平行四辺形の面積は、 ベクトルの y 座標を記述する一種の厄介な回りくどい方法です。 座標について話すのは奇抜な方 法ですが、私と一緒に走りましょう。 そして実際には、もう少し正確に言うと、行列式のビデオで 説明されている意味で、これをその平行四辺形の符号付き領域と考える必要があります。 このようにして、負の y 座標を持つベクトルは、少なくとも i-hat が、ある意味で 、平行四辺形を定義する 2 つのベクトルのうちの最初のものであると考える場合、この平 行四辺形の負の領域に対応します。 そして対称的に、謎の入力ベクトルと 2 番目 の基底 j ハットがまたがる平行四辺形を見ると、その面積がその謎のベクト ルの x 座標になります。"
 },
 {
  "input": "In the example we were looking at, dot products 00:04:27,140 --> 00:04:30,950 66   vectors perpendicular to each other with unit lengths.",
  "model": "nmt",
  "translatedText": "繰り返しますが、これは x 座標を表す奇妙な方法ですが、それ が何をもたらすかはすぐにわかります。 これがどのように一般化されるかを明確にするために、 3 次元で見てみましょう。 通常、ベクトルの座標の 1 つ、たとえば Z 座標に ついて考える方法は、k ハットと呼ばれることが多い 3 番目の標準基底ベクトル との内積を取ることです。 しかし、別の幾何学的な解釈は、他の 2 つの基底ベクトル、i-h at と j-hat を使用して作成される平行六面体を考慮することになります。 i-hat と j-hat がまたがるエリア 1 の正方形をこの形状全体のベースと考えると、その体積はそ の高さと同じになり、これがベクトルの 3 番目の座標になります。"
 },
 {
  "input": "You often think of these as rotation matrices.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "The correspond to rigid motion, with no stretching, 00:04:53,000 --> 00:04:58,920 73  products between the input vector and all the basis vectors, which is the same as finding 00:05:13,599 --> 00:05:18,690 77  most linear systems, it points us in the direction of something to look for: Is there an alternate 00:05:37,780 --> 00:05:42,780 81  vector, i-hat, and the mystery input vector [x; y].",
  "model": "nmt",
  "translatedText": "同様に、ベクトルの他の 座標について考える奇妙な方法は、そのベクトルを使用して平行六面体を形成し、次に、探 している方向に対応するもの以外のすべての基底ベクトルを使用して平行六面体を形成するこ とです。 そして、このボリュームによってコーディネートが決まります。 むしろ、右手の法則を使用した 行列式ビデオで説明されている意味で、平行六面体の符号付き体積について話す べきです。 したがって、これら 3 つのベクトルをリストする順序が重要になります。 そうすれば、負の 座標も意味を持ちます。 では、なぜこのように座標を面積や体積として考えるのでしょうか? ある種の行列変換を適用すると、これらの平行四辺形の面積は同じままでは なく、拡大または縮小される可能性があります。 しかし、これが行列式の重 要な考え方ですが、すべての領域は同じ量、つまり変換行列の行列式でスケ ールされます。 たとえば、最初の基底ベクトルが到達するベクトル (行列の最初 の列) と xy の変換バージョンがまたがる平行四辺形を見ると、その面積は いくらでしょうか。 これは、先ほど見ていた平行四辺形の変形版であ り、その面積が謎の入力ベクトルの y 座標であるものです。 したがって、その面積は、その y 座標を乗じた変換の決定要因になります。 つまり、出力空間内のこの新しい平行四辺形の面積を完全な変換の行列式で割ることによ って y を解くことができるということになります。"
 },
 {
  "input": "The area of this parallelogram is its base, 00:05:59,990 --> 00:06:03,460 86  to talk about coordinates, but run with me.",
  "model": "nmt",
  "translatedText": "そして、どうやってその領域を取得しますか？ そうですね、謎の入力ベクトルが到達する座標はわかっています。 これが 線形方程式系の要点です。 したがって、最初の列が行列と同じであるが、 2 番目の列が出力ベクトルである新しい行列を作成し、その行列式を 取得します。 見てください、変換の出力からのデータ、つまり行 列の列と出力ベクトルの座標を使用するだけで、謎の入力ベクトル の y 座標を回復できます。"
 },
 {
  "input": "Actually, to be more accurate, you should 00:06:19,690 --> 00:06:22,440 90   look at the parallelogram spanned by the vector and the second basis vector, j-hat, its area 00:06:45,099 --> 00:06:49,370 95 96  would be to take its dot product with the third standard basis vector, k-hat.",
  "model": "nmt",
  "translatedText": "これはシステムの解決の途中です。 同様に、同じ考え方で x 座標を得ることができます。 先ほど定義した平行四辺形を見てください。 これは、謎の入力ベクトルの x 座標をエンコードしており、そのベクトルと j ハットが広がっていま す。 この男の変換されたバージョンは、出力ベクトルと行列の 2 列目にまたがり、その面積はその行列の行列式で乗算されます。 したがって、x を解くには、この新しい領域を完全な変換の行列式で割ったものを取 得できます。 また、前に行ったことと同様に、最初の列が出力ベクトルで 2 番目の列が元の行列と同じである新しい行列を作成することによって、出力平 行四辺形の面積を計算できます。 したがって、繰り返しになりますが、出力空間からのデータ、 つまり元の線形システムで表示される数値を使用するだけで、x が何であるべきかを解くことができます。 線形方程式系の解を求めるこの公式は、クラマーの法則として知られています。 ここで、健全性を確認するために、ここにいくつかの数字を入力します。 一番上の変更された行列の行列式 は 4 プラス 2、つまり 6 で、一番下の行列式は 2 なので、x 座標は 3 になる はずです。"
 },
 {
  "input": "But instead, consider the parallelepiped it 00:07:11,870 --> 00:07:13,569 00:07:13,569 --> 00:07:20,030 102  other coordinate of this vector is to form the parallelepiped between this vector an 00:07:34,950 --> 00:07:37,900 00:07:37,900 --> 00:07:42,490 107  rule.",
  "model": "nmt",
  "translatedText": "実際、最初の入力ベクトルを振り返ると、x 座標は 3 です。 同様に、Cramer の法則は、y 座標は 4 を 2 で割った値、つまり 2 である必要があることを示唆しており、それが開始時の入力ベクトルの y 座標です。 3 次元以上の場合も同様です。 一度立ち止まって、自分自身でじっくり考えてみるこ とを強くお勧めします。 ここで、少し勢いを付けてみましょう。 私たちが持っているの は、3x3 行列によって与えられる既知の変換と、線形システムの右側によって与えられる既知の出力ベクト ルです。 そして、どの入力がその出力に到達するかを知りたいと考えています。 そして、たとえば、その 入力ベクトルの Z 座標を、i-hat、j-hat、および謎の入力ベクトルに またがる、先ほど見ていた特別な平行六面体の体積として考えると、その体積はどう なるでしょうか。 変身後は？そして、その体積を計算するさまざまな方法にはどのようなものがあるのでしょうか? 本当に、立ち止まって、これを高次元に一般化し、より大きな線形シ ステムの解の各座標の式を見つける詳細をじっくり考えてください。"
 },
 {
  "input": "That way negative coordinates still make sense.",
  "model": "nmt",
  "translatedText": "このようなより一般的なケースを考えて、それが機能すること、そしてなぜ機能 するのかを自分に納得させることによって、すべての学習が実際に行われます。"
 },
 {
  "input": "Okay, so why think of coordinates as areas 00:07:56,000 --> 00:08:02,039 112 113  matrix.",
  "model": "nmt",
  "translatedText": "Y ouTube で同じ推論をもう一度説明するのを聞くよりもはるかに重要です。"
 },
 {
  "input": "For example, if you look the parallelogram 00:08:17,850 --> 00:08:22,850 117 118  input vector.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "So its area will be the determinant of the 00:08:39,080 --> 00:08:44,590 122    mystery input vector lands, that’s the whole point of a linear system of equations.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "So, create a matrix whose first column is 00:09:05,670 --> 00:09:11,250 129  vector, we can recover the y-coordinate of our mystery input vector.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "Likewise, the same idea can get you the x-coordinate.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "Look at that parallelogram we defined early 00:09:32,580 --> 00:09:36,420 135  multiplied by the determinant of the matrix.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "So the x-coordinate of our mystery input vector 00:09:52,000 --> 00:09:53,980 00:09:53,980 --> 00:09:58,900 140   space, the numbers we see in our original linear system, we can recover the x-coordinate 00:10:13,600 --> 00:10:18,440 145 146  is 4+2, which is 6, and the bottom determinant is 2, so the x-coordinate should be 3.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "And indeed, looking back at that input vector 00:10:35,910 --> 00:10:43,850 151  and I highly recommend you pause to think it through yourself.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "Here, I’ll give you a little momentum.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "We have this known transformation, given by 00:11:02,780 --> 00:11:07,580 157 158  vector, what happens to the volume of this parallelepiped after the transformation?",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "How can you compute that new volume?",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "Really, pause and take a moment to think through 00:11:32,200 --> 00:11:37,330 164 165  some dude on YouTube walk through the reasoning again.",
  "model": "nmt",
  "translatedText": ""
 }
]