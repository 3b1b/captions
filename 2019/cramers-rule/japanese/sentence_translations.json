[
 {
  "input": "In a previous video I've talked about linear systems of equations, and I sort of brushed aside the discussion of actually computing solutions to these systems.",
  "translatedText": "以前のビデオで私は線形方程式系について話しましたが、これらの系の 解を実際に計算することについての議論は脇に置いてしまいました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 11.2,
  "end": 19.14
 },
 {
  "input": "And while it's true that number crunching is typically something we leave to the computers, digging into some of these computational methods is a good litmus test for whether or not you actually understand what's going on, since that's really where the rubber meets the road.",
  "translatedText": "そして、数値計算は一般的にコンピューターに任せているのは事実ですが、これらの計算手法のい くつかを掘り下げることは、何が起こっているのかを実際に理解しているかどうかを知るための良 いリトマス試験紙になります。なぜなら、それがまさに現実の世界と出会うところだからです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 19.7,
  "end": 31.52
 },
 {
  "input": "Here I want to describe the geometry behind a certain method for computing solutions to these systems, known as Cramer's rule.",
  "translatedText": "ここでは、これらのシステムの解を計算するための特定の方法 (クラマー則として知られる) の背後にある 幾何学について説明したいと思います。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 32.12,
  "end": 38.88
 },
 {
  "input": "The relevant background here is understanding determinants, a little bit of dot products, and of course linear systems of equations, so be sure to watch the relevant videos on those topics if you're unfamiliar or rusty.",
  "translatedText": "ここで必要となる関連する背景は、行列式、内積、および線形方程式系の理解です。そのため、慣れていない場合や慣れていない場合は、これらのトピックに関する関連ビデオを必ず視聴してください。 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.68,
  "end": 50.42
 },
 {
  "input": "But first I should say up front that this Cramer's rule is not actually the best way for computing solutions to linear systems of equations, Gaussian elimination for example will always be faster.",
  "translatedText": "しかし最初に言っておくが、このクラマーの法則は、実は連立方程式の解を計算するのに最適な方法ではない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.02,
  "end": 61.26
 },
 {
  "input": "So why learn it?",
  "translatedText": "では、なぜそれを学ぶのでしょうか？ ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 61.98,
  "end": 63.52
 },
 {
  "input": "Well think of it as a sort of cultural excursion.",
  "translatedText": "まあ、一種の文化的な小旅行だと思えばいい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.78,
  "end": 65.84
 },
 {
  "input": "It's a helpful exercise in deepening your knowledge of the theory behind these systems.",
  "translatedText": "これらのシステムの背景にある理論についての知識を深めるのに役立つ練習だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.42,
  "end": 70.46
 },
 {
  "input": "Wrapping your mind around this concept is going to help consolidate ideas from linear algebra, like the determinant and linear systems, by seeing how they relate to each other.",
  "translatedText": "この概念を頭に入れておくと、行列式や線形系な どの線形代数のアイデアが相互にどのように関係しているかを確認することができ、それ らのアイデアを統合するのに役立ちます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 71.04,
  "end": 79.62
 },
 {
  "input": "Also from a purely artistic standpoint the ultimate result here is just really pretty to think about, way more so than Gaussian elimination.",
  "translatedText": "また、純粋に芸術的な観点から見ると、ここで の最終的な結果は、ガウス消去法よりもはるかに美しく、考えるのが非常に美しいです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.1,
  "end": 86.88
 },
 {
  "input": "Alright so the setup here will be some linear system of equations, say with two unknowns x and y and two equations.",
  "translatedText": "さて、ここでの設定は、たとえば 2 つの未知数 x と y、および 2 つ の方程式を含む線形方程式系になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.68,
  "end": 95.62
 },
 {
  "input": "In principle everything we're talking about will also work for systems with larger number of unknowns and the same number of equations, but for simplicity a smaller example is just nicer to hold in our heads.",
  "translatedText": "原理的には、今話していることはすべて、より多くの未知数と同じ数の方程式を持つ系にも当てはまるが、簡単にするために、より小さな例を頭の中に入れておく方がすっきりする。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 96.3,
  "end": 105.58
 },
 {
  "input": "So as I talked about in a previous video you can think of this setup geometrically as a certain known matrix transforming an unknown vector x y where you know what the output is going to be, in this case negative 4 negative 2.",
  "translatedText": "したがって、前のビデオで説明したように、この設定は、未知のベクトル [x; を変換する特定の既知の行列として幾何学的に考えることができます。 y]、出力がどのようになるかがわかっています。この場合は [-4; -2]。 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 106.32,
  "end": 120.04
 },
 {
  "input": "Remember the columns of this matrix are telling you how that matrix acts as a transform, each one telling you where the basis vectors of the input space land.",
  "translatedText": "この行列の列は、その行列が変換としてどのように機 能するかを示しており、各列は入力空間の基底ベクトルがどこに到達するかを示していることに注意してく ださい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.8,
  "end": 130.08
 },
 {
  "input": "So what we have is a sort of puzzle, which input vector x y is going to land on this output negative 4 negative 2.",
  "translatedText": "つまり、どの入力ベクトルx yが、この出力マイナス4マイナス2に着地するのかという、一種のパズルなのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.86,
  "end": 138.6
 },
 {
  "input": "One way to think about our little puzzle here is that we know the given output vector is some linear combination of the columns of the matrix x times the vector where i hat lands plus y times the vector where j hat lands, but what we want is to figure out what exactly that linear combination should be.",
  "translatedText": "この小さなパズルを考える一つの方法は、与えられた出力ベクトルが、行列の列xにiの帽子が乗るベクトルを足したものと、yにjの帽子が乗るベクトルを足したものの線形結合であることはわかっている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 139.7,
  "end": 156.22
 },
 {
  "input": "Remember the type of answer you get here can depend on whether or not the transformation squishes all of space into a lower dimension, that is if it has a zero determinant.",
  "translatedText": "ここで得られる答えの種類は、変換が空間のすべてを低次元に押し込めるかどうか、つまり行列式がゼロであるかどうかに依存することを覚えておいてほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 157.24,
  "end": 166.1
 },
 {
  "input": "In that case either none of the inputs land on our given output, or there's a whole bunch of inputs landing on that output.",
  "translatedText": "その場合、どのインプットも与えられたアウトプットに着地しないか、あるいはそのアウトプットにたくさんのインプットが着地することになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 166.1,
  "end": 173.9
 },
 {
  "input": "But for this video we'll limit our view to the case of a non-zero determinant, meaning the outputs of this transformation still span the full in-dimensional space that it started in.",
  "translatedText": "しかし、このビデオでは、行列式がゼロでない場合に限定して説明する。つまり、この変換の出力は、変換が開始されたときの完全な2次元空間にまだまたがっているということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 187.14
 },
 {
  "input": "Every input lands on one and only one output, and every output has one and only one input.",
  "translatedText": "すべての入力は1つだけの出力に着地し、すべての出力は1つだけの入力を持つ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 187.5,
  "end": 192.7
 },
 {
  "input": "As a first pass let me show you an idea that's wrong but in the right direction.",
  "translatedText": "最初のパスとして、間違っているが正しい方向にあるアイデアをお見せしよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 194.18,
  "end": 198.16
 },
 {
  "input": "The x coordinate of this mystery input vector is what you get by taking its dot product with the first basis vector 1 0.",
  "translatedText": "この謎の入力ベクトルの x 座標は、最初の基底ベクトル [1; 0]。 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 198.8,
  "end": 205.44
 },
 {
  "input": "Likewise the y coordinate is what you get by dotting it with the second basis vector 0 1.",
  "translatedText": "同様に、y 座標は、2 番目の基底ベクトル 0, 1 で点を打つことによって得られます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 206.16,
  "end": 211.4
 },
 {
  "input": "So maybe you hope that after the transformation the dot products with the transformed version of the mystery vector with the transformed version of the basis vectors will also be these coordinates x and y.",
  "translatedText": "したがって、変換後、ミステリー ベクト ルと変換後のバージョンの内積もこれらの座標 x と y になることを 期待しているかもしれません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 211.9,
  "end": 223.24
 },
 {
  "input": "That'd be fantastic because we know what the transformed version of each of those vectors are.",
  "translatedText": "これらの各ベクトルの変換バージョンが何であるかがわかって いるので、それは素晴らしいことです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.94,
  "end": 228.74
 },
 {
  "input": "There's just one problem with it, it's not at all true.",
  "translatedText": "ただ一つ問題があるのですが、それは全く真実ではありません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.18,
  "end": 234.2
 },
 {
  "input": "For most linear transformations the dot product before and after the transformation will look very different.",
  "translatedText": "ほとんどの線形変換では、変換前と変換後の内積は大きく異な ります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 234.64,
  "end": 240.24
 },
 {
  "input": "For example, you could have two vectors generally pointing in the same direction with a positive dot product, which get pulled apart from each other during the transformation in such a way that they end up having a negative dot product.",
  "translatedText": "たとえば、正の内積を持つ 2 つのベクトルが通常は同じ方 向を向いている場合、変換中にこれらのベクトルが互いに引き離されて 、最終的に負の内積になる場合があります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 240.8,
  "end": 251.52
 },
 {
  "input": "Likewise things that start off perpendicular with dot product 0, like the two basis vectors, quite often don't stay perpendicular to each other after the transformation, that is they don't preserve that 0 dot product.",
  "translatedText": "同様に、2 つの基底ベク トルのように、内積 0 で垂直に始まるものは、変換後も互いに垂直のまま ではないことがよくあります。つまり、その内積 0 が保持されません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 252.22,
  "end": 263.48
 },
 {
  "input": "And looking at the example I just showed dot products certainly aren't preserved, they tend to get bigger since most vectors are getting stretched out.",
  "translatedText": "そして、今示した例を見ると、確かにドット積は保存されず、ほとんどのベクトルが引き伸ばされるので、大きくなる傾向がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.1,
  "end": 270.3
 },
 {
  "input": "In fact, worthwhile side note here, transformations which do preserve dot products are special enough to have their own name, orthonormal transformations.",
  "translatedText": "実は、ドット積を保存する変換は、正規直交変換という独自の名前を持つほど特別なものなのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 271.04,
  "end": 279.1
 },
 {
  "input": "These are the ones that leave all of the basis vectors perpendicular to each other and still with unit lengths.",
  "translatedText": "これらは、すべての基底ベクトルを互い に垂直にし、単位長を維持したままにするものです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 279.72,
  "end": 284.66
 },
 {
  "input": "You often think of these as the rotation matrices, they correspond to rigid motion with no stretching or squishing or morphing.",
  "translatedText": "よく回転マトリックスと呼ばれるものは、伸縮やモーフィングを伴わない硬直した動きに対応する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 285.74,
  "end": 292.2
 },
 {
  "input": "Solving a linear system with an orthonormal matrix is actually super easy.",
  "translatedText": "正規直交行列で連立一次方程式を解くのは、実は超簡単だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.0,
  "end": 296.78
 },
 {
  "input": "Because dot products are preserved, taking the dot product between the output vector and all the columns of your matrix will be the same as taking the dot product between the mystery input vector and all of the basis vectors, which is the same as just finding the coordinates of that mystery input.",
  "translatedText": "ドット積は保存されるので、出力ベクトルと行列のすべての列との間のドット積を取ると、謎の入力ベクトルとすべての基底ベクトルとの間のドット積を取るのと同じになり、それは単にその謎の入力の座標を求めるのと同じになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 297.26,
  "end": 312.98
 },
 {
  "input": "So in that very special case, x would be the dot product of the first column with the output vector, and y would be the dot product of the second column with the output vector.",
  "translatedText": "したがっ て、この非常に特殊なケースでは、x は最初の列と出力ベクトルのドット 積になり、y は 2 番目の列と出力ベクトルのドット積になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 313.68,
  "end": 323.76
 },
 {
  "input": "Why am I bringing this up when this idea breaks down for almost all linear systems?",
  "translatedText": "この考え方はほとんどすべての線形システムで破綻するのに、なぜ私はこの話を持ち出したのだろうか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 326.82,
  "end": 330.86
 },
 {
  "input": "Well, it points us in a direction of something to look for.",
  "translatedText": "まあ、探すべきものの方向性を示してくれている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 331.42,
  "end": 334.08
 },
 {
  "input": "Is there an alternate geometric understanding for the coordinates of our input vector that remains unchanged after the transformation?",
  "translatedText": "変換後も変わらない入力ベクトルの座標について、別の幾何学的な理解はあるのだろうか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 334.32,
  "end": 341.66
 },
 {
  "input": "If your mind has been mulling over determinants, you might think of the following clever idea.",
  "translatedText": "もしあなたが決定力について頭を悩ませているのなら、次のような妙案を思いつくかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 342.36,
  "end": 346.8
 },
 {
  "input": "Take the parallelogram defined by the first basis vector i-hat and the mystery input vector xy.",
  "translatedText": "最初の基底ベクトルi-hatと謎の入力ベクトルxyで定義される平行四辺形を取る。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 347.36,
  "end": 353.76
 },
 {
  "input": "The area of this parallelogram is the base, 1, times the height perpendicular to that base, which is the y-coordinate of that input vector.",
  "translatedText": "この平行四辺形の面積は、底辺とその底辺に垂直な高さの 1 倍であり、 これが入力ベクトルの y 座標になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 354.44,
  "end": 362.96
 },
 {
  "input": "So the area of that parallelogram is a sort of screwy roundabout way to describe the vector's y-coordinate.",
  "translatedText": "つまり、その平行四辺形の面積は、ベクトルのy座標を表現するための、ある意味ねじ曲がった回りくどい方法なのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 363.68,
  "end": 369.96
 },
 {
  "input": "It's a wacky way to talk about coordinates, but run with me.",
  "translatedText": "座標について話すには奇妙な方法だが、僕と一緒に走ってくれ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 370.42,
  "end": 373.14
 },
 {
  "input": "And actually, to be a little more accurate, you should think of this as the signed area of that parallelogram, in the sense described in the determinant video.",
  "translatedText": "そして実際には、もう少し正確に言うと、行列式のビデオで 説明されている意味で、これをその平行四辺形の符号付き領域と考える必要があります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 373.7,
  "end": 381.64
 },
 {
  "input": "That way, a vector with a negative y-coordinate would correspond to a negative area for this parallelogram, at least if you think of i-hat as in some sense being the first out of these two vectors defining the parallelogram.",
  "translatedText": "少なくとも、i-hatが平行四辺形を定義する2つのベクトルのうち、ある意味で最初のベクトルであると考えるなら、負のy座標を持つベクトルは、この平行四辺形の負の面積に対応することになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.2,
  "end": 394.5
 },
 {
  "input": "And symmetrically, if you look at the parallelogram spanned by our mystery input vector and the second basis, j-hat, its area is going to be the x-coordinate of that mystery vector.",
  "translatedText": "そして対称的に、謎の入力ベクトルと2つ目の基底であるj-hatにまたがる平行四辺形を見ると、その面積は謎のベクトルのx座標になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 395.16,
  "end": 405.2
 },
 {
  "input": "Again, it's a strange way to represent the x-coordinate, but see what it buys us in a moment.",
  "translatedText": "繰り返しになるが、これはX座標を表す奇妙な方法である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 405.78,
  "end": 410.08
 },
 {
  "input": "And just to make sure it's clear how this might generalize, let's look in three dimensions.",
  "translatedText": "そして、これがどのように一般化されるかを明確にするために、3次元で見てみよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.68,
  "end": 413.76
 },
 {
  "input": "Ordinarily, the way you might think about one of a vector's coordinates, say its z-coordinate, would be to take its dot product with the third standard basis vector, often called k-hat.",
  "translatedText": "通常、ベクトルの座標の1つ、例えばz座標について考えるには、しばしばkハットと呼ばれる3番目の標準基底ベクトルとの内積を取ることになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 414.3,
  "end": 423.56
 },
 {
  "input": "But an alternate geometric interpretation would be to consider the parallelepiped that it creates with the other two basis vectors, i-hat and j-hat.",
  "translatedText": "しかし、別の幾何学的な解釈としては、他の2つの基底ベクトル、i-hatとj-hatで作る平行六面体を考えることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 424.28,
  "end": 432.88
 },
 {
  "input": "If you think of the square with area 1 spanned by i-hat and j-hat as the base of this whole shape, then its volume is the same as its height, which is the third coordinate of our vector.",
  "translatedText": "i-hatとj-hatにまたがる面積1の正方形をこの図形全体の底辺と考えると、その体積はベクトルの3番目の座標である高さと同じになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.42,
  "end": 443.58
 },
 {
  "input": "And likewise, the wacky way to think about the other coordinates of the vector would be to form a parallelepiped using the vector and then all of the basis vectors other than the one corresponding to the direction you're looking for.",
  "translatedText": "同様に、ベクトルの他の座標について考える奇抜な方法は、ベクトルと、探している方向に対応するベクトル以外のすべての基底ベクトルを使って平行六面体を形成することだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 444.34,
  "end": 455.44
 },
 {
  "input": "Then the volume of this gives you the coordinate.",
  "translatedText": "この体積から座標が求められる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 455.9,
  "end": 457.9
 },
 {
  "input": "Or rather, we should be talking about the signed volume of parallelepiped in the sense described in the determinant video using the right-hand rule.",
  "translatedText": "というより、右手の法則を使った行列式のビデオで説明されている意味での平行六面体の符号付き体積について話すべきなのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.44,
  "end": 465.0
 },
 {
  "input": "So the order in which you list these three vectors matters.",
  "translatedText": "つまり、この3つのベクトルを並べる順番が重要なのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.56,
  "end": 468.14
 },
 {
  "input": "That way, negative coordinates still make sense.",
  "translatedText": "そうすれば、負の座標もまだ意味を持つ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 468.8,
  "end": 471.1
 },
 {
  "input": "Okay, so why think of coordinates as areas and volumes like this?",
  "translatedText": "では、なぜこのように座標を面積や体積として考えるのか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 472.04,
  "end": 475.24
 },
 {
  "input": "Well, as you apply some sort of matrix transformation, the areas of these parallelograms, well, they don't stay the same, they might get scaled up or down.",
  "translatedText": "さて、ある種の行列変換を適用すると、これらの平行四辺形の面積は同じままではなく、拡大または縮小されるかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.72,
  "end": 483.78
 },
 {
  "input": "But, and this is the key idea of determinants, all of the areas get scaled by the same amount, namely the determinant of our transformation matrix.",
  "translatedText": "しかし、これが行列式の重要な考え方なのだが、すべての領域は同じ量、つまり変換行列の行列式でスケーリングされる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.16,
  "end": 492.64
 },
 {
  "input": "For example, if you look at the parallelogram spanned by the vector where your first basis vector lands, which is the first column of the matrix, and the transformed version of xy, what is its area?",
  "translatedText": "例えば、行列の最初の列である最初の基底ベクトルが位置するベクトルと、xyを変換したベクトルによってスパンされる平行四辺形を見た場合、その面積はいくらになるだろうか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 493.52,
  "end": 504.58
 },
 {
  "input": "Well, this is the transformed version of the parallelogram we were looking at earlier, the one whose area was the y-coordinate of the mystery input vector.",
  "translatedText": "さて、これは先ほど見た平行四辺形を変形したもので、その面積は謎の入力ベクトルのy座標である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 505.58,
  "end": 513.38
 },
 {
  "input": "So its area is just going to be the determinant of the transformation multiplied by that y-coordinate.",
  "translatedText": "つまり面積は、変換の行列式にy座標を掛けたものになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 513.7,
  "end": 519.28
 },
 {
  "input": "So that means we can solve for y by taking the area of this new parallelogram in the output space divided by the determinant of the full transformation.",
  "translatedText": "つまり、出力空間におけるこの新しい平行四辺形の面積を、完全変換の行列式で割ることによって、yを解くことができるということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 520.18,
  "end": 529.88
 },
 {
  "input": "And how do you get that area?",
  "translatedText": "そのエリアをどうやって確保するのか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 530.9,
  "end": 532.42
 },
 {
  "input": "Well, we know the coordinates for where the mystery input vector lands, that's the whole point of a linear system of equations.",
  "translatedText": "さて、謎の入力ベクトルがどこにあるのか、座標はわかっている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 533.24,
  "end": 539.16
 },
 {
  "input": "So what you might do is create a new matrix whose first column is the same as that of our matrix, but whose second column is the output vector, and then you take its determinant.",
  "translatedText": "つまり、最初の列が我々の行列と同じで、2番目の列が出力ベクトルである新しい行列を作り、その行列式を求めるのである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.72,
  "end": 550.32
 },
 {
  "input": "So look at that, just using data from the output of the transformation, namely the columns of the matrix and the coordinates of our output vector, we can recover the y-coordinate of the mystery input vector, which is halfway to solving the system.",
  "translatedText": "つまり、変換の出力、つまり行列の列と出力ベクトルの座標のデータを使うだけで、謎の入力ベクトルのy座標を復元することができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 551.26,
  "end": 564.4
 },
 {
  "input": "Likewise, the same idea can give us the x-coordinate.",
  "translatedText": "同様に、同じ考え方でx座標も求めることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 565.12,
  "end": 567.54
 },
 {
  "input": "Look at the parallelogram we defined earlier, which encodes the x-coordinate of the mystery input vector spanned by that vector and j-hat.",
  "translatedText": "先に定義した平行四辺形を見てみよう。この平行四辺形は、そのベクトルとj-hatによってスパンされる謎の入力ベクトルのx座標を符号化している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 568.0,
  "end": 575.74
 },
 {
  "input": "The transformed version of this guy is spanned by the output vector and the second column of the matrix, and its area will have been multiplied by the determinant of that matrix.",
  "translatedText": "この行列の変換バージョンは、出力ベクトルと行列の2列目にまたがっており、その面積には行列の行列式が乗じられている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.4,
  "end": 586.92
 },
 {
  "input": "So to solve for x, you can take this new area divided by the determinant of the full transformation.",
  "translatedText": "したがって、xを解くには、この新しい面積を完全変換の行列式で割ればよい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.7,
  "end": 592.94
 },
 {
  "input": "And similar to what we did before, you can compute the area of that output parallelogram by creating a new matrix whose first column is the output vector and whose second column is the same as the original matrix.",
  "translatedText": "そして、前にやったことと同様に、1列目が出力ベクトル、2列目が元の行列と同じである新しい行列を作成することで、出力された平行四辺形の面積を計算することができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 593.86,
  "end": 605.66
 },
 {
  "input": "So again, just using data from the output space, the numbers we see in our original linear system, we can solve for what x must be.",
  "translatedText": "つまり、出力空間からのデータ、つまり元の線形システムで見た数字を使うだけで、xが何でなければならないかを解くことができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.24,
  "end": 612.86
 },
 {
  "input": "This formula for finding the solutions to a linear system of equations is known as Cramer's rule.",
  "translatedText": "連立一次方程式の解を求めるこの公式は、クレーマーの法則として知られている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.42,
  "end": 618.42
 },
 {
  "input": "Here, just to sanity check ourselves, plug in some numbers here.",
  "translatedText": "ここで、自分たちの正気を確認するために、いくつかの数字を差し込んでみよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 619.12,
  "end": 621.9
 },
 {
  "input": "The determinant of that top altered matrix is 4 plus 2, which is 6, and the bottom determinant is 2, so the x-coordinate should be 3.",
  "translatedText": "上の行列の行列式は4＋2で6となり、下の行列式は2なのでx座標は3となる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 622.26,
  "end": 630.82
 },
 {
  "input": "And indeed, looking back at the input vector we started with, the x-coordinate is 3.",
  "translatedText": "そして実際、入力ベクトルを振り返ってみると、x座標は3である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.44,
  "end": 635.46
 },
 {
  "input": "Likewise, Cramer's rule suggests that the y-coordinate should be 4 divided by 2, or 2, and that is in fact the y-coordinate of the input vector we were starting with.",
  "translatedText": "同様に、クレーマーの法則によれば、y座標は4÷2、つまり2であるべきであり、これは実際に我々が最初に入力したベクトルのy座標である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 636.32,
  "end": 646.5
 },
 {
  "input": "The case with three dimensions or more is similar, and I highly recommend you take a moment to pause and think through it yourself.",
  "translatedText": "3次元以上の場合も同様で、ぜひ一度立ち止まって考えてみることをお勧めする。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 647.38,
  "end": 653.48
 },
 {
  "input": "Here, I'll give you a little bit of momentum.",
  "translatedText": "ここで、少し勢いをつけよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.18,
  "end": 655.9
 },
 {
  "input": "What we have is a known transformation given by some 3x3 matrix and a known output vector given by the right side of our linear system, and we want to know what input lands on that output.",
  "translatedText": "我々が持っているのは、ある3x3の行列で与えられる既知の変換と、線形システムの右辺で与えられる既知の出力ベクトルであり、その出力にどのような入力が重なるかを知りたいのである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.34,
  "end": 668.24
 },
 {
  "input": "And if you think of, say, the z-coordinate of that input vector as the volume of that special parallelepiped we were looking at earlier, spanned by i-hat, j-hat, and the mystery input vector, what happens to that volume after the transformation?",
  "translatedText": "そして、その入力ベクトルのz座標を、先ほど見たi-hat、j-hat、そして謎の入力ベクトルにまたがる特殊な平行六面体の体積と考えると、変換後のその体積はどうなるのだろうか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.1,
  "end": 683.78
 },
 {
  "input": "And what are the various ways you can compute that volume?",
  "translatedText": "そして、その体積を計算するさまざまな方法は何か？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 684.8,
  "end": 687.48
 },
 {
  "input": "Really, pause and take a moment to think through the details of generalizing this to higher dimensions, finding an expression for each coordinate of the solution to a larger linear system.",
  "translatedText": "もっと大きな連立一次方程式の解の各座標の式を求め、これをより高い次元に一般化するための詳細を考えるために、少し立ち止まって考えてみてほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 688.34,
  "end": 697.42
 },
 {
  "input": "Thinking through more general cases like this and convincing yourself that it works and why it works is where all the learning really happens, much more so than listening to some dude on YouTube walk you through the same reasoning again.",
  "translatedText": "このような一般的なケースを考え、それが機能し、なぜ機能するのかを自分自身で納得させることが、すべての学習が本当に行われるところであり、YouTubeでどこかの男が同じ推論を繰り返し説明するのを聞くよりもずっと重要なのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 698.06,
  "end": 708.5
 }
]