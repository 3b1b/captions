1
00:00:11,590 --> 00:00:15,800
In un video precedente, ho
systems of equations, and I sort of brushed

2
parlato della linearità 00:00:15,800
aside the discussion of actually computing
solutions to these systems.

--&gt; 00:00:19,690 3 è
00:00:19,690 --> 00:00:23,500
And while it’s true that number-crunching
is something we typically leave to the computers,
una buona cartina di
4
00:00:23,500 --> 00:00:27,430
digging into some of these computational methods
tornasole per capire se effettivamente

5
00:00:27,430 --> 00:00:31,680
capisci o meno cosa
this is really where the rubber meets the

6
sta succedendo, dal momento
road.

7
che 00:00:31,680 --&gt; 00:00:32,680
Here I want to describe the geometry behind
a certain method for computing solutions to

00:00:32,680 --&gt; 00:00:36,379 8 9
00:00:36,379 --> 00:00:39,760
these systems, known as Cramer’s rule.

assicurati di guardare i
00:00:39,760 --> 00:00:44,230
The relevant background needed here is an
understanding of determinants, dot products,
video pertinenti su questi
10
00:00:44,230 --> 00:00:48,140
and of linear systems of equations, so be
argomenti se non hai

11
00:00:48,140 --> 00:00:50,489
familiarità o sei arrugginito.

12
00:00:50,489 --> 00:00:51,489
Ma prima!

13
00:00:51,489 --> 00:00:56,379
Dovrei dire subito che
is not the best way for computing solutions

14
la regola di
to linear systems of equations.

15
Cramer 00:00:56,379 --&gt;
Gaussian elimination, for example, will always
be faster.

00:00:58,010 00:00:58,010 --&gt;
00:01:01,950 --> 00:01:03,950
So why learn it?

00:01:01,950 16 17
00:01:03,950 --> 00:01:07,950
Think of this as a sort of cultural excursion;
it’s a helpful exercise in deepening your
aiuta a consolidare le
18
00:01:07,950 --> 00:01:10,520
knowledge of the theory of these systems.
idee dell&#39;algebra lineare,
19
00:01:10,520 --> 00:01:15,500
Wrapping your mind around this concept will
come il determinante

20
00:01:15,500 --> 00:01:19,960
e i sistemi
seeing how they relate to each other.

21
lineari, entro 00:01:19,960
Also, from a purely artistic standpoint, the
ultimate result is just really pretty to think

--&gt; 00:01:24,619 22 23
00:01:24,619 --> 00:01:28,340
about, much more so that Gaussian elimination.

funzionerà con sistemi
00:01:28,340 --> 00:01:33,740
Alright, so the setup here will be some linear
system of equations, say with two unknowns,
con un numero
24
00:01:33,740 --> 00:01:35,990
x and y, and two equations.
maggiore di incognite
25
00:01:35,990 --> 00:01:40,450
In principle, everything we’re talking about
e lo stesso

26
00:01:40,450 --> 00:01:41,840
numero di equazioni.

27
00:01:41,840 --> 00:01:46,349
Ma per semplicità,
to hold in our heads.

28
un esempio più
So as I talked about in a previous video,
you can think of this setup geometrically

piccolo è più
00:01:50,599 --> 00:01:56,599
as a certain known matrix transforming an
unknown vector, [x; y], where you know what
carino 00:01:46,349 --&gt;
30
00:01:56,599 --> 00:02:00,420
the output is going to be, in this case [-4;
00:01:50,599 29 -2].

31
00:02:00,420 --> 00:02:06,250
Ricorda, le colonne di
you how the matrix acts as a transform, each

32
questa matrice indicano
one telling you where the basis vectors of
the input space land.

00:02:06,250 --&gt; 00:02:10,899 33
00:02:10,899 --> 00:02:23,060
So this is a sort of puzzle, what input [x;
y], is going to give you this
il tipo di
34
00:02:23,060 --> 00:02:28,150
output [-4; -2]?
risposta che ottieni
35
00:02:28,150 --> 00:02:39,680
Remember, the 
qui può dipendere dal

36
00:02:39,680 --> 00:02:44,299
fatto che la
all of space into a lower dimension.

37
trasformazione schiaccia o meno
That is if it has zero determinant.

38
00:02:44,299 --&gt; 00:02:46,080
In that case, either none of the inputs land
on our given output or there are a whole bunch

00:02:46,080 --&gt; 00:02:51,849
00:02:51,849 --> 00:02:57,540
of inputs landing on that output.

39 40 l&#39;intero spazio
00:02:57,540 --> 00:03:01,709
But for this video we’ll limit our view
to the case of a non-zero determinant, meaning
n-dimensionale in cui
41
00:03:01,709 --> 00:03:07,790
the output of this transformation still spans
è iniziato; ogni input

42
00:03:07,790 --> 00:03:12,549
arriva a uno
and every output has one and only one input.

43
e un solo
One way to think about our puzzle is that
we know the given output vector is some linear

output 00:03:12,549 --&gt; 00:03:14,840
00:03:14,840 --> 00:03:15,840
combination of the columns of the matrix;
x*(the vector where i-hat lands) + y*(the
44 calcola cosa
45
00:03:15,840 --> 00:03:16,840
vector where j-hat lands), but we wish to
sono esattamente xey.

46
00:03:16,840 --> 00:03:18,829
Come primo passaggio,
is wrong, but in the right direction.

47
lasciatemi mostrare un&#39;idea
The x-coordinate of this mystery input vector
is what you get by taking its dot product

che 00:03:18,829 --&gt;
00:03:23,340 --> 00:03:25,939
with the first basis vector, [1; 0].

00:03:23,340 48 49
00:03:25,939 --> 00:03:30,830
Likewise, the y-coordinate is what you get
by dotting it with the second basis vector,
i prodotti scalari
50
00:03:30,830 --> 00:03:31,980
[0; 1].
con la versione
51
00:03:31,980 --> 00:03:37,439
So maybe you hope that after the transformation,
trasformata del vettore

52
00:03:37,439 --> 00:03:41,939
misterioso con la
versions of the basis vectors will also be

53
trasformata 00:03:41,939 --&gt;
these coordinates x and y.

54
00: 03:43,590 00:03:43,590
That’d be fantastic because we know the
transformed versions of each of these vectors.

--&gt; 00:03:50,400 55
00:03:50,400 --> 00:03:54,739
There’s just one problem with this: it’s
not at all true!
prima e dopo
56
00:03:54,739 --> 00:03:59,450
For most linear transformations, the dot product
la trasformazione sarà

57
00:03:59,450 --> 00:04:00,840
molto diverso.

58
00:04:00,840 --> 00:04:04,959
Ad esempio, potresti avere
pointing in the same direction, with a positive

59
due vettori che
dot product, which get pulled away from each
other during the transformation, in such a

generalmente 00:04:04,959 --&gt;
00:04:09,270 --> 00:04:11,909
way that they then have a negative dot product.

00:04:09,270 60 61 rimarranno
00:04:11,909 --> 00:04:16,840
Likewise, if things start off perpendicular,
with dot product zero, like the two basis
perpendicolari dopo la
62
00:04:16,840 --> 00:04:22,040
vectors, there’s no guarantee that they
trasformazione, preservando il

63
00:04:22,040 --> 00:04:24,050
prodotto punto zero.

64
00:04:24,050 --> 00:04:27,140
Nell&#39;esempio che stavamo
certainly aren’t preserved.

65
guardando, i prodotti
They tend to get bigger since most vectors
are getting stretched.

scalari 00:04:27,140
00:04:30,950 --> 00:04:36,730
In fact, transformations which do preserve
dot products are special enough to have their
--&gt; 00:04:30,950 66
67
00:04:36,730 --> 00:04:39,800
own name: Orthonormal transformations.
vettori perpendicolari
68
00:04:39,800 --> 00:04:44,259
These are the ones which leave all the basis
tra loro con

69
00:04:44,259 --> 00:04:45,810
lunghezze unitarie.

70
00:04:45,810 --> 00:04:48,470
Spesso si pensa a queste come a matrici di rotazione.

71
00:04:48,470 --> 00:04:53,000
Corrispondono al movimento rigido,
squishing or morphing.

72
senza allungamento, 00:04:53,000 --&gt;
Solving a linear system with an orthonormal
matrix is very easy: Since dot products are

00:04:58,920 73 prodotti tra
00:04:58,920 --> 00:05:03,060
preserved, taking the dot product between
the output vector and all the columns of your
il vettore di input
74
00:05:03,060 --> 00:05:08,380
matrix will be the same as taking the dot
e tutti i vettori

75
00:05:08,380 --> 00:05:13,599
base, che equivale a
the coordinates of the input vector.

76
trovare 00:05:13,599 --&gt; 00
So, in that very special case, x would be
the dot product of the first column with the

:05:18,690 77 nella
00:05:18,690 --> 00:05:24,580
output vector, and y would be the dot product
of the second column with the output vector.
maggior parte dei sistemi
78
00:05:24,580 --> 00:05:32,880
Now, even though this idea breaks down for
lineari, ci indica la

79
00:05:32,880 --> 00:05:37,780
direzione di qualcosa da
geometric understanding for the coordinates

80
cercare: esiste un vettore
of our input vector which remains unchanged
after the transformation?

alternativo 00:05:37,780 --&gt; 00:05:42,780
00:05:42,780 --> 00:05:47,631
If your mind has been mulling over determinants,
you might think of this clever idea: Take
81 vettore, i-hat e
82
00:05:47,631 --> 00:05:53,200
the parallelogram defined by the first basis
il misterioso vettore di

83
00:05:53,200 --> 00:05:54,590
input [X; y].

84
00:05:54,590 --> 00:05:59,990
L&#39;area di questo parallelogramma
1, times the height perpendicular to that

85
è la sua base,
base, which is the y-coordinate of our input
vector.

00:05:59,990 --&gt; 00:06:03,460 86
00:06:03,460 --> 00:06:09,120
So, the area of this parallelogram is sort
of a screwy roundabout way to describe the
per parlare di coordinate,
87
00:06:09,120 --> 00:06:13,590
vector’s y-coordinate; it’s a wacky way
ma corri con me.

88
00:06:13,590 --> 00:06:19,690
In realtà, per essere
think of the signed area of this parallelogram,

89
più precisi, dovresti
in the sense described by the determinant
video.

00:06:19,690 --&gt; 00:06:22,440 90
00:06:22,440 --> 00:06:28,110
That way, a vector with negative y-coordinate
would correspond to a negative area for this
guardare il parallelogramma
91
00:06:28,110 --> 00:06:29,110
parallelogram.
formato dal vettore e
92
00:06:29,110 --> 00:06:39,490
Symmetrically, if you 
dal secondo vettore

93
00:06:39,490 --> 00:06:45,099
base, j-hat, la sua
will be the x-coordinate of the vector.

94
area 00:06:45,099 --
Again, it’s a strange way to represent the
x-coordinate, but you’ll see what it buys

&gt; 00:06:49,370 95 96
00:06:49,370 --> 00:06:50,449
us in a moment.

sarebbe prendere il
00:06:50,449 --> 00:06:56,101
Here’s what this would look like in three-dimensions:
Ordinarily the way you might think of one
suo prodotto scalare con
97
00:06:56,101 --> 00:07:01,060
of a vector’s coordinate, say its z-coordinate,
il terzo vettore

98
00:07:01,060 --> 00:07:04,439
base standard, k-hat.

99
00:07:04,439 --> 00:07:11,870
Ma invece, considera il
creates with the other two basis vectors,

100
parallelepipedo che 00:07:11,870
i-hat and j-hat.

101
--&gt; 00:07:13,569 00:07:13,569
If you think of the square with area 1 spanned
by i-hat and j-hat as the base of this guy,

--&gt; 00:07:20,030 102
00:07:20,030 --> 00:07:24,259
its volume is the same its height, which is
the third coordinate of our vector.
un&#39;altra coordinata di
103
00:07:24,259 --> 00:07:28,370
Likewise, the wacky way to think about any
questo vettore è

104
00:07:28,370 --> 00:07:34,950
quella di formare
all the basis vectors other than the one you’re

105
il parallelepipedo tra
looking for, and get its volume.

106
questo vettore e
Or, rather, we should talk about the signed
volume of these parallelepipeds, in the sense

00: 07:34,950 --&gt;
00:07:42,490 --> 00:07:47,819
described in the determinant video, where
the order in which you list the three vectors
00:07:37,900 00:07:37,900 --&gt;
108
00:07:47,819 --> 00:07:48,900
matters and you’re using the right-hand
00:07:42,490 107 regola.

109
00:07:48,900 --> 00:07:51,610
In questo modo le coordinate negative hanno ancora senso.

110
00:07:51,610 --> 00:07:56,000
Ok, allora perché
and volumes like this?

111
pensare alle
As you apply some matrix transformation, the
areas of the parallelograms don’t stay the

coordinate come ad
00:08:02,039 --> 00:08:04,129
same, they may get scaled up or down.

aree 00:07:56,000
00:08:04,129 --> 00:08:09,940
But(!), and this is a key idea of determinants,
all these areas get scaled by the same amount.
--&gt; 00:08:02,039 112
114
00:08:09,940 --> 00:08:13,560
Namely, the determinant of our transformation
113 matrice.

115
00:08:13,560 --> 00:08:17,850
Ad esempio, se
spanned by the vector where your first basis

116
guardi il
vector lands, which is the first column of
the matrix, and the transformed version of

parallelogramma 00:08:17,850
00:08:22,850 --> 00:08:25,180
[x; y], what is its area?

--&gt; 00:08:22,850 117
00:08:25,180 --> 00:08:30,229
Well, this is the transformed version of that
parallelogram we were looking at earlier,
118 vettore
119
00:08:30,229 --> 00:08:33,950
whose area was the y-coordinate of the mystery
di input.

120
00:08:33,950 --> 00:08:39,080
Quindi la sua area
transformation multiplied by that value.

121
sarà il determinante
So, the y-coordinate of our mystery input
vector is the area of this parallelogram,

dei 00:08:39,080 --&gt; 00:08:44,590
00:08:44,590 --> 00:08:48,510
spanned by the first column of the matrix
and the output vector, divided by the determinant
122 misteriosi vettori
123
00:08:48,510 --> 00:08:51,120
of the full transformation.
di input, questo
124
00:08:51,120 --> 00:08:53,090
And how do you get this area?
è il punto centrale
125
00:08:53,090 --> 00:08:57,360
Well, we know the coordinates for where the
di un sistema

126
00:08:57,360 --> 00:08:59,850
lineare di equazioni.

127
00:08:59,850 --> 00:09:05,670
Quindi, creando una matrice la
the same as that of our matrix, and whose

128
cui prima colonna è
second column is the output vector, and take
its determinant.

00:09:05,670 --&gt; 00:09:11,250 129
00:09:11,250 --> 00:09:16,560
So look at that; just using data from the
output of the transformation, namely the columns
vettore, possiamo recuperare la
130
00:09:16,560 --> 00:09:21,340
of the matrix and the coordinates of our output
coordinata y del nostro

131
00:09:21,340 --> 00:09:23,880
misterioso vettore di input.

132
00:09:23,880 --> 00:09:28,100
Allo stesso modo, la stessa idea può darti la coordinata x.

133
00:09:28,100 --> 00:09:32,580
Guarda quel parallelogramma che
which encodes the x-coordinate of the mystery

134
abbiamo definito all&#39;inizio
input vector, spanned by the input vector
and j-hat.

00:09:32,580 --&gt; 00:09:36,420 135
00:09:36,420 --> 00:09:41,970
The transformed version of this guy is spanned
by the output vector and the second column
moltiplicato per il
136
00:09:41,970 --> 00:09:47,710
of the matrix, and its area will have been
determinante della matrice.

137
00:09:47,710 --> 00:09:52,000
Quindi la coordinata x
is this area divided by the determinant of

138
del nostro misterioso vettore
the transformation.

139
di input 00:09:52,000 --&gt;
Symmetric to what we did before, you can compute
the area of that output parallelogram by creating

00:09:53,980 00:09:53,980 --&gt; 00:09:58,900
00:09:58,900 --> 00:10:04,530
a new matrix whose first column is the output
vector, and whose second column is the same
140 spazio, i numeri
141
00:10:04,530 --> 00:10:06,300
as the original matrix.
che vediamo nel nostro
142
00:10:06,300 --> 00:10:10,120
So again, just using data from the output
sistema lineare originale, noi

143
00:10:10,120 --> 00:10:13,600
può recuperare la coordinata
of our mystery input vector.

144
x 00:10:13,600 --&gt; 00:10:18,440
This formula for finding the solutions to
a linear system of equations is known as Cramer’s

145 146 è 4+2,
00:10:18,440 --> 00:10:19,440
rule.

che è 6, e
00:10:19,440 --> 00:10:22,400
Here, just to sanity check ourselves, plug
in the numbers here.
il determinante inferiore è
147
00:10:22,400 --> 00:10:28,430
The determinant of that top altered matrix
2, quindi la coordinata

148
00:10:28,430 --> 00:10:31,430
x dovrebbe essere 3.

149
00:10:31,430 --> 00:10:35,910
E infatti, guardando indietro
we started with, it’s x-coordinate is 3.

150
a quel vettore di
Likewise, Cramer’s rule suggests the y-coordinate
should be 4/2, or 2, and that is indeed the

input 00:10:35,910 --&gt;
00:10:43,850 --> 00:10:47,540
y-coordinate of the input vector we started
with here.
00:10:43,850 151 e ti
152
00:10:47,540 --> 00:10:52,690
The case with three dimensions is similar,
consiglio vivamente di fermarti

153
00:10:52,690 --> 00:10:53,690
a pensarci attentamente.

154
00:10:53,690 --> 00:10:56,770
Ecco, ti do un piccolo slancio.

155
00:10:56,770 --> 00:11:02,780
Abbiamo questa trasformazione nota,
a 3x3 matrix, and a known output vector, given

156
data dal vettore
by the right side of our linear system, and
we want to know what input vector lands on

00:11:02,780 --&gt; 00:11:07,580
00:11:07,580 --> 00:11:09,200
this output vector.

157 158, cosa
00:11:09,200 --> 00:11:16,700
If you think of, say, the z-coordinate of
the input vector as the volume of this parallelepiped
succede al volume
159
00:11:16,700 --> 00:11:25,530
spanned by i-hat, j-hat, and the mystery input
di questo parallelepipedo

160
00:11:25,530 --> 00:11:26,530
dopo la trasformazione?

161
00:11:26,530 --> 00:11:28,190
Come puoi calcolare quel nuovo volume?

162
00:11:28,190 --> 00:11:32,200
Davvero, fermati e prenditi
the details of generalizing this to higher

163
un momento per
dimensions; finding an expression for each
coordinate of the solution to larger linear

riflettere 00:11:32,200 --&gt;
00:11:37,330 --> 00:11:38,330
systems.

00:11:37,330 164 165
00:11:38,330 --> 00:11:44,140
Thinking through more general cases and convincing
yourself that it works is where all the learning
qualche tizio su
166
00:11:44,140 --> 00:11:48,520
will happen, much more so than listening to
YouTube spiega di

167
00:11:48,520 --> 00:12:09,940
nuovo il ragionamento.

