[
 {
  "input": "In a previous video I've talked about linear systems of equations, and I sort of brushed aside the discussion of actually computing solutions to these systems.",
  "translatedText": "In un video precedente, ho parlato di sistemi lineari di equazioni e ho in un certo senso messo da parte la discussione sulle soluzioni effettivamente computazionali per questi sistemi. ",
  "n_reviews": 0,
  "start": 11.2,
  "end": 19.14
 },
 {
  "input": "And while it's true that number crunching is typically something we leave to the computers, digging into some of these computational methods is a good litmus test for whether or not you actually understand what's going on, since that's really where the rubber meets the road.",
  "translatedText": "E anche se è vero che l'elaborazione dei numeri è qualcosa che di solito lasciamo ai computer, scavare in alcuni di questi metodi computazionali è un buon test del nove per capire se si capisce davvero cosa sta succedendo, dal momento che è proprio qui che la gomma incontra la strada. . ",
  "n_reviews": 0,
  "start": 19.7,
  "end": 31.52
 },
 {
  "input": "Here I want to describe the geometry behind a certain method for computing solutions to these systems, known as Cramer's rule.",
  "translatedText": "Qui voglio descrivere la geometria dietro un certo metodo per calcolare le soluzioni a questi sistemi, noto come regola di Cramer. ",
  "n_reviews": 0,
  "start": 32.12,
  "end": 38.88
 },
 {
  "input": "The relevant background here is understanding determinants, a little bit of dot products, and of course linear systems of equations, so be sure to watch the relevant videos on those topics if you're unfamiliar or rusty.",
  "translatedText": "Il background pertinente necessario qui è la comprensione dei determinanti, dei prodotti scalari e dei sistemi lineari di equazioni, quindi assicurati di guardare i video pertinenti su questi argomenti se non hai familiarità o sei arrugginito. ",
  "n_reviews": 0,
  "start": 39.68,
  "end": 50.42
 },
 {
  "input": "But first",
  "translatedText": "Ma prima! ",
  "n_reviews": 0,
  "start": 51.02,
  "end": 51.44
 },
 {
  "input": "I should say up front that this Cramer's rule is not actually the best way for computing solutions to linear systems of equations,",
  "translatedText": "Dovrei dire subito che la regola di Cramer non è il modo migliore per calcolare soluzioni a sistemi lineari di equazioni. ",
  "n_reviews": 0,
  "start": 51.44,
  "end": 57.26
 },
 {
  "input": "Gaussian elimination for example will always be faster.",
  "translatedText": "L’eliminazione gaussiana, ad esempio, sarà sempre più veloce. ",
  "n_reviews": 0,
  "start": 58.14,
  "end": 61.26
 },
 {
  "input": "So why learn it?",
  "translatedText": "Allora perché impararlo? ",
  "n_reviews": 0,
  "start": 61.98,
  "end": 63.52
 },
 {
  "input": "Well think of it as a sort of cultural excursion. It's a helpful exercise in deepening your knowledge of the theory behind these systems.",
  "translatedText": "Consideratela una sorta di escursione culturale; è un esercizio utile per approfondire la conoscenza della teoria di questi sistemi. ",
  "n_reviews": 0,
  "start": 63.78,
  "end": 70.46
 },
 {
  "input": "Wrapping your mind around this concept is going to help consolidate ideas from linear algebra, like the determinant and linear systems, by seeing how they relate to each other.",
  "translatedText": "Comprendere questo concetto ti aiuterà a consolidare le idee dell'algebra lineare, come il determinante e i sistemi lineari, vedendo come si relazionano tra loro. ",
  "n_reviews": 0,
  "start": 71.04,
  "end": 79.62
 },
 {
  "input": "Also from a purely artistic standpoint the ultimate result here is just really pretty to think about, way more so than Gaussian elimination.",
  "translatedText": "Inoltre, da un punto di vista puramente artistico, il risultato finale è davvero carino da pensare, tanto più che l'eliminazione gaussiana. ",
  "n_reviews": 0,
  "start": 80.1,
  "end": 86.88
 },
 {
  "input": "Alright so the setup here will be some linear system of equations, say with two unknowns x and y and two equations.",
  "translatedText": "Va bene, quindi la configurazione qui sarà un sistema lineare di equazioni, diciamo con due incognite, xey, e due equazioni. ",
  "n_reviews": 0,
  "start": 88.68,
  "end": 95.62
 },
 {
  "input": "In principle everything we're talking about will also work for systems with larger number of unknowns and the same number of equations,",
  "translatedText": "In linea di principio, tutto ciò di cui stiamo parlando funzionerà con sistemi con un numero maggiore di incognite e lo stesso numero di equazioni. ",
  "n_reviews": 0,
  "start": 96.3,
  "end": 101.94
 },
 {
  "input": "but for simplicity a smaller example is just nicer to hold in our heads.",
  "translatedText": "Ma per semplicità, è più bello tenere in testa un esempio più piccolo. ",
  "n_reviews": 0,
  "start": 102.44,
  "end": 105.58
 },
 {
  "input": "So as I talked about in a previous video you can think of this setup geometrically as a certain known matrix transforming an unknown vector x y where you know what the output is going to be, in this case negative 4 negative 2.",
  "translatedText": "Quindi, come ho detto in un video precedente, puoi pensare a questa configurazione geometricamente come una certa matrice conosciuta che trasforma un vettore sconosciuto, [x; y], dove sai quale sarà l'output, in questo caso [-4; -2]. ",
  "n_reviews": 0,
  "start": 106.32,
  "end": 120.04
 },
 {
  "input": "Remember the columns of this matrix are telling you how that matrix acts as a transform, each one telling you where the basis vectors of the input space land.",
  "translatedText": "Ricorda, le colonne di questa matrice ti dicono come la matrice agisce come una trasformazione, ognuna delle quali ti dice dove si fermano i vettori di base dello spazio di input. ",
  "n_reviews": 0,
  "start": 120.8,
  "end": 130.08
 },
 {
  "input": "So what we have is a sort of puzzle, which input vector x y is going to land on this output negative",
  "translatedText": "Quindi questo è una sorta di puzzle, quale input [x; y], ti darà questo output [-4; -2]? ",
  "n_reviews": 0,
  "start": 130.86,
  "end": 137.6
 },
 {
  "input": "4 negative 2. One way to think about our little puzzle here is that we know the given output vector is some linear combination of the columns of the matrix x times the",
  "translatedText": "Ricorda, il tipo di risposta che ottieni qui può dipendere dal fatto che la trasformazione schiacci o meno tutto lo spazio in una dimensione inferiore. ",
  "n_reviews": 0,
  "start": 137.6,
  "end": 148.4
 },
 {
  "input": "vector where i hat lands plus y times the vector where j",
  "translatedText": "Questo se ha determinante zero. ",
  "n_reviews": 0,
  "start": 148.4,
  "end": 151.22
 },
 {
  "input": "hat lands, but what we want is to figure out what exactly that linear combination should be.",
  "translatedText": "In tal caso, o nessuno degli input arriva al nostro dato output oppure ci sono un sacco di input che arrivano a quell'output. ",
  "n_reviews": 0,
  "start": 151.22,
  "end": 156.22
 },
 {
  "input": "Remember the type of answer you get here can depend on whether or not the transformation squishes all of space into a lower dimension, that is if it has a zero determinant. In that case either none of the inputs l",
  "translatedText": "Ma per questo video limiteremo la nostra visione al caso di un determinante diverso da zero, il che significa che l'output di questa trasformazione si estende ancora nell'intero spazio n-dimensionale in cui è iniziata; ogni input finisce su uno e un solo output e ogni output ha uno e un solo input. ",
  "n_reviews": 0,
  "start": 157.24,
  "end": 168.94
 },
 {
  "input": "and on our given output, or there's a whole bunch of inputs landing on that output. But for this video we'll limit our view to the case of a non-zero determinant, meaning the outputs of this transformation still span the full in-dimensional space that it started",
  "translatedText": "Un modo di pensare al nostro puzzle è che sappiamo che il vettore di output dato è una combinazione lineare delle colonne della matrice; x*(il vettore dove si ferma i-hat) + y*(il vettore dove si ferma j-hat), ma vogliamo calcolare cosa sono esattamente xey. ",
  "n_reviews": 0,
  "start": 168.94,
  "end": 186.72
 },
 {
  "input": "in. Every input lands on one and only one output, and every output has one and only one input. As a first pass let me show you an idea that's wrong but in the right direction.",
  "translatedText": "Come primo passo, lasciatemi mostrare un'idea sbagliata, ma nella giusta direzione. ",
  "n_reviews": 0,
  "start": 186.72,
  "end": 198.16
 },
 {
  "input": "The x coordinate of this mystery input vector is what you get by taking its dot product with the first basis vector 1 0.",
  "translatedText": "La coordinata x di questo misterioso vettore di input è quella che ottieni prendendo il suo prodotto scalare con il primo vettore base, [1; 0]. ",
  "n_reviews": 0,
  "start": 198.8,
  "end": 205.44
 },
 {
  "input": "Likewise the y coordinate is what you get by dotting it with the second basis vector 0 1.",
  "translatedText": "Allo stesso modo, la coordinata y è ciò che si ottiene punteggiandola con il secondo vettore base, [0; 1]. ",
  "n_reviews": 0,
  "start": 206.16,
  "end": 211.4
 },
 {
  "input": "So maybe you hope that after the transformation the dot products with the transformed version of the mystery vector with the transformed version of the basis vectors will also be these coordinates x and y.",
  "translatedText": "Quindi forse speri che dopo la trasformazione, i prodotti scalari con la versione trasformata del vettore misterioso con le versioni trasformate dei vettori base siano anch'essi queste coordinate xey. ",
  "n_reviews": 0,
  "start": 211.9,
  "end": 223.24
 },
 {
  "input": "That'd be fantastic because we know what the transformed version of each of those vectors are.",
  "translatedText": "Sarebbe fantastico perché conosciamo le versioni trasformate di ciascuno di questi vettori. ",
  "n_reviews": 0,
  "start": 223.94,
  "end": 228.74
 },
 {
  "input": "There's just one problem with it, it's not at all true.",
  "translatedText": "C'è solo un problema: non è affatto vero! ",
  "n_reviews": 0,
  "start": 231.18,
  "end": 234.2
 },
 {
  "input": "For most linear transformations the dot product before and after the transformation will look very different.",
  "translatedText": "Per la maggior parte delle trasformazioni lineari, il prodotto scalare prima e dopo la trasformazione sarà molto diverso. ",
  "n_reviews": 0,
  "start": 234.64,
  "end": 240.24
 },
 {
  "input": "For example you could have two vectors generally pointing in the same direction with a positive dot product which get pulled apart from each other during the transformation in such a way that they have a negative dot product.",
  "translatedText": "Ad esempio, potresti avere due vettori che puntano generalmente nella stessa direzione, con un prodotto scalare positivo, che vengono allontanati l'uno dall'altro durante la trasformazione, in modo tale da avere poi un prodotto scalare negativo. ",
  "n_reviews": 0,
  "start": 240.8,
  "end": 251.52
 },
 {
  "input": "Likewise things that start off perpendicular with dot product 0, like the two basis vectors, quite often don't stay perpendicular to each other after the transformation, that is they don't preserve that 0 dot product.",
  "translatedText": "Allo stesso modo, se le cose iniziano perpendicolari, con prodotto scalare zero, come i due vettori di base, non c'è garanzia che rimarranno perpendicolari dopo la trasformazione, preservando quel prodotto scalare zero. ",
  "n_reviews": 0,
  "start": 252.22,
  "end": 263.48
 },
 {
  "input": "And looking at the example I just showed dot products certainly aren't preserved,",
  "translatedText": "Nell'esempio che stavamo guardando, i prodotti scalari non vengono certamente preservati. ",
  "n_reviews": 0,
  "start": 264.1,
  "end": 267.16
 },
 {
  "input": "they tend to get bigger since most vectors are getting stretched",
  "translatedText": "Tendono a diventare più grandi poiché la maggior parte dei vettori viene allungata. ",
  "n_reviews": 0,
  "start": 267.5,
  "end": 269.94
 },
 {
  "input": "out. In fact, worthwhile side note here, transformations which do preserve dot products are special enough to have their own name, orthonormal transformations.",
  "translatedText": "In effetti, le trasformazioni che preservano i prodotti scalari sono abbastanza speciali da avere un nome proprio: trasformazioni ortonormali. ",
  "n_reviews": 0,
  "start": 269.94,
  "end": 279.1
 },
 {
  "input": "These are the ones that leave all of the basis vectors perpendicular to each other and still with unit lengths.",
  "translatedText": "Questi sono quelli che lasciano tutti i vettori base perpendicolari tra loro con lunghezze unitarie. ",
  "n_reviews": 0,
  "start": 279.72,
  "end": 284.66
 },
 {
  "input": "You often think of these as the rotation matrices,",
  "translatedText": "Spesso si pensa a queste come a matrici di rotazione. ",
  "n_reviews": 0,
  "start": 285.74,
  "end": 287.88
 },
 {
  "input": "they correspond to rigid motion with no stretching or squishing or morphing.",
  "translatedText": "Corrispondono al movimento rigido, senza allungamento, schiacciamento o morphing. ",
  "n_reviews": 0,
  "start": 288.42,
  "end": 292.2
 },
 {
  "input": "Solving a linear system with an orthonormal matrix is actually super easy. Because dot products are preserved, taking the dot product between the output vector and all the columns of your matrix will be the same as taking the dot product between the mystery input vector and all of the basis vectors, which is the same as just finding the coordinates of that mystery input.",
  "translatedText": "Risolvere un sistema lineare con una matrice ortonormale è molto semplice: poiché i prodotti scalari vengono preservati, prendere il prodotto scalare tra il vettore di output e tutte le colonne della matrice sarà come prendere i prodotti scalari tra il vettore di input e tutte le basi vettori, che equivale a trovare le coordinate del vettore di input. ",
  "n_reviews": 0,
  "start": 293.0,
  "end": 312.98
 },
 {
  "input": "So in that very special case, x would be the dot product of the first column with the output vector, and y would be the dot product of the second column with the output vector.",
  "translatedText": "Quindi, in quel caso molto speciale, x sarebbe il prodotto scalare della prima colonna con il vettore di output e y sarebbe il prodotto scalare della seconda colonna con il vettore di output. ",
  "n_reviews": 0,
  "start": 313.68,
  "end": 323.76
 },
 {
  "input": "Why am I bringing this up when this idea breaks down for almost all linear systems? Well, it points us in a direction of something to look for. Is there an alternate geometric understanding for the coordinates of our input vector that remains unchanged after the transformation?",
  "translatedText": "Ora, anche se questa idea non funziona per la maggior parte dei sistemi lineari, ci indica la direzione di qualcosa da cercare: esiste una comprensione geometrica alternativa per le coordinate del nostro vettore di input che rimane invariata dopo la trasformazione? ",
  "n_reviews": 0,
  "start": 326.82,
  "end": 341.66
 },
 {
  "input": "If your mind has been mulling over determinants, you might think of the following clever idea. Take the parallelogram defined by the first basis vector i-hat and the mystery input vector xy.",
  "translatedText": "Se la tua mente ha riflettuto sui determinanti, potresti pensare a questa idea intelligente: prendi il parallelogramma definito dal primo vettore base, i-hat, e dal misterioso vettore di input [x; y]. ",
  "n_reviews": 0,
  "start": 342.36,
  "end": 353.76
 },
 {
  "input": "The area of this parallelogram is the base, 1, times the height perpendicular to that base, which is the y-coordinate of that input vector.",
  "translatedText": "L'area di questo parallelogramma è la sua base, 1, moltiplicata per l'altezza perpendicolare a quella base, che è la coordinata y del nostro vettore di input. ",
  "n_reviews": 0,
  "start": 354.44,
  "end": 362.96
 },
 {
  "input": "So the area of that parallelogram is a sort of screwy roundabout way to describe the vector's y-coordinate. It's a wacky way to talk about coordinates, but run with me.",
  "translatedText": "Quindi, l'area di questo parallelogramma è una specie di modo intricato per descrivere la coordinata y del vettore; è un modo stravagante di parlare di coordinate, ma corri con me. ",
  "n_reviews": 0,
  "start": 363.68,
  "end": 373.14
 },
 {
  "input": "And actually, to be a little more accurate, you should think of this as the signed area of that parallelogram, in the sense described in the determinant video.",
  "translatedText": "In realtà, per essere più precisi, dovresti pensare all'area con segno di questo parallelogramma, nel senso descritto dal video determinante. ",
  "n_reviews": 0,
  "start": 373.7,
  "end": 381.64
 },
 {
  "input": "That way, a vector with a negative y-coordinate would correspond to a negative area for this parallelo",
  "translatedText": "In questo modo, un vettore con coordinata y negativa corrisponderebbe ad un'area negativa per questo parallelogramma. ",
  "n_reviews": 0,
  "start": 382.2,
  "end": 388.58
 },
 {
  "input": "gram, at least if you think of i-hat as in some sense being the first out of these two vecto",
  "translatedText": "Simmetricamente, se guardi il parallelogramma formato dal vettore e dal secondo vettore base, j-hat, la sua area sarà la coordinata x del vettore. ",
  "n_reviews": 0,
  "start": 388.96,
  "end": 392.96
 },
 {
  "input": "rs defining the parallelogram. And symmetrically, if you look at the parallelogram spanned by our mystery input",
  "translatedText": "Ancora una volta, è un modo strano di rappresentare la coordinata x, ma vedrai cosa ci offre tra un attimo. ",
  "n_reviews": 0,
  "start": 392.96,
  "end": 398.78
 },
 {
  "input": "vector and the second basis, j-hat, its area is going to be the x-coordinate of that mystery vector. Again, it's a strange way to represent the x-coordinate, but see what it buys us in a moment.",
  "translatedText": "Ecco come apparirebbe in tre dimensioni: normalmente il modo in cui potresti pensare a una delle coordinate di un vettore, diciamo la sua coordinata z, sarebbe quello di prendere il suo prodotto scalare con il terzo vettore di base standard, k-hat. ",
  "n_reviews": 0,
  "start": 398.78,
  "end": 410.08
 },
 {
  "input": "And just to make sure it's clear how this might generalize,",
  "translatedText": "Consideriamo invece il parallelepipedo che crea con gli altri due vettori base, i-hat e j-hat. ",
  "n_reviews": 0,
  "start": 410.68,
  "end": 412.64
 },
 {
  "input": "let's look in three dimensions. Ordinarily, the way you might think about one of a vector's c",
  "translatedText": "Se pensi al quadrato con area 1 attraversata da i-hat e j-hat come base di questo ragazzo, il suo volume è uguale alla sua altezza, che è la terza coordinata del nostro vettore. ",
  "n_reviews": 0,
  "start": 412.74,
  "end": 417.14
 },
 {
  "input": "oordinates, say its z-coordinate, would be to take its dot product with the third standard basis vector, often called k-hat. But an alternate geometric interpretation would be to consider the parallelepiped that it creates",
  "translatedText": "Allo stesso modo, il modo stravagante di pensare a qualsiasi altra coordinata di questo vettore è formare il parallelepipedo tra questo vettore e tutti i vettori base diversi da quello che stai cercando e ottenerne il volume. ",
  "n_reviews": 0,
  "start": 417.14,
  "end": 429.72
 },
 {
  "input": "with the other two basis vectors, i-hat and j-hat. If you think of the square with area 1 spanned by i-hat and j-hat as the base of this whole shape, then its volume is the same as its height, which is th",
  "translatedText": "O meglio, dovremmo parlare del volume con segno di questi parallelepipedi, nel senso descritto nel video delle determinanti, dove conta l'ordine in cui elenchi i tre vettori e usi la regola della mano destra. ",
  "n_reviews": 0,
  "start": 429.72,
  "end": 442.38
 },
 {
  "input": "e third coordinate of our vector. And likewise, the",
  "translatedText": "In questo modo le coordinate negative hanno ancora senso. ",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.24
 },
 {
  "input": "wacky way to think about the other coordinates of the vecto",
  "translatedText": "Ok, allora perché pensare alle coordinate come ad aree e volumi in questo modo? ",
  "n_reviews": 0,
  "start": 445.24,
  "end": 447.5
 },
 {
  "input": "r would be to form a parallelepiped using the",
  "translatedText": "Quando applichi una trasformazione della matrice, le aree dei parallelogrammi non rimangono le stesse, potrebbero essere ingrandite o ridotte. ",
  "n_reviews": 0,
  "start": 447.5,
  "end": 450.0
 },
 {
  "input": "vector and then all of the basis vectors other than the one correspon",
  "translatedText": "Ma (!), e questa è un’idea chiave dei determinanti, tutte queste aree vengono ridimensionate nella stessa misura. ",
  "n_reviews": 0,
  "start": 450.0,
  "end": 453.96
 },
 {
  "input": "ding to the direction you're looking for. Then the volume of this gives you the coordinate.",
  "translatedText": "Vale a dire, il determinante della nostra matrice di trasformazione. ",
  "n_reviews": 0,
  "start": 453.96,
  "end": 457.9
 },
 {
  "input": "Or rather, we should be talking about the signed volume of parallelepiped in the sense described in the determinant video using the right-hand rule. So the order in which you list these three vectors matters. That way, negative coordinates still make s",
  "translatedText": "Ad esempio, se guardi il parallelogramma attraversato dal vettore in cui si trova il tuo primo vettore base, che è la prima colonna della matrice, e la versione trasformata di [x; y], qual è la sua area? ",
  "n_reviews": 0,
  "start": 458.44,
  "end": 470.72
 },
 {
  "input": "ense. Okay, so why think of coordinates as areas and volumes like this? Well, as you apply some sort of matrix transformation,",
  "translatedText": "Bene, questa è la versione trasformata del parallelogramma che stavamo guardando prima, la cui area era la coordinata y del misterioso vettore di input. ",
  "n_reviews": 0,
  "start": 470.72,
  "end": 478.28
 },
 {
  "input": "the areas of these parallelograms, well, they don't stay t",
  "translatedText": "Quindi la sua area sarà la determinante della trasformazione moltiplicata per quel valore. ",
  "n_reviews": 0,
  "start": 478.96,
  "end": 481.96
 },
 {
  "input": "he same, they might get scaled up or down. But, and this is the key idea of determinants, all of the areas get scaled by the same amount, namely the determinant of our transformation ma",
  "translatedText": "Quindi, la coordinata y del nostro misterioso vettore di input è l'area di questo parallelogramma, compresa tra la prima colonna della matrice e il vettore di output, divisa per il determinante della trasformazione completa. ",
  "n_reviews": 0,
  "start": 481.96,
  "end": 492.16
 },
 {
  "input": "trix. For example, if you look at the par",
  "translatedText": "E come si ottiene quest'area? ",
  "n_reviews": 0,
  "start": 492.16,
  "end": 494.88
 },
 {
  "input": "allelogram spanned by the vector where your first basis vector lands, which is the first c",
  "translatedText": "Bene, conosciamo le coordinate di dove si ferma il misterioso vettore di input, questo è il punto centrale di un sistema lineare di equazioni. ",
  "n_reviews": 0,
  "start": 494.88,
  "end": 499.88
 },
 {
  "input": "olumn of the matrix, and the transformed version of xy, what is its area? Well, this is the transformed version of the parallelogram we were looking at earlier, the one whose area was the y-coordinate of the mystery input ve",
  "translatedText": "Quindi, crea una matrice la cui prima colonna è la stessa della nostra matrice e la cui seconda colonna è il vettore di output e prendi il suo determinante. ",
  "n_reviews": 0,
  "start": 499.88,
  "end": 512.88
 },
 {
  "input": "ctor. So its area is just going to be the determinant of the transformation multiplied by that y-coordinate. So that means we can solve for y",
  "translatedText": "Quindi guardalo; semplicemente utilizzando i dati dell'output della trasformazione, vale a dire le colonne della matrice e le coordinate del nostro vettore di output, possiamo recuperare la coordinata y del nostro misterioso vettore di input. ",
  "n_reviews": 0,
  "start": 512.88,
  "end": 522.1
 },
 {
  "input": "by taking the area",
  "translatedText": "Allo stesso modo, la stessa idea può darti la coordinata x. ",
  "n_reviews": 0,
  "start": 522.1,
  "end": 523.5
 },
 {
  "input": "of this new parallelogram in the output space divided by the determinant of the full transformation. And how do you get that area? Well, we know the coordinates for where the mystery input vector lands, that",
  "translatedText": "Guarda quel parallelogramma che abbiamo definito in precedenza che codifica la coordinata x del misterioso vettore di input, attraversato dal vettore di input e dal j-hat. ",
  "n_reviews": 0,
  "start": 523.5,
  "end": 537.14
 },
 {
  "input": "'s the whole point of a linear system of equations. So what you might do is create a new matrix whose first column is the same as that of our matrix, b",
  "translatedText": "La versione trasformata di questo tipo è estesa dal vettore di output e dalla seconda colonna della matrice, e la sua area sarà stata moltiplicata per il determinante della matrice. ",
  "n_reviews": 0,
  "start": 537.14,
  "end": 545.04
 },
 {
  "input": "ut whose second column is the output vector, and then you take its determinant. So look at that, just using data from the output of the transformation,",
  "translatedText": "Quindi la coordinata x del nostro misterioso vettore di input è quest'area divisa per il determinante della trasformazione. ",
  "n_reviews": 0,
  "start": 545.06,
  "end": 555.14
 },
 {
  "input": "namely the columns of the matrix and the coordinates of our output vector, we can recover the y-coordinate of the mystery input vector, which is halfway",
  "translatedText": "Simmetrico a quanto fatto prima, puoi calcolare l'area del parallelogramma di output creando una nuova matrice la cui prima colonna è il vettore di output e la cui seconda colonna è uguale alla matrice originale. ",
  "n_reviews": 0,
  "start": 555.6,
  "end": 563.42
 },
 {
  "input": "to solving the system. Likewise, the same idea can give us the x-coordinate. Look at the parallelogram we defined earlier, which encodes the x-coordinate of the mystery input vector s",
  "translatedText": "Quindi, ancora una volta, usando solo i dati dallo spazio di output, i numeri che vediamo nel nostro sistema lineare originale, possiamo recuperare la coordinata x del nostro misterioso vettore di input. ",
  "n_reviews": 0,
  "start": 563.42,
  "end": 573.42
 },
 {
  "input": "panned by that vector and j-hat. The transformed version of this guy is spanned by the output vector and the second column of the matrix, and its area will hav",
  "translatedText": "Questa formula per trovare le soluzioni di un sistema lineare di equazioni è nota come regola di Cramer. ",
  "n_reviews": 0,
  "start": 573.42,
  "end": 584.48
 },
 {
  "input": "e been multiplied by",
  "translatedText": "Ecco, solo per controllarci, inserisci i numeri qui. ",
  "n_reviews": 0,
  "start": 584.48,
  "end": 585.34
 },
 {
  "input": "the determinant of that matrix. So to solve for x, you can take this new area divided by the determinant of the full transformation.",
  "translatedText": "Il determinante della matrice alterata in alto è 4+2, che è 6, e il determinante inferiore è 2, quindi la coordinata x dovrebbe essere 3. ",
  "n_reviews": 0,
  "start": 585.34,
  "end": 592.94
 },
 {
  "input": "And similar to what we did before, you can compute the area of that output parallelogram by creating a new matrix whose first column is the output vector and whose second column is th",
  "translatedText": "E infatti, guardando indietro al vettore di input con cui abbiamo iniziato, la sua coordinata x è 3. ",
  "n_reviews": 0,
  "start": 593.86,
  "end": 604.34
 },
 {
  "input": "e same as the original matrix. So again, just using data fro",
  "translatedText": "Allo stesso modo, la regola di Cramer suggerisce che la coordinata y dovrebbe essere 4/2, o 2, e questa è infatti la coordinata y del vettore di input con cui abbiamo iniziato qui. ",
  "n_reviews": 0,
  "start": 604.34,
  "end": 607.72
 },
 {
  "input": "m the output space, the numbers we see in our original linear system, we can solve for what x must be. This formula for finding the solutions to a linear system of equations is known as Cramer's rule.",
  "translatedText": "Il caso delle tre dimensioni è simile e ti consiglio vivamente di fermarti a pensarci da solo. ",
  "n_reviews": 0,
  "start": 607.72,
  "end": 618.42
 },
 {
  "input": "Here, just to sanity check ourselves, plug in some numb",
  "translatedText": "Ecco, ti do un piccolo slancio. ",
  "n_reviews": 0,
  "start": 619.12,
  "end": 621.58
 },
 {
  "input": "ers here. The determinant of that top altered matrix is 4 plus 2, which is 6, and the bottom determinant is 2, so the x-coordinate should be 3. And indeed, looking back at the input vector",
  "translatedText": "Abbiamo questa trasformazione nota, data da una matrice 3x3, e un vettore di output noto, dato dal lato destro del nostro sistema lineare, e vogliamo sapere quale vettore di input si ferma su questo vettore di output. ",
  "n_reviews": 0,
  "start": 621.58,
  "end": 633.26
 },
 {
  "input": "we started with, the x-coordinate is 3. Likewise, Cramer's rule suggests that the y-coordinate should be 4 divided by 2, or 2, and that is in fact the y-coordinate of",
  "translatedText": "Se pensi, ad esempio, alla coordinata z del vettore di input come al volume di questo parallelepipedo attraversato da i-hat, j-hat e dal misterioso vettore di input, cosa succede al volume di questo parallelepipedo dopo la trasformazione? ",
  "n_reviews": 0,
  "start": 633.26,
  "end": 644.64
 },
 {
  "input": "the input vector we were starting with. The case with three dimensions or more is similar, and I highly recommend you take a m",
  "translatedText": "Come puoi calcolare quel nuovo volume? ",
  "n_reviews": 0,
  "start": 644.64,
  "end": 651.66
 },
 {
  "input": "oment to pause and think through it yourself. Here, I'll give you a little bit of momentum. What we have is a known transformation given by some 3x3 matrix and a known output vector given by the right side of our linear system, a",
  "translatedText": "Davvero, fermatevi e prendetevi un momento per pensare ai dettagli di generalizzare questo a dimensioni superiori; trovare un'espressione per ciascuna coordinata della soluzione a sistemi lineari più grandi. ",
  "n_reviews": 0,
  "start": 651.66,
  "end": 664.68
 },
 {
  "input": "nd we want to know what input lands on that output. And if you think of, say, the z-coordinate of that input vector as the volume of that special parallelepiped we were looking at earlier, spanned by i-hat, j-hat, and the mystery input vector, what happens to that volume after the transformation? And what are the various ways you can compute that volume? Really, pause and think through the details of generalizing this to higher dimensions, finding an expression for each coordinate of the solution to a larger linear system. Thinking through more general cases like this and convincing yourself that it works and why it works is where all the learning really happens, much more so than listening to some dude on YouTube walk you through the same reasoning again.",
  "translatedText": "Pensare a casi più generali e convincersi che funziona è il luogo in cui avverrà tutto l'apprendimento, molto più che ascoltare qualche tizio su YouTube che ripercorre il ragionamento. ",
  "n_reviews": 0,
  "start": 665.1,
  "end": 708.5
 }
]