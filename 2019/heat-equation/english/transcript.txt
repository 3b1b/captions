We last left off studying the heat equation in the one-dimensional case of a rod. 
The question is how the temperature distribution along such a rod will tend to change over time. 
This gave us a nice first example for a partial differential equation. 
It told us that the rate at which the temperature at a given point changes over time depends on the second derivative of that temperature at that point with respect to space. 
Where there's curvature in space, there's change in time. 
Here we're going to look at how to solve that equation. 
Actually, it's a little misleading to refer to all of this as solving an equation. 
The PDE itself only describes one out of three constraints that our temperature function must satisfy if it's going to accurately describe heat flow. 
It must also satisfy certain boundary conditions, which is something we'll talk about momentarily, and a certain initial condition, that is, you don't get to choose how it looks at time t equals zero. 
These added constraints are really where all of the challenge lies. 
There is a vast ocean of functions solving the PDE, in the sense that when you take their partial derivatives the thing is going to be equal, and a sizable subset of that ocean satisfies the right boundary conditions. 
When Joseph Fourier solved this problem in 1822, his key contribution was to gain control of this ocean, turning all of the right knobs and dials, so as to be able to select from it the particular solution fitting a given initial condition. 
We can think of his solution as being broken down into three fundamental observations. 
1. 
Certain sine waves offer a really simple solution to this equation. 
2. 
If you know multiple solutions, the sum of these functions is also a solution. 
3. 
Most surprisingly, any function can be expressed as a sum of sine waves. 
A pedantic mathematician might point out that there are some pathological exceptions, but basically any distribution you would come across in practice, including discontinuous ones, can be written as a sum of sine waves, potentially infinitely many. 
And if you've ever heard of Fourier series, you've at least heard of this last idea. 
And if so, maybe you've wondered, why on earth would anyone care about breaking down a function as a sum of sine waves? 
Well, in many applications, sine waves are nicer to deal with than anything else, and differential equations offer us a really nice context where you can see how that plays out. 
For our heat equation, when you write a function as a sum of these waves, the relatively clean second derivatives make it easy to solve the heat equation for each one of them, and as you'll see, a sum of solutions to this equation gives us another solution, and so in turn that will give us a recipe for solving the heat equation for any complicated distribution as an initial state. 
Here, let's dig into that first step. 
Why exactly would sine waves play nicely with the heat equation? 
To avoid messy constants, let's start simple and say that the temperature function at time t equals 0 is simply sine of x, where x describes the point on the rod. 
Yes, the idea of a rod's temperature just happening to look like sine of x, varying around whatever temperature our conventions arbitrarily label as 0, is clearly absurd, but in math you should always be happy to play with examples that are idealized, potentially well beyond the point of being realistic, because they can offer a good first step in the direction of something more general, and hence more realistic. 
The right-hand side of this heat equation asks about the second derivative of our function, how much our temperature distribution curves as you move along space. 
The derivative of sine of x is cosine of x, whose derivative in turn is negative sine of x. 
The amount the wave curves is, in a sense, equal and opposite to its height at each point. 
So at least at time t equals 0, this has the peculiar effect that each point changes its temperature at a rate proportional to the temperature of the point itself, with the same proportionality constant across all points. 
So after some tiny time step, everything scales down by the same factor, and after that, it's still the same sine curve shape, just scaled down a bit, so the same logic applies, and the next time step would scale it down uniformly again. 
This applies just as well in the limit, as the size of these time steps approaches 0. 
So unlike other temperature distributions, sine waves are peculiar in that they'll get scaled down uniformly, looking like some constant times sine of x for all times t. 
Now when you see that the rate at which some value changes is proportional to that value itself, your mind should burn with the thought of an exponential. 
And if it's not, or if you're a little rusty on the idea of taking derivatives of exponentials, or what makes the number e special, I'd recommend you take a look at this video. 
The upshot is that the derivative of e to some constant times t is equal to that constant times itself. 
If the rate at which your investment grows, for example, is always 0.05 times the total value, then its value over time is going to look like e to the 0.05 times t times whatever the initial investment was. 
If the rate at which the count of carbon-14 atoms in an old bone changes is always equal to some negative constant times that count itself, then over time that number will look approximately like e to that negative constant times t times whatever the initial count was. 
So when you look at our heat equation, and you know that for a sine wave, the right hand side is going to be negative alpha times the temperature function itself, hopefully it won't be too surprising to propose that the solution is to scale down by a factor of e to the negative alpha t. 
Here, go ahead and check the partial derivatives. 
The proposed function of x and t is sine of x times e to the negative alpha t. 
Taking the second partial derivative with respect to x, that e to the negative alpha t term looks like a constant, it doesn't have any x in it. 
So it just comes along for the ride, as if it was any other constant, like 2, and the first derivative with respect to x is cosine of x times e to the negative alpha t. 
Likewise, the second partial derivative with respect to x becomes negative sine of x times e to the negative alpha t. 
And on the flip side, if you look at the partial derivative with respect to t, that sine of x term now looks like a constant, since it doesn't have a t in it. 
So we get negative alpha times e to the negative alpha t times sine of x. 
So indeed, this function does make the partial differential equation true. 
And oh, if it was only that simple, this narrative flow could be so nice, we would just beeline directly to the delicious Fourier series conclusion. 
Sadly, nature is not so nice, knocking us off onto an annoying but highly necessary detour. 
Here's the thing, even if nature were to somehow produce a temperature distribution on this rod, which looks like this perfect sine wave, the exponential decay is not actually how it would evolve. 
Assuming that no heat flows in or out of the rod, here's what that evolution would actually look like. 
The points on the left are heated up a little at first, and those on the right are cooled down by their neighbors to the interior. 
In fact, let me give you an even simpler solution to the PDE which fails to describe actual heat flow, a straight line, that is, the temperature function will be some non-zero constant times x, and never change over time. 
The second partial derivative with respect to x is indeed zero, I mean there is no curvature, and its partial derivative with respect to time is also zero, since it never changes over time. 
And yet, if I throw this into the simulator, it does actually change over time, slowly approaching a uniform temperature at the mean value. 
What's going on here is that the simulation I'm using treats the two boundary points of the rod differently from how it treats all the others, which is a more accurate reflection of what would actually happen in nature. 
If you'll recall from the last video, the intuition for where that second derivative with respect to x actually came from was rooted in having each point tend towards the average value of its two neighbors on either side. 
But at the boundary, there is no neighbor to one side. 
If we went back to thinking of the discrete version, modeling only finitely many points on this rod, you could have each boundary point tend towards its one neighbor at a rate proportional to their difference. 
As we do this for higher and higher resolutions, notice how pretty much immediately after the clock starts, our distribution looks flat at either of those two boundary points. 
In fact, in the limiting case, as these finer and finer discretized setups approach a continuous curve, the slope of our curve at the boundary will be zero for all times after the start. 
One way this is often described is that the slope at any given point is proportional to the rate of heat flow at that point. 
So if you want to model the restriction that no heat flows into or out of the rod, the slope at either end will be zero. 
That's somewhat hand-wavy and incomplete, I know, so if you want the fuller details, I've left links and resources in the description. 
Taking the example of a straight line, whose slope at the boundary points is decidedly not zero, as soon as the clock starts, those boundary values will shift infinitesimally such that the slope there suddenly becomes zero and remains that way through the remainder of the evolution. 
In other words, finding a function satisfying the heat equation itself is not enough. 
It must also satisfy the property that it's flat at each of those endpoints for all times greater than zero. 
Phrased more precisely, the partial derivative with respect to x of our temperature function at 0T and at LT must be zero for all times T greater than zero, where L is the length of the rod. 
This is an example of a boundary condition, and pretty much any time you have to solve a partial differential equation in practice, there will also be some boundary condition hanging along for the ride, which demands just as much attention as the PDE itself. 
All of this may make it feel like we've gotten nowhere, but the function which is a sine wave in space and an exponential decay in time actually gets us quite close, we just need to tweak it a little bit so that it's flat at both endpoints. 
First off, notice that we could just as well use a cosine function instead of a sine. 
I mean, it's the same wave, it's just shifted in phase by a quarter of the period, which would make it flat at x equals zero, as we want. 
The second derivative of cosine of x is also negative one times itself, so for all the same reasons as before, the product cosine of x times e to the negative alpha t still satisfies the PDE. 
To make sure that it also satisfies the boundary condition on that right side, we're going to adjust the frequency of the wave. 
However, that will affect the second derivative, since higher frequency waves curve more sharply, and lower frequency ones curve more gently. 
Changing the frequency means introducing some constant, say omega, multiplied by the input of this function. 
A higher value of omega means the wave oscillates more quickly, since as you increase x, the input to the cosine increases more rapidly. 
Taking the derivative with respect to x, we still get negative sine, but the chain rule tells us to multiply that omega on the outside, and similarly the second derivative will still be negative cosine, but now with omega squared. 
This means that the right hand side of our equation has now picked up this omega squared term. 
So to balance things out, on the left hand side, the exponential decay part should have an additional omega squared term up top. 
Unpacking what that actually means should feel intuitive. 
For a temperature function filled with sharper curves, it decays more quickly towards an equilibrium, and evidently does this quadratically. 
For instance, doubling the frequency results in an exponential decay four times as fast. 
If the length of the rod is L, then the lowest frequency, where that rightmost point of the distribution will be flat, is when omega is equal to pi divided by L. 
You see that way, as x increases up to the value L, the input of our cosine expression goes up to pi, which is half the period of a cosine wave. 
Finding all the other frequencies which satisfy this boundary condition is sort of like finding harmonics, you essentially go through all the whole number multiples of this base frequency, pi over L. 
In fact, even multiplying it by zero works, since that gives us a constant function, which is indeed a valid solution, boundary condition and all. 
And with that, we're off the bumpy boundary condition detour and back onto the freeway. 
Moving forward, we're equipped with an infinite family of functions satisfying both the PDE and the pesky boundary condition. 
Things are definitely looking more intricate now, but it all stems from the one basic observation that a function which looks like a sine curve in space and an exponential decay in time fits this equation, relating second derivatives in space with first derivatives in time. 
And of course, your formulas should start to look more intricate, you're solving a genuinely hard problem. 
This actually makes for a pretty good stopping point, so let's call it an end here, and in the next video, we'll look at how to use this infinite family to construct a more general solution. 
To any of you worried about dwelling too much on a single example in a series that's meant to give a general overview of differential equations, it's worth emphasizing that many of the considerations which pop up here are frequent themes throughout the field. 
First off, the fact that we modeled the boundary with its own special rule, while the main differential equation only characterized the interior, is a very regular theme, and a pattern well worth getting used to, especially in the context of PDEs. 
Also, take note of how what we're doing is breaking down a general situation into simpler idealized cases. 
This strategy comes up all the time, and it's actually quite common for these simpler cases to look like some mixture of sine curves and exponentials that's not at all unique to the heat equation, and as time goes on, we're going to get a deeper feel for why that's true.