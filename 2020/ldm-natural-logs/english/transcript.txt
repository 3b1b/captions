Let's go ahead and open things up today with a question that might seem like it's incredibly unrelated to the title of the video. 
So if you guys go to 3b1b.co. 
Live, link is in the description. 
You've all been there for some of the warm-up animations. 
I want you to just make an estimate for the following question. 
It looks like a couple of you have already started coming in with answers here. 
Consider the numbers between 1 trillion and a trillion plus a thousand. 
Okay, so this range of a thousand integers that's pretty high. 
Which of the following would you guess is closest to the proportion of these numbers that are prime? 
So if you were to go through the painstaking process of, you know, looking at all of those numbers between a trillion and a trillion plus a thousand, considering which of them are prime, what do you guess is the relevant proportion there? 
So don't worry about getting it right or wrong. 
I'm mostly curious where people's intuitions are on this one. 
So while we don't necessarily have to do those painstaking calculations, I have gone ahead and written up a simple program that can do that for us. 
So if we hop on over to Python, it's not exactly the most sophisticated program for getting primes in the world, but it'll get the job done for us. 
So if I type something like get primes between 0 and 50, you know, we take a look. 
Okay, it's got 2, 3, 5, 7, all your favorite prime numbers between 0 and 50, you're gonna find in there. 
But maybe you don't like that range. 
Maybe you want to look between a thousand and a thousand fifty. 
And there you go. 
There's the primes between a thousand and a thousand fifty. 
And there's fewer of them. 
In general, as you get bigger and bigger, the primes get sparser and sparser, essentially because each one has more options for what it could factor into. 
You know, if you're just doing the guess and check on something like 143, you just have to check all the numbers up to around its square root. 
Whereas if you're checking around a trillion, you have to check on the order of, you know, something like a million numbers, all the primes up to a million, whether or not they go into it. 
So you might think that when we, let's say we define the list where we're going to get all of the primes, you know, get the primes between a trillion, which is 10 to the 12th, and a trillion plus a thousand. 
This involves a lot of number crunching to actually find those primes. 
And you can see by the fact that, you know, the program I wrote isn't going to be the most efficient thing in the world. 
It takes a little bit of time before it gets there. 
So we can look at that list in just a moment. 
But before I want to properly take a look at what everybody thinks. 
And we have a great split! 
Oh, I love questions that split up the audience like this. 
That's always fun. 
So what do you think? 
What do you think is the actual density of primes up there? 
Ooh, the correct answer is only third place. 
Usually in lockdown math, the correct answer is always at top. 
So this is exciting for me. 
It seems like most, or the largest number of people thought that D, one out of 250, that there would only be four primes in that range. 
After that, it was people who thought only one in a thousand, that up there, you know, primes are so rare they only come one in every thousand. 
The correct answer, the closest proportion here, would have been one in 25. 
So a lot more frequent than people might have thought. 
And what's interesting is actually if you ask this question to a mathematician, they would be able to pretty quickly tell you what the correct answer is, not because they're doing all that number crunching. 
And in fact, if you were to talk to a mathematician, they would look at this problem, they would do a small little calculation in their head and they'd say, the closest is one in 25, but really it's going to be closer to, you know, one in every 27 or 28. 
That would be a bit more accurate. 
You might wonder how they do that, because if we look at the number crunching that our computer had to do, it had to check all of the potential factors for all of the numbers we were looking at, and it does give us all of the primes between a trillion and a trillion plus a thousand. 
So as you can see, they're, they're sparser than they are for just the numbers between zero and a thousand. 
But there's a meaningful number of them. 
You know, we've got one trillion seven hundred fifty one, one trillion seven eighty seven. 
The Boeing engineers are probably pleased that that's in there. 
And the actual length of that list is 37. 
So there's 37 primes in there. 
So the proportion out of the thousand is 0.37. 
Or the way we were phrasing it, it was one out of something, so we might take the reciprocal of that and do a thousand divided by the number in there. 
And it's about one in every twenty-seven. 
But what the mathematician would do is they would say, I know this very cute fact about prime numbers, which is that the density of prime numbers near a given value, like a trillion, is around the natural log. 
So I'm going to type math dot log here, which takes a natural logarithm of that number. 
And we can, you know, check if you want, what does the documentation tell us. 
It says this returns the logarithm of X to a given base. 
If the base is not specified, it returns the natural logarithm, which is log base e of X. 
And just as a reminder, if anyone needs a little reminding on what logarithms mean, we talked about this all in the last lecture, so feel free to pop over. 
But when you have an expression like L in X, which is telling us the log with base e of X equals Y, that is saying the same thing as e to the power Y equals X. 
It's asking the question e to the what equals X. 
So for example, the natural log of 10, it's around 2.3. 
And that's a kind of useful value to know if you want to make conversions between exponentials written with e versus exponentials written with 10, but the natural log of 10 is around 2.3. 
So that's saying the same thing as e to the 2.3 equals 10. 
Now the cutesy fact here is that prime numbers, their density is actually kind of related to this natural logarithm. 
If we went over and we took the natural log of a trillion, which was the number defining our range at the lower end of that range, you'd see that it's 27. 
And that was about the ratio we were looking at before, right? 
A thousand divided by the length of our list of primes. 
I mean, that's quite close. 
And the fact that that happens at all is, I don't know, I might not call that my favorite piece of math because I use that term a lot, but it's definitely going to make the list of top five favorite pieces of math. 
That the log with base e has anything to do with prime numbers. 
So I want to show you another kind of cute fact associated with the relationship between natural logs and primes, because you might wonder why is it that we call this thing the natural logarithm? 
There's lots of logarithms, lots of different bases. 
What makes this one more natural? 
And I think the more often you see it show up in nature, the more that name starts to kind of make sense. 
So I'm going to play a game that will seem truly strange for a moment. 
I'm going to take an infinite series, for example one that's, you know, a favorite among a lot of people. 
If you take 1 over 1 plus 1 over 4 plus 1 over 9 plus 1 over 16, and in general you're always taking 1 over n squared, and you add up all of those numbers. 
If you keep adding and adding them, they'll stay below a certain bound and they'll actually approach a certain number. 
And it was this open question in Europe, it was posed in Basel by I think one of the Bernoullis for a while, like what is the number that this equals? 
And eventually Euler, genius of the day, was able to prove that it equals pi squared divided by 6, which is very beautiful, the idea that pi is at all related to just adding up the reciprocals of squares. 
But it gets crazier than that. 
I'm going to play this weird game that's going to kick out terms that don't look like prime numbers, keep the prime numbers, and then also scale down ones that are sufficiently prime similar for us. 
I'll show you what I mean. 
So 1 is not at all a prime. 
We're not even going to include it. 
2 is a prime, so I'm going to keep that 1 over 2 squared term. 
3 is a prime, so I'm going to keep the 1 over 3 squared term. 
4, it's not a prime. 
But I'm not going to kick it out because it is the power of a prime. 
So I'm going to say that 1 over 4 squared term can stay, but because you're only the square of a prime, I'm going to scale you down by a half. 
It's kind of a way of saying you look sort of like a prime, but because you're only a power of it, I'm scaling you down by that power. 
5 is a prime, so we keep the fifth term. 
6, not a prime, not a power of a prime. 
We don't like it at all. 
It gets kicked out. 
7, we keep it, and you can sort of see where I'm going here, but I'll write a couple more examples because it's such a bizarre way to manipulate a series. 
You would think that this isn't going to get us anything nice at all. 
8, it's a power of a prime, but because it's a cube, I'm going to scale it down by a factor of 3. 
So I'm going to take one third of the eighth term. 
9, it's the square of a prime, so I'll take one half of the ninth term. 
10 gets kicked out, we ignore it entirely. 
11 stays as is, and on and on. 
So each one of your terms looks like the power of a prime. 
1 over pk, and because this series, we're squaring them all, it's 1 over the power of that prime squared, but we scale it down by whatever that power is. 
Now, because we've manipulated this in a pretty chaotic way, I mean, the primes are distributed in a pretty random fashion, you might think that this is completely uncomputable. 
It's just a crazy situation. 
And it's going to be smaller, you know, for sure it's going to be smaller than the pi squared over 6, because we left out the 1, we left out a lot of composite numbers, and prime powers bigger than, with a power bigger than 1, we scaled down. 
So it's definitely a smaller number. 
You might be able to guess where this is going based on the title of the video. 
What it ends up equaling is the natural log of what it was before, of pi squared over 6. 
And that's not just true for this particular sequence of sums of squares. 
There's a number of other formulas that get us something related to prime, where we could, sorry, something related to pi, which is evidently related to primes, in a way that's, um, I mean, you play the same game and you have this weird fashion of taking logarithms, and not just any logarithm, the log base e. 
So just to talk through what I mean in this other context, if you take 1 minus a third plus a fifth minus a seventh plus a ninth, and kind of alternate back and forth between the odd numbers, you get pi divided by 4. 
I have a video all about this, Mathologer also has a video about it if you're curious. 
Very beautiful why it's true. 
But even stranger is when we play this game of keeping the primes and kicking out others. 
So 1 we're going to kick out. 
If we keep the third term, that's negative 3. 
Then the 5, we keep that. 
The 7, I guess it's a minus 1 seventh. 
The 9, we keep it but we scale it down, because it's the square of a prime. 
And in this case, because we're only looking at odd numbers, it looks like we're keeping quite a few of them. 
We have to get bigger before we kick them out. 
So like 15, that gets kicked out. 
17 is positive, it gets to stay as is. 
19 gets to stay as is. 
21 gets kicked out. 
23 gets to stay. 
So a very bizarre thing. 
I mean now the pluses and minuses, they don't alternate nicely. 
It's like minus plus, minus plus, minus plus plus, minus minus. 
It's a, it almost seems like a random sequence. 
25, we can scale that down because it's 5 squared. 
1 over 25. 
And this doesn't equal pi fourths anymore, but it does equal the natural logarithm of pi fourths. 
So evidently there's this relationship between taking logarithms with base e, which is to say, answering the question e to the what equals a value, and these sort of prime patterns. 
And it's quite beautiful when you think about it, because if you look at like this entire sequence up here, it's going to equal some number such that e to that number is related to pi. 
So you have this formula that's interrelating all of the prime numbers, and their powers for that matter, excluding the composite, so it's still a cool sequence, it's not just like all the integers. 
We're taking e, raising e to the power of that sum, and you get something related to pi. 
I mean, clearly better than Euler's formula. 
I think you'd have to agree. 
So, we might at the end of the lesson get to why that's true. 
It'll depend on how much time there is. 
But the next two facts I'll show you, we will definitely explain why they're true. 
So we're going to play a couple more games with series here. 
These ones are going to be a little bit simpler facts to think about than the first two that I showed. 
But the relation with primes is just, oh man, if that doesn't make you love math, I don't know what would. 
But if we played an alternating game that doesn't go through all the odd numbers, but it goes through every number. 
So I'm going to take 1 minus a half plus a third minus a fourth, on and on. 
And you might visualize this with a number line, where I'm going between 0 and 1 here. 
When we take 1 minus a half, you're hopping backwards by a half. 
And then plus a third, you're hopping forward by a smaller amount. 
Minus a fourth, you hop backwards by a still smaller amount. 
Then plus a fifth, and then backwards might get you here, and then forward might get you something like that. 
And you're alternating back and forth, and because each one is smaller than the last, you can probably see that it has to zero in on some kind of value. 
And as a matter of fact, it does. 
It zeroes in on a value that's around 0.69 or so, and more precisely, it comes in on the natural log of 2. 
The answer to the question, e to what is equal to 2. 
Kind of strange, don't you think? 
That something like e would show up with relation to just odd numbers oscillating back and forth like this. 
And there's another relation that natural logarithms have to a sequence that looks like this. 
You might ask, what happens if we don't alternate back and forth, but we add them all together? 
What does that approach? 
Because in the same way that, you know, when we added all the squares, the sum of the reciprocals of the squares, that approached a beautiful thing, it was pi squared over 6, you might think an even more natural question to ask is, don't square them at all. 
Take 1 plus a half plus a third plus a fourth, on and on, what does this approach? 
Now as it happens, it actually, it doesn't approach anything. 
No matter how small a number you choose, this sequence is eventually going to get bigger than it. 
So I could say, keep adding terms and eventually you'll get bigger than 100. 
If you have some patience, keep adding terms and eventually you'll get bigger than a million. 
And that's kind of surprising, because each one of these numbers is getting smaller and smaller, so as you're making your additions, you would think it's going to slow down, it won't get you past something like 100. 
But I can explain to you why this is going to happen, it's actually a very pretty proof. 
If I group my terms appropriately, so I'm going to group a third and a fourth together, I'm going to group all the numbers between a fifth and an eighth, all of the numbers between a ninth and a sixteenth, all of the numbers between 1 over 17 and 1 over 32. 
So into these groups that grow in size by powers of 2, what I can say is that a third plus a fourth, well both of those numbers are bigger than a fourth. 
A third is bigger than a fourth, and, well a fourth isn't bigger, but it's exactly equal to, but that does mean that their sum, yeah their sum is definitely going to be bigger than 1 fourth times 1 fourth, which is the same as taking 2 times a fourth. 
Similarly, this sum here, 1 fifth plus 1 sixth plus 1 seventh plus 1 eighth, each one of those terms is bigger than an eighth. 
All four of those terms are bigger than 1 eighth. 
So the group of them together is bigger than 4 eighths. 
Similarly over here, all of the numbers between a ninth and a sixteenth, all eight of those numbers are bigger than 1 and 16, so the sum all together is bigger than 8 times 1 over 16. 
And you might see where I'm going with this, you know, here I have 16 numbers that are all bigger than 1 and 30, excuse me, bigger than 1 in 32, talking while writing, and of course all of these are just equal to one half, so this 2 fourths is the same as a half, 4 eighths is the same as a half, 8 sixteenths, that's a half. 
So in other words, what I can do is group all of my terms so that the sum instead looks like taking 1 plus a half plus a half plus a half on and on forever. 
And that you can see, okay, if I keep going sufficiently long, it is going to get bigger. 
And it also gives you a little instinct that this might actually be related to logarithms, because the size of our groupings grow according to powers of 2. 
So if you were wondering, how long do I have to go before the sum gets bigger than 10, you might have the instinct that, hmm, I'm going to have to add together, let's see, I have 1 and then the rest of them are halves, so I'm going to have to add together 18 different groups that each look like a half, so I might have to get up to the point where the size of my group is like 2 to the 17th or 2 to the 18th, something like that. 
And you would be spot on that it grows, well, it doesn't grow exponentially, it grows logarithmically, because if you're asking how far do you need to go in order to get to that point, it would be logarithmic. 
And as you might guess, it's actually the natural logarithm. 
So if I add up all of the terms to about 1 over n, it ends up being approximately the natural log of n. 
And if you want to get even more accurate, it's the natural log of n plus a certain constant, this is a constant we'll talk about later on in the lesson, but just for an order of approximation, this gives you the idea that you need to get up to around the natural log of n. 
So I'm going to go ahead and pull up the quiz and ask you another question, just to see if you've been paying attention so far. 
So our question asks, which of the following is closest to the smallest value of n for which the sum 1 plus a half plus a third plus a fourth on a 9 you keep adding until you get 1 over n. 
How long do you have to go until that sum is bigger than a million? 
So remember, your first instinct here might have been that this entire sum converges to something, in the same way that when you add the reciprocals of squares, it converges to pi squared divided by 6, very beautiful. 
You might have thought that you keep going to this and it converges. 
In fact, what happens is it'll always get larger, but you can be more quantitative than that, and you can ask, how long does it take before this gets larger than a million? 
So I'll give you a little bit of time to think through what the answer to that will be, and I will say, because we're kind of converting between E-related things and base 10-related things, if you wanted a little reminder, I can pull up the fact that the natural log of 10 is around 2.3, if you wanted to use that for estimation purposes. 
So just to see if you were paying attention to the result that I just described of this thing growing like a natural logarithm, let's see how you answer. 
And I'll give another 20 seconds or so here. 
Alright, even if you're not necessarily finished, I'll go ahead and see where people are on this one, lock in the answers, and then explain where it comes from. 
So, the correct answer is that it's around 10 to the 400,000 or so, which is a huge, huge number. 
The estimated number of atoms in the universe is around 10 to the 80th, so it would be as if each atom in the universe had a universe inside of it, that would get us to 10 to the 160, then each one of those had a universe inside it, that only gets us to 10 to the 240. 
You'd have to keep iterating like that over and over, and it would even, even that crazy idea, like mental thought for how you can get up to a big number, would take forever to get you to something of the size 10 to the 400,000. 
Alright, now the way that you would think about something like this is to take a look at what I just said. 
When we add up all these numbers up to the point 1 over n, it's about the natural log of n. 
So what you're looking for is the value when the natural log of n is approximately a million. 
That's how long you have to go before the sum gets bigger than a million. 
This is the same statement as saying that n is about e to the power of a million. 
Okay, but of all of our answers, we're expressed in terms of powers of 10, so in order to make the conversion, I would think to myself, 10 to the what is equal to e? 
That way I'm going to be able to make a little substitution in here with the power of 10. 
Well, this is asking me what is the log base 10 of e? 
And from properties of logarithms, like what we learned last time, this is the same as asking log base e of 10, but we're taking the reciprocal of that. 
Okay, and another way you could think about that is that e to the 1 over x equals 10. 
That these two expressions are the same, so we're looking for the natural log of 10, but we take 1 over it. 
Now our estimation for the natural log of 10, if you happen to know, it's around 2.3. 
All you would really need to know for this one is that it's roughly 2, or even that it's on the order of 1, because all the exponents in our options were looking very different. 
So if you're asking what's 1 divided by 2.3, I mean very roughly, it's like a half. 
So we could think of n as being very, very roughly something that's like 10 to the 1 half to the 1 million, just to get us something kind of close. 
So that looks like it's 10 to the 500,000, and we know that that 1 half really should be a little bit smaller, because we're taking 1 divided by 2.3, not 1 divided by 2, so the number should be something a little smaller than 500,000. 
And indeed, of all the options here, there's one that's much closer to 10 to the 500,000 than anything else, so our very rough approximation would get us there. 
Alright, so that's pretty fun. 
Now, to start explaining where on earth some of these things are coming from, like why is the natural log present in these circumstances, I want to take a moment to start talking about E and the role that E plays in math in a way that I think sometimes it can be a little bit misunderstood. 
So, just to start off, I'm going to pick a number from the audience, so in your own time, feel free to go to 3b1b.co and enter whatever your favorite number is. 
But the thing I want to talk about to begin here is how... 
Ooh, lots of answers coming in. 
That's always fun. 
When you see a family of functions, okay, so let's say we see something that looks like... 
I'll pop on over here... 
Something that looks like e to the r times x for various different values of r. 
This is something you see all the time in engineering, in math, in physics. 
We describe a bunch of different exponentials with some kind of parameter, like r sitting there. 
And we say depending on what r is, this can give us a shallower exponential growth, something that grows exponentially but a little bit more slowly, versus a steep exponential growth. 
Okay? 
Once you're writing things in terms of a family, I think a lot of people have this instinct that these are all of the functions that e produces, like e the number is producing this beautiful family of functions. 
But it's important to realize this is the same statement as creating a family of functions that just look like a to the x with various different bases, where it could tweak what that value of a is and say, you know, sometimes it looks like 2 to the power x, sometimes it looks like 3 to the power x, or 4 to the power x. 
Tweaking that base gets us various different exponentials. 
That's actually playing the same game. 
For any one that you have here, like, you know, the function 3 to the x, taking powers of 3, I can choose an r such that e to the r x equals that. 
And in fact, there's nothing special about e. 
I could have chosen as the base here something like pi. 
I could say look at the family of exponentials that look like pi to the r times x. 
It's not that pi is producing these numbers or pi is related to this family in a particular way. 
It's a choice that we're making to write it that way. 
And almost always, the choice that we make in physics and engineering and math all over the place to write families of exponentials is with e. 
So the right question to ask isn't what does e have to do with such a family, but why is it the right choice? 
I'll give you another example of where this can come up. 
I mean, they're all over the place, things that look like exponential curves. 
But one that's very important for probability and statistics is the bell curve. 
So it's something that we almost always write in the form e to the negative x squared. 
And a way you might think about this, by the way, is if we take just e to the x, we get this thing that grows and it kind of decays as you go to the left. 
And if we made it negative, it would decay as you go to the right. 
So whenever the input to e to the x is getting very negative, it decays. 
So to make it decay on both sides, you could take e to the negative x squared. 
And then it's decaying on both sides and you get this nice bell curve. 
And because of that square, it sort of smooths things out, whereas if we had taken something like the absolute value of x but negated it, okay, then it decays on both sides, but we get this awkward cusp. 
That doesn't explain why this very specific curve comes up in statistics. 
But if you ever kind of want to remember, oh, what was the formula for a bell curve again, you can kind of think through the fact that this should have roughly that shape. 
And quite often it comes with some kind of parameters, though. 
For example, I could put in something, maybe a value I'll call s in there, that will determine how wide and skinny this bell curve is, something like a standard deviation in the context of statistics. 
S wouldn't be that standard deviation. 
We would have to reciprocate it and square it and do some things. 
But the idea that when you tweak what's in that exponent, it changes the bell curve. 
That's the only point I want to make here. 
You can think, just looking at this, that somehow bell curves are produced by the number e. 
But that's not exactly true, because I could also write a to the negative x squared, and I get the same family of curves. 
As I tweak the value of a, I'm also changing what that width is, so I could come up with other ways of describing the standard deviation of this in terms of a. 
And it's the same family of curves. 
It's not just that they look similar. 
They are, in fact, the same thing. 
And this is not too hard to show algebraically. 
It almost makes it look a little bit more deceptively simple than it really is. 
So we've got a lot of answers for what people wanted to enter as an example number to go with, so let's go ahead and see. 
It seems like the most popular answer, by a little margin above i, is 69, which I assume is because if you take all of the natural numbers between 1 and 9, and then you look at the divisors for each one of those, you look at the numbers, you list all their divisors, and you add up the divisors, it adds up to 69. 
And adding up divisors like this is a very fun and common thing in number theory, so I assume that's why people chose it. 
But the point here is that if you see some kind of function like, let's see, how should I write it? 
69 to the power x. 
I could also write that. 
I could write the same thing as e to the power of the natural log of 69. 
Okay, I've written that kind of sloppily. 
Let me do it again. 
e to the natural log of 69. 
That's the same thing as the number 69, right? 
Because it's saying e to the what equals 69, but then I've taken e to that, so I should get 69 back. 
All of that to the power x. 
And by the rules of exponentials, this is the same thing as e to just some constant, whatever the natural log of 69 is, times x. 
So I could replace that with a constant, which happens to be around 4.234, as any mathematician will be able to tell you, well-known constant of nature, the natural log of 69. 
And the point here is that this just looks like e to some constant times x. 
So you might wonder, why do we make this choice, right? 
Because it didn't have to be pi. 
I could write that same function, sorry, it didn't have to be e. 
I could write that same function as pi raised to a special power, namely the log base pi. 
Man, I'm writing quite sloppily here. 
Log base pi of 69 times x, that would be the same function. 
We could describe everything with a base of pi if we wanted to. 
And to just give one more example of where I think this, even though it's a, like it's a simple conversion if you know logarithms, that anything that looks like a to the x can be expressed as e to something times that, I think it's clearly not very appreciated that you can make that conversion, because any time that we talk about imaginary exponentials, and I get that it's kind of a weird thing to talk about e to the i times t, once someone learns, you know, and I've made a number of videos in this series about it, Mathologer has videos, lots of people have videos about it, the idea is that e to the imaginary constant times some value t walks you around the unit circle, and in fact it walks you a distance of t radians. 
And the importance of this, the way that it comes up in, for example, electrical engineering, is it gives you this oscillating pattern. 
As you scale up t, you have something that's oscillating, and it gives a very nice way to describe sine waves and cosine waves and signals that oscillate. 
And there's actually one electrical engineer that I used to know, and he would always say, oh I love the number e. 
What I like about e is it's the number that spins. 
That's really what makes e special, I realized this. 
e is the number that spins. 
But the problem is that's not, that's not exactly true. 
It is true that e to the i t spins around, but that's not special to e. 
I could also take 2 to the i times t, and that will also produce values that walk around a circle. 
And we can think through more exactly, 2 is the same thing as e to the natural log of 2, so 2 to the i t is the same as e to the natural log of 2 times t, all of that times the imaginary number i. 
Which basically means it's doing the same thing as e to the i t, it's just rescaling time. 
It's walking a little bit more slowly. 
And then similarly, if you were to take something like your favorite number to the i times t, that would look like e to the approximately 4.234 times t, all times the imaginary number. 
Which just means you're walking around at a different rate. 
So e is not the number that spins. 
The idea of complex exponents walking around a circle doesn't have to do with e per se, it really just has to do with a lot of what we talked about I think in lectures 4 and 5. 
And I'll go over it again in just a moment here as a quick reminder. 
So the thing that's special about e, the reason that we always choose to write things in this way, has to do with rates of change. 
That when you take the derivative of e to the power t, this is the same thing as itself. 
Which is to say if we were to graph this, you know maybe e to the t represents an amount of money you have over time or something like that. 
And we were to look above a value of t, the slope of this graph, the rate at which it's increasing, is actually equal to its own height. 
So the farther you are along the curve, meaning you have a greater height, the steeper it is. 
So the more money you have, the faster it grows. 
This is the power of compound growth. 
But that's actually true of any exponential. 
What makes e special is that they're exactly the same, it's not just that they grow in proportion with each other. 
So if we were to write a family of exponential curves as e to the r times t, as opposed to writing it as a to the power t, and thinking of changing that value a, the value in doing that is that taking the derivative by the chain rule, we take the derivative of the inside, which looks like r, and we multiply by the derivative of the outside, which is e to the rt. 
And if anybody here doesn't know calculus by the way, we're about to start doing a fair amount of it, I have a whole series on it that you can pop over and take a look at, lots of other places on YouTube and such to give a quick primer. 
But if you're coming in and you're not familiar with calculus, like just be warned that that's where we're about to start going. 
Because if you want to understand natural logarithms, and by extension the number e, the importance that they have has everything to do with rates of change and the inverse of that operation, as you'll see. 
So anyway, why would it be nice to express a function like this? 
Well what it's telling you, let's say this was something like the size of your investment. 
This is an expression saying how much money you have at a given point in time. 
If you want to know the rate of change of that, how much is it changing per unit time, it's proportional to itself, and r gives you that proportionality constant. 
If r was 0.01, it's telling you that the rate of growth is 10% of the size of the thing itself. 
So the choice that we make to write things this way is, it's basically a way to make all the constants involved more readable. 
So if you're to look at these statistics associated with bell curves, and the way that we actually tend to write things, the pattern ends up looking something like 1 divided by s squared, and sometimes that x instead of writing it as x. 
Strange parentheses. 
I might say x minus m for some value of m. 
Both of these terms end up having really nice readable meanings, where m, this isn't specific to the e fact, but it's just a common thing you'll see, m gives you the mean of the distribution, where this pile is, and s gives you the standard deviation. 
And when we choose to write this family with e, it's giving those constants readable meanings. 
And a similar thing happens with how we describe complex exponentials. 
When we choose to write the idea of walking around a circle with e, it gives a very readable meaning to what this term t is. 
It's saying, what is the distance that you've walked along the unit circle? 
And you can actually understand this with derivatives pretty well, where if we say, what is the derivative, what is the rate of change of some value that looks like e to the i times t, by the chain rule, this is going to look like i times itself, e to the i times t. 
Now what would that actually mean? 
That means that if you're sitting at some kind of number, if this is your current value for e to the i times t, the rate of change is i multiplied by that value, which is a 90 degree rotation of this vector. 
Maybe I would draw it like this. 
This right here would give you your rate of change. 
So you might move that over and consider it a velocity vector. 
So this is kind of like your velocity vector, this is kind of like your position vector, which I might write something like an s. 
So what this whole expression of e to the i times t is essentially saying is, whatever this does, if it's somewhere in the complex plane, at every given point, the rate of change of my vector is a 90 degree rotation of itself. 
And so that's why we're walking around the circle at a speed of one unit per second, because the length of the position vector is one, so the length of the velocity vector is one. 
And it's also why, if you were to look at two to the i t, it walks around at a different rate. 
Because there, the constant is not just i, a 90 degree rotation, it's i times natural log of two. 
i times something would mean that this, where are we, this operation here is not just a 90 degree rotation, it's a 90 degree rotation and a scaling. 
So your velocity vector would end up looking a little bit shorter, and you'd be walking around the unit circle more slowly. 
So that's kind of the important thing to understand about e, the fact that it's a choice that we're making to write families of exponentials this way, but because it is its own derivative, that ends up making these things play much more nicely. 
Now, this lets us take derivatives of anything else if you wanted. 
If you did describe your money's rate of growth with a to the t, to take its derivative, you could first do a conversion, write the whole thing as e to the natural log of a times t, and the reason you would is then when we're, I sort of squished my font here, then when you're taking the derivative of that, the derivative of the inside is the natural log of a, and then that's multiplied by itself, e to the natural log of a times t, which you could then spell out even further, convert it back into a to the t. 
So if you did describe all of your investments as a to the power t, which kind of feels more natural to a lot of people, that oh, you might say rather than e to some investment rate times t, just think of 1.05 to the t, and that describes something like 5% growth. 
If you were thinking of that growth in a continuous sense, not year over year, what's the new percentage, but moment by moment, what's the rate of growth, you would have to say the rate of growth is the natural log of that base, which just feels a little bit more awkward. 
You could do it, but it would feel more awkward. 
Now, all of this leaves open the question of why? 
Why on earth is the derivative of e to the t equal to itself? 
It's this very nice property, so you might wonder where this thing comes from. 
And it really has everything to do with how you define the number e. 
And this can be a little bit frustrating, where in some contexts you'll see people say, what is the number e? 
Well, it's the number defined such that this derivative equals itself. 
And then other contexts, you might find e defined in a different way that is very conducive to whatever the circumstance there is. 
So you might wonder, okay, can we come at this a little bit more directly, and try to understand derivatives of exponentials, and see why the special value 2.718 would fit into it. 
And to do that, let me draw myself a new graph here. 
Let's say I have some kind of exponential. 
And if I want to understand the rate of change, the slope tangent to that point x, the way we often think about it is think of two nearby points, so another one that might be x plus a little constant times h. 
And then we're going to look at the slope between those two points and consider what happens as h goes towards zero. 
So if this whole graph was a function a to the x, if we wanted to give a very direct look at what might the derivative of this expression be, we can make an attempt to calculate it ourselves without depending on a pre-established fact handed down from on high that e to the t is, or I guess in this case e to the x is its own derivative, and then manipulate based on natural logs and such. 
So what does this look like if you try to come at it directly? 
Well what you would say is that the change in the height of the graph divided by the change in the width, the sort of rise over run, dy dx, looks like the difference in the outputs at those two values, so the output at the high value, which is x plus h, minus the value at the low value, a to the x, all of that divided by the step in the x direction, which is just of size h. 
And the fact that we are doing calculus here, that we have the little d's, that is a signal to us that we don't just want this ratio for a particular value of h, we want to consider what that ratio change in y, change in x looks like as the change in x goes to zero. 
And here I'm writing that change in x as h, so it's a limit as h goes to zero of this expression. 
And from here you can try to manipulate it a little bit and see what you might find. 
The first step, take advantage of the exponential properties to write this as a to the x times a to the h. 
And what's nice about that is it lets us factor out an a to the x, because it shows up both in the first term and the second, so I could write this whole thing as a limit of a to the x outside of a to the h minus one all over h. 
And this was the limit as h goes to zero. 
Well, x has nothing to do with the h here, so we're allowed to pull out the a to the x term itself. 
As far as h is concerned, it's just some constant rescaling the thing, and the limit of a constant times a thing is that constant times the limit of the thing. 
A times, or a to the x times the limit as h goes to zero of a to the h minus one over h. 
And at this point, we're a little bit stuck. 
We've found a very interesting fact, which is that any kind of exponential, e or whatever you want, base pi to the x, two to the x, 69 to the x, those have derivatives that are proportional to themselves, but we want to understand this proportionality constant. 
And I could ask you to guess, just to see if you can get a feel for it in the context of one particular example. 
So let's say that I'm choosing a base of something like two, and I want to understand rates of change of two to the x. 
Our question asks us, the limit below, I guess it tells us, it tells us a little bit about what it is. 
The limit below is a number between zero and one. 
So this is, we're looking at two to a small value minus one divided by that same small value. 
Don't worry about calculating it exactly, I'm just kind of curious if you guessed. 
Enter some kind of guess for what this value is, and then round it to two decimal places so that we can have some consistency. 
So we'll give you a moment to just think of what it might be, but don't think too hard if you don't want to. 
It's totally okay to get this one wrong, we just want to see what people think. 
Looks like we've got a couple things coming in from the audience here, which is always fun. 
So Robert points out that in French the notation reads, logarithm l'imperillain, and is wondering why this word is used. 
I asked this on Twitter the other day, evidently it's in reference to a person, I think John Napier, and so it's in reference to him. 
And then there was a truly terrible French pun about an exponential and a logarithm walk into a bar and they order a beer and who pays? 
And the answer is that the exponential has to pay because la logarithm n'imperillain, which anyone who speaks French will like groan and laugh at, but that made me laugh a little bit. 
Do I have a personal vendetta against e? 
Yeah, yeah I do, I think it's an overrated constant. 
I think it's beautiful, but I think it's beautiful in ways that aren't what people think they are. 
And I also think that, I'm going to talk about this in a moment, we should write, we shouldn't write the exponential function as e to the x, because when it's more general that doesn't make sense and I think it confuses people. 
We should just write it as what it is, which is a certain polynomial, and just be honest up front rather than letting e, like e has nothing to do with e to the pi i, that's a frustrating fact. 
It shouldn't be in there. 
Anyway, German here is normal for how you do maths on a ruled paper instead of graph paper. 
I mean graph paper is definitely nicer, but I don't know, this was the paper that I just had on hand. 
And in general if you want to make any comments or questions about the lesson, you can do so on Twitter with the hashtag locked down math, and those will be pulled up as we go. 
So it seems like we have strong consensus on our guess here, which is people guessing that the correct answer to this limit is that it's around 0.69, which I assume the reason everybody guessed that is because it's the correct answer, that this limit does in fact approach around 0.69, and we could play with Python if we wanted to see that experimentally. 
Python's kind of overkill here, you could do it with any calculator, but if I raise 2 to some small power, I get some kind of number, and if I subtract 1 from that, then I get a small number, and if I divide it by the same small power, so here I have three zeros and a one, it looks like we get around 0.6931. 
And if I made it a smaller value that I was doing, it seems to stay pretty stable around there, it's around 0.69314. 
So congratulations to the majority of you who had the right guess here. 
And in fact it's no coincidence that that's what it is, because like I said earlier, if you're taking the derivative, where have I written it? 
I've written it somewhere. 
Sloppily as I want to do, I've written that if you take the derivative of something that looks like a to the t, the constant sitting in front is the natural log of a. 
So for something like 2, you would be looking at the natural log of 2, which is in fact around 0.69. 
Now, all of that was dependent on the fact that e to the x is its own derivative, right? 
So there's one avenue that you could take here, if you want to come up with a definition of e. 
What you could say, and this is totally valid, is the number e is defined to be the constant such that this limit is 1. 
If that's the case, then e to the x is its own derivative, by definition, pretty much. 
And then from there, you will get the fact that anything else, its derivative can be expressed in terms of the log base e of itself. 
That's one way that you could go. 
Another avenue that you could take is to say that when we write e to the x, this is actually shorthand for a certain polynomial. 
I'm partial to this because I think this is an honest representation of the role that it plays more generally, like when we start talking about complex numbers. 
It's weird to me that in high school I saw Euler's formula as the polar representation for complex numbers before it was ever really explained that e to the x does not refer to the repeated multiplication, that it's a shorthand for this long polynomial. 
You might give it another name, like exp, right? 
And then that's something that it makes sense to plug in complex numbers to. 
Traditionally, the way that you see this series in high school is you might go through a calculus class where you learn about e to the x being its own derivative, and then maybe at the end of a second calculus class, the fact that it is its own derivative, in conjunction with a very wonderful topic called Taylor series, right? 
So it being its own derivative and Taylor series proves that e to the x must equal this long polynomial, which is absolutely the case, right? 
If you have a function that is its own derivative and at the value zero it equals one, you will find that it has to equal this polynomial. 
An alternate approach that you could take if you wanted in setting the foundations is to say, don't worry about Taylor series, start with this sequence as a primitive object, and then something we talked about a couple lectures ago was, because of a nice property that this function has, which is basically that when you add the inputs, basically this polynomial behaves like an exponential, and you can prove that just from the polynomial itself without calculus or anything, exp of a plus b equals exp of a times exp of b. 
And it's a very pleasing exercise to kind of work out the expansion and see that that works. 
And the fact that that works, we talked about this a couple lectures ago, implies that the whole sequence looks like whatever exp of one is raised to the x. 
So what you could say is the number e is defined to be this particular sequence evaluated at x equals one. 
And if you go that direction, that's all well and good, and it becomes a kind of substantive thing to talk about e to the x being its own derivative. 
And it's one of the most pleasing exercises that you'll ever do, because we can take a look at this, and if you know how to take derivatives of polynomial terms, well let's just work it out actually, I'll turn over a new leaf so that it can be nice and cleanly seen. 
It's really one of the most pleasing, I don't know, times that you'll have in a calculus class. 
If you're just sitting, you're looking at this particular infinite polynomial, and you're saying I wonder what the derivative of this happens to be. 
And all you need to know is the power rule for polynomial terms, and you'll say that the derivative, let me take d dx, well the derivative of a constant ends up being zero, the derivative of x is one, the derivative of x squared over two, you know you might think of that two as kind of hopping down in front and leaving one less than itself, so it becomes two times x to the one, just x to the one over two, and those twos cancel, so we're adding x. 
x cubed over three factorial, I might write this out as x cubed over three times two times one. 
This ends up being three times x squared, you know the exponent kind of hopped down and left behind one minus itself, over three times two times one, the threes cancel, so we can see that that's actually the same as x squared over two factorial. 
And in general, each one of our terms, as the exponent hops down, it cancels out one of the things from the factorials below it, and what we get is the exact same sequence but shifted, which is quite nice. 
And like I said, the traditional way that you see this series is that you're using the fact that e to the x is its own derivative, in conjunction with Taylor series to show that it must equal this, but if you start with this as a primitive, and you say this is the thing that defines a special function for which we use the shorthand, e to the x, then it feels a little bit more contentful and quite fun to say that e to the x ends up being its own derivative. 
And like we showed earlier, that then lets you take the derivative of all sorts of other things, which in turn explains why we adopt the convention of writing all of our exponentials as e to something times t, as opposed to writing them all as a to something times t, even though those are equivalent and often weirdly hard to appreciate. 
So, with all of that said, we can turn ourselves back in the direction of natural logarithms, because let's say I wanted to know the derivative of the natural log. 
You might wonder why I want to know that, but if I have a deeper relationship with the natural log of x, in terms not just of how it relates to these series, but in all facets of math, maybe we can then start drawing connections. 
And if you build up that relationship by knowing things like its derivative, it actually helps you come back and understand things like the alternating series we were looking at before. 
So, can we use the fact that e to the x is its own derivative to figure out the slope of a natural log curve? 
Well, what that slope is asking us is to look at a given input x, we consider a tiny step dx to the right, look at the corresponding step dy up, and we want to understand the ratio, dy over dx. 
Now, at this point, which has some kind of output y, what we can say is by definition, y is the natural log of x. 
Now, this is the same statement as saying e, have I written this right? 
y is natural, yeah, great. 
So, this is the same as saying e to the y is equal to x. 
Now, from there, I can understand the relationship between tiny nudges to x and tiny nudges to y by taking derivatives. 
If I ask about some tiny nudge to the value x and the corresponding tiny nudge to e to the y, well, what it means for e to the x, or in this case, e to the y to be its own derivative, is that the size of that tiny nudge is e to whatever the y value at that point is, times dy. 
And we're saying that that equals dx. 
And what this lets us do then is express the slope that we want, dy over dx. 
If we just rearrange things, it looks like 1 divided by e to the y. 
So, what this is saying is that if we look at our graph, it's got some x coordinate, some y coordinate, and I want to know what the slope is, this change to y over change in x. 
I can't immediately express it in terms of x, maybe, but I do know whatever this value of y is, if I take e to the power of that and then reciprocate, that gives me the slope. 
But of course, what it means to be on our graph is that y is the natural log of x, which is the same as saying e to the y equals x, so this whole thing is the same as taking 1 divided by x. 
So if I want to know that slope, I can say, what is your x coordinate, take 1 divided by that, and that gets me the slope of the natural log. 
Which is, we've just gone through a process called implicit differentiation. 
If you're not inclined to believe that this manipulation is legitimate, that we can just move around the dx's and dy's like that, I have a whole video about implicit differentiation in the calculus series that you can take a look at. 
But the point for us is that we have a very nice fact, that the derivative of ln of x looks like 1 divided by x. 
And that's quite nice, and it kind of passes a gut check that ln of x gets shallower and shallower as you go on, which means the slope gets smaller and smaller. 
And the graph of 1 over x, you know, what does that look like? 
Well, at the input, let's say we have the input 1 somewhere like here, it'll be at 1. 
At the input 2, it'll be sitting at a half. 
At the input 3, it'll be sitting at a third. 
And in general, it gets lower and lower and closer to 0. 
So the idea that this would describe the slope of that, you know, something that gets lower and lower closer to 0, seems to pass a little bit of a sanity check. 
Now, the relevance that this is going to have to us will involve the inverse operation to differentiation. 
So instead of talking about what is the slope of the natural log curve, what I might do is ask about the area under this particular curve. 
Let's say I take the area up to... 
my stomach was just rumbling, I don't know if that's audible on the microphone. 
Clearly, gotta eat lunch before these things. 
So let's say I want to understand the area up to n of something like this. 
What that involves is taking the integral between 1 and our value n of 1 divided by x by dx. 
Now this actually looks quite similar in spirit, the idea of adding up a bunch of things that look like 1 over x, to what we were looking at earlier. 
How much earlier? 
I guess over here. 
Where we were adding up 1 plus a half plus a third plus a fourth, on and on. 
And already it gives a little bit of an intuitive instinct for why something like this sum would be related to natural logs. 
Because we now know that in calculus land, natural logs are intimately related to the idea of 1 divided by x. 
But I want you to think of this spelled out a little bit more exactly, and so we'll pop on over to our quiz one more time. 
Second to last question for today. 
And the question asks us, we're going to let s be the sum from n equals 1 up to capital N of 1 divided by n. 
That's s. 
And then we're going to let i be an analogous integral, where we're integrating dx over x between 1 and n. 
And it asks you to compare s and i. 
I'll give you a moment to think about that. 
Interestingly, we don't have a ton of consensus around this one. 
So there's only three options, and we've got a nice split. 
And as you guys know, this is actually one of my favorite things when we do any of these lockdown live quizzes, is when it's not everyone hopping on to one particular thing, but we have a division among folks. 
And I think that's great. 
I'm curious actually what the answer is going to be here. 
And in fact, even if it's not been enough time to thoroughly think through, I'm going to go ahead and grade it, just so that we can see what it happens to be. 
And a lot of the spirit of it is that you kind of hazard a guess, so feel no shame if you entered an answer and it's not what turns out to be correct. 
So in this case, the sum does in fact end up being bigger than the integral. 
And it looks like 900 of you got that correct, which is awesome. 
And then following that was people thinking that it was less. 
And then to those in B thinking that they were identical, you know, that's a reasonable thought, because they're such similar expressions. 
But there's a picture that really makes the answer kind of shine out to us here, which is, if I look at the curve 1 over x, which is what this white curve is, it's 1 over x, and then I'm going to consider a bunch of bars, each of whose area corresponds to 1 over n for some value of n. 
So for example, for the value 1, this bar has a width of 1, and then the height is 1, and that means that right above the input 1 on its upper left corner, it's hitting the graph. 
Now for the next term, if I want 1 over 2, that means it's going to hit the graph above the input 2, since the graph is defined to be 1 over x, so its upper left corner hits that, and then the area of this bar whose height is 1 half is, well, 1 half, because its width is 1. 
Similarly, this bar has an area of 1 third, this bar has an area of 1 fourth, and so what you have is a sequence of rectangles whose total area is going to be similar to the area under the curve, definitely similar, but you can tell that it's going to be bigger, because some of the area is leaking out. 
In this context, we've got a lot of area leaking out from the first bar, a little bit less leaking out from the second, and on and on, but as you go, because the graph flattens out, it becomes quite a good approximation once you account for the area that's leaked out there. 
Now something kind of bizarre is happening here, where usually we think of these rectangles as being something like a Riemann sum that defines integration, where we say, oh, we don't know what the area under a curve is, but we like areas of rectangles, so we use the rectangles to approximate the curve. 
Here, we're going to do something that's backwards to that. 
If we know calculus, we do know the area under the curve. 
It's very nice. 
It involves the antiderivative of 1 over x, like we'll show in a moment. 
What we don't know is the sum of the areas of the rectangles. 
That was the sum that we were looking at earlier and trying to understand. 
So here we're going backwards and using the area under a curve to approximate the area of a bunch of rectangles, which I think is fun. 
It shows that calculus has this back and forth. 
It's not just geometry informing understanding of curves, but it's an understanding of curves informing an understanding of geometry and number theory and things of that sort. 
So what this means for us is that if we take a look back at our paper and we look at my much more sloppily drawn graph than the beautiful exact illustrations can give us, if we want to understand that area, taking this integral, the task is to do an inverse derivative, to ask what function has a derivative that equals the inside here. 
If that's something you haven't learned about, again, calculus series. 
Take a look at the fundamental theorem of calculus video, or even the first video in that series I think shows a little of an instinct for why you have this relationship between slopes and areas. 
But what it means for us is that we take the inverse derivative, which we now know is the natural log, the thing whose derivative is 1 over x is the natural log, and we evaluate it at the bounds, at n and 1. 
And this notation where I kind of put brackets around it and then a number in the upper right corner and lower right corner means I take that expression evaluated at the top minus that expression evaluated at the bottom. 
And that, well, natural log of 1, what is that? 
e to the what equals 1? 
Well, it's 0. 
Pretty much anything to the 0 will equal 1. 
So this term goes away entirely, and what we're left with is the natural log of n. 
And what this means for us is if we were using our rectangles to approximate the sum, or using the integral to approximate those rectangles, it's saying that 1 over 1 plus 1 over 2 plus 1 over 3, on and on, up to a given bound is about equal to the natural log of n. 
And more specifically, if you were to account for how much area is leaking out here, that area actually does converge. 
As n tends towards infinity, the area that's leaked out approaches a certain constant, and it's called Euler's constant, or the Euler-Macheroni constant, and it happens to be around 0.577. 
So in the same way that pi and e are constants of nature, this is another constant of nature, also bearing Euler's name. 
And what it describes is the deviation between this sum, often called the harmonic sum, and the natural log of x, a thing that is related to e. 
So Euler has really got his fingerprints all over the situation, at least as far as naming is concerned in our little expression here. 
So that's quite nice. 
That's quite fun. 
But that only answers one of the mysteries that we had earlier. 
Because if you remember, I opened this whole thing up by talking not just about this series that grows like the natural log, we also alternated it. 
We went 1 minus a half plus a third minus a fourth, and the claim is that that was the natural log of 2. 
So let's see if we can try to understand why that's true. 
And I might actually postpone explaining the even more bizarre fact that this interrelates with primes in a certain way, depending on how long I want this particular stream to go. 
But at least finish off by understanding the alternating series, because it's extremely satisfying. 
So to do that, let me just rewrite what our series looks like. 
And this is one of those things where, as I go through the answer, it has a feeling of magic. 
And sometimes not in a great way. 
You might find yourself looking at how we go about this and asking, how on earth would anyone ever come up with that? 
And maybe after we plop it all down, we can try to introspect and think about the reasonable ways that someone would come up with the following line of reasoning. 
But it is not unique to this situation, it's kind of a useful set of tricks to be familiar with. 
And there's a couple general principles in there. 
The first general principle is that if we have a hard question, in this case figuring out what this sum approaches, bizarrely it can become easier if we make it more general. 
You might think that making things more general would make it harder, because you have to answer a more powerful fact. 
But math does this bizarre thing, where sometimes by trying to make it more general, you actually make the problem more tractable. 
Which is quite cool, actually, because what it means is when a mathematician is motivated only by making their own life easier, it has the strange effect of making their results applicable to a wider variety of circumstances. 
So the way I'm going to generalize this, again, it might look kind of bizarre and unmotivated, but run with me for a second, is rather than thinking of a single value, I'm going to put an x in here and consider this a function where I'm taking x over 1 minus x squared over 2 plus x cubed over 3, on and on and on. 
And I want to know, in general, what does this approach for various values of x, and then I just have to plug in the value x equals 1. 
Now, like I said, that might make it seem harder, infinitely harder. 
Previously we just had to know one value, now you're asking me to compute infinitely many values? 
But if you know calculus, you might recognize that the exponents of your polynomial terms might just play nicely with the denominators here. 
And in particular, if we were to take the derivative of this series, it behaves quite nicely. 
The derivative of x is 1, the derivative of x squared over 2, well that 2 hops down and cancels out the denominator, so it becomes negative x. 
Similarly, that 3 hops down and cancels the denominator, so it becomes x squared. 
And while you might not know why we're taking a derivative of something here, and how that would be helpful for actually evaluating the ultimate sum that we care about, it is an interesting fact, and it's something that is playful and fun, that we've somehow simplified the expression by taking its derivative. 
And the simplification is actually quite important, because there's a well-known fact within math that you can take a series where each term is the product of the last with a constant kind of product. 
So here, as we go from one term to the next, we're always multiplying by negative x, so to go from negative x to x squared, you multiply by negative x, and then similarly x squared to negative x cubed, you're multiplying by negative x. 
And when that's the case, the series as a whole is going to approach 1 divided by, or wherever you start, but here we started at 1, so the thing you started at divided by 1 minus the thing that you're constantly multiplying by, which is negative x. 
So to give another example of where this comes up, is if we were to take something like 1 plus 1 half plus 1 fourth, where each time in our sequence we are multiplying the last term by 1 half, this will equal 1 divided by 1 minus the thing we were multiplying by, which was 1 half. 
And 1 divided by 1 minus 1 half ends up being the same as 2. 
And that actually kind of feels intuitive, that if we take 1 plus 1 half plus 1 fourth plus 1 eighth, you could even draw out a picture, where let's say I've got a rectangle whose side length is 1, and 1 here, I can say the 1 represents this area, and then half represents this area, and then a fourth represents this area, and an eighth represents that area, and kind of keep playing this game, and eventually it'll fill an area of 2. 
Now the more general version of that is this geometric sum, which someone who's done a lot of problem solving in math is able to recognize kind of quickly, which is why they might enjoy this series much more than they would enjoy the one above it. 
So this whole thing ends up looking like 1 divided by 1 plus x. 
Great. 
But what this suggests is that if we somehow take an antiderivative, if we somehow integrate this, we might have an alternate expression for what the initial sequence was. 
So from here, I'm going to go ahead and pose a quiz, and part of this quiz is seeing who in the audience is comfortable with calculus, and again if you're not, calculus series, go and check it out. 
But what we have here is the question, what is the integral from 0 up to 1 of 1 divided by 1 plus x, dx? 
I want you to evaluate that integral, and I'll give you a little moment for this one. 
So, let's see. 
And you know, tell you what, while answers are rolling in, before locking it in, I'm going to go ahead and just start describing the answer here. 
So, if you want to know the integral from 0 up to 1 of 1 divided by 1 plus x, dx, well we know that the antiderivative of 1 over x is the natural log of x, so it's going to be the natural log of that inside divided by the derivative of the inside. 
That's kind of the inverse chain rule, or something you can get with u-substitution. 
But the derivative of the inside is just 1, so you can check yourself that if you take the derivative of this, you get 1 over the inside, 1 over 1 plus x, but then the chain rule just has you multiplying by 1, so it stays the same. 
So then we evaluate this at the bounds, 1 and 0, and what this ends up getting us is the natural log at the top, which is 1 plus 1, minus the natural log of 1 plus x at the bottom, which was 1 plus 0. 
The natural log of 1 plus 1 is of course ln of 2, and then we're subtracting off the natural log of 1, which is 0. 
So, the proper answer here comes out to be the natural log of 2. 
And it looks like 1600 of you have correctly answered that, so well done, well done indeed. 
And if you wanted to kind of visualize this in your head or get some sort of gut instinct on which of those answers seemed loosely correct, even if you didn't know how to calculate it immediately, the graph of 1 over 1 plus x is going to look just like the graph of 1 over x, but shifted to the left, so it's actually going to pass through the input 0, 1, and then we're looking for the area under here. 
So you know that it's going to be an area somewhere between 0 and 1, probably filling up more than a half of it, and the natural log of 2 is around 0.69, so that actually seems about right. 
But what's quite cool here is that if we say that the, well I can just write it out again here, if I say I want to integrate this bottom expression from 0 up to 1, which is the same as integrating 1 over 1 plus x from 0 up to 1, I know that that should be the value 2, ln of 2. 
Okay? 
But on the other hand, if I picture taking my antiderivative and getting this whole sequence here, and evaluating it between 0 and 1, what I'm doing is I'm plugging in the number 1, which gets me my alternating sequence, then I'm subtracting off the value of 0, which when I plug it in just gets 0, so evaluating this at 0 and 1 is the same thing as integrating the bottom expression from 0 to 1, which is the same thing as integrating 1 over 1 plus x from 0 to 1, which is the same thing as ln of 2, and hence that whole top thing ends up being ln of 2. 
Very clever! 
Just such a sneaky sequence of manipulations that, like I said, if you just see it plopped down, almost as intimidating because you wonder, how on earth do you go from seeing this sequence up here to thinking about, ah yes, if I generalize it with lots of powers of x and then take the derivatives of those and then use a geometric series sum and then integrate that using natural log, yes, then it'll all become obvious. 
And I think the answer is that that's probably not what the problem solving process looks like. 
Instead, you build up a familiarity with a lot of these things, like derivatives of polynomial terms, and then you build up familiarity with other things, like derivatives of natural logs. 
And the more familiarity you have with a lot of different pieces of math, then sometimes when you see one particular pattern, you're able to draw in your mind a connection to things that make you look like a genius. 
I think Euler did this all the time. 
If you look at some of the great discoveries of Euler, they just come really out of nowhere. 
I mean the very opening thing that I talked about, I guess it wasn't the very opening, but early on, where are we? 
I seem to have... 
The sum of the reciprocals of squares, which, somewhere in my notebook here, we were just looking at the pages. 
Maybe I tore it out. 
Oh, yeah, there we are. 
Things can become a little bit of a mess over here. 
But if we look at this long sum where we're taking 1 over 1 squared, 1 over 2 squared, and equals pi squared over 6, the way that Euler found this, I mean, it involves this very strange thing where you start looking at an infinite product associated with sine of pi times x. 
And if you think of it as starting with this sum and then dreaming up out of your head an infinite product associated with sine of pi times x, or it might have been cotangent of pi times x or something like that, it really does seem like it came out of nowhere. 
But in general, I think you build up these bits of familiarity with different pieces, and then once you see something, like Euler seeing that Basel problem sum or someone seeing this alternating harmonic sum, you're able to use those connections to kind of make yourself look smarter than you actually are, which is fun.