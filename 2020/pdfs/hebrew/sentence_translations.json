[
 {
  "input": "Imagine you have a weighted coin, so the probability of flipping heads might not be 50-50 exactly.",
  "translatedText": "תאר לעצמך שיש לך מטבע משוקלל, כך שהסבירות להיפוך ראשים לא תהיה 50-50 בדיוק.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 2.8,
  "end": 8.68
 },
 {
  "input": "It could be 20%, or maybe 90%, or 0%, or 31.41592%.",
  "translatedText": "זה יכול להיות 20%, או אולי 90%, או 0%, או 31.41592%.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.14,
  "end": 18.48
 },
 {
  "input": "The point is that you just don't know.",
  "translatedText": "הנקודה היא שאתה פשוט לא יודע.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.48,
  "end": 20.2
 },
 {
  "input": "But imagine that you flip this coin 10 different times, and 7 of those times it comes up heads.",
  "translatedText": "אבל תאר לעצמך שאתה מטיל את המטבע הזה 10 פעמים שונות, ו-7 מהפעמים האלה הוא עולה בראש.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.78,
  "end": 25.58
 },
 {
  "input": "Do you think that the underlying weight of this coin is such that each flip has a 70% chance of coming up heads?",
  "translatedText": "האם אתה חושב שהמשקל הבסיסי של המטבע הזה הוא כזה שלכל סיבוב יש סיכוי של 70% לעלות?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.58,
  "end": 32.02
 },
 {
  "input": "If I were to ask you, hey, what's the probability that the true probability of flipping heads is 0.7, what would you say?",
  "translatedText": "אם הייתי שואל אותך, היי, מה ההסתברות שההסתברות האמיתית להעיף ראשים היא 0.7, מה היית אומר?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 32.76,
  "end": 39.62
 },
 {
  "input": "This is a pretty weird question, and for two reasons.",
  "translatedText": "זו שאלה די מוזרה ומשתי סיבות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 41.54,
  "end": 44.22
 },
 {
  "input": "First of all, it's asking about a probability of a probability, as in the value we don't know is itself some kind of long-run frequency for a random event, which frankly is hard to think about.",
  "translatedText": "קודם כל, זה שואל לגבי הסתברות של הסתברות, שכן הערך שאנחנו לא יודעים הוא בעצמו סוג של תדר ארוך טווח לאירוע אקראי, שלמען האמת קשה לחשוב עליו.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.7,
  "end": 55.72
 },
 {
  "input": "But the more pressing weirdness comes from asking about probabilities in the setting of continuous values.",
  "translatedText": "אבל המוזרות הדוחקת יותר נובעת משאלה על הסתברויות בקביעת ערכים מתמשכים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.28,
  "end": 61.28
 },
 {
  "input": "Let's give this unknown probability of flipping heads some kind of name, like h.",
  "translatedText": "בוא ניתן להסתברות הלא ידועה הזו להעיף ראשים איזה שם, כמו ח.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 62.54,
  "end": 66.78
 },
 {
  "input": "Keep in mind that h could be any real number from 0 up to 1, ranging from a coin that always flips tails up to one that always flips heads and everything in between.",
  "translatedText": "זכור ש-h יכול להיות כל מספר ממשי מ-0 עד 1, החל ממטבע שתמיד הופך זנבות למעלה ועד אחד שתמיד הופך ראשים וכל מה שביניהם.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 67.54,
  "end": 77.32
 },
 {
  "input": "So if I ask, hey, what's the probability that h is precisely 0.7, as opposed to, say, 0.7000001, or any other nearby value, well, there's going to be a strong possibility for paradox if we're not careful.",
  "translatedText": "אז אם אני שואל, היי, מה ההסתברות ש-h הוא בדיוק 0.7, בניגוד למשל, 0.7000001, או כל ערך קרוב אחר, ובכן, תהיה אפשרות חזקה לפרדוקס אם לא נזהר.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 78.72,
  "end": 94.16
 },
 {
  "input": "It feels like no matter how small the answer to this question, it just wouldn't be small enough.",
  "translatedText": "זה מרגיש כאילו לא משנה כמה קטנה התשובה לשאלה הזו, היא פשוט לא תהיה קטנה מספיק.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 94.86,
  "end": 99.16
 },
 {
  "input": "If every specific value within some range, all uncountably infinitely many of them, has a non-zero probability, well, even if that probability was minuscule, adding them all up to get the total probability of any one of these values will blow up to infinity.",
  "translatedText": "אם לכל ערך ספציפי בטווח כלשהו, כולם אין ספור אינסוף רבים מהם, יש הסתברות שאינה אפס, ובכן, גם אם ההסתברות הזו הייתה זעירה, חיבור של כולם כדי לקבל את ההסתברות הכוללת של כל אחד מהערכים הללו יתפוצץ ל- אינסוף.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 99.94,
  "end": 114.26
 },
 {
  "input": "On the other hand though, if all of these probabilities are 0, aside from the fact that that now gives you no useful information about the coin, the total sum of those probabilities would be 0, when it should be 1.",
  "translatedText": "מצד שני, אם כל ההסתברויות הללו הן 0, מלבד העובדה שכעת זה לא נותן לך מידע שימושי על המטבע, הסכום הכולל של ההסתברויות הללו יהיה 0, כאשר הוא אמור להיות 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 115.86,
  "end": 127.66
 },
 {
  "input": "After all, this weight of the coin h is something, so the probability of it being any one of these values should add up to 1.",
  "translatedText": "אחרי הכל, המשקל הזה של המטבע h הוא משהו, אז ההסתברות שהוא אחד מהערכים האלה אמורה להסתכם ב-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 128.54,
  "end": 136.44
 },
 {
  "input": "So if these values can't all be non-zero, and they can't all be 0, what do you do?",
  "translatedText": "אז אם הערכים האלה לא יכולים להיות כולם לא-אפס, ולא כולם יכולים להיות 0, מה עושים?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 137.32,
  "end": 142.22
 },
 {
  "input": "Where we're going with this, by the way, is that I'd like to talk about the very practical question of using data to create meaningful answers to these sorts of probabilities of probabilities questions.",
  "translatedText": "לאן אנחנו הולכים עם זה, דרך אגב, אני רוצה לדבר על השאלה המעשית מאוד של שימוש בנתונים כדי ליצור תשובות משמעותיות לסוגים אלה של שאלות הסתברויות של הסתברויות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 144.8,
  "end": 154.6
 },
 {
  "input": "But for this video, let's take a moment to appreciate how to work with probabilities over continuous values, and resolve this apparent paradox.",
  "translatedText": "אבל עבור הסרטון הזה, בואו ניקח רגע כדי להעריך איך לעבוד עם הסתברויות על ערכים מתמשכים, ולפתור את הפרדוקס הנראה לעין הזה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.68,
  "end": 162.78
 },
 {
  "input": "The key is not to focus on individual values, but ranges of values.",
  "translatedText": "המפתח הוא לא להתמקד בערכים בודדים, אלא בטווחי ערכים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.32,
  "end": 173.96
 },
 {
  "input": "For example, we might make these buckets to represent the probability that h is between, say, 0.8 and 0.85.",
  "translatedText": "לדוגמה, אנו עשויים להפוך את הדליים האלה לייצג את ההסתברות ש-h הוא בין, נניח, 0.8 ל-0.85.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 174.62,
  "end": 182.16
 },
 {
  "input": "Also, and this is more important than it might seem, rather than thinking of the height of each of these bars as representing the probability, think of the area of each one as representing that probability.",
  "translatedText": "כמו כן, וזה חשוב יותר ממה שזה נראה, במקום לחשוב על הגובה של כל אחד מהפסים האלה כמייצג את ההסתברות, חשבו על השטח של כל אחד כמייצג את ההסתברות הזו.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.16,
  "end": 193.04
 },
 {
  "input": "Where exactly those areas come from is something that we'll answer later.",
  "translatedText": "מאיפה בדיוק מגיעים האזורים האלה זה משהו שנשיב עליו מאוחר יותר.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 193.96,
  "end": 197.48
 },
 {
  "input": "For right now, just know that in principle, there's some answer to the probability of h sitting inside one of these ranges.",
  "translatedText": "נכון לעכשיו, רק דעו שבאופן עקרוני, יש איזושהי תשובה להסתברות ש-h ישב בתוך אחד מהטווחים האלה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 197.96,
  "end": 204.14
 },
 {
  "input": "Our task right now is to take the answers to these very coarse-grained questions, and to get a more exact understanding of the distribution at the level of each individual input.",
  "translatedText": "המשימה שלנו כרגע היא לקבל את התשובות לשאלות המאוד גסות הללו, ולקבל הבנה מדויקת יותר של ההתפלגות ברמה של כל קלט בודד.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.96,
  "end": 214.56
 },
 {
  "input": "The natural thing to do would be consider finer and finer buckets.",
  "translatedText": "הדבר הטבעי לעשות יהיה לשקול דליים עדינים יותר ויותר.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 215.46,
  "end": 218.98
 },
 {
  "input": "And when you do, the smaller probability of falling into any one of them is accounted for in the thinner width of each of these bars, while the heights are going to stay roughly the same.",
  "translatedText": "וכשאתה עושה זאת, ההסתברות הקטנה יותר ליפול לתוך כל אחד מהם נלקחת בחשבון ברוחב הדק יותר של כל אחד מהברים הללו, בעוד שהגבהים יישארו בערך זהים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 219.5,
  "end": 228.92
 },
 {
  "input": "That's important, because it means that as you take this process to the limit, you approach some kind of smooth curve.",
  "translatedText": "זה חשוב, כי זה אומר שכאשר אתה לוקח את התהליך הזה עד הקצה, אתה מתקרב לאיזשהו עקומה חלקה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 229.66,
  "end": 235.22
 },
 {
  "input": "So even though all of the individual probabilities of falling into any one particular bucket will approach zero, the overall shape of the distribution is preserved, and even refined in this limit.",
  "translatedText": "אז למרות שכל ההסתברויות האישיות ליפול לתוך דלי מסוים אחד יתקרבו לאפס, הצורה הכללית של ההתפלגות נשמרת, ואפילו מעודנת בגבול זה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.9,
  "end": 247.22
 },
 {
  "input": "If, on the other hand, we had let the heights of the bars represent probabilities, everything would have gone to zero.",
  "translatedText": "אם היינו נותנים לגבהים של הפסים לייצג הסתברויות, הכל היה עובר ל-0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.7,
  "end": 254.9
 },
 {
  "input": "So in the limit, we would have just had a flat line giving no information about the overall shape of the distribution.",
  "translatedText": "אז בגבול, היה לנו רק קו שטוח שלא נותן מידע על הצורה הכללית של ההתפלגות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 260.04,
  "end": 265.64
 },
 {
  "input": "So, wonderful.",
  "translatedText": "כל כך מדהים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 267.42,
  "end": 268.14
 },
 {
  "input": "Letting area represent probability helps solve this problem.",
  "translatedText": "לתת לשטח לייצג הסתברות עוזר לפתור בעיה זו.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 268.44,
  "end": 271.26
 },
 {
  "input": "But let me ask you, if the y-axis no longer represents probability, what exactly are the units here?",
  "translatedText": "אבל הרשו לי לשאול אתכם, אם ציר ה-y כבר לא מייצג הסתברות, מהן בעצם היחידות כאן?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 271.9,
  "end": 277.14
 },
 {
  "input": "Since probability sits in the area of these bars, or width times height, the height represents a kind of probability per unit in the x-direction, what's known in the business as a probability density.",
  "translatedText": "מכיוון שההסתברות יושבת באזור הפסים הללו, או רוחב כפול גובה, הגובה מייצג סוג של הסתברות ליחידה בכיוון x, מה שמכונה בעסק צפיפות הסתברות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.8,
  "end": 289.64
 },
 {
  "input": "The other thing to keep in mind is that the total area of all these bars has to equal one at every level of the process.",
  "translatedText": "הדבר הנוסף שיש לזכור הוא שהשטח הכולל של כל הסורגים הללו צריך להיות שווה לכל רמה של התהליך.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 290.58,
  "end": 296.54
 },
 {
  "input": "That's something that has to be true for any valid probability distribution.",
  "translatedText": "זה משהו שצריך להיות נכון עבור כל התפלגות הסתברות חוקית.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 297.06,
  "end": 300.5
 },
 {
  "input": "The idea of probability density is actually really clever when you step back to think about it.",
  "translatedText": "הרעיון של צפיפות ההסתברות הוא למעשה ממש חכם כשאתה חוזר אחורה כדי לחשוב על זה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 301.98,
  "end": 306.3
 },
 {
  "input": "As you take things to the limit, even if there's all sorts of paradoxes associated with assigning a probability to each of these uncountably infinitely many values of h between 0 and 1, there's no problem if we associate a probability density to each one of them, giving what's known as a probability density function, or PDF for short.",
  "translatedText": "כשאתה לוקח את הדברים לקצה הגבול, גם אם יש כל מיני פרדוקסים הקשורים בהקצאת הסתברות לכל אחד מהערכים הרבים והאינסופיים האלה של h בין 0 ל-1, אין בעיה אם נשייך צפיפות הסתברות לכל אחד מהם, נותן את מה שמכונה פונקציית צפיפות הסתברות, או בקיצור PDF.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.3,
  "end": 325.64
 },
 {
  "input": "Anytime you see a PDF in the wild, the way to interpret it is that the probability of your random variable lying between two values equals the area under this curve between those values.",
  "translatedText": "בכל פעם שאתה רואה PDF בטבע, הדרך לפרש אותו היא שההסתברות שהמשתנה האקראי שלך נמצא בין שני ערכים שווה לשטח מתחת לעקומה הזו בין הערכים הללו.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 326.42,
  "end": 337.52
 },
 {
  "input": "So, for example, what's the probability of getting any one very specific number, like 0.7?",
  "translatedText": "אז, למשל, מה ההסתברות לקבל מספר מסוים מאוד, כמו 0.7?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.22,
  "end": 343.46
 },
 {
  "input": "Well, the area of an infinitely thin slice is 0, so it's 0.",
  "translatedText": "ובכן, השטח של פרוסה דקה לאין שיעור הוא 0, אז זה 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.22,
  "end": 348.34
 },
 {
  "input": "What's the probability of all of them put together?",
  "translatedText": "מה ההסתברות של כולם ביחד?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 348.9,
  "end": 351.14
 },
 {
  "input": "Well, the area under the full curve is 1.",
  "translatedText": "ובכן, השטח מתחת לעקומה המלאה הוא 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 351.78,
  "end": 353.96
 },
 {
  "input": "You see?",
  "translatedText": "אתה רואה?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 354.62,
  "end": 354.92
 },
 {
  "input": "Paradox sidestepped.",
  "translatedText": "הפרדוקס עקף.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 355.72,
  "end": 356.4
 },
 {
  "input": "And the way that it's been sidestepped is a bit subtle.",
  "translatedText": "והאופן שבו עקפו את זה היא קצת עדינה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 357.5,
  "end": 360.22
 },
 {
  "input": "In normal, finite settings, like rolling a die or drawing a card, the probability that a random value falls into a given collection of possibilities is simply the sum of the probabilities of being any one of them.",
  "translatedText": "בהגדרות רגילות וסופיות, כמו הטלת קובייה או משיכת קלף, ההסתברות שערך אקראי ייפול לאוסף נתון של אפשרויות היא פשוט סכום ההסתברויות להיות כל אחת מהן.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 360.22,
  "end": 372.96
 },
 {
  "input": "This feels very intuitive.",
  "translatedText": "זה מרגיש מאוד אינטואיטיבי.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 373.84,
  "end": 375.02
 },
 {
  "input": "It's even true in a countably infinite context.",
  "translatedText": "זה אפילו נכון בהקשר אינסופי שניתן לספור.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 375.24,
  "end": 377.6
 },
 {
  "input": "But to deal with the continuum, the rules themselves have shifted.",
  "translatedText": "אבל כדי להתמודד עם הרצף, הכללים עצמם השתנו.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 378.12,
  "end": 381.54
 },
 {
  "input": "The probability of falling into a range of values is no longer the sum of the probabilities of each individual value.",
  "translatedText": "ההסתברות ליפול לטווח של ערכים אינה עוד סכום ההסתברויות של כל ערך בודד.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.1,
  "end": 388.66
 },
 {
  "input": "Instead, probabilities associated with ranges are the fundamental primitive objects, and the only sense in which it's meaningful to talk about an individual value here is to think of it as a range of width 0.",
  "translatedText": "במקום זאת, הסתברויות הקשורות לטווחים הן האובייקטים הפרימיטיביים הבסיסיים, והמובן היחיד שבו יש משמעות לדבר על ערך אינדיבידואלי כאן הוא לחשוב עליו כעל טווח של רוחב 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 389.18,
  "end": 401.22
 },
 {
  "input": "If the idea of the rules changing between a finite setting and a continuous one feels unsettling, well, you'll be happy to know that mathematicians are way ahead of you.",
  "translatedText": "אם הרעיון של החוקים המשתנים בין הגדרה סופית להגדרה מתמשכת מרגיש מטריד, ובכן, תשמחו לדעת שמתמטיקאים מקדימים אתכם הרבה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.18,
  "end": 410.4
 },
 {
  "input": "There's a field of math called measure theory, which helps to unite these two settings and make rigorous the idea of associating numbers like probabilities to various subsets of all possibilities in a way that combines and distributes nicely.",
  "translatedText": "יש תחום של מתמטיקה שנקרא תורת המידה, שעוזר לאחד את שתי ההגדרות הללו ולהפוך את הרעיון לשיוך מספרים כמו הסתברויות לקבוצות משנה שונות של כל האפשרויות באופן שמשלב ומתחלק בצורה יפה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.82,
  "end": 423.14
 },
 {
  "input": "For example, let's say you're in a setting where you have a random number that equals 0 with 50% probability, and the rest of the time it's some positive number according to a distribution that looks like half of a bell curve.",
  "translatedText": "לדוגמה, נניח שאתה בהגדרה שבה יש לך מספר אקראי השווה ל-0 בהסתברות של 50%, ובשאר הזמן זה מספר חיובי כלשהו לפי התפלגות שנראית כמו חצי מעקומת פעמון.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 424.04,
  "end": 435.88
 },
 {
  "input": "This is an awkward middle ground between a finite context, where a single value has a non-zero probability, and a continuous one.",
  "translatedText": "זוהי דרך ביניים מביכה בין הקשר סופי, שבו לערך בודד יש הסתברות שאינה אפס, לבין רציף.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 436.48,
  "end": 444.38
 },
 {
  "input": "where probabilities are found according to areas under the appropriate density function.",
  "translatedText": "שבו נמצאות הסתברויות לפי אזורים תחת פונקציית הצפיפות המתאימה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 444.64,
  "end": 448.68
 },
 {
  "input": "This is the sort of thing that measure theory handles very smoothly.",
  "translatedText": "זה מסוג הדברים שתיאוריית המדידות מטפלת בצורה חלקה מאוד.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.46,
  "end": 452.6
 },
 {
  "input": "I mention this mainly for the especially curious viewer, and you can find more reading material in the description.",
  "translatedText": "אני מזכיר זאת בעיקר עבור הצופה הסקרן במיוחד, ותוכלו למצוא חומר קריאה נוסף בתיאור.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.04,
  "end": 458.12
 },
 {
  "input": "It's a pretty common rule of thumb that if you find yourself using a sum in a discrete context, then use an integral in the continuous context, which is the tool from calculus that we use to find areas under curves.",
  "translatedText": "זה כלל אצבע די נפוץ שאם אתה מוצא את עצמך משתמש בסכום בהקשר דיסקרטי, אז השתמש באינטגרל בהקשר הרציף, שהוא הכלי מהחשבון שבו אנו משתמשים כדי למצוא שטחים מתחת לעיקולים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 460.62,
  "end": 471.8
 },
 {
  "input": "In fact, you could argue this video would be way shorter if I just said that at the front and called it good.",
  "translatedText": "למעשה, אתה יכול לטעון שהסרטון הזה יהיה הרבה יותר קצר אם רק אגיד את זה בחזית ואקרא לזה טוב.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 471.8,
  "end": 477.04
 },
 {
  "input": "For my part though, I always found it a little unsatisfying to do this blindly without thinking through what it really means.",
  "translatedText": "אבל מצדי, תמיד מצאתי את זה קצת לא מספק לעשות את זה בצורה עיוורת בלי לחשוב מה זה באמת אומר.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.76,
  "end": 483.28
 },
 {
  "input": "And in fact, if you really dig into the theoretical underpinnings of integrals, what you'd find is that in addition to the way that it's defined in a typical intro calculus class, there is a separate more powerful definition that's based on measure theory, this formal foundation of probability.",
  "translatedText": "ולמעשה, אם באמת תחפור בבסיס התיאורטי של אינטגרלים, מה שתמצא הוא שבנוסף לאופן שבו הוא מוגדר במחלקה טיפוסית של חישוב מבוא, יש הגדרה נפרדת חזקה יותר שמבוססת על תורת המידה, הבסיס הפורמלי הזה של הסתברות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.08,
  "end": 499.02
 },
 {
  "input": "If I look back to when I first learned probability, I definitely remember grappling with this weird idea that in continuous settings, like random variables that are real numbers or throwing a dart at a dartboard, you have a bunch of outcomes that are possible, and yet each one has a probability of zero, and somehow altogether they have a probability of one.",
  "translatedText": "אם אני מסתכל אחורה למועד שבו למדתי הסתברות לראשונה, אני בהחלט זוכר שהתחבטתי עם הרעיון המוזר הזה שבהגדרות רציפות, כמו משתנים אקראיים שהם מספרים אמיתיים או זריקת חץ אל לוח חצים, יש לך המון תוצאות שאפשריות, ו ובכל זאת לכל אחד מהם יש הסתברות של אפס, ואיכשהו בסך הכל יש להם הסתברות של אחד.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 500.28,
  "end": 519.56
 },
 {
  "input": "Now one step of coming to terms with this is to realize that possibility is better tied to probability density than probability, but just swapping out sums of one for integrals of the others never quite scratched the itch for me.",
  "translatedText": "עכשיו צעד אחד להשלים עם זה הוא להבין שהאפשרות קשורה טוב יותר לצפיפות ההסתברות מאשר להסתברות, אבל עצם החלפת סכומים של אחד באינטגרלים של האחרים אף פעם לא ממש גירד לי את הגירוד.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 520.82,
  "end": 532.82
 },
 {
  "input": "I remember that it only really clicked when I realized that the rules for combining probabilities of different sets were not quite what I thought they were, and there was simply a different axiom system underlying it all.",
  "translatedText": "אני זוכר שזה ממש צלצל רק כשהבנתי שהכללים לשילוב הסתברויות של קבוצות שונות לא היו בדיוק מה שחשבתי שהם, ופשוט הייתה מערכת אקסיומות שונה בבסיס הכל.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 533.28,
  "end": 543.24
 },
 {
  "input": "But anyway, steering away from the theory somewhere back in the loose direction of application, look back to our original question about the coin with an unknown weight.",
  "translatedText": "אבל בכל מקרה, להתרחק מהתיאוריה איפשהו בחזרה לכיוון היישום הרופף, הסתכל אחורה לשאלתנו המקורית לגבי המטבע עם משקל לא ידוע.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 544.58,
  "end": 552.44
 },
 {
  "input": "What we've learned here is that the right question to ask is, what's the probability density function that describes this value h after seeing the outcomes of a few tosses?",
  "translatedText": "מה שלמדנו כאן הוא שהשאלה הנכונה לשאול היא, מהי פונקציית צפיפות ההסתברות שמתארת ערך זה h לאחר שראינו את התוצאות של כמה הטלות?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 552.96,
  "end": 562.96
 },
 {
  "input": "If you can find that PDF, you can use it to answer questions like, what's the probability that the true probability of flipping heads falls between 0.6 and 0.8?",
  "translatedText": "אם אתה יכול למצוא את ה-PDF הזה, אתה יכול להשתמש בו כדי לענות על שאלות כמו, מה ההסתברות שההסתברות האמיתית להעיף ראשים היא בין 0.6 ו-0.8?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 563.46,
  "end": 572.8
 },
 {
  "input": "To find that PDF, join me in the next part.",
  "translatedText": "כדי למצוא את ה-PDF הזה, הצטרף אליי בחלק הבא.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.68,
  "end": 576.06
 }
]