[
 {
  "input": "Imagine you have a weighted coin, so the probability of flipping heads might not be 50-50 exactly. ",
  "translatedText": "가중치가 부여된 동전이 있다고 가정해 보겠습니다. 앞면이 나올 확률은 정확히 50-50이 아닐 수도 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2.8,
  "end": 8.68
 },
 {
  "input": "It could be 20%, or maybe 90%, or 0%, or 31.41592%. ",
  "translatedText": "20%일 수도 있고, 90%일 수도 있고, 0%일 수도 있고, 31%일 수도 있습니다. 41592%. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.14,
  "end": 18.48
 },
 {
  "input": "The point is that you just don't know. ",
  "translatedText": "요점은 당신이 모른다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 18.48,
  "end": 20.2
 },
 {
  "input": "But imagine that you flip this coin 10 different times, and 7 of those times it comes up heads. ",
  "translatedText": "하지만 이 동전을 10번 던져서 앞면이 7번 나왔다고 상상해 보세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 20.78,
  "end": 25.58
 },
 {
  "input": "Do you think that the underlying weight of this coin is such that each flip has a 70% chance of coming up heads? ",
  "translatedText": "이 동전의 기본 무게가 앞면이 나올 확률이 70%라고 생각하시나요? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.58,
  "end": 32.02
 },
 {
  "input": "If I were to ask you, hey, what's the probability that the true probability of flipping heads is 0.7, what would you say? ",
  "translatedText": "제가 여러분에게 묻는다면, 앞면이 뒤집힐 실제 확률이 0일 확률은 얼마입니까? 7, 뭐라고 말하겠어요? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 32.76,
  "end": 39.62
 },
 {
  "input": "This is a pretty weird question, and for two reasons. ",
  "translatedText": "이것은 꽤 이상한 질문인데 두 가지 이유가 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 41.54,
  "end": 44.22
 },
 {
  "input": "First of all, it's asking about a probability of a probability, as in the value we don't know is itself some kind of long-run frequency for a random event, which frankly is hard to think about. ",
  "translatedText": "우선, 우리가 모르는 값 자체가 무작위 사건에 대한 일종의 장기 빈도이기 때문에 확률의 확률에 대해 묻고 있는데, 이는 솔직히 생각하기 어렵습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 44.7,
  "end": 55.72
 },
 {
  "input": "But the more pressing weirdness comes from asking about probabilities in the setting of continuous values. ",
  "translatedText": "그러나 더 긴급한 이상한 점은 연속 값 설정의 확률에 대해 묻는 것에서 비롯됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 56.28,
  "end": 61.28
 },
 {
  "input": "Let's give this unknown probability of flipping heads some kind of name, like h. ",
  "translatedText": "이 알 수 없는 앞면 뒤집기 확률에 h와 같은 이름을 붙여보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.54,
  "end": 66.78
 },
 {
  "input": "Keep in mind that h could be any real number from 0 up to 1, ranging from a coin that always flips tails up to one that always flips heads, and everything in between. ",
  "translatedText": "h는 항상 뒷면이 뒤집히는 동전부터 항상 앞면이 뒤집히는 동전까지, 그리고 그 사이의 모든 숫자에 이르기까지 0에서 1까지의 실수일 수 있다는 점을 명심하세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.54,
  "end": 77.32
 },
 {
  "input": "So if I ask, hey, what's the probability that h is precisely 0.7, as opposed to, say, 0.700000001, or any other nearby value, there's going to be a strong possibility for paradox if we're not careful. ",
  "translatedText": "그래서 내가 묻는다면, h가 정확히 0일 확률은 얼마나 될까요? 7은 0이 아니라 0입니다. 700000001 또는 다른 근처 값은 주의하지 않으면 역설이 발생할 가능성이 높습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.72,
  "end": 94.16
 },
 {
  "input": "It feels like no matter how small the answer to this question, it just wouldn't be small enough. ",
  "translatedText": "이 질문에 대한 답이 아무리 작더라도 충분하지 않을 것 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 94.86,
  "end": 99.16
 },
 {
  "input": "If every specific value within some range, all uncountably infinitely many of them, has a non-zero probability, even if that probability was miniscule, adding them all up to get the total probability of any one of these values will blow up to infinity. ",
  "translatedText": "어떤 범위 내의 모든 특정 값, 셀 수 없을 정도로 많은 값이 0이 아닌 확률을 갖는 경우, 그 확률이 미미하더라도 이러한 값 중 하나의 전체 확률을 얻기 위해 모든 값을 더하면 무한대가 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.94,
  "end": 114.26
 },
 {
  "input": "On the other hand, if all of these probabilities are 0, aside from the fact that that now gives you no useful information about the coin, the total sum of those probabilities would be 0, when it should be 1. ",
  "translatedText": "반면, 이러한 확률이 모두 0인 경우, 이제 동전에 대한 유용한 정보를 제공하지 않는다는 사실을 제외하면 해당 확률의 총합은 1이어야 하는데 0이 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 115.86,
  "end": 127.66
 },
 {
  "input": "After all, this weight of the coin h is something, so the probability of it being any one of these values should add up to 1. ",
  "translatedText": "결국, 동전 h의 무게는 무언가이므로 이 값 중 하나일 확률은 1이 되어야 합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 128.54,
  "end": 136.44
 },
 {
  "input": "So if these values can't all be non-zero, and they can't all be 0, what do you do? ",
  "translatedText": "따라서 이들 값이 모두 0이 될 수도 없고 모두 0이 될 수도 없다면 어떻게 해야 할까요? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 137.32,
  "end": 142.22
 },
 {
  "input": "Where we're going with this, by the way, is that I'd like to talk about the very practical question of using data to create meaningful answers to these sorts of probabilities of probabilities questions. ",
  "translatedText": "그런데 우리가 여기서 하려는 것은 이러한 종류의 확률 질문에 대한 의미 있는 답을 만들기 위해 데이터를 사용하는 매우 실용적인 질문에 대해 이야기하고 싶다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 144.8,
  "end": 154.6
 },
 {
  "input": "But for this video, let's take a moment to appreciate how to work with probabilities over continuous values, and resolve this apparent paradox. ",
  "translatedText": "하지만 이 비디오에서는 연속 값에 대한 확률을 사용하여 작업하는 방법을 알아보고 이 명백한 역설을 해결하는 방법을 잠시 살펴보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.68,
  "end": 162.78
 },
 {
  "input": "The key is not to focus on individual values, but ranges of values. ",
  "translatedText": "핵심은 개별 값에 초점을 맞추는 것이 아니라 값 범위에 초점을 맞추는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.32,
  "end": 173.96
 },
 {
  "input": "For example, we might make these buckets to represent the probability that h is between, say 0.8 and 0.85. ",
  "translatedText": "예를 들어, h가 0 사이에 있을 확률을 나타내기 위해 이러한 버킷을 만들 수 있습니다. 8과 0.85. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.62,
  "end": 182.16
 },
 {
  "input": "Also, and this is more important than it might seem, rather than thinking of the height of each of these bars as representing the probability, think of the area of each one as representing that probability. ",
  "translatedText": "또한 이것은 보이는 것보다 더 중요합니다. 각 막대의 높이를 확률로 생각하기보다는 각 막대의 면적을 해당 확률로 생각하세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.16,
  "end": 193.04
 },
 {
  "input": "Where exactly those areas come from is something we'll answer later. ",
  "translatedText": "해당 영역이 정확히 어디에서 왔는지는 나중에 답변해 드리겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 193.96,
  "end": 197.48
 },
 {
  "input": "For right now, just know that in principle, there's some answer to the probability of h sitting inside one of these ranges. ",
  "translatedText": "지금 당장은 원칙적으로 h가 이 범위 중 하나에 속할 확률에 대한 답이 있다는 점만 알아두세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.96,
  "end": 204.14
 },
 {
  "input": "Our task right now is to take the answers to these very coarse-grained questions, and to get a more exact understanding of the distribution at the level of each individual input. ",
  "translatedText": "지금 우리의 임무는 이러한 매우 대략적인 질문에 대한 답을 찾고 각 개별 입력 수준에서 분포를 보다 정확하게 이해하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.96,
  "end": 214.56
 },
 {
  "input": "The natural thing to do is to consider finer and finer buckets, and when you do, the smaller probability of falling into any one of them is accounted for in the thinner width of each of these bars, while the heights are going to stay roughly the same. ",
  "translatedText": "자연스러운 일은 점점 더 미세한 버킷을 고려하는 것이며, 그렇게 할 때 각 막대의 너비가 얇을수록 버킷 중 하나에 빠질 가능성이 더 작아지는 반면 높이는 대략적으로 유지됩니다. 같은. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 215.46,
  "end": 228.92
 },
 {
  "input": "That's important because it means that as you take this process to the limit, you approach some kind of smooth curve. ",
  "translatedText": "이는 이 프로세스를 한계까지 가져갈 때 일종의 부드러운 곡선에 접근한다는 것을 의미하기 때문에 중요합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 229.66,
  "end": 235.22
 },
 {
  "input": "So even though all of the individual probabilities of falling into any one particular bucket will approach 0, the overall shape of the distribution is preserved, and even refined in this limit. ",
  "translatedText": "따라서 하나의 특정 버킷에 속할 모든 개별 확률이 0에 가까워지더라도 분포의 전체 모양은 보존되며 이 한도 내에서 개선됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 235.9,
  "end": 247.22
 },
 {
  "input": "If we had let the heights of the bars represent probabilities, everything would have gone to 0. ",
  "translatedText": "막대의 높이가 확률을 나타내도록 했다면 모든 것이 0이 되었을 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.7,
  "end": 254.9
 },
 {
  "input": "So in the limit, we would have just had a flat line giving no information about the overall shape of the distribution. ",
  "translatedText": "따라서 극한에서는 분포의 전체 형태에 대한 정보를 제공하지 않는 평평한 선만 갖게 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 260.04,
  "end": 265.64
 },
 {
  "input": "So wonderful, letting area represent probability helps solve this problem. ",
  "translatedText": "정말 훌륭합니다. 면적을 확률로 나타내면 이 문제를 해결하는 데 도움이 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.42,
  "end": 271.26
 },
 {
  "input": "But let me ask you, if the y-axis no longer represents probability, what exactly are the units here? ",
  "translatedText": "하지만 y축이 더 이상 확률을 나타내지 않는다면 여기서 단위는 정확히 무엇입니까? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 271.9,
  "end": 277.14
 },
 {
  "input": "Since probability sits in the area of these bars, or width times height, the height represents a kind of probability per unit in the x-direction, what's known in the business as a probability density. ",
  "translatedText": "확률은 이 막대의 영역, 즉 너비와 높이에 있기 때문에 높이는 x 방향의 단위당 확률을 나타내며 업계에서는 확률 밀도라고 알려져 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.8,
  "end": 289.64
 },
 {
  "input": "The other thing to keep in mind is that the total area of all these bars has to equal 1 at every level of the process. ",
  "translatedText": "명심해야 할 또 다른 점은 모든 막대의 총 면적이 프로세스의 모든 수준에서 1과 같아야 한다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 290.58,
  "end": 296.54
 },
 {
  "input": "That's something that has to be true for any valid probability distribution. ",
  "translatedText": "이는 유효한 확률 분포에 대해 참이어야 하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 297.06,
  "end": 300.5
 },
 {
  "input": "The idea of probability density is actually really clever when you step back to think about it. ",
  "translatedText": "확률 밀도에 대한 아이디어는 한발 물러서 생각해 보면 실제로 정말 영리합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.98,
  "end": 306.3
 },
 {
  "input": "As you take things to the limit, even if there's all sorts of paradoxes associated with assigning a probability to each of these uncountably infinitely many values of h between 0 and 1, there's no problem if we associate a probability density to each one of them, giving what's known as a probability density function, or PDF for short. ",
  "translatedText": "한계에 도달하면 0과 1 사이의 셀 수 없이 무한히 많은 h 값 각각에 확률을 할당하는 것과 관련된 모든 종류의 역설이 있더라도 각 값에 확률 밀도를 연결하면 문제가 없습니다. 확률밀도함수, 줄여서 PDF라고 알려진 것을 제공합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.3,
  "end": 325.64
 },
 {
  "input": "Any time you see a PDF in the wild, the way to interpret it is that the probability of your random variable lying between two values equals the area under this curve between those values. ",
  "translatedText": "실제 PDF를 볼 때마다 이를 해석하는 방법은 두 값 사이에 임의 변수가 있을 확률이 해당 값 사이의 이 곡선 아래 면적과 같다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 326.42,
  "end": 337.52
 },
 {
  "input": "So, for example, what's the probability of getting any one very specific number, like 0.7? ",
  "translatedText": "예를 들어, 0과 같은 매우 특정한 숫자 하나를 얻을 확률은 얼마입니까? 7? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.22,
  "end": 343.46
 },
 {
  "input": "Well, the area of an infinitely thin slice is 0, so it's 0. ",
  "translatedText": "음, 무한히 얇은 조각의 면적은 0이므로 0입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.22,
  "end": 348.34
 },
 {
  "input": "What's the probability of all of them put together? ",
  "translatedText": "그것들을 모두 합칠 확률은 얼마입니까? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.9,
  "end": 351.14
 },
 {
  "input": "Well, the area under the full curve is 1. ",
  "translatedText": "음, 전체 곡선 아래의 면적은 1입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.78,
  "end": 353.96
 },
 {
  "input": "You see? ",
  "translatedText": "알겠어요? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 354.62,
  "end": 354.92
 },
 {
  "input": "Paradox sidestepped. ",
  "translatedText": "역설은 회피했다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.72,
  "end": 356.4
 },
 {
  "input": "And the way that it's been sidestepped is a bit subtle. ",
  "translatedText": "그리고 그것이 회피되는 방식은 약간 미묘합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 357.5,
  "end": 360.22
 },
 {
  "input": "In normal, finite settings, like rolling a die or drawing a card, the probability that a random value falls into a given collection of possibilities is simply the sum of the probabilities of being any one of them. ",
  "translatedText": "주사위를 굴리거나 카드를 뽑는 것과 같은 일반적이고 유한한 설정에서 임의의 값이 주어진 가능성 모음에 포함될 확률은 단순히 그 중 하나가 될 확률의 합입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 360.22,
  "end": 372.96
 },
 {
  "input": "This feels very intuitive, it's even true in a countably infinite context. ",
  "translatedText": "이것은 매우 직관적으로 느껴지며 셀 수 없을 정도로 무한한 맥락에서도 마찬가지입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 373.84,
  "end": 377.6
 },
 {
  "input": "But to deal with the continuum, the rules themselves have shifted. ",
  "translatedText": "그러나 연속체를 다루기 위해 규칙 자체가 바뀌었습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.12,
  "end": 381.54
 },
 {
  "input": "The probability of falling into a range of values is no longer the sum of the probabilities of each individual value. ",
  "translatedText": "특정 값 범위에 속할 확률은 더 이상 각 개별 값의 확률의 합이 아닙니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.1,
  "end": 388.66
 },
 {
  "input": "Instead, probabilities associated with ranges are the fundamental primitive objects, and the only sense in which it's meaningful to talk about an individual value here is to think of it as a range of width 0. ",
  "translatedText": "대신, 범위와 관련된 확률은 기본적인 기본 개체이며 여기서 개별 값에 대해 이야기하는 것이 의미 있는 유일한 의미는 이를 너비 0의 범위로 생각하는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 389.18,
  "end": 401.22
 },
 {
  "input": "If the idea of the rules changing between a finite setting and a continuous one feels unsettling, well you'll be happy to know that mathematicians are way ahead of you. ",
  "translatedText": "유한한 설정과 연속적인 설정 사이에서 규칙이 변경된다는 생각이 불안하게 느껴진다면 수학자들이 당신보다 훨씬 앞서 있다는 사실을 알게 되어 기쁠 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.18,
  "end": 410.4
 },
 {
  "input": "There's a field of math called measure theory which helps to unite these two settings and make rigorous the idea of associating numbers like probabilities to various subsets of all possibilities in a way that combines and distributes nicely. ",
  "translatedText": "이 두 가지 설정을 통합하고 확률과 같은 숫자를 잘 결합하고 분배하는 방식으로 모든 가능성의 다양한 하위 집합에 연결하는 아이디어를 엄격하게 만드는 데 도움이 되는 측정 이론이라는 수학 분야가 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 410.82,
  "end": 423.14
 },
 {
  "input": "For example, let's say you're in a setting where you have a random number that equals 0 with 50% probability, and the rest of the time it's some positive number according to a distribution that looks like half of a bell curve. ",
  "translatedText": "예를 들어, 확률이 50%인 0과 동일한 임의의 숫자가 있고 나머지 시간에는 종형 곡선의 절반처럼 보이는 분포에 따라 양수인 환경에 있다고 가정해 보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 424.04,
  "end": 435.88
 },
 {
  "input": "This is an awkward middle ground between a finite context, where a single value has a non-zero probability, and a continuous one, where probabilities are found according to areas under the appropriate density function. ",
  "translatedText": "이는 단일 값의 확률이 0이 아닌 유한 상황과 적절한 밀도 함수 아래의 영역에 따라 확률이 발견되는 연속 상황 사이의 어색한 중간 지점입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.48,
  "end": 448.68
 },
 {
  "input": "This is the sort of thing that measure theory handles very smoothly. ",
  "translatedText": "이것은 측정 이론이 매우 원활하게 처리되는 일종의 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.46,
  "end": 452.6
 },
 {
  "input": "I mention this mainly for the especially curious viewer, and you can find more reading material in the description. ",
  "translatedText": "특히 호기심이 많은 시청자를 위해 이 내용을 언급하고 있으며, 설명에서 더 많은 읽을 거리를 찾을 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 458.12
 },
 {
  "input": "It's a pretty common rule of thumb that if you find yourself using a sum in a discrete context, then use an integral in the continuous context, which is the tool from calculus that we use to find areas under curves. ",
  "translatedText": "이산적 맥락에서 합을 사용하는 경우 연속적 맥락에서 적분을 사용하는 것이 매우 일반적인 경험 법칙입니다. 이는 곡선 아래 영역을 찾는 데 사용하는 미적분의 도구입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 460.62,
  "end": 471.8
 },
 {
  "input": "In fact, you could argue this video would be way shorter if I just said that at the front and called it good. ",
  "translatedText": "사실, 제가 앞에서 그런 말을 하고 좋았다고 하면 이 영상이 훨씬 짧을 것이라고 주장할 수도 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 471.8,
  "end": 477.04
 },
 {
  "input": "For my part though, I always found it a little unsatisfying to do this blindly without thinking through what it really means. ",
  "translatedText": "하지만 내 입장에서는 그것이 실제로 무엇을 의미하는지 생각하지 않고 맹목적으로 이 일을 하는 것이 항상 조금 불만족스럽다는 것을 알았습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 477.76,
  "end": 483.28
 },
 {
  "input": "And in fact, if you really dig into the theoretical underpinnings of integrals, what you'd find is that in addition to the way that it's defined in a typical intro calculus class, there is a separate, more powerful definition that's based on measure theory, this formal foundation of probability. ",
  "translatedText": "실제로 적분의 이론적 토대를 자세히 살펴보면 일반적인 입문 미적분학 수업에서 정의한 방식 외에도 측정 이론에 기초한 별도의 더 강력한 정의가 있다는 것을 알 수 있습니다. , 이것은 확률의 공식적인 기초입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.08,
  "end": 499.02
 },
 {
  "input": "If I look back to when I first learned probability, I definitely remember grappling with this weird idea that in continuous settings, like random variables that are real numbers or throwing a dart at a dartboard, you have a bunch of outcomes that are possible, and yet each one has a probability of zero, and somehow all together they have a probability of one. ",
  "translatedText": "제가 확률을 처음 배웠을 때를 돌이켜보면 실수인 무작위 변수나 다트판에 다트를 던지는 것과 같은 연속적인 설정에서 가능한 결과가 여러 개 있다는 이상한 생각과 씨름했던 기억이 납니다. 그러나 각각의 확률은 0이고, 어떻게든 모두 합쳐서 1의 확률을 갖습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 500.28,
  "end": 519.56
 },
 {
  "input": "One step of coming to terms with this is to realize that possibility is better tied to probability density than probability, but just swapping out sums of one for integrals of the others never quite scratched the itch for me. ",
  "translatedText": "이를 이해하는 한 단계는 가능성이 확률보다 확률 밀도에 더 잘 연결되어 있다는 것을 깨닫는 것입니다. 그러나 하나의 합을 다른 적분으로 바꾸는 것만으로는 가려움증을 전혀 긁지 못했습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 520.82,
  "end": 532.82
 },
 {
  "input": "I remember that it only really clicked when I realized that the rules for combining probabilities of different sets were not quite what I thought they were, and there was simply a different axiom system underlying it all. ",
  "translatedText": "나는 서로 다른 집합의 확률을 결합하는 규칙이 내가 생각했던 것과는 전혀 다르고, 그 모든 것의 기초가 되는 다른 공리 시스템이 있다는 것을 깨달았을 때만 정말 클릭이 되었던 것을 기억합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.28,
  "end": 543.24
 },
 {
  "input": "But anyway, steering away from the theory somewhere back in the loose direction of application, let's look back to our original question about the coin with an unknown weight. ",
  "translatedText": "그러나 어쨌든, 느슨한 적용 방향으로 돌아가는 이론에서 벗어나 무게를 알 수 없는 동전에 대한 원래 질문으로 돌아가 보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 544.58,
  "end": 552.44
 },
 {
  "input": "What we've learned here is that the right question to ask is, what's the probability density function that describes this value h after seeing the outcomes of a few tosses? ",
  "translatedText": "여기서 우리가 배운 것은 올바른 질문은 몇 번의 던지기 결과를 본 후 이 값 h를 설명하는 확률 밀도 함수가 무엇인가라는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 552.96,
  "end": 562.96
 },
 {
  "input": "If you can find that PDF, you can use it to answer questions like, what's the probability that the true probability of flipping heads falls between 0.6 and 0.8? ",
  "translatedText": "해당 PDF를 찾으면 이를 사용하여 머리가 뒤집힐 실제 확률이 0 사이에 있을 확률은 얼마입니까?와 같은 질문에 답할 수 있습니다. 6과 0.8? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.46,
  "end": 572.8
 },
 {
  "input": "To find that PDF, join me in the next part. ",
  "translatedText": "해당 PDF를 찾으려면 다음 부분에 참여하세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.68,
  "end": 576.06
 }
]