[
 {
  "input": "Imagine you have a weighted coin, so the probability of flipping heads might not be 50-50 exactly.",
  "translatedText": "Уявіть, що у вас є зважена монета, тож ймовірність підкидання головок може не дорівнювати 50-50.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 2.8,
  "end": 8.68
 },
 {
  "input": "It could be 20%, or maybe 90%, or 0%, or 31.41592%.",
  "translatedText": "Це може бути 20%, або, можливо, 90%, або 0%, або 31.41592%.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.14,
  "end": 18.48
 },
 {
  "input": "The point is that you just don't know.",
  "translatedText": "Справа в тому, що ви просто не знаєте.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.48,
  "end": 20.2
 },
 {
  "input": "But imagine that you flip this coin 10 different times, and 7 of those times it comes up heads.",
  "translatedText": "Але уявіть, що ви кидаєте цю монету 10 різних разів, і 7 із них вона випадає головами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.78,
  "end": 25.58
 },
 {
  "input": "Do you think that the underlying weight of this coin is such that each flip has a 70% chance of coming up heads?",
  "translatedText": "Як ви думаєте, чи вага цієї монети є такою, що кожне підкидання має 70% шансів отримати голови?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.58,
  "end": 32.02
 },
 {
  "input": "If I were to ask you, hey, what's the probability that the true probability of flipping heads is 0.7, what would you say?",
  "translatedText": "Якби я запитав вас, привіт, яка ймовірність того, що справжня ймовірність перевернути голови дорівнює 0.7, що б ти сказав?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 32.76,
  "end": 39.62
 },
 {
  "input": "This is a pretty weird question, and for two reasons.",
  "translatedText": "Це досить дивне запитання з двох причин.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 41.54,
  "end": 44.22
 },
 {
  "input": "First of all, it's asking about a probability of a probability, as in the value we don't know is itself some kind of long-run frequency for a random event, which frankly is hard to think about.",
  "translatedText": "Перш за все, це питання про ймовірність ймовірності, оскільки значення, яке ми не знаємо, саме по собі є певною довгостроковою частотою для випадкової події, про яку, чесно кажучи, важко думати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.7,
  "end": 55.72
 },
 {
  "input": "But the more pressing weirdness comes from asking about probabilities in the setting of continuous values.",
  "translatedText": "Але більш нагальною дивністю є питання про ймовірності в налаштуванні безперервних значень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 56.28,
  "end": 61.28
 },
 {
  "input": "Let's give this unknown probability of flipping heads some kind of name, like h.",
  "translatedText": "Давайте назвемо цю невідому ймовірність перевертання голов як-небудь, наприклад h.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 62.54,
  "end": 66.78
 },
 {
  "input": "Keep in mind that h could be any real number from 0 up to 1, ranging from a coin that always flips tails up to one that always flips heads and everything in between.",
  "translatedText": "Майте на увазі, що h може бути будь-яким дійсним числом від 0 до 1, починаючи від монети, яка завжди підкидає решку, до монети, яка завжди підкидає орел, і все, що між ними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 67.54,
  "end": 77.32
 },
 {
  "input": "So if I ask, hey, what's the probability that h is precisely 0.7, as opposed to, say, 0.7000001, or any other nearby value, well, there's going to be a strong possibility for paradox if we're not careful.",
  "translatedText": "Отже, якщо я запитаю, яка ймовірність того, що h дорівнює саме 0,7, а не, скажімо, 0,7000001, або будь-якому іншому близькому значенню, що ж, якщо ми не будемо обережними, то виникне велика ймовірність парадоксу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 78.72,
  "end": 94.16
 },
 {
  "input": "It feels like no matter how small the answer to this question, it just wouldn't be small enough.",
  "translatedText": "Таке відчуття, що якою б малою не була відповідь на це запитання, вона просто недостатньо мала.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 94.86,
  "end": 99.16
 },
 {
  "input": "If every specific value within some range, all uncountably infinitely many of them, has a non-zero probability, well, even if that probability was minuscule, adding them all up to get the total probability of any one of these values will blow up to infinity.",
  "translatedText": "Якщо кожне конкретне значення в деякому діапазоні, яких незліченно багато, має ненульову ймовірність, то навіть якщо ця ймовірність мізерно мала, додавання їх усіх для отримання загальної ймовірності будь-якого з цих значень призведе до нескінченності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 99.94,
  "end": 114.26
 },
 {
  "input": "On the other hand though, if all of these probabilities are 0, aside from the fact that that now gives you no useful information about the coin, the total sum of those probabilities would be 0, when it should be 1.",
  "translatedText": "З іншого боку, якщо всі ці ймовірності дорівнюють 0, не враховуючи той факт, що зараз вони не дають вам ніякої корисної інформації про монету, загальна сума цих ймовірностей буде дорівнювати 0, тоді як вона повинна дорівнювати 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 115.86,
  "end": 127.66
 },
 {
  "input": "After all, this weight of the coin h is something, so the probability of it being any one of these values should add up to 1.",
  "translatedText": "Зрештою, ця вага монети h є чимось, тому ймовірність того, що це буде будь-яке з цих значень, повинна дорівнювати 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 128.54,
  "end": 136.44
 },
 {
  "input": "So if these values can't all be non-zero, and they can't all be 0, what do you do?",
  "translatedText": "Отже, якщо всі ці значення не можуть бути відмінними від нуля, і всі вони не можуть бути 0, що ви робите?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 137.32,
  "end": 142.22
 },
 {
  "input": "Where we're going with this, by the way, is that I'd like to talk about the very practical question of using data to create meaningful answers to these sorts of probabilities of probabilities questions.",
  "translatedText": "До речі, я хотів би поговорити про дуже практичне питання використання даних для створення значущих відповідей на подібні запитання про ймовірності ймовірностей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 144.8,
  "end": 154.6
 },
 {
  "input": "But for this video, let's take a moment to appreciate how to work with probabilities over continuous values, and resolve this apparent paradox.",
  "translatedText": "Але для цього відео давайте трохи оцінимо, як працювати з імовірностями безперервних значень, і розв’яжемо цей очевидний парадокс.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.68,
  "end": 162.78
 },
 {
  "input": "The key is not to focus on individual values, but ranges of values.",
  "translatedText": "Головне — зосереджуватися не на окремих значеннях, а на діапазонах значень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.32,
  "end": 173.96
 },
 {
  "input": "For example, we might make these buckets to represent the probability that h is between, say, 0.8 and 0.85.",
  "translatedText": "Наприклад, ми можемо зробити ці відерця, щоб показати ймовірність того, що h знаходиться між, скажімо, 0,8 і 0,85.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 174.62,
  "end": 182.16
 },
 {
  "input": "Also, and this is more important than it might seem, rather than thinking of the height of each of these bars as representing the probability, think of the area of each one as representing that probability.",
  "translatedText": "Крім того, і це важливіше, ніж може здатися, замість того, щоб думати про висоту кожного з цих стовпчиків як про ймовірність, подумайте про площу кожного з них як про ймовірність.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.16,
  "end": 193.04
 },
 {
  "input": "Where exactly those areas come from is something that we'll answer later.",
  "translatedText": "Звідки саме беруться ці райони - це питання, на яке ми відповімо пізніше.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 193.96,
  "end": 197.48
 },
 {
  "input": "For right now, just know that in principle, there's some answer to the probability of h sitting inside one of these ranges.",
  "translatedText": "Наразі просто знайте, що в принципі є певна відповідь на ймовірність того, що h знаходиться в одному з цих діапазонів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 197.96,
  "end": 204.14
 },
 {
  "input": "Our task right now is to take the answers to these very coarse-grained questions, and to get a more exact understanding of the distribution at the level of each individual input.",
  "translatedText": "Зараз наше завдання — отримати відповіді на ці дуже грубі запитання та отримати більш точне розуміння розподілу на рівні кожного окремого входу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.96,
  "end": 214.56
 },
 {
  "input": "The natural thing to do would be consider finer and finer buckets.",
  "translatedText": "Природно було б розглянути все більш і більш дрібні відра.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 215.46,
  "end": 218.98
 },
 {
  "input": "And when you do, the smaller probability of falling into any one of them is accounted for in the thinner width of each of these bars, while the heights are going to stay roughly the same.",
  "translatedText": "І коли ви це зробите, менша ймовірність потрапляння в будь-яку з них пояснюється меншою шириною кожної з цих смуг, в той час як висота залишиться приблизно однаковою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 219.5,
  "end": 228.92
 },
 {
  "input": "That's important, because it means that as you take this process to the limit, you approach some kind of smooth curve.",
  "translatedText": "Це важливо, тому що це означає, що коли ви доводите цей процес до межі, ви наближаєтеся до якоїсь плавної кривої.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 229.66,
  "end": 235.22
 },
 {
  "input": "So even though all of the individual probabilities of falling into any one particular bucket will approach zero, the overall shape of the distribution is preserved, and even refined in this limit.",
  "translatedText": "Таким чином, навіть якщо всі індивідуальні ймовірності потрапляння в будь-яке конкретне відро наближаються до нуля, загальна форма розподілу зберігається, і навіть покращується в цій межі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.9,
  "end": 247.22
 },
 {
  "input": "If, on the other hand, we had let the heights of the bars represent probabilities, everything would have gone to zero.",
  "translatedText": "Якби ми дозволили висоті стовпчиків представляти ймовірності, все було б рівним 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.7,
  "end": 254.9
 },
 {
  "input": "So in the limit, we would have just had a flat line giving no information about the overall shape of the distribution.",
  "translatedText": "Таким чином, у обмеженні ми мали б просто рівну лінію, яка б не давала інформації про загальну форму розподілу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 260.04,
  "end": 265.64
 },
 {
  "input": "So, wonderful.",
  "translatedText": "Чудово.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 267.42,
  "end": 268.14
 },
 {
  "input": "Letting area represent probability helps solve this problem.",
  "translatedText": "Використання площі для представлення ймовірності допомагає вирішити цю проблему.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 268.44,
  "end": 271.26
 },
 {
  "input": "But let me ask you, if the y-axis no longer represents probability, what exactly are the units here?",
  "translatedText": "Але дозвольте запитати вас, якщо вісь y більше не представляє ймовірність, які саме одиниці тут?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 271.9,
  "end": 277.14
 },
 {
  "input": "Since probability sits in the area of these bars, or width times height, the height represents a kind of probability per unit in the x-direction, what's known in the business as a probability density.",
  "translatedText": "Оскільки ймовірність лежить у площі цих смужок або ширині, помноженій на висоту, висота представляє своєрідну ймовірність на одиницю в напрямку x, що в бізнесі називається щільністю ймовірності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.8,
  "end": 289.64
 },
 {
  "input": "The other thing to keep in mind is that the total area of all these bars has to equal one at every level of the process.",
  "translatedText": "Інша річ, про яку слід пам'ятати, - це те, що загальна площа всіх цих смуг повинна дорівнювати одиниці на кожному рівні процесу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 290.58,
  "end": 296.54
 },
 {
  "input": "That's something that has to be true for any valid probability distribution.",
  "translatedText": "Це те, що має бути вірним для будь-якого дійсного розподілу ймовірностей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 297.06,
  "end": 300.5
 },
 {
  "input": "The idea of probability density is actually really clever when you step back to think about it.",
  "translatedText": "Ідея щільності ймовірності насправді дуже розумна, якщо подумати про неї назад.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 301.98,
  "end": 306.3
 },
 {
  "input": "As you take things to the limit, even if there's all sorts of paradoxes associated with assigning a probability to each of these uncountably infinitely many values of h between 0 and 1, there's no problem if we associate a probability density to each one of them, giving what's known as a probability density function, or PDF for short.",
  "translatedText": "Якщо ви доведете речі до межі, навіть якщо є всілякі парадокси, пов’язані з присвоєнням ймовірності кожному з цих незліченної нескінченної кількості значень h між 0 і 1, не буде проблем, якщо ми зв’яжемо щільність ймовірності з кожним із них, дає так звану функцію щільності ймовірності, або скорочено PDF.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.3,
  "end": 325.64
 },
 {
  "input": "Anytime you see a PDF in the wild, the way to interpret it is that the probability of your random variable lying between two values equals the area under this curve between those values.",
  "translatedText": "Щоразу, коли ви бачите PDF-файл у природі, ви можете інтерпретувати його так: ймовірність того, що ваша випадкова величина лежить між двома значеннями, дорівнює площі під цією кривою між цими значеннями.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 326.42,
  "end": 337.52
 },
 {
  "input": "So, for example, what's the probability of getting any one very specific number, like 0.7?",
  "translatedText": "Так, наприклад, яка ймовірність отримати будь-яке дуже конкретне число, наприклад 0.7?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.22,
  "end": 343.46
 },
 {
  "input": "Well, the area of an infinitely thin slice is 0, so it's 0.",
  "translatedText": "Ну, площа нескінченно тонкого шматка дорівнює 0, отже, це 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.22,
  "end": 348.34
 },
 {
  "input": "What's the probability of all of them put together?",
  "translatedText": "Яка ймовірність того, що всі вони разом узяті?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 348.9,
  "end": 351.14
 },
 {
  "input": "Well, the area under the full curve is 1.",
  "translatedText": "Площа під повною кривою дорівнює 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 351.78,
  "end": 353.96
 },
 {
  "input": "You see?",
  "translatedText": "Розумієш?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 354.62,
  "end": 354.92
 },
 {
  "input": "Paradox sidestepped.",
  "translatedText": "Парадокс обійшов стороною.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 355.72,
  "end": 356.4
 },
 {
  "input": "And the way that it's been sidestepped is a bit subtle.",
  "translatedText": "І спосіб, яким його обійшли, є дещо непомітним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 357.5,
  "end": 360.22
 },
 {
  "input": "In normal, finite settings, like rolling a die or drawing a card, the probability that a random value falls into a given collection of possibilities is simply the sum of the probabilities of being any one of them.",
  "translatedText": "У звичайних обмежених умовах, як-от кидання кубика чи витягування карти, ймовірність того, що випадкове значення потрапляє в певний набір можливостей, є просто сумою ймовірностей бути будь-яким із них.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 360.22,
  "end": 372.96
 },
 {
  "input": "This feels very intuitive.",
  "translatedText": "Це відчувається дуже інтуїтивно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 373.84,
  "end": 375.02
 },
 {
  "input": "It's even true in a countably infinite context.",
  "translatedText": "Це навіть вірно в нескінченному контексті.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 375.24,
  "end": 377.6
 },
 {
  "input": "But to deal with the continuum, the rules themselves have shifted.",
  "translatedText": "Але щоб мати справу з континуумом, самі правила змінилися.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 378.12,
  "end": 381.54
 },
 {
  "input": "The probability of falling into a range of values is no longer the sum of the probabilities of each individual value.",
  "translatedText": "Імовірність потрапляння в діапазон значень більше не є сумою ймовірностей кожного окремого значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.1,
  "end": 388.66
 },
 {
  "input": "Instead, probabilities associated with ranges are the fundamental primitive objects, and the only sense in which it's meaningful to talk about an individual value here is to think of it as a range of width 0.",
  "translatedText": "Натомість ймовірності, пов’язані з діапазонами, є фундаментальними примітивними об’єктами, і єдиний сенс, у якому має сенс говорити про окреме значення тут, це розглядати його як діапазон ширини 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 389.18,
  "end": 401.22
 },
 {
  "input": "If the idea of the rules changing between a finite setting and a continuous one feels unsettling, well, you'll be happy to know that mathematicians are way ahead of you.",
  "translatedText": "Якщо думка про те, що правила можуть змінюватися від скінченної множини до неперервної, викликає занепокоєння, що ж, ви будете раді дізнатися, що математики далеко попереду вас.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.18,
  "end": 410.4
 },
 {
  "input": "There's a field of math called measure theory, which helps to unite these two settings and make rigorous the idea of associating numbers like probabilities to various subsets of all possibilities in a way that combines and distributes nicely.",
  "translatedText": "Існує розділ математики, який називається теорія міри, що допомагає об'єднати ці два параметри і зробити сувору ідею асоціювання чисел, таких як ймовірності, з різними підмножинами всіх можливостей у спосіб, який добре комбінує і розподіляє їх.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.82,
  "end": 423.14
 },
 {
  "input": "For example, let's say you're in a setting where you have a random number that equals 0 with 50% probability, and the rest of the time it's some positive number according to a distribution that looks like half of a bell curve.",
  "translatedText": "Наприклад, скажімо, у вас є випадкове число, яке з імовірністю 50% дорівнює 0, а решту часу це додатне число відповідно до розподілу, яке виглядає як половина дзвоноподібної кривої.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 424.04,
  "end": 435.88
 },
 {
  "input": "This is an awkward middle ground between a finite context, where a single value has a non-zero probability, and a continuous one.",
  "translatedText": "Це незручна середина між скінченним контекстом, де одне значення має ненульову ймовірність, і безперервним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 436.48,
  "end": 444.38
 },
 {
  "input": "where probabilities are found according to areas under the appropriate density function.",
  "translatedText": "де ймовірності знайдено відповідно до площ під відповідною функцією густини.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 444.64,
  "end": 448.68
 },
 {
  "input": "This is the sort of thing that measure theory handles very smoothly.",
  "translatedText": "Це те, з чим теорія вимірювання справляється дуже легко.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.46,
  "end": 452.6
 },
 {
  "input": "I mention this mainly for the especially curious viewer, and you can find more reading material in the description.",
  "translatedText": "Я згадую це переважно для особливо допитливого глядача, і ви можете знайти більше матеріалів для читання в описі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.04,
  "end": 458.12
 },
 {
  "input": "It's a pretty common rule of thumb that if you find yourself using a sum in a discrete context, then use an integral in the continuous context, which is the tool from calculus that we use to find areas under curves.",
  "translatedText": "Це досить поширене емпіричне правило: якщо ви використовуєте суму в дискретному контексті, тоді використовуйте інтеграл у безперервному контексті, який є інструментом числення, який ми використовуємо для знаходження площ під кривими.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 460.62,
  "end": 471.8
 },
 {
  "input": "In fact, you could argue this video would be way shorter if I just said that at the front and called it good.",
  "translatedText": "Насправді, ви можете стверджувати, що це відео було б набагато коротшим, якби я просто сказав це на початку та назвав його хорошим.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 471.8,
  "end": 477.04
 },
 {
  "input": "For my part though, I always found it a little unsatisfying to do this blindly without thinking through what it really means.",
  "translatedText": "Зі свого боку, я завжди вважав це трохи незадовільним робити це наосліп, не обмірковуючи, що це насправді означає.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.76,
  "end": 483.28
 },
 {
  "input": "And in fact, if you really dig into the theoretical underpinnings of integrals, what you'd find is that in addition to the way that it's defined in a typical intro calculus class, there is a separate more powerful definition that's based on measure theory, this formal foundation of probability.",
  "translatedText": "І насправді, якщо ви дійсно заглибитесь у теоретичні основи інтегралів, то побачите, що на додаток до того, як його визначають у типовому вступному курсі математичного аналізу, існує окреме більш потужне визначення, яке базується на теорії міри, формальному фундаменті ймовірності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.08,
  "end": 499.02
 },
 {
  "input": "If I look back to when I first learned probability, I definitely remember grappling with this weird idea that in continuous settings, like random variables that are real numbers or throwing a dart at a dartboard, you have a bunch of outcomes that are possible, and yet each one has a probability of zero, and somehow altogether they have a probability of one.",
  "translatedText": "Якщо я згадаю, коли я вперше вивчав теорію ймовірностей, я точно пам'ятаю, як боровся з цією дивною ідеєю, що в безперервних умовах, як випадкові величини, які є дійсними числами, або кидання дротика в дартс, у вас є купа можливих результатів, але кожен з них має ймовірність нуля, а всі разом вони мають ймовірність одиниці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 500.28,
  "end": 519.56
 },
 {
  "input": "Now one step of coming to terms with this is to realize that possibility is better tied to probability density than probability, but just swapping out sums of one for integrals of the others never quite scratched the itch for me.",
  "translatedText": "Тепер одним із кроків до розуміння цього є усвідомлення того, що можливість краще прив'язана до щільності ймовірності, ніж ймовірність, але просто замінити суми одних на інтеграли інших ніколи не викликало у мене свербіння.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 520.82,
  "end": 532.82
 },
 {
  "input": "I remember that it only really clicked when I realized that the rules for combining probabilities of different sets were not quite what I thought they were, and there was simply a different axiom system underlying it all.",
  "translatedText": "Я пам’ятаю, що це по-справжньому клацнуло лише тоді, коли я зрозумів, що правила об’єднання ймовірностей різних наборів не зовсім такі, як я думав, і в основі всього цього лежить просто інша система аксіом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 533.28,
  "end": 543.24
 },
 {
  "input": "But anyway, steering away from the theory somewhere back in the loose direction of application, look back to our original question about the coin with an unknown weight.",
  "translatedText": "Але як би там не було, відійшовши від теорії кудись назад у вільний напрямок застосування, повернімося до нашого початкового питання про монету з невідомою вагою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 544.58,
  "end": 552.44
 },
 {
  "input": "What we've learned here is that the right question to ask is, what's the probability density function that describes this value h after seeing the outcomes of a few tosses?",
  "translatedText": "Те, що ми дізналися тут, полягає в тому, що правильно поставити запитання: яка функція щільності ймовірності описує це значення h після перегляду результатів кількох підкидань?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 552.96,
  "end": 562.96
 },
 {
  "input": "If you can find that PDF, you can use it to answer questions like, what's the probability that the true probability of flipping heads falls between 0.6 and 0.8?",
  "translatedText": "Якщо ви знайдете цей PDF-файл, ви можете використати його, щоб відповісти на запитання, наприклад, яка ймовірність того, що справжня ймовірність перекидання голови падає між 0.6 і 0.8?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 563.46,
  "end": 572.8
 },
 {
  "input": "To find that PDF, join me in the next part.",
  "translatedText": "Щоб знайти цей PDF-файл, приєднайтеся до мене в наступній частині.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.68,
  "end": 576.06
 }
]