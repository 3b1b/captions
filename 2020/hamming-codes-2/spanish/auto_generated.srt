1
00:00:00,000 --> 00:00:02,560
Supongo que todos aquí vienen de la parte 1.

2
00:00:03,060 --> 00:00:06,612
Estábamos hablando de códigos Hamming, una forma de crear un bloque de datos 

3
00:00:06,612 --> 00:00:09,473
donde la mayoría de los bits llevan un mensaje significativo, 

4
00:00:09,473 --> 00:00:12,565
mientras que algunos otros actúan como una especie de redundancia, 

5
00:00:12,565 --> 00:00:16,671
de tal manera que si se voltea algún bit, ya sea un mensaje bit o un bit de redundancia, 

6
00:00:16,671 --> 00:00:20,317
cualquier cosa en este bloque, un receptor podrá identificar que hubo un error 

7
00:00:20,317 --> 00:00:21,240
y cómo solucionarlo.

8
00:00:21,880 --> 00:00:24,631
La idea básica presentada allí fue cómo utilizar múltiples comprobaciones 

9
00:00:24,631 --> 00:00:27,160
de paridad para realizar una búsqueda binaria hasta llegar al error.

10
00:00:28,980 --> 00:00:31,912
En ese vídeo, el objetivo era hacer que los códigos Hamming 

11
00:00:31,912 --> 00:00:34,600
se sintieran lo más prácticos y redescubribles posible.

12
00:00:35,180 --> 00:00:39,588
Pero cuando empiezas a pensar en implementar esto, ya sea en software o hardware, 

13
00:00:39,588 --> 00:00:43,460
ese marco puede subestimar lo elegantes que son realmente estos códigos.

14
00:00:43,920 --> 00:00:47,178
Podría pensar que necesita escribir un algoritmo que realice un seguimiento 

15
00:00:47,178 --> 00:00:50,264
de todas las posibles ubicaciones de error y corte ese grupo a la mitad 

16
00:00:50,264 --> 00:00:53,480
con cada verificación, pero en realidad es mucho, mucho más simple que eso.

17
00:00:53,940 --> 00:00:57,266
Si lees las respuestas a las cuatro comprobaciones de paridad 

18
00:00:57,266 --> 00:01:01,021
que hicimos en el último vídeo, todas como 1 y 0 en lugar de sí y no, 

19
00:01:01,021 --> 00:01:04,080
literalmente se detalla la posición del error en binario.

20
00:01:04,780 --> 00:01:08,314
Por ejemplo, el número 7 en binario se parece a 0111, 

21
00:01:08,314 --> 00:01:11,260
lo que básicamente dice que es 4 más 2 más 1.

22
00:01:12,540 --> 00:01:18,765
Y observen dónde se ubica la posición 7, afecta al primero de nuestros grupos de paridad, 

23
00:01:18,765 --> 00:01:21,740
al segundo y al tercero, pero no al último.

24
00:01:22,220 --> 00:01:24,995
Entonces, leer los resultados de esas cuatro comprobaciones 

25
00:01:24,995 --> 00:01:27,540
de abajo hacia arriba sí explica la posición del error.

26
00:01:28,320 --> 00:01:32,192
No hay nada especial en el ejemplo 7, funciona en general y hace que la lógica 

27
00:01:32,192 --> 00:01:35,820
para implementar todo el esquema en hardware sea sorprendentemente simple.

28
00:01:37,240 --> 00:01:40,276
Ahora, si quieres ver por qué ocurre esta magia, 

29
00:01:40,276 --> 00:01:43,993
toma estas 16 etiquetas de índice para nuestras posiciones, 

30
00:01:43,993 --> 00:01:48,516
pero en lugar de escribirlas en base 10, escribámoslas todas en binario, 

31
00:01:48,516 --> 00:01:49,880
desde 0000 hasta 1111.

32
00:01:50,559 --> 00:01:53,952
Mientras volvemos a colocar estas etiquetas binarias en sus cajas, 

33
00:01:53,952 --> 00:01:57,800
permítanme enfatizar que son distintas de los datos que realmente se envían.

34
00:01:58,320 --> 00:02:00,870
No son más que una etiqueta conceptual para ayudarnos a usted y 

35
00:02:00,870 --> 00:02:03,500
a mí a comprender de dónde provienen los cuatro grupos de paridad.

36
00:02:04,140 --> 00:02:08,298
La elegancia de que todo lo que estamos viendo se describa en binario tal vez se vea 

37
00:02:08,298 --> 00:02:12,360
socavada por la confusión de que todo lo que estamos viendo se describa en binario.

38
00:02:13,020 --> 00:02:14,120
Pero vale la pena.

39
00:02:14,800 --> 00:02:18,661
Concentre su atención solo en la última parte de todas estas 

40
00:02:18,661 --> 00:02:23,220
etiquetas y luego resalte las posiciones donde esa última parte es un 1.

41
00:02:24,240 --> 00:02:27,766
Lo que obtenemos es el primero de nuestros cuatro grupos de paridad, 

42
00:02:27,766 --> 00:02:32,162
lo que significa que puedes interpretar esa primera verificación como si preguntaras, 

43
00:02:32,162 --> 00:02:35,740
oye, si hay un error, ¿el último bit en la posición de ese error es 1?

44
00:02:38,200 --> 00:02:42,002
De manera similar, si se concentra en el penúltimo bit y resalta todas las 

45
00:02:42,002 --> 00:02:46,160
posiciones donde es un 1, obtendrá el segundo grupo de paridad de nuestro esquema.

46
00:02:46,740 --> 00:02:50,402
En otras palabras, esa segunda verificación pregunta, oye, 

47
00:02:50,402 --> 00:02:54,500
otra vez, si hay un error, ¿el penúltimo bit de esa posición es 1?

48
00:02:55,760 --> 00:02:56,900
Etcétera.

49
00:02:57,220 --> 00:03:01,166
La tercera verificación de paridad cubre todas las posiciones 

50
00:03:01,166 --> 00:03:06,321
cuyo penúltimo bit está activado, y la última cubre las últimas ocho posiciones, 

51
00:03:06,321 --> 00:03:08,740
aquellas cuyo bit de mayor orden es 1.

52
00:03:09,740 --> 00:03:14,021
Todo lo que hicimos antes es lo mismo que responder estas cuatro preguntas, 

53
00:03:14,021 --> 00:03:17,740
lo que a su vez es lo mismo que deletrear una posición en binario.

54
00:03:19,620 --> 00:03:21,480
Espero que esto aclare dos cosas.

55
00:03:22,040 --> 00:03:24,206
La primera es cómo generalizar sistemáticamente a 

56
00:03:24,206 --> 00:03:26,460
tamaños de bloques que son potencias de dos mayores.

57
00:03:26,960 --> 00:03:29,773
Si se necesitan más bits para describir cada posición, 

58
00:03:29,773 --> 00:03:32,945
como seis bits para describir 64 puntos, entonces cada uno de 

59
00:03:32,945 --> 00:03:36,680
esos bits le dará uno de los grupos de paridad que necesitamos verificar.

60
00:03:38,400 --> 00:03:40,755
Aquellos de ustedes que vieron el rompecabezas de tablero de ajedrez 

61
00:03:40,755 --> 00:03:43,180
que hice con Matt Parker pueden encontrar todo esto sumamente familiar.

62
00:03:43,660 --> 00:03:46,154
Es la misma lógica central, pero resolviendo un problema 

63
00:03:46,154 --> 00:03:48,780
diferente y aplicada a un tablero de ajedrez de 64 casillas.

64
00:03:49,880 --> 00:03:53,703
Lo segundo que espero que esto aclare es por qué nuestros bits de paridad 

65
00:03:53,703 --> 00:03:57,320
están en posiciones que son potencias de dos, por ejemplo 1, 2, 4 y 8.

66
00:03:58,000 --> 00:04:03,000
Estas son las posiciones cuya representación binaria tiene un solo bit activado.

67
00:04:03,600 --> 00:04:06,462
Lo que eso significa es que cada uno de esos bits de paridad se 

68
00:04:06,462 --> 00:04:09,460
encuentra dentro de uno y sólo uno de los cuatro grupos de paridad.

69
00:04:12,040 --> 00:04:16,112
También puede ver esto en ejemplos más grandes, donde no importa cuán grande sea, 

70
00:04:16,112 --> 00:04:19,339
cada bit de paridad toca convenientemente solo uno de los grupos.

71
00:04:25,600 --> 00:04:29,128
Una vez que comprenda que estas comprobaciones de paridad en las que hemos centrado 

72
00:04:29,128 --> 00:04:32,530
gran parte de nuestro tiempo no son más que una forma inteligente de explicar la 

73
00:04:32,530 --> 00:04:36,016
posición de un error en binario, entonces podremos establecer una conexión con una 

74
00:04:36,016 --> 00:04:38,242
forma diferente de pensar sobre el hamming. códigos, 

75
00:04:38,242 --> 00:04:40,510
uno que posiblemente sea mucho más simple y elegante, 

76
00:04:40,510 --> 00:04:43,240
y que básicamente se puede escribir con una sola línea de código.

77
00:04:43,660 --> 00:04:45,500
Se basa en la función XOR.

78
00:04:46,940 --> 00:04:50,220
XOR, para aquellos que no lo saben, significa exclusivo o.

79
00:04:50,780 --> 00:04:56,375
Cuando tomas el XOR de dos bits, devolverá un 1 si cualquiera de esos bits está activado, 

80
00:04:56,375 --> 00:04:59,360
pero no si ambos están activados o desactivados.

81
00:05:00,100 --> 00:05:02,980
Dicho de otra manera, es la paridad de estos dos bits.

82
00:05:03,540 --> 00:05:06,760
Como persona de matemáticas, prefiero pensar en ello como la suma mod 2.

83
00:05:07,360 --> 00:05:10,869
También hablamos comúnmente del XOR de dos cadenas de bits diferentes, 

84
00:05:10,869 --> 00:05:13,440
que básicamente hace esto componente por componente.

85
00:05:13,680 --> 00:05:15,720
Es como una suma, pero donde nunca se lleva.

86
00:05:16,500 --> 00:05:19,317
Nuevamente, los más inclinados a las matemáticas podrían 

87
00:05:19,317 --> 00:05:22,480
preferir pensar en esto como sumar dos vectores y reducir mod 2.

88
00:05:23,500 --> 00:05:26,375
Si abre algo de Python ahora mismo y aplica la operación de 

89
00:05:26,375 --> 00:05:29,777
intercalación entre dos números enteros, esto es lo que está haciendo, 

90
00:05:29,777 --> 00:05:32,940
pero en las representaciones de bits de esos números bajo el capó.

91
00:05:34,960 --> 00:05:39,055
El punto clave para usted y para mí es que tomar el XOR de muchas cadenas de 

92
00:05:39,055 --> 00:05:42,991
bits diferentes es efectivamente una forma de calcular las parodias de un 

93
00:05:42,991 --> 00:05:47,140
grupo de grupos separados, como ocurre con las columnas, todo de una sola vez.

94
00:05:51,260 --> 00:05:53,884
Esto nos da una forma bastante elegante de pensar en las múltiples 

95
00:05:53,884 --> 00:05:56,430
comprobaciones de paridad de nuestro algoritmo de código Hamming 

96
00:05:56,430 --> 00:05:58,780
como si estuvieran todas empaquetadas en una sola operación.

97
00:05:59,479 --> 00:06:02,180
Aunque a primera vista parece muy diferente.

98
00:06:02,820 --> 00:06:07,840
Escriba específicamente las 16 posiciones en binario, como lo hicimos antes, 

99
00:06:07,840 --> 00:06:12,535
y ahora resalte las posiciones donde el bit del mensaje se activa en 1, 

100
00:06:12,535 --> 00:06:17,100
y luego recopile estas posiciones en una columna grande y tome el XOR.

101
00:06:19,260 --> 00:06:21,523
Probablemente puedas adivinar que, como resultado, 

102
00:06:21,523 --> 00:06:24,762
los 4 bits que se encuentran en la parte inferior son los mismos que los 

103
00:06:24,762 --> 00:06:26,848
4 controles de paridad que conocemos y amamos, 

104
00:06:26,848 --> 00:06:29,200
pero tómate un momento para pensar realmente por qué.

105
00:06:32,220 --> 00:06:36,881
Esta última columna, por ejemplo, cuenta todas las posiciones cuyo último bit es 1, 

106
00:06:36,881 --> 00:06:40,210
pero ya estamos limitados solo a las posiciones resaltadas, 

107
00:06:40,210 --> 00:06:44,816
por lo que efectivamente cuenta cuántas posiciones resaltadas provienen del primer 

108
00:06:44,816 --> 00:06:45,760
grupo de paridad.

109
00:06:46,240 --> 00:06:46,800
¿Tiene sentido?

110
00:06:49,080 --> 00:06:52,739
Asimismo, la siguiente columna cuenta cuántas posiciones hay en 

111
00:06:52,739 --> 00:06:56,855
el segundo grupo de paridad, las posiciones cuyo penúltimo bit es un 1, 

112
00:06:56,855 --> 00:07:00,000
y cuáles también están resaltadas, y así sucesivamente.

113
00:07:00,260 --> 00:07:02,363
En realidad, es solo un pequeño cambio de perspectiva 

114
00:07:02,363 --> 00:07:03,960
sobre lo mismo que hemos estado haciendo.

115
00:07:07,760 --> 00:07:09,600
Y entonces sabes a dónde va desde aquí.

116
00:07:10,000 --> 00:07:12,860
El remitente es responsable de alternar algunos de los bits de 

117
00:07:12,860 --> 00:07:15,720
paridad especiales para asegurarse de que la suma resulte 0000.

118
00:07:15,720 --> 00:07:21,516
Ahora, una vez que lo tenemos así, nos da una muy buena manera de pensar por qué estos 

119
00:07:21,516 --> 00:07:27,180
cuatro bits resultantes en la parte inferior explican directamente la posición de un 

120
00:07:27,180 --> 00:07:27,580
error.

121
00:07:28,460 --> 00:07:31,860
Digamos que una parte de este bloque se cambia de 0 a 1.

122
00:07:32,600 --> 00:07:38,309
Lo que eso significa es que la posición de ese bit ahora se incluirá en el XOR total, 

123
00:07:38,309 --> 00:07:43,820
lo que cambia la suma de 0 a ser este valor recién incluido, la posición del error.

124
00:07:44,460 --> 00:07:49,360
Un poco menos obvio, lo mismo ocurre si hay un error que cambia un 1 por un 0.

125
00:07:50,180 --> 00:07:54,552
Verás, si sumas una cadena de bits dos veces, es lo mismo que no tenerla ahí, 

126
00:07:54,552 --> 00:07:57,580
básicamente porque en este mundo 1 más 1 es igual a 0.

127
00:07:57,580 --> 00:08:00,904
Entonces, agregar una copia de esta posición a 

128
00:08:00,904 --> 00:08:04,300
la suma total tiene el mismo efecto que moverla.

129
00:08:05,160 --> 00:08:07,981
Y ese efecto, nuevamente, es que el resultado total en 

130
00:08:07,981 --> 00:08:10,700
la parte inferior aquí explica la posición del error.

131
00:08:13,039 --> 00:08:17,240
Para ilustrar lo elegante que es esto, permítanme mostrarles una línea de código Python 

132
00:08:17,240 --> 00:08:21,440
a la que hice referencia antes, que capturará casi toda la lógica del lado del receptor.

133
00:08:22,080 --> 00:08:26,525
Comenzaremos creando una matriz aleatoria de 16 1 y 0 para simular el bloque de datos, 

134
00:08:26,525 --> 00:08:29,029
y le daré el nombre de bits, pero, por supuesto, 

135
00:08:29,029 --> 00:08:32,350
en la práctica esto sería algo que recibiríamos de un remitente, 

136
00:08:32,350 --> 00:08:36,080
y en lugar de al ser aleatorio, transportaría 11 bits de datos junto con 

137
00:08:36,080 --> 00:08:37,000
5 bits de paridad.

138
00:08:37,000 --> 00:08:41,900
Si llamo a la función enumerateBits, lo que hace es emparejar cada uno de 

139
00:08:41,900 --> 00:08:47,000
esos bits con un índice correspondiente, en este caso yendo desde 0 hasta 15.

140
00:08:48,180 --> 00:08:52,122
Entonces, si luego creamos una lista que recorre todos estos pares, 

141
00:08:52,122 --> 00:08:56,760
pares que se parecen a i, y luego extraemos solo el valor de i, solo el índice, 

142
00:08:56,760 --> 00:09:01,340
bueno, no es tan emocionante, simplemente recuperamos esos índices del 0 al 15.

143
00:09:01,680 --> 00:09:06,011
Pero si agregamos la condición de hacer esto solo si el bit, es decir, 

144
00:09:06,011 --> 00:09:10,829
si ese bit es un 1 y no un 0, entonces extrae solo las posiciones donde el bit 

145
00:09:10,829 --> 00:09:12,660
correspondiente está activado.

146
00:09:13,380 --> 00:09:20,360
En este caso parece que esas posiciones son 0, 4, 6, 9, etc.

147
00:09:20,720 --> 00:09:23,275
Lo que queremos es reunir todas esas posiciones, 

148
00:09:23,275 --> 00:09:27,240
las posiciones de los bits que están activados, y luego realizar XOR juntas.

149
00:09:29,180 --> 00:09:33,220
Para hacer esto en Python, permítanme primero importar un par de funciones útiles.

150
00:09:33,900 --> 00:09:36,431
De esa manera podemos llamar a reduce() en esta 

151
00:09:36,431 --> 00:09:38,700
lista y usar la función XOR para reducirla.

152
00:09:39,100 --> 00:09:42,680
Básicamente, esto se abre camino a través de la lista, llevando XOR a lo largo del camino.

153
00:09:44,800 --> 00:09:46,988
Si lo prefiere, puede escribir explícitamente esa 

154
00:09:46,988 --> 00:09:49,440
función XOR sin tener que importarla desde ningún lugar.

155
00:09:51,940 --> 00:09:56,448
Entonces, por el momento parece que si hacemos esto en nuestro bloque 

156
00:09:56,448 --> 00:10:01,280
aleatorio de 16 bits, devuelve 9, que tiene la representación binaria 1001.

157
00:10:01,980 --> 00:10:05,232
No lo haremos aquí, pero podría escribir una función en la que el remitente 

158
00:10:05,232 --> 00:10:08,655
use esa representación binaria para establecer los cuatro bits de paridad según 

159
00:10:08,655 --> 00:10:12,036
sea necesario y, en última instancia, llevar este bloque a un estado en el que 

160
00:10:12,036 --> 00:10:15,460
la ejecución de esta línea de código en la lista completa de bits devuelva un 0.

161
00:10:16,080 --> 00:10:20,100
Esto se consideraría un bloque bien preparado.

162
00:10:20,100 --> 00:10:24,077
Lo bueno es que si alternamos cualquiera de los bits en esta lista, 

163
00:10:24,077 --> 00:10:29,050
simulando un error aleatorio debido al ruido, si ejecuta esta misma línea de código, 

164
00:10:29,050 --> 00:10:30,220
imprimirá ese error.

165
00:10:30,960 --> 00:10:31,520
¿No es genial?

166
00:10:31,820 --> 00:10:36,378
Podrías obtener este bloque de la nada, ejecutar esta única línea en él y 

167
00:10:36,378 --> 00:10:41,060
automáticamente mostrará la posición de un error, o un 0 si no hubo ninguno.

168
00:10:42,500 --> 00:10:44,840
Y aquí la talla 16 no tiene nada de especial.

169
00:10:44,840 --> 00:10:49,860
La misma línea de código funcionaría si tuviera una lista de, digamos, 256 bits.

170
00:10:51,880 --> 00:10:54,788
No hace falta decir que hay más código para escribir aquí, 

171
00:10:54,788 --> 00:10:58,485
como hacer la verificación de metaparidad para detectar errores de 2 bits, 

172
00:10:58,485 --> 00:11:02,429
pero la idea es que casi toda la lógica central de nuestro esquema se reduzca a 

173
00:11:02,429 --> 00:11:03,760
una única reducción de XOR.

174
00:11:06,120 --> 00:11:10,635
Ahora, dependiendo de su comodidad con los binarios, los XOR y el software en general, 

175
00:11:10,635 --> 00:11:14,735
puede encontrar esta perspectiva un poco confusa o mucho más elegante y simple 

176
00:11:14,735 --> 00:11:18,420
que se pregunte por qué no comenzamos con ella desde el principio. -ir.

177
00:11:19,140 --> 00:11:22,898
En términos generales, es más fácil pensar en la perspectiva de verificación de paridad 

178
00:11:22,898 --> 00:11:26,400
múltiple cuando se implementan códigos Hamming en hardware de manera muy directa, 

179
00:11:26,400 --> 00:11:29,475
y es más fácil pensar en la perspectiva XOR cuando se hace en software, 

180
00:11:29,475 --> 00:11:30,500
desde un nivel superior.

181
00:11:31,360 --> 00:11:36,102
El primero es más fácil de hacer a mano, y creo que hace un mejor trabajo al inculcar 

182
00:11:36,102 --> 00:11:40,845
la intuición central subyacente a todo esto, que es que la información requerida para 

183
00:11:40,845 --> 00:11:45,367
localizar un solo error está relacionada con el registro del tamaño del bloque. , 

184
00:11:45,367 --> 00:11:50,000
o en otras palabras, crece poco a poco a medida que se duplica el tamaño del bloque.

185
00:11:51,020 --> 00:11:53,276
El hecho relevante aquí es que esa información 

186
00:11:53,276 --> 00:11:56,060
corresponde directamente a cuánta redundancia necesitamos.

187
00:11:56,660 --> 00:11:59,733
Eso es realmente lo que va en contra de la reacción instintiva de la mayoría de las 

188
00:11:59,733 --> 00:12:02,844
personas cuando piensan por primera vez en hacer que un mensaje sea resistente a los 

189
00:12:02,844 --> 00:12:06,137
errores, donde normalmente copiar el mensaje completo es el primer instinto que les viene 

190
00:12:06,137 --> 00:12:06,540
a la mente.

191
00:12:07,500 --> 00:12:10,769
Y luego, por cierto, existe otra forma completamente distinta en la que a veces se 

192
00:12:10,769 --> 00:12:14,000
presentan los códigos Hamming, donde se multiplica el mensaje por una gran matriz.

193
00:12:14,670 --> 00:12:18,839
Es algo bueno porque lo relaciona con la familia más amplia de códigos lineales, 

194
00:12:18,839 --> 00:12:23,060
pero creo que eso casi no da ninguna intuición sobre de dónde viene o cómo escala.

195
00:12:25,200 --> 00:12:28,180
Y hablando de escalamiento, es posible que notes que la eficiencia de 

196
00:12:28,180 --> 00:12:31,160
este esquema solo mejora a medida que aumentamos el tamaño del bloque.

197
00:12:35,000 --> 00:12:38,769
Por ejemplo, vimos que con 256 bits, se utiliza solo el 3% de 

198
00:12:38,769 --> 00:12:42,660
ese espacio para redundancia, y a partir de ahí sigue mejorando.

199
00:12:43,300 --> 00:12:45,716
A medida que el número de bits de paridad crece uno por uno, 

200
00:12:45,716 --> 00:12:47,340
el tamaño del bloque se sigue duplicando.

201
00:12:49,000 --> 00:12:52,526
Y si lleva eso al extremo, podría tener un bloque con, digamos, 

202
00:12:52,526 --> 00:12:56,218
un millón de bits, donde literalmente estaría jugando 20 preguntas 

203
00:12:56,218 --> 00:13:00,020
con sus comprobaciones de paridad, y utiliza sólo 21 bits de paridad.

204
00:13:00,740 --> 00:13:05,360
Y si das un paso atrás y piensas en mirar un millón de bits y localizar un solo error, 

205
00:13:05,360 --> 00:13:07,060
eso realmente parece una locura.

206
00:13:08,199 --> 00:13:11,388
El problema, por supuesto, es que con un bloque más grande, 

207
00:13:11,388 --> 00:13:14,790
la probabilidad de ver más de uno o dos errores de bit aumenta, 

208
00:13:14,790 --> 00:13:17,660
y los códigos Hamming no manejan nada más allá de eso.

209
00:13:18,320 --> 00:13:21,256
Entonces, en la práctica, lo que querrás es encontrar el tamaño correcto para que 

210
00:13:21,256 --> 00:13:24,300
la probabilidad de que se produzcan demasiados cambios de bits no sea demasiado alta.

211
00:13:26,600 --> 00:13:30,219
Además, en la práctica, los errores tienden a ocurrir en pequeñas ráfagas, 

212
00:13:30,219 --> 00:13:33,790
lo que arruinaría totalmente un solo bloque, por lo que una táctica común 

213
00:13:33,790 --> 00:13:37,602
para ayudar a distribuir una ráfaga de errores entre muchos bloques diferentes 

214
00:13:37,602 --> 00:13:40,980
es entrelazar esos bloques, así, antes de que se enviado o almacenado.

215
00:13:45,580 --> 00:13:48,676
Por otra parte, mucho de esto se vuelve completamente discutible con 

216
00:13:48,676 --> 00:13:52,536
códigos más modernos, como el algoritmo Reed-Solomon, mucho más comúnmente utilizado, 

217
00:13:52,536 --> 00:13:55,812
que maneja particularmente bien los errores de ráfaga y se puede ajustar 

218
00:13:55,812 --> 00:13:58,820
para que sea resistente a una mayor cantidad de errores por bloque.

219
00:13:59,360 --> 00:14:01,340
Pero ese es un tema para otro momento.

220
00:14:02,500 --> 00:14:05,081
En su libro El arte de hacer ciencia e ingeniería, 

221
00:14:05,081 --> 00:14:08,421
Hamming es maravillosamente sincero acerca de cuán sinuoso fue su 

222
00:14:08,421 --> 00:14:09,940
descubrimiento de este código.

223
00:14:10,620 --> 00:14:14,130
Primero probó todo tipo de esquemas diferentes que implicaban organizar los 

224
00:14:14,130 --> 00:14:17,780
bits en partes de una red de dimensiones superiores y cosas extrañas como esta.

225
00:14:18,300 --> 00:14:21,324
La idea de que podría ser posible lograr que los controles de paridad 

226
00:14:21,324 --> 00:14:24,564
conspiraran de una manera que detallara la posición de un error solo se le 

227
00:14:24,564 --> 00:14:28,366
ocurrió a Hamming cuando dio un paso atrás después de muchos otros análisis y preguntó, 

228
00:14:28,366 --> 00:14:31,520
bueno, ¿qué es lo más eficiente que puedo? posiblemente se trate de esto?

229
00:14:32,620 --> 00:14:35,423
También fue sincero acerca de lo importante que era que ya 

230
00:14:35,423 --> 00:14:38,274
tuviera en mente los controles de paridad, que habrían sido 

231
00:14:38,274 --> 00:14:41,220
mucho menos comunes en la década de 1940 de lo que lo son hoy.

232
00:14:41,920 --> 00:14:44,904
Hay como media docena de veces a lo largo de este libro en las que hace 

233
00:14:44,904 --> 00:14:48,220
referencia a la cita de Louis Pasteur: La suerte favorece a una mente preparada.

234
00:14:49,320 --> 00:14:52,640
Las ideas inteligentes a menudo parecen engañosamente simples en retrospectiva, 

235
00:14:52,640 --> 00:14:54,300
lo que hace que sea fácil subestimarlas.

236
00:14:54,960 --> 00:14:58,153
En este momento mi sincera esperanza es que los códigos de Hamming, 

237
00:14:58,153 --> 00:15:01,300
o al menos la posibilidad de tales códigos, les parezca casi obvio.

238
00:15:01,660 --> 00:15:05,064
Pero no deberías engañarte pensando que en realidad son obvios, 

239
00:15:05,064 --> 00:15:06,820
porque definitivamente no lo son.

240
00:15:07,880 --> 00:15:11,568
Parte de la razón por la que las ideas inteligentes parecen engañosamente fáciles 

241
00:15:11,568 --> 00:15:14,942
es que sólo vemos el resultado final, limpiando lo que estaba desordenado, 

242
00:15:14,942 --> 00:15:17,146
sin mencionar nunca todos los giros equivocados, 

243
00:15:17,146 --> 00:15:20,835
subestimando cuán vasto es el espacio de posibilidades explorables al comienzo de 

244
00:15:20,835 --> 00:15:22,860
un problema. proceso de resolución, todo eso.

245
00:15:23,820 --> 00:15:24,900
Pero esto es cierto en general.

246
00:15:24,900 --> 00:15:27,373
Creo que para algunos inventos especiales, hay una 

247
00:15:27,373 --> 00:15:30,040
segunda razón más profunda por la que los subestimamos.

248
00:15:30,840 --> 00:15:33,588
Pensar en la información en términos de bits no se había convertido 

249
00:15:33,588 --> 00:15:36,053
realmente en una teoría completa hasta 1948, con el artículo 

250
00:15:36,053 --> 00:15:38,640
fundamental de Claude Shannon sobre la teoría de la información.

251
00:15:39,280 --> 00:15:42,540
Esto fue esencialmente coincidente con el momento en que Hamming desarrolló su algoritmo.

252
00:15:43,300 --> 00:15:46,305
Este fue el mismo artículo fundamental que demostró, en cierto sentido, 

253
00:15:46,305 --> 00:15:48,809
que siempre es posible una corrección de errores eficiente, 

254
00:15:48,809 --> 00:15:52,106
sin importar cuán alta sea la probabilidad de que se produzcan cambios de bit, 

255
00:15:52,106 --> 00:15:52,900
al menos en teoría.

256
00:15:53,700 --> 00:15:57,015
Shannon y Hamming, por cierto, compartían oficina en Bell Labs, 

257
00:15:57,015 --> 00:16:01,160
a pesar de trabajar en cosas muy diferentes, lo que aquí no parece coincidencia.

258
00:16:02,380 --> 00:16:07,417
Varias décadas después, hoy en día muchos de nosotros estamos tan inmersos en pensar en 

259
00:16:07,417 --> 00:16:12,340
bits e información que es fácil pasar por alto cuán distinta era esta forma de pensar.

260
00:16:13,100 --> 00:16:17,535
Irónicamente, las ideas que moldean más profundamente la forma en que piensa 

261
00:16:17,535 --> 00:16:22,260
una generación futura terminarán pareciéndole más simples de lo que realmente son.

