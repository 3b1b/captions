1
00:00:00,000 --> 00:00:02,560
I'm assuming that everybody here is coming from part 1.

2
00:00:03,060 --> 00:00:06,506
We were talking about Hamming codes, a way to create a block of data 

3
00:00:06,506 --> 00:00:09,053
where most of the bits carry a meaningful message, 

4
00:00:09,053 --> 00:00:11,450
while a few others act as a kind of redundancy, 

5
00:00:11,450 --> 00:00:15,746
in such a way that if any bit gets flipped, either a message bit or a redundancy bit, 

6
00:00:15,746 --> 00:00:19,342
anything in this block, a receiver is going to be able to identify that 

7
00:00:19,342 --> 00:00:21,240
there was an error, and how to fix it.

8
00:00:21,880 --> 00:00:24,449
The basic idea presented there was how to use multiple 

9
00:00:24,449 --> 00:00:27,160
parity checks to binary search your way down to the error.

10
00:00:28,980 --> 00:00:31,847
In that video, the goal was to make Hamming codes 

11
00:00:31,847 --> 00:00:34,600
feel as hands-on and rediscoverable as possible.

12
00:00:35,180 --> 00:00:38,227
But as you start to think about actually implementing this, 

13
00:00:38,227 --> 00:00:42,291
either in software or hardware, that framing may actually undersell how elegant 

14
00:00:42,291 --> 00:00:43,460
these codes really are.

15
00:00:43,920 --> 00:00:47,008
You might think that you need to write an algorithm that keeps 

16
00:00:47,008 --> 00:00:51,273
track of all the possible error locations and cuts that group in half with each check, 

17
00:00:51,273 --> 00:00:53,480
but it's actually way, way simpler than that.

18
00:00:53,940 --> 00:00:58,254
If you read out the answers to the four parity checks we did in the last video, 

19
00:00:58,254 --> 00:01:00,843
all as ones and zeros instead of yeses and nos, 

20
00:01:00,843 --> 00:01:04,080
it literally spells out the position of the error in binary.

21
00:01:04,780 --> 00:01:08,284
For example, the number 7 in binary looks like 0111, 

22
00:01:08,284 --> 00:01:11,260
essentially saying that it's 4 plus 2 plus 1.

23
00:01:12,540 --> 00:01:14,460
And notice where the position 7 sits.

24
00:01:14,840 --> 00:01:18,253
It does affect the first of our parity groups, 

25
00:01:18,253 --> 00:01:21,740
and the second, and the third, but not the last.

26
00:01:22,220 --> 00:01:24,903
So reading the results of those four checks from bottom 

27
00:01:24,903 --> 00:01:27,540
to top indeed does spell out the position of the error.

28
00:01:28,320 --> 00:01:31,140
There's nothing special about the example 7, this works in general.

29
00:01:31,780 --> 00:01:35,820
This makes the logic for implementing the whole scheme in hardware shockingly simple.

30
00:01:37,240 --> 00:01:40,302
Now if you want to see why this magic happens, 

31
00:01:40,302 --> 00:01:45,905
take these 16 index labels for our positions, but instead of writing them in base 10, 

32
00:01:45,905 --> 00:01:49,880
let's write them all in binary, running from 0000 up to 1111.

33
00:01:50,560 --> 00:01:53,423
As we put these binary labels back into their boxes, 

34
00:01:53,423 --> 00:01:57,800
let me emphasize that they are distinct from the data that's actually being sent.

35
00:01:58,320 --> 00:02:00,910
They're nothing more than a conceptual label to help you 

36
00:02:00,910 --> 00:02:03,500
and me understand where the four parity groups came from.

37
00:02:04,140 --> 00:02:08,083
The elegance of having everything we're looking at be described in binary is maybe 

38
00:02:08,083 --> 00:02:12,360
undercut by the confusion of having everything we're looking at being described in binary.

39
00:02:13,020 --> 00:02:14,120
It's worth it, though.

40
00:02:14,800 --> 00:02:18,240
Focus your attention just on that last bit of all of these labels.

41
00:02:19,880 --> 00:02:23,220
And then highlight the positions where that final bit is a 1.

42
00:02:24,240 --> 00:02:27,354
What we get is the first of our four parity groups, 

43
00:02:27,354 --> 00:02:31,128
which means that you can interpret that first check as asking, 

44
00:02:31,128 --> 00:02:35,740
hey, if there's an error, is the final bit in the position of that error a 1?

45
00:02:38,200 --> 00:02:40,924
Similarly, if you focus on the second to last bit, 

46
00:02:40,924 --> 00:02:43,595
and highlight all the positions where that's a 1, 

47
00:02:43,595 --> 00:02:46,160
you get the second parity group from our scheme.

48
00:02:46,740 --> 00:02:50,377
In other words, that second check is asking, hey, me again, 

49
00:02:50,377 --> 00:02:54,500
if there's an error, is the second to last bit of that position a 1?

50
00:02:55,760 --> 00:02:56,900
And so on.

51
00:02:57,220 --> 00:03:02,683
The third parity check covers every position whose third to last bit is turned on, 

52
00:03:02,683 --> 00:03:05,975
and the last one covers the last eight positions, 

53
00:03:05,975 --> 00:03:08,740
those ones whose highest order bit is a 1.

54
00:03:09,740 --> 00:03:14,034
Everything we did earlier is the same as answering these four questions, 

55
00:03:14,034 --> 00:03:17,740
which in turn is the same as spelling out a position in binary.

56
00:03:19,620 --> 00:03:21,480
I hope this makes two things clearer.

57
00:03:22,040 --> 00:03:24,274
The first is how to systematically generalize 

58
00:03:24,274 --> 00:03:26,460
to block sizes that are bigger powers of two.

59
00:03:26,960 --> 00:03:31,937
If it takes more bits to describe each position, like six bits to describe 64 spots, 

60
00:03:31,937 --> 00:03:36,680
then each of those bits gives you one of the parity groups that we need to check.

61
00:03:38,400 --> 00:03:40,682
Those of you who watched the chessboard puzzle I did 

62
00:03:40,682 --> 00:03:43,180
with Matt Parker might find all this exceedingly familiar.

63
00:03:43,660 --> 00:03:46,742
It's the same core logic, but solving a different problem, 

64
00:03:46,742 --> 00:03:48,780
and applied to a 64-squared chessboard.

65
00:03:49,880 --> 00:03:53,393
The second thing I hope this makes clear is why our parity bits are 

66
00:03:53,393 --> 00:03:57,320
sitting in the positions that are powers of two, for example 1, 2, 4, and 8.

67
00:03:58,000 --> 00:04:03,000
These are the positions whose binary representation has just a single bit turned on.

68
00:04:03,600 --> 00:04:06,530
What that means is each of those parity bits sits 

69
00:04:06,530 --> 00:04:09,460
inside one and only one of the four parity groups.

70
00:04:12,040 --> 00:04:16,095
You can also see this in larger examples, where no matter how big you get, 

71
00:04:16,095 --> 00:04:19,339
each parity bit conveniently touches only one of the groups.

72
00:04:25,600 --> 00:04:29,156
Once you understand that these parity checks that we've focused so much of 

73
00:04:29,156 --> 00:04:32,618
our time on are nothing more than a clever way to spell out the position 

74
00:04:32,618 --> 00:04:36,127
of an error in binary, then we can draw a connection with a different way 

75
00:04:36,127 --> 00:04:40,062
to think about hamming codes, one that is arguably a lot simpler and more elegant, 

76
00:04:40,062 --> 00:04:43,240
and which can basically be written down with a single line of code.

77
00:04:43,660 --> 00:04:45,500
It's based on the XOR function.

78
00:04:46,940 --> 00:04:50,220
XOR, for those of you who don't know, stands for exclusive or.

79
00:04:50,780 --> 00:04:54,961
When you take the XOR of two bits, it's going to return a 1 if either one of 

80
00:04:54,961 --> 00:04:59,360
those bits is turned on, but not if both are turned on or if both are turned off.

81
00:05:00,100 --> 00:05:02,980
Phrased differently, it's the parity of these two bits.

82
00:05:03,540 --> 00:05:06,760
As a math person, I prefer to think about it as addition mod 2.

83
00:05:07,360 --> 00:05:10,849
We also commonly talk about the XOR of two different bit strings, 

84
00:05:10,849 --> 00:05:13,440
which basically does this component by component.

85
00:05:13,680 --> 00:05:15,720
It's like addition, but where you never carry.

86
00:05:16,500 --> 00:05:19,516
Again, the more mathematically inclined might prefer to 

87
00:05:19,516 --> 00:05:22,480
think of this as adding two vectors and reducing mod 2.

88
00:05:23,500 --> 00:05:26,715
If you open up some Python right now, and you apply the caret 

89
00:05:26,715 --> 00:05:29,672
operation between two integers, this is what it's doing, 

90
00:05:29,672 --> 00:05:32,940
but to the bit representations of those numbers under the hood.

91
00:05:34,960 --> 00:05:39,098
The key point for you and me is that taking the XOR of many different 

92
00:05:39,098 --> 00:05:44,301
bit strings is effectively a way to compute the parities of a bunch of separate groups, 

93
00:05:44,301 --> 00:05:47,140
like so with the columns, all in one fell swoop.

94
00:05:51,260 --> 00:05:54,951
This gives us a rather snazzy way to think about the multiple parity checks from 

95
00:05:54,951 --> 00:05:58,780
our Hamming code algorithm as all being packaged together into one single operation.

96
00:05:59,480 --> 00:06:02,180
Though at first glance it does look very different.

97
00:06:02,820 --> 00:06:07,494
Specifically, write down the 16 positions in binary, like we had before, 

98
00:06:07,494 --> 00:06:12,617
and now highlight only the positions where the message bit is turned on to a 1, 

99
00:06:12,617 --> 00:06:17,100
and then collect these positions into one big column and take the XOR.

100
00:06:19,260 --> 00:06:22,623
You can probably guess that the four bits sitting at the bottom as 

101
00:06:22,623 --> 00:06:26,489
a result are the same as the four parity checks we've come to know and love, 

102
00:06:26,489 --> 00:06:29,200
but take a moment to actually think about why exactly.

103
00:06:32,220 --> 00:06:37,107
This last column, for example, is counting all of the positions whose last bit is a 1, 

104
00:06:37,107 --> 00:06:40,535
but we're already limited only to the highlighted positions, 

105
00:06:40,535 --> 00:06:45,029
so it's effectively counting how many highlighted positions came from the first 

106
00:06:45,029 --> 00:06:45,760
parity group.

107
00:06:46,240 --> 00:06:46,800
Does that make sense?

108
00:06:49,080 --> 00:06:54,382
Likewise, the next column counts how many positions are in the second parity group, 

109
00:06:54,382 --> 00:07:00,000
the positions whose second to last bit is a 1, and which are also highlighted, and so on.

110
00:07:00,260 --> 00:07:03,960
It's really just a small shift in perspective on the same thing we've been doing.

111
00:07:07,760 --> 00:07:09,600
And so you know where it goes from here.

112
00:07:10,000 --> 00:07:13,425
The sender is responsible for toggling some of the special 

113
00:07:13,425 --> 00:07:16,560
parity bits to make sure the sum works out to be 0000.

114
00:07:19,040 --> 00:07:23,151
Once we have it like this, this gives us a really nice way to think about why 

115
00:07:23,151 --> 00:07:27,580
these four resulting bits at the bottom directly spell out the position of an error.

116
00:07:28,460 --> 00:07:31,860
Let's say some bit in this block gets toggled from a 0 to a 1.

117
00:07:32,600 --> 00:07:36,246
What that means is that the position of that bit is now going to 

118
00:07:36,246 --> 00:07:39,893
be included in the total XOR, which changes the sum from being 0 

119
00:07:39,893 --> 00:07:43,820
to instead being this newly included value, the position of the error.

120
00:07:44,460 --> 00:07:49,360
Slightly less obviously, the same is true if there's an error that changes a 1 to a 0.

121
00:07:50,180 --> 00:07:52,820
You see, if you add a bit string together twice, 

122
00:07:52,820 --> 00:07:56,646
it's the same as not having it there at all, basically because in this 

123
00:07:56,646 --> 00:07:57,940
world 1 plus 1 equals 0.

124
00:07:58,920 --> 00:08:04,300
So adding a copy of this position to the total sum has the same effect as removing it.

125
00:08:05,160 --> 00:08:07,903
And that effect, again, is that the total result at 

126
00:08:07,903 --> 00:08:10,700
the bottom here spells out the position of the error.

127
00:08:13,040 --> 00:08:17,084
To illustrate how elegant this is, let me show that one line of Python code I 

128
00:08:17,084 --> 00:08:21,440
referenced before, which will capture almost all of the logic on the receiver's end.

129
00:08:22,080 --> 00:08:26,428
We'll start by creating a random array of 16 ones and zeros to simulate the data block, 

130
00:08:26,428 --> 00:08:30,184
and I'll go ahead and give it the name bits, but of course in practice this 

131
00:08:30,184 --> 00:08:34,336
would be something that we're receiving from a sender, and instead of being random, 

132
00:08:34,336 --> 00:08:37,400
it would be carrying 11 data bits together with 5 parity bits.

133
00:08:38,120 --> 00:08:42,559
If I call the function enumerateBits, what it does is pair together each of 

134
00:08:42,559 --> 00:08:47,000
those bits with a corresponding index, in this case running from 0 up to 15.

135
00:08:48,180 --> 00:08:51,991
So if we then create a list that loops over all of these pairs, 

136
00:08:51,991 --> 00:08:55,980
pairs that look like i,bit, and then we pull out just the i value, 

137
00:08:55,980 --> 00:09:01,340
just the index, well, it's not that exciting, we just get back those indices 0 through 15.

138
00:09:01,680 --> 00:09:05,072
But if we add on the condition to only do this if bit, 

139
00:09:05,072 --> 00:09:10,501
meaning if that bit is a 1 and not a 0, well then it pulls out only the positions where 

140
00:09:10,501 --> 00:09:12,660
the corresponding bit is turned on.

141
00:09:13,380 --> 00:09:17,960
In this case it looks like those positions are 0, 4, 6, 9, etc.

142
00:09:19,980 --> 00:09:23,533
Remember, what we want is to collect together all of those positions, 

143
00:09:23,533 --> 00:09:27,240
the positions of the bits that are turned on, and then XOR them together.

144
00:09:29,180 --> 00:09:33,220
To do this in Python, let me first import a couple helpful functions.

145
00:09:33,900 --> 00:09:38,700
That way we can call reduce() on this list, and use the XOR function to reduce it.

146
00:09:39,100 --> 00:09:42,680
This basically eats its way through the list, taking XORs along the way.

147
00:09:44,800 --> 00:09:47,164
If you prefer, you can explicitly write out that XOR 

148
00:09:47,164 --> 00:09:49,440
function without having to import it from anywhere.

149
00:09:51,940 --> 00:09:57,417
So at the moment, it looks like if we do this on our random block of 16 bits, 

150
00:09:57,417 --> 00:10:01,280
it returns 9, which has the binary representation 1001.

151
00:10:01,980 --> 00:10:06,291
We won't do it here, but you could write a function where the sender uses that 

152
00:10:06,291 --> 00:10:09,456
binary representation to set the 4 parity bits as needed, 

153
00:10:09,456 --> 00:10:13,822
ultimately getting this block to a state where running this line of code on the 

154
00:10:13,822 --> 00:10:15,460
full list of bits returns a 0.

155
00:10:16,080 --> 00:10:18,200
This would be considered a well-prepared block.

156
00:10:19,880 --> 00:10:24,099
Now what's cool is that if we toggle any one of the bits in this list, 

157
00:10:24,099 --> 00:10:28,734
simulating a random error from noise, then if you run this same line of code, 

158
00:10:28,734 --> 00:10:30,220
it prints out that error.

159
00:10:30,960 --> 00:10:31,520
Isn't that neat?

160
00:10:31,820 --> 00:10:36,124
You could get this block from out of the blue, run this single line on it, 

161
00:10:36,124 --> 00:10:41,060
and it'll automatically spit out the position of an error, or a 0 if there wasn't any.

162
00:10:42,500 --> 00:10:45,200
And there's nothing special about the size 16 here.

163
00:10:45,400 --> 00:10:49,860
The same line of code would work if you had a list of 256 bits.

164
00:10:51,880 --> 00:10:54,751
Needless to say, there is more code to write here, 

165
00:10:54,751 --> 00:10:57,960
like doing the meta parity check to detect 2-bit errors, 

166
00:10:57,960 --> 00:11:02,014
but the idea is that almost all of the core logic from our scheme comes 

167
00:11:02,014 --> 00:11:03,760
down to a single XOR reduction.

168
00:11:06,120 --> 00:11:09,966
Now depending on your comfort with binary and XORs and software in general, 

169
00:11:09,966 --> 00:11:13,054
you may either find this perspective a little bit confusing, 

170
00:11:13,054 --> 00:11:17,205
or so much more elegant and simple that you're wondering why we didn't just start 

171
00:11:17,205 --> 00:11:18,420
with it from the get-go.

172
00:11:19,140 --> 00:11:22,895
Loosely speaking, the multiple parity check perspective is easier to think about 

173
00:11:22,895 --> 00:11:25,631
when implementing Hamming codes in hardware very directly, 

174
00:11:25,631 --> 00:11:29,201
and the XOR perspective is easiest to think about when doing it in software, 

175
00:11:29,201 --> 00:11:30,500
from kind of a higher level.

176
00:11:31,360 --> 00:11:34,214
The first one is easiest to actually do by hand, 

177
00:11:34,214 --> 00:11:39,281
and I think it does a better job instilling the core intuition underlying all of this, 

178
00:11:39,281 --> 00:11:43,825
which is that the information required to locate a single error is related to 

179
00:11:43,825 --> 00:11:46,912
the log of the size of the block, or in other words, 

180
00:11:46,912 --> 00:11:50,000
it grows one bit at a time as the block size doubles.

181
00:11:51,020 --> 00:11:53,439
The relevant fact here is that that information 

182
00:11:53,439 --> 00:11:56,060
directly corresponds to how much redundancy we need.

183
00:11:56,660 --> 00:11:59,922
That's really what runs against most people's knee-jerk reaction when 

184
00:11:59,922 --> 00:12:02,765
they first think about making a message resilient to errors, 

185
00:12:02,765 --> 00:12:06,540
where usually copying the whole message is the first instinct that comes to mind.

186
00:12:07,500 --> 00:12:10,793
And then, by the way, there is this whole other way that you sometimes see 

187
00:12:10,793 --> 00:12:14,000
Hamming codes presented where you multiply the message by one big matrix.

188
00:12:14,670 --> 00:12:18,736
It's kind of nice because it relates it to the broader family of linear codes, 

189
00:12:18,736 --> 00:12:23,060
but I think that gives almost no intuition for where it comes from or how it scales.

190
00:12:25,200 --> 00:12:28,180
And speaking of scaling, you might notice that the efficiency 

191
00:12:28,180 --> 00:12:31,160
of this scheme only gets better as we increase the block size.

192
00:12:35,000 --> 00:12:38,915
For example, we saw that with 256 bits, you're using only 3% of that 

193
00:12:38,915 --> 00:12:42,660
space for redundancy, and it just keeps getting better from there.

194
00:12:43,300 --> 00:12:47,340
As the number of parity bits grows one by one, the block size keeps doubling.

195
00:12:49,000 --> 00:12:52,599
And if you take that to an extreme, you could have a block with, 

196
00:12:52,599 --> 00:12:56,309
say, a million bits, where you would quite literally be playing 20 

197
00:12:56,309 --> 00:13:00,020
questions with your parity checks, and it uses only 21 parity bits.

198
00:13:00,740 --> 00:13:03,792
And if you step back to think about looking at a million 

199
00:13:03,792 --> 00:13:07,060
bits and locating a single error, that genuinely feels crazy.

200
00:13:08,200 --> 00:13:11,098
The problem, of course, is that with a larger block, 

201
00:13:11,098 --> 00:13:14,761
the probability of seeing more than one or two bit errors goes up, 

202
00:13:14,761 --> 00:13:17,660
and Hamming codes do not handle anything beyond that.

203
00:13:18,320 --> 00:13:21,234
So in practice, what you'd want is to find the right size 

204
00:13:21,234 --> 00:13:24,300
so that the probability of too many bit flips isn't too high.

205
00:13:26,600 --> 00:13:29,549
Also, in practice, errors tend to come in little bursts, 

206
00:13:29,549 --> 00:13:31,620
which would totally ruin a single block.

207
00:13:32,200 --> 00:13:36,535
So one common tactic to help spread out a burst of errors across many different 

208
00:13:36,535 --> 00:13:40,980
blocks is to interlace those blocks, like this, before they're sent out or stored.

209
00:13:45,580 --> 00:13:49,541
Then again, a lot of this is rendered completely moot by more modern codes, 

210
00:13:49,541 --> 00:13:52,512
like the much more commonly used Reed-Solomon algorithm, 

211
00:13:52,512 --> 00:13:56,943
which handles burst errors particularly well, and it can be tuned to be resilient to 

212
00:13:56,943 --> 00:13:58,820
a larger number of errors per block.

213
00:13:59,360 --> 00:14:01,340
But that is a topic for another time.

214
00:14:02,500 --> 00:14:05,349
In his book The Art of Doing Science and Engineering, 

215
00:14:05,349 --> 00:14:09,940
Hamming is wonderfully candid about just how meandering his discovery of this code was.

216
00:14:10,620 --> 00:14:14,296
He first tried all sorts of different schemes involving organizing the bits 

217
00:14:14,296 --> 00:14:17,780
into parts of a higher dimensional lattice and strange things like this.

218
00:14:18,300 --> 00:14:22,657
The idea that it might be possible to get parity checks to conspire in a way that spells 

219
00:14:22,657 --> 00:14:26,966
out the position of an error only came to Hamming when he stepped back after a bunch of 

220
00:14:26,966 --> 00:14:31,275
other analysis and asked, okay, what is the most efficient I could conceivably be about 

221
00:14:31,275 --> 00:14:31,520
this?

222
00:14:32,620 --> 00:14:36,867
He was also candid about how important it was that parity checks were already on 

223
00:14:36,867 --> 00:14:41,220
his mind, which would have been way less common back in the 1940s than it is today.

224
00:14:41,920 --> 00:14:45,045
There are like half a dozen times throughout this book that he 

225
00:14:45,045 --> 00:14:48,220
references the Louis Pasteur quote, luck favors a prepared mind.

226
00:14:49,320 --> 00:14:52,216
Clever ideas often look deceptively simple in hindsight, 

227
00:14:52,216 --> 00:14:54,300
which makes them easy to underappreciate.

228
00:14:54,960 --> 00:14:57,517
Right now my honest hope is that Hamming codes, 

229
00:14:57,517 --> 00:15:01,300
or at least the possibility of such codes, feels almost obvious to you.

230
00:15:01,660 --> 00:15:05,352
But you shouldn't fool yourself into thinking that they actually are obvious, 

231
00:15:05,352 --> 00:15:06,820
because they definitely aren't.

232
00:15:07,880 --> 00:15:11,701
Part of the reason that clever ideas look deceptively easy is that we only 

233
00:15:11,701 --> 00:15:14,503
ever see the final result, cleaning up what was messy, 

234
00:15:14,503 --> 00:15:18,172
never mentioning all of the wrong turns, underselling just how vast the 

235
00:15:18,172 --> 00:15:22,248
space of explorable possibilities is at the start of a problem solving process, 

236
00:15:22,248 --> 00:15:22,860
all of that.

237
00:15:23,820 --> 00:15:24,900
But this is true in general.

238
00:15:24,900 --> 00:15:27,784
I think for some special inventions, there's a second, 

239
00:15:27,784 --> 00:15:30,040
deeper reason that we underappreciate them.

240
00:15:30,840 --> 00:15:34,612
Thinking of information in terms of bits had only really coalesced into a 

241
00:15:34,612 --> 00:15:38,640
full theory by 1948, with Claude Shannon's seminal paper on information theory.

242
00:15:39,280 --> 00:15:42,540
This was essentially concurrent with when Hamming developed his algorithm.

243
00:15:43,300 --> 00:15:46,836
This was the same foundational paper that showed, in a certain sense, 

244
00:15:46,836 --> 00:15:49,464
that efficient error correction is always possible, 

245
00:15:49,464 --> 00:15:52,900
no matter how high the probability of bit flips, at least in theory.

246
00:15:53,700 --> 00:15:57,038
Shannon and Hamming, by the way, shared an office in Bell Labs, 

247
00:15:57,038 --> 00:16:01,160
despite working on very different things, which hardly seems coincidental here.

248
00:16:02,380 --> 00:16:05,718
Fast forward several decades, and these days, many of us are 

249
00:16:05,718 --> 00:16:09,056
so immersed in thinking about bits and information that it's 

250
00:16:09,056 --> 00:16:12,340
easy to overlook just how distinct this way of thinking was.

251
00:16:13,100 --> 00:16:17,707
Ironically, the ideas that most profoundly shape the ways that a future generation 

252
00:16:17,707 --> 00:16:22,260
thinks will end up looking to that future generation simpler than they really are.

