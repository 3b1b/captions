[
 [
  "Have you ever wondered how it's possible to scratch a CD or a DVD and still have it play back whatever it's storing? The scratch really does affect the 1s and 0s on the disk, so it reads off different data from what was",
  0.0,
  6.9
 ],
 [
  "We were talking about Hamming codes, a way to create a block of data where most of the bits carry a meaningful message, while a few others act as a kind of redundancy, in such a way that if any bit gets flipped, either a message bit or a redundancy bit, anything in this block, a receiver is going to be able to identify that there was an error, and how to fix it.",
  6.9,
  21.24
 ],
 [
  "The basic idea presented there was how to use multiple parity checks to binary search your way down to the error.",
  21.88,
  27.16
 ],
 [
  "In that video, the goal was to make Hamming codes feel as hands-on and rediscoverable as possible.",
  28.98,
  34.6
 ],
 [
  "But as you start to think about actually implementing this, either in software or hardware, that framing may actually undersell how elegant these codes really are.",
  35.18,
  43.46
 ],
 [
  "You might think that you need to write an algorithm that keeps track of all the possible error locations and cuts that group in half with each check, but it's actually way, way simpler than that.",
  43.92,
  53.48
 ],
 [
  "If you read out the answers to the four parity checks we did in the last video, all as ones and zeros instead of yeses and nos, it literally spells out the position of the error in binary.",
  53.94,
  64.08
 ],
 [
  "For the better part of the last century, this field has been a really rich source of surprisingly deep math that gets incorporated into devices we use every day. The goal here is to give you a very thorough understanding of one of the earliest examples, known as a Hamming code.",
  64.78,
  78.44
 ],
 [
  "And notice where the position 7 sits.",
  79.3,
  79.76
 ],
 [
  "It does affect the first of our parity groups, and the second, and the third, but not the last.",
  79.76,
  81.74
 ],
 [
  "So reading the results of those four checks from bottom to top indeed does spell out the position of the error.",
  82.22,
  87.54
 ],
 [
  "There's nothing special about the example 7, this works in general.",
  88.32,
  91.14
 ],
 [
  "This makes the logic for implementing the whole scheme in hardware shockingly simple.",
  91.78,
  95.82
 ],
 [
  "Now if you want to see why this magic happens, take these 16 index labels for our positions, but instead of writing them in base 10, let's write them all in binary, running from 0000 up to 1111.",
  97.24,
  109.88
 ],
 [
  "ask feels at the start, and how utterly reasonable it seems once you learn about Hamming. The basic principle of error correction is that in a vast space of all possible messages, only some subset are going to be considered valid messages. As an analogy, think about correctly spelled words vs incorrectly spelled words.",
  110.56,
  123.5
 ],
 [
  "Whenever a valid message gets altered, the receiver is responsible for correcting what they see back to the nearest valid neighbor, as you might do with a typo. Coming up with a concrete algorithm to efficiently categorize messages like this, though, takes a certain cleverness.",
  124.14,
  136.16
 ],
 [
  "The elegance of having everything we're looking at be described in binary is maybe undercut by the confusion of having everything we're looking at being described in binary.",
  136.16,
  143.22
 ],
 [
  "It's worth it, though.",
  144.24,
  145.64
 ],
 [
  "Focus your attention just on that last bit of all of these labels.",
  145.64,
  152.32
 ],
 [
  "And then highlight the positions where that final bit is a 1.",
  152.54,
  153.92
 ],
 [
  "What we get is the first of our four parity groups, which means that you can interpret that first check as asking, hey, if there's an error, is the final bit in the position of that error a 1?",
  153.92,
  171.4
 ],
 [
  "4 special bits to come nicely packaged together, maybe at the end or something like that, but as you'll see, having them sit in positions which are powers of 2 allows for something that's really elegant by the end. It also might give you a little hint about how this scales f",
  171.66,
  187.12
 ],
 [
  "or larger blocks. Also technically it ends up being only 11 bits of data, you'll find there's a mild nuance for what goes on at position 0, but",
  187.12,
  197.36
 ],
 [
  "don't worry about that for now.",
  197.36,
  201.48
 ],
 [
  "The third parity check covers every position whose third to last bit is turned on, and the last one covers the last eight positions, those ones whose highest order bit is a 1.",
  202.04,
  207.5
 ],
 [
  "ame thing as sending a message just from the past to the future instead of from one place to another. So that's the setup, but before we can dive in we need to talk about a related idea which was fresh on Hamming's mind in the time of his discovery, a",
  207.5,
  218.78
 ],
 [
  "I hope this makes two things clearer.",
  218.78,
  218.78
 ],
 [
  "The only job of this special bit is to make sure that the total number of 1s in the message is an even number. So for example right now, that total number of 1s is",
  218.78,
  232.2
 ],
 [
  "If it takes more bits to describe each position, like six bits to describe 64 spots, then each of those bits gives you one of the parity groups that we need to check.",
  232.2,
  244.9
 ],
 [
  "that special bit to be a 1, making the count even. But if the block had already started off with an even number of 1s, then this special bit would have been kept at a 0. This is pretty simple, deceptively simple, b",
  244.9,
  254.28
 ],
 [
  "It's the same core logic, but solving a different problem, and applied to a 64-squared chessboard.",
  254.36,
  267.1
 ],
 [
  "The second thing I hope this makes clear is why our parity bits are sitting in the positions that are powers of two, for example 1, 2, 4, and 8.",
  267.1,
  271.18
 ],
 [
  "These are the positions whose binary representation has just a single bit turned on.",
  271.18,
  271.4
 ],
 [
  "d say the parity is 0 or 1, which is typically more helpful once you start doing math with the idea. And this special bit that the sender uses to control the parity is called the parity bit.",
  271.4,
  280.86
 ],
 [
  "And actually, we should be clear, if the receiver sees an odd parity, it doesn't necessarily mean there was just one error, there might have been 3 errors, or 5, or any other odd number, but they can know for sure that it wasn't 0.",
  280.86,
  291.4
 ],
 [
  "On the other hand, if there had been 2 errors, or any even number of errors, that final count of 1s would still be even, so the receiver can't have full confidence that an even count necessarily means the message is error-free. You might complain that a message which gets messed up by only 2 bit flips is pretty weak, and you would be absolutely right. Keep in mind, though, there is no method for error detection or correction that could give you 100% confidence that the message you recei",
  291.4,
  315.36
 ],
 [
  "It's based on the XOR function.",
  315.36,
  319.26
 ],
 [
  "XOR, for those of you who don't know, stands for exclusive or.",
  319.26,
  322.16
 ],
 [
  "When you take the XOR of two bits, it's going to return a 1 if either one of those bits is turned on, but not if both are turned on or if both are turned off.",
  322.16,
  338.4
 ],
 [
  "Phrased differently, it's the parity of these two bits.",
  338.4,
  345.38
 ],
 [
  "full message down to a single bit, what they give us is a powerful building block for more sophisticated schemes. For exa",
  345.9,
  354.62
 ],
 [
  "We also commonly talk about the XOR of two different bit strings, which basically does this component by component.",
  354.62,
  357.88
 ],
 [
  "It's like addition, but where you never carry.",
  357.88,
  363.2
 ],
 [
  "Again, the more mathematically inclined might prefer to think of this as adding two vectors and reducing mod 2.",
  363.62,
  371.88
 ],
 [
  "If you open up some Python right now, and you apply the caret operation between two integers, this is what it's doing, but to the bit representations of those numbers under the hood.",
  371.88,
  381.08
 ],
 [
  "The key point for you and me is that taking the XOR of many different bit strings is effectively a way to compute the parities of a bunch of separate groups, like so with the columns, all in one fell swoop.",
  381.08,
  389.2
 ],
 [
  "This gives us a rather snazzy way to think about the multiple parity checks from our Hamming code algorithm as all being packaged together into one single operation.",
  392.22,
  395.78
 ],
 [
  "Though at first glance it does look very different.",
  395.78,
  398.4
 ],
 [
  "Specifically, write down the 16 positions in binary, like we had before, and now highlight only the positions where the message bit is turned on to a 1, and then collect these positions into one big column and take the XOR.",
  398.4,
  410.14
 ],
 [
  "You can probably guess that the four bits sitting at the bottom as a result are the same as the four parity checks we've come to know and love, but take a moment to actually think about why exactly.",
  410.14,
  416.92
 ],
 [
  "This last column, for example, is counting all of the positions whose last bit is a 1, but we're already limited only to the highlighted positions, so it's effectively counting how many highlighted positions came from the first parity group.",
  416.92,
  433.38
 ],
 [
  "hat we'll do. The second check is among the 8 bits on the right half of the grid, at least as we've drawn it here. This time we might use position 2 as a parity bit, so these 8 bits already have an even pari",
  433.38,
  447.58
 ],
 [
  "Likewise, the next column counts how many positions are in the second parity group, the positions whose second to last bit is a 1, and which are also highlighted, and so on.",
  448.46,
  461.34
 ],
 [
  "It's really just a small shift in perspective on the same thing we've been doing.",
  461.34,
  468.04
 ],
 [
  "but for right now we're going to assume that there's at most one error in the entire block. Things break down completely for more than that.",
  468.04,
  482.6
 ],
 [
  "The sender is responsible for toggling some of the special parity bits to make sure the sum works out to be 0000.",
  482.6,
  490.08
 ],
 [
  "Once we have it like this, this gives us a really nice way to think about why these four resulting bits at the bottom directly spell out the position of an error.",
  490.08,
  505.02
 ],
 [
  "Let's say you detect an error among the odd columns, and among the right half. It necessarily means the error is somewhere in the last column. If there was no error in the odd column but there was one in the right",
  505.02,
  520.48
 ],
 [
  "What that means is that the position of that bit is now going to be included in the total XOR, which changes the sum from being 0 to instead being this newly included value, the position of the error.",
  521.04,
  529.14
 ],
 [
  "Slightly less obviously, the same is true if there's an error that changes a 1 to a 0.",
  529.14,
  532.22
 ],
 [
  "You see, if you add a bit string together twice, it's the same as not having it there at all, basically because in this world 1 plus 1 equals 0.",
  532.22,
  542.04
 ],
 [
  "So adding a copy of this position to the total sum has the same effect as removing it.",
  542.04,
  552.66
 ],
 [
  "And that effect, again, is that the total result at the bottom here spells out the position of the error.",
  553.38,
  564.7
 ],
 [
  "To illustrate how elegant this is, let me show that one line of Python code I referenced before, which will capture almost all of the logic on the receiver's end.",
  564.7,
  573.22
 ],
 [
  "We'll start by creating a random array of 16 ones and zeros to simulate the data block, and I'll go ahead and give it the name bits, but of course in practice this would be something that we're receiving from a sender, and instead of being random, it would be carrying 11 data bits together with 5 parity bits.",
  573.9,
  589.44
 ],
 [
  "If I call the function enumerateBits, what it does is pair together each of those bits with a corresponding index, in this case running from 0 up to 15.",
  591.94,
  602.12
 ],
 [
  "So if we then create a list that loops over all of these pairs, pairs that look like i,bit, and then we pull out just the i value, just the index, well, it's not that exciting, we just get back those indices 0 through 15.",
  602.12,
  612.84
 ],
 [
  "But if we add on the condition to only do this if bit, meaning if that bit is a 1 and not a 0, well then it pulls out only the positions where the corresponding bit is turned on.",
  612.84,
  622.76
 ],
 [
  "In this case it looks like those positions are 0, 4, 6, 9, etc.",
  622.76,
  628.48
 ],
 [
  "Remember, what we want is to collect together all of those positions, the positions of the bits that are turned on, and then XOR them together.",
  628.48,
  641.06
 ],
 [
  "To do this in Python, let me first import a couple helpful functions.",
  642.5,
  645.2
 ],
 [
  "That way we can call reduce() on this list, and use the XOR function to reduce it.",
  645.4,
  657.14
 ],
 [
  "This basically eats its way through the list, taking XORs along the way.",
  657.14,
  658.62
 ],
 [
  "ays let you pin down a specific location, no matter where they turn out to be. In fact, the astute among you might even notice a connection between these questions and binary counting. And if you do, again let me emphasize, pause, try for yourself t",
  658.62,
  679.66
 ],
 [
  "So at the moment, it looks like if we do this on our random block of 16 bits, it returns 9, which has the binary representation 1001.",
  679.66,
  681.14
 ],
 [
  "We won't do it here, but you could write a function where the sender uses that binary representation to set the 4 parity bits as needed, ultimately getting this block to a state where running this line of code on the full list of bits returns a 0.",
  681.14,
  692.74
 ],
 [
  "ed, well, you can just try it. Take a moment to think about how any error among these four special bits is going to be tracked down just like any other, with the same group of four questions. It doesn't really matter, since at the end of the d",
  692.74,
  704.12
 ],
 [
  "Now what's cool is that if we toggle any one of the bits in this list, simulating a random error from noise, then if you run this same line of code, it prints out that error.",
  704.12,
  712.42
 ],
 [
  "correction bits are just riding along. But protecting those bits as well is something that natural",
  712.42,
  712.42
 ],
 [
  "You could get this block from out of the blue, run this single line on it, and it'll automatically spit out the position of an error, or a 0 if there wasn't any.",
  712.42,
  720.58
 ],
 [
  "hat these questions are in just a minute or two. Hopefully this sketch is enough to appreciate the efficiency of what we're developing here.",
  720.58,
  724.7
 ],
 [
  "The first thing, except for those eight highlighted parity bits, can be whatever you want it to be, carrying whatever message or data you want. The 8 bits",
  724.7,
  728.02
 ],
 [
  "are redundant in the sense that they're completely determined by the rest of the message, but it's in a much smarter way than simply copying the message as a whole. And still, for so little given up, you would be able to identify and fix any single bit error. Well, almost. Okay, so the one problem here is that if none of the four parity checks detect an e",
  728.02,
  743.06
 ],
 [
  "Now depending on your comfort with binary and XORs and software in general, you may either find this perspective a little bit confusing, or so much more elegant and simple that you're wondering why we didn't just start with it from the get-go.",
  745.2,
  761.88
 ],
 [
  "tended, then it either means there was no error at all, or it narrows us down into position 0. You see, with four yes or no questions, we have 16 possible outcomes for our parity checks, and at first that feels perfect for pinpointing 1 out of 16 positions in the block, but you also need to communicate a 17th outcome, the no error condition.",
  761.88,
  776.8
 ],
 [
  "The first one is easiest to actually do by hand, and I think it does a better job instilling the core intuition underlying all of this, which is that the information required to locate a single error is related to the log of the size of the block, or in other words, it grows one bit at a time as the block size doubles.",
  776.8,
  791.9
 ],
 [
  "The relevant fact here is that that information directly corresponds to how much redundancy we need.",
  791.9,
  800.14
 ],
 [
  "at 0th one so that the parity of the full block is even, just like a normal parity check. Now, if there's a single bit error, then the parity of the full block toggles to be odd, but we would catch that anyway thanks to the four error-correcting checks. However, if there's two errors, then the overall parity is going to toggle back to be",
  800.14,
  818.28
 ],
 [
  "And then, by the way, there is this whole other way that you sometimes see Hamming codes presented where you multiply the message by one big matrix.",
  818.28,
  828.34
 ],
 [
  "It's kind of nice because it relates it to the broader family of linear codes, but I think that gives almost no intuition for where it comes from or how it scales.",
  828.34,
  841.34
 ],
 [
  "And speaking of scaling, you might notice that the efficiency of this scheme only gets better as we increase the block size.",
  842.5,
  849.94
 ],
 [
  "For example, we saw that with 256 bits, you're using only 3% of that space for redundancy, and it just keeps getting better from there.",
  850.62,
  857.78
 ],
 [
  "As the number of parity bits grows one by one, the block size keeps doubling.",
  858.3,
  861.0
 ],
 [
  "And if you take that to an extreme, you could have a block with, say, a million bits, where you would quite literally be playing 20 questions with your parity checks, and it uses only 21 parity bits.",
  861.0,
  871.52
 ],
 [
  "ugh so you can check yourself. To set up a message, whether that's a literal message you're translating over space or some data you want to store over time, the first step is to divide it up into 11-bit chunks.",
  872.62,
  882.06
 ],
 [
  "The problem, of course, is that with a larger block, the probability of seeing more than one or two bit errors goes up, and Hamming codes do not handle anything beyond that.",
  882.06,
  893.34
 ],
 [
  "So in practice, what you'd want is to find the right size so that the probability of too many bit flips isn't too high.",
  893.34,
  896.72
 ],
 [
  "Also, in practice, errors tend to come in little bursts, which would totally ruin a single block.",
  896.72,
  897.1
 ],
 [
  "ow has an even parity, meaning you can set that bit number 0, the overarching parity bit, to be 0. So as this block is sent off, the parity of the four special subsets and the block as a whole will all be even, or 0. A",
  897.24,
  914.86
 ],
 [
  "Then again, a lot of this is rendered completely moot by more modern codes, like the much more commonly used Reed-Solomon algorithm, which handles burst errors particularly well, and it can be tuned to be resilient to a larger number of errors per block.",
  914.86,
  922.86
 ],
 [
  "see that it's even, so any error that exists would have to be in an even column.",
  923.82,
  924.9
 ],
 [
  "In his book The Art of Doing Science and Engineering, Hamming is wonderfully candid about just how meandering his discovery of this code was.",
  924.9,
  931.94
 ],
 [
  "is odd, giving us confidence that there was one flip and not two. If it's three or more, all bets are off. After correcting that bit number 10, pulling out the 11 bits that were not used for correction gives us the relevant segment of the original message, which if you rewind and compare is indeed exactly what we started the example with.",
  931.94,
  945.94
 ],
 [
  "The idea that it might be possible to get parity checks to conspire in a way that spells out the position of an error only came to Hamming when he stepped back after a bunch of other analysis and asked, okay, what is the most efficient I could conceivably be about this?",
  945.94,
  963.96
 ],
 [
  "He was also candid about how important it was that parity checks were already on his mind, which would have been way less common back in the 1940s than it is today.",
  963.96,
  975.26
 ],
 [
  "a machine to point to the position of an error, how to systematically scale it, and how we can frame all of this as one single operation rather than multiple separate parity checks.",
  975.26,
  982.26
 ],
 [
  "Clever ideas often look deceptively simple in hindsight, which makes them easy to underappreciate.",
  982.26,
  982.26
 ],
 [
  "Right now my honest hope is that Hamming codes, or at least the possibility of such codes, feels almost obvious to you.",
  982.26,
  982.26
 ],
 [
  "But you shouldn't fool yourself into thinking that they actually are obvious, because they definitely aren't.",
  982.26,
  982.26
 ],
 [
  "Part of the reason that clever ideas look deceptively easy is that we only ever see the final result, cleaning up what was messy, never mentioning all of the wrong turns, underselling just how vast the space of explorable possibilities is at the start of a problem solving process, all of that.",
  982.26,
  982.26
 ],
 [
  "But this is true in general.",
  982.26,
  982.26
 ],
 [
  "I think for some special inventions, there's a second, deeper reason that we underappreciate them.",
  982.26,
  982.26
 ],
 [
  "Thinking of information in terms of bits had only really coalesced into a full theory by 1948, with Claude Shannon's seminal paper on information theory.",
  982.26,
  982.26
 ],
 [
  "This was essentially concurrent with when Hamming developed his algorithm.",
  982.26,
  982.26
 ],
 [
  "This was the same foundational paper that showed, in a certain sense, that efficient error correction is always possible, no matter how high the probability of bit flips, at least in theory.",
  982.26,
  982.26
 ],
 [
  "Shannon and Hamming, by the way, shared an office in Bell Labs, despite working on very different things, which hardly seems coincidental here.",
  982.26,
  982.26
 ],
 [
  "Fast forward several decades, and these days, many of us are so immersed in thinking about bits and information that it's easy to overlook just how distinct this way of thinking was.",
  982.26,
  982.26
 ],
 [
  "Ironically, the ideas that most profoundly shape the ways that a future generation thinks will end up looking to that future generation simpler than they really are.",
  982.26,
  982.26
 ]
]