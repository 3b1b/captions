1
00:00:00,000 --> 00:00:02,560
I'm assuming that everybody here is coming from part 1.

2
00:00:03,060 --> 00:00:07,020
We were talking about Hamming codes, a way to create a block of data where most of the

3
00:00:07,020 --> 00:00:11,680
bits carry a meaningful message, while a few others act as a kind of redundancy, in such

4
00:00:11,680 --> 00:00:15,800
a way that if any bit gets flipped, either a message bit or a redundancy bit, anything

5
00:00:15,800 --> 00:00:20,620
in this block, a receiver is going to be able to identify that there was an error, and how

6
00:00:20,620 --> 00:00:21,240
to fix it.

7
00:00:21,880 --> 00:00:25,880
The basic idea presented there was how to use multiple parity checks to binary search

8
00:00:25,880 --> 00:00:27,160
your way down to the error.

9
00:00:28,980 --> 00:00:33,980
In that video, the goal was to make Hamming codes feel as hands-on and rediscoverable

10
00:00:33,980 --> 00:00:34,600
as possible.

11
00:00:35,180 --> 00:00:40,080
But as you start to think about actually implementing this, either in software or hardware, that

12
00:00:40,080 --> 00:00:43,460
framing may actually undersell how elegant these codes really are.

13
00:00:43,920 --> 00:00:47,640
You might think that you need to write an algorithm that keeps track of all the possible

14
00:00:47,640 --> 00:00:52,220
error locations and cuts that group in half with each check, but it's actually way,

15
00:00:52,560 --> 00:00:53,480
way simpler than that.

16
00:00:53,940 --> 00:00:58,560
If you read out the answers to the four parity checks we did in the last video, all as ones

17
00:00:58,560 --> 00:01:04,080
and zeros instead of yeses and nos, it literally spells out the position of the error in binary.

18
00:01:04,780 --> 00:01:09,980
For example, the number 7 in binary looks like 0111, essentially saying that it's

19
00:01:09,980 --> 00:01:11,260
4 plus 2 plus 1.

20
00:01:12,540 --> 00:01:14,460
And notice where the position 7 sits.

21
00:01:14,840 --> 00:01:21,340
It does affect the first of our parity groups, and the second, and the third, but not the

22
00:01:21,340 --> 00:01:21,740
last.

23
00:01:22,220 --> 00:01:26,640
So reading the results of those four checks from bottom to top indeed does spell out the

24
00:01:26,640 --> 00:01:27,540
position of the error.

25
00:01:28,320 --> 00:01:31,140
There's nothing special about the example 7, this works in general.

26
00:01:31,780 --> 00:01:35,820
This makes the logic for implementing the whole scheme in hardware shockingly simple.

27
00:01:37,240 --> 00:01:42,800
Now if you want to see why this magic happens, take these 16 index labels for our positions,

28
00:01:43,260 --> 00:01:47,640
but instead of writing them in base 10, let's write them all in binary, running from 0000

29
00:01:48,320 --> 00:01:49,880
up to 1111.

30
00:01:50,560 --> 00:01:55,620
As we put these binary labels back into their boxes, let me emphasize that they are distinct

31
00:01:55,620 --> 00:01:57,800
from the data that's actually being sent.

32
00:01:58,320 --> 00:02:02,300
They're nothing more than a conceptual label to help you and me understand where the four

33
00:02:02,300 --> 00:02:03,500
parity groups came from.

34
00:02:04,140 --> 00:02:08,340
The elegance of having everything we're looking at be described in binary is maybe

35
00:02:08,340 --> 00:02:12,360
undercut by the confusion of having everything we're looking at being described in binary.

36
00:02:13,020 --> 00:02:14,120
It's worth it, though.

37
00:02:14,800 --> 00:02:18,240
Focus your attention just on that last bit of all of these labels.

38
00:02:19,880 --> 00:02:23,220
And then highlight the positions where that final bit is a 1.

39
00:02:24,240 --> 00:02:28,620
What we get is the first of our four parity groups, which means that you can interpret

40
00:02:28,620 --> 00:02:34,300
that first check as asking, hey, if there's an error, is the final bit in the position

41
00:02:34,300 --> 00:02:35,740
of that error a 1?

42
00:02:38,200 --> 00:02:42,640
Similarly, if you focus on the second to last bit, and highlight all the positions where

43
00:02:42,640 --> 00:02:46,160
that's a 1, you get the second parity group from our scheme.

44
00:02:46,740 --> 00:02:52,080
In other words, that second check is asking, hey, me again, if there's an error, is the

45
00:02:52,080 --> 00:02:54,500
second to last bit of that position a 1?

46
00:02:55,760 --> 00:02:56,900
And so on.

47
00:02:57,220 --> 00:03:01,740
The third parity check covers every position whose third to last bit is turned on,

48
00:03:02,740 --> 00:03:08,740
and the last one covers the last eight positions, those ones whose highest order bit is a 1.

49
00:03:09,740 --> 00:03:15,720
Everything we did earlier is the same as answering these four questions, which in turn is the

50
00:03:15,720 --> 00:03:17,740
same as spelling out a position in binary.

51
00:03:19,620 --> 00:03:21,480
I hope this makes two things clearer.

52
00:03:22,040 --> 00:03:26,460
The first is how to systematically generalize to block sizes that are bigger powers of two.

53
00:03:26,960 --> 00:03:33,240
If it takes more bits to describe each position, like six bits to describe 64 spots, then each

54
00:03:33,240 --> 00:03:36,680
of those bits gives you one of the parity groups that we need to check.

55
00:03:38,400 --> 00:03:42,040
Those of you who watched the chessboard puzzle I did with Matt Parker might find all this

56
00:03:42,040 --> 00:03:43,180
exceedingly familiar.

57
00:03:43,660 --> 00:03:48,780
It's the same core logic, but solving a different problem, and applied to a 64-squared chessboard.

58
00:03:49,880 --> 00:03:54,040
The second thing I hope this makes clear is why our parity bits are sitting in the positions

59
00:03:54,040 --> 00:03:57,320
that are powers of two, for example 1, 2, 4, and 8.

60
00:03:58,000 --> 00:04:03,000
These are the positions whose binary representation has just a single bit turned on.

61
00:04:03,600 --> 00:04:09,460
What that means is each of those parity bits sits inside one and only one of the four parity groups.

62
00:04:12,040 --> 00:04:16,680
You can also see this in larger examples, where no matter how big you get, each parity

63
00:04:16,680 --> 00:04:19,340
bit conveniently touches only one of the groups.

64
00:04:25,600 --> 00:04:29,500
Once you understand that these parity checks that we've focused so much of our time on

65
00:04:29,500 --> 00:04:34,240
are nothing more than a clever way to spell out the position of an error in binary, then

66
00:04:34,240 --> 00:04:37,940
we can draw a connection with a different way to think about hamming codes, one that

67
00:04:37,940 --> 00:04:42,220
is arguably a lot simpler and more elegant, and which can basically be written down with

68
00:04:42,220 --> 00:04:43,240
a single line of code.

69
00:04:43,660 --> 00:04:45,500
It's based on the XOR function.

70
00:04:46,940 --> 00:04:50,220
XOR, for those of you who don't know, stands for exclusive or.

71
00:04:50,780 --> 00:04:55,740
When you take the XOR of two bits, it's going to return a 1 if either one of those bits

72
00:04:55,740 --> 00:04:59,360
is turned on, but not if both are turned on or if both are turned off.

73
00:05:00,100 --> 00:05:02,980
Phrased differently, it's the parity of these two bits.

74
00:05:03,540 --> 00:05:06,760
As a math person, I prefer to think about it as addition mod 2.

75
00:05:07,360 --> 00:05:12,060
We also commonly talk about the XOR of two different bit strings, which basically does

76
00:05:12,060 --> 00:05:13,440
this component by component.

77
00:05:13,680 --> 00:05:15,720
It's like addition, but where you never carry.

78
00:05:16,500 --> 00:05:21,040
Again, the more mathematically inclined might prefer to think of this as adding two vectors

79
00:05:21,040 --> 00:05:22,480
and reducing mod 2.

80
00:05:23,500 --> 00:05:27,420
If you open up some Python right now, and you apply the caret operation between two

81
00:05:27,420 --> 00:05:32,520
integers, this is what it's doing, but to the bit representations of those numbers under

82
00:05:32,520 --> 00:05:32,940
the hood.

83
00:05:34,960 --> 00:05:40,880
The key point for you and me is that taking the XOR of many different bit strings is effectively

84
00:05:40,880 --> 00:05:45,380
a way to compute the parities of a bunch of separate groups, like so with the columns,

85
00:05:45,900 --> 00:05:47,140
all in one fell swoop.

86
00:05:51,260 --> 00:05:54,620
This gives us a rather snazzy way to think about the multiple parity checks from our

87
00:05:54,620 --> 00:05:58,780
Hamming code algorithm as all being packaged together into one single operation.

88
00:05:59,480 --> 00:06:02,180
Though at first glance it does look very different.

89
00:06:02,820 --> 00:06:08,220
Specifically, write down the 16 positions in binary, like we had before, and now highlight

90
00:06:08,220 --> 00:06:14,660
only the positions where the message bit is turned on to a 1, and then collect these positions

91
00:06:14,660 --> 00:06:17,100
into one big column and take the XOR.

92
00:06:19,260 --> 00:06:22,980
You can probably guess that the four bits sitting at the bottom as a result are the

93
00:06:22,980 --> 00:06:27,320
same as the four parity checks we've come to know and love, but take a moment to actually

94
00:06:27,320 --> 00:06:29,200
think about why exactly.

95
00:06:32,220 --> 00:06:37,300
This last column, for example, is counting all of the positions whose last bit is a 1,

96
00:06:37,780 --> 00:06:42,100
but we're already limited only to the highlighted positions, so it's effectively counting

97
00:06:42,100 --> 00:06:45,760
how many highlighted positions came from the first parity group.

98
00:06:46,240 --> 00:06:46,800
Does that make sense?

99
00:06:49,080 --> 00:06:53,140
Likewise, the next column counts how many positions are in the second parity group,

100
00:06:53,140 --> 00:06:59,680
the positions whose second to last bit is a 1, and which are also highlighted, and so

101
00:06:59,680 --> 00:07:00,000
on.

102
00:07:00,260 --> 00:07:03,960
It's really just a small shift in perspective on the same thing we've been doing.

103
00:07:07,760 --> 00:07:09,600
And so you know where it goes from here.

104
00:07:10,000 --> 00:07:14,420
The sender is responsible for toggling some of the special parity bits to make sure the

105
00:07:14,420 --> 00:07:16,560
sum works out to be 0000.

106
00:07:19,040 --> 00:07:23,940
Once we have it like this, this gives us a really nice way to think about why these four

107
00:07:23,940 --> 00:07:27,580
resulting bits at the bottom directly spell out the position of an error.

108
00:07:28,460 --> 00:07:31,860
Let's say some bit in this block gets toggled from a 0 to a 1.

109
00:07:32,600 --> 00:07:37,400
What that means is that the position of that bit is now going to be included in the total

110
00:07:37,400 --> 00:07:43,020
XOR, which changes the sum from being 0 to instead being this newly included value, the

111
00:07:43,020 --> 00:07:43,820
position of the error.

112
00:07:44,460 --> 00:07:49,360
Slightly less obviously, the same is true if there's an error that changes a 1 to a 0.

113
00:07:50,180 --> 00:07:54,800
You see, if you add a bit string together twice, it's the same as not having it there

114
00:07:54,800 --> 00:07:57,940
at all, basically because in this world 1 plus 1 equals 0.

115
00:07:58,920 --> 00:08:04,300
So adding a copy of this position to the total sum has the same effect as removing it.

116
00:08:05,160 --> 00:08:10,080
And that effect, again, is that the total result at the bottom here spells out the position

117
00:08:10,080 --> 00:08:10,700
of the error.

118
00:08:13,040 --> 00:08:17,600
To illustrate how elegant this is, let me show that one line of Python code I referenced

119
00:08:17,600 --> 00:08:21,440
before, which will capture almost all of the logic on the receiver's end.

120
00:08:22,080 --> 00:08:26,660
We'll start by creating a random array of 16 ones and zeros to simulate the data block,

121
00:08:27,000 --> 00:08:30,800
and I'll go ahead and give it the name bits, but of course in practice this would be something

122
00:08:30,800 --> 00:08:34,740
that we're receiving from a sender, and instead of being random, it would be carrying

123
00:08:34,740 --> 00:08:37,400
11 data bits together with 5 parity bits.

124
00:08:38,120 --> 00:08:43,160
If I call the function enumerateBits, what it does is pair together each of those bits

125
00:08:43,160 --> 00:08:47,000
with a corresponding index, in this case running from 0 up to 15.

126
00:08:48,180 --> 00:08:53,440
So if we then create a list that loops over all of these pairs, pairs that look like i,bit,

127
00:08:54,000 --> 00:08:58,900
and then we pull out just the i value, just the index, well, it's not that exciting,

128
00:08:59,040 --> 00:09:01,340
we just get back those indices 0 through 15.

129
00:09:01,680 --> 00:09:07,560
But if we add on the condition to only do this if bit, meaning if that bit is a 1 and

130
00:09:07,560 --> 00:09:12,660
not a 0, well then it pulls out only the positions where the corresponding bit is turned on.

131
00:09:13,380 --> 00:09:17,960
In this case it looks like those positions are 0, 4, 6, 9, etc.

132
00:09:19,980 --> 00:09:24,500
Remember, what we want is to collect together all of those positions, the positions of the

133
00:09:24,500 --> 00:09:27,240
bits that are turned on, and then XOR them together.

134
00:09:29,180 --> 00:09:33,220
To do this in Python, let me first import a couple helpful functions.

135
00:09:33,900 --> 00:09:38,700
That way we can call reduce() on this list, and use the XOR function to reduce it.

136
00:09:39,100 --> 00:09:42,680
This basically eats its way through the list, taking XORs along the way.

137
00:09:44,800 --> 00:09:48,800
If you prefer, you can explicitly write out that XOR function without having to import

138
00:09:48,800 --> 00:09:49,440
it from anywhere.

139
00:09:51,940 --> 00:09:56,900
So at the moment, it looks like if we do this on our random block of 16 bits, it returns

140
00:09:56,900 --> 00:10:01,280
9, which has the binary representation 1001.

141
00:10:01,980 --> 00:10:06,000
We won't do it here, but you could write a function where the sender uses that binary

142
00:10:06,000 --> 00:10:11,800
representation to set the 4 parity bits as needed, ultimately getting this block to a

143
00:10:11,800 --> 00:10:15,460
state where running this line of code on the full list of bits returns a 0.

144
00:10:16,080 --> 00:10:18,200
This would be considered a well-prepared block.

145
00:10:19,880 --> 00:10:24,640
Now what's cool is that if we toggle any one of the bits in this list, simulating a random

146
00:10:24,640 --> 00:10:30,220
error from noise, then if you run this same line of code, it prints out that error.

147
00:10:30,960 --> 00:10:31,520
Isn't that neat?

148
00:10:31,820 --> 00:10:37,120
You could get this block from out of the blue, run this single line on it, and it'll automatically

149
00:10:37,120 --> 00:10:41,060
spit out the position of an error, or a 0 if there wasn't any.

150
00:10:42,500 --> 00:10:45,200
And there's nothing special about the size 16 here.

151
00:10:45,400 --> 00:10:49,860
The same line of code would work if you had a list of 256 bits.

152
00:10:51,880 --> 00:10:56,340
Needless to say, there is more code to write here, like doing the meta parity check to

153
00:10:56,340 --> 00:11:01,060
detect 2-bit errors, but the idea is that almost all of the core logic from our scheme

154
00:11:01,060 --> 00:11:03,760
comes down to a single XOR reduction.

155
00:11:06,120 --> 00:11:10,740
Now depending on your comfort with binary and XORs and software in general, you may

156
00:11:10,740 --> 00:11:15,920
either find this perspective a little bit confusing, or so much more elegant and simple

157
00:11:15,920 --> 00:11:18,420
that you're wondering why we didn't just start with it from the get-go.

158
00:11:19,140 --> 00:11:22,920
Loosely speaking, the multiple parity check perspective is easier to think about when

159
00:11:22,920 --> 00:11:27,540
implementing Hamming codes in hardware very directly, and the XOR perspective is easiest

160
00:11:27,540 --> 00:11:30,500
to think about when doing it in software, from kind of a higher level.

161
00:11:31,360 --> 00:11:35,640
The first one is easiest to actually do by hand, and I think it does a better job instilling

162
00:11:35,640 --> 00:11:40,840
the core intuition underlying all of this, which is that the information required to

163
00:11:40,840 --> 00:11:46,540
locate a single error is related to the log of the size of the block, or in other words,

164
00:11:46,540 --> 00:11:50,000
it grows one bit at a time as the block size doubles.

165
00:11:51,020 --> 00:11:55,440
The relevant fact here is that that information directly corresponds to how much redundancy

166
00:11:55,440 --> 00:11:56,060
we need.

167
00:11:56,660 --> 00:12:00,260
That's really what runs against most people's knee-jerk reaction when they first think about

168
00:12:00,260 --> 00:12:05,340
making a message resilient to errors, where usually copying the whole message is the first

169
00:12:05,340 --> 00:12:06,540
instinct that comes to mind.

170
00:12:07,500 --> 00:12:11,020
And then, by the way, there is this whole other way that you sometimes see Hamming codes

171
00:12:11,020 --> 00:12:14,000
presented where you multiply the message by one big matrix.

172
00:12:14,670 --> 00:12:18,740
It's kind of nice because it relates it to the broader family of linear codes, but I

173
00:12:18,740 --> 00:12:23,060
think that gives almost no intuition for where it comes from or how it scales.

174
00:12:25,200 --> 00:12:29,400
And speaking of scaling, you might notice that the efficiency of this scheme only gets

175
00:12:29,400 --> 00:12:31,160
better as we increase the block size.

176
00:12:35,000 --> 00:12:40,460
For example, we saw that with 256 bits, you're using only 3% of that space for redundancy,

177
00:12:40,780 --> 00:12:42,660
and it just keeps getting better from there.

178
00:12:43,300 --> 00:12:47,340
As the number of parity bits grows one by one, the block size keeps doubling.

179
00:12:49,000 --> 00:12:53,900
And if you take that to an extreme, you could have a block with, say, a million bits, where

180
00:12:53,900 --> 00:12:58,740
you would quite literally be playing 20 questions with your parity checks, and it uses only

181
00:12:58,740 --> 00:13:00,020
21 parity bits.

182
00:13:00,740 --> 00:13:05,160
And if you step back to think about looking at a million bits and locating a single error,

183
00:13:05,660 --> 00:13:07,060
that genuinely feels crazy.

184
00:13:08,200 --> 00:13:12,740
The problem, of course, is that with a larger block, the probability of seeing more than

185
00:13:12,740 --> 00:13:17,660
one or two bit errors goes up, and Hamming codes do not handle anything beyond that.

186
00:13:18,320 --> 00:13:22,100
So in practice, what you'd want is to find the right size so that the probability of

187
00:13:22,100 --> 00:13:24,300
too many bit flips isn't too high.

188
00:13:26,600 --> 00:13:31,060
Also, in practice, errors tend to come in little bursts, which would totally ruin a

189
00:13:31,060 --> 00:13:31,620
single block.

190
00:13:32,200 --> 00:13:36,080
So one common tactic to help spread out a burst of errors across many different blocks

191
00:13:36,080 --> 00:13:40,980
is to interlace those blocks, like this, before they're sent out or stored.

192
00:13:45,580 --> 00:13:50,000
Then again, a lot of this is rendered completely moot by more modern codes, like the much more

193
00:13:50,000 --> 00:13:54,980
commonly used Reed-Solomon algorithm, which handles burst errors particularly well, and

194
00:13:54,980 --> 00:13:58,820
it can be tuned to be resilient to a larger number of errors per block.

195
00:13:59,360 --> 00:14:01,340
But that is a topic for another time.

196
00:14:02,500 --> 00:14:07,780
In his book The Art of Doing Science and Engineering, Hamming is wonderfully candid about just how

197
00:14:07,780 --> 00:14:09,940
meandering his discovery of this code was.

198
00:14:10,620 --> 00:14:15,200
He first tried all sorts of different schemes involving organizing the bits into parts of

199
00:14:15,200 --> 00:14:17,780
a higher dimensional lattice and strange things like this.

200
00:14:18,300 --> 00:14:22,480
The idea that it might be possible to get parity checks to conspire in a way that spells

201
00:14:22,480 --> 00:14:26,400
out the position of an error only came to Hamming when he stepped back after a bunch

202
00:14:26,400 --> 00:14:30,860
of other analysis and asked, okay, what is the most efficient I could conceivably be

203
00:14:30,860 --> 00:14:31,520
about this?

204
00:14:32,620 --> 00:14:36,840
He was also candid about how important it was that parity checks were already on his

205
00:14:36,840 --> 00:14:41,220
mind, which would have been way less common back in the 1940s than it is today.

206
00:14:41,920 --> 00:14:46,000
There are like half a dozen times throughout this book that he references the Louis Pasteur

207
00:14:46,000 --> 00:14:48,220
quote, luck favors a prepared mind.

208
00:14:49,320 --> 00:14:54,300
Clever ideas often look deceptively simple in hindsight, which makes them easy to underappreciate.

209
00:14:54,960 --> 00:14:59,100
Right now my honest hope is that Hamming codes, or at least the possibility of such codes,

210
00:14:59,500 --> 00:15:01,300
feels almost obvious to you.

211
00:15:01,660 --> 00:15:05,540
But you shouldn't fool yourself into thinking that they actually are obvious, because they

212
00:15:05,540 --> 00:15:06,820
definitely aren't.

213
00:15:07,880 --> 00:15:12,120
Part of the reason that clever ideas look deceptively easy is that we only ever see

214
00:15:12,120 --> 00:15:17,340
the final result, cleaning up what was messy, never mentioning all of the wrong turns, underselling

215
00:15:17,340 --> 00:15:21,980
just how vast the space of explorable possibilities is at the start of a problem solving process,

216
00:15:22,180 --> 00:15:22,860
all of that.

217
00:15:23,820 --> 00:15:24,900
But this is true in general.

218
00:15:24,900 --> 00:15:29,760
I think for some special inventions, there's a second, deeper reason that we underappreciate

219
00:15:29,760 --> 00:15:30,040
them.

220
00:15:30,840 --> 00:15:34,940
Thinking of information in terms of bits had only really coalesced into a full theory by

221
00:15:34,940 --> 00:15:38,640
1948, with Claude Shannon's seminal paper on information theory.

222
00:15:39,280 --> 00:15:42,540
This was essentially concurrent with when Hamming developed his algorithm.

223
00:15:43,300 --> 00:15:47,240
This was the same foundational paper that showed, in a certain sense, that efficient

224
00:15:47,240 --> 00:15:52,160
error correction is always possible, no matter how high the probability of bit flips, at

225
00:15:52,160 --> 00:15:52,900
least in theory.

226
00:15:53,700 --> 00:15:58,160
Shannon and Hamming, by the way, shared an office in Bell Labs, despite working on very

227
00:15:58,160 --> 00:16:01,160
different things, which hardly seems coincidental here.

228
00:16:02,380 --> 00:16:07,040
Fast forward several decades, and these days, many of us are so immersed in thinking about

229
00:16:07,040 --> 00:16:12,340
bits and information that it's easy to overlook just how distinct this way of thinking was.

230
00:16:13,100 --> 00:16:17,540
Ironically, the ideas that most profoundly shape the ways that a future generation thinks

231
00:16:17,540 --> 00:16:22,560
will end up looking to that future generation simpler than they really are.

