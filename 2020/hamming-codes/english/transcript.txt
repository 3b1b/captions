Have you ever wondered how it's possible to scratch a CD or a DVD and still have it play back whatever it's storing? 
The scratch really does affect the 1s and 0s on the disk, so it reads off different data from what was stored, but unless it's really scratched up, the bits it reads off are decoded into precisely the same file that was encoded onto it, a bit for bit copy, despite all those errors. 
There is a whole pile of mathematical cleverness that allows us to store data, and just as importantly to transmit data, in a way that's resilient to errors. 
Well, actually it doesn't take that much cleverness to come up with a way to do this. 
Any file, whether it's a video, sound, text, code, image, whatever, is ultimately some sequence of 1s and 0s. 
nstead of yeses and nos, it literally spells out the position of the error in binary. For example, the number 7 in binary looks like 0111, essentially saying that it's 4 plus 2 plus 1. And notice
where the position 7 sits, it does affect the first of our parity groups, and the second, and the third, but not the last. So reading the results of those four checks from bottom to top indeed does spell out the position of the error. There's nothing specia
But what that means is using two thirds of your space for redundancy. 
And even then, for all of that space given up, there's no strong guarantee about what happens if more than one bit gets flipped. 
The much more interesting question is how to make it so that errors can be corrected while giving up as little space as possible. 
c for implementing the whole scheme in hardware shockingly simple. Now if you want to see why this magic happens, take these 16 index labels for our positions, but instead of writing them in base 10, let's write them all in binary, running from 0000 up to 1111. As we put these binary labels back into their boxes, let m
e emphasize that they are distinct from the data that's actually being sent. They're nothing more than a conceptual label to help you and m
And it will still be the case that if any bit gets flipped here, just by looking at this block and nothing more, a machine will be able to identify that there was an error and precisely where it was so that it knows how to correct it. 
And honestly, that feels like magic. 
And for this particular scheme, if two bits get flipped, the machine will at least be able to detect that there were two errors, though it won't know how to fix them. 
We'll talk a little bit later about how this scales for blocks with different sizes. 
where that's a 1, you get the second parity group from our scheme. In other words, that second check is asking, hey, me again, if there's an error, is the second to last bit of that position a 1? And
so on. The third parity check covers every position whose third to last bit is turned on, and the last one covers the last eight positions, those ones whose highest order bit is a 1. Everything we
The goal here is to give you a very thorough understanding of one of the earliest examples, known as a Hamming code. 
four questions, which in turn is the same as spelling out a position in binary. I hope this makes two things clearer. The first is how to systematically generalize to block sizes that are bigger powers of two. If it takes more bits to describe each position, like six bits to describe 64 spots, then each of those bits gives you one of the parity groups that we need to check.
So when you feel like you see where it's going at some point, take that moment to pause, actively predict what the scheme is going to be before I tell you. 
Also, if you want your understanding to get down to the hardware level, Ben Eater has made a video in conjunction with this one showing you how to actually implement Hamming codes on breadboards, which is extremely satisfying. 
You should know, Hamming codes are not as widely used as more modern codes, like the Reed-Solomon algorithm, but there is a certain magic to the contrast between just how impossible this task feels at the start, and how utterly reasonable it seems once you learn about Hamming. 
The basic principle of error correction is that in a vast space of all possible messages, only some subset are going to be considered valid messages. 
As an analogy, think about correctly spelled words versus incorrectly spelled words. 
also see this in larger examples, where no matter how big you get, each parity bit conveniently touches only one of the groups. Once you understand that these parity checks that we've focused so much of our time on are nothing more than a clever way to spell out the position of an error in binary, then we can draw a connection with a differ
ent way to think about hamming codes, one that is arguably a lot simpler and more elegant, and which can basically be written down with a single line of
code. It's based on the XOR function. XOR, for those of you who don't know, stands for exclusive or. When you take the XOR of two bits, it's going to return a 1 if either one of those bits is turned on, but not if both are turned on or off. Phrased differently
And the programs he kept putting through it kept failing, because every now and then a bit would get misread. 
Frustration being the crucible of invention, he got so fed up that he invented the world's first error correction code. 
There are many different ways to frame Hamming codes, but as a first pass we're going to go through it the way Hamming himself thought about them. 
Let's use an example that's simple, but not too simple, a block of 16 bits. 
We'll number the positions of these bits from 0 up to 15. 
bit representations of those numbers under the hood. The key point for you and me is that taking the XOR of many different bit strings is effectively a way to compute the parodies of a bunch of separate groups, like so with the columns, all in one fell swoop.
The word redundant here doesn't simply mean copy, after all, those 4 bits don't give us enough room to blindly copy the data. 
Instead, they'll need to be a much more nuanced and clever kind of redundancy, not adding any new information, but adding resilience. 
s, so it's effectively counting how many highlighted positions came from the first parity group. Does that make sense? Likewise, the next column counts how many positions are in the second parity group, the positions whose second to last bit is a 1, and which a
re also highlighted, and so on. It's really just a small shift in perspective on the same thing we've been doing. And so you know where it goes from here. The sender is responsible for toggling some
of the special parity bits to make sure the sum works out to be 0000. Now once we have it like this, this gives us a really nice way to think about why these four resulting bits at the bottom directly spell out the positio
Like any error correction algorithm, this will involve two players, a sender, who's responsible for setting these 4 special bits, and then a receiver, who's responsible for performing some kind of check and then correcting the errors. 
Of course, the words sender and receiver really refer to machines or software that's doing checks, and the idea of a message is meant really broadly, to include things like storage. 
After all, storing data is the same thing as sending a message, just from the past to the future, instead of from one place to another. 
So that's the setup, but before we can dive in, we need to talk about a related idea which was fresh on Hamming's mind in the time of his discovery, a method which lets you detect any single bit errors, but not to correct them, known in the business as a parity check. 
For a parity check, we separate out only one single bit that the sender is responsible for tuning, and the rest are free to carry a message. 
The only job of this special bit is to make sure that the total number of 1s in the message is an even number. 
So for example right now, that total number of 1s is 7, that's odd, so the sender needs to flip that special bit to be a 1, making the count even. 
But if the block had already started off with an even number of 1s, then this special bit would have been kept at a 0. 
This is pretty simple, deceptively simple, but it's an incredibly elegant way to distill the idea of change anywhere in a message to be reflected in a single bit of information. 
ctice this would be something we're receiving from a sender, and instead of being random it would be carrying 11 data bits together with 5 parity bits. If I call the function enumerateBits, what it does is pair together each of those bits with a corresponding index, in this case running from 0 up to 15.
So if you're the receiver, you look at this message, and you see an odd number of 1s, you can know for sure that some error has occurred, even though you might have no idea where it was. 
In the jargon, whether a group of bits has an even or an odd number of 1s is known as its parity. 
o collect together all of those positions, the positions of the bits that are turned on, and then XOR them together. To do this in Python, let me first import a couple helpful functions. That way we can call reduce() on this list, and use the XOR function to reduce it.
And actually, we should be clear, if the receiver sees an odd parity, it doesn't necessarily mean there was just one error, there might have been 3 errors or 5 or any other odd number, but they can know for sure that it wasn't 0. 
On the other hand, if there had been 2 errors, or any even number of errors, that final count of 1s would still be even, so the receiver can't have full confidence that an even count necessarily means the message is error-free. 
So at the moment it looks like if we do this on our random block of 16 bits, it returns 9, which has the binary representation 1001. We won't do it here, but you could write a function where the sender uses that binary representation to set the four parity bits as needed, ultimately getting this block to a state
where running this line of code on the full list of bits returns a 0. This would be considered a well-prepared block. What's cool is that if we toggle any one of the bits in this list, simulating a random error from noise, then if you run this same line of code, it print
s out that error. Isn't that neat? You could get this block from out of the blue, run this single line on it, and it'll automatically spit out the position of an error, or a 0 if there wasn't any. And there's nothing special about the size 16 here.
Instead, the goal is to come up with a scheme that's robust up to a certain maximum number of errors, or maybe to reduce the probability of a false positive like this. 
a parity check to detect 2-bit errors, but the idea is that almost all of the core logic from our scheme comes down to a single XOR reduction. Now, depending on your comfort with binary and XORs and software in general, you may either find this perspective a little bit confusing, or so much m
For example, as Hamming was searching for a way to identify where an error happened, not just that it happened, his key insight was that if you apply some parity checks not to the full message, but to certain carefully selected subsets, you can ask a more refined series of questions that pin down the location of any single bit error. 
The overall feeling is a bit like playing a game of 20 questions, asking yes or no queries that chop the space of possibilities in half. 
of the size of the block, or in other words, it grows one bit at a time as the block size doubles. The relevant fact here is that that information directly corresponds to how much redundancy we need. That's really what runs against most people's knee-jerk reaction
Then, if an error is detected, it gives the receiver a little more information about where specifically the error is, namely that it's in an odd position. 
ent to errors, where usually copying the whole message is the first instinct that comes to mind. And then, by the way, there is this whole other way that you sometimes see Hamming codes presented, where you multiply the message by one big matrix. It's kind of nice because it relates it to the broader family of linear cod
You might think that limiting a parity check to half the bits makes it less effective, but when it's done in conjunction with other well-chosen checks, it counter-intuitively gives us something a lot more powerful. 
To actually set up that parity check, remember, it requires earmarking some special bit that has control for the parity of that full group. 
Here let's just choose position 1. 
For the example shown, the parity of these 8 bits is currently odd, so the sender is responsible for toggling that parity bit, and now it's even. 
This is only 1 out of 4 parity checks that we'll do. 
The second check is among the 8 bits on the right half of the grid, at least as we've drawn it here. 
This time we might use position 2 as a parity bit. 
So these 8 bits already have an even parity, and the sender can feel good leaving that bit number 2 unchanged. 
d if you take that to an extreme, you could have a block with, say, a million bits, where you would quite literally be playing 20 questions with your parity checks, and it uses only 21 parity bits. And if you step back to think about looking at a million bits and locating a single error, that genuinely feels crazy. The problem,
Otherwise, it means either there's no error, or the error is somewhere on the left half. 
Or I guess there could have been two errors, but for right now we're going to assume that there's at most one error in the entire block. 
Things break down completely for more than that. 
Here, before we look at the next two checks, take a moment to think about what these first two allow us to do when you consider them together. 
Let's say you detect an error among the odd columns and among the right half. 
It necessarily means the error is somewhere in the last column. 
If there was no error in the odd column but there was one in the right half, that tells you it's in the second to last column. 
Likewise, if there is an error in the odd columns but not in the right half, you know that it's somewhere in the second column. 
like the much more commonly used Reed-Solomon algorithm, which handles burst errors particularly well, and it can be tuned to be resilient to a larger number of errors per block.
But it also might simply mean there's no error at all. 
Which is all a rather belabored way to say that two parity checks let us pin down the column. 
From here, you can probably guess what follows. 
spire in a way that spells out the position of an error only came to Hamming when he stepped back after a bunch of other analysis and asked, okay, what is the most eff
icient I could conceivably be about this? He was also candid about how important it was that parity che
So in this example, that group already has an even parity, so bit 4 would be set to a 0. 
it is today. There are like half a dozen times throughout this book that he references the Louis Pasteur quote, luck favors a prepared mind. Cl
In this case, it looks like the sender needs to turn that bit 8 on in order to give the group even parity. 
Part of the reason that clever ideas look deceptively easy is that we only ever see the final result, cleaning up what was messy, never mentioning all of the wrong
As an example, imagine that during the transmission there's an error at, say, position 3. 
Well, this affects the first parity group, and it also affects the second parity group, so the receiver knows that there's an error somewhere in that right column. 
But it doesn't affect the third group, and it doesn't affect the fourth group. 
And that lets the receiver pinpoint the error up to the first row, which necessarily means position 3, so they can fix the error. 
You might enjoy taking a moment to convince yourself that the answers to these four questions really will always let you pin down a specific location, no matter where they turn out to be. 
In fact, the astute among you might even notice a connection between these questions and binary counting. 
And if you do, again let me emphasize, pause, try for yourself to draw the connection before I spoil it. 
If you're wondering what happens if a parity bit itself gets affected, well, you can just try it. 
Take a moment to think about how any error among these four special bits is going to be tracked down just like any other, with the same group of four questions. 
It doesn't really matter, since at the end of the day what we want is to protect the message bits, the error correction bits are just riding along. 
But protecting those bits as well is something that naturally falls out of the scheme as a byproduct. 
You might also enjoy anticipating how this scales. 
If we used a block of size 256 bits, for example, in order to pin down a location, you need only eight yes or no questions to binary search your way down to some specific spot. 
And remember, each question requires giving up only a single bit to set the appropriate parity check. 
Some of you may already see it, but we'll talk later about the systematic way to find what these questions are in just a minute or two. 
Hopefully this sketch is enough to appreciate the efficiency of what we're developing here. 
Everything except for those eight highlighted parity bits can be whatever you want it to be, carrying whatever message or data you want. 
The eight bits are redundant in the sense that they're completely determined by the rest of the message, but it's in a much smarter way than simply copying the message as a whole. 
And still, for so little given up, you would be able to identify and fix any single bit error. 
Well, almost. 
Okay, so the one problem here is that if none of the four parity checks detect an error, meaning that the specially selected subsets of eight bits all have even parities, just like the sender intended, then it either means there was no error at all, or it narrows us down into position zero. 
You see, with four yes or no questions, we have 16 possible outcomes for our parity checks, and at first that feels perfect for pinpointing one out of 16 positions in the block, but you also need to communicate a 17th outcome, the no error condition. 
The solution here is actually pretty simple. 
Just forget about that zeroth bit entirely. 
So when we do our four parity checks and we see that they're all even, it unambiguously means that there is no error. 
What that means is rather than working with a 16-bit block, we work with a 15-bit block, where 11 of the bits are free to carry a message and four of them are there for redundancy. 
And with that, we now have what people in the business would refer to as a 15-11 Hamming code. 
That said, it is nice to have a block size that's a clean power of two, and there's a clever way that we can keep that zeroth bit around and get it to do a little extra work for us. 
If we use it as a parity bit across the whole block, it lets us actually detect, even though we can't correct, two-bit errors. 
Here's how it works. 
After setting those four special error correcting bits, we set that zeroth one so that the parity of the full block is even, just like a normal parity check. 
Now, if there's a single bit error, then the parity of the full block toggles to be odd, but we would catch that anyway, thanks to the four error correcting checks. 
However, if there's two errors, then the overall parity is going to toggle back to being even, but the receiver would still see that there's been at least some error because of what's going on with those four usual parity checks. 
So if they notice an even parity overall, but something non-zero happening with the other checks, it tells them there were at least two errors. 
Isn't that clever? 
Even though we can't correct those two-bit errors, just by putting that one little bothersome zeroth bit back to work, it lets us detect them. 
This is pretty standard, it's known as an extended Hamming code. 
Technically speaking, you now have a full description of what a Hamming code does, at least for the example of a 16-bit block, but I think you'll find it more satisfying to check your understanding and solidify everything up to this point by doing one full example from start to finish yourself. 
I'll step through it with you though so you can check yourself. 
To set up a message, whether that's a literal message that you're translating over space, or some data that you want to store over time, the first step is to divide it up into 11-bit chunks. 
Each chunk is going to get packaged into an error-resistant 16-bit block. 
So let's take this one as an example and actually work it out. 
Go ahead, actually do it! 
Pause and try putting together this block. 
Okay, you ready? 
Remember, position 0 along with the other powers of 2 are reserved for error correction duty, so you start by placing the message bits in all of the remaining spots, in order. 
You need this group to have an even parity, which it already does, so you should have set that parity bit in position 1 to be a 0. 
The next group starts off with an odd parity, so you should have set its parity bit to be 1. 
The group after that starts with an odd parity, so again you should have set its parity bit to 1. 
And the final group also has an odd parity, meaning we set that bit in position 8 to be a 1. 
And then as the final step, the full block now has an even parity, meaning that you can set that bit number 0, the overarching parity bit, to be 0. 
So as this block is sent off, the parity of the four special subsets and the block as a whole will all be even, or 0. 
As the second part of the exercise, let's have you play the role of the receiver. 
Of course, that would mean you don't already know what this message is. 
Maybe some of you memorized it, but let's assume that you haven't. 
What I'm going to do is change either 0, 1, or 2 of the bits in that block, and then ask you to figure out what it is that I did. 
So again, pause and try working it out. 
Okay, so you as the receiver now check the first parity group, and you can see that it's even, so any error that exists would have to be in an even column. 
The next check gives us an odd number, telling us both that there's at least one error, and narrowing us down into this specific column. 
The third check is even, chopping down the possibilities even further. 
And the last parity check is odd, telling us there's an error somewhere in the bottom, which by now we can see must be in position number 10. 
What's more, the parity of the whole block is odd, giving us confidence that there was one flip and not two. 
If it's three or more, all bets are off. 
After correcting that bit number 10, pulling out the 11 bits that were not used for correction gives us the relevant segment of the original message, which if you rewind and compare is indeed exactly what we started the example with. 
And now that you know how to do all this by hand, I'd like to show you how you can carry out the core part of all of this logic with a single line of Python code. 
You see, what I haven't told you yet is just how elegant this algorithm really is, how simple it is to get a machine to point to the position of an error, how to systematically scale it, and how we can frame all of this as one single operation rather than multiple separate parity checks. 
To see what I mean, come join me in part two.