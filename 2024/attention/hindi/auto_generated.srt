1
00:00:00,000 --> 00:00:01,949
पिछले अध्याय में, आपने और मैंने ट्रांसफार्मर की 

2
00:00:01,949 --> 00:00:04,019
आंतरिक कार्यप्रणाली के बारे में जानना शुरू किया था।

3
00:00:04,560 --> 00:00:07,982
यह बड़े भाषा मॉडल के अंदर प्रौद्योगिकी के प्रमुख टुकड़ों में से एक है, 

4
00:00:07,982 --> 00:00:10,200
और आधुनिक एआई की लहर में कई अन्य उपकरण भी हैं।

5
00:00:10,980 --> 00:00:14,490
यह पहली बार 2017 में अब प्रसिद्ध हुए एक शोधपत्र में सामने आया जिसका नाम है 

6
00:00:14,490 --> 00:00:18,001
अटेंशन इज ऑल यू नीड, और इस अध्याय में आप और मैं इस ध्यान तंत्र के बारे में 

7
00:00:18,001 --> 00:00:21,700
जानेंगे कि यह क्या है, तथा यह कल्पना करेंगे कि यह डेटा को कैसे संसाधित करता है।

8
00:00:26,140 --> 00:00:29,540
संक्षेप में, मैं चाहता हूं कि आपके ध्यान में एक महत्वपूर्ण संदर्भ हो।

9
00:00:30,000 --> 00:00:32,979
आप और मैं जिस मॉडल का अध्ययन कर रहे हैं उसका लक्ष्य पाठ का 

10
00:00:32,979 --> 00:00:36,060
एक अंश लेकर यह पूर्वानुमान लगाना है कि अगला शब्द कौन सा आएगा।

11
00:00:36,860 --> 00:00:39,770
इनपुट टेक्स्ट को छोटे-छोटे टुकड़ों में तोड़ दिया जाता है, 

12
00:00:39,770 --> 00:00:43,433
जिन्हें हम टोकन कहते हैं, और ये अक्सर शब्द या शब्दों के टुकड़े होते हैं, 

13
00:00:43,433 --> 00:00:47,498
लेकिन इस वीडियो में दिए गए उदाहरणों को आपके और मेरे लिए सोचना आसान बनाने के लिए, 

14
00:00:47,498 --> 00:00:50,560
आइए इसे सरल बनाते हुए मान लें कि टोकन हमेशा शब्द ही होते हैं।

15
00:00:51,480 --> 00:00:56,159
ट्रांसफार्मर में पहला चरण प्रत्येक टोकन को एक उच्च-आयामी वेक्टर के साथ जोड़ना है, 

16
00:00:56,159 --> 00:00:57,700
जिसे हम एम्बेडिंग कहते हैं।

17
00:00:57,700 --> 00:01:02,524
सबसे महत्वपूर्ण विचार जो मैं आपके दिमाग में रखना चाहता हूं वह यह है कि सभी संभावित 

18
00:01:02,524 --> 00:01:07,000
एम्बेडिंग के इस उच्च-आयामी स्थान में दिशाएं अर्थ के साथ कैसे मेल खा सकती हैं।

19
00:01:07,680 --> 00:01:11,777
पिछले अध्याय में हमने एक उदाहरण देखा था कि किस प्रकार दिशा लिंग के अनुरूप हो सकती है, 

20
00:01:11,777 --> 00:01:15,923
इस अर्थ में कि इस स्थान में एक निश्चित चरण जोड़ने से आप एक पुल्लिंग संज्ञा के सन्निहित 

21
00:01:15,923 --> 00:01:19,640
होने से लेकर उसके अनुरूप स्त्रीलिंग संज्ञा के सन्निहित होने तक पहुँच सकते हैं।

22
00:01:20,160 --> 00:01:23,750
यह सिर्फ एक उदाहरण है, आप कल्पना कर सकते हैं कि इस उच्च-आयामी अंतरिक्ष में 

23
00:01:23,750 --> 00:01:27,580
कितनी अन्य दिशाएं किसी शब्द के अर्थ के असंख्य अन्य पहलुओं के अनुरूप हो सकती हैं।

24
00:01:28,800 --> 00:01:33,081
ट्रांसफार्मर का उद्देश्य इन एम्बेडिंग को क्रमिक रूप से समायोजित करना है, 

25
00:01:33,081 --> 00:01:38,358
ताकि वे केवल एक शब्द को एनकोड न करें, बल्कि इसके बजाय वे कुछ अधिक समृद्ध प्रासंगिक अर्थों 

26
00:01:38,358 --> 00:01:39,180
को शामिल करें।

27
00:01:40,140 --> 00:01:43,200
मैं पहले ही कह देना चाहता हूं कि बहुत से लोगों को ध्यान तंत्र, 

28
00:01:43,200 --> 00:01:46,454
जो ट्रांसफार्मर का मुख्य हिस्सा है, बहुत भ्रमित करने वाला लगता है, 

29
00:01:46,454 --> 00:01:48,980
इसलिए यदि इसे समझने में कुछ समय लगे तो चिंता न करें।

30
00:01:49,440 --> 00:01:53,964
मेरा मानना है कि इससे पहले कि हम कम्प्यूटेशनल विवरण और मैट्रिक्स गुणन में उतरें, 

31
00:01:53,964 --> 00:01:58,936
कुछ उदाहरणों के बारे में सोचना उचित होगा कि हम किस प्रकार के व्यवहार पर ध्यान देना चाहते 

32
00:01:58,936 --> 00:01:59,160
हैं।

33
00:02:00,140 --> 00:02:02,969
अमेरिकन ट्रू मोल, कार्बन डाइऑक्साइड का एक मोल, 

34
00:02:02,969 --> 00:02:06,220
और मोल की बायोप्सी लेने जैसे वाक्यांशों पर विचार करें।

35
00:02:06,700 --> 00:02:08,961
आप और मैं जानते हैं कि इनमें से प्रत्येक में मोल 

36
00:02:08,961 --> 00:02:10,900
शब्द का अर्थ संदर्भ के आधार पर अलग-अलग है।

37
00:02:11,360 --> 00:02:16,369
लेकिन ट्रांसफार्मर के पहले चरण के बाद, जो पाठ को तोड़ता है और प्रत्येक टोकन को एक वेक्टर 

38
00:02:16,369 --> 00:02:20,366
के साथ जोड़ता है, मोल के साथ जुड़ा वेक्टर इन सभी मामलों में समान होगा, 

39
00:02:20,366 --> 00:02:25,375
क्योंकि यह प्रारंभिक टोकन एम्बेडिंग प्रभावी रूप से एक लुकअप टेबल है जिसमें संदर्भ का कोई 

40
00:02:25,375 --> 00:02:26,220
संदर्भ नहीं है।

41
00:02:26,620 --> 00:02:33,100
ट्रांसफार्मर के अगले चरण में ही आस-पास के एम्बेडिंग को इसमें सूचना भेजने का मौका मिलता है।

42
00:02:33,820 --> 00:02:38,315
आपके मन में यह चित्र हो सकता है कि इस एम्बेडिंग स्पेस में कई अलग-अलग दिशाएं हैं जो 

43
00:02:38,315 --> 00:02:40,968
मोल शब्द के कई अलग-अलग अर्थों को एनकोड करती हैं, 

44
00:02:40,968 --> 00:02:45,409
और एक अच्छी तरह से प्रशिक्षित ध्यान ब्लॉक गणना करता है कि संदर्भ के एक फ़ंक्शन के 

45
00:02:45,409 --> 00:02:49,850
रूप में आपको जेनेरिक एम्बेडिंग में क्या जोड़ने की आवश्यकता है ताकि इसे इन विशिष्ट 

46
00:02:49,850 --> 00:02:51,800
दिशाओं में से एक में ले जाया जा सके।

47
00:02:53,300 --> 00:02:56,180
दूसरा उदाहरण लें, टावर शब्द के समावेश पर विचार करें।

48
00:02:57,060 --> 00:03:00,925
यह संभवतः अंतरिक्ष में कोई बहुत ही सामान्य, गैर-विशिष्ट दिशा है, 

49
00:03:00,925 --> 00:03:03,720
जो कई अन्य बड़ी, लंबी संज्ञाओं से जुड़ी हुई है।

50
00:03:04,020 --> 00:03:09,014
यदि इस शब्द के ठीक पहले एफिल आता, तो आप कल्पना कर सकते थे कि तंत्र इस वेक्टर को अद्यतन 

51
00:03:09,014 --> 00:03:14,008
करना चाहेगा, ताकि यह उस दिशा में इंगित करे जो अधिक विशिष्ट रूप से एफिल टॉवर को दर्शाता 

52
00:03:14,008 --> 00:03:19,060
हो, शायद पेरिस और फ्रांस तथा स्टील से बनी चीजों से संबंधित वेक्टरों के साथ सहसंबंधित हो।

53
00:03:19,920 --> 00:03:24,952
यदि इसके पहले मिनिएचर शब्द भी आता है, तो वेक्टर को और भी अद्यतन किया जाना चाहिए, 

54
00:03:24,952 --> 00:03:27,500
ताकि यह बड़ी, ऊंची चीजों से संबंधित न हो।

55
00:03:29,480 --> 00:03:32,296
सामान्यतः किसी शब्द के अर्थ को परिष्कृत करने के बजाय, 

56
00:03:32,296 --> 00:03:36,833
ध्यान ब्लॉक मॉडल को एक एम्बेडिंग में एनकोड की गई जानकारी को दूसरे में स्थानांतरित करने 

57
00:03:36,833 --> 00:03:39,858
की अनुमति देता है, संभवतः वे जानकारी जो काफी दूर होती है, 

58
00:03:39,858 --> 00:03:43,300
और संभवतः ऐसी जानकारी जो केवल एक शब्द से कहीं अधिक समृद्ध होती है।

59
00:03:43,300 --> 00:03:48,252
पिछले अध्याय में हमने देखा था कि जब सभी सदिश नेटवर्क से होकर प्रवाहित होते हैं, 

60
00:03:48,252 --> 00:03:51,223
जिसमें अनेक विभिन्न ध्यान ब्लॉक शामिल होते हैं, 

61
00:03:51,223 --> 00:03:54,999
तो अगले टोकन का पूर्वानुमान करने के लिए आप जो गणना करते हैं, 

62
00:03:54,999 --> 00:03:58,280
वह पूरी तरह से अनुक्रम में अंतिम सदिश का फलन होती है।

63
00:03:59,100 --> 00:04:03,227
उदाहरण के लिए, कल्पना कीजिए कि आपने जो पाठ डाला है वह एक संपूर्ण 

64
00:04:03,227 --> 00:04:07,800
रहस्य उपन्यास का अधिकांश भाग है, अंत तक, जहां लिखा है, इसलिए हत्यारा था।

65
00:04:08,400 --> 00:04:12,708
यदि मॉडल अगले शब्द का सटीक पूर्वानुमान लगाने जा रहा है, तो अनुक्रम में अंतिम वेक्टर, 

66
00:04:12,708 --> 00:04:15,648
जिसने अपना जीवन केवल शब्द को एम्बेड करने से शुरू किया था, 

67
00:04:15,648 --> 00:04:19,602
को सभी ध्यान ब्लॉकों द्वारा अद्यतन किया जाना होगा ताकि किसी भी व्यक्तिगत शब्द 

68
00:04:19,602 --> 00:04:22,086
की तुलना में बहुत अधिक प्रतिनिधित्व किया जा सके, 

69
00:04:22,086 --> 00:04:25,989
किसी भी तरह से पूरे संदर्भ विंडो से सभी जानकारी को एनकोड किया जा सके जो अगले 

70
00:04:25,989 --> 00:04:28,220
शब्द की भविष्यवाणी करने के लिए प्रासंगिक है।

71
00:04:29,500 --> 00:04:32,580
हालाँकि, गणना को आगे बढ़ाने के लिए, आइए एक बहुत ही सरल उदाहरण लेते हैं।

72
00:04:32,980 --> 00:04:35,419
कल्पना कीजिए कि इनपुट में यह वाक्यांश शामिल है, 

73
00:04:35,419 --> 00:04:37,960
एक रोयेंदार नीला प्राणी हरे-भरे जंगल में घूमता था।

74
00:04:38,460 --> 00:04:42,650
और फिलहाल, मान लीजिए कि एकमात्र प्रकार का अपडेट जिसकी हमें परवाह है, 

75
00:04:42,650 --> 00:04:46,780
वह है विशेषणों द्वारा उनके संगत संज्ञाओं के अर्थों को समायोजित करना।

76
00:04:47,000 --> 00:04:50,299
मैं जो वर्णन करने जा रहा हूँ, उसे हम ध्यान का एक एकल शीर्ष कहेंगे, 

77
00:04:50,299 --> 00:04:54,287
और बाद में हम देखेंगे कि किस प्रकार ध्यान खंड में समानांतर रूप से चलने वाले अनेक 

78
00:04:54,287 --> 00:04:55,420
विभिन्न शीर्ष होते हैं।

79
00:04:56,140 --> 00:04:59,760
पुनः, प्रत्येक शब्द के लिए प्रारंभिक एम्बेडिंग कुछ उच्च आयामी वेक्टर है 

80
00:04:59,760 --> 00:05:03,380
जो केवल उस विशेष शब्द के अर्थ को ही एनकोड करता है, कोई संदर्भ नहीं देता।

81
00:05:04,000 --> 00:05:05,220
दरअसल, यह बिल्कुल सच नहीं है।

82
00:05:05,380 --> 00:05:07,640
वे शब्द की स्थिति को भी कूटबद्ध करते हैं।

83
00:05:07,980 --> 00:05:11,834
पदों को कोडित करने के तरीके के बारे में कहने को बहुत कुछ है, लेकिन अभी, 

84
00:05:11,834 --> 00:05:15,474
आपको बस इतना जानना है कि इस वेक्टर की प्रविष्टियाँ आपको यह बताने के 

85
00:05:15,474 --> 00:05:18,900
लिए पर्याप्त हैं कि शब्द क्या है और संदर्भ में यह कहाँ मौजूद है।

86
00:05:19,500 --> 00:05:21,660
आइये आगे बढ़ें और इन एम्बेडिंग्स को अक्षर e से निरूपित करें।

87
00:05:22,420 --> 00:05:26,121
इसका लक्ष्य यह है कि संगणनाओं की एक श्रृंखला के माध्यम से अंतःस्थापित 

88
00:05:26,121 --> 00:05:29,665
शब्दों का एक नया परिष्कृत सेट तैयार किया जाए, जहां, उदाहरण के लिए, 

89
00:05:29,665 --> 00:05:33,420
संज्ञाओं के संगत शब्दों ने अपने संगत विशेषणों से अर्थ ग्रहण कर लिया हो।

90
00:05:33,900 --> 00:05:37,340
और गहन शिक्षण का खेल खेलते हुए, हम चाहते हैं कि इसमें सम्मिलित अधिकांश 

91
00:05:37,340 --> 00:05:39,715
संगणनाएं मैट्रिक्स-वेक्टर उत्पादों की तरह दिखें, 

92
00:05:39,715 --> 00:05:43,980
जहां मैट्रिक्स ट्यूनेबल भारों से भरे हों, ऐसी चीजें जिन्हें मॉडल डेटा के आधार पर सीखेगा।

93
00:05:44,660 --> 00:05:47,231
स्पष्ट रूप से कहें तो, मैं संज्ञाओं को अद्यतन करने वाले विशेषणों का 

94
00:05:47,231 --> 00:05:49,877
यह उदाहरण केवल यह दर्शाने के लिए बना रहा हूँ कि आप एक ध्यान केन्द्रित 

95
00:05:49,877 --> 00:05:52,260
करने वाले व्यक्ति द्वारा किस प्रकार का व्यवहार किया जा सकता है।

96
00:05:52,860 --> 00:05:56,144
अधिकांश गहन शिक्षण की तरह, वास्तविक व्यवहार को समझना बहुत कठिन है, 

97
00:05:56,144 --> 00:06:00,261
क्योंकि यह कुछ लागत फ़ंक्शन को न्यूनतम करने के लिए बहुत सारे मापदंडों में फेरबदल और 

98
00:06:00,261 --> 00:06:01,340
ट्यूनिंग पर आधारित है।

99
00:06:01,680 --> 00:06:05,402
बात बस इतनी है कि जैसे-जैसे हम इस प्रक्रिया में शामिल मापदंडों से भरे 

100
00:06:05,402 --> 00:06:09,231
विभिन्न मैट्रिसेस से गुजरते हैं, मुझे लगता है कि किसी ऐसी चीज का कल्पित 

101
00:06:09,231 --> 00:06:13,220
उदाहरण होना बहुत उपयोगी है जो इसे और अधिक ठोस बनाए रखने में मदद कर सकती है।

102
00:06:14,140 --> 00:06:17,996
इस प्रक्रिया के पहले चरण में, आप कल्पना कर सकते हैं कि प्रत्येक संज्ञा, 

103
00:06:17,996 --> 00:06:21,960
जैसे प्राणी, यह प्रश्न पूछ रही है कि, क्या मेरे सामने कोई विशेषण मौजूद है?

104
00:06:22,160 --> 00:06:25,714
और शराबी और नीले शब्दों के लिए, प्रत्येक को जवाब देने में सक्षम होना चाहिए, 

105
00:06:25,714 --> 00:06:27,960
हाँ, मैं एक विशेषण हूं और मैं उस स्थिति में हूं।

106
00:06:28,960 --> 00:06:33,817
यह प्रश्न किसी तरह एक अन्य वेक्टर, संख्याओं की एक अन्य सूची के रूप में एनकोडेड है, 

107
00:06:33,817 --> 00:06:36,100
जिसे हम इस शब्द के लिए क्वेरी कहते हैं।

108
00:06:36,980 --> 00:06:42,020
यद्यपि इस क्वेरी वेक्टर का आयाम एम्बेडिंग वेक्टर की तुलना में बहुत छोटा है, मान लीजिए 128।

109
00:06:42,940 --> 00:06:46,235
इस क्वेरी की गणना एक निश्चित मैट्रिक्स लेने जैसा है, 

110
00:06:46,235 --> 00:06:49,780
जिसे मैं wq लेबल करूंगा, और इसे एम्बेडिंग से गुणा करूंगा।

111
00:06:50,960 --> 00:06:54,994
चीजों को थोड़ा संक्षिप्त करते हुए, आइए उस क्वेरी वेक्टर को q के रूप में लिखें, 

112
00:06:54,994 --> 00:06:58,875
और फिर जब भी आप मुझे इस तरह के तीर के बगल में एक मैट्रिक्स डालते हुए देखें, 

113
00:06:58,875 --> 00:07:02,297
तो इसका मतलब यह है कि इस मैट्रिक्स को तीर के प्रारंभ में वेक्टर से 

114
00:07:02,297 --> 00:07:04,800
गुणा करने पर आपको तीर के अंत में वेक्टर मिलता है।

115
00:07:05,860 --> 00:07:09,850
इस मामले में, आप इस मैट्रिक्स को संदर्भ में सभी एम्बेडिंग से गुणा करते हैं, 

116
00:07:09,850 --> 00:07:12,580
जिससे प्रत्येक टोकन के लिए एक क्वेरी वेक्टर बनता है।

117
00:07:13,740 --> 00:07:16,030
इस मैट्रिक्स की प्रविष्टियाँ मॉडल के पैरामीटर हैं, 

118
00:07:16,030 --> 00:07:19,263
जिसका अर्थ है कि वास्तविक व्यवहार डेटा से सीखा जाता है, और व्यवहार में, 

119
00:07:19,263 --> 00:07:21,688
यह मैट्रिक्स किसी विशेष ध्यान शीर्ष में क्या करता है, 

120
00:07:21,688 --> 00:07:23,440
इसका विश्लेषण करना चुनौतीपूर्ण होता है।

121
00:07:23,900 --> 00:07:28,122
लेकिन हमारे लिए, एक उदाहरण की कल्पना करते हुए, जिससे हम आशा कर सकते हैं कि यह सीखेगा, 

122
00:07:28,122 --> 00:07:31,706
हम मान लेंगे कि यह क्वेरी मैट्रिक्स इस छोटे क्वेरी स्पेस में संज्ञाओं के 

123
00:07:31,706 --> 00:07:34,995
एम्बेडिंग को कुछ दिशाओं में मैप करता है, जो किसी तरह से पूर्ववर्ती 

124
00:07:34,995 --> 00:07:38,040
स्थितियों में विशेषणों की तलाश करने की धारणा को एनकोड करता है।

125
00:07:38,780 --> 00:07:41,440
यह अन्य एम्बेडिंग के साथ क्या करता है, कौन जानता है?

126
00:07:41,720 --> 00:07:44,340
हो सकता है कि यह इनके साथ-साथ कुछ अन्य लक्ष्य भी पूरा करने का प्रयास करता हो।

127
00:07:44,540 --> 00:07:47,160
फिलहाल, हम अपना पूरा ध्यान संज्ञाओं पर केन्द्रित कर रहे हैं।

128
00:07:47,280 --> 00:07:52,116
इसी समय, इसके साथ एक दूसरा मैट्रिक्स जुड़ा होता है जिसे कुंजी मैट्रिक्स कहा जाता है, 

129
00:07:52,116 --> 00:07:54,620
जिसे आप प्रत्येक एम्बेडिंग से गुणा करते हैं।

130
00:07:55,280 --> 00:07:58,500
इससे सदिशों का दूसरा अनुक्रम उत्पन्न होता है जिसे हम कुंजियाँ कहते हैं।

131
00:07:59,420 --> 00:08:01,368
वैचारिक रूप से, आप कुंजियों को संभावित रूप से प्रश्नों 

132
00:08:01,368 --> 00:08:03,140
का उत्तर देने वाली कुंजी के रूप में सोचना चाहेंगे।

133
00:08:03,840 --> 00:08:08,302
यह कुंजी मैट्रिक्स भी ट्यूनेबल पैरामीटरों से भरा हुआ है, और क्वेरी मैट्रिक्स की तरह, 

134
00:08:08,302 --> 00:08:11,400
यह एम्बेडिंग वैक्टर को उसी छोटे आयामी स्थान पर मैप करता है।

135
00:08:12,200 --> 00:08:14,857
जब भी कुंजियाँ एक दूसरे के साथ निकटता से संरेखित होती हैं, 

136
00:08:14,857 --> 00:08:17,020
तो आप उन्हें प्रश्नों से मेल खाते हुए समझते हैं।

137
00:08:17,460 --> 00:08:20,469
हमारे उदाहरण में, आप कल्पना करेंगे कि कुंजी मैट्रिक्स शराबी 

138
00:08:20,469 --> 00:08:23,529
और नीले जैसे विशेषणों को उन वैक्टरों में मैप करता है जो शब्द 

139
00:08:23,529 --> 00:08:26,740
प्राणी द्वारा उत्पादित क्वेरी के साथ निकटता से संरेखित होते हैं।

140
00:08:27,200 --> 00:08:30,774
यह मापने के लिए कि प्रत्येक कुंजी प्रत्येक क्वेरी से कितनी अच्छी तरह मेल खाती है, 

141
00:08:30,774 --> 00:08:34,000
आप प्रत्येक संभावित कुंजी-क्वेरी जोड़ी के बीच डॉट उत्पाद की गणना करते हैं।

142
00:08:34,480 --> 00:08:37,544
मैं बिंदुओं के एक समूह से भरे ग्रिड की कल्पना करना पसंद करता हूं, 

143
00:08:37,544 --> 00:08:40,145
जहां बड़े बिंदु बड़े बिंदु उत्पादों के अनुरूप होते हैं, 

144
00:08:40,145 --> 00:08:42,559
वे स्थान जहां कुंजियां और क्वेरीज़ संरेखित होती हैं।

145
00:08:43,280 --> 00:08:46,736
हमारे विशेषण संज्ञा उदाहरण के लिए, यह कुछ इस तरह दिखेगा, 

146
00:08:46,736 --> 00:08:51,770
जहां यदि फ्लफी और ब्लू द्वारा उत्पादित कुंजियां वास्तव में क्रिएचर द्वारा उत्पादित 

147
00:08:51,770 --> 00:08:56,803
क्वेरी के साथ निकटता से संरेखित होती हैं, तो इन दो स्थानों में डॉट उत्पाद कुछ बड़ी 

148
00:08:56,803 --> 00:08:58,320
सकारात्मक संख्याएं होंगी।

149
00:08:59,100 --> 00:09:02,100
भाषा की भाषा में, मशीन लर्निंग वाले लोग कहेंगे कि इसका मतलब है कि 

150
00:09:02,100 --> 00:09:05,420
रोयेंदार और नीले रंग की एम्बेडिंग, प्राणी की एम्बेडिंग में शामिल होती है।

151
00:09:06,040 --> 00:09:10,381
इसके विपरीत, कुछ अन्य शब्द जैसे कि &#39;द&#39; के लिए कुंजी और &#39;क्रिएचर&#39; 

152
00:09:10,381 --> 00:09:13,812
के लिए क्वेरी के बीच डॉट उत्पाद में कुछ छोटे या नकारात्मक मूल्य 

153
00:09:13,812 --> 00:09:16,600
होंगे जो दर्शाते हैं कि वे एक दूसरे से असंबंधित हैं।

154
00:09:17,700 --> 00:09:21,276
इस प्रकार हमारे पास मानों का एक ग्रिड है जो ऋणात्मक अनंत से अनंत तक कोई 

155
00:09:21,276 --> 00:09:24,853
भी वास्तविक संख्या हो सकती है, जिससे हमें यह स्कोर मिलता है कि प्रत्येक 

156
00:09:24,853 --> 00:09:28,480
शब्द प्रत्येक अन्य शब्द के अर्थ को अद्यतन करने के लिए कितना प्रासंगिक है।

157
00:09:29,200 --> 00:09:32,446
जिस तरह से हम इन अंकों का उपयोग करने जा रहे हैं वह यह है कि प्रत्येक कॉलम 

158
00:09:32,446 --> 00:09:35,780
के लिए एक निश्चित भारित राशि ली जाए, जो प्रासंगिकता के आधार पर निर्धारित हो।

159
00:09:36,520 --> 00:09:40,188
अतः, मानों को ऋणात्मक अनंत से अनंत तक रखने के स्थान पर, 

160
00:09:40,188 --> 00:09:44,053
हम चाहते हैं कि इन स्तंभों में संख्याएं 0 से 1 के बीच हों, 

161
00:09:44,053 --> 00:09:48,180
तथा प्रत्येक स्तंभ का योग 1 हो, मानो वे एक संभाव्यता वितरण हों।

162
00:09:49,280 --> 00:09:52,220
यदि आप पिछले अध्याय से आ रहे हैं, तो आप जानते हैं कि हमें क्या करना है।

163
00:09:52,620 --> 00:09:55,109
हम मानों को सामान्य करने के लिए इनमें से प्रत्येक 

164
00:09:55,109 --> 00:09:57,300
स्तंभ के साथ एक सॉफ्टमैक्स की गणना करते हैं।

165
00:10:00,060 --> 00:10:03,374
हमारे चित्र में, जब आप सभी स्तंभों पर सॉफ्टमैक्स लागू कर देंगे, 

166
00:10:03,374 --> 00:10:05,860
तो हम ग्रिड को इन सामान्यीकृत मानों से भर देंगे।

167
00:10:06,780 --> 00:10:10,578
इस बिंदु पर आप प्रत्येक कॉलम के बारे में यह सोच सकते हैं कि उसे इस आधार पर 

168
00:10:10,578 --> 00:10:14,580
भार दिया जाए कि बाईं ओर का शब्द शीर्ष पर दिए गए संगत मान से कितना प्रासंगिक है।

169
00:10:15,080 --> 00:10:16,840
हम इस ग्रिड को ध्यान पैटर्न कहते हैं।

170
00:10:18,080 --> 00:10:20,382
अब यदि आप मूल ट्रांसफार्मर पेपर को देखें तो पाएंगे 

171
00:10:20,382 --> 00:10:22,820
कि इसमें बहुत ही संक्षिप्त तरीके से यह सब लिखा गया है।

172
00:10:23,880 --> 00:10:29,136
यहां चर q और k क्रमशः क्वेरी और कुंजी वैक्टर की पूर्ण सरणी का प्रतिनिधित्व करते हैं, 

173
00:10:29,136 --> 00:10:34,640
वे छोटे वैक्टर जो आपको क्वेरी और कुंजी मैट्रिसेस द्वारा एम्बेडिंग को गुणा करके मिलते हैं।

174
00:10:35,160 --> 00:10:39,141
अंश में यह अभिव्यक्ति, कुंजियों और प्रश्नों के जोड़ों के बीच सभी संभावित डॉट 

175
00:10:39,141 --> 00:10:43,020
उत्पादों के ग्रिड का प्रतिनिधित्व करने का वास्तव में एक संक्षिप्त तरीका है।

176
00:10:44,000 --> 00:10:47,009
एक छोटा सा तकनीकी विवरण, जिसका मैंने उल्लेख नहीं किया, 

177
00:10:47,009 --> 00:10:50,183
वह यह है कि संख्यात्मक स्थिरता के लिए, इन सभी मानों को उस 

178
00:10:50,183 --> 00:10:53,960
कुंजी क्वेरी स्थान में आयाम के वर्गमूल से विभाजित करना सहायक होता है।

179
00:10:54,480 --> 00:10:57,966
फिर यह सॉफ्टमैक्स जो पूर्ण अभिव्यक्ति के चारों ओर लिपटा हुआ है, 

180
00:10:57,966 --> 00:11:00,800
उसे स्तंभ दर स्तंभ लागू करने के लिए समझा जाना चाहिए।

181
00:11:01,640 --> 00:11:04,700
जहां तक वी शब्द का सवाल है, हम इसके बारे में बस एक सेकंड में बात करेंगे।

182
00:11:05,020 --> 00:11:08,460
इससे पहले, एक अन्य तकनीकी विवरण है जिसे मैंने अब तक छोड़ दिया है।

183
00:11:09,040 --> 00:11:13,512
प्रशिक्षण प्रक्रिया के दौरान, जब आप इस मॉडल को किसी दिए गए पाठ उदाहरण पर चलाते हैं, 

184
00:11:13,512 --> 00:11:18,037
और सभी भारों को थोड़ा समायोजित किया जाता है और इस आधार पर उसे पुरस्कृत या दंडित किया 

185
00:11:18,037 --> 00:11:22,189
जाता है कि वह गद्यांश में सही अगले शब्द को कितनी उच्च संभावना प्रदान करता है, 

186
00:11:22,189 --> 00:11:26,608
तो यह पूरी प्रशिक्षण प्रक्रिया को बहुत अधिक कुशल बनाता है यदि आप एक साथ इस गद्यांश 

187
00:11:26,608 --> 00:11:31,080
में टोकन के प्रत्येक प्रारंभिक उप-अनुक्रम के बाद हर संभावित अगले टोकन की भविष्यवाणी 

188
00:11:31,080 --> 00:11:31,560
करते हैं।

189
00:11:31,940 --> 00:11:34,813
उदाहरण के लिए, जिस वाक्यांश पर हम ध्यान केंद्रित कर रहे हैं, 

190
00:11:34,813 --> 00:11:38,487
वह यह भी अनुमान लगा सकता है कि कौन से शब्द प्राणी के बाद आएंगे और कौन से शब्द 

191
00:11:38,487 --> 00:11:39,100
के बाद आएंगे।

192
00:11:39,940 --> 00:11:42,669
यह वास्तव में अच्छा है, क्योंकि इसका अर्थ यह है कि जो अन्यथा एक एकल 

193
00:11:42,669 --> 00:11:45,560
प्रशिक्षण उदाहरण होता है, वह प्रभावी रूप से कई के रूप में कार्य करता है।

194
00:11:46,100 --> 00:11:49,366
हमारे ध्यान पैटर्न के प्रयोजनों के लिए, इसका अर्थ यह है कि आप कभी भी 

195
00:11:49,366 --> 00:11:53,058
बाद के शब्दों को पहले के शब्दों को प्रभावित करने की अनुमति नहीं देना चाहेंगे, 

196
00:11:53,058 --> 00:11:56,040
क्योंकि अन्यथा वे आगे क्या होने वाला है इसका उत्तर दे सकते हैं।

197
00:11:56,560 --> 00:11:59,015
इसका अर्थ यह है कि हम चाहते हैं कि यहां सभी स्पॉट, 

198
00:11:59,015 --> 00:12:02,240
जो बाद के टोकनों को दर्शाते हैं और पहले वाले को प्रभावित करते हैं, 

199
00:12:02,240 --> 00:12:04,600
उन्हें किसी तरह शून्य होने के लिए बाध्य किया जाए।

200
00:12:05,920 --> 00:12:09,101
सबसे सरल बात जो आप सोच सकते हैं वह है उन्हें शून्य के बराबर सेट करना, 

201
00:12:09,101 --> 00:12:12,420
लेकिन यदि आपने ऐसा किया तो कॉलम एक नहीं होंगे, वे सामान्यीकृत नहीं होंगे।

202
00:12:13,120 --> 00:12:16,577
इसलिए, ऐसा करने का एक सामान्य तरीका यह है कि सॉफ्टमैक्स लागू करने से पहले, 

203
00:12:16,577 --> 00:12:19,020
आप उन सभी प्रविष्टियों को ऋणात्मक अनंत पर सेट कर दें।

204
00:12:19,680 --> 00:12:22,306
यदि आप ऐसा करते हैं, तो सॉफ्टमैक्स लागू करने के बाद, 

205
00:12:22,306 --> 00:12:25,180
वे सभी शून्य हो जाते हैं, लेकिन कॉलम सामान्यीकृत रहते हैं।

206
00:12:26,000 --> 00:12:27,540
इस प्रक्रिया को मास्किंग कहा जाता है।

207
00:12:27,540 --> 00:12:30,572
ध्यान के कुछ ऐसे संस्करण हैं जहां आप इसे लागू नहीं करते हैं, 

208
00:12:30,572 --> 00:12:34,549
लेकिन हमारे GPT उदाहरण में, भले ही यह प्रशिक्षण चरण के दौरान अधिक प्रासंगिक है, 

209
00:12:34,549 --> 00:12:38,079
जैसे कि इसे चैटबॉट या कुछ इस तरह से चलाना, आप बाद के टोकन को पहले वाले 

210
00:12:38,079 --> 00:12:41,460
को प्रभावित करने से रोकने के लिए हमेशा इस मास्किंग को लागू करते हैं।

211
00:12:42,480 --> 00:12:46,122
इस ध्यान पैटर्न के बारे में विचार करने लायक एक और तथ्य 

212
00:12:46,122 --> 00:12:49,500
यह है कि इसका आकार संदर्भ आकार के वर्ग के बराबर है।

213
00:12:49,900 --> 00:12:53,979
इसलिए, बड़े भाषा मॉडलों के लिए संदर्भ का आकार वास्तव में एक बहुत बड़ी बाधा हो सकती है, 

214
00:12:53,979 --> 00:12:55,620
और इसे बढ़ाना कोई आसान काम नहीं है।

215
00:12:56,300 --> 00:13:00,339
जैसा कि आप कल्पना कर सकते हैं, बड़े से बड़े संदर्भ विंडो की इच्छा से प्रेरित होकर, 

216
00:13:00,339 --> 00:13:04,232
हाल के वर्षों में संदर्भ को और अधिक मापनीय बनाने के उद्देश्य से ध्यान तंत्र में 

217
00:13:04,232 --> 00:13:08,320
कुछ बदलाव देखे गए हैं, लेकिन यहां, आप और मैं मूल बातों पर ध्यान केंद्रित कर रहे हैं।

218
00:13:10,560 --> 00:13:12,978
ठीक है, बढ़िया, इस पैटर्न की गणना करने से मॉडल यह पता लगा 

219
00:13:12,978 --> 00:13:15,480
सकता है कि कौन से शब्द किन अन्य शब्दों के लिए प्रासंगिक हैं।

220
00:13:16,020 --> 00:13:18,634
अब आपको वास्तव में एम्बेडिंग को अपडेट करने की आवश्यकता है, 

221
00:13:18,634 --> 00:13:22,046
जिससे शब्दों को उन अन्य शब्दों तक सूचना पहुंचाने की अनुमति मिल सके जिनके लिए 

222
00:13:22,046 --> 00:13:22,800
वे प्रासंगिक हैं।

223
00:13:22,800 --> 00:13:26,725
उदाहरण के लिए, आप चाहते हैं कि फ्लफी की एम्बेडिंग किसी तरह से प्राणी 

224
00:13:26,725 --> 00:13:30,480
में परिवर्तन लाए जो उसे इस 12,000-आयामी एम्बेडिंग स्पेस के एक अलग 

225
00:13:30,480 --> 00:13:34,520
हिस्से में ले जाए जो अधिक विशिष्ट रूप से फ्लफी प्राणी को एनकोड करता है।

226
00:13:35,460 --> 00:13:39,322
यहां मैं सबसे पहले आपको ऐसा करने का सबसे सरल तरीका दिखाने जा रहा हूं, 

227
00:13:39,322 --> 00:13:43,460
हालांकि बहु-सिर वाले ध्यान के संदर्भ में इसमें थोड़ा बदलाव किया जा सकता है।

228
00:13:44,080 --> 00:13:48,812
इसका सबसे सरल तरीका तीसरे मैट्रिक्स का उपयोग करना होगा, जिसे हम मूल्य मैट्रिक्स कहते हैं, 

229
00:13:48,812 --> 00:13:52,440
जिसे आप पहले शब्द के एम्बेडिंग से गुणा करते हैं, उदाहरण के लिए फ्लफी।

230
00:13:53,300 --> 00:13:57,461
इसका परिणाम वह है जिसे आप मूल्य सदिश कहेंगे, और यह कुछ ऐसा है जिसे आप दूसरे शब्द के 

231
00:13:57,461 --> 00:14:01,920
एम्बेडिंग में जोड़ते हैं, इस मामले में कुछ ऐसा जिसे आप प्राणी के एम्बेडिंग में जोड़ते हैं।

232
00:14:02,600 --> 00:14:07,000
अतः यह मान वेक्टर एम्बेडिंग के समान ही उच्च-आयामी स्थान में रहता है।

233
00:14:07,460 --> 00:14:10,847
जब आप इस मान मैट्रिक्स को किसी शब्द के एम्बेडिंग से गुणा करते हैं, 

234
00:14:10,847 --> 00:14:14,234
तो आप इसे इस प्रकार सोच सकते हैं कि, यदि यह शब्द किसी अन्य चीज़ के 

235
00:14:14,234 --> 00:14:17,621
अर्थ को समायोजित करने के लिए प्रासंगिक है, तो इसे प्रतिबिंबित करने 

236
00:14:17,621 --> 00:14:21,160
के लिए उस अन्य चीज़ के एम्बेडिंग में वास्तव में क्या जोड़ा जाना चाहिए?

237
00:14:22,140 --> 00:14:26,294
अपने आरेख पर वापस नजर डालते हुए, आइए सभी कुंजियों और प्रश्नों को अलग रख दें, 

238
00:14:26,294 --> 00:14:30,179
क्योंकि ध्यान पैटर्न की गणना करने के बाद आपने उनका काम पूरा कर लिया है, 

239
00:14:30,179 --> 00:14:34,711
फिर आप इस मान मैट्रिक्स को लेंगे और मान सदिशों का एक क्रम बनाने के लिए इसे प्रत्येक 

240
00:14:34,711 --> 00:14:36,060
एम्बेडिंग से गुणा करेंगे।

241
00:14:37,120 --> 00:14:41,120
आप इन मूल्य सदिशों को संबंधित कुंजियों से संबद्ध मान सकते हैं।

242
00:14:42,320 --> 00:14:45,638
इस आरेख में प्रत्येक स्तंभ के लिए, आप प्रत्येक 

243
00:14:45,638 --> 00:14:49,240
मान सदिश को उस स्तंभ में संगत भार से गुणा करते हैं।

244
00:14:50,080 --> 00:14:52,935
उदाहरण के लिए, यहां क्रिएचर की एम्बेडिंग के तहत, 

245
00:14:52,935 --> 00:14:57,072
आप फ्लफी और ब्लू के लिए मूल्य वैक्टर के बड़े अनुपात को जोड़ रहे होंगे, 

246
00:14:57,072 --> 00:15:01,560
जबकि अन्य सभी मूल्य वैक्टर शून्य हो जाएंगे, या कम से कम लगभग शून्य हो जाएंगे।

247
00:15:02,120 --> 00:15:06,220
और फिर अंत में, इस कॉलम से जुड़े एम्बेडिंग को वास्तव में अपडेट करने का तरीका, 

248
00:15:06,220 --> 00:15:09,007
पहले क्रिएचर के कुछ संदर्भ-मुक्त अर्थ को एनकोड करना, 

249
00:15:09,007 --> 00:15:12,372
आप कॉलम में इन सभी पुनर्निर्धारित मूल्यों को एक साथ जोड़ते हैं, 

250
00:15:12,372 --> 00:15:16,999
जो एक परिवर्तन उत्पन्न करता है जिसे आप जोड़ना चाहते हैं, जिसे मैं डेल्टा-ई लेबल करूंगा, 

251
00:15:16,999 --> 00:15:19,260
और फिर आप इसे मूल एम्बेडिंग में जोड़ते हैं।

252
00:15:19,680 --> 00:15:23,089
आशा है कि इसका परिणाम एक अधिक परिष्कृत वेक्टर होगा जो अधिक संदर्भगत रूप 

253
00:15:23,089 --> 00:15:26,500
से समृद्ध अर्थ को कूटबद्ध करेगा, जैसे कि एक रोयेंदार नीले रंग का प्राणी।

254
00:15:27,380 --> 00:15:30,852
और निश्चित रूप से आप ऐसा केवल एक एम्बेडिंग के लिए नहीं करते हैं, 

255
00:15:30,852 --> 00:15:34,111
आप इस चित्र में सभी स्तंभों पर समान भारित योग लागू करते हैं, 

256
00:15:34,111 --> 00:15:38,171
जिससे परिवर्तनों का एक क्रम बनता है, उन सभी परिवर्तनों को संबंधित एम्बेडिंग 

257
00:15:38,171 --> 00:15:42,177
में जोड़ते हैं, जिससे ध्यान ब्लॉक से बाहर आने वाले अधिक परिष्कृत एम्बेडिंग 

258
00:15:42,177 --> 00:15:43,460
का एक पूरा क्रम बनता है।

259
00:15:44,860 --> 00:15:49,100
ज़ूम आउट करके, इस पूरी प्रक्रिया को आप एकल ध्यान के रूप में वर्णित कर सकते हैं।

260
00:15:49,600 --> 00:15:54,500
जैसा कि मैंने अब तक बताया है, यह प्रक्रिया तीन अलग-अलग मैट्रिसेस द्वारा पैरामीटराइज़ 

261
00:15:54,500 --> 00:15:58,940
की जाती है, जो सभी ट्यूनेबल पैरामीटर्स, कुंजी, क्वेरी और मान से भरे होते हैं।

262
00:15:59,500 --> 00:16:02,498
मैं पिछले अध्याय में शुरू की गई प्रक्रिया को जारी रखना चाहता हूं, 

263
00:16:02,498 --> 00:16:05,359
जिसमें स्कोरकीपिंग शामिल है, जहां हम GPT-3 से प्राप्त संख्याओं 

264
00:16:05,359 --> 00:16:08,040
का उपयोग करके मॉडल मापदंडों की कुल संख्या की गणना करते हैं।

265
00:16:09,300 --> 00:16:13,140
इन कुंजी और क्वेरी मैट्रिसेस में से प्रत्येक में 12,288 कॉलम हैं, 

266
00:16:13,140 --> 00:16:16,457
जो एम्बेडिंग आयाम से मेल खाते हैं, और 128 पंक्तियाँ हैं, 

267
00:16:16,457 --> 00:16:19,600
जो उस छोटे कुंजी क्वेरी स्थान के आयाम से मेल खाती हैं।

268
00:16:20,260 --> 00:16:24,220
इससे हमें प्रत्येक के लिए लगभग 1.5 मिलियन अतिरिक्त पैरामीटर प्राप्त होते हैं।

269
00:16:24,860 --> 00:16:30,172
यदि आप इसके विपरीत मूल्य मैट्रिक्स को देखें, तो जिस तरह से मैंने अब तक चीजों का वर्णन 

270
00:16:30,172 --> 00:16:35,237
किया है, उससे पता चलता है कि यह एक वर्ग मैट्रिक्स है जिसमें 12,288 कॉलम और 12,288 

271
00:16:35,237 --> 00:16:40,672
पंक्तियाँ हैं, क्योंकि इसके इनपुट और आउटपुट दोनों ही बहुत बड़े एम्बेडिंग स्पेस में रहते 

272
00:16:40,672 --> 00:16:40,920
हैं।

273
00:16:41,500 --> 00:16:45,140
यदि यह सच है, तो इसका अर्थ होगा लगभग 150 मिलियन अतिरिक्त पैरामीटर।

274
00:16:45,660 --> 00:16:47,300
और स्पष्ट रूप से कहें तो, आप ऐसा कर सकते हैं।

275
00:16:47,420 --> 00:16:49,698
आप कुंजी और क्वेरी की तुलना में मान मानचित्र के 

276
00:16:49,698 --> 00:16:51,740
लिए बहुत अधिक पैरामीटर समर्पित कर सकते हैं।

277
00:16:52,060 --> 00:16:56,410
लेकिन व्यवहार में, यह अधिक कुशल होगा यदि आप इसे इस प्रकार बनाएं कि इस मूल्य मानचित्र 

278
00:16:56,410 --> 00:17:00,760
के लिए समर्पित पैरामीटरों की संख्या कुंजी और क्वेरी के लिए समर्पित संख्या के समान हो।

279
00:17:01,460 --> 00:17:05,160
यह विशेष रूप से कई ध्यान केन्द्रों को समानांतर रूप से चलाने की स्थिति में प्रासंगिक है।

280
00:17:06,240 --> 00:17:08,131
यह इस प्रकार दिखता है कि मान मानचित्र को दो छोटे 

281
00:17:08,131 --> 00:17:10,099
मैट्रिसेस के गुणनफल के रूप में विभाजित किया गया है।

282
00:17:11,180 --> 00:17:14,267
संकल्पनात्मक रूप से, मैं अब भी आपको समग्र रेखीय मानचित्र के बारे में 

283
00:17:14,267 --> 00:17:17,266
सोचने के लिए प्रोत्साहित करूंगा, जिसमें इनपुट और आउटपुट दोनों हों, 

284
00:17:17,266 --> 00:17:20,488
जो इस बड़े एम्बेडिंग स्पेस में हों, उदाहरण के लिए नीले रंग को एम्बेडिंग 

285
00:17:20,488 --> 00:17:23,800
के माध्यम से इस नीलेपन की दिशा में ले जाना, जिसे आप संज्ञाओं में जोड़ेंगे।

286
00:17:27,040 --> 00:17:29,873
बस बात यह है कि इसमें पंक्तियों की संख्या कम होती है, 

287
00:17:29,873 --> 00:17:32,760
जो आमतौर पर कुंजी क्वेरी स्थान के समान आकार की होती है।

288
00:17:33,100 --> 00:17:35,744
इसका मतलब यह है कि आप इसे बड़े एम्बेडिंग वैक्टर को 

289
00:17:35,744 --> 00:17:38,440
बहुत छोटे स्थान पर मैप करने के रूप में सोच सकते हैं।

290
00:17:39,040 --> 00:17:42,700
यह पारंपरिक नामकरण नहीं है, लेकिन मैं इसे मूल्य डाउन मैट्रिक्स कहूंगा।

291
00:17:43,400 --> 00:17:46,553
दूसरा मैट्रिक्स इस छोटे स्थान से एम्बेडिंग स्थान तक मैप करता है, 

292
00:17:46,553 --> 00:17:50,580
तथा वेक्टर्स का निर्माण करता है जिनका उपयोग आप वास्तविक अपडेट करने के लिए करते हैं।

293
00:17:51,000 --> 00:17:54,740
मैं इसे मूल्य अप मैट्रिक्स कहूंगा, जो कि परम्परागत नहीं है।

294
00:17:55,160 --> 00:17:58,080
आप इसे अधिकांश पत्रों में जिस तरह लिखा हुआ देखेंगे वह थोड़ा अलग दिखता है।

295
00:17:58,380 --> 00:17:59,520
मैं इसके बारे में एक मिनट में बात करूंगा।

296
00:17:59,700 --> 00:18:02,540
मेरी राय में, यह अवधारणात्मक रूप से चीजों को थोड़ा अधिक भ्रमित करने वाला बनाता है।

297
00:18:03,260 --> 00:18:06,125
यहां रैखिक बीजगणित की शब्दावली का प्रयोग करते हुए, 

298
00:18:06,125 --> 00:18:10,340
हम मूलतः समग्र मूल्य मानचित्र को निम्न श्रेणी रूपांतरण तक सीमित कर रहे हैं।

299
00:18:11,420 --> 00:18:15,440
पैरामीटर गणना की ओर लौटते हुए, इन चारों मैट्रिसेस का आकार समान है, 

300
00:18:15,440 --> 00:18:20,780
तथा इन्हें जोड़ने पर हमें एक अटेंशन हेड के लिए लगभग 6.3 मिलियन पैरामीटर प्राप्त होते हैं।

301
00:18:22,040 --> 00:18:24,768
एक त्वरित साइड नोट के रूप में, थोड़ा अधिक सटीक होने के लिए, 

302
00:18:24,768 --> 00:18:27,452
अब तक वर्णित सब कुछ वह है जिसे लोग स्व-ध्यान शीर्ष कहेंगे, 

303
00:18:27,452 --> 00:18:31,500
ताकि इसे अन्य मॉडलों में आने वाले बदलाव से अलग किया जा सके जिसे क्रॉस-अटेंशन कहा जाता है।

304
00:18:32,300 --> 00:18:36,217
यह हमारे GPT उदाहरण के लिए प्रासंगिक नहीं है, लेकिन यदि आप उत्सुक हैं, 

305
00:18:36,217 --> 00:18:40,411
तो बता दें कि क्रॉस-अटेंशन में ऐसे मॉडल शामिल होते हैं जो दो अलग-अलग प्रकार 

306
00:18:40,411 --> 00:18:44,549
के डेटा को संसाधित करते हैं, जैसे एक भाषा में पाठ और दूसरी भाषा में पाठ जो 

307
00:18:44,549 --> 00:18:49,240
अनुवाद की चल रही पीढ़ी का हिस्सा है, या शायद भाषण का ऑडियो इनपुट और चल रही प्रतिलिपि।

308
00:18:50,400 --> 00:18:52,700
क्रॉस-अटेंशन हेड लगभग एक जैसा दिखता है।

309
00:18:52,980 --> 00:18:57,400
अंतर केवल इतना है कि कुंजी और क्वेरी मानचित्र अलग-अलग डेटा सेटों पर कार्य करते हैं।

310
00:18:57,840 --> 00:19:01,889
उदाहरण के लिए, अनुवाद करने वाले मॉडल में, कुंजियाँ एक भाषा से आ सकती हैं, 

311
00:19:01,889 --> 00:19:05,829
जबकि प्रश्न दूसरी भाषा से आ सकते हैं, तथा ध्यान पैटर्न यह वर्णन कर सकता 

312
00:19:05,829 --> 00:19:09,660
है कि एक भाषा के कौन से शब्द दूसरी भाषा के किन शब्दों से मेल खाते हैं।

313
00:19:10,340 --> 00:19:12,578
और इस सेटिंग में आमतौर पर कोई मास्किंग नहीं होगी, 

314
00:19:12,578 --> 00:19:16,340
क्योंकि बाद के टोकनों द्वारा पहले वाले टोकनों को प्रभावित करने की कोई धारणा नहीं है।

315
00:19:17,180 --> 00:19:21,349
आत्म-ध्यान पर ध्यान केंद्रित करते हुए, यदि आपने अब तक सब कुछ समझ लिया है, 

316
00:19:21,349 --> 00:19:25,180
और यदि आप यहीं रुक गए, तो आप समझ जाएंगे कि वास्तव में ध्यान क्या है।

317
00:19:25,760 --> 00:19:31,440
अब हमारे लिए बस यही बचा है कि हम यह स्पष्ट करें कि आप इसे कई बार किस अर्थ में करते हैं।

318
00:19:32,100 --> 00:19:34,493
हमारे केंद्रीय उदाहरण में हमने संज्ञाओं को अद्यतन करने वाले 

319
00:19:34,493 --> 00:19:36,927
विशेषणों पर ध्यान केंद्रित किया, लेकिन निश्चित रूप से ऐसे कई 

320
00:19:36,927 --> 00:19:39,800
अलग-अलग तरीके हैं जिनसे संदर्भ किसी शब्द के अर्थ को प्रभावित कर सकता है।

321
00:19:40,360 --> 00:19:43,850
यदि शब्द &#39;वे दुर्घटनाग्रस्त हो गए&#39; शब्द कार से पहले आता है, 

322
00:19:43,850 --> 00:19:46,520
तो इसका उस कार के आकार और संरचना पर प्रभाव पड़ता है।

323
00:19:47,200 --> 00:19:49,280
और बहुत सारे संबंध कम व्याकरणिक हो सकते हैं।

324
00:19:49,760 --> 00:19:53,473
यदि उसी अनुच्छेद में कहीं भी जादूगर शब्द हैरी के संदर्भ में है, 

325
00:19:53,473 --> 00:19:57,070
तो इससे पता चलता है कि यह हैरी पॉटर के संदर्भ में हो सकता है, 

326
00:19:57,070 --> 00:20:00,436
जबकि यदि उस अनुच्छेद में रानी, ससेक्स और विलियम शब्द हैं, 

327
00:20:00,436 --> 00:20:04,440
तो संभवतः हैरी शब्द को राजकुमार के संदर्भ में अद्यतन किया जाना चाहिए।

328
00:20:05,040 --> 00:20:09,120
प्रत्येक भिन्न प्रकार के प्रासंगिक अद्यतन के लिए, जिसकी आप कल्पना कर सकते हैं, 

329
00:20:09,120 --> 00:20:12,477
इन कुंजी और क्वेरी मैट्रिसेस के पैरामीटर विभिन्न ध्यान पैटर्न को 

330
00:20:12,477 --> 00:20:15,989
पकड़ने के लिए अलग-अलग होंगे, और हमारे मूल्य मानचित्र के पैरामीटर इस 

331
00:20:15,989 --> 00:20:19,140
आधार पर अलग-अलग होंगे कि एम्बेडिंग में क्या जोड़ा जाना चाहिए।

332
00:20:19,980 --> 00:20:23,496
और फिर, व्यवहार में इन मानचित्रों के वास्तविक व्यवहार की व्याख्या करना बहुत अधिक कठिन है, 

333
00:20:23,496 --> 00:20:26,935
जहां भार को वह सब करने के लिए निर्धारित किया जाता है जो मॉडल को अगले टोकन की भविष्यवाणी 

334
00:20:26,935 --> 00:20:30,140
करने के अपने लक्ष्य को सर्वोत्तम रूप से पूरा करने के लिए करने की आवश्यकता होती है।

335
00:20:31,400 --> 00:20:35,278
जैसा कि मैंने पहले कहा था, हमने जो कुछ भी वर्णित किया है वह ध्यान का एक एकल शीर्ष है, 

336
00:20:35,278 --> 00:20:39,110
और एक ट्रांसफार्मर के अंदर एक पूर्ण ध्यान ब्लॉक में वह शामिल होता है जिसे बहु-शीर्षक 

337
00:20:39,110 --> 00:20:42,718
ध्यान कहा जाता है, जहां आप इनमें से बहुत सारे ऑपरेशन समानांतर रूप से चलाते हैं, 

338
00:20:42,718 --> 00:20:45,920
जिनमें से प्रत्येक की अपनी अलग कुंजी क्वेरी और मूल्य मानचित्र होते हैं।

339
00:20:47,420 --> 00:20:51,700
उदाहरण के लिए GPT-3 प्रत्येक ब्लॉक के अंदर 96 अटेंशन हेड का उपयोग करता है।

340
00:20:52,020 --> 00:20:54,498
यह देखते हुए कि इनमें से प्रत्येक पहले से ही थोड़ा भ्रमित करने वाला है, 

341
00:20:54,498 --> 00:20:56,460
निश्चित रूप से यह आपके दिमाग में रखने के लिए बहुत कुछ है।

342
00:20:56,760 --> 00:21:00,794
इसे बहुत स्पष्ट रूप से समझाते हुए, इसका मतलब है कि आपके पास 96 अलग-अलग 

343
00:21:00,794 --> 00:21:05,000
कुंजी और क्वेरी मैट्रिसेस हैं जो 96 अलग-अलग ध्यान पैटर्न उत्पन्न करते हैं।

344
00:21:05,440 --> 00:21:08,656
फिर प्रत्येक शीर्ष का अपना विशिष्ट मान मैट्रिक्स होता है जिसका 

345
00:21:08,656 --> 00:21:12,180
उपयोग मान सदिशों के 96 अनुक्रमों को उत्पन्न करने के लिए किया जाता है।

346
00:21:12,460 --> 00:21:16,680
इन सभी को भार के रूप में संगत ध्यान पैटर्न का उपयोग करके एक साथ जोड़ा जाता है।

347
00:21:17,480 --> 00:21:20,264
इसका अर्थ यह है कि संदर्भ में प्रत्येक स्थिति के लिए, 

348
00:21:20,264 --> 00:21:23,358
प्रत्येक टोकन के लिए, इनमें से प्रत्येक शीर्ष उस स्थिति में 

349
00:21:23,358 --> 00:21:27,020
एम्बेडिंग में जोड़े जाने के लिए एक प्रस्तावित परिवर्तन उत्पन्न करता है।

350
00:21:27,660 --> 00:21:31,570
तो आप जो करते हैं वह यह है कि आप उन सभी प्रस्तावित परिवर्तनों को एक साथ जोड़ते हैं, 

351
00:21:31,570 --> 00:21:35,480
प्रत्येक शीर्ष के लिए एक, और आप परिणाम को उस स्थिति के मूल एम्बेडिंग में जोड़ते हैं।

352
00:21:36,660 --> 00:21:42,659
यह संपूर्ण योग इस बहु-शीर्षक वाले ध्यान ब्लॉक से जो आउटपुट होगा उसका एक टुकड़ा होगा, 

353
00:21:42,659 --> 00:21:47,460
उन परिष्कृत एम्बेडिंग में से एक जो इसके दूसरे छोर से बाहर निकलता है।

354
00:21:48,320 --> 00:21:52,140
पुनः, यह बहुत सोचने वाली बात है, इसलिए यदि इसे समझने में कुछ समय लगे तो चिंता न करें।

355
00:21:52,380 --> 00:21:56,434
समग्र विचार यह है कि कई अलग-अलग शीर्षकों को समानांतर रूप से चलाकर, 

356
00:21:56,434 --> 00:22:01,820
आप मॉडल को कई अलग-अलग तरीकों से सीखने की क्षमता दे रहे हैं जिनसे संदर्भ अर्थ बदल देता है।

357
00:22:03,700 --> 00:22:07,564
96 शीर्षों के साथ पैरामीटर गणना के लिए हमारी चल रही गणना को खींचते हुए, 

358
00:22:07,564 --> 00:22:11,107
जिनमें से प्रत्येक में इन चार मैट्रिसेस की अपनी भिन्नता शामिल है, 

359
00:22:11,107 --> 00:22:15,080
बहु-शीर्षित ध्यान के प्रत्येक ब्लॉक में लगभग 600 मिलियन पैरामीटर होते हैं।

360
00:22:16,420 --> 00:22:18,976
इसमें एक और थोड़ी सी कष्टप्रद बात है जिसका उल्लेख मुझे आपमें से उन 

361
00:22:18,976 --> 00:22:21,800
लोगों से करना चाहिए जो ट्रांसफार्मरों के बारे में और अधिक पढ़ना चाहते हैं।

362
00:22:22,080 --> 00:22:25,716
आपको याद होगा कि मैंने कहा था कि मूल्य मानचित्र को दो अलग-अलग मैट्रिसेस में विभाजित 

363
00:22:25,716 --> 00:22:29,440
किया गया है, जिन्हें मैंने मूल्य नीचे और मूल्य ऊपर मैट्रिसेस के रूप में नामित किया है।

364
00:22:29,960 --> 00:22:34,126
जिस तरह से मैंने चीजों को तैयार किया है, उससे पता चलता है कि आप प्रत्येक ध्यान शीर्ष 

365
00:22:34,126 --> 00:22:38,440
के अंदर मैट्रिसेस की इस जोड़ी को देखते हैं, और आप इसे इस तरह से कार्यान्वित कर सकते हैं।

366
00:22:38,640 --> 00:22:39,920
यह एक वैध डिजाइन होगा.

367
00:22:40,260 --> 00:22:43,924
लेकिन आप इसे कागजों में लिखा हुआ देखते हैं और व्यवहार में इसे लागू किया जाता है, 

368
00:22:43,924 --> 00:22:44,920
यह थोड़ा अलग दिखता है।

369
00:22:45,340 --> 00:22:48,905
प्रत्येक शीर्ष के लिए ये सभी मान मैट्रिक्स एक विशाल मैट्रिक्स 

370
00:22:48,905 --> 00:22:53,275
में एक साथ स्टेपल किए गए दिखाई देते हैं, जिसे हम आउटपुट मैट्रिक्स कहते हैं, 

371
00:22:53,275 --> 00:22:56,380
जो संपूर्ण बहु-शीर्षक ध्यान ब्लॉक के साथ जुड़ा हुआ है।

372
00:22:56,820 --> 00:23:00,307
और जब आप देखते हैं कि लोग किसी दिए गए ध्यान शीर्ष के लिए मूल्य मैट्रिक्स 

373
00:23:00,307 --> 00:23:03,747
का संदर्भ देते हैं, तो वे आमतौर पर केवल इस पहले चरण का संदर्भ देते हैं, 

374
00:23:03,747 --> 00:23:07,140
जिसे मैं छोटे स्थान में मूल्य नीचे प्रक्षेपण के रूप में लेबल कर रहा था।

375
00:23:08,340 --> 00:23:11,040
आपमें से जो लोग उत्सुक हैं, उनके लिए मैंने इस बारे में एक ऑन-स्क्रीन नोट छोड़ा है।

376
00:23:11,260 --> 00:23:14,582
यह उन विवरणों में से एक है, जो मुख्य वैचारिक बिंदुओं से ध्यान भटकाने का जोखिम उठाते हैं, 

377
00:23:14,582 --> 00:23:16,934
लेकिन मैं इसका उल्लेख करना चाहता हूं, ताकि यदि आप अन्य स्रोतों 

378
00:23:16,934 --> 00:23:18,540
में इसके बारे में पढ़ें तो आपको पता चल जाए।

379
00:23:19,240 --> 00:23:22,231
सभी तकनीकी बारीकियों को एक तरफ रखते हुए, पिछले अध्याय के पूर्वावलोकन 

380
00:23:22,231 --> 00:23:25,048
में हमने देखा कि कैसे एक ट्रांसफार्मर के माध्यम से प्रवाहित होने 

381
00:23:25,048 --> 00:23:28,040
वाला डेटा केवल एक एकल ध्यान ब्लॉक के माध्यम से प्रवाहित नहीं होता है।

382
00:23:28,640 --> 00:23:32,700
एक बात यह है कि यह मल्टी-लेयर परसेप्ट्रॉन नामक अन्य ऑपरेशनों से भी गुजरता है।

383
00:23:33,120 --> 00:23:34,880
हम अगले अध्याय में इनके बारे में अधिक बात करेंगे।

384
00:23:35,180 --> 00:23:39,320
और फिर यह बार-बार इन दोनों ऑपरेशनों की कई-कई प्रतियों से गुजरता है।

385
00:23:39,980 --> 00:23:44,240
इसका अर्थ यह है कि जब कोई शब्द अपने कुछ संदर्भों को आत्मसात कर लेता है, 

386
00:23:44,240 --> 00:23:49,507
तो इस सूक्ष्म अंतःस्थापना के अपने अधिक सूक्ष्म परिवेश से प्रभावित होने की अधिक संभावनाएं 

387
00:23:49,507 --> 00:23:50,040
होती हैं।

388
00:23:50,940 --> 00:23:55,007
नेटवर्क में आप जितना आगे बढ़ेंगे, प्रत्येक एम्बेडिंग अन्य सभी एम्बेडिंग से 

389
00:23:55,007 --> 00:23:59,292
अधिक से अधिक अर्थ ग्रहण करेगी, जो स्वयं अधिक से अधिक सूक्ष्मतर होती जा रही है, 

390
00:23:59,292 --> 00:24:03,469
आशा यह है कि किसी दिए गए इनपुट के बारे में केवल विवरणकों और व्याकरणिक संरचना 

391
00:24:03,469 --> 00:24:07,320
से परे उच्च स्तरीय और अधिक अमूर्त विचारों को एनकोड करने की क्षमता होगी।

392
00:24:07,880 --> 00:24:11,327
भावना और लहजा जैसी चीजें और क्या यह एक कविता है और इसमें अंतर्निहित 

393
00:24:11,327 --> 00:24:15,130
वैज्ञानिक सत्य क्या हैं जो इस रचना के लिए प्रासंगिक हैं और इस तरह की चीजें।

394
00:24:16,700 --> 00:24:22,206
एक बार फिर से अपने स्कोरकीपिंग की ओर लौटते हुए, GPT-3 में 96 अलग-अलग परतें शामिल हैं, 

395
00:24:22,206 --> 00:24:27,584
इसलिए कुंजी क्वेरी और मूल्य मापदंडों की कुल संख्या को अन्य 96 से गुणा किया जाता है, 

396
00:24:27,584 --> 00:24:31,874
जिससे कुल योग लगभग 58 बिलियन अलग-अलग मापदंडों के बराबर हो जाता है, 

397
00:24:31,874 --> 00:24:34,500
जो सभी ध्यान शीर्षों को समर्पित होते हैं।

398
00:24:34,980 --> 00:24:37,869
यह निश्चित रूप से बहुत कुछ है, लेकिन यह नेटवर्क 

399
00:24:37,869 --> 00:24:40,940
में शामिल कुल 175 बिलियन लोगों का केवल एक तिहाई है।

400
00:24:41,520 --> 00:24:44,617
इसलिए भले ही सारा ध्यान ध्यान पर ही जाता है, लेकिन 

401
00:24:44,617 --> 00:24:48,140
अधिकांश पैरामीटर इन चरणों के बीच स्थित ब्लॉकों से आते हैं।

402
00:24:48,560 --> 00:24:50,987
अगले अध्याय में आप और मैं उन अन्य ब्लॉकों के बारे 

403
00:24:50,987 --> 00:24:53,560
में और प्रशिक्षण प्रक्रिया के बारे में भी बात करेंगे।

404
00:24:54,120 --> 00:24:58,755
ध्यान तंत्र की सफलता की कहानी का एक बड़ा हिस्सा किसी विशिष्ट प्रकार के व्यवहार 

405
00:24:58,755 --> 00:25:03,391
में नहीं है जिसे यह सक्षम बनाता है, बल्कि यह तथ्य है कि यह अत्यंत समानांतर है, 

406
00:25:03,391 --> 00:25:08,380
जिसका अर्थ है कि आप GPU का उपयोग करके कम समय में बड़ी संख्या में गणनाएं चला सकते हैं।

407
00:25:09,460 --> 00:25:13,425
पिछले एक या दो दशक में डीप लर्निंग के बारे में सबसे बड़ी सीख यह रही है कि अकेले 

408
00:25:13,425 --> 00:25:16,796
पैमाने से ही मॉडल के प्रदर्शन में गुणात्मक सुधार देखने को मिलता है, 

409
00:25:16,796 --> 00:25:21,060
ऐसे में समानांतर आर्किटेक्चर का बहुत बड़ा फायदा है जो आपको ऐसा करने की अनुमति देता है।

410
00:25:22,040 --> 00:25:25,340
यदि आप इस विषय में अधिक जानना चाहते हैं, तो मैंने विवरण में बहुत सारे लिंक छोड़े हैं।

411
00:25:25,920 --> 00:25:30,040
विशेष रूप से, आंद्रेज कारपैथी या क्रिस ओला द्वारा निर्मित कोई भी वस्तु शुद्ध सोना होती है।

412
00:25:30,560 --> 00:25:33,367
इस वीडियो में, मैं इसके वर्तमान स्वरूप पर ध्यान आकर्षित करना चाहता था, 

413
00:25:33,367 --> 00:25:36,411
लेकिन यदि आप इसके इतिहास के बारे में अधिक जानने के लिए उत्सुक हैं कि हम यहां 

414
00:25:36,411 --> 00:25:39,218
कैसे पहुंचे और आप इस विचार को अपने लिए कैसे पुनः आविष्कृत कर सकते हैं, 

415
00:25:39,218 --> 00:25:42,540
तो मेरे मित्र विवेक ने कुछ वीडियो डाले हैं, जो इस बारे में और अधिक प्रेरणा देते हैं।

416
00:25:43,120 --> 00:25:45,790
इसके अलावा, द आर्ट ऑफ द प्रॉब्लम चैनल के ब्रिट क्रूज़ ने बड़े 

417
00:25:45,790 --> 00:25:48,460
भाषा मॉडल के इतिहास के बारे में एक बहुत अच्छा वीडियो बनाया है।

418
00:26:04,960 --> 00:26:09,200
धन्यवाद।

