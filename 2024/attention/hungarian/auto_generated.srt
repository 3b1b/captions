1
00:00:00,000 --> 00:00:04,019
Az előző fejezetben ön és én elkezdtük végigjárni a transzformátor belső működését.

2
00:00:04,560 --> 00:00:07,337
Ez az egyik legfontosabb technológia a nagy nyelvi modellekben és 

3
00:00:07,337 --> 00:00:10,200
a mesterséges intelligencia modern hullámának számos más eszközében.

4
00:00:10,980 --> 00:00:14,599
Először egy 2017-es, ma már híres, Attention is All You Need című tanulmányban 

5
00:00:14,599 --> 00:00:17,714
jelent meg, és ebben a fejezetben te és én beleássuk magunkat abba, 

6
00:00:17,714 --> 00:00:21,700
hogy mi is ez a figyelemmechanizmus, és vizualizáljuk, hogyan dolgozza fel az adatokat.

7
00:00:26,140 --> 00:00:29,540
Gyorsan összefoglalva, itt van a fontos összefüggés, amit szeretném, ha észben tartanál.

8
00:00:30,000 --> 00:00:34,156
A modell célja, amit mi ketten tanulunk, az, hogy befogadjunk egy szövegrészletet, 

9
00:00:34,156 --> 00:00:36,060
és megjósoljuk, melyik szó következik.

10
00:00:36,860 --> 00:00:40,440
A bemeneti szöveget apró darabokra bontjuk, amelyeket tokeneknek nevezünk, 

11
00:00:40,440 --> 00:00:43,638
és ezek nagyon gyakran szavak vagy szavak darabjai, de csak azért, 

12
00:00:43,638 --> 00:00:46,741
hogy a videóban szereplő példákat könnyebben el tudjuk képzelni, 

13
00:00:46,741 --> 00:00:50,560
egyszerűsítsük le, és tegyünk úgy, mintha a tokenek mindig csak szavak lennének.

14
00:00:51,480 --> 00:00:54,590
A transzformátor első lépése az, hogy minden egyes tokent egy 

15
00:00:54,590 --> 00:00:57,700
nagydimenziós vektorral társítunk, amit beágyazásnak nevezünk.

16
00:00:57,700 --> 00:01:00,834
A legfontosabb gondolat, amit szeretném, ha észben tartanál, 

17
00:01:00,834 --> 00:01:03,968
hogy az összes lehetséges beágyazás magas dimenziójú terének 

18
00:01:03,968 --> 00:01:07,000
irányai hogyan feleltethetők meg a szemantikai jelentésnek.

19
00:01:07,680 --> 00:01:12,006
Az előző fejezetben láttunk egy példát arra, hogy az irány hogyan felelhet meg a nemnek, 

20
00:01:12,006 --> 00:01:15,799
abban az értelemben, hogy egy bizonyos lépés hozzáadásával ebben a térben egy 

21
00:01:15,799 --> 00:01:19,640
hímnemű főnév beágyazásától a megfelelő nőnemű főnév beágyazásáig juthatunk el.

22
00:01:20,160 --> 00:01:23,968
Ez csak egy példa, és el tudod képzelni, hogy ebben a nagy dimenziójú térben 

23
00:01:23,968 --> 00:01:27,580
hány más irány felelhet meg egy szó jelentésének számos más aspektusának.

24
00:01:28,800 --> 00:01:33,134
A transzformátor célja, hogy fokozatosan kiigazítsa ezeket a beágyazásokat, 

25
00:01:33,134 --> 00:01:36,100
hogy ne csak egy-egy szót kódoljanak, hanem sokkal, 

26
00:01:36,100 --> 00:01:39,180
de sokkal gazdagabb kontextuális jelentéssel bírjanak.

27
00:01:40,140 --> 00:01:43,101
Elöljáróban el kell mondanom, hogy sokan nagyon zavarosnak találják 

28
00:01:43,101 --> 00:01:46,105
a figyelemmechanizmust, a transzformátor e kulcsfontosságú darabját, 

29
00:01:46,105 --> 00:01:48,980
ezért ne aggódjon, ha egy kis időbe telik, amíg a dolgok megértik.

30
00:01:49,440 --> 00:01:52,630
Úgy gondolom, hogy mielőtt belemerülnénk a számítási részletekbe 

31
00:01:52,630 --> 00:01:55,870
és az összes mátrixszorzásba, érdemes elgondolkodni néhány példán 

32
00:01:55,870 --> 00:01:59,160
arra a fajta viselkedésre, amit a figyelemnek lehetővé kell tennie.

33
00:02:00,140 --> 00:02:02,937
Tekintsük a kifejezéseket amerikai valódi anyajegy, 

34
00:02:02,937 --> 00:02:06,220
egy széndioxid-molekula, és vegyünk biopsziát az anyajegyből.

35
00:02:06,700 --> 00:02:09,924
Mindketten tudjuk, hogy a vakond szónak mindegyikben más-más jelentése van, 

36
00:02:09,924 --> 00:02:10,900
a kontextustól függően.

37
00:02:11,360 --> 00:02:15,007
De a transzformátor első lépése után, amely felbontja a szöveget és 

38
00:02:15,007 --> 00:02:18,548
minden egyes tokent egy vektorhoz társít, a molekulához társított 

39
00:02:18,548 --> 00:02:21,874
vektor minden ilyen esetben ugyanaz lenne, mivel ez a kezdeti 

40
00:02:21,874 --> 00:02:26,220
tokenbeágyazás gyakorlatilag egy keresőtábla, amely nem hivatkozik a kontextusra.

41
00:02:26,620 --> 00:02:30,792
Csak a transzformátor következő lépésében van esélye a környező beágyazásoknak arra, 

42
00:02:30,792 --> 00:02:33,100
hogy információt adjanak át ebbe a beágyazásba.

43
00:02:33,820 --> 00:02:36,241
Az a kép, amit önök talán szem előtt tartanak, 

44
00:02:36,241 --> 00:02:39,384
hogy ebben a beágyazási térben több különböző irány létezik, 

45
00:02:39,384 --> 00:02:42,320
amelyek a vakond szó több különböző jelentését kódolják, 

46
00:02:42,320 --> 00:02:44,896
és hogy egy jól képzett figyelemblokk kiszámítja, 

47
00:02:44,896 --> 00:02:47,523
hogy mit kell hozzáadni az általános beágyazáshoz, 

48
00:02:47,523 --> 00:02:51,800
hogy az a kontextus függvényében elmozduljon ezen specifikus irányok valamelyikébe.

49
00:02:53,300 --> 00:02:56,180
Vegyünk egy másik példát, nézzük meg a torony szó beágyazását.

50
00:02:57,060 --> 00:03:01,244
Ez feltehetően valami nagyon általános, nem specifikus irány a térben, 

51
00:03:01,244 --> 00:03:03,720
amely sok más nagy, magas főnévvel társul.

52
00:03:04,020 --> 00:03:07,474
Ha ezt a szót közvetlenül megelőzné az Eiffel szó, elképzelhető, 

53
00:03:07,474 --> 00:03:11,407
hogy a mechanizmus frissítené ezt a vektort, hogy olyan irányba mutasson, 

54
00:03:11,407 --> 00:03:14,808
amely pontosabban kódolja az Eiffel-tornyot, esetleg Párizshoz, 

55
00:03:14,808 --> 00:03:19,060
Franciaországhoz és acélból készült dolgokhoz kapcsolódó vektorokkal korrelálva.

56
00:03:19,920 --> 00:03:24,492
Ha a miniatűr szó is megelőzte, akkor a vektort még tovább kell frissíteni, 

57
00:03:24,492 --> 00:03:27,500
hogy többé ne a nagy, magas dolgokkal korreláljon.

58
00:03:29,480 --> 00:03:32,622
A figyelem blokkja általánosabban, mint egy szó jelentésének finomítása, 

59
00:03:32,622 --> 00:03:36,024
lehetővé teszi a modell számára, hogy az egyik beágyazásban kódolt információt 

60
00:03:36,024 --> 00:03:39,640
átvigye egy másik beágyazásba, potenciálisan olyanokba, amelyek elég messze vannak, 

61
00:03:39,640 --> 00:03:43,300
és potenciálisan olyan információkkal, amelyek sokkal gazdagabbak, mint egyetlen szó.

62
00:03:43,300 --> 00:03:47,856
Az előző fejezetben azt láttuk, hogy miután az összes vektor átfut a hálózaton, 

63
00:03:47,856 --> 00:03:50,875
beleértve sok különböző figyelemblokkot, a számítás, 

64
00:03:50,875 --> 00:03:54,976
amelyet a következő token előrejelzésének előállítása érdekében végzel, 

65
00:03:54,976 --> 00:03:58,280
teljes mértékben a szekvencia utolsó vektorának függvénye.

66
00:03:59,100 --> 00:04:03,481
Képzeljük el például, hogy a beírt szöveg egy egész krimi nagy része, 

67
00:04:03,481 --> 00:04:07,800
egészen a végéhez közeli pontig, ahol ez áll: "Ezért volt a gyilkos".

68
00:04:08,400 --> 00:04:11,302
Ha a modell pontosan meg akarja jósolni a következő szót, 

69
00:04:11,302 --> 00:04:15,106
akkor a szekvencia utolsó vektorának, amely az életét egyszerűen a volt szó 

70
00:04:15,106 --> 00:04:19,060
beágyazásával kezdte, az összes figyelemblokknak frissülnie kell, hogy sokkal, 

71
00:04:19,060 --> 00:04:21,563
de sokkal többet képviseljen, mint bármelyik szó, 

72
00:04:21,563 --> 00:04:25,317
valahogyan kódolva a teljes kontextusablakból származó összes információt, 

73
00:04:25,317 --> 00:04:28,220
amely releváns a következő szó előrejelzése szempontjából.

74
00:04:29,500 --> 00:04:32,580
A számítások áttekintéséhez azonban vegyünk egy sokkal egyszerűbb példát.

75
00:04:32,980 --> 00:04:35,892
Képzelje el, hogy a bemenet tartalmazza a következő mondatot: 

76
00:04:35,892 --> 00:04:37,960
Egy bolyhos kék lény járta a zöldellő erdőt.

77
00:04:38,460 --> 00:04:42,535
És egyelőre tegyük fel, hogy az egyetlen frissítés, ami minket érdekel, 

78
00:04:42,535 --> 00:04:46,780
az az, hogy a melléknevek módosítják a hozzájuk tartozó főnevek jelentését.

79
00:04:47,000 --> 00:04:50,522
Amit most le fogok írni, azt egyetlen figyelemfejnek neveznénk, 

80
00:04:50,522 --> 00:04:55,420
és később látni fogjuk, hogy a figyelemblokk sok különböző, párhuzamosan futó fejből áll.

81
00:04:56,140 --> 00:04:59,929
A kezdeti beágyazás minden egyes szóhoz egy nagy dimenziós vektor, 

82
00:04:59,929 --> 00:05:03,380
amely csak az adott szó jelentését kódolja, kontextus nélkül.

83
00:05:04,000 --> 00:05:05,220
Valójában ez nem teljesen igaz.

84
00:05:05,380 --> 00:05:07,640
A szó pozícióját is kódolják.

85
00:05:07,980 --> 00:05:11,026
A pozíciók kódolásának módja még sok mindent elmond, 

86
00:05:11,026 --> 00:05:15,509
de most csak annyit kell tudnod, hogy a vektor bejegyzései elégségesek ahhoz, 

87
00:05:15,509 --> 00:05:18,900
hogy megmondják, mi a szó, és hol van a szövegkörnyezetben.

88
00:05:19,500 --> 00:05:21,660
Menjünk tovább, és jelöljük ezeket a beágyazásokat e betűvel.

89
00:05:22,420 --> 00:05:27,766
A cél az, hogy egy sor számítással a beágyazások új, finomított halmazát hozzuk létre, 

90
00:05:27,766 --> 00:05:32,744
ahol például a főneveknek megfelelő beágyazások átvették a megfelelő melléknevek 

91
00:05:32,744 --> 00:05:33,420
jelentését.

92
00:05:33,900 --> 00:05:36,145
És a mélytanulás játékát játszva azt szeretnénk, 

93
00:05:36,145 --> 00:05:38,756
ha a legtöbb számítás mátrix-vektor termékként nézne ki, 

94
00:05:38,756 --> 00:05:41,826
ahol a mátrixok tele vannak hangolható súlyokkal, olyan dolgokkal, 

95
00:05:41,826 --> 00:05:43,980
amelyeket a modell az adatok alapján tanul meg.

96
00:05:44,660 --> 00:05:48,177
Hogy világos legyen, csak azért találtam ki ezt a példát a főneveket aktualizáló 

97
00:05:48,177 --> 00:05:51,391
melléknevekkel, hogy szemléltessem, milyen viselkedést képzelhetsz el egy 

98
00:05:51,391 --> 00:05:52,260
figyelemfelkeltőnek.

99
00:05:52,860 --> 00:05:56,452
Mint oly sok más mélytanulás esetében, a valódi viselkedést sokkal nehezebb elemezni, 

100
00:05:56,452 --> 00:05:59,334
mivel az egy hatalmas számú paraméter csípésén és hangolásán alapul, 

101
00:05:59,334 --> 00:06:01,340
hogy minimalizáljon valamilyen költségfüggvényt.

102
00:06:01,680 --> 00:06:04,857
Csak ahogy végigmegyünk a különböző paraméterekkel teli mátrixokon, 

103
00:06:04,857 --> 00:06:08,501
amelyek ebben a folyamatban részt vesznek, úgy gondolom, hogy nagyon hasznos, 

104
00:06:08,501 --> 00:06:11,211
ha van egy elképzelt példa arra, hogy mit is csinálhatna, 

105
00:06:11,211 --> 00:06:13,220
hogy segítsen konkrétabbá tenni az egészet.

106
00:06:14,140 --> 00:06:17,965
A folyamat első lépéseként elképzelhetjük, hogy minden egyes főnév, 

107
00:06:17,965 --> 00:06:21,960
mint például a lény, felteszi a kérdést: hé, ülnek előttem melléknevek?

108
00:06:22,160 --> 00:06:25,440
És a bolyhos és a kék szavakra, hogy mindketten tudjanak válaszolni, 

109
00:06:25,440 --> 00:06:27,960
igen, melléknév vagyok, és ebben a helyzetben vagyok.

110
00:06:28,960 --> 00:06:33,345
Ez a kérdés valahogy egy másik vektorba, egy másik számjegyzékbe van kódolva, 

111
00:06:33,345 --> 00:06:36,100
amelyet a szóra vonatkozó lekérdezésnek nevezünk.

112
00:06:36,980 --> 00:06:40,025
Ez a lekérdezési vektor azonban sokkal kisebb dimenziójú, 

113
00:06:40,025 --> 00:06:42,020
mint a beágyazási vektor, mondjuk 128.

114
00:06:42,940 --> 00:06:46,986
Ennek a lekérdezésnek a kiszámítása úgy néz ki, hogy veszünk egy bizonyos mátrixot, 

115
00:06:46,986 --> 00:06:49,780
amelyet wq-nek nevezek el, és megszorozzuk a beágyazással.

116
00:06:50,960 --> 00:06:54,394
Kicsit tömörítve a dolgokat, írjuk ezt a lekérdezés vektorát q-nak, 

117
00:06:54,394 --> 00:06:58,031
és bármikor, amikor azt látod, hogy egy nyíl mellé egy mátrixot teszek, 

118
00:06:58,031 --> 00:07:01,516
mint például ez itt, ez azt jelenti, hogy ezt a mátrixot megszorozva 

119
00:07:01,516 --> 00:07:04,800
a nyíl elején lévő vektorral, megkapod a nyíl végén lévő vektort.

120
00:07:05,860 --> 00:07:09,901
Ebben az esetben ezt a mátrixot megszorozzuk a kontextusban lévő összes beágyazással, 

121
00:07:09,901 --> 00:07:12,580
így minden egyes tokenhez egy lekérdezési vektort kapunk.

122
00:07:13,740 --> 00:07:17,051
Ennek a mátrixnak a bejegyzései a modell paraméterei, ami azt jelenti, 

123
00:07:17,051 --> 00:07:20,968
hogy a valódi viselkedést az adatokból tanulják, és a gyakorlatban kihívást jelent, 

124
00:07:20,968 --> 00:07:23,440
hogy mit csinál ez a mátrix egy adott figyelemfejben.

125
00:07:23,900 --> 00:07:27,326
De a mi kedvünkért, elképzelve egy példát, amit remélhetőleg megtanul, 

126
00:07:27,326 --> 00:07:30,849
tegyük fel, hogy ez a lekérdezési mátrix a főnevek beágyazásait bizonyos 

127
00:07:30,849 --> 00:07:33,503
irányokba képezi le ebben a kisebb lekérdezési térben, 

128
00:07:33,503 --> 00:07:37,026
ami valamilyen módon kódolja azt a fogalmat, hogy mellékneveket keresünk 

129
00:07:37,026 --> 00:07:38,040
az előző pozíciókban.

130
00:07:38,780 --> 00:07:41,440
Hogy mit tesz más beágyazásokkal, ki tudja?

131
00:07:41,720 --> 00:07:44,340
Lehet, hogy ezzel egyidejűleg más célt is megpróbál elérni.

132
00:07:44,540 --> 00:07:47,160
Jelenleg a főnevekre koncentrálunk.

133
00:07:47,280 --> 00:07:51,227
Ugyanakkor ehhez kapcsolódik egy második mátrix, a kulcsmátrix, 

134
00:07:51,227 --> 00:07:54,620
amelyet szintén megszorozunk minden egyes beágyazással.

135
00:07:55,280 --> 00:07:58,500
Ez egy második vektorsorozatot eredményez, amelyet kulcsoknak nevezünk.

136
00:07:59,420 --> 00:08:01,473
Koncepcionálisan úgy kell elképzelni, hogy a kulcsok 

137
00:08:01,473 --> 00:08:03,140
potenciálisan válaszolnak a lekérdezésekre.

138
00:08:03,840 --> 00:08:06,552
Ez a kulcsmátrix szintén tele van hangolható paraméterekkel, 

139
00:08:06,552 --> 00:08:10,243
és a lekérdezési mátrixhoz hasonlóan a beágyazási vektorokat is ugyanabba a kisebb 

140
00:08:10,243 --> 00:08:11,400
dimenziós térbe képezi le.

141
00:08:12,200 --> 00:08:15,237
A kulcsokra úgy tekinthetünk, mint a lekérdezésekhez illeszkedő kulcsokra, 

142
00:08:15,237 --> 00:08:17,020
amikor azok szorosan illeszkednek egymáshoz.

143
00:08:17,460 --> 00:08:20,838
Példánkban úgy képzelhetjük el, hogy a kulcsmátrix az olyan mellékneveket, 

144
00:08:20,838 --> 00:08:23,271
mint a bolyhos és a kék, olyan vektorokhoz képezi le, 

145
00:08:23,271 --> 00:08:26,740
amelyek szorosan illeszkednek az élőlény szó által létrehozott lekérdezéshez.

146
00:08:27,200 --> 00:08:30,897
Annak méréséhez, hogy az egyes kulcsok mennyire felelnek meg az egyes lekérdezéseknek, 

147
00:08:30,897 --> 00:08:34,000
minden lehetséges kulcs-lekérdezés pár között kiszámítja a pontszorzatot.

148
00:08:34,480 --> 00:08:36,695
Szeretek egy csomó pontból álló rácsot elképzelni, 

149
00:08:36,695 --> 00:08:39,345
ahol a nagyobb pontok megfelelnek a nagyobb ponttermékeknek, 

150
00:08:39,345 --> 00:08:42,559
azoknak a helyeknek, ahol a kulcsok és a lekérdezések egymáshoz igazodnak.

151
00:08:43,280 --> 00:08:47,096
A mi melléknévi főnévi példánk esetében ez egy kicsit így nézne ki, 

152
00:08:47,096 --> 00:08:51,866
ahol ha a bolyhos és a kék által előállított kulcsok valóban szorosan illeszkednek a 

153
00:08:51,866 --> 00:08:56,692
teremtmény által előállított lekérdezéshez, akkor a pontproduktumok ezen a két helyen 

154
00:08:56,692 --> 00:08:58,320
nagy pozitív számok lennének.

155
00:08:59,100 --> 00:09:01,872
A szakzsargonban a gépi tanulással foglalkozó emberek azt mondanák, 

156
00:09:01,872 --> 00:09:05,420
hogy ez azt jelenti, hogy a bolyhos és a kék beágyazásai a lény beágyazására figyelnek.

157
00:09:06,040 --> 00:09:11,504
Ezzel szemben a pontproduktum egy másik szó, mint például a és a lény lekérdezése között 

158
00:09:11,504 --> 00:09:16,600
egy kis vagy negatív érték lenne, ami azt tükrözi, hogy nem kapcsolódnak egymáshoz.

159
00:09:17,700 --> 00:09:21,101
Tehát van egy értékekből álló rács, amely a negatív végtelentől a végtelenig 

160
00:09:21,101 --> 00:09:24,106
bármilyen valós szám lehet, ami egy pontszámot ad arra vonatkozóan, 

161
00:09:24,106 --> 00:09:27,861
hogy az egyes szavak mennyire relevánsak az összes többi szó jelentésének frissítése 

162
00:09:27,861 --> 00:09:28,480
szempontjából.

163
00:09:29,200 --> 00:09:31,770
A mód, ahogyan ezeket a pontszámokat használni fogjuk, az, 

164
00:09:31,770 --> 00:09:34,690
hogy minden oszlop mentén egy bizonyos súlyozott összeget veszünk, 

165
00:09:34,690 --> 00:09:35,780
a relevanciával súlyozva.

166
00:09:36,520 --> 00:09:40,702
Tehát ahelyett, hogy az értékek a negatív végtelentől a végtelenig terjednének, 

167
00:09:40,702 --> 00:09:44,467
azt szeretnénk, hogy az oszlopokban lévő számok 0 és 1 között legyenek, 

168
00:09:44,467 --> 00:09:48,180
és minden oszlop összege 1 legyen, mintha valószínűségi eloszlás lenne.

169
00:09:49,280 --> 00:09:52,220
Ha az előző fejezetből jöttök, akkor tudjátok, hogy akkor mit kell tennünk.

170
00:09:52,620 --> 00:09:57,300
Az értékek normalizálásához minden egyes oszlop mentén kiszámítunk egy softmaxot.

171
00:10:00,060 --> 00:10:03,276
A mi képünkön, miután a softmaxot az összes oszlopra alkalmazzuk, 

172
00:10:03,276 --> 00:10:05,860
a rácsot ezekkel a normalizált értékekkel töltjük ki.

173
00:10:06,780 --> 00:10:10,750
Ezen a ponton már nyugodtan gondolhatsz arra, hogy az egyes oszlopok súlyokat adnak 

174
00:10:10,750 --> 00:10:14,580
aszerint, hogy a bal oldali szó mennyire releváns a fent lévő megfelelő értékhez.

175
00:10:15,080 --> 00:10:16,840
Ezt a rácsot figyelemmintának nevezzük.

176
00:10:18,080 --> 00:10:20,746
Ha most megnézzük az eredeti transzformátoros papírt, 

177
00:10:20,746 --> 00:10:22,820
akkor nagyon tömör módon írják le mindezt.

178
00:10:23,880 --> 00:10:28,330
Itt a q és k változók a lekérdezési és kulcsvektorok teljes tömbjeit jelentik, 

179
00:10:28,330 --> 00:10:32,048
azokat a kis vektorokat, amelyeket a beágyazásoknak a lekérdezési 

180
00:10:32,048 --> 00:10:34,640
és a kulcsmátrixokkal való szorzásával kapunk.

181
00:10:35,160 --> 00:10:38,762
Ez a kifejezés a számlálóban egy igazán kompakt módja a kulcs- és 

182
00:10:38,762 --> 00:10:43,020
lekérdezéspárok közötti összes lehetséges pontproduktum rácsának ábrázolására.

183
00:10:44,000 --> 00:10:46,528
Egy apró technikai részlet, amit nem említettem, 

184
00:10:46,528 --> 00:10:48,954
hogy a numerikus stabilitás érdekében hasznos, 

185
00:10:48,954 --> 00:10:52,256
ha az összes ilyen értéket elosztjuk a dimenzió négyzetgyökével 

186
00:10:52,256 --> 00:10:53,960
az adott kulcslekérdezési térben.

187
00:10:54,480 --> 00:10:58,225
Ezután ez a softmax, amely a teljes kifejezés köré van tekerve, 

188
00:10:58,225 --> 00:11:00,800
úgy értendő, hogy oszloponként alkalmazandó.

189
00:11:01,640 --> 00:11:04,700
Ami a v kifejezést illeti, erről mindjárt beszélünk.

190
00:11:05,020 --> 00:11:08,460
Előtte van még egy technikai részlet, amit eddig kihagytam.

191
00:11:09,040 --> 00:11:13,120
A képzési folyamat során, amikor ezt a modellt egy adott szövegpéldán futtatjuk, 

192
00:11:13,120 --> 00:11:15,639
és az összes súlyt kissé módosítjuk és hangoljuk, 

193
00:11:15,639 --> 00:11:18,007
hogy vagy jutalmazzuk vagy büntessük aszerint, 

194
00:11:18,007 --> 00:11:22,189
hogy mekkora valószínűséget rendel a szövegben a valódi következő szóhoz, kiderül, 

195
00:11:22,189 --> 00:11:25,212
hogy sokkal hatékonyabbá teszi az egész képzési folyamatot, 

196
00:11:25,212 --> 00:11:29,695
ha egyszerre minden lehetséges következő jelet megjósolunk a szövegben lévő jelek minden 

197
00:11:29,695 --> 00:11:31,560
egyes kezdeti részsorozatát követően.

198
00:11:31,940 --> 00:11:35,765
Például az általunk vizsgált kifejezéssel kapcsolatban azt is megjósolhatjuk, 

199
00:11:35,765 --> 00:11:39,100
hogy milyen szavak követik a teremtményt és milyen szavak követik a.

200
00:11:39,940 --> 00:11:43,837
Ez nagyon szép, mert ez azt jelenti, hogy ami egyébként egyetlen képzési példa lenne, 

201
00:11:43,837 --> 00:11:45,560
az ténylegesen több példaként működik.

202
00:11:46,100 --> 00:11:49,866
A figyelem mintánk szempontjából ez azt jelenti, hogy soha ne engedjük, 

203
00:11:49,866 --> 00:11:52,796
hogy a későbbi szavak befolyásolják a korábbi szavakat, 

204
00:11:52,796 --> 00:11:56,040
mert különben elárulhatják a választ arra, hogy mi következik.

205
00:11:56,560 --> 00:11:59,325
Ez azt jelenti, hogy azt akarjuk, hogy ezek a pontok, 

206
00:11:59,325 --> 00:12:02,602
amelyek a korábbiakat befolyásoló későbbi tokeneket képviselik, 

207
00:12:02,602 --> 00:12:04,600
valahogy nullára legyenek kényszerítve.

208
00:12:05,920 --> 00:12:08,861
A legegyszerűbb, ha nullára állítjuk őket, de ha ezt tennénk, 

209
00:12:08,861 --> 00:12:12,420
akkor az oszlopok már nem adódnának össze eggyé, nem lennének normalizálva.

210
00:12:13,120 --> 00:12:16,180
Ehelyett a softmax alkalmazása előtt az egyik általános megoldás az, 

211
00:12:16,180 --> 00:12:19,020
hogy az összes ilyen bejegyzést negatív végtelennek állítjuk be.

212
00:12:19,680 --> 00:12:23,346
Ha ezt megteszi, akkor a softmax alkalmazása után ezek mind nullává válnak, 

213
00:12:23,346 --> 00:12:25,180
de az oszlopok normalizáltak maradnak.

214
00:12:26,000 --> 00:12:27,540
Ezt a folyamatot maszkolásnak nevezik.

215
00:12:27,540 --> 00:12:30,526
A figyelemnek vannak olyan változatai, ahol ezt nem alkalmazzuk, 

216
00:12:30,526 --> 00:12:33,696
de a mi GPT példánkban, még ha ez a képzési fázisban relevánsabb is, 

217
00:12:33,696 --> 00:12:36,406
mint mondjuk chatbotként vagy valami ilyesmiként futtatva, 

218
00:12:36,406 --> 00:12:39,117
mindig alkalmazzuk ezt a maszkolást, hogy megakadályozzuk, 

219
00:12:39,117 --> 00:12:41,460
hogy a későbbi tokenek befolyásolják a korábbiakat.

220
00:12:42,480 --> 00:12:46,562
Egy másik tény, amin érdemes elgondolkodni ezzel a figyelemmintával kapcsolatban, 

221
00:12:46,562 --> 00:12:49,500
hogy a mérete megegyezik a kontextus méretének négyzetével.

222
00:12:49,900 --> 00:12:52,829
Ezért lehet a kontextus mérete igazán nagy szűk keresztmetszet 

223
00:12:52,829 --> 00:12:55,620
a nagy nyelvi modellek számára, és a skálázás nem triviális.

224
00:12:56,300 --> 00:13:00,112
Ahogy azt elképzelheted, az egyre nagyobb és nagyobb kontextusablakok iránti vágytól 

225
00:13:00,112 --> 00:13:03,834
vezérelve az elmúlt években a figyelem mechanizmusának néhány változata született, 

226
00:13:03,834 --> 00:13:06,212
amelyek célja a kontextus skálázhatóbbá tétele volt, 

227
00:13:06,212 --> 00:13:08,320
de itt most te és én az alapokra koncentrálunk.

228
00:13:10,560 --> 00:13:13,331
Oké, nagyszerű, ennek a mintának a kiszámítása lehetővé teszi a modell számára, 

229
00:13:13,331 --> 00:13:15,480
hogy levezesse, mely szavak milyen más szavakhoz kapcsolódnak.

230
00:13:16,020 --> 00:13:20,069
Most már ténylegesen frissíteni kell a beágyazásokat, lehetővé téve a szavak számára, 

231
00:13:20,069 --> 00:13:22,800
hogy információt adjanak át a számukra releváns szavaknak.

232
00:13:22,800 --> 00:13:26,913
Például azt akarod, hogy a Pelyhes beágyazása valahogyan olyan változást 

233
00:13:26,913 --> 00:13:30,688
idézzen elő a Lényben, amely a 12 000 dimenziós beágyazási tér egy 

234
00:13:30,688 --> 00:13:34,520
másik részébe helyezi át, amely pontosabban kódol egy Pelyhes lényt.

235
00:13:35,460 --> 00:13:38,954
Amit itt tenni fogok, az az, hogy először megmutatom a legegyszerűbb módot, 

236
00:13:38,954 --> 00:13:42,816
ahogyan ezt megteheted, bár van egy kis mód, ahogyan ez módosul a többfejű figyelem 

237
00:13:42,816 --> 00:13:43,460
kontextusában.

238
00:13:44,080 --> 00:13:47,628
Ez a legegyszerűbb módja az lenne, ha használnánk egy harmadik mátrixot, 

239
00:13:47,628 --> 00:13:50,884
amit mi értékmátrixnak nevezünk, és amit megszorozunk az első szó, 

240
00:13:50,884 --> 00:13:52,440
például a Pelyhes beágyazásával.

241
00:13:53,300 --> 00:13:56,483
Ennek eredménye az, amit értékvektornak neveznénk, és ez valami, 

242
00:13:56,483 --> 00:13:59,911
amit hozzáadunk a második szó beágyazásához, ebben az esetben valami, 

243
00:13:59,911 --> 00:14:01,920
amit hozzáadunk a Creature beágyazásához.

244
00:14:02,600 --> 00:14:07,000
Ez az értékvektor tehát ugyanabban a nagyon nagy dimenziójú térben él, mint a beágyazások.

245
00:14:07,460 --> 00:14:12,255
Ha ezt az értékmátrixot megszorozzuk egy szó beágyazásával, akkor úgy gondolhatjuk, 

246
00:14:12,255 --> 00:14:16,593
hogy ha ez a szó releváns valami más jelentésének beállítása szempontjából, 

247
00:14:16,593 --> 00:14:21,160
akkor pontosan mit kell hozzáadni a valami más beágyazásához, hogy ezt tükrözze?

248
00:14:22,140 --> 00:14:26,592
Ha visszatekintünk a diagramunkra, tegyük félre az összes kulcsot és a lekérdezéseket, 

249
00:14:26,592 --> 00:14:29,867
mivel miután kiszámítottuk a figyelemmintát, azokkal végeztünk, 

250
00:14:29,867 --> 00:14:33,808
majd fogjuk ezt az értékmátrixot, és megszorozzuk minden egyes beágyazással, 

251
00:14:33,808 --> 00:14:36,060
hogy létrehozzuk az értékvektorok sorozatát.

252
00:14:37,120 --> 00:14:39,206
Úgy gondolhatsz ezekre az értékvektorokra, mint 

253
00:14:39,206 --> 00:14:41,120
amelyek a megfelelő kulcsokhoz kapcsolódnak.

254
00:14:42,320 --> 00:14:45,910
A diagram minden egyes oszlopához megszorozza az egyes 

255
00:14:45,910 --> 00:14:49,240
értékvektorokat az adott oszlop megfelelő súlyával.

256
00:14:50,080 --> 00:14:55,564
Például itt, a Creature beágyazása alatt a Pelyhes és a Kék értékvektorok nagy részét 

257
00:14:55,564 --> 00:14:59,264
hozzáadnád, míg az összes többi értékvektort lenulláznád, 

258
00:14:59,264 --> 00:15:01,560
vagy legalábbis majdnem lenulláznád.

259
00:15:02,120 --> 00:15:06,022
És végül, az ehhez az oszlophoz kapcsolódó beágyazás frissítésének módja, 

260
00:15:06,022 --> 00:15:09,872
amely korábban a Creature valamilyen kontextusmentes jelentését kódolta, 

261
00:15:09,872 --> 00:15:12,931
összeadjuk az oszlopban lévő összes átméretezett értéket, 

262
00:15:12,931 --> 00:15:17,044
létrehozva egy hozzáadni kívánt változást, amelyet delta-e-nek fogok nevezni, 

263
00:15:17,044 --> 00:15:19,260
és ezt hozzáadjuk az eredeti beágyazáshoz.

264
00:15:19,680 --> 00:15:22,447
Remélhetőleg az eredmény egy kifinomultabb vektor lesz, 

265
00:15:22,447 --> 00:15:26,500
amely a kontextuálisan gazdagabb jelentést kódolja, például egy bolyhos kék lényt.

266
00:15:27,380 --> 00:15:30,025
És persze ezt nem csak egy beágyazással csináljuk, 

267
00:15:30,025 --> 00:15:33,708
hanem ugyanazt a súlyozott összeget alkalmazzuk a kép összes oszlopán, 

268
00:15:33,708 --> 00:15:37,806
ami a változások sorozatát eredményezi, és ha ezeket a változásokat hozzáadjuk 

269
00:15:37,806 --> 00:15:41,592
a megfelelő beágyazásokhoz, akkor a figyelemblokkból előbukkanó finomabb 

270
00:15:41,592 --> 00:15:43,460
beágyazások teljes sorozatát kapjuk.

271
00:15:44,860 --> 00:15:49,100
Ha kicsinyítjük, az egész folyamatot egyetlen figyelemfelkeltő fejnek neveznénk.

272
00:15:49,600 --> 00:15:53,903
Ahogyan eddig leírtam a dolgokat, ezt a folyamatot három különböző mátrix paraméterezi, 

273
00:15:53,903 --> 00:15:56,739
amelyek mindegyike hangolható paraméterekkel, a kulccsal, 

274
00:15:56,739 --> 00:15:58,940
a lekérdezéssel és az értékkel van feltöltve.

275
00:15:59,500 --> 00:16:03,942
Szeretném egy pillanatra folytatni azt, amit az előző fejezetben kezdtünk, a pontozással, 

276
00:16:03,942 --> 00:16:08,040
ahol a GPT-3 számok segítségével számoljuk össze a modellparaméterek teljes számát.

277
00:16:09,300 --> 00:16:13,296
Ezek a kulcs- és lekérdezési mátrixok mindegyike 12 288 oszlopból áll, 

278
00:16:13,296 --> 00:16:16,335
ami megfelel a beágyazási dimenziónak, és 128 sorból, 

279
00:16:16,335 --> 00:16:19,600
ami megfelel a kisebb kulcs-lekérdezési tér dimenziójának.

280
00:16:20,260 --> 00:16:24,220
Ez további körülbelül 1,5 millió paramétert ad mindegyikhez.

281
00:16:24,860 --> 00:16:28,020
Ha ezzel szemben megnézzük ezt az értékmátrixot, 

282
00:16:28,020 --> 00:16:32,599
akkor az eddigi leírásom alapján úgy tűnik, hogy ez egy négyzetmátrix, 

283
00:16:32,599 --> 00:16:36,985
amelynek 12 288 oszlopa és 12 288 sora van, mivel mind a bemenetei, 

284
00:16:36,985 --> 00:16:40,920
mind a kimenetei ebben a nagyon nagy beágyazási térben élnek.

285
00:16:41,500 --> 00:16:45,140
Ha ez igaz, akkor ez körülbelül 150 millió új paramétert jelentene.

286
00:16:45,660 --> 00:16:47,300
És hogy tisztázzuk, ezt megteheted.

287
00:16:47,420 --> 00:16:50,272
Az értéktérképnek nagyságrendekkel több paramétert tudna szentelni, 

288
00:16:50,272 --> 00:16:51,740
mint a kulcsnak és a lekérdezésnek.

289
00:16:52,060 --> 00:16:55,367
A gyakorlatban azonban sokkal hatékonyabb, ha ehelyett úgy alakítjuk ki, 

290
00:16:55,367 --> 00:16:58,177
hogy az értéktérképhez tartozó paraméterek száma megegyezik a 

291
00:16:58,177 --> 00:17:00,760
kulcshoz és a lekérdezéshez tartozó paraméterek számával.

292
00:17:01,460 --> 00:17:05,160
Ez különösen fontos abban a helyzetben, amikor több figyelemfej párhuzamosan fut.

293
00:17:06,240 --> 00:17:10,099
Ez úgy néz ki, hogy az értéktérképet két kisebb mátrix szorzataként faktoráljuk.

294
00:17:11,180 --> 00:17:14,210
Koncepcionálisan még mindig arra bátorítanám, hogy gondolkodjon az 

295
00:17:14,210 --> 00:17:16,427
általános lineáris térképen, egy olyan térképen, 

296
00:17:16,427 --> 00:17:20,000
amelynek bemenetei és kimenetei mind ebben a nagyobb beágyazási térben vannak, 

297
00:17:20,000 --> 00:17:23,800
például a kék beágyazását a kékesség irányába véve, amelyet a főnevekhez adna hozzá.

298
00:17:27,040 --> 00:17:29,825
Csupán arról van szó, hogy kisebb számú sorról van szó, 

299
00:17:29,825 --> 00:17:32,760
jellemzően ugyanolyan méretű, mint a kulcslekérdezés helye.

300
00:17:33,100 --> 00:17:38,440
Ez azt jelenti, hogy a nagy beágyazási vektorokat egy sokkal kisebb térbe képezzük le.

301
00:17:39,040 --> 00:17:42,700
Ez nem a hagyományos elnevezés, de én ezt érték lefelé mátrixnak fogom nevezni.

302
00:17:43,400 --> 00:17:46,850
A második mátrix ebből a kisebb térből visszatérképez a beágyazási térbe, 

303
00:17:46,850 --> 00:17:50,580
és létrehozza azokat a vektorokat, amelyeket a tényleges frissítésekhez használ.

304
00:17:51,000 --> 00:17:54,740
Ezt a mátrixot értéknövelő mátrixnak fogom nevezni, ami megint csak nem szokványos.

305
00:17:55,160 --> 00:17:58,080
A legtöbb újságban ezt egy kicsit másképp írják le.

306
00:17:58,380 --> 00:17:59,520
Egy perc múlva beszélek róla.

307
00:17:59,700 --> 00:18:02,540
Véleményem szerint ez egy kicsit zavarosabbá teszi a dolgokat fogalmilag.

308
00:18:03,260 --> 00:18:06,154
A lineáris algebrai szakzsargonnal élve, alapvetően azt tesszük, 

309
00:18:06,154 --> 00:18:09,360
hogy a teljes értéktérképet úgy korlátozzuk, hogy az egy alacsony rangú 

310
00:18:09,360 --> 00:18:10,340
transzformáció legyen.

311
00:18:11,420 --> 00:18:15,643
Visszatérve a paraméterek számához, mind a négy mátrix mérete megegyezik, 

312
00:18:15,643 --> 00:18:20,780
és ha mindet összeadjuk, akkor egy figyelmi fejre körülbelül 6,3 millió paramétert kapunk.

313
00:18:22,040 --> 00:18:24,928
Egy gyors mellékes megjegyzésként, hogy egy kicsit pontosabbak legyünk, minden, 

314
00:18:24,928 --> 00:18:27,456
amit eddig leírtunk, az az, amit az emberek önfigyelemnek neveznének, 

315
00:18:27,456 --> 00:18:30,272
hogy megkülönböztessük egy olyan változattól, ami más modellekben fordul elő, 

316
00:18:30,272 --> 00:18:31,500
és amit keresztfigyelemnek hívnak.

317
00:18:32,300 --> 00:18:35,678
Ez nem releváns a GPT példánk szempontjából, de ha kíváncsi vagy rá, 

318
00:18:35,678 --> 00:18:38,224
a kereszt-figyelem olyan modelleket foglal magában, 

319
00:18:38,224 --> 00:18:40,721
amelyek két különböző típusú adatot dolgoznak fel, 

320
00:18:40,721 --> 00:18:43,560
például egy nyelvi szöveget és egy másik nyelvi szöveget, 

321
00:18:43,560 --> 00:18:46,351
amely egy fordítás folyamatban lévő generálásának része, 

322
00:18:46,351 --> 00:18:49,240
vagy esetleg beszédhangot és egy folyamatban lévő átiratot.

323
00:18:50,400 --> 00:18:52,700
A keresztirányú figyelemfelkeltő fej majdnem ugyanúgy néz ki.

324
00:18:52,980 --> 00:18:55,550
Az egyetlen különbség az, hogy a kulcs- és a lekérdezési 

325
00:18:55,550 --> 00:18:57,400
térképek különböző adathalmazokra hatnak.

326
00:18:57,840 --> 00:19:01,359
Egy fordítást végző modellben például a kulcsok az egyik nyelvből, 

327
00:19:01,359 --> 00:19:05,772
míg a lekérdezések egy másik nyelvből származhatnak, és a figyelmi minta leírhatja, 

328
00:19:05,772 --> 00:19:09,660
hogy az egyik nyelv melyik szavának melyik szó felel meg a másik nyelvben.

329
00:19:10,340 --> 00:19:12,641
És ebben a környezetben jellemzően nem lenne maszkolás, 

330
00:19:12,641 --> 00:19:16,340
mivel nem igazán van olyan elképzelés, hogy a későbbi tokenek befolyásolnák a korábbiakat.

331
00:19:17,180 --> 00:19:21,031
Ha azonban az önfigyelésre összpontosítanál, és ha eddig mindent megértettél, 

332
00:19:21,031 --> 00:19:25,180
és ha itt megállnál, akkor a lényegét kapnád annak, hogy mi is a figyelem valójában.

333
00:19:25,760 --> 00:19:29,475
Már csak az van hátra, hogy sok-sok különböző alkalommal lefektessük, 

334
00:19:29,475 --> 00:19:31,440
hogy milyen értelemben csináljuk ezt.

335
00:19:32,100 --> 00:19:35,732
Központi példánkban a főneveket frissítő melléknevekre összpontosítottunk, 

336
00:19:35,732 --> 00:19:39,800
de természetesen a szövegkörnyezet sokféleképpen befolyásolhatja egy szó jelentését.

337
00:19:40,360 --> 00:19:43,408
Ha az összetört szavak megelőzték az autó szót, 

338
00:19:43,408 --> 00:19:46,520
az hatással van az autó alakjára és szerkezetére.

339
00:19:47,200 --> 00:19:49,280
És sok asszociáció talán kevésbé nyelvtani.

340
00:19:49,760 --> 00:19:53,591
Ha a varázsló szó valahol ugyanabban a szövegben szerepel, mint Harry, 

341
00:19:53,591 --> 00:19:56,560
akkor ez azt sugallja, hogy ez Harry Potterre utalhat, 

342
00:19:56,560 --> 00:20:00,716
míg ha ehelyett a királynő, Sussex és Vilmos szavak szerepelnek a szövegben, 

343
00:20:00,716 --> 00:20:04,440
akkor talán a Harry beágyazását inkább a hercegre kellene frissíteni.

344
00:20:05,040 --> 00:20:08,615
A kontextuális frissítés minden egyes elképzelhető különböző típusához 

345
00:20:08,615 --> 00:20:12,140
más-más kulcs- és lekérdezési mátrix paraméterei lennének a különböző 

346
00:20:12,140 --> 00:20:15,514
figyelemminták megragadásához, és az értéktérképünk paraméterei is 

347
00:20:15,514 --> 00:20:19,140
különbözőek lennének aszerint, hogy mit kell hozzáadni a beágyazásokhoz.

348
00:20:19,980 --> 00:20:23,190
És ismétlem, a gyakorlatban ezeknek a térképeknek a valódi viselkedését sokkal 

349
00:20:23,190 --> 00:20:26,197
nehezebb értelmezni, ahol a súlyok úgy vannak beállítva, hogy azt tegyék, 

350
00:20:26,197 --> 00:20:29,855
amire a modellnek szüksége van, hogy a legjobban elérje a következő token előrejelzésének 

351
00:20:29,855 --> 00:20:30,140
célját.

352
00:20:31,400 --> 00:20:34,908
Mint már említettem, minden, amit leírtunk, egy-egy figyelemfej, 

353
00:20:34,908 --> 00:20:39,226
és egy teljes figyelemblokk egy transzformátoron belül egy úgynevezett többfejű 

354
00:20:39,226 --> 00:20:42,789
figyelemből áll, ahol sok ilyen műveletet futtatunk párhuzamosan, 

355
00:20:42,789 --> 00:20:45,920
mindegyiknek saját kulcslekérdezésével és értéktérképével.

356
00:20:47,420 --> 00:20:51,700
A GPT-3 például 96 figyelemfelhívó fejet használ minden egyes blokkban.

357
00:20:52,020 --> 00:20:54,433
Figyelembe véve, hogy mindegyik már egy kicsit zavaros, 

358
00:20:54,433 --> 00:20:56,460
ez bizonyára sok, amit a fejedben kell tartani.

359
00:20:56,760 --> 00:20:59,850
Csak hogy mindent nagyon világosan kifejtsünk, ez azt jelenti, 

360
00:20:59,850 --> 00:21:02,547
hogy 96 különböző kulcs- és lekérdezési mátrixunk van, 

361
00:21:02,547 --> 00:21:05,000
amelyek 96 különböző figyelemmintát eredményeznek.

362
00:21:05,440 --> 00:21:09,039
Ezután minden fejnek megvan a maga külön értékmátrixa, 

363
00:21:09,039 --> 00:21:12,180
amelyből 96 értékvektor-sorozatot állítanak elő.

364
00:21:12,460 --> 00:21:16,680
Ezeket mind összeadjuk a megfelelő figyelemminták súlyként való felhasználásával.

365
00:21:17,480 --> 00:21:20,252
Ez azt jelenti, hogy a kontextus minden egyes pozíciójára, 

366
00:21:20,252 --> 00:21:24,106
minden egyes tokenre, minden egyes ilyen fej egy javasolt változtatást hoz létre, 

367
00:21:24,106 --> 00:21:27,020
amelyet hozzá kell adni az adott pozícióban lévő beágyazáshoz.

368
00:21:27,660 --> 00:21:31,032
Tehát azt kell tennie, hogy összeadja az összes javasolt módosítást, 

369
00:21:31,032 --> 00:21:34,795
minden egyes fejre egyet, és az eredményt hozzáadja az adott pozíció eredeti 

370
00:21:34,795 --> 00:21:35,480
beágyazásához.

371
00:21:36,660 --> 00:21:42,924
Ez a teljes összeg itt egy szelete lenne annak, amit ez a többfejű figyelemblokk kiad, 

372
00:21:42,924 --> 00:21:47,460
egyetlen ilyen finomított beágyazás, ami a másik végén kiugrik.

373
00:21:48,320 --> 00:21:50,520
Ismétlem, ez nagyon sok gondolat, ezért ne aggódjon, 

374
00:21:50,520 --> 00:21:52,140
ha egy kis időbe telik, amíg belemerül.

375
00:21:52,380 --> 00:21:57,044
Az általános elképzelés az, hogy sok különböző fej párhuzamos futtatásával a modell 

376
00:21:57,044 --> 00:22:01,820
képes megtanulni, hogy a kontextus milyen különböző módon változtatja meg a jelentést.

377
00:22:03,700 --> 00:22:06,890
Ha a paraméterek számának számbavételét 96 fejjel végezzük, 

378
00:22:06,890 --> 00:22:10,294
amelyek mindegyike tartalmazza e négy mátrix saját variációját, 

379
00:22:10,294 --> 00:22:15,080
akkor a többfejű figyelem minden egyes blokkja körülbelül 600 millió paramétert tartalmaz.

380
00:22:16,420 --> 00:22:20,107
Van egy további, kissé bosszantó dolog, amit tényleg meg kell említenem mindazoknak, 

381
00:22:20,107 --> 00:22:21,800
akik tovább olvasnak a Transformersről.

382
00:22:22,080 --> 00:22:26,314
Emlékeztek, hogy azt mondtam, hogy az értéktérképet két különböző mátrixra osztjuk, 

383
00:22:26,314 --> 00:22:29,440
amelyeket érték lefelé és érték felfelé mátrixnak neveztem el.

384
00:22:29,960 --> 00:22:33,012
Ahogyan én a dolgokat kialakítottam, az azt sugallja, 

385
00:22:33,012 --> 00:22:36,235
hogy ezt a mátrixpárt minden egyes figyelemfejben látod, 

386
00:22:36,235 --> 00:22:38,440
és ezt teljesen így is megvalósíthatod.

387
00:22:38,640 --> 00:22:39,920
Ez egy érvényes konstrukció lenne.

388
00:22:40,260 --> 00:22:44,920
De a papírokban leírtak és a gyakorlati megvalósítás módja kissé eltér egymástól.

389
00:22:45,340 --> 00:22:50,676
Az egyes fejekhez tartozó összes ilyen értékmátrix egy hatalmas mátrixban jelenik meg, 

390
00:22:50,676 --> 00:22:55,644
amelyet kimeneti mátrixnak nevezünk, és amely a teljes többfejű figyelemblokkhoz 

391
00:22:55,644 --> 00:22:56,380
kapcsolódik.

392
00:22:56,820 --> 00:23:00,577
És amikor az emberek egy adott figyelemfej értékmátrixára hivatkoznak, 

393
00:23:00,577 --> 00:23:03,594
akkor általában csak erre az első lépésre utalnak, arra, 

394
00:23:03,594 --> 00:23:07,140
amit én úgy neveztem, hogy az érték lefelé vetítése a kisebb térbe.

395
00:23:08,340 --> 00:23:11,040
A kíváncsiskodóknak hagytam egy megjegyzést a képernyőn.

396
00:23:11,260 --> 00:23:13,652
Ez egyike azoknak a részleteknek, amelyeknél fennáll a veszélye annak, 

397
00:23:13,652 --> 00:23:15,574
hogy elvonják a figyelmet a fő koncepcionális pontokról, 

398
00:23:15,574 --> 00:23:18,540
de azért szeretném felhívni rá a figyelmet, hogy ha más forrásokban olvasol erről, tudd.

399
00:23:19,240 --> 00:23:23,162
Félretéve minden technikai árnyalatot, az előző fejezet előzeteseiben láttuk, 

400
00:23:23,162 --> 00:23:27,537
hogy a transzformátoron keresztül áramló adatok nem csak egy figyelemblokkon keresztül 

401
00:23:27,537 --> 00:23:28,040
áramlanak.

402
00:23:28,640 --> 00:23:32,700
Először is, átmegy ezeken a többrétegű perceptronoknak nevezett egyéb műveleteken is.

403
00:23:33,120 --> 00:23:34,880
Ezekről a következő fejezetben többet fogunk beszélni.

404
00:23:35,180 --> 00:23:39,320
Ezután mindkét művelet sok-sok másolatát ismételgeti.

405
00:23:39,980 --> 00:23:44,476
Ez azt jelenti, hogy miután egy adott szó magába szívja a kontextus egy részét, 

406
00:23:44,476 --> 00:23:49,309
sokkal több esély van arra, hogy ezt az árnyaltabb beágyazódást árnyaltabb környezete 

407
00:23:49,309 --> 00:23:50,040
befolyásolja.

408
00:23:50,940 --> 00:23:54,763
Minél lejjebb haladunk a hálózatban, és minden egyes beágyazás egyre több 

409
00:23:54,763 --> 00:23:57,243
jelentést vesz át az összes többi beágyazásból, 

410
00:23:57,243 --> 00:24:00,137
amelyek maguk is egyre árnyaltabbá válnak, a remény az, 

411
00:24:00,137 --> 00:24:04,168
hogy képesek leszünk magasabb szintű és absztraktabb gondolatokat kódolni egy 

412
00:24:04,168 --> 00:24:07,320
adott bemenetről a leírásokon és a nyelvtani szerkezeten túl.

413
00:24:07,880 --> 00:24:11,314
Olyan dolgok, mint az érzelmek és a hangnem, és hogy versről van-e szó, 

414
00:24:11,314 --> 00:24:15,130
és hogy milyen tudományos igazságok állnak a mű mögött, és ehhez hasonló dolgok.

415
00:24:16,700 --> 00:24:21,499
Még egyszer visszatérve a pontozáshoz, a GPT-3 96 különböző réteget tartalmaz, 

416
00:24:21,499 --> 00:24:26,480
így a kulcskérdés- és értékparaméterek teljes számát megszorozzuk további 96-tal, 

417
00:24:26,480 --> 00:24:31,826
ami a teljes összeget valamivel kevesebb, mint 58 milliárd különböző paraméterre teszi, 

418
00:24:31,826 --> 00:24:34,500
amelyek az összes figyelemfejre vonatkoznak.

419
00:24:34,980 --> 00:24:40,940
Ez bizonyára sok, de ez csak egyharmada a hálózatban lévő összesen 175 milliárdnak.

420
00:24:41,520 --> 00:24:44,062
Tehát bár a figyelem kapja az összes figyelmet, 

421
00:24:44,062 --> 00:24:48,140
a paraméterek többsége az e lépések között elhelyezkedő blokkokból származik.

422
00:24:48,560 --> 00:24:51,329
A következő fejezetben többet fogunk beszélni ezekről a többi blokkról, 

423
00:24:51,329 --> 00:24:53,560
és sokkal többet fogunk beszélni a képzési folyamatról is.

424
00:24:54,120 --> 00:24:58,680
A figyelemmechanizmus sikerének nagy része nem annyira az általa lehetővé tett 

425
00:24:58,680 --> 00:25:02,953
konkrét viselkedés, hanem az a tény, hogy rendkívül jól párhuzamosítható, 

426
00:25:02,953 --> 00:25:07,802
ami azt jelenti, hogy a GPU-k segítségével rövid idő alatt rengeteg számítást lehet 

427
00:25:07,802 --> 00:25:08,380
elvégezni.

428
00:25:09,460 --> 00:25:13,210
Mivel az elmúlt egy-két évtized egyik nagy tanulsága a mélytanulással kapcsolatban az 

429
00:25:13,210 --> 00:25:16,611
volt, hogy a skála önmagában hatalmas minőségi javulást eredményez a modellek 

430
00:25:16,611 --> 00:25:20,275
teljesítményében, a párhuzamosítható architektúráknak, amelyek ezt lehetővé teszik, 

431
00:25:20,275 --> 00:25:21,060
óriási előnye van.

432
00:25:22,040 --> 00:25:25,340
Ha többet szeretnél megtudni ezekről a dolgokról, sok linket hagytam a leírásban.

433
00:25:25,920 --> 00:25:30,040
Különösen az Andrej Karpathy vagy Chris Ola által készített lemezek aranyat érnek.

434
00:25:30,560 --> 00:25:33,851
Ebben a videóban csak a jelenlegi formában akartam belevágni a figyelembe, 

435
00:25:33,851 --> 00:25:36,264
de ha kíváncsiak vagytok arra, hogyan jutottunk idáig, 

436
00:25:36,264 --> 00:25:38,897
és hogyan találhatjátok fel újra ezt az ötletet magatoknak, 

437
00:25:38,897 --> 00:25:42,540
Vivek barátom most tett fel néhány videót, amelyekben sokkal több motivációt adnak.

438
00:25:43,120 --> 00:25:45,767
Britt Cruz a The Art of the Problem csatornáról készített 

439
00:25:45,767 --> 00:25:48,460
egy nagyon szép videót a nagy nyelvi modellek történetéről.

440
00:26:04,960 --> 00:26:09,200
Köszönöm.

