[
 {
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "translatedText": "Az előző fejezetben ön és én elkezdtük végigjárni a transzformátor belső működését.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "translatedText": "Ez az egyik legfontosabb technológia a nagy nyelvi modellekben és a mesterséges intelligencia modern hullámának számos más eszközében.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "translatedText": "Először egy 2017-es, ma már híres, Attention is All You Need című tanulmányban jelent meg, és ebben a fejezetben te és én beleássuk magunkat abba, hogy mi is ez a figyelemmechanizmus, és vizualizáljuk, hogyan dolgozza fel az adatokat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "translatedText": "Gyorsan összefoglalva, itt van a fontos összefüggés, amit szeretném, ha észben tartanál.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "translatedText": "A modell célja, amit mi ketten tanulunk, az, hogy befogadjunk egy szövegrészletet, és megjósoljuk, melyik szó következik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "translatedText": "A bemeneti szöveget apró darabokra bontjuk, amelyeket tokeneknek nevezünk, és ezek nagyon gyakran szavak vagy szavak darabjai, de csak azért, hogy a videóban szereplő példákat könnyebben el tudjuk képzelni, egyszerűsítsük le, és tegyünk úgy, mintha a tokenek mindig csak szavak lennének.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "translatedText": "A transzformátor első lépése az, hogy minden egyes tokent egy nagydimenziós vektorral társítunk, amit beágyazásnak nevezünk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "translatedText": "A legfontosabb gondolat, amit szeretném, ha észben tartanál, hogy az összes lehetséges beágyazás magas dimenziójú terének irányai hogyan feleltethetők meg a szemantikai jelentésnek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "translatedText": "Az előző fejezetben láttunk egy példát arra, hogy az irány hogyan felelhet meg a nemnek, abban az értelemben, hogy egy bizonyos lépés hozzáadásával ebben a térben egy hímnemű főnév beágyazásától a megfelelő nőnemű főnév beágyazásáig juthatunk el.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "translatedText": "Ez csak egy példa, és el tudod képzelni, hogy ebben a nagy dimenziójú térben hány más irány felelhet meg egy szó jelentésének számos más aspektusának.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "translatedText": "A transzformátor célja, hogy fokozatosan kiigazítsa ezeket a beágyazásokat, hogy ne csak egy-egy szót kódoljanak, hanem sokkal, de sokkal gazdagabb kontextuális jelentéssel bírjanak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "translatedText": "Elöljáróban el kell mondanom, hogy sokan nagyon zavarosnak találják a figyelemmechanizmust, a transzformátor e kulcsfontosságú darabját, ezért ne aggódjon, ha egy kis időbe telik, amíg a dolgok megértik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "translatedText": "Úgy gondolom, hogy mielőtt belemerülnénk a számítási részletekbe és az összes mátrixszorzásba, érdemes elgondolkodni néhány példán arra a fajta viselkedésre, amit a figyelemnek lehetővé kell tennie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "translatedText": "Tekintsük a kifejezéseket amerikai valódi anyajegy, egy széndioxid-molekula, és vegyünk biopsziát az anyajegyből.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "translatedText": "Mindketten tudjuk, hogy a vakond szónak mindegyikben más-más jelentése van, a kontextustól függően.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "translatedText": "De a transzformátor első lépése után, amely felbontja a szöveget és minden egyes tokent egy vektorhoz társít, a molekulához társított vektor minden ilyen esetben ugyanaz lenne, mivel ez a kezdeti tokenbeágyazás gyakorlatilag egy keresőtábla, amely nem hivatkozik a kontextusra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "translatedText": "Csak a transzformátor következő lépésében van esélye a környező beágyazásoknak arra, hogy információt adjanak át ebbe a beágyazásba.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "translatedText": "Az a kép, amit önök talán szem előtt tartanak, hogy ebben a beágyazási térben több különböző irány létezik, amelyek a vakond szó több különböző jelentését kódolják, és hogy egy jól képzett figyelemblokk kiszámítja, hogy mit kell hozzáadni az általános beágyazáshoz, hogy az a kontextus függvényében elmozduljon ezen specifikus irányok valamelyikébe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "input": "To take another example, consider the embedding of the word tower.",
  "translatedText": "Vegyünk egy másik példát, nézzük meg a torony szó beágyazását.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "translatedText": "Ez feltehetően valami nagyon általános, nem specifikus irány a térben, amely sok más nagy, magas főnévvel társul.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "translatedText": "Ha ezt a szót közvetlenül megelőzné az Eiffel szó, elképzelhető, hogy a mechanizmus frissítené ezt a vektort, hogy olyan irányba mutasson, amely pontosabban kódolja az Eiffel-tornyot, esetleg Párizshoz, Franciaországhoz és acélból készült dolgokhoz kapcsolódó vektorokkal korrelálva.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "translatedText": "Ha a miniatűr szó is megelőzte, akkor a vektort még tovább kell frissíteni, hogy többé ne a nagy, magas dolgokkal korreláljon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "translatedText": "A figyelem blokkja általánosabban, mint egy szó jelentésének finomítása, lehetővé teszi a modell számára, hogy az egyik beágyazásban kódolt információt átvigye egy másik beágyazásba, potenciálisan olyanokba, amelyek elég messze vannak, és potenciálisan olyan információkkal, amelyek sokkal gazdagabbak, mint egyetlen szó.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "translatedText": "Az előző fejezetben azt láttuk, hogy miután az összes vektor átfut a hálózaton, beleértve sok különböző figyelemblokkot, a számítás, amelyet a következő token előrejelzésének előállítása érdekében végzel, teljes mértékben a szekvencia utolsó vektorának függvénye.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "translatedText": "Képzeljük el például, hogy a beírt szöveg egy egész krimi nagy része, egészen a végéhez közeli pontig, ahol ez áll: \"Ezért volt a gyilkos\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "translatedText": "Ha a modell pontosan meg akarja jósolni a következő szót, akkor a szekvencia utolsó vektorának, amely az életét egyszerűen a volt szó beágyazásával kezdte, az összes figyelemblokknak frissülnie kell, hogy sokkal, de sokkal többet képviseljen, mint bármelyik szó, valahogyan kódolva a teljes kontextusablakból származó összes információt, amely releváns a következő szó előrejelzése szempontjából.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "input": "To step through the computations, though, let's take a much simpler example.",
  "translatedText": "A számítások áttekintéséhez azonban vegyünk egy sokkal egyszerűbb példát.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "translatedText": "Képzelje el, hogy a bemenet tartalmazza a következő mondatot: Egy bolyhos kék lény járta a zöldellő erdőt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "translatedText": "És egyelőre tegyük fel, hogy az egyetlen frissítés, ami minket érdekel, az az, hogy a melléknevek módosítják a hozzájuk tartozó főnevek jelentését.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "translatedText": "Amit most le fogok írni, azt egyetlen figyelemfejnek neveznénk, és később látni fogjuk, hogy a figyelemblokk sok különböző, párhuzamosan futó fejből áll.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "translatedText": "A kezdeti beágyazás minden egyes szóhoz egy nagy dimenziós vektor, amely csak az adott szó jelentését kódolja, kontextus nélkül.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "input": "Actually, that's not quite true.",
  "translatedText": "Valójában ez nem teljesen igaz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "input": "They also encode the position of the word.",
  "translatedText": "A szó pozícióját is kódolják.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "translatedText": "A pozíciók kódolásának módja még sok mindent elmond, de most csak annyit kell tudnod, hogy a vektor bejegyzései elégségesek ahhoz, hogy megmondják, mi a szó, és hol van a szövegkörnyezetben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "translatedText": "Menjünk tovább, és jelöljük ezeket a beágyazásokat e betűvel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "translatedText": "A cél az, hogy egy sor számítással a beágyazások új, finomított halmazát hozzuk létre, ahol például a főneveknek megfelelő beágyazások átvették a megfelelő melléknevek jelentését.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "translatedText": "És a mélytanulás játékát játszva azt szeretnénk, ha a legtöbb számítás mátrix-vektor termékként nézne ki, ahol a mátrixok tele vannak hangolható súlyokkal, olyan dolgokkal, amelyeket a modell az adatok alapján tanul meg.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "translatedText": "Hogy világos legyen, csak azért találtam ki ezt a példát a főneveket aktualizáló melléknevekkel, hogy szemléltessem, milyen viselkedést képzelhetsz el egy figyelemfelkeltőnek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "translatedText": "Mint oly sok más mélytanulás esetében, a valódi viselkedést sokkal nehezebb elemezni, mivel az egy hatalmas számú paraméter csípésén és hangolásán alapul, hogy minimalizáljon valamilyen költségfüggvényt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "translatedText": "Csak ahogy végigmegyünk a különböző paraméterekkel teli mátrixokon, amelyek ebben a folyamatban részt vesznek, úgy gondolom, hogy nagyon hasznos, ha van egy elképzelt példa arra, hogy mit is csinálhatna, hogy segítsen konkrétabbá tenni az egészet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "translatedText": "A folyamat első lépéseként elképzelhetjük, hogy minden egyes főnév, mint például a lény, felteszi a kérdést: hé, ülnek előttem melléknevek?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "translatedText": "És a bolyhos és a kék szavakra, hogy mindketten tudjanak válaszolni, igen, melléknév vagyok, és ebben a helyzetben vagyok.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "translatedText": "Ez a kérdés valahogy egy másik vektorba, egy másik számjegyzékbe van kódolva, amelyet a szóra vonatkozó lekérdezésnek nevezünk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "translatedText": "Ez a lekérdezési vektor azonban sokkal kisebb dimenziójú, mint a beágyazási vektor, mondjuk 128.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "translatedText": "Ennek a lekérdezésnek a kiszámítása úgy néz ki, hogy veszünk egy bizonyos mátrixot, amelyet wq-nek nevezek el, és megszorozzuk a beágyazással.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "translatedText": "Kicsit tömörítve a dolgokat, írjuk ezt a lekérdezés vektorát q-nak, és bármikor, amikor azt látod, hogy egy nyíl mellé egy mátrixot teszek, mint például ez itt, ez azt jelenti, hogy ezt a mátrixot megszorozva a nyíl elején lévő vektorral, megkapod a nyíl végén lévő vektort.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "translatedText": "Ebben az esetben ezt a mátrixot megszorozzuk a kontextusban lévő összes beágyazással, így minden egyes tokenhez egy lekérdezési vektort kapunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "translatedText": "Ennek a mátrixnak a bejegyzései a modell paraméterei, ami azt jelenti, hogy a valódi viselkedést az adatokból tanulják, és a gyakorlatban kihívást jelent, hogy mit csinál ez a mátrix egy adott figyelemfejben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "translatedText": "De a mi kedvünkért, elképzelve egy példát, amit remélhetőleg megtanul, tegyük fel, hogy ez a lekérdezési mátrix a főnevek beágyazásait bizonyos irányokba képezi le ebben a kisebb lekérdezési térben, ami valamilyen módon kódolja azt a fogalmat, hogy mellékneveket keresünk az előző pozíciókban.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "input": "As to what it does to other embeddings, who knows?",
  "translatedText": "Hogy mit tesz más beágyazásokkal, ki tudja?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "translatedText": "Lehet, hogy ezzel egyidejűleg más célt is megpróbál elérni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "input": "Right now, we're laser focused on the nouns.",
  "translatedText": "Jelenleg a főnevekre koncentrálunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "translatedText": "Ugyanakkor ehhez kapcsolódik egy második mátrix, a kulcsmátrix, amelyet szintén megszorozunk minden egyes beágyazással.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "input": "This produces a second sequence of vectors that we call the keys.",
  "translatedText": "Ez egy második vektorsorozatot eredményez, amelyet kulcsoknak nevezünk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "translatedText": "Koncepcionálisan úgy kell elképzelni, hogy a kulcsok potenciálisan válaszolnak a lekérdezésekre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "translatedText": "Ez a kulcsmátrix szintén tele van hangolható paraméterekkel, és a lekérdezési mátrixhoz hasonlóan a beágyazási vektorokat is ugyanabba a kisebb dimenziós térbe képezi le.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "translatedText": "A kulcsokra úgy tekinthetünk, mint a lekérdezésekhez illeszkedő kulcsokra, amikor azok szorosan illeszkednek egymáshoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "translatedText": "Példánkban úgy képzelhetjük el, hogy a kulcsmátrix az olyan mellékneveket, mint a bolyhos és a kék, olyan vektorokhoz képezi le, amelyek szorosan illeszkednek az élőlény szó által létrehozott lekérdezéshez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "translatedText": "Annak méréséhez, hogy az egyes kulcsok mennyire felelnek meg az egyes lekérdezéseknek, minden lehetséges kulcs-lekérdezés pár között kiszámítja a pontszorzatot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "translatedText": "Szeretek egy csomó pontból álló rácsot elképzelni, ahol a nagyobb pontok megfelelnek a nagyobb ponttermékeknek, azoknak a helyeknek, ahol a kulcsok és a lekérdezések egymáshoz igazodnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "translatedText": "A mi melléknévi főnévi példánk esetében ez egy kicsit így nézne ki, ahol ha a bolyhos és a kék által előállított kulcsok valóban szorosan illeszkednek a teremtmény által előállított lekérdezéshez, akkor a pontproduktumok ezen a két helyen nagy pozitív számok lennének.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "translatedText": "A szakzsargonban a gépi tanulással foglalkozó emberek azt mondanák, hogy ez azt jelenti, hogy a bolyhos és a kék beágyazásai a lény beágyazására figyelnek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "translatedText": "Ezzel szemben a pontproduktum egy másik szó, mint például a és a lény lekérdezése között egy kis vagy negatív érték lenne, ami azt tükrözi, hogy nem kapcsolódnak egymáshoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "translatedText": "Tehát van egy értékekből álló rács, amely a negatív végtelentől a végtelenig bármilyen valós szám lehet, ami egy pontszámot ad arra vonatkozóan, hogy az egyes szavak mennyire relevánsak az összes többi szó jelentésének frissítése szempontjából.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "translatedText": "A mód, ahogyan ezeket a pontszámokat használni fogjuk, az, hogy minden oszlop mentén egy bizonyos súlyozott összeget veszünk, a relevanciával súlyozva.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "translatedText": "Tehát ahelyett, hogy az értékek a negatív végtelentől a végtelenig terjednének, azt szeretnénk, hogy az oszlopokban lévő számok 0 és 1 között legyenek, és minden oszlop összege 1 legyen, mintha valószínűségi eloszlás lenne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "translatedText": "Ha az előző fejezetből jöttök, akkor tudjátok, hogy akkor mit kell tennünk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "translatedText": "Az értékek normalizálásához minden egyes oszlop mentén kiszámítunk egy softmaxot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "translatedText": "A mi képünkön, miután a softmaxot az összes oszlopra alkalmazzuk, a rácsot ezekkel a normalizált értékekkel töltjük ki.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "translatedText": "Ezen a ponton már nyugodtan gondolhatsz arra, hogy az egyes oszlopok súlyokat adnak aszerint, hogy a bal oldali szó mennyire releváns a fent lévő megfelelő értékhez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "input": "We call this grid an attention pattern.",
  "translatedText": "Ezt a rácsot figyelemmintának nevezzük.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "translatedText": "Ha most megnézzük az eredeti transzformátoros papírt, akkor nagyon tömör módon írják le mindezt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "translatedText": "Itt a q és k változók a lekérdezési és kulcsvektorok teljes tömbjeit jelentik, azokat a kis vektorokat, amelyeket a beágyazásoknak a lekérdezési és a kulcsmátrixokkal való szorzásával kapunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "translatedText": "Ez a kifejezés a számlálóban egy igazán kompakt módja a kulcs- és lekérdezéspárok közötti összes lehetséges pontproduktum rácsának ábrázolására.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "translatedText": "Egy apró technikai részlet, amit nem említettem, hogy a numerikus stabilitás érdekében hasznos, ha az összes ilyen értéket elosztjuk a dimenzió négyzetgyökével az adott kulcslekérdezési térben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "translatedText": "Ezután ez a softmax, amely a teljes kifejezés köré van tekerve, úgy értendő, hogy oszloponként alkalmazandó.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "input": "As to that v term, we'll talk about it in just a second.",
  "translatedText": "Ami a v kifejezést illeti, erről mindjárt beszélünk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "translatedText": "Előtte van még egy technikai részlet, amit eddig kihagytam.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "translatedText": "A képzési folyamat során, amikor ezt a modellt egy adott szövegpéldán futtatjuk, és az összes súlyt kissé módosítjuk és hangoljuk, hogy vagy jutalmazzuk vagy büntessük aszerint, hogy mekkora valószínűséget rendel a szövegben a valódi következő szóhoz, kiderül, hogy sokkal hatékonyabbá teszi az egész képzési folyamatot, ha egyszerre minden lehetséges következő jelet megjósolunk a szövegben lévő jelek minden egyes kezdeti részsorozatát követően.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "translatedText": "Például az általunk vizsgált kifejezéssel kapcsolatban azt is megjósolhatjuk, hogy milyen szavak követik a teremtményt és milyen szavak követik a.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "translatedText": "Ez nagyon szép, mert ez azt jelenti, hogy ami egyébként egyetlen képzési példa lenne, az ténylegesen több példaként működik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "translatedText": "A figyelem mintánk szempontjából ez azt jelenti, hogy soha ne engedjük, hogy a későbbi szavak befolyásolják a korábbi szavakat, mert különben elárulhatják a választ arra, hogy mi következik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "translatedText": "Ez azt jelenti, hogy azt akarjuk, hogy ezek a pontok, amelyek a korábbiakat befolyásoló későbbi tokeneket képviselik, valahogy nullára legyenek kényszerítve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "translatedText": "A legegyszerűbb, ha nullára állítjuk őket, de ha ezt tennénk, akkor az oszlopok már nem adódnának össze eggyé, nem lennének normalizálva.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "translatedText": "Ehelyett a softmax alkalmazása előtt az egyik általános megoldás az, hogy az összes ilyen bejegyzést negatív végtelennek állítjuk be.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "translatedText": "Ha ezt megteszi, akkor a softmax alkalmazása után ezek mind nullává válnak, de az oszlopok normalizáltak maradnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "input": "This process is called masking.",
  "translatedText": "Ezt a folyamatot maszkolásnak nevezik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "translatedText": "A figyelemnek vannak olyan változatai, ahol ezt nem alkalmazzuk, de a mi GPT példánkban, még ha ez a képzési fázisban relevánsabb is, mint mondjuk chatbotként vagy valami ilyesmiként futtatva, mindig alkalmazzuk ezt a maszkolást, hogy megakadályozzuk, hogy a későbbi tokenek befolyásolják a korábbiakat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "translatedText": "Egy másik tény, amin érdemes elgondolkodni ezzel a figyelemmintával kapcsolatban, hogy a mérete megegyezik a kontextus méretének négyzetével.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "translatedText": "Ezért lehet a kontextus mérete igazán nagy szűk keresztmetszet a nagy nyelvi modellek számára, és a skálázás nem triviális.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "translatedText": "Ahogy azt elképzelheted, az egyre nagyobb és nagyobb kontextusablakok iránti vágytól vezérelve az elmúlt években a figyelem mechanizmusának néhány változata született, amelyek célja a kontextus skálázhatóbbá tétele volt, de itt most te és én az alapokra koncentrálunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "translatedText": "Oké, nagyszerű, ennek a mintának a kiszámítása lehetővé teszi a modell számára, hogy levezesse, mely szavak milyen más szavakhoz kapcsolódnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "translatedText": "Most már ténylegesen frissíteni kell a beágyazásokat, lehetővé téve a szavak számára, hogy információt adjanak át a számukra releváns szavaknak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "translatedText": "Például azt akarod, hogy a Pelyhes beágyazása valahogyan olyan változást idézzen elő a Lényben, amely a 12 000 dimenziós beágyazási tér egy másik részébe helyezi át, amely pontosabban kódol egy Pelyhes lényt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "translatedText": "Amit itt tenni fogok, az az, hogy először megmutatom a legegyszerűbb módot, ahogyan ezt megteheted, bár van egy kis mód, ahogyan ez módosul a többfejű figyelem kontextusában.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "translatedText": "Ez a legegyszerűbb módja az lenne, ha használnánk egy harmadik mátrixot, amit mi értékmátrixnak nevezünk, és amit megszorozunk az első szó, például a Pelyhes beágyazásával.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "translatedText": "Ennek eredménye az, amit értékvektornak neveznénk, és ez valami, amit hozzáadunk a második szó beágyazásához, ebben az esetben valami, amit hozzáadunk a Creature beágyazásához.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "translatedText": "Ez az értékvektor tehát ugyanabban a nagyon nagy dimenziójú térben él, mint a beágyazások.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "translatedText": "Ha ezt az értékmátrixot megszorozzuk egy szó beágyazásával, akkor úgy gondolhatjuk, hogy ha ez a szó releváns valami más jelentésének beállítása szempontjából, akkor pontosan mit kell hozzáadni a valami más beágyazásához, hogy ezt tükrözze?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "translatedText": "Ha visszatekintünk a diagramunkra, tegyük félre az összes kulcsot és a lekérdezéseket, mivel miután kiszámítottuk a figyelemmintát, azokkal végeztünk, majd fogjuk ezt az értékmátrixot, és megszorozzuk minden egyes beágyazással, hogy létrehozzuk az értékvektorok sorozatát.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "translatedText": "Úgy gondolhatsz ezekre az értékvektorokra, mint amelyek a megfelelő kulcsokhoz kapcsolódnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "translatedText": "A diagram minden egyes oszlopához megszorozza az egyes értékvektorokat az adott oszlop megfelelő súlyával.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "translatedText": "Például itt, a Creature beágyazása alatt a Pelyhes és a Kék értékvektorok nagy részét hozzáadnád, míg az összes többi értékvektort lenulláznád, vagy legalábbis majdnem lenulláznád.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "translatedText": "És végül, az ehhez az oszlophoz kapcsolódó beágyazás frissítésének módja, amely korábban a Creature valamilyen kontextusmentes jelentését kódolta, összeadjuk az oszlopban lévő összes átméretezett értéket, létrehozva egy hozzáadni kívánt változást, amelyet delta-e-nek fogok nevezni, és ezt hozzáadjuk az eredeti beágyazáshoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "translatedText": "Remélhetőleg az eredmény egy kifinomultabb vektor lesz, amely a kontextuálisan gazdagabb jelentést kódolja, például egy bolyhos kék lényt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "translatedText": "És persze ezt nem csak egy beágyazással csináljuk, hanem ugyanazt a súlyozott összeget alkalmazzuk a kép összes oszlopán, ami a változások sorozatát eredményezi, és ha ezeket a változásokat hozzáadjuk a megfelelő beágyazásokhoz, akkor a figyelemblokkból előbukkanó finomabb beágyazások teljes sorozatát kapjuk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "translatedText": "Ha kicsinyítjük, az egész folyamatot egyetlen figyelemfelkeltő fejnek neveznénk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "translatedText": "Ahogyan eddig leírtam a dolgokat, ezt a folyamatot három különböző mátrix paraméterezi, amelyek mindegyike hangolható paraméterekkel, a kulccsal, a lekérdezéssel és az értékkel van feltöltve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "translatedText": "Szeretném egy pillanatra folytatni azt, amit az előző fejezetben kezdtünk, a pontozással, ahol a GPT-3 számok segítségével számoljuk össze a modellparaméterek teljes számát.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "translatedText": "Ezek a kulcs- és lekérdezési mátrixok mindegyike 12 288 oszlopból áll, ami megfelel a beágyazási dimenziónak, és 128 sorból, ami megfelel a kisebb kulcs-lekérdezési tér dimenziójának.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "translatedText": "Ez további körülbelül 1,5 millió paramétert ad mindegyikhez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "translatedText": "Ha ezzel szemben megnézzük ezt az értékmátrixot, akkor az eddigi leírásom alapján úgy tűnik, hogy ez egy négyzetmátrix, amelynek 12 288 oszlopa és 12 288 sora van, mivel mind a bemenetei, mind a kimenetei ebben a nagyon nagy beágyazási térben élnek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "input": "If true, that would mean about 150 million added parameters.",
  "translatedText": "Ha ez igaz, akkor ez körülbelül 150 millió új paramétert jelentene.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "input": "And to be clear, you could do that.",
  "translatedText": "És hogy tisztázzuk, ezt megteheted.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "translatedText": "Az értéktérképnek nagyságrendekkel több paramétert tudna szentelni, mint a kulcsnak és a lekérdezésnek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "translatedText": "A gyakorlatban azonban sokkal hatékonyabb, ha ehelyett úgy alakítjuk ki, hogy az értéktérképhez tartozó paraméterek száma megegyezik a kulcshoz és a lekérdezéshez tartozó paraméterek számával.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "translatedText": "Ez különösen fontos abban a helyzetben, amikor több figyelemfej párhuzamosan fut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "translatedText": "Ez úgy néz ki, hogy az értéktérképet két kisebb mátrix szorzataként faktoráljuk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "translatedText": "Koncepcionálisan még mindig arra bátorítanám, hogy gondolkodjon az általános lineáris térképen, egy olyan térképen, amelynek bemenetei és kimenetei mind ebben a nagyobb beágyazási térben vannak, például a kék beágyazását a kékesség irányába véve, amelyet a főnevekhez adna hozzá.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "translatedText": "Csupán arról van szó, hogy kisebb számú sorról van szó, jellemzően ugyanolyan méretű, mint a kulcslekérdezés helye.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "translatedText": "Ez azt jelenti, hogy a nagy beágyazási vektorokat egy sokkal kisebb térbe képezzük le.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "translatedText": "Ez nem a hagyományos elnevezés, de én ezt érték lefelé mátrixnak fogom nevezni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "translatedText": "A második mátrix ebből a kisebb térből visszatérképez a beágyazási térbe, és létrehozza azokat a vektorokat, amelyeket a tényleges frissítésekhez használ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "translatedText": "Ezt a mátrixot értéknövelő mátrixnak fogom nevezni, ami megint csak nem szokványos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "input": "The way that you would see this written in most papers looks a little different.",
  "translatedText": "A legtöbb újságban ezt egy kicsit másképp írják le.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "input": "I'll talk about it in a minute.",
  "translatedText": "Egy perc múlva beszélek róla.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "translatedText": "Véleményem szerint ez egy kicsit zavarosabbá teszi a dolgokat fogalmilag.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "translatedText": "A lineáris algebrai szakzsargonnal élve, alapvetően azt tesszük, hogy a teljes értéktérképet úgy korlátozzuk, hogy az egy alacsony rangú transzformáció legyen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "translatedText": "Visszatérve a paraméterek számához, mind a négy mátrix mérete megegyezik, és ha mindet összeadjuk, akkor egy figyelmi fejre körülbelül 6,3 millió paramétert kapunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "translatedText": "Egy gyors mellékes megjegyzésként, hogy egy kicsit pontosabbak legyünk, minden, amit eddig leírtunk, az az, amit az emberek önfigyelemnek neveznének, hogy megkülönböztessük egy olyan változattól, ami más modellekben fordul elő, és amit keresztfigyelemnek hívnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "translatedText": "Ez nem releváns a GPT példánk szempontjából, de ha kíváncsi vagy rá, a kereszt-figyelem olyan modelleket foglal magában, amelyek két különböző típusú adatot dolgoznak fel, például egy nyelvi szöveget és egy másik nyelvi szöveget, amely egy fordítás folyamatban lévő generálásának része, vagy esetleg beszédhangot és egy folyamatban lévő átiratot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "input": "A cross-attention head looks almost identical.",
  "translatedText": "A keresztirányú figyelemfelkeltő fej majdnem ugyanúgy néz ki.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "input": "The only difference is that the key and query maps act on different data sets.",
  "translatedText": "Az egyetlen különbség az, hogy a kulcs- és a lekérdezési térképek különböző adathalmazokra hatnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "translatedText": "Egy fordítást végző modellben például a kulcsok az egyik nyelvből, míg a lekérdezések egy másik nyelvből származhatnak, és a figyelmi minta leírhatja, hogy az egyik nyelv melyik szavának melyik szó felel meg a másik nyelvben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "translatedText": "És ebben a környezetben jellemzően nem lenne maszkolás, mivel nem igazán van olyan elképzelés, hogy a későbbi tokenek befolyásolnák a korábbiakat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "translatedText": "Ha azonban az önfigyelésre összpontosítanál, és ha eddig mindent megértettél, és ha itt megállnál, akkor a lényegét kapnád annak, hogy mi is a figyelem valójában.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "translatedText": "Már csak az van hátra, hogy sok-sok különböző alkalommal lefektessük, hogy milyen értelemben csináljuk ezt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "translatedText": "Központi példánkban a főneveket frissítő melléknevekre összpontosítottunk, de természetesen a szövegkörnyezet sokféleképpen befolyásolhatja egy szó jelentését.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "translatedText": "Ha az összetört szavak megelőzték az autó szót, az hatással van az autó alakjára és szerkezetére.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "input": "And a lot of associations might be less grammatical.",
  "translatedText": "És sok asszociáció talán kevésbé nyelvtani.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "translatedText": "Ha a varázsló szó valahol ugyanabban a szövegben szerepel, mint Harry, akkor ez azt sugallja, hogy ez Harry Potterre utalhat, míg ha ehelyett a királynő, Sussex és Vilmos szavak szerepelnek a szövegben, akkor talán a Harry beágyazását inkább a hercegre kellene frissíteni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "translatedText": "A kontextuális frissítés minden egyes elképzelhető különböző típusához más-más kulcs- és lekérdezési mátrix paraméterei lennének a különböző figyelemminták megragadásához, és az értéktérképünk paraméterei is különbözőek lennének aszerint, hogy mit kell hozzáadni a beágyazásokhoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "translatedText": "És ismétlem, a gyakorlatban ezeknek a térképeknek a valódi viselkedését sokkal nehezebb értelmezni, ahol a súlyok úgy vannak beállítva, hogy azt tegyék, amire a modellnek szüksége van, hogy a legjobban elérje a következő token előrejelzésének célját.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "translatedText": "Mint már említettem, minden, amit leírtunk, egy-egy figyelemfej, és egy teljes figyelemblokk egy transzformátoron belül egy úgynevezett többfejű figyelemből áll, ahol sok ilyen műveletet futtatunk párhuzamosan, mindegyiknek saját kulcslekérdezésével és értéktérképével.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "translatedText": "A GPT-3 például 96 figyelemfelhívó fejet használ minden egyes blokkban.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "translatedText": "Figyelembe véve, hogy mindegyik már egy kicsit zavaros, ez bizonyára sok, amit a fejedben kell tartani.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "translatedText": "Csak hogy mindent nagyon világosan kifejtsünk, ez azt jelenti, hogy 96 különböző kulcs- és lekérdezési mátrixunk van, amelyek 96 különböző figyelemmintát eredményeznek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "translatedText": "Ezután minden fejnek megvan a maga külön értékmátrixa, amelyből 96 értékvektor-sorozatot állítanak elő.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "translatedText": "Ezeket mind összeadjuk a megfelelő figyelemminták súlyként való felhasználásával.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "translatedText": "Ez azt jelenti, hogy a kontextus minden egyes pozíciójára, minden egyes tokenre, minden egyes ilyen fej egy javasolt változtatást hoz létre, amelyet hozzá kell adni az adott pozícióban lévő beágyazáshoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "translatedText": "Tehát azt kell tennie, hogy összeadja az összes javasolt módosítást, minden egyes fejre egyet, és az eredményt hozzáadja az adott pozíció eredeti beágyazásához.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "translatedText": "Ez a teljes összeg itt egy szelete lenne annak, amit ez a többfejű figyelemblokk kiad, egyetlen ilyen finomított beágyazás, ami a másik végén kiugrik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "translatedText": "Ismétlem, ez nagyon sok gondolat, ezért ne aggódjon, ha egy kis időbe telik, amíg belemerül.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "translatedText": "Az általános elképzelés az, hogy sok különböző fej párhuzamos futtatásával a modell képes megtanulni, hogy a kontextus milyen különböző módon változtatja meg a jelentést.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "translatedText": "Ha a paraméterek számának számbavételét 96 fejjel végezzük, amelyek mindegyike tartalmazza e négy mátrix saját variációját, akkor a többfejű figyelem minden egyes blokkja körülbelül 600 millió paramétert tartalmaz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "translatedText": "Van egy további, kissé bosszantó dolog, amit tényleg meg kell említenem mindazoknak, akik tovább olvasnak a Transformersről.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "translatedText": "Emlékeztek, hogy azt mondtam, hogy az értéktérképet két különböző mátrixra osztjuk, amelyeket érték lefelé és érték felfelé mátrixnak neveztem el.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "translatedText": "Ahogyan én a dolgokat kialakítottam, az azt sugallja, hogy ezt a mátrixpárt minden egyes figyelemfejben látod, és ezt teljesen így is megvalósíthatod.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "input": "That would be a valid design.",
  "translatedText": "Ez egy érvényes konstrukció lenne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "translatedText": "De a papírokban leírtak és a gyakorlati megvalósítás módja kissé eltér egymástól.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "translatedText": "Az egyes fejekhez tartozó összes ilyen értékmátrix egy hatalmas mátrixban jelenik meg, amelyet kimeneti mátrixnak nevezünk, és amely a teljes többfejű figyelemblokkhoz kapcsolódik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "translatedText": "És amikor az emberek egy adott figyelemfej értékmátrixára hivatkoznak, akkor általában csak erre az első lépésre utalnak, arra, amit én úgy neveztem, hogy az érték lefelé vetítése a kisebb térbe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "input": "For the curious among you, I've left an on-screen note about it.",
  "translatedText": "A kíváncsiskodóknak hagytam egy megjegyzést a képernyőn.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "translatedText": "Ez egyike azoknak a részleteknek, amelyeknél fennáll a veszélye annak, hogy elvonják a figyelmet a fő koncepcionális pontokról, de azért szeretném felhívni rá a figyelmet, hogy ha más forrásokban olvasol erről, tudd.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "translatedText": "Félretéve minden technikai árnyalatot, az előző fejezet előzeteseiben láttuk, hogy a transzformátoron keresztül áramló adatok nem csak egy figyelemblokkon keresztül áramlanak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "translatedText": "Először is, átmegy ezeken a többrétegű perceptronoknak nevezett egyéb műveleteken is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "input": "We'll talk more about those in the next chapter.",
  "translatedText": "Ezekről a következő fejezetben többet fogunk beszélni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "translatedText": "Ezután mindkét művelet sok-sok másolatát ismételgeti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "translatedText": "Ez azt jelenti, hogy miután egy adott szó magába szívja a kontextus egy részét, sokkal több esély van arra, hogy ezt az árnyaltabb beágyazódást árnyaltabb környezete befolyásolja.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "translatedText": "Minél lejjebb haladunk a hálózatban, és minden egyes beágyazás egyre több jelentést vesz át az összes többi beágyazásból, amelyek maguk is egyre árnyaltabbá válnak, a remény az, hogy képesek leszünk magasabb szintű és absztraktabb gondolatokat kódolni egy adott bemenetről a leírásokon és a nyelvtani szerkezeten túl.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "translatedText": "Olyan dolgok, mint az érzelmek és a hangnem, és hogy versről van-e szó, és hogy milyen tudományos igazságok állnak a mű mögött, és ehhez hasonló dolgok.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "translatedText": "Még egyszer visszatérve a pontozáshoz, a GPT-3 96 különböző réteget tartalmaz, így a kulcskérdés- és értékparaméterek teljes számát megszorozzuk további 96-tal, ami a teljes összeget valamivel kevesebb, mint 58 milliárd különböző paraméterre teszi, amelyek az összes figyelemfejre vonatkoznak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "translatedText": "Ez bizonyára sok, de ez csak egyharmada a hálózatban lévő összesen 175 milliárdnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "translatedText": "Tehát bár a figyelem kapja az összes figyelmet, a paraméterek többsége az e lépések között elhelyezkedő blokkokból származik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "translatedText": "A következő fejezetben többet fogunk beszélni ezekről a többi blokkról, és sokkal többet fogunk beszélni a képzési folyamatról is.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "translatedText": "A figyelemmechanizmus sikerének nagy része nem annyira az általa lehetővé tett konkrét viselkedés, hanem az a tény, hogy rendkívül jól párhuzamosítható, ami azt jelenti, hogy a GPU-k segítségével rövid idő alatt rengeteg számítást lehet elvégezni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "translatedText": "Mivel az elmúlt egy-két évtized egyik nagy tanulsága a mélytanulással kapcsolatban az volt, hogy a skála önmagában hatalmas minőségi javulást eredményez a modellek teljesítményében, a párhuzamosítható architektúráknak, amelyek ezt lehetővé teszik, óriási előnye van.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "translatedText": "Ha többet szeretnél megtudni ezekről a dolgokról, sok linket hagytam a leírásban.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "translatedText": "Különösen az Andrej Karpathy vagy Chris Ola által készített lemezek aranyat érnek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "translatedText": "Ebben a videóban csak a jelenlegi formában akartam belevágni a figyelembe, de ha kíváncsiak vagytok arra, hogyan jutottunk idáig, és hogyan találhatjátok fel újra ezt az ötletet magatoknak, Vivek barátom most tett fel néhány videót, amelyekben sokkal több motivációt adnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "translatedText": "Britt Cruz a The Art of the Problem csatornáról készített egy nagyon szép videót a nagy nyelvi modellek történetéről.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "input": "Thank you.",
  "translatedText": "Köszönöm.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]