[
 {
  "input": "Demystifying self-attention, multiple heads, and cross-attention.",
  "translatedText": "Демистификация самовнимания, множественных голов и перекрестного внимания.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "translatedText": "Вместо спонсорского чтения рекламы эти уроки финансируются непосредственно зрителями: https://3b1b.co/support.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "An equally valuable form of support is to simply share the videos.",
  "translatedText": "Не менее ценная форма поддержки - просто поделиться видео.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Other resources about transformers",
  "translatedText": "Другие ресурсы о трансформаторах",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Andrej Karpathy's videos",
  "translatedText": "Видеоролики Андрея Карпати",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The Transformer Circuits posts by Anthropic",
  "translatedText": "Посты по теме \"Трансформаторные схемы\", автор Anthropic",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://transformer-circuits.pub/2021/framework/index.html",
  "translatedText": "https://transformer-circuits.pub/2021/framework/index.html",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "In particular, it was only after I read this post that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources.",
  "translatedText": "В частности, только прочитав этот пост, я начал думать о комбинации матриц значения и выхода как о комбинированной низкоранговой карте из пространства вложения в себя, что, по крайней мере, в моем сознании, сделало все намного яснее, чем другие источники.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "History of language models by Brit Cruise, @ArtOfTheProblem ",
  "translatedText": "История языковых моделей от Брит Круз, @ArtOfTheProblem",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://youtu.be/OFS90-FX6pg",
  "translatedText": "https://youtu.be/OFS90-FX6pg",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "What is a Language Model by @vcubingx ",
  "translatedText": "Что такое языковая модель от @vcubingx",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "translatedText": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Site with exercises related to ML programming and GPTs",
  "translatedText": "Сайт с упражнениями, связанными с ML-программированием и GPT",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://www.gptandchill.ai/codingproblems",
  "translatedText": "https://www.gptandchill.ai/codingproblems",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Early paper on how directions in embedding spaces have meaning:",
  "translatedText": "Ранняя статья о том, как направления в пространствах встраивания имеют смысл:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://arxiv.org/pdf/1301.3781.pdf",
  "translatedText": "https://arxiv.org/pdf/1301.3781.pdf",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Timestamps:",
  "translatedText": "Временные метки:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "0:00 - Recap on embeddings",
  "translatedText": "0:00 - Обзор вкраплений",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "1:39 - Motivating examples",
  "translatedText": "1:39 - Мотивирующие примеры",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "4:29 - The attention pattern",
  "translatedText": "4:29 - Шаблон внимания",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "11:08 - Masking",
  "translatedText": "11:08 - Маскировка",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "12:42 - Context size",
  "translatedText": "12:42 - Размер контекста",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "13:10 - Values",
  "translatedText": "13:10 - Ценности",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "15:44 - Counting parameters",
  "translatedText": "15:44 - Параметры подсчета",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "18:21 - Cross-attention",
  "translatedText": "18:21 - Перекрестное внимание",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "19:19 - Multiple heads",
  "translatedText": "19:19 - Множественные головы",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "22:16 - The output matrix",
  "translatedText": "22:16 - Матрица вывода",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "23:19 - Going deeper",
  "translatedText": "23:19 - Идти глубже",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "24:54 - Ending",
  "translatedText": "24:54 - Окончание",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 }
]