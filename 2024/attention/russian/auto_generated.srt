1
00:00:00,000 --> 00:00:04,019
В прошлой главе мы с тобой начали пошагово разбирать внутреннее устройство трансформатора.

2
00:00:04,560 --> 00:00:07,425
Это одна из ключевых частей технологии внутри больших языковых 

3
00:00:07,425 --> 00:00:10,200
моделей и множества других инструментов современной волны ИИ.

4
00:00:10,980 --> 00:00:14,694
Впервые о нем заговорили в нашумевшей статье 2017 года под названием Attention 

5
00:00:14,694 --> 00:00:17,562
is All You Need, и в этой главе мы с тобой покопаемся в том, 

6
00:00:17,562 --> 00:00:21,700
что представляет собой этот механизм внимания, визуализируя, как он обрабатывает данные.

7
00:00:26,140 --> 00:00:29,540
В качестве краткого резюме вот важный контекст, который я хочу, чтобы ты имел в виду.

8
00:00:30,000 --> 00:00:34,444
Цель модели, которую мы с тобой изучаем, - взять кусок текста и предсказать, 

9
00:00:34,444 --> 00:00:36,060
какое слово будет следующим.

10
00:00:36,860 --> 00:00:41,043
Вводимый текст разбивается на маленькие кусочки, которые мы называем лексемами, 

11
00:00:41,043 --> 00:00:45,487
и очень часто это слова или части слов, но чтобы нам с тобой было легче воспринимать 

12
00:00:45,487 --> 00:00:48,572
примеры в этом видео, давай упростим ситуацию, представив, 

13
00:00:48,572 --> 00:00:50,560
что лексемы - это всегда только слова.

14
00:00:51,480 --> 00:00:55,752
Первый шаг в трансформаторе - связать каждый токен с высокоразмерным вектором, 

15
00:00:55,752 --> 00:00:57,700
который мы называем его вкраплением.

16
00:00:57,700 --> 00:01:00,624
Самая важная идея, которую я хочу, чтобы ты держал в голове, 

17
00:01:00,624 --> 00:01:03,596
- это то, как направления в этом высокоразмерном пространстве 

18
00:01:03,596 --> 00:01:07,000
всех возможных вкраплений могут соотноситься с семантическим значением.

19
00:01:07,680 --> 00:01:11,250
В прошлой главе мы видели пример того, как направление может соответствовать полу, 

20
00:01:11,250 --> 00:01:14,176
в том смысле, что добавление определенного шага в этом пространстве 

21
00:01:14,176 --> 00:01:17,101
может перенести тебя от вкрапления существительного мужского рода к 

22
00:01:17,101 --> 00:01:19,640
вкраплению соответствующего существительного женского рода.

23
00:01:20,160 --> 00:01:23,587
Это лишь один пример, ты можешь представить, сколько других направлений в этом 

24
00:01:23,587 --> 00:01:27,319
высокоразмерном пространстве могут соответствовать множеству других аспектов значения 

25
00:01:27,319 --> 00:01:27,580
слова.

26
00:01:28,800 --> 00:01:32,604
Задача трансформатора - постепенно корректировать эти вкрапления так, 

27
00:01:32,604 --> 00:01:36,788
чтобы они не просто кодировали отдельное слово, а встраивали в него гораздо, 

28
00:01:36,788 --> 00:01:39,180
гораздо более богатый контекстуальный смысл.

29
00:01:40,140 --> 00:01:42,384
Сразу скажу, что многим людям механизм внимания, 

30
00:01:42,384 --> 00:01:45,315
эта ключевая деталь в трансформаторе, кажется очень запутанным, 

31
00:01:45,315 --> 00:01:48,980
так что не переживай, если потребуется некоторое время, чтобы все стало понятно.

32
00:01:49,440 --> 00:01:54,185
Думаю, прежде чем мы погрузимся в вычислительные детали и все матричные умножения, 

33
00:01:54,185 --> 00:01:59,160
стоит подумать о паре примеров такого поведения, которое мы хотим обеспечить вниманием.

34
00:02:00,140 --> 00:02:03,058
Рассмотри фразы Американская настоящая родинка, 

35
00:02:03,058 --> 00:02:06,220
один моль углекислого газа и возьми биопсию родинки.

36
00:02:06,700 --> 00:02:08,841
Мы с тобой знаем, что слово "крот" в каждом из них 

37
00:02:08,841 --> 00:02:10,900
имеет разное значение в зависимости от контекста.

38
00:02:11,360 --> 00:02:16,294
Но после первого шага трансформатора, который разбивает текст и связывает каждую лексему 

39
00:02:16,294 --> 00:02:20,675
с вектором, вектор, связанный с кротом, будет одинаковым во всех этих случаях, 

40
00:02:20,675 --> 00:02:25,554
потому что это первоначальное встраивание лексем - фактически таблица поиска без ссылок 

41
00:02:25,554 --> 00:02:26,220
на контекст.

42
00:02:26,620 --> 00:02:29,544
Только на следующем этапе трансформации окружающие 

43
00:02:29,544 --> 00:02:33,100
вкрапления получают шанс передать информацию в это вкрапление.

44
00:02:33,820 --> 00:02:38,437
Ты можешь представить себе, что в этом пространстве вкраплений есть несколько разных 

45
00:02:38,437 --> 00:02:41,913
направлений, кодирующих несколько разных значений слова "крот", 

46
00:02:41,913 --> 00:02:44,847
и что хорошо натренированный блок внимания вычисляет, 

47
00:02:44,847 --> 00:02:49,029
что нужно добавить к общему вкраплению, чтобы переместить его в одно из этих 

48
00:02:49,029 --> 00:02:51,800
конкретных направлений, в зависимости от контекста.

49
00:02:53,300 --> 00:02:56,180
В качестве другого примера рассмотрим встраивание слова tower.

50
00:02:57,060 --> 00:03:00,941
Предположительно, это какое-то очень общее, неспецифическое направление в пространстве, 

51
00:03:00,941 --> 00:03:03,720
связанное с множеством других больших, высоких существительных.

52
00:03:04,020 --> 00:03:08,076
Если бы этому слову сразу предшествовало слово Eiffel, можно было бы представить, 

53
00:03:08,076 --> 00:03:11,935
что механизм хочет обновить этот вектор так, чтобы он указывал в направлении, 

54
00:03:11,935 --> 00:03:16,042
которое более конкретно кодирует Эйфелеву башню, возможно, коррелируя с векторами, 

55
00:03:16,042 --> 00:03:19,060
связанными с Парижем, Францией и вещами, сделанными из стали.

56
00:03:19,920 --> 00:03:24,498
Если ему также предшествовало слово miniature, то вектор должен быть обновлен еще больше, 

57
00:03:24,498 --> 00:03:27,500
чтобы он больше не соотносился с большими, высокими вещами.

58
00:03:29,480 --> 00:03:32,455
В более общем смысле, чем просто уточнение значения слова, 

59
00:03:32,455 --> 00:03:35,179
блок внимания позволяет модели перемещать информацию, 

60
00:03:35,179 --> 00:03:39,466
закодированную в одном вложении, в другое, потенциально находящееся довольно далеко, 

61
00:03:39,466 --> 00:03:43,300
и потенциально с информацией, которая намного богаче, чем просто одно слово.

62
00:03:43,300 --> 00:03:47,788
В прошлой главе мы видели, как после того, как все векторы проходят через сеть, 

63
00:03:47,788 --> 00:03:50,986
включая множество различных блоков внимания, вычисления, 

64
00:03:50,986 --> 00:03:54,857
которые ты выполняешь для получения предсказания следующего маркера, 

65
00:03:54,857 --> 00:03:58,280
полностью зависят от последнего вектора в последовательности.

66
00:03:59,100 --> 00:04:03,527
Представь, например, что вводимый тобой текст - это большая часть всего таинственного 

67
00:04:03,527 --> 00:04:07,800
романа, вплоть до точки ближе к концу, которая гласит: "Следовательно, убийца был".

68
00:04:08,400 --> 00:04:11,266
Если модель собирается точно предсказать следующее слово, 

69
00:04:11,266 --> 00:04:15,121
то последний вектор в последовательности, который начинал свою жизнь просто с 

70
00:04:15,121 --> 00:04:18,482
встраивания слова was, должен быть обновлен всеми блоками внимания, 

71
00:04:18,482 --> 00:04:21,992
чтобы представлять гораздо, гораздо больше, чем любое отдельное слово, 

72
00:04:21,992 --> 00:04:25,452
каким-то образом кодируя всю информацию из полного контекстного окна, 

73
00:04:25,452 --> 00:04:28,220
которая имеет отношение к предсказанию следующего слова.

74
00:04:29,500 --> 00:04:32,580
Однако чтобы разобраться с вычислениями, давай рассмотрим более простой пример.

75
00:04:32,980 --> 00:04:35,691
Представь, что в исходном тексте есть фраза: "Пушистое 

76
00:04:35,691 --> 00:04:37,960
голубое существо бродило по зеленеющему лесу".

77
00:04:38,460 --> 00:04:42,828
А пока предположим, что единственный тип обновления, который нас волнует, - это то, 

78
00:04:42,828 --> 00:04:46,780
что прилагательные корректируют значения соответствующих им существительных.

79
00:04:47,000 --> 00:04:51,315
То, что я сейчас опишу, мы бы назвали одной головкой внимания, а позже мы увидим, 

80
00:04:51,315 --> 00:04:55,420
как блок внимания состоит из множества разных головок, работающих параллельно.

81
00:04:56,140 --> 00:05:00,126
Опять же, начальное вложение для каждого слова - это некоторый высокоразмерный вектор, 

82
00:05:00,126 --> 00:05:03,380
который кодирует только значение этого конкретного слова без контекста.

83
00:05:04,000 --> 00:05:05,220
На самом деле, это не совсем так.

84
00:05:05,380 --> 00:05:07,640
Они также кодируют позицию слова.

85
00:05:07,980 --> 00:05:11,256
Можно еще много чего сказать о том, как кодируются позиции, 

86
00:05:11,256 --> 00:05:15,132
но сейчас тебе достаточно знать, что записей этого вектора достаточно, 

87
00:05:15,132 --> 00:05:18,900
чтобы сказать тебе, что это за слово и где оно находится в контексте.

88
00:05:19,500 --> 00:05:21,660
Давай обозначим эти вкрапления буквой e.

89
00:05:22,420 --> 00:05:26,121
Цель состоит в том, чтобы в результате серии вычислений получить новый 

90
00:05:26,121 --> 00:05:28,936
уточненный набор вкраплений, в котором, например, те, 

91
00:05:28,936 --> 00:05:33,420
что соответствуют существительным, проглотили значение соответствующих прилагательных.

92
00:05:33,900 --> 00:05:37,452
А играя в игру глубокого обучения, мы хотим, чтобы большинство вычислений 

93
00:05:37,452 --> 00:05:39,708
выглядело как матрично-векторные произведения, 

94
00:05:39,708 --> 00:05:43,980
где матрицы полны настраиваемых весов - того, чему модель будет учиться на основе данных.

95
00:05:44,660 --> 00:05:46,861
Чтобы было понятно, я придумал этот пример с прилагательными, 

96
00:05:46,861 --> 00:05:49,916
обновляющими существительные, только для того, чтобы проиллюстрировать тип поведения, 

97
00:05:49,916 --> 00:05:52,260
который ты можешь представить себе, как поступает голова внимания.

98
00:05:52,860 --> 00:05:54,799
Как и во многих других случаях глубокого обучения, 

99
00:05:54,799 --> 00:05:57,575
истинное поведение гораздо сложнее разобрать, потому что оно основано на 

100
00:05:57,575 --> 00:06:00,275
подстройке и настройке огромного количества параметров для минимизации 

101
00:06:00,275 --> 00:06:01,340
некоторой функции стоимости.

102
00:06:01,680 --> 00:06:05,478
Просто, когда мы проходим через всевозможные матрицы, наполненные параметрами, 

103
00:06:05,478 --> 00:06:09,373
которые участвуют в этом процессе, я думаю, что очень полезно иметь воображаемый 

104
00:06:09,373 --> 00:06:13,220
пример того, что он может делать, чтобы помочь сделать все это более конкретным.

105
00:06:14,140 --> 00:06:18,026
На первом этапе этого процесса ты можешь представить, что каждое существительное, 

106
00:06:18,026 --> 00:06:21,960
например creature, задает вопрос: "Эй, есть ли прилагательные, сидящие передо мной?

107
00:06:22,160 --> 00:06:25,299
А на слова пушистый и голубой, чтобы каждый смог ответить, 

108
00:06:25,299 --> 00:06:27,960
да, я прилагательное и нахожусь в таком положении.

109
00:06:28,960 --> 00:06:32,556
Этот вопрос каким-то образом закодирован в виде еще одного вектора, 

110
00:06:32,556 --> 00:06:36,100
еще одного списка чисел, который мы называем запросом на это слово.

111
00:06:36,980 --> 00:06:40,168
Однако этот вектор запроса имеет гораздо меньшую размерность, 

112
00:06:40,168 --> 00:06:42,020
чем вектор встраивания, скажем, 128.

113
00:06:42,940 --> 00:06:46,713
Вычисление этого запроса выглядит как взятие некоторой матрицы, 

114
00:06:46,713 --> 00:06:49,780
которую я обозначу wq, и умножение ее на вкрапления.

115
00:06:50,960 --> 00:06:55,182
Если немного сжать, то запишем этот вектор запроса как q, и каждый раз, 

116
00:06:55,182 --> 00:06:59,991
когда ты видишь, что я ставлю матрицу рядом со стрелкой, как здесь, это означает, 

117
00:06:59,991 --> 00:07:04,800
что умножение этой матрицы на вектор в начале стрелки дает вектор в конце стрелки.

118
00:07:05,860 --> 00:07:09,880
В этом случае ты умножаешь эту матрицу на все вкрапления в контексте, 

119
00:07:09,880 --> 00:07:12,580
получая один вектор запроса для каждого токена.

120
00:07:13,740 --> 00:07:16,202
Записи этой матрицы являются параметрами модели, 

121
00:07:16,202 --> 00:07:19,469
то есть истинное поведение узнается из данных, и на практике то, 

122
00:07:19,469 --> 00:07:23,440
что делает эта матрица в конкретной голове внимания, разобрать довольно сложно.

123
00:07:23,900 --> 00:07:27,544
Но ради интереса, представляя пример, который, как мы надеемся, он сможет выучить, 

124
00:07:27,544 --> 00:07:31,101
мы предположим, что эта матрица запросов сопоставляет вкрапления существительных 

125
00:07:31,101 --> 00:07:34,087
с определенными направлениями в этом меньшем пространстве запросов, 

126
00:07:34,087 --> 00:07:38,040
которое каким-то образом кодирует понятие поиска прилагательных в предшествующих позициях.

127
00:07:38,780 --> 00:07:41,440
Что касается того, как это влияет на другие вкрапления, кто знает?

128
00:07:41,720 --> 00:07:44,340
Может быть, он одновременно пытается достичь с их помощью какой-то другой цели.

129
00:07:44,540 --> 00:07:47,160
Сейчас мы сосредоточены на существительных.

130
00:07:47,280 --> 00:07:51,576
В то же время с ней связана вторая матрица, называемая матрицей ключей, 

131
00:07:51,576 --> 00:07:54,620
которую ты также умножаешь на каждое из вкраплений.

132
00:07:55,280 --> 00:07:58,500
В результате получается вторая последовательность векторов, которую мы называем ключами.

133
00:07:59,420 --> 00:08:03,140
Концептуально ты хочешь думать о ключах как о потенциальных ответах на запросы.

134
00:08:03,840 --> 00:08:07,709
Эта ключевая матрица также полна настраиваемых параметров, и, как и матрица запросов, 

135
00:08:07,709 --> 00:08:11,400
она отображает векторы встраивания в то же самое пространство меньшей размерности.

136
00:08:12,200 --> 00:08:14,954
Считай, что ключи совпадают с запросами всякий раз, 

137
00:08:14,954 --> 00:08:17,020
когда они близко подходят друг к другу.

138
00:08:17,460 --> 00:08:20,375
В нашем примере ты можешь представить, что ключевая матрица 

139
00:08:20,375 --> 00:08:23,727
сопоставляет прилагательные типа "пушистый" и "голубой" с векторами, 

140
00:08:23,727 --> 00:08:26,740
которые тесно связаны с запросом, порожденным словом creature.

141
00:08:27,200 --> 00:08:30,577
Чтобы измерить, насколько хорошо каждый ключ соответствует каждому запросу, 

142
00:08:30,577 --> 00:08:34,000
ты вычисляешь точечное произведение между каждой возможной парой ключ-запрос.

143
00:08:34,480 --> 00:08:37,339
Мне нравится представлять себе сетку, заполненную кучей точек, 

144
00:08:37,339 --> 00:08:41,016
где более крупные точки соответствуют более крупным точечным продуктам - местам, 

145
00:08:41,016 --> 00:08:42,559
где ключи и запросы выравниваются.

146
00:08:43,280 --> 00:08:47,660
Для нашего примера с прилагательным существительным это выглядело бы примерно так: 

147
00:08:47,660 --> 00:08:51,934
если ключи, произведенные fluffy и blue, действительно тесно связаны с запросом, 

148
00:08:51,934 --> 00:08:55,523
произведенным creature, то точечные произведения в этих двух местах 

149
00:08:55,523 --> 00:08:58,320
будут представлять собой большие положительные числа.

150
00:08:59,100 --> 00:09:02,426
На жаргоне люди, занимающиеся машинным обучением, сказали бы, что это означает, 

151
00:09:02,426 --> 00:09:05,420
что вкрапления пушистого и голубого присутствуют во вкраплении существа.

152
00:09:06,040 --> 00:09:09,814
В отличие от этого, точечное произведение между ключом для какого-то другого слова, 

153
00:09:09,814 --> 00:09:13,364
например the, и запросом creature будет представлять собой некую маленькую или 

154
00:09:13,364 --> 00:09:16,600
отрицательную величину, отражающую то, что они не связаны друг с другом.

155
00:09:17,700 --> 00:09:21,216
Таким образом, у нас есть сетка значений, которая может быть любым реальным 

156
00:09:21,216 --> 00:09:24,871
числом от отрицательной бесконечности до бесконечности, давая нам оценку того, 

157
00:09:24,871 --> 00:09:28,480
насколько каждое слово релевантно для обновления значения всех остальных слов.

158
00:09:29,200 --> 00:09:32,279
Способ, которым мы собираемся использовать эти баллы, заключается в том, 

159
00:09:32,279 --> 00:09:35,780
чтобы взять некую взвешенную сумму по каждому столбцу, взвешенную по релевантности.

160
00:09:36,520 --> 00:09:40,453
Поэтому вместо того, чтобы значения варьировались от отрицательной бесконечности до 

161
00:09:40,453 --> 00:09:44,059
бесконечности, мы хотим, чтобы числа в этих столбцах находились между 0 и 1, 

162
00:09:44,059 --> 00:09:48,180
и чтобы каждый столбец складывался в 1, как если бы это было распределение вероятностей.

163
00:09:49,280 --> 00:09:52,220
Если ты зашел с прошлой главы, то знаешь, что нам нужно сделать тогда.

164
00:09:52,620 --> 00:09:57,300
Мы вычисляем softmax вдоль каждого из этих столбцов, чтобы нормализовать значения.

165
00:10:00,060 --> 00:10:03,427
На нашем рисунке, после того как ты применишь softmax ко всем столбцам, 

166
00:10:03,427 --> 00:10:05,860
мы заполним сетку этими нормализованными значениями.

167
00:10:06,780 --> 00:10:11,085
На этом этапе ты можешь считать, что каждый столбец имеет вес в зависимости от того, 

168
00:10:11,085 --> 00:10:14,580
насколько слово слева соответствует соответствующему значению вверху.

169
00:10:15,080 --> 00:10:16,840
Мы называем эту сетку паттерном внимания.

170
00:10:18,080 --> 00:10:20,342
Если ты посмотришь на оригинальный документ о трансформаторах, 

171
00:10:20,342 --> 00:10:22,820
то увидишь там очень компактный способ, которым они все это записали.

172
00:10:23,880 --> 00:10:27,578
Здесь переменные q и k представляют собой полные массивы векторов 

173
00:10:27,578 --> 00:10:30,717
запросов и ключей соответственно, те маленькие векторы, 

174
00:10:30,717 --> 00:10:34,640
которые ты получаешь, умножая вкрапления на матрицы запросов и ключей.

175
00:10:35,160 --> 00:10:39,036
Это выражение в числителе - действительно компактный способ представить 

176
00:10:39,036 --> 00:10:43,020
сетку всех возможных точечных произведений между парами ключей и запросов.

177
00:10:44,000 --> 00:10:47,577
Небольшая техническая деталь, о которой я не упомянул, заключается в том, 

178
00:10:47,577 --> 00:10:50,768
что для численной стабильности полезно делить все эти значения на 

179
00:10:50,768 --> 00:10:53,960
квадратный корень из размерности в пространстве ключевых запросов.

180
00:10:54,480 --> 00:10:57,726
Тогда этот softmax, обернутый вокруг полного выражения, 

181
00:10:57,726 --> 00:11:00,800
должен быть понят так, чтобы применяться по столбцам.

182
00:11:01,640 --> 00:11:04,700
Что касается этого термина v, то мы поговорим о нем буквально через секунду.

183
00:11:05,020 --> 00:11:08,460
Перед этим есть еще одна техническая деталь, которую до сих пор я пропускал.

184
00:11:09,040 --> 00:11:12,748
В процессе обучения, когда ты запускаешь эту модель на заданном текстовом примере, 

185
00:11:12,748 --> 00:11:16,144
а все веса немного корректируются и настраиваются, чтобы либо вознаградить, 

186
00:11:16,144 --> 00:11:19,987
либо наказать ее в зависимости от того, насколько высокую вероятность она приписывает 

187
00:11:19,987 --> 00:11:22,266
истинному следующему слову в отрывке, оказывается, 

188
00:11:22,266 --> 00:11:24,678
что весь процесс обучения станет намного эффективнее, 

189
00:11:24,678 --> 00:11:28,208
если ты одновременно попросишь ее предсказать все возможные следующие лексемы, 

190
00:11:28,208 --> 00:11:31,560
следующие за каждой начальной подпоследовательностью лексем в этом отрывке.

191
00:11:31,940 --> 00:11:35,000
Например, на примере фразы, на которой мы остановились, 

192
00:11:35,000 --> 00:11:39,100
можно также предсказать, какие слова следуют за creature, а какие - за the.

193
00:11:39,940 --> 00:11:41,914
Это очень здорово, потому что это означает, что то, 

194
00:11:41,914 --> 00:11:44,230
что в противном случае было бы одним тренировочным примером, 

195
00:11:44,230 --> 00:11:45,560
эффективно действует как множество.

196
00:11:46,100 --> 00:11:48,382
Для целей нашего шаблона внимания это означает, 

197
00:11:48,382 --> 00:11:52,187
что ты никогда не хочешь позволять более поздним словам влиять на более ранние, 

198
00:11:52,187 --> 00:11:56,040
так как в противном случае они могут как бы выдать ответ на то, что будет дальше.

199
00:11:56,560 --> 00:12:00,908
Это означает, что мы хотим, чтобы все эти места, представляющие более поздние токены, 

200
00:12:00,908 --> 00:12:04,600
влияющие на более ранние, каким-то образом были принудительно равны нулю.

201
00:12:05,920 --> 00:12:08,637
Самое простое, что ты можешь придумать, - это установить их равными нулю, 

202
00:12:08,637 --> 00:12:11,428
но если ты сделаешь это, то столбцы больше не будут складываться в единицу, 

203
00:12:11,428 --> 00:12:12,420
они не будут нормализованы.

204
00:12:13,120 --> 00:12:15,856
Так что вместо этого, как правило, перед применением softmax ты 

205
00:12:15,856 --> 00:12:19,020
устанавливаешь для всех этих записей значение отрицательной бесконечности.

206
00:12:19,680 --> 00:12:23,534
Если ты сделаешь это, то после применения softmax все эти показатели превратятся в ноль, 

207
00:12:23,534 --> 00:12:25,180
но столбцы останутся нормализованными.

208
00:12:26,000 --> 00:12:27,540
Этот процесс называется маскировкой.

209
00:12:27,540 --> 00:12:31,129
Есть версии внимания, где ты не применяешь его, но в нашем примере с GPT, 

210
00:12:31,129 --> 00:12:34,572
несмотря на то что на этапе обучения это более актуально, чем, скажем, 

211
00:12:34,572 --> 00:12:38,258
при запуске чатбота или чего-то подобного, ты всегда применяешь маскировку, 

212
00:12:38,258 --> 00:12:41,460
чтобы предотвратить влияние более поздних токенов на более ранние.

213
00:12:42,480 --> 00:12:46,528
Еще один факт, над которым стоит задуматься в связи с этим паттерном внимания, 

214
00:12:46,528 --> 00:12:49,500
- это то, что его размер равен квадрату размера контекста.

215
00:12:49,900 --> 00:12:52,678
Вот почему размер контекста может быть действительно огромным узким 

216
00:12:52,678 --> 00:12:55,620
местом для больших языковых моделей, и масштабирование его нетривиально.

217
00:12:56,300 --> 00:13:00,215
Как ты понимаешь, мотивированные желанием иметь все большие и большие окна контекста, 

218
00:13:00,215 --> 00:13:04,131
в последние годы появились некоторые вариации механизма внимания, направленные на то, 

219
00:13:04,131 --> 00:13:07,955
чтобы сделать контекст более масштабируемым, но сейчас мы с тобой сосредоточимся на 

220
00:13:07,955 --> 00:13:08,320
основах.

221
00:13:10,560 --> 00:13:13,463
Отлично, вычисление этого шаблона позволяет модели сделать вывод о том, 

222
00:13:13,463 --> 00:13:15,480
какие слова имеют отношение к каким другим словам.

223
00:13:16,020 --> 00:13:19,127
Теперь тебе нужно обновить вкрапления, позволяя словам 

224
00:13:19,127 --> 00:13:22,800
передавать информацию тем другим словам, к которым они относятся.

225
00:13:22,800 --> 00:13:26,584
Например, ты хочешь, чтобы встраивание Fluffy каким-то образом вызывало 

226
00:13:26,584 --> 00:13:30,262
изменение Creature, которое перемещало бы его в другую часть этого 12 

227
00:13:30,262 --> 00:13:34,520
000-мерного пространства встраивания, более конкретно кодирующую существо Fluffy.

228
00:13:35,460 --> 00:13:39,490
Сначала я покажу тебе самый простой способ, как это можно сделать, 

229
00:13:39,490 --> 00:13:43,460
хотя в контексте многоголового внимания он немного видоизменяется.

230
00:13:44,080 --> 00:13:47,146
Самым простым способом было бы использование третьей матрицы, 

231
00:13:47,146 --> 00:13:51,302
которую мы называем матрицей значений, и которую ты умножаешь на вкрапление первого 

232
00:13:51,302 --> 00:13:52,440
слова, например Fluffy.

233
00:13:53,300 --> 00:13:57,366
В результате получается то, что можно назвать вектором значений, и это то, 

234
00:13:57,366 --> 00:14:01,920
что ты добавляешь к вкраплению второго слова, в данном случае к вкраплению Creature.

235
00:14:02,600 --> 00:14:06,228
Так что этот вектор значений живет в том же очень высокоразмерном пространстве, 

236
00:14:06,228 --> 00:14:07,000
что и эмбеддинги.

237
00:14:07,460 --> 00:14:11,953
Когда ты умножаешь эту матрицу значений на вложенность слова, ты можешь подумать, 

238
00:14:11,953 --> 00:14:16,447
что это означает: если это слово имеет отношение к корректировке значения чего-то 

239
00:14:16,447 --> 00:14:21,160
другого, то что именно нужно добавить к вложенности этого другого, чтобы отразить это?

240
00:14:22,140 --> 00:14:26,073
Оглядываясь назад на нашу схему, давай отложим в сторону все ключи и запросы, 

241
00:14:26,073 --> 00:14:29,301
так как после вычисления паттерна внимания ты закончишь с ними, 

242
00:14:29,301 --> 00:14:33,437
а затем возьмешь эту матрицу значений и умножишь ее на каждое из этих вкраплений, 

243
00:14:33,437 --> 00:14:36,060
чтобы получить последовательность векторов значений.

244
00:14:37,120 --> 00:14:41,120
Ты можешь считать, что эти векторы значений как бы связаны с соответствующими ключами.

245
00:14:42,320 --> 00:14:45,840
Для каждого столбца этой диаграммы ты умножаешь каждый из 

246
00:14:45,840 --> 00:14:49,240
векторов значений на соответствующий вес в этом столбце.

247
00:14:50,080 --> 00:14:53,755
Например, здесь, при встраивании Creature, ты добавляешь большую 

248
00:14:53,755 --> 00:14:57,771
долю векторов значений для Fluffy и Blue, в то время как все остальные 

249
00:14:57,771 --> 00:15:01,560
векторы значений обнуляются или, по крайней мере, почти обнуляются.

250
00:15:02,120 --> 00:15:05,648
И наконец, чтобы обновить вложение, связанное с этим столбцом, 

251
00:15:05,648 --> 00:15:09,289
ранее кодировавшим некое контекстно-свободное значение Creature, 

252
00:15:09,289 --> 00:15:13,378
ты складываешь все эти измененные значения в столбце, получая изменение, 

253
00:15:13,378 --> 00:15:16,795
которое ты хочешь добавить и которое я обозначу как delta-e, 

254
00:15:16,795 --> 00:15:19,260
и затем добавляешь его к исходному вложению.

255
00:15:19,680 --> 00:15:22,398
Надеюсь, в результате получится более изысканный вектор, 

256
00:15:22,398 --> 00:15:26,500
кодирующий более контекстуально насыщенный смысл, например, пушистое голубое существо.

257
00:15:27,380 --> 00:15:30,258
И, конечно, ты делаешь это не только с одним вкраплением, 

258
00:15:30,258 --> 00:15:33,831
ты применяешь ту же взвешенную сумму ко всем столбцам на этой картинке, 

259
00:15:33,831 --> 00:15:38,000
создавая последовательность изменений, добавляя все эти изменения к соответствующим 

260
00:15:38,000 --> 00:15:41,871
вкраплениям, получаешь полную последовательность более утонченных вкраплений, 

261
00:15:41,871 --> 00:15:43,460
выскакивающих из блока внимания.

262
00:15:44,860 --> 00:15:49,100
Если увеличить масштаб, то весь этот процесс можно описать как единую голову внимания.

263
00:15:49,600 --> 00:15:54,837
Как я описывал до сих пор, этот процесс параметризован тремя отдельными матрицами, 

264
00:15:54,837 --> 00:15:58,940
заполненными настраиваемыми параметрами: ключ, запрос и значение.

265
00:15:59,500 --> 00:16:01,811
Я хочу уделить немного времени продолжению того, 

266
00:16:01,811 --> 00:16:04,123
что мы начали в прошлой главе, - подсчету очков, 

267
00:16:04,123 --> 00:16:08,040
когда мы подсчитываем общее количество параметров модели, используя числа из GPT-3.

268
00:16:09,300 --> 00:16:12,504
Эти матрицы ключей и запросов имеют по 12 288 столбцов, 

269
00:16:12,504 --> 00:16:15,708
что соответствует размерности встраивания, и 128 строк, 

270
00:16:15,708 --> 00:16:19,600
что соответствует размерности меньшего пространства запросов ключей.

271
00:16:20,260 --> 00:16:24,220
Это дает нам дополнительные 1,5 миллиона или около того параметров для каждого из них.

272
00:16:24,860 --> 00:16:29,736
Если ты посмотришь на эту матрицу значений, то, как я описывал до сих пор, 

273
00:16:29,736 --> 00:16:35,458
можно предположить, что это квадратная матрица, имеющая 12 288 столбцов и 12 288 строк, 

274
00:16:35,458 --> 00:16:40,920
поскольку и входы, и выходы находятся в этом очень большом пространстве встраивания.

275
00:16:41,500 --> 00:16:45,140
Если это правда, то это будет означать около 150 миллионов дополнительных параметров.

276
00:16:45,660 --> 00:16:47,300
И если говорить начистоту, то ты можешь это сделать.

277
00:16:47,420 --> 00:16:51,740
Ты можешь посвятить карте значений на порядки больше параметров, чем ключу и запросу.

278
00:16:52,060 --> 00:16:55,029
Но на практике гораздо эффективнее, если вместо этого ты сделаешь так, 

279
00:16:55,029 --> 00:16:57,706
чтобы количество параметров, отведенных под эту карту значений, 

280
00:16:57,706 --> 00:17:00,760
было таким же, как и количество параметров, отведенных под ключ и запрос.

281
00:17:01,460 --> 00:17:05,160
Это особенно актуально в условиях параллельного запуска нескольких головок внимания.

282
00:17:06,240 --> 00:17:10,099
Выглядит это так: карта значений разлагается как произведение двух меньших матриц.

283
00:17:11,180 --> 00:17:15,034
Концептуально я бы все еще призывал тебя думать об общей линейной карте, 

284
00:17:15,034 --> 00:17:19,047
с входами и выходами, как в этом большем пространстве вкраплений, например, 

285
00:17:19,047 --> 00:17:23,800
взять вкрапление синего в это направление синевы, которое ты бы добавил к существительным.

286
00:17:27,040 --> 00:17:30,643
Просто это меньшее количество строк, обычно такого же размера, 

287
00:17:30,643 --> 00:17:32,760
как и пространство ключевого запроса.

288
00:17:33,100 --> 00:17:35,626
Это означает, что ты можешь считать, что отображаешь 

289
00:17:35,626 --> 00:17:38,440
большие векторы встраивания в гораздо меньшее пространство.

290
00:17:39,040 --> 00:17:42,700
Это не совсем обычное название, но я буду называть это матрицей снижения стоимости.

291
00:17:43,400 --> 00:17:46,967
Вторая матрица переходит из этого меньшего пространства обратно в пространство 

292
00:17:46,967 --> 00:17:50,580
встраивания, создавая векторы, которые ты используешь для выполнения обновлений.

293
00:17:51,000 --> 00:17:54,740
Я назову эту матрицу "ценность вверх", что, опять же, не совсем обычно.

294
00:17:55,160 --> 00:17:58,080
То, как это написано в большинстве газет, выглядит немного иначе.

295
00:17:58,380 --> 00:17:59,520
Я расскажу об этом через минуту.

296
00:17:59,700 --> 00:18:02,540
На мой взгляд, это делает вещи немного более концептуально запутанными.

297
00:18:03,260 --> 00:18:06,483
Если говорить на жаргоне линейной алгебры, то, по сути, 

298
00:18:06,483 --> 00:18:10,340
мы ограничиваем общую карту значений преобразованием низкого ранга.

299
00:18:11,420 --> 00:18:16,100
Если вернуться к подсчету параметров, то все четыре матрицы имеют одинаковый размер, 

300
00:18:16,100 --> 00:18:20,780
и, сложив их все, мы получим около 6,3 миллиона параметров для одной головы внимания.

301
00:18:22,040 --> 00:18:25,678
Небольшое примечание, чтобы быть немного более точным, все описанное до сих пор - это то, 

302
00:18:25,678 --> 00:18:28,548
что люди называют головой самовнушения, чтобы отличить ее от вариации, 

303
00:18:28,548 --> 00:18:31,500
которая встречается в других моделях и называется перекрестным вниманием.

304
00:18:32,300 --> 00:18:35,646
Это не относится к нашему примеру с GPT, но если тебе интересно, 

305
00:18:35,646 --> 00:18:40,126
то кросс-внимание включает в себя модели, которые обрабатывают два разных типа данных, 

306
00:18:40,126 --> 00:18:43,009
например, текст на одном языке и текст на другом языке, 

307
00:18:43,009 --> 00:18:46,922
который является частью продолжающейся генерации перевода, или, может быть, 

308
00:18:46,922 --> 00:18:49,240
аудиовход речи и продолжающаяся транскрипция.

309
00:18:50,400 --> 00:18:52,700
Насадка для перекрестного захвата выглядит практически идентично.

310
00:18:52,980 --> 00:18:57,400
Разница лишь в том, что карты ключей и запросов действуют на разных наборах данных.

311
00:18:57,840 --> 00:19:02,294
Например, в модели, выполняющей перевод, ключи могут быть из одного языка, 

312
00:19:02,294 --> 00:19:05,858
а запросы - из другого, и паттерн внимания может описывать, 

313
00:19:05,858 --> 00:19:09,660
какие слова из одного языка соответствуют каким словам в другом.

314
00:19:10,340 --> 00:19:12,887
И в этом случае, как правило, не будет никакого маскирования, 

315
00:19:12,887 --> 00:19:16,340
так как нет никакого понятия о том, что более поздние токены влияют на более ранние.

316
00:19:17,180 --> 00:19:25,180
Если бы ты понял все до конца и остановился на этом, ты бы понял, что такое внимание.

317
00:19:25,760 --> 00:19:31,440
Все, что нам остается, - это изложить смысл, в котором ты делаешь это много-много раз.

318
00:19:32,100 --> 00:19:34,497
В нашем центральном примере мы сосредоточились на прилагательных, 

319
00:19:34,497 --> 00:19:37,620
обновляющих существительные, но, конечно же, существует множество различных способов, 

320
00:19:37,620 --> 00:19:39,800
с помощью которых контекст может повлиять на значение слова.

321
00:19:40,360 --> 00:19:43,632
Если слова, которые они разбили, предшествовали слову "автомобиль", 

322
00:19:43,632 --> 00:19:46,520
то это имеет отношение к форме и структуре этого автомобиля.

323
00:19:47,200 --> 00:19:49,280
И многие ассоциации могут быть менее грамматичными.

324
00:19:49,760 --> 00:19:52,981
Если слово wizard встречается в том же отрывке, что и Harry, 

325
00:19:52,981 --> 00:19:56,096
это наводит на мысль, что речь может идти о Гарри Поттере, 

326
00:19:56,096 --> 00:20:00,162
тогда как если бы вместо этого в отрывке были слова Queen, Sussex и William, 

327
00:20:00,162 --> 00:20:04,440
то, возможно, вставку Harry следовало бы обновить, чтобы она относилась к принцу.

328
00:20:05,040 --> 00:20:08,819
Для каждого типа контекстного обновления, который ты можешь себе представить, 

329
00:20:08,819 --> 00:20:11,484
параметры этих матриц ключей и запросов будут разными, 

330
00:20:11,484 --> 00:20:14,876
чтобы улавливать различные паттерны внимания, а параметры нашей карты 

331
00:20:14,876 --> 00:20:19,140
значений будут отличаться в зависимости от того, что должно быть добавлено в эмбеддинги.

332
00:20:19,980 --> 00:20:24,016
И опять же, на практике истинное поведение этих карт гораздо сложнее интерпретировать, 

333
00:20:24,016 --> 00:20:26,243
где веса устанавливаются так, чтобы делать все, 

334
00:20:26,243 --> 00:20:30,140
что нужно модели для наилучшего достижения ее цели - предсказания следующего токена.

335
00:20:31,400 --> 00:20:34,852
Как я уже говорил, все, что мы описали, - это одна голова внимания, 

336
00:20:34,852 --> 00:20:38,507
а полный блок внимания внутри трансформатора состоит из так называемого 

337
00:20:38,507 --> 00:20:42,518
многоголового внимания, где ты выполняешь множество этих операций параллельно, 

338
00:20:42,518 --> 00:20:45,920
каждая со своими отдельными ключевыми запросами и картами значений.

339
00:20:47,420 --> 00:20:51,700
Например, GPT-3 использует 96 головок внимания внутри каждого блока.

340
00:20:52,020 --> 00:20:54,378
Учитывая, что каждый из них и так немного запутан, 

341
00:20:54,378 --> 00:20:56,460
это, конечно, много, чтобы удержать в голове.

342
00:20:56,760 --> 00:21:02,508
Если говорить прямо, то это означает, что у тебя есть 96 разных матриц ключей и запросов, 

343
00:21:02,508 --> 00:21:05,000
создающих 96 разных паттернов внимания.

344
00:21:05,440 --> 00:21:08,490
Затем у каждой головы есть свои собственные матрицы значений, 

345
00:21:08,490 --> 00:21:12,180
которые используются для создания 96 последовательностей векторов значений.

346
00:21:12,460 --> 00:21:16,680
Все это складывается вместе, используя соответствующие паттерны внимания в качестве весов.

347
00:21:17,480 --> 00:21:21,089
Это означает, что для каждой позиции в контексте, для каждого токена, 

348
00:21:21,089 --> 00:21:23,977
каждая из этих голов производит предлагаемое изменение, 

349
00:21:23,977 --> 00:21:27,020
которое должно быть добавлено к вкраплениям в этой позиции.

350
00:21:27,660 --> 00:21:32,120
Итак, ты суммируешь все эти предложенные изменения, по одному для каждой головы, 

351
00:21:32,120 --> 00:21:35,480
и добавляешь результат к оригинальному вложению этой позиции.

352
00:21:36,660 --> 00:21:41,860
Вся эта сумма будет одним кусочком того, что выводится из этого многоголового 

353
00:21:41,860 --> 00:21:47,460
блока внимания, одним из тех уточненных вкраплений, которые выходят с другого конца.

354
00:21:48,320 --> 00:21:50,442
Опять же, об этом нужно много думать, так что не переживай, 

355
00:21:50,442 --> 00:21:52,140
если тебе понадобится время, чтобы это осознать.

356
00:21:52,380 --> 00:21:56,745
Общая идея заключается в том, что, запуская параллельно множество разных голов, 

357
00:21:56,745 --> 00:22:00,128
ты даешь модели возможность узнать множество разных способов, 

358
00:22:00,128 --> 00:22:01,820
которыми контекст меняет смысл.

359
00:22:03,700 --> 00:22:07,209
Если поднять наш бегущий счетчик количества параметров с 96 головами, 

360
00:22:07,209 --> 00:22:10,919
каждая из которых включает свою собственную вариацию этих четырех матриц, 

361
00:22:10,919 --> 00:22:15,080
то каждый блок многоголового внимания в итоге имеет около 600 миллионов параметров.

362
00:22:16,420 --> 00:22:18,201
Есть одна дополнительная слегка раздражающая вещь, 

363
00:22:18,201 --> 00:22:20,262
о которой я действительно должен упомянуть для тех из вас, 

364
00:22:20,262 --> 00:22:21,800
кто продолжит читать дальше о трансформерах.

365
00:22:22,080 --> 00:22:26,039
Помнишь, я говорил, что карта ценностей раскладывается на две разные матрицы, 

366
00:22:26,039 --> 00:22:29,440
которые я обозначил как матрицы "ценность вниз" и "ценность вверх".

367
00:22:29,960 --> 00:22:34,120
То, как я изложил ситуацию, предполагает, что ты видишь эту пару матриц внутри 

368
00:22:34,120 --> 00:22:38,440
каждой головы внимания, и ты абсолютно точно можешь реализовать это таким образом.

369
00:22:38,640 --> 00:22:39,920
Это был бы правильный дизайн.

370
00:22:40,260 --> 00:22:43,653
Но то, как ты видишь это в бумагах, и то, как это реализуется на практике, 

371
00:22:43,653 --> 00:22:44,920
выглядит немного по-разному.

372
00:22:45,340 --> 00:22:49,182
Все эти матрицы значений для каждой головы оказываются сшитыми 

373
00:22:49,182 --> 00:22:53,513
вместе в одну гигантскую матрицу, которую мы называем матрицей выхода, 

374
00:22:53,513 --> 00:22:56,380
связанной со всем многоголовым блоком внимания.

375
00:22:56,820 --> 00:23:01,075
И когда ты видишь, что люди ссылаются на матрицу ценностей для данной головы внимания, 

376
00:23:01,075 --> 00:23:03,667
они обычно имеют в виду только этот первый шаг, тот, 

377
00:23:03,667 --> 00:23:07,140
который я обозначил как проекцию ценности вниз, в меньшее пространство.

378
00:23:08,340 --> 00:23:11,040
Для самых любопытных из вас я оставил заметку на экране об этом.

379
00:23:11,260 --> 00:23:14,590
Это одна из тех деталей, которые рискуют отвлечь от основных концептуальных моментов, 

380
00:23:14,590 --> 00:23:16,836
но я все же хочу обратить на нее внимание, чтобы ты знал, 

381
00:23:16,836 --> 00:23:18,540
если прочитаешь об этом в других источниках.

382
00:23:19,240 --> 00:23:23,402
Если отбросить все технические нюансы, то в превью из прошлой главы мы видели, 

383
00:23:23,402 --> 00:23:28,040
как данные, проходящие через трансформатор, проходят не только через один блок внимания.

384
00:23:28,640 --> 00:23:31,033
Во-первых, он также проходит через эти другие операции, 

385
00:23:31,033 --> 00:23:32,700
называемые многослойными перцептронами.

386
00:23:33,120 --> 00:23:34,880
Подробнее о них мы поговорим в следующей главе.

387
00:23:35,180 --> 00:23:39,320
А затем он многократно проходит через множество копий обеих этих операций.

388
00:23:39,980 --> 00:23:44,870
Это значит, что после того, как слово впитает в себя часть контекста, 

389
00:23:44,870 --> 00:23:50,040
у него будет еще много шансов попасть под влияние более тонкого окружения.

390
00:23:50,940 --> 00:23:54,091
Чем дальше по сети ты продвигаешься, с каждым вкраплением принимая все 

391
00:23:54,091 --> 00:23:56,311
больше и больше смысла от всех других вкраплений, 

392
00:23:56,311 --> 00:24:00,084
которые сами становятся все более и более нюансированными, тем больше надежда на то, 

393
00:24:00,084 --> 00:24:03,546
что появится способность кодировать более высокий уровень и более абстрактные 

394
00:24:03,546 --> 00:24:07,320
идеи о данном вводе, выходящие за рамки простого описания и грамматической структуры.

395
00:24:07,880 --> 00:24:11,700
Такие вещи, как настроение и тон, а также то, является ли это стихотворением, 

396
00:24:11,700 --> 00:24:15,130
какие научные истины лежат в основе произведения и тому подобные вещи.

397
00:24:16,700 --> 00:24:21,617
Вернемся еще раз к нашему подсчету, GPT-3 включает 96 отдельных слоев, 

398
00:24:21,617 --> 00:24:27,643
поэтому общее количество ключевых параметров запросов и значений умножается еще на 96, 

399
00:24:27,643 --> 00:24:32,145
что в сумме дает чуть меньше 58 миллиардов отдельных параметров, 

400
00:24:32,145 --> 00:24:34,500
посвященных всем головам внимания.

401
00:24:34,980 --> 00:24:38,672
Это, конечно, много, но это всего лишь около трети от 175 миллиардов, 

402
00:24:38,672 --> 00:24:40,940
которые в общей сложности находятся в сети.

403
00:24:41,520 --> 00:24:44,312
Поэтому, несмотря на то, что внимание достается всем, 

404
00:24:44,312 --> 00:24:48,140
большинство параметров поступает из блоков, сидящих между этими ступенями.

405
00:24:48,560 --> 00:24:51,560
В следующей главе мы с тобой подробнее поговорим об этих других блоках, 

406
00:24:51,560 --> 00:24:53,560
а также гораздо больше о тренировочном процессе.

407
00:24:54,120 --> 00:24:57,498
Большую роль в успехе механизма внимания играет не столько какой-то 

408
00:24:57,498 --> 00:25:00,927
конкретный тип поведения, который он обеспечивает, сколько тот факт, 

409
00:25:00,927 --> 00:25:04,256
что он чрезвычайно распараллеливается, то есть ты можешь выполнять 

410
00:25:04,256 --> 00:25:08,380
огромное количество вычислений за короткое время с помощью графических процессоров.

411
00:25:09,460 --> 00:25:12,258
Учитывая, что одним из главных уроков глубокого обучения в последние 

412
00:25:12,258 --> 00:25:14,570
десять лет или два стало то, что только масштаб, похоже, 

413
00:25:14,570 --> 00:25:17,166
дает огромное качественное улучшение производительности модели, 

414
00:25:17,166 --> 00:25:19,680
есть огромное преимущество в распараллеливаемых архитектурах, 

415
00:25:19,680 --> 00:25:21,060
которые позволяют тебе это делать.

416
00:25:22,040 --> 00:25:25,340
Если ты хочешь узнать больше об этих вещах, я оставил множество ссылок в описании.

417
00:25:25,920 --> 00:25:28,515
В частности, все, что создано Андреем Карпати или Крисом Олой, 

418
00:25:28,515 --> 00:25:30,040
как правило, является чистым золотом.

419
00:25:30,560 --> 00:25:33,503
В этом видео я хотел просто рассказать о внимании в его нынешней форме, 

420
00:25:33,503 --> 00:25:35,711
но если тебе интересно узнать больше об истории того, 

421
00:25:35,711 --> 00:25:38,614
как мы пришли к этому и как ты можешь переосмыслить эту идею для себя, 

422
00:25:38,614 --> 00:25:40,618
то мой друг Вивек только что выложил пару видео, 

423
00:25:40,618 --> 00:25:42,540
в которых дается гораздо больше этой мотивации.

424
00:25:43,120 --> 00:25:45,699
Кроме того, у Бритт Круз с канала The Art of the Problem 

425
00:25:45,699 --> 00:25:48,460
есть очень хорошее видео об истории больших языковых моделей.

426
00:26:04,960 --> 00:26:09,200
Спасибо.

