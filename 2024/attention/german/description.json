[
 {
  "input": "Demystifying self-attention, multiple heads, and cross-attention.",
  "translatedText": "Entmystifizierung von Selbstaufmerksamkeit, Mehrfachköpfen und Kreuzaufmerksamkeit.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "translatedText": "Anstelle von gesponserten Anzeigen werden diese Lektionen direkt von den Zuschauern finanziert: https://3b1b.co/support",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "An equally valuable form of support is to simply share the videos.",
  "translatedText": "Eine ebenso wertvolle Form der Unterstützung ist es, die Videos einfach zu teilen.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Other resources about transformers",
  "translatedText": "Andere Ressourcen über Transformatoren",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Andrej Karpathy's videos",
  "translatedText": "Andrej Karpathys Videos",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The Transformer Circuits posts by Anthropic",
  "translatedText": "Die Transformatorstromkreise Beiträge von Anthropic",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://transformer-circuits.pub/2021/framework/index.html",
  "translatedText": "https://transformer-circuits.pub/2021/framework/index.html",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "In particular, it was only after I read this post that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources.",
  "translatedText": "Insbesondere habe ich erst nach dem Lesen dieses Beitrags angefangen, die Kombination der Wert- und Ausgabematrizen als eine kombinierte Low-Rank-Map vom Einbettungsraum zu sich selbst zu betrachten, was die Dinge zumindest in meinem Kopf viel klarer machte als andere Quellen.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "History of language models by Brit Cruise, @ArtOfTheProblem ",
  "translatedText": "Geschichte der Sprachmodelle by Brit Cruise, @ArtOfTheProblem",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://youtu.be/OFS90-FX6pg",
  "translatedText": "https://youtu.be/OFS90-FX6pg",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "What is a Language Model by @vcubingx ",
  "translatedText": "Was ist ein Sprachmodell by @vcubingx",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "translatedText": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Site with exercises related to ML programming and GPTs",
  "translatedText": "Seite mit Übungen zur ML-Programmierung und zu GPTs",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://www.gptandchill.ai/codingproblems",
  "translatedText": "https://www.gptandchill.ai/codingproblems",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Early paper on how directions in embedding spaces have meaning:",
  "translatedText": "Ein frühes Papier darüber, wie Richtungen in Einbettungsräumen Bedeutung haben:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://arxiv.org/pdf/1301.3781.pdf",
  "translatedText": "https://arxiv.org/pdf/1301.3781.pdf",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Timestamps:",
  "translatedText": "Zeitstempel:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "0:00 - Recap on embeddings",
  "translatedText": "0:00 - Zusammenfassung über Einbettungen",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "1:39 - Motivating examples",
  "translatedText": "1:39 - Motivierende Beispiele",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "4:29 - The attention pattern",
  "translatedText": "4:29 - Das Aufmerksamkeitsmuster",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "11:08 - Masking",
  "translatedText": "11:08 - Maskierung",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "12:42 - Context size",
  "translatedText": "12:42 - Kontextgröße",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "13:10 - Values",
  "translatedText": "13:10 - Werte",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "15:44 - Counting parameters",
  "translatedText": "15:44 - Zählende Parameter",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "18:21 - Cross-attention",
  "translatedText": "18:21 - Kreuzaufmerksamkeit",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "19:19 - Multiple heads",
  "translatedText": "19:19 - Mehrere Köpfe",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "22:16 - The output matrix",
  "translatedText": "22:16 - Die Ausgangsmatrix",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "23:19 - Going deeper",
  "translatedText": "23:19 - Tiefer gehen",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "24:54 - Ending",
  "translatedText": "24:54 - Ende",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 }
]