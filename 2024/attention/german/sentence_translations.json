[
 {
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "translatedText": "Im letzten Kapitel haben du und ich damit begonnen, das Innenleben eines Transformators zu erkunden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "translatedText": "Dies ist eine der Schlüsseltechnologien in großen Sprachmodellen und vielen anderen Tools der modernen KI-Welle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "translatedText": "In diesem Kapitel werden wir uns damit beschäftigen, was dieser Aufmerksamkeitsmechanismus ist, und zeigen, wie er Daten verarbeitet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "translatedText": "Um es kurz zusammenzufassen, hier der wichtige Kontext, den du im Hinterkopf behalten solltest.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "translatedText": "Das Ziel des Modells, das du und ich studieren, ist es, einen Text zu lesen und vorherzusagen, welches Wort als nächstes kommt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "translatedText": "Der Eingabetext wird in kleine Teile zerlegt, die wir Token nennen, und das sind sehr oft Wörter oder Wortteile, aber damit die Beispiele in diesem Video für dich und mich leichter zu verstehen sind, tun wir einfach so, als ob Token immer nur Wörter wären.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "translatedText": "Der erste Schritt in einem Transformator besteht darin, jedes Token mit einem hochdimensionalen Vektor zu verknüpfen, den wir seine Einbettung nennen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "translatedText": "Der wichtigste Gedanke, den ich dir mit auf den Weg geben möchte, ist, wie die Richtungen in diesem hochdimensionalen Raum aller möglichen Einbettungen mit der semantischen Bedeutung korrespondieren können.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "translatedText": "Im letzten Kapitel haben wir ein Beispiel dafür gesehen, wie die Richtung mit dem Geschlecht korrespondieren kann, in dem Sinne, dass das Hinzufügen eines bestimmten Schritts in diesem Raum dich von der Einbettung eines männlichen Substantivs zur Einbettung des entsprechenden weiblichen Substantivs führen kann.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "translatedText": "Das ist nur ein Beispiel. Du kannst dir vorstellen, wie viele andere Richtungen in diesem hochdimensionalen Raum zahlreichen anderen Aspekten der Bedeutung eines Wortes entsprechen könnten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "translatedText": "Das Ziel eines Transformators ist es, diese Einbettungen schrittweise so anzupassen, dass sie nicht nur ein einzelnes Wort kodieren, sondern eine viel, viel reichhaltigere kontextuelle Bedeutung einbinden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "translatedText": "Ich sollte gleich zu Beginn sagen, dass viele Leute den Aufmerksamkeitsmechanismus, dieses Schlüsselteil eines Transformators, sehr verwirrend finden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "translatedText": "Bevor wir uns in die rechnerischen Details und all die Matrixmultiplikationen stürzen, lohnt es sich, über ein paar Beispiele für die Art von Verhalten nachzudenken, die wir mit Aufmerksamkeit ermöglichen wollen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "translatedText": "Überlege dir die Sätze Amerikanischer echter Maulwurf, ein Maulwurf aus Kohlendioxid, und entnehme eine Biopsie des Maulwurfs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "translatedText": "Du und ich wissen, dass das Wort \"Maulwurf\" in jedem dieser Fälle eine andere Bedeutung hat, je nach Kontext.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "translatedText": "Aber nach dem ersten Schritt eines Transformators, der den Text aufbricht und jedes Token mit einem Vektor verknüpft, wäre der Vektor, der mit dem Maulwurf verknüpft ist, in all diesen Fällen derselbe, denn diese anfängliche Token-Einbettung ist praktisch eine Nachschlagetabelle ohne Bezug zum Kontext.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "translatedText": "Erst im nächsten Schritt des Transformators haben die umliegenden Einbettungen die Möglichkeit, Informationen in diese einzubringen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "translatedText": "Du stellst dir vielleicht vor, dass es in diesem Einbettungsraum mehrere verschiedene Richtungen gibt, die die verschiedenen Bedeutungen des Wortes Maulwurf kodieren, und dass ein gut trainierter Aufmerksamkeitsblock berechnet, was du der allgemeinen Einbettung hinzufügen musst, um sie in eine dieser spezifischen Richtungen zu bewegen, und zwar in Abhängigkeit vom Kontext.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "input": "To take another example, consider the embedding of the word tower.",
  "translatedText": "Um ein anderes Beispiel zu nennen, betrachte die Einbettung des Wortes Turm.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "translatedText": "Dies ist vermutlich eine sehr allgemeine, unspezifische Richtung im Raum, die mit vielen anderen großen, hohen Substantiven verbunden ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "translatedText": "Wenn diesem Wort unmittelbar Eiffel vorausgeht, könntest du dir vorstellen, dass der Mechanismus diesen Vektor so aktualisieren soll, dass er in eine Richtung zeigt, die den Eiffelturm spezifischer kodiert, vielleicht in Verbindung mit Vektoren, die mit Paris und Frankreich und Dingen aus Stahl verbunden sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "translatedText": "Wenn ihm außerdem das Wort Miniatur vorangestellt wurde, sollte der Vektor noch weiter aktualisiert werden, so dass er nicht mehr mit großen, hohen Dingen korreliert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "translatedText": "Der Aufmerksamkeitsblock dient nicht nur dazu, die Bedeutung eines Wortes zu verfeinern, sondern ermöglicht es dem Modell, Informationen, die in einer Einbettung kodiert sind, in eine andere Einbettung zu verschieben - möglicherweise in eine, die ziemlich weit entfernt ist, und möglicherweise mit Informationen, die viel umfangreicher sind als nur ein einzelnes Wort.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "translatedText": "Im letzten Kapitel haben wir gesehen, dass, nachdem alle Vektoren durch das Netzwerk geflossen sind, einschließlich vieler verschiedener Aufmerksamkeitsblöcke, die Berechnung, die du durchführst, um eine Vorhersage für das nächste Token zu treffen, vollständig vom letzten Vektor in der Sequenz abhängt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "translatedText": "Stell dir zum Beispiel vor, dass der Text, den du eingibst, der größte Teil eines ganzen Krimis ist, bis hin zu einer Stelle nahe dem Ende, an der es heißt, dass der Mörder also der Mörder war.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "translatedText": "Wenn das Modell das nächste Wort richtig vorhersagen soll, muss der letzte Vektor in der Sequenz, der zu Beginn nur das Wort \"was\" enthielt, von allen Aufmerksamkeitsblöcken aktualisiert worden sein, damit er viel mehr als nur ein einzelnes Wort repräsentiert, indem er alle Informationen aus dem gesamten Kontextfenster kodiert, die für die Vorhersage des nächsten Wortes relevant sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "input": "To step through the computations, though, let's take a much simpler example.",
  "translatedText": "Um die Berechnungen zu verdeutlichen, nehmen wir ein viel einfacheres Beispiel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "translatedText": "Stell dir vor, die Eingabe enthält den Satz: Ein flauschiges blaues Wesen durchstreifte den grünen Wald.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "translatedText": "Und nehmen wir einmal an, dass die einzige Art der Aktualisierung, die uns interessiert, darin besteht, dass die Adjektive die Bedeutungen der entsprechenden Substantive anpassen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "translatedText": "Was ich jetzt beschreibe, würden wir als einen einzigen Aufmerksamkeitsblock bezeichnen. Später werden wir sehen, dass der Aufmerksamkeitsblock aus vielen verschiedenen Köpfen besteht, die parallel laufen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "translatedText": "Auch hier ist die anfängliche Einbettung für jedes Wort ein hochdimensionaler Vektor, der nur die Bedeutung des jeweiligen Wortes ohne Kontext kodiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "input": "Actually, that's not quite true.",
  "translatedText": "Eigentlich stimmt das nicht ganz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "input": "They also encode the position of the word.",
  "translatedText": "Sie kodieren auch die Position des Wortes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "translatedText": "Es gibt noch viel mehr darüber zu sagen, wie die Positionen kodiert werden, aber im Moment musst du nur wissen, dass die Einträge dieses Vektors ausreichen, um dir zu sagen, was das Wort ist und wo es im Kontext steht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "translatedText": "Lass uns diese Einbettungen mit dem Buchstaben e bezeichnen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "translatedText": "Das Ziel ist es, durch eine Reihe von Berechnungen eine neue, verfeinerte Menge von Einbettungen zu erzeugen, bei denen zum Beispiel die Einbettungen für die Substantive die Bedeutung der entsprechenden Adjektive aufgenommen haben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "translatedText": "Und beim Deep Learning wollen wir, dass die meisten Berechnungen wie Matrix-Vektor-Produkte aussehen, bei denen die Matrizen voller einstellbarer Gewichte sind, die das Modell anhand der Daten lernt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "translatedText": "Um das klarzustellen: Ich habe dieses Beispiel mit den Adjektiven, die Substantive aktualisieren, nur erfunden, um das Verhalten zu veranschaulichen, das du dir bei einem Aufmerksamkeitskopf vorstellen kannst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "translatedText": "Wie bei vielen anderen Deep-Learning-Programmen ist es viel schwieriger, das wahre Verhalten zu erkennen, weil es auf der Einstellung einer großen Anzahl von Parametern basiert, um eine Kostenfunktion zu minimieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "translatedText": "Wenn wir durch all die verschiedenen Matrizen voller Parameter gehen, die in diesem Prozess eine Rolle spielen, ist es wirklich hilfreich, ein Beispiel für etwas zu haben, das es tun könnte, um das Ganze konkreter zu machen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "translatedText": "Für den ersten Schritt dieses Prozesses könntest du dir vorstellen, dass jedes Substantiv, z. B. die Kreatur, die Frage stellt: \"Hey, gibt es Adjektive, die vor mir sitzen?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "translatedText": "Und für die Wörter flauschig und blau, um jeweils antworten zu können, ja, ich bin ein Adjektiv und ich bin in dieser Position.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "translatedText": "Diese Frage ist irgendwie als ein weiterer Vektor kodiert, eine weitere Liste von Zahlen, die wir die Abfrage für dieses Wort nennen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "translatedText": "Dieser Abfragevektor hat jedoch eine viel kleinere Dimension als der Einbettungsvektor, etwa 128.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "translatedText": "Die Berechnung dieser Abfrage sieht so aus, dass du eine bestimmte Matrix nimmst, die ich wq nenne, und sie mit der Einbettung multiplizierst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "translatedText": "Um die Sache etwas zu komprimieren, schreiben wir den Abfragevektor als q. Immer wenn ich eine Matrix neben einen Pfeil wie diesen stelle, bedeutet das, dass du durch Multiplikation dieser Matrix mit dem Vektor am Anfang des Pfeils den Vektor am Ende des Pfeils erhältst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "translatedText": "In diesem Fall multiplizierst du diese Matrix mit allen Einbettungen im Kontext, sodass ein Abfragevektor für jedes Token entsteht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "translatedText": "Die Einträge dieser Matrix sind Parameter des Modells, d.h. das wahre Verhalten wird aus den Daten gelernt, und in der Praxis ist es schwierig zu analysieren, was diese Matrix in einem bestimmten Aufmerksamkeitskopf bewirkt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "translatedText": "Aber um uns ein Beispiel vorzustellen, von dem wir hoffen, dass es gelernt werden kann, nehmen wir an, dass diese Abfragematrix die Einbettungen der Substantive auf bestimmte Richtungen in diesem kleineren Abfrageraum abbildet, die irgendwie die Vorstellung von der Suche nach Adjektiven an vorhergehenden Positionen kodieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "input": "As to what it does to other embeddings, who knows?",
  "translatedText": "Was es mit anderen Einbettungen macht, wer weiß?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "translatedText": "Vielleicht versucht sie damit gleichzeitig, ein anderes Ziel zu erreichen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "input": "Right now, we're laser focused on the nouns.",
  "translatedText": "Im Moment konzentrieren wir uns auf die Substantive.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "translatedText": "Gleichzeitig gibt es eine zweite Matrix, die sogenannte Schlüsselmatrix, die du ebenfalls mit jeder der Einbettungen multiplizierst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "input": "This produces a second sequence of vectors that we call the keys.",
  "translatedText": "So entsteht eine zweite Folge von Vektoren, die wir Schlüssel nennen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "translatedText": "Vom Konzept her solltest du dir die Schlüssel als potenzielle Antworten auf die Abfragen vorstellen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "translatedText": "Diese Schlüsselmatrix ist ebenfalls voller einstellbarer Parameter, und genau wie die Abfragematrix bildet sie die Einbettungsvektoren auf denselben kleindimensionalen Raum ab.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "translatedText": "Du stellst dir vor, dass die Schlüssel zu den Abfragen passen, wenn sie eng beieinander liegen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "translatedText": "In unserem Beispiel könntest du dir vorstellen, dass die Schlüsselmatrix die Adjektive wie flauschig und blau auf Vektoren abbildet, die eng mit der Abfrage übereinstimmen, die das Wort Kreatur erzeugt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "translatedText": "Um zu messen, wie gut jeder Schlüssel zu jeder Anfrage passt, berechnest du ein Punktprodukt zwischen jedem möglichen Schlüssel-Abfrage-Paar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "translatedText": "Ich stelle mir gerne ein Raster mit vielen Punkten vor, bei dem die größeren Punkte den größeren Punktprodukten entsprechen, also den Stellen, an denen die Schlüssel und Abfragen übereinstimmen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "translatedText": "Für unser Beispiel mit dem Adjektiv Substantiv würde das in etwa so aussehen: Wenn die Schlüssel von flauschig und blau wirklich eng mit der Abfrage von Kreatur übereinstimmen, dann wären die Punktprodukte an diesen beiden Stellen große positive Zahlen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "translatedText": "In der Fachsprache des maschinellen Lernens würde man sagen, dass die Einbettung von flauschig und blau der Einbettung von Kreatur entspricht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "translatedText": "Im Gegensatz dazu wäre das Punktprodukt zwischen dem Schlüssel für ein anderes Wort wie \"the\" und der Abfrage für \"creature\" ein kleiner oder negativer Wert, der widerspiegelt, dass sie nichts miteinander zu tun haben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "translatedText": "Wir haben also ein Raster von Werten, die jede reale Zahl von negativ unendlich bis unendlich sein können und uns einen Wert dafür geben, wie relevant jedes Wort für die Aktualisierung der Bedeutung jedes anderen Wortes ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "translatedText": "Die Art und Weise, wie wir diese Werte verwenden werden, ist eine bestimmte gewichtete Summe entlang jeder Spalte, gewichtet nach der Relevanz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "translatedText": "Anstatt also Werte im Bereich von negativ unendlich bis unendlich zu haben, wollen wir, dass die Zahlen in diesen Spalten zwischen 0 und 1 liegen und sich jede Spalte zu 1 addiert, so als ob es sich um eine Wahrscheinlichkeitsverteilung handelt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "translatedText": "Wenn du aus dem letzten Kapitel kommst, weißt du, was wir dann tun müssen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "translatedText": "Wir berechnen einen Softmax entlang jeder dieser Spalten, um die Werte zu normalisieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "translatedText": "In unserem Bild werden wir, nachdem du Softmax auf alle Spalten angewendet hast, das Raster mit diesen normalisierten Werten ausfüllen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "translatedText": "An diesem Punkt kannst du dir vorstellen, dass jede Spalte danach gewichtet wird, wie relevant das Wort auf der linken Seite für den entsprechenden Wert am oberen Rand ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "input": "We call this grid an attention pattern.",
  "translatedText": "Wir nennen dieses Raster ein Aufmerksamkeitsmuster.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "translatedText": "Wenn du dir das Original-Papier über Transformatoren ansiehst, findest du eine sehr kompakte Art und Weise, wie sie das alles aufschreiben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "translatedText": "Hier stehen die Variablen q und k für die vollständigen Arrays der Abfrage- bzw. Schlüsselvektoren, also die kleinen Vektoren, die du erhältst, wenn du die Einbettungen mit den Abfrage- und Schlüsselmatrizen multiplizierst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "translatedText": "Dieser Ausdruck oben im Zähler ist eine wirklich kompakte Art, das Raster aller möglichen Punktprodukte zwischen Paaren von Schlüsseln und Abfragen darzustellen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "translatedText": "Ein kleines technisches Detail, das ich nicht erwähnt habe, ist, dass es für die numerische Stabilität hilfreich ist, alle diese Werte durch die Quadratwurzel der Dimension in diesem Schlüsselabfragebereich zu teilen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "translatedText": "Dann soll dieser Softmax, der um den vollständigen Ausdruck gewickelt ist, so verstanden werden, dass er spaltenweise angewendet wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "input": "As to that v term, we'll talk about it in just a second.",
  "translatedText": "Über den Begriff \"V\" werden wir gleich sprechen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "translatedText": "Davor gibt es noch ein technisches Detail, das ich bisher übersprungen habe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "translatedText": "Wenn du dieses Modell während des Trainingsprozesses auf ein bestimmtes Textbeispiel anwendest und alle Gewichte leicht anpasst und abstimmst, um es entweder zu belohnen oder zu bestrafen, je nachdem, wie hoch die Wahrscheinlichkeit ist, dass es das wahre nächste Wort in der Passage vorhersagt, wird der gesamte Trainingsprozess sehr viel effizienter, wenn du es gleichzeitig jedes mögliche nächste Token vorhersagen lässt, das auf jede anfängliche Teilsequenz von Token in dieser Passage folgt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "translatedText": "Bei dem Satz, auf den wir uns konzentriert haben, könnte es zum Beispiel auch darum gehen, vorherzusagen, welche Wörter auf creature und welche Wörter auf the folgen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "translatedText": "Das ist wirklich gut, denn es bedeutet, dass ein einziges Trainingsbeispiel effektiv als mehrere fungiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "translatedText": "Für unser Aufmerksamkeitsmuster bedeutet das, dass du niemals zulassen solltest, dass spätere Wörter frühere Wörter beeinflussen, da sie sonst die Antwort auf das, was als nächstes kommt, verraten könnten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "translatedText": "Das bedeutet, dass wir wollen, dass alle diese Punkte hier, die für spätere Token stehen, die frühere Token beeinflussen, irgendwie gezwungen werden, Null zu sein.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "translatedText": "Das Einfachste, was du tun könntest, wäre, sie gleich Null zu setzen, aber wenn du das tust, würden sich die Spalten nicht mehr zu Eins addieren, sie wären nicht normalisiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "translatedText": "Eine gängige Methode ist, dass du vor der Anwendung von Softmax alle Einträge auf den negativen Wert Unendlich setzt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "translatedText": "Wenn du das tust, werden nach der Anwendung von Softmax alle diese Werte auf Null gesetzt, aber die Spalten bleiben normalisiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "input": "This process is called masking.",
  "translatedText": "Dieser Vorgang wird Maskierung genannt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "translatedText": "Es gibt Versionen von Aufmerksamkeit, bei denen du sie nicht anwendest, aber in unserem GPT-Beispiel, auch wenn dies in der Trainingsphase relevanter ist als z.B. beim Einsatz als Chatbot, wendest du diese Maskierung immer an, um zu verhindern, dass spätere Token die früheren beeinflussen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "translatedText": "Eine weitere Tatsache, die es wert ist, über dieses Aufmerksamkeitsmuster nachzudenken, ist die Tatsache, dass seine Größe gleich dem Quadrat der Kontextgröße ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "translatedText": "Das ist der Grund, warum die Kontextgröße ein großer Engpass für große Sprachmodelle sein kann, und die Skalierung ist nicht trivial.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "translatedText": "Wie du dir vorstellen kannst, hat der Wunsch nach immer größeren Kontextfenstern in den letzten Jahren dazu geführt, dass der Aufmerksamkeitsmechanismus verändert wurde, um den Kontext skalierbarer zu machen, aber hier konzentrieren wir uns auf die Grundlagen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "translatedText": "Okay, großartig. Durch die Berechnung dieses Musters kann das Modell ableiten, welche Wörter für welche anderen Wörter relevant sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "translatedText": "Jetzt musst du die Einbettungen aktualisieren, damit die Wörter Informationen an die anderen Wörter weitergeben können, für die sie relevant sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "translatedText": "Du möchtest zum Beispiel, dass die Einbettung von Fluffy irgendwie eine Veränderung an Creature bewirkt, die es in einen anderen Teil dieses 12.000-dimensionalen Einbettungsraums verschiebt, der spezifischer eine Fluffy-Kreatur kodiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "translatedText": "Ich zeige dir hier zunächst die einfachste Art, wie du das machen kannst, obwohl es im Zusammenhang mit der mehrköpfigen Aufmerksamkeit eine kleine Änderung gibt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "translatedText": "Der einfachste Weg wäre, eine dritte Matrix zu verwenden, die wir als Wertmatrix bezeichnen, die du mit der Einbettung des ersten Wortes multiplizierst, zum Beispiel Fluffy.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "translatedText": "Das Ergebnis ist ein so genannter Wertvektor, den du zur Einbettung des zweiten Wortes hinzufügst, in diesem Fall zur Einbettung von Kreatur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "translatedText": "Dieser Wertvektor befindet sich also im selben hochdimensionalen Raum wie die Einbettungen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "translatedText": "Wenn du diese Wertmatrix mit der Einbettung eines Wortes multiplizierst, könntest du sagen: Wenn dieses Wort für die Anpassung der Bedeutung von etwas anderem relevant ist, was genau sollte zur Einbettung dieses anderen Wortes hinzugefügt werden, um dies zu berücksichtigen?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "translatedText": "Wenn wir in unserem Diagramm zurückblicken, lassen wir alle Schlüssel und Abfragen beiseite, denn nachdem du das Aufmerksamkeitsmuster berechnet hast, nimmst du diese Wertmatrix und multiplizierst sie mit jeder einzelnen dieser Einbettungen, um eine Folge von Wertvektoren zu erhalten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "translatedText": "Du kannst dir vorstellen, dass diese Wertvektoren mit den entsprechenden Schlüsseln verbunden sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "translatedText": "Für jede Spalte in diesem Diagramm multiplizierst du jeden der Wertvektoren mit dem entsprechenden Gewicht in dieser Spalte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "translatedText": "Hier zum Beispiel würdest du bei der Einbettung von Kreatur einen großen Teil der Wertvektoren für Fluffy und Blue hinzufügen, während alle anderen Wertvektoren auf Null gesetzt werden oder zumindest fast auf Null gesetzt werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "translatedText": "Um die Einbettung dieser Spalte zu aktualisieren, die zuvor eine kontextfreie Bedeutung von \"Kreatur\" kodiert hat, addierst du alle neu skalierten Werte in der Spalte und ermittelst so eine Änderung, die du hinzufügen möchtest, die ich als delta-e bezeichne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "translatedText": "Das Ergebnis ist hoffentlich ein verfeinerter Vektor, der die kontextreiche Bedeutung kodiert, wie die eines flauschigen blauen Wesens.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "translatedText": "Du wendest dieselbe gewichtete Summe auf alle Spalten in diesem Bild an und erzeugst so eine Abfolge von Änderungen. Wenn du all diese Änderungen zu den entsprechenden Einbettungen hinzufügst, entsteht eine vollständige Abfolge von verfeinerten Einbettungen, die aus dem Aufmerksamkeitsblock herausragen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "translatedText": "Wenn du herauszoomst, ist dieser ganze Prozess das, was du als einen einzigen Kopf der Aufmerksamkeit beschreiben würdest.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "translatedText": "So wie ich es bisher beschrieben habe, wird dieser Prozess durch drei verschiedene Matrizen parametrisiert, die alle mit einstellbaren Parametern gefüllt sind: dem Schlüssel, der Abfrage und dem Wert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "translatedText": "Ich möchte einen Moment damit fortfahren, was wir im letzten Kapitel begonnen haben, nämlich mit der Zählung der Modellparameter anhand der Zahlen aus GPT-3.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "translatedText": "Diese Schlüssel- und Abfragematrizen haben jeweils 12.288 Spalten, die der Dimension der Einbettung entsprechen, und 128 Zeilen, die der Dimension des kleineren Schlüsselabfragebereichs entsprechen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "translatedText": "Damit haben wir etwa 1,5 Millionen zusätzliche Parameter für jeden von ihnen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "translatedText": "Wenn du dir dagegen die Wertmatrix ansiehst, würde die Art und Weise, wie ich die Dinge bisher beschrieben habe, darauf hindeuten, dass es sich um eine quadratische Matrix mit 12.288 Spalten und 12.288 Zeilen handelt, da sowohl die Eingaben als auch die Ausgaben in diesem sehr großen Einbettungsraum liegen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "input": "If true, that would mean about 150 million added parameters.",
  "translatedText": "Wenn das stimmt, würde das etwa 150 Millionen zusätzliche Parameter bedeuten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "input": "And to be clear, you could do that.",
  "translatedText": "Und um das klarzustellen: Du könntest das tun.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "translatedText": "Du könntest der Value Map um Größenordnungen mehr Parameter widmen als dem Schlüssel und der Abfrage.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "translatedText": "In der Praxis ist es aber viel effizienter, wenn du stattdessen dafür sorgst, dass die Anzahl der Parameter für die Value Map gleich der Anzahl der Parameter für den Schlüssel und die Abfrage ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "translatedText": "Dies ist besonders wichtig, wenn mehrere Aufmerksamkeitsköpfe parallel laufen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "translatedText": "Das sieht so aus, dass die Wertkarte als Produkt zweier kleinerer Matrizen faktorisiert wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "translatedText": "Konzeptionell würde ich dich immer noch dazu ermutigen, über die gesamte lineare Karte nachzudenken, eine mit Inputs und Outputs, beide in diesem größeren Einbettungsraum, z.B. die Einbettung von Blau in diese Blau-Richtung, die du zu Nomen hinzufügen würdest.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "translatedText": "Es handelt sich nur um eine geringere Anzahl von Zeilen, die in der Regel genauso groß ist wie der Platz für die Schlüsselabfrage.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "translatedText": "Das bedeutet, dass du die großen Einbettungsvektoren auf einen viel kleineren Raum abbilden kannst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "translatedText": "Das ist zwar nicht die übliche Bezeichnung, aber ich nenne das mal die Value Down Matrix.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "translatedText": "Die zweite Matrix bildet von diesem kleineren Raum zurück auf den Einbettungsraum ab und erzeugt die Vektoren, die du für die eigentlichen Aktualisierungen verwendest.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "translatedText": "Ich nenne das hier die Value-Up-Matrix, die wiederum nicht konventionell ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "input": "The way that you would see this written in most papers looks a little different.",
  "translatedText": "Die Art und Weise, wie du das in den meisten Zeitungen lesen kannst, sieht ein bisschen anders aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "input": "I'll talk about it in a minute.",
  "translatedText": "Ich werde gleich darüber sprechen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "translatedText": "Meiner Meinung nach macht es die Dinge konzeptionell ein wenig verwirrender.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "translatedText": "Um es mit dem Jargon der linearen Algebra auszudrücken: Im Grunde geht es darum, dass die gesamte Wertkarte eine Transformation niedrigen Ranges sein muss.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "translatedText": "Um auf die Anzahl der Parameter zurückzukommen: Alle vier Matrizen sind gleich groß, und wenn wir sie alle zusammenzählen, kommen wir auf etwa 6,3 Millionen Parameter für einen Aufmerksamkeitskopf.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "translatedText": "Eine kurze Anmerkung am Rande: Alles, was bisher beschrieben wurde, ist das, was man als Selbstaufmerksamkeitskopf bezeichnen würde, um es von einer Variante zu unterscheiden, die in anderen Modellen auftaucht und Cross-Attention genannt wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "translatedText": "Das ist für unser GPT-Beispiel zwar nicht relevant, aber falls du neugierig bist: Cross-Attention beinhaltet Modelle, die zwei verschiedene Arten von Daten verarbeiten, z. B. Text in einer Sprache und Text in einer anderen Sprache, der Teil einer laufenden Übersetzung ist, oder vielleicht Audio-Input von Sprache und eine laufende Transkription.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "input": "A cross-attention head looks almost identical.",
  "translatedText": "Ein Kreuzanschlagskopf sieht fast genauso aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "input": "The only difference is that the key and query maps act on different data sets.",
  "translatedText": "Der einzige Unterschied ist, dass die Key- und Query-Maps auf unterschiedliche Datensätze wirken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "translatedText": "In einem Übersetzungsmodell könnten zum Beispiel die Schlüssel aus einer Sprache stammen, während die Abfragen aus einer anderen kommen, und das Aufmerksamkeitsmuster könnte beschreiben, welche Wörter aus einer Sprache welchen Wörtern in einer anderen entsprechen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "translatedText": "Und in diesem Fall gibt es normalerweise keine Maskierung, da spätere Token keine Auswirkungen auf frühere Token haben können.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "translatedText": "Wenn du dich aber auf die Selbstaufmerksamkeit konzentrierst und alles bis hierher verstanden hast, wirst du die Essenz dessen, was Aufmerksamkeit wirklich ist, erkennen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "translatedText": "Alles, was uns wirklich bleibt, ist, den Sinn zu erklären, in dem du das viele, viele Male machst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "translatedText": "In unserem zentralen Beispiel haben wir uns auf Adjektive konzentriert, die Substantive aktualisieren, aber natürlich gibt es viele verschiedene Möglichkeiten, wie der Kontext die Bedeutung eines Wortes beeinflussen kann.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "translatedText": "Wenn die Wörter, mit denen sie zusammengestoßen sind, dem Wort Auto vorausgingen, hat das Auswirkungen auf die Form und Struktur des Autos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "input": "And a lot of associations might be less grammatical.",
  "translatedText": "Und viele Assoziationen sind vielleicht weniger grammatikalisch.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "translatedText": "Wenn das Wort Zauberer irgendwo in der gleichen Passage wie Harry vorkommt, deutet das darauf hin, dass es sich um Harry Potter handeln könnte, während wenn stattdessen die Wörter Königin, Sussex und William in dieser Passage vorkommen würden, dann sollte die Einbettung von Harry vielleicht aktualisiert werden, um sich auf den Prinzen zu beziehen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "translatedText": "Für jede Art von kontextbezogener Aktualisierung, die du dir vorstellen kannst, würden die Parameter dieser Schlüssel- und Abfragematrizen unterschiedlich sein, um die verschiedenen Aufmerksamkeitsmuster zu erfassen, und die Parameter unserer Value Map würden sich danach richten, was zu den Einbettungen hinzugefügt werden soll.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "translatedText": "In der Praxis ist das tatsächliche Verhalten dieser Karten viel schwieriger zu interpretieren, da die Gewichte so gesetzt werden, dass sie das tun, was das Modell braucht, um sein Ziel, die Vorhersage des nächsten Tokens, am besten zu erreichen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "translatedText": "Wie ich schon sagte, ist alles, was wir beschrieben haben, ein einzelner Aufmerksamkeitsblock. Ein kompletter Aufmerksamkeitsblock in einem Transformator besteht aus einer sogenannten Multi-Head-Attention, bei der du viele dieser Operationen parallel durchführst, jede mit ihren eigenen Schlüsselabfragen und Value Maps.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "translatedText": "GPT-3 verwendet zum Beispiel 96 Aufmerksamkeitsköpfe in jedem Block.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "translatedText": "Wenn man bedenkt, dass jeder von ihnen schon ein bisschen verwirrend ist, ist das sicherlich eine Menge, die man im Kopf behalten muss.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "translatedText": "Um es ganz klar zu sagen, bedeutet das, dass du 96 verschiedene Schlüssel- und Abfragematrizen hast, die 96 verschiedene Aufmerksamkeitsmuster erzeugen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "translatedText": "Dann hat jeder Kopf seine eigenen Wertmatrizen, die verwendet werden, um 96 Folgen von Wertvektoren zu erzeugen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "translatedText": "Diese werden addiert, wobei die entsprechenden Aufmerksamkeitsmuster als Gewichte verwendet werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "translatedText": "Das bedeutet, dass für jede Position im Kontext, jedes Token, jeder dieser Köpfe eine Änderung vorschlägt, die der Einbettung an dieser Position hinzugefügt werden soll.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "translatedText": "Du addierst also alle vorgeschlagenen Änderungen zusammen, eine für jeden Kopf, und fügst das Ergebnis der ursprünglichen Einbettung dieser Position hinzu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "translatedText": "Diese ganze Summe hier wäre ein Teil dessen, was von diesem mehrköpfigen Aufmerksamkeitsblock ausgegeben wird, eine einzelne dieser verfeinerten Einbettungen, die am anderen Ende herauskommt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "translatedText": "Auch hier gibt es viel zu bedenken, also mach dir keine Sorgen, wenn es eine Weile dauert, bis du es verinnerlicht hast.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "translatedText": "Der Grundgedanke ist, dass du dem Modell die Fähigkeit gibst, viele verschiedene Arten zu lernen, wie der Kontext die Bedeutung verändert, indem du viele verschiedene Köpfe parallel laufen lässt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "translatedText": "Wenn wir unsere laufende Zählung der Parameter mit 96 Köpfen durchführen, von denen jeder seine eigene Variante dieser vier Matrizen enthält, kommt jeder Block mit mehreren Köpfen auf rund 600 Millionen Parameter.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "translatedText": "Es gibt noch eine weitere, etwas ärgerliche Sache, die ich unbedingt erwähnen sollte, wenn ihr noch mehr über Transformers lesen wollt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "translatedText": "Du erinnerst dich daran, dass ich gesagt habe, dass die Wertekarte in zwei verschiedene Matrizen aufgeteilt ist, die ich als \"Value Down\"- und \"Value Up\"-Matrizen bezeichnet habe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "translatedText": "Die Art und Weise, wie ich die Dinge formuliert habe, legt nahe, dass du dieses Matrizenpaar in jedem Aufmerksamkeitskopf siehst, und du könntest es auf jeden Fall so umsetzen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "input": "That would be a valid design.",
  "translatedText": "Das wäre ein gültiges Design.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "translatedText": "Aber die Art und Weise, wie das in den Papieren steht und wie es in der Praxis umgesetzt wird, sieht ein bisschen anders aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "translatedText": "Alle diese Aufwertungsmatrizen für jeden Kopf werden in einer riesigen Matrix zusammengefasst, die wir als Ausgangsmatrix bezeichnen und die mit dem gesamten mehrköpfigen Aufmerksamkeitsblock verbunden ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "translatedText": "Wenn man sich auf die Wertmatrix für einen bestimmten Aufmerksamkeitskopf bezieht, meint man in der Regel nur diesen ersten Schritt, den ich als die Projektion des Wertes in den kleineren Raum bezeichnet habe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "input": "For the curious among you, I've left an on-screen note about it.",
  "translatedText": "Für die Neugierigen unter euch habe ich einen Hinweis auf dem Bildschirm hinterlassen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "translatedText": "Das ist eines dieser Details, die vom eigentlichen Konzept ablenken könnten, aber ich möchte es trotzdem erwähnen, damit du es weißt, falls du in anderen Quellen darüber liest.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "translatedText": "Abgesehen von allen technischen Feinheiten haben wir in der Vorschau des letzten Kapitels gesehen, dass Daten, die durch einen Transformator fließen, nicht nur durch einen einzelnen Aufmerksamkeitsblock fließen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "translatedText": "Zum einen durchläuft er auch diese anderen Operationen, die als mehrschichtige Perzeptronen bezeichnet werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "input": "We'll talk more about those in the next chapter.",
  "translatedText": "Wir werden im nächsten Kapitel mehr darüber sprechen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "translatedText": "Und dann durchläuft es viele, viele Kopien dieser beiden Vorgänge.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "translatedText": "Das bedeutet, dass, nachdem ein bestimmtes Wort einen Teil seines Kontextes aufgenommen hat, es viele weitere Möglichkeiten gibt, dass diese nuanciertere Einbettung von seiner nuancierteren Umgebung beeinflusst wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "translatedText": "Je weiter das Netzwerk nach unten geht und jede Einbettung mehr und mehr Bedeutung aus den anderen Einbettungen aufnimmt, die ihrerseits immer nuancierter werden, desto größer ist die Hoffnung, dass es möglich ist, über die Deskriptoren und die grammatikalische Struktur hinaus höhere und abstraktere Ideen über einen bestimmten Input zu kodieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "translatedText": "Dinge wie Stimmung und Tonfall und ob es ein Gedicht ist und welche wissenschaftlichen Wahrheiten dem Stück zugrunde liegen und solche Dinge.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "translatedText": "Um noch einmal auf unsere Punktezählung zurückzukommen: GPT-3 umfasst 96 verschiedene Ebenen, also wird die Gesamtzahl der Schlüsselabfrage- und Wertparameter mit weiteren 96 multipliziert, was die Gesamtsumme auf knapp 58 Milliarden verschiedene Parameter für alle Aufmerksamkeitsköpfe bringt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "translatedText": "Das ist zwar eine Menge, aber nur etwa ein Drittel der 175 Milliarden, die insgesamt im Netz sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "translatedText": "Auch wenn die Aufmerksamkeit die ganze Aufmerksamkeit bekommt, kommen die meisten Parameter aus den Blöcken, die zwischen diesen Schritten liegen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "translatedText": "Im nächsten Kapitel werden du und ich mehr über diese anderen Blöcke und auch viel mehr über den Ausbildungsprozess sprechen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "translatedText": "Ein großer Teil des Erfolgs des Aufmerksamkeitsmechanismus ist nicht so sehr ein bestimmtes Verhalten, das er ermöglicht, sondern die Tatsache, dass er extrem parallelisierbar ist, d.h. dass man mit GPUs eine große Anzahl von Berechnungen in kurzer Zeit durchführen kann.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "translatedText": "Da eine der wichtigsten Erkenntnisse über Deep Learning in den letzten zehn Jahren darin besteht, dass allein die Skalierung zu enormen qualitativen Verbesserungen der Modellleistung führt, sind parallelisierbare Architekturen, die dies ermöglichen, von großem Vorteil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "translatedText": "Wenn du mehr über diese Dinge erfahren willst, habe ich in der Beschreibung viele Links angegeben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "translatedText": "Vor allem alles, was von Andrej Karpathy oder Chris Ola produziert wird, ist meist pures Gold.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "translatedText": "In diesem Video möchte ich nur auf die Aufmerksamkeit in ihrer jetzigen Form eingehen, aber wenn du dich für die Geschichte interessierst, wie wir hierher gekommen sind und wie du diese Idee für dich neu erfinden kannst, hat mein Freund Vivek gerade ein paar Videos veröffentlicht, in denen er mehr über die Motivation erzählt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "translatedText": "Außerdem hat Britt Cruz vom Kanal The Art of the Problem ein wirklich schönes Video über die Geschichte der großen Sprachmodelle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "input": "Thank you.",
  "translatedText": "Vielen Dank!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]