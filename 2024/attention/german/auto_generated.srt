1
00:00:00,000 --> 00:00:02,090
Im letzten Kapitel haben du und ich damit begonnen, 

2
00:00:02,090 --> 00:00:04,019
das Innenleben eines Transformators zu erkunden.

3
00:00:04,560 --> 00:00:07,077
Dies ist eine der Schlüsseltechnologien in großen 

4
00:00:07,077 --> 00:00:10,200
Sprachmodellen und vielen anderen Tools der modernen KI-Welle.

5
00:00:10,980 --> 00:00:15,251
In diesem Kapitel werden wir uns damit beschäftigen, 

6
00:00:15,251 --> 00:00:21,700
was dieser Aufmerksamkeitsmechanismus ist, und zeigen, wie er Daten verarbeitet.

7
00:00:26,140 --> 00:00:28,144
Um es kurz zusammenzufassen, hier der wichtige Kontext, 

8
00:00:28,144 --> 00:00:29,540
den du im Hinterkopf behalten solltest.

9
00:00:30,000 --> 00:00:32,672
Das Ziel des Modells, das du und ich studieren, ist es, 

10
00:00:32,672 --> 00:00:36,060
einen Text zu lesen und vorherzusagen, welches Wort als nächstes kommt.

11
00:00:36,860 --> 00:00:40,499
Der Eingabetext wird in kleine Teile zerlegt, die wir Token nennen, 

12
00:00:40,499 --> 00:00:45,101
und das sind sehr oft Wörter oder Wortteile, aber damit die Beispiele in diesem Video 

13
00:00:45,101 --> 00:00:48,633
für dich und mich leichter zu verstehen sind, tun wir einfach so, 

14
00:00:48,633 --> 00:00:50,560
als ob Token immer nur Wörter wären.

15
00:00:51,480 --> 00:00:53,802
Der erste Schritt in einem Transformator besteht darin, 

16
00:00:53,802 --> 00:00:56,373
jedes Token mit einem hochdimensionalen Vektor zu verknüpfen, 

17
00:00:56,373 --> 00:00:57,700
den wir seine Einbettung nennen.

18
00:00:57,700 --> 00:01:00,679
Der wichtigste Gedanke, den ich dir mit auf den Weg geben möchte, 

19
00:01:00,679 --> 00:01:03,523
ist, wie die Richtungen in diesem hochdimensionalen Raum aller 

20
00:01:03,523 --> 00:01:07,000
möglichen Einbettungen mit der semantischen Bedeutung korrespondieren können.

21
00:01:07,680 --> 00:01:09,865
Im letzten Kapitel haben wir ein Beispiel dafür gesehen, 

22
00:01:09,865 --> 00:01:12,625
wie die Richtung mit dem Geschlecht korrespondieren kann, in dem Sinne, 

23
00:01:12,625 --> 00:01:15,461
dass das Hinzufügen eines bestimmten Schritts in diesem Raum dich von der 

24
00:01:15,461 --> 00:01:18,298
Einbettung eines männlichen Substantivs zur Einbettung des entsprechenden 

25
00:01:18,298 --> 00:01:19,640
weiblichen Substantivs führen kann.

26
00:01:20,160 --> 00:01:22,201
Das ist nur ein Beispiel. Du kannst dir vorstellen, 

27
00:01:22,201 --> 00:01:24,596
wie viele andere Richtungen in diesem hochdimensionalen Raum 

28
00:01:24,596 --> 00:01:27,580
zahlreichen anderen Aspekten der Bedeutung eines Wortes entsprechen könnten.

29
00:01:28,800 --> 00:01:33,103
Das Ziel eines Transformators ist es, diese Einbettungen schrittweise so anzupassen, 

30
00:01:33,103 --> 00:01:36,496
dass sie nicht nur ein einzelnes Wort kodieren, sondern eine viel, 

31
00:01:36,496 --> 00:01:39,180
viel reichhaltigere kontextuelle Bedeutung einbinden.

32
00:01:40,140 --> 00:01:45,090
Ich sollte gleich zu Beginn sagen, dass viele Leute den Aufmerksamkeitsmechanismus, 

33
00:01:45,090 --> 00:01:48,980
dieses Schlüsselteil eines Transformators, sehr verwirrend finden.

34
00:01:49,440 --> 00:01:53,428
Bevor wir uns in die rechnerischen Details und all die Matrixmultiplikationen stürzen, 

35
00:01:53,428 --> 00:01:57,050
lohnt es sich, über ein paar Beispiele für die Art von Verhalten nachzudenken, 

36
00:01:57,050 --> 00:01:59,160
die wir mit Aufmerksamkeit ermöglichen wollen.

37
00:02:00,140 --> 00:02:02,793
Überlege dir die Sätze Amerikanischer echter Maulwurf, 

38
00:02:02,793 --> 00:02:06,220
ein Maulwurf aus Kohlendioxid, und entnehme eine Biopsie des Maulwurfs.

39
00:02:06,700 --> 00:02:08,742
Du und ich wissen, dass das Wort "Maulwurf" in jedem 

40
00:02:08,742 --> 00:02:10,900
dieser Fälle eine andere Bedeutung hat, je nach Kontext.

41
00:02:11,360 --> 00:02:13,861
Aber nach dem ersten Schritt eines Transformators, 

42
00:02:13,861 --> 00:02:17,147
der den Text aufbricht und jedes Token mit einem Vektor verknüpft, 

43
00:02:17,147 --> 00:02:21,266
wäre der Vektor, der mit dem Maulwurf verknüpft ist, in all diesen Fällen derselbe, 

44
00:02:21,266 --> 00:02:25,092
denn diese anfängliche Token-Einbettung ist praktisch eine Nachschlagetabelle 

45
00:02:25,092 --> 00:02:26,220
ohne Bezug zum Kontext.

46
00:02:26,620 --> 00:02:29,860
Erst im nächsten Schritt des Transformators haben die umliegenden 

47
00:02:29,860 --> 00:02:33,100
Einbettungen die Möglichkeit, Informationen in diese einzubringen.

48
00:02:33,820 --> 00:02:37,320
Du stellst dir vielleicht vor, dass es in diesem Einbettungsraum mehrere 

49
00:02:37,320 --> 00:02:40,916
verschiedene Richtungen gibt, die die verschiedenen Bedeutungen des Wortes 

50
00:02:40,916 --> 00:02:44,751
Maulwurf kodieren, und dass ein gut trainierter Aufmerksamkeitsblock berechnet, 

51
00:02:44,751 --> 00:02:47,245
was du der allgemeinen Einbettung hinzufügen musst, 

52
00:02:47,245 --> 00:02:50,025
um sie in eine dieser spezifischen Richtungen zu bewegen, 

53
00:02:50,025 --> 00:02:51,800
und zwar in Abhängigkeit vom Kontext.

54
00:02:53,300 --> 00:02:56,180
Um ein anderes Beispiel zu nennen, betrachte die Einbettung des Wortes Turm.

55
00:02:57,060 --> 00:03:00,631
Dies ist vermutlich eine sehr allgemeine, unspezifische Richtung im Raum, 

56
00:03:00,631 --> 00:03:03,720
die mit vielen anderen großen, hohen Substantiven verbunden ist.

57
00:03:04,020 --> 00:03:07,695
Wenn diesem Wort unmittelbar Eiffel vorausgeht, könntest du dir vorstellen, 

58
00:03:07,695 --> 00:03:12,047
dass der Mechanismus diesen Vektor so aktualisieren soll, dass er in eine Richtung zeigt, 

59
00:03:12,047 --> 00:03:15,916
die den Eiffelturm spezifischer kodiert, vielleicht in Verbindung mit Vektoren, 

60
00:03:15,916 --> 00:03:19,060
die mit Paris und Frankreich und Dingen aus Stahl verbunden sind.

61
00:03:19,920 --> 00:03:22,522
Wenn ihm außerdem das Wort Miniatur vorangestellt wurde, 

62
00:03:22,522 --> 00:03:26,404
sollte der Vektor noch weiter aktualisiert werden, so dass er nicht mehr mit großen, 

63
00:03:26,404 --> 00:03:27,500
hohen Dingen korreliert.

64
00:03:29,480 --> 00:03:32,822
Der Aufmerksamkeitsblock dient nicht nur dazu, die Bedeutung eines Wortes zu verfeinern, 

65
00:03:32,822 --> 00:03:36,089
sondern ermöglicht es dem Modell, Informationen, die in einer Einbettung kodiert sind, 

66
00:03:36,089 --> 00:03:38,605
in eine andere Einbettung zu verschieben - möglicherweise in eine, 

67
00:03:38,605 --> 00:03:41,234
die ziemlich weit entfernt ist, und möglicherweise mit Informationen, 

68
00:03:41,234 --> 00:03:43,300
die viel umfangreicher sind als nur ein einzelnes Wort.

69
00:03:43,300 --> 00:03:47,057
Im letzten Kapitel haben wir gesehen, dass, nachdem alle Vektoren durch das 

70
00:03:47,057 --> 00:03:51,210
Netzwerk geflossen sind, einschließlich vieler verschiedener Aufmerksamkeitsblöcke, 

71
00:03:51,210 --> 00:03:55,610
die Berechnung, die du durchführst, um eine Vorhersage für das nächste Token zu treffen, 

72
00:03:55,610 --> 00:03:58,280
vollständig vom letzten Vektor in der Sequenz abhängt.

73
00:03:59,100 --> 00:04:01,804
Stell dir zum Beispiel vor, dass der Text, den du eingibst, 

74
00:04:01,804 --> 00:04:05,410
der größte Teil eines ganzen Krimis ist, bis hin zu einer Stelle nahe dem Ende, 

75
00:04:05,410 --> 00:04:07,800
an der es heißt, dass der Mörder also der Mörder war.

76
00:04:08,400 --> 00:04:11,421
Wenn das Modell das nächste Wort richtig vorhersagen soll, 

77
00:04:11,421 --> 00:04:15,621
muss der letzte Vektor in der Sequenz, der zu Beginn nur das Wort "was" enthielt, 

78
00:04:15,621 --> 00:04:18,642
von allen Aufmerksamkeitsblöcken aktualisiert worden sein, 

79
00:04:18,642 --> 00:04:21,766
damit er viel mehr als nur ein einzelnes Wort repräsentiert, 

80
00:04:21,766 --> 00:04:25,300
indem er alle Informationen aus dem gesamten Kontextfenster kodiert, 

81
00:04:25,300 --> 00:04:28,220
die für die Vorhersage des nächsten Wortes relevant sind.

82
00:04:29,500 --> 00:04:32,580
Um die Berechnungen zu verdeutlichen, nehmen wir ein viel einfacheres Beispiel.

83
00:04:32,980 --> 00:04:35,326
Stell dir vor, die Eingabe enthält den Satz: Ein 

84
00:04:35,326 --> 00:04:37,960
flauschiges blaues Wesen durchstreifte den grünen Wald.

85
00:04:38,460 --> 00:04:42,596
Und nehmen wir einmal an, dass die einzige Art der Aktualisierung, die uns interessiert, 

86
00:04:42,596 --> 00:04:46,780
darin besteht, dass die Adjektive die Bedeutungen der entsprechenden Substantive anpassen.

87
00:04:47,000 --> 00:04:50,655
Was ich jetzt beschreibe, würden wir als einen einzigen Aufmerksamkeitsblock bezeichnen. 

88
00:04:50,655 --> 00:04:53,366
Später werden wir sehen, dass der Aufmerksamkeitsblock aus vielen 

89
00:04:53,366 --> 00:04:55,420
verschiedenen Köpfen besteht, die parallel laufen.

90
00:04:56,140 --> 00:05:00,263
Auch hier ist die anfängliche Einbettung für jedes Wort ein hochdimensionaler Vektor, 

91
00:05:00,263 --> 00:05:03,380
der nur die Bedeutung des jeweiligen Wortes ohne Kontext kodiert.

92
00:05:04,000 --> 00:05:05,220
Eigentlich stimmt das nicht ganz.

93
00:05:05,380 --> 00:05:07,640
Sie kodieren auch die Position des Wortes.

94
00:05:07,980 --> 00:05:11,786
Es gibt noch viel mehr darüber zu sagen, wie die Positionen kodiert werden, 

95
00:05:11,786 --> 00:05:15,844
aber im Moment musst du nur wissen, dass die Einträge dieses Vektors ausreichen, 

96
00:05:15,844 --> 00:05:18,900
um dir zu sagen, was das Wort ist und wo es im Kontext steht.

97
00:05:19,500 --> 00:05:21,660
Lass uns diese Einbettungen mit dem Buchstaben e bezeichnen.

98
00:05:22,420 --> 00:05:25,372
Das Ziel ist es, durch eine Reihe von Berechnungen eine neue, 

99
00:05:25,372 --> 00:05:27,658
verfeinerte Menge von Einbettungen zu erzeugen, 

100
00:05:27,658 --> 00:05:31,372
bei denen zum Beispiel die Einbettungen für die Substantive die Bedeutung der 

101
00:05:31,372 --> 00:05:33,420
entsprechenden Adjektive aufgenommen haben.

102
00:05:33,900 --> 00:05:37,192
Und beim Deep Learning wollen wir, dass die meisten Berechnungen 

103
00:05:37,192 --> 00:05:40,586
wie Matrix-Vektor-Produkte aussehen, bei denen die Matrizen voller 

104
00:05:40,586 --> 00:05:43,980
einstellbarer Gewichte sind, die das Modell anhand der Daten lernt.

105
00:05:44,660 --> 00:05:47,096
Um das klarzustellen: Ich habe dieses Beispiel mit den Adjektiven, 

106
00:05:47,096 --> 00:05:50,114
die Substantive aktualisieren, nur erfunden, um das Verhalten zu veranschaulichen, 

107
00:05:50,114 --> 00:05:52,260
das du dir bei einem Aufmerksamkeitskopf vorstellen kannst.

108
00:05:52,860 --> 00:05:55,739
Wie bei vielen anderen Deep-Learning-Programmen ist es viel schwieriger, 

109
00:05:55,739 --> 00:05:58,657
das wahre Verhalten zu erkennen, weil es auf der Einstellung einer großen 

110
00:05:58,657 --> 00:06:01,340
Anzahl von Parametern basiert, um eine Kostenfunktion zu minimieren.

111
00:06:01,680 --> 00:06:05,286
Wenn wir durch all die verschiedenen Matrizen voller Parameter gehen, 

112
00:06:05,286 --> 00:06:08,840
die in diesem Prozess eine Rolle spielen, ist es wirklich hilfreich, 

113
00:06:08,840 --> 00:06:13,220
ein Beispiel für etwas zu haben, das es tun könnte, um das Ganze konkreter zu machen.

114
00:06:14,140 --> 00:06:17,231
Für den ersten Schritt dieses Prozesses könntest du dir vorstellen, 

115
00:06:17,231 --> 00:06:21,096
dass jedes Substantiv, z. B. die Kreatur, die Frage stellt: "Hey, gibt es Adjektive, 

116
00:06:21,096 --> 00:06:21,960
die vor mir sitzen?

117
00:06:22,160 --> 00:06:25,402
Und für die Wörter flauschig und blau, um jeweils antworten zu können, 

118
00:06:25,402 --> 00:06:27,960
ja, ich bin ein Adjektiv und ich bin in dieser Position.

119
00:06:28,960 --> 00:06:32,127
Diese Frage ist irgendwie als ein weiterer Vektor kodiert, 

120
00:06:32,127 --> 00:06:36,100
eine weitere Liste von Zahlen, die wir die Abfrage für dieses Wort nennen.

121
00:06:36,980 --> 00:06:41,552
Dieser Abfragevektor hat jedoch eine viel kleinere Dimension als der Einbettungsvektor, 

122
00:06:41,552 --> 00:06:42,020
etwa 128.

123
00:06:42,940 --> 00:06:46,889
Die Berechnung dieser Abfrage sieht so aus, dass du eine bestimmte Matrix nimmst, 

124
00:06:46,889 --> 00:06:49,780
die ich wq nenne, und sie mit der Einbettung multiplizierst.

125
00:06:50,960 --> 00:06:54,790
Um die Sache etwas zu komprimieren, schreiben wir den Abfragevektor als q. 

126
00:06:54,790 --> 00:06:58,058
Immer wenn ich eine Matrix neben einen Pfeil wie diesen stelle, 

127
00:06:58,058 --> 00:07:01,378
bedeutet das, dass du durch Multiplikation dieser Matrix mit dem 

128
00:07:01,378 --> 00:07:04,800
Vektor am Anfang des Pfeils den Vektor am Ende des Pfeils erhältst.

129
00:07:05,860 --> 00:07:10,015
In diesem Fall multiplizierst du diese Matrix mit allen Einbettungen im Kontext, 

130
00:07:10,015 --> 00:07:12,580
sodass ein Abfragevektor für jedes Token entsteht.

131
00:07:13,740 --> 00:07:16,338
Die Einträge dieser Matrix sind Parameter des Modells, d.h. 

132
00:07:16,338 --> 00:07:18,416
das wahre Verhalten wird aus den Daten gelernt, 

133
00:07:18,416 --> 00:07:20,625
und in der Praxis ist es schwierig zu analysieren, 

134
00:07:20,625 --> 00:07:23,440
was diese Matrix in einem bestimmten Aufmerksamkeitskopf bewirkt.

135
00:07:23,900 --> 00:07:27,716
Aber um uns ein Beispiel vorzustellen, von dem wir hoffen, dass es gelernt werden kann, 

136
00:07:27,716 --> 00:07:31,056
nehmen wir an, dass diese Abfragematrix die Einbettungen der Substantive auf 

137
00:07:31,056 --> 00:07:33,789
bestimmte Richtungen in diesem kleineren Abfrageraum abbildet, 

138
00:07:33,789 --> 00:07:37,172
die irgendwie die Vorstellung von der Suche nach Adjektiven an vorhergehenden 

139
00:07:37,172 --> 00:07:38,040
Positionen kodieren.

140
00:07:38,780 --> 00:07:41,440
Was es mit anderen Einbettungen macht, wer weiß?

141
00:07:41,720 --> 00:07:44,340
Vielleicht versucht sie damit gleichzeitig, ein anderes Ziel zu erreichen.

142
00:07:44,540 --> 00:07:47,160
Im Moment konzentrieren wir uns auf die Substantive.

143
00:07:47,280 --> 00:07:51,339
Gleichzeitig gibt es eine zweite Matrix, die sogenannte Schlüsselmatrix, 

144
00:07:51,339 --> 00:07:54,620
die du ebenfalls mit jeder der Einbettungen multiplizierst.

145
00:07:55,280 --> 00:07:58,500
So entsteht eine zweite Folge von Vektoren, die wir Schlüssel nennen.

146
00:07:59,420 --> 00:08:01,280
Vom Konzept her solltest du dir die Schlüssel als 

147
00:08:01,280 --> 00:08:03,140
potenzielle Antworten auf die Abfragen vorstellen.

148
00:08:03,840 --> 00:08:06,760
Diese Schlüsselmatrix ist ebenfalls voller einstellbarer Parameter, 

149
00:08:06,760 --> 00:08:10,240
und genau wie die Abfragematrix bildet sie die Einbettungsvektoren auf denselben 

150
00:08:10,240 --> 00:08:11,400
kleindimensionalen Raum ab.

151
00:08:12,200 --> 00:08:15,396
Du stellst dir vor, dass die Schlüssel zu den Abfragen passen, 

152
00:08:15,396 --> 00:08:17,020
wenn sie eng beieinander liegen.

153
00:08:17,460 --> 00:08:19,665
In unserem Beispiel könntest du dir vorstellen, 

154
00:08:19,665 --> 00:08:23,570
dass die Schlüsselmatrix die Adjektive wie flauschig und blau auf Vektoren abbildet, 

155
00:08:23,570 --> 00:08:26,740
die eng mit der Abfrage übereinstimmen, die das Wort Kreatur erzeugt.

156
00:08:27,200 --> 00:08:30,190
Um zu messen, wie gut jeder Schlüssel zu jeder Anfrage passt, 

157
00:08:30,190 --> 00:08:34,000
berechnest du ein Punktprodukt zwischen jedem möglichen Schlüssel-Abfrage-Paar.

158
00:08:34,480 --> 00:08:36,812
Ich stelle mir gerne ein Raster mit vielen Punkten vor, 

159
00:08:36,812 --> 00:08:39,686
bei dem die größeren Punkte den größeren Punktprodukten entsprechen, 

160
00:08:39,686 --> 00:08:42,559
also den Stellen, an denen die Schlüssel und Abfragen übereinstimmen.

161
00:08:43,280 --> 00:08:47,916
Für unser Beispiel mit dem Adjektiv Substantiv würde das in etwa so aussehen: 

162
00:08:47,916 --> 00:08:52,850
Wenn die Schlüssel von flauschig und blau wirklich eng mit der Abfrage von Kreatur 

163
00:08:52,850 --> 00:08:57,903
übereinstimmen, dann wären die Punktprodukte an diesen beiden Stellen große positive 

164
00:08:57,903 --> 00:08:58,320
Zahlen.

165
00:08:59,100 --> 00:09:01,814
In der Fachsprache des maschinellen Lernens würde man sagen, 

166
00:09:01,814 --> 00:09:05,420
dass die Einbettung von flauschig und blau der Einbettung von Kreatur entspricht.

167
00:09:06,040 --> 00:09:09,479
Im Gegensatz dazu wäre das Punktprodukt zwischen dem Schlüssel für ein 

168
00:09:09,479 --> 00:09:13,693
anderes Wort wie "the" und der Abfrage für "creature" ein kleiner oder negativer Wert, 

169
00:09:13,693 --> 00:09:16,600
der widerspiegelt, dass sie nichts miteinander zu tun haben.

170
00:09:17,700 --> 00:09:21,068
Wir haben also ein Raster von Werten, die jede reale Zahl von negativ 

171
00:09:21,068 --> 00:09:24,341
unendlich bis unendlich sein können und uns einen Wert dafür geben, 

172
00:09:24,341 --> 00:09:28,480
wie relevant jedes Wort für die Aktualisierung der Bedeutung jedes anderen Wortes ist.

173
00:09:29,200 --> 00:09:31,822
Die Art und Weise, wie wir diese Werte verwenden werden, 

174
00:09:31,822 --> 00:09:35,780
ist eine bestimmte gewichtete Summe entlang jeder Spalte, gewichtet nach der Relevanz.

175
00:09:36,520 --> 00:09:40,136
Anstatt also Werte im Bereich von negativ unendlich bis unendlich zu haben, 

176
00:09:40,136 --> 00:09:43,896
wollen wir, dass die Zahlen in diesen Spalten zwischen 0 und 1 liegen und sich 

177
00:09:43,896 --> 00:09:48,180
jede Spalte zu 1 addiert, so als ob es sich um eine Wahrscheinlichkeitsverteilung handelt.

178
00:09:49,280 --> 00:09:52,220
Wenn du aus dem letzten Kapitel kommst, weißt du, was wir dann tun müssen.

179
00:09:52,620 --> 00:09:57,300
Wir berechnen einen Softmax entlang jeder dieser Spalten, um die Werte zu normalisieren.

180
00:10:00,060 --> 00:10:03,540
In unserem Bild werden wir, nachdem du Softmax auf alle Spalten angewendet hast, 

181
00:10:03,540 --> 00:10:05,860
das Raster mit diesen normalisierten Werten ausfüllen.

182
00:10:06,780 --> 00:10:10,498
An diesem Punkt kannst du dir vorstellen, dass jede Spalte danach gewichtet wird, 

183
00:10:10,498 --> 00:10:14,580
wie relevant das Wort auf der linken Seite für den entsprechenden Wert am oberen Rand ist.

184
00:10:15,080 --> 00:10:16,840
Wir nennen dieses Raster ein Aufmerksamkeitsmuster.

185
00:10:18,080 --> 00:10:20,228
Wenn du dir das Original-Papier über Transformatoren ansiehst, 

186
00:10:20,228 --> 00:10:22,820
findest du eine sehr kompakte Art und Weise, wie sie das alles aufschreiben.

187
00:10:23,880 --> 00:10:27,788
Hier stehen die Variablen q und k für die vollständigen Arrays der Abfrage- bzw. 

188
00:10:27,788 --> 00:10:30,828
Schlüsselvektoren, also die kleinen Vektoren, die du erhältst, 

189
00:10:30,828 --> 00:10:34,640
wenn du die Einbettungen mit den Abfrage- und Schlüsselmatrizen multiplizierst.

190
00:10:35,160 --> 00:10:38,235
Dieser Ausdruck oben im Zähler ist eine wirklich kompakte Art, 

191
00:10:38,235 --> 00:10:41,945
das Raster aller möglichen Punktprodukte zwischen Paaren von Schlüsseln und 

192
00:10:41,945 --> 00:10:43,020
Abfragen darzustellen.

193
00:10:44,000 --> 00:10:46,983
Ein kleines technisches Detail, das ich nicht erwähnt habe, ist, 

194
00:10:46,983 --> 00:10:49,416
dass es für die numerische Stabilität hilfreich ist, 

195
00:10:49,416 --> 00:10:52,399
alle diese Werte durch die Quadratwurzel der Dimension in diesem 

196
00:10:52,399 --> 00:10:53,960
Schlüsselabfragebereich zu teilen.

197
00:10:54,480 --> 00:10:58,017
Dann soll dieser Softmax, der um den vollständigen Ausdruck gewickelt ist, 

198
00:10:58,017 --> 00:11:00,800
so verstanden werden, dass er spaltenweise angewendet wird.

199
00:11:01,640 --> 00:11:04,700
Über den Begriff "V" werden wir gleich sprechen.

200
00:11:05,020 --> 00:11:08,460
Davor gibt es noch ein technisches Detail, das ich bisher übersprungen habe.

201
00:11:09,040 --> 00:11:12,899
Wenn du dieses Modell während des Trainingsprozesses auf ein bestimmtes Textbeispiel 

202
00:11:12,899 --> 00:11:15,532
anwendest und alle Gewichte leicht anpasst und abstimmst, 

203
00:11:15,532 --> 00:11:18,166
um es entweder zu belohnen oder zu bestrafen, je nachdem, 

204
00:11:18,166 --> 00:11:21,934
wie hoch die Wahrscheinlichkeit ist, dass es das wahre nächste Wort in der Passage 

205
00:11:21,934 --> 00:11:25,067
vorhersagt, wird der gesamte Trainingsprozess sehr viel effizienter, 

206
00:11:25,067 --> 00:11:28,336
wenn du es gleichzeitig jedes mögliche nächste Token vorhersagen lässt, 

207
00:11:28,336 --> 00:11:31,560
das auf jede anfängliche Teilsequenz von Token in dieser Passage folgt.

208
00:11:31,940 --> 00:11:34,096
Bei dem Satz, auf den wir uns konzentriert haben, 

209
00:11:34,096 --> 00:11:36,512
könnte es zum Beispiel auch darum gehen, vorherzusagen, 

210
00:11:36,512 --> 00:11:39,100
welche Wörter auf creature und welche Wörter auf the folgen.

211
00:11:39,940 --> 00:11:42,537
Das ist wirklich gut, denn es bedeutet, dass ein 

212
00:11:42,537 --> 00:11:45,560
einziges Trainingsbeispiel effektiv als mehrere fungiert.

213
00:11:46,100 --> 00:11:50,027
Für unser Aufmerksamkeitsmuster bedeutet das, dass du niemals zulassen solltest, 

214
00:11:50,027 --> 00:11:54,051
dass spätere Wörter frühere Wörter beeinflussen, da sie sonst die Antwort auf das, 

215
00:11:54,051 --> 00:11:56,040
was als nächstes kommt, verraten könnten.

216
00:11:56,560 --> 00:12:00,999
Das bedeutet, dass wir wollen, dass alle diese Punkte hier, die für spätere Token stehen, 

217
00:12:00,999 --> 00:12:04,600
die frühere Token beeinflussen, irgendwie gezwungen werden, Null zu sein.

218
00:12:05,920 --> 00:12:08,520
Das Einfachste, was du tun könntest, wäre, sie gleich Null zu setzen, 

219
00:12:08,520 --> 00:12:11,342
aber wenn du das tust, würden sich die Spalten nicht mehr zu Eins addieren, 

220
00:12:11,342 --> 00:12:12,420
sie wären nicht normalisiert.

221
00:12:13,120 --> 00:12:15,943
Eine gängige Methode ist, dass du vor der Anwendung von 

222
00:12:15,943 --> 00:12:19,020
Softmax alle Einträge auf den negativen Wert Unendlich setzt.

223
00:12:19,680 --> 00:12:22,280
Wenn du das tust, werden nach der Anwendung von Softmax alle 

224
00:12:22,280 --> 00:12:25,180
diese Werte auf Null gesetzt, aber die Spalten bleiben normalisiert.

225
00:12:26,000 --> 00:12:27,540
Dieser Vorgang wird Maskierung genannt.

226
00:12:27,540 --> 00:12:30,996
Es gibt Versionen von Aufmerksamkeit, bei denen du sie nicht anwendest, 

227
00:12:30,996 --> 00:12:35,316
aber in unserem GPT-Beispiel, auch wenn dies in der Trainingsphase relevanter ist als z.B.

228
00:12:35,316 --> 00:12:38,436
 beim Einsatz als Chatbot, wendest du diese Maskierung immer an, 

229
00:12:38,436 --> 00:12:41,460
um zu verhindern, dass spätere Token die früheren beeinflussen.

230
00:12:42,480 --> 00:12:46,269
Eine weitere Tatsache, die es wert ist, über dieses Aufmerksamkeitsmuster nachzudenken, 

231
00:12:46,269 --> 00:12:49,500
ist die Tatsache, dass seine Größe gleich dem Quadrat der Kontextgröße ist.

232
00:12:49,900 --> 00:12:52,695
Das ist der Grund, warum die Kontextgröße ein großer Engpass für 

233
00:12:52,695 --> 00:12:55,620
große Sprachmodelle sein kann, und die Skalierung ist nicht trivial.

234
00:12:56,300 --> 00:13:00,183
Wie du dir vorstellen kannst, hat der Wunsch nach immer größeren Kontextfenstern in 

235
00:13:00,183 --> 00:13:04,159
den letzten Jahren dazu geführt, dass der Aufmerksamkeitsmechanismus verändert wurde, 

236
00:13:04,159 --> 00:13:08,320
um den Kontext skalierbarer zu machen, aber hier konzentrieren wir uns auf die Grundlagen.

237
00:13:10,560 --> 00:13:13,482
Okay, großartig. Durch die Berechnung dieses Musters kann das Modell ableiten, 

238
00:13:13,482 --> 00:13:15,480
welche Wörter für welche anderen Wörter relevant sind.

239
00:13:16,020 --> 00:13:18,202
Jetzt musst du die Einbettungen aktualisieren, 

240
00:13:18,202 --> 00:13:21,592
damit die Wörter Informationen an die anderen Wörter weitergeben können, 

241
00:13:21,592 --> 00:13:22,800
für die sie relevant sind.

242
00:13:22,800 --> 00:13:26,375
Du möchtest zum Beispiel, dass die Einbettung von Fluffy irgendwie eine 

243
00:13:26,375 --> 00:13:29,802
Veränderung an Creature bewirkt, die es in einen anderen Teil dieses 

244
00:13:29,802 --> 00:13:32,285
12.000-dimensionalen Einbettungsraums verschiebt, 

245
00:13:32,285 --> 00:13:34,520
der spezifischer eine Fluffy-Kreatur kodiert.

246
00:13:35,460 --> 00:13:39,114
Ich zeige dir hier zunächst die einfachste Art, wie du das machen kannst, 

247
00:13:39,114 --> 00:13:43,460
obwohl es im Zusammenhang mit der mehrköpfigen Aufmerksamkeit eine kleine Änderung gibt.

248
00:13:44,080 --> 00:13:46,882
Der einfachste Weg wäre, eine dritte Matrix zu verwenden, 

249
00:13:46,882 --> 00:13:50,700
die wir als Wertmatrix bezeichnen, die du mit der Einbettung des ersten Wortes 

250
00:13:50,700 --> 00:13:52,440
multiplizierst, zum Beispiel Fluffy.

251
00:13:53,300 --> 00:13:57,457
Das Ergebnis ist ein so genannter Wertvektor, den du zur Einbettung 

252
00:13:57,457 --> 00:14:01,920
des zweiten Wortes hinzufügst, in diesem Fall zur Einbettung von Kreatur.

253
00:14:02,600 --> 00:14:04,872
Dieser Wertvektor befindet sich also im selben 

254
00:14:04,872 --> 00:14:07,000
hochdimensionalen Raum wie die Einbettungen.

255
00:14:07,460 --> 00:14:11,109
Wenn du diese Wertmatrix mit der Einbettung eines Wortes multiplizierst, 

256
00:14:11,109 --> 00:14:14,510
könntest du sagen: Wenn dieses Wort für die Anpassung der Bedeutung 

257
00:14:14,510 --> 00:14:18,060
von etwas anderem relevant ist, was genau sollte zur Einbettung dieses 

258
00:14:18,060 --> 00:14:21,160
anderen Wortes hinzugefügt werden, um dies zu berücksichtigen?

259
00:14:22,140 --> 00:14:25,558
Wenn wir in unserem Diagramm zurückblicken, lassen wir alle Schlüssel 

260
00:14:25,558 --> 00:14:29,515
und Abfragen beiseite, denn nachdem du das Aufmerksamkeitsmuster berechnet hast, 

261
00:14:29,515 --> 00:14:32,934
nimmst du diese Wertmatrix und multiplizierst sie mit jeder einzelnen 

262
00:14:32,934 --> 00:14:36,060
dieser Einbettungen, um eine Folge von Wertvektoren zu erhalten.

263
00:14:37,120 --> 00:14:39,140
Du kannst dir vorstellen, dass diese Wertvektoren 

264
00:14:39,140 --> 00:14:41,120
mit den entsprechenden Schlüsseln verbunden sind.

265
00:14:42,320 --> 00:14:45,835
Für jede Spalte in diesem Diagramm multiplizierst du jeden der 

266
00:14:45,835 --> 00:14:49,240
Wertvektoren mit dem entsprechenden Gewicht in dieser Spalte.

267
00:14:50,080 --> 00:14:53,990
Hier zum Beispiel würdest du bei der Einbettung von Kreatur einen großen Teil 

268
00:14:53,990 --> 00:14:56,446
der Wertvektoren für Fluffy und Blue hinzufügen, 

269
00:14:56,446 --> 00:15:00,356
während alle anderen Wertvektoren auf Null gesetzt werden oder zumindest fast 

270
00:15:00,356 --> 00:15:01,560
auf Null gesetzt werden.

271
00:15:02,120 --> 00:15:05,441
Um die Einbettung dieser Spalte zu aktualisieren, 

272
00:15:05,441 --> 00:15:09,759
die zuvor eine kontextfreie Bedeutung von "Kreatur" kodiert hat, 

273
00:15:09,759 --> 00:15:15,406
addierst du alle neu skalierten Werte in der Spalte und ermittelst so eine Änderung, 

274
00:15:15,406 --> 00:15:19,260
die du hinzufügen möchtest, die ich als delta-e bezeichne.

275
00:15:19,680 --> 00:15:22,408
Das Ergebnis ist hoffentlich ein verfeinerter Vektor, 

276
00:15:22,408 --> 00:15:26,500
der die kontextreiche Bedeutung kodiert, wie die eines flauschigen blauen Wesens.

277
00:15:27,380 --> 00:15:31,400
Du wendest dieselbe gewichtete Summe auf alle Spalten in diesem Bild an und 

278
00:15:31,400 --> 00:15:35,472
erzeugst so eine Abfolge von Änderungen. Wenn du all diese Änderungen zu den 

279
00:15:35,472 --> 00:15:39,440
entsprechenden Einbettungen hinzufügst, entsteht eine vollständige Abfolge 

280
00:15:39,440 --> 00:15:43,460
von verfeinerten Einbettungen, die aus dem Aufmerksamkeitsblock herausragen.

281
00:15:44,860 --> 00:15:46,667
Wenn du herauszoomst, ist dieser ganze Prozess das, 

282
00:15:46,667 --> 00:15:49,100
was du als einen einzigen Kopf der Aufmerksamkeit beschreiben würdest.

283
00:15:49,600 --> 00:15:52,635
So wie ich es bisher beschrieben habe, wird dieser Prozess durch 

284
00:15:52,635 --> 00:15:55,904
drei verschiedene Matrizen parametrisiert, die alle mit einstellbaren 

285
00:15:55,904 --> 00:15:58,940
Parametern gefüllt sind: dem Schlüssel, der Abfrage und dem Wert.

286
00:15:59,500 --> 00:16:04,123
Ich möchte einen Moment damit fortfahren, was wir im letzten Kapitel begonnen haben, 

287
00:16:04,123 --> 00:16:08,040
nämlich mit der Zählung der Modellparameter anhand der Zahlen aus GPT-3.

288
00:16:09,300 --> 00:16:12,785
Diese Schlüssel- und Abfragematrizen haben jeweils 12.288 Spalten, 

289
00:16:12,785 --> 00:16:16,010
die der Dimension der Einbettung entsprechen, und 128 Zeilen, 

290
00:16:16,010 --> 00:16:19,600
die der Dimension des kleineren Schlüsselabfragebereichs entsprechen.

291
00:16:20,260 --> 00:16:24,220
Damit haben wir etwa 1,5 Millionen zusätzliche Parameter für jeden von ihnen.

292
00:16:24,860 --> 00:16:28,510
Wenn du dir dagegen die Wertmatrix ansiehst, würde die Art und Weise, 

293
00:16:28,510 --> 00:16:31,690
wie ich die Dinge bisher beschrieben habe, darauf hindeuten, 

294
00:16:31,690 --> 00:16:36,227
dass es sich um eine quadratische Matrix mit 12.288 Spalten und 12.288 Zeilen handelt, 

295
00:16:36,227 --> 00:16:40,920
da sowohl die Eingaben als auch die Ausgaben in diesem sehr großen Einbettungsraum liegen.

296
00:16:41,500 --> 00:16:45,140
Wenn das stimmt, würde das etwa 150 Millionen zusätzliche Parameter bedeuten.

297
00:16:45,660 --> 00:16:47,300
Und um das klarzustellen: Du könntest das tun.

298
00:16:47,420 --> 00:16:49,558
Du könntest der Value Map um Größenordnungen mehr 

299
00:16:49,558 --> 00:16:51,740
Parameter widmen als dem Schlüssel und der Abfrage.

300
00:16:52,060 --> 00:16:55,522
In der Praxis ist es aber viel effizienter, wenn du stattdessen dafür sorgst, 

301
00:16:55,522 --> 00:16:58,451
dass die Anzahl der Parameter für die Value Map gleich der Anzahl 

302
00:16:58,451 --> 00:17:00,760
der Parameter für den Schlüssel und die Abfrage ist.

303
00:17:01,460 --> 00:17:05,160
Dies ist besonders wichtig, wenn mehrere Aufmerksamkeitsköpfe parallel laufen.

304
00:17:06,240 --> 00:17:08,273
Das sieht so aus, dass die Wertkarte als Produkt 

305
00:17:08,273 --> 00:17:10,099
zweier kleinerer Matrizen faktorisiert wird.

306
00:17:11,180 --> 00:17:13,898
Konzeptionell würde ich dich immer noch dazu ermutigen, 

307
00:17:13,898 --> 00:17:17,490
über die gesamte lineare Karte nachzudenken, eine mit Inputs und Outputs, 

308
00:17:17,490 --> 00:17:19,771
beide in diesem größeren Einbettungsraum, z.B. 

309
00:17:19,771 --> 00:17:23,800
die Einbettung von Blau in diese Blau-Richtung, die du zu Nomen hinzufügen würdest.

310
00:17:27,040 --> 00:17:29,548
Es handelt sich nur um eine geringere Anzahl von Zeilen, 

311
00:17:29,548 --> 00:17:32,760
die in der Regel genauso groß ist wie der Platz für die Schlüsselabfrage.

312
00:17:33,100 --> 00:17:35,958
Das bedeutet, dass du die großen Einbettungsvektoren 

313
00:17:35,958 --> 00:17:38,440
auf einen viel kleineren Raum abbilden kannst.

314
00:17:39,040 --> 00:17:42,700
Das ist zwar nicht die übliche Bezeichnung, aber ich nenne das mal die Value Down Matrix.

315
00:17:43,400 --> 00:17:46,925
Die zweite Matrix bildet von diesem kleineren Raum zurück auf den Einbettungsraum 

316
00:17:46,925 --> 00:17:50,580
ab und erzeugt die Vektoren, die du für die eigentlichen Aktualisierungen verwendest.

317
00:17:51,000 --> 00:17:54,740
Ich nenne das hier die Value-Up-Matrix, die wiederum nicht konventionell ist.

318
00:17:55,160 --> 00:17:57,195
Die Art und Weise, wie du das in den meisten Zeitungen lesen kannst, 

319
00:17:57,195 --> 00:17:58,080
sieht ein bisschen anders aus.

320
00:17:58,380 --> 00:17:59,520
Ich werde gleich darüber sprechen.

321
00:17:59,700 --> 00:18:02,540
Meiner Meinung nach macht es die Dinge konzeptionell ein wenig verwirrender.

322
00:18:03,260 --> 00:18:06,959
Um es mit dem Jargon der linearen Algebra auszudrücken: Im Grunde geht es darum, 

323
00:18:06,959 --> 00:18:10,340
dass die gesamte Wertkarte eine Transformation niedrigen Ranges sein muss.

324
00:18:11,420 --> 00:18:13,700
Um auf die Anzahl der Parameter zurückzukommen: 

325
00:18:13,700 --> 00:18:17,264
Alle vier Matrizen sind gleich groß, und wenn wir sie alle zusammenzählen, 

326
00:18:17,264 --> 00:18:20,780
kommen wir auf etwa 6,3 Millionen Parameter für einen Aufmerksamkeitskopf.

327
00:18:22,040 --> 00:18:25,037
Eine kurze Anmerkung am Rande: Alles, was bisher beschrieben wurde, ist das, 

328
00:18:25,037 --> 00:18:27,217
was man als Selbstaufmerksamkeitskopf bezeichnen würde, 

329
00:18:27,217 --> 00:18:30,371
um es von einer Variante zu unterscheiden, die in anderen Modellen auftaucht und 

330
00:18:30,371 --> 00:18:31,500
Cross-Attention genannt wird.

331
00:18:32,300 --> 00:18:36,314
Das ist für unser GPT-Beispiel zwar nicht relevant, aber falls du neugierig bist: 

332
00:18:36,314 --> 00:18:40,574
Cross-Attention beinhaltet Modelle, die zwei verschiedene Arten von Daten verarbeiten, 

333
00:18:40,574 --> 00:18:43,658
z. B. Text in einer Sprache und Text in einer anderen Sprache, 

334
00:18:43,658 --> 00:18:47,869
der Teil einer laufenden Übersetzung ist, oder vielleicht Audio-Input von Sprache und 

335
00:18:47,869 --> 00:18:49,240
eine laufende Transkription.

336
00:18:50,400 --> 00:18:52,700
Ein Kreuzanschlagskopf sieht fast genauso aus.

337
00:18:52,980 --> 00:18:55,121
Der einzige Unterschied ist, dass die Key- und 

338
00:18:55,121 --> 00:18:57,400
Query-Maps auf unterschiedliche Datensätze wirken.

339
00:18:57,840 --> 00:19:01,869
In einem Übersetzungsmodell könnten zum Beispiel die Schlüssel aus einer Sprache stammen, 

340
00:19:01,869 --> 00:19:03,973
während die Abfragen aus einer anderen kommen, 

341
00:19:03,973 --> 00:19:06,212
und das Aufmerksamkeitsmuster könnte beschreiben, 

342
00:19:06,212 --> 00:19:09,660
welche Wörter aus einer Sprache welchen Wörtern in einer anderen entsprechen.

343
00:19:10,340 --> 00:19:13,149
Und in diesem Fall gibt es normalerweise keine Maskierung, 

344
00:19:13,149 --> 00:19:16,340
da spätere Token keine Auswirkungen auf frühere Token haben können.

345
00:19:17,180 --> 00:19:21,085
Wenn du dich aber auf die Selbstaufmerksamkeit konzentrierst und alles bis hierher 

346
00:19:21,085 --> 00:19:25,180
verstanden hast, wirst du die Essenz dessen, was Aufmerksamkeit wirklich ist, erkennen.

347
00:19:25,760 --> 00:19:29,179
Alles, was uns wirklich bleibt, ist, den Sinn zu erklären, 

348
00:19:29,179 --> 00:19:31,440
in dem du das viele, viele Male machst.

349
00:19:32,100 --> 00:19:34,608
In unserem zentralen Beispiel haben wir uns auf Adjektive konzentriert, 

350
00:19:34,608 --> 00:19:37,674
die Substantive aktualisieren, aber natürlich gibt es viele verschiedene Möglichkeiten, 

351
00:19:37,674 --> 00:19:39,800
wie der Kontext die Bedeutung eines Wortes beeinflussen kann.

352
00:19:40,360 --> 00:19:43,993
Wenn die Wörter, mit denen sie zusammengestoßen sind, dem Wort Auto vorausgingen, 

353
00:19:43,993 --> 00:19:46,520
hat das Auswirkungen auf die Form und Struktur des Autos.

354
00:19:47,200 --> 00:19:49,280
Und viele Assoziationen sind vielleicht weniger grammatikalisch.

355
00:19:49,760 --> 00:19:52,956
Wenn das Wort Zauberer irgendwo in der gleichen Passage wie Harry vorkommt, 

356
00:19:52,956 --> 00:19:55,817
deutet das darauf hin, dass es sich um Harry Potter handeln könnte, 

357
00:19:55,817 --> 00:19:59,266
während wenn stattdessen die Wörter Königin, Sussex und William in dieser Passage 

358
00:19:59,266 --> 00:20:02,925
vorkommen würden, dann sollte die Einbettung von Harry vielleicht aktualisiert werden, 

359
00:20:02,925 --> 00:20:04,440
um sich auf den Prinzen zu beziehen.

360
00:20:05,040 --> 00:20:08,468
Für jede Art von kontextbezogener Aktualisierung, die du dir vorstellen kannst, 

361
00:20:08,468 --> 00:20:11,940
würden die Parameter dieser Schlüssel- und Abfragematrizen unterschiedlich sein, 

362
00:20:11,940 --> 00:20:14,340
um die verschiedenen Aufmerksamkeitsmuster zu erfassen, 

363
00:20:14,340 --> 00:20:17,082
und die Parameter unserer Value Map würden sich danach richten, 

364
00:20:17,082 --> 00:20:19,140
was zu den Einbettungen hinzugefügt werden soll.

365
00:20:19,980 --> 00:20:23,256
In der Praxis ist das tatsächliche Verhalten dieser Karten viel schwieriger zu 

366
00:20:23,256 --> 00:20:26,117
interpretieren, da die Gewichte so gesetzt werden, dass sie das tun, 

367
00:20:26,117 --> 00:20:29,186
was das Modell braucht, um sein Ziel, die Vorhersage des nächsten Tokens, 

368
00:20:29,186 --> 00:20:30,140
am besten zu erreichen.

369
00:20:31,400 --> 00:20:34,077
Wie ich schon sagte, ist alles, was wir beschrieben haben, 

370
00:20:34,077 --> 00:20:37,752
ein einzelner Aufmerksamkeitsblock. Ein kompletter Aufmerksamkeitsblock in einem 

371
00:20:37,752 --> 00:20:40,747
Transformator besteht aus einer sogenannten Multi-Head-Attention, 

372
00:20:40,747 --> 00:20:43,379
bei der du viele dieser Operationen parallel durchführst, 

373
00:20:43,379 --> 00:20:45,920
jede mit ihren eigenen Schlüsselabfragen und Value Maps.

374
00:20:47,420 --> 00:20:51,700
GPT-3 verwendet zum Beispiel 96 Aufmerksamkeitsköpfe in jedem Block.

375
00:20:52,020 --> 00:20:54,453
Wenn man bedenkt, dass jeder von ihnen schon ein bisschen verwirrend ist, 

376
00:20:54,453 --> 00:20:56,460
ist das sicherlich eine Menge, die man im Kopf behalten muss.

377
00:20:56,760 --> 00:21:00,825
Um es ganz klar zu sagen, bedeutet das, dass du 96 verschiedene Schlüssel- 

378
00:21:00,825 --> 00:21:05,000
und Abfragematrizen hast, die 96 verschiedene Aufmerksamkeitsmuster erzeugen.

379
00:21:05,440 --> 00:21:08,328
Dann hat jeder Kopf seine eigenen Wertmatrizen, 

380
00:21:08,328 --> 00:21:12,180
die verwendet werden, um 96 Folgen von Wertvektoren zu erzeugen.

381
00:21:12,460 --> 00:21:14,463
Diese werden addiert, wobei die entsprechenden 

382
00:21:14,463 --> 00:21:16,680
Aufmerksamkeitsmuster als Gewichte verwendet werden.

383
00:21:17,480 --> 00:21:20,979
Das bedeutet, dass für jede Position im Kontext, jedes Token, 

384
00:21:20,979 --> 00:21:25,665
jeder dieser Köpfe eine Änderung vorschlägt, die der Einbettung an dieser Position 

385
00:21:25,665 --> 00:21:27,020
hinzugefügt werden soll.

386
00:21:27,660 --> 00:21:31,696
Du addierst also alle vorgeschlagenen Änderungen zusammen, eine für jeden Kopf, 

387
00:21:31,696 --> 00:21:35,480
und fügst das Ergebnis der ursprünglichen Einbettung dieser Position hinzu.

388
00:21:36,660 --> 00:21:40,052
Diese ganze Summe hier wäre ein Teil dessen, was von diesem 

389
00:21:40,052 --> 00:21:42,936
mehrköpfigen Aufmerksamkeitsblock ausgegeben wird, 

390
00:21:42,936 --> 00:21:47,460
eine einzelne dieser verfeinerten Einbettungen, die am anderen Ende herauskommt.

391
00:21:48,320 --> 00:21:50,357
Auch hier gibt es viel zu bedenken, also mach dir keine Sorgen, 

392
00:21:50,357 --> 00:21:52,140
wenn es eine Weile dauert, bis du es verinnerlicht hast.

393
00:21:52,380 --> 00:21:55,381
Der Grundgedanke ist, dass du dem Modell die Fähigkeit gibst, 

394
00:21:55,381 --> 00:21:59,109
viele verschiedene Arten zu lernen, wie der Kontext die Bedeutung verändert, 

395
00:21:59,109 --> 00:22:01,820
indem du viele verschiedene Köpfe parallel laufen lässt.

396
00:22:03,700 --> 00:22:07,653
Wenn wir unsere laufende Zählung der Parameter mit 96 Köpfen durchführen, 

397
00:22:07,653 --> 00:22:11,286
von denen jeder seine eigene Variante dieser vier Matrizen enthält, 

398
00:22:11,286 --> 00:22:15,080
kommt jeder Block mit mehreren Köpfen auf rund 600 Millionen Parameter.

399
00:22:16,420 --> 00:22:18,452
Es gibt noch eine weitere, etwas ärgerliche Sache, 

400
00:22:18,452 --> 00:22:21,800
die ich unbedingt erwähnen sollte, wenn ihr noch mehr über Transformers lesen wollt.

401
00:22:22,080 --> 00:22:24,012
Du erinnerst dich daran, dass ich gesagt habe, 

402
00:22:24,012 --> 00:22:26,726
dass die Wertekarte in zwei verschiedene Matrizen aufgeteilt ist, 

403
00:22:26,726 --> 00:22:29,440
die ich als "Value Down"- und "Value Up"-Matrizen bezeichnet habe.

404
00:22:29,960 --> 00:22:33,091
Die Art und Weise, wie ich die Dinge formuliert habe, legt nahe, 

405
00:22:33,091 --> 00:22:36,223
dass du dieses Matrizenpaar in jedem Aufmerksamkeitskopf siehst, 

406
00:22:36,223 --> 00:22:38,440
und du könntest es auf jeden Fall so umsetzen.

407
00:22:38,640 --> 00:22:39,920
Das wäre ein gültiges Design.

408
00:22:40,260 --> 00:22:42,571
Aber die Art und Weise, wie das in den Papieren steht und wie 

409
00:22:42,571 --> 00:22:44,920
es in der Praxis umgesetzt wird, sieht ein bisschen anders aus.

410
00:22:45,340 --> 00:22:49,145
Alle diese Aufwertungsmatrizen für jeden Kopf werden in einer riesigen 

411
00:22:49,145 --> 00:22:52,682
Matrix zusammengefasst, die wir als Ausgangsmatrix bezeichnen und 

412
00:22:52,682 --> 00:22:56,380
die mit dem gesamten mehrköpfigen Aufmerksamkeitsblock verbunden ist.

413
00:22:56,820 --> 00:23:00,918
Wenn man sich auf die Wertmatrix für einen bestimmten Aufmerksamkeitskopf bezieht, 

414
00:23:00,918 --> 00:23:03,387
meint man in der Regel nur diesen ersten Schritt, 

415
00:23:03,387 --> 00:23:07,140
den ich als die Projektion des Wertes in den kleineren Raum bezeichnet habe.

416
00:23:08,340 --> 00:23:11,040
Für die Neugierigen unter euch habe ich einen Hinweis auf dem Bildschirm hinterlassen.

417
00:23:11,260 --> 00:23:14,445
Das ist eines dieser Details, die vom eigentlichen Konzept ablenken könnten, 

418
00:23:14,445 --> 00:23:16,802
aber ich möchte es trotzdem erwähnen, damit du es weißt, 

419
00:23:16,802 --> 00:23:18,540
falls du in anderen Quellen darüber liest.

420
00:23:19,240 --> 00:23:22,117
Abgesehen von allen technischen Feinheiten haben wir in der Vorschau 

421
00:23:22,117 --> 00:23:25,495
des letzten Kapitels gesehen, dass Daten, die durch einen Transformator fließen, 

422
00:23:25,495 --> 00:23:28,040
nicht nur durch einen einzelnen Aufmerksamkeitsblock fließen.

423
00:23:28,640 --> 00:23:30,706
Zum einen durchläuft er auch diese anderen Operationen, 

424
00:23:30,706 --> 00:23:32,700
die als mehrschichtige Perzeptronen bezeichnet werden.

425
00:23:33,120 --> 00:23:34,880
Wir werden im nächsten Kapitel mehr darüber sprechen.

426
00:23:35,180 --> 00:23:39,320
Und dann durchläuft es viele, viele Kopien dieser beiden Vorgänge.

427
00:23:39,980 --> 00:23:43,519
Das bedeutet, dass, nachdem ein bestimmtes Wort einen Teil seines Kontextes 

428
00:23:43,519 --> 00:23:46,034
aufgenommen hat, es viele weitere Möglichkeiten gibt, 

429
00:23:46,034 --> 00:23:50,040
dass diese nuanciertere Einbettung von seiner nuancierteren Umgebung beeinflusst wird.

430
00:23:50,940 --> 00:23:55,046
Je weiter das Netzwerk nach unten geht und jede Einbettung mehr und mehr Bedeutung aus 

431
00:23:55,046 --> 00:23:58,634
den anderen Einbettungen aufnimmt, die ihrerseits immer nuancierter werden, 

432
00:23:58,634 --> 00:24:01,088
desto größer ist die Hoffnung, dass es möglich ist, 

433
00:24:01,088 --> 00:24:05,148
über die Deskriptoren und die grammatikalische Struktur hinaus höhere und abstraktere 

434
00:24:05,148 --> 00:24:07,320
Ideen über einen bestimmten Input zu kodieren.

435
00:24:07,880 --> 00:24:11,376
Dinge wie Stimmung und Tonfall und ob es ein Gedicht ist und welche 

436
00:24:11,376 --> 00:24:15,130
wissenschaftlichen Wahrheiten dem Stück zugrunde liegen und solche Dinge.

437
00:24:16,700 --> 00:24:20,090
Um noch einmal auf unsere Punktezählung zurückzukommen: 

438
00:24:20,090 --> 00:24:24,146
GPT-3 umfasst 96 verschiedene Ebenen, also wird die Gesamtzahl der 

439
00:24:24,146 --> 00:24:28,203
Schlüsselabfrage- und Wertparameter mit weiteren 96 multipliziert, 

440
00:24:28,203 --> 00:24:32,502
was die Gesamtsumme auf knapp 58 Milliarden verschiedene Parameter für 

441
00:24:32,502 --> 00:24:34,500
alle Aufmerksamkeitsköpfe bringt.

442
00:24:34,980 --> 00:24:39,297
Das ist zwar eine Menge, aber nur etwa ein Drittel der 175 Milliarden, 

443
00:24:39,297 --> 00:24:40,940
die insgesamt im Netz sind.

444
00:24:41,520 --> 00:24:44,376
Auch wenn die Aufmerksamkeit die ganze Aufmerksamkeit bekommt, 

445
00:24:44,376 --> 00:24:48,140
kommen die meisten Parameter aus den Blöcken, die zwischen diesen Schritten liegen.

446
00:24:48,560 --> 00:24:51,040
Im nächsten Kapitel werden du und ich mehr über diese anderen 

447
00:24:51,040 --> 00:24:53,560
Blöcke und auch viel mehr über den Ausbildungsprozess sprechen.

448
00:24:54,120 --> 00:24:57,750
Ein großer Teil des Erfolgs des Aufmerksamkeitsmechanismus ist nicht 

449
00:24:57,750 --> 00:25:01,697
so sehr ein bestimmtes Verhalten, das er ermöglicht, sondern die Tatsache, 

450
00:25:01,697 --> 00:25:05,117
dass er extrem parallelisierbar ist, d.h. dass man mit GPUs eine 

451
00:25:05,117 --> 00:25:08,380
große Anzahl von Berechnungen in kurzer Zeit durchführen kann.

452
00:25:09,460 --> 00:25:13,297
Da eine der wichtigsten Erkenntnisse über Deep Learning in den letzten zehn Jahren darin 

453
00:25:13,297 --> 00:25:16,704
besteht, dass allein die Skalierung zu enormen qualitativen Verbesserungen der 

454
00:25:16,704 --> 00:25:20,240
Modellleistung führt, sind parallelisierbare Architekturen, die dies ermöglichen, 

455
00:25:20,240 --> 00:25:21,060
von großem Vorteil.

456
00:25:22,040 --> 00:25:23,622
Wenn du mehr über diese Dinge erfahren willst, 

457
00:25:23,622 --> 00:25:25,340
habe ich in der Beschreibung viele Links angegeben.

458
00:25:25,920 --> 00:25:29,119
Vor allem alles, was von Andrej Karpathy oder Chris Ola produziert wird, 

459
00:25:29,119 --> 00:25:30,040
ist meist pures Gold.

460
00:25:30,560 --> 00:25:33,747
In diesem Video möchte ich nur auf die Aufmerksamkeit in ihrer jetzigen Form eingehen, 

461
00:25:33,747 --> 00:25:35,652
aber wenn du dich für die Geschichte interessierst, 

462
00:25:35,652 --> 00:25:38,656
wie wir hierher gekommen sind und wie du diese Idee für dich neu erfinden kannst, 

463
00:25:38,656 --> 00:25:40,891
hat mein Freund Vivek gerade ein paar Videos veröffentlicht, 

464
00:25:40,891 --> 00:25:42,540
in denen er mehr über die Motivation erzählt.

465
00:25:43,120 --> 00:25:45,645
Außerdem hat Britt Cruz vom Kanal The Art of the Problem ein 

466
00:25:45,645 --> 00:25:48,460
wirklich schönes Video über die Geschichte der großen Sprachmodelle.

467
00:26:04,960 --> 00:26:09,200
Vielen Dank!

