[
 {
  "translatedText": "No último capítulo, você e eu começamos a examinar o funcionamento interno de um transformador.",
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "translatedText": "Esta é uma das principais peças de tecnologia dentro de grandes modelos de linguagem e de muitas outras ferramentas na onda moderna de IA.",
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "translatedText": "Ele apareceu pela primeira vez em um artigo agora famoso de 2017 chamado Atenção é tudo que você precisa, e neste capítulo você e eu vamos nos aprofundar no que é esse mecanismo de atenção, visualizando como ele processa os dados.",
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "translatedText": "Para recapitular rapidamente, aqui está o contexto importante que quero que você tenha em mente.",
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "translatedText": "O objetivo do modelo que você e eu estamos estudando é pegar um trecho de texto e prever qual palavra virá a seguir.",
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "translatedText": "O texto de entrada é dividido em pequenos pedaços que chamamos de tokens, e muitas vezes são palavras ou pedaços de palavras, mas apenas para tornar os exemplos deste vídeo mais fáceis para você e para mim, vamos simplificar fingindo que os tokens são sempre apenas palavras.",
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "translatedText": "O primeiro passo em um transformador é associar cada token a um vetor de alta dimensão, o que chamamos de incorporação.",
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "translatedText": "A ideia mais importante que quero que você tenha em mente é como as direções neste espaço de alta dimensão de todas as incorporações possíveis podem corresponder ao significado semântico.",
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "translatedText": "No último capítulo vimos um exemplo de como a direção pode corresponder ao gênero, no sentido de que adicionar um determinado passo neste espaço pode levá-lo da incorporação de um substantivo masculino à incorporação do substantivo feminino correspondente.",
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "translatedText": "Esse é apenas um exemplo: você poderia imaginar quantas outras direções neste espaço de alta dimensão poderiam corresponder a vários outros aspectos do significado de uma palavra.",
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "translatedText": "O objetivo de um transformador é ajustar progressivamente essas incorporações para que elas não apenas codifiquem uma palavra individual, mas, em vez disso, incluam um significado contextual muito, muito mais rico.",
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "translatedText": "Devo dizer desde já que muitas pessoas acham o mecanismo de atenção, essa peça-chave de um transformador, muito confuso, então não se preocupe se levar algum tempo para que as coisas sejam absorvidas.",
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "translatedText": "Acho que antes de mergulharmos nos detalhes computacionais e em todas as multiplicações de matrizes, vale a pena pensar em alguns exemplos do tipo de comportamento que queremos que a atenção possibilite.",
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "translatedText": "Considere as frases toupeira verdadeira americana, um mol de dióxido de carbono, e faça uma biópsia da toupeira.",
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "translatedText": "Você e eu sabemos que a palavra toupeira tem significados diferentes em cada um deles, com base no contexto.",
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "translatedText": "Mas após a primeira etapa de um transformador, aquele que divide o texto e associa cada token a um vetor, o vetor associado ao mol seria o mesmo em todos esses casos, porque essa incorporação inicial do token é efetivamente uma tabela de consulta sem referência ao contexto.",
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "translatedText": "É somente na próxima etapa do transformador que os embeddings circundantes têm a chance de passar informações para este.",
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "translatedText": "A imagem que você pode ter em mente é que existem múltiplas direções distintas neste espaço de incorporação, codificando os múltiplos significados distintos da palavra toupeira, e que um bloco de atenção bem treinado calcula o que você precisa adicionar à incorporação genérica para movê-la para uma dessas direções específicas, em função do contexto.",
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "translatedText": "Para dar outro exemplo, considere a incorporação da palavra torre.",
  "input": "To take another example, consider the embedding of the word tower.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "translatedText": "Presumivelmente, esta é uma direção muito genérica e não específica no espaço, associada a muitos outros substantivos grandes e altos.",
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "translatedText": "Se esta palavra fosse imediatamente precedida por Eiffel, você poderia imaginar querer que o mecanismo atualizasse esse vetor para que ele aponte em uma direção que codifique mais especificamente a Torre Eiffel, talvez correlacionada com vetores associados a Paris e França e coisas feitas de aço.",
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "translatedText": "Se também fosse precedido pela palavra miniatura, então o vetor deveria ser atualizado ainda mais, para que não se correlacione mais com coisas grandes e altas.",
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "translatedText": "De forma mais geral do que apenas refinar o significado de uma palavra, o bloco de atenção permite que o modelo mova informações codificadas em uma incorporação para outra, potencialmente aquelas que estão muito distantes e potencialmente com informações muito mais ricas do que apenas uma única palavra.",
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "translatedText": "O que vimos no último capítulo foi como, depois de todos os vetores fluírem pela rede, incluindo muitos blocos de atenção diferentes, o cálculo realizado para produzir uma previsão do próximo token é inteiramente uma função do último vetor na sequência.",
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "translatedText": "Imagine, por exemplo, que o texto que você insere é a maior parte de um romance de mistério inteiro, até um ponto próximo ao final, onde se lê: portanto, o assassino foi.",
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "translatedText": "Se o modelo for prever com precisão a próxima palavra, o vetor final da sequência, que começou sua vida simplesmente incorporando a palavra was, terá que ter sido atualizado por todos os blocos de atenção para representar muito, muito mais do que qualquer indivíduo. palavra, codificando de alguma forma todas as informações da janela de contexto completa que são relevantes para prever a próxima palavra.",
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "translatedText": "Para avançar nos cálculos, porém, vamos dar um exemplo muito mais simples.",
  "input": "To step through the computations, though, let's take a much simpler example.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "translatedText": "Imagine que a entrada inclui a frase, uma criatura azul fofa vagava pela floresta verdejante.",
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "translatedText": "E, por enquanto, suponha que o único tipo de atualização com o qual nos importamos é fazer com que os adjetivos ajustem os significados de seus substantivos correspondentes.",
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "translatedText": "O que estou prestes a descrever é o que chamaríamos de uma única cabeça de atenção, e mais tarde veremos como o bloco de atenção consiste em muitas cabeças diferentes que correm em paralelo.",
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "translatedText": "Novamente, a incorporação inicial de cada palavra é algum vetor de alta dimensão que codifica apenas o significado daquela palavra específica, sem contexto.",
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "translatedText": "Na verdade, isso não é bem verdade.",
  "input": "Actually, that's not quite true.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "translatedText": "Eles também codificam a posição da palavra.",
  "input": "They also encode the position of the word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "translatedText": "Há muito mais a dizer sobre a forma como as posições são codificadas, mas agora, tudo o que você precisa saber é que as entradas desse vetor são suficientes para dizer o que é a palavra e onde ela existe no contexto.",
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "translatedText": "Vamos em frente e denotaremos esses embeddings com a letra e.",
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "translatedText": "O objetivo é fazer com que uma série de cálculos produza um novo conjunto refinado de embeddings onde, por exemplo, aqueles correspondentes aos substantivos ingeriram o significado de seus adjetivos correspondentes.",
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "translatedText": "E jogando o jogo do aprendizado profundo, queremos que a maioria dos cálculos envolvidos se pareçam com produtos de matrizes-vetores, onde as matrizes estão cheias de pesos ajustáveis, coisas que o modelo aprenderá com base nos dados.",
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "translatedText": "Para ser claro, estou inventando este exemplo de adjetivos atualizando substantivos apenas para ilustrar o tipo de comportamento que você poderia imaginar uma cabeça de atenção fazendo.",
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "translatedText": "Tal como acontece com tanto aprendizado profundo, o verdadeiro comportamento é muito mais difícil de analisar porque se baseia em ajustes e ajustes de um grande número de parâmetros para minimizar alguma função de custo.",
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "translatedText": "Acontece que, à medida que percorremos todas as diferentes matrizes preenchidas com parâmetros envolvidos neste processo, acho que é realmente útil ter um exemplo imaginado de algo que poderia ser feito para ajudar a manter tudo mais concreto.",
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "translatedText": "Para a primeira etapa deste processo, você pode imaginar cada substantivo, como criatura, fazendo a pergunta: ei, há algum adjetivo na minha frente?",
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "translatedText": "E para as palavras fofo e azul, para que cada um possa responder, sim, sou um adjetivo e estou nessa posição.",
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "translatedText": "Essa questão está de alguma forma codificada como mais um vetor, outra lista de números, que chamamos de consulta desta palavra.",
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "translatedText": "Este vetor de consulta, porém, tem uma dimensão muito menor que o vetor de incorporação, digamos 128.",
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "translatedText": "Calcular esta consulta parece pegar uma determinada matriz, que rotularei de wq, e multiplicá-la pela incorporação.",
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "translatedText": "Comprimindo um pouco as coisas, vamos escrever esse vetor de consulta como q, e então sempre que você me ver colocando uma matriz ao lado de uma seta como esta, ela significa que multiplicar essa matriz pelo vetor no início da seta fornece o vetor em a ponta da flecha.",
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "translatedText": "Nesse caso, você multiplica essa matriz por todos os embeddings no contexto, produzindo um vetor de consulta para cada token.",
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "translatedText": "As entradas desta matriz são parâmetros do modelo, o que significa que o verdadeiro comportamento é aprendido a partir dos dados e, na prática, o que esta matriz faz em uma determinada cabeça de atenção é difícil de analisar.",
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "translatedText": "Mas, para nosso bem, imaginando um exemplo que esperamos que aprenda, vamos supor que esta matriz de consulta mapeia os embeddings de substantivos para certas direções neste espaço de consulta menor que de alguma forma codifica a noção de procurar adjetivos em posições anteriores .",
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "translatedText": "Quanto ao que isso faz com outras incorporações, quem sabe?",
  "input": "As to what it does to other embeddings, who knows?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "translatedText": "Talvez tente simultaneamente atingir algum outro objetivo com eles.",
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "translatedText": "No momento, estamos focados nos substantivos.",
  "input": "Right now, we're laser focused on the nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "translatedText": "Ao mesmo tempo, associada a isso está uma segunda matriz chamada matriz chave, que você também multiplica por cada um dos embeddings.",
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "translatedText": "Isto produz uma segunda sequência de vetores que chamamos de chaves.",
  "input": "This produces a second sequence of vectors that we call the keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "translatedText": "Conceitualmente, você deseja pensar nas chaves como potencialmente respondendo às perguntas.",
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "translatedText": "Essa matriz chave também está repleta de parâmetros ajustáveis e, assim como a matriz de consulta, mapeia os vetores de incorporação para o mesmo espaço dimensional menor.",
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "translatedText": "Você pensa nas chaves como correspondendo às consultas sempre que elas se alinham umas com as outras.",
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "translatedText": "Em nosso exemplo, você imaginaria que a matriz chave mapeia os adjetivos como fofo e azul para vetores que estão intimamente alinhados com a consulta produzida pela palavra criatura.",
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "translatedText": "Para medir o quão bem cada chave corresponde a cada consulta, você calcula um produto escalar entre cada par de consulta-chave possível.",
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "translatedText": "Gosto de visualizar uma grade cheia de pontos, onde os pontos maiores correspondem aos produtos escalares maiores, os locais onde as chaves e as consultas se alinham.",
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "translatedText": "Para nosso exemplo de adjetivo e substantivo, seria um pouco mais parecido com isto, onde se as chaves produzidas por fofo e azul realmente se alinham estreitamente com a consulta produzida por criatura, então os produtos escalares nesses dois pontos seriam alguns grandes números positivos.",
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "translatedText": "No jargão, o pessoal do aprendizado de máquina diria que isso significa que as incorporações de fofo e azul atendem à incorporação da criatura.",
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "translatedText": "Em contraste com o produto escalar entre a chave para alguma outra palavra como o e a consulta para criatura, haveria algum valor pequeno ou negativo que reflete que não estão relacionados entre si.",
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "translatedText": "Portanto, temos esta grelha de valores que pode ser qualquer número real desde infinito negativo até infinito, dando-nos uma pontuação da relevância de cada palavra para atualizar o significado de todas as outras palavras.",
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "translatedText": "A maneira como usaremos essas pontuações é calcular uma certa soma ponderada em cada coluna, ponderada pela relevância.",
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "translatedText": "Então, em vez de os valores variarem de infinito negativo a infinito, o que queremos é que os números nessas colunas estejam entre 0 e 1, e que cada coluna some 1, como se fossem uma distribuição de probabilidade.",
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "translatedText": "Se você está vindo do último capítulo, sabe o que precisamos fazer então.",
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "translatedText": "Calculamos um softmax ao longo de cada uma dessas colunas para normalizar os valores.",
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "translatedText": "Na nossa imagem, depois de aplicar softmax a todas as colunas, preencheremos a grade com esses valores normalizados.",
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "translatedText": "Neste ponto, você pode pensar em cada coluna dando pesos de acordo com a relevância da palavra à esquerda em relação ao valor correspondente no topo.",
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "translatedText": "Chamamos essa grade de padrão de atenção.",
  "input": "We call this grid an attention pattern.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "translatedText": "Agora, se você olhar para o papel original do transformador, há uma maneira bem compacta de eles escreverem tudo isso.",
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "translatedText": "Aqui, as variáveis q e k representam as matrizes completas dos vetores de consulta e de chave, respectivamente, aqueles pequenos vetores que você obtém multiplicando os embeddings pela consulta e pelas matrizes de chave.",
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "translatedText": "Esta expressão no numerador é uma forma realmente compacta de representar a grade de todos os produtos escalares possíveis entre pares de chaves e consultas.",
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "translatedText": "Um pequeno detalhe técnico que não mencionei é que, para estabilidade numérica, é útil dividir todos esses valores pela raiz quadrada da dimensão nesse espaço de consulta chave.",
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "translatedText": "Então, esse softmax que envolve a expressão completa deve ser entendido como aplicado coluna por coluna.",
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "translatedText": "Quanto ao termo v, falaremos sobre isso em um segundo.",
  "input": "As to that v term, we'll talk about it in just a second.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "translatedText": "Antes disso, há outro detalhe técnico que até agora pulei.",
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "translatedText": "Durante o processo de treinamento, quando você executa este modelo em um determinado exemplo de texto, e todos os pesos são ligeiramente ajustados e ajustados para recompensá-lo ou puni-lo com base na probabilidade elevada que ele atribui à próxima palavra verdadeira na passagem, ele acaba tornando todo o processo de treinamento muito mais eficiente se você fizer com que ele preveja simultaneamente todos os próximos tokens possíveis após cada subsequência inicial de tokens nesta passagem.",
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "translatedText": "Por exemplo, com a frase que estamos focando, ela também pode prever quais palavras seguem a criatura e quais palavras seguem a.",
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "translatedText": "Isso é muito bom, porque significa que o que de outra forma seria um único exemplo de treinamento efetivamente funciona como muitos.",
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "translatedText": "Para os propósitos do nosso padrão de atenção, isso significa que você nunca deve permitir que palavras posteriores influenciem palavras anteriores, pois, caso contrário, elas poderiam revelar a resposta para o que vem a seguir.",
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "translatedText": "O que isso significa é que queremos que todos esses pontos aqui, aqueles que representam os tokens posteriores que influenciam os anteriores, sejam de alguma forma forçados a zero.",
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "translatedText": "A coisa mais simples que você pode pensar em fazer é defini-los iguais a zero, mas se você fizesse isso as colunas não somariam mais um, elas não seriam normalizadas.",
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "translatedText": "Então, em vez disso, uma maneira comum de fazer isso é antes de aplicar softmax, você define todas essas entradas como infinito negativo.",
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "translatedText": "Se você fizer isso, depois de aplicar o softmax, todos serão transformados em zero, mas as colunas permanecerão normalizadas.",
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "translatedText": "Este processo é chamado de mascaramento.",
  "input": "This process is called masking.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "translatedText": "Existem versões de atenção onde você não aplica, mas em nosso exemplo GPT, mesmo que isso seja mais relevante durante a fase de treinamento do que seria, digamos, executá-lo como um chatbot ou algo parecido, você sempre aplica esse mascaramento para evitar que tokens posteriores influenciem os anteriores.",
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "translatedText": "Outro fato que vale a pena refletir sobre esse padrão de atenção é como seu tamanho é igual ao quadrado do tamanho do contexto.",
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "translatedText": "É por isso que o tamanho do contexto pode ser um grande gargalo para modelos de linguagem grandes, e aumentá-lo não é trivial.",
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "translatedText": "Como você imagina, motivado por um desejo por janelas de contexto cada vez maiores, nos últimos anos assistimos a algumas variações no mecanismo de atenção que visa tornar o contexto mais escalável, mas aqui mesmo, você e eu continuamos focados no básico.",
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "translatedText": "Ok, ótimo, calcular esse padrão permite que o modelo deduza quais palavras são relevantes para quais outras palavras.",
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "translatedText": "Agora você precisa realmente atualizar os embeddings, permitindo que as palavras passem informações para quaisquer outras palavras para as quais sejam relevantes.",
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "translatedText": "Por exemplo, você deseja que a incorporação de Fluffy cause de alguma forma uma alteração na Criatura que a mova para uma parte diferente deste espaço de incorporação de 12.000 dimensões que codifica mais especificamente uma criatura Fluffy.",
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "translatedText": "O que vou fazer aqui é primeiro mostrar a maneira mais direta de fazer isso, embora haja uma pequena maneira de isso ser modificado no contexto da atenção multifacetada.",
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "translatedText": "A maneira mais direta seria usar uma terceira matriz, o que chamamos de matriz de valores, que você multiplica pela incorporação da primeira palavra, por exemplo Fluffy.",
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "translatedText": "O resultado disso é o que você chamaria de vetor de valor, e isso é algo que você adiciona à incorporação da segunda palavra, neste caso, algo que você adiciona à incorporação da Criatura.",
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "translatedText": "Portanto, esse vetor de valor vive no mesmo espaço de dimensão muito alta que os embeddings.",
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "translatedText": "Quando você multiplica essa matriz de valores pela incorporação de uma palavra, você pode pensar nisso como se esta palavra fosse relevante para ajustar o significado de outra coisa, o que exatamente deveria ser adicionado à incorporação dessa outra coisa para refletir esse?",
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "translatedText": "Olhando para trás em nosso diagrama, vamos deixar de lado todas as chaves e as consultas, já que depois de calcular o padrão de atenção você terminou com elas, então você vai pegar essa matriz de valores e multiplicá-la por cada uma dessas incorporações para produzir uma sequência de vetores de valor.",
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "translatedText": "Você pode pensar nesses vetores de valor como estando associados às chaves correspondentes.",
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "translatedText": "Para cada coluna neste diagrama, você multiplica cada um dos vetores de valor pelo peso correspondente nessa coluna.",
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "translatedText": "Por exemplo, aqui, na incorporação de Creature, você adicionaria grandes proporções dos vetores de valor para Fluffy e Blue, enquanto todos os outros vetores de valor seriam zerados, ou pelo menos quase zerados.",
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "translatedText": "E finalmente, a maneira de realmente atualizar a incorporação associada a esta coluna, previamente codificando algum significado livre de contexto de Criatura, você adiciona todos esses valores redimensionados na coluna, produzindo uma alteração que você deseja adicionar, que eu &#39; rotularemos delta-e e, em seguida, adicionaremos isso à incorporação original.",
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "translatedText": "Esperançosamente, o resultado é um vetor mais refinado que codifica o significado mais contextualmente rico, como o de uma criatura azul fofa.",
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "translatedText": "E é claro que você não faz isso apenas em uma incorporação, você aplica a mesma soma ponderada em todas as colunas desta imagem, produzindo uma sequência de alterações, adicionando todas essas alterações às incorporações correspondentes, produz uma sequência completa de incorporações mais refinadas saindo do bloco de atenção.",
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "translatedText": "Diminuindo o zoom, todo esse processo é o que você descreveria como uma única cabeça de atenção.",
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "translatedText": "Como descrevi até agora, esse processo é parametrizado por três matrizes distintas, todas preenchidas com parâmetros ajustáveis, a chave, a consulta e o valor.",
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "translatedText": "Quero reservar um momento para continuar o que começamos no capítulo anterior, com a pontuação onde contamos o número total de parâmetros do modelo usando os números do GPT-3.",
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "translatedText": "Cada uma dessas matrizes de chave e de consulta tem 12.288 colunas, correspondendo à dimensão de incorporação, e 128 linhas, correspondendo à dimensão desse espaço de consulta de chave menor.",
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "translatedText": "Isso nos dá cerca de 1,5 milhão de parâmetros adicionais para cada um.",
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "translatedText": "Se você olhar para essa matriz de valor, por outro lado, a maneira como descrevi as coisas até agora sugeriria que é uma matriz quadrada que tem 12.288 colunas e 12.288 linhas, uma vez que tanto suas entradas quanto suas saídas residem neste espaço de incorporação muito grande.",
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "translatedText": "Se for verdade, isso significaria cerca de 150 milhões de parâmetros adicionados.",
  "input": "If true, that would mean about 150 million added parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "translatedText": "E para ser claro, você poderia fazer isso.",
  "input": "And to be clear, you could do that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "translatedText": "Você poderia dedicar ordens de magnitude a mais parâmetros ao mapa de valores do que à chave e à consulta.",
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "translatedText": "Mas, na prática, é muito mais eficiente se, em vez disso, você fizer com que o número de parâmetros dedicados a esse mapa de valores seja igual ao número dedicado à chave e à consulta.",
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "translatedText": "Isto é especialmente relevante no cenário de execução de múltiplas cabeças de atenção em paralelo.",
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "translatedText": "A aparência disso é que o mapa de valores é fatorado como um produto de duas matrizes menores.",
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "translatedText": "Conceitualmente, eu ainda encorajaria você a pensar no mapa linear geral, um com entradas e saídas, ambos neste espaço de incorporação maior, por exemplo, levando a incorporação do azul nesta direção de azul que você adicionaria aos substantivos.",
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "translatedText": "Acontece que é um número menor de linhas, normalmente do mesmo tamanho que o espaço de consulta da chave.",
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "translatedText": "O que isso significa é que você pode pensar nisso como mapear os grandes vetores de incorporação em um espaço muito menor.",
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "translatedText": "Esta não é a nomenclatura convencional, mas vou chamá-la de matriz de valores decrescentes.",
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "translatedText": "A segunda matriz mapeia esse espaço menor de volta ao espaço de incorporação, produzindo os vetores que você usa para fazer as atualizações reais.",
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "translatedText": "Vou chamar esta de matriz de aumento de valor, o que novamente não é convencional.",
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "translatedText": "A maneira como você veria isso escrito na maioria dos artigos parece um pouco diferente.",
  "input": "The way that you would see this written in most papers looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "translatedText": "Falarei sobre isso em um minuto.",
  "input": "I'll talk about it in a minute.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "translatedText": "Na minha opinião, isso tende a tornar as coisas um pouco mais confusas conceitualmente.",
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "translatedText": "Para usar o jargão da álgebra linear aqui, o que estamos basicamente fazendo é restringir o mapa de valor geral a uma transformação de classificação baixa.",
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "translatedText": "Voltando à contagem de parâmetros, todas essas quatro matrizes têm o mesmo tamanho e, somando-as, obtemos cerca de 6,3 milhões de parâmetros para uma cabeça de atenção.",
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "translatedText": "Como uma observação rápida, para ser um pouco mais preciso, tudo o que foi descrito até agora é o que as pessoas chamariam de cabeça de autoatenção, para distingui-la de uma variação que surge em outros modelos chamada atenção cruzada.",
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "translatedText": "Isso não é relevante para nosso exemplo GPT, mas se você estiver curioso, a atenção cruzada envolve modelos que processam dois tipos distintos de dados, como texto em um idioma e texto em outro idioma que faz parte de uma geração contínua de uma tradução, ou talvez entrada de áudio de fala e uma transcrição contínua.",
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "translatedText": "Uma cabeça de atenção cruzada parece quase idêntica.",
  "input": "A cross-attention head looks almost identical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "translatedText": "A única diferença é que os mapas de chave e de consulta atuam em conjuntos de dados diferentes.",
  "input": "The only difference is that the key and query maps act on different data sets.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "translatedText": "Num modelo que faz tradução, por exemplo, as chaves podem vir de um idioma, enquanto as consultas vêm de outro, e o padrão de atenção pode descrever quais palavras de um idioma correspondem a quais palavras de outro.",
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "translatedText": "E nesta configuração normalmente não haveria mascaramento, uma vez que não há realmente qualquer noção de tokens posteriores afetando os anteriores.",
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "translatedText": "Porém, mantendo o foco na autoatenção, se você entendesse tudo até agora e parasse por aqui, chegaria à essência do que realmente é a atenção.",
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "translatedText": "Tudo o que nos resta é definir o sentido em que você faz isso muitas vezes diferentes.",
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "translatedText": "Em nosso exemplo central, nos concentramos nos adjetivos que atualizam os substantivos, mas é claro que há muitas maneiras diferentes pelas quais o contexto pode influenciar o significado de uma palavra.",
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "translatedText": "Se as palavras bateram precederam a palavra carro, isso tem implicações para a forma e estrutura desse carro.",
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "translatedText": "E muitas associações podem ser menos gramaticais.",
  "input": "And a lot of associations might be less grammatical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "translatedText": "Se a palavra bruxo estiver em algum lugar na mesma passagem que Harry, isso sugere que isso pode estar se referindo a Harry Potter, ao passo que se, em vez disso, as palavras Rainha, Sussex e William estivessem nessa passagem, então talvez a incorporação de Harry devesse ser atualizada para se referir ao príncipe.",
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "translatedText": "Para cada tipo diferente de atualização contextual que você possa imaginar, os parâmetros dessas matrizes de chave e de consulta seriam diferentes para capturar os diferentes padrões de atenção, e os parâmetros do nosso mapa de valor seriam diferentes com base no que deveria ser adicionado aos embeddings.",
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "translatedText": "E, novamente, na prática, o verdadeiro comportamento desses mapas é muito mais difícil de interpretar, onde os pesos são definidos para fazer tudo o que o modelo precisa que eles façam para melhor cumprir seu objetivo de prever o próximo token.",
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "translatedText": "Como eu disse antes, tudo o que descrevemos é uma única cabeça de atenção, e um bloco de atenção completo dentro de um transformador consiste no que chamamos de atenção multicabeças, onde você executa muitas dessas operações em paralelo, cada uma com sua própria consulta chave distinta. e mapas de valor.",
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "translatedText": "O GPT-3, por exemplo, usa 96 cabeças de atenção dentro de cada bloco.",
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "translatedText": "Considerando que cada um já é um pouco confuso, certamente é muita coisa para guardar na cabeça.",
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "translatedText": "Apenas para explicar tudo explicitamente, isso significa que você tem 96 matrizes de chave e de consulta distintas, produzindo 96 padrões de atenção distintos.",
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "translatedText": "Então, cada cabeça tem suas próprias matrizes de valores distintas usadas para produzir 96 sequências de vetores de valores.",
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "translatedText": "Todos eles são somados usando os padrões de atenção correspondentes como pesos.",
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "translatedText": "O que isto significa é que para cada posição no contexto, cada token, cada uma dessas cabeças produz uma proposta de mudança a ser adicionada à incorporação naquela posição.",
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "translatedText": "Então o que você faz é somar todas as alterações propostas, uma para cada cabeça, e adicionar o resultado à incorporação original daquela posição.",
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "translatedText": "Toda essa soma aqui seria uma fatia do que é produzido por esse bloco de atenção com múltiplas cabeças, uma única daquelas incorporações refinadas que aparecem na outra extremidade dele.",
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "translatedText": "Novamente, isso é muito em que pensar, então não se preocupe se levar algum tempo para entender.",
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "translatedText": "A ideia geral é que, ao executar muitas cabeças distintas em paralelo, você está dando ao modelo a capacidade de aprender muitas maneiras distintas pelas quais o contexto muda de significado.",
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "translatedText": "Aumentando nossa contagem contínua de parâmetros com 96 cabeças, cada uma incluindo sua própria variação dessas quatro matrizes, cada bloco de atenção multicabeças termina com cerca de 600 milhões de parâmetros.",
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "translatedText": "Há uma coisa um pouco irritante que eu realmente deveria mencionar para qualquer um de vocês que ler mais sobre transformadores.",
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "translatedText": "Você se lembra de como eu disse que o mapa de valores é fatorado nessas duas matrizes distintas, que rotulei como matrizes de valor inferior e matrizes de valor superior.",
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "translatedText": "A maneira como enquadrei as coisas sugeriria que você visse esse par de matrizes dentro de cada cabeça de atenção e poderia implementá-lo dessa maneira.",
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "translatedText": "Esse seria um design válido.",
  "input": "That would be a valid design.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "translatedText": "Mas a maneira como você vê isso escrito nos documentos e a forma como é implementado na prática parece um pouco diferente.",
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "translatedText": "Todas essas matrizes de aumento de valor para cada cabeça aparecem grampeadas em uma matriz gigante que chamamos de matriz de saída, associada a todo o bloco de atenção com várias cabeças.",
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "translatedText": "E quando você vê as pessoas se referindo à matriz de valores para uma determinada cabeça de atenção, elas normalmente estão se referindo apenas a essa primeira etapa, aquela que eu estava rotulando como a projeção descendente do valor no espaço menor.",
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "translatedText": "Para os curiosos, deixei uma nota na tela sobre isso.",
  "input": "For the curious among you, I've left an on-screen note about it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "translatedText": "É um daqueles detalhes que corre o risco de desviar a atenção dos principais pontos conceituais, mas quero destacá-lo apenas para que você saiba se leu sobre isso em outras fontes.",
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "translatedText": "Deixando de lado todas as nuances técnicas, na prévia do capítulo anterior vimos como os dados que fluem através de um transformador não fluem apenas através de um único bloco de atenção.",
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "translatedText": "Por um lado, ele também passa por outras operações chamadas perceptrons multicamadas.",
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "translatedText": "Falaremos mais sobre eles no próximo capítulo.",
  "input": "We'll talk more about those in the next chapter.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "translatedText": "E então ele passa repetidamente por muitas cópias de ambas as operações.",
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "translatedText": "O que isto significa é que depois de uma determinada palavra absorver parte do seu contexto, há muito mais hipóteses de esta incorporação mais matizada ser influenciada pelo seu ambiente mais matizado.",
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "translatedText": "Quanto mais você avança na rede, com cada incorporação absorvendo cada vez mais significado de todas as outras incorporações, que por sua vez estão ficando cada vez mais matizadas, a esperança é que haja a capacidade de codificar ideias de nível superior e mais abstratas sobre um determinado informações além de apenas descritores e estrutura gramatical.",
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "translatedText": "Coisas como sentimento e tom e se é um poema e quais verdades científicas subjacentes são relevantes para a peça e coisas assim.",
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "translatedText": "Voltando mais uma vez à nossa pontuação, o GPT-3 inclui 96 camadas distintas, de modo que o número total de parâmetros principais de consulta e valor é multiplicado por outros 96, o que eleva a soma total para pouco menos de 58 bilhões de parâmetros distintos dedicados a todos os cabeças de atenção.",
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "translatedText": "Isso é muito, com certeza, mas representa apenas cerca de um terço dos 175 bilhões que estão na rede no total.",
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "translatedText": "Portanto, mesmo que a atenção receba toda a atenção, a maioria dos parâmetros vem dos blocos situados entre essas etapas.",
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "translatedText": "No próximo capítulo, você e eu falaremos mais sobre esses outros blocos e também muito mais sobre o processo de treinamento.",
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "translatedText": "Uma grande parte da história do sucesso do mecanismo de atenção não é tanto qualquer tipo específico de comportamento que ele permite, mas o fato de ser extremamente paralelizável, o que significa que você pode executar um grande número de cálculos em um curto espaço de tempo usando GPUs. .",
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "translatedText": "Dado que uma das grandes lições sobre aprendizagem profunda nas últimas duas décadas foi que a escala por si só parece proporcionar enormes melhorias qualitativas no desempenho do modelo, há uma enorme vantagem nas arquiteturas paralelizáveis que permitem fazer isso.",
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "translatedText": "Se você quiser saber mais sobre essas coisas, deixei muitos links na descrição.",
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "translatedText": "Em particular, qualquer coisa produzida por Andrej Karpathy ou Chris Ola tende a ser ouro puro.",
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "translatedText": "Neste vídeo, eu queria apenas chamar a atenção em sua forma atual, mas se você está curioso para saber mais sobre a história de como chegamos aqui e como você pode reinventar essa ideia por si mesmo, meu amigo Vivek acabou de colocar alguns vídeos dando muito mais dessa motivação.",
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "translatedText": "Além disso, Britt Cruz, do canal The Art of the Problem, tem um vídeo muito legal sobre a história dos grandes modelos de linguagem.",
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "translatedText": "Obrigado.",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]