[
 {
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "translatedText": "У попередньому розділі ми з вами почали вивчати внутрішню будову трансформатора.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "translatedText": "Це одна з ключових технологій у великих мовних моделях, а також багато інших інструментів у сучасній хвилі ШІ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "translatedText": "Вперше він з'явився у відомій статті 2017 року під назвою \"Увага - це все, що вам потрібно\", і в цій главі ми з вами розберемося, що це за механізм уваги, візуалізуючи, як він обробляє дані.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "translatedText": "Якщо коротко, то ось важливий контекст, який я хочу, щоб ви мали на увазі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "translatedText": "Мета моделі, яку ми з вами вивчаємо, полягає в тому, щоб взяти шматок тексту і передбачити, яке слово буде наступним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "translatedText": "Вхідний текст розбивається на маленькі шматочки, які ми називаємо токенами, і дуже часто це слова або частини слів, але для того, щоб нам з вами було легше думати над прикладами у цьому відео, давайте спростимо, уявивши, що токени - це завжди просто слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "translatedText": "Першим кроком у трансформаторі є асоціювання кожного токена з вектором високої розмірності, що ми називаємо його вбудовуванням.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "translatedText": "Найважливіша ідея, яку я хочу, щоб ви запам'ятали, - це те, як напрямки в цьому багатовимірному просторі всіх можливих вбудовувань можуть співвідноситися зі смисловим значенням.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "translatedText": "У попередньому розділі ми бачили приклад того, як напрямок може відповідати гендеру, в тому сенсі, що додавання певного кроку в цьому просторі може привести вас від вбудовування іменника чоловічого роду до вбудовування відповідного іменника жіночого роду.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "translatedText": "Це лише один приклад, і ви можете собі уявити, скільки інших напрямків у цьому багатовимірному просторі можуть відповідати численним іншим аспектам значення слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "translatedText": "Мета трансформатора полягає в тому, щоб поступово коригувати ці вкраплення так, щоб вони не просто кодували окреме слово, а запікали в собі набагато, набагато багатше контекстуальне значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "translatedText": "Заздалегідь скажу, що багато людей вважають механізм уваги, цей ключовий елемент трансформатора, дуже заплутаним, тому не хвилюйтеся, якщо вам знадобиться деякий час, щоб все зрозуміли.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "translatedText": "Я думаю, що перш ніж ми зануримося в обчислювальні деталі і всі матричні множення, варто подумати про пару прикладів поведінки, на яку ми хочемо звернути увагу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "translatedText": "Розглянемо фрази американська справжня родимка, один моль вуглекислого газу та взяти біопсію родимки.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "translatedText": "Ми з вами знаємо, що слово \"кріт\" має різні значення в кожній з цих мов, залежно від контексту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "translatedText": "Але після першого кроку трансформатора, який розбиває текст і пов'язує кожну лексему з вектором, вектор, який асоціюється з кротом, буде однаковим у всіх цих випадках, тому що це початкове вбудовування лексеми є фактично таблицею пошуку без посилання на контекст.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "translatedText": "Лише на наступному кроці трансформатора навколишні вбудовування мають шанс передати інформацію до цього.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "translatedText": "Можливо, ви уявляєте собі, що в цьому просторі вбудовування є кілька різних напрямків, які кодують кілька різних значень слова \"кріт\", і що добре натренований блок уваги обчислює, що потрібно додати до загального вбудовування, щоб перемістити його в один з цих конкретних напрямків, залежно від контексту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "input": "To take another example, consider the embedding of the word tower.",
  "translatedText": "Візьмемо інший приклад, розглянемо вбудовування слова \"вежа\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "translatedText": "Ймовірно, це якийсь дуже загальний, неконкретний напрямок у просторі, пов'язаний з багатьма іншими великими, високими іменниками.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "translatedText": "Якби цьому слову безпосередньо передувало слово \"Ейфелева вежа\", можна було б уявити собі бажання, щоб механізм оновив цей вектор так, щоб він вказував у напрямку, який більш конкретно кодує Ейфелеву вежу, можливо, співвідносився з векторами, пов'язаними з Парижем і Францією та речами зі сталі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "translatedText": "Якщо йому ще й передувало слово мініатюрний, то вектор слід ще більше актуалізувати, щоб він більше не співвідносився з великими, високими речами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "translatedText": "У більш широкому сенсі, ніж просто уточнення значення слова, блок уваги дозволяє моделі переміщати інформацію, закодовану в одному вбудовуванні, в інше, потенційно досить віддалене, і потенційно з інформацією, яка набагато багатша, ніж просто одне слово.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "translatedText": "У попередньому розділі ми бачили, як після того, як всі вектори проходять через мережу, включаючи багато різних блоків уваги, обчислення, які ви виконуєте, щоб передбачити наступний токен, повністю залежать від останнього вектора в послідовності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "translatedText": "Уявіть, наприклад, що текст, який ви вводите, - це більша частина цілого детективного роману, аж до точки в кінці, яка говорить: \"Отже, вбивця був\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "translatedText": "Якщо модель збирається точно передбачити наступне слово, цей кінцевий вектор послідовності, який починав своє життя просто вбудовуванням слова, повинен бути оновлений усіма блоками уваги, щоб представляти набагато більше, ніж будь-яке окреме слово, якимось чином кодуючи всю інформацію з повного вікна контексту, яка має відношення до передбачення наступного слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "input": "To step through the computations, though, let's take a much simpler example.",
  "translatedText": "Щоб розібратися в розрахунках, давайте візьмемо набагато простіший приклад.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "translatedText": "Уявіть, що вхідні дані містять фразу \"Пухнасте блакитне створіння блукало зеленим лісом\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "translatedText": "Наразі припустімо, що єдиний тип оновлення, який нас цікавить, - це зміна прикметників у значеннях відповідних іменників.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "translatedText": "Те, що я збираюся описати, - це те, що ми називаємо однією головою уваги, а пізніше ми побачимо, як блок уваги складається з безлічі різних голів, що працюють паралельно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "translatedText": "Знову ж таки, початкове вбудовування для кожного слова - це деякий вектор високої розмірності, який кодує лише значення цього слова без контексту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "input": "Actually, that's not quite true.",
  "translatedText": "Насправді, це не зовсім так.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "input": "They also encode the position of the word.",
  "translatedText": "Вони також кодують позицію слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "translatedText": "Можна ще багато чого сказати про те, як кодуються позиції, але зараз все, що вам потрібно знати, це те, що записів цього вектора достатньо, щоб сказати вам, що це за слово і де воно існує в контексті.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "translatedText": "Позначимо ці вбудовування літерою e.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "translatedText": "Мета полягає в тому, щоб в результаті серії обчислень отримати новий уточнений набір вбудовувань, де, наприклад, ті, що відповідають іменникам, ввібрали в себе значення відповідних прикметників.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "translatedText": "Граючи в гру глибокого навчання, ми хочемо, щоб більшість обчислень виглядали як матрично-векторні добутки, де матриці повні вагових коефіцієнтів, що налаштовуються, тобто речей, які модель вивчає на основі даних.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "translatedText": "Щоб було зрозуміло, я вигадую цей приклад, коли прикметники доповнюють іменники, лише для того, щоб проілюструвати тип поведінки, який ви можете собі уявити, як поводиться голова, що концентрується на увазі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "translatedText": "Як і в більшості випадків глибокого навчання, справжню поведінку набагато складніше розібрати, оскільки вона базується на налаштуванні величезної кількості параметрів для мінімізації певної функції витрат.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "translatedText": "Просто, коли ми перебираємо різні матриці, заповнені параметрами, які беруть участь у цьому процесі, я вважаю, що дуже корисно мати уявний приклад того, що він може робити, щоб допомогти зробити все більш конкретним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "translatedText": "На першому етапі цього процесу ви можете уявити, що кожен іменник, як істота, ставить запитання: \"Гей, а чи є переді мною прикметники?\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "translatedText": "А на слова пухнастий і блакитний, щоб кожен зміг відповісти: так, я прикметник і я в такому положенні.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "translatedText": "Це питання якимось чином закодоване як ще один вектор, ще один список чисел, який ми називаємо запитом на це слово.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "translatedText": "Цей вектор запиту має набагато меншу розмірність, ніж вектор вбудовування, скажімо, 128.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "translatedText": "Обчислення цього запиту виглядає так: беремо певну матрицю, яку я позначу wq, і множимо її на вбудовування.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "translatedText": "Для стислості, давайте запишемо вектор запиту як q, і тоді кожного разу, коли ви бачите, що я ставлю матрицю поруч зі стрілкою, як ця, це означає, що множення цієї матриці на вектор на початку стрілки дасть вам вектор в кінці стрілки.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "translatedText": "У цьому випадку ви множите цю матрицю на всі вбудовування в контексті, створюючи один вектор запиту для кожного токена.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "translatedText": "Елементи цієї матриці є параметрами моделі, а це означає, що справжню поведінку можна дізнатися з даних, і на практиці те, що робить ця матриця в конкретній голові уваги, складно проаналізувати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "translatedText": "Але для прикладу, який, як ми сподіваємося, він навчиться, припустимо, що ця матриця запитів відображає включення іменників у певних напрямках у цьому меншому просторі запитів, що якимось чином кодує ідею пошуку прикметників у попередніх позиціях.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "input": "As to what it does to other embeddings, who knows?",
  "translatedText": "Щодо того, як це впливає на інші вбудовування, хто знає?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "translatedText": "Можливо, вона одночасно намагається досягти якоїсь іншої мети за допомогою цих засобів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "input": "Right now, we're laser focused on the nouns.",
  "translatedText": "Зараз ми зосереджені на іменниках.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "translatedText": "В той же час, з нею пов'язана друга матриця, яка називається ключовою, яку ви також множите на кожне з вбудовувань.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "input": "This produces a second sequence of vectors that we call the keys.",
  "translatedText": "Це створює другу послідовність векторів, які ми називаємо ключами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "translatedText": "Концептуально, ви хочете думати про ключі як про потенційні відповіді на запити.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "translatedText": "Ця ключова матриця також має безліч параметрів, що налаштовуються, і так само, як і матриця запитів, відображає вектори вбудовування в той самий простір меншої розмірності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "translatedText": "Ви вважаєте, що ключі відповідають запитам, коли вони тісно пов'язані один з одним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "translatedText": "У нашому прикладі можна уявити, що матриця ключів відображає прикметники \"пухнастий\" і \"синій\" на вектори, які тісно пов'язані із запитом, створеним за словом \"істота\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "translatedText": "Щоб виміряти, наскільки добре кожен ключ відповідає кожному запиту, ви обчислюєте точковий добуток між кожною можливою парою ключ-запит.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "translatedText": "Мені подобається візуалізувати сітку, заповнену купою точок, де більші точки відповідають більшим точковим продуктам, місцям, де ключі та запити узгоджуються.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "translatedText": "Для нашого прикладу з прикметником та іменником це виглядатиме дещо інакше: якщо ключі, створені за допомогою fluffy та blue, дійсно збігаються із запитом, створеним за допомогою creature, то добутки точок у цих двох точках будуть великими додатними числами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "translatedText": "На жаргоні людей, які займаються машинним навчанням, це означає, що вбудовування пухнастого і блакитного відповідають за вбудовування істоти.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "translatedText": "На відміну від крапкового добутку між ключем для якогось іншого слова, наприклад, the, і запитом для creature буде якесь невелике або від'ємне значення, що відображає не пов'язані між собою речі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "translatedText": "Отже, ми маємо цю сітку значень, яка може бути будь-яким дійсним числом від від'ємної нескінченності до нескінченності, що дає нам оцінку того, наскільки важливим є кожне слово для оновлення значення кожного іншого слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "translatedText": "Спосіб, яким ми збираємося використовувати ці бали, полягає в тому, щоб взяти певну зважену суму по кожному стовпчику, зважену на релевантність.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "translatedText": "Отже, замість того, щоб значення варіювалися від від'ємної нескінченності до нескінченності, ми хочемо, щоб числа в цих стовпчиках були в діапазоні від 0 до 1, і в кожному стовпчику в сумі дорівнювали 1, як якщо б вони були розподілом ймовірностей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "translatedText": "Якщо ви читали попередній розділ, то знаєте, що нам потрібно зробити.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "translatedText": "Ми обчислюємо softmax для кожного з цих стовпців, щоб нормалізувати значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "translatedText": "На нашому малюнку, після застосування функції softmax до всіх стовпців, ми заповнимо сітку цими нормалізованими значеннями.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "translatedText": "На цьому етапі ви можете вважати, що кожен стовпчик має вагу відповідно до того, наскільки слово зліва релевантне відповідному значенню вгорі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "input": "We call this grid an attention pattern.",
  "translatedText": "Ми називаємо цю сітку патерном уваги.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "translatedText": "Якщо ви подивитеся на оригінальний трансформаторний папір, то побачите дуже компактний спосіб, у який вони все це записують.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "translatedText": "Тут змінні q та k представляють повні масиви векторів запиту та ключів відповідно, ті маленькі вектори, які ви отримуєте, множачи вкладки на матриці запиту та ключів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "translatedText": "Цей вираз у чисельнику є дійсно компактним способом представлення сітки всіх можливих точкових добутків між парами ключів і запитів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "translatedText": "Невелика технічна деталь, про яку я не згадав, полягає в тому, що для числової стабільності корисно ділити всі ці значення на квадратний корінь з розмірності в цьому просторі ключових запитів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "translatedText": "Тоді цей софтмакс, який обертається навколо повного виразу, слід розуміти як такий, що застосовується стовпчик за стовпчиком.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "input": "As to that v term, we'll talk about it in just a second.",
  "translatedText": "Щодо цього v-терміну, ми поговоримо про нього за секунду.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "translatedText": "Перед цим є ще одна технічна деталь, яку я поки що пропустив.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "translatedText": "Під час навчання, коли ви запускаєте цю модель на заданому прикладі тексту, і всі ваги трохи коригуються і налаштовуються, щоб заохочувати або карати її залежно від того, наскільки високу ймовірність вона призначає істинному наступному слову в уривку, виявляється, що весь процес навчання стає набагато ефективнішим, якщо ви одночасно змушуєте її передбачати кожну можливу наступну лексему, що слідує за кожною початковою послідовністю лексем в цьому уривку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "translatedText": "Наприклад, у фразі, на якій ми зосередилися, можна також передбачити, які слова слідують за словом \"істота\", а які - за словом \"після\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "translatedText": "Це дуже приємно, адже це означає, що те, що в іншому випадку було б одним навчальним прикладом, ефективно діє як багато.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "translatedText": "Для цілей нашої моделі уваги це означає, що ви ніколи не повинні дозволяти наступним словам впливати на попередні, оскільки інакше вони можуть видати відповідь на те, що буде далі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "translatedText": "Це означає, що ми хочемо, щоб всі ці плями, які представляють пізніші токени, що впливають на попередні, якимось чином були примушені дорівнювати нулю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "translatedText": "Найпростіше, що ви можете зробити, це встановити їх рівними нулю, але якщо ви це зробите, то стовпці більше не складатимуться в одиницю, вони не будуть нормалізовані.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "translatedText": "Натомість, найпоширеніший спосіб зробити це - перед застосуванням softmax встановити для всіх цих записів від'ємну нескінченність.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "translatedText": "Якщо ви це зробите, то після застосування softmax всі вони перетворяться на нуль, але стовпці залишаться нормалізованими.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "input": "This process is called masking.",
  "translatedText": "Цей процес називається маскуванням.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "translatedText": "Існують версії уваги, де ви не застосовуєте її, але в нашому прикладі GPT, навіть незважаючи на те, що це більш доречно на етапі навчання, ніж, скажімо, запуск його як чат-бота або щось подібне, ви завжди застосовуєте це маскування, щоб запобігти впливу пізніших токенів на попередні.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "translatedText": "Ще один факт, над яким варто замислитися щодо цього шаблону уваги - це те, що його розмір дорівнює квадрату розміру контексту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "translatedText": "Ось чому розмір контексту може бути справді величезним вузьким місцем у великих мовних моделях, і масштабування його нетривіальне.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "translatedText": "Як ви розумієте, в останні роки, мотивовані бажанням мати все більші і більші вікна контексту, з'явилися деякі варіації механізму уваги, спрямовані на те, щоб зробити контекст більш масштабованим, але зараз ми з вами зосередимося на основах.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "translatedText": "Гаразд, чудово, обчислення цього шаблону дозволяє моделі зробити висновок, які слова релевантні до інших слів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "translatedText": "Тепер вам потрібно оновити вбудовування, дозволивши словам передавати інформацію тим іншим словам, до яких вони мають відношення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "translatedText": "Наприклад, ви хочете, щоб вбудовування Пухнастого якимось чином спричинило зміну в Істоті, яка перемістить його в іншу частину цього 12,000-вимірного простору вбудовування, яка більш конкретно кодує Пухнасту істоту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "translatedText": "Спочатку я покажу вам найпростіший спосіб, яким ви можете це зробити, хоча в контексті багатоголової уваги він дещо видозмінюється.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "translatedText": "Найпростішим способом буде використання третьої матриці, яку ми називаємо матрицею значень, яку ви помножите на вбудовування першого слова, наприклад, \"Пухнастий\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "translatedText": "Результатом цього є те, що можна назвати вектором значення, і це те, що ви додаєте до вбудовування другого слова, в даному випадку те, що ви додаєте до вбудовування слова \"Творіння\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "translatedText": "Отже, цей вектор значень живе в тому ж дуже високовимірному просторі, що й вбудовування.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "translatedText": "Коли ви множите цю матрицю значень на вбудовування слова, ви можете подумати про це так: якщо це слово має відношення до коригування значення чогось іншого, то що саме слід додати до вбудовування цього іншого, щоб відобразити це?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "translatedText": "Озираючись на нашу діаграму, давайте відкладемо всі ключі та запити, оскільки після того, як ви обчислите шаблон уваги, ви закінчите з ними, ви візьмете цю матрицю значень і помножите її на кожне з цих вбудовувань, щоб отримати послідовність векторів значень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "translatedText": "Ви можете думати про ці вектори значень як про певним чином пов'язані з відповідними ключами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "translatedText": "Для кожного стовпчика на цій діаграмі ви множите кожен з векторів значень на відповідну вагу в цьому стовпчику.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "translatedText": "Наприклад, тут, під час вбудовування Істоти, ви додасте великі частки векторів значень для Пухнастого і Синього, тоді як всі інші вектори значень будуть обнулені, або, принаймні, майже обнулені.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "translatedText": "І, нарешті, спосіб оновити вбудовування, пов'язане з цим стовпчиком, попередньо закодувавши деяке контекстно-вільне значення Creature, ви додаєте всі ці перемасштабовані значення в стовпчику, створюючи зміну, яку ви хочете додати, і яку я позначу як delta-e, а потім додаєте її до початкового вбудовування.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "translatedText": "Сподіваємося, що в результаті вийде більш витончений вектор, що кодує більш контекстуально багате значення, як у пухнастої блакитної істоти.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "translatedText": "І, звичайно, ви робите це не лише з одним вбудовуванням, ви застосовуєте ту саму зважену суму до всіх стовпчиків на цьому зображенні, створюючи послідовність змін, додаючи всі ці зміни до відповідних вбудовувань, створюючи повну послідовність більш досконалих вбудовувань, що з'являються з блоку уваги.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "translatedText": "Зменшуючи масштаб, весь цей процес можна описати як єдиний центр уваги.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "translatedText": "Як я вже описував, цей процес параметризується трьома окремими матрицями, кожна з яких заповнена параметрами, що налаштовуються, - ключем, запитом і значенням.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "translatedText": "Я хочу скористатися моментом, щоб продовжити те, що ми почали в попередньому розділі, з підрахунку балів, де ми підраховуємо загальну кількість параметрів моделі, використовуючи числа з GPT-3.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "translatedText": "Кожна з цих матриць ключів і запитів має 12 288 стовпців, що відповідає розмірності вбудовування, і 128 рядків, що відповідає розмірності цього меншого простору ключових запитів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "translatedText": "Це дає нам додаткові 1,5 мільйона параметрів для кожного з них.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "translatedText": "Якщо ви подивитеся на цю матрицю значень з іншого боку, то з того, як я описав її досі, можна припустити, що це квадратна матриця, яка має 12 288 стовпців і 12 288 рядків, оскільки і її входи, і виходи знаходяться в цьому дуже великому просторі вбудовування.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "input": "If true, that would mean about 150 million added parameters.",
  "translatedText": "Якщо це правда, то це означає близько 150 мільйонів доданих параметрів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "input": "And to be clear, you could do that.",
  "translatedText": "І якщо бути точним, ви можете це зробити.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "translatedText": "Ви можете присвятити карті значень на порядки більше параметрів, ніж ключу і запиту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "translatedText": "Але на практиці набагато ефективніше зробити так, щоб кількість параметрів, присвячених цій карті значень, дорівнювала кількості параметрів, присвячених ключу і запиту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "translatedText": "Це особливо актуально в умовах паралельної роботи декількох головок уваги.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "translatedText": "Це виглядає так: карта цінностей факторизується як добуток двох менших матриць.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "translatedText": "Концептуально, я б рекомендував вам подумати про загальну лінійну карту, яка має входи і виходи, обидва в цьому більшому просторі вбудовування, наприклад, вбудовування синього кольору в цей напрямок синяви, який ви додасте до іменників.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "translatedText": "Це просто менша кількість рядків, зазвичай такого ж розміру, як і простір ключового запиту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "translatedText": "Це означає, що ви можете думати про це як про відображення великих векторів вбудовування на значно менший простір.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "translatedText": "Це не загальноприйнята назва, але я буду називати її матрицею зниження значень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "translatedText": "Друга матриця відображає з цього меншого простору назад до простору вбудовування, створюючи вектори, які ви використовуєте для фактичних оновлень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "translatedText": "Я буду називати цю матрицю матрицею підняття значень, що знову ж таки не є традиційним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "input": "The way that you would see this written in most papers looks a little different.",
  "translatedText": "Те, як це написано в більшості газет, виглядає дещо інакше.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "input": "I'll talk about it in a minute.",
  "translatedText": "Я розповім про це за хвилину.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "translatedText": "На мою думку, це робить речі трохи більш концептуально заплутаними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "translatedText": "Використовуючи жаргон лінійної алгебри, ми, по суті, обмежуємо загальну карту значень перетворенням низького рангу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "translatedText": "Повертаючись до кількості параметрів, всі чотири матриці мають однаковий розмір, і, склавши їх разом, ми отримаємо близько 6,3 мільйона параметрів для однієї голови уваги.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "translatedText": "Якщо бути трохи точнішим, то все описане вище - це те, що люди називають головкою з самоуважністю, щоб відрізнити її від варіації, яка зустрічається в інших моделях, що називається перехресною увагою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "translatedText": "Це не стосується нашого прикладу з GPT, але якщо вам цікаво, перехресна увага передбачає моделі, які обробляють два різні типи даних, наприклад, текст однією мовою і текст іншою мовою, що є частиною поточного покоління перекладу, або, можливо, аудіозапис мовлення і поточна транскрипція.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "input": "A cross-attention head looks almost identical.",
  "translatedText": "Голова з перехресною увагою виглядає майже ідентично.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "input": "The only difference is that the key and query maps act on different data sets.",
  "translatedText": "Єдина відмінність полягає в тому, що карти ключів і запитів діють на різні набори даних.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "translatedText": "Наприклад, у моделі, що виконує переклад, ключі можуть бути з однієї мови, а запити - з іншої, і модель уваги може описувати, які слова з однієї мови відповідають яким словам в іншій.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "translatedText": "І в цьому випадку маскування, як правило, не відбувається, оскільки не існує поняття, що пізніші токени впливають на попередні.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "translatedText": "Залишаючись зосередженими на увазі до себе, якщо ви все зрозуміли, і якщо ви зупинитеся на цьому, ви зрозумієте суть того, чим насправді є увага.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "translatedText": "Все, що нам залишається, - це викласти сенс, в якому ви робите це багато-багато разів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "translatedText": "У нашому центральному прикладі ми зосередилися на прикметниках, які актуалізують іменники, але, звичайно, існує безліч різних способів, як контекст може впливати на значення слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "translatedText": "Якщо слова, які вони розбили, передують слову \"автомобіль\", це має значення для форми та структури цього автомобіля.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "input": "And a lot of associations might be less grammatical.",
  "translatedText": "І багато асоціацій можуть бути менш граматичними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "translatedText": "Якщо слово \"чарівник\" зустрічається в тому ж уривку, що й слово \"Гаррі\", то можна припустити, що йдеться про Гаррі Поттера, тоді як якщо замість нього в уривку зустрічаються слова \"королева\", \"Сассекський\" і \"Вільям\", то, можливо, слід замінити слово \"Гаррі\" на \"принц\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "translatedText": "Для кожного типу контекстного оновлення, який ви можете собі уявити, параметри цих матриць ключів і запитів будуть відрізнятися, щоб охопити різні моделі уваги, а параметри нашої карти цінностей будуть відрізнятися залежно від того, що слід додати до вбудовувань.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "translatedText": "І знову ж таки, на практиці справжню поведінку цих карт набагато складніше інтерпретувати, оскільки ваги встановлюються так, як потрібно моделі для найкращого досягнення її мети - передбачення наступного токена.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "translatedText": "Як я вже казав раніше, все, що ми описали, є однією головою уваги, а повний блок уваги всередині трансформатора складається з так званої багатоголової уваги, де ви виконуєте багато цих операцій паралельно, кожна з яких має свій власний ключовий запит і карти значень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "translatedText": "Наприклад, GPT-3 використовує 96 головок уваги всередині кожного блоку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "translatedText": "Враховуючи, що кожна з них вже трохи заплутана, це, безумовно, дуже багато, щоб утримати в голові.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "translatedText": "Щоб пояснити все дуже чітко, це означає, що у вас є 96 різних матриць ключів і запитів, які створюють 96 різних патернів уваги.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "translatedText": "Тоді кожна голова має свої власні матриці значень, які використовуються для створення 96 послідовностей векторів значень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "translatedText": "Всі вони додаються разом, використовуючи відповідні патерни уваги як ваги.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "translatedText": "Це означає, що для кожної позиції в контексті, кожної лексеми, кожна з цих головок створює запропоновану зміну, яку потрібно додати до вбудовування в цій позиції.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "translatedText": "Отже, ви підсумовуєте всі ці запропоновані зміни, по одній для кожного керівника, і додаєте результат до початкового варіанту впровадження цієї посади.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "translatedText": "Уся ця сума - лише один зріз того, що виходить з цього багатоголового блоку уваги, одна з тих витончених вкраплень, які вискакують з іншого його кінця.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "translatedText": "Знову ж таки, тут є над чим подумати, тому не хвилюйтеся, якщо вам знадобиться деякий час, щоб це усвідомити.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "translatedText": "Загальна ідея полягає в тому, що, запускаючи багато різних голів паралельно, ви даєте моделі можливість вивчити багато різних способів, якими контекст змінює значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "translatedText": "Якщо підбити підсумки підрахунку параметрів за допомогою 96 голів, кожна з яких включає власну варіацію цих чотирьох матриць, то кожен блок багатоголової уваги налічує близько 600 мільйонів параметрів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "translatedText": "Існує одна додаткова, трохи дратівлива річ, про яку я повинен згадати для тих з вас, хто продовжить читати далі про трансформатори.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "translatedText": "Ви пам'ятаєте, як я говорив, що карта цінностей розбивається на дві окремі матриці, які я позначив як матриці зниження та підвищення цінності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "translatedText": "Те, як я представив речі, припускає, що ви бачите цю пару матриць всередині кожної голови уваги, і ви можете абсолютно точно реалізувати це таким чином.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "input": "That would be a valid design.",
  "translatedText": "Це був би правильний дизайн.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "translatedText": "Але те, як ви бачите це на папері, і те, як це реалізується на практиці, виглядає дещо по-різному.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "translatedText": "Всі ці матриці значень для кожної голови з'єднуються в одну гігантську матрицю, яку ми називаємо вихідною матрицею, пов'язану з усім багатоголовим блоком уваги.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "translatedText": "І коли ви бачите, що люди посилаються на матрицю значень для певної голови уваги, вони зазвичай мають на увазі лише перший крок, той, який я позначив як проекцію значення вниз у менший простір.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "input": "For the curious among you, I've left an on-screen note about it.",
  "translatedText": "Для допитливих я залишив на екрані примітку про це.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "translatedText": "Це одна з тих деталей, яка ризикує відволікти від основних концептуальних моментів, але я хочу згадати про неї, щоб ви знали, якщо прочитаєте про це в інших джерелах.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "translatedText": "Якщо відкинути всі технічні нюанси, то в попередньому розділі ми бачили, що дані, які проходять через трансформатор, не просто проходять через один блок уваги.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "translatedText": "З одного боку, він також проходить через інші операції, які називаються багатошаровими перцептронами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "input": "We'll talk more about those in the next chapter.",
  "translatedText": "Детальніше про них ми поговоримо в наступному розділі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "translatedText": "А потім вона багаторазово проходить через багато-багато копій обох цих операцій.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "translatedText": "Це означає, що після того, як певне слово вбирає в себе частину контексту, існує набагато більше шансів, що на це більш нюансоване вбудовування вплине його більш нюансоване оточення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "translatedText": "Чим далі вниз по мережі, коли кожне вбудовування вбирає в себе все більше і більше значень від усіх інших вбудовувань, які в свою чергу стають все більш і більш нюансованими, є надія, що існує можливість кодування більш високого рівня і більш абстрактних ідей про даний вхід, що виходять за рамки просто дескрипторів і граматичної структури.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "translatedText": "Такі речі, як настрій і тон, а також те, чи це вірш, які основні наукові істини мають відношення до твору і тому подібні речі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "translatedText": "Повертаючись ще раз до нашого підрахунку, GPT-3 включає 96 різних шарів, тому загальна кількість ключових параметрів запитів і значень множиться ще на 96, що дає загальну суму трохи менше 58 мільярдів різних параметрів, присвячених усім головам, що привертають увагу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "translatedText": "Це, безумовно, багато, але це лише близько третини від 175 мільярдів, які знаходяться в мережі загалом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "translatedText": "Отже, хоча увага приділяється всім, більшість параметрів надходить від блоків, розташованих між цими кроками.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "translatedText": "У наступному розділі ми з вами поговоримо про ці блоки, а також про процес навчання.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "translatedText": "Велику роль в успіху механізму уваги відіграє не стільки якийсь конкретний тип поведінки, який він забезпечує, скільки той факт, що він надзвичайно розпаралелюється, а це означає, що ви можете виконувати величезну кількість обчислень за короткий час, використовуючи графічні процесори.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "translatedText": "Враховуючи, що одним з важливих уроків глибокого навчання за останні десятиліття або два було те, що масштабування саме по собі дає величезні якісні покращення продуктивності моделі, розпаралелювані архітектури, які дозволяють це зробити, мають величезну перевагу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "translatedText": "Якщо ви хочете дізнатися більше про ці речі, я залишив багато посилань в описі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "translatedText": "Зокрема, все, що виробляють Андрій Карпати або Кріс Ола, як правило, є чистим золотом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "translatedText": "У цьому відео я хотів просто привернути увагу в його нинішньому вигляді, але якщо вам цікаво дізнатися більше про історію того, як ми прийшли до цього, і як ви можете переосмислити цю ідею для себе, мій друг Вівек щойно виклав кілька відео, які дають набагато більше мотивації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "translatedText": "Крім того, Брітт Круз з каналу The Art of the Problem має дуже гарне відео про історію великих мовних моделей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "input": "Thank you.",
  "translatedText": "Дякую.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]