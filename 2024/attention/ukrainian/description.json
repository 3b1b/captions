[
 {
  "input": "Demystifying self-attention, multiple heads, and cross-attention.",
  "translatedText": "Демістифікація самоуваги, багатоголовості та перехресної уваги.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "translatedText": "Замість спонсорських рекламних читань, ці уроки фінансуються безпосередньо глядачами: https://3b1b.co/support",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "An equally valuable form of support is to simply share the videos.",
  "translatedText": "Не менш цінна форма підтримки - просто поділитися відео.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Other resources about transformers",
  "translatedText": "Інші ресурси про трансформатори",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Andrej Karpathy's videos",
  "translatedText": "Відео Андрія Карпатія",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The Transformer Circuits posts by Anthropic",
  "translatedText": "The Transformer Circuits posts by Anthropic",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://transformer-circuits.pub/2021/framework/index.html",
  "translatedText": "https://transformer-circuits.pub/2021/framework/index.html",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "In particular, it was only after I read this post that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources.",
  "translatedText": "Зокрема, лише після того, як я прочитав цю публікацію, я почав думати про комбінацію матриць значень і результатів як про комбіновану низькорангову карту з простору вбудовування до самої себе, що, принаймні, в моїй голові, зробило речі набагато зрозумілішими, ніж інші джерела.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "History of language models by Brit Cruise, @ArtOfTheProblem ",
  "translatedText": "Історія мовних моделей від Бріта Круза, @ArtOfTheProblem",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://youtu.be/OFS90-FX6pg",
  "translatedText": "https://youtu.be/OFS90-FX6pg",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "What is a Language Model by @vcubingx ",
  "translatedText": "Що таке мовна модель від @vcubingx",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "translatedText": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Site with exercises related to ML programming and GPTs",
  "translatedText": "Сайт з вправами, пов'язаними з ML-програмуванням та GPT",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://www.gptandchill.ai/codingproblems",
  "translatedText": "https://www.gptandchill.ai/codingproblems",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Early paper on how directions in embedding spaces have meaning:",
  "translatedText": "Рання стаття про те, як напрямки у просторі вбудовування мають значення:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://arxiv.org/pdf/1301.3781.pdf",
  "translatedText": "https://arxiv.org/pdf/1301.3781.pdf",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Timestamps:",
  "translatedText": "Мітки часу:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "0:00 - Recap on embeddings",
  "translatedText": "0:00 - Підсумки про вбудовування",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "1:39 - Motivating examples",
  "translatedText": "1:39 - Мотивуючі приклади",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "4:29 - The attention pattern",
  "translatedText": "4:29 - Структура уваги",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "11:08 - Masking",
  "translatedText": "11:08 - Маскування",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "12:42 - Context size",
  "translatedText": "12:42 - Розмір контексту",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "13:10 - Values",
  "translatedText": "13:10 - Цінності",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "15:44 - Counting parameters",
  "translatedText": "15:44 - Параметри підрахунку",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "18:21 - Cross-attention",
  "translatedText": "18:21 - Перехресна увага",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "19:19 - Multiple heads",
  "translatedText": "19:19 - Кілька голів",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "22:16 - The output matrix",
  "translatedText": "22:16 - Вихідна матриця",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "23:19 - Going deeper",
  "translatedText": "23:19 - Заглиблюємося",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "24:54 - Ending",
  "translatedText": "24:54 - Закінчення",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 }
]