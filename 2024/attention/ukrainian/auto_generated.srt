1
00:00:00,000 --> 00:00:04,019
У попередньому розділі ми з вами почали вивчати внутрішню будову трансформатора.

2
00:00:04,560 --> 00:00:07,431
Це одна з ключових технологій у великих мовних моделях, 

3
00:00:07,431 --> 00:00:10,200
а також багато інших інструментів у сучасній хвилі ШІ.

4
00:00:10,980 --> 00:00:15,211
Вперше він з'явився у відомій статті 2017 року під назвою "Увага - це все, 

5
00:00:15,211 --> 00:00:18,314
що вам потрібно", і в цій главі ми з вами розберемося, 

6
00:00:18,314 --> 00:00:21,700
що це за механізм уваги, візуалізуючи, як він обробляє дані.

7
00:00:26,140 --> 00:00:29,540
Якщо коротко, то ось важливий контекст, який я хочу, щоб ви мали на увазі.

8
00:00:30,000 --> 00:00:32,745
Мета моделі, яку ми з вами вивчаємо, полягає в тому, 

9
00:00:32,745 --> 00:00:36,060
щоб взяти шматок тексту і передбачити, яке слово буде наступним.

10
00:00:36,860 --> 00:00:40,927
Вхідний текст розбивається на маленькі шматочки, які ми називаємо токенами, 

11
00:00:40,927 --> 00:00:43,817
і дуже часто це слова або частини слів, але для того, 

12
00:00:43,817 --> 00:00:48,205
щоб нам з вами було легше думати над прикладами у цьому відео, давайте спростимо, 

13
00:00:48,205 --> 00:00:50,560
уявивши, що токени - це завжди просто слова.

14
00:00:51,480 --> 00:00:54,516
Першим кроком у трансформаторі є асоціювання кожного токена з 

15
00:00:54,516 --> 00:00:57,700
вектором високої розмірності, що ми називаємо його вбудовуванням.

16
00:00:57,700 --> 00:01:00,852
Найважливіша ідея, яку я хочу, щоб ви запам'ятали, - це те, 

17
00:01:00,852 --> 00:01:04,950
як напрямки в цьому багатовимірному просторі всіх можливих вбудовувань можуть 

18
00:01:04,950 --> 00:01:07,000
співвідноситися зі смисловим значенням.

19
00:01:07,680 --> 00:01:11,666
У попередньому розділі ми бачили приклад того, як напрямок може відповідати гендеру, 

20
00:01:11,666 --> 00:01:15,418
в тому сенсі, що додавання певного кроку в цьому просторі може привести вас від 

21
00:01:15,418 --> 00:01:19,640
вбудовування іменника чоловічого роду до вбудовування відповідного іменника жіночого роду.

22
00:01:20,160 --> 00:01:22,299
Це лише один приклад, і ви можете собі уявити, 

23
00:01:22,299 --> 00:01:25,759
скільки інших напрямків у цьому багатовимірному просторі можуть відповідати 

24
00:01:25,759 --> 00:01:27,580
численним іншим аспектам значення слова.

25
00:01:28,800 --> 00:01:33,147
Мета трансформатора полягає в тому, щоб поступово коригувати ці вкраплення так, 

26
00:01:33,147 --> 00:01:36,951
щоб вони не просто кодували окреме слово, а запікали в собі набагато, 

27
00:01:36,951 --> 00:01:39,180
набагато багатше контекстуальне значення.

28
00:01:40,140 --> 00:01:43,007
Заздалегідь скажу, що багато людей вважають механізм уваги, 

29
00:01:43,007 --> 00:01:45,587
цей ключовий елемент трансформатора, дуже заплутаним, 

30
00:01:45,587 --> 00:01:48,980
тому не хвилюйтеся, якщо вам знадобиться деякий час, щоб все зрозуміли.

31
00:01:49,440 --> 00:01:54,482
Я думаю, що перш ніж ми зануримося в обчислювальні деталі і всі матричні множення, 

32
00:01:54,482 --> 00:01:59,160
варто подумати про пару прикладів поведінки, на яку ми хочемо звернути увагу.

33
00:02:00,140 --> 00:02:03,029
Розглянемо фрази американська справжня родимка, 

34
00:02:03,029 --> 00:02:06,220
один моль вуглекислого газу та взяти біопсію родимки.

35
00:02:06,700 --> 00:02:09,927
Ми з вами знаємо, що слово "кріт" має різні значення в кожній з цих мов, 

36
00:02:09,927 --> 00:02:10,900
залежно від контексту.

37
00:02:11,360 --> 00:02:16,275
Але після першого кроку трансформатора, який розбиває текст і пов'язує кожну лексему з 

38
00:02:16,275 --> 00:02:20,852
вектором, вектор, який асоціюється з кротом, буде однаковим у всіх цих випадках, 

39
00:02:20,852 --> 00:02:25,711
тому що це початкове вбудовування лексеми є фактично таблицею пошуку без посилання на 

40
00:02:25,711 --> 00:02:26,220
контекст.

41
00:02:26,620 --> 00:02:29,797
Лише на наступному кроці трансформатора навколишні 

42
00:02:29,797 --> 00:02:33,100
вбудовування мають шанс передати інформацію до цього.

43
00:02:33,820 --> 00:02:38,948
Можливо, ви уявляєте собі, що в цьому просторі вбудовування є кілька різних напрямків, 

44
00:02:38,948 --> 00:02:41,778
які кодують кілька різних значень слова "кріт", 

45
00:02:41,778 --> 00:02:46,376
і що добре натренований блок уваги обчислює, що потрібно додати до загального 

46
00:02:46,376 --> 00:02:50,503
вбудовування, щоб перемістити його в один з цих конкретних напрямків, 

47
00:02:50,503 --> 00:02:51,800
залежно від контексту.

48
00:02:53,300 --> 00:02:56,180
Візьмемо інший приклад, розглянемо вбудовування слова "вежа".

49
00:02:57,060 --> 00:03:00,673
Ймовірно, це якийсь дуже загальний, неконкретний напрямок у просторі, 

50
00:03:00,673 --> 00:03:03,720
пов'язаний з багатьма іншими великими, високими іменниками.

51
00:03:04,020 --> 00:03:07,333
Якби цьому слову безпосередньо передувало слово "Ейфелева вежа", 

52
00:03:07,333 --> 00:03:10,902
можна було б уявити собі бажання, щоб механізм оновив цей вектор так, 

53
00:03:10,902 --> 00:03:14,522
щоб він вказував у напрямку, який більш конкретно кодує Ейфелеву вежу, 

54
00:03:14,522 --> 00:03:19,060
можливо, співвідносився з векторами, пов'язаними з Парижем і Францією та речами зі сталі.

55
00:03:19,920 --> 00:03:24,333
Якщо йому ще й передувало слово мініатюрний, то вектор слід ще більше актуалізувати, 

56
00:03:24,333 --> 00:03:27,500
щоб він більше не співвідносився з великими, високими речами.

57
00:03:29,480 --> 00:03:32,798
У більш широкому сенсі, ніж просто уточнення значення слова, 

58
00:03:32,798 --> 00:03:37,369
блок уваги дозволяє моделі переміщати інформацію, закодовану в одному вбудовуванні, 

59
00:03:37,369 --> 00:03:42,102
в інше, потенційно досить віддалене, і потенційно з інформацією, яка набагато багатша, 

60
00:03:42,102 --> 00:03:43,300
ніж просто одне слово.

61
00:03:43,300 --> 00:03:48,658
У попередньому розділі ми бачили, як після того, як всі вектори проходять через мережу, 

62
00:03:48,658 --> 00:03:52,799
включаючи багато різних блоків уваги, обчислення, які ви виконуєте, 

63
00:03:52,799 --> 00:03:58,280
щоб передбачити наступний токен, повністю залежать від останнього вектора в послідовності.

64
00:03:59,100 --> 00:04:03,391
Уявіть, наприклад, що текст, який ви вводите, - це більша частина цілого 

65
00:04:03,391 --> 00:04:07,800
детективного роману, аж до точки в кінці, яка говорить: "Отже, вбивця був".

66
00:04:08,400 --> 00:04:11,495
Якщо модель збирається точно передбачити наступне слово, 

67
00:04:11,495 --> 00:04:16,219
цей кінцевий вектор послідовності, який починав своє життя просто вбудовуванням слова, 

68
00:04:16,219 --> 00:04:20,454
повинен бути оновлений усіма блоками уваги, щоб представляти набагато більше, 

69
00:04:20,454 --> 00:04:24,473
ніж будь-яке окреме слово, якимось чином кодуючи всю інформацію з повного 

70
00:04:24,473 --> 00:04:28,220
вікна контексту, яка має відношення до передбачення наступного слова.

71
00:04:29,500 --> 00:04:32,580
Щоб розібратися в розрахунках, давайте візьмемо набагато простіший приклад.

72
00:04:32,980 --> 00:04:37,960
Уявіть, що вхідні дані містять фразу "Пухнасте блакитне створіння блукало зеленим лісом".

73
00:04:38,460 --> 00:04:42,791
Наразі припустімо, що єдиний тип оновлення, який нас цікавить, 

74
00:04:42,791 --> 00:04:46,780
- це зміна прикметників у значеннях відповідних іменників.

75
00:04:47,000 --> 00:04:50,665
Те, що я збираюся описати, - це те, що ми називаємо однією головою уваги, 

76
00:04:50,665 --> 00:04:54,280
а пізніше ми побачимо, як блок уваги складається з безлічі різних голів, 

77
00:04:54,280 --> 00:04:55,420
що працюють паралельно.

78
00:04:56,140 --> 00:04:59,809
Знову ж таки, початкове вбудовування для кожного слова - це деякий вектор 

79
00:04:59,809 --> 00:05:03,380
високої розмірності, який кодує лише значення цього слова без контексту.

80
00:05:04,000 --> 00:05:05,220
Насправді, це не зовсім так.

81
00:05:05,380 --> 00:05:07,640
Вони також кодують позицію слова.

82
00:05:07,980 --> 00:05:11,201
Можна ще багато чого сказати про те, як кодуються позиції, 

83
00:05:11,201 --> 00:05:15,623
але зараз все, що вам потрібно знати, це те, що записів цього вектора достатньо, 

84
00:05:15,623 --> 00:05:18,900
щоб сказати вам, що це за слово і де воно існує в контексті.

85
00:05:19,500 --> 00:05:21,660
Позначимо ці вбудовування літерою e.

86
00:05:22,420 --> 00:05:25,992
Мета полягає в тому, щоб в результаті серії обчислень отримати 

87
00:05:25,992 --> 00:05:29,054
новий уточнений набір вбудовувань, де, наприклад, ті, 

88
00:05:29,054 --> 00:05:33,420
що відповідають іменникам, ввібрали в себе значення відповідних прикметників.

89
00:05:33,900 --> 00:05:37,118
Граючи в гру глибокого навчання, ми хочемо, щоб більшість обчислень 

90
00:05:37,118 --> 00:05:40,856
виглядали як матрично-векторні добутки, де матриці повні вагових коефіцієнтів, 

91
00:05:40,856 --> 00:05:43,980
що налаштовуються, тобто речей, які модель вивчає на основі даних.

92
00:05:44,660 --> 00:05:47,633
Щоб було зрозуміло, я вигадую цей приклад, коли прикметники доповнюють іменники, 

93
00:05:47,633 --> 00:05:50,460
лише для того, щоб проілюструвати тип поведінки, який ви можете собі уявити, 

94
00:05:50,460 --> 00:05:52,260
як поводиться голова, що концентрується на увазі.

95
00:05:52,860 --> 00:05:55,548
Як і в більшості випадків глибокого навчання, справжню поведінку 

96
00:05:55,548 --> 00:05:58,444
набагато складніше розібрати, оскільки вона базується на налаштуванні 

97
00:05:58,444 --> 00:06:01,340
величезної кількості параметрів для мінімізації певної функції витрат.

98
00:06:01,680 --> 00:06:05,206
Просто, коли ми перебираємо різні матриці, заповнені параметрами, 

99
00:06:05,206 --> 00:06:09,854
які беруть участь у цьому процесі, я вважаю, що дуже корисно мати уявний приклад того, 

100
00:06:09,854 --> 00:06:13,220
що він може робити, щоб допомогти зробити все більш конкретним.

101
00:06:14,140 --> 00:06:17,992
На першому етапі цього процесу ви можете уявити, що кожен іменник, 

102
00:06:17,992 --> 00:06:21,960
як істота, ставить запитання: "Гей, а чи є переді мною прикметники?".

103
00:06:22,160 --> 00:06:25,628
А на слова пухнастий і блакитний, щоб кожен зміг відповісти: 

104
00:06:25,628 --> 00:06:27,960
так, я прикметник і я в такому положенні.

105
00:06:28,960 --> 00:06:32,374
Це питання якимось чином закодоване як ще один вектор, 

106
00:06:32,374 --> 00:06:36,100
ще один список чисел, який ми називаємо запитом на це слово.

107
00:06:36,980 --> 00:06:42,020
Цей вектор запиту має набагато меншу розмірність, ніж вектор вбудовування, скажімо, 128.

108
00:06:42,940 --> 00:06:46,775
Обчислення цього запиту виглядає так: беремо певну матрицю, 

109
00:06:46,775 --> 00:06:49,780
яку я позначу wq, і множимо її на вбудовування.

110
00:06:50,960 --> 00:06:55,277
Для стислості, давайте запишемо вектор запиту як q, і тоді кожного разу, 

111
00:06:55,277 --> 00:06:59,654
коли ви бачите, що я ставлю матрицю поруч зі стрілкою, як ця, це означає, 

112
00:06:59,654 --> 00:07:04,800
що множення цієї матриці на вектор на початку стрілки дасть вам вектор в кінці стрілки.

113
00:07:05,860 --> 00:07:09,869
У цьому випадку ви множите цю матрицю на всі вбудовування в контексті, 

114
00:07:09,869 --> 00:07:12,580
створюючи один вектор запиту для кожного токена.

115
00:07:13,740 --> 00:07:16,640
Елементи цієї матриці є параметрами моделі, а це означає, 

116
00:07:16,640 --> 00:07:19,890
що справжню поведінку можна дізнатися з даних, і на практиці те, 

117
00:07:19,890 --> 00:07:23,440
що робить ця матриця в конкретній голові уваги, складно проаналізувати.

118
00:07:23,900 --> 00:07:27,964
Але для прикладу, який, як ми сподіваємося, він навчиться, припустимо, 

119
00:07:27,964 --> 00:07:32,544
що ця матриця запитів відображає включення іменників у певних напрямках у цьому 

120
00:07:32,544 --> 00:07:37,524
меншому просторі запитів, що якимось чином кодує ідею пошуку прикметників у попередніх 

121
00:07:37,524 --> 00:07:38,040
позиціях.

122
00:07:38,780 --> 00:07:41,440
Щодо того, як це впливає на інші вбудовування, хто знає?

123
00:07:41,720 --> 00:07:44,340
Можливо, вона одночасно намагається досягти якоїсь іншої мети за допомогою цих засобів.

124
00:07:44,540 --> 00:07:47,160
Зараз ми зосереджені на іменниках.

125
00:07:47,280 --> 00:07:51,811
В той же час, з нею пов'язана друга матриця, яка називається ключовою, 

126
00:07:51,811 --> 00:07:54,620
яку ви також множите на кожне з вбудовувань.

127
00:07:55,280 --> 00:07:58,500
Це створює другу послідовність векторів, які ми називаємо ключами.

128
00:07:59,420 --> 00:08:03,140
Концептуально, ви хочете думати про ключі як про потенційні відповіді на запити.

129
00:08:03,840 --> 00:08:07,312
Ця ключова матриця також має безліч параметрів, що налаштовуються, і так само, 

130
00:08:07,312 --> 00:08:10,872
як і матриця запитів, відображає вектори вбудовування в той самий простір меншої 

131
00:08:10,872 --> 00:08:11,400
розмірності.

132
00:08:12,200 --> 00:08:17,020
Ви вважаєте, що ключі відповідають запитам, коли вони тісно пов'язані один з одним.

133
00:08:17,460 --> 00:08:22,155
У нашому прикладі можна уявити, що матриця ключів відображає прикметники "пухнастий" 

134
00:08:22,155 --> 00:08:26,740
і "синій" на вектори, які тісно пов'язані із запитом, створеним за словом "істота".

135
00:08:27,200 --> 00:08:30,600
Щоб виміряти, наскільки добре кожен ключ відповідає кожному запиту, 

136
00:08:30,600 --> 00:08:34,000
ви обчислюєте точковий добуток між кожною можливою парою ключ-запит.

137
00:08:34,480 --> 00:08:37,599
Мені подобається візуалізувати сітку, заповнену купою точок, 

138
00:08:37,599 --> 00:08:40,923
де більші точки відповідають більшим точковим продуктам, місцям, 

139
00:08:40,923 --> 00:08:42,559
де ключі та запити узгоджуються.

140
00:08:43,280 --> 00:08:48,409
Для нашого прикладу з прикметником та іменником це виглядатиме дещо інакше: якщо ключі, 

141
00:08:48,409 --> 00:08:52,373
створені за допомогою fluffy та blue, дійсно збігаються із запитом, 

142
00:08:52,373 --> 00:08:57,270
створеним за допомогою creature, то добутки точок у цих двох точках будуть великими 

143
00:08:57,270 --> 00:08:58,320
додатними числами.

144
00:08:59,100 --> 00:09:02,013
На жаргоні людей, які займаються машинним навчанням, це означає, 

145
00:09:02,013 --> 00:09:05,420
що вбудовування пухнастого і блакитного відповідають за вбудовування істоти.

146
00:09:06,040 --> 00:09:09,884
На відміну від крапкового добутку між ключем для якогось іншого слова, 

147
00:09:09,884 --> 00:09:14,325
наприклад, the, і запитом для creature буде якесь невелике або від'ємне значення, 

148
00:09:14,325 --> 00:09:16,600
що відображає не пов'язані між собою речі.

149
00:09:17,700 --> 00:09:21,212
Отже, ми маємо цю сітку значень, яка може бути будь-яким дійсним числом 

150
00:09:21,212 --> 00:09:24,724
від від'ємної нескінченності до нескінченності, що дає нам оцінку того, 

151
00:09:24,724 --> 00:09:28,480
наскільки важливим є кожне слово для оновлення значення кожного іншого слова.

152
00:09:29,200 --> 00:09:32,307
Спосіб, яким ми збираємося використовувати ці бали, полягає в тому, 

153
00:09:32,307 --> 00:09:35,780
щоб взяти певну зважену суму по кожному стовпчику, зважену на релевантність.

154
00:09:36,520 --> 00:09:40,140
Отже, замість того, щоб значення варіювалися від від'ємної нескінченності до 

155
00:09:40,140 --> 00:09:44,042
нескінченності, ми хочемо, щоб числа в цих стовпчиках були в діапазоні від 0 до 1, 

156
00:09:44,042 --> 00:09:48,180
і в кожному стовпчику в сумі дорівнювали 1, як якщо б вони були розподілом ймовірностей.

157
00:09:49,280 --> 00:09:52,220
Якщо ви читали попередній розділ, то знаєте, що нам потрібно зробити.

158
00:09:52,620 --> 00:09:57,300
Ми обчислюємо softmax для кожного з цих стовпців, щоб нормалізувати значення.

159
00:10:00,060 --> 00:10:03,455
На нашому малюнку, після застосування функції softmax до всіх стовпців, 

160
00:10:03,455 --> 00:10:05,860
ми заповнимо сітку цими нормалізованими значеннями.

161
00:10:06,780 --> 00:10:11,229
На цьому етапі ви можете вважати, що кожен стовпчик має вагу відповідно до того, 

162
00:10:11,229 --> 00:10:14,580
наскільки слово зліва релевантне відповідному значенню вгорі.

163
00:10:15,080 --> 00:10:16,840
Ми називаємо цю сітку патерном уваги.

164
00:10:18,080 --> 00:10:20,335
Якщо ви подивитеся на оригінальний трансформаторний папір, 

165
00:10:20,335 --> 00:10:22,820
то побачите дуже компактний спосіб, у який вони все це записують.

166
00:10:23,880 --> 00:10:29,260
Тут змінні q та k представляють повні масиви векторів запиту та ключів відповідно, 

167
00:10:29,260 --> 00:10:34,640
ті маленькі вектори, які ви отримуєте, множачи вкладки на матриці запиту та ключів.

168
00:10:35,160 --> 00:10:39,089
Цей вираз у чисельнику є дійсно компактним способом представлення 

169
00:10:39,089 --> 00:10:43,020
сітки всіх можливих точкових добутків між парами ключів і запитів.

170
00:10:44,000 --> 00:10:47,268
Невелика технічна деталь, про яку я не згадав, полягає в тому, 

171
00:10:47,268 --> 00:10:50,536
що для числової стабільності корисно ділити всі ці значення на 

172
00:10:50,536 --> 00:10:53,960
квадратний корінь з розмірності в цьому просторі ключових запитів.

173
00:10:54,480 --> 00:10:57,513
Тоді цей софтмакс, який обертається навколо повного виразу, 

174
00:10:57,513 --> 00:11:00,800
слід розуміти як такий, що застосовується стовпчик за стовпчиком.

175
00:11:01,640 --> 00:11:04,700
Щодо цього v-терміну, ми поговоримо про нього за секунду.

176
00:11:05,020 --> 00:11:08,460
Перед цим є ще одна технічна деталь, яку я поки що пропустив.

177
00:11:09,040 --> 00:11:12,776
Під час навчання, коли ви запускаєте цю модель на заданому прикладі тексту, 

178
00:11:12,776 --> 00:11:15,087
і всі ваги трохи коригуються і налаштовуються, 

179
00:11:15,087 --> 00:11:17,448
щоб заохочувати або карати її залежно від того, 

180
00:11:17,448 --> 00:11:21,430
наскільки високу ймовірність вона призначає істинному наступному слову в уривку, 

181
00:11:21,430 --> 00:11:24,626
виявляється, що весь процес навчання стає набагато ефективнішим, 

182
00:11:24,626 --> 00:11:28,265
якщо ви одночасно змушуєте її передбачати кожну можливу наступну лексему, 

183
00:11:28,265 --> 00:11:31,560
що слідує за кожною початковою послідовністю лексем в цьому уривку.

184
00:11:31,940 --> 00:11:35,652
Наприклад, у фразі, на якій ми зосередилися, можна також передбачити, 

185
00:11:35,652 --> 00:11:39,100
які слова слідують за словом "істота", а які - за словом "після".

186
00:11:39,940 --> 00:11:42,796
Це дуже приємно, адже це означає, що те, що в іншому випадку 

187
00:11:42,796 --> 00:11:45,560
було б одним навчальним прикладом, ефективно діє як багато.

188
00:11:46,100 --> 00:11:50,793
Для цілей нашої моделі уваги це означає, що ви ніколи не повинні дозволяти наступним 

189
00:11:50,793 --> 00:11:55,322
словам впливати на попередні, оскільки інакше вони можуть видати відповідь на те, 

190
00:11:55,322 --> 00:11:56,040
що буде далі.

191
00:11:56,560 --> 00:12:00,713
Це означає, що ми хочемо, щоб всі ці плями, які представляють пізніші токени, 

192
00:12:00,713 --> 00:12:04,600
що впливають на попередні, якимось чином були примушені дорівнювати нулю.

193
00:12:05,920 --> 00:12:09,444
Найпростіше, що ви можете зробити, це встановити їх рівними нулю, але якщо ви це зробите, 

194
00:12:09,444 --> 00:12:12,420
то стовпці більше не складатимуться в одиницю, вони не будуть нормалізовані.

195
00:12:13,120 --> 00:12:16,115
Натомість, найпоширеніший спосіб зробити це - перед застосуванням 

196
00:12:16,115 --> 00:12:19,020
softmax встановити для всіх цих записів від'ємну нескінченність.

197
00:12:19,680 --> 00:12:23,407
Якщо ви це зробите, то після застосування softmax всі вони перетворяться на нуль, 

198
00:12:23,407 --> 00:12:25,180
але стовпці залишаться нормалізованими.

199
00:12:26,000 --> 00:12:27,540
Цей процес називається маскуванням.

200
00:12:27,540 --> 00:12:31,216
Існують версії уваги, де ви не застосовуєте її, але в нашому прикладі GPT, 

201
00:12:31,216 --> 00:12:34,598
навіть незважаючи на те, що це більш доречно на етапі навчання, ніж, 

202
00:12:34,598 --> 00:12:38,960
скажімо, запуск його як чат-бота або щось подібне, ви завжди застосовуєте це маскування, 

203
00:12:38,960 --> 00:12:41,460
щоб запобігти впливу пізніших токенів на попередні.

204
00:12:42,480 --> 00:12:46,658
Ще один факт, над яким варто замислитися щодо цього шаблону уваги - це те, 

205
00:12:46,658 --> 00:12:49,500
що його розмір дорівнює квадрату розміру контексту.

206
00:12:49,900 --> 00:12:52,672
Ось чому розмір контексту може бути справді величезним вузьким 

207
00:12:52,672 --> 00:12:55,620
місцем у великих мовних моделях, і масштабування його нетривіальне.

208
00:12:56,300 --> 00:13:00,190
Як ви розумієте, в останні роки, мотивовані бажанням мати все більші і більші 

209
00:13:00,190 --> 00:13:04,030
вікна контексту, з'явилися деякі варіації механізму уваги, спрямовані на те, 

210
00:13:04,030 --> 00:13:08,320
щоб зробити контекст більш масштабованим, але зараз ми з вами зосередимося на основах.

211
00:13:10,560 --> 00:13:13,914
Гаразд, чудово, обчислення цього шаблону дозволяє моделі зробити висновок, 

212
00:13:13,914 --> 00:13:15,480
які слова релевантні до інших слів.

213
00:13:16,020 --> 00:13:19,120
Тепер вам потрібно оновити вбудовування, дозволивши словам 

214
00:13:19,120 --> 00:13:22,800
передавати інформацію тим іншим словам, до яких вони мають відношення.

215
00:13:22,800 --> 00:13:26,888
Наприклад, ви хочете, щоб вбудовування Пухнастого якимось чином спричинило 

216
00:13:26,888 --> 00:13:30,976
зміну в Істоті, яка перемістить його в іншу частину цього 12,000-вимірного 

217
00:13:30,976 --> 00:13:34,520
простору вбудовування, яка більш конкретно кодує Пухнасту істоту.

218
00:13:35,460 --> 00:13:39,734
Спочатку я покажу вам найпростіший спосіб, яким ви можете це зробити, 

219
00:13:39,734 --> 00:13:43,460
хоча в контексті багатоголової уваги він дещо видозмінюється.

220
00:13:44,080 --> 00:13:47,003
Найпростішим способом буде використання третьої матриці, 

221
00:13:47,003 --> 00:13:51,260
яку ми називаємо матрицею значень, яку ви помножите на вбудовування першого слова, 

222
00:13:51,260 --> 00:13:52,440
наприклад, "Пухнастий".

223
00:13:53,300 --> 00:13:56,141
Результатом цього є те, що можна назвати вектором значення, 

224
00:13:56,141 --> 00:13:58,699
і це те, що ви додаєте до вбудовування другого слова, 

225
00:13:58,699 --> 00:14:01,920
в даному випадку те, що ви додаєте до вбудовування слова "Творіння".

226
00:14:02,600 --> 00:14:07,000
Отже, цей вектор значень живе в тому ж дуже високовимірному просторі, що й вбудовування.

227
00:14:07,460 --> 00:14:12,784
Коли ви множите цю матрицю значень на вбудовування слова, ви можете подумати про це так: 

228
00:14:12,784 --> 00:14:16,852
якщо це слово має відношення до коригування значення чогось іншого, 

229
00:14:16,852 --> 00:14:21,160
то що саме слід додати до вбудовування цього іншого, щоб відобразити це?

230
00:14:22,140 --> 00:14:26,977
Озираючись на нашу діаграму, давайте відкладемо всі ключі та запити, оскільки після того, 

231
00:14:26,977 --> 00:14:29,718
як ви обчислите шаблон уваги, ви закінчите з ними, 

232
00:14:29,718 --> 00:14:33,695
ви візьмете цю матрицю значень і помножите її на кожне з цих вбудовувань, 

233
00:14:33,695 --> 00:14:36,060
щоб отримати послідовність векторів значень.

234
00:14:37,120 --> 00:14:39,141
Ви можете думати про ці вектори значень як про 

235
00:14:39,141 --> 00:14:41,120
певним чином пов'язані з відповідними ключами.

236
00:14:42,320 --> 00:14:45,748
Для кожного стовпчика на цій діаграмі ви множите кожен 

237
00:14:45,748 --> 00:14:49,240
з векторів значень на відповідну вагу в цьому стовпчику.

238
00:14:50,080 --> 00:14:53,808
Наприклад, тут, під час вбудовування Істоти, ви додасте великі 

239
00:14:53,808 --> 00:14:56,766
частки векторів значень для Пухнастого і Синього, 

240
00:14:56,766 --> 00:15:01,560
тоді як всі інші вектори значень будуть обнулені, або, принаймні, майже обнулені.

241
00:15:02,120 --> 00:15:05,984
І, нарешті, спосіб оновити вбудовування, пов'язане з цим стовпчиком, 

242
00:15:05,984 --> 00:15:09,681
попередньо закодувавши деяке контекстно-вільне значення Creature, 

243
00:15:09,681 --> 00:15:13,826
ви додаєте всі ці перемасштабовані значення в стовпчику, створюючи зміну, 

244
00:15:13,826 --> 00:15:16,627
яку ви хочете додати, і яку я позначу як delta-e, 

245
00:15:16,627 --> 00:15:19,260
а потім додаєте її до початкового вбудовування.

246
00:15:19,680 --> 00:15:22,651
Сподіваємося, що в результаті вийде більш витончений вектор, 

247
00:15:22,651 --> 00:15:26,500
що кодує більш контекстуально багате значення, як у пухнастої блакитної істоти.

248
00:15:27,380 --> 00:15:30,414
І, звичайно, ви робите це не лише з одним вбудовуванням, 

249
00:15:30,414 --> 00:15:34,514
ви застосовуєте ту саму зважену суму до всіх стовпчиків на цьому зображенні, 

250
00:15:34,514 --> 00:15:38,721
створюючи послідовність змін, додаючи всі ці зміни до відповідних вбудовувань, 

251
00:15:38,721 --> 00:15:43,460
створюючи повну послідовність більш досконалих вбудовувань, що з'являються з блоку уваги.

252
00:15:44,860 --> 00:15:49,100
Зменшуючи масштаб, весь цей процес можна описати як єдиний центр уваги.

253
00:15:49,600 --> 00:15:53,946
Як я вже описував, цей процес параметризується трьома окремими матрицями, 

254
00:15:53,946 --> 00:15:58,940
кожна з яких заповнена параметрами, що налаштовуються, - ключем, запитом і значенням.

255
00:15:59,500 --> 00:16:01,679
Я хочу скористатися моментом, щоб продовжити те, 

256
00:16:01,679 --> 00:16:04,214
що ми почали в попередньому розділі, з підрахунку балів, 

257
00:16:04,214 --> 00:16:08,040
де ми підраховуємо загальну кількість параметрів моделі, використовуючи числа з GPT-3.

258
00:16:09,300 --> 00:16:12,656
Кожна з цих матриць ключів і запитів має 12 288 стовпців, 

259
00:16:12,656 --> 00:16:15,780
що відповідає розмірності вбудовування, і 128 рядків, 

260
00:16:15,780 --> 00:16:19,600
що відповідає розмірності цього меншого простору ключових запитів.

261
00:16:20,260 --> 00:16:24,220
Це дає нам додаткові 1,5 мільйона параметрів для кожного з них.

262
00:16:24,860 --> 00:16:30,337
Якщо ви подивитеся на цю матрицю значень з іншого боку, то з того, як я описав її досі, 

263
00:16:30,337 --> 00:16:35,566
можна припустити, що це квадратна матриця, яка має 12 288 стовпців і 12 288 рядків, 

264
00:16:35,566 --> 00:16:40,920
оскільки і її входи, і виходи знаходяться в цьому дуже великому просторі вбудовування.

265
00:16:41,500 --> 00:16:45,140
Якщо це правда, то це означає близько 150 мільйонів доданих параметрів.

266
00:16:45,660 --> 00:16:47,300
І якщо бути точним, ви можете це зробити.

267
00:16:47,420 --> 00:16:51,740
Ви можете присвятити карті значень на порядки більше параметрів, ніж ключу і запиту.

268
00:16:52,060 --> 00:16:55,995
Але на практиці набагато ефективніше зробити так, щоб кількість параметрів, 

269
00:16:55,995 --> 00:16:59,361
присвячених цій карті значень, дорівнювала кількості параметрів, 

270
00:16:59,361 --> 00:17:00,760
присвячених ключу і запиту.

271
00:17:01,460 --> 00:17:05,160
Це особливо актуально в умовах паралельної роботи декількох головок уваги.

272
00:17:06,240 --> 00:17:10,099
Це виглядає так: карта цінностей факторизується як добуток двох менших матриць.

273
00:17:11,180 --> 00:17:15,083
Концептуально, я б рекомендував вам подумати про загальну лінійну карту, 

274
00:17:15,083 --> 00:17:19,468
яка має входи і виходи, обидва в цьому більшому просторі вбудовування, наприклад, 

275
00:17:19,468 --> 00:17:23,800
вбудовування синього кольору в цей напрямок синяви, який ви додасте до іменників.

276
00:17:27,040 --> 00:17:30,874
Це просто менша кількість рядків, зазвичай такого ж розміру, 

277
00:17:30,874 --> 00:17:32,760
як і простір ключового запиту.

278
00:17:33,100 --> 00:17:35,863
Це означає, що ви можете думати про це як про відображення 

279
00:17:35,863 --> 00:17:38,440
великих векторів вбудовування на значно менший простір.

280
00:17:39,040 --> 00:17:42,700
Це не загальноприйнята назва, але я буду називати її матрицею зниження значень.

281
00:17:43,400 --> 00:17:47,432
Друга матриця відображає з цього меншого простору назад до простору вбудовування, 

282
00:17:47,432 --> 00:17:50,580
створюючи вектори, які ви використовуєте для фактичних оновлень.

283
00:17:51,000 --> 00:17:54,740
Я буду називати цю матрицю матрицею підняття значень, що знову ж таки не є традиційним.

284
00:17:55,160 --> 00:17:58,080
Те, як це написано в більшості газет, виглядає дещо інакше.

285
00:17:58,380 --> 00:17:59,520
Я розповім про це за хвилину.

286
00:17:59,700 --> 00:18:02,540
На мою думку, це робить речі трохи більш концептуально заплутаними.

287
00:18:03,260 --> 00:18:06,522
Використовуючи жаргон лінійної алгебри, ми, по суті, 

288
00:18:06,522 --> 00:18:10,340
обмежуємо загальну карту значень перетворенням низького рангу.

289
00:18:11,420 --> 00:18:16,017
Повертаючись до кількості параметрів, всі чотири матриці мають однаковий розмір, і, 

290
00:18:16,017 --> 00:18:20,780
склавши їх разом, ми отримаємо близько 6,3 мільйона параметрів для однієї голови уваги.

291
00:18:22,040 --> 00:18:24,641
Якщо бути трохи точнішим, то все описане вище - це те, 

292
00:18:24,641 --> 00:18:28,236
що люди називають головкою з самоуважністю, щоб відрізнити її від варіації, 

293
00:18:28,236 --> 00:18:31,500
яка зустрічається в інших моделях, що називається перехресною увагою.

294
00:18:32,300 --> 00:18:35,900
Це не стосується нашого прикладу з GPT, але якщо вам цікаво, 

295
00:18:35,900 --> 00:18:40,209
перехресна увага передбачає моделі, які обробляють два різні типи даних, 

296
00:18:40,209 --> 00:18:43,219
наприклад, текст однією мовою і текст іншою мовою, 

297
00:18:43,219 --> 00:18:46,701
що є частиною поточного покоління перекладу, або, можливо, 

298
00:18:46,701 --> 00:18:49,240
аудіозапис мовлення і поточна транскрипція.

299
00:18:50,400 --> 00:18:52,700
Голова з перехресною увагою виглядає майже ідентично.

300
00:18:52,980 --> 00:18:57,400
Єдина відмінність полягає в тому, що карти ключів і запитів діють на різні набори даних.

301
00:18:57,840 --> 00:19:02,684
Наприклад, у моделі, що виконує переклад, ключі можуть бути з однієї мови, 

302
00:19:02,684 --> 00:19:05,978
а запити - з іншої, і модель уваги може описувати, 

303
00:19:05,978 --> 00:19:09,660
які слова з однієї мови відповідають яким словам в іншій.

304
00:19:10,340 --> 00:19:13,127
І в цьому випадку маскування, як правило, не відбувається, 

305
00:19:13,127 --> 00:19:16,340
оскільки не існує поняття, що пізніші токени впливають на попередні.

306
00:19:17,180 --> 00:19:20,876
Залишаючись зосередженими на увазі до себе, якщо ви все зрозуміли, 

307
00:19:20,876 --> 00:19:25,180
і якщо ви зупинитеся на цьому, ви зрозумієте суть того, чим насправді є увага.

308
00:19:25,760 --> 00:19:31,440
Все, що нам залишається, - це викласти сенс, в якому ви робите це багато-багато разів.

309
00:19:32,100 --> 00:19:34,837
У нашому центральному прикладі ми зосередилися на прикметниках, 

310
00:19:34,837 --> 00:19:37,917
які актуалізують іменники, але, звичайно, існує безліч різних способів, 

311
00:19:37,917 --> 00:19:39,800
як контекст може впливати на значення слова.

312
00:19:40,360 --> 00:19:43,546
Якщо слова, які вони розбили, передують слову "автомобіль", 

313
00:19:43,546 --> 00:19:46,520
це має значення для форми та структури цього автомобіля.

314
00:19:47,200 --> 00:19:49,280
І багато асоціацій можуть бути менш граматичними.

315
00:19:49,760 --> 00:19:53,729
Якщо слово "чарівник" зустрічається в тому ж уривку, що й слово "Гаррі", 

316
00:19:53,729 --> 00:19:56,501
то можна припустити, що йдеться про Гаррі Поттера, 

317
00:19:56,501 --> 00:20:00,199
тоді як якщо замість нього в уривку зустрічаються слова "королева", 

318
00:20:00,199 --> 00:20:04,440
"Сассекський" і "Вільям", то, можливо, слід замінити слово "Гаррі" на "принц".

319
00:20:05,040 --> 00:20:08,739
Для кожного типу контекстного оновлення, який ви можете собі уявити, 

320
00:20:08,739 --> 00:20:11,955
параметри цих матриць ключів і запитів будуть відрізнятися, 

321
00:20:11,955 --> 00:20:16,566
щоб охопити різні моделі уваги, а параметри нашої карти цінностей будуть відрізнятися 

322
00:20:16,566 --> 00:20:19,140
залежно від того, що слід додати до вбудовувань.

323
00:20:19,980 --> 00:20:23,109
І знову ж таки, на практиці справжню поведінку цих карт набагато 

324
00:20:23,109 --> 00:20:25,998
складніше інтерпретувати, оскільки ваги встановлюються так, 

325
00:20:25,998 --> 00:20:30,140
як потрібно моделі для найкращого досягнення її мети - передбачення наступного токена.

326
00:20:31,400 --> 00:20:35,043
Як я вже казав раніше, все, що ми описали, є однією головою уваги, 

327
00:20:35,043 --> 00:20:39,611
а повний блок уваги всередині трансформатора складається з так званої багатоголової 

328
00:20:39,611 --> 00:20:42,602
уваги, де ви виконуєте багато цих операцій паралельно, 

329
00:20:42,602 --> 00:20:45,920
кожна з яких має свій власний ключовий запит і карти значень.

330
00:20:47,420 --> 00:20:51,700
Наприклад, GPT-3 використовує 96 головок уваги всередині кожного блоку.

331
00:20:52,020 --> 00:20:54,194
Враховуючи, що кожна з них вже трохи заплутана, 

332
00:20:54,194 --> 00:20:56,460
це, безумовно, дуже багато, щоб утримати в голові.

333
00:20:56,760 --> 00:21:02,469
Щоб пояснити все дуже чітко, це означає, що у вас є 96 різних матриць ключів і запитів, 

334
00:21:02,469 --> 00:21:05,000
які створюють 96 різних патернів уваги.

335
00:21:05,440 --> 00:21:08,280
Тоді кожна голова має свої власні матриці значень, 

336
00:21:08,280 --> 00:21:12,180
які використовуються для створення 96 послідовностей векторів значень.

337
00:21:12,460 --> 00:21:16,680
Всі вони додаються разом, використовуючи відповідні патерни уваги як ваги.

338
00:21:17,480 --> 00:21:21,190
Це означає, що для кожної позиції в контексті, кожної лексеми, 

339
00:21:21,190 --> 00:21:24,075
кожна з цих головок створює запропоновану зміну, 

340
00:21:24,075 --> 00:21:27,020
яку потрібно додати до вбудовування в цій позиції.

341
00:21:27,660 --> 00:21:31,906
Отже, ви підсумовуєте всі ці запропоновані зміни, по одній для кожного керівника, 

342
00:21:31,906 --> 00:21:35,480
і додаєте результат до початкового варіанту впровадження цієї посади.

343
00:21:36,660 --> 00:21:42,596
Уся ця сума - лише один зріз того, що виходить з цього багатоголового блоку уваги, 

344
00:21:42,596 --> 00:21:47,460
одна з тих витончених вкраплень, які вискакують з іншого його кінця.

345
00:21:48,320 --> 00:21:50,352
Знову ж таки, тут є над чим подумати, тому не хвилюйтеся, 

346
00:21:50,352 --> 00:21:52,140
якщо вам знадобиться деякий час, щоб це усвідомити.

347
00:21:52,380 --> 00:21:56,732
Загальна ідея полягає в тому, що, запускаючи багато різних голів паралельно, 

348
00:21:56,732 --> 00:22:01,820
ви даєте моделі можливість вивчити багато різних способів, якими контекст змінює значення.

349
00:22:03,700 --> 00:22:07,493
Якщо підбити підсумки підрахунку параметрів за допомогою 96 голів, 

350
00:22:07,493 --> 00:22:10,833
кожна з яких включає власну варіацію цих чотирьох матриць, 

351
00:22:10,833 --> 00:22:15,080
то кожен блок багатоголової уваги налічує близько 600 мільйонів параметрів.

352
00:22:16,420 --> 00:22:19,910
Існує одна додаткова, трохи дратівлива річ, про яку я повинен згадати для тих з вас, 

353
00:22:19,910 --> 00:22:21,800
хто продовжить читати далі про трансформатори.

354
00:22:22,080 --> 00:22:26,433
Ви пам'ятаєте, як я говорив, що карта цінностей розбивається на дві окремі матриці, 

355
00:22:26,433 --> 00:22:29,440
які я позначив як матриці зниження та підвищення цінності.

356
00:22:29,960 --> 00:22:34,200
Те, як я представив речі, припускає, що ви бачите цю пару матриць всередині 

357
00:22:34,200 --> 00:22:38,440
кожної голови уваги, і ви можете абсолютно точно реалізувати це таким чином.

358
00:22:38,640 --> 00:22:39,920
Це був би правильний дизайн.

359
00:22:40,260 --> 00:22:43,731
Але те, як ви бачите це на папері, і те, як це реалізується на практиці, 

360
00:22:43,731 --> 00:22:44,920
виглядає дещо по-різному.

361
00:22:45,340 --> 00:22:50,860
Всі ці матриці значень для кожної голови з'єднуються в одну гігантську матрицю, 

362
00:22:50,860 --> 00:22:56,380
яку ми називаємо вихідною матрицею, пов'язану з усім багатоголовим блоком уваги.

363
00:22:56,820 --> 00:23:01,204
І коли ви бачите, що люди посилаються на матрицю значень для певної голови уваги, 

364
00:23:01,204 --> 00:23:03,985
вони зазвичай мають на увазі лише перший крок, той, 

365
00:23:03,985 --> 00:23:07,140
який я позначив як проекцію значення вниз у менший простір.

366
00:23:08,340 --> 00:23:11,040
Для допитливих я залишив на екрані примітку про це.

367
00:23:11,260 --> 00:23:14,943
Це одна з тих деталей, яка ризикує відволікти від основних концептуальних моментів, 

368
00:23:14,943 --> 00:23:18,540
але я хочу згадати про неї, щоб ви знали, якщо прочитаєте про це в інших джерелах.

369
00:23:19,240 --> 00:23:23,778
Якщо відкинути всі технічні нюанси, то в попередньому розділі ми бачили, що дані, 

370
00:23:23,778 --> 00:23:28,040
які проходять через трансформатор, не просто проходять через один блок уваги.

371
00:23:28,640 --> 00:23:30,891
З одного боку, він також проходить через інші операції, 

372
00:23:30,891 --> 00:23:32,700
які називаються багатошаровими перцептронами.

373
00:23:33,120 --> 00:23:34,880
Детальніше про них ми поговоримо в наступному розділі.

374
00:23:35,180 --> 00:23:39,320
А потім вона багаторазово проходить через багато-багато копій обох цих операцій.

375
00:23:39,980 --> 00:23:44,125
Це означає, що після того, як певне слово вбирає в себе частину контексту, 

376
00:23:44,125 --> 00:23:47,220
існує набагато більше шансів, що на це більш нюансоване 

377
00:23:47,220 --> 00:23:50,040
вбудовування вплине його більш нюансоване оточення.

378
00:23:50,940 --> 00:23:54,926
Чим далі вниз по мережі, коли кожне вбудовування вбирає в себе все більше і більше 

379
00:23:54,926 --> 00:23:58,673
значень від усіх інших вбудовувань, які в свою чергу стають все більш і більш 

380
00:23:58,673 --> 00:24:02,612
нюансованими, є надія, що існує можливість кодування більш високого рівня і більш 

381
00:24:02,612 --> 00:24:06,839
абстрактних ідей про даний вхід, що виходять за рамки просто дескрипторів і граматичної 

382
00:24:06,839 --> 00:24:07,320
структури.

383
00:24:07,880 --> 00:24:10,929
Такі речі, як настрій і тон, а також те, чи це вірш, 

384
00:24:10,929 --> 00:24:15,130
які основні наукові істини мають відношення до твору і тому подібні речі.

385
00:24:16,700 --> 00:24:21,566
Повертаючись ще раз до нашого підрахунку, GPT-3 включає 96 різних шарів, 

386
00:24:21,566 --> 00:24:27,033
тому загальна кількість ключових параметрів запитів і значень множиться ще на 96, 

387
00:24:27,033 --> 00:24:31,366
що дає загальну суму трохи менше 58 мільярдів різних параметрів, 

388
00:24:31,366 --> 00:24:34,500
присвячених усім головам, що привертають увагу.

389
00:24:34,980 --> 00:24:39,030
Це, безумовно, багато, але це лише близько третини від 175 мільярдів, 

390
00:24:39,030 --> 00:24:40,940
які знаходяться в мережі загалом.

391
00:24:41,520 --> 00:24:46,317
Отже, хоча увага приділяється всім, більшість параметрів надходить від блоків, 

392
00:24:46,317 --> 00:24:48,140
розташованих між цими кроками.

393
00:24:48,560 --> 00:24:53,560
У наступному розділі ми з вами поговоримо про ці блоки, а також про процес навчання.

394
00:24:54,120 --> 00:24:58,623
Велику роль в успіху механізму уваги відіграє не стільки якийсь конкретний тип поведінки, 

395
00:24:58,623 --> 00:25:02,425
який він забезпечує, скільки той факт, що він надзвичайно розпаралелюється, 

396
00:25:02,425 --> 00:25:06,678
а це означає, що ви можете виконувати величезну кількість обчислень за короткий час, 

397
00:25:06,678 --> 00:25:08,380
використовуючи графічні процесори.

398
00:25:09,460 --> 00:25:13,370
Враховуючи, що одним з важливих уроків глибокого навчання за останні десятиліття або два 

399
00:25:13,370 --> 00:25:17,149
було те, що масштабування саме по собі дає величезні якісні покращення продуктивності 

400
00:25:17,149 --> 00:25:21,060
моделі, розпаралелювані архітектури, які дозволяють це зробити, мають величезну перевагу.

401
00:25:22,040 --> 00:25:25,340
Якщо ви хочете дізнатися більше про ці речі, я залишив багато посилань в описі.

402
00:25:25,920 --> 00:25:30,040
Зокрема, все, що виробляють Андрій Карпати або Кріс Ола, як правило, є чистим золотом.

403
00:25:30,560 --> 00:25:33,717
У цьому відео я хотів просто привернути увагу в його нинішньому вигляді, 

404
00:25:33,717 --> 00:25:37,133
але якщо вам цікаво дізнатися більше про історію того, як ми прийшли до цього, 

405
00:25:37,133 --> 00:25:40,983
і як ви можете переосмислити цю ідею для себе, мій друг Вівек щойно виклав кілька відео, 

406
00:25:40,983 --> 00:25:42,540
які дають набагато більше мотивації.

407
00:25:43,120 --> 00:25:45,741
Крім того, Брітт Круз з каналу The Art of the Problem 

408
00:25:45,741 --> 00:25:48,460
має дуже гарне відео про історію великих мовних моделей.

409
00:26:04,960 --> 00:26:09,200
Дякую.

