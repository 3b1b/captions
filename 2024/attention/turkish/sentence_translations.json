[
 {
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "translatedText": "Son bölümde, siz ve ben bir transformatörün iç işleyişini adım adım incelemeye başladık.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "translatedText": "Bu, büyük dil modellerinin ve modern yapay zeka dalgasındaki diğer birçok aracın içindeki önemli teknoloji parçalarından biridir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "translatedText": "İlk olarak 2017'de yayınlanan Attention is All You Need (İhtiyacınız Olan Tek Şey Dikkat) adlı ünlü makalede ortaya çıktı ve bu bölümde siz ve ben bu dikkat mekanizmasının ne olduğunu araştırıp verileri nasıl işlediğini görselleştireceğiz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "translatedText": "Kısa bir özet olarak, aklınızda tutmanızı istediğim önemli bağlam şudur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "translatedText": "Sizin ve benim üzerinde çalıştığımız modelin amacı, bir metin parçasını alıp ardından hangi kelimenin geleceğini tahmin etmektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "translatedText": "Girdi metni, belirteçler dediğimiz küçük parçalara ayrılır ve bunlar genellikle kelimeler veya kelime parçalarıdır, ancak bu videodaki örnekleri sizin ve benim için düşünmeyi kolaylaştırmak için, belirteçlerin her zaman sadece kelimeler olduğunu varsayarak basitleştirelim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "translatedText": "Bir dönüştürücünün ilk adımı, her bir belirteci gömme olarak adlandırdığımız yüksek boyutlu bir vektörle ilişkilendirmektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "translatedText": "Aklınızda tutmanızı istediğim en önemli fikir, tüm olası yerleştirmelerin bu yüksek boyutlu uzaydaki yönlerinin anlamsal anlama nasıl karşılık gelebileceğidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "translatedText": "Geçen bölümde, yönün cinsiyete nasıl karşılık gelebileceğinin bir örneğini gördük; bu uzayda belirli bir adım eklemek sizi eril bir ismin gömülmesinden karşılık gelen dişil ismin gömülmesine götürebilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "translatedText": "Bu sadece bir örnek, bu yüksek boyutlu uzaydaki diğer birçok yönün bir kelimenin anlamının sayısız başka yönüne karşılık gelebileceğini hayal edebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "translatedText": "Bir dönüştürücünün amacı, bu katıştırmaları aşamalı olarak ayarlamaktır, böylece yalnızca tek bir kelimeyi kodlamakla kalmazlar, bunun yerine çok daha zengin bir bağlamsal anlam katarlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "translatedText": "Baştan söylemeliyim ki birçok insan dikkat mekanizmasını, transformatördeki bu kilit parçayı çok kafa karıştırıcı buluyor, bu yüzden bazı şeylerin oturması biraz zaman alırsa endişelenmeyin.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "translatedText": "Hesaplama ayrıntılarına ve tüm matris çarpımlarına dalmadan önce, dikkatin etkinleştirmesini istediğimiz davranış türü için birkaç örnek üzerinde düşünmeye değer olduğunu düşünüyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "translatedText": "Amerikan gerçek beni, bir mol karbondioksit ifadelerini düşünün ve benin biyopsisini alın.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "translatedText": "Siz ve ben, köstebek kelimesinin bunların her birinde bağlama göre farklı anlamlar taşıdığını biliyoruz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "translatedText": "Ancak dönüştürücünün metni parçalayan ve her bir belirteci bir vektörle ilişkilendiren ilk adımından sonra, ben ile ilişkilendirilen vektör tüm bu durumlarda aynı olacaktır, çünkü bu ilk belirteç gömme işlemi bağlama referans vermeyen bir arama tablosudur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "translatedText": "Yalnızca dönüştürücünün bir sonraki adımında, çevredeki katıştırmalar bu katıştırmaya bilgi aktarma şansına sahip olur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "translatedText": "Aklınızdaki resim, köstebek kelimesinin çoklu farklı anlamlarını kodlayan bu gömme uzayında çoklu farklı yönler olduğu ve iyi eğitilmiş bir dikkat bloğunun, bağlamın bir fonksiyonu olarak bu belirli yönlerden birine taşımak için genel gömmeye ne eklemeniz gerektiğini hesapladığıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "input": "To take another example, consider the embedding of the word tower.",
  "translatedText": "Başka bir örnek vermek gerekirse, kule kelimesinin gömülmesini ele alalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "translatedText": "Bu muhtemelen uzayda çok genel, spesifik olmayan bir yöndür ve diğer birçok büyük, uzun isimle ilişkilidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "translatedText": "Bu kelimeden hemen önce Eyfel kelimesi geliyorsa, mekanizmanın bu vektörü Eyfel kulesini daha spesifik olarak kodlayan bir yöne işaret edecek şekilde güncellemesini isteyebilirsiniz, belki de Paris, Fransa ve çelikten yapılmış şeylerle ilişkili vektörlerle ilişkilendirilebilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "translatedText": "Eğer öncesinde minyatür kelimesi de varsa, o zaman vektör daha da güncellenmelidir, böylece artık büyük, uzun şeylerle ilişkili olmayacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "translatedText": "Dikkat bloğu, bir kelimenin anlamını iyileştirmekten daha genel olarak, modelin bir gömüde kodlanmış bilgiyi, potansiyel olarak oldukça uzakta olan ve potansiyel olarak tek bir kelimeden çok daha zengin bilgiler içeren başka bir gömüye taşımasına olanak tanır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "translatedText": "Son bölümde gördüğümüz şey, birçok farklı dikkat bloğu da dahil olmak üzere tüm vektörler ağdan geçtikten sonra, bir sonraki belirtecin tahminini üretmek için yaptığınız hesaplamanın tamamen dizideki son vektörün bir fonksiyonu olduğuydu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "translatedText": "Örneğin, girdiğiniz metnin bütün bir gizem romanının büyük bir kısmı olduğunu düşünün, sonuna yakın bir noktaya kadar, bu nedenle katil şuydu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "translatedText": "Eğer model bir sonraki kelimeyi doğru bir şekilde tahmin edecekse, hayatına sadece \"was\" kelimesini gömerek başlayan dizideki bu son vektörün, tüm dikkat blokları tarafından herhangi bir kelimeden çok daha fazlasını temsil edecek şekilde güncellenmesi, bir şekilde bir sonraki kelimeyi tahmin etmekle ilgili tüm bağlam penceresindeki tüm bilgileri kodlaması gerekecektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "input": "To step through the computations, though, let's take a much simpler example.",
  "translatedText": "Hesaplamaların üzerinden geçmek için çok daha basit bir örnek ele alalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "translatedText": "Girdinin, yemyeşil ormanda dolaşan kabarık mavi bir yaratık ifadesini içerdiğini düşünün.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "translatedText": "Ve şimdilik, önemsediğimiz tek güncelleme türünün sıfatların kendilerine karşılık gelen isimlerin anlamlarını değiştirmesi olduğunu varsayalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "translatedText": "Anlatmak üzere olduğum şey, tek bir dikkat kafası olarak adlandırabileceğimiz şeydir ve daha sonra dikkat bloğunun nasıl paralel olarak çalışan birçok farklı kafadan oluştuğunu göreceğiz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "translatedText": "Yine, her kelime için ilk gömme, bağlam olmaksızın yalnızca söz konusu kelimenin anlamını kodlayan bazı yüksek boyutlu vektörlerdir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "input": "Actually, that's not quite true.",
  "translatedText": "Aslında, bu tam olarak doğru değil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "input": "They also encode the position of the word.",
  "translatedText": "Ayrıca kelimenin konumunu da kodlarlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "translatedText": "Konumların kodlanma şekliyle ilgili söylenecek çok şey var, ancak şu anda bilmeniz gereken tek şey, bu vektörün girdilerinin size hem kelimenin ne olduğunu hem de bağlam içinde nerede bulunduğunu söylemek için yeterli olduğudur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "translatedText": "Devam edelim ve bu yerleştirmeleri e harfi ile gösterelim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "translatedText": "Amaç, bir dizi hesaplamanın, örneğin isimlere karşılık gelenlerin karşılık gelen sıfatlardan anlam aldığı yeni bir rafine gömme kümesi üretmesini sağlamaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "translatedText": "Ve derin öğrenme oyununu oynarken, ilgili hesaplamaların çoğunun matris-vektör çarpımları gibi görünmesini istiyoruz; burada matrisler, modelin verilere dayalı olarak öğreneceği ayarlanabilir ağırlıklarla dolu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "translatedText": "Açık olmak gerekirse, sıfatların isimleri güncellemesine ilişkin bu örneği sadece bir dikkat kafasının yaptığını hayal edebileceğiniz davranış türünü göstermek için uyduruyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "translatedText": "Pek çok derin öğrenmede olduğu gibi, gerçek davranışı ayrıştırmak çok daha zordur çünkü bazı maliyet fonksiyonlarını en aza indirmek için çok sayıda parametreyi ayarlamaya ve ayarlamaya dayanır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "translatedText": "Sadece, bu sürece dahil olan parametrelerle dolu farklı matrisler boyunca adım atarken, her şeyi daha somut tutmaya yardımcı olmak için yapabileceği bir şeyin hayali bir örneğine sahip olmanın gerçekten yararlı olduğunu düşünüyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "translatedText": "Bu sürecin ilk adımı için, yaratık gibi her bir ismin şu soruyu sorduğunu hayal edebilirsiniz: Hey, önümde oturan herhangi bir sıfat var mı?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "translatedText": "Ve kabarık ve mavi kelimeleri için, her birinin cevap verebilmesi için, evet, ben bir sıfatım ve o konumdayım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "translatedText": "Bu soru bir şekilde başka bir vektör, başka bir sayı listesi olarak kodlanır ve biz buna bu kelime için sorgu adını veririz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "translatedText": "Ancak bu sorgu vektörü gömme vektöründen çok daha küçük bir boyuta sahiptir, örneğin 128.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "translatedText": "Bu sorguyu hesaplamak, wq olarak etiketleyeceğim belirli bir matrisi almak ve onu gömme ile çarpmak gibi görünüyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "translatedText": "İşleri biraz sıkıştırarak, bu sorgu vektörünü q olarak yazalım ve sonra bunun gibi bir okun yanına bir matris koyduğumu gördüğünüzde, bu matrisin okun başlangıcındaki vektörle çarpılmasının size okun sonundaki vektörü verdiğini temsil etmek içindir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "translatedText": "Bu durumda, bu matrisi bağlamdaki tüm katıştırmalarla çarparak her belirteç için bir sorgu vektörü üretirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "translatedText": "Bu matrisin girdileri modelin parametreleridir, yani gerçek davranış verilerden öğrenilir ve pratikte bu matrisin belirli bir dikkat kafasında ne yaptığını ayrıştırmak zordur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "translatedText": "Ancak bizim iyiliğimiz için, öğrenmesini umabileceğimiz bir örnek hayal ederek, bu sorgu matrisinin, isimlerin gömülmelerini, bir şekilde önceki konumlarda sıfat arama kavramını kodlayan bu daha küçük sorgu uzayında belirli yönlere eşlediğini varsayacağız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "input": "As to what it does to other embeddings, who knows?",
  "translatedText": "Diğer yerleştirmelere ne yaptığını ise kim bilebilir?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "translatedText": "Belki de bunlarla aynı anda başka bir hedefi gerçekleştirmeye çalışıyordur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "input": "Right now, we're laser focused on the nouns.",
  "translatedText": "Şu anda isimlere odaklanmış durumdayız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "translatedText": "Aynı zamanda, bununla ilişkili olarak anahtar matrisi adı verilen ikinci bir matris vardır ve bunu da her bir gömme ile çarparsınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "input": "This produces a second sequence of vectors that we call the keys.",
  "translatedText": "Bu, anahtar olarak adlandırdığımız ikinci bir vektör dizisi üretir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "translatedText": "Kavramsal olarak, anahtarları potansiyel olarak sorguları yanıtlıyor olarak düşünmek istersiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "translatedText": "Bu anahtar matrisi de ayarlanabilir parametrelerle doludur ve tıpkı sorgu matrisi gibi gömme vektörlerini aynı küçük boyutlu uzaya eşler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "translatedText": "Anahtarları, birbirleriyle yakın hizalandıklarında sorgularla eşleşiyor olarak düşünürsünüz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "translatedText": "Örneğimizde, anahtar matrisinin kabarık ve mavi gibi sıfatları, yaratık kelimesinin ürettiği sorguyla yakından hizalanan vektörlerle eşleştirdiğini hayal edebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "translatedText": "Her bir anahtarın her bir sorguyla ne kadar iyi eşleştiğini ölçmek için, her bir olası anahtar-sorgu çifti arasında bir nokta çarpımı hesaplarsınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "translatedText": "Büyük noktaların daha büyük nokta ürünlerine, anahtarların ve sorguların hizalandığı yerlere karşılık geldiği bir grup nokta ile dolu bir ızgarayı görselleştirmeyi seviyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "translatedText": "Sıfat isim örneğimiz için, bu biraz daha şuna benzer; eğer fluffy ve blue tarafından üretilen anahtarlar gerçekten creature tarafından üretilen sorguyla yakın bir şekilde hizalanırsa, bu iki noktadaki nokta çarpımları bazı büyük pozitif sayılar olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "translatedText": "Makine öğrenimi uzmanları bunun, kabarık ve mavinin gömülmelerinin yaratığın gömülmesine katıldığı anlamına geldiğini söyleyecektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "translatedText": "Bunun aksine, the gibi başka bir kelimenin anahtarı ile creature sorgusu arasındaki nokta çarpımı, birbiriyle ilgisiz olduğunu yansıtan küçük veya negatif bir değer olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "translatedText": "Böylece, negatif sonsuzdan sonsuza kadar herhangi bir gerçek sayı olabilen ve bize her bir kelimenin diğer tüm kelimelerin anlamını güncellemekle ne kadar ilgili olduğuna dair bir puan veren bu değerler tablosuna sahip oluyoruz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "translatedText": "Bu puanları kullanmanın yolu, her sütun boyunca alaka düzeyine göre ağırlıklandırılmış belirli bir ağırlıklı toplam almaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "translatedText": "Dolayısıyla, değerlerin negatif sonsuzdan sonsuza kadar uzanması yerine, istediğimiz şey bu sütunlardaki sayıların 0 ile 1 arasında olması ve her sütunun bir olasılık dağılımı gibi toplamının 1 olmasıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "translatedText": "Eğer son bölümden geliyorsanız, o zaman ne yapmamız gerektiğini biliyorsunuz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "translatedText": "Değerleri normalleştirmek için bu sütunların her biri boyunca bir softmax hesaplıyoruz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "translatedText": "Resmimizde, tüm sütunlara softmax uyguladıktan sonra, ızgarayı bu normalleştirilmiş değerlerle dolduracağız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "translatedText": "Bu noktada, her bir sütuna soldaki kelimenin üstteki karşılık gelen değerle ne kadar alakalı olduğuna göre ağırlık verdiğinizi düşünebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "input": "We call this grid an attention pattern.",
  "translatedText": "Bu ızgaraya dikkat örüntüsü diyoruz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "translatedText": "Şimdi orijinal transformatör belgesine bakarsanız, tüm bunları yazdıkları gerçekten kompakt bir yol var.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "translatedText": "Burada q ve k değişkenleri sırasıyla sorgu ve anahtar vektörlerinin tam dizilerini temsil eder; gömülmeleri sorgu ve anahtar matrisleriyle çarparak elde ettiğiniz küçük vektörler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "translatedText": "Paydaki bu ifade, anahtar ve sorgu çiftleri arasındaki tüm olası nokta çarpımlarının ızgarasını temsil etmenin gerçekten kompakt bir yoludur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "translatedText": "Bahsetmediğim küçük bir teknik ayrıntı, sayısal istikrar için bu değerlerin tümünü söz konusu anahtar sorgu alanındaki boyutun kareköküne bölmenin yararlı olacağıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "translatedText": "Ardından, tam ifadenin etrafına sarılan bu softmax'ın sütun sütun uygulanacağı anlaşılmalıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "input": "As to that v term, we'll talk about it in just a second.",
  "translatedText": "Bu terim hakkında birazdan konuşacağız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "translatedText": "Bundan önce, şimdiye kadar atladığım bir teknik ayrıntı daha var.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "translatedText": "Eğitim süreci sırasında, bu modeli belirli bir metin örneği üzerinde çalıştırdığınızda ve tüm ağırlıklar, pasajdaki bir sonraki gerçek kelimeye ne kadar yüksek bir olasılık atadığına bağlı olarak onu ödüllendirmek veya cezalandırmak için hafifçe ayarlandığında, bu pasajdaki her bir ilk belirteç dizisini takip eden olası her bir sonraki belirteci aynı anda tahmin etmesini sağlarsanız, tüm eğitim sürecini çok daha verimli hale getirdiği ortaya çıkar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "translatedText": "Örneğin, üzerinde durduğumuz ifadede, creature kelimesinden sonra hangi kelimelerin geleceğini ve the kelimesinden sonra hangi kelimelerin geleceğini tahmin etmek de olabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "translatedText": "Bu gerçekten güzel, çünkü aksi takdirde tek bir eğitim örneğinin etkili bir şekilde birçok eğitim örneği olarak işlev göreceği anlamına geliyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "translatedText": "Dikkat modelimizin amaçları doğrultusunda, bu, sonraki kelimelerin önceki kelimeleri etkilemesine asla izin vermek istemediğiniz anlamına gelir, çünkü aksi takdirde bir sonraki kelimenin cevabını bir şekilde ele verebilirler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "translatedText": "Bunun anlamı, buradaki tüm bu noktaların, daha öncekileri etkileyen sonraki belirteçleri temsil edenlerin, bir şekilde sıfır olmaya zorlanmasını istiyoruz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "translatedText": "Yapmayı düşünebileceğiniz en basit şey bunları sıfıra eşitlemektir, ancak bunu yaparsanız sütunların toplamı artık bir olmaz, normalleştirilmezler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "translatedText": "Bunun yerine, bunu yapmanın yaygın bir yolu, softmax uygulamadan önce tüm bu girişleri negatif sonsuz olarak ayarlamaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "translatedText": "Bunu yaparsanız, softmax uygulandıktan sonra bunların tümü sıfıra dönüştürülür, ancak sütunlar normalleştirilmiş olarak kalır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "input": "This process is called masking.",
  "translatedText": "Bu işleme maskeleme adı verilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "translatedText": "Bunu uygulamadığınız dikkat versiyonları vardır, ancak GPT örneğimizde, bu eğitim aşamasında, örneğin bir sohbet robotu veya benzeri bir şey olarak çalıştırmaktan daha alakalı olsa da, daha sonraki belirteçlerin öncekileri etkilemesini önlemek için her zaman bu maskelemeyi uygularsınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "translatedText": "Bu dikkat örüntüsü hakkında üzerinde düşünmeye değer bir başka gerçek de boyutunun bağlam boyutunun karesine eşit olmasıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "translatedText": "İşte bu yüzden bağlam boyutu büyük dil modelleri için gerçekten büyük bir darboğaz olabilir ve bunu ölçeklendirmek önemsiz değildir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "translatedText": "Tahmin edebileceğiniz gibi, daha büyük ve daha büyük bağlam pencereleri arzusuyla, son yıllarda bağlamı daha ölçeklenebilir hale getirmeyi amaçlayan dikkat mekanizmasında bazı varyasyonlar görüldü, ancak burada siz ve ben temellere odaklanıyoruz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "translatedText": "Tamam, harika, bu örüntüyü hesaplamak modelin hangi kelimelerin hangi diğer kelimelerle ilgili olduğunu çıkarmasını sağlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "translatedText": "Şimdi, kelimelerin ilgili oldukları diğer kelimelere bilgi aktarmasına izin vererek katıştırmaları gerçekten güncellemeniz gerekir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "translatedText": "Örneğin, Fluffy'nin gömülmesinin bir şekilde Creature'da bir değişikliğe neden olmasını ve onu bu 12.000 boyutlu gömme uzayının daha spesifik olarak Fluffy yaratığını kodlayan farklı bir bölümüne taşımasını istiyorsunuz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "translatedText": "Burada yapacağım şey, öncelikle size bunu yapabileceğiniz en basit yolu göstermektir, ancak bunun çok başlı dikkat bağlamında değiştirilmesinin küçük bir yolu vardır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "translatedText": "Bunun en basit yolu, değer matrisi dediğimiz ve ilk kelimenin, örneğin Fluffy'nin gömülmesiyle çarpacağınız üçüncü bir matris kullanmak olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "translatedText": "Bunun sonucunda bir değer vektörü elde edilir ve bu, ikinci sözcüğün gömülmesine eklediğiniz bir şeydir, bu durumda Creature sözcüğünün gömülmesine eklediğiniz bir şeydir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "translatedText": "Dolayısıyla bu değer vektörü, gömülmelerle aynı çok yüksek boyutlu uzayda yaşar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "translatedText": "Bu değer matrisini bir kelimenin gömülmesiyle çarptığınızda, bunu şöyle düşünebilirsiniz: Eğer bu kelime başka bir şeyin anlamını ayarlamakla ilgiliyse, bunu yansıtmak için o başka şeyin gömülmesine tam olarak ne eklenmelidir?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "translatedText": "Diyagramımıza geri dönersek, tüm anahtarları ve sorguları bir kenara bırakalım, çünkü dikkat örüntüsünü hesapladıktan sonra bunlarla işiniz bitecek, sonra bu değer matrisini alacak ve bir dizi değer vektörü üretmek için bu katıştırmaların her biriyle çarpacaksınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "translatedText": "Bu değer vektörlerini ilgili anahtarlarla ilişkilendirilmiş olarak düşünebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "translatedText": "Bu diyagramdaki her sütun için, değer vektörlerinin her birini o sütundaki karşılık gelen ağırlıkla çarparsınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "translatedText": "Örneğin burada, Yaratık'ın gömülmesi altında, Fluffy ve Blue için büyük oranda değer vektörü eklerken, diğer tüm değer vektörlerini sıfırlarsınız ya da en azından neredeyse sıfırlarsınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "translatedText": "Ve son olarak, daha önce Creature'ın bağlamdan bağımsız bir anlamını kodlayan bu sütunla ilişkili katıştırmayı gerçekten güncellemenin yolu, sütundaki tüm bu yeniden ölçeklendirilmiş değerleri bir araya getirerek eklemek istediğiniz bir değişiklik üretirsiniz, bunu delta-e olarak etiketleyeceğim ve sonra bunu orijinal katıştırmaya eklersiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "translatedText": "Sonuç olarak, kabarık mavi bir yaratık gibi bağlamsal açıdan daha zengin bir anlamı kodlayan daha rafine bir vektör ortaya çıkmasını umuyoruz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "translatedText": "Ve elbette bunu sadece bir gömüye yapmazsınız, aynı ağırlıklı toplamı bu resimdeki tüm sütunlara uygularsınız, bir dizi değişiklik üretirsiniz, tüm bu değişiklikleri ilgili gömülere eklersiniz, dikkat bloğundan çıkan daha rafine gömülerin tam bir dizisini üretirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "translatedText": "Uzaklaştırıldığında, tüm bu süreç tek bir dikkat olarak tanımlanabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "translatedText": "Şimdiye kadar anlattığım gibi, bu süreç üç farklı matris tarafından parametrelendirilir ve hepsi ayarlanabilir parametrelerle doldurulur: anahtar, sorgu ve değer.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "translatedText": "GPT-3'teki sayıları kullanarak model parametrelerinin toplam sayısını saydığımız puanlama ile geçen bölümde başladığımız şeye devam etmek için bir dakikanızı ayırmak istiyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "translatedText": "Bu anahtar ve sorgu matrislerinin her biri, gömme boyutuyla eşleşen 12.288 sütuna ve bu daha küçük anahtar sorgu uzayının boyutuyla eşleşen 128 satıra sahiptir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "translatedText": "Bu da bize her biri için 1,5 milyon civarında ek parametre sağlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "translatedText": "Buna karşın değer matrisine bakarsanız, şu ana kadar anlattığım şekilde 12.288 sütun ve 12.288 satıra sahip bir kare matris olduğunu görürsünüz, çünkü hem girdileri hem de çıktıları bu çok büyük gömme uzayında yaşamaktadır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "input": "If true, that would mean about 150 million added parameters.",
  "translatedText": "Eğer doğruysa, bu yaklaşık 150 milyon ilave parametre anlamına gelecektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "input": "And to be clear, you could do that.",
  "translatedText": "Ve açık olmak gerekirse, bunu yapabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "translatedText": "Değer haritasına anahtar ve sorgudan çok daha fazla parametre ayırabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "translatedText": "Ancak pratikte, bunun yerine bu değer haritasına ayrılan parametre sayısının anahtar ve sorguya ayrılan sayı ile aynı olmasını sağlarsanız çok daha verimli olur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "translatedText": "Bu, özellikle birden fazla dikkat kafasının paralel olarak çalıştırıldığı ortamlar için geçerlidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "translatedText": "Bunun görünüşü, değer haritasının iki küçük matrisin çarpımı olarak çarpanlara ayrılmasıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "translatedText": "Kavramsal olarak, sizi yine de genel doğrusal harita hakkında düşünmeye teşvik ediyorum, her ikisi de bu daha büyük gömme uzayında girdileri ve çıktıları olan bir harita, örneğin mavinin gömülmesini isimlere ekleyeceğiniz bu mavilik yönüne götürmek gibi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "translatedText": "Sadece daha az sayıda satırdır, tipik olarak anahtar sorgu alanıyla aynı boyuttadır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "translatedText": "Bunun anlamı, büyük gömme vektörlerini çok daha küçük bir uzaya eşlemek olarak düşünebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "translatedText": "Bu geleneksel bir adlandırma değil, ancak ben buna değer aşağı matrisi diyeceğim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "translatedText": "İkinci matris, bu daha küçük uzaydan gömme uzayına geri eşlenerek gerçek güncellemeleri yapmak için kullandığınız vektörleri üretir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "translatedText": "Ben buna yine geleneksel olmayan değer yukarı matrisi diyeceğim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "input": "The way that you would see this written in most papers looks a little different.",
  "translatedText": "Bunun çoğu gazetede yazılış şekli biraz farklıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "input": "I'll talk about it in a minute.",
  "translatedText": "Birazdan bundan bahsedeceğim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "translatedText": "Bana göre bu, işleri kavramsal olarak biraz daha kafa karıştırıcı hale getirme eğiliminde.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "translatedText": "Burada doğrusal cebir jargonunu kullanmak gerekirse, temelde yaptığımız şey genel değer haritasını düşük rütbeli bir dönüşüm olacak şekilde kısıtlamaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "translatedText": "Parametre sayısına geri dönecek olursak, bu matrislerin dördü de aynı boyuta sahiptir ve hepsini topladığımızda bir dikkat kafası için yaklaşık 6,3 milyon parametre elde ederiz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "translatedText": "Kısa bir not olarak, biraz daha doğru olmak gerekirse, şu ana kadar açıklanan her şey, diğer modellerde ortaya çıkan ve çapraz dikkat olarak adlandırılan bir varyasyondan ayırt etmek için insanların kendi kendine dikkat kafası olarak adlandırdıkları şeydir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "translatedText": "Bu bizim GPT örneğimizle ilgili değil, ancak merak ediyorsanız, çapraz dikkat, bir dildeki metin ve devam eden bir çeviri üretiminin parçası olan başka bir dildeki metin veya belki de konuşma sesi girişi ve devam eden bir transkripsiyon gibi iki farklı veri türünü işleyen modelleri içerir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "input": "A cross-attention head looks almost identical.",
  "translatedText": "Bir çapraz dikkat kafası neredeyse aynı görünür.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "input": "The only difference is that the key and query maps act on different data sets.",
  "translatedText": "Tek fark, anahtar ve sorgu eşlemelerinin farklı veri kümeleri üzerinde hareket etmesidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "translatedText": "Örneğin çeviri yapan bir modelde, anahtarlar bir dilden gelirken sorgular başka bir dilden gelebilir ve dikkat örüntüsü bir dildeki hangi kelimelerin diğer dildeki hangi kelimelere karşılık geldiğini tanımlayabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "translatedText": "Ve bu ortamda tipik olarak maskeleme olmayacaktır, çünkü sonraki belirteçlerin öncekileri etkilemesi gibi bir kavram yoktur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "translatedText": "Öz-dikkate odaklanmaya devam edersek, buraya kadar her şeyi anladıysanız ve burada duracak olsaydınız, dikkatin gerçekte ne olduğunun özüne ulaşırdınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "translatedText": "Bize kalan tek şey, bunu birçok farklı kez yapmanın anlamını ortaya koymaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "translatedText": "Ana örneğimizde isimleri güncelleyen sıfatlara odaklandık, ancak elbette bağlamın bir kelimenin anlamını etkileyebileceği pek çok farklı yol vardır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "translatedText": "Eğer çarptıkları kelimeler araba kelimesinden önce geliyorsa, bu arabanın şekli ve yapısıyla ilgili sonuçlar doğurur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "input": "And a lot of associations might be less grammatical.",
  "translatedText": "Ve birçok çağrışım daha az dilbilgisel olabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "translatedText": "Eğer büyücü kelimesi Harry ile aynı pasajın herhangi bir yerinde geçiyorsa, bu Harry Potter'a atıfta bulunulduğunu düşündürür; oysa bunun yerine Kraliçe, Sussex ve William kelimeleri bu pasajda geçiyorsa, belki de Harry'nin gömülmesi prensi ifade edecek şekilde güncellenmelidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "translatedText": "Hayal edebileceğiniz her farklı bağlamsal güncelleme türü için, bu anahtar ve sorgu matrislerinin parametreleri farklı dikkat kalıplarını yakalamak için farklı olacaktır ve değer haritamızın parametreleri, katıştırmalara neyin eklenmesi gerektiğine bağlı olarak farklı olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "translatedText": "Ve yine pratikte bu haritaların gerçek davranışını yorumlamak çok daha zordur; burada ağırlıklar, modelin bir sonraki jetonu tahmin etme hedefine en iyi şekilde ulaşmak için ne yapması gerekiyorsa onu yapacak şekilde ayarlanır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "translatedText": "Daha önce de söylediğim gibi, anlattığımız her şey tek bir dikkat başlığıdır ve bir dönüştürücü içindeki tam bir dikkat bloğu, her biri kendi farklı anahtar sorgusu ve değer eşlemeleri ile bu işlemlerin çoğunu paralel olarak çalıştırdığınız çok başlı dikkat olarak adlandırılan şeyden oluşur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "translatedText": "Örneğin GPT-3, her blok içinde 96 dikkat başlığı kullanır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "translatedText": "Her birinin zaten biraz kafa karıştırıcı olduğu düşünüldüğünde, kafanızda tutmanız gereken çok şey olduğu kesin.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "translatedText": "Her şeyi çok açık bir şekilde ifade etmek gerekirse, bu 96 farklı dikkat modeli üreten 96 farklı anahtar ve sorgu matrisine sahip olduğunuz anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "translatedText": "Daha sonra her bir başlığın 96 değer vektörü dizisi üretmek için kullanılan kendi farklı değer matrisleri vardır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "translatedText": "Bunların hepsi, karşılık gelen dikkat kalıpları ağırlık olarak kullanılarak toplanır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "translatedText": "Bunun anlamı, bağlamdaki her bir konum için, her bir belirteç, bu başlıkların her biri, o konumdaki gömüye eklenmek üzere önerilen bir değişiklik üretir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "translatedText": "Yaptığınız şey, her bir başlık için bir tane olmak üzere önerilen tüm değişiklikleri toplamak ve sonucu o pozisyonun orijinal yerleştirmesine eklemektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "translatedText": "Buradaki toplamın tamamı, bu çok başlı dikkat bloğundan elde edilen çıktının bir dilimi, diğer ucundan çıkan rafine gömülerden tek bir tanesi olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "translatedText": "Tekrar söylüyorum, bunlar düşünülmesi gereken çok şey, bu yüzden kafanızda oturması biraz zaman alırsa hiç endişelenmeyin.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "translatedText": "Genel fikir, birçok farklı başlığı paralel olarak çalıştırarak, modele bağlamın anlamı değiştirdiği birçok farklı yolu öğrenme kapasitesi vermenizdir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "translatedText": "Her biri bu dört matrisin kendi varyasyonunu içeren 96 başlıkla parametre sayısı için çalışan çetelemizi çıkardığımızda, çok başlı dikkatin her bloğu yaklaşık 600 milyon parametre ile sonuçlanır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "translatedText": "Transformers hakkında daha fazla okumaya devam edecek olanlarınız için bahsetmem gereken biraz can sıkıcı bir şey daha var.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "translatedText": "Değer haritasının, değer aşağı ve değer yukarı matrisleri olarak etiketlediğim bu iki farklı matrise ayrıştırıldığını söylediğimi hatırlıyorsunuz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "translatedText": "Olayları çerçeveleme şeklim, bu matris çiftini her bir dikkat kafasının içinde görmenizi önerir ve bunu kesinlikle bu şekilde uygulayabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "input": "That would be a valid design.",
  "translatedText": "Bu geçerli bir tasarım olurdu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "translatedText": "Ancak bunun makalelerde yazılma şekli ile pratikte uygulanma şekli biraz farklı görünüyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "translatedText": "Her bir kafa için tüm bu değer matrisleri, çıktı matrisi olarak adlandırdığımız ve çok başlı dikkat bloğunun tamamıyla ilişkili olan dev bir matriste bir araya getirilmiş olarak görünür.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "translatedText": "Ve insanların belirli bir dikkat başlığı için değer matrisine atıfta bulunduklarını gördüğünüzde, genellikle yalnızca bu ilk adıma, daha küçük alana değer aşağı projeksiyonu olarak etiketlediğim adıma atıfta bulunurlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "input": "For the curious among you, I've left an on-screen note about it.",
  "translatedText": "Aranızdaki meraklılar için ekrana bununla ilgili bir not bıraktım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "translatedText": "Bu, ana kavramsal noktaların dikkatini dağıtma riski taşıyan ayrıntılardan biri, ancak bunu başka kaynaklarda okursanız bilmeniz için belirtmek istiyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "translatedText": "Tüm teknik nüansları bir kenara bırakırsak, son bölümdeki önizlemede bir transformatörden akan verilerin nasıl sadece tek bir dikkat bloğundan akmadığını gördük.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "translatedText": "Bir kere, çok katmanlı algılayıcılar adı verilen diğer işlemlerden de geçiyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "input": "We'll talk more about those in the next chapter.",
  "translatedText": "Bir sonraki bölümde bunlar hakkında daha fazla konuşacağız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "translatedText": "Ve sonra bu işlemlerin her ikisinin de birçok kopyasından tekrar tekrar geçer.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "translatedText": "Bunun anlamı, belirli bir kelime bağlamının bir kısmını özümsedikten sonra, bu daha nüanslı yerleştirmenin daha nüanslı çevresinden etkilenmesi için çok daha fazla şans olduğudur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "translatedText": "Ağda ne kadar aşağıya inerseniz, her bir katıştırma diğer tüm katıştırmalardan giderek daha fazla anlam alır ve bu katıştırmaların kendileri de giderek daha incelikli hale gelir; umut, belirli bir girdi hakkında yalnızca tanımlayıcılar ve gramer yapısının ötesinde daha üst düzey ve daha soyut fikirleri kodlama kapasitesinin olmasıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "translatedText": "Duygu, ton, şiir olup olmadığı, eserin altında yatan bilimsel gerçekler ve bunun gibi şeyler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "translatedText": "Puanlamamıza bir kez daha dönecek olursak, GPT-3 96 farklı katman içermektedir, dolayısıyla anahtar sorgu ve değer parametrelerinin toplam sayısı bir 96 ile daha çarpılır, bu da toplamı tüm dikkat başlıklarına ayrılmış 58 milyar farklı parametrenin biraz altına getirir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "translatedText": "Bu elbette çok fazla, ancak toplamda ağda bulunan 175 milyarın yalnızca üçte biri.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "translatedText": "Dolayısıyla, tüm dikkati dikkat çekse de, parametrelerin çoğu bu adımlar arasında yer alan bloklardan gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "translatedText": "Bir sonraki bölümde, siz ve ben bu diğer bloklar ve eğitim süreci hakkında daha fazla konuşacağız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "translatedText": "Dikkat mekanizmasının başarısının hikayesinin büyük bir kısmı, sağladığı belirli bir davranış türünden çok, son derece paralelleştirilebilir olması, yani GPU'ları kullanarak kısa sürede çok sayıda hesaplama yapabilmenizdir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "translatedText": "Son on ya da iki yılda derin öğrenmeyle ilgili en büyük derslerden birinin, ölçeğin tek başına model performansında büyük niteliksel gelişmeler sağladığı olduğu göz önüne alındığında, bunu yapmanıza izin veren paralelleştirilebilir mimariler için büyük bir avantaj var.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "translatedText": "Bu konuda daha fazla bilgi edinmek isterseniz, açıklama kısmına çok sayıda bağlantı bıraktım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "translatedText": "Özellikle Andrej Karpathy veya Chris Ola tarafından üretilen her şey saf altın olma eğilimindedir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "translatedText": "Bu videoda, mevcut haliyle dikkat konusuna değinmek istedim, ancak bu noktaya nasıl geldiğimizi ve bu fikri kendiniz için nasıl yeniden keşfedebileceğinizi merak ediyorsanız, arkadaşım Vivek bu motivasyondan çok daha fazlasını veren birkaç video yayınladı.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "translatedText": "Ayrıca, The Art of the Problem kanalından Britt Cruz'un büyük dil modellerinin tarihi hakkında gerçekten güzel bir videosu var.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "input": "Thank you.",
  "translatedText": "Teşekkür ederim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]