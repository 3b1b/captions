[
 {
  "input": "Demystifying self-attention, multiple heads, and cross-attention.",
  "translatedText": "Öz-dikkat, çoklu kafa ve çapraz-dikkatin gizemini çözmek.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "translatedText": "Sponsorlu reklam okumaları yerine, bu dersler doğrudan izleyiciler tarafından finanse edilmektedir: https://3b1b.co/support",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "An equally valuable form of support is to simply share the videos.",
  "translatedText": "Aynı derecede değerli bir destek biçimi de videoları paylaşmaktır.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Other resources about transformers",
  "translatedText": "Transformatörlerle ilgili diğer kaynaklar",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Andrej Karpathy's videos",
  "translatedText": "Andrej Karpathy'nin videoları",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "The Transformer Circuits posts by Anthropic",
  "translatedText": "Anthropic tarafından gönderilen Transformatör Devreleri",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://transformer-circuits.pub/2021/framework/index.html",
  "translatedText": "https://transformer-circuits.pub/2021/framework/index.html",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "In particular, it was only after I read this post that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources.",
  "translatedText": "Özellikle, bu yazıyı okuduktan sonra değer ve çıktı matrislerinin kombinasyonunu, gömme uzayından kendisine birleşik bir düşük rütbeli harita olarak düşünmeye başladım, bu da en azından benim zihnimde, diğer kaynaklardan çok daha net şeyler yaptı.",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "History of language models by Brit Cruise, @ArtOfTheProblem ",
  "translatedText": "Brit Cruise'dan dil modellerinin tarihi, @ArtOfTheProblem",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://youtu.be/OFS90-FX6pg",
  "translatedText": "https://youtu.be/OFS90-FX6pg",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "What is a Language Model by @vcubingx ",
  "translatedText": "Dil Modeli Nedir by @vcubingx",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "translatedText": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Site with exercises related to ML programming and GPTs",
  "translatedText": "ML programlama ve GPT'lerle ilgili alıştırmalar içeren site",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://www.gptandchill.ai/codingproblems",
  "translatedText": "https://www.gptandchill.ai/codingproblems",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Early paper on how directions in embedding spaces have meaning:",
  "translatedText": "Gömme uzaylarındaki yönlerin nasıl bir anlamı olduğuna dair ilk makale:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "https://arxiv.org/pdf/1301.3781.pdf",
  "translatedText": "https://arxiv.org/pdf/1301.3781.pdf",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "Timestamps:",
  "translatedText": "Zaman damgaları:",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "0:00 - Recap on embeddings",
  "translatedText": "0:00 - Gömülmeler hakkında özet",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "1:39 - Motivating examples",
  "translatedText": "1:39 - Motive edici örnekler",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "4:29 - The attention pattern",
  "translatedText": "4:29 - Dikkat örüntüsü",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "11:08 - Masking",
  "translatedText": "11:08 - Maskeleme",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "12:42 - Context size",
  "translatedText": "12:42 - Bağlam boyutu",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "13:10 - Values",
  "translatedText": "13:10 - Değerler",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "15:44 - Counting parameters",
  "translatedText": "15:44 - Parametreleri sayma",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "18:21 - Cross-attention",
  "translatedText": "18:21 - Çapraz dikkat",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "19:19 - Multiple heads",
  "translatedText": "19:19 - Çoklu kafalar",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "22:16 - The output matrix",
  "translatedText": "22:16 - Çıktı matrisi",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "23:19 - Going deeper",
  "translatedText": "23:19 - Daha derine inmek",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "24:54 - Ending",
  "translatedText": "24:54 - Bitiş",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0
 }
]