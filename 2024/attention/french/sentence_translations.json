[
 {
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "translatedText": "Dans le dernier chapitre, toi et moi avons commencé à parcourir le fonctionnement interne d'un transformateur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "translatedText": "C'est l'un des éléments clés de la technologie à l'intérieur des grands modèles de langage, et de nombreux autres outils de la vague moderne de l'IA.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "translatedText": "Il est apparu pour la première fois dans un article désormais célèbre de 2017 intitulé Attention is All You Need, et dans ce chapitre, toi et moi allons creuser ce qu'est ce mécanisme d'attention, en visualisant la façon dont il traite les données.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "translatedText": "En guise de récapitulation rapide, voici le contexte important que je veux que tu aies à l'esprit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "translatedText": "L'objectif du modèle que toi et moi étudions est d'assimiler un texte et de prédire le mot qui suit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "translatedText": "Le texte d'entrée est décomposé en petits morceaux que nous appelons des tokens, et ce sont très souvent des mots ou des morceaux de mots, mais pour que les exemples de cette vidéo soient plus faciles à comprendre pour toi et moi, simplifions en faisant comme si les tokens n'étaient jamais que des mots.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "translatedText": "La première étape d'un transformateur consiste à associer chaque jeton à un vecteur de haute dimension, ce que nous appelons son intégration.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "translatedText": "L'idée la plus importante que je veux que tu aies à l'esprit est de savoir comment les directions dans cet espace à haute dimension de tous les enchâssements possibles peuvent correspondre à une signification sémantique.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "translatedText": "Dans le dernier chapitre, nous avons vu un exemple de la façon dont la direction peut correspondre au genre, dans le sens où l'ajout d'un certain pas dans cet espace peut te faire passer de l'encastrement d'un nom masculin à l'encastrement du nom féminin correspondant.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "translatedText": "Ce n'est qu'un exemple, tu pourrais imaginer combien d'autres directions dans cet espace à haute dimension pourraient correspondre à de nombreux autres aspects de la signification d'un mot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "translatedText": "L'objectif d'un transformateur est d'ajuster progressivement ces enchâssements afin qu'ils ne se contentent pas d'encoder un mot individuel, mais qu'ils intègrent une signification contextuelle beaucoup plus riche.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "translatedText": "Je dois dire d'emblée que beaucoup de gens trouvent le mécanisme d'attention, cette pièce maîtresse d'un transformateur, très déroutant, alors ne t'inquiète pas s'il te faut un peu de temps pour assimiler les choses.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "translatedText": "Je pense qu'avant de nous plonger dans les détails informatiques et toutes les multiplications de matrices, cela vaut la peine de réfléchir à quelques exemples du type de comportement que nous voulons que l'attention permette.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "translatedText": "Considère les phrases Une vraie taupe américaine, une taupe de dioxyde de carbone, et fais une biopsie de la taupe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "translatedText": "Toi et moi savons que le mot taupe a des significations différentes dans chacune d'entre elles, en fonction du contexte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "translatedText": "Mais après la première étape d'un transformateur, celle qui décompose le texte et associe chaque jeton à un vecteur, le vecteur associé à taupe serait le même dans tous ces cas, parce que cet enchâssement initial de jetons est en fait une table de recherche sans référence au contexte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "translatedText": "Ce n'est qu'à l'étape suivante du transformateur que les enchâssements environnants ont la possibilité de transmettre des informations à celui-ci.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "translatedText": "L'image que tu peux avoir à l'esprit est qu'il existe plusieurs directions distinctes dans cet espace d'intégration codant les multiples significations distinctes du mot taupe, et qu'un bloc d'attention bien entraîné calcule ce que tu dois ajouter à l'intégration générique pour la déplacer vers l'une de ces directions spécifiques, en fonction du contexte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "input": "To take another example, consider the embedding of the word tower.",
  "translatedText": "Pour prendre un autre exemple, considère l'encastrement du mot tour.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "translatedText": "Il s'agit vraisemblablement d'une direction très générique et non spécifique dans l'espace, associée à beaucoup d'autres grands noms.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "translatedText": "Si ce mot était immédiatement précédé de Eiffel, tu pourrais imaginer vouloir que le mécanisme mette à jour ce vecteur pour qu'il pointe dans une direction qui encode plus spécifiquement la tour Eiffel, peut-être en corrélation avec des vecteurs associés à Paris et à la France et à des choses faites en acier.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "translatedText": "S'il était également précédé du mot miniature, alors le vecteur devrait être encore plus mis à jour, afin qu'il ne soit plus en corrélation avec des choses grandes et hautes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "translatedText": "Plus généralement que le simple affinage du sens d'un mot, le bloc d'attention permet au modèle de faire passer des informations codées dans un encastrement à celui d'un autre, potentiellement ceux qui sont assez éloignés, et potentiellement avec des informations beaucoup plus riches qu'un simple mot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "translatedText": "Ce que nous avons vu dans le dernier chapitre, c'est qu'après que tous les vecteurs aient circulé dans le réseau, y compris de nombreux blocs d'attention différents, le calcul que tu effectues pour produire une prédiction du prochain jeton est entièrement fonction du dernier vecteur de la séquence.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "translatedText": "Imagine, par exemple, que le texte que tu saisis est la plus grande partie d'un roman policier, jusqu'à un point situé vers la fin, où l'on peut lire, donc le meurtrier était.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "translatedText": "Pour que le modèle puisse prédire avec précision le mot suivant, ce dernier vecteur de la séquence, qui a commencé sa vie en intégrant simplement le mot était, devra avoir été mis à jour par tous les blocs d'attention pour représenter beaucoup plus que n'importe quel mot individuel, en encodant en quelque sorte toutes les informations de la fenêtre contextuelle complète qui sont pertinentes pour prédire le mot suivant.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "input": "To step through the computations, though, let's take a much simpler example.",
  "translatedText": "Mais pour mieux comprendre les calculs, prenons un exemple beaucoup plus simple.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "translatedText": "Imagine que l'entrée comprenne la phrase, une créature bleue duveteuse parcourait la forêt verdoyante.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "translatedText": "Et pour l'instant, suppose que le seul type de mise à jour qui nous intéresse est que les adjectifs ajustent les significations de leurs noms correspondants.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "translatedText": "Ce que je vais décrire est ce que nous appellerions une seule tête d'attention, et nous verrons plus tard comment le bloc d'attention se compose de plusieurs têtes différentes exécutées en parallèle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "translatedText": "Encore une fois, l'intégration initiale de chaque mot est un vecteur à haute dimension qui n'encode que la signification de ce mot particulier, sans contexte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "input": "Actually, that's not quite true.",
  "translatedText": "En fait, ce n'est pas tout à fait vrai.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "input": "They also encode the position of the word.",
  "translatedText": "Ils codent également la position du mot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "translatedText": "Il y a beaucoup plus à dire sur la façon dont les positions sont codées, mais pour l'instant, tout ce que tu as besoin de savoir, c'est que les entrées de ce vecteur suffisent à te dire à la fois ce qu'est le mot et où il se trouve dans le contexte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "translatedText": "Allons-y et désignons ces encastrements par la lettre e.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "translatedText": "L'objectif est de faire en sorte qu'une série de calculs produise un nouvel ensemble raffiné d'enchâssements où, par exemple, ceux qui correspondent aux noms ont ingéré le sens des adjectifs correspondants.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "translatedText": "Et en jouant le jeu de l'apprentissage profond, nous voulons que la plupart des calculs impliqués ressemblent à des produits matrice-vecteur, où les matrices sont pleines de poids réglables, des choses que le modèle apprendra en fonction des données.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "translatedText": "Pour être clair, j'invente cet exemple d'adjectifs mettant à jour des noms juste pour illustrer le type de comportement que tu pourrais imaginer de la part d'une tête d'attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "translatedText": "Comme pour une grande partie du deep learning, le véritable comportement est beaucoup plus difficile à analyser parce qu'il est basé sur le réglage et l'ajustement d'un très grand nombre de paramètres pour minimiser une certaine fonction de coût.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "translatedText": "C'est juste qu'au fur et à mesure que nous avançons dans les différentes matrices remplies de paramètres qui sont impliqués dans ce processus, je pense qu'il est vraiment utile d'avoir un exemple imaginé de quelque chose qu'il pourrait faire pour aider à garder tout cela plus concret.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "translatedText": "Pour la première étape de ce processus, tu peux imaginer que chaque nom, comme la créature, pose la question suivante : \"Hé, y a-t-il des adjectifs devant moi ?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "translatedText": "Et pour les mots pelucheux et bleu, que chacun puisse répondre, oui, je suis un adjectif et je suis dans cette position.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "translatedText": "Cette question est en quelque sorte codée sous la forme d'un autre vecteur, d'une autre liste de nombres, que nous appelons la requête pour ce mot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "translatedText": "Ce vecteur de requête a cependant une dimension beaucoup plus petite que le vecteur d'intégration, disons 128.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "translatedText": "Pour calculer cette requête, il suffit de prendre une certaine matrice, que j'appellerai wq, et de la multiplier par l'intégration.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "translatedText": "En comprimant un peu les choses, écrivons ce vecteur de requête comme q, et chaque fois que tu me vois mettre une matrice à côté d'une flèche comme celle-ci, c'est pour représenter le fait que la multiplication de cette matrice par le vecteur au début de la flèche te donne le vecteur à la fin de la flèche.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "translatedText": "Dans ce cas, tu multiplies cette matrice par tous les enchâssements du contexte, ce qui produit un vecteur de requête pour chaque token.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "translatedText": "Les entrées de cette matrice sont des paramètres du modèle, ce qui signifie que le véritable comportement est appris à partir des données, et dans la pratique, ce que cette matrice fait dans une tête d'attention particulière est difficile à analyser.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "translatedText": "Mais pour notre bien, en imaginant un exemple que nous pourrions espérer qu'il apprenne, nous supposerons que cette matrice de requête fait correspondre les enchâssements de noms à certaines directions dans cet espace de requête plus petit qui encode en quelque sorte la notion de recherche d'adjectifs dans les positions précédentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "input": "As to what it does to other embeddings, who knows?",
  "translatedText": "Quant à savoir ce qu'il fait à d'autres encastrements, qui sait ?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "translatedText": "Peut-être qu'il essaie en même temps d'atteindre un autre objectif avec ceux-ci.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "input": "Right now, we're laser focused on the nouns.",
  "translatedText": "Pour l'instant, nous nous concentrons sur les noms.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "translatedText": "En même temps, associée à cela, il y a une deuxième matrice appelée la matrice clé, que tu multiplies également par chacun des embeddings.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "input": "This produces a second sequence of vectors that we call the keys.",
  "translatedText": "Cela produit une deuxième séquence de vecteurs que nous appelons les clés.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "translatedText": "D'un point de vue conceptuel, tu dois considérer que les clés sont susceptibles de répondre aux requêtes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "translatedText": "Cette matrice de clé est également pleine de paramètres réglables, et tout comme la matrice de requête, elle fait correspondre les vecteurs d'intégration à ce même espace de plus petite dimension.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "translatedText": "Tu considères que les clés correspondent aux requêtes chaque fois qu'elles sont étroitement alignées les unes sur les autres.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "translatedText": "Dans notre exemple, tu peux imaginer que la matrice clé fait correspondre les adjectifs comme pelucheux et bleu à des vecteurs qui sont étroitement alignés avec la requête produite par le mot créature.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "translatedText": "Pour mesurer à quel point chaque clé correspond à chaque requête, tu calcules un produit de points entre chaque paire clé-requête possible.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "translatedText": "J'aime visualiser une grille remplie d'un tas de points, où les points les plus gros correspondent aux produits de points les plus gros, les endroits où les clés et les requêtes s'alignent.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "translatedText": "Pour notre exemple de nom adjectif, cela ressemblerait un peu plus à ceci, où si les clés produites par peluche et bleu s'alignent vraiment étroitement avec la requête produite par créature, alors les produits de points dans ces deux endroits seraient des nombres positifs importants.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "translatedText": "Dans le jargon, les spécialistes de l'apprentissage automatique diraient que cela signifie que l'intégration de peluche et de bleu s'occupe de l'intégration de créature.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "translatedText": "Par contre, le produit de points entre la clé d'un autre mot comme le et la requête pour créature serait une valeur petite ou négative qui reflète le fait qu'ils ne sont pas liés l'un à l'autre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "translatedText": "Nous avons donc cette grille de valeurs qui peut être n'importe quel nombre réel allant de l'infini négatif à l'infini, ce qui nous donne une note sur la pertinence de chaque mot pour mettre à jour le sens de tous les autres mots.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "translatedText": "La façon dont nous allons utiliser ces scores consiste à faire une certaine somme pondérée le long de chaque colonne, pondérée par la pertinence.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "translatedText": "Ainsi, au lieu d'avoir des valeurs allant de l'infini négatif à l'infini, nous voulons que les nombres de ces colonnes soient compris entre 0 et 1, et que la somme de chaque colonne soit égale à 1, comme s'il s'agissait d'une distribution de probabilités.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "translatedText": "Si tu viens du dernier chapitre, tu sais ce qu'il faut faire à ce moment-là.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "translatedText": "Nous calculons un softmax le long de chacune de ces colonnes pour normaliser les valeurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "translatedText": "Dans notre image, après avoir appliqué softmax à toutes les colonnes, nous remplirons la grille avec ces valeurs normalisées.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "translatedText": "À ce stade, tu peux considérer que chaque colonne donne un poids en fonction de la pertinence du mot à gauche par rapport à la valeur correspondante en haut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "input": "We call this grid an attention pattern.",
  "translatedText": "Nous appelons cette grille un modèle d'attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "translatedText": "Si tu regardes le document original sur les transformateurs, tu verras qu'il y a une façon très compacte d'écrire tout cela.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "translatedText": "Ici, les variables q et k représentent respectivement les tableaux complets des vecteurs de requête et de clé, ces petits vecteurs que tu obtiens en multipliant les embeddings par les matrices de requête et de clé.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "translatedText": "Cette expression dans le numérateur est une façon vraiment compacte de représenter la grille de tous les produits de points possibles entre les paires de clés et de requêtes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "translatedText": "Un petit détail technique que je n'ai pas mentionné est que pour la stabilité numérique, il s'avère utile de diviser toutes ces valeurs par la racine carrée de la dimension dans cet espace d'interrogation clé.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "translatedText": "Ensuite, cette softmax qui est enveloppée autour de l'expression complète est censée être comprise pour s'appliquer colonne par colonne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "input": "As to that v term, we'll talk about it in just a second.",
  "translatedText": "Quant à ce terme v, nous en parlerons dans un instant.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "translatedText": "Avant cela, il y a un autre détail technique que jusqu'à présent j'ai zappé.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "translatedText": "Pendant le processus de formation, lorsque tu exécutes ce modèle sur un exemple de texte donné, et que tous les poids sont légèrement ajustés et réglés pour le récompenser ou le punir en fonction de la probabilité qu'il attribue au vrai mot suivant dans le passage, il s'avère que l'ensemble du processus de formation est beaucoup plus efficace si tu lui demandes simultanément de prédire chaque jeton suivant possible après chaque sous-séquence initiale de jetons dans ce passage.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "translatedText": "Par exemple, avec la phrase sur laquelle nous nous sommes concentrés, il pourrait aussi s'agir de prédire quels mots suivent créature et quels mots suivent le.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "translatedText": "C'est vraiment bien, parce que cela signifie que ce qui serait autrement un seul exemple de formation agit effectivement comme plusieurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "translatedText": "Dans le cadre de notre modèle d'attention, cela signifie que tu ne dois jamais permettre aux mots ultérieurs d'influencer les mots antérieurs, car sinon, ils pourraient en quelque sorte donner la réponse à ce qui vient ensuite.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "translatedText": "Ce que cela signifie, c'est que nous voulons que toutes ces taches, celles qui représentent les jetons ultérieurs influençant les jetons antérieurs, soient forcées d'être nulles.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "translatedText": "La chose la plus simple que tu puisses penser à faire est de les mettre à zéro, mais si tu faisais cela, la somme des colonnes ne serait plus égale à un, elles ne seraient pas normalisées.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "translatedText": "Au lieu de cela, une façon courante de procéder consiste à définir toutes ces entrées comme étant des infinis négatifs avant d'appliquer le softmax.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "translatedText": "Si tu fais cela, après l'application de softmax, tous ces éléments seront transformés en zéro, mais les colonnes resteront normalisées.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "input": "This process is called masking.",
  "translatedText": "Ce processus s'appelle le masquage.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "translatedText": "Il existe des versions de l'attention où tu ne l'appliques pas, mais dans notre exemple GPT, même si c'est plus pertinent pendant la phase de formation que ça ne le serait, disons, en l'exécutant en tant que chatbot ou quelque chose comme ça, tu appliques toujours ce masquage pour éviter que les tokens ultérieurs n'influencent les tokens antérieurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "translatedText": "Un autre fait qui mérite réflexion à propos de ce modèle d'attention est que sa taille est égale au carré de la taille du contexte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "translatedText": "C'est pourquoi la taille du contexte peut être un goulot d'étranglement vraiment énorme pour les grands modèles de langage, et la mise à l'échelle n'est pas triviale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "translatedText": "Comme tu l'imagines, motivées par le désir d'avoir des fenêtres de contexte de plus en plus grandes, ces dernières années ont vu apparaître quelques variations du mécanisme d'attention visant à rendre le contexte plus évolutif, mais pour l'instant, toi et moi restons concentrés sur les principes de base.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "translatedText": "D'accord, super, le calcul de ce schéma permet au modèle de déduire quels mots sont pertinents par rapport à quels autres mots.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "translatedText": "Il te faut maintenant mettre à jour les enchâssements, ce qui permet aux mots de transmettre des informations aux autres mots avec lesquels ils sont pertinents.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "translatedText": "Par exemple, tu veux que l'intégration de Fluffy entraîne une modification de Creature qui la déplace dans une autre partie de cet espace d'intégration à 12 000 dimensions qui encode plus spécifiquement une créature Fluffy.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "translatedText": "Ce que je vais faire ici, c'est d'abord te montrer la façon la plus simple de procéder, bien qu'il y ait une légère modification dans le contexte de l'attention à plusieurs têtes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "translatedText": "La façon la plus simple serait d'utiliser une troisième matrice, que nous appelons la matrice de valeur, que tu multiplierais par l'intégration de ce premier mot, par exemple Fluffy.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "translatedText": "Le résultat est ce que tu appellerais un vecteur de valeur, et c'est quelque chose que tu ajoutes à l'intégration du deuxième mot, dans ce cas quelque chose que tu ajoutes à l'intégration de Créature.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "translatedText": "Ce vecteur de valeurs vit donc dans le même espace à très haute dimension que les encastrements.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "translatedText": "Lorsque tu multiplies cette matrice de valeurs par l'intégration d'un mot, tu peux penser que cela revient à dire : si ce mot est pertinent pour ajuster le sens d'autre chose, que faut-il ajouter exactement à l'intégration de cette autre chose pour le refléter ?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "translatedText": "En regardant notre diagramme, mettons de côté toutes les clés et les requêtes, car une fois que tu as calculé le modèle d'attention, tu en as terminé avec ces éléments, puis tu vas prendre cette matrice de valeurs et la multiplier par chacun de ces enchâssements pour produire une séquence de vecteurs de valeurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "translatedText": "Tu peux considérer que ces vecteurs de valeurs sont en quelque sorte associés aux clés correspondantes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "translatedText": "Pour chaque colonne de ce diagramme, tu multiplies chacun des vecteurs de valeurs par le poids correspondant dans cette colonne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "translatedText": "Par exemple ici, sous l'intégration de Créature, tu ajouterais de grandes proportions de vecteurs de valeurs pour Fluffy et Blue, alors que tous les autres vecteurs de valeurs sont réduits à zéro, ou du moins presque.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "translatedText": "Enfin, pour mettre à jour l'intégration associée à cette colonne, qui codait auparavant une signification contextuelle de Creature, tu additionnes toutes ces valeurs rééchelonnées dans la colonne, ce qui produit un changement que tu veux ajouter, que j'appellerai delta-e, et tu l'ajoutes à l'intégration d'origine.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "translatedText": "Avec un peu de chance, il en résulte un vecteur plus raffiné qui code le sens le plus riche en contexte, comme celui d'une créature bleue duveteuse.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "translatedText": "Tu appliques la même somme pondérée à toutes les colonnes de cette image, ce qui produit une séquence de changements. En ajoutant tous ces changements aux encastrements correspondants, tu obtiens une séquence complète d'encastrements plus raffinés qui sortent du bloc d'attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "translatedText": "En faisant un zoom arrière, tout ce processus est ce que tu décrirais comme une seule tête d'attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "translatedText": "Comme je l'ai décrit jusqu'à présent, ce processus est paramétré par trois matrices distinctes, toutes remplies de paramètres réglables, la clé, la requête et la valeur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "translatedText": "Je voudrais prendre un moment pour continuer ce que nous avons commencé dans le dernier chapitre, avec le décompte des points où nous comptons le nombre total de paramètres du modèle en utilisant les chiffres de GPT-3.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "translatedText": "Ces matrices de clés et de requêtes ont chacune 12 288 colonnes, ce qui correspond à la dimension de l'intégration, et 128 lignes, ce qui correspond à la dimension de cet espace de requêtes de clés plus petit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "translatedText": "Cela nous donne environ 1,5 million de paramètres supplémentaires pour chacun d'entre eux.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "translatedText": "Si tu regardes cette matrice de valeurs par contraste, la façon dont j'ai décrit les choses jusqu'à présent suggérerait qu'il s'agit d'une matrice carrée qui a 12 288 colonnes et 12 288 lignes, puisque ses entrées et ses sorties vivent dans ce très grand espace d'intégration.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "input": "If true, that would mean about 150 million added parameters.",
  "translatedText": "Si c'est vrai, cela signifierait environ 150 millions de paramètres supplémentaires.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "input": "And to be clear, you could do that.",
  "translatedText": "Et pour être clair, tu pourrais le faire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "translatedText": "Tu pourrais consacrer des ordres de grandeur de plus de paramètres à la carte de valeurs qu'à la clé et à la requête.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "translatedText": "Mais dans la pratique, il est beaucoup plus efficace de faire en sorte que le nombre de paramètres consacrés à cette carte de valeurs soit le même que celui consacré à la clé et à la requête.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "translatedText": "Ceci est particulièrement pertinent dans le cadre de l'exécution de plusieurs têtes d'attention en parallèle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "translatedText": "Cela signifie que la carte des valeurs est transformée en un produit de deux matrices plus petites.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "translatedText": "D'un point de vue conceptuel, je t'encourage toujours à penser à la carte linéaire globale, avec des entrées et des sorties, toutes deux dans cet espace d'intégration plus large, par exemple en prenant l'intégration du bleu dans cette direction de bleuité que tu ajouterais aux noms.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "translatedText": "C'est juste qu'il s'agit d'un plus petit nombre de lignes, généralement de la même taille que l'espace d'interrogation des clés.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "translatedText": "Ce que cela signifie, c'est que tu peux considérer que les grands vecteurs d'intégration sont représentés dans un espace beaucoup plus petit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "translatedText": "Ce n'est pas l'appellation conventionnelle, mais je vais l'appeler la matrice de valeur vers le bas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "translatedText": "La deuxième matrice fait remonter cet espace plus petit vers l'espace d'intégration, produisant ainsi les vecteurs que tu utilises pour effectuer les mises à jour réelles.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "translatedText": "Je vais appeler celle-ci la matrice de valeur, ce qui, une fois encore, n'est pas conventionnel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "input": "The way that you would see this written in most papers looks a little different.",
  "translatedText": "La façon dont tu verrais cela écrit dans la plupart des journaux est un peu différente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "input": "I'll talk about it in a minute.",
  "translatedText": "J'en parlerai dans une minute.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "translatedText": "À mon avis, cela a tendance à rendre les choses un peu plus confuses sur le plan conceptuel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "translatedText": "Pour utiliser le jargon de l'algèbre linéaire, ce que nous faisons essentiellement, c'est contraindre la carte de valeur globale à être une transformation de rang inférieur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "translatedText": "Pour en revenir au nombre de paramètres, ces quatre matrices ont la même taille, et en les additionnant toutes, nous obtenons environ 6,3 millions de paramètres pour une tête d'attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "translatedText": "Pour être un peu plus précis, tout ce qui a été décrit jusqu'à présent est ce qu'on appelle une tête d'auto-attention, pour la distinguer d'une variante qui apparaît dans d'autres modèles et qu'on appelle l'attention croisée.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "translatedText": "Cela ne concerne pas notre exemple GPT, mais si tu es curieux, l'attention croisée implique des modèles qui traitent deux types de données distincts, comme du texte dans une langue et du texte dans une autre langue qui fait partie d'une génération continue d'une traduction, ou peut-être une entrée audio de la parole et une transcription en cours.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "input": "A cross-attention head looks almost identical.",
  "translatedText": "Une tête d'attention croisée a un aspect presque identique.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "input": "The only difference is that the key and query maps act on different data sets.",
  "translatedText": "La seule différence est que les cartes clés et les cartes de requête agissent sur des ensembles de données différents.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "translatedText": "Dans un modèle de traduction, par exemple, les clés peuvent provenir d'une langue, tandis que les requêtes viennent d'une autre, et le modèle d'attention peut décrire quels mots d'une langue correspondent à quels mots d'une autre langue.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "translatedText": "Et dans ce contexte, il n'y aurait généralement pas de masquage, puisqu'il n'y a pas vraiment de notion d'influence des jetons ultérieurs sur les jetons antérieurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "translatedText": "En restant concentré sur l'attention à soi, si tu as tout compris jusqu'à présent et si tu t'arrêtes ici, tu comprendras l'essence de ce qu'est vraiment l'attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "translatedText": "Tout ce qu'il nous reste à faire, c'est d'exposer le sens dans lequel tu fais cela de nombreuses fois différentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "translatedText": "Dans notre exemple central, nous nous sommes concentrés sur les adjectifs actualisant les noms, mais il existe bien sûr de nombreuses façons différentes dont le contexte peut influencer le sens d'un mot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "translatedText": "Si les mots qu'ils ont écrasés précèdent le mot voiture, cela a des implications sur la forme et la structure de cette voiture.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "input": "And a lot of associations might be less grammatical.",
  "translatedText": "Et beaucoup d'associations pourraient être moins grammaticales.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "translatedText": "Si le mot sorcier se trouve dans le même passage que Harry, cela suggère qu'il pourrait s'agir de Harry Potter, alors que si les mots Reine, Sussex et William se trouvaient dans ce passage, l'intégration de Harry devrait peut-être être mise à jour pour faire référence au prince.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "translatedText": "Pour chaque type différent de mise à jour contextuelle que tu pourrais imaginer, les paramètres de ces matrices de clés et de requêtes seraient différents pour capturer les différents modèles d'attention, et les paramètres de notre carte de valeurs seraient différents en fonction de ce qui devrait être ajouté aux enchâssements.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "translatedText": "Et encore une fois, dans la pratique, le véritable comportement de ces cartes est beaucoup plus difficile à interpréter, les poids étant réglés pour faire tout ce dont le modèle a besoin pour accomplir au mieux son objectif de prédire le prochain jeton.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "translatedText": "Comme je l'ai déjà dit, tout ce que nous avons décrit est une seule tête d'attention, et un bloc d'attention complet à l'intérieur d'un transformateur consiste en ce qu'on appelle une attention à plusieurs têtes, où tu exécutes un grand nombre de ces opérations en parallèle, chacune avec sa propre requête de clé distincte et ses cartes de valeur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "translatedText": "GPT-3, par exemple, utilise 96 têtes d'attention à l'intérieur de chaque bloc.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "translatedText": "Si l'on considère que chacun d'entre eux est déjà un peu confus, c'est certainement beaucoup à retenir dans ta tête.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "translatedText": "Pour l'expliquer de façon très explicite, cela signifie que tu as 96 matrices de clés et de requêtes distinctes qui produisent 96 modèles d'attention distincts.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "translatedText": "Ensuite, chaque tête a ses propres matrices de valeurs distinctes utilisées pour produire 96 séquences de vecteurs de valeurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "translatedText": "Ils sont tous additionnés en utilisant les modèles d'attention correspondants comme poids.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "translatedText": "Cela signifie que pour chaque position dans le contexte, chaque token, chacune de ces têtes produit une proposition de changement à ajouter à l'encastrement dans cette position.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "translatedText": "Ce que tu fais, c'est que tu additionnes tous ces changements proposés, un pour chaque tête, et tu ajoutes le résultat à l'intégration originale de cette position.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "translatedText": "La somme totale ici serait une tranche de ce qui est produit par ce bloc d'attention à plusieurs têtes, un seul de ces enchâssements raffinés qui sort de l'autre extrémité.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "translatedText": "Encore une fois, cela fait beaucoup de choses à penser, alors ne t'inquiète pas du tout si cela prend un peu de temps à assimiler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "translatedText": "L'idée générale est qu'en exécutant plusieurs têtes distinctes en parallèle, tu donnes au modèle la capacité d'apprendre de nombreuses façons distinctes dont le contexte change le sens.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "translatedText": "Si l'on fait le décompte des paramètres avec 96 têtes, chacune comprenant sa propre variation de ces quatre matrices, chaque bloc d'attention à têtes multiples se retrouve avec environ 600 millions de paramètres.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "translatedText": "Il y a une chose supplémentaire légèrement ennuyeuse que je devrais vraiment mentionner pour tous ceux d'entre vous qui continueront à lire d'autres articles sur les transformateurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "translatedText": "Tu te souviens que j'ai dit que la carte des valeurs est divisée en deux matrices distinctes, que j'ai appelées les matrices de la valeur vers le bas et de la valeur vers le haut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "translatedText": "La façon dont j'ai formulé les choses suggère que tu vois cette paire de matrices à l'intérieur de chaque tête d'attention, et tu pourrais absolument l'implémenter de cette façon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "input": "That would be a valid design.",
  "translatedText": "Ce serait une conception valable.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "translatedText": "Mais la façon dont tu vois cela écrit dans les journaux et la façon dont c'est mis en œuvre dans la pratique semblent un peu différentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "translatedText": "Toutes ces matrices de valeur pour chaque tête apparaissent agrafées ensemble dans une matrice géante que nous appelons la matrice de sortie, associée à l'ensemble du bloc d'attention à plusieurs têtes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "translatedText": "Et lorsque tu vois les gens se référer à la matrice de valeur pour une tête d'attention donnée, ils ne se réfèrent généralement qu'à cette première étape, celle que j'appelais la projection de la valeur vers le bas dans l'espace plus petit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "input": "For the curious among you, I've left an on-screen note about it.",
  "translatedText": "Pour les plus curieux d'entre vous, j'ai laissé une note à l'écran à ce sujet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "translatedText": "C'est l'un de ces détails qui risquent de détourner l'attention des principaux points conceptuels, mais je tiens à le signaler pour que tu le saches si tu lis des articles à ce sujet dans d'autres sources.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "translatedText": "En mettant de côté toutes les nuances techniques, nous avons vu dans l'aperçu du dernier chapitre que les données qui passent par un transformateur ne passent pas seulement par un seul bloc d'attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "translatedText": "D'une part, il passe aussi par ces autres opérations appelées perceptrons multicouches.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "input": "We'll talk more about those in the next chapter.",
  "translatedText": "Nous en parlerons plus en détail dans le prochain chapitre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "translatedText": "Et ensuite, il passe par de nombreuses copies de ces deux opérations.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "translatedText": "Cela signifie qu'une fois qu'un mot donné s'est imprégné d'une partie de son contexte, il y a beaucoup plus de chances pour que cet ancrage plus nuancé soit influencé par son environnement plus nuancé.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "translatedText": "Plus on descend dans le réseau, plus chaque intégration prend du sens à partir de toutes les autres intégrations, qui elles-mêmes deviennent de plus en plus nuancées, plus on espère avoir la capacité d'encoder des idées plus abstraites et de plus haut niveau à propos d'une entrée donnée, au-delà des simples descripteurs et de la structure grammaticale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "translatedText": "Des choses comme le sentiment et le ton et s'il s'agit d'un poème et quelles vérités scientifiques sous-jacentes sont pertinentes pour l'œuvre et des choses comme ça.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "translatedText": "En revenant une fois de plus à notre comptabilité, GPT-3 comprend 96 couches distinctes, le nombre total de paramètres clés de requête et de valeur est donc multiplié par 96 autres, ce qui porte la somme totale à un peu moins de 58 milliards de paramètres distincts consacrés à l'ensemble des têtes d'attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "translatedText": "C'est beaucoup, certes, mais cela ne représente qu'environ un tiers des 175 milliards qui se trouvent au total dans le réseau.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "translatedText": "Ainsi, même si l'attention retient toute l'attention, la majorité des paramètres proviennent des blocs assis entre ces étapes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "translatedText": "Dans le prochain chapitre, toi et moi parlerons davantage de ces autres blocs et aussi beaucoup plus du processus de formation.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "translatedText": "Une grande partie du succès du mécanisme d'attention n'est pas tant un type de comportement spécifique qu'il permet, mais le fait qu'il est extrêmement parallélisable, ce qui signifie que tu peux exécuter un grand nombre de calculs en peu de temps à l'aide des GPU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "translatedText": "Étant donné que l'une des grandes leçons sur l'apprentissage profond au cours des dix ou vingt dernières années a été que l'échelle seule semble donner d'énormes améliorations qualitatives dans la performance du modèle, il y a un énorme avantage aux architectures parallélisables qui te permettent de faire cela.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "translatedText": "Si tu veux en savoir plus, j'ai laissé de nombreux liens dans la description.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "translatedText": "En particulier, tout ce qui est produit par Andrej Karpathy ou Chris Ola a tendance à être de l'or pur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "translatedText": "Dans cette vidéo, je voulais simplement parler de l'attention sous sa forme actuelle, mais si tu es curieux de savoir comment nous en sommes arrivés là et comment tu pourrais réinventer cette idée pour toi-même, mon ami Vivek vient de mettre en ligne deux vidéos qui donnent beaucoup plus de motivation.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "translatedText": "Par ailleurs, Britt Cruz de la chaîne The Art of the Problem a réalisé une très belle vidéo sur l'histoire des grands modèles de langage.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "input": "Thank you.",
  "translatedText": "Merci.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]