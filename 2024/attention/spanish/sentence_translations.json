[
 {
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "translatedText": "En el último capítulo, tú y yo empezamos a recorrer el funcionamiento interno de un transformador.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0,
  "end": 4.02
 },
 {
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "translatedText": "Se trata de una de las piezas clave de la tecnología de los grandes modelos lingüísticos y de muchas otras herramientas de la moderna ola de IA.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "translatedText": "Apareció por primera vez en un artículo ya famoso de 2017 titulado Attention is All You Need (La atención es todo lo que necesitas), y en este capítulo tú y yo profundizaremos en lo que es este mecanismo de atención, visualizando cómo procesa los datos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "translatedText": "A modo de resumen rápido, éste es el contexto importante que quiero que tengas en mente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "translatedText": "El objetivo del modelo que tú y yo estamos estudiando es tomar un trozo de texto y predecir qué palabra viene a continuación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 30,
  "end": 36.06
 },
 {
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "translatedText": "El texto de entrada se divide en pequeños trozos que llamamos tokens, y que muy a menudo son palabras o trozos de palabras, pero para que los ejemplos de este vídeo sean más fáciles de entender para ti y para mí, vamos a simplificar suponiendo que los tokens son siempre sólo palabras.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "translatedText": "El primer paso de un transformador es asociar a cada token un vector de alta dimensión, lo que llamamos su incrustación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "translatedText": "La idea más importante que quiero que tengas en mente es cómo las direcciones en este espacio de alta dimensión de todas las incrustaciones posibles pueden corresponderse con el significado semántico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67
 },
 {
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "translatedText": "En el último capítulo vimos un ejemplo de cómo la dirección puede corresponder al género, en el sentido de que añadir un determinado paso en este espacio puede llevarte de la incrustación de un sustantivo masculino a la incrustación del sustantivo femenino correspondiente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "translatedText": "Ése es sólo un ejemplo, puedes imaginar cuántas otras direcciones en este espacio de alta dimensión podrían corresponder a otros muchos aspectos del significado de una palabra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "translatedText": "El objetivo de un transformador es ajustar progresivamente estas incrustaciones para que no se limiten a codificar una palabra individual, sino que incorporen un significado contextual mucho más rico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "translatedText": "Debo decir de entrada que mucha gente encuentra muy confuso el mecanismo de atención, esta pieza clave en un transformador, así que no te preocupes si tardas un poco en asimilar las cosas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "translatedText": "Creo que antes de sumergirnos en los detalles computacionales y en todas las multiplicaciones de matrices, merece la pena pensar en un par de ejemplos del tipo de comportamiento que queremos que permita la atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "translatedText": "Considera las frases topo verdadero americano, un topo de dióxido de carbono, y toma una biopsia del topo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "translatedText": "Tú y yo sabemos que la palabra topo tiene distintos significados en cada uno de ellos, según el contexto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "translatedText": "Pero después del primer paso de un transformador, el que descompone el texto y asocia cada token a un vector, el vector asociado al token sería el mismo en todos estos casos, porque esta incrustación inicial de token es efectivamente una tabla de búsqueda sin referencia al contexto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "translatedText": "Sólo en el siguiente paso del transformador, las incrustaciones circundantes tienen la oportunidad de pasar información a ésta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "translatedText": "La imagen que puedes tener en mente es que hay múltiples direcciones distintas en este espacio de incrustación que codifica los múltiples significados distintos de la palabra topo, y que un bloque de atención bien entrenado calcula lo que hay que añadir a la incrustación genérica para moverla a una de estas direcciones específicas, en función del contexto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "input": "To take another example, consider the embedding of the word tower.",
  "translatedText": "Por poner otro ejemplo, considera la incrustación de la palabra torre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "translatedText": "Se trata presumiblemente de una dirección muy genérica e inespecífica en el espacio, asociada a muchos otros sustantivos grandes y altos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "translatedText": "Si esta palabra fuera inmediatamente precedida de Eiffel, podrías imaginar que quisieras que el mecanismo actualizara este vector para que apuntara en una dirección que codificara más específicamente la torre Eiffel, quizá correlacionada con vectores asociados a París y Francia y a cosas de acero.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "translatedText": "Si además iba precedida de la palabra miniatura, entonces el vector debería actualizarse aún más, para que ya no se correlacione con cosas grandes y altas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "translatedText": "En términos más generales que el mero refinamiento del significado de una palabra, el bloque de atención permite al modelo trasladar la información codificada en una incrustación a la de otra, potencialmente muy lejanas, y potencialmente con información mucho más rica que una sola palabra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "translatedText": "Lo que vimos en el último capítulo fue cómo después de que todos los vectores fluyan a través de la red, incluyendo muchos bloques de atención diferentes, el cálculo que se realiza para producir una predicción de la siguiente ficha es totalmente una función del último vector de la secuencia.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "translatedText": "Imagina, por ejemplo, que el texto que introduces es la mayor parte de toda una novela de misterio, hasta un punto cerca del final, que dice: por tanto, el asesino fue.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "translatedText": "Si el modelo va a predecir con exactitud la siguiente palabra, ese vector final de la secuencia, que empezó su vida incrustando simplemente la palabra era, tendrá que haber sido actualizado por todos los bloques de atención para representar mucho, mucho más que cualquier palabra individual, codificando de algún modo toda la información de la ventana de contexto completa que sea relevante para predecir la siguiente palabra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "input": "To step through the computations, though, let's take a much simpler example.",
  "translatedText": "Sin embargo, para explicar los cálculos, tomemos un ejemplo mucho más sencillo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "translatedText": "Imagina que la entrada incluye la frase, una esponjosa criatura azul vagaba por el verde bosque.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "translatedText": "Y por el momento, supongamos que el único tipo de actualización que nos importa es que los adjetivos ajusten los significados de sus sustantivos correspondientes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "translatedText": "Lo que voy a describir es lo que llamaríamos una sola cabeza de atención, y más adelante veremos cómo el bloque de atención consta de muchas cabezas diferentes que funcionan en paralelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287,
  "end": 295.42
 },
 {
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "translatedText": "De nuevo, la incrustación inicial de cada palabra es un vector de alta dimensión que sólo codifica el significado de esa palabra concreta, sin contexto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "input": "Actually, that's not quite true.",
  "translatedText": "En realidad, eso no es del todo cierto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 304,
  "end": 305.22
 },
 {
  "input": "They also encode the position of the word.",
  "translatedText": "También codifican la posición de la palabra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "translatedText": "Hay mucho más que decir sobre la forma en que se codifican las posiciones, pero ahora mismo, todo lo que necesitas saber es que las entradas de este vector son suficientes para decirte tanto qué palabra es como dónde existe en el contexto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "translatedText": "Vamos a denotar estas incrustaciones con la letra e.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "translatedText": "El objetivo es que una serie de cálculos produzca un nuevo conjunto refinado de incrustaciones en el que, por ejemplo, las correspondientes a los sustantivos hayan ingerido el significado de sus correspondientes adjetivos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "translatedText": "Y jugando al juego del aprendizaje profundo, queremos que la mayoría de los cálculos implicados parezcan productos matriz-vector, donde las matrices están llenas de pesos sintonizables, cosas que el modelo aprenderá basándose en los datos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "translatedText": "Para que quede claro, me estoy inventando este ejemplo de adjetivos que actualizan sustantivos sólo para ilustrar el tipo de comportamiento que podrías imaginar que hace un cabeza de atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "translatedText": "Como ocurre con gran parte del aprendizaje profundo, el verdadero comportamiento es mucho más difícil de analizar porque se basa en ajustar y afinar un gran número de parámetros para minimizar alguna función de coste.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "translatedText": "Es sólo que, a medida que avanzamos por todas las diferentes matrices llenas de parámetros que intervienen en este proceso, creo que es realmente útil tener un ejemplo imaginado de algo que podría estar haciendo para ayudar a que todo sea más concreto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "translatedText": "Para el primer paso de este proceso, puedes imaginar que cada sustantivo, como criatura, hace la pregunta, oye, ¿hay algún adjetivo sentado delante de mí?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "translatedText": "Y para las palabras esponjoso y azul, que cada una pueda responder, sí, soy un adjetivo y estoy en esa posición.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "translatedText": "Esa pregunta se codifica de algún modo como otro vector, otra lista de números, que llamamos la consulta de esta palabra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "translatedText": "Pero este vector de consulta tiene una dimensión mucho menor que el vector de incrustación, digamos 128.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "translatedText": "El cálculo de esta consulta consiste en tomar una matriz determinada, que denominaré wq, y multiplicarla por la incrustación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "translatedText": "Comprimiendo un poco las cosas, vamos a escribir ese vector de consulta como q, y cada vez que me veas poner una matriz junto a una flecha como ésta, es para representar que multiplicando esta matriz por el vector del inicio de la flecha se obtiene el vector del final de la flecha.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "translatedText": "En este caso, multiplicas esta matriz por todas las incrustaciones del contexto, produciendo un vector de consulta para cada token.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "translatedText": "Las entradas de esta matriz son parámetros del modelo, lo que significa que el verdadero comportamiento se aprende de los datos, y en la práctica, lo que hace esta matriz en una cabeza de atención concreta es difícil de analizar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "translatedText": "Pero por nuestro bien, imaginando un ejemplo que podríamos esperar que aprendiera, supondremos que esta matriz de consulta asigna las incrustaciones de los sustantivos a ciertas direcciones en este espacio de consulta más pequeño que, de algún modo, codifica la noción de buscar adjetivos en posiciones precedentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "input": "As to what it does to other embeddings, who knows?",
  "translatedText": "En cuanto a lo que hace con otras incrustaciones, ¿quién sabe?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "translatedText": "Tal vez intente alcanzar simultáneamente algún otro objetivo con ellas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "input": "Right now, we're laser focused on the nouns.",
  "translatedText": "Ahora mismo, estamos centrados en los sustantivos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "translatedText": "Al mismo tiempo, asociada a ésta hay una segunda matriz llamada matriz clave, que también multiplicas por cada una de las incrustaciones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "input": "This produces a second sequence of vectors that we call the keys.",
  "translatedText": "Esto produce una segunda secuencia de vectores que llamamos claves.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "translatedText": "Conceptualmente, debes pensar que las claves responden potencialmente a las consultas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "translatedText": "Esta matriz clave también está llena de parámetros sintonizables, y al igual que la matriz de consulta, mapea los vectores de incrustación a ese mismo espacio dimensional más pequeño.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "translatedText": "Piensa que las claves coinciden con las consultas siempre que se alineen estrechamente entre sí.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "translatedText": "En nuestro ejemplo, imaginarías que la matriz clave asigna los adjetivos como esponjoso y azul a vectores que están estrechamente alineados con la consulta producida por la palabra criatura.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "translatedText": "Para medir lo bien que coincide cada clave con cada consulta, calcula un producto punto entre cada posible par clave-consulta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514
 },
 {
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "translatedText": "Me gusta visualizar una cuadrícula llena de un montón de puntos, donde los puntos más grandes corresponden a los productos de puntos más grandes, los lugares donde se alinean las claves y las consultas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "translatedText": "En nuestro ejemplo del sustantivo adjetivo, esto se parecería un poco más a esto, donde si las claves producidas por esponjoso y azul realmente se alinean estrechamente con la consulta producida por criatura, entonces los productos de punto en estos dos puntos serían algunos números positivos grandes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "translatedText": "En la jerga, la gente del aprendizaje automático diría que esto significa que las incrustaciones de esponjoso y azul atienden a la incrustación de criatura.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "translatedText": "Por el contrario, el producto punto entre la clave de otra palabra como la y la consulta de criatura sería algún valor pequeño o negativo que reflejara que no están relacionadas entre sí.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "translatedText": "Así que tenemos esta cuadrícula de valores que puede ser cualquier número real desde el infinito negativo hasta el infinito, lo que nos da una puntuación de lo relevante que es cada palabra para actualizar el significado de todas las demás.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "translatedText": "La forma en que vamos a utilizar estas puntuaciones es tomar una determinada suma ponderada a lo largo de cada columna, ponderada por la relevancia.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "translatedText": "Así, en lugar de que los valores oscilen entre infinito negativo e infinito, lo que queremos es que los números de estas columnas estén entre 0 y 1, y que cada columna sume 1, como si fueran una distribución de probabilidad.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "translatedText": "Si vienes del último capítulo, ya sabes lo que tenemos que hacer entonces.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "translatedText": "Calculamos un softmax a lo largo de cada una de estas columnas para normalizar los valores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "translatedText": "En nuestra imagen, después de aplicar softmax a todas las columnas, rellenaremos la cuadrícula con estos valores normalizados.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "translatedText": "Llegados a este punto, puedes pensar que cada columna tiene una ponderación según la relevancia de la palabra de la izquierda para el valor correspondiente de la parte superior.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "input": "We call this grid an attention pattern.",
  "translatedText": "A esta cuadrícula la llamamos patrón de atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "translatedText": "Ahora bien, si te fijas en el documento original del transformador, hay una forma muy compacta en la que escriben todo esto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "translatedText": "Aquí las variables q y k representan las matrices completas de los vectores de consulta y clave respectivamente, esos pequeños vectores que se obtienen multiplicando las incrustaciones por las matrices de consulta y clave.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "translatedText": "Esta expresión arriba en el numerador es una forma realmente compacta de representar la red de todos los productos punto posibles entre pares de claves y consultas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "translatedText": "Un pequeño detalle técnico que no he mencionado es que, para la estabilidad numérica, resulta útil dividir todos estos valores por la raíz cuadrada de la dimensión en ese espacio de consulta clave.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 644,
  "end": 653.96
 },
 {
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "translatedText": "Entonces este softmax que se envuelve alrededor de la expresión completa se entiende que se aplica columna por columna.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "input": "As to that v term, we'll talk about it in just a second.",
  "translatedText": "En cuanto a ese v término, hablaremos de él en un segundo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "translatedText": "Antes de eso, hay otro detalle técnico que hasta ahora me he saltado.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "translatedText": "Durante el proceso de entrenamiento, cuando ejecutas este modelo en un ejemplo de texto dado, y todos los pesos se ajustan ligeramente y se afinan para recompensarlo o castigarlo en función de la probabilidad que asigne a la siguiente palabra verdadera del pasaje, resulta que todo el proceso de entrenamiento es mucho más eficaz si haces que prediga simultáneamente cada posible siguiente token que siga a cada subsecuencia inicial de tokens de este pasaje.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "translatedText": "Por ejemplo, con la frase en la que nos hemos centrado, también podría predecir qué palabras siguen a criatura y qué palabras siguen a.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "translatedText": "Esto está muy bien, porque significa que lo que de otro modo sería un único ejemplo de entrenamiento actúa como muchos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "translatedText": "A efectos de nuestro patrón de atención, significa que nunca debes permitir que las palabras posteriores influyan en las anteriores, ya que, de lo contrario, podrían desvelar la respuesta de lo que viene a continuación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "translatedText": "Lo que esto significa es que queremos que todos estos puntos de aquí, los que representan fichas posteriores que influyen en las anteriores, sean de alguna manera forzados a ser cero.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "translatedText": "Lo más sencillo que se te ocurre hacer es ponerlas igual a cero, pero si hicieras eso las columnas ya no sumarían uno, no estarían normalizadas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "translatedText": "Por eso, una forma habitual de hacerlo es que, antes de aplicar softmax, fijes todas esas entradas en infinito negativo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "translatedText": "Si haces eso, después de aplicar softmax, todos esos se convierten en cero, pero las columnas siguen normalizadas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "input": "This process is called masking.",
  "translatedText": "Este proceso se denomina enmascaramiento.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746,
  "end": 747.54
 },
 {
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "translatedText": "Hay versiones de atención en las que no lo aplicas, pero en nuestro ejemplo de GPT, aunque esto es más relevante durante la fase de entrenamiento de lo que sería, digamos, ejecutándolo como un chatbot o algo así, siempre aplicas este enmascaramiento para evitar que los tokens posteriores influyan en los anteriores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "translatedText": "Otro hecho sobre el que merece la pena reflexionar acerca de este patrón de atención es cómo su tamaño es igual al cuadrado del tamaño del contexto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "translatedText": "Por eso el tamaño del contexto puede ser un cuello de botella realmente enorme para los grandes modelos lingüísticos, y ampliarlo no es trivial.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "translatedText": "Como imaginarás, motivado por el deseo de tener ventanas de contexto cada vez más grandes, en los últimos años se han producido algunas variaciones en el mecanismo de atención destinadas a hacer que el contexto sea más escalable, pero aquí, tú y yo nos centramos en lo básico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "translatedText": "Vale, genial, calcular este patrón permite al modelo deducir qué palabras son relevantes para qué otras palabras.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "translatedText": "Ahora tienes que actualizar realmente las incrustaciones, permitiendo que las palabras pasen información a cualquier otra palabra para la que sean relevantes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "translatedText": "Por ejemplo, quieres que la incrustación de Pelusa provoque de algún modo un cambio en la Criatura que la desplace a una parte diferente de este espacio de incrustación de 12.000 dimensiones que codifica más específicamente una criatura Pelusa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "translatedText": "Lo que voy a hacer aquí es mostrarte primero la forma más directa de hacerlo, aunque hay una pequeña modificación en el contexto de la atención multicabezal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "translatedText": "La forma más directa sería utilizar una tercera matriz, que llamamos matriz de valores, que multiplicas por la incrustación de esa primera palabra, por ejemplo Fluffy.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "translatedText": "El resultado de esto es lo que llamarías un vector de valores, y es algo que añades a la incrustación de la segunda palabra, en este caso algo que añades a la incrustación de Criatura.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "translatedText": "Así que este vector de valores vive en el mismo espacio de muy alta dimensión que las incrustaciones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847
 },
 {
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "translatedText": "Cuando multiplicas esta matriz de valores por la incrustación de una palabra, puedes pensar que dice: si esta palabra es relevante para ajustar el significado de otra cosa, ¿qué debe añadirse exactamente a la incrustación de esa otra cosa para reflejarlo?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "translatedText": "Volviendo a nuestro diagrama, dejemos a un lado todas las claves y las consultas, ya que después de calcular el patrón de atención ya has terminado con ellas, entonces vas a tomar esta matriz de valores y multiplicarla por cada una de esas incrustaciones para producir una secuencia de vectores de valores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "translatedText": "Puedes pensar en estos vectores de valores como si estuvieran asociados a las claves correspondientes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "translatedText": "Para cada columna de este diagrama, multiplica cada uno de los vectores de valores por el peso correspondiente de esa columna.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "translatedText": "Por ejemplo, aquí, en la incrustación de Criatura, estarías añadiendo grandes proporciones de los vectores de valor de Pelusa y Azul, mientras que todos los demás vectores de valor se reducen a cero, o al menos casi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "translatedText": "Y por último, la forma de actualizar realmente la incrustación asociada a esta columna, codificando previamente algún significado libre de contexto de Criatura, es sumar todos estos valores reescalados en la columna, produciendo un cambio que quieres añadir, que etiquetaré delta-e, y luego lo añades a la incrustación original.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "translatedText": "Con suerte, lo que resulta es un vector más refinado que codifica el significado más rico contextualmente, como el de una criatura azul esponjosa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "translatedText": "Y, por supuesto, no lo haces sólo a una incrustación, sino que aplicas la misma suma ponderada a todas las columnas de esta imagen, produciendo una secuencia de cambios, y sumando todos esos cambios a las incrustaciones correspondientes, se produce una secuencia completa de incrustaciones más refinadas que salen del bloque de atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "translatedText": "Ampliando la imagen, todo este proceso es lo que se describiría como una sola cabeza de atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "translatedText": "Tal y como he descrito las cosas hasta ahora, este proceso está parametrizado por tres matrices distintas, todas ellas llenas de parámetros sintonizables: la clave, la consulta y el valor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "translatedText": "Quiero dedicar un momento a continuar lo que empezamos en el último capítulo, con el recuento en el que contamos el número total de parámetros del modelo utilizando los números de GPT-3.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "translatedText": "Estas matrices de clave y consulta tienen cada una 12.288 columnas, que coinciden con la dimensión de incrustación, y 128 filas, que coinciden con la dimensión de ese espacio de consulta de clave más pequeño.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "translatedText": "Esto nos da 1,5 millones de parámetros más o menos para cada uno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "translatedText": "En cambio, si observas esa matriz de valores, la forma en que he descrito las cosas hasta ahora sugeriría que es una matriz cuadrada que tiene 12.288 columnas y 12.288 filas, ya que tanto sus entradas como sus salidas viven en este espacio de incrustación tan grande.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "input": "If true, that would mean about 150 million added parameters.",
  "translatedText": "De ser cierto, eso significaría unos 150 millones de parámetros añadidos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "input": "And to be clear, you could do that.",
  "translatedText": "Y para que quede claro, podrías hacerlo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "translatedText": "Podrías dedicar órdenes de magnitud más de parámetros al mapa de valores que a la clave y la consulta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "translatedText": "Pero, en la práctica, es mucho más eficaz si, en lugar de eso, haces que el número de parámetros dedicados a este mapa de valores sea el mismo que el dedicado a la clave y a la consulta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "translatedText": "Esto es especialmente relevante en el caso de ejecutar varias cabezas de atención en paralelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "translatedText": "El aspecto es que el mapa de valores se factoriza como producto de dos matrices más pequeñas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "translatedText": "Conceptualmente, aún te animaría a pensar en el mapa lineal general, uno con entradas y salidas, ambos en este espacio de incrustación mayor, por ejemplo, llevando la incrustación del azul a esta dirección de azulidad que añadirías a los sustantivos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "translatedText": "Es que se trata de un número menor de filas, normalmente del mismo tamaño que el espacio de consulta de la clave.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "translatedText": "Lo que esto significa es que puedes pensar que se trata de reducir los grandes vectores de incrustación a un espacio mucho más pequeño.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "translatedText": "No es la denominación convencional, pero voy a llamarla matriz de valores descendentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "translatedText": "La segunda matriz mapea desde este espacio más pequeño hasta el espacio de incrustación, produciendo los vectores que utilizas para realizar las actualizaciones reales.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "translatedText": "Voy a llamar a ésta la matriz del valor hacia arriba, que tampoco es convencional.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1071,
  "end": 1074.74
 },
 {
  "input": "The way that you would see this written in most papers looks a little different.",
  "translatedText": "La forma en que verías esto escrito en la mayoría de los artículos científicos es un poco diferente.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "input": "I'll talk about it in a minute.",
  "translatedText": "Hablaré de ello dentro de un momento.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "translatedText": "En mi opinión, tiende a hacer las cosas un poco más confusas conceptualmente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "translatedText": "Para utilizar la jerga del álgebra lineal, lo que estamos haciendo básicamente es restringir el mapa de valores global para que sea una transformación de bajo rango.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "translatedText": "Volviendo al recuento de parámetros, las cuatro matrices tienen el mismo tamaño, y sumándolas todas obtenemos unos 6,3 millones de parámetros para una cabeza de atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "translatedText": "Como nota al margen, para ser un poco más precisos, todo lo descrito hasta ahora es lo que la gente llamaría una cabeza de autoatención, para distinguirla de una variación que aparece en otros modelos y que se llama atención cruzada.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "translatedText": "Esto no es relevante para nuestro ejemplo de GPT, pero si tienes curiosidad, la atención cruzada implica modelos que procesan dos tipos distintos de datos, como texto en un idioma y texto en otro idioma que forma parte de una generación en curso de una traducción, o quizá entrada de audio del habla y una transcripción en curso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "input": "A cross-attention head looks almost identical.",
  "translatedText": "Una cabeza de atención cruzada tiene un aspecto casi idéntico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "input": "The only difference is that the key and query maps act on different data sets.",
  "translatedText": "La única diferencia es que los mapas clave y de consulta actúan sobre conjuntos de datos distintos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "translatedText": "En un modelo que hace traducciones, por ejemplo, las claves podrían proceder de una lengua, mientras que las consultas proceden de otra, y el patrón de atención podría describir qué palabras de una lengua corresponden a qué palabras de otra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "translatedText": "Y en este escenario no suele haber enmascaramiento, ya que no existe realmente la noción de que las fichas posteriores afecten a las anteriores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "translatedText": "Sin embargo, si te mantuvieras centrado en la autoatención, si lo comprendieras todo hasta ahora, y si te detuvieras aquí, llegarías a la esencia de lo que es realmente la atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "translatedText": "Lo único que nos queda es exponer el sentido en que lo haces muchas veces.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "translatedText": "En nuestro ejemplo central nos hemos centrado en los adjetivos que actualizan los sustantivos, pero, por supuesto, hay muchas formas distintas en que el contexto puede influir en el significado de una palabra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "translatedText": "Si las palabras que chocaron precedieron a la palabra coche, tiene implicaciones para la forma y la estructura de ese coche.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "input": "And a lot of associations might be less grammatical.",
  "translatedText": "Y muchas asociaciones podrían ser menos gramaticales.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "translatedText": "Si la palabra mago está en algún lugar del mismo pasaje que Harry, sugiere que podría referirse a Harry Potter, mientras que si en cambio las palabras Reina, Sussex y Guillermo estuvieran en ese pasaje, quizá habría que actualizar la incrustación de Harry para referirse al príncipe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "translatedText": "Para cada tipo diferente de actualización contextual que puedas imaginar, los parámetros de estas matrices clave y de consulta serían diferentes para captar los distintos patrones de atención, y los parámetros de nuestro mapa de valores serían diferentes en función de lo que hubiera que añadir a las incrustaciones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "translatedText": "Y de nuevo, en la práctica el verdadero comportamiento de estos mapas es mucho más difícil de interpretar, ya que los pesos se ajustan para que hagan lo que el modelo necesite que hagan para cumplir mejor su objetivo de predecir la siguiente ficha.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "translatedText": "Como he dicho antes, todo lo que hemos descrito es un único cabezal de atención, y un bloque de atención completo dentro de un transformador consiste en lo que se denomina atención multicabezal, en la que ejecutas muchas de estas operaciones en paralelo, cada una con su propia consulta de claves y mapas de valores distintos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "translatedText": "GPT-3, por ejemplo, utiliza 96 cabezas de atención dentro de cada bloque.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "translatedText": "Teniendo en cuenta que cada uno de ellos ya es un poco confuso, sin duda es mucho que retener en la cabeza.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "translatedText": "Para explicarlo todo muy explícitamente, esto significa que tienes 96 matrices de claves y consultas distintas que producen 96 patrones de atención distintos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265
 },
 {
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "translatedText": "Luego, cada cabeza tiene sus propias matrices de valores distintas, que se utilizan para producir 96 secuencias de vectores de valores.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "translatedText": "Todos ellos se suman utilizando como pesos los patrones de atención correspondientes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "translatedText": "Lo que esto significa es que para cada posición del contexto, cada token, cada una de estas cabezas produce una propuesta de cambio que se añadirá a la incrustación en esa posición.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "translatedText": "Así que lo que haces es sumar todos esos cambios propuestos, uno por cada cabeza, y añades el resultado a la incrustación original de esa posición.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "translatedText": "Toda esta suma de aquí sería una porción de lo que sale de este bloque de atención múltiple, una sola de esas incrustaciones refinadas que sale por el otro extremo del mismo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "translatedText": "De nuevo, hay mucho en lo que pensar, así que no te preocupes si tardas un poco en asimilarlo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "translatedText": "La idea general es que, al ejecutar muchas cabezas distintas en paralelo, estás dando al modelo la capacidad de aprender muchas formas distintas en que el contexto cambia el significado.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "translatedText": "Si sacamos nuestra cuenta corriente de recuento de parámetros con 96 cabezas, cada una de las cuales incluye su propia variación de estas cuatro matrices, cada bloque de atención multicabezas acaba teniendo unos 600 millones de parámetros.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "translatedText": "Hay una cosa añadida un poco molesta que realmente debería mencionar para cualquiera de vosotros que siga leyendo más sobre los transformers.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "translatedText": "Recuerdas que dije que el mapa de valores se descompone en dos matrices distintas, que denominé matrices de valor hacia abajo y hacia arriba.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "translatedText": "El modo en que he planteado las cosas sugeriría que ves este par de matrices dentro de cada cabeza de atención, y podrías implementarlo absolutamente de este modo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "input": "That would be a valid design.",
  "translatedText": "Sería un diseño válido.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "translatedText": "Pero la forma en que ves esto escrito en los artículos científicos y la forma en que se aplica en la práctica es un poco diferente.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "translatedText": "Todas estas matrices de valores de cada cabeza aparecen grapadas en una matriz gigante que llamamos matriz de salida, asociada a todo el bloque de atención multicabezal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "translatedText": "Y cuando ves a la gente referirse a la matriz de valores para una cabeza de atención determinada, normalmente sólo se refieren a este primer paso, el que yo etiqueté como la proyección del valor hacia abajo en el espacio más pequeño.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "input": "For the curious among you, I've left an on-screen note about it.",
  "translatedText": "Para los más curiosos, he dejado una nota en pantalla al respecto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "translatedText": "Es uno de esos detalles que corren el riesgo de distraer de los puntos conceptuales principales, pero quiero señalarlo para que lo sepas si lees sobre esto en otras fuentes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "translatedText": "Dejando a un lado todos los matices técnicos, en el avance del último capítulo vimos cómo los datos que fluyen a través de un transformador no sólo fluyen a través de un único bloque de atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "translatedText": "Por un lado, también pasa por estas otras operaciones llamadas perceptrones multicapa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "input": "We'll talk more about those in the next chapter.",
  "translatedText": "Hablaremos más de ellos en el próximo capítulo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "translatedText": "Y luego pasa repetidamente por muchas muchas copias de estas dos operaciones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "translatedText": "Lo que esto significa es que, después de que una palabra determinada se impregne de parte de su contexto, hay muchas más posibilidades de que esta impregnación más matizada se vea influida por su entorno más matizado.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "translatedText": "Cuanto más se desciende en la red, y cada incrustación adquiere cada vez más significado de todas las demás incrustaciones, que a su vez son cada vez más matizadas, se espera que exista la capacidad de codificar ideas de nivel superior y más abstractas sobre una entrada determinada, más allá de los meros descriptores y la estructura gramatical.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "translatedText": "Cosas como el sentimiento y el tono y si es un poema y qué verdades científicas subyacentes son relevantes para la pieza y cosas así.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "translatedText": "Volviendo una vez más a nuestro marcador, GPT-3 incluye 96 capas distintas, por lo que el número total de parámetros clave de consulta y valor se multiplica por otros 96, lo que hace que la suma total sea de algo menos de 58.000 millones de parámetros distintos dedicados a todas las cabezas de atención.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "translatedText": "Es mucho, sin duda, pero sólo es un tercio de los 175.000 millones que hay en total en la red.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "translatedText": "Así que, aunque la atención acapare toda la atención, la mayoría de los parámetros proceden de los bloques que se encuentran entre estos pasos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "translatedText": "En el próximo capítulo, tú y yo hablaremos más sobre esos otros bloques y también mucho más sobre el proceso de entrenamiento.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "translatedText": "Gran parte del éxito del mecanismo de atención no radica tanto en el tipo específico de comportamiento que permite, sino en el hecho de que es extremadamente paralelizable, lo que significa que puedes ejecutar un gran número de cálculos en poco tiempo utilizando GPU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "translatedText": "Dado que una de las grandes lecciones sobre el aprendizaje profundo en la última década o dos ha sido que la escala por sí sola parece dar enormes mejoras cualitativas en el rendimiento del modelo, hay una gran ventaja en las arquitecturas paralelizables que te permiten hacer esto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "translatedText": "Si quieres saber más sobre este tema, he dejado muchos enlaces en la descripción.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "translatedText": "En particular, todo lo producido por Andrej Karpathy o Chris Ola suele ser oro puro.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "translatedText": "En este vídeo, sólo quería hablar de la atención en su forma actual, pero si tienes curiosidad por saber más sobre la historia de cómo hemos llegado hasta aquí y cómo podrías reinventar esta idea para ti, mi amigo Vivek acaba de publicar un par de vídeos en los que ofrece mucha más motivación.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "translatedText": "Además, Britt Cruz, del canal El Arte del Problema, tiene un vídeo muy bueno sobre la historia de los grandes modelos lingüísticos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "input": "Thank you.",
  "translatedText": "Gracias.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]
