[
 {
  "input": "Demystifying self-attention, multiple heads, and cross-attention.",
  "translatedText": "ไขปริศนาความสนใจในตนเอง มีหลายหัว และให้ความสนใจข้าม",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "translatedText": "แทนที่จะอ่านโฆษณาที่ได้รับการสนับสนุน บทเรียนเหล่านี้ได้รับทุนโดยตรงจากผู้ดู: https://3b1b.co/support",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "An equally valuable form of support is to simply share the videos.",
  "translatedText": "การสนับสนุนที่มีคุณค่าไม่แพ้กันคือการแชร์วิดีโอ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Other resources about transformers",
  "translatedText": "แหล่งข้อมูลอื่นๆ เกี่ยวกับหม้อแปลงไฟฟ้า",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Andrej Karpathy's videos",
  "translatedText": "วิดีโอของ Andrej Karpathy",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "The Transformer Circuits posts by Anthropic",
  "translatedText": "โพสต์ของ The Transformer Circuits โดย Anthropic",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "https://transformer-circuits.pub/2021/framework/index.html",
  "translatedText": "https://transformer-circuits.pub/2021/framework/index.html",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "In particular, it was only after I read this post that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources.",
  "translatedText": "โดยเฉพาะอย่างยิ่ง หลังจากที่ฉันอ่านโพสต์นี้เท่านั้น ฉันจึงเริ่มคิดถึงการรวมกันของค่าและเมทริกซ์เอาท์พุตว่าเป็นแผนที่ระดับต่ำที่รวมกันตั้งแต่พื้นที่ฝังไปจนถึงตัวมันเอง ซึ่งอย่างน้อยก็ในใจของฉัน ได้สร้างสิ่งต่างๆ มากมาย ชัดเจนกว่าแหล่งอื่นๆ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "History of language models by Brit Cruise, @ArtOfTheProblem ",
  "translatedText": "ประวัติความเป็นมาของแบบจำลองภาษาโดย Brit Cruise, @ArtOfTheProblem ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "https://youtu.be/OFS90-FX6pg",
  "translatedText": "https://youtu.be/OFS90-FX6pg",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "What is a Language Model by @vcubingx ",
  "translatedText": "โมเดลภาษาโดย @vcubingx คืออะไร ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "translatedText": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Site with exercises related to ML programming and GPTs",
  "translatedText": "ไซต์ที่มีแบบฝึกหัดที่เกี่ยวข้องกับการเขียนโปรแกรม ML และ GPT",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "https://www.gptandchill.ai/codingproblems",
  "translatedText": "https://www.gptandchill.ai/codingproblems",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Early paper on how directions in embedding spaces have meaning:",
  "translatedText": "บทความฉบับแรกเกี่ยวกับทิศทางในการฝังช่องว่างมีความหมาย:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "https://arxiv.org/pdf/1301.3781.pdf",
  "translatedText": "https://arxiv.org/pdf/1301.3781.pdf",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "Timestamps:",
  "translatedText": "การประทับเวลา:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "0:00 - Recap on embeddings",
  "translatedText": "0:00 - สรุปเกี่ยวกับการฝัง",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "1:39 - Motivating examples",
  "translatedText": "1:39 - ตัวอย่างที่สร้างแรงบันดาลใจ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "4:29 - The attention pattern",
  "translatedText": "4:29 - รูปแบบความสนใจ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "11:08 - Masking",
  "translatedText": "11:08 - การมาสก์",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "12:42 - Context size",
  "translatedText": "12:42 - ขนาดบริบท",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "13:10 - Values",
  "translatedText": "13:10 - ค่านิยม",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "15:44 - Counting parameters",
  "translatedText": "15:44 - การนับพารามิเตอร์",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "18:21 - Cross-attention",
  "translatedText": "18:21 - ความสนใจแบบข้ามๆ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "19:19 - Multiple heads",
  "translatedText": "19:19 - หลายหัว",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "22:16 - The output matrix",
  "translatedText": "22:16 - เมทริกซ์เอาท์พุต",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "23:19 - Going deeper",
  "translatedText": "23:19 - เจาะลึกยิ่งขึ้น",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "24:54 - Ending",
  "translatedText": "24:54 - สิ้นสุด",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0
 }
]
