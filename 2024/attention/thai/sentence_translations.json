[
 {
  "translatedText": "ในบทสุดท้าย คุณและฉันเริ่มก้าวเข้าสู่การทำงานภายในของหม้อแปลงไฟฟ้า",
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "translatedText": "นี่เป็นหนึ่งในเทคโนโลยีที่สำคัญในโมเดลภาษาขนาดใหญ่ และเครื่องมืออื่นๆ อีกมากมายในคลื่นลูกใหม่ของ AI",
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "translatedText": "เรื่องนี้เริ่มแพร่หลายในรายงานที่โด่งดังในปี 2017 ชื่อว่า Attention is All You Need และในบทนี้คุณและฉันจะเจาะลึกว่ากลไกความสนใจนี้คืออะไร โดยแสดงภาพว่ากลไกการประมวลผลข้อมูลเป็นอย่างไร",
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "translatedText": "เพื่อสรุปสั้นๆ ต่อไปนี้คือบริบทสำคัญที่ฉันอยากให้คุณคำนึงถึง",
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "translatedText": "เป้าหมายของแบบจำลองที่คุณและฉันกำลังศึกษาคือการนำข้อความมาส่วนหนึ่งและคาดเดาคำถัดไป",
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "translatedText": "ข้อความที่ป้อนแบ่งออกเป็นส่วนเล็กๆ ที่เราเรียกว่าโทเค็น และบ่อยครั้งเป็นคำหรือส่วนของคำ แต่เพียงเพื่อให้คุณและฉันคิดตัวอย่างในวิดีโอนี้ได้ง่ายขึ้น มาทำให้ง่ายขึ้นโดยแกล้งทำเป็นว่าโทเค็นเป็น เป็นเพียงคำพูดเสมอ",
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "translatedText": "ขั้นตอนแรกในหม้อแปลงไฟฟ้าคือการเชื่อมโยงแต่ละโทเค็นกับเวกเตอร์ที่มีมิติสูง ซึ่งเราเรียกว่าการฝัง",
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "translatedText": "แนวคิดที่สำคัญที่สุดที่ฉันอยากให้คุณคำนึงถึงคือทิศทางในพื้นที่มิติสูงของการฝังที่เป็นไปได้ทั้งหมดจะสอดคล้องกับความหมายเชิงความหมายได้อย่างไร",
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "translatedText": "ในบทที่แล้ว เราได้เห็นตัวอย่างว่าทิศทางสามารถสอดคล้องกับเพศได้อย่างไร ในแง่ที่ว่าการเพิ่มขั้นตอนบางอย่างในพื้นที่นี้สามารถพาคุณจากการฝังคำนามเพศชายไปเป็นการฝังคำนามเพศหญิงที่สอดคล้องกัน",
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "translatedText": "นั่นเป็นเพียงตัวอย่างหนึ่งที่คุณสามารถจินตนาการได้ว่ามีทิศทางอื่นๆ อีกกี่ทิศทางในพื้นที่มิติสูงนี้สามารถสอดคล้องกับความหมายด้านอื่นๆ มากมายของคำได้",
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "translatedText": "จุดมุ่งหมายของหม้อแปลงไฟฟ้าคือการปรับการฝังเหล่านี้อย่างต่อเนื่อง เพื่อที่จะไม่เพียงแค่เข้ารหัสคำแต่ละคำเท่านั้น แต่ยังรวมเอาความหมายตามบริบทที่สมบูรณ์ยิ่งขึ้นอีกด้วย",
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "translatedText": "ฉันควรจะบอกล่วงหน้าว่าผู้คนจำนวนมากพบว่ากลไกความสนใจ ซึ่งเป็นชิ้นส่วนสำคัญในหม้อแปลงไฟฟ้า ทำให้เกิดความสับสนมาก ดังนั้นอย่ากังวลหากต้องใช้เวลาสักระยะกว่าสิ่งต่างๆ จะจมลงไป",
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "translatedText": "ฉันคิดว่าก่อนที่เราจะเจาะลึกรายละเอียดการคำนวณและการคูณเมทริกซ์ทั้งหมด คุณควรคิดถึงตัวอย่างสองสามตัวอย่างสำหรับพฤติกรรมประเภทที่เราต้องการให้ให้ความสนใจก่อน",
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "translatedText": "พิจารณาวลีของไฝแท้ของอเมริกา คาร์บอนไดออกไซด์หนึ่งโมล และตัดชิ้นเนื้อของไฝ",
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "translatedText": "คุณและฉันรู้ว่าคำว่าตุ่นมีความหมายที่แตกต่างกันในแต่ละคำขึ้นอยู่กับบริบท",
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "translatedText": "แต่หลังจากขั้นตอนแรกของหม้อแปลง ตัวที่แบ่งข้อความและเชื่อมโยงแต่ละโทเค็นกับเวกเตอร์ เวกเตอร์ที่เกี่ยวข้องกับโมลจะเหมือนกันในทุกกรณี เนื่องจากการฝังโทเค็นเริ่มต้นนี้ ถือเป็นตารางการค้นหาที่มีประสิทธิภาพ โดยไม่มีการอ้างอิงถึงบริบท",
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "translatedText": "เฉพาะในขั้นตอนต่อไปของหม้อแปลงเท่านั้นที่การฝังที่อยู่โดยรอบมีโอกาสที่จะส่งข้อมูลไปยังอันนี้",
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "translatedText": "ภาพที่คุณอาจนึกถึงคือ มีพื้นที่การฝังหลายทิศทางที่แตกต่างกัน ซึ่งเข้ารหัสความหมายที่แตกต่างกันหลายประการของคำว่า mole และบล็อกความสนใจที่ได้รับการฝึกอบรมมาอย่างดีจะคำนวณสิ่งที่คุณต้องเพิ่มลงในการฝังทั่วไปเพื่อย้ายไปที่ หนึ่งในทิศทางเฉพาะเหล่านี้เป็นหน้าที่ของบริบท",
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "translatedText": "หากต้องการยกตัวอย่างอื่น ให้พิจารณาการฝังคำว่า หอคอย",
  "input": "To take another example, consider the embedding of the word tower.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "translatedText": "ทิศทางนี้น่าจะเป็นทิศทางทั่วไปและไม่เฉพาะเจาะจงในพื้นที่ ซึ่งเชื่อมโยงกับคำนามขนาดใหญ่และสูงอื่นๆ อีกมากมาย",
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "translatedText": "หากคำนี้นำหน้าด้วยคำว่า &quot;Eiffel&quot; นำหน้าทันที คุณอาจจินตนาการได้ว่าต้องการให้กลไกอัปเดตเวกเตอร์นี้เพื่อให้ชี้ไปในทิศทางที่เข้ารหัสหอไอเฟลโดยเฉพาะมากขึ้น ซึ่งอาจสัมพันธ์กับเวกเตอร์ที่เกี่ยวข้องกับปารีสและฝรั่งเศส และสิ่งต่างๆ ที่ทำจากเหล็ก",
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "translatedText": "หากนำหน้าด้วยคำว่าจิ๋ว ก็ควรอัปเดตเวกเตอร์ให้ละเอียดยิ่งขึ้น เพื่อไม่ให้สัมพันธ์กับสิ่งสูงใหญ่อีกต่อไป",
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "translatedText": "โดยทั่วไปแล้ว มากกว่าแค่การปรับปรุงความหมายของคำ บล็อกความสนใจช่วยให้โมเดลสามารถย้ายข้อมูลที่เข้ารหัสไว้ในข้อมูลหนึ่งที่ฝังไว้ไปยังอีกข้อมูลหนึ่ง ซึ่งอาจเป็นข้อมูลที่อยู่ห่างไกล และอาจมีข้อมูลที่สมบูรณ์มากกว่าคำเพียงคำเดียว",
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "translatedText": "สิ่งที่เราเห็นในบทที่แล้วคือหลังจากที่เวกเตอร์ทั้งหมดไหลผ่านเครือข่าย รวมถึงบล็อกความสนใจที่แตกต่างกันจำนวนมาก การคำนวณที่คุณดำเนินการเพื่อสร้างการทำนายโทเค็นถัดไปนั้นเป็นฟังก์ชันของเวกเตอร์สุดท้ายในลำดับทั้งหมด",
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "translatedText": "ลองนึกภาพว่าข้อความที่คุณป้อนส่วนใหญ่เป็นนวนิยายลึกลับทั้งเล่ม ไปจนถึงจุดใกล้จบซึ่งอ่านได้ ดังนั้นฆาตกรจึงเป็นเช่นนั้น",
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "translatedText": "หากแบบจำลองจะทำนายคำถัดไปได้อย่างแม่นยำ เวกเตอร์สุดท้ายในลำดับซึ่งเริ่มต้นชีวิตเพียงแค่ฝังคำนั้นไว้ จะต้องได้รับการอัปเดตโดยบล็อคความสนใจทั้งหมดเพื่อเป็นตัวแทนมากกว่าบุคคลใดๆ มาก word เข้ารหัสข้อมูลทั้งหมดจากหน้าต่างบริบททั้งหมดที่เกี่ยวข้องกับการทำนายคำถัดไป",
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "translatedText": "เพื่อที่จะก้าวผ่านการคำนวณ มาดูตัวอย่างที่ง่ายกว่านี้กัน",
  "input": "To step through the computations, though, let's take a much simpler example.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "translatedText": "ลองนึกภาพว่าอินพุตมีวลี สิ่งมีชีวิตสีฟ้าขนปุยท่องไปในป่าเขียวขจี",
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "translatedText": "และในขณะนี้ สมมติว่าการอัปเดตประเภทเดียวที่เราสนใจคือการให้คำคุณศัพท์ปรับความหมายของคำนามที่เกี่ยวข้อง",
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "translatedText": "สิ่งที่ผมจะอธิบายคือสิ่งที่เราจะเรียกว่าความสนใจแบบหัวเดียว และต่อมาเราจะมาดูว่าบล็อกความสนใจประกอบด้วยหัวต่างๆ มากมายที่วิ่งขนานกันอย่างไร",
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "translatedText": "ขอย้ำอีกครั้งว่า การฝังครั้งแรกสำหรับแต่ละคำคือเวกเตอร์ที่มีมิติสูงซึ่งเข้ารหัสเฉพาะความหมายของคำนั้นโดยไม่มีบริบท",
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "translatedText": "จริงๆแล้วนั่นไม่เป็นความจริงเลย",
  "input": "Actually, that's not quite true.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "translatedText": "พวกเขายังเข้ารหัสตำแหน่งของคำด้วย",
  "input": "They also encode the position of the word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "translatedText": "มีอะไรอีกมากมายที่จะบอกว่าตำแหน่งถูกเข้ารหัส แต่ตอนนี้ สิ่งเดียวที่คุณต้องรู้ก็คือค่าของเวกเตอร์นี้เพียงพอที่จะบอกคุณทั้งว่าคำนั้นคืออะไรและอยู่ที่ไหนในบริบท",
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "translatedText": "เรามาแสดงการฝังเหล่านี้ด้วยตัวอักษร e กันดีกว่า",
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "translatedText": "เป้าหมายคือการมีชุดการคำนวณที่สร้างชุดการฝังที่ได้รับการปรับปรุงใหม่ โดยที่ ตัวอย่างเช่น กลุ่มที่สอดคล้องกับคำนามได้นำเข้าความหมายจากคำคุณศัพท์ที่เกี่ยวข้อง",
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "translatedText": "และการเล่นเกมการเรียนรู้เชิงลึก เราต้องการให้การคำนวณส่วนใหญ่ที่เกี่ยวข้อง มีลักษณะเหมือนผลคูณเมทริกซ์-เวกเตอร์ โดยที่เมทริกซ์เต็มไปด้วยตุ้มน้ำหนักที่ปรับได้ ซึ่งเป็นสิ่งที่แบบจำลองจะเรียนรู้จากข้อมูล",
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "translatedText": "เพื่อให้ชัดเจน ฉันกำลังสร้างตัวอย่างคำคุณศัพท์ที่อัปเดตคำนามเพียงเพื่อแสดงให้เห็นประเภทของพฤติกรรมที่คุณสามารถจินตนาการได้ว่าหัวความสนใจกำลังทำอยู่",
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "translatedText": "เช่นเดียวกับการเรียนรู้เชิงลึก พฤติกรรมที่แท้จริงนั้นแยกวิเคราะห์ได้ยากกว่ามาก เนื่องจากขึ้นอยู่กับการปรับแต่งและปรับแต่งพารามิเตอร์จำนวนมากเพื่อลดฟังก์ชันต้นทุนบางอย่าง",
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "translatedText": "แค่ว่าเมื่อเราก้าวผ่านเมทริกซ์ต่างๆ ทั้งหมดที่เต็มไปด้วยพารามิเตอร์ที่เกี่ยวข้องในกระบวนการนี้ ฉันคิดว่าการจินตนาการถึงตัวอย่างที่จินตนาการไว้ของบางสิ่งที่สามารถทำได้เพื่อช่วยให้ทุกอย่างเป็นรูปธรรมมากขึ้นนั้นมีประโยชน์มาก",
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "translatedText": "ในขั้นตอนแรกของกระบวนการนี้ คุณอาจจินตนาการถึงคำนามแต่ละคำ เช่น สิ่งมีชีวิต ที่ถามคำถามว่า เฮ้ มีคำคุณศัพท์อยู่ข้างหน้าฉันบ้างไหม?",
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "translatedText": "และสำหรับคำว่า ปุย และ สีน้ำเงิน แต่ละคนสามารถตอบได้ ใช่แล้ว ฉันเป็นคำคุณศัพท์ และฉันอยู่ในตำแหน่งนั้น",
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "translatedText": "คำถามนั้นถูกเข้ารหัสเป็นเวกเตอร์อีกตัวหนึ่ง ซึ่งเป็นรายการตัวเลขอีกชุดหนึ่ง ซึ่งเราเรียกว่าแบบสอบถามสำหรับคำนี้",
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "translatedText": "เวกเตอร์แบบสอบถามนี้มีขนาดเล็กกว่าเวกเตอร์ที่ฝังมาก เช่น 128",
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "translatedText": "การคำนวณแบบสอบถามนี้ดูเหมือนว่าจะใช้เมทริกซ์บางตัว ซึ่งฉันจะติดป้ายกำกับ wq และคูณด้วยการฝัง",
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "translatedText": "บีบอัดสิ่งต่างๆ สักหน่อย, ลองเขียนเวกเตอร์เคียวรีนั่นเป็น q แล้วทุกครั้งที่คุณเห็นผมใส่เมทริกซ์ไว้ข้างลูกศรแบบนี้, มันหมายถึงว่า การคูณเมทริกซ์นี้ด้วยเวกเตอร์ที่จุดเริ่มต้นของลูกศร จะได้เวกเตอร์ที่ ปลายลูกศร",
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "translatedText": "ในกรณีนี้ คุณจะคูณเมทริกซ์นี้ด้วยการฝังทั้งหมดในบริบท โดยสร้างเวกเตอร์คิวรีหนึ่งรายการสำหรับแต่ละโทเค็น",
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "translatedText": "รายการของเมทริกซ์นี้เป็นพารามิเตอร์ของแบบจำลอง ซึ่งหมายความว่าพฤติกรรมที่แท้จริงจะได้รับการเรียนรู้จากข้อมูล และในทางปฏิบัติ สิ่งที่เมทริกซ์นี้ทำในหัวความสนใจเฉพาะนั้นยากที่จะแยกวิเคราะห์",
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "translatedText": "แต่เพื่อประโยชน์ของเรา ลองนึกภาพตัวอย่างที่เราหวังว่าจะได้เรียนรู้ เราจะสมมติว่าเมทริกซ์คิวรีนี้แมปการฝังคำนามไปยังทิศทางที่แน่นอนในพื้นที่คิวรีขนาดเล็กนี้ที่เข้ารหัสแนวคิดในการมองหาคำคุณศัพท์ในตำแหน่งก่อนหน้า .",
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "translatedText": "การฝังอื่นๆ มีผลอย่างไร ใครจะรู้?",
  "input": "As to what it does to other embeddings, who knows?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "translatedText": "บางทีมันอาจจะพยายามบรรลุเป้าหมายอื่นไปพร้อมๆ กัน",
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "translatedText": "ตอนนี้ เรากำลังเน้นไปที่คำนามแบบเลเซอร์",
  "input": "Right now, we're laser focused on the nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "translatedText": "ในเวลาเดียวกัน เมทริกซ์ตัวที่สองที่เกี่ยวข้องกับสิ่งนี้คือเมทริกซ์ตัวที่สอง ซึ่งคุณคูณด้วยทุกๆ เมทริกซ์ที่ฝังไว้ด้วย",
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "translatedText": "สิ่งนี้จะสร้างลำดับที่สองของเวกเตอร์ที่เราเรียกว่าคีย์",
  "input": "This produces a second sequence of vectors that we call the keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "translatedText": "ตามแนวคิดแล้ว คุณต้องคิดว่าคีย์ต่างๆ อาจตอบคำถามได้",
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "translatedText": "เมทริกซ์หลักนี้ยังเต็มไปด้วยพารามิเตอร์ที่ปรับแต่งได้ และเช่นเดียวกับเมทริกซ์คิวรี มันจะแมปเวกเตอร์ที่ฝังเข้ากับพื้นที่มิติที่เล็กกว่าเดียวกันนั้น",
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "translatedText": "คุณคิดว่าคีย์ต่างๆ ตรงกับคิวรีเมื่อใดก็ตามที่คีย์เหล่านั้นสอดคล้องกันอย่างใกล้ชิด",
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "translatedText": "ในตัวอย่างของเรา คุณจะจินตนาการว่าคีย์เมทริกซ์จับคู่คำคุณศัพท์ เช่น ปุย และสีน้ำเงิน กับเวกเตอร์ที่สอดคล้องอย่างใกล้ชิดกับข้อความค้นหาที่สร้างโดยคำว่า Creature",
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "translatedText": "หากต้องการวัดว่าแต่ละคีย์ตรงกับแต่ละข้อความค้นหามากน้อยเพียงใด คุณจะต้องคำนวณ dot product ระหว่างคู่คีย์-ข้อความค้นหาที่เป็นไปได้แต่ละคู่",
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "translatedText": "ฉันชอบเห็นภาพตารางที่เต็มไปด้วยจุดจำนวนมาก โดยที่จุดที่ใหญ่กว่านั้นสอดคล้องกับผลคูณของดอทที่ใหญ่กว่า ซึ่งเป็นจุดที่คีย์และคิวรีเรียงกัน",
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "translatedText": "สำหรับตัวอย่างคำคุณศัพท์ของเรา นั่นจะมีลักษณะเช่นนี้อีกเล็กน้อย โดยที่หากคีย์ที่สร้างโดย fury และ blue สอดคล้องอย่างใกล้ชิดกับการสืบค้นที่สร้างโดย Creature ดังนั้น dot product ในสองจุดนี้จะเป็นจำนวนบวกจำนวนมาก",
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "translatedText": "ในศัพท์แสง ผู้คนที่เรียนรู้เกี่ยวกับเครื่องจักรจะพูดว่า นี่หมายความว่าการฝังของปุยและสีน้ำเงินจะเข้ามาเกี่ยวข้องกับการฝังของสิ่งมีชีวิต",
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "translatedText": "ในทางตรงกันข้ามกับดอทโปรดัคระหว่างคีย์ของคำอื่น เช่น the และคิวรีสำหรับสิ่งมีชีวิต จะเป็นค่าเล็กน้อยหรือค่าลบที่สะท้อนถึงความไม่เกี่ยวข้องกัน",
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "translatedText": "ดังนั้นเราจึงมีตารางค่าที่สามารถเป็นจำนวนจริงใดๆ ได้ตั้งแต่ลบอนันต์จนถึงอนันต์ ทำให้เราให้คะแนนว่าแต่ละคำมีความเกี่ยวข้องอย่างไรในการอัปเดตความหมายของคำอื่นๆ",
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "translatedText": "วิธีที่เรากำลังจะใช้คะแนนเหล่านี้คือการหาผลรวมถ่วงน้ำหนักที่แน่นอนในแต่ละคอลัมน์ โดยถ่วงน้ำหนักตามความเกี่ยวข้อง",
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "translatedText": "แทนที่จะมีค่าอยู่ในช่วงตั้งแต่ลบอนันต์ไปจนถึงอนันต์ สิ่งที่เราต้องการคือให้ตัวเลขในคอลัมน์เหล่านี้อยู่ระหว่าง 0 ถึง 1 และให้แต่ละคอลัมน์รวมกันได้ 1 ราวกับว่าพวกมันเป็นการแจกแจงความน่าจะเป็น",
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "translatedText": "ถ้าคุณมาจากบทที่แล้ว คุณคงรู้ว่าเราต้องทำอะไร",
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "translatedText": "เราคำนวณซอฟต์แม็กซ์ตามแต่ละคอลัมน์เหล่านี้เพื่อทำให้ค่าเป็นมาตรฐาน",
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "translatedText": "ในภาพของเรา หลังจากที่คุณใช้ softmax กับคอลัมน์ทั้งหมดแล้ว เราจะเติมค่าที่ทำให้เป็นมาตรฐานลงในตาราง",
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "translatedText": "ณ จุดนี้ คุณมั่นใจได้เลยว่าแต่ละคอลัมน์จะต้องให้น้ำหนักตามความเกี่ยวข้องของคำทางซ้ายกับค่าที่ตรงกันด้านบน",
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "translatedText": "เราเรียกตารางนี้ว่ารูปแบบความสนใจ",
  "input": "We call this grid an attention pattern.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "translatedText": "ทีนี้ ถ้าคุณดูที่กระดาษหม้อแปลงต้นฉบับ มันมีวิธีที่กะทัดรัดมากในการเขียนทั้งหมดนี้",
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "translatedText": "ในที่นี้ ตัวแปร q และ k แสดงถึงอาร์เรย์แบบเต็มของเวกเตอร์คิวรีและคีย์ตามลำดับ เวกเตอร์เล็กๆ เหล่านั้นที่คุณได้รับจากการคูณการฝังด้วยคิวรีและเมทริกซ์ของคีย์",
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "translatedText": "การแสดงออกในตัวเศษนี้เป็นวิธีที่กะทัดรัดมากในการแสดงตารางของผลิตภัณฑ์ดอทที่เป็นไปได้ทั้งหมดระหว่างคู่ของคีย์และข้อความค้นหา",
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "translatedText": "รายละเอียดทางเทคนิคเล็กๆ น้อยๆ ที่ฉันไม่ได้พูดถึงก็คือ เพื่อความเสถียรของตัวเลข การหารค่าเหล่านี้ทั้งหมดด้วยรากที่สองของมิติในพื้นที่คิวรีคีย์นั้นจะเป็นประโยชน์",
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "translatedText": "จากนั้นซอฟต์แม็กซ์ที่พันรอบนิพจน์แบบเต็มนี้ควรเข้าใจว่าจะใช้ทีละคอลัมน์",
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "translatedText": "สำหรับเทอม v นั้น เราจะพูดถึงมันในอีกสักครู่",
  "input": "As to that v term, we'll talk about it in just a second.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "translatedText": "ก่อนหน้านั้น มีรายละเอียดทางเทคนิคอีกอย่างหนึ่งที่ฉันข้ามไป",
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "translatedText": "ในระหว่างกระบวนการฝึกอบรม เมื่อคุณรันโมเดลนี้ตามตัวอย่างข้อความที่กำหนด และน้ำหนักทั้งหมดจะถูกปรับเล็กน้อยและปรับแต่งเพื่อให้รางวัลหรือลงโทษโดยพิจารณาจากความน่าจะเป็นที่จะกำหนดให้กับคำถัดไปที่แท้จริงในเนื้อเรื่อง จะทำให้กระบวนการฝึกอบรมทั้งหมดมีประสิทธิภาพมากขึ้นหากคุณคาดการณ์โทเค็นถัดไปที่เป็นไปได้ไปพร้อม ๆ กันตามลำดับเริ่มต้นของโทเค็นในข้อนี้",
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "translatedText": "ตัวอย่างเช่น ด้วยวลีที่เรามุ่งเน้น มันอาจจะเป็นการคาดเดาด้วยว่าคำใดตามหลังสิ่งมีชีวิต และคำใดตามหลัง",
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "translatedText": "นี่เป็นสิ่งที่ดีจริงๆ เพราะมันหมายความว่าตัวอย่างการฝึกอบรมเดี่ยวๆ จะทำหน้าที่หลายอย่างได้อย่างมีประสิทธิภาพ",
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "translatedText": "สำหรับวัตถุประสงค์ของรูปแบบความสนใจของเรา หมายความว่าคุณไม่ควรปล่อยให้คำที่ตามมามีอิทธิพลเหนือคำที่กล่าวมาก่อนหน้านี้ เพราะไม่เช่นนั้นคำเหล่านั้นก็สามารถให้คำตอบสำหรับสิ่งที่จะเกิดขึ้นต่อไปได้",
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "translatedText": "ความหมายก็คือเราต้องการให้จุดเหล่านี้ทั้งหมดที่นี่ ซึ่งเป็นตัวแทนของโทเค็นที่ตามมาซึ่งมีอิทธิพลต่อโทเค็นก่อนหน้านี้ ถูกบังคับให้เป็นศูนย์",
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "translatedText": "สิ่งที่ง่ายที่สุดที่คุณอาจคิดจะทำคือตั้งค่าให้เท่ากับศูนย์ แต่ถ้าคุณทำอย่างนั้นคอลัมน์จะไม่รวมกันเป็นหนึ่งอีกต่อไป คอลัมน์เหล่านั้นจะไม่ถูกทำให้เป็นมาตรฐาน",
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "translatedText": "วิธีทั่วไปในการทำเช่นนี้คือ ก่อนใช้ซอฟต์แม็กซ์ คุณต้องตั้งค่าค่าทั้งหมดให้เป็นลบอนันต์",
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "translatedText": "หากคุณทำเช่นนั้น หลังจากใช้ softmax คอลัมน์ทั้งหมดจะกลายเป็นศูนย์ แต่คอลัมน์จะยังคงเป็นมาตรฐาน",
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "translatedText": "กระบวนการนี้เรียกว่าการมาสก์",
  "input": "This process is called masking.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "translatedText": "มีความสนใจหลายรูปแบบที่คุณไม่ได้ใช้ แต่ในตัวอย่าง GPT ของเรา แม้ว่าสิ่งนี้จะมีความเกี่ยวข้องมากกว่าในระหว่างขั้นตอนการฝึกอบรมมากกว่าที่ควรจะเป็น เช่น การเรียกใช้เป็นแชทบอทหรืออะไรทำนองนั้น แต่คุณมักจะนำไปใช้เสมอ การปิดบังนี้เพื่อป้องกันไม่ให้โทเค็นในภายหลังมีอิทธิพลต่อโทเค็นก่อนหน้า",
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "translatedText": "ข้อเท็จจริงอีกประการหนึ่งที่ควรค่าแก่การพิจารณาเกี่ยวกับรูปแบบความสนใจนี้คือขนาดของรูปแบบนี้เท่ากับกำลังสองของขนาดบริบท",
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "translatedText": "นี่คือสาเหตุที่ขนาดบริบทอาจเป็นปัญหาคอขวดอย่างมากสำหรับโมเดลภาษาขนาดใหญ่ และการขยายขนาดก็ไม่ใช่เรื่องเล็กน้อย",
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "translatedText": "ตามที่คุณจินตนาการ โดยได้รับแรงบันดาลใจจากความปรารถนาที่จะมีกรอบเวลาบริบทที่ใหญ่ขึ้นเรื่อยๆ ในช่วงไม่กี่ปีที่ผ่านมาได้เห็นการเปลี่ยนแปลงบางอย่างในกลไกความสนใจที่มีจุดมุ่งหมายเพื่อทำให้บริบทสามารถปรับขนาดได้มากขึ้น แต่ที่นี่ คุณและฉันยังคงมุ่งเน้นไปที่พื้นฐาน",
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "translatedText": "โอเค เยี่ยมเลย การคำนวณรูปแบบนี้จะทำให้แบบจำลองสามารถอนุมานได้ว่าคำไหนเกี่ยวข้องกับคำอื่นบ้าง",
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "translatedText": "ตอนนี้ คุณต้องอัปเดตการฝังจริง โดยอนุญาตให้คำต่างๆ ส่งข้อมูลไปยังคำอื่นๆ ที่เกี่ยวข้องได้",
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "translatedText": "ตัวอย่างเช่น คุณต้องการให้การฝัง Fluffy ทำให้เกิดการเปลี่ยนแปลงกับ Creature โดยจะย้ายมันไปยังส่วนอื่นของพื้นที่การฝัง 12,000 มิติที่เข้ารหัสสิ่งมีชีวิต Fluffy โดยเฉพาะ",
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "translatedText": "สิ่งที่ฉันจะทำที่นี่คือขั้นแรกแสดงให้คุณเห็นวิธีที่ตรงไปตรงมาที่สุดที่คุณสามารถทำสิ่งนี้ได้ แม้ว่าจะมีวิธีเล็กน้อยที่สิ่งนี้จะได้รับการแก้ไข ในบริบทของความสนใจแบบหลายหัว",
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "translatedText": "วิธีที่ตรงไปตรงมาที่สุดคือการใช้เมทริกซ์ตัวที่สาม ซึ่งเราเรียกว่าเมทริกซ์ค่า ซึ่งคุณคูณด้วยการฝังคำแรกนั้น เช่น Fluffy",
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "translatedText": "ผลลัพธ์ของสิ่งนี้คือสิ่งที่คุณจะเรียกว่าเวกเตอร์ค่า และนี่คือสิ่งที่คุณเพิ่มลงในการฝังคำที่สอง ในกรณีนี้คือสิ่งที่คุณเพิ่มลงในการฝังของ Creature",
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "translatedText": "ดังนั้นเวกเตอร์ค่านี้จึงอยู่ในสเปซมิติที่สูงมากเหมือนกับการฝัง",
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "translatedText": "เมื่อคุณคูณเมทริกซ์ค่านี้ด้วยการฝังคำ คุณอาจคิดว่ามันเป็นคำพูดว่า ถ้าคำนี้เกี่ยวข้องกับการปรับความหมายของสิ่งอื่น สิ่งที่ควรเพิ่มเข้าไปในการฝังสิ่งอื่นนั้นเพื่อที่จะสะท้อนให้เห็น นี้?",
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "translatedText": "มองย้อนกลับไปในไดอะแกรมของเรา ลองแยกคีย์และการสืบค้นทั้งหมดออกไป เนื่องจากหลังจากที่คุณคำนวณรูปแบบความสนใจที่คุณทำกับสิ่งเหล่านั้นแล้ว คุณจะต้องนำเมทริกซ์ค่านี้มาคูณด้วยทุกๆ การฝังเหล่านั้น เพื่อสร้างลำดับของเวกเตอร์ค่า",
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "translatedText": "คุณอาจคิดว่าเวกเตอร์ค่าพวกนี้สัมพันธ์กับคีย์ที่เกี่ยวข้องกัน",
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "translatedText": "สำหรับแต่ละคอลัมน์ในแผนภาพนี้ คุณจะคูณเวกเตอร์ค่าแต่ละตัวด้วยน้ำหนักที่สอดคล้องกันในคอลัมน์นั้น",
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "translatedText": "ตัวอย่างเช่น ที่นี่ ภายใต้การฝัง Creature คุณจะเพิ่มสัดส่วนขนาดใหญ่ของเวกเตอร์ค่าสำหรับ Fluffy และ Blue ในขณะที่เวกเตอร์ค่าอื่นๆ ทั้งหมดมีค่าเป็นศูนย์ หรืออย่างน้อยก็เกือบเป็นศูนย์",
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "translatedText": "และสุดท้าย วิธีอัปเดตการฝังที่เกี่ยวข้องกับคอลัมน์นี้ โดยก่อนหน้านี้เข้ารหัสความหมายแบบไม่มีบริบทของ Creature คุณเพิ่มค่าที่ปรับขนาดใหม่เหล่านี้ทั้งหมดในคอลัมน์ ทำให้เกิดการเปลี่ยนแปลงที่คุณต้องการเพิ่ม ซึ่งฉัน จะติดป้ายกำกับ delta-e จากนั้นคุณเพิ่มสิ่งนั้นลงในการฝังต้นฉบับ",
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "translatedText": "หวังว่าผลลัพธ์ที่ได้จะเป็นเวกเตอร์ที่ละเอียดยิ่งขึ้นในการเข้ารหัสความหมายที่มีบริบทมากขึ้น เหมือนกับสิ่งมีชีวิตสีฟ้าขนปุย",
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "translatedText": "และแน่นอน คุณไม่เพียงแค่ทำเช่นนี้กับการฝังเพียงครั้งเดียว แต่คุณใช้ผลรวมถ่วงน้ำหนักเท่ากันกับคอลัมน์ทั้งหมดในรูปภาพนี้ สร้างลำดับของการเปลี่ยนแปลง เพิ่มการเปลี่ยนแปลงทั้งหมดเหล่านั้นลงในการฝังที่สอดคล้องกัน ทำให้เกิดลำดับที่สมบูรณ์ของ การฝังที่ละเอียดยิ่งขึ้นโผล่ออกมาจากบล็อกความสนใจ",
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "translatedText": "เมื่อขยายออก กระบวนการทั้งหมดนี้คือสิ่งที่คุณจะเรียกว่าเป็นความสนใจหัวเดียว",
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "translatedText": "ดังที่ฉันได้อธิบายไปแล้ว กระบวนการนี้ถูกกำหนดพารามิเตอร์ด้วยเมทริกซ์ที่แตกต่างกันสามตัว ซึ่งทั้งหมดเต็มไปด้วยพารามิเตอร์ที่ปรับแต่งได้ คีย์ การสืบค้น และค่า",
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "translatedText": "ฉันต้องการใช้เวลาสักครู่เพื่อดำเนินการต่อในสิ่งที่เราเริ่มต้นในบทที่แล้ว โดยมีการเก็บคะแนนโดยที่เรานับจำนวนพารามิเตอร์โมเดลทั้งหมดโดยใช้ตัวเลขจาก GPT-3",
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "translatedText": "เมทริกซ์คีย์และคิวรีเหล่านี้แต่ละเมทริกซ์มี 12,288 คอลัมน์ ซึ่งตรงกับมิติข้อมูลการฝัง และ 128 แถว ซึ่งตรงกับมิติของพื้นที่การสืบค้นคีย์ที่มีขนาดเล็กกว่านั้น",
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "translatedText": "นี่ทำให้เรามีพารามิเตอร์เพิ่มเติมประมาณ 1.5 ล้านพารามิเตอร์สำหรับแต่ละรายการ",
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "translatedText": "หากคุณดูเมทริกซ์ค่านั้นในทางตรงข้าม วิธีที่ฉันเคยอธิบายไปแล้วน่าจะแนะนำว่าเป็นเมทริกซ์จัตุรัสที่มี 12,288 คอลัมน์และ 12,288 แถว เนื่องจากทั้งอินพุตและเอาต์พุตอาศัยอยู่ในพื้นที่ฝังขนาดใหญ่มากนี้",
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "translatedText": "หากเป็นจริง นั่นหมายถึงต้องมีพารามิเตอร์เพิ่มประมาณ 150 ล้านพารามิเตอร์",
  "input": "If true, that would mean about 150 million added parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "translatedText": "และเพื่อความชัดเจน คุณสามารถทำเช่นนั้นได้",
  "input": "And to be clear, you could do that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "translatedText": "คุณสามารถจัดสรรพารามิเตอร์ที่มีขนาดมากขึ้นให้กับแมปค่ามากกว่าคีย์และคิวรี",
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "translatedText": "แต่ในทางปฏิบัติ จะมีประสิทธิภาพมากกว่ามากหากคุณสร้างเพื่อให้จำนวนพารามิเตอร์ที่ทุ่มเทให้กับการแมปค่านี้เท่ากับจำนวนที่ทุ่มเทให้กับคีย์และแบบสอบถาม",
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "translatedText": "สิ่งนี้มีความเกี่ยวข้องอย่างยิ่งในการตั้งค่าการดำเนินการให้ความสนใจหลายรายการพร้อมกัน",
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "translatedText": "ลักษณะที่ปรากฏคือแผนผังค่าจะถูกแยกตัวประกอบเป็นผลคูณของเมทริกซ์ขนาดเล็กสองตัว",
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "translatedText": "ตามแนวคิดแล้ว ฉันยังคงแนะนำให้คุณคิดถึงแผนที่เชิงเส้นโดยรวม ซึ่งมีอินพุตและเอาท์พุต ทั้งสองอย่างในพื้นที่ฝังขนาดใหญ่นี้ เช่น นำสีน้ำเงินฝังมาในทิศทางสีน้ำเงินที่คุณจะเพิ่มให้กับคำนาม",
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "translatedText": "เพียงแต่ว่ามีจำนวนแถวน้อยกว่า ซึ่งโดยทั่วไปจะมีขนาดเท่ากับพื้นที่คิวรีคีย์",
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "translatedText": "ความหมายคือ คุณสามารถคิดว่ามันเป็นการแมปเวกเตอร์ที่ฝังขนาดใหญ่ลงไปยังพื้นที่ที่เล็กกว่ามาก",
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "translatedText": "นี่ไม่ใช่การตั้งชื่อทั่วไป แต่ผมจะเรียกมันว่าเมทริกซ์ค่าล่าง",
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "translatedText": "เมทริกซ์ที่สองแมปจากพื้นที่ขนาดเล็กนี้สำรองไปยังพื้นที่ฝัง สร้างเวกเตอร์ที่คุณใช้ในการอัปเดตจริง",
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "translatedText": "ผมจะเรียกอันนี้ว่าเมทริกซ์เพิ่มค่า ซึ่งไม่ธรรมดาอีกแล้ว",
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "translatedText": "วิธีที่คุณเห็นสิ่งนี้เขียนในเอกสารส่วนใหญ่ดูแตกต่างออกไปเล็กน้อย",
  "input": "The way that you would see this written in most papers looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "translatedText": "ฉันจะพูดถึงมันในอีกสักครู่",
  "input": "I'll talk about it in a minute.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "translatedText": "ในความคิดของฉัน มันมีแนวโน้มที่จะทำให้สิ่งต่าง ๆ เกิดความสับสนทางแนวคิดมากขึ้นเล็กน้อย",
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "translatedText": "หากต้องการใช้ศัพท์เฉพาะพีชคณิตเชิงเส้นตรงนี้ สิ่งที่เรากำลังทำคือจำกัดแผนผังค่าโดยรวมให้เป็นการแปลงระดับต่ำ",
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "translatedText": "เมื่อย้อนกลับไปที่การนับพารามิเตอร์ เมทริกซ์ทั้งสี่นี้มีขนาดเท่ากัน และเมื่อบวกทั้งหมดเข้าด้วยกัน เราจะได้พารามิเตอร์ประมาณ 6.3 ล้านพารามิเตอร์สำหรับหัวความสนใจเดียว",
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "translatedText": "เพื่อให้แม่นยำยิ่งขึ้นอีกหน่อย ทุกอย่างที่อธิบายไว้จนถึงขณะนี้คือสิ่งที่ผู้คนเรียกว่าการใส่ใจในตนเอง เพื่อแยกความแตกต่างจากรูปแบบอื่น ๆ ที่เรียกว่าการสนใจตนเอง",
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "translatedText": "สิ่งนี้ไม่เกี่ยวข้องกับตัวอย่าง GPT ของเรา แต่หากคุณสงสัย ความสนใจข้ามจะเกี่ยวข้องกับโมเดลที่ประมวลผลข้อมูลที่แตกต่างกันสองประเภท เช่น ข้อความในภาษาหนึ่งและข้อความในภาษาอื่นที่เป็นส่วนหนึ่งของการแปลรุ่นที่กำลังดำเนินอยู่ หรืออาจเป็นอินพุตเสียงของคำพูดและการถอดเสียงที่กำลังดำเนินอยู่",
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "translatedText": "หัวแบบ cross-attention มีลักษณะเกือบจะเหมือนกัน",
  "input": "A cross-attention head looks almost identical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "translatedText": "ข้อแตกต่างเพียงอย่างเดียวคือแมปคีย์และคิวรีทำงานกับชุดข้อมูลที่ต่างกัน",
  "input": "The only difference is that the key and query maps act on different data sets.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "translatedText": "ในแบบจำลองที่ทำการแปล คีย์อาจมาจากภาษาหนึ่ง ในขณะที่ข้อความค้นหามาจากอีกภาษาหนึ่ง และรูปแบบความสนใจสามารถอธิบายว่าคำใดจากภาษาหนึ่งตรงกับคำใดในอีกภาษาหนึ่ง",
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "translatedText": "และในการตั้งค่านี้ โดยทั่วไปจะไม่มีการมาสก์ เนื่องจากไม่มีแนวคิดใด ๆ เกี่ยวกับโทเค็นรุ่นหลังที่ส่งผลกระทบต่อโทเค็นรุ่นก่อน ๆ",
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "translatedText": "แต่หากคุณเข้าใจทุกอย่างจนถึงตอนนี้ และหากคุณหยุดอยู่แค่นี้ คุณจะค้นพบแก่นแท้ของความสนใจที่แท้จริง",
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "translatedText": "สิ่งที่เหลืออยู่สำหรับเราก็คือการวางความรู้สึกที่คุณทำสิ่งนี้หลายๆ ครั้ง",
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "translatedText": "ในตัวอย่างหลักของเรา เรามุ่งเน้นไปที่คำคุณศัพท์ที่อัปเดตคำนาม แต่แน่นอนว่ามีวิธีต่างๆ มากมายที่บริบทสามารถมีอิทธิพลต่อความหมายของคำได้",
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "translatedText": "ถ้าคำว่าชนกับคำว่ารถที่นำหน้าก็มีผลกระทบต่อรูปทรงและโครงสร้างของรถคันนั้นด้วย",
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "translatedText": "และการเชื่อมโยงหลายรายการอาจมีหลักไวยากรณ์น้อยกว่า",
  "input": "And a lot of associations might be less grammatical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "translatedText": "หากคำว่าพ่อมดอยู่ในข้อความเดียวกับแฮร์รี่ ก็บ่งบอกว่าคำนี้อาจหมายถึงแฮร์รี่ พอตเตอร์ แต่หากคำว่าราชินี ซัสเซ็กซ์ และวิลเลียมอยู่ในข้อความนั้นแทน บางทีการฝังแฮร์รี่ก็ควรได้รับการอัปเดตแทน เพื่ออ้างถึงเจ้าชาย",
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "translatedText": "สำหรับการอัปเดตตามบริบททุกประเภทที่คุณอาจจินตนาการได้ พารามิเตอร์ของคีย์และเมทริกซ์คิวรีเหล่านี้จะแตกต่างกันเพื่อจับรูปแบบความสนใจที่แตกต่างกัน และพารามิเตอร์ของแผนผังคุณค่าของเราจะแตกต่างกันขึ้นอยู่กับสิ่งที่ควรเพิ่มลงในการฝัง",
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "translatedText": "และอีกครั้ง ในทางปฏิบัติพฤติกรรมที่แท้จริงของแผนที่เหล่านี้ตีความได้ยากกว่ามาก โดยมีการกำหนดน้ำหนักให้ทำทุกอย่างที่แบบจำลองต้องการให้ทำเพื่อให้บรรลุเป้าหมายในการทำนายโทเค็นถัดไปได้ดีที่สุด",
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "translatedText": "ดังที่ผมได้กล่าวไปแล้ว ทุกสิ่งที่เราอธิบายไว้นั้นเป็นความสนใจเพียงหัวเดียว และบล็อกความสนใจเต็มรูปแบบภายในหม้อแปลงไฟฟ้าประกอบด้วยสิ่งที่เรียกว่าความสนใจแบบหลายหัว โดยที่คุณดำเนินการต่างๆ มากมายเหล่านี้ไปพร้อมๆ กัน โดยแต่ละรายการมีการสืบค้นคีย์ที่แตกต่างกันออกไป และแผนที่คุณค่า",
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "translatedText": "ตัวอย่างเช่น GPT-3 ใช้หัวความสนใจ 96 หัวในแต่ละบล็อก",
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "translatedText": "เมื่อพิจารณาว่าแต่ละรายการมีความสับสนเล็กน้อยอยู่แล้ว จึงมีเรื่องให้คิดมากมายในหัวของคุณ",
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "translatedText": "เพียงเพื่อสะกดให้ชัดเจน นั่นหมายความว่าคุณมีเมทริกซ์คีย์และคิวรีที่แตกต่างกัน 96 รายการ ซึ่งสร้างรูปแบบความสนใจที่แตกต่างกัน 96 รูปแบบ",
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "translatedText": "จากนั้นแต่ละหัวจะมีเมทริกซ์ค่าที่แตกต่างกันซึ่งใช้ในการสร้างลำดับเวกเตอร์ค่า 96 ลำดับ",
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "translatedText": "ทั้งหมดนี้นำมารวมกันโดยใช้รูปแบบความสนใจที่สอดคล้องกันเป็นน้ำหนัก",
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "translatedText": "ความหมายก็คือ สำหรับแต่ละตำแหน่งในบริบท แต่ละโทเค็น แต่ละส่วนหัวเหล่านี้จะสร้างการเปลี่ยนแปลงที่เสนอเพื่อเพิ่มลงในการฝังในตำแหน่งนั้น",
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "translatedText": "ดังนั้นสิ่งที่คุณทำคือรวมการเปลี่ยนแปลงที่เสนอทั้งหมดเข้าด้วยกัน หนึ่งรายการสำหรับแต่ละหัว และเพิ่มผลลัพธ์ลงในตำแหน่งเดิมที่ฝังไว้",
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "translatedText": "ผลรวมทั้งหมดนี้จะเป็นส่วนหนึ่งจากสิ่งที่ส่งออกมาจากบล็อกความสนใจแบบหลายหัว ซึ่งเป็นชิ้นเดียวของการฝังแบบละเอียดที่โผล่ออกมาที่ปลายอีกด้านของมัน",
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "translatedText": "ขอย้ำอีกครั้งว่านี่เป็นเรื่องที่ต้องคิดมาก ดังนั้นอย่ากังวลเลยหากต้องใช้เวลาพอสมควร",
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "translatedText": "แนวคิดโดยรวมก็คือ การใช้หัวที่แตกต่างกันหลายๆ หัวพร้อมกัน จะทำให้โมเดลมีความสามารถในการเรียนรู้วิธีที่แตกต่างกันมากมายที่ทำให้บริบทเปลี่ยนความหมาย",
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "translatedText": "เมื่อดึงการนับพารามิเตอร์ของเราขึ้นมาด้วย 96 หัว ซึ่งแต่ละชุดรวมเมทริกซ์ทั้ง 4 แบบที่แตกต่างกันออกไป แต่ละบล็อกของความสนใจแบบหลายหัวจะจบลงด้วยพารามิเตอร์ประมาณ 600 ล้านพารามิเตอร์",
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "translatedText": "มีสิ่งหนึ่งที่น่ารำคาญเพิ่มเติมเล็กน้อยที่ฉันควรพูดถึงสำหรับทุกคนที่อ่านเพิ่มเติมเกี่ยวกับหม้อแปลงไฟฟ้า",
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "translatedText": "คุณจำได้ไหมที่ฉันบอกว่าแผนผังค่าถูกแยกออกเป็นเมทริกซ์ที่แตกต่างกันสองตัวนี้ ซึ่งฉันเรียกว่าเมทริกซ์ค่าลงและค่าเมทริกซ์เพิ่ม",
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "translatedText": "วิธีที่ฉันวางกรอบสิ่งต่างๆ อาจแนะนำให้คุณเห็นเมทริกซ์คู่นี้ในหัวความสนใจแต่ละหัว และคุณก็สามารถนำไปใช้ในลักษณะนี้ได้",
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "translatedText": "นั่นจะเป็นการออกแบบที่ถูกต้อง",
  "input": "That would be a valid design.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "translatedText": "แต่วิธีที่คุณเห็นสิ่งนี้เขียนในเอกสาร และวิธีการนำไปใช้ในทางปฏิบัติ ดูแตกต่างออกไปเล็กน้อย",
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "translatedText": "เมทริกซ์การเพิ่มมูลค่าทั้งหมดนี้สำหรับแต่ละหัวจะปรากฏถูกเย็บเข้าด้วยกันในเมทริกซ์ขนาดยักษ์ตัวเดียวที่เราเรียกว่าเมทริกซ์เอาท์พุต ซึ่งเชื่อมโยงกับบล็อกความสนใจแบบหลายหัวทั้งหมด",
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "translatedText": "และเมื่อคุณเห็นคนอ้างถึงเมทริกซ์ค่าสำหรับหัวความสนใจที่กำหนด, โดยทั่วไปแล้ว พวกเขาจะหมายถึงขั้นตอนแรกนี้เท่านั้น, ขั้นตอนที่ผมเรียกว่าเป็นค่าที่ลดลงลงในพื้นที่ขนาดเล็ก",
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "translatedText": "สำหรับผู้ที่อยากรู้อยากเห็น ฉันได้ฝากข้อความไว้บนหน้าจอไว้",
  "input": "For the curious among you, I've left an on-screen note about it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "translatedText": "เป็นหนึ่งในรายละเอียดที่เสี่ยงต่อการหันเหความสนใจไปจากประเด็นหลักทางแนวคิด แต่ฉันอยากจะแจ้งให้ทราบเพียงเพื่อให้คุณทราบว่าคุณได้อ่านเกี่ยวกับเรื่องนี้จากแหล่งอื่นหรือไม่",
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "translatedText": "นอกเหนือจากความแตกต่างทางเทคนิคทั้งหมดแล้ว ในการดูตัวอย่างจากบทที่แล้ว เราเห็นว่าข้อมูลที่ไหลผ่านหม้อแปลงไม่เพียงแค่ไหลผ่านบล็อกความสนใจเดียว",
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "translatedText": "ประการหนึ่ง มันยังต้องผ่านการดำเนินการอื่นๆ ที่เรียกว่าการรับรู้แบบหลายชั้นด้วย",
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "translatedText": "เราจะพูดถึงสิ่งเหล่านั้นเพิ่มเติมในบทถัดไป",
  "input": "We'll talk more about those in the next chapter.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "translatedText": "จากนั้นมันก็ผ่านการดำเนินการทั้งสองนี้ซ้ำไปซ้ำมาหลายชุด",
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "translatedText": "ความหมายก็คือ หลังจากที่คำใดคำหนึ่งซึมซับบริบทบางส่วนไปแล้ว ก็มีโอกาสอีกมากมายที่การฝังที่ละเอียดยิ่งขึ้นนี้จะได้รับอิทธิพลจากสภาพแวดล้อมที่เหมาะสมยิ่งขึ้น",
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "translatedText": "ยิ่งคุณไปไกลจากเครือข่ายเท่าไร การฝังแต่ละครั้งจะมีความหมายมากขึ้นเรื่อยๆ จากการฝังอื่นๆ ทั้งหมด ซึ่งตัวมันเองกำลังได้รับการปรับปรุงให้เหมาะสมมากขึ้นเรื่อยๆ ความหวังก็คือว่าจะมีความสามารถในการเข้ารหัสระดับที่สูงขึ้น และแนวคิดที่เป็นนามธรรมมากขึ้นเกี่ยวกับสิ่งที่กำหนด ข้อมูลเข้าที่นอกเหนือไปจากคำอธิบายและโครงสร้างไวยากรณ์",
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "translatedText": "สิ่งต่างๆ เช่น ความรู้สึกและน้ำเสียง ไม่ว่าจะเป็นบทกวี และความจริงทางวิทยาศาสตร์ที่ซ่อนอยู่เกี่ยวข้องกับงานชิ้นนี้และอะไรทำนองนั้น",
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "translatedText": "ย้อนกลับไปอีกครั้งที่การเก็บคะแนนของเรา GPT-3 มีเลเยอร์ที่แตกต่างกัน 96 เลเยอร์ ดังนั้นจำนวนรวมของการสืบค้นคีย์และพารามิเตอร์ค่าจึงคูณด้วยอีก 96 ซึ่งทำให้ผลรวมทั้งหมดเหลือน้อยกว่า 58 พันล้านพารามิเตอร์ที่ต่างกันซึ่งใช้สำหรับพารามิเตอร์ทั้งหมด หัวความสนใจ",
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "translatedText": "นั่นเป็นสิ่งที่ต้องแน่ใจมาก แต่มีเพียงประมาณหนึ่งในสามของ 175 พันล้านที่อยู่ในเครือข่ายทั้งหมด",
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "translatedText": "ดังนั้นแม้ว่าความสนใจจะได้รับความสนใจทั้งหมด แต่พารามิเตอร์ส่วนใหญ่มาจากบล็อกที่อยู่ระหว่างขั้นตอนเหล่านี้",
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "translatedText": "ในบทถัดไป คุณและฉันจะพูดคุยเพิ่มเติมเกี่ยวกับช่วงอื่นๆ เหล่านั้นและกระบวนการฝึกอบรมอีกมากมาย",
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "translatedText": "ส่วนสำคัญของเรื่องราวความสำเร็จของกลไกความสนใจนั้นไม่ใช่พฤติกรรมเฉพาะเจาะจงใดๆ มากนัก แต่เป็นความจริงที่ว่ากลไกนี้สามารถขนานกันได้อย่างมาก ซึ่งหมายความว่าคุณสามารถเรียกใช้การคำนวณจำนวนมากได้ในเวลาอันสั้นโดยใช้ GPU .",
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "translatedText": "เนื่องจากหนึ่งในบทเรียนสำคัญเกี่ยวกับการเรียนรู้เชิงลึกในช่วงทศวรรษหรือสองปีที่ผ่านมาคือขนาดเพียงอย่างเดียวที่ดูเหมือนว่าจะให้การปรับปรุงเชิงคุณภาพอย่างมากในประสิทธิภาพของโมเดล มีข้อได้เปรียบอย่างมากสำหรับสถาปัตยกรรมแบบขนานที่ช่วยให้คุณทำเช่นนี้ได้",
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "translatedText": "หากคุณต้องการเรียนรู้เพิ่มเติมเกี่ยวกับสิ่งนี้ ฉันได้ทิ้งลิงก์จำนวนมากไว้ในคำอธิบาย",
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "translatedText": "โดยเฉพาะอย่างยิ่ง อะไรก็ตามที่ผลิตโดย Andrej Karpathy หรือ Chris Ola มักจะเป็นทองคำบริสุทธิ์",
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "translatedText": "ในวิดีโอนี้ ฉันอยากจะกระโจนเข้าสู่ความสนใจในรูปแบบปัจจุบัน แต่ถ้าคุณอยากรู้เพิ่มเติมเกี่ยวกับประวัติความเป็นมาว่าเรามาถึงที่นี่ได้อย่างไร และคุณจะสร้างสรรค์แนวคิดนี้ขึ้นมาใหม่ให้กับตัวคุณเองได้อย่างไร วิเวก เพื่อนของฉันก็แค่เสนอสองสามข้อ วิดีโอที่ให้แรงจูงใจมากขึ้น",
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "translatedText": "นอกจากนี้ Britt Cruz จากช่อง The Art of the Problem ยังมีวิดีโอที่ดีมากเกี่ยวกับประวัติความเป็นมาของโมเดลภาษาขนาดใหญ่",
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "translatedText": "ขอบคุณ",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]