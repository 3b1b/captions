1
00:00:00,000 --> 00:00:04,019
ในบทที่แล้ว คุณและผมได้เริ่มศึกษาการทำงานภายในของ transformer

2
00:00:04,560 --> 00:00:07,669
นี่คือหนึ่งในเทคโนโลยีหลักที่อยู่ภายในแบบจำลองภาษาขนาดใหญ่ 

3
00:00:07,669 --> 00:00:10,200
และเครื่องมืออื่น ๆ อีกมากมายในยุคปัจจุบันของ AI

4
00:00:10,980 --> 00:00:14,673
มันเริ่มเป็นที่รู้จักครั้งแรกในงานวิจัยชื่อดังเมื่อปี 2017 ที่มีชื่อว่า 

5
00:00:14,673 --> 00:00:18,006
Attention is All You Need และในบทนี้ คุณและผมจะขุดลึกลงไปว่ากลไก 

6
00:00:18,006 --> 00:00:21,700
attention นี้คืออะไร พร้อมทั้งนำเสนอภาพประกอบว่ามันประมวลผลข้อมูลอย่างไร

7
00:00:26,140 --> 00:00:29,540
สรุปสั้น ๆ นี่คือบริบทสำคัญที่ผมอยากให้คุณนึกถึง

8
00:00:30,000 --> 00:00:36,060
เป้าหมายของแบบจำลองที่คุณและผมกำลังศึกษาอยู่คือ การรับข้อความและทำนายว่าคำถัดไปคืออะไร

9
00:00:36,860 --> 00:00:40,881
ข้อความที่ป้อนถูกแบ่งออกเป็นชิ้นเล็กๆ ที่เราเรียกว่า token 

10
00:00:40,881 --> 00:00:47,015
ซึ่งมักจะเป็นคำหรือส่วนของคำ แต่เพื่อให้ตัวอย่างในวิดีโอนี้ง่ายขึ้นสำหรับคุณและผมที่จะคิด 

11
00:00:47,015 --> 00:00:50,560
ลองมาทำให้มันง่ายขึ้นโดยสมมติว่า token เป็นแค่คำเสมอ

12
00:00:51,480 --> 00:00:54,137
ขั้นตอนแรกใน transformer คือการเชื่อมโยง token 

13
00:00:54,137 --> 00:00:57,700
แต่ละตัวเข้ากับเวกเตอร์มิติสูง ซึ่งเราเรียกว่า embedding ของมัน

14
00:00:57,700 --> 00:01:02,253
แนวคิดที่สำคัญที่สุดที่ผมอยากให้คุณเข้าใจคือ ทิศทางในพื้นที่มิติสูงของ 

15
00:01:02,253 --> 00:01:07,000
embedding ที่เป็นไปได้ทั้งหมดนี้ สามารถสอดคล้องกับความหมายของคำ ได้อย่างไร

16
00:01:07,680 --> 00:01:11,951
ในบทที่แล้ว เราได้เห็นตัวอย่างว่าทิศทางสามารถสอดคล้องกับเพศได้อย่างไร 

17
00:01:11,951 --> 00:01:15,612
ในแง่ที่ว่าการเติมขั้นตอนในพื้นที่นี้ สามารถพาคุณเปลี่ยนจาก 

18
00:01:15,612 --> 00:01:19,640
embedding คำนามเพศชายไปสู่ embedding คำนามเพศหญิงที่สอดคล้องกันได้

19
00:01:20,160 --> 00:01:23,845
นั่นเป็นเพียงตัวอย่างหนึ่งเท่านั้น คุณสามารถจินตนาการได้ว่ามีอีกมากมายหลาย

20
00:01:23,845 --> 00:01:27,580
ทิศทางในพื้นที่มิติสูงนี้ ที่อาจสอดคล้องกับความหมายด้านอื่นๆ มากมายของคำได้

21
00:01:28,800 --> 00:01:32,927
เป้าหมายของ transformer คือการปรับ embedding เหล่านี้อย่างต่อเนื่อง 

22
00:01:32,927 --> 00:01:35,780
เพื่อที่จะไม่เพียงแค่เข้ารหัสคำแต่ละคำเท่านั้น 

23
00:01:35,780 --> 00:01:39,180
แต่ยังรวมเอาความหมายตามบริบทที่สมบูรณ์ยิ่งขึ้นเข้าไปด้วย

24
00:01:40,140 --> 00:01:45,038
ผมควรจะบอกไว้ก่อนเลยว่า ผู้คนจำนวนมากรู้สึกสับสนกับกลไก attention ซึ่งเป็นส่วนสำคัญของ 

25
00:01:45,038 --> 00:01:48,980
transformer ดังนั้นไม่ต้องกังวลหากคุณใช้เวลาสักพักในการทำความเข้าใจมัน

26
00:01:49,440 --> 00:01:53,622
ผมคิดว่าก่อนที่เราจะลงลึกในรายละเอียดเชิงคำนวณและการคูณเมทริกซ์ทั้งหมด 

27
00:01:53,622 --> 00:01:58,629
มันคุ้มค่าที่จะคิดถึงตัวอย่างสองสามตัวอย่างสำหรับพฤติกรรมชนิดที่เราอยากให้ attention 

28
00:01:58,629 --> 00:01:59,160
ทำอะไรได้

29
00:02:00,140 --> 00:02:06,220
พิจารณาวลี American shrew mole, One mole of carbon dioxide, และ Take a biopsy of the mole

30
00:02:06,700 --> 00:02:10,900
คุณและผมรู้ว่าคำว่า mole มีความหมายที่แตกต่างกันในแต่ละวลี ขึ้นอยู่กับบริบทที่ใช้

31
00:02:11,360 --> 00:02:16,187
แต่หลังจากขั้นตอนแรกของ transformer ที่แบ่งข้อความออกเป็น token และเชื่อมโยง 

32
00:02:16,187 --> 00:02:21,266
token เข้ากับเวกเตอร์, เวกเตอร์ของคำว่า mole จะเหมือนกันในทุกกรณี เนื่องจากกำหนด 

33
00:02:21,266 --> 00:02:26,220
embedding เริ่มต้นนี้ เป็นแค่การเปิดตาราง lookup table โดยไม่ได้อ้างอิงถึงบริบท

34
00:02:26,620 --> 00:02:29,860
ต้องรอถึงขั้นตอนถัดไปของ transformer เท่านั้น ที่ 

35
00:02:29,860 --> 00:02:33,100
embedding รอบข้างจะมีโอกาสส่งผ่านข้อมูลมาให้ตัวนี้

36
00:02:33,820 --> 00:02:38,605
ภาพที่คุณอาจนึกถึงคือ มีหลายทิศทางที่แตกต่างกันในพื้นที่ฝังตัวนี้ 

37
00:02:38,605 --> 00:02:44,912
ซึ่งเข้ารหัสความหมายที่แตกต่างกันหลายอย่างของคำว่า mole และ attention block ที่ฝึกมาดี 

38
00:02:44,912 --> 00:02:50,422
จะคำนวณสิ่งที่คุณต้องเพิ่มเติมให้กับ embedding ฐาน เพื่อย้ายไปในทิศทางเฉพาะ 

39
00:02:50,422 --> 00:02:51,800
โดยขึ้นอยู่กับบริบท

40
00:02:53,300 --> 00:02:56,180
หากต้องการยกตัวอย่างอื่น ให้พิจารณาการฝังคำว่า หอคอย

41
00:02:57,060 --> 00:03:00,607
ทิศทางนี้น่าจะเป็นทิศทางทั่วไปและไม่เฉพาะเจาะจงในพื้นที่ 

42
00:03:00,607 --> 00:03:03,720
ซึ่งเชื่อมโยงกับคำนามขนาดใหญ่และสูงอื่นๆ อีกมากมาย

43
00:03:04,020 --> 00:03:07,368
หากคำนี้นำหน้าด้วยคำว่า &quot;Eiffel&quot; นำหน้าทันที 

44
00:03:07,368 --> 00:03:12,362
คุณอาจจินตนาการได้ว่าต้องการให้กลไกอัปเดตเวกเตอร์นี้เพื่อให้ชี้ไปในทิศทางที่เข้ารห

45
00:03:12,362 --> 00:03:17,476
ัสหอไอเฟลโดยเฉพาะมากขึ้น ซึ่งอาจสัมพันธ์กับเวกเตอร์ที่เกี่ยวข้องกับปารีสและฝรั่งเศส 

46
00:03:17,476 --> 00:03:19,060
และสิ่งต่างๆ ที่ทำจากเหล็ก

47
00:03:19,920 --> 00:03:24,453
หากนำหน้าด้วยคำว่าจิ๋ว ก็ควรอัปเดตเวกเตอร์ให้ละเอียดยิ่งขึ้น 

48
00:03:24,453 --> 00:03:27,500
เพื่อไม่ให้สัมพันธ์กับสิ่งสูงใหญ่อีกต่อไป

49
00:03:29,480 --> 00:03:32,516
โดยทั่วไปแล้ว มากกว่าแค่การปรับปรุงความหมายของคำ 

50
00:03:32,516 --> 00:03:37,102
บล็อกความสนใจช่วยให้โมเดลสามารถย้ายข้อมูลที่เข้ารหัสไว้ในข้อมูลหนึ่งที่ฝัง

51
00:03:37,102 --> 00:03:40,511
ไว้ไปยังอีกข้อมูลหนึ่ง ซึ่งอาจเป็นข้อมูลที่อยู่ห่างไกล 

52
00:03:40,511 --> 00:03:43,300
และอาจมีข้อมูลที่สมบูรณ์มากกว่าคำเพียงคำเดียว

53
00:03:43,300 --> 00:03:48,269
สิ่งที่เราเห็นในบทที่แล้วคือหลังจากที่เวกเตอร์ทั้งหมดไหลผ่านเครือข่าย 

54
00:03:48,269 --> 00:03:53,239
รวมถึงบล็อกความสนใจที่แตกต่างกันจำนวนมาก การคำนวณที่คุณดำเนินการเพื่อส

55
00:03:53,239 --> 00:03:58,280
ร้างการทำนายโทเค็นถัดไปนั้นเป็นฟังก์ชันของเวกเตอร์สุดท้ายในลำดับทั้งหมด

56
00:03:59,100 --> 00:04:03,744
ลองนึกภาพว่าข้อความที่คุณป้อนส่วนใหญ่เป็นนวนิยายลึกลับทั้งเล่ม 

57
00:04:03,744 --> 00:04:07,800
ไปจนถึงจุดใกล้จบซึ่งอ่านได้ ดังนั้นฆาตกรจึงเป็นเช่นนั้น

58
00:04:08,400 --> 00:04:14,981
หากแบบจำลองจะทำนายคำถัดไปได้อย่างแม่นยำ เวกเตอร์สุดท้ายในลำดับซึ่งเริ่มต้นชีวิตเพียงแค

59
00:04:14,981 --> 00:04:21,791
่ฝังคำนั้นไว้ จะต้องได้รับการอัปเดตโดยบล็อคความสนใจทั้งหมดเพื่อเป็นตัวแทนมากกว่าบุคคลใดๆ 

60
00:04:21,791 --> 00:04:28,220
มาก word เข้ารหัสข้อมูลทั้งหมดจากหน้าต่างบริบททั้งหมดที่เกี่ยวข้องกับการทำนายคำถัดไป

61
00:04:29,500 --> 00:04:32,580
เพื่อที่จะก้าวผ่านการคำนวณ มาดูตัวอย่างที่ง่ายกว่านี้กัน

62
00:04:32,980 --> 00:04:37,960
ลองนึกภาพว่าอินพุตมีวลี สิ่งมีชีวิตสีฟ้าขนปุยท่องไปในป่าเขียวขจี

63
00:04:38,460 --> 00:04:42,620
และในขณะนี้ สมมติว่าการอัปเดตประเภทเดียวที่เราสนใจค

64
00:04:42,620 --> 00:04:46,780
ือการให้คำคุณศัพท์ปรับความหมายของคำนามที่เกี่ยวข้อง

65
00:04:47,000 --> 00:04:50,634
สิ่งที่ผมจะอธิบายคือสิ่งที่เราจะเรียกว่าความสนใจแบบหัวเดียว 

66
00:04:50,634 --> 00:04:55,420
และต่อมาเราจะมาดูว่าบล็อกความสนใจประกอบด้วยหัวต่างๆ มากมายที่วิ่งขนานกันอย่างไร

67
00:04:56,140 --> 00:04:59,760
ขอย้ำอีกครั้งว่า การฝังครั้งแรกสำหรับแต่ละคำคือเวกเตอร์ที

68
00:04:59,760 --> 00:05:03,380
่มีมิติสูงซึ่งเข้ารหัสเฉพาะความหมายของคำนั้นโดยไม่มีบริบท

69
00:05:04,000 --> 00:05:05,220
จริงๆแล้วนั่นไม่เป็นความจริงเลย

70
00:05:05,380 --> 00:05:07,640
พวกเขายังเข้ารหัสตำแหน่งของคำด้วย

71
00:05:07,980 --> 00:05:11,805
มีอะไรอีกมากมายที่จะบอกว่าตำแหน่งถูกเข้ารหัส แต่ตอนนี้ 

72
00:05:11,805 --> 00:05:17,230
สิ่งเดียวที่คุณต้องรู้ก็คือค่าของเวกเตอร์นี้เพียงพอที่จะบอกคุณทั้งว่าคำนั้นคือ

73
00:05:17,230 --> 00:05:18,900
อะไรและอยู่ที่ไหนในบริบท

74
00:05:19,500 --> 00:05:21,660
เรามาแสดงการฝังเหล่านี้ด้วยตัวอักษร e กันดีกว่า

75
00:05:22,420 --> 00:05:27,778
เป้าหมายคือการมีชุดการคำนวณที่สร้างชุดการฝังที่ได้รับการปรับปรุงใหม่ โดยที่ 

76
00:05:27,778 --> 00:05:33,420
ตัวอย่างเช่น กลุ่มที่สอดคล้องกับคำนามได้นำเข้าความหมายจากคำคุณศัพท์ที่เกี่ยวข้อง

77
00:05:33,900 --> 00:05:37,680
และการเล่นเกมการเรียนรู้เชิงลึก เราต้องการให้การคำนวณส่วนใหญ่ที่เกี่ยวข้อง 

78
00:05:37,680 --> 00:05:41,863
มีลักษณะเหมือนผลคูณเมทริกซ์-เวกเตอร์ โดยที่เมทริกซ์เต็มไปด้วยตุ้มน้ำหนักที่ปรับได้ 

79
00:05:41,863 --> 00:05:43,980
ซึ่งเป็นสิ่งที่แบบจำลองจะเรียนรู้จากข้อมูล

80
00:05:44,660 --> 00:05:48,434
เพื่อให้ชัดเจน ฉันกำลังสร้างตัวอย่างคำคุณศัพท์ที่อัปเดตคำนามเพียงเพื่อแสด

81
00:05:48,434 --> 00:05:52,260
งให้เห็นประเภทของพฤติกรรมที่คุณสามารถจินตนาการได้ว่าหัวความสนใจกำลังทำอยู่

82
00:05:52,860 --> 00:05:56,824
เช่นเดียวกับการเรียนรู้เชิงลึก พฤติกรรมที่แท้จริงนั้นแยกวิเคราะห์ได้ยากกว่ามาก 

83
00:05:56,824 --> 00:06:01,340
เนื่องจากขึ้นอยู่กับการปรับแต่งและปรับแต่งพารามิเตอร์จำนวนมากเพื่อลดฟังก์ชันต้นทุนบางอย่าง

84
00:06:01,680 --> 00:06:05,491
แค่ว่าเมื่อเราก้าวผ่านเมทริกซ์ต่างๆ ทั้งหมดที่เต็มไปด้วยพารามิเตอร์ที่เกี

85
00:06:05,491 --> 00:06:09,303
่ยวข้องในกระบวนการนี้ ฉันคิดว่าการจินตนาการถึงตัวอย่างที่จินตนาการไว้ของบ

86
00:06:09,303 --> 00:06:13,220
างสิ่งที่สามารถทำได้เพื่อช่วยให้ทุกอย่างเป็นรูปธรรมมากขึ้นนั้นมีประโยชน์มาก

87
00:06:14,140 --> 00:06:17,989
ในขั้นตอนแรกของกระบวนการนี้ คุณอาจจินตนาการถึงคำนามแต่ละคำ เช่น 

88
00:06:17,989 --> 00:06:21,960
สิ่งมีชีวิต ที่ถามคำถามว่า เฮ้ มีคำคุณศัพท์อยู่ข้างหน้าฉันบ้างไหม?

89
00:06:22,160 --> 00:06:25,173
และสำหรับคำว่า ปุย และ สีน้ำเงิน แต่ละคนสามารถตอบได้ 

90
00:06:25,173 --> 00:06:27,960
ใช่แล้ว ฉันเป็นคำคุณศัพท์ และฉันอยู่ในตำแหน่งนั้น

91
00:06:28,960 --> 00:06:33,848
คำถามนั้นถูกเข้ารหัสเป็นเวกเตอร์อีกตัวหนึ่ง ซึ่งเป็นรายการตัวเลขอีกชุดหนึ่ง 

92
00:06:33,848 --> 00:06:36,100
ซึ่งเราเรียกว่าแบบสอบถามสำหรับคำนี้

93
00:06:36,980 --> 00:06:42,020
เวกเตอร์แบบสอบถามนี้มีขนาดเล็กกว่าเวกเตอร์ที่ฝังมาก เช่น 128

94
00:06:42,940 --> 00:06:46,731
การคำนวณแบบสอบถามนี้ดูเหมือนว่าจะใช้เมทริกซ์บางตัว 

95
00:06:46,731 --> 00:06:49,780
ซึ่งฉันจะติดป้ายกำกับ wq และคูณด้วยการฝัง

96
00:06:50,960 --> 00:06:54,971
บีบอัดสิ่งต่างๆ สักหน่อย, ลองเขียนเวกเตอร์เคียวรีนั่นเป็น q 

97
00:06:54,971 --> 00:06:58,648
แล้วทุกครั้งที่คุณเห็นผมใส่เมทริกซ์ไว้ข้างลูกศรแบบนี้, 

98
00:06:58,648 --> 00:07:03,061
มันหมายถึงว่า การคูณเมทริกซ์นี้ด้วยเวกเตอร์ที่จุดเริ่มต้นของลูกศร 

99
00:07:03,061 --> 00:07:04,800
จะได้เวกเตอร์ที่ ปลายลูกศร

100
00:07:05,860 --> 00:07:09,383
ในกรณีนี้ คุณจะคูณเมทริกซ์นี้ด้วยการฝังทั้งหมดในบริบท 

101
00:07:09,383 --> 00:07:12,580
โดยสร้างเวกเตอร์คิวรีหนึ่งรายการสำหรับแต่ละโทเค็น

102
00:07:13,740 --> 00:07:16,177
รายการของเมทริกซ์นี้เป็นพารามิเตอร์ของแบบจำลอง 

103
00:07:16,177 --> 00:07:19,394
ซึ่งหมายความว่าพฤติกรรมที่แท้จริงจะได้รับการเรียนรู้จากข้อมูล 

104
00:07:19,394 --> 00:07:23,440
และในทางปฏิบัติ สิ่งที่เมทริกซ์นี้ทำในหัวความสนใจเฉพาะนั้นยากที่จะแยกวิเคราะห์

105
00:07:23,900 --> 00:07:28,389
แต่เพื่อประโยชน์ของเรา ลองนึกภาพตัวอย่างที่เราหวังว่าจะได้เรียนรู้ 

106
00:07:28,389 --> 00:07:33,080
เราจะสมมติว่าเมทริกซ์คิวรีนี้แมปการฝังคำนามไปยังทิศทางที่แน่นอนในพื้นท

107
00:07:33,080 --> 00:07:38,040
ี่คิวรีขนาดเล็กนี้ที่เข้ารหัสแนวคิดในการมองหาคำคุณศัพท์ในตำแหน่งก่อนหน้า .

108
00:07:38,780 --> 00:07:41,440
การฝังอื่นๆ มีผลอย่างไร ใครจะรู้?

109
00:07:41,720 --> 00:07:44,340
บางทีมันอาจจะพยายามบรรลุเป้าหมายอื่นไปพร้อมๆ กัน

110
00:07:44,540 --> 00:07:47,160
ตอนนี้ เรากำลังเน้นไปที่คำนามแบบเลเซอร์

111
00:07:47,280 --> 00:07:52,088
ในเวลาเดียวกัน เมทริกซ์ตัวที่สองที่เกี่ยวข้องกับสิ่งนี้คือเมทริกซ์ตัวที่สอง 

112
00:07:52,088 --> 00:07:54,620
ซึ่งคุณคูณด้วยทุกๆ เมทริกซ์ที่ฝังไว้ด้วย

113
00:07:55,280 --> 00:07:58,500
สิ่งนี้จะสร้างลำดับที่สองของเวกเตอร์ที่เราเรียกว่าคีย์

114
00:07:59,420 --> 00:08:03,140
ตามแนวคิดแล้ว คุณต้องคิดว่าคีย์ต่างๆ อาจตอบคำถามได้

115
00:08:03,840 --> 00:08:08,137
เมทริกซ์หลักนี้ยังเต็มไปด้วยพารามิเตอร์ที่ปรับแต่งได้ และเช่นเดียวกับเมทริกซ์คิวรี 

116
00:08:08,137 --> 00:08:11,400
มันจะแมปเวกเตอร์ที่ฝังเข้ากับพื้นที่มิติที่เล็กกว่าเดียวกันนั้น

117
00:08:12,200 --> 00:08:17,020
คุณคิดว่าคีย์ต่างๆ ตรงกับคิวรีเมื่อใดก็ตามที่คีย์เหล่านั้นสอดคล้องกันอย่างใกล้ชิด

118
00:08:17,460 --> 00:08:22,421
ในตัวอย่างของเรา คุณจะจินตนาการว่าคีย์เมทริกซ์จับคู่คำคุณศัพท์ เช่น ปุย และสีน้ำเงิน 

119
00:08:22,421 --> 00:08:26,740
กับเวกเตอร์ที่สอดคล้องอย่างใกล้ชิดกับข้อความค้นหาที่สร้างโดยคำว่า Creature

120
00:08:27,200 --> 00:08:30,327
หากต้องการวัดว่าแต่ละคีย์ตรงกับแต่ละข้อความค้นหามากน้อยเพียงใด 

121
00:08:30,327 --> 00:08:34,000
คุณจะต้องคำนวณ dot product ระหว่างคู่คีย์-ข้อความค้นหาที่เป็นไปได้แต่ละคู่

122
00:08:34,480 --> 00:08:38,490
ฉันชอบเห็นภาพตารางที่เต็มไปด้วยจุดจำนวนมาก โดยที่จุดที่ใหญ่กว่านั้น

123
00:08:38,490 --> 00:08:42,559
สอดคล้องกับผลคูณของดอทที่ใหญ่กว่า ซึ่งเป็นจุดที่คีย์และคิวรีเรียงกัน

124
00:08:43,280 --> 00:08:47,885
สำหรับตัวอย่างคำคุณศัพท์ของเรา นั่นจะมีลักษณะเช่นนี้อีกเล็กน้อย 

125
00:08:47,885 --> 00:08:53,858
โดยที่หากคีย์ที่สร้างโดย fury และ blue สอดคล้องอย่างใกล้ชิดกับการสืบค้นที่สร้างโดย 

126
00:08:53,858 --> 00:08:58,320
Creature ดังนั้น dot product ในสองจุดนี้จะเป็นจำนวนบวกจำนวนมาก

127
00:08:59,100 --> 00:09:01,721
ในศัพท์แสง ผู้คนที่เรียนรู้เกี่ยวกับเครื่องจักรจะพูดว่า 

128
00:09:01,721 --> 00:09:05,420
นี่หมายความว่าการฝังของปุยและสีน้ำเงินจะเข้ามาเกี่ยวข้องกับการฝังของสิ่งมีชีวิต

129
00:09:06,040 --> 00:09:10,308
ในทางตรงกันข้ามกับดอทโปรดัคระหว่างคีย์ของคำอื่น เช่น the 

130
00:09:10,308 --> 00:09:16,600
และคิวรีสำหรับสิ่งมีชีวิต จะเป็นค่าเล็กน้อยหรือค่าลบที่สะท้อนถึงความไม่เกี่ยวข้องกัน

131
00:09:17,700 --> 00:09:23,124
ดังนั้นเราจึงมีตารางค่าที่สามารถเป็นจำนวนจริงใดๆ ได้ตั้งแต่ลบอนันต์จนถึงอนันต์ 

132
00:09:23,124 --> 00:09:28,480
ทำให้เราให้คะแนนว่าแต่ละคำมีความเกี่ยวข้องอย่างไรในการอัปเดตความหมายของคำอื่นๆ

133
00:09:29,200 --> 00:09:33,958
วิธีที่เรากำลังจะใช้คะแนนเหล่านี้คือการหาผลรวมถ่วงน้ำหนักที่แน่นอนในแต่ละคอลัมน์ 

134
00:09:33,958 --> 00:09:35,780
โดยถ่วงน้ำหนักตามความเกี่ยวข้อง

135
00:09:36,520 --> 00:09:39,728
แทนที่จะมีค่าอยู่ในช่วงตั้งแต่ลบอนันต์ไปจนถึงอนันต์ 

136
00:09:39,728 --> 00:09:43,676
สิ่งที่เราต้องการคือให้ตัวเลขในคอลัมน์เหล่านี้อยู่ระหว่าง 0 ถึง 

137
00:09:43,676 --> 00:09:48,180
1 และให้แต่ละคอลัมน์รวมกันได้ 1 ราวกับว่าพวกมันเป็นการแจกแจงความน่าจะเป็น

138
00:09:49,280 --> 00:09:52,220
ถ้าคุณมาจากบทที่แล้ว คุณคงรู้ว่าเราต้องทำอะไร

139
00:09:52,620 --> 00:09:57,300
เราคำนวณซอฟต์แม็กซ์ตามแต่ละคอลัมน์เหล่านี้เพื่อทำให้ค่าเป็นมาตรฐาน

140
00:10:00,060 --> 00:10:03,516
ในภาพของเรา หลังจากที่คุณใช้ softmax กับคอลัมน์ทั้งหมดแล้ว 

141
00:10:03,516 --> 00:10:05,860
เราจะเติมค่าที่ทำให้เป็นมาตรฐานลงในตาราง

142
00:10:06,780 --> 00:10:10,642
ณ จุดนี้ คุณมั่นใจได้เลยว่าแต่ละคอลัมน์จะต้องให้น้ำห

143
00:10:10,642 --> 00:10:14,580
นักตามความเกี่ยวข้องของคำทางซ้ายกับค่าที่ตรงกันด้านบน

144
00:10:15,080 --> 00:10:16,840
เราเรียกตารางนี้ว่ารูปแบบความสนใจ

145
00:10:18,080 --> 00:10:22,820
ทีนี้ ถ้าคุณดูที่กระดาษหม้อแปลงต้นฉบับ มันมีวิธีที่กะทัดรัดมากในการเขียนทั้งหมดนี้

146
00:10:23,880 --> 00:10:29,294
ในที่นี้ ตัวแปร q และ k แสดงถึงอาร์เรย์แบบเต็มของเวกเตอร์คิวรีและคีย์ตามลำดับ 

147
00:10:29,294 --> 00:10:34,640
เวกเตอร์เล็กๆ เหล่านั้นที่คุณได้รับจากการคูณการฝังด้วยคิวรีและเมทริกซ์ของคีย์

148
00:10:35,160 --> 00:10:39,058
การแสดงออกในตัวเศษนี้เป็นวิธีที่กะทัดรัดมากในการแสดงตารางของผ

149
00:10:39,058 --> 00:10:43,020
ลิตภัณฑ์ดอทที่เป็นไปได้ทั้งหมดระหว่างคู่ของคีย์และข้อความค้นหา

150
00:10:44,000 --> 00:10:48,980
รายละเอียดทางเทคนิคเล็กๆ น้อยๆ ที่ฉันไม่ได้พูดถึงก็คือ เพื่อความเสถียรของตัวเลข 

151
00:10:48,980 --> 00:10:53,960
การหารค่าเหล่านี้ทั้งหมดด้วยรากที่สองของมิติในพื้นที่คิวรีคีย์นั้นจะเป็นประโยชน์

152
00:10:54,480 --> 00:11:00,800
จากนั้นซอฟต์แม็กซ์ที่พันรอบนิพจน์แบบเต็มนี้ควรเข้าใจว่าจะใช้ทีละคอลัมน์

153
00:11:01,640 --> 00:11:04,700
สำหรับเทอม v นั้น เราจะพูดถึงมันในอีกสักครู่

154
00:11:05,020 --> 00:11:08,460
ก่อนหน้านั้น มีรายละเอียดทางเทคนิคอีกอย่างหนึ่งที่ฉันข้ามไป

155
00:11:09,040 --> 00:11:13,726
ในระหว่างกระบวนการฝึกอบรม เมื่อคุณรันโมเดลนี้ตามตัวอย่างข้อความที่กำหนด 

156
00:11:13,726 --> 00:11:19,323
และน้ำหนักทั้งหมดจะถูกปรับเล็กน้อยและปรับแต่งเพื่อให้รางวัลหรือลงโทษโดยพิจารณาจากความน

157
00:11:19,323 --> 00:11:22,903
่าจะเป็นที่จะกำหนดให้กับคำถัดไปที่แท้จริงในเนื้อเรื่อง 

158
00:11:22,903 --> 00:11:28,500
จะทำให้กระบวนการฝึกอบรมทั้งหมดมีประสิทธิภาพมากขึ้นหากคุณคาดการณ์โทเค็นถัดไปที่เป็นไปได

159
00:11:28,500 --> 00:11:31,560
้ไปพร้อม ๆ กันตามลำดับเริ่มต้นของโทเค็นในข้อนี้

160
00:11:31,940 --> 00:11:38,097
ตัวอย่างเช่น ด้วยวลีที่เรามุ่งเน้น มันอาจจะเป็นการคาดเดาด้วยว่าคำใดตามหลังสิ่งมีชีวิต 

161
00:11:38,097 --> 00:11:39,100
และคำใดตามหลัง

162
00:11:39,940 --> 00:11:43,426
นี่เป็นสิ่งที่ดีจริงๆ เพราะมันหมายความว่าตัวอย่างการฝึกอบรมเดี่ยวๆ 

163
00:11:43,426 --> 00:11:45,560
จะทำหน้าที่หลายอย่างได้อย่างมีประสิทธิภาพ

164
00:11:46,100 --> 00:11:49,413
สำหรับวัตถุประสงค์ของรูปแบบความสนใจของเรา หมายความว่าคุณไม่ควรปล

165
00:11:49,413 --> 00:11:52,208
่อยให้คำที่ตามมามีอิทธิพลเหนือคำที่กล่าวมาก่อนหน้านี้ 

166
00:11:52,208 --> 00:11:56,040
เพราะไม่เช่นนั้นคำเหล่านั้นก็สามารถให้คำตอบสำหรับสิ่งที่จะเกิดขึ้นต่อไปได้

167
00:11:56,560 --> 00:11:59,552
ความหมายก็คือเราต้องการให้จุดเหล่านี้ทั้งหมดที่นี่ 

168
00:11:59,552 --> 00:12:04,600
ซึ่งเป็นตัวแทนของโทเค็นที่ตามมาซึ่งมีอิทธิพลต่อโทเค็นก่อนหน้านี้ ถูกบังคับให้เป็นศูนย์

169
00:12:05,920 --> 00:12:08,394
สิ่งที่ง่ายที่สุดที่คุณอาจคิดจะทำคือตั้งค่าให้เท่ากับศูนย์ 

170
00:12:08,394 --> 00:12:10,742
แต่ถ้าคุณทำอย่างนั้นคอลัมน์จะไม่รวมกันเป็นหนึ่งอีกต่อไป 

171
00:12:10,742 --> 00:12:12,420
คอลัมน์เหล่านั้นจะไม่ถูกทำให้เป็นมาตรฐาน

172
00:12:13,120 --> 00:12:19,020
วิธีทั่วไปในการทำเช่นนี้คือ ก่อนใช้ซอฟต์แม็กซ์ คุณต้องตั้งค่าค่าทั้งหมดให้เป็นลบอนันต์

173
00:12:19,680 --> 00:12:23,541
หากคุณทำเช่นนั้น หลังจากใช้ softmax คอลัมน์ทั้งหมดจะกลายเป็นศูนย์ 

174
00:12:23,541 --> 00:12:25,180
แต่คอลัมน์จะยังคงเป็นมาตรฐาน

175
00:12:26,000 --> 00:12:27,540
กระบวนการนี้เรียกว่าการมาสก์

176
00:12:27,540 --> 00:12:30,540
มีความสนใจหลายรูปแบบที่คุณไม่ได้ใช้ แต่ในตัวอย่าง GPT ของเรา 

177
00:12:30,540 --> 00:12:34,672
แม้ว่าสิ่งนี้จะมีความเกี่ยวข้องมากกว่าในระหว่างขั้นตอนการฝึกอบรมมากกว่าที่ควรจะเป็น 

178
00:12:34,672 --> 00:12:37,967
เช่น การเรียกใช้เป็นแชทบอทหรืออะไรทำนองนั้น แต่คุณมักจะนำไปใช้เสมอ 

179
00:12:37,967 --> 00:12:41,460
การปิดบังนี้เพื่อป้องกันไม่ให้โทเค็นในภายหลังมีอิทธิพลต่อโทเค็นก่อนหน้า

180
00:12:42,480 --> 00:12:45,960
ข้อเท็จจริงอีกประการหนึ่งที่ควรค่าแก่การพิจารณาเกี่ยวกับรูป

181
00:12:45,960 --> 00:12:49,500
แบบความสนใจนี้คือขนาดของรูปแบบนี้เท่ากับกำลังสองของขนาดบริบท

182
00:12:49,900 --> 00:12:53,730
นี่คือสาเหตุที่ขนาดบริบทอาจเป็นปัญหาคอขวดอย่างมากสำหรับโมเดลภาษาขนาดใหญ่ 

183
00:12:53,730 --> 00:12:55,620
และการขยายขนาดก็ไม่ใช่เรื่องเล็กน้อย

184
00:12:56,300 --> 00:13:00,306
ตามที่คุณจินตนาการ โดยได้รับแรงบันดาลใจจากความปรารถนาที่จะมีกรอบเวลาบริบทที่ใหญ่ขึ้นเร

185
00:13:00,306 --> 00:13:04,313
ื่อยๆ ในช่วงไม่กี่ปีที่ผ่านมาได้เห็นการเปลี่ยนแปลงบางอย่างในกลไกความสนใจที่มีจุดมุ่งหม

186
00:13:04,313 --> 00:13:08,320
ายเพื่อทำให้บริบทสามารถปรับขนาดได้มากขึ้น แต่ที่นี่ คุณและฉันยังคงมุ่งเน้นไปที่พื้นฐาน

187
00:13:10,560 --> 00:13:12,993
โอเค เยี่ยมเลย การคำนวณรูปแบบนี้จะทำให้แบบจำลอ

188
00:13:12,993 --> 00:13:15,480
งสามารถอนุมานได้ว่าคำไหนเกี่ยวข้องกับคำอื่นบ้าง

189
00:13:16,020 --> 00:13:22,800
ตอนนี้ คุณต้องอัปเดตการฝังจริง โดยอนุญาตให้คำต่างๆ ส่งข้อมูลไปยังคำอื่นๆ ที่เกี่ยวข้องได้

190
00:13:22,800 --> 00:13:28,133
ตัวอย่างเช่น คุณต้องการให้การฝัง Fluffy ทำให้เกิดการเปลี่ยนแปลงกับ Creature 

191
00:13:28,133 --> 00:13:33,958
โดยจะย้ายมันไปยังส่วนอื่นของพื้นที่การฝัง 12,000 มิติที่เข้ารหัสสิ่งมีชีวิต Fluffy 

192
00:13:33,958 --> 00:13:34,520
โดยเฉพาะ

193
00:13:35,460 --> 00:13:39,436
สิ่งที่ฉันจะทำที่นี่คือขั้นแรกแสดงให้คุณเห็นวิธีที่ตรงไปตรงมาที่สุดที่คุณสามารถทำสิ่

194
00:13:39,436 --> 00:13:43,460
งนี้ได้ แม้ว่าจะมีวิธีเล็กน้อยที่สิ่งนี้จะได้รับการแก้ไข ในบริบทของความสนใจแบบหลายหัว

195
00:13:44,080 --> 00:13:47,622
วิธีที่ตรงไปตรงมาที่สุดคือการใช้เมทริกซ์ตัวที่สาม 

196
00:13:47,622 --> 00:13:52,440
ซึ่งเราเรียกว่าเมทริกซ์ค่า ซึ่งคุณคูณด้วยการฝังคำแรกนั้น เช่น Fluffy

197
00:13:53,300 --> 00:13:56,412
ผลลัพธ์ของสิ่งนี้คือสิ่งที่คุณจะเรียกว่าเวกเตอร์ค่า 

198
00:13:56,412 --> 00:14:01,441
และนี่คือสิ่งที่คุณเพิ่มลงในการฝังคำที่สอง ในกรณีนี้คือสิ่งที่คุณเพิ่มลงในการฝังของ 

199
00:14:01,441 --> 00:14:01,920
Creature

200
00:14:02,600 --> 00:14:07,000
ดังนั้นเวกเตอร์ค่านี้จึงอยู่ในสเปซมิติที่สูงมากเหมือนกับการฝัง

201
00:14:07,460 --> 00:14:12,400
เมื่อคุณคูณเมทริกซ์ค่านี้ด้วยการฝังคำ คุณอาจคิดว่ามันเป็นคำพูดว่า 

202
00:14:12,400 --> 00:14:15,994
ถ้าคำนี้เกี่ยวข้องกับการปรับความหมายของสิ่งอื่น 

203
00:14:15,994 --> 00:14:21,160
สิ่งที่ควรเพิ่มเข้าไปในการฝังสิ่งอื่นนั้นเพื่อที่จะสะท้อนให้เห็น นี้?

204
00:14:22,140 --> 00:14:26,271
มองย้อนกลับไปในไดอะแกรมของเรา ลองแยกคีย์และการสืบค้นทั้งหมดออกไป 

205
00:14:26,271 --> 00:14:30,720
เนื่องจากหลังจากที่คุณคำนวณรูปแบบความสนใจที่คุณทำกับสิ่งเหล่านั้นแล้ว 

206
00:14:30,720 --> 00:14:36,060
คุณจะต้องนำเมทริกซ์ค่านี้มาคูณด้วยทุกๆ การฝังเหล่านั้น เพื่อสร้างลำดับของเวกเตอร์ค่า

207
00:14:37,120 --> 00:14:41,120
คุณอาจคิดว่าเวกเตอร์ค่าพวกนี้สัมพันธ์กับคีย์ที่เกี่ยวข้องกัน

208
00:14:42,320 --> 00:14:45,743
สำหรับแต่ละคอลัมน์ในแผนภาพนี้ คุณจะคูณเวกเตอร์ค

209
00:14:45,743 --> 00:14:49,240
่าแต่ละตัวด้วยน้ำหนักที่สอดคล้องกันในคอลัมน์นั้น

210
00:14:50,080 --> 00:14:55,692
ตัวอย่างเช่น ที่นี่ ภายใต้การฝัง Creature คุณจะเพิ่มสัดส่วนขนาดใหญ่ของเวกเตอร์ค่าสำหรับ 

211
00:14:55,692 --> 00:14:59,710
Fluffy และ Blue ในขณะที่เวกเตอร์ค่าอื่นๆ ทั้งหมดมีค่าเป็นศูนย์ 

212
00:14:59,710 --> 00:15:01,560
หรืออย่างน้อยก็เกือบเป็นศูนย์

213
00:15:02,120 --> 00:15:05,497
และสุดท้าย วิธีอัปเดตการฝังที่เกี่ยวข้องกับคอลัมน์นี้ 

214
00:15:05,497 --> 00:15:09,001
โดยก่อนหน้านี้เข้ารหัสความหมายแบบไม่มีบริบทของ Creature 

215
00:15:09,001 --> 00:15:12,191
คุณเพิ่มค่าที่ปรับขนาดใหม่เหล่านี้ทั้งหมดในคอลัมน์ 

216
00:15:12,191 --> 00:15:16,257
ทำให้เกิดการเปลี่ยนแปลงที่คุณต้องการเพิ่ม ซึ่งฉัน จะติดป้ายกำกับ 

217
00:15:16,257 --> 00:15:19,260
delta-e จากนั้นคุณเพิ่มสิ่งนั้นลงในการฝังต้นฉบับ

218
00:15:19,680 --> 00:15:23,061
หวังว่าผลลัพธ์ที่ได้จะเป็นเวกเตอร์ที่ละเอียดยิ่งขึ้นในการเข้

219
00:15:23,061 --> 00:15:26,500
ารหัสความหมายที่มีบริบทมากขึ้น เหมือนกับสิ่งมีชีวิตสีฟ้าขนปุย

220
00:15:27,380 --> 00:15:30,663
และแน่นอน คุณไม่เพียงแค่ทำเช่นนี้กับการฝังเพียงครั้งเดียว 

221
00:15:30,663 --> 00:15:34,117
แต่คุณใช้ผลรวมถ่วงน้ำหนักเท่ากันกับคอลัมน์ทั้งหมดในรูปภาพนี้ 

222
00:15:34,117 --> 00:15:39,100
สร้างลำดับของการเปลี่ยนแปลง เพิ่มการเปลี่ยนแปลงทั้งหมดเหล่านั้นลงในการฝังที่สอดคล้องกัน 

223
00:15:39,100 --> 00:15:43,460
ทำให้เกิดลำดับที่สมบูรณ์ของ การฝังที่ละเอียดยิ่งขึ้นโผล่ออกมาจากบล็อกความสนใจ

224
00:15:44,860 --> 00:15:49,100
เมื่อขยายออก กระบวนการทั้งหมดนี้คือสิ่งที่คุณจะเรียกว่าเป็นความสนใจหัวเดียว

225
00:15:49,600 --> 00:15:54,868
ดังที่ฉันได้อธิบายไปแล้ว กระบวนการนี้ถูกกำหนดพารามิเตอร์ด้วยเมทริกซ์ที่แตกต่างกันสามตัว 

226
00:15:54,868 --> 00:15:58,940
ซึ่งทั้งหมดเต็มไปด้วยพารามิเตอร์ที่ปรับแต่งได้ คีย์ การสืบค้น และค่า

227
00:15:59,500 --> 00:16:03,628
ฉันต้องการใช้เวลาสักครู่เพื่อดำเนินการต่อในสิ่งที่เราเริ่มต้นในบทที่แล้ว 

228
00:16:03,628 --> 00:16:08,040
โดยมีการเก็บคะแนนโดยที่เรานับจำนวนพารามิเตอร์โมเดลทั้งหมดโดยใช้ตัวเลขจาก GPT-3

229
00:16:09,300 --> 00:16:14,978
เมทริกซ์คีย์และคิวรีเหล่านี้แต่ละเมทริกซ์มี 12,288 คอลัมน์ ซึ่งตรงกับมิติข้อมูลการฝัง 

230
00:16:14,978 --> 00:16:19,600
และ 128 แถว ซึ่งตรงกับมิติของพื้นที่การสืบค้นคีย์ที่มีขนาดเล็กกว่านั้น

231
00:16:20,260 --> 00:16:24,220
นี่ทำให้เรามีพารามิเตอร์เพิ่มเติมประมาณ 1.5 ล้านพารามิเตอร์สำหรับแต่ละรายการ

232
00:16:24,860 --> 00:16:30,185
หากคุณดูเมทริกซ์ค่านั้นในทางตรงข้าม วิธีที่ฉันเคยอธิบายไปแล้วน่า

233
00:16:30,185 --> 00:16:35,511
จะแนะนำว่าเป็นเมทริกซ์จัตุรัสที่มี 12,288 คอลัมน์และ 12,288 แถว 

234
00:16:35,511 --> 00:16:40,920
เนื่องจากทั้งอินพุตและเอาต์พุตอาศัยอยู่ในพื้นที่ฝังขนาดใหญ่มากนี้

235
00:16:41,500 --> 00:16:45,140
หากเป็นจริง นั่นหมายถึงต้องมีพารามิเตอร์เพิ่มประมาณ 150 ล้านพารามิเตอร์

236
00:16:45,660 --> 00:16:47,300
และเพื่อความชัดเจน คุณสามารถทำเช่นนั้นได้

237
00:16:47,420 --> 00:16:51,740
คุณสามารถจัดสรรพารามิเตอร์ที่มีขนาดมากขึ้นให้กับแมปค่ามากกว่าคีย์และคิวรี

238
00:16:52,060 --> 00:16:56,410
แต่ในทางปฏิบัติ จะมีประสิทธิภาพมากกว่ามากหากคุณสร้างเพื่อให้จำนวนพารามิเต

239
00:16:56,410 --> 00:17:00,760
อร์ที่ทุ่มเทให้กับการแมปค่านี้เท่ากับจำนวนที่ทุ่มเทให้กับคีย์และแบบสอบถาม

240
00:17:01,460 --> 00:17:05,160
สิ่งนี้มีความเกี่ยวข้องอย่างยิ่งในการตั้งค่าการดำเนินการให้ความสนใจหลายรายการพร้อมกัน

241
00:17:06,240 --> 00:17:10,099
ลักษณะที่ปรากฏคือแผนผังค่าจะถูกแยกตัวประกอบเป็นผลคูณของเมทริกซ์ขนาดเล็กสองตัว

242
00:17:11,180 --> 00:17:15,340
ตามแนวคิดแล้ว ฉันยังคงแนะนำให้คุณคิดถึงแผนที่เชิงเส้นโดยรวม 

243
00:17:15,340 --> 00:17:19,500
ซึ่งมีอินพุตและเอาท์พุต ทั้งสองอย่างในพื้นที่ฝังขนาดใหญ่นี้ 

244
00:17:19,500 --> 00:17:23,800
เช่น นำสีน้ำเงินฝังมาในทิศทางสีน้ำเงินที่คุณจะเพิ่มให้กับคำนาม

245
00:17:27,040 --> 00:17:32,760
เพียงแต่ว่ามีจำนวนแถวน้อยกว่า ซึ่งโดยทั่วไปจะมีขนาดเท่ากับพื้นที่คิวรีคีย์

246
00:17:33,100 --> 00:17:38,440
ความหมายคือ คุณสามารถคิดว่ามันเป็นการแมปเวกเตอร์ที่ฝังขนาดใหญ่ลงไปยังพื้นที่ที่เล็กกว่ามาก

247
00:17:39,040 --> 00:17:42,700
นี่ไม่ใช่การตั้งชื่อทั่วไป แต่ผมจะเรียกมันว่าเมทริกซ์ค่าล่าง

248
00:17:43,400 --> 00:17:47,812
เมทริกซ์ที่สองแมปจากพื้นที่ขนาดเล็กนี้สำรองไปยังพื้นที่ฝัง 

249
00:17:47,812 --> 00:17:50,580
สร้างเวกเตอร์ที่คุณใช้ในการอัปเดตจริง

250
00:17:51,000 --> 00:17:54,740
ผมจะเรียกอันนี้ว่าเมทริกซ์เพิ่มค่า ซึ่งไม่ธรรมดาอีกแล้ว

251
00:17:55,160 --> 00:17:58,080
วิธีที่คุณเห็นสิ่งนี้เขียนในเอกสารส่วนใหญ่ดูแตกต่างออกไปเล็กน้อย

252
00:17:58,380 --> 00:17:59,520
ฉันจะพูดถึงมันในอีกสักครู่

253
00:17:59,700 --> 00:18:02,540
ในความคิดของฉัน มันมีแนวโน้มที่จะทำให้สิ่งต่าง ๆ เกิดความสับสนทางแนวคิดมากขึ้นเล็กน้อย

254
00:18:03,260 --> 00:18:06,766
หากต้องการใช้ศัพท์เฉพาะพีชคณิตเชิงเส้นตรงนี้ สิ่งที่เ

255
00:18:06,766 --> 00:18:10,340
รากำลังทำคือจำกัดแผนผังค่าโดยรวมให้เป็นการแปลงระดับต่ำ

256
00:18:11,420 --> 00:18:15,277
เมื่อย้อนกลับไปที่การนับพารามิเตอร์ เมทริกซ์ทั้งสี่นี้มีขนาดเท่ากัน 

257
00:18:15,277 --> 00:18:18,681
และเมื่อบวกทั้งหมดเข้าด้วยกัน เราจะได้พารามิเตอร์ประมาณ 6.3 

258
00:18:18,681 --> 00:18:20,780
ล้านพารามิเตอร์สำหรับหัวความสนใจเดียว

259
00:18:22,040 --> 00:18:26,740
เพื่อให้แม่นยำยิ่งขึ้นอีกหน่อย ทุกอย่างที่อธิบายไว้จนถึงขณะนี้คือสิ่งที่ผู้คนเร

260
00:18:26,740 --> 00:18:31,500
ียกว่าการใส่ใจในตนเอง เพื่อแยกความแตกต่างจากรูปแบบอื่น ๆ ที่เรียกว่าการสนใจตนเอง

261
00:18:32,300 --> 00:18:35,771
สิ่งนี้ไม่เกี่ยวข้องกับตัวอย่าง GPT ของเรา แต่หากคุณสงสัย 

262
00:18:35,771 --> 00:18:40,081
ความสนใจข้ามจะเกี่ยวข้องกับโมเดลที่ประมวลผลข้อมูลที่แตกต่างกันสองประเภท 

263
00:18:40,081 --> 00:18:44,271
เช่น ข้อความในภาษาหนึ่งและข้อความในภาษาอื่นที่เป็นส่วนหนึ่งของการแปลรุ

264
00:18:44,271 --> 00:18:49,240
่นที่กำลังดำเนินอยู่ หรืออาจเป็นอินพุตเสียงของคำพูดและการถอดเสียงที่กำลังดำเนินอยู่

265
00:18:50,400 --> 00:18:52,700
หัวแบบ cross-attention มีลักษณะเกือบจะเหมือนกัน

266
00:18:52,980 --> 00:18:57,400
ข้อแตกต่างเพียงอย่างเดียวคือแมปคีย์และคิวรีทำงานกับชุดข้อมูลที่ต่างกัน

267
00:18:57,840 --> 00:19:04,133
ในแบบจำลองที่ทำการแปล คีย์อาจมาจากภาษาหนึ่ง ในขณะที่ข้อความค้นหามาจากอีกภาษาหนึ่ง 

268
00:19:04,133 --> 00:19:09,660
และรูปแบบความสนใจสามารถอธิบายว่าคำใดจากภาษาหนึ่งตรงกับคำใดในอีกภาษาหนึ่ง

269
00:19:10,340 --> 00:19:13,581
และในการตั้งค่านี้ โดยทั่วไปจะไม่มีการมาสก์ เนื่องจากไม่มีแนวคิดใด 

270
00:19:13,581 --> 00:19:16,340
ๆ เกี่ยวกับโทเค็นรุ่นหลังที่ส่งผลกระทบต่อโทเค็นรุ่นก่อน ๆ

271
00:19:17,180 --> 00:19:22,045
แต่หากคุณเข้าใจทุกอย่างจนถึงตอนนี้ และหากคุณหยุดอยู่แค่นี้ 

272
00:19:22,045 --> 00:19:25,180
คุณจะค้นพบแก่นแท้ของความสนใจที่แท้จริง

273
00:19:25,760 --> 00:19:31,440
สิ่งที่เหลืออยู่สำหรับเราก็คือการวางความรู้สึกที่คุณทำสิ่งนี้หลายๆ ครั้ง

274
00:19:32,100 --> 00:19:35,662
ในตัวอย่างหลักของเรา เรามุ่งเน้นไปที่คำคุณศัพท์ที่อัปเดตคำนาม 

275
00:19:35,662 --> 00:19:39,800
แต่แน่นอนว่ามีวิธีต่างๆ มากมายที่บริบทสามารถมีอิทธิพลต่อความหมายของคำได้

276
00:19:40,360 --> 00:19:46,520
ถ้าคำว่าชนกับคำว่ารถที่นำหน้าก็มีผลกระทบต่อรูปทรงและโครงสร้างของรถคันนั้นด้วย

277
00:19:47,200 --> 00:19:49,280
และการเชื่อมโยงหลายรายการอาจมีหลักไวยากรณ์น้อยกว่า

278
00:19:49,760 --> 00:19:55,175
หากคำว่าพ่อมดอยู่ในข้อความเดียวกับแฮร์รี่ ก็บ่งบอกว่าคำนี้อาจหมายถึงแฮร์รี่ 

279
00:19:55,175 --> 00:20:00,093
พอตเตอร์ แต่หากคำว่าราชินี ซัสเซ็กซ์ และวิลเลียมอยู่ในข้อความนั้นแทน 

280
00:20:00,093 --> 00:20:04,440
บางทีการฝังแฮร์รี่ก็ควรได้รับการอัปเดตแทน เพื่ออ้างถึงเจ้าชาย

281
00:20:05,040 --> 00:20:08,394
สำหรับการอัปเดตตามบริบททุกประเภทที่คุณอาจจินตนาการได้ 

282
00:20:08,394 --> 00:20:13,984
พารามิเตอร์ของคีย์และเมทริกซ์คิวรีเหล่านี้จะแตกต่างกันเพื่อจับรูปแบบความสนใจที่แตกต่างกัน 

283
00:20:13,984 --> 00:20:19,140
และพารามิเตอร์ของแผนผังคุณค่าของเราจะแตกต่างกันขึ้นอยู่กับสิ่งที่ควรเพิ่มลงในการฝัง

284
00:20:19,980 --> 00:20:24,249
และอีกครั้ง ในทางปฏิบัติพฤติกรรมที่แท้จริงของแผนที่เหล่านี้ตีความได้ยากกว่ามาก 

285
00:20:24,249 --> 00:20:27,600
โดยมีการกำหนดน้ำหนักให้ทำทุกอย่างที่แบบจำลองต้องการให้ทำเพื่อใ

286
00:20:27,600 --> 00:20:30,140
ห้บรรลุเป้าหมายในการทำนายโทเค็นถัดไปได้ดีที่สุด

287
00:20:31,400 --> 00:20:35,331
ดังที่ผมได้กล่าวไปแล้ว ทุกสิ่งที่เราอธิบายไว้นั้นเป็นความสนใจเพียงหัวเดียว 

288
00:20:35,331 --> 00:20:39,944
และบล็อกความสนใจเต็มรูปแบบภายในหม้อแปลงไฟฟ้าประกอบด้วยสิ่งที่เรียกว่าความสนใจแบบหลายหัว 

289
00:20:39,944 --> 00:20:42,617
โดยที่คุณดำเนินการต่างๆ มากมายเหล่านี้ไปพร้อมๆ กัน 

290
00:20:42,617 --> 00:20:45,920
โดยแต่ละรายการมีการสืบค้นคีย์ที่แตกต่างกันออกไป และแผนที่คุณค่า

291
00:20:47,420 --> 00:20:51,700
ตัวอย่างเช่น GPT-3 ใช้หัวความสนใจ 96 หัวในแต่ละบล็อก

292
00:20:52,020 --> 00:20:56,460
เมื่อพิจารณาว่าแต่ละรายการมีความสับสนเล็กน้อยอยู่แล้ว จึงมีเรื่องให้คิดมากมายในหัวของคุณ

293
00:20:56,760 --> 00:21:01,556
เพียงเพื่อสะกดให้ชัดเจน นั่นหมายความว่าคุณมีเมทริกซ์คีย์และคิวรีที่แตกต่างกัน 

294
00:21:01,556 --> 00:21:05,000
96 รายการ ซึ่งสร้างรูปแบบความสนใจที่แตกต่างกัน 96 รูปแบบ

295
00:21:05,440 --> 00:21:12,180
จากนั้นแต่ละหัวจะมีเมทริกซ์ค่าที่แตกต่างกันซึ่งใช้ในการสร้างลำดับเวกเตอร์ค่า 96 ลำดับ

296
00:21:12,460 --> 00:21:16,680
ทั้งหมดนี้นำมารวมกันโดยใช้รูปแบบความสนใจที่สอดคล้องกันเป็นน้ำหนัก

297
00:21:17,480 --> 00:21:21,209
ความหมายก็คือ สำหรับแต่ละตำแหน่งในบริบท แต่ละโทเค็น 

298
00:21:21,209 --> 00:21:27,020
แต่ละส่วนหัวเหล่านี้จะสร้างการเปลี่ยนแปลงที่เสนอเพื่อเพิ่มลงในการฝังในตำแหน่งนั้น

299
00:21:27,660 --> 00:21:31,570
ดังนั้นสิ่งที่คุณทำคือรวมการเปลี่ยนแปลงที่เสนอทั้งหมดเข้าด้วยกัน 

300
00:21:31,570 --> 00:21:35,480
หนึ่งรายการสำหรับแต่ละหัว และเพิ่มผลลัพธ์ลงในตำแหน่งเดิมที่ฝังไว้

301
00:21:36,660 --> 00:21:42,324
ผลรวมทั้งหมดนี้จะเป็นส่วนหนึ่งจากสิ่งที่ส่งออกมาจากบล็อกความสนใจแบบหลายหัว 

302
00:21:42,324 --> 00:21:47,460
ซึ่งเป็นชิ้นเดียวของการฝังแบบละเอียดที่โผล่ออกมาที่ปลายอีกด้านของมัน

303
00:21:48,320 --> 00:21:52,140
ขอย้ำอีกครั้งว่านี่เป็นเรื่องที่ต้องคิดมาก ดังนั้นอย่ากังวลเลยหากต้องใช้เวลาพอสมควร

304
00:21:52,380 --> 00:21:56,130
แนวคิดโดยรวมก็คือ การใช้หัวที่แตกต่างกันหลายๆ หัวพร้อมกัน 

305
00:21:56,130 --> 00:22:01,820
จะทำให้โมเดลมีความสามารถในการเรียนรู้วิธีที่แตกต่างกันมากมายที่ทำให้บริบทเปลี่ยนความหมาย

306
00:22:03,700 --> 00:22:08,722
เมื่อดึงการนับพารามิเตอร์ของเราขึ้นมาด้วย 96 หัว ซึ่งแต่ละชุดรวมเมทริกซ์ทั้ง 4 

307
00:22:08,722 --> 00:22:14,126
แบบที่แตกต่างกันออกไป แต่ละบล็อกของความสนใจแบบหลายหัวจะจบลงด้วยพารามิเตอร์ประมาณ 600 

308
00:22:14,126 --> 00:22:15,080
ล้านพารามิเตอร์

309
00:22:16,420 --> 00:22:19,083
มีสิ่งหนึ่งที่น่ารำคาญเพิ่มเติมเล็กน้อยที่ฉันควรพูด

310
00:22:19,083 --> 00:22:21,800
ถึงสำหรับทุกคนที่อ่านเพิ่มเติมเกี่ยวกับหม้อแปลงไฟฟ้า

311
00:22:22,080 --> 00:22:26,627
คุณจำได้ไหมที่ฉันบอกว่าแผนผังค่าถูกแยกออกเป็นเมทริกซ์ที่แตกต่างกันสองตัวนี้ 

312
00:22:26,627 --> 00:22:29,440
ซึ่งฉันเรียกว่าเมทริกซ์ค่าลงและค่าเมทริกซ์เพิ่ม

313
00:22:29,960 --> 00:22:35,881
วิธีที่ฉันวางกรอบสิ่งต่างๆ อาจแนะนำให้คุณเห็นเมทริกซ์คู่นี้ในหัวความสนใจแต่ละหัว 

314
00:22:35,881 --> 00:22:38,440
และคุณก็สามารถนำไปใช้ในลักษณะนี้ได้

315
00:22:38,640 --> 00:22:39,920
นั่นจะเป็นการออกแบบที่ถูกต้อง

316
00:22:40,260 --> 00:22:44,920
แต่วิธีที่คุณเห็นสิ่งนี้เขียนในเอกสาร และวิธีการนำไปใช้ในทางปฏิบัติ ดูแตกต่างออกไปเล็กน้อย

317
00:22:45,340 --> 00:22:50,828
เมทริกซ์การเพิ่มมูลค่าทั้งหมดนี้สำหรับแต่ละหัวจะปรากฏถูกเย็บเข้าด้วยกันในเมทริกซ์ขนาดยั

318
00:22:50,828 --> 00:22:56,380
กษ์ตัวเดียวที่เราเรียกว่าเมทริกซ์เอาท์พุต ซึ่งเชื่อมโยงกับบล็อกความสนใจแบบหลายหัวทั้งหมด

319
00:22:56,820 --> 00:23:00,651
และเมื่อคุณเห็นคนอ้างถึงเมทริกซ์ค่าสำหรับหัวความสนใจที่กำหนด, 

320
00:23:00,651 --> 00:23:03,864
โดยทั่วไปแล้ว พวกเขาจะหมายถึงขั้นตอนแรกนี้เท่านั้น, 

321
00:23:03,864 --> 00:23:07,140
ขั้นตอนที่ผมเรียกว่าเป็นค่าที่ลดลงลงในพื้นที่ขนาดเล็ก

322
00:23:08,340 --> 00:23:11,040
สำหรับผู้ที่อยากรู้อยากเห็น ฉันได้ฝากข้อความไว้บนหน้าจอไว้

323
00:23:11,260 --> 00:23:14,510
เป็นหนึ่งในรายละเอียดที่เสี่ยงต่อการหันเหความสนใจไปจากประเด็นหลักทางแนวคิด 

324
00:23:14,510 --> 00:23:18,149
แต่ฉันอยากจะแจ้งให้ทราบเพียงเพื่อให้คุณทราบว่าคุณได้อ่านเกี่ยวกับเรื่องนี้จากแหล่งอื

325
00:23:18,149 --> 00:23:18,540
่นหรือไม่

326
00:23:19,240 --> 00:23:23,671
นอกเหนือจากความแตกต่างทางเทคนิคทั้งหมดแล้ว ในการดูตัวอย่างจากบทที่แล้ว 

327
00:23:23,671 --> 00:23:28,040
เราเห็นว่าข้อมูลที่ไหลผ่านหม้อแปลงไม่เพียงแค่ไหลผ่านบล็อกความสนใจเดียว

328
00:23:28,640 --> 00:23:32,700
ประการหนึ่ง มันยังต้องผ่านการดำเนินการอื่นๆ ที่เรียกว่าการรับรู้แบบหลายชั้นด้วย

329
00:23:33,120 --> 00:23:34,880
เราจะพูดถึงสิ่งเหล่านั้นเพิ่มเติมในบทถัดไป

330
00:23:35,180 --> 00:23:39,320
จากนั้นมันก็ผ่านการดำเนินการทั้งสองนี้ซ้ำไปซ้ำมาหลายชุด

331
00:23:39,980 --> 00:23:43,874
ความหมายก็คือ หลังจากที่คำใดคำหนึ่งซึมซับบริบทบางส่วนไปแล้ว 

332
00:23:43,874 --> 00:23:48,871
ก็มีโอกาสอีกมากมายที่การฝังที่ละเอียดยิ่งขึ้นนี้จะได้รับอิทธิพลจากสภาพแวดล้อม

333
00:23:48,871 --> 00:23:50,040
ที่เหมาะสมยิ่งขึ้น

334
00:23:50,940 --> 00:23:54,704
ยิ่งคุณไปไกลจากเครือข่ายเท่าไร การฝังแต่ละครั้งจะมีความหมายมากขึ้นเรื่อยๆ 

335
00:23:54,704 --> 00:23:58,926
จากการฝังอื่นๆ ทั้งหมด ซึ่งตัวมันเองกำลังได้รับการปรับปรุงให้เหมาะสมมากขึ้นเรื่อยๆ 

336
00:23:58,926 --> 00:24:01,927
ความหวังก็คือว่าจะมีความสามารถในการเข้ารหัสระดับที่สูงขึ้น 

337
00:24:01,927 --> 00:24:04,573
และแนวคิดที่เป็นนามธรรมมากขึ้นเกี่ยวกับสิ่งที่กำหนด 

338
00:24:04,573 --> 00:24:07,320
ข้อมูลเข้าที่นอกเหนือไปจากคำอธิบายและโครงสร้างไวยากรณ์

339
00:24:07,880 --> 00:24:10,947
สิ่งต่างๆ เช่น ความรู้สึกและน้ำเสียง ไม่ว่าจะเป็นบทกวี 

340
00:24:10,947 --> 00:24:15,130
และความจริงทางวิทยาศาสตร์ที่ซ่อนอยู่เกี่ยวข้องกับงานชิ้นนี้และอะไรทำนองนั้น

341
00:24:16,700 --> 00:24:22,306
ย้อนกลับไปอีกครั้งที่การเก็บคะแนนของเรา GPT-3 มีเลเยอร์ที่แตกต่างกัน 96 เลเยอร์ 

342
00:24:22,306 --> 00:24:26,861
ดังนั้นจำนวนรวมของการสืบค้นคีย์และพารามิเตอร์ค่าจึงคูณด้วยอีก 96 

343
00:24:26,861 --> 00:24:32,748
ซึ่งทำให้ผลรวมทั้งหมดเหลือน้อยกว่า 58 พันล้านพารามิเตอร์ที่ต่างกันซึ่งใช้สำหรับพาราม

344
00:24:32,748 --> 00:24:34,500
ิเตอร์ทั้งหมด หัวความสนใจ

345
00:24:34,980 --> 00:24:38,657
นั่นเป็นสิ่งที่ต้องแน่ใจมาก แต่มีเพียงประมาณหนึ่งในสามของ 

346
00:24:38,657 --> 00:24:40,940
175 พันล้านที่อยู่ในเครือข่ายทั้งหมด

347
00:24:41,520 --> 00:24:44,830
ดังนั้นแม้ว่าความสนใจจะได้รับความสนใจทั้งหมด แต่พาราม

348
00:24:44,830 --> 00:24:48,140
ิเตอร์ส่วนใหญ่มาจากบล็อกที่อยู่ระหว่างขั้นตอนเหล่านี้

349
00:24:48,560 --> 00:24:51,549
ในบทถัดไป คุณและฉันจะพูดคุยเพิ่มเติมเกี่ยวกับช่วงอื่นๆ 

350
00:24:51,549 --> 00:24:53,560
เหล่านั้นและกระบวนการฝึกอบรมอีกมากมาย

351
00:24:54,120 --> 00:24:59,359
ส่วนสำคัญของเรื่องราวความสำเร็จของกลไกความสนใจนั้นไม่ใช่พฤติกรรมเฉพาะเจาะจงใดๆ 

352
00:24:59,359 --> 00:25:03,339
มากนัก แต่เป็นความจริงที่ว่ากลไกนี้สามารถขนานกันได้อย่างมาก 

353
00:25:03,339 --> 00:25:08,380
ซึ่งหมายความว่าคุณสามารถเรียกใช้การคำนวณจำนวนมากได้ในเวลาอันสั้นโดยใช้ GPU .

354
00:25:09,460 --> 00:25:13,295
เนื่องจากหนึ่งในบทเรียนสำคัญเกี่ยวกับการเรียนรู้เชิงลึกในช่วงทศวรรษหรือสองปีที่ผ่า

355
00:25:13,295 --> 00:25:17,130
นมาคือขนาดเพียงอย่างเดียวที่ดูเหมือนว่าจะให้การปรับปรุงเชิงคุณภาพอย่างมากในประสิทธ

356
00:25:17,130 --> 00:25:21,060
ิภาพของโมเดล มีข้อได้เปรียบอย่างมากสำหรับสถาปัตยกรรมแบบขนานที่ช่วยให้คุณทำเช่นนี้ได้

357
00:25:22,040 --> 00:25:25,340
หากคุณต้องการเรียนรู้เพิ่มเติมเกี่ยวกับสิ่งนี้ ฉันได้ทิ้งลิงก์จำนวนมากไว้ในคำอธิบาย

358
00:25:25,920 --> 00:25:28,338
โดยเฉพาะอย่างยิ่ง อะไรก็ตามที่ผลิตโดย Andrej Karpathy 

359
00:25:28,338 --> 00:25:30,040
หรือ Chris Ola มักจะเป็นทองคำบริสุทธิ์

360
00:25:30,560 --> 00:25:33,227
ในวิดีโอนี้ ฉันอยากจะกระโจนเข้าสู่ความสนใจในรูปแบบปัจจุบัน 

361
00:25:33,227 --> 00:25:36,798
แต่ถ้าคุณอยากรู้เพิ่มเติมเกี่ยวกับประวัติความเป็นมาว่าเรามาถึงที่นี่ได้อย่างไร 

362
00:25:36,798 --> 00:25:39,917
และคุณจะสร้างสรรค์แนวคิดนี้ขึ้นมาใหม่ให้กับตัวคุณเองได้อย่างไร วิเวก 

363
00:25:39,917 --> 00:25:42,540
เพื่อนของฉันก็แค่เสนอสองสามข้อ วิดีโอที่ให้แรงจูงใจมากขึ้น

364
00:25:43,120 --> 00:25:45,493
นอกจากนี้ Britt Cruz จากช่อง The Art of the Problem 

365
00:25:45,493 --> 00:25:48,460
ยังมีวิดีโอที่ดีมากเกี่ยวกับประวัติความเป็นมาของโมเดลภาษาขนาดใหญ่

366
00:26:04,960 --> 00:26:09,200
ขอบคุณ

