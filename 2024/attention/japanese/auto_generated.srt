1
00:00:00,000 --> 00:00:04,019
前章では、トランスの内部構造について説明した。

2
00:00:04,560 --> 00:00:07,379
これは、大規模な言語モデルや、現代のAIの波における

3
00:00:07,379 --> 00:00:10,200
他の多くのツールの中にある、重要な技術の1つである。

4
00:00:10,980 --> 00:00:14,553
そしてこの章では、この注意メカニズムがどのよ

5
00:00:14,553 --> 00:00:18,126
うにデータを処理するのかを可視化しながら、こ

6
00:00:18,126 --> 00:00:21,700
の注意メカニズムが何なのかを掘り下げていく。

7
00:00:26,140 --> 00:00:27,840
簡単な復習として、私が心に留めて

8
00:00:27,840 --> 00:00:29,540
おいてほしい重要な文脈はこうだ。

9
00:00:30,000 --> 00:00:32,970
あなたと私が勉強しているモデルの目標は、テキストの

10
00:00:32,970 --> 00:00:36,060
一部分を取り込み、次に来る単語を予測することである。

11
00:00:36,860 --> 00:00:40,185
入力テキストはトークンと呼ばれる小さな断片に分割さ

12
00:00:40,185 --> 00:00:43,510
れ、それらは単語や単語の一部であることが非常に多い

13
00:00:43,510 --> 00:00:46,835
が、このビデオの例をあなたや私に考えやすくするため

14
00:00:46,835 --> 00:00:50,560
に、トークンは常に単語であると仮定して単純化してみよう。

15
00:00:51,480 --> 00:00:54,590
変換器の最初のステップは、各トークンを

16
00:00:54,590 --> 00:00:57,700
高次元ベクトルに関連付けることである。

17
00:00:57,700 --> 00:01:02,271
最も重要な考え方は、この高次元空間のあらゆる埋め込み可能な

18
00:01:02,271 --> 00:01:07,000
方向が、意味論的な意味とどのように対応しうるかということだ。

19
00:01:07,680 --> 00:01:11,619
前章では、方向が性別に対応する例を見たが、それは、この空

20
00:01:11,619 --> 00:01:15,559
間にあるステップを追加することで、男性名詞の埋め込みから

21
00:01:15,559 --> 00:01:19,640
、対応する女性名詞の埋め込みに移行できるという意味である。

22
00:01:20,160 --> 00:01:22,633
これはほんの一例に過ぎないが、この高次元空間

23
00:01:22,633 --> 00:01:25,106
における他の多くの方向が、単語の意味の他の数

24
00:01:25,106 --> 00:01:27,580
多くの側面に対応しうることは想像に難くない。

25
00:01:28,800 --> 00:01:32,260
トランスフォーマーの目的は、これらの埋め込みを段階的に調整す

26
00:01:32,260 --> 00:01:35,720
ることで、単に個々の単語をエンコードするだけでなく、その代わ

27
00:01:35,720 --> 00:01:39,180
りにもっともっと豊かな文脈的意味を埋め込むようにすることだ。

28
00:01:40,140 --> 00:01:44,482
先に言っておくが、多くの人は、トランスの重要な部品である

29
00:01:44,482 --> 00:01:48,980
アテンション・メカニズムが非常にわかりにくいと感じている。

30
00:01:49,440 --> 00:01:54,214
計算の詳細やすべての行列の乗算に飛び込む前に、私たちが注

31
00:01:54,214 --> 00:01:59,160
目してほしい動作の例をいくつか考えておく価値があると思う。

32
00:02:00,140 --> 00:02:03,105
アメリカの真のほくろ、二酸化炭素の1モル

33
00:02:03,105 --> 00:02:06,220
というフレーズを考え、ほくろの生検を行う。

34
00:02:06,700 --> 00:02:08,800
モグラという言葉が、文脈によってそれぞれ異な

35
00:02:08,800 --> 00:02:10,900
る意味を持つことは、あなたも私も知っている。

36
00:02:11,360 --> 00:02:14,286
しかし、変換器の最初のステップ、つまりテキストを分割

37
00:02:14,286 --> 00:02:17,213
して各トークンをベクトルに関連付けるステップの後では

38
00:02:17,213 --> 00:02:20,140
、モグラに関連付けられるベクトルはこれらのケースです

39
00:02:20,140 --> 00:02:23,067
べて同じになる。なぜなら、この最初のトークン埋め込み

40
00:02:23,067 --> 00:02:26,220
は事実上、文脈を参照しないルックアップテーブルだからだ。

41
00:02:26,620 --> 00:02:29,791
周囲の埋め込みがこの埋め込みに情報を渡すチャン

42
00:02:29,791 --> 00:02:33,100
スがあるのは、変換器の次のステップのときだけだ。

43
00:02:33,820 --> 00:02:39,668
そして、よく訓練されたアテンション・ブロックは、一般的

44
00:02:39,668 --> 00:02:45,517
な埋め込みに何を付け加えれば、文脈の関数として、これら

45
00:02:45,517 --> 00:02:51,800
の特定の方向のひとつに移動させることができるかを計算する。

46
00:02:53,300 --> 00:02:56,180
別の例として、towerという単語の埋め込みを考えてみよう。

47
00:02:57,060 --> 00:03:00,390
これはおそらく、他の多くの大きくて背の高い名詞に関連す

48
00:03:00,390 --> 00:03:03,720
る、非常に一般的で特定的でない空間の方向性なのだろう。

49
00:03:04,020 --> 00:03:07,780
もしこの単語の直前にエッフェル塔があったとしたら、このベ

50
00:03:07,780 --> 00:03:11,540
クトルを更新して、より具体的にエッフェル塔を意味する方向

51
00:03:11,540 --> 00:03:15,300
に向け、パリやフランス、鉄でできたものに関連するベクトル

52
00:03:15,300 --> 00:03:19,060
と相関させるようなメカニズムにしたいと想像できるだろう。

53
00:03:19,920 --> 00:03:22,405
もしミニチュアという言葉が先行するのであ

54
00:03:22,405 --> 00:03:24,890
れば、ベクトルはさらに更新され、大きくて

55
00:03:24,890 --> 00:03:27,500
背の高いものとの相関関係がなくなるはずだ。

56
00:03:29,480 --> 00:03:34,086
アテンション・ブロックは、単に単語の意味を洗練させる

57
00:03:34,086 --> 00:03:38,693
だけでなく、あるエンベッディングにエンコードされた情

58
00:03:38,693 --> 00:03:43,300
報を、別のエンベッディングに移動させることができる。

59
00:03:43,300 --> 00:03:46,935
前章で見たのは、多くの異なるアテンション・ブロック

60
00:03:46,935 --> 00:03:50,571
を含むすべてのベクトルがネットワークを流れた後、次

61
00:03:50,571 --> 00:03:54,207
のトークンの予測を生成するために実行する計算が、完

62
00:03:54,207 --> 00:03:58,280
全にシーケンスの最後のベクトルの関数であるということだ。

63
00:03:59,100 --> 00:04:03,450
たとえば、入力されたテキストが推理小説の大部分で、

64
00:04:03,450 --> 00:04:07,800
最後のほうにある「したがって、犯人はこうであった。

65
00:04:08,400 --> 00:04:13,355
このモデルが次の単語を正確に予測しようとするならば、最初

66
00:04:13,355 --> 00:04:18,310
は単にwasという単語を埋め込んでいた配列の最後のベクト

67
00:04:18,310 --> 00:04:23,265
ルが、すべての注意ブロックによって更新され、個々の単語よ

68
00:04:23,265 --> 00:04:28,220
りもはるかに多くの情報を表すようにならなければならない。

69
00:04:29,500 --> 00:04:30,993
しかし、計算を段階的に進めるため

70
00:04:30,993 --> 00:04:32,580
に、もっと単純な例を挙げてみよう。

71
00:04:32,980 --> 00:04:37,960
ふわふわした青い生き物が緑豊かな森を歩き回っていた。

72
00:04:38,460 --> 00:04:42,531
とりあえず、私たちが気にする唯一の更新は、形容

73
00:04:42,531 --> 00:04:46,780
詞が対応する名詞の意味を調整することだとしよう。

74
00:04:47,000 --> 00:04:49,772
これから説明するのは、私たちが注意の単一ヘッドと呼んで

75
00:04:49,772 --> 00:04:52,544
いるもので、注意のブロックがいかに多くの異なるヘッドか

76
00:04:52,544 --> 00:04:55,420
ら構成され、並行して実行されているかは後でわかるだろう。

77
00:04:56,140 --> 00:04:59,694
繰り返しになるが、各単語の初期埋め込みは、文脈のない特

78
00:04:59,694 --> 00:05:03,380
定の単語の意味だけをエンコードする高次元ベクトルである。

79
00:05:04,000 --> 00:05:05,220
実は、それはちょっと違う。

80
00:05:05,380 --> 00:05:07,640
また、単語の位置も符号化される。

81
00:05:07,980 --> 00:05:10,627
位置がエンコードされる方法についてはもっとたくさ

82
00:05:10,627 --> 00:05:13,274
んあるが、今知っておく必要があるのは、このベクト

83
00:05:13,274 --> 00:05:15,921
ルのエントリーが、その単語が何であり、文脈のどこ

84
00:05:15,921 --> 00:05:18,900
に存在するかを教えてくれるのに十分だということだけだ。

85
00:05:19,500 --> 00:05:21,660
これらの埋め込みをeで表してみよう。

86
00:05:22,420 --> 00:05:26,086
目標は、一連の計算によって、例えば名詞に対応する

87
00:05:26,086 --> 00:05:29,753
ものが対応する形容詞から意味を取り込んだ、新しい

88
00:05:29,753 --> 00:05:33,420
洗練された埋め込みセットを生成させることである。

89
00:05:33,900 --> 00:05:38,817
ディープラーニングのゲームでは、ほとんど

90
00:05:38,817 --> 00:05:43,980
の計算を行列とベクトルの積のようにしたい。

91
00:05:44,660 --> 00:05:47,124
誤解のないように言っておくが、形容詞が名詞を更新

92
00:05:47,124 --> 00:05:49,589
するというこの例は、アテンション・ヘッドがどのよ

93
00:05:49,589 --> 00:05:52,260
うな行動を取るかを想像してもらうために作ったものだ。

94
00:05:52,860 --> 00:05:54,979
多くのディープラーニングがそうであるように、真の挙

95
00:05:54,979 --> 00:05:57,100
動を解析するのはずっと難しい。なぜなら、あるコスト

96
00:05:57,100 --> 00:05:59,220
関数を最小化するために、膨大な数のパラメーターを微

97
00:05:59,220 --> 00:06:01,340
調整し、チューニングすることに基づいているからだ。

98
00:06:01,680 --> 00:06:05,526
ただ、このプロセスに関係するパラメータで満たされたさまざま

99
00:06:05,526 --> 00:06:09,373
なマトリックスをステップを踏んでいく中で、より具体的にする

100
00:06:09,373 --> 00:06:13,220
ために、何かできることの想像例があると本当に助かると思う。

101
00:06:14,140 --> 00:06:18,049
このプロセスの最初のステップでは、クリーチャー

102
00:06:18,049 --> 00:06:21,960
のような名詞が、「ねえ、私の前に形容詞はある？

103
00:06:22,160 --> 00:06:24,061
そして、フワフワとブルーという言葉に対し

104
00:06:24,061 --> 00:06:25,963
て、それぞれが答えられるように、そう、僕

105
00:06:25,963 --> 00:06:27,960
は形容詞で、そういうポジションにいるんだ。

106
00:06:28,960 --> 00:06:32,421
その質問は、別のベクトル、別の数

107
00:06:32,421 --> 00:06:36,100
字のリストとしてエンコードされる。

108
00:06:36,980 --> 00:06:39,500
しかし、このクエリーベクトルは、埋め込み

109
00:06:39,500 --> 00:06:42,020
ベクトルよりもはるかに小さい次元を持つ。

110
00:06:42,940 --> 00:06:46,295
このクエリを計算するのは、ある行列（ここではwqと呼

111
00:06:46,295 --> 00:06:49,780
ぶ）を取り出し、それに埋め込みを乗算するようなものだ。

112
00:06:50,960 --> 00:06:55,573
このように矢印の横に行列を置くのは、この行

113
00:06:55,573 --> 00:07:00,186
列に矢印の始点のベクトルを掛けると、矢印の

114
00:07:00,186 --> 00:07:04,800
終点のベクトルが得られることを表している。

115
00:07:05,860 --> 00:07:09,220
この場合、この行列にコンテキスト内のすべての埋め込みを掛け

116
00:07:09,220 --> 00:07:12,580
合わせ、各トークンに対して1つのクエリベクトルを生成する。

117
00:07:13,740 --> 00:07:16,892
この行列のエントリーはモデルのパラメータであり、つま

118
00:07:16,892 --> 00:07:18,953
り真の挙動はデータから学習される。

119
00:07:18,953 --> 00:07:22,106
実際には、特定のアテンションヘッドでこの行列が何をす

120
00:07:22,106 --> 00:07:23,440
るのかは解析が難しい。

121
00:07:23,900 --> 00:07:26,728
しかし、私たちのために、このクエリ行列が学習するこ

122
00:07:26,728 --> 00:07:28,877
とを期待するような例を想像してみよう。

123
00:07:28,877 --> 00:07:31,705
このクエリ行列は、名詞の埋め込みをこの小さなクエリ

124
00:07:31,705 --> 00:07:34,533
空間のある方向にマッピングし、先行する位置にある形

125
00:07:34,533 --> 00:07:37,361
容詞を探すという概念を何らかの形でコード化している

126
00:07:37,361 --> 00:07:38,040
と仮定する。

127
00:07:38,780 --> 00:07:41,440
それが他の埋め込みにどう影響するかは、誰にもわからない。

128
00:07:41,720 --> 00:07:42,990
もしかしたら、同時に他の目標も達

129
00:07:42,990 --> 00:07:44,340
成しようとしているのかもしれない。

130
00:07:44,540 --> 00:07:47,160
今は名詞に集中している。

131
00:07:47,280 --> 00:07:50,950
同時に、これと関連しているのがキー行列と呼ばれる2番

132
00:07:50,950 --> 00:07:54,620
目の行列で、これも埋め込みの1つ1つを掛け合わせる。

133
00:07:55,280 --> 00:07:58,500
これにより、キーと呼ぶ2番目のベクトル列が生成される。

134
00:07:59,420 --> 00:08:03,140
概念的には、キーはクエリーに答える可能性があると考えたい。

135
00:08:03,840 --> 00:08:06,291
このキーマトリックスも調整可能なパラメーターでい

136
00:08:06,291 --> 00:08:08,743
っぱいで、クエリーマトリックスと同じように、埋め

137
00:08:08,743 --> 00:08:11,400
込みベクトルをより小さな次元の空間にマッピングする。

138
00:08:12,200 --> 00:08:14,610
キーとクエリが密接に一致するときは、

139
00:08:14,610 --> 00:08:17,020
キーがクエリに一致すると考えるのだ。

140
00:08:17,460 --> 00:08:20,553
この例では、キーマトリクスはfluffyやblueといった

141
00:08:20,553 --> 00:08:23,646
形容詞を、creatureという単語が生成するクエリと密接

142
00:08:23,646 --> 00:08:26,740
に整合するベクトルにマッピングしていると想像できるだろう。

143
00:08:27,200 --> 00:08:30,600
各キーが各クエリにどの程度マッチするかを測定するために、

144
00:08:30,600 --> 00:08:34,000
可能性のある各キーとクエリのペア間のドット積を計算する。

145
00:08:34,480 --> 00:08:38,416
大きなドットが大きなドット積、つまりキ

146
00:08:38,416 --> 00:08:42,559
ーとクエリが整列する場所に対応している。

147
00:08:43,280 --> 00:08:48,293
もし、fluffyとblueが生成するキーが、crea

148
00:08:48,293 --> 00:08:53,306
tureが生成するクエリと本当に密接に一致するのであれ

149
00:08:53,306 --> 00:08:58,320
ば、この2つの点のドット積は大きな正の数になるだろう。

150
00:08:59,100 --> 00:09:02,204
機械学習の専門用語では、これはふわふわと青の埋め込みがク

151
00:09:02,204 --> 00:09:05,420
リーチャーの埋め込みに関与することを意味すると言うだろう。

152
00:09:06,040 --> 00:09:09,511
対照的に、theのような他の単語のキーとcrea

153
00:09:09,511 --> 00:09:12,983
tureのクエリとの間のドット積は、互いに無関係

154
00:09:12,983 --> 00:09:16,600
であることを反映する、小さいか負の値になるだろう。

155
00:09:17,700 --> 00:09:21,244
つまり、負の無限大から無限大までの実数値をグリッ

156
00:09:21,244 --> 00:09:24,788
ド状に並べ、各単語が他のすべての単語の意味を更新

157
00:09:24,788 --> 00:09:28,480
するのにどれだけ関連性があるかをスコアで示すのだ。

158
00:09:29,200 --> 00:09:32,490
これらのスコアの使い方は、各列に沿って、関連性

159
00:09:32,490 --> 00:09:35,780
によって重み付けされた合計を取るというものだ。

160
00:09:36,520 --> 00:09:40,406
つまり、負の無限大から無限大までの値を持つ代わりに、私たち

161
00:09:40,406 --> 00:09:44,293
が望むのは、これらの列の数値が0から1の間であり、あたかも

162
00:09:44,293 --> 00:09:48,180
確率分布であるかのように、各列を足して1になることである。

163
00:09:49,280 --> 00:09:50,702
前章からの参加者なら、その時に

164
00:09:50,702 --> 00:09:52,220
何をすべきか分かっているはずだ。

165
00:09:52,620 --> 00:09:57,300
それぞれの列に沿ってソフトマックスを計算し、値を正規化する。

166
00:10:00,060 --> 00:10:02,960
この写真では、すべての列にソフトマックスを適用し

167
00:10:02,960 --> 00:10:05,860
た後、これらの正規化された値でグリッドを埋める。

168
00:10:06,780 --> 00:10:10,613
この時点で、各列は、左側の単語が上側の対応する値にどれだけ

169
00:10:10,613 --> 00:10:14,580
関連しているかによって重みを与えていると考えてもいいだろう。

170
00:10:15,080 --> 00:10:16,840
私たちはこのグリッドをアテンション・パターンと呼んでいる。

171
00:10:18,080 --> 00:10:20,450
オリジナルのトランスフォーマーの論文を

172
00:10:20,450 --> 00:10:22,820
見ると、実にコンパクトに書かれている。

173
00:10:23,880 --> 00:10:29,122
ここで変数qとkは、それぞれクエリーベ

174
00:10:29,122 --> 00:10:34,640
クトルとキーベクトルの完全な配列を表す。

175
00:10:35,160 --> 00:10:39,089
この分子式は、キーとクエリーのペア間のドット積の

176
00:10:39,089 --> 00:10:43,020
グリッドを表現する、実にコンパクトな方法である。

177
00:10:44,000 --> 00:10:47,320
技術的な細かいことだが、数値的に安定させる

178
00:10:47,320 --> 00:10:50,640
ためには、これらの値をすべてそのキークエリ

179
00:10:50,640 --> 00:10:53,960
ー空間の次元の平方根で割るのが有効である。

180
00:10:54,480 --> 00:10:57,640
そして、このソフトマックスは、列

181
00:10:57,640 --> 00:11:00,800
ごとに適用されることを意味する。

182
00:11:01,640 --> 00:11:04,700
そのVタームについては、すぐ後で話そう。

183
00:11:05,020 --> 00:11:08,460
その前に、もうひとつ技術的なディテールがある。

184
00:11:09,040 --> 00:11:12,688
学習プロセスにおいて、与えられたテキスト例に対してこのモデ

185
00:11:12,688 --> 00:11:16,336
ルを実行し、すべての重みをわずかに調整し、パッセージ内の真

186
00:11:16,336 --> 00:11:19,985
の次の単語に割り当てる確率の高さに基づいて報酬を与えたり罰

187
00:11:19,985 --> 00:11:23,633
を与えたりするように調整すると、このパッセージ内の各トーク

188
00:11:23,633 --> 00:11:27,282
ンの最初の続きに続く可能性のあるすべての次のトークンを同時

189
00:11:27,282 --> 00:11:30,930
に予測させると、学習プロセス全体が非常に効率的になることが

190
00:11:30,930 --> 00:11:31,560
判明した。

191
00:11:31,940 --> 00:11:34,256
例えば、私たちが注目しているフレーズでは、c

192
00:11:34,256 --> 00:11:36,572
reatureの後にどんな単語が続き、the

193
00:11:36,572 --> 00:11:39,100
の後にどんな単語が続くかを予測することもできる。

194
00:11:39,940 --> 00:11:41,813
これは本当に素晴らしいことで、そうでなければ

195
00:11:41,813 --> 00:11:43,686
1つのトレーニング例となるものが、効果的に多

196
00:11:43,686 --> 00:11:45,560
くのトレーニング例として機能することになる。

197
00:11:46,100 --> 00:11:50,985
私たちのアテンション・パターンの目的からすれば、後の言葉が

198
00:11:50,985 --> 00:11:56,040
前の言葉に影響を与えることを決して許さないということである。

199
00:11:56,560 --> 00:11:59,207
これが意味するのは、ここにあるすべてのスポット、つまり

200
00:11:59,207 --> 00:12:01,854
後のトークンが前のトークンに影響を与えたことを表すスポ

201
00:12:01,854 --> 00:12:04,600
ットを、何らかの方法で強制的にゼロにしたいということだ。

202
00:12:05,920 --> 00:12:09,170
一番簡単なのは、これらの列をゼロにすることだが、そうする

203
00:12:09,170 --> 00:12:12,420
と列の足し算が1でなくなり、正規化されなくなってしまう。

204
00:12:13,120 --> 00:12:16,016
その代わりに、ソフトマックスを適用する前に、これらのエ

205
00:12:16,016 --> 00:12:19,020
ントリーをすべて負の無限大に設定するのが一般的な方法だ。

206
00:12:19,680 --> 00:12:22,430
そうすれば、ソフトマックスを適用した後、それらの値

207
00:12:22,430 --> 00:12:25,180
はすべてゼロになるが、列は正規化されたままになる。

208
00:12:26,000 --> 00:12:27,540
このプロセスはマスキングと呼ばれる。

209
00:12:27,540 --> 00:12:30,988
しかし、GPTの例では、例えばチャットボットとして実行

210
00:12:30,988 --> 00:12:34,436
する場合などよりも、トレーニング段階の方がより関連性が

211
00:12:34,436 --> 00:12:37,884
高いにもかかわらず、後のトークンが前のトークンに影響を

212
00:12:37,884 --> 00:12:41,460
与えるのを防ぐために、常にこのマスキングを適用している。

213
00:12:42,480 --> 00:12:44,748
このアテンション・パターンについて考える価

214
00:12:44,748 --> 00:12:47,016
値のあるもうひとつの事実は、その大きさがコ

215
00:12:47,016 --> 00:12:49,500
ンテクストの大きさの2乗に等しいということだ。

216
00:12:49,900 --> 00:12:52,690
コンテキストの大きさは、大規模な言語モデ

217
00:12:52,690 --> 00:12:55,620
ルにとって本当に大きなボトルネックになる。

218
00:12:56,300 --> 00:12:59,220
ご想像の通り、コンテキスト・ウィンドウをより大きくし

219
00:12:59,220 --> 00:13:02,141
たいという欲求に突き動かされ、近年、コンテキストをよ

220
00:13:02,141 --> 00:13:05,062
りスケーラブルにすることを目的としたアテンション・メ

221
00:13:05,062 --> 00:13:08,320
カニズムにいくつかのバリエーションが見られるようになった。

222
00:13:10,560 --> 00:13:12,975
なるほど、このパターンを計算することで、モデルはどの単

223
00:13:12,975 --> 00:13:15,480
語が他のどの単語に関連しているかを推測することができる。

224
00:13:16,020 --> 00:13:19,410
あとは実際に埋め込みを更新して、単語が関連する他

225
00:13:19,410 --> 00:13:22,800
のどの単語にも情報を渡せるようにする必要がある。

226
00:13:22,800 --> 00:13:25,672
例えば、「ふわふわ」を埋め込むことによって、「クリ

227
00:13:25,672 --> 00:13:28,545
ーチャー」に何らかの変化を与え、「ふわふわ」のクリ

228
00:13:28,545 --> 00:13:31,417
ーチャーをより具体的にエンコードする、この12,0

229
00:13:31,417 --> 00:13:34,520
00次元の埋め込み空間の別の部分に移動させたいとする。

230
00:13:35,460 --> 00:13:38,745
ここでは、まず、最も簡単な方法をお見せしよう。

231
00:13:38,745 --> 00:13:42,745
ただし、多頭注目という文脈では、この方法を少し修正する必

232
00:13:42,745 --> 00:13:43,460
要がある。

233
00:13:44,080 --> 00:13:48,260
この最も簡単な方法は、3番目の行列、いわゆる値行列を使うこと

234
00:13:48,260 --> 00:13:52,440
で、これに最初の単語の埋め込み、例えばFluffyを掛ける。

235
00:13:53,300 --> 00:13:56,092
この結果は値ベクトルと呼ばれるもので、これは2

236
00:13:56,092 --> 00:13:58,884
番目の単語の埋め込みに追加するもので、この場合

237
00:13:58,884 --> 00:14:01,920
はCreatureの埋め込みに追加するものである。

238
00:14:02,600 --> 00:14:04,800
つまりこの値ベクトルは、埋め込みと

239
00:14:04,800 --> 00:14:07,000
同じ非常に高次元の空間に存在する。

240
00:14:07,460 --> 00:14:10,855
このバリューマトリックスに単語のエンベッディングを掛け合わ

241
00:14:10,855 --> 00:14:14,251
せるとき、この単語が他の何かの意味を調整するのに関連してい

242
00:14:14,251 --> 00:14:17,647
る場合、それを反映させるために、その他の何かのエンベッディ

243
00:14:17,647 --> 00:14:21,160
ングに具体的に何を加えるべきか、と考えることができるだろう。

244
00:14:22,140 --> 00:14:26,158
図を振り返って、キーとクエリーはすべて脇に置いておこう。

245
00:14:26,158 --> 00:14:29,602
アテンション・パターンを計算した後は、この値行列

246
00:14:29,602 --> 00:14:33,046
を取り出し、埋め込みの1つ1つに掛け合わせて、一

247
00:14:33,046 --> 00:14:36,060
連の値ベクトルを生成することになるからだ。

248
00:14:37,120 --> 00:14:39,120
これらの値ベクトルは、対応するキーに関連づ

249
00:14:39,120 --> 00:14:41,120
けられたものだと考えてもいいかもしれない。

250
00:14:42,320 --> 00:14:45,691
この図の各列について、それぞれの値ベク

251
00:14:45,691 --> 00:14:49,240
トルにその列の対応するウェイトを掛ける。

252
00:14:50,080 --> 00:14:53,817
例えば、Creatureの埋め込みでは、FluffyとB

253
00:14:53,817 --> 00:14:57,555
lueの値ベクトルの大きな割合を追加することになるが、他

254
00:14:57,555 --> 00:15:01,560
の値ベクトルはすべてゼロになるか、少なくともほぼゼロになる。

255
00:15:02,120 --> 00:15:05,548
そして最後に、この列に関連する埋め込みを実際に更新す

256
00:15:05,548 --> 00:15:08,975
る方法は、以前はCreatureの文脈自由な意味をエ

257
00:15:08,975 --> 00:15:12,404
ンコードしていたが、列内のこれらの再スケーリングされ

258
00:15:12,404 --> 00:15:15,832
た値をすべて足し合わせて、追加したい変更（delta

259
00:15:15,832 --> 00:15:19,260
-eと呼ぶ）を生成し、それを元の埋め込みに追加する。

260
00:15:19,680 --> 00:15:23,089
うまくいけば、より洗練されたベクトルが、ふわふわした青い生

261
00:15:23,089 --> 00:15:26,500
き物のような、より文脈に富んだ意味を符号化することになる。

262
00:15:27,380 --> 00:15:30,496
もちろん、1つのエンベッディングだけでなく、この写

263
00:15:30,496 --> 00:15:33,612
真の列すべてに同じ加重和を適用し、一連の変更を生み

264
00:15:33,612 --> 00:15:36,728
出し、対応するエンベッディングにそれらの変更をすべ

265
00:15:36,728 --> 00:15:39,845
て加えることで、より洗練されたエンベッディングの完

266
00:15:39,845 --> 00:15:43,460
全なシーケンスがアテンション・ブロックから飛び出してくる。

267
00:15:44,860 --> 00:15:46,980
ズームアウトしてみると、このプロセス

268
00:15:46,980 --> 00:15:49,100
全体が、一頭注目と表現できるものだ。

269
00:15:49,600 --> 00:15:52,713
これまで説明してきたように、このプロセスは3つの異なる

270
00:15:52,713 --> 00:15:55,826
マトリックスによってパラメータ化されており、すべて調整

271
00:15:55,826 --> 00:15:58,940
可能なパラメータ、キー、クエリー、値で満たされている。

272
00:15:59,500 --> 00:16:02,346
GPT-3の数字を使ってモデル・パラメーターの

273
00:16:02,346 --> 00:16:05,193
総数をカウントアップするスコアキーピングについ

274
00:16:05,193 --> 00:16:08,040
て、前章で始めたことの続きを少しやってみたい。

275
00:16:09,300 --> 00:16:12,632
これらのキー行列とクエリ行列はそれぞれ、埋め

276
00:16:12,632 --> 00:16:15,964
込み次元に一致する12,288列と、より小さ

277
00:16:15,964 --> 00:16:19,600
なキークエリ空間の次元に一致する128行を持つ。

278
00:16:20,260 --> 00:16:22,183
これにより、1つにつき150万ほど

279
00:16:22,183 --> 00:16:24,220
のパラメータが追加されることになる。

280
00:16:24,860 --> 00:16:32,890
対照的に、その値行列を見てみると、これまでの説明では、12,

281
00:16:32,890 --> 00:16:40,920
288の列と12,288の行を持つ正方行列であると思われる。

282
00:16:41,500 --> 00:16:43,319
もしそれが本当なら、約1億5000万人

283
00:16:43,319 --> 00:16:45,140
分のパラメータが追加されることになる。

284
00:16:45,660 --> 00:16:47,300
はっきり言って、それは可能だ。

285
00:16:47,420 --> 00:16:49,529
バリュー・マップには、キーやクエリよりも桁

286
00:16:49,529 --> 00:16:51,740
違いに多くのパラメーターを割くことができる。

287
00:16:52,060 --> 00:16:54,912
しかし実際には、このバリュー・マップに割

288
00:16:54,912 --> 00:16:57,764
くパラメーターの数を、キーとクエリーに割

289
00:16:57,764 --> 00:17:00,760
く数と同じにする方がはるかに効率的である。

290
00:17:01,460 --> 00:17:03,264
これは、複数のアテンション・ヘッドを並行

291
00:17:03,264 --> 00:17:05,160
して走らせるという設定に特に関連している。

292
00:17:06,240 --> 00:17:08,170
このように見えるのは、バリュー・マップが2つ

293
00:17:08,170 --> 00:17:10,099
の小さな行列の積として因数分解されるからだ。

294
00:17:11,180 --> 00:17:14,335
概念的には、私はまだ、全体的な線形マップ、入力

295
00:17:14,335 --> 00:17:17,490
と出力を持つもの、この大きな埋め込み空間の両方

296
00:17:17,490 --> 00:17:20,645
、例えば、名詞に追加するこの青さの方向に青の埋

297
00:17:20,645 --> 00:17:23,800
め込みを取ることについて考えることを奨励する。

298
00:17:27,040 --> 00:17:32,760
行数が少ないだけで、通常はキークエリスペースと同じサイズだ。

299
00:17:33,100 --> 00:17:35,770
これはどういうことかというと、大きな埋め込みベクトルをも

300
00:17:35,770 --> 00:17:38,440
っと小さな空間にマッピングしていると考えることができる。

301
00:17:39,040 --> 00:17:40,870
これは従来のネーミングとは違うが、私はこれを

302
00:17:40,870 --> 00:17:42,700
バリューダウンマトリックスと呼ぶことにする。

303
00:17:43,400 --> 00:17:46,913
2番目の行列は、この小さな空間から埋め込み空間

304
00:17:46,913 --> 00:17:50,580
にマップし、実際の更新に使うベクトルを生成する。

305
00:17:51,000 --> 00:17:54,740
これをバリューアップ・マトリックスと呼ぶことにする。

306
00:17:55,160 --> 00:17:58,080
この書き方は、多くの新聞では少し違っている。

307
00:17:58,380 --> 00:17:59,520
それについてはすぐに話す。

308
00:17:59,700 --> 00:18:01,120
個人的な意見だが、これは物事を少

309
00:18:01,120 --> 00:18:02,540
し概念的に混乱させる傾向がある。

310
00:18:03,260 --> 00:18:06,739
ここで線形代数の専門用語を使うと、基本的にやっていることは

311
00:18:06,739 --> 00:18:10,340
、全体の値マップが低ランクの変換になるように制約することだ。

312
00:18:11,420 --> 00:18:14,462
パラメータ数に話を戻すと、これらの4つの行列はすべて

313
00:18:14,462 --> 00:18:17,504
同じサイズであり、それらをすべて足すと、1つのアテン

314
00:18:17,504 --> 00:18:20,780
ションヘッドに対して約630万個のパラメータが得られる。

315
00:18:22,040 --> 00:18:24,381
ちょっとした余談だが、もう少し正確に言うと、これま

316
00:18:24,381 --> 00:18:26,723
で説明したものはすべて、他のモデルで見られるクロス

317
00:18:26,723 --> 00:18:29,064
アテンションと呼ばれるバリエーションと区別するため

318
00:18:29,064 --> 00:18:31,500
に、人々がセルフアテンションヘッドと呼ぶものである。

319
00:18:32,300 --> 00:18:35,661
これはGPTの例とは関係ないが、もし興味があれば、

320
00:18:35,661 --> 00:18:39,022
クロスアテンションは、2つの異なるタイプのデータを

321
00:18:39,022 --> 00:18:42,383
処理するモデルを含む。例えば、ある言語のテキストと

322
00:18:42,383 --> 00:18:45,744
、現在進行中の翻訳生成の一部である別の言語のテキス

323
00:18:45,744 --> 00:18:49,240
ト、あるいは音声入力と現在進行中の書き起こしなどだ。

324
00:18:50,400 --> 00:18:52,700
クロスアテンションヘッドもほとんど同じように見える。

325
00:18:52,980 --> 00:18:55,190
唯一の違いは、キー・マップとクエリ・マッ

326
00:18:55,190 --> 00:18:57,400
プが異なるデータセットに作用することだ。

327
00:18:57,840 --> 00:19:01,780
例えば、翻訳モデルでは、キーはある言語から、クエリーは別の言

328
00:19:01,780 --> 00:19:05,720
語から来るかもしれず、アテンションパターンは、ある言語のどの

329
00:19:05,720 --> 00:19:09,660
単語が別の言語のどの単語に対応するかを記述することができる。

330
00:19:10,340 --> 00:19:13,285
そしてこの設定では、後のトークンが前のトークンに影響を

331
00:19:13,285 --> 00:19:16,340
与えるという概念がないため、通常マスキングは行われない。

332
00:19:17,180 --> 00:19:19,811
しかし、セルフ・アテンションに集中することで、これ

333
00:19:19,811 --> 00:19:22,443
までのすべてを理解し、ここで立ち止まれば、アテンシ

334
00:19:22,443 --> 00:19:25,180
ョンとは何かという本質を理解することができるだろう。

335
00:19:25,760 --> 00:19:28,600
私たちに本当に残されているのは、これを何

336
00:19:28,600 --> 00:19:31,440
度も何度も行うセンスを並べることだけだ。

337
00:19:32,100 --> 00:19:35,949
私たちの中心的な例では、名詞を更新する形容詞に焦点を当てたが

338
00:19:35,949 --> 00:19:39,800
、もちろん文脈が単語の意味に影響を与える方法はたくさんある。

339
00:19:40,360 --> 00:19:43,371
クラッシュした言葉がクルマという言葉の前にあ

340
00:19:43,371 --> 00:19:46,520
るのなら、そのクルマの形や構造にも意味がある。

341
00:19:47,200 --> 00:19:49,280
そして、多くの連想は文法的でないかもしれない。

342
00:19:49,760 --> 00:19:52,696
もしハリーと同じ箇所のどこかにwizardという単語

343
00:19:52,696 --> 00:19:55,632
があれば、これはハリー・ポッターを指している可能性を

344
00:19:55,632 --> 00:19:58,568
示唆している。一方、もしその箇所にQueen、Sus

345
00:19:58,568 --> 00:20:01,504
sex、Williamという単語があれば、ハリーの埋

346
00:20:01,504 --> 00:20:04,440
め込みは王子を指すように更新されるべきかもしれない。

347
00:20:05,040 --> 00:20:07,813
あなたが想像するような異なるタイプのコンテキスト

348
00:20:07,813 --> 00:20:10,587
の更新のために、これらのキーとクエリ行列のパラメ

349
00:20:10,587 --> 00:20:13,361
ータは、異なる注意パターンをキャプチャするために

350
00:20:13,361 --> 00:20:16,135
異なるだろうし、私たちの値マップのパラメータは、

351
00:20:16,135 --> 00:20:19,140
埋め込みに追加されるべきものに基づいて異なるだろう。

352
00:20:19,980 --> 00:20:25,060
重みは、次のトークンを予測するという目標を達成するた

353
00:20:25,060 --> 00:20:30,140
めにモデルが必要とするものであれば何でも設定される。

354
00:20:31,400 --> 00:20:34,994
トランスフォーマー内の完全なアテンション・ブロック

355
00:20:34,994 --> 00:20:38,588
は、マルチヘッド・アテンションと呼ばれるものから構

356
00:20:38,588 --> 00:20:42,182
成され、それぞれが独自のキークエリーとバリュー・マ

357
00:20:42,182 --> 00:20:45,920
ップを持ち、多くのオペレーションを並行して実行する。

358
00:20:47,420 --> 00:20:49,505
例えばGPT-3は、各ブロック内に96

359
00:20:49,505 --> 00:20:51,700
個のアテンション・ヘッドを使用している。

360
00:20:52,020 --> 00:20:54,240
ひとつひとつがすでに少々混乱していることを考え

361
00:20:54,240 --> 00:20:56,460
れば、頭の中に抱えているものが多いのは確かだ。

362
00:20:56,760 --> 00:21:00,880
つまり、96の異なるキーとクエリー行列があり、96

363
00:21:00,880 --> 00:21:05,000
の異なるアテンション・パターンがあるということだ。

364
00:21:05,440 --> 00:21:08,727
そして各ヘッドは、96列の値ベクトルを生

365
00:21:08,727 --> 00:21:12,180
成するために使用される独自の値行列を持つ。

366
00:21:12,460 --> 00:21:14,570
これらはすべて、対応するアテンション・パ

367
00:21:14,570 --> 00:21:16,680
ターンを重みとして使って足し合わされる。

368
00:21:17,480 --> 00:21:20,660
これが意味するのは、コンテキストの各ポジション、各トークン

369
00:21:20,660 --> 00:21:23,840
について、これらのヘッドのひとつひとつが、そのポジションの

370
00:21:23,840 --> 00:21:27,020
エンベッディングに加えるべき変更案を生み出すということだ。

371
00:21:27,660 --> 00:21:31,493
つまり、これらの変更案を各ヘッドごとに1つずつ合計

372
00:21:31,493 --> 00:21:35,480
し、その結果を元のポジションの埋め込みに加えるのだ。

373
00:21:36,660 --> 00:21:40,260
この全体の合計は、この多頭の注目ブロックから出力さ

374
00:21:40,260 --> 00:21:43,860
れたものの一片であり、そのもう一方の端から飛び出し

375
00:21:43,860 --> 00:21:47,460
てくる、洗練されたエンベッディングのひとつである。

376
00:21:48,320 --> 00:21:50,192
繰り返しになるが、これは考えることが多いので、理解

377
00:21:50,192 --> 00:21:52,140
するのに時間がかかってもまったく心配する必要はない。

378
00:21:52,380 --> 00:21:55,481
全体的な考え方は、多くの異なるヘッドを並行して

379
00:21:55,481 --> 00:21:58,583
実行することで、文脈が意味を変える多くの異なる

380
00:21:58,583 --> 00:22:01,820
方法を学習する能力をモデルに与えるというものだ。

381
00:22:03,700 --> 00:22:07,493
この4つのマトリックスのそれぞれのバリエーションを含む

382
00:22:07,493 --> 00:22:11,286
96のヘッドでパラメータ数を集計すると、マルチヘッドの

383
00:22:11,286 --> 00:22:15,080
各ブロックには約6億のパラメータが含まれることになる。

384
00:22:16,420 --> 00:22:18,213
トランスフォーマーについてもっと読みたいと

385
00:22:18,213 --> 00:22:20,006
思う人のために、本当に言っておかなければな

386
00:22:20,006 --> 00:22:21,800
らない、ちょっと腹立たしいことが1つある。

387
00:22:22,080 --> 00:22:24,495
バリュー・マップは、バリュー・ダウン行列と

388
00:22:24,495 --> 00:22:26,910
バリュー・アップ行列という2つの異なる行列

389
00:22:26,910 --> 00:22:29,440
に分解されると言ったのを覚えているだろうか。

390
00:22:29,960 --> 00:22:34,109
私が説明した方法だと、それぞれのアテンション・

391
00:22:34,109 --> 00:22:38,440
ヘッドの中にマトリックスのペアを見ることになる。

392
00:22:38,640 --> 00:22:39,920
それは有効なデザインだろう。

393
00:22:40,260 --> 00:22:42,530
しかし、論文に書かれていることと、実際

394
00:22:42,530 --> 00:22:44,920
に実施されることは少し違っているようだ。

395
00:22:45,340 --> 00:22:48,916
各ヘッドのこれらのバリューアップ行列はすべて、

396
00:22:48,916 --> 00:22:52,492
出力行列と呼ぶ巨大な行列にまとめられ、マルチヘ

397
00:22:52,492 --> 00:22:56,380
ッドのアテンション・ブロック全体に関連づけられる。

398
00:22:56,820 --> 00:22:59,327
そして、あるアテンション・ヘッドのバリュー・マトリッ

399
00:22:59,327 --> 00:23:01,835
クスに言及する人々を見るとき、彼らは通常、この最初の

400
00:23:01,835 --> 00:23:04,342
ステップ、つまり、私がバリュー・ダウンの投影としてラ

401
00:23:04,342 --> 00:23:07,140
ベルを貼った、より小さな空間への投影に限って言及している。

402
00:23:08,340 --> 00:23:11,040
好奇心旺盛な皆さんのために、画面上にメモを残しておいた。

403
00:23:11,260 --> 00:23:13,686
これは、主要なコンセプトのポイントから注意をそらす危険性

404
00:23:13,686 --> 00:23:16,113
がある細部のひとつだが、他の情報源でこのことについて読ん

405
00:23:16,113 --> 00:23:18,540
だ場合に知ってもらえるように、念のため指摘しておきたい。

406
00:23:19,240 --> 00:23:22,134
技術的なニュアンスはさておき、前章のプレビューでは

407
00:23:22,134 --> 00:23:25,029
、トランスフォーマーを流れるデータが、単一のアテン

408
00:23:25,029 --> 00:23:28,040
ション・ブロックを流れるだけではないことを確認した。

409
00:23:28,640 --> 00:23:32,700
ひとつは、多層パーセプトロンと呼ばれる他の演算を経ることだ。

410
00:23:33,120 --> 00:23:34,880
それらについては、次の章で詳しく説明しよう。

411
00:23:35,180 --> 00:23:39,320
そして、この2つのオペレーションを何度も何度も繰り返す。

412
00:23:39,980 --> 00:23:43,249
これが意味するのは、ある単語がその文脈の一部を吸収し

413
00:23:43,249 --> 00:23:46,519
た後、よりニュアンスの強い埋め込みが、よりニュアンス

414
00:23:46,519 --> 00:23:50,040
の強い周囲の環境に影響される機会が多くなるということだ。

415
00:23:50,940 --> 00:23:56,400
ネットワークの下層に行くほど、各埋め込みが他のすべ

416
00:23:56,400 --> 00:24:01,860
ての埋め込みからより多くの意味を取り込むようになり

417
00:24:01,860 --> 00:24:07,320
、それ自体がよりニュアンスのあるものになっていく。

418
00:24:07,880 --> 00:24:11,427
心情やトーン、詩なのかどうか、その作品に関連す

419
00:24:11,427 --> 00:24:15,130
る根本的な科学的真理は何かとか、そういうことだ。

420
00:24:16,700 --> 00:24:21,104
GPT-3には96のレイヤーがあり、キーとなるク

421
00:24:21,104 --> 00:24:25,508
エリーパラメーターとバリューパラメーターの総数に

422
00:24:25,508 --> 00:24:29,912
さらに96をかけると、580億弱のパラメーターが

423
00:24:29,912 --> 00:24:34,500
すべてのアテンションヘッドに費やされることになる。

424
00:24:34,980 --> 00:24:37,960
これは確かに多いが、ネットワーク全体

425
00:24:37,960 --> 00:24:40,940
では1750億の3分の1に過ぎない。

426
00:24:41,520 --> 00:24:44,767
つまり、注目はすべて注目されるが、パラメータの大部分

427
00:24:44,767 --> 00:24:48,140
は、これらのステップの間に位置するブロックに由来する。

428
00:24:48,560 --> 00:24:51,016
次の章では、そのほかのブロックについて、またトレーニング

429
00:24:51,016 --> 00:24:53,560
のプロセスについて、あなたと私がもっと話をすることになる。

430
00:24:54,120 --> 00:24:57,612
アテンション・メカニズムが成功した理由の大部分は

431
00:24:57,612 --> 00:25:01,104
、それが可能にした特定の種類の動作というよりも、

432
00:25:01,104 --> 00:25:04,596
それが極めて並列化可能であること、つまりGPUを

433
00:25:04,596 --> 00:25:08,380
使って短時間で膨大な数の計算を実行できることにある。

434
00:25:09,460 --> 00:25:12,310
ここ10年か20年のディープラーニングに関する大きな教訓の

435
00:25:12,310 --> 00:25:15,161
ひとつは、スケールを大きくするだけで、モデルのパフォーマン

436
00:25:15,161 --> 00:25:18,012
スが質的に大きく向上するということであることを考えると、こ

437
00:25:18,012 --> 00:25:20,863
れを可能にする並列化可能なアーキテクチャには大きな利点があ

438
00:25:20,863 --> 00:25:21,060
る。

439
00:25:22,040 --> 00:25:23,645
このことについてもっと知りたければ、

440
00:25:23,645 --> 00:25:25,340
説明文にたくさんのリンクを残してある。

441
00:25:25,920 --> 00:25:27,980
特に、アンドレイ・カルパシーやクリス・オラが

442
00:25:27,980 --> 00:25:30,040
プロデュースしたものは純金であることが多い。

443
00:25:30,560 --> 00:25:32,533
このビデオでは、現在のアテンションに飛びつきたかったのだ

444
00:25:32,533 --> 00:25:34,506
が、もしあなたが、私たちがどのようにしてここにたどり着い

445
00:25:34,506 --> 00:25:36,479
たのか、また、あなた自身がこのアイデアをどのように再発明

446
00:25:36,479 --> 00:25:38,452
することができるのか、その歴史についてもっと知りたければ

447
00:25:38,452 --> 00:25:40,425
、私の友人であるヴィヴェックが、その動機付けについてもっ

448
00:25:40,425 --> 00:25:42,540
と多くのことを語っているビデオをいくつかアップしたところだ。

449
00:25:43,120 --> 00:25:44,306
また、The Art of the 

450
00:25:44,306 --> 00:25:45,757
ProblemというチャンネルのBritt 

451
00:25:45,757 --> 00:25:47,537
Cruzが、大規模な言語モデルの歴史についてとても素晴

452
00:25:47,537 --> 00:25:48,460
らしいビデオを見せてくれた。

453
00:26:04,960 --> 00:26:09,200
ありがとう。

