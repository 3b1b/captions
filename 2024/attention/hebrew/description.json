[
 {
  "input": "Demystifying self-attention, multiple heads, and cross-attention.",
  "translatedText": "הסרת המסתורין מעל מיקוד עצמי (self-attention), ריבוי ראשים ומיקוד צולב.",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "translatedText": "במקום קריאת מודעות ממומנות, שיעורים אלה ממומנים ישירות על ידי הצופים: https://3b1b.co/support",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "An equally valuable form of support is to simply share the videos.",
  "translatedText": "צורת תמיכה בעלת ערך לא פחות היא פשוט לשתף את הסרטונים.",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "Other resources about transformers",
  "translatedText": "משאבים נוספים על טרמספורמרים",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "Andrej Karpathy's videos",
  "translatedText": "הסרטונים של Andrej Karpathy",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "The Transformer Circuits posts by Anthropic",
  "translatedText": "פוסטים על מעגלי טרנספורמר מאת Anthropic",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "https://transformer-circuits.pub/2021/framework/index.html",
  "translatedText": "https://transformer-circuits.pub/2021/framework/index.html",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "In particular, it was only after I read this post that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources.",
  "translatedText": "בפרט, רק לאחר שקראתי את הפוסט הזה התחלתי לחשוב על השילוב של מטריצות הערך והפלט כעל מיפוי משולב בדרג נמוך ממרחב השיכון אל עצמו, מה שלפחות אצלי, עשה הרבה דברים ברורים יותר מאשר ממקורות אחרים.",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "History of language models by Brit Cruise, @ArtOfTheProblem ",
  "translatedText": "היסטוריה של מודלים לשפות מאת Brit Cruise, @ArtOfTheProblem ",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "https://youtu.be/OFS90-FX6pg",
  "translatedText": "https://youtu.be/OFS90-FX6pg",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "What is a Language Model by @vcubingx ",
  "translatedText": "מהו מודל שפה מאת @vcubingx ",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "translatedText": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "Site with exercises related to ML programming and GPTs",
  "translatedText": "אתר עם תרגילים הקשורים לתכנות ML ו-GPTs",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "https://www.gptandchill.ai/codingproblems",
  "translatedText": "https://www.gptandchill.ai/codingproblems",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "Early paper on how directions in embedding spaces have meaning:",
  "translatedText": "מאמר מוקדם על האופן שבו לכיוונים במרחבי שיכון יש משמעות:",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "https://arxiv.org/pdf/1301.3781.pdf",
  "translatedText": "https://arxiv.org/pdf/1301.3781.pdf",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "Timestamps:",
  "translatedText": "חותמות זמן:",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "0:00 - Recap on embeddings",
  "translatedText": "0:00 - סיכום על שיכונים",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "1:39 - Motivating examples",
  "translatedText": "1:39 - דוגמאות למוטיבציה",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "4:29 - The attention pattern",
  "translatedText": "4:29 - דפוס המיקוד (attention pattern)",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "11:08 - Masking",
  "translatedText": "11:08 - מיסוך",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "12:42 - Context size",
  "translatedText": "12:42 - גודל הקונטקסט",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "13:10 - Values",
  "translatedText": "13:10 - ערכים",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "15:44 - Counting parameters",
  "translatedText": "15:44 - ספירת פרמטרים",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "18:21 - Cross-attention",
  "translatedText": "18:21 - מיקוד (attention) צולב",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "19:19 - Multiple heads",
  "translatedText": "19:19 - מספר ראשים",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "22:16 - The output matrix",
  "translatedText": "22:16 - מטריצת הפלט",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "23:19 - Going deeper",
  "translatedText": "23:19 - העמקה",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "24:54 - Ending",
  "translatedText": "24:54 - סיום",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 },
 {
  "input": "",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 1
 }
]
