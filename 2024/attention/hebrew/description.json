[
 {
  "translatedText": "מבטל תשומת לב עצמית, ריבוי ראשים ותשומת לב צולבת.",
  "input": "Demystifying self-attention, multiple heads, and cross-attention.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "במקום קריאת מודעות ממומנות, שיעורים אלה ממומנים ישירות על ידי הצופים: https://3b1b.co/support",
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "צורת תמיכה בעלת ערך לא פחות היא פשוט לשתף את הסרטונים.",
  "input": "An equally valuable form of support is to simply share the videos.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "משאבים נוספים על שנאים",
  "input": "Other resources about transformers",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "הסרטונים של אנדריי קרפטי",
  "input": "Andrej Karpathy's videos",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "The Transformer Circuits פוסטים מאת Anthropic",
  "input": "The Transformer Circuits posts by Anthropic",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://transformer-circuits.pub/2021/framework/index.html",
  "input": "https://transformer-circuits.pub/2021/framework/index.html",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "בפרט, רק לאחר שקראתי את הפוסט הזה התחלתי לחשוב על השילוב של מטריצות הערך והפלט כעל מפה משולבת בדרג נמוך ממרחב ההטמעה אל עצמה, מה שלפחות במוחי, עשה הרבה דברים. ברור יותר ממקורות אחרים.",
  "input": "In particular, it was only after I read this post that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "היסטוריה של מודלים לשפות מאת בריט קרוז, @ArtOfTheProblem ",
  "input": "History of language models by Brit Cruise, @ArtOfTheProblem ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://youtu.be/OFS90-FX6pg",
  "input": "https://youtu.be/OFS90-FX6pg",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "מהו מודל שפה מאת @vcubingx ",
  "input": "What is a Language Model by @vcubingx ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "input": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "אתר עם תרגילים הקשורים לתכנות ML ו-GPTs",
  "input": "Site with exercises related to ML programming and GPTs",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://www.gptandchill.ai/codingproblems",
  "input": "https://www.gptandchill.ai/codingproblems",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "מאמר מוקדם על האופן שבו יש משמעות להנחיות בהטמעת חללים:",
  "input": "Early paper on how directions in embedding spaces have meaning:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://arxiv.org/pdf/1301.3781.pdf",
  "input": "https://arxiv.org/pdf/1301.3781.pdf",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "חותמות זמן:",
  "input": "Timestamps:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "0:00 - סיכום על הטבעות",
  "input": "0:00 - Recap on embeddings",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "1:39 - דוגמאות מעודדות",
  "input": "1:39 - Motivating examples",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "4:29 - דפוס הקשב",
  "input": "4:29 - The attention pattern",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "11:08 - מיסוך",
  "input": "11:08 - Masking",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "12:42 - גודל הקשר",
  "input": "12:42 - Context size",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "13:10 - ערכים",
  "input": "13:10 - Values",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "15:44 - ספירת פרמטרים",
  "input": "15:44 - Counting parameters",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "18:21 - תשומת לב צולבת",
  "input": "18:21 - Cross-attention",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "19:19 - מספר ראשים",
  "input": "19:19 - Multiple heads",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "22:16 - מטריצת הפלט",
  "input": "22:16 - The output matrix",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "23:19 - הולך עמוק יותר",
  "input": "23:19 - Going deeper",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "24:54 - סיום",
  "input": "24:54 - Ending",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 }
]