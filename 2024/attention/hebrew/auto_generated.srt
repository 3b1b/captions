1
00:00:00,000 --> 00:00:04,019
בפרק האחרון, אתה ואני התחלנו לעבור דרך פעולתו הפנימית של שנאי.

2
00:00:04,560 --> 00:00:08,077
זהו אחד מחלקי הטכנולוגיה המרכזיים במודלים של שפות גדולות, 

3
00:00:08,077 --> 00:00:10,200
והרבה כלים אחרים בגל המודרני של AI.

4
00:00:10,980 --> 00:00:16,232
זה הגיע לראשונה לסצנה במאמר מפורסם מ-2017 בשם Attention is All You Need, 

5
00:00:16,232 --> 00:00:21,700
ובפרק זה אתה ואני נחפור מהו מנגנון הקשב הזה, תוך חזותי כיצד הוא מעבד נתונים.

6
00:00:26,140 --> 00:00:29,540
כסיכום קצר, הנה ההקשר החשוב שאני רוצה שתזכור.

7
00:00:30,000 --> 00:00:36,060
המטרה של המודל שאתה ואני לומדים היא לקחת חלק מטקסט ולחזות איזו מילה מגיעה אחר כך.

8
00:00:36,860 --> 00:00:40,285
טקסט הקלט מחולק לחתיכות קטנות שאנו מכנים אסימונים, 

9
00:00:40,285 --> 00:00:44,851
ולעתים קרובות אלו הן מילים או פיסות מילים, אבל רק כדי להקל עליך ולי 

10
00:00:44,851 --> 00:00:50,560
לחשוב על הדוגמאות בסרטון הזה, בואו נפשט על ידי העמדת פנים שאסימונים הם תמיד רק מילים.

11
00:00:51,480 --> 00:00:57,700
השלב הראשון בשנאי הוא לשייך כל אסימון לוקטור בעל מימד גבוה, מה שאנו מכנים הטבעה שלו.

12
00:00:57,700 --> 00:01:01,986
הרעיון החשוב ביותר שאני רוצה שתזכרו הוא כיצד כיוונים במרחב 

13
00:01:01,986 --> 00:01:07,000
הגבוה-ממדי הזה של כל ההטבעות האפשריות יכולים להתכתב עם משמעות סמנטית.

14
00:01:07,680 --> 00:01:12,009
בפרק האחרון ראינו דוגמה לאופן שבו כיוון יכול להתאים למגדר, 

15
00:01:12,009 --> 00:01:17,879
במובן זה שהוספת שלב מסוים במרחב הזה יכולה לקחת אותך מהטבעה של שם עצם זכר להטבעה 

16
00:01:17,879 --> 00:01:19,640
של שם העצם הנקבי המקביל.

17
00:01:20,160 --> 00:01:23,719
זו רק דוגמה אחת שתוכלו לתאר לעצמכם כמה כיוונים אחרים במרחב 

18
00:01:23,719 --> 00:01:27,580
הגבוה-ממדי הזה יכולים להתאים להרבה היבטים אחרים של משמעות המילה.

19
00:01:28,800 --> 00:01:34,805
המטרה של שנאי היא להתאים בהדרגה את ההטבעות הללו כך שהן לא רק מקודדות מילה בודדת, 

20
00:01:34,805 --> 00:01:39,180
אלא במקום זאת הן אופות במשמעות הקשרית הרבה הרבה יותר עשירה.

21
00:01:40,140 --> 00:01:44,979
אני צריך לומר מראש שהרבה אנשים מוצאים את מנגנון הקשב, חלק המפתח הזה בשנאי, 

22
00:01:44,979 --> 00:01:48,980
מאוד מבלבל, אז אל תדאג אם לוקח קצת זמן עד שהדברים ישקעו פנימה.

23
00:01:49,440 --> 00:01:54,229
אני חושב שלפני שאנחנו צוללים לפרטים החישוביים ולכל הכפלות המטריצות, 

24
00:01:54,229 --> 00:01:59,160
כדאי לחשוב על כמה דוגמאות לסוג ההתנהגות שאנחנו רוצים שתשומת הלב תאפשר.

25
00:02:00,140 --> 00:02:06,220
שקול את הביטויים American true mole, שומה אחת של פחמן דו חמצני, ולקחת ביופסיה של השומה.

26
00:02:06,700 --> 00:02:10,900
אתה ואני יודעים שלמילה שומה יש משמעויות שונות בכל אחת מהן, בהתבסס על ההקשר.

27
00:02:11,360 --> 00:02:16,922
אבל לאחר השלב הראשון של שנאי, זה שמפרק את הטקסט ומשייך כל אסימון לוקטור, 

28
00:02:16,922 --> 00:02:20,504
הווקטור המשויך לשומה יהיה זהה בכל המקרים האלה, 

29
00:02:20,504 --> 00:02:26,220
מכיוון שהטמעת האסימון הראשונית הזו היא למעשה טבלת חיפוש ללא התייחסות להקשר.

30
00:02:26,620 --> 00:02:33,100
רק בשלב הבא של השנאי יש להטמעות שמסביב הזדמנות להעביר מידע לתוך זה.

31
00:02:33,820 --> 00:02:39,634
התמונה שאולי תזכור היא שיש מספר כיוונים ברורים במרחב ההטמעה הזה המקודדים את 

32
00:02:39,634 --> 00:02:45,602
המשמעויות המובהקות של המילה שומה, ושגוש קשב מיומן מחשב את מה שאתה צריך להוסיף 

33
00:02:45,602 --> 00:02:51,800
להטמעה הגנרית כדי להעביר אותו אל אחד מהכיוונים הספציפיים הללו, כפונקציה של ההקשר.

34
00:02:53,300 --> 00:02:56,180
כדי לקחת דוגמה נוספת, שקול את הטבעת המילה מגדל.

35
00:02:57,060 --> 00:03:03,720
זה כנראה איזה כיוון מאוד כללי, לא ספציפי במרחב, הקשור להרבה שמות עצם גדולים וגבוהים אחרים.

36
00:03:04,020 --> 00:03:09,233
אם המילה הזאת קודמה מיד על ידי אייפל, הייתם יכולים לדמיין שאתם רוצים שהמנגנון 

37
00:03:09,233 --> 00:03:14,581
יעדכן את הווקטור הזה כך שיצביע על כיוון שמקודד באופן ספציפי יותר את מגדל אייפל, 

38
00:03:14,581 --> 00:03:19,060
אולי בקורלציה עם וקטורים הקשורים לפריז וצרפת ולדברים העשויים מפלדה.

39
00:03:19,920 --> 00:03:24,440
אם קדמה לה גם המילה מיניאטורה, אז הווקטור צריך להתעדכן עוד יותר, 

40
00:03:24,440 --> 00:03:27,500
כך שהוא לא מתאם עוד עם דברים גדולים וגבוהים.

41
00:03:29,480 --> 00:03:32,810
באופן כללי יותר מאשר רק חידוד המשמעות של מילה, 

42
00:03:32,810 --> 00:03:37,417
בלוק הקשב מאפשר למודל להעביר מידע המקודד בהטמעה אחת לזו של אחרת, 

43
00:03:37,417 --> 00:03:43,300
פוטנציאלית כאלה שנמצאים די רחוק, ואפשר עם מידע שהוא הרבה יותר עשיר מסתם מילה בודדת.

44
00:03:43,300 --> 00:03:50,706
מה שראינו בפרק האחרון היה איך אחרי שכל הוקטורים זורמים ברשת, כולל בלוקי קשב רבים ושונים, 

45
00:03:50,706 --> 00:03:57,863
החישוב שאתה מבצע כדי לייצר חיזוי של האסימון הבא הוא לחלוטין פונקציה של הווקטור האחרון 

46
00:03:57,863 --> 00:03:58,280
ברצף.

47
00:03:59,100 --> 00:04:04,352
תאר לעצמך, למשל, שהטקסט שאתה מזין הוא רובו של רומן מסתורין שלם, 

48
00:04:04,352 --> 00:04:07,800
עד לנקודה סמוך לסוף, שקורא, לכן הרוצח היה.

49
00:04:08,400 --> 00:04:13,507
אם המודל מתכוון לחזות במדויק את המילה הבאה, אותו וקטור אחרון ברצף, 

50
00:04:13,507 --> 00:04:20,139
שהתחיל את חייו פשוט להטביע את המילה הייתה, יצטרך להיות מעודכן על ידי כל בלוקי הקשב כדי 

51
00:04:20,139 --> 00:04:26,847
לייצג הרבה, הרבה יותר מכל אדם מילה, איכשהו מקודדת את כל המידע מחלון ההקשר המלא הרלוונטי 

52
00:04:26,847 --> 00:04:28,220
לניבוי המילה הבאה.

53
00:04:29,500 --> 00:04:32,580
עם זאת, כדי לעבור דרך החישובים, בואו ניקח דוגמה הרבה יותר פשוטה.

54
00:04:32,980 --> 00:04:37,960
תארו לעצמכם שהקלט כולל את הביטוי, יצור כחול רך שוטט ביער המוריק.

55
00:04:38,460 --> 00:04:42,463
ולרגע זה, נניח שסוג העדכון היחיד שמעניין אותנו הוא 

56
00:04:42,463 --> 00:04:46,780
התאמת שמות התואר את המשמעויות של שמות העצם התואמים להם.

57
00:04:47,000 --> 00:04:50,670
מה שאני עומד לתאר זה מה שהיינו מכנים ראש קשב בודד, 

58
00:04:50,670 --> 00:04:55,420
ובהמשך נראה כיצד בלוק הקשב מורכב מהרבה ראשים שונים הפועלים במקביל.

59
00:04:56,140 --> 00:04:59,760
שוב, ההטבעה הראשונית של כל מילה היא איזה וקטור בעל מימד 

60
00:04:59,760 --> 00:05:03,380
גבוה שמקודד רק את המשמעות של המילה המסוימת הזו ללא הקשר.

61
00:05:04,000 --> 00:05:05,220
למעשה, זה לא ממש נכון.

62
00:05:05,380 --> 00:05:07,640
הם גם מקודדים את המיקום של המילה.

63
00:05:07,980 --> 00:05:11,527
יש עוד הרבה מה לומר איך מיקומים מקודדים, אבל כרגע, 

64
00:05:11,527 --> 00:05:16,743
כל מה שאתה צריך לדעת הוא שהערכים של הווקטור הזה מספיקים כדי לספר לך גם מהי 

65
00:05:16,743 --> 00:05:18,900
המילה וגם היכן היא קיימת בהקשר.

66
00:05:19,500 --> 00:05:21,660
נמשיך ונציין את ההטבעות הללו באות ה.

67
00:05:22,420 --> 00:05:27,799
המטרה היא שסדרה של חישובים תייצר קבוצה מעודנת חדשה של הטבעות שבהן, 

68
00:05:27,799 --> 00:05:33,420
למשל, אלו התואמות לשמות העצם ספגו את המשמעות משמות התואר התואמים שלהם.

69
00:05:33,900 --> 00:05:38,580
ולשחק במשחק הלמידה העמוקה, אנחנו רוצים שרוב החישובים המעורבים ייראו כמו תוצרי 

70
00:05:38,580 --> 00:05:43,980
מטריצה-וקטור, שבהם המטריצות מלאות במשקלים הניתנים לשינוי, דברים שהמודל ילמד על סמך נתונים.

71
00:05:44,660 --> 00:05:48,376
כדי להיות ברור, אני ממציא את הדוגמה הזו של שמות תואר המעדכנים שמות 

72
00:05:48,376 --> 00:05:52,260
עצם רק כדי להמחיש את סוג ההתנהגות שאתה יכול לדמיין שראש תשומת לב עושה.

73
00:05:52,860 --> 00:05:57,126
כמו בכל כך הרבה למידה עמוקה, ההתנהגות האמיתית היא הרבה יותר קשה לנתח מכיוון שהיא 

74
00:05:57,126 --> 00:06:01,340
מבוססת על כוונון וכיוונון של מספר עצום של פרמטרים כדי למזער פונקציית עלות כלשהי.

75
00:06:01,680 --> 00:06:06,949
רק שכשאנחנו עוברים דרך כל המטריצות השונות המלאות בפרמטרים שמעורבים בתהליך הזה, 

76
00:06:06,949 --> 00:06:12,686
אני חושב שזה ממש מועיל לקבל דוגמה דמיונית למשהו שהוא יכול לעשות כדי לשמור על הכל יותר 

77
00:06:12,686 --> 00:06:13,220
קונקרטי.

78
00:06:14,140 --> 00:06:17,770
בשלב הראשון של תהליך זה, אתה עשוי לדמיין כל שם עצם, 

79
00:06:17,770 --> 00:06:21,960
כמו יצור, שואל את השאלה, היי, האם יש שמות תואר שיושבים מולי?

80
00:06:22,160 --> 00:06:27,960
ולגבי המילים פלאפי וכחול, שכל אחד יוכל לענות, כן, אני שם תואר ואני בעמדה הזו.

81
00:06:28,960 --> 00:06:33,336
השאלה הזו מקודדת איכשהו כעוד וקטור, עוד רשימה של מספרים, 

82
00:06:33,336 --> 00:06:36,100
שאנו קוראים לה השאילתה של המילה הזו.

83
00:06:36,980 --> 00:06:42,020
עם זאת, לוקטור השאילתה הזה יש ממד קטן בהרבה מהוקטור המוטבע, נניח 128.

84
00:06:42,940 --> 00:06:49,780
חישוב השאילתה הזו נראה כמו לקיחת מטריצה מסוימת, שאותה אתן ל-wq, ולהכפיל אותה בהטמעה.

85
00:06:50,960 --> 00:06:55,207
אם נדחס קצת דברים, בוא נכתוב את וקטור השאילתה הזה בתור q, 

86
00:06:55,207 --> 00:06:59,015
ואז בכל פעם שאתה רואה אותי שם מטריצה ליד חץ כמו זה, 

87
00:06:59,015 --> 00:07:04,800
זה נועד לייצג שכפל המטריצה הזו בווקטור בתחילת החץ נותן לך את הווקטור ב קצה החץ.

88
00:07:05,860 --> 00:07:09,761
במקרה זה, אתה מכפיל את המטריצה הזו בכל ההטמעות בהקשר, 

89
00:07:09,761 --> 00:07:12,580
ומייצר וקטור שאילתה אחד עבור כל אסימון.

90
00:07:13,740 --> 00:07:19,433
הערכים של המטריצה הזו הם פרמטרים של המודל, כלומר ההתנהגות האמיתית נלמדת מנתונים, 

91
00:07:19,433 --> 00:07:23,440
ובפועל, מה שמטריצה זו עושה בראש קשב מסוים הוא מאתגר לנתח.

92
00:07:23,900 --> 00:07:27,350
אבל למעננו, כשנדמיינו דוגמה שאולי נקווה שהיא תלמד, 

93
00:07:27,350 --> 00:07:32,086
נניח שמטריצת השאילתה הזו ממפה את ההטבעות של שמות עצם לכיוונים מסוימים 

94
00:07:32,086 --> 00:07:38,040
במרחב השאילתה הקטן יותר הזה, שמקודד איכשהו את הרעיון של חיפוש שמות תואר בעמדות קודמות. .

95
00:07:38,780 --> 00:07:41,440
לגבי מה זה עושה להטבעות אחרות, מי יודע?

96
00:07:41,720 --> 00:07:44,340
אולי הוא מנסה במקביל להשיג מטרה אחרת עם אלה.

97
00:07:44,540 --> 00:07:47,160
כרגע, אנחנו מתמקדים בלייזר בשמות העצם.

98
00:07:47,280 --> 00:07:54,620
יחד עם זאת, קשורה לזה מטריצה שנייה שנקראת מטריצת המפתח, שגם אותה מכפילים בכל אחת מההטבעות.

99
00:07:55,280 --> 00:07:58,500
זה מייצר רצף שני של וקטורים שאנו קוראים להם המפתחות.

100
00:07:59,420 --> 00:08:03,140
מבחינה קונספטואלית, אתה רוצה לחשוב על המפתחות כמענה פוטנציאלי לשאילתות.

101
00:08:03,840 --> 00:08:08,400
מטריצת מפתח זו מלאה גם בפרמטרים הניתנים לכוונון, ובדיוק כמו מטריצת השאילתה, 

102
00:08:08,400 --> 00:08:11,400
היא ממפה את וקטורי הטבעה לאותו מרחב ממדי קטן יותר.

103
00:08:12,200 --> 00:08:17,020
אתה חושב על המפתחות כמתאימים לשאילתות בכל פעם שהם מתאימים זה לזה.

104
00:08:17,460 --> 00:08:21,871
בדוגמה שלנו, תדמיינו שמטריצת המפתח ממפה את שמות התואר כמו 

105
00:08:21,871 --> 00:08:26,740
פלאפי וכחול לוקטורים שמיושרים היטב עם השאילתה שמפיקה המילה יצור.

106
00:08:27,200 --> 00:08:30,636
כדי למדוד עד כמה כל מפתח מתאים לכל שאילתה, אתה 

107
00:08:30,636 --> 00:08:34,000
מחשב מוצר נקודות בין כל זוג מפתח-שאילתה אפשרי.

108
00:08:34,480 --> 00:08:38,269
אני אוהב לדמיין רשת מלאה בחבורה של נקודות, שבה הנקודות הגדולות יותר 

109
00:08:38,269 --> 00:08:42,559
מתאימות למוצרי הנקודות הגדולות יותר, המקומות שבהם המפתחות והשאילתות מתיישרים.

110
00:08:43,280 --> 00:08:46,942
לדוגמא של שם התואר שלנו, זה ייראה קצת יותר כך, 

111
00:08:46,942 --> 00:08:53,488
כאשר אם המקשים המופקים על ידי fluffy וכחול באמת מתאימים לשאילתה המיוצר על ידי יצור, 

112
00:08:53,488 --> 00:08:58,320
אז תוצרי הנקודות בשני הנקודות הללו יהיו מספרים חיוביים גדולים.

113
00:08:59,100 --> 00:09:02,327
בשפה השפה, אנשי למידת מכונה היו אומרים שזה אומר 

114
00:09:02,327 --> 00:09:05,420
שההטבעות של פלאפי וכחול מטפלות בהטבעה של יצור.

115
00:09:06,040 --> 00:09:11,453
בניגוד לתוצר הנקודה בין המפתח למילה אחרת כמו ה- לשאילתה עבור 

116
00:09:11,453 --> 00:09:16,600
יצור יהיה איזה ערך קטן או שלילי שמשקף שאינם קשורים זה לזה.

117
00:09:17,700 --> 00:09:23,586
אז יש לנו את הרשת הזו של ערכים שיכולים להיות כל מספר ממשי מאינסוף שלילי עד אינסוף, 

118
00:09:23,586 --> 00:09:28,480
נותן לנו ציון עד כמה כל מילה רלוונטית לעדכון המשמעות של כל מילה אחרת.

119
00:09:29,200 --> 00:09:34,427
הדרך שבה אנחנו עומדים להשתמש בציונים האלה היא לקחת סכום משוקלל מסוים לאורך כל עמודה, 

120
00:09:34,427 --> 00:09:35,780
משוקלל לפי הרלוונטיות.

121
00:09:36,520 --> 00:09:40,356
אז במקום שיהיו לנו ערכים בין אינסוף שלילי לאינסוף, 

122
00:09:40,356 --> 00:09:46,224
מה שאנחנו רוצים זה שהמספרים בעמודות האלה יהיו בין 0 ל-1, וכל עמודה תצטבר ל-1, 

123
00:09:46,224 --> 00:09:48,180
כאילו היו התפלגות הסתברות.

124
00:09:49,280 --> 00:09:52,220
אם אתה מגיע מהפרק האחרון, אתה יודע מה אנחנו צריכים לעשות אז.

125
00:09:52,620 --> 00:09:57,300
אנו מחשבים softmax לאורך כל אחת מהעמודות הללו כדי לנרמל את הערכים.

126
00:10:00,060 --> 00:10:05,860
בתמונה שלנו, לאחר שתחיל את softmax על כל העמודות, נמלא את הרשת בערכים המנורמלים הללו.

127
00:10:06,780 --> 00:10:10,644
בשלב זה אתה בטוח לחשוב על כל עמודה כעל מתן משקלים לפי 

128
00:10:10,644 --> 00:10:14,580
מידת הרלוונטיות של המילה משמאל לערך המתאים בחלק העליון.

129
00:10:15,080 --> 00:10:16,840
אנו קוראים לרשת הזו דפוס קשב.

130
00:10:18,080 --> 00:10:22,820
עכשיו אם אתה מסתכל על נייר השנאי המקורי, יש דרך ממש קומפקטית שהם כותבים את כל זה.

131
00:10:23,880 --> 00:10:29,369
כאן המשתנים q ו-k מייצגים את המערכים המלאים של וקטורי שאילתה ומפתח בהתאמה, 

132
00:10:29,369 --> 00:10:34,640
אותם וקטורים קטנים שמקבלים על ידי הכפלת ההטמעות בשאילתה ובמטריצות המפתח.

133
00:10:35,160 --> 00:10:39,089
הביטוי הזה למעלה במונה הוא דרך ממש קומפקטית לייצג את הרשת 

134
00:10:39,089 --> 00:10:43,020
של כל מוצרי הנקודות האפשריים בין זוגות של מפתחות ושאילתות.

135
00:10:44,000 --> 00:10:47,890
פרט טכני קטן שלא הזכרתי הוא שלצורך יציבות מספרית, 

136
00:10:47,890 --> 00:10:53,960
במקרה מועיל לחלק את כל הערכים הללו בשורש הריבועי של הממד במרחב שאילתת מפתח זה.

137
00:10:54,480 --> 00:11:00,800
אז ה-softmax הזה שעוטף את הביטוי המלא אמור להיות מובן ליישם טור אחר טור.

138
00:11:01,640 --> 00:11:04,700
לגבי המונח V הזה, נדבר עליו רק בעוד שנייה.

139
00:11:05,020 --> 00:11:08,460
לפני כן, יש עוד פרט טכני אחד שעד כה דילגתי עליו.

140
00:11:09,040 --> 00:11:14,052
במהלך תהליך האימון, כאשר אתה מפעיל את המודל הזה על דוגמה של טקסט נתון, 

141
00:11:14,052 --> 00:11:19,417
וכל המשקולות מותאמות מעט ומכווננות כדי לתגמל או להעניש אותו על סמך ההסתברות 

142
00:11:19,417 --> 00:11:24,994
הגבוהה שהוא מקצה למילה הבאה האמיתית בקטע, מסתבר שהופך את כל תהליך האימון להרבה 

143
00:11:24,994 --> 00:11:30,289
יותר יעיל אם אתה מנבא בו זמנית כל אסימון הבא אפשרי בעקבות כל רצף ראשוני של 

144
00:11:30,289 --> 00:11:31,560
אסימונים בקטע הזה.

145
00:11:31,940 --> 00:11:35,657
לדוגמה, עם הביטוי שבו התמקדנו, זה עשוי להיות גם חיזוי 

146
00:11:35,657 --> 00:11:39,100
אילו מילים עוקבות אחר יצור ואיזה מילים עוקבות אחר.

147
00:11:39,940 --> 00:11:45,560
זה ממש נחמד, כי זה אומר שמה שאחרת יהיה דוגמה אחת לאימון פועל ביעילות כמו רבים.

148
00:11:46,100 --> 00:11:51,202
למטרות דפוס הקשב שלנו, זה אומר שאתה אף פעם לא רוצה לאפשר למילים מאוחרות יותר 

149
00:11:51,202 --> 00:11:56,040
להשפיע על מילים קודמות, כי אחרת הם יכולים לתת את התשובה למה שיבוא אחר כך.

150
00:11:56,560 --> 00:11:59,559
המשמעות היא שאנחנו רוצים שכל הנקודות האלה כאן, 

151
00:11:59,559 --> 00:12:04,600
אלו שמייצגות אסימונים מאוחרים יותר המשפיעים על קודמים, ייאלצו איכשהו להיות אפס.

152
00:12:05,920 --> 00:12:09,074
הדבר הפשוט ביותר שאתה עשוי לחשוב לעשות הוא להגדיר אותם שווה לאפס, 

153
00:12:09,074 --> 00:12:12,420
אבל אם אתה עושה את זה העמודות לא יצטברו לאחד יותר, הם לא היו מנורמלים.

154
00:12:13,120 --> 00:12:16,379
אז במקום זאת, דרך נפוצה לעשות זאת היא שלפני החלת softmax, 

155
00:12:16,379 --> 00:12:19,020
אתה מגדיר את כל הערכים האלה להיות אינסוף שלילי.

156
00:12:19,680 --> 00:12:25,180
אם אתה עושה את זה, אז לאחר החלת softmax, כל אלה הופכים לאפס, אבל העמודות נשארות מנורמלות.

157
00:12:26,000 --> 00:12:27,540
תהליך זה נקרא מיסוך.

158
00:12:27,540 --> 00:12:31,773
ישנן גרסאות של תשומת לב שבהן אתה לא מיישם את זה, אבל בדוגמה של GPT שלנו, 

159
00:12:31,773 --> 00:12:34,964
למרות שזה רלוונטי יותר בשלב ההכשרה ממה שזה יהיה, נגיד, 

160
00:12:34,964 --> 00:12:39,720
הפעלת אותו כצ&#39;אט בוט או משהו כזה, אתה תמיד מיישם מיסוך זה כדי למנוע מאסימונים 

161
00:12:39,720 --> 00:12:41,460
מאוחרים יותר להשפיע על קודמים.

162
00:12:42,480 --> 00:12:49,500
עובדה נוספת שכדאי לחשוב עליה לגבי דפוס הקשב הזה היא כיצד גודלו שווה לריבוע של גודל ההקשר.

163
00:12:49,900 --> 00:12:54,176
אז זו הסיבה שגודל ההקשר יכול להוות צוואר בקבוק עצום עבור מודלים של שפות גדולות, 

164
00:12:54,176 --> 00:12:55,620
והגדלה שלו אינה טריוויאלית.

165
00:12:56,300 --> 00:13:00,670
כפי שאתה מתאר לעצמך, מונעים על ידי רצון לחלונות הקשר גדולים יותר ויותר, 

166
00:13:00,670 --> 00:13:05,588
בשנים האחרונות נראו כמה וריאציות למנגנון הקשב שמטרתו להפוך את ההקשר להרחבה יותר, 

167
00:13:05,588 --> 00:13:08,320
אבל ממש כאן, אתה ואני נשארים ממוקדים ביסודות.

168
00:13:10,560 --> 00:13:15,480
אוקיי, נהדר, מחשוב הדפוס הזה מאפשר למודל להסיק אילו מילים רלוונטיות לאיזה מילים אחרות.

169
00:13:16,020 --> 00:13:22,800
כעת עליך לעדכן בפועל את ההטמעות, ולאפשר למילים להעביר מידע למילים אחרות שהן רלוונטיות לה.

170
00:13:22,800 --> 00:13:28,548
לדוגמה, אתה רוצה שההטבעה של פלאפי תגרום איכשהו לשינוי ב-Creature שיעביר אותו 

171
00:13:28,548 --> 00:13:34,520
לחלק אחר של מרחב ההטמעה הזה ב-12,000 מימדים שמקודד באופן ספציפי יותר יצור פלאפי.

172
00:13:35,460 --> 00:13:40,374
מה שאני הולך לעשות כאן הוא קודם כל להראות לך את הדרך הכי פשוטה שאתה יכול לעשות את זה, 

173
00:13:40,374 --> 00:13:43,460
אם כי יש דרך קלה שזה משתנה בהקשר של תשומת לב רב-ראשית.

174
00:13:44,080 --> 00:13:49,070
הדרך הפשוטה ביותר הזו תהיה להשתמש במטריצה שלישית, מה שאנו מכנים מטריצת הערך, 

175
00:13:49,070 --> 00:13:52,440
אותה אתה מכפיל בהטמעה של המילה הראשונה, למשל Fluffy.

176
00:13:53,300 --> 00:13:58,958
התוצאה של זה היא מה שהיית מכנה וקטור ערך, וזה משהו שאתה מוסיף להטבעה של המילה השנייה, 

177
00:13:58,958 --> 00:14:01,920
במקרה הזה משהו שאתה מוסיף להטבעה של Creature.

178
00:14:02,600 --> 00:14:07,000
אז וקטור הערך הזה חי באותו מרחב מאוד גבוה כמו ההטבעות.

179
00:14:07,460 --> 00:14:10,920
כשאתה מכפיל את מטריצת הערכים הזו בהטמעה של מילה, 

180
00:14:10,920 --> 00:14:17,064
אתה עלול לחשוב על זה כאילו אתה אומר, אם המילה הזו רלוונטית להתאמת המשמעות של משהו אחר, 

181
00:14:17,064 --> 00:14:21,160
מה בדיוק צריך להוסיף להטבעה של אותו משהו אחר כדי לשקף זֶה?

182
00:14:22,140 --> 00:14:26,197
במבט לאחור בתרשים שלנו, נניח בצד את כל המפתחות והשאילתות, 

183
00:14:26,197 --> 00:14:30,603
שכן לאחר שתחשב את דפוס הקשב שסיימת עם אלה, אז אתה הולך לקחת את 

184
00:14:30,603 --> 00:14:36,060
מטריצת הערכים הזו ולהכפיל אותה בכל אחת מההטבעות הללו. לייצר רצף של וקטורי ערך.

185
00:14:37,120 --> 00:14:41,120
אתה עשוי לחשוב על וקטורי ערכים אלה כמקושרים למפתחות המתאימים.

186
00:14:42,320 --> 00:14:49,240
עבור כל עמודה בתרשים זה, אתה מכפיל כל אחד מהוקטורי הערך במשקל המתאים באותה עמודה.

187
00:14:50,080 --> 00:14:55,707
לדוגמה כאן, תחת ההטמעה של Creature, תוסיף פרופורציות גדולות של וקטורי הערך 

188
00:14:55,707 --> 00:15:01,560
עבור Fluffy וכחול, בעוד שכל וקטורי הערך האחרים מתאפסים, או לפחות כמעט מתאפסים.

189
00:15:02,120 --> 00:15:06,126
ולבסוף, הדרך למעשה לעדכן את ההטמעה הקשורה לעמודה הזו, 

190
00:15:06,126 --> 00:15:12,062
שקודדה בעבר איזו משמעות נטולת הקשר של יצור, אתה מוסיף יחד את כל הערכים המותאמים 

191
00:15:12,062 --> 00:15:16,959
מחדש בעמודה, ומייצרים שינוי שאתה רוצה להוסיף, שאני אתייג delta-e, 

192
00:15:16,959 --> 00:15:19,260
ואז תוסיף את זה להטמעה המקורית.

193
00:15:19,680 --> 00:15:25,034
אני מקווה שהתוצאה היא וקטור מעודן יותר המקודד את המשמעות העשירה יותר מבחינה הקשרית, 

194
00:15:25,034 --> 00:15:26,500
כמו זו של יצור כחול רך.

195
00:15:27,380 --> 00:15:32,740
וכמובן שאתה לא עושה את זה רק להטבעה אחת, אתה מחיל את אותו סכום משוקלל על 

196
00:15:32,740 --> 00:15:39,348
פני כל העמודות בתמונה הזו, מייצר רצף של שינויים, הוספת כל השינויים האלה להטבעות המתאימות, 

197
00:15:39,348 --> 00:15:43,460
מייצר רצף מלא של הטבעות מעודנות יותר שיוצאות מבלוק הקשב.

198
00:15:44,860 --> 00:15:49,100
אם תתרחבו, כל התהליך הזה הוא מה שתגדירו כראש תשומת לב יחיד.

199
00:15:49,600 --> 00:15:54,967
כפי שתיארתי דברים עד כה, תהליך זה מותאם לפרמטרים על ידי שלוש מטריצות נפרדות, 

200
00:15:54,967 --> 00:15:58,940
כולן מלאות בפרמטרים הניתנים לשינוי, המפתח, השאילתה והערך.

201
00:15:59,500 --> 00:16:02,753
אני רוצה לקחת רגע כדי להמשיך את מה שהתחלנו בפרק האחרון, 

202
00:16:02,753 --> 00:16:07,110
עם שמירת הניקוד שבה אנחנו סופרים את המספר הכולל של פרמטרים של הדגם באמצעות 

203
00:16:07,110 --> 00:16:08,040
המספרים מ-GPT-3.

204
00:16:09,300 --> 00:16:14,994
למטריצות המפתח והשאילתה הללו יש 12,288 עמודות, התואמות לממד ההטמעה, 

205
00:16:14,994 --> 00:16:19,600
ו-128 שורות, התואמות לממד של מרחב שאילתת מפתח קטן יותר.

206
00:16:20,260 --> 00:16:24,220
זה נותן לנו 1.5 מיליון פרמטרים נוספים לערך עבור כל אחד מהם.

207
00:16:24,860 --> 00:16:30,239
אם אתה מסתכל על מטריצת הערכים הזו לעומת זאת, הדרך שתיארתי את הדברים 

208
00:16:30,239 --> 00:16:36,014
עד כה תצביע על כך שזו מטריצה מרובעת שיש לה 12,288 עמודות ו-12,288 שורות, 

209
00:16:36,014 --> 00:16:40,920
מכיוון שגם הכניסות וגם הפלטים שלה חיים במרחב ההטמעה הגדול הזה.

210
00:16:41,500 --> 00:16:45,140
אם זה נכון, המשמעות היא כ-150 מיליון פרמטרים נוספים.

211
00:16:45,660 --> 00:16:47,300
וכדי להיות ברור, אתה יכול לעשות את זה.

212
00:16:47,420 --> 00:16:51,740
אתה יכול להקדיש למפת הערכים יותר פרמטרים בסדרי גודל מאשר למפתח ולשאילתה.

213
00:16:52,060 --> 00:16:56,129
אבל בפועל, זה הרבה יותר יעיל אם במקום זאת תגרום לכך שמספר 

214
00:16:56,129 --> 00:17:00,760
הפרמטרים המוקדשים למפת הערכים הזו זהה למספר המוקדש למפתח ולשאילתה.

215
00:17:01,460 --> 00:17:05,160
זה רלוונטי במיוחד בהגדרה של הפעלת מספר ראשי קשב במקביל.

216
00:17:06,240 --> 00:17:10,099
האופן שבו זה נראה הוא שמפת הערכים מוערכת כמכפלה של שתי מטריצות קטנות יותר.

217
00:17:11,180 --> 00:17:17,384
מבחינה קונספטואלית, אני עדיין ממליץ לך לחשוב על המפה הליניארית הכוללת, כזו עם קלט ופלט, 

218
00:17:17,384 --> 00:17:23,517
הן במרחב ההטבעה הגדול יותר הזה, למשל לקחת את הטבעת הכחול לכיוון הכחול הזה שתוסיף לשמות 

219
00:17:23,517 --> 00:17:23,800
עצם.

220
00:17:27,040 --> 00:17:32,760
רק שזהו מספר קטן יותר של שורות, בדרך כלל בגודל זהה למרחב שאילתת המפתח.

221
00:17:33,100 --> 00:17:38,440
המשמעות היא שאתה יכול לחשוב על זה כעל מיפוי וקטורי הטבעה גדולים עד למרחב קטן בהרבה.

222
00:17:39,040 --> 00:17:42,700
זה לא השם המקובל, אבל אני הולך לקרוא לזה מטריצת הערך למטה.

223
00:17:43,400 --> 00:17:46,990
המטריצה השנייה ממפה מהמרחב הקטן יותר הזה בחזרה למרחב ההטמעה, 

224
00:17:46,990 --> 00:17:50,580
מייצרת את הוקטורים שבהם אתה משתמש כדי לבצע את העדכונים בפועל.

225
00:17:51,000 --> 00:17:54,740
אני הולך לקרוא לזה מטריצת הערך למעלה, וזה שוב לא קונבנציונלי.

226
00:17:55,160 --> 00:17:58,080
הדרך שבה היית רואה את זה כתוב ברוב העיתונים נראית קצת אחרת.

227
00:17:58,380 --> 00:17:59,520
אני אדבר על זה בעוד דקה.

228
00:17:59,700 --> 00:18:02,540
לדעתי, זה נוטה להפוך את הדברים לקצת יותר מבלבלים מבחינה רעיונית.

229
00:18:03,260 --> 00:18:06,742
אם לזרוק כאן ז&#39;רגון אלגברה ליניארי, מה שאנחנו בעצם עושים 

230
00:18:06,742 --> 00:18:10,340
זה להגביל את מפת הערכים הכוללת להיות טרנספורמציה של דירוג נמוך.

231
00:18:11,420 --> 00:18:16,290
אם נחזור לספירת הפרמטרים, לכל ארבעת המטריצות הללו יש אותו גודל, 

232
00:18:16,290 --> 00:18:20,780
ובחיבור של כולם נקבל כ-6.3 מיליון פרמטרים עבור ראש קשב אחד.

233
00:18:22,040 --> 00:18:26,601
כהערת צד מהירה, אם לדייק קצת יותר, כל מה שתואר עד כה הוא מה שאנשים היו מכנים ראש 

234
00:18:26,601 --> 00:18:31,500
תשומת לב עצמית, כדי להבחין בינו לבין וריאציה שעולה במודלים אחרים שנקראת תשומת לב צולבת.

235
00:18:32,300 --> 00:18:36,203
זה לא רלוונטי לדוגמה שלנו ב-GPT, אבל אם אתה סקרן, 

236
00:18:36,203 --> 00:18:41,121
תשומת לב צולבת כוללת מודלים המעבדים שני סוגים שונים של נתונים, 

237
00:18:41,121 --> 00:18:46,117
כמו טקסט בשפה אחת וטקסט בשפה אחרת שהוא חלק מדור מתמשך של תרגום, 

238
00:18:46,117 --> 00:18:49,240
או אולי קלט אודיו של דיבור ותמלול מתמשך.

239
00:18:50,400 --> 00:18:52,700
ראש עם תשומת לב צולבת נראה כמעט זהה.

240
00:18:52,980 --> 00:18:57,400
ההבדל היחיד הוא שמפות המפתח והמפות פועלות על מערכי נתונים שונים.

241
00:18:57,840 --> 00:19:04,414
במודל שעושה תרגום, למשל, המפתחות עשויים להגיע משפה אחת, בעוד שהשאילתות מגיעות משפה אחרת, 

242
00:19:04,414 --> 00:19:09,660
ודפוס הקשב יכול לתאר אילו מילים משפה אחת מתאימות לאיזה מילים בשפה אחרת.

243
00:19:10,340 --> 00:19:13,191
ובהגדרה הזו בדרך כלל לא יהיה מיסוך, מכיוון שאין 

244
00:19:13,191 --> 00:19:16,340
ממש מושג של אסימונים מאוחרים יותר המשפיעים על קודמים.

245
00:19:17,180 --> 00:19:21,299
עם זאת, אם הייתם מרוכזים בתשומת לב עצמית, אם הייתם מבינים הכל עד כה, 

246
00:19:21,299 --> 00:19:25,180
ואם הייתם עוצרים כאן, הייתם יוצאים עם המהות של מהי תשומת לב באמת.

247
00:19:25,760 --> 00:19:31,440
כל מה שבאמת נשאר לנו הוא לפרט את המובן שבו אתה עושה זאת פעמים רבות ושונות.

248
00:19:32,100 --> 00:19:35,535
בדוגמה המרכזית שלנו התמקדנו בשמות תואר המעדכנים שמות עצם, 

249
00:19:35,535 --> 00:19:39,800
אבל כמובן שיש הרבה דרכים שונות שבהן הקשר יכול להשפיע על המשמעות של מילה.

250
00:19:40,360 --> 00:19:46,520
אם המילים שהם התרסקו לפני המילה מכונית, יש לכך השלכות על הצורה והמבנה של אותה מכונית.

251
00:19:47,200 --> 00:19:49,280
והרבה אסוציאציות עשויות להיות פחות דקדוקיות.

252
00:19:49,760 --> 00:19:53,308
אם המילה אשף נמצאת במקום כלשהו באותו קטע כמו הארי, 

253
00:19:53,308 --> 00:19:58,595
זה מצביע על כך שאולי זה מתייחס להארי פוטר, בעוד שאם במקום זאת המילים קווין, 

254
00:19:58,595 --> 00:20:04,440
סאסקס וויליאם היו בקטע הזה, אז אולי יש לעדכן את הטבעת הארי במקום זאת. להתייחס לנסיך.

255
00:20:05,040 --> 00:20:08,426
עבור כל סוג שונה של עדכון הקשר שאתה יכול לדמיין, 

256
00:20:08,426 --> 00:20:14,025
הפרמטרים של מטריצות מפתח ושאילתות אלה יהיו שונים כדי ללכוד את דפוסי הקשב השונים, 

257
00:20:14,025 --> 00:20:19,140
והפרמטרים של מפת הערכים שלנו יהיו שונים בהתבסס על מה שצריך להוסיף להטמעות.

258
00:20:19,980 --> 00:20:23,695
ושוב, בפועל קשה הרבה יותר לפרש את ההתנהגות האמיתית של מפות אלה, 

259
00:20:23,695 --> 00:20:28,572
כאשר המשקולות מוגדרות לעשות כל מה שהמודל צריך לעשות כדי להגשים בצורה הטובה ביותר את 

260
00:20:28,572 --> 00:20:30,140
מטרתו לחזות את האסימון הבא.

261
00:20:31,400 --> 00:20:35,144
כפי שאמרתי קודם, כל מה שתיארנו הוא ראש קשב יחיד, 

262
00:20:35,144 --> 00:20:39,194
ובלוק קשב מלא בתוך שנאי מורכב ממה שנקרא קשב רב-ראשי, 

263
00:20:39,194 --> 00:20:45,920
שבו אתה מפעיל הרבה מהפעולות האלה במקביל, כל אחת עם שאילתת מפתח נפרדת משלה. ומפות ערכיות.

264
00:20:47,420 --> 00:20:51,700
GPT-3 למשל משתמש ב-96 ראשי קשב בתוך כל בלוק.

265
00:20:52,020 --> 00:20:56,460
בהתחשב בכך שכל אחד מהם כבר קצת מבלבל, אין ספק שזה הרבה מה להחזיק בראש.

266
00:20:56,760 --> 00:21:00,844
רק כדי לאיית את הכל בצורה מאוד מפורשת, זה אומר שיש לך 96 

267
00:21:00,844 --> 00:21:05,000
מטריצות מפתח ושאילתות נפרדות המייצרות 96 דפוסי קשב ברורים.

268
00:21:05,440 --> 00:21:12,180
אז לכל ראש יש מטריצות ערך נפרדות משלו המשמשות לייצור 96 רצפים של וקטורי ערך.

269
00:21:12,460 --> 00:21:16,680
כל אלה מתווספים יחד באמצעות דפוסי הקשב המתאימים כמשקולות.

270
00:21:17,480 --> 00:21:21,919
המשמעות היא שלכל עמדה בהקשר, כל אסימון, כל אחד 

271
00:21:21,919 --> 00:21:27,020
מהראשים הללו מייצר שינוי מוצע שיתווסף להטמעה בעמדה זו.

272
00:21:27,660 --> 00:21:31,408
אז מה שאתה עושה זה שאתה מסכם את כל השינויים המוצעים האלה, 

273
00:21:31,408 --> 00:21:35,480
אחד לכל ראש, ואתה מוסיף את התוצאה להטבעה המקורית של המיקום הזה.

274
00:21:36,660 --> 00:21:42,752
כל הסכום הזה כאן יהיה נתח אחד ממה שמופק מבלוק הקשב רב-הראשים הזה, 

275
00:21:42,752 --> 00:21:47,460
אחד מההטבעות המעודנות האלה שמבצבצות מהקצה השני שלו.

276
00:21:48,320 --> 00:21:52,140
שוב, זה הרבה לחשוב על כך, אז אל תדאג בכלל אם לוקח קצת זמן לשקוע.

277
00:21:52,380 --> 00:21:56,358
הרעיון הכללי הוא שעל ידי הפעלת ראשים שונים במקביל, 

278
00:21:56,358 --> 00:22:01,820
אתה נותן למודל את היכולת ללמוד דרכים רבות ומובחנות שבהקשר משנה משמעות.

279
00:22:03,700 --> 00:22:09,322
כשמגדילים את ספירת הפרמטרים שלנו עם 96 ראשים, שכל אחד מהם כולל את הווריאציה שלו של 

280
00:22:09,322 --> 00:22:15,080
ארבעת המטריצות הללו, כל בלוק של תשומת לב רב-ראשית מסתיימת בסביבות 600 מיליון פרמטרים.

281
00:22:16,420 --> 00:22:21,800
יש עוד דבר אחד קצת מעצבן שאני באמת צריך להזכיר עבור כל אחד מכם שימשיך לקרוא עוד על שנאים.

282
00:22:22,080 --> 00:22:26,536
אתה זוכר איך אמרתי שמפת הערכים מחולקת לשתי המטריצות הנבדלות הללו, 

283
00:22:26,536 --> 00:22:29,440
אותן תייגתי כמטריצות הערך למטה והערך למעלה.

284
00:22:29,960 --> 00:22:36,259
האופן שבו ניסחתי את הדברים יציע לך לראות את צמד המטריצות הזה בתוך כל ראש קשב, 

285
00:22:36,259 --> 00:22:38,440
ותוכל ליישם את זה בצורה זו.

286
00:22:38,640 --> 00:22:39,920
זה יהיה עיצוב תקף.

287
00:22:40,260 --> 00:22:44,920
אבל הדרך שבה אתה רואה את זה כתוב בעיתונים והדרך שבה זה מיושם בפועל נראית קצת אחרת.

288
00:22:45,340 --> 00:22:50,860
כל מטריצות הערך למעלה הללו עבור כל ראש מופיעות מהודקות יחדיו במטריצה 

289
00:22:50,860 --> 00:22:56,380
ענקית אחת שאנו קוראים לה מטריצת הפלט, הקשורה לכל בלוק הקשב רב הראשים.

290
00:22:56,820 --> 00:23:01,202
וכשאתה רואה אנשים מתייחסים למטריצת הערך של ראש תשומת לב נתון, 

291
00:23:01,202 --> 00:23:07,140
הם בדרך כלל מתייחסים רק לשלב הראשון הזה, זה שסימנתי כהקרנת הערך למטה לחלל הקטן יותר.

292
00:23:08,340 --> 00:23:11,040
לסקרנים שביניכם, השארתי על כך הערה על המסך.

293
00:23:11,260 --> 00:23:14,874
זה אחד מהפרטים האלה שמסתכנים להסיח את הדעת מהנקודות המושגיות העיקריות, 

294
00:23:14,874 --> 00:23:18,540
אבל אני כן רוצה לקרוא את זה רק כדי שתדע אם אתה קורא על זה במקורות אחרים.

295
00:23:19,240 --> 00:23:23,639
אם שמים בצד את כל הניואנסים הטכניים, בתצוגה המקדימה מהפרק האחרון 

296
00:23:23,639 --> 00:23:28,040
ראינו כיצד נתונים שזורמים דרך שנאי לא זורמים רק דרך בלוק קשב אחד.

297
00:23:28,640 --> 00:23:32,700
ראשית, הוא עובר גם את הפעולות האחרות הנקראות תפיסות רב-שכבתיות.

298
00:23:33,120 --> 00:23:34,880
על אלה נדבר יותר בפרק הבא.

299
00:23:35,180 --> 00:23:39,320
ואז זה עובר שוב ושוב דרך הרבה הרבה עותקים של שתי הפעולות האלה.

300
00:23:39,980 --> 00:23:44,066
המשמעות היא שאחרי שמילה נתונה תופסת חלק מההקשר שלה, 

301
00:23:44,066 --> 00:23:50,040
יש הרבה יותר סיכויים שההטבעה הניואנסית הזו תושפע מהסביבה הניואנסית יותר שלה.

302
00:23:50,940 --> 00:23:57,064
ככל שאתה הולך למטה ברשת, כאשר כל הטבעה מקבלת יותר ויותר משמעות מכל ההטבעות האחרות, 

303
00:23:57,064 --> 00:24:02,671
שהן עצמן נעשות יותר ויותר ניואנסים, התקווה היא שיש יכולת לקודד רעיונות ברמה 

304
00:24:02,671 --> 00:24:07,320
גבוהה ומופשטת יותר לגבי נתון. קלט מעבר רק לתארים ולמבנה דקדוקי.

305
00:24:07,880 --> 00:24:15,130
דברים כמו סנטימנט וטון והאם זה שיר ואיזה אמיתות מדעיות בבסיס רלוונטיות ליצירה ודברים כאלה.

306
00:24:16,700 --> 00:24:22,415
אם נחזור פעם נוספת לשמירה על הניקוד שלנו, GPT-3 כולל 96 שכבות נפרדות, 

307
00:24:22,415 --> 00:24:27,233
כך שהמספר הכולל של שאילתת מפתח וערך פרמטרים מוכפל בעוד 96, 

308
00:24:27,233 --> 00:24:34,500
מה שמביא את הסכום הכולל לקצת פחות מ-58 מיליארד פרמטרים נפרדים המוקדשים לכל ראשי תשומת לב.

309
00:24:34,980 --> 00:24:40,940
זה הרבה מה להיות בטוח, אבל זה רק כשליש מתוך 175 מיליארד שיש ברשת בסך הכל.

310
00:24:41,520 --> 00:24:44,761
אז למרות שתשומת הלב מקבלת את כל תשומת הלב, רוב 

311
00:24:44,761 --> 00:24:48,140
הפרמטרים מגיעים מהבלוקים שיושבים בין השלבים האלה.

312
00:24:48,560 --> 00:24:53,560
בפרק הבא, אתה ואני נדבר יותר על הבלוקים האחרים האלה וגם הרבה יותר על תהליך האימון.

313
00:24:54,120 --> 00:25:00,769
חלק גדול מהסיפור להצלחת מנגנון הקשב הוא לא כל כך סוג ספציפי של התנהגות שהוא מאפשר, 

314
00:25:00,769 --> 00:25:07,899
אלא העובדה שהוא מקביל ביותר, כלומר, אתה יכול להריץ מספר עצום של חישובים בזמן קצר באמצעות 

315
00:25:07,899 --> 00:25:08,380
GPUs .

316
00:25:09,460 --> 00:25:13,231
בהתחשב בעובדה שאחד הלקחים הגדולים לגבי למידה עמוקה בעשור או שניים 

317
00:25:13,231 --> 00:25:17,402
האחרונים היה שקנה המידה לבדו נותן שיפורים איכותיים עצומים בביצועי המודל, 

318
00:25:17,402 --> 00:25:21,060
יש יתרון עצום לארכיטקטורות הניתנות להקבלה שמאפשרות לך לעשות זאת.

319
00:25:22,040 --> 00:25:25,340
אם אתה רוצה ללמוד עוד על החומר הזה, השארתי הרבה קישורים בתיאור.

320
00:25:25,920 --> 00:25:30,040
בפרט, כל דבר שמיוצר על ידי אנדריי קרפטי או כריס אולה נוטה להיות זהב טהור.

321
00:25:30,560 --> 00:25:33,433
בסרטון הזה, רציתי רק לקפוץ לתשומת לב בצורתו הנוכחית, 

322
00:25:33,433 --> 00:25:37,498
אבל אם אתה סקרן לגבי עוד מההיסטוריה של איך הגענו לכאן ואיך אתה יכול להמציא 

323
00:25:37,498 --> 00:25:41,455
את הרעיון הזה מחדש לעצמך, ידידי Vivek פשוט העלה זוג סרטונים שנותנים הרבה 

324
00:25:41,455 --> 00:25:42,540
יותר מהמוטיבציה הזו.

325
00:25:43,120 --> 00:25:45,736
כמו כן, לבריט קרוז מהערוץ The Art of the Problem 

326
00:25:45,736 --> 00:25:48,460
יש סרטון ממש נחמד על ההיסטוריה של דגמי שפות גדולים.

327
00:26:04,960 --> 00:26:09,200
תודה.

