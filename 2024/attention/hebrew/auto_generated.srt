1
00:00:00,000 --> 00:00:04,019
בפרק האחרון, אתם ואני התחלנו לעבור דרך פעולתו הפנימית של טרמספורמר.

2
00:00:04,560 --> 00:00:10,200
זוהי אחת מהטכנולוגיות המרכזיות במודלים של שפות גדולות, והרבה כלים אחרים בגל המודרני של AI.

3
00:00:10,980 --> 00:00:15,240
הוא פורסם לראשונה במאמר מ-2017 בשם Attention is All You Need, 

4
00:00:15,240 --> 00:00:20,188
ובפרק זה אתם ואני נעמיק באופן חזותי במהו מנגנון המיקוד (attention) הזה, 

5
00:00:20,188 --> 00:00:21,700
וכיצד הוא מעבד נתונים.

6
00:00:26,140 --> 00:00:29,540
כסיכום קצר, הנה ההקשר החשוב שאני רוצה שתזכרו.

7
00:00:30,000 --> 00:00:36,060
המטרה של המודל שאתם ואני לומדים היא לקחת חלק מטקסט ולנבא איזו מילה מגיעה אחר כך.

8
00:00:36,860 --> 00:00:40,267
טקסט הקלט מחולק לחתיכות קטנות שאנו מכנים טוקנים, 

9
00:00:40,267 --> 00:00:44,787
ולעתים קרובות אלו הן מילים או פיסות מילים, אבל רק כדי להקל עלינו 

10
00:00:44,787 --> 00:00:50,560
לחשוב על הדוגמאות בסרטון הזה, בואו נפשט על ידי העמדת פנים שטוקנים הם תמיד רק מילים.

11
00:00:51,480 --> 00:00:57,700
השלב הראשון בטרמספורמר הוא לשייך כל טוקן לוקטור בעל מימד גבוה, מה שאנחנו מכנים שיכון שלו.

12
00:00:57,700 --> 00:01:02,020
הרעיון החשוב ביותר שאני רוצה שתזכרו הוא כיצד כיוונים במרחב 

13
00:01:02,020 --> 00:01:07,000
הרב-ממדי הזה של כל השיכונים האפשריים יכולים להתכתב עם משמעות סמנטית.

14
00:01:07,680 --> 00:01:11,930
בפרק האחרון ראינו דוגמה לאופן שבו כיוון יכול להתאים למגדר, 

15
00:01:11,930 --> 00:01:17,910
במובן זה שהוספת וקטור מסוים במרחב הזה יכולה לקחת אותך משיכון של שם עצם זכרי לשיכון 

16
00:01:17,910 --> 00:01:19,640
של שם העצם הנקבי המקביל.

17
00:01:20,160 --> 00:01:23,907
זו רק דוגמה אחת וכיוונים אחרים במרחב הרב-ממדי הזה 

18
00:01:23,907 --> 00:01:27,580
יכולים להתאים להרבה היבטים אחרים של משמעות המילה.

19
00:01:28,800 --> 00:01:35,296
המטרה של טרמספורמר היא להתאים בהדרגה את השיכונים הללו כך שהם לא רק מקודדים מילה בודדת, 

20
00:01:35,296 --> 00:01:39,180
אלא במקום זאת הם מכילים משמעות הרבה הרבה יותר עשירה.

21
00:01:40,140 --> 00:01:45,638
אני צריך לומר מראש שהרבה אנשים מוצאים את מנגנון המיקוד (attention) מאוד מבלבל, 

22
00:01:45,638 --> 00:01:48,980
אז אל תדאגו אם לוקח קצת זמן עד שתעכלו אם הדברים.

23
00:01:49,440 --> 00:01:54,372
אני חושב שלפני שאנחנו צוללים לפרטים החישוביים ולכל מכפלות המטריצות, 

24
00:01:54,372 --> 00:01:59,160
כדאי לחשוב על כמה דוגמאות לסוג ההתנהגות שאנחנו רוצים שמיקוד יאפשר.

25
00:02:00,140 --> 00:02:03,150
חישבו על הביטויים מול (mole) אמריקאי (חדף בעברית), 

26
00:02:03,150 --> 00:02:06,220
מול של פחמן דו חמצני, וביופסיה של שומה (מול באגלית).

27
00:02:06,700 --> 00:02:10,900
אנחנו יודעים שלמילה mole יש משמעות שונה בכל אחד מהם, בהתבסס על ההקשר.

28
00:02:11,360 --> 00:02:17,006
אבל לאחר השלב הראשון של טרמספורמר, זה שמפרק את הטקסט ומשייך כל טוקן לוקטור, 

29
00:02:17,006 --> 00:02:20,573
הווקטור המשויך ל-mole יהיה זהה בכל המקרים האלה, 

30
00:02:20,573 --> 00:02:26,220
מכיוון שהשיכון הראשוני של הטוקן הזה הוא למעשה טבלת חיפוש ללא התייחסות להקשר.

31
00:02:26,620 --> 00:02:33,100
רק בשלב הבא של הטרמספורמר יש לשיכונים שמסביב הזדמנות להעביר מידע לתוכו.

32
00:02:33,820 --> 00:02:39,836
התמונה שאולי תזכרו היא שיש מספר כיוונים ברורים במרחב השיכון הזה המקודדים את המשמעויות 

33
00:02:39,836 --> 00:02:45,713
המובהקות של המילה mole, ושבלוק מיקוד (attention block) מיומן מחשב את מה שאתם צריכים 

34
00:02:45,713 --> 00:02:51,800
להוסיף לשיכון הגנרי כדי להעביר אותו אל אחד מהכיוונים הספציפיים הללו, כפונקציה של ההקשר.

35
00:02:53,300 --> 00:02:56,180
כדוגמה נוספת, חישבו על שיכון המילה מגדל.

36
00:02:57,060 --> 00:03:03,720
זהו כנראה איזשהו כיוון מאוד כללי במרחב, הקשור להרבה שמות עצם גדולים וגבוהים אחרים.

37
00:03:04,020 --> 00:03:08,885
אם מיד אחרי המילה הזאת מופיע אייפל, הייתם רוצים שהמנגנון יעדכן את 

38
00:03:08,885 --> 00:03:14,120
הווקטור הזה כך שיצביע על כיוון שמקודד באופן ספציפי יותר את מגדל אייפל, 

39
00:03:14,120 --> 00:03:19,060
אולי בקורלציה עם וקטורים הקשורים לפריז וצרפת ולדברים העשויים מפלדה.

40
00:03:19,920 --> 00:03:24,270
אם הופיע לפניה המילה מיניאטורה, אז הווקטור צריך להתעדכן עוד יותר, 

41
00:03:24,270 --> 00:03:27,500
כך שהוא כבר לא יהי מתואם עם דברים גדולים וגבוהים.

42
00:03:29,480 --> 00:03:32,991
באופן כללי יותר מאשר רק חידוד המשמעות של מילה, 

43
00:03:32,991 --> 00:03:37,921
בלוק המיקוד מאפשר למודל להעביר מידע המקודד בשיכון אחד לזה של אחר, 

44
00:03:37,921 --> 00:03:43,300
אולי כאלו שנמצאים די רחוק, ואולי עם מידע הרבה יותר עשיר מסתם מילה בודדת.

45
00:03:43,300 --> 00:03:48,534
מה שראינו בפרק האחרון היה איך אחרי שכל הוקטורים עוברים דרך הרשת, 

46
00:03:48,534 --> 00:03:53,286
כולל בלוקי מיקוד רבים ושונים, החישוב שאתם מבצעים כדי לייצר 

47
00:03:53,286 --> 00:03:58,280
חיזוי של הטוקן הבא הוא לחלוטין פונקציה של הווקטור האחרון ברצף.

48
00:03:59,100 --> 00:04:04,175
תארו לעצמכם, למשל, שהטקסט שאתם מזינים הוא רובו של של רומן מתח, 

49
00:04:04,175 --> 00:04:07,800
עד לנקודה סמוך לסוף, שם נכתב "לכן הרוצח היה".

50
00:04:08,400 --> 00:04:13,003
אם המודל ינבא במדויק את המילה הבאה, אותו וקטור אחרון ברצף, 

51
00:04:13,003 --> 00:04:19,558
שהתחיל את חייו פשוט כשיכון שלהמילה "היה", יצטרך להיות מעודכן על ידי כל בלוקי המיקוד 

52
00:04:19,558 --> 00:04:26,113
כדי לייצג הרבה הרבה יותר מכל מילה בודדת, ואיכשהו יקודד את כל המידע מחלון ההקשר המלא 

53
00:04:26,113 --> 00:04:28,220
הרלוונטי לניבוי המילה הבאה.

54
00:04:29,500 --> 00:04:32,580
עם זאת, כדי לעבור דרך החישובים, בואו ניקח דוגמה הרבה יותר פשוטה.

55
00:04:32,980 --> 00:04:37,960
תארו לעצמכם שהקלט כולל את הביטוי "יצור כחול פרוותי שוטט ביער המוריק".

56
00:04:38,460 --> 00:04:42,620
וכרגע נניח שסוג העדכון היחיד שמעניין אותנו הוא שינוי 

57
00:04:42,620 --> 00:04:46,780
המשמעויות של שמות העצם על ידי שמות התואר התואמים להם.

58
00:04:47,000 --> 00:04:50,688
מה שאני עומד לתאר זה מה שאנחנו מכנים ראש מיקוד בודד, 

59
00:04:50,688 --> 00:04:55,420
ובהמשך נראה כיצד בלוק המיקוד מורכב מהרבה ראשים שונים הפועלים במקביל.

60
00:04:56,140 --> 00:04:59,896
שוב, השיכון הראשוני של כל מילה הוא וקטור בעל מימד גבוה 

61
00:04:59,896 --> 00:05:03,380
שמקודד רק את המשמעות של המילה המסוימת הזו ללא הקשר.

62
00:05:04,000 --> 00:05:05,220
למעשה, זה לא ממש נכון.

63
00:05:05,380 --> 00:05:07,640
הם גם מקודדים את המיקום של המילה.

64
00:05:07,980 --> 00:05:11,318
יש עוד הרבה מה לומר על קידוד מיקומים, אבל כרגע, 

65
00:05:11,318 --> 00:05:16,743
כל מה שאתם צריכים לדעת הוא שהערכים של הווקטור הזה מספיקים כדי לספר לכם גם מהי 

66
00:05:16,743 --> 00:05:18,900
המילה וגם היכן היא קיימת בהקשר.

67
00:05:19,500 --> 00:05:21,660
בואו נמשיך ונציין את השיכונים הללו באות E.

68
00:05:22,420 --> 00:05:27,840
המטרה היא שסדרה של חישובים תייצר קבוצה מעודנת חדשה של שיכונים שבהם, 

69
00:05:27,840 --> 00:05:33,420
למשל, אלו התואמים לשמות העצם ספגו את המשמעות משמות התואר התואמים שלהם.

70
00:05:33,900 --> 00:05:38,975
ובלמידה העמוקה, אנחנו רוצים שרוב החישובים ייראו כמו מכפלת מטריצה-וקטור, 

71
00:05:38,975 --> 00:05:43,980
שבהם המטריצות מלאות במשקלים הניתנים לשינוי, ושהמודל ילמד על סמך נתונים.

72
00:05:44,660 --> 00:05:48,404
כדי להיות ברור, אני ממציא את הדוגמה הזו של שמות תואר המעדכנים שמות 

73
00:05:48,404 --> 00:05:52,260
עצם רק כדי להמחיש את סוג ההתנהגות שאתם יכולים לדמיין שראש מיקוד עושה.

74
00:05:52,860 --> 00:05:57,044
כמו בהרבה נושאים בלמידה עמוקה, ההתנהגות האמיתית הרבה יותר קשה לנתוח מכיוון 

75
00:05:57,044 --> 00:06:01,340
שהיא מבוססת על כיוונון של מספר עצום של פרמטרים כדי למזער פונקציית עלות כלשהי.

76
00:06:01,680 --> 00:06:06,893
כשאנחנו עוברים דרך כל המטריצות השונות המלאות בפרמטרים שמעורבים בתהליך הזה, 

77
00:06:06,893 --> 00:06:12,663
אני חושב שממש מועיל לקבל דוגמה דמיונית למשהו שהוא יכול לעשות כדי לשמור על הכל יותר 

78
00:06:12,663 --> 00:06:13,220
קונקרטי.

79
00:06:14,140 --> 00:06:17,780
בשלב הראשון של התהליך, אתם עשויים לחשוב על כל שם עצם, 

80
00:06:17,780 --> 00:06:21,960
כמו יצור, שואל את השאלה: "היי, האם יש שמות תואר שיושבים מולי"?

81
00:06:22,160 --> 00:06:27,960
ולגבי המילים פרוותי וכחול, שכל אחת תוכל לענות: "כן, אני שם תואר ואני במיקום הזה".

82
00:06:28,960 --> 00:06:33,336
השאלה הזו מקודדת איכשהו כעוד וקטור, עוד רשימה של מספרים, 

83
00:06:33,336 --> 00:06:36,100
שאנו קוראים לה השאילתה של המילה הזו.

84
00:06:36,980 --> 00:06:42,020
עם זאת, לוקטור השאילתה הזה יש ממד קטן בהרבה מהוקטור המשוכן, נניח 128.

85
00:06:42,940 --> 00:06:49,780
חישוב השאילתה הזו נראה כמו לקיחת מטריצה מסוימת, שאתן לה שם Wq, ולהכפיל אותה בשיכון.

86
00:06:50,960 --> 00:06:55,252
אם נדחס קצת את הדברים, נכתוב את וקטור השאילתה הזה בתור Q, 

87
00:06:55,252 --> 00:06:59,175
ואז בכל פעם שאתם רואים אותי שם מטריצה ליד חץ כמו זה, 

88
00:06:59,175 --> 00:07:04,800
הכוונה היא שכפל המטריצה הזו בווקטור בתחילת החץ נותן לכם את הווקטור בקצה החץ.

89
00:07:05,860 --> 00:07:09,837
במקרה זה, אתם מכפילים את המטריצה הזו בכל השיכונים שבהקשר, 

90
00:07:09,837 --> 00:07:12,580
ומייצרים וקטור שאילתה אחד עבור כל טוקן .

91
00:07:13,740 --> 00:07:19,517
הערכים של המטריצה הזו הם פרמטרים של המודל, כלומר ההתנהגות האמיתית נלמדת מנתונים, 

92
00:07:19,517 --> 00:07:23,440
ובפועל, מאתגר לנתח מה שמטריצה זו עושה בראש מיקוד מסוים.

93
00:07:23,900 --> 00:07:28,304
אבל לצרכינו, נדמיין דוגמה שבה מטריצת השאילתה הזו ממפה את 

94
00:07:28,304 --> 00:07:33,481
השיכונים של שמות עצם לכיוונים מסוימים במרחב השאילתה הקטן יותר הזה, 

95
00:07:33,481 --> 00:07:38,040
שמקודד איכשהו את הרעיון של חיפוש שמות תואר במיקומים קודמים.

96
00:07:38,780 --> 00:07:41,440
לגבי מה זה עושה לשיכונים אחרים, מי יודע?

97
00:07:41,720 --> 00:07:44,340
אולי הוא מנסה במקביל להשיג מטרה אחרת עם אלה.

98
00:07:44,540 --> 00:07:47,160
כרגע, אנחנו מתמקדים רק בשמות העצם.

99
00:07:47,280 --> 00:07:51,796
יחד עם זאת, קשורה לזה מטריצה שנייה שנקראת מטריצת המפתח, 

100
00:07:51,796 --> 00:07:54,620
שגם אותה מכפילים בכל אחד מהשיכונים.

101
00:07:55,280 --> 00:07:58,500
זה מייצר רצף שני של וקטורים שאנחנו קוראים להם המפתחות.

102
00:07:59,420 --> 00:08:03,140
קונספטואלית, אתם רוצים לחשוב על המפתחות כמענה פוטנציאלי לשאילתות.

103
00:08:03,840 --> 00:08:08,328
מטריצת מפתח זו מלאה גם בפרמטרים הניתנים לכוונון, ובדיוק כמו מטריצת השאילתה, 

104
00:08:08,328 --> 00:08:11,400
היא ממפה את וקטורי השיכון לאותו מרחב ממימד קטן יותר.

105
00:08:12,200 --> 00:08:17,020
תחשבו על המפתחות כמתאימים לשאילתות בכל פעם שהם חופפים במידה רבה זה לזה.

106
00:08:17,460 --> 00:08:22,288
בדוגמה שלנו, דמיינו שמטריצת המפתח ממפה את שמות התואר כמו פרוותי 

107
00:08:22,288 --> 00:08:26,740
וכחול לוקטורים שחופפים במידה רבה לשאילתה שמפיקה המילה יצור.

108
00:08:27,200 --> 00:08:30,364
כדי למדוד עד כמה כל מפתח מתאים לכל שאילתה, אתם 

109
00:08:30,364 --> 00:08:34,000
מחשבים את המכפלה הפנימית בין כל זוג מפתח-שאילתה אפשרי.

110
00:08:34,480 --> 00:08:38,431
אני אוהב לדמיין רשת מלאה בנקודות, שבה הנקודות הגדולות יותר מתאימות 

111
00:08:38,431 --> 00:08:42,559
למכפלות הפנימיות הגדולות יותר, המקומות שבהם המפתחות והשאילתות מתאימים.

112
00:08:43,280 --> 00:08:46,850
לדוגמא של שם התואר שלנו, זה ייראה קצת יותר כך, 

113
00:08:46,850 --> 00:08:53,382
כאשר אם המפתחות המופקים על ידי פרוותי וכחול באמת מתאימים לשאילתה המיוצרת על ידי יצור, 

114
00:08:53,382 --> 00:08:58,320
אז המכפלות הפנימיות בשתי הנקודות הללו יהיו מספרים חיוביים גדולים.

115
00:08:59,100 --> 00:09:02,260
 אנשי למידת מכונה היו אומרים שהשיכונים של פרוותי 

116
00:09:02,260 --> 00:09:05,420
וכחול "...Attend to" (ממקדים ל...) שיכון של יצור.

117
00:09:06,040 --> 00:09:10,432
בניגוד למכפלה פנימית בין המפתח למילה אחרת, כמו the, 

118
00:09:10,432 --> 00:09:16,600
לשאילתה עבור יצור יהיה איזשהו ערך קטן או שלילי שמשקף שאינם קשורים זה לזה.

119
00:09:17,700 --> 00:09:23,510
אז יש לנו את הרשת הזו של ערכים שיכולים להיות כל מספר ממשי ממינוס אינסוף עד אינסוף, 

120
00:09:23,510 --> 00:09:28,480
שנותנת לנו ציון עד כמה כל מילה רלוונטית לעדכון המשמעות של כל מילה אחרת.

121
00:09:29,200 --> 00:09:32,454
הדרך שבה אנחנו עומדים להשתמש בציונים האלה היא 

122
00:09:32,454 --> 00:09:35,780
לקחת סכום משוקלל לפי הרלוונטיות לאורך כל עמודה.

123
00:09:36,520 --> 00:09:40,307
אז במקום שיהיו לנו ערכים בין מינוס אינסוף לאינסוף, 

124
00:09:40,307 --> 00:09:46,100
מה שאנחנו רוצים זה שהמספרים בעמודות האלה יהיו בין 0 ל-1, וכל עמודה תסתכם ל-1, 

125
00:09:46,100 --> 00:09:48,180
כאילו היו התפלגות הסתברותית.

126
00:09:49,280 --> 00:09:52,220
אם אתם מגיעים מהפרק האחרון, אתם יודעים מה אנחנו צריכים לעשות.

127
00:09:52,620 --> 00:09:57,300
אנחנו מחשבים softmax לאורך כל אחת מהעמודות הללו כדי לנרמל את הערכים.

128
00:10:00,060 --> 00:10:05,860
בתמונה שלנו, לאחר שתחילו את softmax על כל העמודות, נמלא את הרשת בערכים המנורמלים הללו.

129
00:10:06,780 --> 00:10:10,715
בשלב זה אתם יכולים לחשוב על כל עמודה כעל מתן משקלים לפי 

130
00:10:10,715 --> 00:10:14,580
מידת הרלוונטיות של המילה משמאל לערך המתאים בחלק העליון.

131
00:10:15,080 --> 00:10:16,840
אנו קוראים לרשת הזו דפוס מיקוד (attention pattern).

132
00:10:18,080 --> 00:10:20,580
עכשיו אם אתם מסתכלים על מאמר הטרמספורמר המקורי, 

133
00:10:20,580 --> 00:10:22,820
יש דרך ממש קומפקטית שבה הם כותבים את כל זה.

134
00:10:23,880 --> 00:10:29,332
כאן המשתנים Q ו-K מייצגים את המערכים המלאים של וקטורי שאילתה ומפתח בהתאמה, 

135
00:10:29,332 --> 00:10:34,640
אותם וקטורים קטנים שמקבלים על ידי הכפלת השיכונים במטריצות השאילתה והמפתח.

136
00:10:35,160 --> 00:10:38,990
הביטוי הזה למעלה במונה הוא דרך ממש קומפקטית לייצג את הרשת 

137
00:10:38,990 --> 00:10:43,020
של כל המכפלות הפנימיות האפשריות בין זוגות של מפתחות ושאילתות.

138
00:10:44,000 --> 00:10:47,890
פרט טכני קטן שלא הזכרתי הוא שלצורך יציבות מספרית, 

139
00:10:47,890 --> 00:10:53,960
במקרה מועיל לחלק את כל הערכים הללו בשורש הריבועי של הממד במרחב שאילתת מפתח זה.

140
00:10:54,480 --> 00:11:00,800
אז ה-softmax הזה שעוטף את הביטוי המלא אמור להיות מובן כמיושם טור אחרי טור.

141
00:11:01,640 --> 00:11:04,700
לגבי המונח V הזה, נדבר עליו בעוד שנייה.

142
00:11:05,020 --> 00:11:08,460
לפני כן, יש עוד פרט טכני אחד שעד כה דילגתי עליו.

143
00:11:09,040 --> 00:11:14,242
במהלך תהליך האימון, כאשר אתם מפעילים את המודל הזה על דוגמה של טקסט נתון, 

144
00:11:14,242 --> 00:11:19,658
וכל המשקולות מותאמות מעט ומכווננות כדי לתגמל או להעניש אותו על סמך ההסתברות 

145
00:11:19,658 --> 00:11:25,431
הגבוהה שהוא מקצה למילה הבאה האמיתית בקטע, מסתבר שכל תהליך האימון הופך להרבה יותר 

146
00:11:25,431 --> 00:11:31,560
יעיל אם אתם גורמים לו לנבא בו זמנית כל טוקן הבא האפשרי בעקבות כל רצף התחלתי של טוקנים.

147
00:11:31,940 --> 00:11:35,454
לדוגמה, עם הביטוי שבו התמקדנו, המודל עשוי להיות גם של 

148
00:11:35,454 --> 00:11:39,100
אילו מילים עוקבות אחרי יצור ואילו מילים עוקבות אחרי the.

149
00:11:39,940 --> 00:11:45,560
זה ממש נחמד, כי זה אומר שמה שאחרת היה דוגמה אחת לאימון פועל ביעילות כמו הרבה מהן.

150
00:11:46,100 --> 00:11:51,206
למטרות דפוס המיקוד שלנו, זה אומר שאתם אף פעם לא רוצים לאפשר למילים מאוחרות 

151
00:11:51,206 --> 00:11:56,040
יותר להשפיע על מילים קודמות, כי אחרת הן יכולות להשפיע למה שיבוא אחר כך.

152
00:11:56,560 --> 00:12:00,784
המשמעות היא שאנחנו רוצים שכל הנקודות האלה כאן שמייצגות טוקנים 

153
00:12:00,784 --> 00:12:04,600
מאוחרים יותר המשפיעים על הקודמים, ייאולצו איכשהו להתאפס.

154
00:12:05,920 --> 00:12:09,518
הדבר הפשוט ביותר שאתם עשויים לחשוב עליו הוא להגדיר אותם כאפס, 

155
00:12:09,518 --> 00:12:12,420
אבל אם אתם עושים את זה סכום העמודות לא יסתכמו ל-1.

156
00:12:13,120 --> 00:12:16,228
אז במקום זאת, דרך נפוצה היא שלפני הפעלת softmax, 

157
00:12:16,228 --> 00:12:19,020
אתם מגדירים את כל הערכים האלה כמינוס אינסוף.

158
00:12:19,680 --> 00:12:25,180
אם אתם עושים את זה, אז לאחר הפעלת softmax הם הופכים לאפס, אבל העמודות נשארות מנורמלות.

159
00:12:26,000 --> 00:12:27,540
תהליך זה נקרא מיסוך.

160
00:12:27,540 --> 00:12:32,227
ישנן גרסאות של מיקוד שבהן לא מיישמים את הזה, אבל בדוגמת GPT שלנו, 

161
00:12:32,227 --> 00:12:36,417
למרות שזה רלוונטי יותר בשלב הלימוד מהפעלתו למשל כצ'אט בוט, 

162
00:12:36,417 --> 00:12:41,460
אתם תמיד מיישמים מיסוך כדי למנוע מטוקנים מאוחרים יותר להשפיע על קודמים.

163
00:12:42,480 --> 00:12:45,915
עובדה נוספת שכדאי לחשוב עליה לגבי דפוס המיקוד 

164
00:12:45,915 --> 00:12:49,500
הזה היא כיצד גודלו שווה לריבוע של גודל הקונטקסט.

165
00:12:49,900 --> 00:12:54,176
זו הסיבה שגודל הקונטקסט יכול להוות צוואר בקבוק עצום עבור מודלים של שפות גדולות, 

166
00:12:54,176 --> 00:12:55,620
והגדלה שלו אינה טריוויאלית.

167
00:12:56,300 --> 00:13:02,228
כפי שאתם מתארים לעצמכם, בשנים האחרונות פותחו כמה וריאציות למנגנון המיקוד 

168
00:13:02,228 --> 00:13:08,320
שמטרתן להפוך את הקונטקסט לניתן יותר להרחבה, אבל אנחנו נשאר ממוקדים ביסודות.

169
00:13:10,560 --> 00:13:15,480
אוקיי, נהדר, חישוב הדפוס הזה מאפשר למודל להסיק אילו מילים רלוונטיות לאילו מילים אחרות.

170
00:13:16,020 --> 00:13:19,631
כעת עליכם לעדכן בפועל את השיכונים, ולאפשר למילים 

171
00:13:19,631 --> 00:13:22,800
להעביר מידע למילים אחרות שהן רלוונטיות להן.

172
00:13:22,800 --> 00:13:28,773
לדוגמה, אתם רוצים שהשיכון של פרוותי יגרום איכשהו לשינוי ביצור שיעביר אותו לחלק 

173
00:13:28,773 --> 00:13:34,520
אחר של מרחב השיכון הזה ב-12,000 מימדים שמקודד באופן ספציפי יותר יצור פרוותי.

174
00:13:35,460 --> 00:13:40,734
מה שאני הולך לעשות כאן הוא קודם כל להראות לכם את הדרך הכי פשוטה שאתם יכולים לעשות את זה, 

175
00:13:40,734 --> 00:13:43,460
אם כי יש דרך קצת שונה בהקשר של מיקוד רב-ראשים.

176
00:13:44,080 --> 00:13:48,967
הדרך הפשוטה ביותר הזו תהיה להשתמש במטריצה שלישית, שאנחנו מכנים מטריצת הערך, 

177
00:13:48,967 --> 00:13:52,440
אותה אתם מכפילים בשיכון של המילה הראשונה, למשל פרוותי.

178
00:13:53,300 --> 00:13:59,785
התוצאה היא מה שמכנים וקטור ערך, וזה משהו שאתם מוסיפים לשיכון  של המילה השנייה, 

179
00:13:59,785 --> 00:14:01,920
במקרה הזה לשיכון  של יצור.

180
00:14:02,600 --> 00:14:07,000
וקטור הערך הזה נמצא באותו מרחב מאוד גבוה כמו השיכונים.

181
00:14:07,460 --> 00:14:10,936
כשאתם מכפילים את מטריצת הערכים הזו בשיכון של מילה, 

182
00:14:10,936 --> 00:14:15,707
אתם עשויים לחשוב על זה כאילו אתם אומרים, אם המילה הזו רלוונטית להתאמת 

183
00:14:15,707 --> 00:14:21,160
המשמעות של משהו אחר, מה בדיוק צריך להוסיף לשיכון של אותו משהו אחר כדי לשקף זֶאת?

184
00:14:22,140 --> 00:14:26,136
במבט לאחור בתרשים שלנו, נניח בצד את כל המפתחות והשאילתות, 

185
00:14:26,136 --> 00:14:30,822
שכן לאחר שתחשבו את דפוס המיקוד סיימתם איתם, אז אתם לוקחים את מטריצת 

186
00:14:30,822 --> 00:14:36,060
הערכים הזו ומכפילים אותה בכל אחד מהשיכונים האלו כדי לייצר רצף של וקטורי ערך.

187
00:14:37,120 --> 00:14:41,120
אתם עשויים לחשוב על וקטורי הערך אלה כמקושרים למפתחות המתאימים.

188
00:14:42,320 --> 00:14:49,240
עבור כל עמודה בתרשים הזה, אתם מכפילים כל אחד מוקטורי הערך במשקל המתאים באותה עמודה.

189
00:14:50,080 --> 00:14:56,790
לדוגמה כאן, תחת השיכון של יצור, תוסיפו חלק משמעותי מוקטורי הערך עבור פרוותי וכחול, 

190
00:14:56,790 --> 00:15:01,560
בעוד שכל וקטורי הערך האחרים מתאפסים, או לפחות כמעט מתאפסים.

191
00:15:02,120 --> 00:15:05,607
ולבסוף, הדרך לעדכן את השיכון הקשור לעמודה הזו, 

192
00:15:05,607 --> 00:15:11,394
שקודדה בעבר איזו משמעות נטולת הקשר של יצור, היא שאתם מוסיפים יחד את כל הערכים 

193
00:15:11,394 --> 00:15:17,034
המותאמים מחדש בעמודה, ומייצרים שינוי שאתם רוצים להוסיף, שאני אתייג כדלתא-E, 

194
00:15:17,034 --> 00:15:19,260
ואז תוסיפו אותו לשיכון המקורי.

195
00:15:19,680 --> 00:15:24,810
אני מקווה שהתוצאה היא וקטור המקודד טוב יותר את המשמעות העשירה יותר מבחינה הקשרית, 

196
00:15:24,810 --> 00:15:26,500
כמו זו של יצור כחול פרוותי.

197
00:15:27,380 --> 00:15:32,580
וכמובן שאתם לא עושים את זה רק לשיכון אחד, אתם מחילים את אותו סכום משוקלל על 

198
00:15:32,580 --> 00:15:36,070
פני כל העמודות בתמונה הזו, מייצרים רצף של שינויים, 

199
00:15:36,070 --> 00:15:39,286
מוסיפים את כל השינויים האלה לשיכונים המתאימים, 

200
00:15:39,286 --> 00:15:43,460
מייצרים רצף מלא של שיכונים מותאמים יותר שיוצאים מבלוק המיקוד.

201
00:15:44,860 --> 00:15:49,100
אם תסתכלו על התמונה הרחבה, כל התהליך הזה הוא מה שתגדירו כראש מיקוד יחיד.

202
00:15:49,600 --> 00:15:54,967
כפי שתיארתי דברים עד כה, תהליך זה מותאם לפרמטרים על ידי שלוש מטריצות נפרדות, 

203
00:15:54,967 --> 00:15:58,940
כולן מלאות בפרמטרים הניתנים לשינוי: המפתח, השאילתה והערך.

204
00:15:59,500 --> 00:16:02,753
אני רוצה לקחת רגע כדי להמשיך את מה שהתחלנו בפרק האחרון, 

205
00:16:02,753 --> 00:16:07,110
עם שמירת הניקוד שבה אנחנו סופרים את המספר הכולל של פרמטרים של הדגם באמצעות 

206
00:16:07,110 --> 00:16:08,040
המספרים מ-GPT-3.

207
00:16:09,300 --> 00:16:13,235
למטריצות המפתח והשאילתה האלו יש 12,288 עמודות, 

208
00:16:13,235 --> 00:16:19,600
התואמות לממד השיכון  ו-128 שורות, התואמות לממד של מרחב שאילתת מפתח קטן יותר.

209
00:16:20,260 --> 00:16:24,220
זה נותן לנו 1.5 מיליון פרמטרים נוספים לערך עבור כל אחד מהם.

210
00:16:24,860 --> 00:16:28,524
אם אתם מסתכלים על מטריצת הערכים הזו לעומת זאת, 

211
00:16:28,524 --> 00:16:33,669
הדרך שתיארתי את הדברים עד כה תצביע על כך שזו מטריצה מרובעת שיש לה 

212
00:16:33,669 --> 00:16:39,126
12,288 עמודות ו-12,288 שורות, מכיוון שגם הקלטים וגם הפלטים שלה נמצאים 

213
00:16:39,126 --> 00:16:40,920
במרחב השיכון הגדול הזה.

214
00:16:41,500 --> 00:16:45,140
אם זה נכון, המשמעות היא כ-150 מיליון פרמטרים נוספים.

215
00:16:45,660 --> 00:16:47,300
וכדי להיות ברור, אתם יכולים לעשות את זה.

216
00:16:47,420 --> 00:16:51,740
אתם יכולים להקדיש למפוי הערכים יותר פרמטרים בסדרי גודל מאשר למפתח ולשאילתה.

217
00:16:52,060 --> 00:16:56,444
אבל בפועל, הרבה יותר יעיל אם במקום זאת תגרמו לכך שמספר הפרמטרים 

218
00:16:56,444 --> 00:17:00,760
המוקדשים למפוי הערכים הזה יהיה זהה למספר המוקדש למפתח ולשאילתה.

219
00:17:01,460 --> 00:17:05,160
זה רלוונטי במיוחד בהגדרה של הפעלת מספר ראשי מיקוד במקביל.

220
00:17:06,240 --> 00:17:10,099
האופן שבו זה נראה הוא שמפוי הערכים מפורק למכפלה של שתי מטריצות קטנות יותר.

221
00:17:11,180 --> 00:17:16,118
מבחינה קונספטואלית, אני עדיין ממליץ לכם לחשוב על המיפוי הליניארי הכולל, 

222
00:17:16,118 --> 00:17:19,684
כזו עם קלטים ופלטים שגם הם במרחב השיכון הגדול יותר, 

223
00:17:19,684 --> 00:17:23,800
למשל לקחת את השיכון כחול לכיוון הכחול הזה שתוסיפו לשמות עצם.

224
00:17:27,040 --> 00:17:32,760
רק שזהו מספר קטן יותר של שורות, בדרך כלל כמו במרחב שאילתת המפתח.

225
00:17:33,100 --> 00:17:38,440
המשמעות היא שאתם יכולים לחשוב על זה כעל מיפוי וקטורי השיכון הגדולים למרחב קטן בהרבה.

226
00:17:39,040 --> 00:17:42,700
זה לא השם המקובל, אבל אני אקרא לה מטריצת הערך למטה.

227
00:17:43,400 --> 00:17:46,932
המטריצה השנייה ממפה מהמרחב הקטן יותר הזה בחזרה למרחב השיכון, 

228
00:17:46,932 --> 00:17:50,580
מייצרת את הוקטורים שבהם אתם משתמשים כדי לבצע את העדכונים בפועל.

229
00:17:51,000 --> 00:17:54,740
אקרא לה מטריצת הערך למעלה, ושוב זה לא שם קונבנציונלי.

230
00:17:55,160 --> 00:17:58,080
הדרך שבה תראו את זה כתוב ברוב המאמרים נראית קצת אחרת.

231
00:17:58,380 --> 00:17:59,520
אני אדבר על כך בעוד דקה.

232
00:17:59,700 --> 00:18:02,540
לדעתי, זה נוטה להפוך את הדברים לקצת יותר מבלבלים מבחינה רעיונית.

233
00:18:03,260 --> 00:18:06,738
אם להזכיר מונח מאלגברה ליניארית, מה שאנחנו בעצם עושים זה 

234
00:18:06,738 --> 00:18:10,340
להגביל את מיפוי הערכים הכולל להיות Low Rank Transformation.

235
00:18:11,420 --> 00:18:16,212
אם נחזור לספירת הפרמטרים, לכל ארבעת המטריצות הללו יש אותו גודל, 

236
00:18:16,212 --> 00:18:20,780
ובחיבור של כולן נקבל כ-6.3 מיליון פרמטרים עבור ראש מיקוד אחד.

237
00:18:22,040 --> 00:18:27,302
כהערת צד, אם לדייק קצת יותר, מה שתואר עד כה הוא מה שאנשים היו מכנים ראש מיקוד עצמי, 

238
00:18:27,302 --> 00:18:31,500
כדי להבחין בינו לבין וריאציה שעולה במודלים אחרים שנקראת מיקוד צולב.

239
00:18:32,300 --> 00:18:36,435
זה לא רלוונטי לדוגמה שלנו ב-GPT, אבל אם אתם סקרנים, 

240
00:18:36,435 --> 00:18:41,048
מיקוד צולב כולל מודלים המעבדים שני סוגים שונים של נתונים, 

241
00:18:41,048 --> 00:18:46,058
כמו טקסט בשפה אחת וטקסט בשפה אחרת שהוא חלק מיצירת תרגום מתמשך, 

242
00:18:46,058 --> 00:18:49,240
או אולי קלט אודיו של דיבור ותמלול מתמשך.

243
00:18:50,400 --> 00:18:52,700
ראש עם מיקוד צולב נראה כמעט זהה.

244
00:18:52,980 --> 00:18:57,400
ההבדל היחיד הוא שמיפויי השאילתה והמפתח פועלים על נתונים שונים.

245
00:18:57,840 --> 00:19:04,164
במודל שמתרגם, למשל, המפתחות עשויים להגיע משפה אחת, בעוד שהשאילתות מגיעות משפה אחרת, 

246
00:19:04,164 --> 00:19:09,660
ודפוס המיקוד יכול לתאר אילו מילים משפה אחת מתאימות לאילו מילים בשפה אחרת.

247
00:19:10,340 --> 00:19:13,249
ובמסגרת הזו בדרך כלל לא יהיה מיסוך, מכיוון שאין 

248
00:19:13,249 --> 00:19:16,340
ממש מושג של טוקנים מאוחרים יותר המשפיעים על קודמים.

249
00:19:17,180 --> 00:19:20,829
נישאר מרוכזים במיקוד עצמי, אם אתם מבינים הכל עד כה, 

250
00:19:20,829 --> 00:19:25,180
ואם הייתם עוצרים כאן, הייתם יוצאים עם המהות של מהו מיקוד באמת.

251
00:19:25,760 --> 00:19:31,440
כל מה שבאמת נשאר לנו הוא לפרט את הדרך שבה אתם עושים זאת פעמים רבות ושונות.

252
00:19:32,100 --> 00:19:35,509
בדוגמה המרכזית שלנו התמקדנו בשמות תואר המעדכנים שמות עצם, 

253
00:19:35,509 --> 00:19:39,800
אבל כמובן שיש הרבה דרכים שונות שבהן ההקשר יכול להשפיע על המשמעות של מילה.

254
00:19:40,360 --> 00:19:43,506
אם המילים "הם ריסקו" באות לפני המילה "מכונית", 

255
00:19:43,506 --> 00:19:46,520
יש לכך השלכות על הצורה והמבנה של אותה מכונית.

256
00:19:47,200 --> 00:19:49,280
והרבה אסוציאציות עשויות להיות פחות דקדוקיות.

257
00:19:49,760 --> 00:19:53,429
אם המילה מכשף נמצאת במקום כלשהו באותו קטע כמו הארי, 

258
00:19:53,429 --> 00:19:58,864
זה מצביע על כך שאולי היא מתייחסת להארי פוטר, בעוד שאם במקום זאת המילים מלכה, 

259
00:19:58,864 --> 00:20:04,440
סאסקס וויליאם היו בקטע הזה, אז אולי יש לעדכן את השיכון של הארי להתייחסות לנסיך.

260
00:20:05,040 --> 00:20:08,732
עבור כל סוג שונה של עדכון הקשר שאתם יכולים לחשוב עליו, 

261
00:20:08,732 --> 00:20:13,970
הפרמטרים של מטריצות מפתח ושאילתה יהיו שונים כדי ללכוד את דפוסי המיקוד השונים, 

262
00:20:13,970 --> 00:20:19,140
והפרמטרים של מיפוי הערכים שלנו יהיו שונים בהתבסס על מה שצריך להוסיף לשיכונים.

263
00:20:19,980 --> 00:20:23,853
ושוב, בפועל קשה הרבה יותר לפרש את ההתנהגות האמיתית של המיפויים האלו, 

264
00:20:23,853 --> 00:20:27,164
כאשר המשקולות מוגדרות כך שיעשו כל מה שהמודל צריך לעשות כדי 

265
00:20:27,164 --> 00:20:30,140
להגשים בצורה הטובה ביותר את מטרתו לחזות את הטוקן הבא.

266
00:20:31,400 --> 00:20:35,065
כפי שאמרתי קודם, כל מה שתיארנו הוא ראש מיקוד יחיד, 

267
00:20:35,065 --> 00:20:39,594
ובלוק מיקוד מלא בתוך טרמספורמר מורכב ממה שנקרא מיקוד רב-ראשים, 

268
00:20:39,594 --> 00:20:45,920
שבו אתם מפעילים הרבה מהפעולות האלו במקביל, כל אחת עם שאילתת מפתח נפרדת ומפוי ערכים משלה.

269
00:20:47,420 --> 00:20:51,700
GPT-3 למשל משתמשת ב-96 ראשי מיקוד בתוך כל בלוק.

270
00:20:52,020 --> 00:20:56,460
בהתחשב בכך שכל אחד מהם כבר קצת מבלבל, אין ספק שזה הרבה מה להחזיק בראש שלכם.

271
00:20:56,760 --> 00:21:00,810
רק כדי לאיית את הכל בצורה מאוד מפורשת, זה אומר שיש לכם 96 

272
00:21:00,810 --> 00:21:05,000
מטריצות מפתח ושאילתות נפרדות המייצרות 96 דפוסי מיקוד ברורים.

273
00:21:05,440 --> 00:21:12,180
אז לכל ראש יש מטריצות ערך נפרדות משלו המשמשות לייצור 96 רצפים של וקטורי ערך.

274
00:21:12,460 --> 00:21:16,680
כל אלה מתווספים יחד באמצעות דפוסי המיקוד המתאימים כמשקולות.

275
00:21:17,480 --> 00:21:21,824
המשמעות היא שלכל מיקום בהקשר, כל טוקן, כל אחד 

276
00:21:21,824 --> 00:21:27,020
מהראשים הללו מייצר שינוי מוצע שיתווסף לשיכון במיקום זה.

277
00:21:27,660 --> 00:21:31,269
אז מה שאתם עושים זה לסכם את כל השינויים המוצעים האלה, 

278
00:21:31,269 --> 00:21:35,480
אחד לכל ראש, ואז מוסיפים את התוצאה לשיכון המקורי של המיקום הזה.

279
00:21:36,660 --> 00:21:42,831
כל הסכום הזה כאן יהיה חלק אחד ממה שמופק מבלוק המיקוד הרב-ראשים הזה, 

280
00:21:42,831 --> 00:21:47,460
אחד מהשיכונים המעודנים האלה שיוצאים מהקצה השני שלו.

281
00:21:48,320 --> 00:21:52,140
שוב, זה הרבה לחשוב עליו, אז אל תדאגו אם לוקח לזה קצת זמן לשקוע.

282
00:21:52,380 --> 00:21:56,231
הרעיון הכללי הוא שעל ידי הפעלת ראשים שונים במקביל, 

283
00:21:56,231 --> 00:22:01,820
אתם נותנים למודל את היכולת ללמוד דרכים רבות ושונות שבהן ההקשר משנה משמעות.

284
00:22:03,700 --> 00:22:09,319
כשמגדילים את ספירת הפרמטרים שלנו עם 96 ראשים, שכל אחד מהם כולל את הווריאציה שלו 

285
00:22:09,319 --> 00:22:15,080
של ארבעת המטריצות האלו, כל בלוק של מיקוד רב-ראשים מכיל בסביבות 600 מיליון פרמטרים.

286
00:22:16,420 --> 00:22:21,800
יש עוד דבר אחד שאני רוצה להזכיר לכל אחד מכם שימשיך לקרוא עוד על טרמספורמרים.

287
00:22:22,080 --> 00:22:26,509
אתם זוכרים שאמרתי שמפוי הערכים מחולק לשתי המטריצות הנבדלות הללו, 

288
00:22:26,509 --> 00:22:29,440
אותן תייגתי כמטריצות הערך למטה והערך למעלה.

289
00:22:29,960 --> 00:22:36,261
האופן שבו ניסחתי את הדברים מציע לכם לראות את צמד המטריצות הזה בתוך כל ראש מיקוד, 

290
00:22:36,261 --> 00:22:38,440
ותוכלו ליישם את זה בצורה זו.

291
00:22:38,640 --> 00:22:39,920
זה יהיה עיצוב תקף.

292
00:22:40,260 --> 00:22:44,920
אבל הדרך שבה אתם רואים את זה כתוב במאמרים והדרך שבה זה מיושם בפועל נראית קצת אחרת.

293
00:22:45,340 --> 00:22:50,778
כל מטריצות הערך למעלה הללו עבור כל ראש מופיעות יחדיו במטריצה ענקית 

294
00:22:50,778 --> 00:22:56,380
אחת שאנחנו קוראים לה מטריצת הפלט, הקשורה לכל בלוקי המיקוד הרב-הראשים.

295
00:22:56,820 --> 00:23:01,120
וכשאתם רואים אנשים מתייחסים למטריצת הערך של ראש מיקוד נתון, 

296
00:23:01,120 --> 00:23:07,140
הם בדרך כלל מתייחסים רק לשלב הראשון הזה, זה שסימנתי כהטלת הערך למטה למרחב הקטן יותר.

297
00:23:08,340 --> 00:23:11,040
לסקרנים שביניכם, השארתי על כך הערה על המסך.

298
00:23:11,260 --> 00:23:14,530
זה אחד מהפרטים האלה שעללולים להסיח את הדעת מהנקודות העיקריות, 

299
00:23:14,530 --> 00:23:18,540
אבל אני כן רוצה להצביע על כך רק כדי שתדעו אם אתם קוראים על זה במקורות אחרים.

300
00:23:19,240 --> 00:23:23,415
אם שמים בצד את כל הניואנסים הטכניים, בתצוגה המקדימה מהפרק האחרון 

301
00:23:23,415 --> 00:23:28,040
ראינו כיצד נתונים שזורמים דרך טרמספורמר לא זורמים רק דרך בלוק מיקוד אחד.

302
00:23:28,640 --> 00:23:32,700
ראשית, הם עוברים גם את הפעולות האחרות הנקראות פרספטרונים רב-שכבתיים.

303
00:23:33,120 --> 00:23:34,880
על אלה נדבר יותר בפרק הבא.

304
00:23:35,180 --> 00:23:39,320
ואז הם עוברים שוב ושוב דרך הרבה הרבה עותקים של שתי הפעולות האלה.

305
00:23:39,980 --> 00:23:44,145
המשמעות היא שאחרי שמילה נתונה מטמיעה חלק מההקשר שלה, 

306
00:23:44,145 --> 00:23:50,040
יש הרבה יותר סיכויים שהשיכון הניואנסי הזה יושפע מהסביבה הניואנסית יותר שלו.

307
00:23:50,940 --> 00:23:57,256
ככל שאתם מתקדמים למטה ברשת, כאשר כל שיכון מקבל יותר ויותר משמעות מכל השיכונים האחרים, 

308
00:23:57,256 --> 00:24:02,545
שהם עצמם נעשים יותר ויותר ניואנסיים, התקווה היא שיש יכולת לקודד רעיונות 

309
00:24:02,545 --> 00:24:07,320
ברמה גבוהה ומופשטת יותר לגבי קלט נתון מעבר לתארים ולמבנה הדקדוקי.

310
00:24:07,880 --> 00:24:15,130
דברים כמו סנטימנט וטון והאם זה שיר ואיזה אמיתות מדעיות רלוונטיות ליצירה ודברים כאלה.

311
00:24:16,700 --> 00:24:21,859
אם נחזור פעם נוספת לניקוד שלנו, GPT-3 כולל 96 שכבות נפרדות, 

312
00:24:21,859 --> 00:24:27,018
כך שהמספר הכולל של שאילתות מפתח וערך פרמטרים מוכפל בעוד 96, 

313
00:24:27,018 --> 00:24:34,500
מה שמביא את הסכום הכולל לקצת פחות מ-58 מיליארד פרמטרים נפרדים המוקדשים לכל ראשי המיקוד.

314
00:24:34,980 --> 00:24:40,940
זה בטוח הרבה, אבל זה רק כשליש מתוך 175 המיליארד שיש ברשת בסך הכל.

315
00:24:41,520 --> 00:24:45,230
אז למרות שמיקוד מקבל את כל תשומת הלב, רוב הפרמטרים 

316
00:24:45,230 --> 00:24:48,140
מגיעים מהבלוקים שנמצאים בין השלבים האלה.

317
00:24:48,560 --> 00:24:53,560
בפרק הבא, אתם ואני נדבר יותר על הבלוקים האחרים האלה וגם הרבה יותר על תהליך האימון.

318
00:24:54,120 --> 00:24:58,873
חלק גדול מסיפור ההצלחה של מנגנון המיקוד הוא לא כל כך איזשהו סוג 

319
00:24:58,873 --> 00:25:03,849
ספציפי של התנהגות שהוא מאפשר, אלא העובדה שהוא מקבילי ביותר, כלומר, 

320
00:25:03,849 --> 00:25:08,380
אתם יכולים להריץ מספר עצום של חישובים בזמן קצר באמצעות GPUs .

321
00:25:09,460 --> 00:25:13,212
בהתחשב בעובדה שאחד הלקחים הגדולים לגבי למידה עמוקה בעשור או שניים 

322
00:25:13,212 --> 00:25:17,363
האחרונים היה שקנה המידה לבדו נותן שיפורים איכותיים עצומים בביצועי המודל, 

323
00:25:17,363 --> 00:25:21,060
יש יתרון עצום לארכיטקטורות הניתנות להקבלה שמאפשרות לכם לעשות זאת.

324
00:25:22,040 --> 00:25:25,340
אם אתם רוצים ללמוד עוד על הנושא הזה, השארתי הרבה קישורים בתיאור.

325
00:25:25,920 --> 00:25:30,040
בפרט, כל דבר שמיוצר על ידי Andrej Karpathy או Chris Ola נוטה להיות זהב טהור.

326
00:25:30,560 --> 00:25:33,246
בסרטון הזה, רציתי רק לקפוץ למיקוד בצורתו הנוכחית, 

327
00:25:33,246 --> 00:25:37,114
אבל אם אתם סקרנים לגבי עוד מההיסטוריה של איך הגענו לכאן ואיך אתם יכולים 

328
00:25:37,114 --> 00:25:41,196
להמציא את הרעיון הזה מחדש לעצמכם, ידידי Vivek פשוט העלה זוג סרטונים שנותנים 

329
00:25:41,196 --> 00:25:42,540
הרבה יותר מהמוטיבציה הזו.

330
00:25:43,120 --> 00:25:45,790
כמו כן, ל-Britt Cruz מהערוץ The Art of the Problem 

331
00:25:45,790 --> 00:25:48,460
יש סרטון ממש נחמד על ההיסטוריה של דגמי שפות גדולים.

332
00:26:04,960 --> 00:26:09,200
תודה.

