[
 {
  "translatedText": "בפרק האחרון, אתה ואני התחלנו לעבור דרך פעולתו הפנימית של שנאי.",
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "translatedText": "זהו אחד מחלקי הטכנולוגיה המרכזיים במודלים של שפות גדולות, והרבה כלים אחרים בגל המודרני של AI.",
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "translatedText": "זה הגיע לראשונה לסצנה במאמר מפורסם מ-2017 בשם Attention is All You Need, ובפרק זה אתה ואני נחפור מהו מנגנון הקשב הזה, תוך חזותי כיצד הוא מעבד נתונים.",
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "translatedText": "כסיכום קצר, הנה ההקשר החשוב שאני רוצה שתזכור.",
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "translatedText": "המטרה של המודל שאתה ואני לומדים היא לקחת חלק מטקסט ולחזות איזו מילה מגיעה אחר כך.",
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "translatedText": "טקסט הקלט מחולק לחתיכות קטנות שאנו מכנים אסימונים, ולעתים קרובות אלו הן מילים או פיסות מילים, אבל רק כדי להקל עליך ולי לחשוב על הדוגמאות בסרטון הזה, בואו נפשט על ידי העמדת פנים שאסימונים הם תמיד רק מילים.",
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "translatedText": "השלב הראשון בשנאי הוא לשייך כל אסימון לוקטור בעל מימד גבוה, מה שאנו מכנים הטבעה שלו.",
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "translatedText": "הרעיון החשוב ביותר שאני רוצה שתזכרו הוא כיצד כיוונים במרחב הגבוה-ממדי הזה של כל ההטבעות האפשריות יכולים להתכתב עם משמעות סמנטית.",
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "translatedText": "בפרק האחרון ראינו דוגמה לאופן שבו כיוון יכול להתאים למגדר, במובן זה שהוספת שלב מסוים במרחב הזה יכולה לקחת אותך מהטבעה של שם עצם זכר להטבעה של שם העצם הנקבי המקביל.",
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "translatedText": "זו רק דוגמה אחת שתוכלו לתאר לעצמכם כמה כיוונים אחרים במרחב הגבוה-ממדי הזה יכולים להתאים להרבה היבטים אחרים של משמעות המילה.",
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "translatedText": "המטרה של שנאי היא להתאים בהדרגה את ההטבעות הללו כך שהן לא רק מקודדות מילה בודדת, אלא במקום זאת הן אופות במשמעות הקשרית הרבה הרבה יותר עשירה.",
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "translatedText": "אני צריך לומר מראש שהרבה אנשים מוצאים את מנגנון הקשב, חלק המפתח הזה בשנאי, מאוד מבלבל, אז אל תדאג אם לוקח קצת זמן עד שהדברים ישקעו פנימה.",
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "translatedText": "אני חושב שלפני שאנחנו צוללים לפרטים החישוביים ולכל הכפלות המטריצות, כדאי לחשוב על כמה דוגמאות לסוג ההתנהגות שאנחנו רוצים שתשומת הלב תאפשר.",
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "translatedText": "שקול את הביטויים American true mole, שומה אחת של פחמן דו חמצני, ולקחת ביופסיה של השומה.",
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "translatedText": "אתה ואני יודעים שלמילה שומה יש משמעויות שונות בכל אחת מהן, בהתבסס על ההקשר.",
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "translatedText": "אבל לאחר השלב הראשון של שנאי, זה שמפרק את הטקסט ומשייך כל אסימון לוקטור, הווקטור המשויך לשומה יהיה זהה בכל המקרים האלה, מכיוון שהטמעת האסימון הראשונית הזו היא למעשה טבלת חיפוש ללא התייחסות להקשר.",
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "translatedText": "רק בשלב הבא של השנאי יש להטמעות שמסביב הזדמנות להעביר מידע לתוך זה.",
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "translatedText": "התמונה שאולי תזכור היא שיש מספר כיוונים ברורים במרחב ההטמעה הזה המקודדים את המשמעויות המובהקות של המילה שומה, ושגוש קשב מיומן מחשב את מה שאתה צריך להוסיף להטמעה הגנרית כדי להעביר אותו אל אחד מהכיוונים הספציפיים הללו, כפונקציה של ההקשר.",
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "translatedText": "כדי לקחת דוגמה נוספת, שקול את הטבעת המילה מגדל.",
  "input": "To take another example, consider the embedding of the word tower.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "translatedText": "זה כנראה איזה כיוון מאוד כללי, לא ספציפי במרחב, הקשור להרבה שמות עצם גדולים וגבוהים אחרים.",
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "translatedText": "אם המילה הזאת קודמה מיד על ידי אייפל, הייתם יכולים לדמיין שאתם רוצים שהמנגנון יעדכן את הווקטור הזה כך שיצביע על כיוון שמקודד באופן ספציפי יותר את מגדל אייפל, אולי בקורלציה עם וקטורים הקשורים לפריז וצרפת ולדברים העשויים מפלדה.",
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "translatedText": "אם קדמה לה גם המילה מיניאטורה, אז הווקטור צריך להתעדכן עוד יותר, כך שהוא לא מתאם עוד עם דברים גדולים וגבוהים.",
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "translatedText": "באופן כללי יותר מאשר רק חידוד המשמעות של מילה, בלוק הקשב מאפשר למודל להעביר מידע המקודד בהטמעה אחת לזו של אחרת, פוטנציאלית כאלה שנמצאים די רחוק, ואפשר עם מידע שהוא הרבה יותר עשיר מסתם מילה בודדת.",
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "translatedText": "מה שראינו בפרק האחרון היה איך אחרי שכל הוקטורים זורמים ברשת, כולל בלוקי קשב רבים ושונים, החישוב שאתה מבצע כדי לייצר חיזוי של האסימון הבא הוא לחלוטין פונקציה של הווקטור האחרון ברצף.",
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "translatedText": "תאר לעצמך, למשל, שהטקסט שאתה מזין הוא רובו של רומן מסתורין שלם, עד לנקודה סמוך לסוף, שקורא, לכן הרוצח היה.",
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "translatedText": "אם המודל מתכוון לחזות במדויק את המילה הבאה, אותו וקטור אחרון ברצף, שהתחיל את חייו פשוט להטביע את המילה הייתה, יצטרך להיות מעודכן על ידי כל בלוקי הקשב כדי לייצג הרבה, הרבה יותר מכל אדם מילה, איכשהו מקודדת את כל המידע מחלון ההקשר המלא הרלוונטי לניבוי המילה הבאה.",
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "translatedText": "עם זאת, כדי לעבור דרך החישובים, בואו ניקח דוגמה הרבה יותר פשוטה.",
  "input": "To step through the computations, though, let's take a much simpler example.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "translatedText": "תארו לעצמכם שהקלט כולל את הביטוי, יצור כחול רך שוטט ביער המוריק.",
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "translatedText": "ולרגע זה, נניח שסוג העדכון היחיד שמעניין אותנו הוא התאמת שמות התואר את המשמעויות של שמות העצם התואמים להם.",
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "translatedText": "מה שאני עומד לתאר זה מה שהיינו מכנים ראש קשב בודד, ובהמשך נראה כיצד בלוק הקשב מורכב מהרבה ראשים שונים הפועלים במקביל.",
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "translatedText": "שוב, ההטבעה הראשונית של כל מילה היא איזה וקטור בעל מימד גבוה שמקודד רק את המשמעות של המילה המסוימת הזו ללא הקשר.",
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "translatedText": "למעשה, זה לא ממש נכון.",
  "input": "Actually, that's not quite true.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "translatedText": "הם גם מקודדים את המיקום של המילה.",
  "input": "They also encode the position of the word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "translatedText": "יש עוד הרבה מה לומר איך מיקומים מקודדים, אבל כרגע, כל מה שאתה צריך לדעת הוא שהערכים של הווקטור הזה מספיקים כדי לספר לך גם מהי המילה וגם היכן היא קיימת בהקשר.",
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "translatedText": "נמשיך ונציין את ההטבעות הללו באות ה.",
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "translatedText": "המטרה היא שסדרה של חישובים תייצר קבוצה מעודנת חדשה של הטבעות שבהן, למשל, אלו התואמות לשמות העצם ספגו את המשמעות משמות התואר התואמים שלהם.",
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "translatedText": "ולשחק במשחק הלמידה העמוקה, אנחנו רוצים שרוב החישובים המעורבים ייראו כמו תוצרי מטריצה-וקטור, שבהם המטריצות מלאות במשקלים הניתנים לשינוי, דברים שהמודל ילמד על סמך נתונים.",
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "translatedText": "כדי להיות ברור, אני ממציא את הדוגמה הזו של שמות תואר המעדכנים שמות עצם רק כדי להמחיש את סוג ההתנהגות שאתה יכול לדמיין שראש תשומת לב עושה.",
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "translatedText": "כמו בכל כך הרבה למידה עמוקה, ההתנהגות האמיתית היא הרבה יותר קשה לנתח מכיוון שהיא מבוססת על כוונון וכיוונון של מספר עצום של פרמטרים כדי למזער פונקציית עלות כלשהי.",
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "translatedText": "רק שכשאנחנו עוברים דרך כל המטריצות השונות המלאות בפרמטרים שמעורבים בתהליך הזה, אני חושב שזה ממש מועיל לקבל דוגמה דמיונית למשהו שהוא יכול לעשות כדי לשמור על הכל יותר קונקרטי.",
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "translatedText": "בשלב הראשון של תהליך זה, אתה עשוי לדמיין כל שם עצם, כמו יצור, שואל את השאלה, היי, האם יש שמות תואר שיושבים מולי?",
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "translatedText": "ולגבי המילים פלאפי וכחול, שכל אחד יוכל לענות, כן, אני שם תואר ואני בעמדה הזו.",
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "translatedText": "השאלה הזו מקודדת איכשהו כעוד וקטור, עוד רשימה של מספרים, שאנו קוראים לה השאילתה של המילה הזו.",
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "translatedText": "עם זאת, לוקטור השאילתה הזה יש ממד קטן בהרבה מהוקטור המוטבע, נניח 128.",
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "translatedText": "חישוב השאילתה הזו נראה כמו לקיחת מטריצה מסוימת, שאותה אתן ל-wq, ולהכפיל אותה בהטמעה.",
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "translatedText": "אם נדחס קצת דברים, בוא נכתוב את וקטור השאילתה הזה בתור q, ואז בכל פעם שאתה רואה אותי שם מטריצה ליד חץ כמו זה, זה נועד לייצג שכפל המטריצה הזו בווקטור בתחילת החץ נותן לך את הווקטור ב קצה החץ.",
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "translatedText": "במקרה זה, אתה מכפיל את המטריצה הזו בכל ההטמעות בהקשר, ומייצר וקטור שאילתה אחד עבור כל אסימון.",
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "translatedText": "הערכים של המטריצה הזו הם פרמטרים של המודל, כלומר ההתנהגות האמיתית נלמדת מנתונים, ובפועל, מה שמטריצה זו עושה בראש קשב מסוים הוא מאתגר לנתח.",
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "translatedText": "אבל למעננו, כשנדמיינו דוגמה שאולי נקווה שהיא תלמד, נניח שמטריצת השאילתה הזו ממפה את ההטבעות של שמות עצם לכיוונים מסוימים במרחב השאילתה הקטן יותר הזה, שמקודד איכשהו את הרעיון של חיפוש שמות תואר בעמדות קודמות. .",
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "translatedText": "לגבי מה זה עושה להטבעות אחרות, מי יודע?",
  "input": "As to what it does to other embeddings, who knows?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "translatedText": "אולי הוא מנסה במקביל להשיג מטרה אחרת עם אלה.",
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "translatedText": "כרגע, אנחנו מתמקדים בלייזר בשמות העצם.",
  "input": "Right now, we're laser focused on the nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "translatedText": "יחד עם זאת, קשורה לזה מטריצה שנייה שנקראת מטריצת המפתח, שגם אותה מכפילים בכל אחת מההטבעות.",
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "translatedText": "זה מייצר רצף שני של וקטורים שאנו קוראים להם המפתחות.",
  "input": "This produces a second sequence of vectors that we call the keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "translatedText": "מבחינה קונספטואלית, אתה רוצה לחשוב על המפתחות כמענה פוטנציאלי לשאילתות.",
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "translatedText": "מטריצת מפתח זו מלאה גם בפרמטרים הניתנים לכוונון, ובדיוק כמו מטריצת השאילתה, היא ממפה את וקטורי הטבעה לאותו מרחב ממדי קטן יותר.",
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "translatedText": "אתה חושב על המפתחות כמתאימים לשאילתות בכל פעם שהם מתאימים זה לזה.",
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "translatedText": "בדוגמה שלנו, תדמיינו שמטריצת המפתח ממפה את שמות התואר כמו פלאפי וכחול לוקטורים שמיושרים היטב עם השאילתה שמפיקה המילה יצור.",
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "translatedText": "כדי למדוד עד כמה כל מפתח מתאים לכל שאילתה, אתה מחשב מוצר נקודות בין כל זוג מפתח-שאילתה אפשרי.",
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "translatedText": "אני אוהב לדמיין רשת מלאה בחבורה של נקודות, שבה הנקודות הגדולות יותר מתאימות למוצרי הנקודות הגדולות יותר, המקומות שבהם המפתחות והשאילתות מתיישרים.",
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "translatedText": "לדוגמא של שם התואר שלנו, זה ייראה קצת יותר כך, כאשר אם המקשים המופקים על ידי fluffy וכחול באמת מתאימים לשאילתה המיוצר על ידי יצור, אז תוצרי הנקודות בשני הנקודות הללו יהיו מספרים חיוביים גדולים.",
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "translatedText": "בשפה השפה, אנשי למידת מכונה היו אומרים שזה אומר שההטבעות של פלאפי וכחול מטפלות בהטבעה של יצור.",
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "translatedText": "בניגוד לתוצר הנקודה בין המפתח למילה אחרת כמו ה- לשאילתה עבור יצור יהיה איזה ערך קטן או שלילי שמשקף שאינם קשורים זה לזה.",
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "translatedText": "אז יש לנו את הרשת הזו של ערכים שיכולים להיות כל מספר ממשי מאינסוף שלילי עד אינסוף, נותן לנו ציון עד כמה כל מילה רלוונטית לעדכון המשמעות של כל מילה אחרת.",
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "translatedText": "הדרך שבה אנחנו עומדים להשתמש בציונים האלה היא לקחת סכום משוקלל מסוים לאורך כל עמודה, משוקלל לפי הרלוונטיות.",
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "translatedText": "אז במקום שיהיו לנו ערכים בין אינסוף שלילי לאינסוף, מה שאנחנו רוצים זה שהמספרים בעמודות האלה יהיו בין 0 ל-1, וכל עמודה תצטבר ל-1, כאילו היו התפלגות הסתברות.",
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "translatedText": "אם אתה מגיע מהפרק האחרון, אתה יודע מה אנחנו צריכים לעשות אז.",
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "translatedText": "אנו מחשבים softmax לאורך כל אחת מהעמודות הללו כדי לנרמל את הערכים.",
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "translatedText": "בתמונה שלנו, לאחר שתחיל את softmax על כל העמודות, נמלא את הרשת בערכים המנורמלים הללו.",
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "translatedText": "בשלב זה אתה בטוח לחשוב על כל עמודה כעל מתן משקלים לפי מידת הרלוונטיות של המילה משמאל לערך המתאים בחלק העליון.",
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "translatedText": "אנו קוראים לרשת הזו דפוס קשב.",
  "input": "We call this grid an attention pattern.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "translatedText": "עכשיו אם אתה מסתכל על נייר השנאי המקורי, יש דרך ממש קומפקטית שהם כותבים את כל זה.",
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "translatedText": "כאן המשתנים q ו-k מייצגים את המערכים המלאים של וקטורי שאילתה ומפתח בהתאמה, אותם וקטורים קטנים שמקבלים על ידי הכפלת ההטמעות בשאילתה ובמטריצות המפתח.",
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "translatedText": "הביטוי הזה למעלה במונה הוא דרך ממש קומפקטית לייצג את הרשת של כל מוצרי הנקודות האפשריים בין זוגות של מפתחות ושאילתות.",
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "translatedText": "פרט טכני קטן שלא הזכרתי הוא שלצורך יציבות מספרית, במקרה מועיל לחלק את כל הערכים הללו בשורש הריבועי של הממד במרחב שאילתת מפתח זה.",
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "translatedText": "אז ה-softmax הזה שעוטף את הביטוי המלא אמור להיות מובן ליישם טור אחר טור.",
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "translatedText": "לגבי המונח V הזה, נדבר עליו רק בעוד שנייה.",
  "input": "As to that v term, we'll talk about it in just a second.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "translatedText": "לפני כן, יש עוד פרט טכני אחד שעד כה דילגתי עליו.",
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "translatedText": "במהלך תהליך האימון, כאשר אתה מפעיל את המודל הזה על דוגמה של טקסט נתון, וכל המשקולות מותאמות מעט ומכווננות כדי לתגמל או להעניש אותו על סמך ההסתברות הגבוהה שהוא מקצה למילה הבאה האמיתית בקטע, מסתבר שהופך את כל תהליך האימון להרבה יותר יעיל אם אתה מנבא בו זמנית כל אסימון הבא אפשרי בעקבות כל רצף ראשוני של אסימונים בקטע הזה.",
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "translatedText": "לדוגמה, עם הביטוי שבו התמקדנו, זה עשוי להיות גם חיזוי אילו מילים עוקבות אחר יצור ואיזה מילים עוקבות אחר.",
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "translatedText": "זה ממש נחמד, כי זה אומר שמה שאחרת יהיה דוגמה אחת לאימון פועל ביעילות כמו רבים.",
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "translatedText": "למטרות דפוס הקשב שלנו, זה אומר שאתה אף פעם לא רוצה לאפשר למילים מאוחרות יותר להשפיע על מילים קודמות, כי אחרת הם יכולים לתת את התשובה למה שיבוא אחר כך.",
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "translatedText": "המשמעות היא שאנחנו רוצים שכל הנקודות האלה כאן, אלו שמייצגות אסימונים מאוחרים יותר המשפיעים על קודמים, ייאלצו איכשהו להיות אפס.",
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "translatedText": "הדבר הפשוט ביותר שאתה עשוי לחשוב לעשות הוא להגדיר אותם שווה לאפס, אבל אם אתה עושה את זה העמודות לא יצטברו לאחד יותר, הם לא היו מנורמלים.",
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "translatedText": "אז במקום זאת, דרך נפוצה לעשות זאת היא שלפני החלת softmax, אתה מגדיר את כל הערכים האלה להיות אינסוף שלילי.",
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "translatedText": "אם אתה עושה את זה, אז לאחר החלת softmax, כל אלה הופכים לאפס, אבל העמודות נשארות מנורמלות.",
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "translatedText": "תהליך זה נקרא מיסוך.",
  "input": "This process is called masking.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "translatedText": "ישנן גרסאות של תשומת לב שבהן אתה לא מיישם את זה, אבל בדוגמה של GPT שלנו, למרות שזה רלוונטי יותר בשלב ההכשרה ממה שזה יהיה, נגיד, הפעלת אותו כצ&#39;אט בוט או משהו כזה, אתה תמיד מיישם מיסוך זה כדי למנוע מאסימונים מאוחרים יותר להשפיע על קודמים.",
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "translatedText": "עובדה נוספת שכדאי לחשוב עליה לגבי דפוס הקשב הזה היא כיצד גודלו שווה לריבוע של גודל ההקשר.",
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "translatedText": "אז זו הסיבה שגודל ההקשר יכול להוות צוואר בקבוק עצום עבור מודלים של שפות גדולות, והגדלה שלו אינה טריוויאלית.",
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "translatedText": "כפי שאתה מתאר לעצמך, מונעים על ידי רצון לחלונות הקשר גדולים יותר ויותר, בשנים האחרונות נראו כמה וריאציות למנגנון הקשב שמטרתו להפוך את ההקשר להרחבה יותר, אבל ממש כאן, אתה ואני נשארים ממוקדים ביסודות.",
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "translatedText": "אוקיי, נהדר, מחשוב הדפוס הזה מאפשר למודל להסיק אילו מילים רלוונטיות לאיזה מילים אחרות.",
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "translatedText": "כעת עליך לעדכן בפועל את ההטמעות, ולאפשר למילים להעביר מידע למילים אחרות שהן רלוונטיות לה.",
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "translatedText": "לדוגמה, אתה רוצה שההטבעה של פלאפי תגרום איכשהו לשינוי ב-Creature שיעביר אותו לחלק אחר של מרחב ההטמעה הזה ב-12,000 מימדים שמקודד באופן ספציפי יותר יצור פלאפי.",
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "translatedText": "מה שאני הולך לעשות כאן הוא קודם כל להראות לך את הדרך הכי פשוטה שאתה יכול לעשות את זה, אם כי יש דרך קלה שזה משתנה בהקשר של תשומת לב רב-ראשית.",
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "translatedText": "הדרך הפשוטה ביותר הזו תהיה להשתמש במטריצה שלישית, מה שאנו מכנים מטריצת הערך, אותה אתה מכפיל בהטמעה של המילה הראשונה, למשל Fluffy.",
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "translatedText": "התוצאה של זה היא מה שהיית מכנה וקטור ערך, וזה משהו שאתה מוסיף להטבעה של המילה השנייה, במקרה הזה משהו שאתה מוסיף להטבעה של Creature.",
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "translatedText": "אז וקטור הערך הזה חי באותו מרחב מאוד גבוה כמו ההטבעות.",
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "translatedText": "כשאתה מכפיל את מטריצת הערכים הזו בהטמעה של מילה, אתה עלול לחשוב על זה כאילו אתה אומר, אם המילה הזו רלוונטית להתאמת המשמעות של משהו אחר, מה בדיוק צריך להוסיף להטבעה של אותו משהו אחר כדי לשקף זֶה?",
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "translatedText": "במבט לאחור בתרשים שלנו, נניח בצד את כל המפתחות והשאילתות, שכן לאחר שתחשב את דפוס הקשב שסיימת עם אלה, אז אתה הולך לקחת את מטריצת הערכים הזו ולהכפיל אותה בכל אחת מההטבעות הללו. לייצר רצף של וקטורי ערך.",
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "translatedText": "אתה עשוי לחשוב על וקטורי ערכים אלה כמקושרים למפתחות המתאימים.",
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "translatedText": "עבור כל עמודה בתרשים זה, אתה מכפיל כל אחד מהוקטורי הערך במשקל המתאים באותה עמודה.",
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "translatedText": "לדוגמה כאן, תחת ההטמעה של Creature, תוסיף פרופורציות גדולות של וקטורי הערך עבור Fluffy וכחול, בעוד שכל וקטורי הערך האחרים מתאפסים, או לפחות כמעט מתאפסים.",
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "translatedText": "ולבסוף, הדרך למעשה לעדכן את ההטמעה הקשורה לעמודה הזו, שקודדה בעבר איזו משמעות נטולת הקשר של יצור, אתה מוסיף יחד את כל הערכים המותאמים מחדש בעמודה, ומייצרים שינוי שאתה רוצה להוסיף, שאני אתייג delta-e, ואז תוסיף את זה להטמעה המקורית.",
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "translatedText": "אני מקווה שהתוצאה היא וקטור מעודן יותר המקודד את המשמעות העשירה יותר מבחינה הקשרית, כמו זו של יצור כחול רך.",
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "translatedText": "וכמובן שאתה לא עושה את זה רק להטבעה אחת, אתה מחיל את אותו סכום משוקלל על פני כל העמודות בתמונה הזו, מייצר רצף של שינויים, הוספת כל השינויים האלה להטבעות המתאימות, מייצר רצף מלא של הטבעות מעודנות יותר שיוצאות מבלוק הקשב.",
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "translatedText": "אם תתרחבו, כל התהליך הזה הוא מה שתגדירו כראש תשומת לב יחיד.",
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "translatedText": "כפי שתיארתי דברים עד כה, תהליך זה מותאם לפרמטרים על ידי שלוש מטריצות נפרדות, כולן מלאות בפרמטרים הניתנים לשינוי, המפתח, השאילתה והערך.",
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "translatedText": "אני רוצה לקחת רגע כדי להמשיך את מה שהתחלנו בפרק האחרון, עם שמירת הניקוד שבה אנחנו סופרים את המספר הכולל של פרמטרים של הדגם באמצעות המספרים מ-GPT-3.",
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "translatedText": "למטריצות המפתח והשאילתה הללו יש 12,288 עמודות, התואמות לממד ההטמעה, ו-128 שורות, התואמות לממד של מרחב שאילתת מפתח קטן יותר.",
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "translatedText": "זה נותן לנו 1.5 מיליון פרמטרים נוספים לערך עבור כל אחד מהם.",
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "translatedText": "אם אתה מסתכל על מטריצת הערכים הזו לעומת זאת, הדרך שתיארתי את הדברים עד כה תצביע על כך שזו מטריצה מרובעת שיש לה 12,288 עמודות ו-12,288 שורות, מכיוון שגם הכניסות וגם הפלטים שלה חיים במרחב ההטמעה הגדול הזה.",
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "translatedText": "אם זה נכון, המשמעות היא כ-150 מיליון פרמטרים נוספים.",
  "input": "If true, that would mean about 150 million added parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "translatedText": "וכדי להיות ברור, אתה יכול לעשות את זה.",
  "input": "And to be clear, you could do that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "translatedText": "אתה יכול להקדיש למפת הערכים יותר פרמטרים בסדרי גודל מאשר למפתח ולשאילתה.",
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "translatedText": "אבל בפועל, זה הרבה יותר יעיל אם במקום זאת תגרום לכך שמספר הפרמטרים המוקדשים למפת הערכים הזו זהה למספר המוקדש למפתח ולשאילתה.",
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "translatedText": "זה רלוונטי במיוחד בהגדרה של הפעלת מספר ראשי קשב במקביל.",
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "translatedText": "האופן שבו זה נראה הוא שמפת הערכים מוערכת כמכפלה של שתי מטריצות קטנות יותר.",
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "translatedText": "מבחינה קונספטואלית, אני עדיין ממליץ לך לחשוב על המפה הליניארית הכוללת, כזו עם קלט ופלט, הן במרחב ההטבעה הגדול יותר הזה, למשל לקחת את הטבעת הכחול לכיוון הכחול הזה שתוסיף לשמות עצם.",
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "translatedText": "רק שזהו מספר קטן יותר של שורות, בדרך כלל בגודל זהה למרחב שאילתת המפתח.",
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "translatedText": "המשמעות היא שאתה יכול לחשוב על זה כעל מיפוי וקטורי הטבעה גדולים עד למרחב קטן בהרבה.",
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "translatedText": "זה לא השם המקובל, אבל אני הולך לקרוא לזה מטריצת הערך למטה.",
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "translatedText": "המטריצה השנייה ממפה מהמרחב הקטן יותר הזה בחזרה למרחב ההטמעה, מייצרת את הוקטורים שבהם אתה משתמש כדי לבצע את העדכונים בפועל.",
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "translatedText": "אני הולך לקרוא לזה מטריצת הערך למעלה, וזה שוב לא קונבנציונלי.",
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "translatedText": "הדרך שבה היית רואה את זה כתוב ברוב העיתונים נראית קצת אחרת.",
  "input": "The way that you would see this written in most papers looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "translatedText": "אני אדבר על זה בעוד דקה.",
  "input": "I'll talk about it in a minute.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "translatedText": "לדעתי, זה נוטה להפוך את הדברים לקצת יותר מבלבלים מבחינה רעיונית.",
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "translatedText": "אם לזרוק כאן ז&#39;רגון אלגברה ליניארי, מה שאנחנו בעצם עושים זה להגביל את מפת הערכים הכוללת להיות טרנספורמציה של דירוג נמוך.",
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "translatedText": "אם נחזור לספירת הפרמטרים, לכל ארבעת המטריצות הללו יש אותו גודל, ובחיבור של כולם נקבל כ-6.3 מיליון פרמטרים עבור ראש קשב אחד.",
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "translatedText": "כהערת צד מהירה, אם לדייק קצת יותר, כל מה שתואר עד כה הוא מה שאנשים היו מכנים ראש תשומת לב עצמית, כדי להבחין בינו לבין וריאציה שעולה במודלים אחרים שנקראת תשומת לב צולבת.",
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "translatedText": "זה לא רלוונטי לדוגמה שלנו ב-GPT, אבל אם אתה סקרן, תשומת לב צולבת כוללת מודלים המעבדים שני סוגים שונים של נתונים, כמו טקסט בשפה אחת וטקסט בשפה אחרת שהוא חלק מדור מתמשך של תרגום, או אולי קלט אודיו של דיבור ותמלול מתמשך.",
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "translatedText": "ראש עם תשומת לב צולבת נראה כמעט זהה.",
  "input": "A cross-attention head looks almost identical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "translatedText": "ההבדל היחיד הוא שמפות המפתח והמפות פועלות על מערכי נתונים שונים.",
  "input": "The only difference is that the key and query maps act on different data sets.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "translatedText": "במודל שעושה תרגום, למשל, המפתחות עשויים להגיע משפה אחת, בעוד שהשאילתות מגיעות משפה אחרת, ודפוס הקשב יכול לתאר אילו מילים משפה אחת מתאימות לאיזה מילים בשפה אחרת.",
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "translatedText": "ובהגדרה הזו בדרך כלל לא יהיה מיסוך, מכיוון שאין ממש מושג של אסימונים מאוחרים יותר המשפיעים על קודמים.",
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "translatedText": "עם זאת, אם הייתם מרוכזים בתשומת לב עצמית, אם הייתם מבינים הכל עד כה, ואם הייתם עוצרים כאן, הייתם יוצאים עם המהות של מהי תשומת לב באמת.",
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "translatedText": "כל מה שבאמת נשאר לנו הוא לפרט את המובן שבו אתה עושה זאת פעמים רבות ושונות.",
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "translatedText": "בדוגמה המרכזית שלנו התמקדנו בשמות תואר המעדכנים שמות עצם, אבל כמובן שיש הרבה דרכים שונות שבהן הקשר יכול להשפיע על המשמעות של מילה.",
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "translatedText": "אם המילים שהם התרסקו לפני המילה מכונית, יש לכך השלכות על הצורה והמבנה של אותה מכונית.",
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "translatedText": "והרבה אסוציאציות עשויות להיות פחות דקדוקיות.",
  "input": "And a lot of associations might be less grammatical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "translatedText": "אם המילה אשף נמצאת במקום כלשהו באותו קטע כמו הארי, זה מצביע על כך שאולי זה מתייחס להארי פוטר, בעוד שאם במקום זאת המילים קווין, סאסקס וויליאם היו בקטע הזה, אז אולי יש לעדכן את הטבעת הארי במקום זאת. להתייחס לנסיך.",
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "translatedText": "עבור כל סוג שונה של עדכון הקשר שאתה יכול לדמיין, הפרמטרים של מטריצות מפתח ושאילתות אלה יהיו שונים כדי ללכוד את דפוסי הקשב השונים, והפרמטרים של מפת הערכים שלנו יהיו שונים בהתבסס על מה שצריך להוסיף להטמעות.",
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "translatedText": "ושוב, בפועל קשה הרבה יותר לפרש את ההתנהגות האמיתית של מפות אלה, כאשר המשקולות מוגדרות לעשות כל מה שהמודל צריך לעשות כדי להגשים בצורה הטובה ביותר את מטרתו לחזות את האסימון הבא.",
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "translatedText": "כפי שאמרתי קודם, כל מה שתיארנו הוא ראש קשב יחיד, ובלוק קשב מלא בתוך שנאי מורכב ממה שנקרא קשב רב-ראשי, שבו אתה מפעיל הרבה מהפעולות האלה במקביל, כל אחת עם שאילתת מפתח נפרדת משלה. ומפות ערכיות.",
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "translatedText": "GPT-3 למשל משתמש ב-96 ראשי קשב בתוך כל בלוק.",
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "translatedText": "בהתחשב בכך שכל אחד מהם כבר קצת מבלבל, אין ספק שזה הרבה מה להחזיק בראש.",
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "translatedText": "רק כדי לאיית את הכל בצורה מאוד מפורשת, זה אומר שיש לך 96 מטריצות מפתח ושאילתות נפרדות המייצרות 96 דפוסי קשב ברורים.",
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "translatedText": "אז לכל ראש יש מטריצות ערך נפרדות משלו המשמשות לייצור 96 רצפים של וקטורי ערך.",
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "translatedText": "כל אלה מתווספים יחד באמצעות דפוסי הקשב המתאימים כמשקולות.",
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "translatedText": "המשמעות היא שלכל עמדה בהקשר, כל אסימון, כל אחד מהראשים הללו מייצר שינוי מוצע שיתווסף להטמעה בעמדה זו.",
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "translatedText": "אז מה שאתה עושה זה שאתה מסכם את כל השינויים המוצעים האלה, אחד לכל ראש, ואתה מוסיף את התוצאה להטבעה המקורית של המיקום הזה.",
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "translatedText": "כל הסכום הזה כאן יהיה נתח אחד ממה שמופק מבלוק הקשב רב-הראשים הזה, אחד מההטבעות המעודנות האלה שמבצבצות מהקצה השני שלו.",
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "translatedText": "שוב, זה הרבה לחשוב על כך, אז אל תדאג בכלל אם לוקח קצת זמן לשקוע.",
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "translatedText": "הרעיון הכללי הוא שעל ידי הפעלת ראשים שונים במקביל, אתה נותן למודל את היכולת ללמוד דרכים רבות ומובחנות שבהקשר משנה משמעות.",
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "translatedText": "כשמגדילים את ספירת הפרמטרים שלנו עם 96 ראשים, שכל אחד מהם כולל את הווריאציה שלו של ארבעת המטריצות הללו, כל בלוק של תשומת לב רב-ראשית מסתיימת בסביבות 600 מיליון פרמטרים.",
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "translatedText": "יש עוד דבר אחד קצת מעצבן שאני באמת צריך להזכיר עבור כל אחד מכם שימשיך לקרוא עוד על שנאים.",
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "translatedText": "אתה זוכר איך אמרתי שמפת הערכים מחולקת לשתי המטריצות הנבדלות הללו, אותן תייגתי כמטריצות הערך למטה והערך למעלה.",
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "translatedText": "האופן שבו ניסחתי את הדברים יציע לך לראות את צמד המטריצות הזה בתוך כל ראש קשב, ותוכל ליישם את זה בצורה זו.",
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "translatedText": "זה יהיה עיצוב תקף.",
  "input": "That would be a valid design.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "translatedText": "אבל הדרך שבה אתה רואה את זה כתוב בעיתונים והדרך שבה זה מיושם בפועל נראית קצת אחרת.",
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "translatedText": "כל מטריצות הערך למעלה הללו עבור כל ראש מופיעות מהודקות יחדיו במטריצה ענקית אחת שאנו קוראים לה מטריצת הפלט, הקשורה לכל בלוק הקשב רב הראשים.",
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "translatedText": "וכשאתה רואה אנשים מתייחסים למטריצת הערך של ראש תשומת לב נתון, הם בדרך כלל מתייחסים רק לשלב הראשון הזה, זה שסימנתי כהקרנת הערך למטה לחלל הקטן יותר.",
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "translatedText": "לסקרנים שביניכם, השארתי על כך הערה על המסך.",
  "input": "For the curious among you, I've left an on-screen note about it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "translatedText": "זה אחד מהפרטים האלה שמסתכנים להסיח את הדעת מהנקודות המושגיות העיקריות, אבל אני כן רוצה לקרוא את זה רק כדי שתדע אם אתה קורא על זה במקורות אחרים.",
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "translatedText": "אם שמים בצד את כל הניואנסים הטכניים, בתצוגה המקדימה מהפרק האחרון ראינו כיצד נתונים שזורמים דרך שנאי לא זורמים רק דרך בלוק קשב אחד.",
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "translatedText": "ראשית, הוא עובר גם את הפעולות האחרות הנקראות תפיסות רב-שכבתיות.",
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "translatedText": "על אלה נדבר יותר בפרק הבא.",
  "input": "We'll talk more about those in the next chapter.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "translatedText": "ואז זה עובר שוב ושוב דרך הרבה הרבה עותקים של שתי הפעולות האלה.",
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "translatedText": "המשמעות היא שאחרי שמילה נתונה תופסת חלק מההקשר שלה, יש הרבה יותר סיכויים שההטבעה הניואנסית הזו תושפע מהסביבה הניואנסית יותר שלה.",
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "translatedText": "ככל שאתה הולך למטה ברשת, כאשר כל הטבעה מקבלת יותר ויותר משמעות מכל ההטבעות האחרות, שהן עצמן נעשות יותר ויותר ניואנסים, התקווה היא שיש יכולת לקודד רעיונות ברמה גבוהה ומופשטת יותר לגבי נתון. קלט מעבר רק לתארים ולמבנה דקדוקי.",
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "translatedText": "דברים כמו סנטימנט וטון והאם זה שיר ואיזה אמיתות מדעיות בבסיס רלוונטיות ליצירה ודברים כאלה.",
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "translatedText": "אם נחזור פעם נוספת לשמירה על הניקוד שלנו, GPT-3 כולל 96 שכבות נפרדות, כך שהמספר הכולל של שאילתת מפתח וערך פרמטרים מוכפל בעוד 96, מה שמביא את הסכום הכולל לקצת פחות מ-58 מיליארד פרמטרים נפרדים המוקדשים לכל ראשי תשומת לב.",
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "translatedText": "זה הרבה מה להיות בטוח, אבל זה רק כשליש מתוך 175 מיליארד שיש ברשת בסך הכל.",
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "translatedText": "אז למרות שתשומת הלב מקבלת את כל תשומת הלב, רוב הפרמטרים מגיעים מהבלוקים שיושבים בין השלבים האלה.",
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "translatedText": "בפרק הבא, אתה ואני נדבר יותר על הבלוקים האחרים האלה וגם הרבה יותר על תהליך האימון.",
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "translatedText": "חלק גדול מהסיפור להצלחת מנגנון הקשב הוא לא כל כך סוג ספציפי של התנהגות שהוא מאפשר, אלא העובדה שהוא מקביל ביותר, כלומר, אתה יכול להריץ מספר עצום של חישובים בזמן קצר באמצעות GPUs .",
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "translatedText": "בהתחשב בעובדה שאחד הלקחים הגדולים לגבי למידה עמוקה בעשור או שניים האחרונים היה שקנה המידה לבדו נותן שיפורים איכותיים עצומים בביצועי המודל, יש יתרון עצום לארכיטקטורות הניתנות להקבלה שמאפשרות לך לעשות זאת.",
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "translatedText": "אם אתה רוצה ללמוד עוד על החומר הזה, השארתי הרבה קישורים בתיאור.",
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "translatedText": "בפרט, כל דבר שמיוצר על ידי אנדריי קרפטי או כריס אולה נוטה להיות זהב טהור.",
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "translatedText": "בסרטון הזה, רציתי רק לקפוץ לתשומת לב בצורתו הנוכחית, אבל אם אתה סקרן לגבי עוד מההיסטוריה של איך הגענו לכאן ואיך אתה יכול להמציא את הרעיון הזה מחדש לעצמך, ידידי Vivek פשוט העלה זוג סרטונים שנותנים הרבה יותר מהמוטיבציה הזו.",
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "translatedText": "כמו כן, לבריט קרוז מהערוץ The Art of the Problem יש סרטון ממש נחמד על ההיסטוריה של דגמי שפות גדולים.",
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "translatedText": "תודה.",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]