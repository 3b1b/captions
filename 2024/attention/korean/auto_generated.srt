1
00:00:00,000 --> 00:00:02,172
지난 장에서 여러분과 저는 변압기의 

2
00:00:02,172 --> 00:00:04,019
내부 작동 원리를 살펴봤습니다.

3
00:00:04,560 --> 00:00:07,214
이는 대규모 언어 모델과 최신 AI 물결의 

4
00:00:07,214 --> 00:00:10,200
다른 많은 도구에서 핵심적인 기술 중 하나입니다.

5
00:00:10,980 --> 00:00:13,496
주의 집중은 이제 유명한 2017년 논문 '주의 

6
00:00:13,496 --> 00:00:16,013
집중이 필요한 모든 것'에서 처음 소개되었으며, 

7
00:00:16,013 --> 00:00:18,437
이 장에서는 이 주의 집중 메커니즘이 무엇인지 

8
00:00:18,437 --> 00:00:21,140
자세히 살펴보고 데이터를 처리하는 방법을 시각화하여 

9
00:00:21,140 --> 00:00:21,700
설명합니다.

10
00:00:26,140 --> 00:00:27,610
간단히 요약하면 다음과 같은 

11
00:00:27,610 --> 00:00:29,540
중요한 맥락을 염두에 두시기 바랍니다.

12
00:00:30,000 --> 00:00:32,828
여러분과 제가 연구하고 있는 모델의 목표는 텍스트 

13
00:00:32,828 --> 00:00:35,555
조각을 받아 다음에 어떤 단어가 나올지 예측하는 

14
00:00:35,555 --> 00:00:36,060
것입니다.

15
00:00:36,860 --> 00:00:39,878
입력 텍스트는 토큰이라고 부르는 작은 조각으로 

16
00:00:39,878 --> 00:00:43,013
나뉘는데, 토큰은 단어 또는 단어 조각인 경우가 

17
00:00:43,013 --> 00:00:46,148
많지만 이 비디오의 예를 여러분과 제가 더 쉽게 

18
00:00:46,148 --> 00:00:49,282
생각할 수 있도록 토큰은 항상 단어라고 가정하여 

19
00:00:49,282 --> 00:00:50,560
단순화해 보겠습니다.

20
00:00:51,480 --> 00:00:54,474
트랜스포머의 첫 번째 단계는 각 토큰을 고차원 

21
00:00:54,474 --> 00:00:57,700
벡터와 연결하는 것으로, 이를 임베딩이라고 합니다.

22
00:00:57,700 --> 00:01:00,693
제가 염두에 두었으면 하는 가장 중요한 아이디어는 

23
00:01:00,693 --> 00:01:03,579
이 고차원 공간에서 가능한 모든 임베딩의 방향이 

24
00:01:03,579 --> 00:01:06,465
의미론적 의미와 어떻게 일치할 수 있는지에 대한 

25
00:01:06,465 --> 00:01:07,000
것입니다.

26
00:01:07,680 --> 00:01:10,670
지난 장에서 우리는 방향이 성별에 어떻게 대응할 

27
00:01:10,670 --> 00:01:12,774
수 있는지에 대한 예를 보았는데, 

28
00:01:12,774 --> 00:01:15,542
이 공간에 특정 단계를 추가하면 남성 명사의 

29
00:01:15,542 --> 00:01:18,311
내포에서 해당 여성 명사의 내포로 이동할 수 

30
00:01:18,311 --> 00:01:19,640
있다는 의미에서입니다.

31
00:01:20,160 --> 00:01:22,600
이 고차원 공간에서 얼마나 많은 다른 방향이 

32
00:01:22,600 --> 00:01:25,041
단어의 의미의 수많은 다른 측면에 대응할 수 

33
00:01:25,041 --> 00:01:27,580
있는지 상상할 수 있는 한 가지 예일 뿐입니다.

34
00:01:28,800 --> 00:01:31,832
트랜스포머의 목표는 이러한 임베딩을 점진적으로 

35
00:01:31,832 --> 00:01:35,214
조정하여 단순히 개별 단어를 인코딩하는 것이 아니라 

36
00:01:35,214 --> 00:01:38,596
훨씬 더 풍부한 문맥적 의미를 담을 수 있도록 하는 

37
00:01:38,596 --> 00:01:39,180
것입니다.

38
00:01:40,140 --> 00:01:42,187
많은 사람들이 트랜스포머의 핵심 요소인 

39
00:01:42,187 --> 00:01:44,141
주의 집중 메커니즘을 매우 혼란스럽게 

40
00:01:44,141 --> 00:01:46,002
생각하기 때문에 적응하는 데 시간이 

41
00:01:46,002 --> 00:01:48,049
걸리더라도 걱정하지 않아도 된다는 점을 

42
00:01:48,049 --> 00:01:48,980
미리 말씀드립니다.

43
00:01:49,440 --> 00:01:52,640
계산 세부 사항과 모든 행렬 곱셈에 대해 자세히 

44
00:01:52,640 --> 00:01:55,840
알아보기 전에 주의가 필요한 동작의 종류에 대해 

45
00:01:55,840 --> 00:01:59,160
몇 가지 예를 생각해 볼 필요가 있다고 생각합니다.

46
00:02:00,140 --> 00:02:03,312
미국의 진정한 두더지, 이산화탄소 한 점, 

47
00:02:03,312 --> 00:02:06,220
두더지 생검이라는 문구를 생각해 보세요.

48
00:02:06,700 --> 00:02:08,877
여러분과 저는 두더지라는 단어가 문맥에 따라 각각 

49
00:02:08,877 --> 00:02:10,900
다른 의미를 가지고 있다는 것을 알고 있습니다.

50
00:02:11,360 --> 00:02:15,075
그러나 텍스트를 분할하고 각 토큰을 벡터와 연결하는 

51
00:02:15,075 --> 00:02:18,661
트랜스포머의 첫 번째 단계 이후에는 이 초기 토큰 

52
00:02:18,661 --> 00:02:21,992
임베딩이 사실상 컨텍스트를 참조하지 않는 조회 

53
00:02:21,992 --> 00:02:25,451
테이블이므로 두더지와 연관된 벡터는 모든 경우에 

54
00:02:25,451 --> 00:02:26,220
동일합니다.

55
00:02:26,620 --> 00:02:29,575
트랜스포머의 다음 단계에서만 주변 임베딩이 이 

56
00:02:29,575 --> 00:02:32,645
트랜스포머로 정보를 전달할 수 있는 기회를 갖게 

57
00:02:32,645 --> 00:02:33,100
됩니다.

58
00:02:33,820 --> 00:02:37,287
이 임베딩 공간에는 두더지라는 단어의 여러 가지 

59
00:02:37,287 --> 00:02:40,498
의미를 인코딩하는 여러 가지 방향이 있으며, 

60
00:02:40,498 --> 00:02:43,965
잘 훈련된 주의 블록은 문맥의 함수에 따라 일반 

61
00:02:43,965 --> 00:02:47,561
임베딩에 추가해야 할 내용을 계산하여 이러한 특정 

62
00:02:47,561 --> 00:02:51,157
방향 중 하나로 이동시킨다는 그림을 염두에 둘 수 

63
00:02:51,157 --> 00:02:51,800
있습니다.

64
00:02:53,300 --> 00:02:54,664
다른 예를 들어 타워라는 단어를 

65
00:02:54,664 --> 00:02:56,180
임베드하는 경우를 생각해 보겠습니다.

66
00:02:57,060 --> 00:03:00,504
이것은 아마도 다른 많은 크고 키가 큰 명사와 연관된 

67
00:03:00,504 --> 00:03:03,720
매우 일반적이고 비특이적인 공간의 방향일 것입니다.

68
00:03:04,020 --> 00:03:06,674
이 단어 앞에 에펠이 바로 붙는다면, 

69
00:03:06,674 --> 00:03:10,339
이 벡터가 에펠탑을 더 구체적으로 인코딩하는 방향, 

70
00:03:10,339 --> 00:03:14,004
즉 파리와 프랑스 및 강철로 만들어진 것들과 연관된 

71
00:03:14,004 --> 00:03:17,669
벡터를 가리키도록 메커니즘이 업데이트되기를 원한다고 

72
00:03:17,669 --> 00:03:19,060
상상할 수 있습니다.

73
00:03:19,920 --> 00:03:22,156
앞에 미니어처라는 단어가 붙으면 

74
00:03:22,156 --> 00:03:24,766
벡터가 더 이상 크고 키가 큰 물건과 

75
00:03:24,766 --> 00:03:27,500
연관되지 않도록 더 업데이트해야 합니다.

76
00:03:29,480 --> 00:03:32,200
주의 블록은 단순히 단어의 의미를 세분화하는 

77
00:03:32,200 --> 00:03:35,029
것 이상으로, 모델이 하나의 임베딩에 인코딩된 

78
00:03:35,029 --> 00:03:37,532
정보를 다른 임베딩의 정보, 잠재적으로는 

79
00:03:37,532 --> 00:03:39,491
상당히 멀리 떨어져 있는 정보, 

80
00:03:39,491 --> 00:03:42,211
단일 단어보다 훨씬 더 풍부한 정보로 이동할 

81
00:03:42,211 --> 00:03:43,300
수 있게 해줍니다.

82
00:03:43,300 --> 00:03:47,045
지난 장에서 다양한 주의 블록을 포함한 모든 

83
00:03:47,045 --> 00:03:49,591
벡터가 네트워크를 통과한 후, 

84
00:03:49,591 --> 00:03:53,186
다음 토큰을 예측하기 위해 수행하는 계산은 

85
00:03:53,186 --> 00:03:57,081
전적으로 시퀀스의 마지막 벡터의 함수라는 것을 

86
00:03:57,081 --> 00:03:58,280
살펴보았습니다.

87
00:03:59,100 --> 00:04:02,107
예를 들어, 입력한 텍스트가 추리 소설의 대부분을 

88
00:04:02,107 --> 00:04:04,792
차지하며, 마지막에 가까운 부분까지 '따라서 

89
00:04:04,792 --> 00:04:07,800
범인은 살인자였다'라고 쓰여 있다고 상상해 보세요.

90
00:04:08,400 --> 00:04:11,978
모델이 다음 단어를 정확하게 예측하려면 단순히 

91
00:04:11,978 --> 00:04:15,694
단어가 포함된 시퀀스의 마지막 벡터가 모든 주의 

92
00:04:15,694 --> 00:04:19,411
블록에 의해 업데이트되어 개별 단어보다 훨씬 더 

93
00:04:19,411 --> 00:04:23,127
많은 정보를 나타내야 하며, 어떻게든 다음 단어 

94
00:04:23,127 --> 00:04:26,843
예측과 관련된 전체 컨텍스트 창에서 모든 정보를 

95
00:04:26,843 --> 00:04:28,220
인코딩해야 합니다.

96
00:04:29,500 --> 00:04:31,039
하지만 계산을 단계별로 살펴보기 위해 

97
00:04:31,039 --> 00:04:32,580
훨씬 더 간단한 예를 들어 보겠습니다.

98
00:04:32,980 --> 00:04:35,278
입력에 '푸른 숲을 돌아다니는 푹신한 푸른 

99
00:04:35,278 --> 00:04:37,960
생물'이라는 문구가 포함되어 있다고 상상해 보세요.

100
00:04:38,460 --> 00:04:41,497
지금은 우리가 신경 쓰는 유일한 업데이트 

101
00:04:41,497 --> 00:04:44,138
유형이 형용사가 해당 명사의 의미를 

102
00:04:44,138 --> 00:04:46,780
조정하는 것이라고 가정해 보겠습니다.

103
00:04:47,000 --> 00:04:48,991
제가 지금 설명하려는 것은 하나의 주의 

104
00:04:48,991 --> 00:04:50,530
집중 헤드라고 할 수 있으며, 

105
00:04:50,530 --> 00:04:52,703
나중에 주의 집중 블록이 어떻게 여러 개의 

106
00:04:52,703 --> 00:04:55,420
다른 헤드로 구성되어 병렬로 실행되는지 살펴보겠습니다.

107
00:04:56,140 --> 00:04:59,698
다시 말하지만, 각 단어에 대한 초기 임베딩은 문맥 

108
00:04:59,698 --> 00:05:03,380
없이 특정 단어의 의미만 인코딩하는 고차원 벡터입니다.

109
00:05:04,000 --> 00:05:05,220
사실 그렇지 않습니다.

110
00:05:05,380 --> 00:05:07,640
또한 단어의 위치도 인코딩합니다.

111
00:05:07,980 --> 00:05:10,768
위치가 인코딩되는 방식에 대해서는 더 많은 

112
00:05:10,768 --> 00:05:13,672
이야기가 있지만, 지금은 이 벡터의 항목으로 

113
00:05:13,672 --> 00:05:16,576
단어가 무엇인지, 문맥에서 어디에 존재하는지 

114
00:05:16,576 --> 00:05:18,900
알 수 있다는 것만 알아두면 됩니다.

115
00:05:19,500 --> 00:05:21,660
이제 이러한 임베딩을 문자 e로 표시해 보겠습니다.

116
00:05:22,420 --> 00:05:26,086
목표는 일련의 계산을 통해, 예를 들어 명사에 

117
00:05:26,086 --> 00:05:29,753
해당하는 단어가 해당 형용사에서 의미를 가져온 

118
00:05:29,753 --> 00:05:33,420
새로운 세련된 임베딩 집합을 생성하는 것입니다.

119
00:05:33,900 --> 00:05:36,314
그리고 딥러닝 게임에서는 대부분의 계산이 

120
00:05:36,314 --> 00:05:38,520
행렬-벡터 곱처럼 보이기를 원하는데, 

121
00:05:38,520 --> 00:05:40,935
여기서 행렬은 조정 가능한 가중치로 가득 

122
00:05:40,935 --> 00:05:43,980
차 있으며 모델이 데이터를 기반으로 학습하게 됩니다.

123
00:05:44,660 --> 00:05:47,069
명확히 말씀드리자면, 저는 주의 집중력이 있는 

124
00:05:47,069 --> 00:05:49,664
사람이 하는 행동의 유형을 설명하기 위해 형용사가 

125
00:05:49,664 --> 00:05:52,260
명사를 업데이트하는 이 예시를 만들어낸 것뿐입니다.

126
00:05:52,860 --> 00:05:55,718
많은 딥러닝이 그렇듯이, 비용 함수를 최소화하기 위해 

127
00:05:55,718 --> 00:05:58,386
수많은 매개변수를 조정하고 조정하는 것을 기반으로 

128
00:05:58,386 --> 00:06:00,768
하기 때문에 실제 동작은 분석하기가 훨씬 더 

129
00:06:00,768 --> 00:06:01,340
어렵습니다.

130
00:06:01,680 --> 00:06:04,427
다만 이 프로세스에 관련된 매개변수로 채워진 

131
00:06:04,427 --> 00:06:06,405
다양한 행렬을 모두 살펴볼 때, 

132
00:06:06,405 --> 00:06:09,043
이 모든 것을 보다 구체적으로 파악하는 데 

133
00:06:09,043 --> 00:06:11,791
도움이 될 수 있는 상상의 예가 있으면 정말 

134
00:06:11,791 --> 00:06:13,220
도움이 될 것 같습니다.

135
00:06:14,140 --> 00:06:16,746
이 과정의 첫 번째 단계에서는 생물과 같은 

136
00:06:16,746 --> 00:06:19,353
각 명사가 '내 앞에 형용사가 있나요'라는 

137
00:06:19,353 --> 00:06:21,960
질문을 던지는 것을 상상해 볼 수 있습니다.

138
00:06:22,160 --> 00:06:24,009
그리고 푹신푹신하고 파란색이라는 단어에 

139
00:06:24,009 --> 00:06:25,774
대해 각각 '예, 저는 형용사이고 그 

140
00:06:25,774 --> 00:06:27,960
위치에 있습니다'라고 대답할 수 있어야 합니다.

141
00:06:28,960 --> 00:06:31,340
이 질문은 어떻게든 또 다른 벡터, 

142
00:06:31,340 --> 00:06:33,482
즉 숫자의 목록으로 인코딩되며, 

143
00:06:33,482 --> 00:06:36,100
이를 이 단어에 대한 쿼리라고 부릅니다.

144
00:06:36,980 --> 00:06:39,443
하지만 이 쿼리 벡터는 임베딩 벡터보다 

145
00:06:39,443 --> 00:06:42,020
훨씬 작은 크기(예: 128)를 가집니다.

146
00:06:42,940 --> 00:06:45,116
이 쿼리를 계산하는 것은 특정 행렬을 

147
00:06:45,116 --> 00:06:47,396
가져와서 여기에 임베딩을 곱하는 것처럼 

148
00:06:47,396 --> 00:06:49,780
보입니다(wq라고 레이블을 붙이겠습니다).

149
00:06:50,960 --> 00:06:53,991
조금 압축해서 쿼리 벡터를 q라고 쓰고, 

150
00:06:53,991 --> 00:06:57,155
이렇게 화살표 옆에 행렬을 표시한 것은 이 

151
00:06:57,155 --> 00:07:00,713
행렬에 화살표 시작 부분의 벡터를 곱하면 화살표 

152
00:07:00,713 --> 00:07:04,140
끝 부분의 벡터가 나온다는 것을 나타내기 위한 

153
00:07:04,140 --> 00:07:04,800
것입니다.

154
00:07:05,860 --> 00:07:08,914
이 경우 이 행렬에 컨텍스트의 모든 임베딩을 

155
00:07:08,914 --> 00:07:12,580
곱하여 각 토큰에 대해 하나의 쿼리 벡터를 생성합니다.

156
00:07:13,740 --> 00:07:15,954
이 행렬의 항목은 모델의 매개변수로, 

157
00:07:15,954 --> 00:07:18,590
데이터에서 실제 행동을 학습한다는 의미이며, 

158
00:07:18,590 --> 00:07:20,909
실제로 이 행렬이 특정 주의력 헤드에서 

159
00:07:20,909 --> 00:07:23,440
수행하는 작업은 구문 분석하기가 어렵습니다.

160
00:07:23,900 --> 00:07:27,588
그러나 우리가 학습하기를 바라는 예를 상상하기 위해, 

161
00:07:27,588 --> 00:07:31,277
이 쿼리 행렬이 명사의 포함을 이 작은 쿼리 공간에서 

162
00:07:31,277 --> 00:07:34,843
특정 방향으로 매핑하여 앞의 위치에서 형용사를 찾는 

163
00:07:34,843 --> 00:07:38,040
개념을 어떻게든 인코딩한다고 가정해 보겠습니다.

164
00:07:38,780 --> 00:07:41,440
다른 임베딩에 어떤 영향을 미칠지는 누가 알겠습니까?

165
00:07:41,720 --> 00:07:42,884
어쩌면 그것으로 다른 목표를 

166
00:07:42,884 --> 00:07:44,340
동시에 달성하려고 할 수도 있습니다.

167
00:07:44,540 --> 00:07:47,160
지금은 명사에 집중하고 있습니다.

168
00:07:47,280 --> 00:07:51,044
동시에 이와 관련된 두 번째 행렬인 

169
00:07:51,044 --> 00:07:54,620
키 행렬도 모든 임베딩에 곱합니다.

170
00:07:55,280 --> 00:07:56,890
이렇게 하면 키라고 부르는 두 

171
00:07:56,890 --> 00:07:58,500
번째 벡터 시퀀스가 생성됩니다.

172
00:07:59,420 --> 00:08:01,073
개념적으로 키는 쿼리에 대한 

173
00:08:01,073 --> 00:08:03,140
잠재적인 답변이라고 생각하면 됩니다.

174
00:08:03,840 --> 00:08:06,291
이 키 행렬은 또한 조정 가능한 파라미터로 

175
00:08:06,291 --> 00:08:08,539
가득 차 있으며 쿼리 행렬과 마찬가지로 

176
00:08:08,539 --> 00:08:11,400
임베딩 벡터를 동일한 작은 차원 공간에 매핑합니다.

177
00:08:12,200 --> 00:08:14,433
키는 쿼리가 서로 밀접하게 일치할 

178
00:08:14,433 --> 00:08:17,020
때마다 일치하는 것으로 생각하면 됩니다.

179
00:08:17,460 --> 00:08:19,854
이 예제에서는 키 매트릭스가 fluffy, 

180
00:08:19,854 --> 00:08:22,349
blue와 같은 형용사를 creature라는 

181
00:08:22,349 --> 00:08:24,644
단어가 생성하는 쿼리와 밀접하게 일치하는 

182
00:08:24,644 --> 00:08:26,740
벡터에 매핑한다고 상상할 수 있습니다.

183
00:08:27,200 --> 00:08:30,600
각 키가 각 쿼리와 얼마나 잘 일치하는지 측정하려면 

184
00:08:30,600 --> 00:08:34,000
가능한 각 키-쿼리 쌍 사이의 도트 곱을 계산합니다.

185
00:08:34,480 --> 00:08:37,061
저는 점으로 가득 찬 그리드를 시각화하는 

186
00:08:37,061 --> 00:08:39,866
것을 좋아하는데, 큰 점이 더 큰 점 제품, 

187
00:08:39,866 --> 00:08:42,559
즉 키와 쿼리가 정렬되는 위치에 해당합니다.

188
00:08:43,280 --> 00:08:47,556
형용사 명사 예제의 경우, 다음과 같이 표시되는데, 

189
00:08:47,556 --> 00:08:51,094
만약 플러피와 블루가 생성하는 키가 실제로 

190
00:08:51,094 --> 00:08:54,781
크리처가 생성하는 쿼리와 거의 일치한다면 이 

191
00:08:54,781 --> 00:08:58,320
두 지점의 점 곱은 큰 양수가 될 것입니다.

192
00:08:59,100 --> 00:09:01,206
머신 러닝 전문 용어로 말하자면, 

193
00:09:01,206 --> 00:09:04,422
솜털과 파란색의 임베딩은 생물의 임베딩을 의미한다고 

194
00:09:04,422 --> 00:09:05,420
할 수 있습니다.

195
00:09:06,040 --> 00:09:09,057
반면, the와 같은 다른 단어의 키와 

196
00:09:09,057 --> 00:09:12,485
creature에 대한 쿼리 사이의 점 곱은 

197
00:09:12,485 --> 00:09:16,600
서로 관련이 없는 작은 값 또는 음수 값을 반영합니다.

198
00:09:17,700 --> 00:09:20,095
따라서 음의 무한대에서 무한대까지 모든 

199
00:09:20,095 --> 00:09:22,599
실수가 될 수 있는 값 그리드를 통해 각 

200
00:09:22,599 --> 00:09:25,322
단어가 다른 모든 단어의 의미를 업데이트하는 

201
00:09:25,322 --> 00:09:28,480
데 얼마나 관련성이 있는지에 대한 점수를 부여합니다.

202
00:09:29,200 --> 00:09:31,357
이 점수를 사용하는 방법은 각 열에 

203
00:09:31,357 --> 00:09:33,622
따라 관련성에 따라 가중치를 부여하여 

204
00:09:33,622 --> 00:09:35,780
특정 가중치 합계를 구하는 것입니다.

205
00:09:36,520 --> 00:09:40,305
따라서 음의 무한대에서 무한대까지의 값 범위 

206
00:09:40,305 --> 00:09:43,939
대신 0에서 1 사이의 값과 확률 분포처럼 

207
00:09:43,939 --> 00:09:48,180
각 열의 숫자가 합산되어 1이 되는 것을 원합니다.

208
00:09:49,280 --> 00:09:50,789
마지막 챕터부터 읽으셨다면 어떻게 

209
00:09:50,789 --> 00:09:52,220
해야 할지 잘 알고 계실 겁니다.

210
00:09:52,620 --> 00:09:55,235
이러한 각 열을 따라 소프트맥스를 

211
00:09:55,235 --> 00:09:57,300
계산하여 값을 정규화합니다.

212
00:10:00,060 --> 00:10:02,839
그림에서는 모든 열에 소프트맥스를 적용한 

213
00:10:02,839 --> 00:10:05,860
후 이러한 정규화된 값으로 그리드를 채웁니다.

214
00:10:06,780 --> 00:10:09,493
이 시점에서 각 열은 왼쪽의 단어가 상단의 

215
00:10:09,493 --> 00:10:12,093
해당 값과 얼마나 관련성이 있는지에 따라 

216
00:10:12,093 --> 00:10:14,580
가중치를 부여한다고 생각해도 안전합니다.

217
00:10:15,080 --> 00:10:16,840
우리는 이 그리드를 주의 패턴이라고 부릅니다.

218
00:10:18,080 --> 00:10:20,342
이제 원본 트랜스포머 종이를 보면 이 

219
00:10:20,342 --> 00:10:22,820
모든 것을 간결하게 적는 방식이 있습니다.

220
00:10:23,880 --> 00:10:27,365
여기서 변수 q와 k는 각각 쿼리 및 키 

221
00:10:27,365 --> 00:10:30,699
벡터의 전체 배열, 즉 임베딩에 쿼리와 

222
00:10:30,699 --> 00:10:34,640
키 행렬을 곱하여 얻는 작은 벡터를 나타냅니다.

223
00:10:35,160 --> 00:10:37,655
분자에 있는 이 표현식은 키와 쿼리 

224
00:10:37,655 --> 00:10:40,025
쌍 사이에 가능한 모든 도트 곱의 

225
00:10:40,025 --> 00:10:43,020
그리드를 매우 간결하게 표현하는 방법입니다.

226
00:10:44,000 --> 00:10:47,205
제가 언급하지 않은 작은 기술적 세부 사항은 수치 

227
00:10:47,205 --> 00:10:50,525
안정성을 위해 이 모든 값을 해당 키 쿼리 공간에서 

228
00:10:50,525 --> 00:10:53,960
차원의 제곱근으로 나누는 것이 도움이 된다는 것입니다.

229
00:10:54,480 --> 00:10:57,991
그런 다음 전체 표현식을 감싸고 있는 이 소프트맥스는 

230
00:10:57,991 --> 00:11:00,800
열 단위로 적용하는 것으로 이해해야 합니다.

231
00:11:01,640 --> 00:11:04,700
이 V 용어에 대해서는 잠시 후에 설명하겠습니다.

232
00:11:05,020 --> 00:11:06,507
그 전에 지금까지 건너뛰었던 

233
00:11:06,507 --> 00:11:08,460
기술적 세부 사항이 하나 더 있습니다.

234
00:11:09,040 --> 00:11:12,344
훈련 과정에서 주어진 텍스트 예제에서 이 모델을 

235
00:11:12,344 --> 00:11:15,404
실행하고 모든 가중치를 약간 조정하여 구절의 

236
00:11:15,404 --> 00:11:18,831
실제 다음 단어에 얼마나 높은 확률을 할당하는지에 

237
00:11:18,831 --> 00:11:21,523
따라 보상하거나 처벌하도록 조정할 때, 

238
00:11:21,523 --> 00:11:24,583
이 구절의 각 초기 토큰 다음에 가능한 모든 

239
00:11:24,583 --> 00:11:27,765
다음 토큰을 동시에 예측하도록 하면 전체 훈련 

240
00:11:27,765 --> 00:11:30,703
과정이 훨씬 더 효율적으로 수행되는 것으로 

241
00:11:30,703 --> 00:11:31,560
밝혀졌습니다.

242
00:11:31,940 --> 00:11:34,552
예를 들어, 우리가 집중하고 있는 문구의 경우, 

243
00:11:34,552 --> 00:11:36,874
어떤 단어가 크리처 뒤에 오고 어떤 단어가 

244
00:11:36,874 --> 00:11:39,100
크리처 뒤에 오는지 예측할 수도 있습니다.

245
00:11:39,940 --> 00:11:41,724
이는 하나의 교육 예시가 여러 개의 

246
00:11:41,724 --> 00:11:43,508
교육 예시처럼 효과적으로 작동한다는 

247
00:11:43,508 --> 00:11:45,560
것을 의미하기 때문에 정말 좋은 일입니다.

248
00:11:46,100 --> 00:11:48,506
주의 집중 패턴의 목적상, 나중에 나오는 

249
00:11:48,506 --> 00:11:50,913
단어가 앞의 단어에 영향을 미치지 않도록 

250
00:11:50,913 --> 00:11:53,424
해야 하는데, 그렇지 않으면 다음에 나오는 

251
00:11:53,424 --> 00:11:56,040
단어에 대한 답을 알려줄 수 있기 때문입니다.

252
00:11:56,560 --> 00:11:59,140
이것이 의미하는 바는 여기에 있는 모든 지점, 

253
00:11:59,140 --> 00:12:02,019
즉 이전 토큰에 영향을 미치는 이후 토큰을 나타내는 

254
00:12:02,019 --> 00:12:04,600
지점이 어떻게든 0이 되기를 원한다는 것입니다.

255
00:12:05,920 --> 00:12:07,842
가장 간단하게 생각할 수 있는 방법은 

256
00:12:07,842 --> 00:12:09,856
0으로 설정하는 것이지만, 그렇게 하면 

257
00:12:09,856 --> 00:12:12,420
열이 더 이상 1이 되지 않아 정규화되지 않습니다.

258
00:12:13,120 --> 00:12:14,963
따라서 이 작업을 수행하는 일반적인 

259
00:12:14,963 --> 00:12:16,991
방법은 소프트맥스를 적용하기 전에 모든 

260
00:12:16,991 --> 00:12:19,020
항목을 음의 무한대로 설정하는 것입니다.

261
00:12:19,680 --> 00:12:22,378
이렇게 하면 소프트맥스를 적용한 후 모든 열이 

262
00:12:22,378 --> 00:12:25,180
0으로 바뀌지만 열은 정규화된 상태로 유지됩니다.

263
00:12:26,000 --> 00:12:27,540
이 프로세스를 마스킹이라고 합니다.

264
00:12:27,540 --> 00:12:30,646
이 마스킹을 적용하지 않는 주의 버전도 있지만, 

265
00:12:30,646 --> 00:12:33,177
GPT 예시에서는 챗봇 등으로 실행하는 

266
00:12:33,177 --> 00:12:36,168
것보다 트레이닝 단계에서 더 관련성이 높지만, 

267
00:12:36,168 --> 00:12:39,044
항상 이 마스킹을 적용하여 이후 토큰이 이전 

268
00:12:39,044 --> 00:12:41,460
토큰에 영향을 미치는 것을 방지합니다.

269
00:12:42,480 --> 00:12:44,666
이 주의 집중 패턴에 대해 생각해 

270
00:12:44,666 --> 00:12:46,968
볼 만한 또 다른 사실은 그 크기가 

271
00:12:46,968 --> 00:12:49,500
컨텍스트 크기의 제곱과 같다는 점입니다.

272
00:12:49,900 --> 00:12:51,965
그렇기 때문에 대규모 언어 모델에서는 컨텍스트 

273
00:12:51,965 --> 00:12:53,951
크기가 정말 큰 병목 현상이 될 수 있으며, 

274
00:12:53,951 --> 00:12:55,620
이를 확장하는 것은 간단하지 않습니다.

275
00:12:56,300 --> 00:12:59,241
더 큰 컨텍스트 창에 대한 열망에 힘입어 

276
00:12:59,241 --> 00:13:02,182
최근에는 컨텍스트의 확장성을 높이기 위해 

277
00:13:02,182 --> 00:13:05,890
주의 메커니즘에 몇 가지 변형이 이루어지고 있지만, 

278
00:13:05,890 --> 00:13:08,320
여기서는 기본에 집중하고 있습니다.

279
00:13:10,560 --> 00:13:12,890
좋아요, 이 패턴을 계산하면 모델이 어떤 단어가 

280
00:13:12,890 --> 00:13:15,480
어떤 다른 단어와 연관성이 있는지 추론할 수 있습니다.

281
00:13:16,020 --> 00:13:19,469
이제 임베딩을 실제로 업데이트하여 단어가 관련 있는 

282
00:13:19,469 --> 00:13:22,800
다른 단어에 정보를 전달할 수 있도록 해야 합니다.

283
00:13:22,800 --> 00:13:25,877
예를 들어, 플러피 임베딩이 어떻게든 크리처를 

284
00:13:25,877 --> 00:13:28,837
변경하여 12,000차원 임베딩 공간의 다른 

285
00:13:28,837 --> 00:13:31,442
부분으로 이동시켜 플러피 크리처를 보다 

286
00:13:31,442 --> 00:13:34,520
구체적으로 인코딩하고 싶다고 가정해 보겠습니다.

287
00:13:35,460 --> 00:13:37,728
여기서는 먼저 가장 간단한 방법을 

288
00:13:37,728 --> 00:13:40,116
보여드리려고 합니다만, 여러 사람이 

289
00:13:40,116 --> 00:13:43,460
주목하는 상황에서는 약간 수정되는 방법이 있습니다.

290
00:13:44,080 --> 00:13:46,251
가장 간단한 방법은 세 번째 행렬, 

291
00:13:46,251 --> 00:13:48,965
즉 값 행렬이라고 부르는 행렬을 사용하여 첫 

292
00:13:48,965 --> 00:13:51,571
번째 단어의 임베딩을 곱하는 것입니다(예: 

293
00:13:51,571 --> 00:13:52,440
Fluffy).

294
00:13:53,300 --> 00:13:55,670
그 결과는 값 벡터라고 할 수 있으며, 

295
00:13:55,670 --> 00:13:58,472
이것은 두 번째 단어의 임베딩에 추가하는 것, 

296
00:13:58,472 --> 00:14:01,381
이 경우에는 Creature의 임베딩에 추가하는 

297
00:14:01,381 --> 00:14:01,920
것입니다.

298
00:14:02,600 --> 00:14:04,910
따라서 이 값 벡터는 임베딩과 동일한 

299
00:14:04,910 --> 00:14:07,000
매우 고차원적인 공간에 존재합니다.

300
00:14:07,460 --> 00:14:10,250
이 값 행렬에 단어의 임베딩을 곱하면, 

301
00:14:10,250 --> 00:14:13,422
이 단어가 다른 단어의 의미를 조정하는 것과 

302
00:14:13,422 --> 00:14:16,720
관련이 있다면 이를 반영하기 위해 다른 단어의 

303
00:14:16,720 --> 00:14:20,272
임베딩에 정확히 무엇을 추가해야 하는가라고 생각할 

304
00:14:20,272 --> 00:14:21,160
수 있습니다.

305
00:14:22,140 --> 00:14:25,620
주의 패턴을 계산한 후에는 이 값 행렬을 

306
00:14:25,620 --> 00:14:28,948
가져와 모든 임베딩에 곱하여 일련의 값 

307
00:14:28,948 --> 00:14:32,579
벡터를 생성할 것이므로, 다이어그램을 다시 

308
00:14:32,579 --> 00:14:36,060
살펴보면 키와 쿼리는 모두 제쳐두겠습니다.

309
00:14:37,120 --> 00:14:39,120
이러한 값 벡터는 해당 키와 일종의 

310
00:14:39,120 --> 00:14:41,120
연관성이 있다고 생각할 수 있습니다.

311
00:14:42,320 --> 00:14:45,622
이 다이어그램의 각 열에 대해 각 값 

312
00:14:45,622 --> 00:14:49,240
벡터에 해당 열의 해당 가중치를 곱합니다.

313
00:14:50,080 --> 00:14:52,859
예를 들어 여기에서는 크리처를 임베드하면 

314
00:14:52,859 --> 00:14:55,638
Fluffy와 Blue에 대한 값 벡터의 

315
00:14:55,638 --> 00:14:58,538
많은 부분을 추가하고 다른 모든 값 벡터는 

316
00:14:58,538 --> 00:15:01,560
0이 되거나 적어도 거의 0에 가깝게 됩니다.

317
00:15:02,120 --> 00:15:05,034
마지막으로, 이 열과 관련된 임베딩을 실제로 

318
00:15:05,034 --> 00:15:08,532
업데이트하는 방법은 이전에 Creature의 컨텍스트 

319
00:15:08,532 --> 00:15:11,797
없는 의미를 인코딩한 후 열의 모든 재조정된 값을 

320
00:15:11,797 --> 00:15:15,179
합산하여 추가하려는 변경 사항(델타-e라고 레이블을 

321
00:15:15,179 --> 00:15:18,677
지정하겠습니다)을 생성한 다음 원래 임베딩에 추가하는 

322
00:15:18,677 --> 00:15:19,260
것입니다.

323
00:15:19,680 --> 00:15:23,216
푹신푹신한 파란색 생명체처럼 맥락이 풍부한 의미를 

324
00:15:23,216 --> 00:15:26,500
인코딩하는 더 세련된 벡터가 탄생하길 바랍니다.

325
00:15:27,380 --> 00:15:30,503
물론 하나의 임베딩에만 이 작업을 수행하는 것이 

326
00:15:30,503 --> 00:15:33,395
아니라 이 그림의 모든 열에 동일한 가중치를 

327
00:15:33,395 --> 00:15:36,750
적용하여 일련의 변경 사항을 생성하고 해당 임베딩에 

328
00:15:36,750 --> 00:15:39,873
이러한 변경 사항을 모두 추가하면 관심 블록에서 

329
00:15:39,873 --> 00:15:42,765
더 세련된 임베딩이 튀어나오는 전체 시퀀스를 

330
00:15:42,765 --> 00:15:43,460
생성합니다.

331
00:15:44,860 --> 00:15:49,100
이 모든 과정을 축소하면 한 번에 집중할 수 있습니다.

332
00:15:49,600 --> 00:15:52,453
지금까지 설명했듯이 이 프로세스는 조정 

333
00:15:52,453 --> 00:15:54,788
가능한 매개 변수인 키, 쿼리, 

334
00:15:54,788 --> 00:15:57,772
값으로 채워진 세 개의 서로 다른 행렬로 

335
00:15:57,772 --> 00:15:58,940
매개변수화됩니다.

336
00:15:59,500 --> 00:16:02,541
지난 장에서 시작한 내용을 이어서 GPT-3의 

337
00:16:02,541 --> 00:16:05,232
숫자를 사용하여 모델 파라미터의 총 수를 

338
00:16:05,232 --> 00:16:08,040
세는 점수 계산에 대해 잠시 설명하겠습니다.

339
00:16:09,300 --> 00:16:12,413
이러한 키 및 쿼리 행렬은 각각 임베딩 차원과 

340
00:16:12,413 --> 00:16:15,647
일치하는 12,288개의 열과 더 작은 키 쿼리 

341
00:16:15,647 --> 00:16:19,001
공간의 차원과 일치하는 128개의 행으로 구성되어 

342
00:16:19,001 --> 00:16:19,600
있습니다.

343
00:16:20,260 --> 00:16:22,095
이렇게 하면 각각에 대해 150만 

344
00:16:22,095 --> 00:16:24,220
개 정도의 매개변수가 추가로 제공됩니다.

345
00:16:24,860 --> 00:16:27,464
이 값 행렬을 대조적으로 보면, 

346
00:16:27,464 --> 00:16:31,226
지금까지 설명한 방식대로라면 입력과 출력 모두 

347
00:16:31,226 --> 00:16:35,421
매우 큰 임베딩 공간에 있기 때문에 12,288개의 

348
00:16:35,421 --> 00:16:39,617
열과 12,288개의 행이 있는 정사각형 행렬이라고 

349
00:16:39,617 --> 00:16:40,920
볼 수 있습니다.

350
00:16:41,500 --> 00:16:43,319
사실이라면 약 1억 5천만 개의 

351
00:16:43,319 --> 00:16:45,140
매개변수가 추가되었다는 뜻입니다.

352
00:16:45,660 --> 00:16:47,300
확실히 말씀드리자면, 그렇게 할 수 있습니다.

353
00:16:47,420 --> 00:16:49,238
키와 쿼리보다 훨씬 더 많은 

354
00:16:49,238 --> 00:16:51,740
매개변수를 값 맵에 할당할 수 있습니다.

355
00:16:52,060 --> 00:16:54,778
그러나 실제로는 이 값 맵에 할당된 

356
00:16:54,778 --> 00:16:57,497
매개변수의 수가 키와 쿼리에 할당된 

357
00:16:57,497 --> 00:17:00,760
수와 같도록 만드는 것이 훨씬 효율적입니다.

358
00:17:01,460 --> 00:17:03,310
이는 특히 여러 개의 주의 집중 헤드를 

359
00:17:03,310 --> 00:17:05,160
동시에 실행하는 설정과 관련이 있습니다.

360
00:17:06,240 --> 00:17:08,028
이 방식은 값 맵이 두 개의 작은 

361
00:17:08,028 --> 00:17:10,099
행렬의 곱으로 인수 분해되는 방식입니다.

362
00:17:11,180 --> 00:17:14,102
개념적으로는 이 더 큰 임베딩 공간에서 

363
00:17:14,102 --> 00:17:16,759
입력과 출력이 있는 전체 선형 맵, 

364
00:17:16,759 --> 00:17:19,549
예를 들어 명사에 추가할 파란색을 이 

365
00:17:19,549 --> 00:17:22,737
파란색 방향에 임베딩하는 것을 생각해 보는 

366
00:17:22,737 --> 00:17:23,800
것이 좋습니다.

367
00:17:27,040 --> 00:17:29,969
다만 일반적으로 키 쿼리 공간과 같은 

368
00:17:29,969 --> 00:17:32,760
크기의 더 적은 수의 행일 뿐입니다.

369
00:17:33,100 --> 00:17:35,770
즉, 큰 임베딩 벡터를 훨씬 작은 

370
00:17:35,770 --> 00:17:38,440
공간에 매핑한다고 생각하면 됩니다.

371
00:17:39,040 --> 00:17:40,564
이것은 일반적인 네이밍은 아니지만, 

372
00:17:40,564 --> 00:17:42,700
저는 이것을 밸류 다운 매트릭스라고 부를 것입니다.

373
00:17:43,400 --> 00:17:45,831
두 번째 매트릭스는 이 작은 공간에서 

374
00:17:45,831 --> 00:17:48,148
임베딩 공간으로 다시 매핑하여 실제 

375
00:17:48,148 --> 00:17:50,580
업데이트에 사용하는 벡터를 생성합니다.

376
00:17:51,000 --> 00:17:53,195
저는 이것을 밸류 업 매트릭스라고 부를 것인데, 

377
00:17:53,195 --> 00:17:54,740
이 역시 기존과는 다른 방식입니다.

378
00:17:55,160 --> 00:17:58,080
대부분의 논문에서 볼 수 있는 방식은 조금 다릅니다.

379
00:17:58,380 --> 00:17:59,520
잠시 후에 말씀드리겠습니다.

380
00:17:59,700 --> 00:18:00,998
제 생각에는 개념적으로 조금 

381
00:18:00,998 --> 00:18:02,540
더 혼란스러워지는 경향이 있습니다.

382
00:18:03,260 --> 00:18:06,092
여기서 선형 대수 전문 용어를 사용하자면, 

383
00:18:06,092 --> 00:18:09,159
기본적으로 전체 값 맵을 낮은 순위의 변환으로 

384
00:18:09,159 --> 00:18:10,340
제한하는 것입니다.

385
00:18:11,420 --> 00:18:14,503
매개변수 수로 돌아가서, 이 네 개의 행렬은 모두 

386
00:18:14,503 --> 00:18:17,696
크기가 같으며, 이를 모두 더하면 하나의 주의 집중 

387
00:18:17,696 --> 00:18:20,780
헤드에 대해 약 630만 개의 매개변수가 생깁니다.

388
00:18:22,040 --> 00:18:25,084
조금 더 정확하게 말하자면, 지금까지 설명한 것은 

389
00:18:25,084 --> 00:18:28,129
다른 모델에서 나타나는 크로스 어텐션이라는 변형과 

390
00:18:28,129 --> 00:18:30,956
구별하기 위해 '자기 주의력 헤드'라고 부르는 

391
00:18:30,956 --> 00:18:31,500
것입니다.

392
00:18:32,300 --> 00:18:35,562
GPT 예시와는 관련이 없지만 궁금하신 분들을 

393
00:18:35,562 --> 00:18:39,076
위해 설명하자면, 교차 주의는 한 언어의 텍스트와 

394
00:18:39,076 --> 00:18:42,589
진행 중인 번역 생성의 일부인 다른 언어의 텍스트 

395
00:18:42,589 --> 00:18:45,852
또는 음성 입력과 진행 중인 전사처럼 두 가지 

396
00:18:45,852 --> 00:18:49,240
유형의 데이터를 처리하는 모델과 관련이 있습니다.

397
00:18:50,400 --> 00:18:52,700
크로스 어텐션 헤드는 거의 동일하게 보입니다.

398
00:18:52,980 --> 00:18:55,190
유일한 차이점은 키 맵과 쿼리 맵이 서로 

399
00:18:55,190 --> 00:18:57,400
다른 데이터 세트에서 작동한다는 점입니다.

400
00:18:57,840 --> 00:19:00,502
예를 들어 번역을 수행하는 모델에서 키는 한 

401
00:19:00,502 --> 00:19:03,377
언어에서 나오는 반면 쿼리는 다른 언어에서 나올 

402
00:19:03,377 --> 00:19:06,252
수 있으며, 주의 패턴은 한 언어의 어떤 단어가 

403
00:19:06,252 --> 00:19:09,127
다른 언어의 어떤 단어에 해당하는지를 설명할 수 

404
00:19:09,127 --> 00:19:09,660
있습니다.

405
00:19:10,340 --> 00:19:12,230
그리고 이 설정에서는 일반적으로 마스킹이 

406
00:19:12,230 --> 00:19:14,203
발생하지 않는데, 이는 나중에 토큰이 이전 

407
00:19:14,203 --> 00:19:16,340
토큰에 영향을 미친다는 개념이 없기 때문입니다.

408
00:19:17,180 --> 00:19:19,700
하지만 자기 주의력에 집중하면서 지금까지 

409
00:19:19,700 --> 00:19:22,549
모든 것을 이해했다면, 그리고 여기서 멈춘다면 

410
00:19:22,549 --> 00:19:25,180
주의력의 본질이 무엇인지 알게 될 것입니다.

411
00:19:25,760 --> 00:19:28,600
이제 우리에게 남은 것은 이 작업을 여러 번 

412
00:19:28,600 --> 00:19:31,440
다양하게 수행하는 감각을 배치하는 것뿐입니다.

413
00:19:32,100 --> 00:19:34,632
중심 예제에서는 명사를 업데이트하는 형용사에 

414
00:19:34,632 --> 00:19:37,267
초점을 맞추었지만, 물론 문맥이 단어의 의미에 

415
00:19:37,267 --> 00:19:39,800
영향을 미칠 수 있는 방법은 매우 다양합니다.

416
00:19:40,360 --> 00:19:43,311
충돌한 단어가 자동차라는 단어 앞에 오면 

417
00:19:43,311 --> 00:19:46,520
해당 자동차의 모양과 구조에 영향을 미칩니다.

418
00:19:47,200 --> 00:19:49,280
그리고 많은 연관어가 문법적이지 않을 수도 있습니다.

419
00:19:49,760 --> 00:19:52,768
마법사라는 단어가 해리와 같은 구절에 있다면 

420
00:19:52,768 --> 00:19:55,415
해리 포터를 가리키는 것일 수 있지만, 

421
00:19:55,415 --> 00:19:58,303
대신 여왕, 서 섹스, 윌리엄이라는 단어가 

422
00:19:58,303 --> 00:20:01,311
해당 구절에 있다면 해리라는 임베딩을 왕자를 

423
00:20:01,311 --> 00:20:04,440
가리키는 것으로 업데이트해야 할 수도 있습니다.

424
00:20:05,040 --> 00:20:08,476
상상할 수 있는 모든 다양한 유형의 문맥 업데이트에 

425
00:20:08,476 --> 00:20:11,793
대해 다양한 관심 패턴을 포착하기 위해 이러한 키 

426
00:20:11,793 --> 00:20:14,282
및 쿼리 행렬의 매개변수가 달라지고, 

427
00:20:14,282 --> 00:20:17,362
임베딩에 추가해야 하는 항목에 따라 가치 맵의 

428
00:20:17,362 --> 00:20:19,140
매개변수도 달라질 것입니다.

429
00:20:19,980 --> 00:20:22,715
그리고 실제로 이러한 맵의 실제 동작은 해석하기가 

430
00:20:22,715 --> 00:20:25,450
훨씬 더 어려운데, 가중치는 다음 토큰을 예측하는 

431
00:20:25,450 --> 00:20:27,893
목표를 가장 잘 달성하기 위해 모델에 필요한 

432
00:20:27,893 --> 00:20:30,140
모든 작업을 수행하도록 설정되어 있습니다.

433
00:20:31,400 --> 00:20:34,375
앞서 말했듯이, 앞서 설명한 모든 것은 단일 

434
00:20:34,375 --> 00:20:37,350
주의 블록이며, 트랜스포머 내부의 전체 주의 

435
00:20:37,350 --> 00:20:40,683
블록은 멀티 헤드 주의라고 하는 것으로 구성되며, 

436
00:20:40,683 --> 00:20:43,420
이러한 많은 작업을 병렬로 실행하고 각각 

437
00:20:43,420 --> 00:20:45,920
고유한 키 쿼리와 값 맵을 사용합니다.

438
00:20:47,420 --> 00:20:49,346
예를 들어 GPT-3는 각 블록 

439
00:20:49,346 --> 00:20:51,700
내부에 96개의 주의 헤드를 사용합니다.

440
00:20:52,020 --> 00:20:54,410
하나하나가 이미 약간 혼란스럽다는 점을 고려하면, 

441
00:20:54,410 --> 00:20:56,460
머릿속에 담아두기에는 확실히 많은 양입니다.

442
00:20:56,760 --> 00:20:59,592
간단히 설명하자면, 96개의 서로 다른 

443
00:20:59,592 --> 00:21:02,167
키와 쿼리 매트릭스가 96개의 서로 

444
00:21:02,167 --> 00:21:05,000
다른 관심도 패턴을 생성한다는 뜻입니다.

445
00:21:05,440 --> 00:21:08,810
그런 다음 각 헤드는 96개의 값 벡터 시퀀스를 

446
00:21:08,810 --> 00:21:12,180
생성하는 데 사용되는 고유한 값 행렬을 갖습니다.

447
00:21:12,460 --> 00:21:14,633
이는 모두 해당 관심도 패턴을 

448
00:21:14,633 --> 00:21:16,680
가중치로 사용하여 합산됩니다.

449
00:21:17,480 --> 00:21:20,750
즉, 컨텍스트의 각 위치, 각 토큰에 대해 

450
00:21:20,750 --> 00:21:23,885
이러한 모든 헤드가 해당 위치의 임베딩에 

451
00:21:23,885 --> 00:21:27,020
추가할 변경 제안을 생성한다는 의미입니다.

452
00:21:27,660 --> 00:21:29,960
따라서 제안된 모든 변경 사항을 각 

453
00:21:29,960 --> 00:21:32,260
헤드에 대해 하나씩 합산한 다음 그 

454
00:21:32,260 --> 00:21:35,480
결과를 해당 위치의 원래 임베딩에 추가하면 됩니다.

455
00:21:36,660 --> 00:21:40,166
이 전체 합계는 이 다중 헤드 주의 블록에서 

456
00:21:40,166 --> 00:21:43,813
출력되는 것의 한 조각, 즉 그 반대쪽 끝에서 

457
00:21:43,813 --> 00:21:47,460
튀어나오는 세련된 임베딩 중 하나에 해당합니다.

458
00:21:48,320 --> 00:21:50,268
다시 말하지만, 생각할 것이 많으니 적응하는 

459
00:21:50,268 --> 00:21:52,140
데 시간이 걸리더라도 전혀 걱정하지 마세요.

460
00:21:52,380 --> 00:21:54,765
전체적인 아이디어는 여러 개의 서로 다른 

461
00:21:54,765 --> 00:21:56,736
헤드를 병렬로 실행함으로써 모델에 

462
00:21:56,736 --> 00:21:59,019
컨텍스트에 따라 의미가 달라지는 다양한 

463
00:21:59,019 --> 00:22:01,820
방식을 학습할 수 있는 능력을 부여하는 것입니다.

464
00:22:03,700 --> 00:22:07,493
이 네 가지 행렬의 각 변형을 포함하여 96개의 

465
00:22:07,493 --> 00:22:10,303
헤드로 매개변수 수를 집계한 결과, 

466
00:22:10,303 --> 00:22:14,377
각 멀티헤드 관심 블록에는 약 6억 개의 매개변수가 

467
00:22:14,377 --> 00:22:15,080
있습니다.

468
00:22:16,420 --> 00:22:18,865
트랜스포머에 대해 더 자세히 알아보고자 하는 

469
00:22:18,865 --> 00:22:21,800
분들을 위해 한 가지 더 언급하고 싶은 것이 있습니다.

470
00:22:22,080 --> 00:22:24,729
값 맵이 이 두 개의 서로 다른 행렬로 나뉘며, 

471
00:22:24,729 --> 00:22:27,182
제가 값 다운 행렬과 값 업 행렬로 레이블을 

472
00:22:27,182 --> 00:22:29,440
붙였다고 말씀드린 것을 기억하실 것입니다.

473
00:22:29,960 --> 00:22:32,744
제가 구성한 방식은 각 주의 집중 헤드 

474
00:22:32,744 --> 00:22:36,161
안에 이 한 쌍의 행렬이 있다고 생각하면 되고, 

475
00:22:36,161 --> 00:22:38,440
이런 식으로 구현할 수 있습니다.

476
00:22:38,640 --> 00:22:39,920
이는 유효한 디자인이 될 수 있습니다.

477
00:22:40,260 --> 00:22:42,275
하지만 논문에서 보는 방식과 

478
00:22:42,275 --> 00:22:44,920
실제로 구현되는 방식은 조금 다릅니다.

479
00:22:45,340 --> 00:22:49,146
각 헤드에 대한 이러한 모든 값 업 행렬은 전체 멀티 

480
00:22:49,146 --> 00:22:52,953
헤드 주의 블록과 연결된 출력 행렬이라고 하는 하나의 

481
00:22:52,953 --> 00:22:56,380
거대한 행렬에 함께 스테이플 처리되어 나타납니다.

482
00:22:56,820 --> 00:22:59,237
그리고 사람들이 주어진 주의 집중 헤드에 대한 

483
00:22:59,237 --> 00:23:01,282
가치 매트릭스를 언급하는 것을 볼 때, 

484
00:23:01,282 --> 00:23:02,863
일반적으로 이 첫 번째 단계, 

485
00:23:02,863 --> 00:23:05,466
즉 제가 작은 공간에 가치 하향 투영이라고 표시한 

486
00:23:05,466 --> 00:23:07,140
단계만 언급하는 경우가 많습니다.

487
00:23:08,340 --> 00:23:11,040
궁금하신 분들을 위해 화면에 메모를 남겨두었습니다.

488
00:23:11,260 --> 00:23:13,753
주요 개념에서 벗어날 위험이 있는 세부 사항 

489
00:23:13,753 --> 00:23:16,046
중 하나이지만, 다른 출처에서 이에 대해 

490
00:23:16,046 --> 00:23:18,540
읽은 경우 알 수 있도록 언급하고 싶었습니다.

491
00:23:19,240 --> 00:23:21,019
모든 기술적 뉘앙스를 제쳐두고, 

492
00:23:21,019 --> 00:23:23,689
지난 장의 미리보기에서 트랜스포머를 통해 흐르는 

493
00:23:23,689 --> 00:23:26,457
데이터가 단순히 하나의 주의 블록을 통과하는 것이 

494
00:23:26,457 --> 00:23:28,040
아니라는 점을 살펴보았습니다.

495
00:23:28,640 --> 00:23:30,908
우선, 멀티 레이어 퍼셉트론이라고 

496
00:23:30,908 --> 00:23:32,700
하는 다른 작업도 거칩니다.

497
00:23:33,120 --> 00:23:34,880
다음 장에서 이에 대해 자세히 설명하겠습니다.

498
00:23:35,180 --> 00:23:37,583
그리고 이 두 가지 작업을 여러 

499
00:23:37,583 --> 00:23:39,320
번 반복해서 수행합니다.

500
00:23:39,980 --> 00:23:43,180
즉, 특정 단어가 문맥을 일부 흡수한 

501
00:23:43,180 --> 00:23:46,229
후에는 더 미묘한 주변 환경에 의해 

502
00:23:46,229 --> 00:23:50,040
영향을 받을 가능성이 더 많아진다는 뜻입니다.

503
00:23:50,940 --> 00:23:53,565
네트워크 아래로 내려갈수록 각 임베딩이 다른 

504
00:23:53,565 --> 00:23:56,400
모든 임베딩에서 점점 더 많은 의미를 가져오고, 

505
00:23:56,400 --> 00:23:59,235
그 자체도 점점 더 미묘한 차이를 갖게 되면서, 

506
00:23:59,235 --> 00:24:02,070
단순한 설명자와 문법 구조를 넘어 주어진 입력에 

507
00:24:02,070 --> 00:24:05,010
대해 더 높은 수준의 추상적인 아이디어를 인코딩할 

508
00:24:05,010 --> 00:24:07,320
수 있게 될 것이라는 희망이 생겼습니다.

509
00:24:07,880 --> 00:24:10,205
정서와 어조, 시인지 아닌지, 

510
00:24:10,205 --> 00:24:13,762
작품과 관련된 과학적 진실이 무엇인지 등 여러 

511
00:24:13,762 --> 00:24:15,130
가지를 고려합니다.

512
00:24:16,700 --> 00:24:19,548
점수 관리로 다시 한 번 돌아가서, 

513
00:24:19,548 --> 00:24:23,392
GPT-3에는 96개의 고유한 레이어가 포함되어 

514
00:24:23,392 --> 00:24:26,810
있으므로 총 키 쿼리 및 값 매개변수 수에 

515
00:24:26,810 --> 00:24:30,370
96개를 곱하면 총 합계는 모든 관심 헤드에 

516
00:24:30,370 --> 00:24:34,500
할당된 580억 개 미만의 고유한 매개변수가 됩니다.

517
00:24:34,980 --> 00:24:38,070
확실히 많은 양이지만, 이는 전체 네트워크에 있는 

518
00:24:38,070 --> 00:24:40,940
1,750억 개 중 약 3분의 1에 불과합니다.

519
00:24:41,520 --> 00:24:44,539
따라서 모든 관심이 집중되기는 하지만 대부분의 

520
00:24:44,539 --> 00:24:47,443
매개변수는 이러한 단계 사이에 있는 블록에서 

521
00:24:47,443 --> 00:24:48,140
발생합니다.

522
00:24:48,560 --> 00:24:51,060
다음 장에서는 이러한 다른 블록과 교육 

523
00:24:51,060 --> 00:24:53,560
과정에 대해 더 자세히 이야기하겠습니다.

524
00:24:54,120 --> 00:24:56,857
주의 메커니즘이 성공할 수 있었던 가장 큰 

525
00:24:56,857 --> 00:24:59,595
이유는 이 메커니즘이 구현하는 특정 종류의 

526
00:24:59,595 --> 00:25:02,333
동작이 아니라, 이 메커니즘이 매우 병렬화 

527
00:25:02,333 --> 00:25:05,299
가능하기 때문에 GPU를 사용하여 짧은 시간에 

528
00:25:05,299 --> 00:25:08,380
엄청난 수의 연산을 실행할 수 있다는 사실입니다.

529
00:25:09,460 --> 00:25:12,244
지난 10~20년 동안 딥 러닝에 대한 큰 

530
00:25:12,244 --> 00:25:15,260
교훈 중 하나는 규모만으로도 모델 성능이 크게 

531
00:25:15,260 --> 00:25:18,160
향상된다는 것이었으므로, 이를 가능하게 하는 

532
00:25:18,160 --> 00:25:21,060
병렬화 가능한 아키텍처는 큰 이점이 있습니다.

533
00:25:22,040 --> 00:25:23,598
이에 대해 자세히 알아보시려면 

534
00:25:23,598 --> 00:25:25,340
설명에 많은 링크를 남겨두었습니다.

535
00:25:25,920 --> 00:25:27,890
특히 안드레이 카르파시나 크리스 올라가 

536
00:25:27,890 --> 00:25:30,040
제작한 모든 제품은 순금인 경우가 많습니다.

537
00:25:30,560 --> 00:25:33,087
이 영상에서는 현재의 모습에 주목하고 싶었지만, 

538
00:25:33,087 --> 00:25:34,771
우리가 어떻게 여기까지 왔는지, 

539
00:25:34,771 --> 00:25:37,111
이 아이디어를 어떻게 재창조할 수 있는지 더 

540
00:25:37,111 --> 00:25:39,451
많은 역사가 궁금하다면 제 친구 Vivek이 

541
00:25:39,451 --> 00:25:41,978
방금 더 많은 동기를 부여하는 몇 가지 동영상을 

542
00:25:41,978 --> 00:25:42,540
올렸습니다.

543
00:25:43,120 --> 00:25:44,996
또한 The Art of the Problem 

544
00:25:44,996 --> 00:25:46,655
채널의 Britt Cruz는 대규모 언어 

545
00:25:46,655 --> 00:25:48,460
모델의 역사에 대한 멋진 동영상을 제공합니다.

546
00:26:04,960 --> 00:26:09,200
감사합니다.

