1
00:00:00,000 --> 00:00:02,009
In the last chapter, you and I started to step 

2
00:00:02,009 --> 00:00:04,019
through the internal workings of a transformer.

3
00:00:04,560 --> 00:00:07,925
This is one of the key pieces of technology inside large language models, 

4
00:00:07,925 --> 00:00:10,200
and a lot of other tools in the modern wave of AI.

5
00:00:10,980 --> 00:00:15,574
It first hit the scene in a now-famous 2017 paper called Attention is All You Need, 

6
00:00:15,574 --> 00:00:19,840
and in this chapter you and I will dig into what this attention mechanism is, 

7
00:00:19,840 --> 00:00:21,700
visualizing how it processes data.

8
00:00:26,140 --> 00:00:29,540
As a quick recap, here's the important context I want you to have in mind.

9
00:00:30,000 --> 00:00:33,003
The goal of the model that you and I are studying is to 

10
00:00:33,003 --> 00:00:36,060
take in a piece of text and predict what word comes next.

11
00:00:36,860 --> 00:00:40,388
The input text is broken up into little pieces that we call tokens, 

12
00:00:40,388 --> 00:00:43,035
and these are very often words or pieces of words, 

13
00:00:43,035 --> 00:00:47,290
but just to make the examples in this video easier for you and me to think about, 

14
00:00:47,290 --> 00:00:50,560
let's simplify by pretending that tokens are always just words.

15
00:00:51,480 --> 00:00:54,590
The first step in a transformer is to associate each token 

16
00:00:54,590 --> 00:00:57,700
with a high-dimensional vector, what we call its embedding.

17
00:00:57,700 --> 00:01:02,066
The most important idea I want you to have in mind is how directions in this 

18
00:01:02,066 --> 00:01:07,000
high-dimensional space of all possible embeddings can correspond with semantic meaning.

19
00:01:07,680 --> 00:01:11,766
In the last chapter we saw an example for how direction can correspond to gender, 

20
00:01:11,766 --> 00:01:15,553
in the sense that adding a certain step in this space can take you from the 

21
00:01:15,553 --> 00:01:19,640
embedding of a masculine noun to the embedding of the corresponding feminine noun.

22
00:01:20,160 --> 00:01:23,640
That's just one example you could imagine how many other directions in this 

23
00:01:23,640 --> 00:01:27,580
high-dimensional space could correspond to numerous other aspects of a word's meaning.

24
00:01:28,800 --> 00:01:31,985
The aim of a transformer is to progressively adjust these 

25
00:01:31,985 --> 00:01:35,500
embeddings so that they don't merely encode an individual word, 

26
00:01:35,500 --> 00:01:39,180
but instead they bake in some much, much richer contextual meaning.

27
00:01:40,140 --> 00:01:43,705
I should say up front that a lot of people find the attention mechanism, 

28
00:01:43,705 --> 00:01:46,098
this key piece in a transformer, very confusing, 

29
00:01:46,098 --> 00:01:48,980
so don't worry if it takes some time for things to sink in.

30
00:01:49,440 --> 00:01:52,548
I think that before we dive into the computational details and 

31
00:01:52,548 --> 00:01:55,854
all the matrix multiplications, it's worth thinking about a couple 

32
00:01:55,854 --> 00:01:59,160
examples for the kind of behavior that we want attention to enable.

33
00:02:00,140 --> 00:02:04,377
Consider the phrases American true mole, one mole of carbon dioxide, 

34
00:02:04,377 --> 00:02:06,220
and take a biopsy of the mole.

35
00:02:06,700 --> 00:02:10,018
You and I know that the word mole has different meanings in each one of these, 

36
00:02:10,018 --> 00:02:10,900
based on the context.

37
00:02:11,360 --> 00:02:15,125
But after the first step of a transformer, the one that breaks up the text 

38
00:02:15,125 --> 00:02:18,890
and associates each token with a vector, the vector that's associated with 

39
00:02:18,890 --> 00:02:22,555
mole would be the same in all of these cases, because this initial token 

40
00:02:22,555 --> 00:02:26,220
embedding is effectively a lookup table with no reference to the context.

41
00:02:26,620 --> 00:02:30,011
It's only in the next step of the transformer that the surrounding 

42
00:02:30,011 --> 00:02:33,100
embeddings have the chance to pass information into this one.

43
00:02:33,820 --> 00:02:38,341
The picture you might have in mind is that there are multiple distinct directions in 

44
00:02:38,341 --> 00:02:42,544
this embedding space encoding the multiple distinct meanings of the word mole, 

45
00:02:42,544 --> 00:02:47,172
and that a well-trained attention block calculates what you need to add to the generic 

46
00:02:47,172 --> 00:02:51,800
embedding to move it to one of these specific directions, as a function of the context.

47
00:02:53,300 --> 00:02:56,180
To take another example, consider the embedding of the word tower.

48
00:02:57,060 --> 00:03:01,120
This is presumably some very generic, non-specific direction in the space, 

49
00:03:01,120 --> 00:03:03,720
associated with lots of other large, tall nouns.

50
00:03:04,020 --> 00:03:06,642
If this word was immediately preceded by Eiffel, 

51
00:03:06,642 --> 00:03:10,389
you could imagine wanting the mechanism to update this vector so that 

52
00:03:10,389 --> 00:03:14,349
it points in a direction that more specifically encodes the Eiffel tower, 

53
00:03:14,349 --> 00:03:19,060
maybe correlated with vectors associated with Paris and France and things made of steel.

54
00:03:19,920 --> 00:03:22,279
If it was also preceded by the word miniature, 

55
00:03:22,279 --> 00:03:24,688
then the vector should be updated even further, 

56
00:03:24,688 --> 00:03:27,500
so that it no longer correlates with large, tall things.

57
00:03:29,480 --> 00:03:32,323
More generally than just refining the meaning of a word, 

58
00:03:32,323 --> 00:03:35,716
the attention block allows the model to move information encoded in 

59
00:03:35,716 --> 00:03:39,508
one embedding to that of another, potentially ones that are quite far away, 

60
00:03:39,508 --> 00:03:43,300
and potentially with information that's much richer than just a single word.

61
00:03:43,300 --> 00:03:47,988
What we saw in the last chapter was how after all of the vectors flow through the 

62
00:03:47,988 --> 00:03:50,961
network, including many different attention blocks, 

63
00:03:50,961 --> 00:03:55,764
the computation you perform to produce a prediction of the next token is entirely a 

64
00:03:55,764 --> 00:03:58,280
function of the last vector in the sequence.

65
00:03:59,100 --> 00:04:03,503
Imagine, for example, that the text you input is most of an entire mystery novel, 

66
00:04:03,503 --> 00:04:07,800
all the way up to a point near the end, which reads, therefore the murderer was.

67
00:04:08,400 --> 00:04:11,510
If the model is going to accurately predict the next word, 

68
00:04:11,510 --> 00:04:16,096
that final vector in the sequence, which began its life simply embedding the word was, 

69
00:04:16,096 --> 00:04:20,365
will have to have been updated by all of the attention blocks to represent much, 

70
00:04:20,365 --> 00:04:24,371
much more than any individual word, somehow encoding all of the information 

71
00:04:24,371 --> 00:04:28,220
from the full context window that's relevant to predicting the next word.

72
00:04:29,500 --> 00:04:32,580
To step through the computations, though, let's take a much simpler example.

73
00:04:32,980 --> 00:04:35,443
Imagine that the input includes the phrase, a 

74
00:04:35,443 --> 00:04:37,960
fluffy blue creature roamed the verdant forest.

75
00:04:38,460 --> 00:04:42,675
And for the moment, suppose that the only type of update that we care about 

76
00:04:42,675 --> 00:04:46,780
is having the adjectives adjust the meanings of their corresponding nouns.

77
00:04:47,000 --> 00:04:50,769
What I'm about to describe is what we would call a single head of attention, 

78
00:04:50,769 --> 00:04:54,979
and later we will see how the attention block consists of many different heads run in 

79
00:04:54,979 --> 00:04:55,420
parallel.

80
00:04:56,140 --> 00:04:59,884
Again, the initial embedding for each word is some high dimensional vector 

81
00:04:59,884 --> 00:05:03,380
that only encodes the meaning of that particular word with no context.

82
00:05:04,000 --> 00:05:05,220
Actually, that's not quite true.

83
00:05:05,380 --> 00:05:07,640
They also encode the position of the word.

84
00:05:07,980 --> 00:05:11,794
There's a lot more to say way that positions are encoded, but right now, 

85
00:05:11,794 --> 00:05:15,451
all you need to know is that the entries of this vector are enough to 

86
00:05:15,451 --> 00:05:18,900
tell you both what the word is and where it exists in the context.

87
00:05:19,500 --> 00:05:21,660
Let's go ahead and denote these embeddings with the letter e.

88
00:05:22,420 --> 00:05:26,105
The goal is to have a series of computations produce a new refined 

89
00:05:26,105 --> 00:05:29,680
set of embeddings where, for example, those corresponding to the 

90
00:05:29,680 --> 00:05:33,420
nouns have ingested the meaning from their corresponding adjectives.

91
00:05:33,900 --> 00:05:37,212
And playing the deep learning game, we want most of the computations 

92
00:05:37,212 --> 00:05:40,524
involved to look like matrix-vector products, where the matrices are 

93
00:05:40,524 --> 00:05:43,980
full of tunable weights, things that the model will learn based on data.

94
00:05:44,660 --> 00:05:48,411
To be clear, I'm making up this example of adjectives updating nouns just to 

95
00:05:48,411 --> 00:05:52,260
illustrate the type of behavior that you could imagine an attention head doing.

96
00:05:52,860 --> 00:05:57,051
As with so much deep learning, the true behavior is much harder to parse because it's 

97
00:05:57,051 --> 00:06:01,340
based on tweaking and tuning a huge number of parameters to minimize some cost function.

98
00:06:01,680 --> 00:06:05,605
It's just that as we step through all of different matrices filled with parameters 

99
00:06:05,605 --> 00:06:09,530
that are involved in this process, I think it's really helpful to have an imagined 

100
00:06:09,530 --> 00:06:13,220
example of something that it could be doing to help keep it all more concrete.

101
00:06:14,140 --> 00:06:18,202
For the first step of this process, you might imagine each noun, like creature, 

102
00:06:18,202 --> 00:06:21,960
asking the question, hey, are there any adjectives sitting in front of me?

103
00:06:22,160 --> 00:06:25,429
And for the words fluffy and blue, to each be able to answer, 

104
00:06:25,429 --> 00:06:27,960
yeah, I'm an adjective and I'm in that position.

105
00:06:28,960 --> 00:06:32,320
That question is somehow encoded as yet another vector, 

106
00:06:32,320 --> 00:06:36,100
another list of numbers, which we call the query for this word.

107
00:06:36,980 --> 00:06:42,020
This query vector though has a much smaller dimension than the embedding vector, say 128.

108
00:06:42,940 --> 00:06:46,360
Computing this query looks like taking a certain matrix, 

109
00:06:46,360 --> 00:06:49,780
which I'll label wq, and multiplying it by the embedding.

110
00:06:50,960 --> 00:06:54,222
Compressing things a bit, let's write that query vector as q, 

111
00:06:54,222 --> 00:06:58,064
and then anytime you see me put a matrix next to an arrow like this one, 

112
00:06:58,064 --> 00:07:02,695
it's meant to represent that multiplying this matrix by the vector at the arrow's start 

113
00:07:02,695 --> 00:07:04,800
gives you the vector at the arrow's end.

114
00:07:05,860 --> 00:07:10,266
In this case, you multiply this matrix by all of the embeddings in the context, 

115
00:07:10,266 --> 00:07:12,580
producing one query vector for each token.

116
00:07:13,740 --> 00:07:16,429
The entries of this matrix are parameters of the model, 

117
00:07:16,429 --> 00:07:19,742
which means the true behavior is learned from data, and in practice, 

118
00:07:19,742 --> 00:07:23,440
what this matrix does in a particular attention head is challenging to parse.

119
00:07:23,900 --> 00:07:27,947
But for our sake, imagining an example that we might hope that it would learn, 

120
00:07:27,947 --> 00:07:31,482
we'll suppose that this query matrix maps the embeddings of nouns to 

121
00:07:31,482 --> 00:07:34,966
certain directions in this smaller query space that somehow encodes 

122
00:07:34,966 --> 00:07:38,040
the notion of looking for adjectives in preceding positions.

123
00:07:38,780 --> 00:07:41,440
As to what it does to other embeddings, who knows?

124
00:07:41,720 --> 00:07:44,340
Maybe it simultaneously tries to accomplish some other goal with those.

125
00:07:44,540 --> 00:07:47,160
Right now, we're laser focused on the nouns.

126
00:07:47,280 --> 00:07:51,651
At the same time, associated with this is a second matrix called the key matrix, 

127
00:07:51,651 --> 00:07:54,620
which you also multiply by every one of the embeddings.

128
00:07:55,280 --> 00:07:58,500
This produces a second sequence of vectors that we call the keys.

129
00:07:59,420 --> 00:08:03,140
Conceptually, you want to think of the keys as potentially answering the queries.

130
00:08:03,840 --> 00:08:07,990
This key matrix is also full of tunable parameters, and just like the query matrix, 

131
00:08:07,990 --> 00:08:11,400
it maps the embedding vectors to that same smaller dimensional space.

132
00:08:12,200 --> 00:08:17,020
You think of the keys as matching the queries whenever they closely align with each other.

133
00:08:17,460 --> 00:08:21,994
In our example, you would imagine that the key matrix maps the adjectives like fluffy 

134
00:08:21,994 --> 00:08:26,740
and blue to vectors that are closely aligned with the query produced by the word creature.

135
00:08:27,200 --> 00:08:30,175
To measure how well each key matches each query, 

136
00:08:30,175 --> 00:08:34,000
you compute a dot product between each possible key-query pair.

137
00:08:34,480 --> 00:08:37,156
I like to visualize a grid full of a bunch of dots, 

138
00:08:37,156 --> 00:08:40,295
where the bigger dots correspond to the larger dot products, 

139
00:08:40,295 --> 00:08:42,559
the places where the keys and queries align.

140
00:08:43,280 --> 00:08:47,535
For our adjective noun example, that would look a little more like this, 

141
00:08:47,535 --> 00:08:52,490
where if the keys produced by fluffy and blue really do align closely with the query 

142
00:08:52,490 --> 00:08:57,328
produced by creature, then the dot products in these two spots would be some large 

143
00:08:57,328 --> 00:08:58,320
positive numbers.

144
00:08:59,100 --> 00:09:02,307
In the lingo, machine learning people would say that this means the 

145
00:09:02,307 --> 00:09:05,420
embeddings of fluffy and blue attend to the embedding of creature.

146
00:09:06,040 --> 00:09:09,522
By contrast to the dot product between the key for some other 

147
00:09:09,522 --> 00:09:12,948
word like the and the query for creature would be some small 

148
00:09:12,948 --> 00:09:16,600
or negative value that reflects that are unrelated to each other.

149
00:09:17,700 --> 00:09:21,389
So we have this grid of values that can be any real number from 

150
00:09:21,389 --> 00:09:25,194
negative infinity to infinity, giving us a score for how relevant 

151
00:09:25,194 --> 00:09:28,480
each word is to updating the meaning of every other word.

152
00:09:29,200 --> 00:09:32,572
The way we're about to use these scores is to take a certain 

153
00:09:32,572 --> 00:09:35,780
weighted sum along each column, weighted by the relevance.

154
00:09:36,520 --> 00:09:40,213
So instead of having values range from negative infinity to infinity, 

155
00:09:40,213 --> 00:09:44,011
what we want is for the numbers in these columns to be between 0 and 1, 

156
00:09:44,011 --> 00:09:48,180
and for each column to add up to 1, as if they were a probability distribution.

157
00:09:49,280 --> 00:09:52,220
If you're coming in from the last chapter, you know what we need to do then.

158
00:09:52,620 --> 00:09:57,300
We compute a softmax along each one of these columns to normalize the values.

159
00:10:00,060 --> 00:10:03,237
In our picture, after you apply softmax to all of the columns, 

160
00:10:03,237 --> 00:10:05,860
we'll fill in the grid with these normalized values.

161
00:10:06,780 --> 00:10:10,753
At this point you're safe to think about each column as giving weights according 

162
00:10:10,753 --> 00:10:14,580
to how relevant the word on the left is to the corresponding value at the top.

163
00:10:15,080 --> 00:10:16,840
We call this grid an attention pattern.

164
00:10:18,080 --> 00:10:20,277
Now if you look at the original transformer paper, 

165
00:10:20,277 --> 00:10:22,820
there's a really compact way that they write this all down.

166
00:10:23,880 --> 00:10:27,486
Here the variables q and k represent the full arrays of query 

167
00:10:27,486 --> 00:10:31,092
and key vectors respectively, those little vectors you get by 

168
00:10:31,092 --> 00:10:34,640
multiplying the embeddings by the query and the key matrices.

169
00:10:35,160 --> 00:10:39,117
This expression up in the numerator is a really compact way to represent 

170
00:10:39,117 --> 00:10:43,020
the grid of all possible dot products between pairs of keys and queries.

171
00:10:44,000 --> 00:10:48,086
A small technical detail that I didn't mention is that for numerical stability, 

172
00:10:48,086 --> 00:10:51,252
it happens to be helpful to divide all of these values by the 

173
00:10:51,252 --> 00:10:53,960
square root of the dimension in that key query space.

174
00:10:54,480 --> 00:10:57,865
Then this softmax that's wrapped around the full expression 

175
00:10:57,865 --> 00:11:00,800
is meant to be understood to apply column by column.

176
00:11:01,640 --> 00:11:04,700
As to that v term, we'll talk about it in just a second.

177
00:11:05,020 --> 00:11:08,460
Before that, there's one other technical detail that so far I've skipped.

178
00:11:09,040 --> 00:11:13,050
During the training process, when you run this model on a given text example, 

179
00:11:13,050 --> 00:11:17,420
and all of the weights are slightly adjusted and tuned to either reward or punish it 

180
00:11:17,420 --> 00:11:21,585
based on how high a probability it assigns to the true next word in the passage, 

181
00:11:21,585 --> 00:11:25,492
it turns out to make the whole training process a lot more efficient if you 

182
00:11:25,492 --> 00:11:29,606
simultaneously have it predict every possible next token following each initial 

183
00:11:29,606 --> 00:11:31,560
subsequence of tokens in this passage.

184
00:11:31,940 --> 00:11:34,927
For example, with the phrase that we've been focusing on, 

185
00:11:34,927 --> 00:11:39,100
it might also be predicting what words follow creature and what words follow the.

186
00:11:39,940 --> 00:11:42,874
This is really nice, because it means what would otherwise 

187
00:11:42,874 --> 00:11:45,560
be a single training example effectively acts as many.

188
00:11:46,100 --> 00:11:49,480
For the purposes of our attention pattern, it means that you never 

189
00:11:49,480 --> 00:11:52,205
want to allow later words to influence earlier words, 

190
00:11:52,205 --> 00:11:56,040
since otherwise they could kind of give away the answer for what comes next.

191
00:11:56,560 --> 00:11:59,615
What this means is that we want all of these spots here, 

192
00:11:59,615 --> 00:12:02,884
the ones representing later tokens influencing earlier ones, 

193
00:12:02,884 --> 00:12:04,600
to somehow be forced to be zero.

194
00:12:05,920 --> 00:12:08,751
The simplest thing you might think to do is to set them equal to zero, 

195
00:12:08,751 --> 00:12:11,303
but if you did that the columns wouldn't add up to one anymore, 

196
00:12:11,303 --> 00:12:12,420
they wouldn't be normalized.

197
00:12:13,120 --> 00:12:16,456
So instead, a common way to do this is that before applying softmax, 

198
00:12:16,456 --> 00:12:19,020
you set all of those entries to be negative infinity.

199
00:12:19,680 --> 00:12:23,608
If you do that, then after applying softmax, all of those get turned into zero, 

200
00:12:23,608 --> 00:12:25,180
but the columns stay normalized.

201
00:12:26,000 --> 00:12:27,540
This process is called masking.

202
00:12:27,540 --> 00:12:31,344
There are versions of attention where you don't apply it, but in our GPT example, 

203
00:12:31,344 --> 00:12:34,964
even though this is more relevant during the training phase than it would be, 

204
00:12:34,964 --> 00:12:37,423
say, running it as a chatbot or something like that, 

205
00:12:37,423 --> 00:12:41,460
you do always apply this masking to prevent later tokens from influencing earlier ones.

206
00:12:42,480 --> 00:12:45,825
Another fact that's worth reflecting on about this attention 

207
00:12:45,825 --> 00:12:49,500
pattern is how its size is equal to the square of the context size.

208
00:12:49,900 --> 00:12:54,047
So this is why context size can be a really huge bottleneck for large language models, 

209
00:12:54,047 --> 00:12:55,620
and scaling it up is non-trivial.

210
00:12:56,300 --> 00:13:00,124
As you imagine, motivated by a desire for bigger and bigger context windows, 

211
00:13:00,124 --> 00:13:04,197
recent years have seen some variations to the attention mechanism aimed at making 

212
00:13:04,197 --> 00:13:08,320
context more scalable, but right here, you and I are staying focused on the basics.

213
00:13:10,560 --> 00:13:12,972
Okay, great, computing this pattern lets the model 

214
00:13:12,972 --> 00:13:15,480
deduce which words are relevant to which other words.

215
00:13:16,020 --> 00:13:18,562
Now you need to actually update the embeddings, 

216
00:13:18,562 --> 00:13:22,800
allowing words to pass information to whichever other words they're relevant to.

217
00:13:22,800 --> 00:13:26,818
For example, you want the embedding of Fluffy to somehow cause a change 

218
00:13:26,818 --> 00:13:30,892
to Creature that moves it to a different part of this 12,000-dimensional 

219
00:13:30,892 --> 00:13:34,520
embedding space that more specifically encodes a Fluffy creature.

220
00:13:35,460 --> 00:13:38,365
What I'm going to do here is first show you the most straightforward 

221
00:13:38,365 --> 00:13:40,933
way that you could do this, though there's a slight way that 

222
00:13:40,933 --> 00:13:43,460
this gets modified in the context of multi-headed attention.

223
00:13:44,080 --> 00:13:47,165
This most straightforward way would be to use a third matrix, 

224
00:13:47,165 --> 00:13:51,494
what we call the value matrix, which you multiply by the embedding of that first word, 

225
00:13:51,494 --> 00:13:52,440
for example Fluffy.

226
00:13:53,300 --> 00:13:55,931
The result of this is what you would call a value vector, 

227
00:13:55,931 --> 00:13:59,197
and this is something that you add to the embedding of the second word, 

228
00:13:59,197 --> 00:14:01,920
in this case something you add to the embedding of Creature.

229
00:14:02,600 --> 00:14:07,000
So this value vector lives in the same very high-dimensional space as the embeddings.

230
00:14:07,460 --> 00:14:10,832
When you multiply this value matrix by the embedding of a word, 

231
00:14:10,832 --> 00:14:15,363
you might think of it as saying, if this word is relevant to adjusting the meaning of 

232
00:14:15,363 --> 00:14:19,842
something else, what exactly should be added to the embedding of that something else 

233
00:14:19,842 --> 00:14:21,160
in order to reflect this?

234
00:14:22,140 --> 00:14:26,017
Looking back in our diagram, let's set aside all of the keys and the queries, 

235
00:14:26,017 --> 00:14:29,497
since after you compute the attention pattern you're done with those, 

236
00:14:29,497 --> 00:14:32,928
then you're going to take this value matrix and multiply it by every 

237
00:14:32,928 --> 00:14:36,060
one of those embeddings to produce a sequence of value vectors.

238
00:14:37,120 --> 00:14:39,141
You might think of these value vectors as being 

239
00:14:39,141 --> 00:14:41,120
kind of associated with the corresponding keys.

240
00:14:42,320 --> 00:14:45,810
For each column in this diagram, you multiply each of the 

241
00:14:45,810 --> 00:14:49,240
value vectors by the corresponding weight in that column.

242
00:14:50,080 --> 00:14:52,815
For example here, under the embedding of Creature, 

243
00:14:52,815 --> 00:14:57,107
you would be adding large proportions of the value vectors for Fluffy and Blue, 

244
00:14:57,107 --> 00:15:01,560
while all of the other value vectors get zeroed out, or at least nearly zeroed out.

245
00:15:02,120 --> 00:15:06,804
And then finally, the way to actually update the embedding associated with this column, 

246
00:15:06,804 --> 00:15:09,944
previously encoding some context-free meaning of Creature, 

247
00:15:09,944 --> 00:15:13,191
you add together all of these rescaled values in the column, 

248
00:15:13,191 --> 00:15:16,704
producing a change that you want to add, that I'll label delta-e, 

249
00:15:16,704 --> 00:15:19,260
and then you add that to the original embedding.

250
00:15:19,680 --> 00:15:23,169
Hopefully what results is a more refined vector encoding the more 

251
00:15:23,169 --> 00:15:26,500
contextually rich meaning, like that of a fluffy blue creature.

252
00:15:27,380 --> 00:15:30,223
And of course you don't just do this to one embedding, 

253
00:15:30,223 --> 00:15:34,101
you apply the same weighted sum across all of the columns in this picture, 

254
00:15:34,101 --> 00:15:38,341
producing a sequence of changes, adding all of those changes to the corresponding 

255
00:15:38,341 --> 00:15:42,270
embeddings, produces a full sequence of more refined embeddings popping out 

256
00:15:42,270 --> 00:15:43,460
of the attention block.

257
00:15:44,860 --> 00:15:49,100
Zooming out, this whole process is what you would describe as a single head of attention.

258
00:15:49,600 --> 00:15:54,299
As I've described things so far, this process is parameterized by three distinct 

259
00:15:54,299 --> 00:15:58,940
matrices, all filled with tunable parameters, the key, the query, and the value.

260
00:15:59,500 --> 00:16:02,982
I want to take a moment to continue what we started in the last chapter, 

261
00:16:02,982 --> 00:16:07,133
with the scorekeeping where we count up the total number of model parameters using the 

262
00:16:07,133 --> 00:16:08,040
numbers from GPT-3.

263
00:16:09,300 --> 00:16:15,101
These key and query matrices each have 12,288 columns, matching the embedding dimension, 

264
00:16:15,101 --> 00:16:19,600
and 128 rows, matching the dimension of that smaller key query space.

265
00:16:20,260 --> 00:16:24,220
This gives us an additional 1.5 million or so parameters for each one.

266
00:16:24,860 --> 00:16:30,190
If you look at that value matrix by contrast, the way I've described things so 

267
00:16:30,190 --> 00:16:35,926
far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, 

268
00:16:35,926 --> 00:16:40,920
since both its inputs and outputs live in this very large embedding space.

269
00:16:41,500 --> 00:16:45,140
If true, that would mean about 150 million added parameters.

270
00:16:45,660 --> 00:16:47,300
And to be clear, you could do that.

271
00:16:47,420 --> 00:16:49,805
You could devote orders of magnitude more parameters 

272
00:16:49,805 --> 00:16:51,740
to the value map than to the key and query.

273
00:16:52,060 --> 00:16:55,038
But in practice, it is much more efficient if instead you make 

274
00:16:55,038 --> 00:16:57,970
it so that the number of parameters devoted to this value map 

275
00:16:57,970 --> 00:17:00,760
is the same as the number devoted to the key and the query.

276
00:17:01,460 --> 00:17:03,330
This is especially relevant in the setting of 

277
00:17:03,330 --> 00:17:05,160
running multiple attention heads in parallel.

278
00:17:06,240 --> 00:17:10,099
The way this looks is that the value map is factored as a product of two smaller matrices.

279
00:17:11,180 --> 00:17:15,386
Conceptually, I would still encourage you to think about the overall linear map, 

280
00:17:15,386 --> 00:17:18,814
one with inputs and outputs, both in this larger embedding space, 

281
00:17:18,814 --> 00:17:23,124
for example taking the embedding of blue to this blueness direction that you would 

282
00:17:23,124 --> 00:17:23,800
add to nouns.

283
00:17:27,040 --> 00:17:29,869
It's just that it's a smaller number of rows, 

284
00:17:29,869 --> 00:17:32,760
typically the same size as the key query space.

285
00:17:33,100 --> 00:17:35,794
What this means is you can think of it as mapping the 

286
00:17:35,794 --> 00:17:38,440
large embedding vectors down to a much smaller space.

287
00:17:39,040 --> 00:17:42,700
This is not the conventional naming, but I'm going to call this the value down matrix.

288
00:17:43,400 --> 00:17:47,422
The second matrix maps from this smaller space back up to the embedding space, 

289
00:17:47,422 --> 00:17:50,580
producing the vectors that you use to make the actual updates.

290
00:17:51,000 --> 00:17:54,740
I'm going to call this one the value up matrix, which again is not conventional.

291
00:17:55,160 --> 00:17:58,080
The way that you would see this written in most papers looks a little different.

292
00:17:58,380 --> 00:17:59,520
I'll talk about it in a minute.

293
00:17:59,700 --> 00:18:02,540
In my opinion, it tends to make things a little more conceptually confusing.

294
00:18:03,260 --> 00:18:06,722
To throw in linear algebra jargon here, what we're basically doing 

295
00:18:06,722 --> 00:18:10,340
is constraining the overall value map to be a low rank transformation.

296
00:18:11,420 --> 00:18:16,156
Turning back to the parameter count, all four of these matrices have the same size, 

297
00:18:16,156 --> 00:18:20,780
and adding them all up we get about 6.3 million parameters for one attention head.

298
00:18:22,040 --> 00:18:24,236
As a quick side note, to be a little more accurate, 

299
00:18:24,236 --> 00:18:27,487
everything described so far is what people would call a self-attention head, 

300
00:18:27,487 --> 00:18:30,528
to distinguish it from a variation that comes up in other models that's 

301
00:18:30,528 --> 00:18:31,500
called cross-attention.

302
00:18:32,300 --> 00:18:35,787
This isn't relevant to our GPT example, but if you're curious, 

303
00:18:35,787 --> 00:18:39,828
cross-attention involves models that process two distinct types of data, 

304
00:18:39,828 --> 00:18:43,870
like text in one language and text in another language that's part of an 

305
00:18:43,870 --> 00:18:48,022
ongoing generation of a translation, or maybe audio input of speech and an 

306
00:18:48,022 --> 00:18:49,240
ongoing transcription.

307
00:18:50,400 --> 00:18:52,700
A cross-attention head looks almost identical.

308
00:18:52,980 --> 00:18:57,400
The only difference is that the key and query maps act on different data sets.

309
00:18:57,840 --> 00:19:02,109
In a model doing translation, for example, the keys might come from one language, 

310
00:19:02,109 --> 00:19:06,171
while the queries come from another, and the attention pattern could describe 

311
00:19:06,171 --> 00:19:09,660
which words from one language correspond to which words in another.

312
00:19:10,340 --> 00:19:12,930
And in this setting there would typically be no masking, 

313
00:19:12,930 --> 00:19:16,340
since there's not really any notion of later tokens affecting earlier ones.

314
00:19:17,180 --> 00:19:20,812
Staying focused on self-attention though, if you understood everything so far, 

315
00:19:20,812 --> 00:19:24,720
and if you were to stop here, you would come away with the essence of what attention 

316
00:19:24,720 --> 00:19:25,180
really is.

317
00:19:25,760 --> 00:19:28,429
All that's really left to us is to lay out the 

318
00:19:28,429 --> 00:19:31,440
sense in which you do this many many different times.

319
00:19:32,100 --> 00:19:35,179
In our central example we focused on adjectives updating nouns, 

320
00:19:35,179 --> 00:19:38,933
but of course there are lots of different ways that context can influence the 

321
00:19:38,933 --> 00:19:39,800
meaning of a word.

322
00:19:40,360 --> 00:19:43,249
If the words they crashed the preceded the word car, 

323
00:19:43,249 --> 00:19:46,520
it has implications for the shape and structure of that car.

324
00:19:47,200 --> 00:19:49,280
And a lot of associations might be less grammatical.

325
00:19:49,760 --> 00:19:52,935
If the word wizard is anywhere in the same passage as Harry, 

326
00:19:52,935 --> 00:19:55,954
it suggests that this might be referring to Harry Potter, 

327
00:19:55,954 --> 00:20:00,015
whereas if instead the words Queen, Sussex, and William were in that passage, 

328
00:20:00,015 --> 00:20:04,440
then perhaps the embedding of Harry should instead be updated to refer to the prince.

329
00:20:05,040 --> 00:20:08,589
For every different type of contextual updating that you might imagine, 

330
00:20:08,589 --> 00:20:11,991
the parameters of these key and query matrices would be different to 

331
00:20:11,991 --> 00:20:15,343
capture the different attention patterns, and the parameters of our 

332
00:20:15,343 --> 00:20:19,140
value map would be different based on what should be added to the embeddings.

333
00:20:19,980 --> 00:20:23,163
And again, in practice the true behavior of these maps is much more 

334
00:20:23,163 --> 00:20:26,394
difficult to interpret, where the weights are set to do whatever the 

335
00:20:26,394 --> 00:20:30,140
model needs them to do to best accomplish its goal of predicting the next token.

336
00:20:31,400 --> 00:20:35,240
As I said before, everything we described is a single head of attention, 

337
00:20:35,240 --> 00:20:38,765
and a full attention block inside a transformer consists of what's 

338
00:20:38,765 --> 00:20:43,184
called multi-headed attention, where you run a lot of these operations in parallel, 

339
00:20:43,184 --> 00:20:45,920
each with its own distinct key query and value maps.

340
00:20:47,420 --> 00:20:51,700
GPT-3 for example uses 96 attention heads inside each block.

341
00:20:52,020 --> 00:20:54,517
Considering that each one is already a bit confusing, 

342
00:20:54,517 --> 00:20:56,460
it's certainly a lot to hold in your head.

343
00:20:56,760 --> 00:21:00,641
Just to spell it all out very explicitly, this means you have 96 

344
00:21:00,641 --> 00:21:05,000
distinct key and query matrices producing 96 distinct attention patterns.

345
00:21:05,440 --> 00:21:08,983
Then each head has its own distinct value matrices 

346
00:21:08,983 --> 00:21:12,180
used to produce 96 sequences of value vectors.

347
00:21:12,460 --> 00:21:16,680
These are all added together using the corresponding attention patterns as weights.

348
00:21:17,480 --> 00:21:21,455
What this means is that for each position in the context, each token, 

349
00:21:21,455 --> 00:21:26,225
every one of these heads produces a proposed change to be added to the embedding in 

350
00:21:26,225 --> 00:21:27,020
that position.

351
00:21:27,660 --> 00:21:31,078
So what you do is you sum together all of those proposed changes, 

352
00:21:31,078 --> 00:21:35,480
one for each head, and you add the result to the original embedding of that position.

353
00:21:36,660 --> 00:21:41,782
This entire sum here would be one slice of what's outputted from this multi-headed 

354
00:21:41,782 --> 00:21:47,089
attention block, a single one of those refined embeddings that pops out the other end 

355
00:21:47,089 --> 00:21:47,460
of it.

356
00:21:48,320 --> 00:21:50,230
Again, this is a lot to think about, so don't 

357
00:21:50,230 --> 00:21:52,140
worry at all if it takes some time to sink in.

358
00:21:52,380 --> 00:21:56,376
The overall idea is that by running many distinct heads in parallel, 

359
00:21:56,376 --> 00:22:00,893
you're giving the model the capacity to learn many distinct ways that context 

360
00:22:00,893 --> 00:22:01,820
changes meaning.

361
00:22:03,700 --> 00:22:07,323
Pulling up our running tally for parameter count with 96 heads, 

362
00:22:07,323 --> 00:22:10,550
each including its own variation of these four matrices, 

363
00:22:10,550 --> 00:22:15,080
each block of multi-headed attention ends up with around 600 million parameters.

364
00:22:16,420 --> 00:22:19,067
There's one added slightly annoying thing that I should really 

365
00:22:19,067 --> 00:22:21,800
mention for any of you who go on to read more about transformers.

366
00:22:22,080 --> 00:22:25,639
You remember how I said that the value map is factored out into these two 

367
00:22:25,639 --> 00:22:29,440
distinct matrices, which I labeled as the value down and the value up matrices.

368
00:22:29,960 --> 00:22:34,283
The way that I framed things would suggest that you see this pair of matrices 

369
00:22:34,283 --> 00:22:38,440
inside each attention head, and you could absolutely implement it this way.

370
00:22:38,640 --> 00:22:39,920
That would be a valid design.

371
00:22:40,260 --> 00:22:42,609
But the way that you see this written in papers and the way 

372
00:22:42,609 --> 00:22:44,920
that it's implemented in practice looks a little different.

373
00:22:45,340 --> 00:22:50,891
All of these value up matrices for each head appear stapled together in one giant matrix 

374
00:22:50,891 --> 00:22:56,380
that we call the output matrix, associated with the entire multi-headed attention block.

375
00:22:56,820 --> 00:23:00,634
And when you see people refer to the value matrix for a given attention head, 

376
00:23:00,634 --> 00:23:03,227
they're typically only referring to this first step, 

377
00:23:03,227 --> 00:23:07,140
the one that I was labeling as the value down projection into the smaller space.

378
00:23:08,340 --> 00:23:11,040
For the curious among you, I've left an on-screen note about it.

379
00:23:11,260 --> 00:23:13,633
It's one of those details that runs the risk of distracting 

380
00:23:13,633 --> 00:23:16,086
from the main conceptual points, but I do want to call it out 

381
00:23:16,086 --> 00:23:18,540
just so that you know if you read about this in other sources.

382
00:23:19,240 --> 00:23:23,514
Setting aside all the technical nuances, in the preview from the last chapter we saw 

383
00:23:23,514 --> 00:23:28,040
how data flowing through a transformer doesn't just flow through a single attention block.

384
00:23:28,640 --> 00:23:32,700
For one thing, it also goes through these other operations called multi-layer perceptrons.

385
00:23:33,120 --> 00:23:34,880
We'll talk more about those in the next chapter.

386
00:23:35,180 --> 00:23:39,320
And then it repeatedly goes through many many copies of both of these operations.

387
00:23:39,980 --> 00:23:43,959
What this means is that after a given word imbibes some of its context, 

388
00:23:43,959 --> 00:23:47,276
there are many more chances for this more nuanced embedding 

389
00:23:47,276 --> 00:23:50,040
to be influenced by its more nuanced surroundings.

390
00:23:50,940 --> 00:23:54,997
The further down the network you go, with each embedding taking in more and more 

391
00:23:54,997 --> 00:23:59,104
meaning from all the other embeddings, which themselves are getting more and more 

392
00:23:59,104 --> 00:24:03,062
nuanced, the hope is that there's the capacity to encode higher level and more 

393
00:24:03,062 --> 00:24:07,320
abstract ideas about a given input beyond just descriptors and grammatical structure.

394
00:24:07,880 --> 00:24:11,763
Things like sentiment and tone and whether it's a poem and what underlying 

395
00:24:11,763 --> 00:24:15,130
scientific truths are relevant to the piece and things like that.

396
00:24:16,700 --> 00:24:22,052
Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, 

397
00:24:22,052 --> 00:24:27,405
so the total number of key query and value parameters is multiplied by another 96, 

398
00:24:27,405 --> 00:24:32,049
which brings the total sum to just under 58 billion distinct parameters 

399
00:24:32,049 --> 00:24:34,500
devoted to all of the attention heads.

400
00:24:34,980 --> 00:24:38,016
That is a lot to be sure, but it's only about a third 

401
00:24:38,016 --> 00:24:40,940
of the 175 billion that are in the network in total.

402
00:24:41,520 --> 00:24:44,147
So even though attention gets all of the attention, 

403
00:24:44,147 --> 00:24:48,140
the majority of parameters come from the blocks sitting in between these steps.

404
00:24:48,560 --> 00:24:51,017
In the next chapter, you and I will talk more about those 

405
00:24:51,017 --> 00:24:53,560
other blocks and also a lot more about the training process.

406
00:24:54,120 --> 00:24:58,836
A big part of the story for the success of the attention mechanism is not so much any 

407
00:24:58,836 --> 00:25:03,005
specific kind of behavior that it enables, but the fact that it's extremely 

408
00:25:03,005 --> 00:25:07,776
parallelizable, meaning that you can run a huge number of computations in a short time 

409
00:25:07,776 --> 00:25:08,380
using GPUs.

410
00:25:09,460 --> 00:25:13,357
Given that one of the big lessons about deep learning in the last decade or two has 

411
00:25:13,357 --> 00:25:17,440
been that scale alone seems to give huge qualitative improvements in model performance, 

412
00:25:17,440 --> 00:25:21,060
there's a huge advantage to parallelizable architectures that let you do this.

413
00:25:22,040 --> 00:25:25,340
If you want to learn more about this stuff, I've left lots of links in the description.

414
00:25:25,920 --> 00:25:30,040
In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.

415
00:25:30,560 --> 00:25:33,763
In this video, I wanted to just jump into attention in its current form, 

416
00:25:33,763 --> 00:25:36,747
but if you're curious about more of the history for how we got here 

417
00:25:36,747 --> 00:25:38,985
and how you might reinvent this idea for yourself, 

418
00:25:38,985 --> 00:25:42,540
my friend Vivek just put up a couple videos giving a lot more of that motivation.

419
00:25:43,120 --> 00:25:45,746
Also, Britt Cruz from the channel The Art of the Problem has 

420
00:25:45,746 --> 00:25:48,460
a really nice video about the history of large language models.

421
00:26:04,960 --> 00:26:09,200
Thank you.

