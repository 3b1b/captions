[
  {
    "input": " In the last chapter, you and I started to step",
    "translatedText": "在上一章中，我们开始探讨",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 0,
    "end": 2.009
  },
  {
    "input": " through the internal workings of a transformer.",
    "translatedText": "Transformer 的内部运作机制。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 2.009,
    "end": 4.019
  },
  {
    "input": " This is one of the key pieces of technology inside large language models,",
    "translatedText": "Transformer 是大语言模型中关键的技术组成部分，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 4.5600000000000005,
    "end": 7.925
  },
  {
    "input": " and a lot of other tools in the modern wave of AI.",
    "translatedText": "也被广泛应用于现代 AI 领域的诸多工具中。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 7.925,
    "end": 10.6398046875
  },
  {
    "input": " It first hit the scene in a now-famous 2017 paper called Attention is All You Need,",
    "translatedText": "它首次亮相是在 2017 年一篇广为人知的论文《Attention is All You Need》中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 10.98,
    "end": 15.574
  },
  {
    "input": " and in this chapter you and I will dig into what this attention mechanism is,",
    "translatedText": "本章我们将深入探讨这种注意力机制，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 15.574,
    "end": 19.84
  },
  {
    "input": " visualizing how it processes data.",
    "translatedText": "以及可视化展示它如何处理数据。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 19.84,
    "end": 21.7
  },
  {
    "input": " As a quick recap, here's the important context I want you to have in mind.",
    "translatedText": "在此，我想快速回顾一些重要的背景信息。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 26.14,
    "end": 29.54
  },
  {
    "input": " The goal of the model that you and I are studying is to",
    "translatedText": "我们正在研究的模型的目标是",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 30,
    "end": 33.003
  },
  {
    "input": " take in a piece of text and predict what word comes next.",
    "translatedText": "读取一段文本并预测下一个词。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 33.003,
    "end": 36.06
  },
  {
    "input": " The input text is broken up into little pieces that we call tokens,",
    "translatedText": "输入文本被分割成我们称之为 Tokens 的小部分，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 36.86,
    "end": 40.388
  },
  {
    "input": " and these are very often words or pieces of words,",
    "translatedText": "它们通常是完整的单词或单词的一部分。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 40.388,
    "end": 43.035
  },
  {
    "input": " but just to make the examples in this video easier for you and me to think about,",
    "translatedText": "但为了让我们在这个视频中的例子更加简单易懂，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 43.035,
    "end": 47.29
  },
  {
    "input": " let's simplify by pretending that tokens are always just words.",
    "translatedText": "让我们假设 Token 总是完整的单词。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 47.29,
    "end": 50.56
  },
  {
    "input": " The first step in a transformer is to associate each token",
    "translatedText": "Transformer 的第一步是将每个 Token",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 51.48,
    "end": 54.59
  },
  {
    "input": " with a high-dimensional vector, what we call its embedding.",
    "translatedText": "与一个高维向量关联，这就是我们所说的嵌入向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 54.59,
    "end": 57.7
  },
  {
    "input": " The most important idea I want you to have in mind is ",
    "translatedText": "我希望你能理解的关键概念是，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 57.7,
    "end": 60.4813515625
  },
  {
    "input": " how directions in this high-dimensional space of all possible embeddings ",
    "translatedText": "如何理解在所有可能的嵌入向量所构成的高维空间中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 60.481742187500004,
    "end": 64.77774609375
  },
  {
    "input": " can correspond with semantic meaning.",
    "translatedText": "不同的方向能够代表不同的语义含义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 64.77774609375,
    "end": 67
  },
  {
    "input": " In the last chapter we saw an example for how direction can correspond to gender,",
    "translatedText": "在上一章中，我们给出了一个例子，说明了方向如何对应性别，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 67.68,
    "end": 71.766
  },
  {
    "input": " in the sense that adding a certain step in this space can take you from the",
    "translatedText": "即在这个空间中添加一定的变化可以从",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 71.766,
    "end": 75.553
  },
  {
    "input": " embedding of a masculine noun to the embedding of the corresponding feminine noun.",
    "translatedText": "一个男性名词的嵌入转到对应的女性名词的嵌入。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 75.553,
    "end": 79.64
  },
  {
    "input": " That's just one example you could imagine ",
    "translatedText": "这只是一个例子，你可以设想，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 79.6998046875,
    "end": 81.6390625
  },
  {
    "input": " how many other directions in this high-dimensional space ",
    "translatedText": "在这样一个复杂的空间中，有无数的方向，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 81.6390625,
    "end": 84.60953125
  },
  {
    "input": " could correspond to numerous other aspects of a word's meaning.",
    "translatedText": "每一个都可能代表着词义的不同方面。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 84.60953125,
    "end": 87.58
  },
  {
    "input": " The aim of a transformer is to progressively adjust these embeddings ",
    "translatedText": "Transformer 的目标是逐步调整这些嵌入，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 88.3224609375,
    "end": 92.586171875
  },
  {
    "input": " so that they don't merely encode an individual word,",
    "translatedText": "使之不仅仅编码单词本身，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 92.6187890625,
    "end": 95.5
  },
  {
    "input": " but instead they bake in some much, much richer contextual meaning.",
    "translatedText": "而是包含更丰富、更深层次的上下文含义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 95.5,
    "end": 99.62875
  },
  {
    "input": " I should say up front that a lot of people find the attention mechanism,",
    "translatedText": "需要提前说明的是，很多人对于注意力机制",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 99.8880078125,
    "end": 103.705
  },
  {
    "input": " this key piece in a transformer, very confusing,",
    "translatedText": "—— Transformer 的关键部分，感到非常困惑，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 103.705,
    "end": 106.098
  },
  {
    "input": " so don't worry if it takes some time for things to sink in.",
    "translatedText": "因此，请不要着急，需要一些时间来消化理解。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 106.098,
    "end": 108.98
  },
  {
    "input": " I think that before we dive into the computational details and all the matrix multiplications,",
    "translatedText": "在我们深入到计算细节和矩阵运算之前，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 109.44,
    "end": 113.883703125
  },
  {
    "input": " it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
    "translatedText": "有必要先了解一些我们期望注意力机制能实现的行为示例。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 113.88378125,
    "end": 119.16
  },
  {
    "input": " Consider the phrases American true mole, one mole of carbon dioxide,",
    "translatedText": "考虑一下这几个短语：美国真鼹鼠（mole）、一摩尔（mole）二氧化碳，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 120.14,
    "end": 124.377
  },
  {
    "input": " and take a biopsy of the mole.",
    "translatedText": "对肿瘤（mole）进行活检。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 124.377,
    "end": 126.22
  },
  {
    "input": " You and I know that ",
    "translatedText": "我们都明白，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 126.394921875,
    "end": 127.39819531250001
  },
  {
    "input": " the word mole has different meanings in each one of these, based on the context.",
    "translatedText": "在不同的上下文环境下，“mole”这个词会有不同的意思。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 127.39819531250001,
    "end": 130.9
  },
  {
    "input": " But after the first step of a transformer, ",
    "translatedText": "然而，在 Transformer 的第一步中，文本被拆分，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 131.36,
    "end": 133.185546875
  },
  {
    "input": " the one that breaks up the text and associates each token with a vector, ",
    "translatedText": "每个 Token 都被关联到一个向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 133.185546875,
    "end": 136.72449218749998
  },
  {
    "input": " the vector that's associated with mole would be the same in all of these cases, ",
    "translatedText": "这时，“mole”这个词对应的向量在所有情况下都是相同的，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 136.72449218749998,
    "end": 141.1905078125
  },
  {
    "input": " because this initial token embedding ",
    "translatedText": "因为初始的 Token 嵌入向量",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 141.2219140625,
    "end": 143.1466796875
  },
  {
    "input": " is effectively a lookup table with no reference to the context.",
    "translatedText": "本质上是一个不参考上下文的查找表。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 143.1467578125,
    "end": 146.22
  },
  {
    "input": " It's only in the next step of the transformer ",
    "translatedText": "直到 Transformer 的下一步，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 146.62,
    "end": 149.50451562499998
  },
  {
    "input": " that the surrounding embeddings have the chance to pass information into this one.",
    "translatedText": "周围的嵌入才有机会向这个 Token 传递信息。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 149.50451562499998,
    "end": 153.1
  },
  {
    "input": " The picture you might have in mind is that there are multiple distinct directions in",
    "translatedText": "你可以想象，嵌入空间里有多个不同的方向，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 153.82,
    "end": 158.341
  },
  {
    "input": " this embedding space encoding the multiple distinct meanings of the word mole,",
    "translatedText": "这些方向分别编码了“mole”这个词的多种不同含义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 158.341,
    "end": 161.37071875
  },
  {
    "input": " and that a well-trained attention block calculates ",
    "translatedText": "如果 Token 经过了良好的训练，注意力模块就可以计算出",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 161.37071875,
    "end": 164.1386015625
  },
  {
    "input": " what you need to add to the generic embedding ",
    "translatedText": "需要根据上下文在通用嵌入向量中添加什么内容，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 164.143875,
    "end": 166.84533593749998
  },
  {
    "input": " to move it to one of these specific directions, as a function of the context.",
    "translatedText": "使其指向其中一个特定的方向。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 166.89232812499998,
    "end": 171.8
  },
  {
    "input": " To take another example, consider the embedding of the word tower.",
    "translatedText": "再来看一个例子，比如单词“Tower”的嵌入向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 172.9983984375,
    "end": 176.43519531250004
  },
  {
    "input": " This is presumably some very generic, non-specific direction in the space,",
    "translatedText": "这可能是个非常通用、不特定的方向，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 177.06,
    "end": 181.12
  },
  {
    "input": " associated with lots of other large, tall nouns.",
    "translatedText": "与许多大型、高大的名词关联。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 181.12,
    "end": 183.72
  },
  {
    "input": " If this word was immediately preceded by Eiffel,",
    "translatedText": "如果“Tower”前面是“埃菲尔”，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 184.02,
    "end": 186.642
  },
  {
    "input": " you could imagine wanting the mechanism to update this vector so that",
    "translatedText": "你可能希望更新这个向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 186.642,
    "end": 190.389
  },
  {
    "input": " it points in a direction that more specifically encodes the Eiffel tower,",
    "translatedText": "使其更具体地指向埃菲尔铁塔的方向，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 190.389,
    "end": 194.349
  },
  {
    "input": " maybe correlated with vectors associated with Paris and France and things made of steel.",
    "translatedText": "可能与巴黎、法国或者钢铁制品相关的向量有关。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 194.349,
    "end": 199.06
  },
  {
    "input": " If it was also preceded by the word miniature,",
    "translatedText": "如果前面还有“微型”这个词，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 199.92,
    "end": 202.279
  },
  {
    "input": " then the vector should be updated even further,",
    "translatedText": "那么这个向量应该进一步更新，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 202.279,
    "end": 204.688
  },
  {
    "input": " so that it no longer correlates with large, tall things.",
    "translatedText": "使其不再与大型、高大的事物相关。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 204.688,
    "end": 207.5
  },
  {
    "input": " More generally than just refining the meaning of a word,",
    "translatedText": "更进一步讲，注意力模块不仅可以精确一个词的含义，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 209.48,
    "end": 212.323
  },
  {
    "input": " the attention block allows the model to move information encoded in",
    "translatedText": "还能将一个嵌入向量中的信息传递到另一个嵌入向量中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 212.32299999999998,
    "end": 217.710765625
  },
  {
    "input": " potentially ones that are quite far away,",
    "translatedText": "即使这两个嵌入向量相距很远，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 217.69767968750003,
    "end": 219.91663281250007
  },
  {
    "input": " and potentially with information that's much richer than just a single word.",
    "translatedText": "信息也可能比单一单词要丰富得多。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 219.93280468749998,
    "end": 223.3
  },
  {
    "input": " What we saw in the last chapter was how after all of the vectors flow through the network, ",
    "translatedText": "如我们在上一章中看到的，所有向量通过网络流动，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 223.3,
    "end": 227.988
  },
  {
    "input": " including many different attention blocks,",
    "translatedText": "包括经过许多不同的注意力模块后，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 227.988,
    "end": 230.961
  },
  {
    "input": " the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
    "translatedText": "预测下一个 Token 的计算过程完全取决于序列中的最后一个向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 230.961,
    "end": 238.28
  },
  {
    "input": " Imagine, for example, that the text you input is most of an entire mystery novel,",
    "translatedText": "例如，你输入的文字是一整部悬疑小说，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 239.1,
    "end": 243.503
  },
  {
    "input": " all the way up to a point near the end, which reads, therefore the murderer was.",
    "translatedText": "到了接近尾声的部分，写着\"所以，凶手是\"。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 243.503,
    "end": 247.8
  },
  {
    "input": " If the model is going to accurately predict the next word,",
    "translatedText": "如果模型要准确地预测下一个词，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 248.4,
    "end": 251.51
  },
  {
    "input": " that final vector in the sequence, which began its life simply embedding the word was,",
    "translatedText": "那么这个序列中的最后一个向量，它最初只是嵌入了单词\"是\"，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 251.51,
    "end": 256.096
  },
  {
    "input": " will have to have been updated by all of the attention blocks ",
    "translatedText": "它必须经过所有的注意力模块的更新，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 256.096,
    "end": 259.101640625
  },
  {
    "input": " to represent much, much more than any individual word, ",
    "translatedText": "以包含远超过任何单个单词的信息，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 259.101640625,
    "end": 262.16092187499993
  },
  {
    "input": " somehow encoding all of the information from the full context window ",
    "translatedText": "通过某种方式编码了所有来自完整的上下文窗口中",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 262.16092187499993,
    "end": 266.24839062499996
  },
  {
    "input": " that's relevant to predicting the next word.",
    "translatedText": "与预测下一个词相关的信息。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 266.24839062499996,
    "end": 268.22
  },
  {
    "input": " To step through the computations, though, let's take a much simpler example.",
    "translatedText": "但为了逐步解析计算过程，我们先看一个更简单的例子。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 269.5,
    "end": 272.58
  },
  {
    "input": " Imagine that the input includes the phrase, ",
    "translatedText": "假设输入包含了一个句子，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 272.98,
    "end": 275.0695234375
  },
  {
    "input": " a fluffy blue creature roamed the verdant forest.",
    "translatedText": "\"一个蓬松的蓝色生物在葱郁的森林中游荡\"。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 275.07467968749995,
    "end": 277.96
  },
  {
    "input": " And for the moment, suppose that the only type of update that we care about",
    "translatedText": "假设我们此刻关注的只是",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 278.46,
    "end": 282.1803515625
  },
  {
    "input": " is having the adjectives adjust the meanings of their corresponding nouns.",
    "translatedText": "让形容词调整其对应名词的含义的这种更新方式。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 282.18035156249994,
    "end": 286.78
  },
  {
    "input": " What I'm about to describe is what we would call a single head of attention,",
    "translatedText": "我马上要讲的是我们通常所说的单个注意力分支，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 287,
    "end": 290.769
  },
  {
    "input": " and later we will see ",
    "translatedText": "稍后我们会看到",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 290.769,
    "end": 291.85274999999996
  },
  {
    "input": " how the attention block consists of many different heads run in parallel.",
    "translatedText": "一个注意力模块是由许多不同的分支并行运行组成的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 291.83833593749995,
    "end": 295.42
  },
  {
    "input": " Again, the initial embedding for each word is some high dimensional vector",
    "translatedText": "需要强调的是，每个词的初始嵌入是一个高维向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 296.14,
    "end": 299.884
  },
  {
    "input": " that only encodes the meaning of that particular word with no context.",
    "translatedText": "只编码了该特定词的含义，不包含任何上下文。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 299.884,
    "end": 303.38
  },
  {
    "input": " Actually, that's not quite true.",
    "translatedText": "实际上，这并不完全正确。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 303.682890625,
    "end": 305.38
  },
  {
    "input": " They also encode the position of the word.",
    "translatedText": "它们还编码了词的位置。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 305.38,
    "end": 307.64
  },
  {
    "input": " There's a lot more to say way that positions are encoded, ",
    "translatedText": "关于位置如何被编码的细节有很多，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 307.98,
    "end": 311.3684140625
  },
  {
    "input": " but right now, all you need to know ",
    "translatedText": "但你现在只需要知道，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 311.367515625,
    "end": 313.1596328125001
  },
  {
    "input": " is that the entries of this vector are enough to tell you both what the word is ",
    "translatedText": "这个向量的条目足以告诉你这个词是什么，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 313.1591640625001,
    "end": 316.84589062500004
  },
  {
    "input": " and where it exists in the context.",
    "translatedText": "以及它在上下文中的位置。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 316.846359375,
    "end": 318.9
  },
  {
    "input": " Let's go ahead and denote these embeddings with the letter e.",
    "translatedText": "让我们用字母 E 来表示这些嵌入。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 319.1767578125,
    "end": 322.10167968750005
  },
  {
    "input": " The goal is to have a series of computations ",
    "translatedText": "我们的目标是，通过一系列计算，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 322.1719140625,
    "end": 324.5960546875
  },
  {
    "input": " produce a new refined set of embeddings, ",
    "translatedText": "产生一组新的、更为精细的嵌入向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 324.5960546875,
    "end": 327.0201953125
  },
  {
    "input": " where for example, those corresponding to the nouns ",
    "translatedText": "比如说，这样做可以让名词的嵌入向量",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 327.0207421875,
    "end": 329.68
  },
  {
    "input": " have ingested the meaning from their corresponding adjectives.",
    "translatedText": "捕捉并融合了与它们相对应的形容词的含义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 329.68,
    "end": 333.42
  },
  {
    "input": " And playing the deep learning game,",
    "translatedText": "而在深度学习的过程中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 333.5330859375,
    "end": 335.41481250000004
  },
  {
    "input": " we want most of the  involved to look like matrix-vector products, ",
    "translatedText": "我们希望大部分的计算都像矩阵 - 向量的乘积，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 335.41481250000004,
    "end": 339.3216171875
  },
  {
    "input": " where the matrices are full of tunable weights, ",
    "translatedText": "其中的矩阵充满了可调的权重，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 339.3216171875,
    "end": 341.8611796875001
  },
  {
    "input": " things that the model will learn based on data.",
    "translatedText": "模型将根据数据来学习这些权重。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 341.8627421875001,
    "end": 344.46015625000007
  },
  {
    "input": " To be clear, I'm making up this example of adjectives updating nouns ",
    "translatedText": "需要明确的是，我构造这个形容词调整名词的例子，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 344.66,
    "end": 348.2127578125
  },
  {
    "input": " just to illustrate the type of behavior that you could imagine an attention head doing.",
    "translatedText": "只是为了说明你可以设想一个注意力分支可能做的事情。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 348.2565859375,
    "end": 352.26
  },
  {
    "input": " As with so much deep learning, the true behavior is much harder to parse ",
    "translatedText": "正如深度学习常见的情况，真实的行为更为复杂，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 352.86,
    "end": 355.94053125
  },
  {
    "input": " because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
    "translatedText": "因为它涉及到调整和微调海量参数以最小化某种成本函数。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 355.94053125,
    "end": 361.3399999999999
  },
  {
    "input": " It's just that as we step through all of different matrices filled with parameters that are involved in this process, ",
    "translatedText": "逐一审视这一过程中涉及的各种参数矩阵时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 361.68,
    "end": 367.33484375
  },
  {
    "input": " I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
    "translatedText": "设想一个具体的应用场景能帮助我们更好地理解其背后的逻辑。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 367.3315234375,
    "end": 373.22
  },
  {
    "input": " For the first step of this process, you might imagine each noun, like creature,",
    "translatedText": "在这个过程的第一步，你可以想象每个名词，比如\"生物\"，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 374.14,
    "end": 378.202
  },
  {
    "input": " asking the question, hey, are there any adjectives sitting in front of me?",
    "translatedText": "都在问，有没有形容词在我前面？",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 378.202,
    "end": 381.96
  },
  {
    "input": " And for the words fluffy and blue, to each be able to answer,",
    "translatedText": "对于\"毛茸茸的\"和\"蓝色的\"这两个词，都会回答，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 382.16,
    "end": 385.429
  },
  {
    "input": " yeah, I'm an adjective and I'm in that position.",
    "translatedText": "对，我是一个形容词，我就在那个位置。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 385.429,
    "end": 387.96
  },
  {
    "input": " That question is somehow encoded as yet another vector,",
    "translatedText": "这个问题会被编码成另一个向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 388.96,
    "end": 392.32
  },
  {
    "input": " another list of numbers, which we call the query for this word.",
    "translatedText": "也就是一组数字，我们称之为这个词的查询向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 392.32,
    "end": 396.1
  },
  {
    "input": " This query vector though has a much smaller dimension than the embedding vector, say 128.",
    "translatedText": "这个查询向量的维度比嵌入向量要小得多，比如说 128 维。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 396.98,
    "end": 402.02
  },
  {
    "input": " Computing this query looks like taking a certain matrix,",
    "translatedText": "计算这个查询就是取一个特定的矩阵，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 402.94,
    "end": 406.36
  },
  {
    "input": " which I'll label wq, and multiplying it by the embedding.",
    "translatedText": "也就是 WQ，并将其与嵌入向量相乘。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 406.36,
    "end": 409.78
  },
  {
    "input": " Compressing things a bit, let's write that query vector as q,",
    "translatedText": "简化一下，我们把查询向量记作 Q，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 410.96,
    "end": 414.222
  },
  {
    "input": " and then anytime you see me put a matrix next to an arrow like this one,",
    "translatedText": "然后你看到我把一个矩阵放在一个箭头旁边，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 414.222,
    "end": 418.064
  },
  {
    "input": " it's meant to represent that multiplying this matrix by the vector at the arrow's start",
    "translatedText": "就表示通过这个矩阵与箭头起点的向量相乘，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 418.064,
    "end": 422.695
  },
  {
    "input": " gives you the vector at the arrow's end.",
    "translatedText": "可以得到箭头终点的向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 422.695,
    "end": 424.8
  },
  {
    "input": " In this case, you multiply this matrix by all of the embeddings in the context,",
    "translatedText": "在这种情况下，你将这个矩阵与上下文中的所有嵌入向量相乘，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 425.86,
    "end": 430.266
  },
  {
    "input": " producing one query vector for each token.",
    "translatedText": "得到的是每个 Token 对应的一个查询向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 430.266,
    "end": 432.58
  },
  {
    "input": " The entries of this matrix are parameters of the model,",
    "translatedText": "这个矩阵由模型的参数组成，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 433.74,
    "end": 436.429
  },
  {
    "input": " which means the true behavior is learned from data, ",
    "translatedText": "意味着它能从数据中学习到真实的行为模式。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 436.4289999999999,
    "end": 438.98442187499995
  },
  {
    "input": " and in practice, ",
    "translatedText": "但在实际应用中， ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 438.98442187499995,
    "end": 439.76213281250006
  },
  {
    "input": " what this matrix does in a particular attention head is challenging to parse.",
    "translatedText": "要明确这个矩阵在特定注意力机制中的具体作用是相当复杂的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 439.77775781250006,
    "end": 443.44
  },
  {
    "input": " But for our sake, imagining an example that we might hope that it would learn,",
    "translatedText": "不过，让我们尝试想象一个理想情况：",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 443.9,
    "end": 447.947
  },
  {
    "input": " we'll suppose that this query matrix maps the embeddings of nouns to",
    "translatedText": "我们希望这个查询矩阵能将名词的嵌入信息映射到",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 447.94699999999995,
    "end": 451.05797656249996
  },
  {
    "input": " certain directions in this smaller query space ",
    "translatedText": "一个较小的空间中的特定方向上，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 451.05797656249996,
    "end": 453.6965468750001
  },
  {
    "input": " that somehow encodes the notion of looking for adjectives in preceding positions.",
    "translatedText": "这种映射方式能够捕捉到一种特殊的寻找前置形容词的规律。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 453.6965468750001,
    "end": 458.04
  },
  {
    "input": " As to what it does to other embeddings, who knows?",
    "translatedText": "至于它对其他嵌入向量做什么，我们不知道。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 458.78,
    "end": 461.44
  },
  {
    "input": " Maybe it simultaneously tries to accomplish some other goal with those.",
    "translatedText": "也许它同时试图用这些实现一些其他的目标。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 461.72,
    "end": 464.34
  },
  {
    "input": " Right now, we're laser focused on the nouns.",
    "translatedText": "现在，我们的注意力集中在名词上。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 464.54,
    "end": 467.16
  },
  {
    "input": " At the same time, associated with this is a second matrix called the key matrix,",
    "translatedText": "同时，这里还有另一个我们称之为键矩阵的矩阵，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 467.28,
    "end": 471.651
  },
  {
    "input": " which you also multiply by every one of the embeddings.",
    "translatedText": "同样需要与所有嵌入向量相乘。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 471.651,
    "end": 474.62
  },
  {
    "input": " This produces a second sequence of vectors that we call the keys.",
    "translatedText": "这会生成一个我们称之为键的向量序列。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 475.28,
    "end": 478.5
  },
  {
    "input": " Conceptually, you want to think of the keys as potentially answering the queries.",
    "translatedText": "从概念上讲，你可以把键想象成是潜在的查询回答者。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 479.42,
    "end": 483.14
  },
  {
    "input": " This key matrix is also full of tunable parameters, and just like the query matrix,",
    "translatedText": "这个键矩阵也充满了可调整的参数，同查询矩阵一样，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 483.84,
    "end": 487.99
  },
  {
    "input": " it maps the embedding vectors to that same smaller dimensional space.",
    "translatedText": "将嵌入向量映射到同一个较小的维度空间中。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 487.99,
    "end": 491.4
  },
  {
    "input": " You think of the keys as matching the queries whenever they closely align with each other.",
    "translatedText": "当键与查询密切对齐时，你可以将键视为与查询相匹配。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 492.2,
    "end": 497.02
  },
  {
    "input": " In our example, you would imagine that the key matrix maps the adjectives like fluffy",
    "translatedText": "以我们的例子来说，你可以想象键矩阵将形容词\"毛茸茸的\"和\"蓝色的\"",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 497.46,
    "end": 501.994
  },
  {
    "input": " and blue to vectors that are closely aligned with the query produced by the word creature.",
    "translatedText": "映射到与单词\"生物\"生成的查询紧密对齐的向量上。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 501.994,
    "end": 506.74
  },
  {
    "input": " To measure how well each key matches each query,",
    "translatedText": "为了衡量每个键与每个查询的匹配程度，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 507.2,
    "end": 510.175
  },
  {
    "input": " you compute a dot product between each possible key-query pair.",
    "translatedText": "你需要计算每一对可能的键 - 查询组合之间的点积。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 510.175,
    "end": 514
  },
  {
    "input": " I like to visualize a grid full of a bunch of dots,",
    "translatedText": "我喜欢将其想象为一个充满各种点的网格，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 514.48,
    "end": 517.156
  },
  {
    "input": " where the bigger dots correspond to the larger dot products,",
    "translatedText": "其中较大的点对应着较大的点积，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 517.156,
    "end": 520.295
  },
  {
    "input": " the places where the keys and queries align.",
    "translatedText": "即键与查询对齐的地方。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 520.295,
    "end": 523.0113437499999
  },
  {
    "input": " For our adjective noun example, that would look a little more like this, ",
    "translatedText": "就我们讨论的形容词与名词的例子而言，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 523.28,
    "end": 527.25109375
  },
  {
    "input": " where if the keys produced by fluffy and blue ",
    "translatedText": "如果\"毛茸茸的\"和\"蓝色的\"生成的键",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 527.2462890625,
    "end": 530.3229296875
  },
  {
    "input": " really do align closely with the query produced by creature,",
    "translatedText": "确实与\"生物\"产生的查询非常吻合，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 530.3298437500001,
    "end": 533.722453125
  },
  {
    "input": " then the dot products in these two spots would be some large positive numbers.",
    "translatedText": "那么这两个点的点积会是一些较大的正数。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 533.722453125,
    "end": 538.5601171875
  },
  {
    "input": " In the lingo, machine learning people would say that this means the",
    "translatedText": "用机器学习的术语来说，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 539.1,
    "end": 542.307
  },
  {
    "input": " embeddings of fluffy and blue attend to the embedding of creature.",
    "translatedText": "\"毛茸茸的\"和\"蓝色\"的嵌入向量会关注\"生物\"的嵌入向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 542.307,
    "end": 545.42
  },
  {
    "input": " By contrast to the dot product between the key for some other word like \"the\" ",
    "translatedText": "相比之下，像\"the\"这样的词的键",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 546.04,
    "end": 550.2815312500001
  },
  {
    "input": " and the query for creature would be some small or negative value that reflects ",
    "translatedText": "与\"生物\"的查询之间的点积会是一些较小或者负值，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 550.2822734375001,
    "end": 554.1969062499999
  },
  {
    "input": " that are unrelated to each other.",
    "translatedText": "这反映出它们之间没有关联。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 554.203625,
    "end": 556.6
  },
  {
    "input": " So we have this grid of values that can be any real number from negative infinity to infinity, ",
    "translatedText": "因此，我们面前展开了一个值域横跨负无穷到正无穷的实数网格，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 557.7,
    "end": 563.173375
  },
  {
    "input": " giving us a score for how relevant each word is to updating the meaning of every other word.",
    "translatedText": "这个网格赋予了我们评估每个单词在更新其它单词含义上的相关性得分的能力。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 563.20353125,
    "end": 568.48
  },
  {
    "input": " The way we're about to use these scores",
    "translatedText": "接下来，我们将利用这些分数",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 568.9076171875,
    "end": 571.0423515625
  },
  {
    "input": "  is to take a certain weighted sum along each column, ",
    "translatedText": "执行一种操作：按照每个词的相关重要性，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 571.0423515625,
    "end": 574.19922265625
  },
  {
    "input": " weighted by the relevance.",
    "translatedText": "沿着每一列计算加权平均值。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 574.19922265625,
    "end": 576.3846875
  },
  {
    "input": " So instead of having values range from negative infinity to infinity,",
    "translatedText": "因此，我们的目标不是让这些数据列的数值范围无限扩展，从负无穷到正无穷。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 576.3411328125,
    "end": 580.213
  },
  {
    "input": " what we want is for the numbers in these columns to be between 0 and 1,",
    "translatedText": "相反，我们希望这些数据列中的每个数值都介于0和1之间，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 580.213,
    "end": 584.011
  },
  {
    "input": " and for each column to add up to 1, as if they were a probability distribution.",
    "translatedText": "并且每列的数值总和为 1，正如它们构成一个概率分布那样。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 584.011,
    "end": 588.560390625
  },
  {
    "input": " If you're coming in from the last chapter, you know what we need to do then.",
    "translatedText": "如果你从上一章继续阅读，就知道我们接下来要做什么。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 588.90828125,
    "end": 592.22
  },
  {
    "input": " We compute a softmax along each one of these columns to normalize the values.",
    "translatedText": "我们会按列计算 softmax 函数以标准化这些值。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 592.62,
    "end": 597.3
  },
  {
    "input": " In our picture, after you apply softmax to all of the columns,",
    "translatedText": "在我们的示意图中，对所有列应用 softmax 函数后，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 600.06,
    "end": 603.237
  },
  {
    "input": " we'll fill in the grid with these normalized values.",
    "translatedText": "我们会用这些标准化的值填充网格。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 603.237,
    "end": 605.86
  },
  {
    "input": " At this point you're safe to think about ",
    "translatedText": "此时，可以将每一列理解为",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 606.4214453125,
    "end": 608.4101093750002
  },
  {
    "input": " each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
    "translatedText": "根据左侧的单词与顶部对应值的相关性赋予的权重。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 608.4138203125001,
    "end": 614.723515625
  },
  {
    "input": " We call this grid an attention pattern.",
    "translatedText": "我们将这种网格称为“注意力模式”。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 614.6551562500001,
    "end": 616.84
  },
  {
    "input": " Now if you look at the original transformer paper,",
    "translatedText": "如果你看原始的 Transformer 论文，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 617.8612109375001,
    "end": 620.277
  },
  {
    "input": " there's a really compact way that they write this all down.",
    "translatedText": "你会发现他们用一种非常简洁的方式描述了所有这些。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 620.277,
    "end": 622.82
  },
  {
    "input": " Here the variables q and k represent the full arrays of query and key vectors respectively, ",
    "translatedText": "这里的变量 Q 和 K 分别代表查询向量和键向量的完整数组，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 623.6391796875,
    "end": 630.5836171875
  },
  {
    "input": " those little vectors you get by multiplying the embeddings by the query and the key matrices.",
    "translatedText": "这些都是通过将嵌入向量与查询矩阵和键矩阵相乘得到的小型向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 630.5844375,
    "end": 635.1359375
  },
  {
    "input": " This expression up in the numerator is a really compact way to represent",
    "translatedText": "这个在分子上的表达式，是一种简洁的表示方式，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 635.16,
    "end": 639.117
  },
  {
    "input": " the grid of all possible dot products between pairs of keys and queries.",
    "translatedText": "可以表示所有可能的键 - 查询对的点积网格。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 639.117,
    "end": 643.02
  },
  {
    "input": " A small technical detail that I didn't mention is that for numerical stability,",
    "translatedText": "这里有一个我之前没有提到的小细节，那就是为了数值稳定性，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 643.5891015625,
    "end": 648.086
  },
  {
    "input": " it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
    "translatedText": "我们将所有这些值除以键 - 查询空间维度的平方根。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 648.086,
    "end": 653.96
  },
  {
    "input": " Then this softmax that's wrapped around the full expression",
    "translatedText": "然后，包裹全表达式的 softmax 函数，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 654.48,
    "end": 657.865
  },
  {
    "input": " is meant to be understood to apply column by column.",
    "translatedText": "我们应理解为是按列应用的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 657.865,
    "end": 661.2198046875
  },
  {
    "input": " As to that v term, we'll talk about it in just a second.",
    "translatedText": "至于那个 V 项，我们稍后再讲。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 661.64,
    "end": 664.7
  },
  {
    "input": " Before that, there's one other technical detail that so far I've skipped.",
    "translatedText": "在此之前，还有一个我之前没有提到的技术细节。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 665.02,
    "end": 668.46
  },
  {
    "input": " During the training process, when you run this model on a given text example,",
    "translatedText": "在训练过程中，对给定文本进行处理时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 669.04,
    "end": 673.05
  },
  {
    "input": " and all of the weights are slightly adjusted and tuned to either reward or punish it",
    "translatedText": "模型会通过调整权重来奖励或惩罚预测的准确性，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 673.05,
    "end": 676.920234375
  },
  {
    "input": " based on how high a probability it assigns to the true next word in the passage,",
    "translatedText": "即根据模型对文中下一个词的预测概率的高低。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 676.920234375,
    "end": 680.7960546875001
  },
  {
    "input": " it turns out to make the whole training process a lot more efficient",
    "translatedText": "有种做法能显著提高整个训练过程的效率，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 680.7960546875001,
    "end": 683.7979765625
  },
  {
    "input": " if you simultaneously have it predict ",
    "translatedText": "那就是同时让模型预测",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 683.7979765625,
    "end": 685.8117812500001
  },
  {
    "input": " every possible next token following each initial subsequence of tokens in this passage.",
    "translatedText": "该段落中每个初始 Token 子序列之后所有可能出现的下一个 Token。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 685.8117812500001,
    "end": 691.5599999999998
  },
  {
    "input": " For example, with the phrase that we've been focusing on,",
    "translatedText": "例如，对于我们正在关注的那个短语，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 691.94,
    "end": 694.211765625
  },
  {
    "input": " it might also be predicting what words follow creature ",
    "translatedText": "它也可能在预测 \"生物\"  后面应该跟什么单词，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 694.2206328125,
    "end": 697.4179921875001
  },
  {
    "input": " and what words follow \"the\".",
    "translatedText": "以及 \"the\" 后面应该跟什么单词。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 697.4179921875001,
    "end": 699.858125
  },
  {
    "input": " This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
    "translatedText": "这样一来，一个训练样本就能提供更多的学习机会。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 699.94,
    "end": 705.56
  },
  {
    "input": " For the purposes of our attention pattern, ",
    "translatedText": "在设计注意力模式时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 705.998671875,
    "end": 708.0120703125
  },
  {
    "input": " it means that you never want to allow later words to influence earlier words,",
    "translatedText": "一个基本原则是不允许后出现的词汇影响先出现的词汇，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 708.1140234375,
    "end": 712.2049609375
  },
  {
    "input": " since otherwise they could kind of give away the answer for what comes next.",
    "translatedText": "如果不这样做，后面的词汇可能会提前泄露接下来内容的线索。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 712.205,
    "end": 716.04
  },
  {
    "input": " What this means is that we want all of these spots here,",
    "translatedText": "这就要求我们在模型中设置一种机制，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 716.56,
    "end": 719.615
  },
  {
    "input": " the ones representing later tokens influencing earlier ones,",
    "translatedText": "确保代表后续 Token 对前面 Token 的影响力",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 719.615,
    "end": 722.884
  },
  {
    "input": " to somehow be forced to be zero.",
    "translatedText": "能够被有效地削弱到零。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 722.8839999999999,
    "end": 725.4049609375
  },
  {
    "input": " The simplest thing you might think to do is to set them equal to zero,",
    "translatedText": "直觉上，我们可能会考虑直接将这些影响力设置为零。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 725.92,
    "end": 728.751
  },
  {
    "input": " but if you did that the columns wouldn't add up to one anymore,",
    "translatedText": "但这样做会导致一个问题：那些影响力值的列加和不再是一，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 728.751,
    "end": 731.303
  },
  {
    "input": " they wouldn't be normalized.",
    "translatedText": "也就失去了标准化的效果。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 731.303,
    "end": 732.42
  },
  {
    "input": " So instead, a common way to do this is that before applying softmax,",
    "translatedText": "为了解决这个问题，一个常见的做法是在进行 softmax 归一化操作之前，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 733.12,
    "end": 736.456
  },
  {
    "input": " you set all of those entries to be negative infinity.",
    "translatedText": "将这些影响力值设置为负无穷大。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 736.456,
    "end": 739.02
  },
  {
    "input": " If you do that, then after applying softmax, all of those get turned into zero,",
    "translatedText": "这样，经过 softmax 处理后，这些位置的数值会变成零，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 739.68,
    "end": 743.608
  },
  {
    "input": " but the columns stay normalized.",
    "translatedText": "同时保证了整体的归一化条件不被破坏。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 743.608,
    "end": 745.18
  },
  {
    "input": " This process is called masking.",
    "translatedText": "这就是所谓的掩蔽过程。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 746,
    "end": 747.54
  },
  {
    "input": " There are versions of attention where you don't apply it, ",
    "translatedText": "在注意力机制的某些应用场景中，我们不会采用掩码技术。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 747.54,
    "end": 749.9977031249999
  },
  {
    "input": " but in our GPT example,",
    "translatedText": "但在我们的 GPT 示例中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 750.014578125,
    "end": 751.3439999999999
  },
  {
    "input": " even though this is more relevant during the training phase than it would be,",
    "translatedText": "无论是在训练阶段还是在运作阶段",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 751.344,
    "end": 754.964
  },
  {
    "input": " say, running it as a chatbot or something like that,",
    "translatedText": "（比如作为聊天机器人），",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 754.964,
    "end": 757.423
  },
  {
    "input": " you do always apply this masking to prevent later tokens from influencing earlier ones.",
    "translatedText": "都会应用这种掩蔽，以防止后面的 Token 对前面的 Token 产生影响。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 757.423,
    "end": 761.46
  },
  {
    "input": " Another fact that's worth reflecting on about this attention",
    "translatedText": "值得一提的是，这个注意力模式的大小",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 762.48,
    "end": 765.825
  },
  {
    "input": " pattern is how its size is equal to the square of the context size.",
    "translatedText": "等于上下文大小的平方。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 765.825,
    "end": 769.5
  },
  {
    "input": " So this is why context size can be a really huge bottleneck for large language models,",
    "translatedText": "这就解释了上下文大小可能会对大型语言模型构成巨大瓶颈，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 769.9,
    "end": 774.047
  },
  {
    "input": " and scaling it up is non-trivial.",
    "translatedText": "而且要扩大它的话并非易事。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 774.047,
    "end": 775.62
  },
  {
    "input": " As you imagine, motivated by a desire for bigger and bigger context windows,",
    "translatedText": "近年来，出于对更大上下文窗口的追求，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 776.3,
    "end": 780.124
  },
  {
    "input": " recent years have seen some variations to the attention mechanism ",
    "translatedText": "出现了一些对注意力机制的改进，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 780.1240000000001,
    "end": 783.4964765625002
  },
  {
    "input": " aimed at making context more scalable, ",
    "translatedText": " 使其在处理上下文方面更具可扩展性，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 783.4965156250001,
    "end": 785.7459062500001
  },
  {
    "input": " but right here, you and I are staying focused on the basics.",
    "translatedText": "但在这里，我们还是专注于基础知识的讲解。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 785.7544218749999,
    "end": 788.8683984375001
  },
  {
    "input": " Okay, great, computing this pattern ",
    "translatedText": "通过计算这个模式，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 790.06796875,
    "end": 792.216375
  },
  {
    "input": " lets the model deduce which words are relevant to which other words.",
    "translatedText": "模型能够推断哪些词与其他词相关。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 792.216375,
    "end": 795.48
  },
  {
    "input": " Now you need to actually update the embeddings,",
    "translatedText": "然后就需要实际更新嵌入向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 796.02,
    "end": 798.562
  },
  {
    "input": " allowing words to pass information to whichever other words they're relevant to.",
    "translatedText": "让词语可以将信息传递给它们相关的其他词。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 798.562,
    "end": 802.8
  },
  {
    "input": " For example, you want the embedding of Fluffy to somehow cause a change",
    "translatedText": "比如说，你希望 \"毛茸茸的\" 的嵌入向量能够使得 \"生物\" 发生改变，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 802.8,
    "end": 806.818
  },
  {
    "input": " to Creature that moves it to a different part of this 12,000-dimensional embedding space ",
    "translatedText": "从而将它移动到这个 12000 维嵌入空间的另一部分，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 806.818,
    "end": 811.8463750000001
  },
  {
    "input": " that more specifically encodes a Fluffy creature.",
    "translatedText": "以更具体地表达一个 \"毛茸茸的生物\"。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 811.8470390625001,
    "end": 815.006328125
  },
  {
    "input": " What I'm going to do here is first show you the most straightforward way that you could do this, ",
    "translatedText": "我首先向你展示的是执行这个操作的最简单方法，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 815.122734375,
    "end": 819.339609375
  },
  {
    "input": " though there's a slight way that this gets modified in the context of multi-headed attention.",
    "translatedText": "不过要注意，在多头注意力的情境下，这个方法会有些许调整。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 819.339609375,
    "end": 823.46
  },
  {
    "input": " This most straightforward way would be to use a third matrix,",
    "translatedText": "这个方法的核心是使用一个第三个矩阵，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 824.08,
    "end": 827.165
  },
  {
    "input": " what we call the value matrix, ",
    "translatedText": "也就是我们所说的“值矩阵”。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 827.165,
    "end": 828.7763359375001
  },
  {
    "input": " which you multiply by the embedding of that first word,",
    "translatedText": "你需要将它与某个单词的嵌入相乘，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 828.8098125000001,
    "end": 831.4940000000003
  },
  {
    "input": " for example Fluffy.",
    "translatedText": "比如“毛茸茸的”。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 831.4940000000001,
    "end": 832.8948437500002
  },
  {
    "input": " The result of this is what you would call a value vector,",
    "translatedText": "得出的结果我们称之为“值向量”，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 833.3,
    "end": 835.931
  },
  {
    "input": " and this is something that you add to the embedding of the second word,",
    "translatedText": "是你要加入到第二个单词的嵌入向量中的元素，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 835.931,
    "end": 839.197
  },
  {
    "input": " in this case something you add to the embedding of Creature.",
    "translatedText": "例如，在这个情境下，就是要加入到“生物”的嵌入向量中。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 839.197,
    "end": 841.92
  },
  {
    "input": " So this value vector lives in the same very high-dimensional space as the embeddings.",
    "translatedText": "因此，这个值向量就存在于和嵌入向量一样的，非常高维的空间中。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 842.6,
    "end": 847
  },
  {
    "input": " When you multiply this value matrix by the embedding of a word,",
    "translatedText": "当你用这个值矩阵乘以某个单词的嵌入向量时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 847.46,
    "end": 850.832
  },
  {
    "input": " you might think of it as saying,",
    "translatedText": "可以理解为你在询问：",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 850.832,
    "end": 852.0486328125
  },
  {
    "input": " if this word is relevant to adjusting the meaning of something else, ",
    "translatedText": "如果这个单词对于调整其他内容的含义具有相关性，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 852.0486328125,
    "end": 855.6108125000001
  },
  {
    "input": " what exactly should be added to the embedding of that something else in order to reflect this?",
    "translatedText": "那么为了反映这一点，我们需要向那个内容的嵌入中添加什么呢？",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 855.6108125000001,
    "end": 861.16
  },
  {
    "input": " Looking back in our diagram, let's set aside all of the keys and the queries,",
    "translatedText": "回到我们的图表，我们先不考虑所有的键和查询，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 861.62359375,
    "end": 866.0170000000002
  },
  {
    "input": " since after you compute the attention pattern you're done with those,",
    "translatedText": "因为一旦计算出注意力模式，这些就不再需要了。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 866.017,
    "end": 869.497
  },
  {
    "input": " then you're going to take this value matrix and multiply it by every one of those embeddings ",
    "translatedText": "接下来，我们将使用这个值矩阵，将其与每一个嵌入向量相乘，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 869.497,
    "end": 873.9031953125
  },
  {
    "input": " to produce a sequence of value vectors.",
    "translatedText": "从而生成一系列的值向量。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 873.9076875000001,
    "end": 876.4681640625
  },
  {
    "input": " You might think of these value vectors as being kind of associated with the corresponding keys.",
    "translatedText": "你可以将这些值向量视作在某种程度上与它们相对应的“键”有关。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 877.12,
    "end": 881.12
  },
  {
    "input": " For each column in this diagram, ",
    "translatedText": "对图表中的每一列来说，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 882.32,
    "end": 884.6191796874998
  },
  {
    "input": " value vectors by the corresponding weight in that column.",
    "translatedText": "你需要将每个值向量与该列中相应的权重相乘。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 884.6191796874997,
    "end": 889.4656640625
  },
  {
    "input": " For example here, under the embedding of Creature,",
    "translatedText": "举个例子，对于代表“生物”的嵌入向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 889.8709375,
    "end": 892.815
  },
  {
    "input": " you would be adding large proportions of the value vectors for Fluffy and Blue,",
    "translatedText": "你会主要加入“毛茸茸的”和“蓝色的”这两个值向量的较大比例，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 892.815,
    "end": 897.107
  },
  {
    "input": " while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
    "translatedText": "而其他的值向量则被减少为零，或者至少接近零。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 897.107,
    "end": 901.56
  },
  {
    "input": " And then finally, the way to actually update the embedding associated with this column,",
    "translatedText": "最后，为了更新这一列与之相关联的嵌入向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 902.12,
    "end": 906.804
  },
  {
    "input": " previously encoding some context-free meaning of Creature,",
    "translatedText": "原本这个向量编码了“生物”这一概念的某种基本含义（不考虑具体上下文），",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 906.804,
    "end": 909.944
  },
  {
    "input": " you add together all of these rescaled values in the column,",
    "translatedText": "你需要将列中所有这些经过重新调整比例的值加总起来，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 909.944,
    "end": 913.191
  },
  {
    "input": " producing a change that you want to add, that I'll label delta-e,",
    "translatedText": "这一步骤产生了一个我们想要引入的变化量，我将其称为delta-e。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 913.191,
    "end": 916.704
  },
  {
    "input": " and then you add that to the original embedding.",
    "translatedText": "接着，你就将这个变化量叠加到原有的嵌入向量上。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 916.704,
    "end": 919.26
  },
  {
    "input": " Hopefully what results is a more refined vector ",
    "translatedText": "希望最终得到的是一个更精细的向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 919.68,
    "end": 922.4960312500001
  },
  {
    "input": " encoding the more contextually rich meaning, ",
    "translatedText": "是一个更加细致和含有丰富上下文信息的向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 922.4960312500001,
    "end": 924.8691484375001
  },
  {
    "input": " like that of a fluffy blue creature.",
    "translatedText": "比如描绘了一个毛绒绒、蓝色的奇妙生物。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 924.8691484375001,
    "end": 926.8898046875
  },
  {
    "input": " And of course you don't just do this to one embedding,",
    "translatedText": "显然，我们不仅仅对一个嵌入向量进行这种处理，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 927.38,
    "end": 930.223
  },
  {
    "input": " you apply the same weighted sum across all of the columns in this picture,",
    "translatedText": "而是将同样的加权求和方法应用于图像中所有的列，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 930.2229999999998,
    "end": 933.7328749999999
  },
  {
    "input": " producing a sequence of changes, ",
    "translatedText": "由此产生一连串的调整。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 933.7328749999999,
    "end": 936.0244765625
  },
  {
    "input": " adding all of those changes to the corresponding embeddings, ",
    "translatedText": "将这些调整加到相应的嵌入向量上，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 936.0244765625,
    "end": 939.14723828125
  },
  {
    "input": " produces a full sequence of more refined embeddings popping out of the attention block.",
    "translatedText": "便形成了一组更为细腻且富含信息的嵌入向量序列。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 939.14723828125,
    "end": 943.46
  },
  {
    "input": " Zooming out, this whole process is what you would describe as a single head of attention.",
    "translatedText": "从宏观角度来看，我们讨论的整个过程构成了所谓的“单头注意力”机制。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 944.515546875,
    "end": 949.1
  },
  {
    "input": " As I've described things so far, ",
    "translatedText": "正如之前所解释的，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 949.3011328125,
    "end": 951.2644296875
  },
  {
    "input": " this process is parameterized by three distinct matrices, all filled with tunable parameters, ",
    "translatedText": "这一机制是通过三种不同的、充满可调整参数的矩阵来实现的，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 951.2644296875,
    "end": 956.6195
  },
  {
    "input": " the key, the query, and the value.",
    "translatedText": " 即“键”、“查询”和“值”。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 956.6207109375,
    "end": 959.278515625
  },
  {
    "input": " I want to take a moment to continue what we started in the last chapter,",
    "translatedText": "接下来，我想继续上一章节我们开始探讨的内容，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 959.5,
    "end": 962.982
  },
  {
    "input": " with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
    "translatedText": "也就是通过统计 GPT-3 模型的参数数量来进行“计分”。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 962.982,
    "end": 968.04
  },
  {
    "input": " These key and query matrices each have 12,288 columns, matching the embedding dimension,",
    "translatedText": "这些键矩阵和查询矩阵每个都有 12,288 列，与嵌入维度匹配，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 968.96796875,
    "end": 975.1010000000001
  },
  {
    "input": " and 128 rows, matching the dimension of that smaller key query space.",
    "translatedText": "以及 128 行，与较小的键查询空间的维度匹配。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 975.101,
    "end": 979.6
  },
  {
    "input": " This gives us an additional 1.5 million or so parameters for each one.",
    "translatedText": "这给我们每个矩阵增加了大约 150 万个参数。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 980.26,
    "end": 984.22
  },
  {
    "input": " If you look at that value matrix by contrast, the way I've described things so",
    "translatedText": "如果你看看值矩阵，按照我目前为止的描述，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 984.86,
    "end": 990.19
  },
  {
    "input": " far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows,",
    "translatedText": "它看上去是一个正方形的矩阵，有 12,288 列和 12,288 行，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 990.19,
    "end": 995.926
  },
  {
    "input": " since both its inputs and outputs live in this very large embedding space.",
    "translatedText": "因为它的输入和输出都存在于这个庞大的嵌入空间中。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 995.926,
    "end": 1000.92
  },
  {
    "input": " If true, that would mean about 150 million added parameters.",
    "translatedText": "如果这是真的，那就意味着要增加大约 1500 万个参数。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1001.2685546875,
    "end": 1005.14
  },
  {
    "input": " And to be clear, you could do that.",
    "translatedText": "当然，你可以这样做，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1005.66,
    "end": 1007.3
  },
  {
    "input": " You could devote orders of magnitude more parameters",
    "translatedText": "你可以让值矩阵拥有数量级更多的参数，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1007.42,
    "end": 1009.805
  },
  {
    "input": " to the value map than to the key and query.",
    "translatedText": "而不是键矩阵和查询矩阵。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1009.805,
    "end": 1011.74
  },
  {
    "input": " But in practice, it is much more efficient ",
    "translatedText": "但在实践中，如果想效率更高，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1012.06,
    "end": 1013.9172187500001
  },
  {
    "input": " if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
    "translatedText": "你可以让值矩阵的参数数量与键矩阵和查询矩阵的参数数量相同。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1013.9172187500002,
    "end": 1020.76
  },
  {
    "input": " This is especially relevant in the setting of running multiple attention heads in parallel.",
    "translatedText": "特别是在同时运行多个注意力机制的场景下，这一点尤为重要。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1021.14140625,
    "end": 1025.16
  },
  {
    "input": " The way this looks is that the value map is factored as a product of two smaller matrices.",
    "translatedText": "具体来说，值映射实际上是两个小矩阵乘积的形式。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1025.8588671875,
    "end": 1030.099
  },
  {
    "input": " Conceptually, I would still encourage you to think about the overall linear map,",
    "translatedText": "我还是建议从整体上去理解这个线性映射过程，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1030.5690625,
    "end": 1035.386
  },
  {
    "input": " one with inputs and outputs, both in this larger embedding space,",
    "translatedText": "输入和输出都在这个更大的嵌入空间中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1035.386,
    "end": 1038.814
  },
  {
    "input": " for example taking the embedding of blue to this blueness direction that you would add to nouns.",
    "translatedText": "比如将“蓝色”的嵌入向量指向加到名词上以表示“蓝色”的方向。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1038.814,
    "end": 1043.7581015625
  },
  {
    "input": " It's just that it's broken up into two separate steps.",
    "translatedText": "它只是被分成了两个单独的步骤。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1043.73634375,
    "end": 1046.478671875
  },
  {
    "input": " It's just that it's a smaller number of rows,",
    "translatedText": "这里的变化只是它的行数比较少，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1046.7492578125,
    "end": 1049.869
  },
  {
    "input": " typically the same size as the key query space.",
    "translatedText": "通常和键查询的空间大小一致。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1049.869,
    "end": 1052.76
  },
  {
    "input": " What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
    "translatedText": "可以理解为，它将较大的嵌入向量映射到了一个更小的空间。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1053.1,
    "end": 1058.44
  },
  {
    "input": " This is not the conventional naming, but I'm going to call this the value down matrix.",
    "translatedText": "虽然这不是通用的术语，但我决定将其称作“值降维矩阵”。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1059.04,
    "end": 1062.7
  },
  {
    "input": " The second matrix maps from this smaller space back up to the embedding space,",
    "translatedText": "而第二个矩阵则是从这个较小的空间映射回原来的嵌入空间，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1063.4,
    "end": 1067.422
  },
  {
    "input": " producing the vectors that you use to make the actual updates.",
    "translatedText": "生成了用于实际更新的向量，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1067.422,
    "end": 1070.58
  },
  {
    "input": " I'm going to call this one the value up matrix, which again is not conventional.",
    "translatedText": "我将其称为“值升维矩阵”，这个命名同样非传统。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1071,
    "end": 1074.74
  },
  {
    "input": " The way that you would see this written in most papers looks a little different.",
    "translatedText": "在大多数论文中，你会看到的描述方式可能和我说的有所不同，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1075.16,
    "end": 1078.08
  },
  {
    "input": " I'll talk about it in a minute.",
    "translatedText": "我稍后会解释原因。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1078.2401562500002,
    "end": 1079.5441015625
  },
  {
    "input": " In my opinion, it tends to make things a little more conceptually confusing.",
    "translatedText": "但我个人认为，那种描述方式可能会让概念理解变得更加混乱。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1079.532734375,
    "end": 1082.54
  },
  {
    "input": " To throw in linear algebra jargon here, ",
    "translatedText": "在这里借用一下线性代数的专业术语，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1083.26,
    "end": 1085.2057890625
  },
  {
    "input": " what we're basically doing is constraining the overall value map",
    "translatedText": "我们实际上在做的，就是把整个数据的变换过程",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1085.2057890625,
    "end": 1088.4756484375
  },
  {
    "input": " to be a low rank transformation.",
    "translatedText": "限制成一种比较简单的形式。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1088.4756484375,
    "end": 1090.8390625
  },
  {
    "input": " Turning back to the parameter count, all four of these matrices have the same size,",
    "translatedText": "回到参数计数，这四个矩阵的尺寸相同，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1091.42,
    "end": 1096.156
  },
  {
    "input": " and adding them all up we get about 6.3 million parameters ",
    "translatedText": "将他们全部加起来，就得到了大约 630 万个参数，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1096.156,
    "end": 1099.429875
  },
  {
    "input": " for one attention head.",
    "translatedText": "这是一个注意力机制所需要的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1099.43565625,
    "end": 1101.0824609375
  },
  {
    "input": " As a quick side note, to be a little more accurate,",
    "translatedText": "顺带一提，为了更准确，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1102.04,
    "end": 1104.236
  },
  {
    "input": " everything described so far is what people would call a self-attention head,",
    "translatedText": "到目前为止我们讨论的这部分通常被称为“自我注意力”机制，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1104.236,
    "end": 1107.487
  },
  {
    "input": " to distinguish it from a variation that comes up in other models ",
    "translatedText": "这是为了将其与其他模型中出现的一个不同版本区分开来，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1107.487,
    "end": 1110.3018671875
  },
  {
    "input": " that's called cross-attention.",
    "translatedText": "那就是被称为“交叉注意力”的版本。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1110.3018671875,
    "end": 1112.0625390625
  },
  {
    "input": " This isn't relevant to our GPT example, but if you're curious,",
    "translatedText": "这与我们的 GPT 示例无关，但如果你感兴趣的话，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1112.3,
    "end": 1115.787
  },
  {
    "input": " cross-attention involves models that process two distinct types of data,",
    "translatedText": "交叉注意力涉及的模型会处理两种不同类型的数据，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1115.787,
    "end": 1119.7856562499999
  },
  {
    "input": " like text in one language and text in another language that's part of an ongoing generation of a translation, ",
    "translatedText": "比如一种语言的文本和正在翻译的另一种语言的文本， ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1119.828,
    "end": 1125.3815703125
  },
  {
    "input": " or maybe audio input of speech and an ongoing transcription.",
    "translatedText": "或者可能是语音输入和正在进行的转录。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1125.3815703125,
    "end": 1129.6540234375
  },
  {
    "input": " A cross-attention head looks almost identical.",
    "translatedText": "交叉注意力机制看起来几乎和自注意力机制一样。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1130.4,
    "end": 1132.7
  },
  {
    "input": " The only difference is that the key and query maps act on different data sets.",
    "translatedText": "唯一的区别是，键和查询映射在交叉注意力机制中会作用于不同的数据集。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1132.98,
    "end": 1137.4
  },
  {
    "input": " In a model doing translation, for example, the keys might come from one language,",
    "translatedText": "例如，在进行翻译的模型中，键可能来自一种语言，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1137.84,
    "end": 1142.109
  },
  {
    "input": " while the queries come from another, ",
    "translatedText": "而查询来自另一种语言，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1142.109,
    "end": 1143.9674453125
  },
  {
    "input": " and the attention pattern could describe ",
    "translatedText": "注意力模式可以描述 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1143.9674453125,
    "end": 1145.83809765625
  },
  {
    "input": " which words from one language correspond to which words in another.",
    "translatedText": "一种语言的哪些词对应另一种语言的哪些词。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1145.83930859375,
    "end": 1149.8224609375
  },
  {
    "input": " And in this setting there would typically be no masking,",
    "translatedText": "在这种情况下，通常不会有遮蔽，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1150.34,
    "end": 1152.93
  },
  {
    "input": " since there's not really any notion of later tokens affecting earlier ones.",
    "translatedText": "因为并不存在后面的词会影响前面的词的概念。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1152.93,
    "end": 1156.34
  },
  {
    "input": " Staying focused on self-attention though,",
    "translatedText": "继续讨论自注意力机制， ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1157.18,
    "end": 1159.060140625
  },
  {
    "input": " if you understood everything so far,",
    "translatedText": "如果你已经理解了到目前为止的所有内容，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1159.060140625,
    "end": 1160.812
  },
  {
    "input": " and if you were to stop here, ",
    "translatedText": "那么即使你现在停下，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1160.812,
    "end": 1162.3589453125
  },
  {
    "input": " you would come away with the essence of what attention really is.",
    "translatedText": "也已经领会了注意力模型的核心要义。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1162.3589453125,
    "end": 1165.18
  },
  {
    "input": " All that's really left to us is to lay out the sense in which you do this many many different times.",
    "translatedText": "我们剩下要讲的就是这个过程需要做多次的原因。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1165.76,
    "end": 1171.44
  },
  {
    "input": " In our central example we focused on adjectives updating nouns,",
    "translatedText": "在之前的例子中，我们专注于形容词如何改变名词的含义，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1172.1,
    "end": 1175.179
  },
  {
    "input": " but of course there are lots of different ways that context can influence the meaning of a word.",
    "translatedText": "但实际上，语境对词语含义的影响方式有很多种。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1175.179,
    "end": 1179.8
  },
  {
    "input": " If the words they crashed the preceded the word car,",
    "translatedText": "比如，如果“他们撞毁了”出现在\"车\"之前，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1180.36,
    "end": 1183.249
  },
  {
    "input": " it has implications for the shape and structure of that car.",
    "translatedText": "会对车子的形状和结构产生预设。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1183.249,
    "end": 1186.52
  },
  {
    "input": " And a lot of associations might be less grammatical.",
    "translatedText": "而且很多时候，这种联系可能并不遵循语法规则。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1186.8926953125,
    "end": 1189.548984375
  },
  {
    "input": " If the word wizard is anywhere in the same passage as Harry,",
    "translatedText": "比如，如果“魔法师”和“哈利”出现在同一篇文章中，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1189.76,
    "end": 1192.935
  },
  {
    "input": " it suggests that this might be referring to Harry Potter,",
    "translatedText": "可能暗示的是哈利·波特，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1192.935,
    "end": 1195.954
  },
  {
    "input": " whereas if instead the words Queen, Sussex, and William were in that passage,",
    "translatedText": "而如果这篇文章中还出现了“女王”，“萨塞克斯”和“威廉”，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1195.954,
    "end": 1200.015
  },
  {
    "input": " then perhaps the embedding of Harry should instead be updated to refer to the prince.",
    "translatedText": "那么哈利的词嵌入应该更新为指代王子。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1200.015,
    "end": 1204.44
  },
  {
    "input": " For every different type of contextual updating that you might imagine,",
    "translatedText": "你能想象的每一种上下文更新方式，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1205.04,
    "end": 1208.589
  },
  {
    "input": " the parameters of these key and query matrices would be different to",
    "translatedText": "键和查询矩阵的参数都会有所不同，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1208.589,
    "end": 1211.991
  },
  {
    "input": " capture the different attention patterns, and the parameters of our",
    "translatedText": "用以捕捉不同的注意力模式，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1211.991,
    "end": 1215.343
  },
  {
    "input": " value map would be different based on what should be added to the embeddings.",
    "translatedText": "而值映射的参数会根据需要添加到嵌入中的信息有所改变。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1215.343,
    "end": 1219.14
  },
  {
    "input": " And again, in practice the true behavior of these maps is much more difficult to interpret, ",
    "translatedText": "再次强调，虽然这些映射的真实行为更复杂难懂，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1219.770234375,
    "end": 1224.4015546875
  },
  {
    "input": " where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
    "translatedText": "但权重设置是为了让模型能够更好地完成预测下一个 Token 的任务。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1224.4028046875,
    "end": 1230.14
  },
  {
    "input": " As I said before, everything we described is a single head of attention,",
    "translatedText": "前面所描述的都只是单个注意力头，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1231.4,
    "end": 1235.24
  },
  {
    "input": " and a full attention block inside a transformer consists of  what's called multi-headed attention,",
    "translatedText": "在 Transformer 中完整的注意力块由多头注意力组成，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1235.24,
    "end": 1240.266640625
  },
  {
    "input": " where you run a lot of these operations in parallel,",
    "translatedText": "同时运行多个操作，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1240.2630078125,
    "end": 1243.184
  },
  {
    "input": " each with its own distinct key query and value maps.",
    "translatedText": "每个操作都有其独特的键、查询和值映射。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1243.184,
    "end": 1245.92
  },
  {
    "input": " GPT-3 for example uses 96 attention heads inside each block.",
    "translatedText": "例如，GPT-3 在每个块中都使用了 96 个注意力头。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1246.9861328125,
    "end": 1251.7
  },
  {
    "input": " Considering that each one is already a bit confusing,",
    "translatedText": "考虑到每一个都相当复杂，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1252.02,
    "end": 1254.517
  },
  {
    "input": " it's certainly a lot to hold in your head.",
    "translatedText": "的确需要花费一些精力理解。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1254.517,
    "end": 1256.46
  },
  {
    "input": " Just to spell it all out very explicitly, this means you have 96",
    "translatedText": "也就是说，你有 96 套不同的键和查询矩阵，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1256.76,
    "end": 1260.641
  },
  {
    "input": " distinct key and query matrices producing 96 distinct attention patterns.",
    "translatedText": "产生 96 种不同的注意力模式。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1260.641,
    "end": 1265
  },
  {
    "input": " Then each head has its own distinct value matrices",
    "translatedText": "然后，每个注意力头都有独特的值矩阵",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1265.44,
    "end": 1268.983
  },
  {
    "input": " used to produce 96 sequences of value vectors.",
    "translatedText": "用来产生 96 个值向量的序列。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1268.983,
    "end": 1272.18
  },
  {
    "input": " These are all added together using the corresponding attention patterns as weights.",
    "translatedText": "所有这些都通过使用对应的注意力模式作为权重进行加总。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1272.46,
    "end": 1276.68
  },
  {
    "input": " What this means is that for each position in the context, each token,",
    "translatedText": "这意味着，在上下文中的每个位置，每个 Token，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1277.48,
    "end": 1281.455
  },
  {
    "input": " every one of these heads produces a proposed change",
    "translatedText": "所有的头都会产生一个建议的变化，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1281.455,
    "end": 1284.8147265625
  },
  {
    "input": " to be added to the embedding in that position.",
    "translatedText": "以便添加到该位置的嵌入中。 ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1284.8147265625003,
    "end": 1287.02
  },
  {
    "input": " So what you do is you sum together all of those proposed changes,",
    "translatedText": "因此，你需要将所有这些建议的变化加在一起，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1287.66,
    "end": 1291.078
  },
  {
    "input": " one for each head, and you add the result to the original embedding of that position.",
    "translatedText": "每个头对应一个，然后将结果加入到该位置的原始嵌入中。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1291.078,
    "end": 1295.48
  },
  {
    "input": " This entire sum here would be one slice of what's outputted from this multi-headed attention block, ",
    "translatedText": "这个总和就是从多头注意力块输出的一个部分，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1296.66,
    "end": 1303.2229765625
  },
  {
    "input": " a single one of those refined embeddings",
    "translatedText": "是精炼后的嵌入之一，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1303.1766875,
    "end": 1305.80603125
  },
  {
    "input": " that pops out the other end of it.",
    "translatedText": "它从另一端弹出来。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1305.80603125,
    "end": 1307.46
  },
  {
    "input": " Again, this is a lot to think about,",
    "translatedText": "再次强调，这需要考虑的东西很多，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1308.15796875,
    "end": 1309.8644921875002
  },
  {
    "input": " so don't worry at all if it takes some time to sink in.",
    "translatedText": "所以如果需要一段时间来理解，完全不用担心。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1309.8644921875002,
    "end": 1312.14
  },
  {
    "input": " The overall idea is that by running many distinct heads in parallel,",
    "translatedText": "总的来说，通过并行运行多个不同的头，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1312.38,
    "end": 1316.376
  },
  {
    "input": " you're giving the model the capacity to learn many distinct ways that context changes meaning.",
    "translatedText": "你就能让模型有能力学习上下文改变含义的多种不同方式。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1316.376,
    "end": 1321.8199999999997
  },
  {
    "input": " Pulling up our running tally for parameter count with 96 heads,",
    "translatedText": "我们计算下来，每个包含 96 个头的参数，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1323.7,
    "end": 1327.323
  },
  {
    "input": " each including its own variation of these four matrices,",
    "translatedText": "各自有四个矩阵的变体，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1327.323,
    "end": 1330.55
  },
  {
    "input": " each block of multi-headed attention ends up with around 600 million parameters.",
    "translatedText": "每个多头注意力块最后大约有 6 亿个参数。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1330.55,
    "end": 1335.08
  },
  {
    "input": " There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
    "translatedText": "对于那些想深入了解 transformer 的人，这里有个小插曲我必须提一下。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1336.42,
    "end": 1341.8
  },
  {
    "input": " You remember how I said that the value map is factored out into these two",
    "translatedText": "你可能还记得我曾经说过，值映射被分解成两个不同的矩阵，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1342.08,
    "end": 1345.639
  },
  {
    "input": " distinct matrices, which I labeled as the value down and the value up matrices.",
    "translatedText": "我把它们标记为值下降和值上升矩阵。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1345.639,
    "end": 1349.44
  },
  {
    "input": " The way that I framed things would suggest ",
    "translatedText": "我之前的描述可能会",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1349.96,
    "end": 1352.04858984375
  },
  {
    "input": " that you see this pair of matrices inside each attention head, ",
    "translatedText": "让你觉得在每个注意力头里都会看到这一对矩阵，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1352.04858984375,
    "end": 1355.8321796874995
  },
  {
    "input": " and you could absolutely implement it this way.",
    "translatedText": "并且实际上也确实可以这样实现，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1355.8213984375,
    "end": 1358.44
  },
  {
    "input": " That would be a valid design.",
    "translatedText": "这种设计是可行的。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1358.64,
    "end": 1359.92
  },
  {
    "input": " But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
    "translatedText": "但是在论文中，以及在实践中的实现方式看起来有些不同。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1360.26,
    "end": 1364.92
  },
  {
    "input": " All of these value up matrices for each head appear stapled together in one giant matrix",
    "translatedText": "所有这些每个头的值向上矩阵，都像是被合在一起的一个巨大的矩阵，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1365.34,
    "end": 1370.891
  },
  {
    "input": " that we call the output matrix, associated with the entire multi-headed attention block.",
    "translatedText": "我们称之为输出矩阵，它与整个多头注意力块相关联。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1370.891,
    "end": 1376.38
  },
  {
    "input": " And when you see people refer to the value matrix for a given attention head,",
    "translatedText": "当你看到人们谈论某个 attention head 的值矩阵时，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1376.82,
    "end": 1380.634
  },
  {
    "input": " they're typically only referring to this first step,",
    "translatedText": "他们通常只指的是这个第一步，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1380.634,
    "end": 1383.227
  },
  {
    "input": " the one that I was labeling as the value down projection into the smaller space.",
    "translatedText": "也就是我所说的值向下投影到小空间的步骤。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1383.227,
    "end": 1387.14
  },
  {
    "input": " For the curious among you, I've left an on-screen note about it.",
    "translatedText": "对那些好奇的人，我在这里注明了这一点。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1388.34,
    "end": 1391.140859375
  },
  {
    "input": " It's one of those details that runs the risk of distracting from the main conceptual points, ",
    "translatedText": "虽然这是一个可能会让人偏离主要概念的细节，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1391.14484375,
    "end": 1395.0653437500002
  },
  {
    "input": " but I do want to call it out",
    "translatedText": "但我还是想提一下，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1395.0431953125,
    "end": 1396.086
  },
  {
    "input": " just so that you know if you read about this in other sources.",
    "translatedText": "这样当你在其他地方看到这些讨论时，你就会知道它的来龙去脉。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1396.086,
    "end": 1398.6495703125
  },
  {
    "input": " Setting aside all the technical nuances,",
    "translatedText": "抛开所有的技术细节，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1399.24,
    "end": 1401.377
  },
  {
    "input": " in the preview from the last chapter we saw how data flowing through a transformer",
    "translatedText": "我们在上一章的概览中了解到，数据在 Transformer 中的流动",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1401.3769999999997,
    "end": 1405.4502499999999
  },
  {
    "input": " doesn't just flow through a single attention block.",
    "translatedText": "并不局限于单个注意力模块。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1405.4512265624999,
    "end": 1408.04
  },
  {
    "input": " For one thing, it also goes through these other operations called multi-layer perceptrons.",
    "translatedText": "首先，数据还会经过其他被称为多层感知器的操作。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1408.329453125,
    "end": 1412.8490234375
  },
  {
    "input": " We'll talk more about those in the next chapter.",
    "translatedText": "我们会在下一章详细介绍这个。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1412.8530859375,
    "end": 1414.9979687500004
  },
  {
    "input": " And then it repeatedly goes through many many copies of both of these operations.",
    "translatedText": "然后，数据会反复经过这两种操作的多个副本。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1414.990625,
    "end": 1419.5666015625
  },
  {
    "input": " What this means is that after a given word imbibes some of its context,",
    "translatedText": "这意味着，一个单词在吸收了一些上下文信息后，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1419.6362109375,
    "end": 1423.9589999999998
  },
  {
    "input": " there are many more chances for this more nuanced embedding",
    "translatedText": "这个更细致的 embedding 仍有更多的机会",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1423.959,
    "end": 1427.276
  },
  {
    "input": " to be influenced by its more nuanced surroundings.",
    "translatedText": "受到其周围更为细致环境的影响。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1427.276,
    "end": 1430.04
  },
  {
    "input": " The further down the network you go, ",
    "translatedText": "你在网络中越深入，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1430.6034375,
    "end": 1432.5806874999998
  },
  {
    "input": " with each embedding taking in more and more meaning from all the other embeddings, ",
    "translatedText": "每个 embedding 从所有其他 embedding 中获取的含义就越多，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1432.6020937499998,
    "end": 1436.4507109375002
  },
  {
    "input": " which themselves are getting more and more nuanced,",
    "translatedText": "这些 embedding 本身也变得越来越复杂，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1436.4514921875002,
    "end": 1439.1040000000003
  },
  {
    "input": " the hope is that there's the capacity to encode higher level and more abstract ideas about a given input ",
    "translatedText": "我们希望这能有助于编码关于给定输入的更高级别和更抽象的概念，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1439.104,
    "end": 1444.5996171875
  },
  {
    "input": " beyond just descriptors and grammatical structure.",
    "translatedText": "而不仅仅是描述和语法结构。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1444.591296875,
    "end": 1447.445234375
  },
  {
    "input": " Things like sentiment and tone and whether it's a poem and what underlying",
    "translatedText": "这些概念可以是情感、语调，是否是一首诗，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1447.6483984375,
    "end": 1451.763
  },
  {
    "input": " scientific truths are relevant to the piece and things like that.",
    "translatedText": "以及与这个作品相关的基础科学真理等等。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1451.763,
    "end": 1455.13
  },
  {
    "input": " Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers,",
    "translatedText": "再回到我们的统计，GPT-3 包括 96 个不同的层，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1456.7,
    "end": 1462.052
  },
  {
    "input": " so the total number of key query and value parameters is multiplied by another 96,",
    "translatedText": "因此关键查询和值参数的总数需要乘以 96，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1462.052,
    "end": 1467.405
  },
  {
    "input": " which brings the total sum to just under 58 billion distinct parameters",
    "translatedText": "这使得总数达到将近 580 亿个，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1467.405,
    "end": 1472.049
  },
  {
    "input": " devoted to all of the attention heads.",
    "translatedText": "这些参数全部用于各种 attention head。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1472.049,
    "end": 1474.5
  },
  {
    "input": " That is a lot to be sure, ",
    "translatedText": "这确实是一个巨大的数字，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1474.677890625,
    "end": 1476.4106093750001
  },
  {
    "input": " but it's only about a third of the 175 billion that are in the network in total.",
    "translatedText": "但它只占网络总计 1750 亿参数的大约三分之一。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1476.4173671875,
    "end": 1480.94
  },
  {
    "input": " So even though attention gets all of the attention,",
    "translatedText": "所以，尽管注意力模型吸引了所有的关注，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1481.52,
    "end": 1484.147
  },
  {
    "input": " the majority of parameters come from the blocks sitting in between these steps.",
    "translatedText": "但大部分的参数其实来自那些位于这些步骤之间的模块。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1484.147,
    "end": 1488.14
  },
  {
    "input": " In the next chapter, you and I will talk more about those",
    "translatedText": "在下一章，我们将更深入地讨论这些模块，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1488.56,
    "end": 1491.017
  },
  {
    "input": " other blocks and also a lot more about the training process.",
    "translatedText": "以及更多关于训练过程的信息。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1491.017,
    "end": 1493.56
  },
  {
    "input": " A big part of the story for the success of the attention mechanism",
    "translatedText": "注意力机制的成功之处",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1494.12,
    "end": 1497.2341640625
  },
  {
    "input": " is not so much any specific kind of behavior that it enables, ",
    "translatedText": "并非在于它所启动的任何特定类型的行为， ",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1497.2505703125,
    "end": 1500.9885546875003
  },
  {
    "input": " but the fact that it's extremely parallelizable, ",
    "translatedText": "而在于它极其适合并行运算，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1501.0321875000002,
    "end": 1503.9982656250002
  },
  {
    "input": " meaning that you can run a huge number of computations in a short time using GPUs.",
    "translatedText": "这意味着你可以使用 GPU 在短时间内完成大量的计算任务。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1503.9982656250004,
    "end": 1508.8457421875
  },
  {
    "input": " Given that one of the big lessons about deep learning in the last decade or two has",
    "translatedText": "在过去十年或二十年的深度学习研究中，我们得到了一个重要启示，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1509.0451953125,
    "end": 1513.3569999999997
  },
  {
    "input": " been that scale alone seems to give huge qualitative improvements in model performance,",
    "translatedText": "那就是规模的放大似乎可以带来模型性能的巨大定量提升。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1513.357,
    "end": 1517.44
  },
  {
    "input": " there's a huge advantage to parallelizable architectures that let you do this.",
    "translatedText": "因此，适合并行运算的架构具有巨大的优势。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1517.44,
    "end": 1521.06
  },
  {
    "input": " If you want to learn more about this stuff, I've left lots of links in the description.",
    "translatedText": "如果你想了解更多关于这个主题的信息，我在视频简介中留下了很多链接。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1522.04,
    "end": 1525.71109375
  },
  {
    "input": " In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
    "translatedText": "特别是，由 Andrej Karpathy 或 Chris Ola 制作的任何内容都非常值得一看。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1525.7458203125,
    "end": 1530.04
  },
  {
    "input": " In this video, I wanted to just jump into attention in its current form,",
    "translatedText": "在这个视频中，我只是想直接介绍现在的注意力机制，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1530.56,
    "end": 1533.763
  },
  {
    "input": " but if you're curious about more of the history for how we got here",
    "translatedText": "但是如果你对我们是如何达到这里的历程",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1533.763,
    "end": 1536.747
  },
  {
    "input": " and how you might reinvent this idea for yourself,",
    "translatedText": "以及你可能如何为自己重新创新这个想法感到好奇，",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1536.747,
    "end": 1538.985
  },
  {
    "input": " my friend Vivek just put up a couple videos giving a lot more of that motivation.",
    "translatedText": "我的朋友 Vivek 最近就发布了几个视频，深入讲解了这个动机。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1538.985,
    "end": 1542.7363671875
  },
  {
    "input": " Also, Britt Cruz from the channel The Art of the Problem has",
    "translatedText": "此外，来自\"The Art of the Problem\"频道的 Britt Cruz",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1542.74421875,
    "end": 1545.7460000000003
  },
  {
    "input": " a really nice video about the history of large language models.",
    "translatedText": "有一部非常精彩的视频，介绍了大语言模型的历史。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1545.746,
    "end": 1548.46
  },
  {
    "input": " Thank you.",
    "translatedText": "谢谢。",
    "model": "JimLiu",
    "n_reviews": 0,
    "start": 1564.96,
    "end": 1569.2
  }
]
