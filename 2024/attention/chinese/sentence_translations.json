[
 {
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "translatedText": "在上一章中，我们开始逐步了解变压器的内部工作原理。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "translatedText": "这是大型语言模型以及现代人工智能浪潮中许多其他工具的关键技术之一。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "translatedText": "在本章中，你和我将深入探讨这种注意力机制是什么，直观地了解它是如何处理数据的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "translatedText": "在此，我想快速回顾一些重要的背景信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "translatedText": "你我正在学习的这一模型的目标是接收一段文字，并预测下一个单词是什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "translatedText": "输入文本被分割成小块，我们称之为标记，这些标记通常是单词或单词的片段，但为了让你我更容易理解视频中的例子，我们不妨简化一下，假装标记始终只是单词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "translatedText": "转换器的第一步是将每个标记与一个高维向量关联起来，我们称之为嵌入。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "translatedText": "我希望你们记住的最重要的观点是，在这个由所有可能的嵌入组成的高维空间中，方向如何与语义相对应。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "translatedText": "在上一章中，我们举例说明了方向如何与性别相对应，也就是说，在这个空间中增加某一步，就可以将一个阳性名词嵌入到相应的阴性名词中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "translatedText": "这只是一个例子，你可以想象，在这个高维空间中，还有多少其他方向可以对应一个词的许多其他方面的含义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "translatedText": "转换器的目的是逐步调整这些嵌入，使它们不仅仅是对单个单词进行编码，而是将一些更丰富的上下文含义融入其中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "translatedText": "我得事先声明，很多人对变压器中的关键部件--注意装置感到非常困惑，所以如果需要一些时间才能理解，也不用担心。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "translatedText": "我认为，在我们深入探讨计算细节和所有矩阵乘法之前，值得思考几个例子，来说明我们希望注意力能够实现的行为。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "translatedText": "考虑使用 \"美国真痣\"、\"一摩尔二氧化碳 \"等短语，并对 \"痣 \"进行活检。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "translatedText": "你我都知道，根据上下文，\"鼹鼠 \"一词在每种情况下都有不同的含义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "translatedText": "但是，在转换器的第一步，即分解文本并将每个标记与一个向量关联起来之后，与 mole 关联的向量在所有这些情况下都是一样的，因为最初的标记嵌入实际上是一个查找表，并没有参考上下文。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "translatedText": "只有在转换器的下一步中，周围的嵌入才有机会将信息传递到这个嵌入中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "translatedText": "你可能会想到，在这个嵌入空间中，存在着多个不同的方向，编码着 \"鼹鼠 \"这个词的多种不同含义，而训练有素的注意力区块会根据上下文，计算出你需要在通用嵌入中添加哪些内容，才能将其移动到这些特定的方向之一。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "input": "To take another example, consider the embedding of the word tower.",
  "translatedText": "再来看一个例子，比如单词“Tower”的嵌入向量，",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "translatedText": "这大概是空间中某个非常通用的、非特定的方向，与许多其他高大的名词相关联。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "translatedText": "如果这个词的前面紧跟着埃菲尔铁塔，你可以想象一下，希望机制更新这个向量，使其指向一个更具体地编码埃菲尔铁塔的方向，或许与巴黎、法国和钢铁制品相关的向量有关。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "translatedText": "如果前面还有微型一词，那么矢量就应该进一步更新，使其不再与高大的事物相关。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "translatedText": "注意力区块不仅仅是完善一个词的含义，它还能让模型将一个嵌入式编码的信息转移到另一个嵌入式编码的信息中，这些嵌入式编码的信息可能相距甚远，而且可能包含比单个词丰富得多的信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "translatedText": "我们在上一章中看到，在所有向量（包括许多不同的注意力区块）流经网络后，预测下一个标记所进行的计算完全是序列中最后一个向量的函数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "translatedText": "例如，想象一下，您输入的文本是整本推理小说的大部分内容，一直到接近结尾的部分，其中写道：\"因此，凶手是......\"。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "translatedText": "如果模型要准确预测下一个单词，那么序列中的最后一个向量--它一开始只是嵌入了 \"是 \"这个单词--必须经过所有注意力区块的更新，才能代表比任何单个单词都要多得多的信息，并以某种方式编码整个上下文窗口中与预测下一个单词相关的所有信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "input": "To step through the computations, though, let's take a much simpler example.",
  "translatedText": "但为了逐步解析计算过程，我们先看一个更简单的例子。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "translatedText": "想象一下，输入中包含这样一个短语：一只毛茸茸的蓝色动物在葱郁的森林中漫步。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "translatedText": "目前，假设我们唯一关心的更新类型是让形容词调整相应名词的含义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "translatedText": "我接下来要描述的就是我们所说的单个注意头，稍后我们将看到注意块是如何由多个并行运行的不同注意头组成的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "translatedText": "同样，每个单词的初始嵌入都是某个高维向量，只编码该特定单词的含义，没有上下文。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "input": "Actually, that's not quite true.",
  "translatedText": "实际上，这并不完全正确。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "input": "They also encode the position of the word.",
  "translatedText": "它们还编码了词的位置。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "translatedText": "位置编码的方式还有很多，但现在您只需知道，这个向量的条目足以告诉您这个词是什么，以及它在上下文中的位置。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "translatedText": "让我们用字母 E 来表示这些嵌入。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "translatedText": "我们的目标是通过一系列计算，生成一组新的精炼嵌入，例如，与名词相对应的嵌入已经从相应的形容词中摄取了意义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "translatedText": "在深度学习游戏中，我们希望涉及的大部分计算看起来像矩阵-向量乘积，其中的矩阵充满了可调整的权重，模型将根据数据学习这些权重。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "translatedText": "说白了，我编造这个形容词更新名词的例子，只是为了说明你能想象到的注意力集中者的行为类型。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "translatedText": "与许多深度学习一样，真正的行为更难解析，因为它基于对大量参数的调整和调试，以最小化某些成本函数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "translatedText": "我认为，当我们在这个过程中步入各种充满参数的矩阵时，如果能有一个想象中的例子来帮助我们更具体地理解这个过程，那将会非常有帮助。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "translatedText": "在这个过程的第一步，你可以想象每个名词，比如生物，都会问这样一个问题：嘿，我面前有形容词吗？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "translatedText": "至于 \"蓬松 \"和 \"蓝色 \"这两个词，每个人都能回答：是的，我是形容词，我就在那个位置上。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "translatedText": "这个问题以某种方式被编码为另一个向量、另一个数字列表，我们称之为这个词的查询。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "translatedText": "这个查询向量的维度比嵌入向量要小得多，比如说 128 维。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "translatedText": "计算这个查询的过程就像把某个矩阵（我把它称为 wq）乘以嵌入。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "translatedText": "稍微压缩一下，让我们把查询向量写成 q，然后无论何时你看到我在像这样的箭头旁边放一个矩阵，它都表示用这个矩阵乘以箭头起点的向量，就得到了箭头终点的向量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "translatedText": "在这种情况下，将该矩阵与上下文中的所有嵌入相乘，就能为每个标记生成一个查询向量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "translatedText": "这个矩阵的条目是模型的参数，这意味着真正的行为是从数据中学习出来的，而在实践中，要解析这个矩阵在特定注意力头中的作用是很有挑战性的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "translatedText": "但为了方便起见，我们可以想象一个希望它能学会的例子，假设这个查询矩阵将名词的嵌入映射到这个较小的查询空间中的某些方向，而这些方向以某种方式编码了在前面位置寻找形容词的概念。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "input": "As to what it does to other embeddings, who knows?",
  "translatedText": "至于它对其他嵌入向量做什么，我们不知道。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "translatedText": "也许它同时试图用这些实现一些其他的目标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "input": "Right now, we're laser focused on the nouns.",
  "translatedText": "现在，我们的注意力集中在名词上。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "translatedText": "与此同时，与之相关的是第二个矩阵，称为密钥矩阵，也是与每个嵌入式相乘的矩阵。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "input": "This produces a second sequence of vectors that we call the keys.",
  "translatedText": "这会生成一个我们称之为键的向量序列。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "translatedText": "从概念上讲，你可以把键想象成是潜在的查询回答者。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "translatedText": "这个密钥矩阵也充满了可调整的参数，就像查询矩阵一样，它将嵌入向量映射到同样较小的维空间中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "translatedText": "当键与查询密切对齐时，你可以将键视为与查询相匹配。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "translatedText": "在我们的例子中，你可以想象关键矩阵会将蓬松和蓝色等形容词映射到与生物一词所产生的查询密切相关的向量上。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "translatedText": "要衡量每个密钥与每个查询的匹配程度，需要计算每个可能的密钥-查询对之间的点积。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "translatedText": "我喜欢把网格想象成一堆点，其中较大的点对应较大的点乘积，也就是键和查询对齐的地方。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "translatedText": "在我们的形容词名词示例中，这看起来更像这样：如果 fluffy 和 blue 产生的键确实与 creature 产生的查询密切相关，那么这两个点的点积将是一些大的正数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "translatedText": "用机器学习人员的行话说，这意味着 \"蓬松 \"和 \"蓝色 \"的嵌入与 \"生物 \"的嵌入相关联。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "translatedText": "相比之下，\"the \"等其他词的关键字与 \"creature \"查询词之间的点乘积将是一个很小的或负的值，这反映出它们之间没有关联。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "translatedText": "因此，我们就有了这样一个数值网格，它可以是负无穷大到无穷大之间的任何实数，从而为每个单词与更新其他单词含义的相关性打分。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "translatedText": "我们使用这些分数的方法是，根据相关性对每一列进行加权求和。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "translatedText": "因此，我们不希望数值范围从负无穷大到无穷大，而是希望这些列中的数字介于 0 和 1 之间，并且每列相加等于 1，就像概率分布一样。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "translatedText": "如果你从上一章继续阅读，就知道我们接下来要做什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "translatedText": "我们会按列计算 softmax 函数以标准化这些值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "translatedText": "在我们的图片中，对所有列应用 softmax 后，我们将用这些归一化值填充网格。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "translatedText": "这时，你可以把每一列看作是根据左边的单词与顶部相应数值的相关程度来赋予权重。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "input": "We call this grid an attention pattern.",
  "translatedText": "我们将这种网格称为“注意力模式”。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "translatedText": "现在，如果你看看变压器的原始论文，就会发现他们有一种非常简洁的写法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "translatedText": "在这里，变量 q 和 k 分别代表查询向量和密钥向量的完整数组，也就是将嵌入向量与查询矩阵和密钥矩阵相乘后得到的小向量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "translatedText": "分子中的这个表达式是表示键对和查询之间所有可能点乘网格的一种非常简洁的方法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "translatedText": "我没有提到的一个技术细节是，为了保证数值的稳定性，将所有这些值除以关键字查询空间维度的平方根是非常有用的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "translatedText": "那么这个包裹着完整表达式的 softmax 就可以理解为逐列应用。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "input": "As to that v term, we'll talk about it in just a second.",
  "translatedText": "至于那个 V 项，我们稍后再讲。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "translatedText": "在此之前，还有一个我之前没有提到的技术细节。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "translatedText": "在训练过程中，当你在一个给定的文本示例上运行这个模型时，所有的权重都会根据它对该段落中真正的下一个单词所赋予的概率的高低而稍作调整，以对其进行奖励或惩罚，如果你同时让它预测该段落中每个最初的单词子序列之后的每个可能的下一个单词，那么整个训练过程就会变得更加高效。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "translatedText": "例如，对于我们一直关注的短语，也可以预测 creature 后面有哪些词，the 后面有哪些词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "translatedText": "这样一来，一个训练样本就能提供更多的学习机会。 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "translatedText": "就我们的注意力模式而言，这意味着永远不要让后面的词语影响前面的词语，否则它们就会泄露接下来的答案。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "translatedText": "这意味着，我们希望所有这些点，即代表后来代币影响先前代币的点，以某种方式强制为零。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "translatedText": "最简单的方法可能是将它们设为零，但如果这样做，列的总和就不再是 1，它们就不会被归一化。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "translatedText": "因此，一种常见的方法是在应用 softmax 之前，将所有条目设置为负无穷大。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "translatedText": "如果这样做了，那么在应用 softmax 之后，所有这些都会变为零，但列仍会保持归一化。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "input": "This process is called masking.",
  "translatedText": "这就是所谓的掩蔽过程。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "translatedText": "有些版本的注意力可以不使用这种方法，但在我们的 GPT 示例中，尽管在训练阶段使用这种方法比将其作为聊天机器人或类似功能运行更有意义，但我们还是会一直使用这种屏蔽方法，以防止后面的标记影响前面的标记。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "translatedText": "关于这种注意力模式的另一个值得思考的事实是，它的大小等于上下文大小的平方。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "translatedText": "因此，这就是为什么上下文大小会成为大型语言模型的一个巨大瓶颈，而且扩大其规模并非易事。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "translatedText": "正如你所想象的那样，出于对越来越大的上下文窗口的渴望，近年来注意力机制发生了一些变化，目的是使上下文更具可扩展性，但在这里，你和我都将专注于最基本的东西。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "translatedText": "好的，很好，计算这个模式可以让模型推断出哪些词与哪些其他词相关。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "translatedText": "现在，您需要对嵌入进行实际更新，允许单词向与之相关的其他单词传递信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "translatedText": "例如，你想让 \"蓬松 \"的嵌入以某种方式导致 \"生物 \"发生变化，从而将其移动到这个 12000 维嵌入空间的另一部分，更具体地编码 \"蓬松 \"生物。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "translatedText": "我在这里要做的是，首先向大家展示最直接的方法，不过在多头关注的背景下，这种方法会有一些微小的变化。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "translatedText": "最直接的方法是使用第三个矩阵，也就是我们所说的值矩阵，乘以第一个单词的嵌入，例如 Fluffy。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "translatedText": "这样做的结果就是所谓的 \"值向量\"，它是你添加到第二个单词嵌入中的东西，在本例中就是你添加到 \"Creature \"嵌入中的东西。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "translatedText": "因此，这个值向量就存在于和嵌入向量一样的，非常高维的空间中。 ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "translatedText": "将这个值矩阵乘以一个词的嵌入，你可以认为这是在说，如果这个词与调整其他词的含义有关，那么为了反映这一点，其他词的嵌入应该增加什么？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "translatedText": "回过头来看我们的图表，让我们先把所有的键和查询放在一边，因为在计算完注意力模式后，你就完成了这些工作，然后你要把这个值矩阵与每个嵌入相乘，生成一个值向量序列。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "translatedText": "你可以将这些值向量视作在某种程度上与它们相对应的“键”有关。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "translatedText": "对于图中的每一列，你都要用每一个值向量乘以该列中相应的权重。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "translatedText": "例如，在这里，在 \"Creature \"的嵌入下，你将添加大部分的 \"Fluffy \"和 \"Blue \"的值向量，而所有其他的值向量都被清零，或者至少几乎被清零。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "translatedText": "最后，更新与这一列相关的嵌入的方法是，在对 \"Creature \"进行无上下文含义编码之前，将这一列中所有重新缩放的值相加，产生一个想要添加的变化（我将标为 delta-e），然后将其添加到原始嵌入中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "translatedText": "希望最终得到的是一个更精细的矢量，编码出更丰富的语境含义，比如一个毛茸茸的蓝色生物。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "translatedText": "当然，你也不能只对一个嵌入式做这样的处理，你要对图片中的所有列应用相同的加权和，产生一连串的变化，再把所有这些变化加到相应的嵌入式中，就会从注意力区块中跳出一连串更精细的嵌入式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "translatedText": "从宏观角度来看，我们讨论的整个过程构成了所谓的“单头注意力”机制。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "translatedText": "正如我迄今为止所描述的那样，这个过程由三个不同的矩阵参数化，所有矩阵都充满了可调整的参数，即键、查询和值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "translatedText": "我想花点时间继续我们在上一章开始的记分方法，即使用 GPT-3 中的数字计算模型参数的总数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "translatedText": "这些密钥和查询矩阵各有 12 288 列（与嵌入维度相匹配）和 128 行（与较小的密钥查询空间维度相匹配）。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "translatedText": "这给我们每个矩阵增加了大约 150 万个参数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "translatedText": "相比之下，如果你看一下那个值矩阵，根据我目前的描述，它是一个有 12,288 列和 12,288 行的正方形矩阵，因为它的输入和输出都在这个非常大的嵌入空间中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "input": "If true, that would mean about 150 million added parameters.",
  "translatedText": "如果这是真的，那就意味着要增加大约 1500 万个参数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "input": "And to be clear, you could do that.",
  "translatedText": "当然，你可以这样做，",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "translatedText": "与键和查询相比，你可以在值映射中使用更多的参数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "translatedText": "但在实际操作中，如果能使值映射的参数数与键和查询的参数数相同，则效率会高得多。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "translatedText": "特别是在同时运行多个注意力机制的场景下，这一点尤为重要。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "translatedText": "具体来说，值映射实际上是两个小矩阵乘积的形式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "translatedText": "从概念上讲，我仍然鼓励你们思考整体的线性地图，一个有输入和输出的地图，两者都在这个更大的嵌入空间中，例如，将蓝色嵌入到你会添加到名词中的蓝色方向。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "translatedText": "只是行数较少，通常与关键字查询空间大小相同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "translatedText": "可以理解为，它将较大的嵌入向量映射到了一个更小的空间。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "translatedText": "虽然这不是通用的术语，但我决定将其称作“值降维矩阵”。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "translatedText": "第二个矩阵会从这个较小的空间映射回嵌入空间，产生用于实际更新的向量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "translatedText": "我将其称为“值升维矩阵”，这个命名同样非传统。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "input": "The way that you would see this written in most papers looks a little different.",
  "translatedText": "在大多数论文中，你会看到的描述方式可能和我说的有所不同，",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "input": "I'll talk about it in a minute.",
  "translatedText": "我稍后会解释原因。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "translatedText": "但我个人认为，那种描述方式可能会让概念理解变得更加混乱。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "translatedText": "用线性代数的术语来说，我们基本上是在限制整个值映射为低阶变换。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "translatedText": "回过头来看参数数量，所有这四个矩阵的大小都是一样的，将它们全部加起来，我们可以得到一个注意头的大约 630 万个参数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "translatedText": "顺便提一下，为了更准确一点，目前所描述的一切都是人们所说的自我注意头，以区别于其他模型中出现的一种叫做交叉注意的变体。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "translatedText": "这与我们的 GPT 例子并不相关，但如果你好奇的话，交叉关注涉及处理两种不同类型数据的模型，比如一种语言的文本和另一种语言的文本，后者是正在生成的翻译的一部分，或者是语音音频输入和正在进行的转录。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "input": "A cross-attention head looks almost identical.",
  "translatedText": "交叉注意力机制看起来几乎和自注意力机制一样。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "input": "The only difference is that the key and query maps act on different data sets.",
  "translatedText": "唯一的区别是，键和查询映射在交叉注意力机制中会作用于不同的数据集。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "translatedText": "例如，在一个进行翻译的模型中，键可能来自一种语言，而查询则来自另一种语言，注意力模式可以描述一种语言中的哪些词与另一种语言中的哪些词相对应。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "translatedText": "在这种情况下，通常不会出现掩码，因为并不存在后来的令牌会影响先前令牌的概念。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "translatedText": "不过，如果你专注于自我关注，如果你理解了目前的一切，如果你就此打住，你就会领悟到关注的真谛。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "translatedText": "我们剩下要讲的就是这个过程需要做多次的原因。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "translatedText": "在我们的中心示例中，我们重点讨论了形容词更新名词的问题，当然，上下文有很多不同的方式可以影响一个单词的含义。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "translatedText": "如果 \"他们撞坏了 \"这个词出现在 \"汽车 \"这个词之前，那么它就会对汽车的形状和结构产生影响。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "input": "And a lot of associations might be less grammatical.",
  "translatedText": "而且很多时候，这种联系可能并不遵循语法规则。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "translatedText": "如果 \"巫师 \"一词与 \"哈利 \"出现在同一段落中的任何地方，则表明这可能指的是哈利-波特，而如果该段落中出现了 \"女王\"、\"苏塞克斯 \"和 \"威廉 \"等词，那么 \"哈利 \"的嵌入可能应该更新为指的是王子。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "translatedText": "对于你可能想象到的每一种不同类型的上下文更新，这些关键矩阵和查询矩阵的参数都会不同，以捕捉不同的关注模式，而我们的值映射的参数也会根据应该添加到嵌入中的内容而不同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "translatedText": "同样，在实践中，这些映射的真实行为更难解释，权重的设置是为了满足模型的任何需要，以最好地实现预测下一个标记的目标。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "translatedText": "正如我之前所说，我们所描述的都是单头注意力，而转换器内的完整注意力区块由所谓的多头注意力组成，你可以并行运行大量这些操作，每个操作都有自己不同的键查询和值映射。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "translatedText": "例如，GPT-3 在每个块中都使用了 96 个注意力头。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "translatedText": "考虑到每个人的情况都已经有点混乱，这肯定会让你头疼不已。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "translatedText": "为了清楚地说明这一切，这意味着你有 96 个不同的关键字和查询矩阵，产生 96 种不同的注意模式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "translatedText": "然后，每个头部都有自己不同的值矩阵，用来产生 96 个值向量序列。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "translatedText": "所有这些都通过使用对应的注意力模式作为权重进行加总。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "translatedText": "这意味着，对于上下文中的每个位置、每个标记，每一个标题都会产生一个建议的变化，以添加到该位置的嵌入中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "translatedText": "因此，你要做的就是把所有这些建议的修改加在一起，每个标题一个，然后把结果加到该位置的原始嵌入中。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "translatedText": "这里的全部总和将是这个多头注意力区块输出的一个片段，是它的另一端弹出的精炼嵌入中的一个。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "translatedText": "再次强调，这需要考虑的东西很多，所以如果需要一些时间来消化，也完全不用担心。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "translatedText": "总体思路是，通过并行运行多个不同的 \"头\"，让模型有能力学习语境改变意义的多种不同方式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "translatedText": "我们对 96 个头的参数数量进行了统计，每个头都包括这四个矩阵的各自变体，每个多头关注块最终都有大约 6 亿个参数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "translatedText": "对于那些想深入了解 transformer 的人，这里有个小插曲我必须提一下。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "translatedText": "你还记得我说过，价值映射被分解成两个不同的矩阵，我把它们分别称为价值向下矩阵和价值向上矩阵。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "translatedText": "我的构思表明，你会在每个注意力头中看到这对矩阵，你完全可以这样实现它。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "input": "That would be a valid design.",
  "translatedText": "这种设计是可行的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "translatedText": "但是在论文中，以及在实践中的实现方式看起来有些不同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "translatedText": "每个磁头的所有这些增值矩阵都被装订在一个巨大的矩阵中，我们称之为输出矩阵，与整个多磁头注意力模块相关联。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "translatedText": "当你看到人们提到某个注意力头的价值矩阵时，他们通常只是指第一步，也就是我所说的价值向下投射到更小空间的那一步。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "input": "For the curious among you, I've left an on-screen note about it.",
  "translatedText": "对那些好奇的人，我在这里注明了这一点。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "translatedText": "这个细节有可能会分散对主要概念要点的注意力，但我还是想把它说出来，以便你在其他资料中读到这个问题时能有所了解。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "translatedText": "抛开所有技术上的细微差别不谈，在上一章的预览中，我们看到了流经变压器的数据并不只是流经一个单一的注意块。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "translatedText": "首先，数据还会经过其他被称为多层感知器的操作。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "input": "We'll talk more about those in the next chapter.",
  "translatedText": "我们会在下一章详细介绍这个。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "translatedText": "然后，数据会反复经过这两种操作的多个副本。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "translatedText": "这意味着，一个特定的单词在吸收了它的一些语境之后，还有更多的机会受到它周围更细微的环境的影响。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "translatedText": "网络越往下延伸，每个嵌入点从所有其他嵌入点吸收的意义就会越来越多，而其他嵌入点本身的意义也会越来越细微，因此我们希望有能力对给定输入进行更高层次、更抽象的编码，而不仅仅是描述和语法结构。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "translatedText": "比如情感、语气、是否是一首诗、作品中蕴含的科学真理等等。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "translatedText": "再回过头来看看我们的记分方法，GPT-3 包含 96 个不同的层，因此关键查询和值参数的总数再乘以 96，总和就接近 580 亿个不同的参数，用于所有注意头。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "translatedText": "这的确是个大数字，但它只占网络总容量 1,750 亿的三分之一。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "translatedText": "因此，尽管注意力得到了所有的关注，但大部分参数都来自这些步骤之间的区块。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "translatedText": "在下一章中，你和我将会更多地讨论这些其他模块，也会更多地讨论培训过程。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "translatedText": "注意力机制之所以成功，很大一部分原因并不在于它能实现什么特定的行为，而在于它的可并行性极强，这意味着你可以使用 GPU 在短时间内运行大量的计算。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "translatedText": "在过去的一二十年里，深度学习的一个重要经验就是，单凭规模似乎就能在模型性能上带来巨大的质的提升，因此，可并行架构能让你做到这一点，这也是一个巨大的优势。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "translatedText": "如果你想了解更多关于这个主题的信息，我在视频简介中留下了很多链接。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "translatedText": "特别是，由 Andrej Karpathy 或 Chris Ola 制作的任何内容都非常值得一看。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "translatedText": "在这段视频中，我想直接介绍当前形式的注意力，但如果你对我们如何走到这一步的更多历史以及如何重塑自己的这一想法感到好奇，我的朋友 Vivek 刚刚上传了几段视频，提供了更多这方面的信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "translatedText": "此外，\"问题的艺术 \"频道的布里特-克鲁兹（Britt Cruz）有一段非常精彩的视频，讲述了大型语言模型的历史。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "input": "Thank you.",
  "translatedText": "谢谢。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]