1
00:00:00,000 --> 00:00:04,019
在上一章中，我们开始逐步了解变压器的内部工作原理。

2
00:00:04,560 --> 00:00:07,294
这是大型语言模型以及现代人工智能

3
00:00:07,294 --> 00:00:10,200
浪潮中许多其他工具的关键技术之一。

4
00:00:10,980 --> 00:00:17,576
在本章中，你和我将深入探讨这种注意力机制是什么，

5
00:00:17,576 --> 00:00:21,700
直观地了解它是如何处理数据的。

6
00:00:26,140 --> 00:00:29,540
简单回顾一下，我希望你们记住以下重要背景。

7
00:00:30,000 --> 00:00:33,921
你我正在学习的这一模型的目标是接收一段文字，

8
00:00:33,921 --> 00:00:36,060
并预测下一个单词是什么。

9
00:00:36,860 --> 00:00:40,425
输入文本被分割成小块，我们称之为标记，

10
00:00:40,425 --> 00:00:43,428
这些标记通常是单词或单词的片段，

11
00:00:43,428 --> 00:00:48,495
但为了让你我更容易理解视频中的例子，我们不妨简化一下，

12
00:00:48,495 --> 00:00:50,560
假装标记始终只是单词。

13
00:00:51,480 --> 00:00:56,192
转换器的第一步是将每个标记与一个高维向量关联起来，

14
00:00:56,192 --> 00:00:57,700
我们称之为嵌入。

15
00:00:57,700 --> 00:01:00,865
我希望你们记住的最重要的观点是，

16
00:01:00,865 --> 00:01:04,823
在这个由所有可能的嵌入组成的高维空间中，

17
00:01:04,823 --> 00:01:07,000
方向如何与语义相对应。

18
00:01:07,680 --> 00:01:12,236
在上一章中，我们举例说明了方向如何与性别相对应，

19
00:01:12,236 --> 00:01:15,463
也就是说，在这个空间中增加某一步，

20
00:01:15,463 --> 00:01:19,640
就可以将一个阳性名词嵌入到相应的阴性名词中。

21
00:01:20,160 --> 00:01:23,642
这只是一个例子，你可以想象，在这个高维空间中，

22
00:01:23,642 --> 00:01:27,580
还有多少其他方向可以对应一个词的许多其他方面的含义。

23
00:01:28,800 --> 00:01:31,993
转换器的目的是逐步调整这些嵌入，

24
00:01:31,993 --> 00:01:35,387
使它们不仅仅是对单个单词进行编码，

25
00:01:35,387 --> 00:01:39,180
而是将一些更丰富的上下文含义融入其中。

26
00:01:40,140 --> 00:01:44,560
我得事先声明，很多人对变压器中的关键部件--注意装置感

27
00:01:44,560 --> 00:01:48,980
到非常困惑，所以如果需要一些时间才能理解，也不用担心。

28
00:01:49,440 --> 00:01:54,113
我认为，在我们深入探讨计算细节和所有矩阵乘法之前，

29
00:01:54,113 --> 00:01:59,160
值得思考几个例子，来说明我们希望注意力能够实现的行为。

30
00:02:00,140 --> 00:02:04,299
考虑使用 "美国真痣"、"一摩尔二氧化碳 "等短语，

31
00:02:04,299 --> 00:02:06,220
并对 "痣 "进行活检。

32
00:02:06,700 --> 00:02:08,736
你我都知道，根据上下文，"鼹鼠 

33
00:02:08,736 --> 00:02:10,900
"一词在每种情况下都有不同的含义。

34
00:02:11,360 --> 00:02:16,202
但是，在转换器的第一步，即分解文本并将每个标记与一个向量关

35
00:02:16,202 --> 00:02:21,044
联起来之后，与 mole 关联的向量在所有这些情况下都是一

36
00:02:21,044 --> 00:02:24,717
样的，因为最初的标记嵌入实际上是一个查找表，

37
00:02:24,717 --> 00:02:26,220
并没有参考上下文。

38
00:02:26,620 --> 00:02:29,761
只有在转换器的下一步中，周围的嵌

39
00:02:29,761 --> 00:02:33,100
入才有机会将信息传递到这个嵌入中。

40
00:02:33,820 --> 00:02:38,626
你可能会想到，在这个嵌入空间中，存在着多个不同的方向，

41
00:02:38,626 --> 00:02:42,186
编码着 "鼹鼠 "这个词的多种不同含义，

42
00:02:42,186 --> 00:02:45,391
而训练有素的注意力区块会根据上下文，

43
00:02:45,391 --> 00:02:48,773
计算出你需要在通用嵌入中添加哪些内容，

44
00:02:48,773 --> 00:02:51,800
才能将其移动到这些特定的方向之一。

45
00:02:53,300 --> 00:02:56,180
再举一个例子，考虑一下单词 "塔 "的嵌入。

46
00:02:57,060 --> 00:03:01,130
这大概是空间中某个非常通用的、非特定的方向，

47
00:03:01,130 --> 00:03:03,720
与许多其他高大的名词相关联。

48
00:03:04,020 --> 00:03:08,840
如果这个词的前面紧跟着埃菲尔铁塔，你可以想象一下，

49
00:03:08,840 --> 00:03:13,853
希望机制更新这个向量，使其指向一个更具体地编码埃菲尔

50
00:03:13,853 --> 00:03:19,060
铁塔的方向，或许与巴黎、法国和钢铁制品相关的向量有关。

51
00:03:19,920 --> 00:03:24,836
如果前面还有微型一词，那么矢量就应该进一步更新，

52
00:03:24,836 --> 00:03:27,500
使其不再与高大的事物相关。

53
00:03:29,480 --> 00:03:32,372
注意力区块不仅仅是完善一个词的含义，

54
00:03:32,372 --> 00:03:36,872
它还能让模型将一个嵌入式编码的信息转移到另一个嵌入式编码

55
00:03:36,872 --> 00:03:40,407
的信息中，这些嵌入式编码的信息可能相距甚远，

56
00:03:40,407 --> 00:03:43,300
而且可能包含比单个词丰富得多的信息。

57
00:03:43,300 --> 00:03:48,215
我们在上一章中看到，在所有向量（包括许多不

58
00:03:48,215 --> 00:03:53,130
同的注意力区块）流经网络后，预测下一个标记

59
00:03:53,130 --> 00:03:58,280
所进行的计算完全是序列中最后一个向量的函数。

60
00:03:59,100 --> 00:04:03,228
例如，想象一下，您输入的文本是整本推理小说的大部分内容，

61
00:04:03,228 --> 00:04:06,177
一直到接近结尾的部分，其中写道："因此，

62
00:04:06,177 --> 00:04:07,800
凶手是......"。

63
00:04:08,400 --> 00:04:13,355
如果模型要准确预测下一个单词，那么序列中的最后一个向量--

64
00:04:13,355 --> 00:04:18,310
它一开始只是嵌入了 "是 "这个单词--必须经过所有注意力

65
00:04:18,310 --> 00:04:22,752
区块的更新，才能代表比任何单个单词都要多得多的信息，

66
00:04:22,752 --> 00:04:27,707
并以某种方式编码整个上下文窗口中与预测下一个单词相关的所有

67
00:04:27,707 --> 00:04:28,220
信息。

68
00:04:29,500 --> 00:04:32,580
不过，要逐步完成计算，让我们举一个简单得多的例子。

69
00:04:32,980 --> 00:04:35,402
想象一下，输入中包含这样一个短语：一

70
00:04:35,402 --> 00:04:37,960
只毛茸茸的蓝色动物在葱郁的森林中漫步。

71
00:04:38,460 --> 00:04:42,485
目前，假设我们唯一关心的更新类

72
00:04:42,485 --> 00:04:46,780
型是让形容词调整相应名词的含义。

73
00:04:47,000 --> 00:04:50,467
我接下来要描述的就是我们所说的单个注意头，

74
00:04:50,467 --> 00:04:55,420
稍后我们将看到注意块是如何由多个并行运行的不同注意头组成的。

75
00:04:56,140 --> 00:05:00,038
同样，每个单词的初始嵌入都是某个高维向量，

76
00:05:00,038 --> 00:05:03,380
只编码该特定单词的含义，没有上下文。

77
00:05:04,000 --> 00:05:05,220
事实上，这并不完全正确。

78
00:05:05,380 --> 00:05:07,640
它们还对单词的位置进行编码。

79
00:05:07,980 --> 00:05:12,390
位置编码的方式还有很多，但现在您只需知道，

80
00:05:12,390 --> 00:05:16,380
这个向量的条目足以告诉您这个词是什么，

81
00:05:16,380 --> 00:05:18,900
以及它在上下文中的位置。

82
00:05:19,500 --> 00:05:21,660
让我们用字母 e 来表示这些嵌入。

83
00:05:22,420 --> 00:05:27,608
我们的目标是通过一系列计算，生成一组新的精炼嵌入，

84
00:05:27,608 --> 00:05:33,420
例如，与名词相对应的嵌入已经从相应的形容词中摄取了意义。

85
00:05:33,900 --> 00:05:37,151
在深度学习游戏中，我们希望涉及的大部分计

86
00:05:37,151 --> 00:05:41,703
算看起来像矩阵-向量乘积，其中的矩阵充满了可调整的权重，

87
00:05:41,703 --> 00:05:43,980
模型将根据数据学习这些权重。

88
00:05:44,660 --> 00:05:48,114
说白了，我编造这个形容词更新名词的例子，

89
00:05:48,114 --> 00:05:52,260
只是为了说明你能想象到的注意力集中者的行为类型。

90
00:05:52,860 --> 00:05:56,393
与许多深度学习一样，真正的行为更难解析，

91
00:05:56,393 --> 00:06:01,340
因为它基于对大量参数的调整和调试，以最小化某些成本函数。

92
00:06:01,680 --> 00:06:06,442
我认为，当我们在这个过程中步入各种充满参数的矩阵时，

93
00:06:06,442 --> 00:06:11,571
如果能有一个想象中的例子来帮助我们更具体地理解这个过程，

94
00:06:11,571 --> 00:06:13,220
那将会非常有帮助。

95
00:06:14,140 --> 00:06:18,390
在这个过程的第一步，你可以想象每个名词，比如生物，

96
00:06:18,390 --> 00:06:21,960
都会问这样一个问题：嘿，我面前有形容词吗？

97
00:06:22,160 --> 00:06:24,681
至于 "蓬松 "和 "蓝色 "这两个词，

98
00:06:24,681 --> 00:06:27,960
每个人都能回答：是的，我是形容词，我就在那个位置上。

99
00:06:28,960 --> 00:06:33,903
这个问题以某种方式被编码为另一个向量、另一个数字列表，

100
00:06:33,903 --> 00:06:36,100
我们称之为这个词的查询。

101
00:06:36,980 --> 00:06:42,020
不过，这个查询向量的维度要比嵌入向量小得多，比如 128。

102
00:06:42,940 --> 00:06:48,070
计算这个查询的过程就像把某个矩阵（我把它命名为 

103
00:06:48,070 --> 00:06:49,780
wq）乘以嵌入。

104
00:06:50,960 --> 00:06:54,602
稍微压缩一下，让我们把查询向量写成 q，

105
00:06:54,602 --> 00:06:59,154
然后无论何时你看到我在像这样的箭头旁边放一个矩阵，

106
00:06:59,154 --> 00:07:02,614
它都表示用这个矩阵乘以箭头起点的向量，

107
00:07:02,614 --> 00:07:04,800
就得到了箭头终点的向量。

108
00:07:05,860 --> 00:07:09,892
在这种情况下，将该矩阵与上下文中的所有嵌入相乘，

109
00:07:09,892 --> 00:07:12,580
就能为每个标记生成一个查询向量。

110
00:07:13,740 --> 00:07:16,973
这个矩阵的条目是模型的参数，这意味着真正的行

111
00:07:16,973 --> 00:07:19,618
为是从数据中学习出来的，而在实践中，

112
00:07:19,618 --> 00:07:23,440
要解析这个矩阵在特定注意力头中的作用是很有挑战性的。

113
00:07:23,900 --> 00:07:28,174
但为了方便起见，我们可以想象一个希望它能学会的例子，

114
00:07:28,174 --> 00:07:32,778
假设这个查询矩阵将名词的嵌入映射到这个较小的查询空间中的

115
00:07:32,778 --> 00:07:37,382
某些方向，而这些方向以某种方式编码了在前面位置寻找形容词

116
00:07:37,382 --> 00:07:38,040
的概念。

117
00:07:38,780 --> 00:07:41,440
至于它对其他嵌入式有什么影响，谁知道呢？

118
00:07:41,720 --> 00:07:44,340
也许它同时还试图通过这些来实现其他目标。

119
00:07:44,540 --> 00:07:47,160
现在，我们的重点是名词。

120
00:07:47,280 --> 00:07:50,563
与此同时，与之相关的是第二个矩阵，

121
00:07:50,563 --> 00:07:54,620
称为密钥矩阵，也是与每个嵌入式相乘的矩阵。

122
00:07:55,280 --> 00:07:58,500
这样就产生了第二个向量序列，我们称之为密钥。

123
00:07:59,420 --> 00:08:03,140
从概念上讲，你要把键看作是对查询的潜在回答。

124
00:08:03,840 --> 00:08:08,207
这个密钥矩阵也充满了可调整的参数，就像查询矩阵一样，

125
00:08:08,207 --> 00:08:11,400
它将嵌入向量映射到同样较小的维空间中。

126
00:08:12,200 --> 00:08:17,020
只要键与键之间的关系密切，就可以认为键与查询匹配。

127
00:08:17,460 --> 00:08:22,009
在我们的例子中，你可以想象关键矩阵会将蓬松和蓝色等

128
00:08:22,009 --> 00:08:26,740
形容词映射到与生物一词所产生的查询密切相关的向量上。

129
00:08:27,200 --> 00:08:30,338
要衡量每个密钥与每个查询的匹配程度，

130
00:08:30,338 --> 00:08:34,000
需要计算每个可能的密钥-查询对之间的点积。

131
00:08:34,480 --> 00:08:39,998
我喜欢把网格想象成一堆点，其中较大的点对应较大的点乘积，

132
00:08:39,998 --> 00:08:42,559
也就是键和查询对齐的地方。

133
00:08:43,280 --> 00:08:47,756
在我们的形容词名词示例中，这看起来更像这样：如果 

134
00:08:47,756 --> 00:08:51,695
fluffy 和 blue 产生的键确实与 

135
00:08:51,695 --> 00:08:55,097
creature 产生的查询密切相关，

136
00:08:55,097 --> 00:08:58,320
那么这两个点的点积将是一些大的正数。

137
00:08:59,100 --> 00:09:02,397
用机器学习人员的行话说，这意味着 "蓬松 "和 

138
00:09:02,397 --> 00:09:05,420
"蓝色 "的嵌入与 "生物 "的嵌入相关联。

139
00:09:06,040 --> 00:09:09,616
相比之下，"the "等其他词的关键字与 

140
00:09:09,616 --> 00:09:14,556
"creature "查询词之间的点积将是一个小值或负值，

141
00:09:14,556 --> 00:09:16,600
反映出两者之间互不相关。

142
00:09:17,700 --> 00:09:20,754
因此，我们就有了这样一个数值网格，

143
00:09:20,754 --> 00:09:24,347
它可以是负无穷大到无穷大之间的任何实数，

144
00:09:24,347 --> 00:09:28,480
从而为每个单词与更新其他单词含义的相关性打分。

145
00:09:29,200 --> 00:09:35,780
我们使用这些分数的方法是，根据相关性对每一列进行加权求和。

146
00:09:36,520 --> 00:09:40,466
因此，我们不希望数值范围从负无穷大到无穷大，

147
00:09:40,466 --> 00:09:44,592
而是希望这些列中的数字介于 0 和 1 之间，

148
00:09:44,592 --> 00:09:48,180
并且每列相加等于 1，就像概率分布一样。

149
00:09:49,280 --> 00:09:52,220
如果你是从上一章来的，你就知道我们当时需要做什么。

150
00:09:52,620 --> 00:09:57,300
我们计算每一列的 softmax，使数值正常化。

151
00:10:00,060 --> 00:10:03,685
在我们的图片中，对所有列应用 softmax 后，

152
00:10:03,685 --> 00:10:05,860
我们将用这些归一化值填充网格。

153
00:10:06,780 --> 00:10:10,680
这时，你可以把每一列看作是根据左边的单

154
00:10:10,680 --> 00:10:14,580
词与顶部相应数值的相关程度来赋予权重。

155
00:10:15,080 --> 00:10:16,840
我们称这种网格为注意力模式。

156
00:10:18,080 --> 00:10:20,450
现在，如果你看看变压器的原始论文，

157
00:10:20,450 --> 00:10:22,820
就会发现他们有一种非常简洁的写法。

158
00:10:23,880 --> 00:10:29,260
在这里，变量 q 和 k 分别代表查询向量和密钥向量的完整数

159
00:10:29,260 --> 00:10:34,640
组，也就是将嵌入向量与查询矩阵和密钥矩阵相乘后得到的小向量。

160
00:10:35,160 --> 00:10:39,089
分子中的这个表达式是表示键对和查询之间

161
00:10:39,089 --> 00:10:43,020
所有可能点乘网格的一种非常简洁的方法。

162
00:10:44,000 --> 00:10:48,698
我没有提到的一个技术细节是，为了保证数值的稳定性，

163
00:10:48,698 --> 00:10:53,960
将所有这些值除以关键字查询空间维度的平方根是很有帮助的。

164
00:10:54,480 --> 00:10:58,693
那么这个包裹着完整表达式的 softmax 

165
00:10:58,693 --> 00:11:00,800
就可以理解为逐列应用。

166
00:11:01,640 --> 00:11:04,700
至于 "V "这个词，我们稍后再谈。

167
00:11:05,020 --> 00:11:08,460
在此之前，还有一个技术细节，到目前为止我都没有提及。

168
00:11:09,040 --> 00:11:13,853
在训练过程中，当你在一个给定的文本示例上运行这个模型时，

169
00:11:13,853 --> 00:11:18,323
所有的权重都会根据它对该段落中真正的下一个单词所赋予

170
00:11:18,323 --> 00:11:22,276
的概率的高低而稍作调整，以对其进行奖励或惩罚，

171
00:11:22,276 --> 00:11:26,746
如果你同时让它预测该段落中每个最初的单词子序列之后的

172
00:11:26,746 --> 00:11:31,560
每个可能的下一个单词，那么整个训练过程就会变得更加高效。

173
00:11:31,940 --> 00:11:35,072
例如，对于我们一直关注的短语，也可以预测 

174
00:11:35,072 --> 00:11:39,100
creature 后面有哪些词，the 后面有哪些词。

175
00:11:39,940 --> 00:11:43,830
这一点非常好，因为它意味着原本只有一个训练示例的内容，

176
00:11:43,830 --> 00:11:45,560
实际上可以充当多个示例。

177
00:11:46,100 --> 00:11:50,968
就我们的注意力模式而言，这意味着永远不要让后面的

178
00:11:50,968 --> 00:11:56,040
词语影响前面的词语，否则它们就会泄露接下来的答案。

179
00:11:56,560 --> 00:12:00,481
这意味着，我们希望所有这些点，即代表后来

180
00:12:00,481 --> 00:12:04,600
代币影响先前代币的点，以某种方式强制为零。

181
00:12:05,920 --> 00:12:09,239
最简单的方法可能是将它们设置为零，但如果这样做，

182
00:12:09,239 --> 00:12:12,420
列的总和就不再是 1 了，它们就不会被归一化。

183
00:12:13,120 --> 00:12:17,053
因此，一种常见的方法是在应用 softmax 之前，

184
00:12:17,053 --> 00:12:19,020
将所有条目设置为负无穷大。

185
00:12:19,680 --> 00:12:22,680
如果这样做了，那么在应用 softmax 之后，

186
00:12:22,680 --> 00:12:25,180
所有这些都会变为零，但列仍会保持归一化。

187
00:12:26,000 --> 00:12:27,540
这一过程称为遮蔽。

188
00:12:27,540 --> 00:12:29,996
有些版本的注意力可以不使用这种方法，

189
00:12:29,996 --> 00:12:33,408
但在我们的 GPT 示例中，尽管在训练阶段使用这种

190
00:12:33,408 --> 00:12:36,956
方法比将其作为聊天机器人或其他类似程序运行更有意义，

191
00:12:36,956 --> 00:12:39,276
但我们还是会一直使用这种屏蔽方法，

192
00:12:39,276 --> 00:12:41,460
以防止后面的标记影响前面的标记。

193
00:12:42,480 --> 00:12:46,654
关于这种注意力模式的另一个值得思考的事实是，

194
00:12:46,654 --> 00:12:49,500
它的大小等于上下文大小的平方。

195
00:12:49,900 --> 00:12:52,693
因此，这就是为什么上下文大小会成为大型语言

196
00:12:52,693 --> 00:12:55,620
模型的一个巨大瓶颈，而且扩大其规模并非易事。

197
00:12:56,300 --> 00:13:01,993
正如你所想象的那样，出于对越来越大的上下文窗口的渴望，

198
00:13:01,993 --> 00:13:08,320
近年来注意力机制出现了一些变化，目的是使上下文更具可扩展性。

199
00:13:10,560 --> 00:13:13,020
好的，很好，计算这个模式可以让模

200
00:13:13,020 --> 00:13:15,480
型推断出哪些词与哪些其他词相关。

201
00:13:16,020 --> 00:13:19,119
现在，您需要对嵌入进行实际更新，

202
00:13:19,119 --> 00:13:22,800
允许单词向与之相关的其他单词传递信息。

203
00:13:22,800 --> 00:13:26,394
例如，你想让 "蓬松 "的嵌入以某种方式导致 

204
00:13:26,394 --> 00:13:30,457
"生物 "发生变化，从而将其移动到这个 12000 

205
00:13:30,457 --> 00:13:34,520
维嵌入空间的另一部分，更具体地编码 "蓬松 "生物。

206
00:13:35,460 --> 00:13:39,715
我在这里要做的是，首先向大家展示一种最直接的方法，

207
00:13:39,715 --> 00:13:43,460
尽管在多头关注的情况下，这种方法会稍作修改。

208
00:13:44,080 --> 00:13:48,782
最直接的方法是使用第三个矩阵，也就是我们所说的值矩阵，

209
00:13:48,782 --> 00:13:52,440
乘以第一个单词的嵌入，例如 Fluffy。

210
00:13:53,300 --> 00:13:55,687
这样做的结果就是所谓的 "值向量"，

211
00:13:55,687 --> 00:13:58,074
它是你添加到第二个单词嵌入中的东西，

212
00:13:58,074 --> 00:14:01,920
在本例中就是你添加到 "Creature "嵌入中的东西。

213
00:14:02,600 --> 00:14:07,000
因此，这个值向量和嵌入向量一样，都处于非常高的维空间中。

214
00:14:07,460 --> 00:14:11,958
将这个值矩阵乘以一个词的嵌入，可以认为是说：

215
00:14:11,958 --> 00:14:15,434
如果这个词与调整其他词的含义有关，

216
00:14:15,434 --> 00:14:21,160
那么为了反映这一点，应该在其他词的嵌入中添加什么内容呢？

217
00:14:22,140 --> 00:14:27,073
回过头来看我们的图表，让我们先把所有的键和查询放在一边，

218
00:14:27,073 --> 00:14:31,126
因为在计算完注意力模式后，你就完成了这些工作，

219
00:14:31,126 --> 00:14:36,060
然后你要把这个值矩阵与每个嵌入相乘，生成一个值向量序列。

220
00:14:37,120 --> 00:14:41,120
你可以把这些值向量看作是与相应的键相关联的。

221
00:14:42,320 --> 00:14:49,240
对于图中的每一列，你都要用每一个值向量乘以该列中相应的权重。

222
00:14:50,080 --> 00:14:53,623
例如，在这里，在 "Creature "的嵌入下，

223
00:14:53,623 --> 00:14:57,308
你将添加大部分的 "Fluffy "和 "Blue 

224
00:14:57,308 --> 00:15:01,560
"的值向量，而所有其他的值向量都被清零，或者至少几乎被清零。

225
00:15:02,120 --> 00:15:05,344
最后，更新与这一列相关的嵌入的方法是，

226
00:15:05,344 --> 00:15:09,926
在对 "Creature "进行无上下文含义编码之前，

227
00:15:09,926 --> 00:15:12,641
将这一列中所有重新缩放的值相加，

228
00:15:12,641 --> 00:15:17,053
产生一个想要添加的变化（我将标为 delta-e），

229
00:15:17,053 --> 00:15:19,260
然后将其添加到原始嵌入中。

230
00:15:19,680 --> 00:15:22,440
希望最终得到的是一个更精细的矢量，

231
00:15:22,440 --> 00:15:26,500
编码出更丰富的语境含义，比如一个毛茸茸的蓝色生物。

232
00:15:27,380 --> 00:15:31,174
当然，你也不能只对一个嵌入式做这样的处理，

233
00:15:31,174 --> 00:15:36,233
你要对图片中的所有列应用相同的加权和，产生一连串的变化，

234
00:15:36,233 --> 00:15:39,485
再把所有这些变化加到相应的嵌入式中，

235
00:15:39,485 --> 00:15:43,460
就会从注意力区块中跳出一连串更精细的嵌入式。

236
00:15:44,860 --> 00:15:49,100
把镜头拉远，整个过程就像你所说的 "单头关注"。

237
00:15:49,600 --> 00:15:54,886
正如我迄今为止所描述的那样，这个过程由三个不同的矩阵参数化，

238
00:15:54,886 --> 00:15:58,940
所有矩阵都充满了可调整的参数，即键、查询和值。

239
00:15:59,500 --> 00:16:03,584
我想花点时间继续我们在上一章开始的记分方法，

240
00:16:03,584 --> 00:16:08,040
即使用 GPT-3 中的数字计算模型参数的总数。

241
00:16:09,300 --> 00:16:12,858
这些密钥和查询矩阵各有 12 288 

242
00:16:12,858 --> 00:16:16,041
列（与嵌入维度相匹配）和 128 

243
00:16:16,041 --> 00:16:19,600
行（与较小的密钥查询空间维度相匹配）。

244
00:16:20,260 --> 00:16:24,220
这就为我们提供了每一个额外的 150 万左右的参数。

245
00:16:24,860 --> 00:16:30,079
相比之下，如果你看一下那个值矩阵，根据我目前的描述，

246
00:16:30,079 --> 00:16:34,696
它是一个有 12,288 列和 12,288 

247
00:16:34,696 --> 00:16:39,916
行的正方形矩阵，因为它的输入和输出都在这个非常大的嵌

248
00:16:39,916 --> 00:16:40,920
入空间中。

249
00:16:41,500 --> 00:16:45,140
如果属实，这将意味着增加约 1.5 亿个参数。

250
00:16:45,660 --> 00:16:47,300
说白了，你可以这么做。

251
00:16:47,420 --> 00:16:51,740
与键和查询相比，你可以在值映射中使用更多的参数。

252
00:16:52,060 --> 00:16:56,298
但在实际操作中，如果能使值映射的参数数

253
00:16:56,298 --> 00:17:00,760
与键和查询的参数数相同，则效率会高得多。

254
00:17:01,460 --> 00:17:05,160
在并行运行多个注意力头的情况下，这一点尤为重要。

255
00:17:06,240 --> 00:17:10,099
其原理是将值映射分解为两个较小矩阵的乘积。

256
00:17:11,180 --> 00:17:15,156
从概念上讲，我仍然鼓励你们思考整体的线性地图，

257
00:17:15,156 --> 00:17:19,823
一个有输入和输出的地图，两者都在这个更大的嵌入空间中，

258
00:17:19,823 --> 00:17:23,800
例如，将蓝色嵌入到你会添加到名词中的蓝色方向。

259
00:17:27,040 --> 00:17:32,760
只是行数较少，通常与关键字查询空间大小相同。

260
00:17:33,100 --> 00:17:35,683
这意味着你可以把它看作是将大型

261
00:17:35,683 --> 00:17:38,440
嵌入向量映射到一个小得多的空间。

262
00:17:39,040 --> 00:17:42,700
这不是传统的命名方式，但我要把它叫做 "降值矩阵"。

263
00:17:43,400 --> 00:17:48,045
第二个矩阵会从这个较小的空间映射回嵌入空间，

264
00:17:48,045 --> 00:17:50,580
产生用于实际更新的向量。

265
00:17:51,000 --> 00:17:54,740
我把这个矩阵称为 "价值上升矩阵"，这也不是传统的矩阵。

266
00:17:55,160 --> 00:17:58,080
你在大多数报纸上看到的写法都有些不同。

267
00:17:58,380 --> 00:17:59,520
我稍后再谈。

268
00:17:59,700 --> 00:18:02,540
在我看来，这往往会让事情在概念上更加混乱。

269
00:18:03,260 --> 00:18:06,685
用线性代数的术语来说，我们基本

270
00:18:06,685 --> 00:18:10,340
上是在限制整个值映射为低阶变换。

271
00:18:11,420 --> 00:18:15,544
回过头来看参数数量，所有这四个矩阵的大小都是一样的，

272
00:18:15,544 --> 00:18:19,986
将它们全部加起来，我们可以得到一个注意头的大约 630 

273
00:18:19,986 --> 00:18:20,780
万个参数。

274
00:18:22,040 --> 00:18:26,689
顺便提一下，为了更准确一点，目前所描述的一切都是人们所说的

275
00:18:26,689 --> 00:18:31,500
自我注意头，以区别于其他模型中出现的一种叫做交叉注意的变体。

276
00:18:32,300 --> 00:18:36,794
这与我们的 GPT 例子并不相关，但如果你好奇的话，

277
00:18:36,794 --> 00:18:40,251
交叉关注涉及处理两种不同类型数据的模型，

278
00:18:40,251 --> 00:18:43,535
比如一种语言的文本和另一种语言的文本，

279
00:18:43,535 --> 00:18:47,684
后者是正在生成的翻译的一部分，或者是语音音频输入

280
00:18:47,684 --> 00:18:49,240
和正在进行的转录。

281
00:18:50,400 --> 00:18:52,700
交叉注意头看起来几乎一模一样。

282
00:18:52,980 --> 00:18:57,400
唯一不同的是，键映射和查询映射作用于不同的数据集。

283
00:18:57,840 --> 00:19:01,837
例如，在进行翻译的模型中，键可能来自一种语言，

284
00:19:01,837 --> 00:19:05,662
而查询则来自另一种语言，注意力模式可以描述一

285
00:19:05,662 --> 00:19:09,660
种语言中的哪些词与另一种语言中的哪些词相对应。

286
00:19:10,340 --> 00:19:12,866
在这种情况下，通常不会出现掩码，

287
00:19:12,866 --> 00:19:16,340
因为并不存在后来的令牌会影响先前令牌的概念。

288
00:19:17,180 --> 00:19:21,914
不过，如果你专注于自我关注，如果你理解了到目前为止的一切，

289
00:19:21,914 --> 00:19:25,180
如果你就此打住，你就会领悟到关注的真谛。

290
00:19:25,760 --> 00:19:31,440
我们所要做的，其实就是把你这样做的意义多次重复地摆出来。

291
00:19:32,100 --> 00:19:36,092
在我们的中心示例中，我们重点讨论了形容词更新名词的问题，

292
00:19:36,092 --> 00:19:39,800
当然，上下文有很多不同的方式可以影响一个单词的含义。

293
00:19:40,360 --> 00:19:44,081
如果 "他们撞坏了 "这个词出现在 "汽车 "这个词之前，

294
00:19:44,081 --> 00:19:46,520
那么它就会对汽车的形状和结构产生影响。

295
00:19:47,200 --> 00:19:49,280
很多联想可能不那么符合语法。

296
00:19:49,760 --> 00:19:52,132
如果 "巫师 "一词与 "哈利 

297
00:19:52,132 --> 00:19:55,691
"出现在同一段落中的任何地方，则表明这可能指的是

298
00:19:55,691 --> 00:19:59,843
哈利-波特；而如果该段落中出现了 "女王"、"苏塞克斯 

299
00:19:59,843 --> 00:20:02,364
"和 "威廉 "等词，则 "哈利 

300
00:20:02,364 --> 00:20:04,440
"的嵌入可能应更新为指王子。

301
00:20:05,040 --> 00:20:08,994
对于你可能想象到的每一种不同类型的上下文更新，

302
00:20:08,994 --> 00:20:14,153
这些关键矩阵和查询矩阵的参数都会不同，以捕捉不同的关注模式，

303
00:20:14,153 --> 00:20:19,140
而我们的值映射的参数也会根据应该添加到嵌入中的内容而不同。

304
00:20:19,980 --> 00:20:23,940
同样，在实践中，这些映射的真实行为也更难解释，

305
00:20:23,940 --> 00:20:27,040
权重的设置是根据模型的需要来决定的，

306
00:20:27,040 --> 00:20:30,140
以便最好地实现预测下一个标记的目标。

307
00:20:31,400 --> 00:20:35,202
正如我之前所说，我们所描述的都是单头注意力，

308
00:20:35,202 --> 00:20:40,215
而转换器内的完整注意力块则由所谓的多头注意力组成，在这里，

309
00:20:40,215 --> 00:20:45,055
你可以并行运行大量此类操作，每个操作都有自己不同的键查询

310
00:20:45,055 --> 00:20:45,920
和值映射。

311
00:20:47,420 --> 00:20:51,700
例如，GPT-3 每个区块内使用 96 个注意头。

312
00:20:52,020 --> 00:20:56,460
考虑到每个人的情况都已经有点混乱，这肯定会让你头疼不已。

313
00:20:56,760 --> 00:21:00,291
为了清楚地说明这一切，这意味着你有 96 

314
00:21:00,291 --> 00:21:05,000
个不同的关键字和查询矩阵，产生 96 种不同的注意模式。

315
00:21:05,440 --> 00:21:09,116
然后，每个头部都有自己不同的值矩阵，

316
00:21:09,116 --> 00:21:12,180
用来产生 96 个值向量序列。

317
00:21:12,460 --> 00:21:16,680
所有这些都以相应的注意力模式作为权重相加。

318
00:21:17,480 --> 00:21:21,595
这意味着，对于上下文中的每个位置、每个标记，

319
00:21:21,595 --> 00:21:27,020
每一个标题都会产生一个建议的变化，以添加到该位置的嵌入中。

320
00:21:27,660 --> 00:21:31,570
因此，你要做的就是把所有这些建议的修改加在一起，

321
00:21:31,570 --> 00:21:35,480
每个标题一个，然后把结果加到该位置的原始嵌入中。

322
00:21:36,660 --> 00:21:43,508
这里的全部总和将是这个多头注意力区块输出的一个片段，

323
00:21:43,508 --> 00:21:47,460
是它另一端弹出的一个精炼嵌入。

324
00:21:48,320 --> 00:21:49,928
再次强调，这需要考虑的东西很多，

325
00:21:49,928 --> 00:21:52,140
所以如果需要一些时间来消化，也完全不用担心。

326
00:21:52,380 --> 00:21:57,100
总体思路是，通过并行运行多个不同的 "头"，

327
00:21:57,100 --> 00:22:01,820
让模型有能力学习语境改变意义的多种不同方式。

328
00:22:03,700 --> 00:22:07,624
我们对 96 个头的参数数量进行了统计，

329
00:22:07,624 --> 00:22:10,959
每个头都包括这四个矩阵的各自变体，

330
00:22:10,959 --> 00:22:15,080
每个多头关注块最终都有大约 6 亿个参数。

331
00:22:16,420 --> 00:22:19,110
还有一件让人有点烦的事，我真的应该提一下，

332
00:22:19,110 --> 00:22:21,800
希望你们能继续阅读更多关于变形金刚的内容。

333
00:22:22,080 --> 00:22:25,920
你还记得我说过，价值映射被分解成两个不同的矩阵，

334
00:22:25,920 --> 00:22:29,440
我把它们分别称为价值向下矩阵和价值向上矩阵。

335
00:22:29,960 --> 00:22:35,774
我的构思表明，你会在每个注意力头中看到这对矩阵，

336
00:22:35,774 --> 00:22:38,440
你完全可以这样实现它。

337
00:22:38,640 --> 00:22:39,920
这将是一个有效的设计。

338
00:22:40,260 --> 00:22:42,514
但是，你在论文中看到的写法和在

339
00:22:42,514 --> 00:22:44,920
实践中的执行方式看起来有些不同。

340
00:22:45,340 --> 00:22:51,072
每个磁头的所有这些增值矩阵都被装订在一个巨大的矩阵中，

341
00:22:51,072 --> 00:22:56,380
我们称之为输出矩阵，与整个多磁头注意力模块相关联。

342
00:22:56,820 --> 00:23:00,760
当你看到人们提到某个注意力头的价值矩阵时，

343
00:23:00,760 --> 00:23:05,826
他们通常只是指第一步，也就是我所说的价值向下投射到更小

344
00:23:05,826 --> 00:23:07,140
空间的那一步。

345
00:23:08,340 --> 00:23:11,040
为了满足好奇心，我在屏幕上留下了相关说明。

346
00:23:11,260 --> 00:23:14,172
这个细节有可能会分散对主要概念要点的注意力，

347
00:23:14,172 --> 00:23:17,745
但我还是想把它说出来，以便你在其他资料中读到这个问题时

348
00:23:17,745 --> 00:23:18,540
能有所了解。

349
00:23:19,240 --> 00:23:23,301
抛开所有技术上的细微差别不谈，在上一章的预览中，

350
00:23:23,301 --> 00:23:28,040
我们看到了流经变压器的数据并不只是流经一个单一的注意块。

351
00:23:28,640 --> 00:23:32,700
首先，它还需要经过称为多层感知器的其他操作。

352
00:23:33,120 --> 00:23:34,880
我们将在下一章详细讨论这些问题。

353
00:23:35,180 --> 00:23:39,320
然后，它会反复多次地进行这两种操作。

354
00:23:39,980 --> 00:23:44,316
这意味着，一个特定的单词在吸收了它的一些语境之后，

355
00:23:44,316 --> 00:23:48,132
还有更多的机会受到它周围更微妙的环境的影响，

356
00:23:48,132 --> 00:23:50,040
从而产生更微妙的嵌入。

357
00:23:50,940 --> 00:23:54,990
网络越往下延伸，每个嵌入点从所有其他嵌入点中吸

358
00:23:54,990 --> 00:24:00,098
收的意义就会越来越多，而这些嵌入点本身也会变得越来越细微，

359
00:24:00,098 --> 00:24:05,030
因此我们希望有能力对给定输入进行更高层次、更抽象的编码，

360
00:24:05,030 --> 00:24:07,320
而不仅仅是描述和语法结构。

361
00:24:07,880 --> 00:24:15,130
比如情感、语气、是否是一首诗、作品中蕴含的科学真理等等。

362
00:24:16,700 --> 00:24:22,783
再回过头来看看我们的记分方法，GPT-3 包含 96 

363
00:24:22,783 --> 00:24:28,641
个不同的层，因此关键查询和值参数的总数再乘以 96，

364
00:24:28,641 --> 00:24:34,500
总和略低于 580 亿个不同的参数，用于所有注意头。

365
00:24:34,980 --> 00:24:38,518
这的确是个大数字，但它只占网络总容量 

366
00:24:38,518 --> 00:24:40,940
1,750 亿的三分之一。

367
00:24:41,520 --> 00:24:44,646
因此，尽管注意力得到了所有的关注，

368
00:24:44,646 --> 00:24:48,140
但大部分参数都来自这些步骤之间的区块。

369
00:24:48,560 --> 00:24:51,845
在下一章中，你和我将会更多地讨论这些其他模块，

370
00:24:51,845 --> 00:24:53,560
也会更多地讨论培训过程。

371
00:24:54,120 --> 00:24:58,808
注意力机制之所以成功，很大一部分原因并不在于它能

372
00:24:58,808 --> 00:25:03,105
实现什么特定的行为，而在于它的可并行性极强，

373
00:25:03,105 --> 00:25:08,380
这意味着你可以使用 GPU 在短时间内运行大量的计算。

374
00:25:09,460 --> 00:25:13,123
在过去的一二十年里，深度学习的一个重要经验就是，

375
00:25:13,123 --> 00:25:16,786
单靠规模似乎就能在模型性能上带来巨大的质的提升，

376
00:25:16,786 --> 00:25:21,060
因此，可并行架构能让你做到这一点，这也是一个巨大的优势。

377
00:25:22,040 --> 00:25:25,340
如果您想了解更多相关信息，我在说明中提供了大量链接。

378
00:25:25,920 --> 00:25:27,980
尤其是 Andrej Karpathy 或 

379
00:25:27,980 --> 00:25:30,040
Chris Ola 的作品，往往是纯金打造。

380
00:25:30,560 --> 00:25:33,424
在这段视频中，我想直接介绍当前形式的注意力，

381
00:25:33,424 --> 00:25:36,419
但如果你对我们如何走到这一步的更多历史以及如何

382
00:25:36,419 --> 00:25:39,675
重塑自己的这一想法感到好奇，我的朋友 Vivek 

383
00:25:39,675 --> 00:25:42,540
刚刚上传了几段视频，提供了更多这方面的信息。

384
00:25:43,120 --> 00:25:45,743
此外，"问题的艺术 "频道的布里特-克鲁兹（Britt 

385
00:25:45,743 --> 00:25:48,460
Cruz）有一段非常精彩的视频，讲述了大型语言模型的历史。

386
00:26:04,960 --> 00:26:09,200
谢谢。

