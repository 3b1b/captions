1
00:00:00,000 --> 00:00:02,009
在上一章中，我们开始探讨

2
00:00:02,009 --> 00:00:04,019
Transformer 的内部运作机制。

3
00:00:04,560 --> 00:00:07,925
Transformer 是大语言模型中关键的技术组成部分，

4
00:00:07,925 --> 00:00:10,639
也被广泛应用于现代 AI 领域的诸多工具中。

5
00:00:10,980 --> 00:00:13,277
它首次亮相是在 2017 年一篇广为人知的论文《At

6
00:00:13,277 --> 00:00:15,574
tention is All You Need》中，

7
00:00:15,574 --> 00:00:19,840
本章我们将深入探讨这种注意力机制，

8
00:00:19,840 --> 00:00:21,700
以及可视化展示它如何处理数据。

9
00:00:26,140 --> 00:00:29,540
在此，我想快速回顾一些重要的背景信息。

10
00:00:30,000 --> 00:00:33,003
我们正在研究的模型的目标是

11
00:00:33,003 --> 00:00:36,060
读取一段文本并预测下一个词。

12
00:00:36,860 --> 00:00:40,388
输入文本被分割成我们称之为 Tokens 的小部分，

13
00:00:40,388 --> 00:00:43,035
它们通常是完整的单词或单词的一部分。

14
00:00:43,035 --> 00:00:47,290
但为了让我们在这个视频中的例子更加简单易懂，

15
00:00:47,290 --> 00:00:50,560
让我们假设 Token 总是完整的单词。

16
00:00:51,480 --> 00:00:54,590
Transformer 的第一步是将每个 Token

17
00:00:54,590 --> 00:00:57,700
与一个高维向量关联，这就是我们所说的嵌入向量。

18
00:00:57,700 --> 00:01:00,481
我希望你能理解的关键概念是，

19
00:01:00,481 --> 00:01:04,777
如何理解在所有可能的嵌入向量所构成的高维空间中，

20
00:01:04,777 --> 00:01:07,000
不同的方向能够代表不同的语义含义。

21
00:01:07,680 --> 00:01:11,766
在上一章中，我们给出了一个例子，说明了方向如何对应性别，

22
00:01:11,766 --> 00:01:15,553
即在这个空间中添加一定的变化可以从

23
00:01:15,553 --> 00:01:19,640
一个男性名词的嵌入转到对应的女性名词的嵌入。

24
00:01:19,699 --> 00:01:21,639
这只是一个例子，你可以设想，

25
00:01:21,639 --> 00:01:24,609
在这样一个复杂的空间中，有无数的方向，

26
00:01:24,609 --> 00:01:27,580
每一个都可能代表着词义的不同方面。

27
00:01:28,322 --> 00:01:32,586
Transformer 的目标是逐步调整这些嵌入，

28
00:01:32,618 --> 00:01:35,500
使之不仅仅编码单词本身，

29
00:01:35,500 --> 00:01:39,628
而是包含更丰富、更深层次的上下文含义。

30
00:01:39,888 --> 00:01:43,705
需要提前说明的是，很多人对于注意力机制

31
00:01:43,705 --> 00:01:46,098
—— Transformer 的关键部分，感到非常困惑，

32
00:01:46,098 --> 00:01:48,980
因此，请不要着急，需要一些时间来消化理解。

33
00:01:49,440 --> 00:01:53,883
在我们深入到计算细节和矩阵运算之前，

34
00:01:53,883 --> 00:01:59,160
有必要先了解一些我们期望注意力机制能实现的行为示例。

35
00:02:00,140 --> 00:02:02,258
考虑一下这几个短语：美国真鼹鼠（mo

36
00:02:02,258 --> 00:02:04,377
le）、一摩尔（mole）二氧化碳，

37
00:02:04,377 --> 00:02:06,220
对肿瘤（mole）进行活检。

38
00:02:06,394 --> 00:02:07,398
我们都明白，

39
00:02:07,398 --> 00:02:10,900
在不同的上下文环境下，“mole”这个词会有不同的意思。 

40
00:02:11,360 --> 00:02:13,185
然而，在 Transformer 的第一步中，文本被拆分，

41
00:02:13,185 --> 00:02:16,724
每个 Token 都被关联到一个向量，

42
00:02:16,724 --> 00:02:21,190
这时，“mole”这个词对应的向量在所有情况下都是相同的，

43
00:02:21,221 --> 00:02:23,146
因为初始的 Token 嵌入向量

44
00:02:23,146 --> 00:02:26,220
本质上是一个不参考上下文的查找表。

45
00:02:26,620 --> 00:02:29,504
直到 Transformer 的下一步，

46
00:02:29,504 --> 00:02:33,100
周围的嵌入才有机会向这个 Token 传递信息。

47
00:02:33,820 --> 00:02:38,341
你可以想象，嵌入空间里有多个不同的方向，

48
00:02:38,341 --> 00:02:41,370
这些方向分别编码了“mole”这个词的多种不同含义。

49
00:02:41,370 --> 00:02:44,138
如果 Token 经过了良好的训练，注意力模块就可以计算出

50
00:02:44,143 --> 00:02:46,845
需要根据上下文在通用嵌入向量中添加什么内容，

51
00:02:46,892 --> 00:02:51,800
使其指向其中一个特定的方向。

52
00:02:52,998 --> 00:02:56,435
再来看一个例子，比如单词“Tower”的嵌入向量，

53
00:02:57,060 --> 00:03:01,120
这可能是个非常通用、不特定的方向，

54
00:03:01,120 --> 00:03:03,720
与许多大型、高大的名词关联。

55
00:03:04,020 --> 00:03:06,642
如果“Tower”前面是“埃菲尔”，

56
00:03:06,642 --> 00:03:10,389
你可能希望更新这个向量，

57
00:03:10,389 --> 00:03:14,349
使其更具体地指向埃菲尔铁塔的方向，

58
00:03:14,349 --> 00:03:19,060
可能与巴黎、法国或者钢铁制品相关的向量有关。

59
00:03:19,920 --> 00:03:22,279
如果前面还有“微型”这个词，

60
00:03:22,279 --> 00:03:24,688
那么这个向量应该进一步更新，

61
00:03:24,688 --> 00:03:27,500
使其不再与大型、高大的事物相关。

62
00:03:29,480 --> 00:03:32,323
更进一步讲，注意力模块不仅可以精确一个词的含义，

63
00:03:32,322 --> 00:03:37,710
还能将一个嵌入向量中的信息传递到另一个嵌入向量中，

64
00:03:37,697 --> 00:03:39,916
即使这两个嵌入向量相距很远，

65
00:03:39,932 --> 00:03:43,300
信息也可能比单一单词要丰富得多。

66
00:03:43,300 --> 00:03:47,988
如我们在上一章中看到的，所有向量通过网络流动，

67
00:03:47,988 --> 00:03:50,961
包括经过许多不同的注意力模块后，

68
00:03:50,961 --> 00:03:54,509
预测下一个 Token 的计算过

69
00:03:54,509 --> 00:03:58,280
程完全取决于序列中的最后一个向量。

70
00:03:59,100 --> 00:04:03,503
例如，你输入的文字是一整部悬疑小说，

71
00:04:03,503 --> 00:04:07,800
到了接近尾声的部分，写着"所以，凶手是"。

72
00:04:08,400 --> 00:04:11,510
如果模型要准确地预测下一个词，

73
00:04:11,510 --> 00:04:16,096
那么这个序列中的最后一个向量，它最初只是嵌入了单词"是"，

74
00:04:16,096 --> 00:04:19,101
它必须经过所有的注意力模块的更新，

75
00:04:19,101 --> 00:04:22,160
以包含远超过任何单个单词的信息，

76
00:04:22,160 --> 00:04:26,248
通过某种方式编码了所有来自完整的上下文窗口中

77
00:04:26,248 --> 00:04:28,220
与预测下一个词相关的信息。

78
00:04:29,500 --> 00:04:32,580
但为了逐步解析计算过程，我们先看一个更简单的例子。

79
00:04:32,980 --> 00:04:35,069
假设输入包含了一个句子，

80
00:04:35,074 --> 00:04:37,960
"一个蓬松的蓝色生物在葱郁的森林中游荡"。

81
00:04:38,460 --> 00:04:42,180
假设我们此刻关注的只是

82
00:04:42,180 --> 00:04:46,780
让形容词调整其对应名词的含义的这种更新方式。

83
00:04:47,000 --> 00:04:50,769
我马上要讲的是我们通常所说的单个注意力分支，

84
00:04:50,769 --> 00:04:51,852
稍后我们会看到

85
00:04:51,838 --> 00:04:55,420
一个注意力模块是由许多不同的分支并行运行组成的。

86
00:04:56,140 --> 00:04:59,884
需要强调的是，每个词的初始嵌入是一个高维向量，

87
00:04:59,884 --> 00:05:03,380
只编码了该特定词的含义，不包含任何上下文。

88
00:05:03,682 --> 00:05:05,380
实际上，这并不完全正确。

89
00:05:05,380 --> 00:05:07,640
它们还编码了词的位置。

90
00:05:07,980 --> 00:05:11,368
关于位置如何被编码的细节有很多，

91
00:05:11,367 --> 00:05:13,159
但你现在只需要知道，

92
00:05:13,159 --> 00:05:16,845
这个向量的条目足以告诉你这个词是什么，

93
00:05:16,846 --> 00:05:18,900
以及它在上下文中的位置。

94
00:05:19,176 --> 00:05:22,101
让我们用字母 E 来表示这些嵌入。

95
00:05:22,171 --> 00:05:24,596
我们的目标是，通过一系列计算，

96
00:05:24,596 --> 00:05:27,020
产生一组新的、更为精细的嵌入向量，

97
00:05:27,020 --> 00:05:29,680
比如说，这样做可以让名词的嵌入向量

98
00:05:29,680 --> 00:05:33,420
捕捉并融合了与它们相对应的形容词的含义。

99
00:05:33,533 --> 00:05:35,414
而在深度学习的过程中，

100
00:05:35,414 --> 00:05:39,321
我们希望大部分的计算都像矩阵 - 向量的乘积，

101
00:05:39,321 --> 00:05:41,861
其中的矩阵充满了可调的权重，

102
00:05:41,862 --> 00:05:44,460
模型将根据数据来学习这些权重。

103
00:05:44,660 --> 00:05:48,212
需要明确的是，我构造这个形容词调整名词的例子，

104
00:05:48,256 --> 00:05:52,260
只是为了说明你可以设想一个注意力分支可能做的事情。

105
00:05:52,860 --> 00:05:55,940
正如深度学习常见的情况，真实的行为更为复杂，

106
00:05:55,940 --> 00:06:01,339
因为它涉及到调整和微调海量参数以最小化某种成本函数。

107
00:06:01,680 --> 00:06:07,334
逐一审视这一过程中涉及的各种参数矩阵时，

108
00:06:07,331 --> 00:06:13,220
设想一个具体的应用场景能帮助我们更好地理解其背后的逻辑。

109
00:06:14,140 --> 00:06:18,202
在这个过程的第一步，你可以想象每个名词，比如"生物"，

110
00:06:18,202 --> 00:06:21,960
都在问，有没有形容词在我前面？

111
00:06:22,160 --> 00:06:25,429
对于"毛茸茸的"和"蓝色的"这两个词，都会回答，

112
00:06:25,429 --> 00:06:27,960
对，我是一个形容词，我就在那个位置。

113
00:06:28,960 --> 00:06:32,320
这个问题会被编码成另一个向量，

114
00:06:32,320 --> 00:06:36,100
也就是一组数字，我们称之为这个词的查询向量。

115
00:06:36,980 --> 00:06:42,020
这个查询向量的维度比嵌入向量要小得多，比如说 128 维。

116
00:06:42,940 --> 00:06:46,360
计算这个查询就是取一个特定的矩阵，

117
00:06:46,360 --> 00:06:49,780
也就是 WQ，并将其与嵌入向量相乘。

118
00:06:50,960 --> 00:06:54,222
简化一下，我们把查询向量记作 Q，

119
00:06:54,222 --> 00:06:58,064
然后你看到我把一个矩阵放在一个箭头旁边，

120
00:06:58,064 --> 00:07:02,695
就表示通过这个矩阵与箭头起点的向量相乘，

121
00:07:02,695 --> 00:07:04,800
可以得到箭头终点的向量。

122
00:07:05,860 --> 00:07:10,266
在这种情况下，你将这个矩阵与上下文中的所有嵌入向量相乘，

123
00:07:10,266 --> 00:07:12,580
得到的是每个 Token 对应的一个查询向量。

124
00:07:13,740 --> 00:07:16,429
这个矩阵由模型的参数组成，

125
00:07:16,428 --> 00:07:18,984
意味着它能从数据中学习到真实的行为模式。

126
00:07:18,984 --> 00:07:19,762
但在实际应用中， 

127
00:07:19,777 --> 00:07:23,440
要明确这个矩阵在特定注意力机制中的具体作用是相当复杂的。

128
00:07:23,900 --> 00:07:27,947
不过，让我们尝试想象一个理想情况：

129
00:07:27,946 --> 00:07:31,057
我们希望这个查询矩阵能将名词的嵌入信息映射到

130
00:07:31,057 --> 00:07:33,696
一个较小的空间中的特定方向上，

131
00:07:33,696 --> 00:07:38,040
这种映射方式能够捕捉到一种特殊的寻找前置形容词的规律。

132
00:07:38,780 --> 00:07:41,440
至于它对其他嵌入向量做什么，我们不知道。

133
00:07:41,720 --> 00:07:44,340
也许它同时试图用这些实现一些其他的目标。

134
00:07:44,540 --> 00:07:47,160
现在，我们的注意力集中在名词上。

135
00:07:47,280 --> 00:07:51,651
同时，这里还有另一个我们称之为键矩阵的矩阵，

136
00:07:51,651 --> 00:07:54,620
同样需要与所有嵌入向量相乘。

137
00:07:55,280 --> 00:07:58,500
这会生成一个我们称之为键的向量序列。

138
00:07:59,420 --> 00:08:03,140
从概念上讲，你可以把键想象成是潜在的查询回答者。

139
00:08:03,840 --> 00:08:07,990
这个键矩阵也充满了可调整的参数，同查询矩阵一样，

140
00:08:07,990 --> 00:08:11,400
将嵌入向量映射到同一个较小的维度空间中。

141
00:08:12,200 --> 00:08:17,020
当键与查询密切对齐时，你可以将键视为与查询相匹配。

142
00:08:17,460 --> 00:08:19,658
以我们的例子来说，你可以想象键矩

143
00:08:19,658 --> 00:08:21,994
阵将形容词"毛茸茸的"和"蓝色的"

144
00:08:21,994 --> 00:08:26,740
映射到与单词"生物"生成的查询紧密对齐的向量上。

145
00:08:27,200 --> 00:08:30,175
为了衡量每个键与每个查询的匹配程度，

146
00:08:30,175 --> 00:08:34,000
你需要计算每一对可能的键 - 查询组合之间的点积。

147
00:08:34,480 --> 00:08:37,155
我喜欢将其想象为一个充满各种点的网格，

148
00:08:37,155 --> 00:08:40,294
其中较大的点对应着较大的点积，

149
00:08:40,294 --> 00:08:43,011
即键与查询对齐的地方。

150
00:08:43,280 --> 00:08:47,251
就我们讨论的形容词与名词的例子而言，

151
00:08:47,246 --> 00:08:50,322
如果"毛茸茸的"和"蓝色的"生成的键

152
00:08:50,329 --> 00:08:53,722
确实与"生物"产生的查询非常吻合，

153
00:08:53,722 --> 00:08:58,560
那么这两个点的点积会是一些较大的正数。 

154
00:08:59,100 --> 00:09:02,307
用机器学习的术语来说，

155
00:09:02,307 --> 00:09:05,420
"毛茸茸的"和"蓝色"的嵌入向量会关注"生物"的嵌入向量。

156
00:09:06,040 --> 00:09:10,281
相比之下，像"the"这样的词的键

157
00:09:10,282 --> 00:09:14,196
与"生物"的查询之间的点积会是一些较小或者负值，

158
00:09:14,203 --> 00:09:16,600
这反映出它们之间没有关联。

159
00:09:17,700 --> 00:09:23,173
因此，我们面前展开了一个值域横跨负无穷到正无穷的实数网格，

160
00:09:23,203 --> 00:09:25,766
这个网格赋予了我们评估每个单词在更

161
00:09:25,766 --> 00:09:28,480
新其它单词含义上的相关性得分的能力。

162
00:09:28,907 --> 00:09:31,042
接下来，我们将利用这些分数

163
00:09:31,042 --> 00:09:34,199
执行一种操作：按照每个词的相关重要性，

164
00:09:34,199 --> 00:09:36,384
沿着每一列计算加权平均值。

165
00:09:36,341 --> 00:09:39,217
因此，我们的目标不是让这些数据列的数值范围无限扩展，

166
00:09:39,217 --> 00:09:40,213
从负无穷到正无穷。

167
00:09:40,213 --> 00:09:44,011
相反，我们希望这些数据列中的每个数值都介于0和1之间，

168
00:09:44,011 --> 00:09:48,560
并且每列的数值总和为 1，正如它们构成一个概率分布那样。

169
00:09:48,908 --> 00:09:52,220
如果你从上一章继续阅读，就知道我们接下来要做什么。

170
00:09:52,620 --> 00:09:57,300
我们会按列计算 softmax 函数以标准化这些值。

171
00:10:00,060 --> 00:10:03,237
在我们的示意图中，对所有列应用 softmax 函数后，

172
00:10:03,237 --> 00:10:05,860
我们会用这些标准化的值填充网格。

173
00:10:06,421 --> 00:10:08,410
此时，可以将每一列理解为

174
00:10:08,413 --> 00:10:14,723
根据左侧的单词与顶部对应值的相关性赋予的权重。

175
00:10:14,655 --> 00:10:16,840
我们将这种网格称为“注意力模式”。

176
00:10:17,861 --> 00:10:20,277
如果你看原始的 Transformer 论文，

177
00:10:20,277 --> 00:10:22,820
你会发现他们用一种非常简洁的方式描述了所有这些。

178
00:10:23,639 --> 00:10:30,583
这里的变量 Q 和 K 分别代表查询向量和键向量的完整数组，

179
00:10:30,584 --> 00:10:35,135
这些都是通过将嵌入向量与查询矩阵和键矩阵相乘得到的小型向量。

180
00:10:35,160 --> 00:10:39,117
这个在分子上的表达式，是一种简洁的表示方式，

181
00:10:39,117 --> 00:10:43,020
可以表示所有可能的键 - 查询对的点积网格。

182
00:10:43,589 --> 00:10:48,086
这里有一个我之前没有提到的小细节，那就是为了数值稳定性，

183
00:10:48,086 --> 00:10:53,960
我们将所有这些值除以键 - 查询空间维度的平方根。

184
00:10:54,480 --> 00:10:57,865
然后，包裹全表达式的 softmax 函数，

185
00:10:57,865 --> 00:11:01,219
我们应理解为是按列应用的。

186
00:11:01,640 --> 00:11:04,700
至于那个 V 项，我们稍后再讲。

187
00:11:05,020 --> 00:11:08,460
在此之前，还有一个我之前没有提到的技术细节。

188
00:11:09,040 --> 00:11:13,050
在训练过程中，对给定文本进行处理时，

189
00:11:13,050 --> 00:11:16,920
模型会通过调整权重来奖励或惩罚预测的准确性，

190
00:11:16,920 --> 00:11:20,796
即根据模型对文中下一个词的预测概率的高低。

191
00:11:20,796 --> 00:11:23,797
有种做法能显著提高整个训练过程的效率，

192
00:11:23,797 --> 00:11:25,811
那就是同时让模型预测

193
00:11:25,811 --> 00:11:28,608
该段落中每个初始 Token 子序列

194
00:11:28,608 --> 00:11:31,559
之后所有可能出现的下一个 Token。

195
00:11:31,940 --> 00:11:34,211
例如，对于我们正在关注的那个短语，

196
00:11:34,220 --> 00:11:37,417
它也可能在预测 "生物"  后面应该跟什么单词，

197
00:11:37,417 --> 00:11:39,858
以及 "the" 后面应该跟什么单词。

198
00:11:39,940 --> 00:11:45,560
这样一来，一个训练样本就能提供更多的学习机会。 

199
00:11:45,998 --> 00:11:48,012
在设计注意力模式时，

200
00:11:48,114 --> 00:11:52,204
一个基本原则是不允许后出现的词汇影响先出现的词汇，

201
00:11:52,205 --> 00:11:56,040
如果不这样做，后面的词汇可能会提前泄露接下来内容的线索。 

202
00:11:56,560 --> 00:11:59,615
这就要求我们在模型中设置一种机制，

203
00:11:59,615 --> 00:12:02,884
确保代表后续 Token 对前面 Token 的影响力

204
00:12:02,883 --> 00:12:05,404
能够被有效地削弱到零。 

205
00:12:05,920 --> 00:12:08,751
直觉上，我们可能会考虑直接将这些影响力设置为零。

206
00:12:08,751 --> 00:12:11,303
但这样做会导致一个问题：那些影响力值的列加和不再是一，

207
00:12:11,303 --> 00:12:12,420
也就失去了标准化的效果。

208
00:12:13,120 --> 00:12:15,013
为了解决这个问题，一个常见的做法是在进行 

209
00:12:15,013 --> 00:12:16,456
softmax 归一化操作之前，

210
00:12:16,456 --> 00:12:19,020
将这些影响力值设置为负无穷大。

211
00:12:19,680 --> 00:12:23,608
这样，经过 softmax 处理后，这些位置的数值会变成零，

212
00:12:23,608 --> 00:12:25,180
同时保证了整体的归一化条件不被破坏。

213
00:12:26,000 --> 00:12:27,540
这就是所谓的掩蔽过程。

214
00:12:27,540 --> 00:12:29,997
在注意力机制的某些应用场景中，我们不会采用掩码技术。

215
00:12:30,014 --> 00:12:31,343
但在我们的 GPT 示例中，

216
00:12:31,344 --> 00:12:34,964
无论是在训练阶段还是在运作阶段

217
00:12:34,964 --> 00:12:37,423
（比如作为聊天机器人），

218
00:12:37,423 --> 00:12:39,122
都会应用这种掩蔽，以防止后面的 

219
00:12:39,122 --> 00:12:41,460
Token 对前面的 Token 产生影响。

220
00:12:42,480 --> 00:12:45,825
值得一提的是，这个注意力模式的大小

221
00:12:45,825 --> 00:12:49,500
等于上下文大小的平方。

222
00:12:49,900 --> 00:12:54,047
这就解释了上下文大小可能会对大型语言模型构成巨大瓶颈，

223
00:12:54,047 --> 00:12:55,620
而且要扩大它的话并非易事。

224
00:12:56,300 --> 00:13:00,124
近年来，出于对更大上下文窗口的追求，

225
00:13:00,124 --> 00:13:03,496
出现了一些对注意力机制的改进，

226
00:13:03,496 --> 00:13:05,745
 使其在处理上下文方面更具可扩展性，

227
00:13:05,754 --> 00:13:08,868
但在这里，我们还是专注于基础知识的讲解。

228
00:13:10,067 --> 00:13:12,216
通过计算这个模式，

229
00:13:12,216 --> 00:13:15,480
模型能够推断哪些词与其他词相关。

230
00:13:16,020 --> 00:13:18,562
然后就需要实际更新嵌入向量，

231
00:13:18,562 --> 00:13:22,800
让词语可以将信息传递给它们相关的其他词。

232
00:13:22,800 --> 00:13:25,670
比如说，你希望 "毛茸茸的" 的嵌入向量能够使得 

233
00:13:25,670 --> 00:13:26,818
"生物" 发生改变，

234
00:13:26,818 --> 00:13:31,846
从而将它移动到这个 12000 维嵌入空间的另一部分，

235
00:13:31,847 --> 00:13:35,006
以更具体地表达一个 "毛茸茸的生物"。

236
00:13:35,122 --> 00:13:39,339
我首先向你展示的是执行这个操作的最简单方法，

237
00:13:39,339 --> 00:13:43,460
不过要注意，在多头注意力的情境下，这个方法会有些许调整。

238
00:13:44,080 --> 00:13:47,165
这个方法的核心是使用一个第三个矩阵，

239
00:13:47,165 --> 00:13:48,776
也就是我们所说的“值矩阵”。

240
00:13:48,809 --> 00:13:51,494
你需要将它与某个单词的嵌入相乘，

241
00:13:51,494 --> 00:13:52,894
比如“毛茸茸的”。

242
00:13:53,300 --> 00:13:55,931
得出的结果我们称之为“值向量”，

243
00:13:55,931 --> 00:13:59,197
是你要加入到第二个单词的嵌入向量中的元素，

244
00:13:59,197 --> 00:14:01,920
例如，在这个情境下，就是要加入到“生物”的嵌入向量中。

245
00:14:02,600 --> 00:14:05,580
因此，这个值向量就存在于和嵌入向量一样的，

246
00:14:05,580 --> 00:14:07,000
非常高维的空间中。 

247
00:14:07,460 --> 00:14:10,832
当你用这个值矩阵乘以某个单词的嵌入向量时，

248
00:14:10,832 --> 00:14:12,048
可以理解为你在询问：

249
00:14:12,048 --> 00:14:15,610
如果这个单词对于调整其他内容的含义具有相关性，

250
00:14:15,610 --> 00:14:21,160
那么为了反映这一点，我们需要向那个内容的嵌入中添加什么呢？

251
00:14:21,623 --> 00:14:26,017
回到我们的图表，我们先不考虑所有的键和查询，

252
00:14:26,017 --> 00:14:29,497
因为一旦计算出注意力模式，这些就不再需要了。

253
00:14:29,497 --> 00:14:33,903
接下来，我们将使用这个值矩阵，将其与每一个嵌入向量相乘，

254
00:14:33,907 --> 00:14:36,468
从而生成一系列的值向量。

255
00:14:37,120 --> 00:14:41,120
你可以将这些值向量视作在某种程度上与它们相对应的“键”有关。

256
00:14:42,320 --> 00:14:44,619
对图表中的每一列来说，

257
00:14:44,619 --> 00:14:49,465
你需要将每个值向量与该列中相应的权重相乘。

258
00:14:49,870 --> 00:14:52,815
举个例子，对于代表“生物”的嵌入向量，

259
00:14:52,815 --> 00:14:57,107
你会主要加入“毛茸茸的”和“蓝色的”这两个值向量的较大比例，

260
00:14:57,107 --> 00:15:01,560
而其他的值向量则被减少为零，或者至少接近零。

261
00:15:02,120 --> 00:15:06,804
最后，为了更新这一列与之相关联的嵌入向量，

262
00:15:06,804 --> 00:15:08,329
原本这个向量编码了“生物”这一概念

263
00:15:08,329 --> 00:15:09,944
的某种基本含义（不考虑具体上下文），

264
00:15:09,944 --> 00:15:13,191
你需要将列中所有这些经过重新调整比例的值加总起来，

265
00:15:13,191 --> 00:15:15,320
这一步骤产生了一个我们想要引入的变化量，

266
00:15:15,320 --> 00:15:16,704
我将其称为delta-e。

267
00:15:16,704 --> 00:15:19,260
接着，你就将这个变化量叠加到原有的嵌入向量上。

268
00:15:19,680 --> 00:15:22,496
希望最终得到的是一个更精细的向量，

269
00:15:22,496 --> 00:15:24,869
是一个更加细致和含有丰富上下文信息的向量，

270
00:15:24,869 --> 00:15:26,889
比如描绘了一个毛绒绒、蓝色的奇妙生物。

271
00:15:27,380 --> 00:15:30,223
显然，我们不仅仅对一个嵌入向量进行这种处理，

272
00:15:30,222 --> 00:15:33,732
而是将同样的加权求和方法应用于图像中所有的列，

273
00:15:33,732 --> 00:15:36,024
由此产生一连串的调整。

274
00:15:36,024 --> 00:15:39,147
将这些调整加到相应的嵌入向量上，

275
00:15:39,147 --> 00:15:43,460
便形成了一组更为细腻且富含信息的嵌入向量序列。

276
00:15:44,515 --> 00:15:46,738
从宏观角度来看，我们讨论的整个过

277
00:15:46,738 --> 00:15:49,100
程构成了所谓的“单头注意力”机制。

278
00:15:49,301 --> 00:15:51,264
正如之前所解释的，

279
00:15:51,264 --> 00:15:56,619
这一机制是通过三种不同的、充满可调整参数的矩阵来实现的，

280
00:15:56,620 --> 00:15:59,278
 即“键”、“查询”和“值”。

281
00:15:59,500 --> 00:16:02,982
接下来，我想继续上一章节我们开始探讨的内容，

282
00:16:02,982 --> 00:16:08,040
也就是通过统计 GPT-3 模型的参数数量来进行“计分”。

283
00:16:08,967 --> 00:16:13,567
这些键矩阵和查询矩阵每个都有 12,288 列，

284
00:16:13,567 --> 00:16:15,101
与嵌入维度匹配，

285
00:16:15,101 --> 00:16:19,600
以及 128 行，与较小的键查询空间的维度匹配。

286
00:16:20,260 --> 00:16:24,220
这给我们每个矩阵增加了大约 150 万个参数。

287
00:16:24,860 --> 00:16:30,190
如果你看看值矩阵，按照我目前为止的描述，

288
00:16:30,190 --> 00:16:32,812
它看上去是一个正方形的矩阵，有 

289
00:16:32,812 --> 00:16:35,926
12,288 列和 12,288 行，

290
00:16:35,926 --> 00:16:40,920
因为它的输入和输出都存在于这个庞大的嵌入空间中。

291
00:16:41,268 --> 00:16:45,140
如果这是真的，那就意味着要增加大约 1500 万个参数。

292
00:16:45,660 --> 00:16:47,300
当然，你可以这样做，

293
00:16:47,420 --> 00:16:49,805
你可以让值矩阵拥有数量级更多的参数，

294
00:16:49,805 --> 00:16:51,740
而不是键矩阵和查询矩阵。

295
00:16:52,060 --> 00:16:53,917
但在实践中，如果想效率更高，

296
00:16:53,917 --> 00:17:00,760
你可以让值矩阵的参数数量与键矩阵和查询矩阵的参数数量相同。

297
00:17:01,141 --> 00:17:05,160
特别是在同时运行多个注意力机制的场景下，这一点尤为重要。

298
00:17:05,858 --> 00:17:10,098
具体来说，值映射实际上是两个小矩阵乘积的形式。

299
00:17:10,569 --> 00:17:15,386
我还是建议从整体上去理解这个线性映射过程，

300
00:17:15,386 --> 00:17:18,814
输入和输出都在这个更大的嵌入空间中，

301
00:17:18,814 --> 00:17:23,758
比如将“蓝色”的嵌入向量指向加到名词上以表示“蓝色”的方向。

302
00:17:23,736 --> 00:17:26,478
它只是被分成了两个单独的步骤。

303
00:17:26,749 --> 00:17:29,869
这里的变化只是它的行数比较少，

304
00:17:29,869 --> 00:17:32,760
通常和键查询的空间大小一致。

305
00:17:33,100 --> 00:17:38,440
可以理解为，它将较大的嵌入向量映射到了一个更小的空间。

306
00:17:39,040 --> 00:17:42,700
虽然这不是通用的术语，但我决定将其称作“值降维矩阵”。

307
00:17:43,400 --> 00:17:47,422
而第二个矩阵则是从这个较小的空间映射回原来的嵌入空间，

308
00:17:47,422 --> 00:17:50,580
生成了用于实际更新的向量，

309
00:17:51,000 --> 00:17:54,740
我将其称为“值升维矩阵”，这个命名同样非传统。

310
00:17:55,160 --> 00:17:58,080
在大多数论文中，你会看到的描述方式可能和我说的有所不同，

311
00:17:58,240 --> 00:17:59,544
我稍后会解释原因。

312
00:17:59,532 --> 00:18:02,540
但我个人认为，那种描述方式可能会让概念理解变得更加混乱。

313
00:18:03,260 --> 00:18:05,205
在这里借用一下线性代数的专业术语，

314
00:18:05,205 --> 00:18:08,475
我们实际上在做的，就是把整个数据的变换过程

315
00:18:08,475 --> 00:18:10,839
限制成一种比较简单的形式。

316
00:18:11,420 --> 00:18:16,156
回到参数计数，这四个矩阵的尺寸相同，

317
00:18:16,156 --> 00:18:19,429
将他们全部加起来，就得到了大约 630 万个参数，

318
00:18:19,435 --> 00:18:21,082
这是一个注意力机制所需要的。

319
00:18:22,040 --> 00:18:24,236
顺带一提，为了更准确，

320
00:18:24,236 --> 00:18:27,487
到目前为止我们讨论的这部分通常被称为“自我注意力”机制，

321
00:18:27,487 --> 00:18:30,301
这是为了将其与其他模型中出现的一个不同版本区分开来，

322
00:18:30,301 --> 00:18:32,062
那就是被称为“交叉注意力”的版本。

323
00:18:32,300 --> 00:18:35,787
这与我们的 GPT 示例无关，但如果你感兴趣的话，

324
00:18:35,787 --> 00:18:39,785
交叉注意力涉及的模型会处理两种不同类型的数据，

325
00:18:39,828 --> 00:18:45,381
比如一种语言的文本和正在翻译的另一种语言的文本， 

326
00:18:45,381 --> 00:18:49,654
或者可能是语音输入和正在进行的转录。

327
00:18:50,400 --> 00:18:52,700
交叉注意力机制看起来几乎和自注意力机制一样。

328
00:18:52,980 --> 00:18:55,123
唯一的区别是，键和查询映射在交叉

329
00:18:55,123 --> 00:18:57,400
注意力机制中会作用于不同的数据集。

330
00:18:57,840 --> 00:19:02,109
例如，在进行翻译的模型中，键可能来自一种语言，

331
00:19:02,109 --> 00:19:03,967
而查询来自另一种语言，

332
00:19:03,967 --> 00:19:05,838
注意力模式可以描述 

333
00:19:05,839 --> 00:19:09,822
一种语言的哪些词对应另一种语言的哪些词。

334
00:19:10,340 --> 00:19:12,930
在这种情况下，通常不会有遮蔽，

335
00:19:12,930 --> 00:19:16,340
因为并不存在后面的词会影响前面的词的概念。

336
00:19:17,180 --> 00:19:19,060
继续讨论自注意力机制， 

337
00:19:19,060 --> 00:19:20,812
如果你已经理解了到目前为止的所有内容，

338
00:19:20,812 --> 00:19:22,358
那么即使你现在停下，

339
00:19:22,358 --> 00:19:25,180
也已经领会了注意力模型的核心要义。

340
00:19:25,760 --> 00:19:31,440
我们剩下要讲的就是这个过程需要做多次的原因。

341
00:19:32,100 --> 00:19:35,179
在之前的例子中，我们专注于形容词如何改变名词的含义，

342
00:19:35,179 --> 00:19:39,800
但实际上，语境对词语含义的影响方式有很多种。

343
00:19:40,360 --> 00:19:43,249
比如，如果“他们撞毁了”出现在"车"之前，

344
00:19:43,249 --> 00:19:46,520
会对车子的形状和结构产生预设。

345
00:19:46,892 --> 00:19:49,548
而且很多时候，这种联系可能并不遵循语法规则。

346
00:19:49,760 --> 00:19:52,935
比如，如果“魔法师”和“哈利”出现在同一篇文章中，

347
00:19:52,935 --> 00:19:55,954
可能暗示的是哈利·波特，

348
00:19:55,954 --> 00:20:00,015
而如果这篇文章中还出现了“女王”，“萨塞克斯”和“威廉”，

349
00:20:00,015 --> 00:20:04,440
那么哈利的词嵌入应该更新为指代王子。

350
00:20:05,040 --> 00:20:08,589
你能想象的每一种上下文更新方式，

351
00:20:08,589 --> 00:20:11,991
键和查询矩阵的参数都会有所不同，

352
00:20:11,991 --> 00:20:15,343
用以捕捉不同的注意力模式，

353
00:20:15,343 --> 00:20:19,140
而值映射的参数会根据需要添加到嵌入中的信息有所改变。

354
00:20:19,770 --> 00:20:24,401
再次强调，虽然这些映射的真实行为更复杂难懂，

355
00:20:24,402 --> 00:20:28,452
但权重设置是为了让模型能够更好地完成预测下一个 

356
00:20:28,452 --> 00:20:30,140
Token 的任务。

357
00:20:31,400 --> 00:20:35,240
前面所描述的都只是单个注意力头，

358
00:20:35,240 --> 00:20:37,672
在 Transformer 中

359
00:20:37,672 --> 00:20:40,266
完整的注意力块由多头注意力组成，

360
00:20:40,263 --> 00:20:43,184
同时运行多个操作，

361
00:20:43,184 --> 00:20:45,920
每个操作都有其独特的键、查询和值映射。

362
00:20:46,986 --> 00:20:51,700
例如，GPT-3 在每个块中都使用了 96 个注意力头。

363
00:20:52,020 --> 00:20:54,517
考虑到每一个都相当复杂，

364
00:20:54,517 --> 00:20:56,460
的确需要花费一些精力理解。

365
00:20:56,760 --> 00:21:00,641
也就是说，你有 96 套不同的键和查询矩阵，

366
00:21:00,641 --> 00:21:05,000
产生 96 种不同的注意力模式。

367
00:21:05,440 --> 00:21:08,983
然后，每个注意力头都有独特的值矩阵

368
00:21:08,983 --> 00:21:12,180
用来产生 96 个值向量的序列。

369
00:21:12,460 --> 00:21:16,680
所有这些都通过使用对应的注意力模式作为权重进行加总。

370
00:21:17,480 --> 00:21:21,455
这意味着，在上下文中的每个位置，每个 Token，

371
00:21:21,455 --> 00:21:24,814
所有的头都会产生一个建议的变化，

372
00:21:24,814 --> 00:21:27,020
以便添加到该位置的嵌入中。 

373
00:21:27,660 --> 00:21:31,078
因此，你需要将所有这些建议的变化加在一起，

374
00:21:31,078 --> 00:21:35,480
每个头对应一个，然后将结果加入到该位置的原始嵌入中。

375
00:21:36,660 --> 00:21:43,222
这个总和就是从多头注意力块输出的一个部分，

376
00:21:43,176 --> 00:21:45,806
是精炼后的嵌入之一，

377
00:21:45,806 --> 00:21:47,460
它从另一端弹出来。

378
00:21:48,157 --> 00:21:49,864
再次强调，这需要考虑的东西很多，

379
00:21:49,864 --> 00:21:52,140
所以如果需要一段时间来理解，完全不用担心。

380
00:21:52,380 --> 00:21:56,376
总的来说，通过并行运行多个不同的头，

381
00:21:56,376 --> 00:22:01,819
你就能让模型有能力学习上下文改变含义的多种不同方式。

382
00:22:03,700 --> 00:22:07,323
我们计算下来，每个包含 96 个头的参数，

383
00:22:07,323 --> 00:22:10,550
各自有四个矩阵的变体，

384
00:22:10,550 --> 00:22:15,080
每个多头注意力块最后大约有 6 亿个参数。

385
00:22:16,420 --> 00:22:19,868
对于那些想深入了解 transformer 的人，

386
00:22:19,868 --> 00:22:21,800
这里有个小插曲我必须提一下。

387
00:22:22,080 --> 00:22:25,639
你可能还记得我曾经说过，值映射被分解成两个不同的矩阵，

388
00:22:25,639 --> 00:22:29,440
我把它们标记为值下降和值上升矩阵。

389
00:22:29,960 --> 00:22:32,048
我之前的描述可能会

390
00:22:32,048 --> 00:22:35,832
让你觉得在每个注意力头里都会看到这一对矩阵，

391
00:22:35,821 --> 00:22:38,440
并且实际上也确实可以这样实现，

392
00:22:38,640 --> 00:22:39,920
这种设计是可行的。

393
00:22:40,260 --> 00:22:44,920
但是在论文中，以及在实践中的实现方式看起来有些不同。

394
00:22:45,340 --> 00:22:48,025
所有这些每个头的值向上矩阵，都

395
00:22:48,025 --> 00:22:50,891
像是被合在一起的一个巨大的矩阵，

396
00:22:50,891 --> 00:22:56,380
我们称之为输出矩阵，它与整个多头注意力块相关联。

397
00:22:56,820 --> 00:22:59,322
当你看到人们谈论某个 attention 

398
00:22:59,322 --> 00:23:00,634
head 的值矩阵时，

399
00:23:00,634 --> 00:23:03,227
他们通常只指的是这个第一步，

400
00:23:03,227 --> 00:23:07,140
也就是我所说的值向下投影到小空间的步骤。

401
00:23:08,340 --> 00:23:11,140
对那些好奇的人，我在这里注明了这一点。

402
00:23:11,144 --> 00:23:15,065
虽然这是一个可能会让人偏离主要概念的细节，

403
00:23:15,043 --> 00:23:16,086
但我还是想提一下，

404
00:23:16,086 --> 00:23:18,649
这样当你在其他地方看到这些讨论时，你就会知道它的来龙去脉。

405
00:23:19,240 --> 00:23:21,377
抛开所有的技术细节，

406
00:23:21,376 --> 00:23:23,533
我们在上一章的概览中了解到，数据在 

407
00:23:23,533 --> 00:23:25,450
Transformer 中的流动

408
00:23:25,451 --> 00:23:28,040
并不局限于单个注意力模块。

409
00:23:28,329 --> 00:23:32,849
首先，数据还会经过其他被称为多层感知器的操作。

410
00:23:32,853 --> 00:23:34,997
我们会在下一章详细介绍这个。

411
00:23:34,990 --> 00:23:39,566
然后，数据会反复经过这两种操作的多个副本。

412
00:23:39,636 --> 00:23:43,958
这意味着，一个单词在吸收了一些上下文信息后，

413
00:23:43,959 --> 00:23:47,276
这个更细致的 embedding 仍有更多的机会

414
00:23:47,276 --> 00:23:50,040
受到其周围更为细致环境的影响。

415
00:23:50,603 --> 00:23:52,580
你在网络中越深入，

416
00:23:52,602 --> 00:23:54,477
每个 embedding 从所有其他 

417
00:23:54,477 --> 00:23:56,450
embedding 中获取的含义就越多，

418
00:23:56,451 --> 00:23:59,104
这些 embedding 本身也变得越来越复杂，

419
00:23:59,104 --> 00:24:04,599
我们希望这能有助于编码关于给定输入的更高级别和更抽象的概念，

420
00:24:04,591 --> 00:24:07,445
而不仅仅是描述和语法结构。

421
00:24:07,648 --> 00:24:11,763
这些概念可以是情感、语调，是否是一首诗，

422
00:24:11,763 --> 00:24:15,130
以及与这个作品相关的基础科学真理等等。

423
00:24:16,700 --> 00:24:22,052
再回到我们的统计，GPT-3 包括 96 个不同的层，

424
00:24:22,052 --> 00:24:27,405
因此关键查询和值参数的总数需要乘以 96，

425
00:24:27,405 --> 00:24:32,049
这使得总数达到将近 580 亿个，

426
00:24:32,049 --> 00:24:34,500
这些参数全部用于各种 attention head。

427
00:24:34,677 --> 00:24:36,410
这确实是一个巨大的数字，

428
00:24:36,417 --> 00:24:40,940
但它只占网络总计 1750 亿参数的大约三分之一。

429
00:24:41,520 --> 00:24:44,147
所以，尽管注意力模型吸引了所有的关注，

430
00:24:44,147 --> 00:24:48,140
但大部分的参数其实来自那些位于这些步骤之间的模块。

431
00:24:48,560 --> 00:24:51,017
在下一章，我们将更深入地讨论这些模块，

432
00:24:51,017 --> 00:24:53,560
以及更多关于训练过程的信息。

433
00:24:54,120 --> 00:24:57,234
注意力机制的成功之处

434
00:24:57,250 --> 00:25:00,988
并非在于它所启动的任何特定类型的行为， 

435
00:25:01,032 --> 00:25:03,998
而在于它极其适合并行运算，

436
00:25:03,998 --> 00:25:08,845
这意味着你可以使用 GPU 在短时间内完成大量的计算任务。

437
00:25:09,045 --> 00:25:13,356
在过去十年或二十年的深度学习研究中，我们得到了一个重要启示，

438
00:25:13,357 --> 00:25:17,440
那就是规模的放大似乎可以带来模型性能的巨大定量提升。

439
00:25:17,440 --> 00:25:21,060
因此，适合并行运算的架构具有巨大的优势。

440
00:25:22,040 --> 00:25:24,042
如果你想了解更多关于这个主题的信息，

441
00:25:24,042 --> 00:25:25,711
我在视频简介中留下了很多链接。

442
00:25:25,745 --> 00:25:27,849
特别是，由 Andrej Karpathy 或 

443
00:25:27,849 --> 00:25:30,040
Chris Ola 制作的任何内容都非常值得一看。

444
00:25:30,560 --> 00:25:33,763
在这个视频中，我只是想直接介绍现在的注意力机制，

445
00:25:33,763 --> 00:25:36,747
但是如果你对我们是如何达到这里的历程

446
00:25:36,747 --> 00:25:38,985
以及你可能如何为自己重新创新这个想法感到好奇，

447
00:25:38,985 --> 00:25:41,564
我的朋友 Vivek 最近就发布了几个视频，

448
00:25:41,564 --> 00:25:42,736
深入讲解了这个动机。

449
00:25:42,744 --> 00:25:44,210
此外，来自"The Art of the 

450
00:25:44,210 --> 00:25:45,746
Problem"频道的 Britt Cruz

451
00:25:45,746 --> 00:25:48,460
有一部非常精彩的视频，介绍了大语言模型的历史。

452
00:26:04,960 --> 00:26:09,200
谢谢。

