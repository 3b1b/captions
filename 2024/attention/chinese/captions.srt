1
00:00:00,000 --> 00:00:02,009
在上一章中，我们开始探讨

2
00:00:02,009 --> 00:00:04,019
Transformer 的内部运作机制。

3
00:00:04,560.0000000000009 --> 00:00:07,925
Transformer 是大语言模型中关键的技术组成部分，

4
00:00:07,925 --> 00:00:10,639.8046875
也被广泛应用于现代 AI 领域的诸多工具中。

5
00:00:10,980 --> 00:00:15,574
它首次亮相是在 2017 年一篇广为人知的论文《Attention is All You Need》中，

6
00:00:15,574 --> 00:00:19,840
本章我们将深入探讨这种注意力机制，

7
00:00:19,840 --> 00:00:21,700
以及可视化展示它如何处理数据。

8
00:00:26,140 --> 00:00:29,540
在此，我想快速回顾一些重要的背景信息。

9
00:00:30,000 --> 00:00:33,003
我们正在研究的模型的目标是

10
00:00:33,003 --> 00:00:36,060
读取一段文本并预测下一个词。

11
00:00:36,860 --> 00:00:40,388
输入文本被分割成我们称之为 Tokens 的小部分，

12
00:00:40,388 --> 00:00:43,035
它们通常是完整的单词或单词的一部分。

13
00:00:43,035 --> 00:00:47,290
但为了让我们在这个视频中的例子更加简单易懂，

14
00:00:47,290 --> 00:00:50,560
让我们假设 Token 总是完整的单词。

15
00:00:51,480 --> 00:00:54,590
Transformer 的第一步是将每个 Token

16
00:00:54,590 --> 00:00:57,700
与一个高维向量关联，这就是我们所说的嵌入向量。

17
00:00:57,700 --> 00:01:00,481.3515625
我希望你能理解的关键概念是，

18
00:01:00,481.7421875 --> 00:01:04,777.74609375
如何理解在所有可能的嵌入向量所构成的高维空间中，

19
00:01:04,777.74609375 --> 00:01:07,000
不同的方向能够代表不同的语义含义。

20
00:01:07,680 --> 00:01:11,766
在上一章中，我们给出了一个例子，说明了方向如何对应性别，

21
00:01:11,766 --> 00:01:15,553
即在这个空间中添加一定的变化可以从

22
00:01:15,553 --> 00:01:19,640
一个男性名词的嵌入转到对应的女性名词的嵌入。

23
00:01:19,699.8046875 --> 00:01:21,639.0625
这只是一个例子，你可以设想，

24
00:01:21,639.0625 --> 00:01:24,609.53125
在这样一个复杂的空间中，有无数的方向，

25
00:01:24,609.53125 --> 00:01:27,580
每一个都可能代表着词义的不同方面。

26
00:01:28,322.4609375 --> 00:01:32,586.171875
Transformer 的目标是逐步调整这些嵌入，

27
00:01:32,618.7890625 --> 00:01:35,500
使之不仅仅编码单词本身，

28
00:01:35,500 --> 00:01:39,628.75
而是包含更丰富、更深层次的上下文含义。

29
00:01:39,888.0078125 --> 00:01:43,705
需要提前说明的是，很多人对于注意力机制

30
00:01:43,705 --> 00:01:46,098
—— Transformer 的关键部分，感到非常困惑，

31
00:01:46,098 --> 00:01:48,980
因此，请不要着急，需要一些时间来消化理解。

32
00:01:49,440 --> 00:01:53,883.703125
在我们深入到计算细节和矩阵运算之前，

33
00:01:53,883.78125 --> 00:01:59,160
有必要先了解一些我们期望注意力机制能实现的行为示例。

34
00:02:00,140 --> 00:02:04,377
考虑一下这几个短语：美国真鼹鼠（mole）、一摩尔（mole）二氧化碳，

35
00:02:04,377 --> 00:02:06,220
对肿瘤（mole）进行活检。

36
00:02:06,394.921875 --> 00:02:07,398.19531250001455
我们都明白，

37
00:02:07,398.19531250001455 --> 00:02:10,900
在不同的上下文环境下，“mole”这个词会有不同的意思。 

38
00:02:11,360 --> 00:02:13,185.546875
然而，在 Transformer 的第一步中，文本被拆分，

39
00:02:13,185.546875 --> 00:02:16,724.4921874999709
每个 Token 都被关联到一个向量，

40
00:02:16,724.4921874999709 --> 00:02:21,190.5078125
这时，“mole”这个词对应的向量在所有情况下都是相同的，

41
00:02:21,221.9140625 --> 00:02:23,146.6796875
因为初始的 Token 嵌入向量

42
00:02:23,146.7578125 --> 00:02:26,220
本质上是一个不参考上下文的查找表。

43
00:02:26,620 --> 00:02:29,504.5156249999709
直到 Transformer 的下一步，

44
00:02:29,504.5156249999709 --> 00:02:33,100
周围的嵌入才有机会向这个 Token 传递信息。

45
00:02:33,820 --> 00:02:38,341
你可以想象，嵌入空间里有多个不同的方向，

46
00:02:38,341 --> 00:02:41,370.71875
这些方向分别编码了“mole”这个词的多种不同含义。

47
00:02:41,370.71875 --> 00:02:44,138.6015625
如果 Token 经过了良好的训练，注意力模块就可以计算出

48
00:02:44,143.875 --> 00:02:46,845.3359374999709
需要根据上下文在通用嵌入向量中添加什么内容，

49
00:02:46,892.3281249999709 --> 00:02:51,800
使其指向其中一个特定的方向。

50
00:02:52,998.3984375 --> 00:02:56,435.1953125000291
再来看一个例子，比如单词“Tower”的嵌入向量，

51
00:02:57,060 --> 00:03:01,120
这可能是个非常通用、不特定的方向，

52
00:03:01,120 --> 00:03:03,720
与许多大型、高大的名词关联。

53
00:03:04,020 --> 00:03:06,642
如果“Tower”前面是“埃菲尔”，

54
00:03:06,642 --> 00:03:10,389
你可能希望更新这个向量，

55
00:03:10,389 --> 00:03:14,349
使其更具体地指向埃菲尔铁塔的方向，

56
00:03:14,349 --> 00:03:19,060
可能与巴黎、法国或者钢铁制品相关的向量有关。

57
00:03:19,920 --> 00:03:22,279
如果前面还有“微型”这个词，

58
00:03:22,279 --> 00:03:24,688
那么这个向量应该进一步更新，

59
00:03:24,688 --> 00:03:27,500
使其不再与大型、高大的事物相关。

60
00:03:29,480 --> 00:03:32,323
更进一步讲，注意力模块不仅可以精确一个词的含义，

61
00:03:32,322.9999999999709 --> 00:03:37,710.765625
还能将一个嵌入向量中的信息传递到另一个嵌入向量中，

62
00:03:37,697.6796875000291 --> 00:03:39,916.6328125000582
即使这两个嵌入向量相距很远，

63
00:03:39,932.8046874999709 --> 00:03:43,300
信息也可能比单一单词要丰富得多。

64
00:03:43,300 --> 00:03:47,988
如我们在上一章中看到的，所有向量通过网络流动，

65
00:03:47,988 --> 00:03:50,961
包括经过许多不同的注意力模块后，

66
00:03:50,961 --> 00:03:58,280
预测下一个 Token 的计算过程完全取决于序列中的最后一个向量。

67
00:03:59,100 --> 00:04:03,503
例如，你输入的文字是一整部悬疑小说，

68
00:04:03,503 --> 00:04:07,800
到了接近尾声的部分，写着"所以，凶手是"。

69
00:04:08,400 --> 00:04:11,510
如果模型要准确地预测下一个词，

70
00:04:11,510 --> 00:04:16,096
那么这个序列中的最后一个向量，它最初只是嵌入了单词"是"，

71
00:04:16,096 --> 00:04:19,101.6406250000291
它必须经过所有的注意力模块的更新，

72
00:04:19,101.6406250000291 --> 00:04:22,160.9218749999418
以包含远超过任何单个单词的信息，

73
00:04:22,160.9218749999418 --> 00:04:26,248.3906249999418
通过某种方式编码了所有来自完整的上下文窗口中

74
00:04:26,248.3906249999418 --> 00:04:28,220
与预测下一个词相关的信息。

75
00:04:29,500 --> 00:04:32,580
但为了逐步解析计算过程，我们先看一个更简单的例子。

76
00:04:32,980 --> 00:04:35,69.5234375
假设输入包含了一个句子，

77
00:04:35,74.67968749994179 --> 00:04:37,960
"一个蓬松的蓝色生物在葱郁的森林中游荡"。

78
00:04:38,460 --> 00:04:42,180.3515625
假设我们此刻关注的只是

79
00:04:42,180.3515624999418 --> 00:04:46,780
让形容词调整其对应名词的含义的这种更新方式。

80
00:04:47,000 --> 00:04:50,769
我马上要讲的是我们通常所说的单个注意力分支，

81
00:04:50,769 --> 00:04:51,852.7499999999418
稍后我们会看到

82
00:04:51,838.3359374999418 --> 00:04:55,420
一个注意力模块是由许多不同的分支并行运行组成的。

83
00:04:56,140 --> 00:04:59,884
需要强调的是，每个词的初始嵌入是一个高维向量，

84
00:04:59,884 --> 00:05:03,380
只编码了该特定词的含义，不包含任何上下文。

85
00:05:03,682.890625 --> 00:05:05,380
实际上，这并不完全正确。

86
00:05:05,380 --> 00:05:07,640
它们还编码了词的位置。

87
00:05:07,980 --> 00:05:11,368.4140625
关于位置如何被编码的细节有很多，

88
00:05:11,367.515625 --> 00:05:13,159.63281250011642
但你现在只需要知道，

89
00:05:13,159.16406250011642 --> 00:05:16,845.8906250000582
这个向量的条目足以告诉你这个词是什么，

90
00:05:16,846.359375 --> 00:05:18,900
以及它在上下文中的位置。

91
00:05:19,176.7578125 --> 00:05:22,101.67968750005821
让我们用字母 E 来表示这些嵌入。

92
00:05:22,171.9140625 --> 00:05:24,596.0546875
我们的目标是，通过一系列计算，

93
00:05:24,596.0546875 --> 00:05:27,20.1953125
产生一组新的、更为精细的嵌入向量，

94
00:05:27,20.7421875 --> 00:05:29,680
比如说，这样做可以让名词的嵌入向量

95
00:05:29,680 --> 00:05:33,420
捕捉并融合了与它们相对应的形容词的含义。

96
00:05:33,533.0859375 --> 00:05:35,414.8125000000582
而在深度学习的过程中，

97
00:05:35,414.8125000000582 --> 00:05:39,321.6171875
我们希望大部分的计算都像矩阵 - 向量的乘积，

98
00:05:39,321.6171875 --> 00:05:41,861.1796875001164
其中的矩阵充满了可调的权重，

99
00:05:41,862.7421875001164 --> 00:05:44,460.1562500000582
模型将根据数据来学习这些权重。

100
00:05:44,660 --> 00:05:48,212.7578125
需要明确的是，我构造这个形容词调整名词的例子，

101
00:05:48,256.5859375 --> 00:05:52,260
只是为了说明你可以设想一个注意力分支可能做的事情。

102
00:05:52,860 --> 00:05:55,940.53125
正如深度学习常见的情况，真实的行为更为复杂，

103
00:05:55,940.53125 --> 00:06:01,339.9999999999418
因为它涉及到调整和微调海量参数以最小化某种成本函数。

104
00:06:01,680 --> 00:06:07,334.84375
逐一审视这一过程中涉及的各种参数矩阵时，

105
00:06:07,331.5234375 --> 00:06:13,220
设想一个具体的应用场景能帮助我们更好地理解其背后的逻辑。

106
00:06:14,140 --> 00:06:18,202
在这个过程的第一步，你可以想象每个名词，比如"生物"，

107
00:06:18,202 --> 00:06:21,960
都在问，有没有形容词在我前面？

108
00:06:22,160 --> 00:06:25,429
对于"毛茸茸的"和"蓝色的"这两个词，都会回答，

109
00:06:25,429 --> 00:06:27,960
对，我是一个形容词，我就在那个位置。

110
00:06:28,960 --> 00:06:32,320
这个问题会被编码成另一个向量，

111
00:06:32,320 --> 00:06:36,100
也就是一组数字，我们称之为这个词的查询向量。

112
00:06:36,980 --> 00:06:42,020
这个查询向量的维度比嵌入向量要小得多，比如说 128 维。

113
00:06:42,940 --> 00:06:46,360
计算这个查询就是取一个特定的矩阵，

114
00:06:46,360 --> 00:06:49,780
也就是 WQ，并将其与嵌入向量相乘。

115
00:06:50,960 --> 00:06:54,222
简化一下，我们把查询向量记作 Q，

116
00:06:54,222 --> 00:06:58,064
然后你看到我把一个矩阵放在一个箭头旁边，

117
00:06:58,064 --> 00:07:02,695
就表示通过这个矩阵与箭头起点的向量相乘，

118
00:07:02,695 --> 00:07:04,800
可以得到箭头终点的向量。

119
00:07:05,860 --> 00:07:10,266
在这种情况下，你将这个矩阵与上下文中的所有嵌入向量相乘，

120
00:07:10,266 --> 00:07:12,580
得到的是每个 Token 对应的一个查询向量。

121
00:07:13,740 --> 00:07:16,429
这个矩阵由模型的参数组成，

122
00:07:16,428.9999999999418 --> 00:07:18,984.4218749999418
意味着它能从数据中学习到真实的行为模式。

123
00:07:18,984.4218749999418 --> 00:07:19,762.1328125000582
但在实际应用中， 

124
00:07:19,777.7578125000582 --> 00:07:23,440
要明确这个矩阵在特定注意力机制中的具体作用是相当复杂的。

125
00:07:23,900 --> 00:07:27,947
不过，让我们尝试想象一个理想情况：

126
00:07:27,946.9999999999418 --> 00:07:31,57.97656249994179
我们希望这个查询矩阵能将名词的嵌入信息映射到

127
00:07:31,57.97656249994179 --> 00:07:33,696.5468750000582
一个较小的空间中的特定方向上，

128
00:07:33,696.5468750000582 --> 00:07:38,040
这种映射方式能够捕捉到一种特殊的寻找前置形容词的规律。

129
00:07:38,780 --> 00:07:41,440
至于它对其他嵌入向量做什么，我们不知道。

130
00:07:41,720 --> 00:07:44,340
也许它同时试图用这些实现一些其他的目标。

131
00:07:44,540 --> 00:07:47,160
现在，我们的注意力集中在名词上。

132
00:07:47,280 --> 00:07:51,651
同时，这里还有另一个我们称之为键矩阵的矩阵，

133
00:07:51,651 --> 00:07:54,620
同样需要与所有嵌入向量相乘。

134
00:07:55,280 --> 00:07:58,500
这会生成一个我们称之为键的向量序列。

135
00:07:59,420 --> 00:08:03,140
从概念上讲，你可以把键想象成是潜在的查询回答者。

136
00:08:03,840 --> 00:08:07,990
这个键矩阵也充满了可调整的参数，同查询矩阵一样，

137
00:08:07,990 --> 00:08:11,400
将嵌入向量映射到同一个较小的维度空间中。

138
00:08:12,200 --> 00:08:17,020
当键与查询密切对齐时，你可以将键视为与查询相匹配。

139
00:08:17,460 --> 00:08:21,994
以我们的例子来说，你可以想象键矩阵将形容词"毛茸茸的"和"蓝色的"

140
00:08:21,994 --> 00:08:26,740
映射到与单词"生物"生成的查询紧密对齐的向量上。

141
00:08:27,200 --> 00:08:30,175
为了衡量每个键与每个查询的匹配程度，

142
00:08:30,175 --> 00:08:34,000
你需要计算每一对可能的键 - 查询组合之间的点积。

143
00:08:34,480 --> 00:08:37,155.9999999999418
我喜欢将其想象为一个充满各种点的网格，

144
00:08:37,155.9999999999418 --> 00:08:40,294.9999999999418
其中较大的点对应着较大的点积，

145
00:08:40,294.9999999999418 --> 00:08:43,11.343749999941792
即键与查询对齐的地方。

146
00:08:43,280 --> 00:08:47,251.09375
就我们讨论的形容词与名词的例子而言，

147
00:08:47,246.2890625 --> 00:08:50,322.9296875
如果"毛茸茸的"和"蓝色的"生成的键

148
00:08:50,329.8437500001164 --> 00:08:53,722.453125
确实与"生物"产生的查询非常吻合，

149
00:08:53,722.453125 --> 00:08:58,560.1171875
那么这两个点的点积会是一些较大的正数。 

150
00:08:59,100 --> 00:09:02,307
用机器学习的术语来说，

151
00:09:02,307 --> 00:09:05,420
"毛茸茸的"和"蓝色"的嵌入向量会关注"生物"的嵌入向量。

152
00:09:06,040 --> 00:09:10,281.53125
相比之下，像"the"这样的词的键

153
00:09:10,282.2734375001164 --> 00:09:14,196.90624999988358
与"生物"的查询之间的点积会是一些较小或者负值，

154
00:09:14,203.625 --> 00:09:16,600
这反映出它们之间没有关联。

155
00:09:17,700 --> 00:09:23,173.375
因此，我们面前展开了一个值域横跨负无穷到正无穷的实数网格，

156
00:09:23,203.53125 --> 00:09:28,480
这个网格赋予了我们评估每个单词在更新其它单词含义上的相关性得分的能力。

157
00:09:28,907.6171875 --> 00:09:31,42.3515625
接下来，我们将利用这些分数

158
00:09:31,42.3515625 --> 00:09:34,199.22265625
执行一种操作：按照每个词的相关重要性，

159
00:09:34,199.22265625 --> 00:09:36,384.6875
沿着每一列计算加权平均值。

160
00:09:36,341.1328125 --> 00:09:40,213
因此，我们的目标不是让这些数据列的数值范围无限扩展，从负无穷到正无穷。

161
00:09:40,213 --> 00:09:44,011
相反，我们希望这些数据列中的每个数值都介于0和1之间，

162
00:09:44,011 --> 00:09:48,560.390625
并且每列的数值总和为 1，正如它们构成一个概率分布那样。

163
00:09:48,908.28125 --> 00:09:52,220
如果你从上一章继续阅读，就知道我们接下来要做什么。

164
00:09:52,620 --> 00:09:57,300
我们会按列计算 softmax 函数以标准化这些值。

165
00:10:00,060 --> 00:10:03,237
在我们的示意图中，对所有列应用 softmax 函数后，

166
00:10:03,237 --> 00:10:05,860
我们会用这些标准化的值填充网格。

167
00:10:06,421.4453125 --> 00:10:08,410.1093750001164
此时，可以将每一列理解为

168
00:10:08,413.8203125001164 --> 00:10:14,723.515625
根据左侧的单词与顶部对应值的相关性赋予的权重。

169
00:10:14,655.1562500001164 --> 00:10:16,840
我们将这种网格称为“注意力模式”。

170
00:10:17,861.2109375001164 --> 00:10:20,277
如果你看原始的 Transformer 论文，

171
00:10:20,277 --> 00:10:22,820
你会发现他们用一种非常简洁的方式描述了所有这些。

172
00:10:23,639.1796875 --> 00:10:30,583.6171875
这里的变量 Q 和 K 分别代表查询向量和键向量的完整数组，

173
00:10:30,584.4375 --> 00:10:35,135.9375
这些都是通过将嵌入向量与查询矩阵和键矩阵相乘得到的小型向量。

174
00:10:35,160 --> 00:10:39,117
这个在分子上的表达式，是一种简洁的表示方式，

175
00:10:39,117 --> 00:10:43,020
可以表示所有可能的键 - 查询对的点积网格。

176
00:10:43,589.1015625 --> 00:10:48,086
这里有一个我之前没有提到的小细节，那就是为了数值稳定性，

177
00:10:48,086 --> 00:10:53,960
我们将所有这些值除以键 - 查询空间维度的平方根。

178
00:10:54,480 --> 00:10:57,865
然后，包裹全表达式的 softmax 函数，

179
00:10:57,865 --> 00:11:01,219.8046875
我们应理解为是按列应用的。

180
00:11:01,640 --> 00:11:04,700
至于那个 V 项，我们稍后再讲。

181
00:11:05,020 --> 00:11:08,460
在此之前，还有一个我之前没有提到的技术细节。

182
00:11:09,040 --> 00:11:13,050
在训练过程中，对给定文本进行处理时，

183
00:11:13,050 --> 00:11:16,920.234375
模型会通过调整权重来奖励或惩罚预测的准确性，

184
00:11:16,920.234375 --> 00:11:20,796.0546875001164
即根据模型对文中下一个词的预测概率的高低。

185
00:11:20,796.0546875001164 --> 00:11:23,797.9765625
有种做法能显著提高整个训练过程的效率，

186
00:11:23,797.9765625 --> 00:11:25,811.7812500001164
那就是同时让模型预测

187
00:11:25,811.7812500001164 --> 00:11:31,559.9999999998836
该段落中每个初始 Token 子序列之后所有可能出现的下一个 Token。

188
00:11:31,940 --> 00:11:34,211.765625
例如，对于我们正在关注的那个短语，

189
00:11:34,220.6328125 --> 00:11:37,417.9921875001164
它也可能在预测 "生物"  后面应该跟什么单词，

190
00:11:37,417.9921875001164 --> 00:11:39,858.125
以及 "the" 后面应该跟什么单词。

191
00:11:39,940 --> 00:11:45,560
这样一来，一个训练样本就能提供更多的学习机会。 

192
00:11:45,998.671875 --> 00:11:48,12.0703125
在设计注意力模式时，

193
00:11:48,114.0234375 --> 00:11:52,204.9609375
一个基本原则是不允许后出现的词汇影响先出现的词汇，

194
00:11:52,205 --> 00:11:56,040
如果不这样做，后面的词汇可能会提前泄露接下来内容的线索。 

195
00:11:56,560 --> 00:11:59,615
这就要求我们在模型中设置一种机制，

196
00:11:59,615 --> 00:12:02,884
确保代表后续 Token 对前面 Token 的影响力

197
00:12:02,883.9999999998836 --> 00:12:05,404.9609375
能够被有效地削弱到零。 

198
00:12:05,920 --> 00:12:08,751
直觉上，我们可能会考虑直接将这些影响力设置为零。

199
00:12:08,751 --> 00:12:11,303
但这样做会导致一个问题：那些影响力值的列加和不再是一，

200
00:12:11,303 --> 00:12:12,420
也就失去了标准化的效果。

201
00:12:13,120 --> 00:12:16,456
为了解决这个问题，一个常见的做法是在进行 softmax 归一化操作之前，

202
00:12:16,456 --> 00:12:19,020
将这些影响力值设置为负无穷大。

203
00:12:19,680 --> 00:12:23,608
这样，经过 softmax 处理后，这些位置的数值会变成零，

204
00:12:23,608 --> 00:12:25,180
同时保证了整体的归一化条件不被破坏。

205
00:12:26,000 --> 00:12:27,540
这就是所谓的掩蔽过程。

206
00:12:27,540 --> 00:12:29,997.7031249998836
在注意力机制的某些应用场景中，我们不会采用掩码技术。

207
00:12:30,14.578125 --> 00:12:31,343.9999999998836
但在我们的 GPT 示例中，

208
00:12:31,344 --> 00:12:34,964
无论是在训练阶段还是在运作阶段

209
00:12:34,964 --> 00:12:37,423
（比如作为聊天机器人），

210
00:12:37,423 --> 00:12:41,460
都会应用这种掩蔽，以防止后面的 Token 对前面的 Token 产生影响。

211
00:12:42,480 --> 00:12:45,825
值得一提的是，这个注意力模式的大小

212
00:12:45,825 --> 00:12:49,500
等于上下文大小的平方。

213
00:12:49,900 --> 00:12:54,047
这就解释了上下文大小可能会对大型语言模型构成巨大瓶颈，

214
00:12:54,047 --> 00:12:55,620
而且要扩大它的话并非易事。

215
00:12:56,300 --> 00:13:00,124
近年来，出于对更大上下文窗口的追求，

216
00:13:00,124.00000000011642 --> 00:13:03,496.47656250023283
出现了一些对注意力机制的改进，

217
00:13:03,496.5156250001164 --> 00:13:05,745.9062500001164
 使其在处理上下文方面更具可扩展性，

218
00:13:05,754.4218749998836 --> 00:13:08,868.3984375001164
但在这里，我们还是专注于基础知识的讲解。

219
00:13:10,67.96875 --> 00:13:12,216.375
通过计算这个模式，

220
00:13:12,216.375 --> 00:13:15,480
模型能够推断哪些词与其他词相关。

221
00:13:16,020 --> 00:13:18,562
然后就需要实际更新嵌入向量，

222
00:13:18,562 --> 00:13:22,800
让词语可以将信息传递给它们相关的其他词。

223
00:13:22,800 --> 00:13:26,818
比如说，你希望 "毛茸茸的" 的嵌入向量能够使得 "生物" 发生改变，

224
00:13:26,818 --> 00:13:31,846.3750000001164
从而将它移动到这个 12000 维嵌入空间的另一部分，

225
00:13:31,847.0390625001164 --> 00:13:35,6.328125
以更具体地表达一个 "毛茸茸的生物"。

226
00:13:35,122.734375 --> 00:13:39,339.609375
我首先向你展示的是执行这个操作的最简单方法，

227
00:13:39,339.609375 --> 00:13:43,460
不过要注意，在多头注意力的情境下，这个方法会有些许调整。

228
00:13:44,080 --> 00:13:47,165
这个方法的核心是使用一个第三个矩阵，

229
00:13:47,165 --> 00:13:48,776.3359375001164
也就是我们所说的“值矩阵”。

230
00:13:48,809.8125000001164 --> 00:13:51,494.00000000023283
你需要将它与某个单词的嵌入相乘，

231
00:13:51,494.0000000001164 --> 00:13:52,894.8437500002328
比如“毛茸茸的”。

232
00:13:53,300 --> 00:13:55,931
得出的结果我们称之为“值向量”，

233
00:13:55,931 --> 00:13:59,197
是你要加入到第二个单词的嵌入向量中的元素，

234
00:13:59,197 --> 00:14:01,920
例如，在这个情境下，就是要加入到“生物”的嵌入向量中。

235
00:14:02,600 --> 00:14:07,000
因此，这个值向量就存在于和嵌入向量一样的，非常高维的空间中。 

236
00:14:07,460 --> 00:14:10,832
当你用这个值矩阵乘以某个单词的嵌入向量时，

237
00:14:10,832 --> 00:14:12,48.6328125
可以理解为你在询问：

238
00:14:12,48.6328125 --> 00:14:15,610.8125000001164
如果这个单词对于调整其他内容的含义具有相关性，

239
00:14:15,610.8125000001164 --> 00:14:21,160
那么为了反映这一点，我们需要向那个内容的嵌入中添加什么呢？

240
00:14:21,623.59375 --> 00:14:26,17.000000000116415
回到我们的图表，我们先不考虑所有的键和查询，

241
00:14:26,017 --> 00:14:29,497
因为一旦计算出注意力模式，这些就不再需要了。

242
00:14:29,497 --> 00:14:33,903.1953125
接下来，我们将使用这个值矩阵，将其与每一个嵌入向量相乘，

243
00:14:33,907.6875000001164 --> 00:14:36,468.1640625
从而生成一系列的值向量。

244
00:14:37,120 --> 00:14:41,120
你可以将这些值向量视作在某种程度上与它们相对应的“键”有关。

245
00:14:42,320 --> 00:14:44,619.1796874997672
对图表中的每一列来说，

246
00:14:44,619.1796874996508 --> 00:14:49,465.6640625
你需要将每个值向量与该列中相应的权重相乘。

247
00:14:49,870.9375 --> 00:14:52,815
举个例子，对于代表“生物”的嵌入向量，

248
00:14:52,815 --> 00:14:57,107
你会主要加入“毛茸茸的”和“蓝色的”这两个值向量的较大比例，

249
00:14:57,107 --> 00:15:01,560
而其他的值向量则被减少为零，或者至少接近零。

250
00:15:02,120 --> 00:15:06,804
最后，为了更新这一列与之相关联的嵌入向量，

251
00:15:06,804 --> 00:15:09,944
原本这个向量编码了“生物”这一概念的某种基本含义（不考虑具体上下文），

252
00:15:09,944 --> 00:15:13,191
你需要将列中所有这些经过重新调整比例的值加总起来，

253
00:15:13,191 --> 00:15:16,704
这一步骤产生了一个我们想要引入的变化量，我将其称为delta-e。

254
00:15:16,704 --> 00:15:19,260
接着，你就将这个变化量叠加到原有的嵌入向量上。

255
00:15:19,680 --> 00:15:22,496.0312500001164
希望最终得到的是一个更精细的向量，

256
00:15:22,496.0312500001164 --> 00:15:24,869.1484375001164
是一个更加细致和含有丰富上下文信息的向量，

257
00:15:24,869.1484375001164 --> 00:15:26,889.8046875
比如描绘了一个毛绒绒、蓝色的奇妙生物。

258
00:15:27,380 --> 00:15:30,223
显然，我们不仅仅对一个嵌入向量进行这种处理，

259
00:15:30,222.99999999988358 --> 00:15:33,732.8749999998836
而是将同样的加权求和方法应用于图像中所有的列，

260
00:15:33,732.8749999998836 --> 00:15:36,24.4765625
由此产生一连串的调整。

261
00:15:36,24.4765625 --> 00:15:39,147.23828125
将这些调整加到相应的嵌入向量上，

262
00:15:39,147.23828125 --> 00:15:43,460
便形成了一组更为细腻且富含信息的嵌入向量序列。

263
00:15:44,515.546875 --> 00:15:49,100
从宏观角度来看，我们讨论的整个过程构成了所谓的“单头注意力”机制。

264
00:15:49,301.1328125 --> 00:15:51,264.4296875
正如之前所解释的，

265
00:15:51,264.4296875 --> 00:15:56,619.5
这一机制是通过三种不同的、充满可调整参数的矩阵来实现的，

266
00:15:56,620.7109375 --> 00:15:59,278.515625
 即“键”、“查询”和“值”。

267
00:15:59,500 --> 00:16:02,982
接下来，我想继续上一章节我们开始探讨的内容，

268
00:16:02,982 --> 00:16:08,040
也就是通过统计 GPT-3 模型的参数数量来进行“计分”。

269
00:16:08,967.96875 --> 00:16:15,101.00000000011642
这些键矩阵和查询矩阵每个都有 12,288 列，与嵌入维度匹配，

270
00:16:15,101 --> 00:16:19,600
以及 128 行，与较小的键查询空间的维度匹配。

271
00:16:20,260 --> 00:16:24,220
这给我们每个矩阵增加了大约 150 万个参数。

272
00:16:24,860 --> 00:16:30,190
如果你看看值矩阵，按照我目前为止的描述，

273
00:16:30,190 --> 00:16:35,926
它看上去是一个正方形的矩阵，有 12,288 列和 12,288 行，

274
00:16:35,926 --> 00:16:40,920
因为它的输入和输出都存在于这个庞大的嵌入空间中。

275
00:16:41,268.5546875 --> 00:16:45,140
如果这是真的，那就意味着要增加大约 1500 万个参数。

276
00:16:45,660 --> 00:16:47,300
当然，你可以这样做，

277
00:16:47,420 --> 00:16:49,805
你可以让值矩阵拥有数量级更多的参数，

278
00:16:49,805 --> 00:16:51,740
而不是键矩阵和查询矩阵。

279
00:16:52,060 --> 00:16:53,917.2187500001164
但在实践中，如果想效率更高，

280
00:16:53,917.2187500002328 --> 00:17:00,760
你可以让值矩阵的参数数量与键矩阵和查询矩阵的参数数量相同。

281
00:17:01,141.40625 --> 00:17:05,160.00000000011642
特别是在同时运行多个注意力机制的场景下，这一点尤为重要。

282
00:17:05,858.8671875 --> 00:17:10,98.99999999988358
具体来说，值映射实际上是两个小矩阵乘积的形式。

283
00:17:10,569.0625 --> 00:17:15,386
我还是建议从整体上去理解这个线性映射过程，

284
00:17:15,386 --> 00:17:18,814.0000000001164
输入和输出都在这个更大的嵌入空间中，

285
00:17:18,814.0000000001164 --> 00:17:23,758.1015625001164
比如将“蓝色”的嵌入向量指向加到名词上以表示“蓝色”的方向。

286
00:17:23,736.3437500001164 --> 00:17:26,478.6718749998836
它只是被分成了两个单独的步骤。

287
00:17:26,749.2578125 --> 00:17:29,869
这里的变化只是它的行数比较少，

288
00:17:29,869 --> 00:17:32,760
通常和键查询的空间大小一致。

289
00:17:33,100 --> 00:17:38,440
可以理解为，它将较大的嵌入向量映射到了一个更小的空间。

290
00:17:39,040 --> 00:17:42,700
虽然这不是通用的术语，但我决定将其称作“值降维矩阵”。

291
00:17:43,400 --> 00:17:47,422
而第二个矩阵则是从这个较小的空间映射回原来的嵌入空间，

292
00:17:47,422 --> 00:17:50,580
生成了用于实际更新的向量，

293
00:17:51,000 --> 00:17:54,740
我将其称为“值升维矩阵”，这个命名同样非传统。

294
00:17:55,160 --> 00:17:58,080
在大多数论文中，你会看到的描述方式可能和我说的有所不同，

295
00:17:58,240.15625000023283 --> 00:17:59,544.1015625
我稍后会解释原因。

296
00:17:59,532.734375 --> 00:18:02,540
但我个人认为，那种描述方式可能会让概念理解变得更加混乱。

297
00:18:03,260 --> 00:18:05,205.7890625
在这里借用一下线性代数的专业术语，

298
00:18:05,205.7890625 --> 00:18:08,475.6484375
我们实际上在做的，就是把整个数据的变换过程

299
00:18:08,475.6484375 --> 00:18:10,839.0625
限制成一种比较简单的形式。

300
00:18:11,420 --> 00:18:16,156
回到参数计数，这四个矩阵的尺寸相同，

301
00:18:16,156 --> 00:18:19,429.875
将他们全部加起来，就得到了大约 630 万个参数，

302
00:18:19,435.65625 --> 00:18:21,82.4609375
这是一个注意力机制所需要的。

303
00:18:22,040 --> 00:18:24,236
顺带一提，为了更准确，

304
00:18:24,236 --> 00:18:27,487
到目前为止我们讨论的这部分通常被称为“自我注意力”机制，

305
00:18:27,487 --> 00:18:30,301.8671875
这是为了将其与其他模型中出现的一个不同版本区分开来，

306
00:18:30,301.8671875 --> 00:18:32,62.5390625
那就是被称为“交叉注意力”的版本。

307
00:18:32,300 --> 00:18:35,787
这与我们的 GPT 示例无关，但如果你感兴趣的话，

308
00:18:35,787 --> 00:18:39,785.6562499997672
交叉注意力涉及的模型会处理两种不同类型的数据，

309
00:18:39,828 --> 00:18:45,381.5703125
比如一种语言的文本和正在翻译的另一种语言的文本， 

310
00:18:45,381.5703125 --> 00:18:49,654.0234375
或者可能是语音输入和正在进行的转录。

311
00:18:50,400 --> 00:18:52,700
交叉注意力机制看起来几乎和自注意力机制一样。

312
00:18:52,980 --> 00:18:57,400
唯一的区别是，键和查询映射在交叉注意力机制中会作用于不同的数据集。

313
00:18:57,840 --> 00:19:02,109
例如，在进行翻译的模型中，键可能来自一种语言，

314
00:19:02,109 --> 00:19:03,967.4453125
而查询来自另一种语言，

315
00:19:03,967.4453125 --> 00:19:05,838.09765625
注意力模式可以描述 

316
00:19:05,839.30859375 --> 00:19:09,822.4609375
一种语言的哪些词对应另一种语言的哪些词。

317
00:19:10,340 --> 00:19:12,930
在这种情况下，通常不会有遮蔽，

318
00:19:12,930 --> 00:19:16,340
因为并不存在后面的词会影响前面的词的概念。

319
00:19:17,180 --> 00:19:19,60.140625
继续讨论自注意力机制， 

320
00:19:19,60.140625 --> 00:19:20,812
如果你已经理解了到目前为止的所有内容，

321
00:19:20,812 --> 00:19:22,358.9453125
那么即使你现在停下，

322
00:19:22,358.9453125 --> 00:19:25,180
也已经领会了注意力模型的核心要义。

323
00:19:25,760 --> 00:19:31,440
我们剩下要讲的就是这个过程需要做多次的原因。

324
00:19:32,100 --> 00:19:35,179
在之前的例子中，我们专注于形容词如何改变名词的含义，

325
00:19:35,179 --> 00:19:39,800
但实际上，语境对词语含义的影响方式有很多种。

326
00:19:40,360 --> 00:19:43,249
比如，如果“他们撞毁了”出现在"车"之前，

327
00:19:43,249 --> 00:19:46,520
会对车子的形状和结构产生预设。

328
00:19:46,892.6953125 --> 00:19:49,548.984375
而且很多时候，这种联系可能并不遵循语法规则。

329
00:19:49,760 --> 00:19:52,935
比如，如果“魔法师”和“哈利”出现在同一篇文章中，

330
00:19:52,935 --> 00:19:55,954
可能暗示的是哈利·波特，

331
00:19:55,954 --> 00:20:00,015
而如果这篇文章中还出现了“女王”，“萨塞克斯”和“威廉”，

332
00:20:00,015 --> 00:20:04,440
那么哈利的词嵌入应该更新为指代王子。

333
00:20:05,040 --> 00:20:08,589
你能想象的每一种上下文更新方式，

334
00:20:08,589 --> 00:20:11,991
键和查询矩阵的参数都会有所不同，

335
00:20:11,991 --> 00:20:15,343
用以捕捉不同的注意力模式，

336
00:20:15,343 --> 00:20:19,140
而值映射的参数会根据需要添加到嵌入中的信息有所改变。

337
00:20:19,770.234375 --> 00:20:24,401.5546875
再次强调，虽然这些映射的真实行为更复杂难懂，

338
00:20:24,402.8046875 --> 00:20:30,140
但权重设置是为了让模型能够更好地完成预测下一个 Token 的任务。

339
00:20:31,400 --> 00:20:35,240
前面所描述的都只是单个注意力头，

340
00:20:35,240 --> 00:20:40,266.640625
在 Transformer 中完整的注意力块由多头注意力组成，

341
00:20:40,263.0078125 --> 00:20:43,184
同时运行多个操作，

342
00:20:43,184 --> 00:20:45,920
每个操作都有其独特的键、查询和值映射。

343
00:20:46,986.1328125 --> 00:20:51,700
例如，GPT-3 在每个块中都使用了 96 个注意力头。

344
00:20:52,020 --> 00:20:54,517
考虑到每一个都相当复杂，

345
00:20:54,517 --> 00:20:56,460
的确需要花费一些精力理解。

346
00:20:56,760 --> 00:21:00,641
也就是说，你有 96 套不同的键和查询矩阵，

347
00:21:00,641 --> 00:21:05,000
产生 96 种不同的注意力模式。

348
00:21:05,440 --> 00:21:08,983
然后，每个注意力头都有独特的值矩阵

349
00:21:08,983 --> 00:21:12,180
用来产生 96 个值向量的序列。

350
00:21:12,460 --> 00:21:16,680
所有这些都通过使用对应的注意力模式作为权重进行加总。

351
00:21:17,480 --> 00:21:21,455
这意味着，在上下文中的每个位置，每个 Token，

352
00:21:21,455 --> 00:21:24,814.7265625
所有的头都会产生一个建议的变化，

353
00:21:24,814.7265625002328 --> 00:21:27,020
以便添加到该位置的嵌入中。 

354
00:21:27,660 --> 00:21:31,078
因此，你需要将所有这些建议的变化加在一起，

355
00:21:31,078 --> 00:21:35,480
每个头对应一个，然后将结果加入到该位置的原始嵌入中。

356
00:21:36,660 --> 00:21:43,222.9765625
这个总和就是从多头注意力块输出的一个部分，

357
00:21:43,176.6875 --> 00:21:45,806.03125
是精炼后的嵌入之一，

358
00:21:45,806.03125 --> 00:21:47,460
它从另一端弹出来。

359
00:21:48,157.96875 --> 00:21:49,864.4921875002328
再次强调，这需要考虑的东西很多，

360
00:21:49,864.4921875002328 --> 00:21:52,140
所以如果需要一段时间来理解，完全不用担心。

361
00:21:52,380 --> 00:21:56,376
总的来说，通过并行运行多个不同的头，

362
00:21:56,376 --> 00:22:01,819.9999999997672
你就能让模型有能力学习上下文改变含义的多种不同方式。

363
00:22:03,700 --> 00:22:07,323
我们计算下来，每个包含 96 个头的参数，

364
00:22:07,323 --> 00:22:10,550
各自有四个矩阵的变体，

365
00:22:10,550 --> 00:22:15,080
每个多头注意力块最后大约有 6 亿个参数。

366
00:22:16,420 --> 00:22:21,800
对于那些想深入了解 transformer 的人，这里有个小插曲我必须提一下。

367
00:22:22,080 --> 00:22:25,639
你可能还记得我曾经说过，值映射被分解成两个不同的矩阵，

368
00:22:25,639 --> 00:22:29,440
我把它们标记为值下降和值上升矩阵。

369
00:22:29,960 --> 00:22:32,48.58984375
我之前的描述可能会

370
00:22:32,48.58984375 --> 00:22:35,832.1796874995343
让你觉得在每个注意力头里都会看到这一对矩阵，

371
00:22:35,821.3984375 --> 00:22:38,440
并且实际上也确实可以这样实现，

372
00:22:38,640 --> 00:22:39,920
这种设计是可行的。

373
00:22:40,260 --> 00:22:44,920
但是在论文中，以及在实践中的实现方式看起来有些不同。

374
00:22:45,340 --> 00:22:50,891
所有这些每个头的值向上矩阵，都像是被合在一起的一个巨大的矩阵，

375
00:22:50,891 --> 00:22:56,380
我们称之为输出矩阵，它与整个多头注意力块相关联。

376
00:22:56,820 --> 00:23:00,634
当你看到人们谈论某个 attention head 的值矩阵时，

377
00:23:00,634 --> 00:23:03,227
他们通常只指的是这个第一步，

378
00:23:03,227 --> 00:23:07,140
也就是我所说的值向下投影到小空间的步骤。

379
00:23:08,340 --> 00:23:11,140.859375
对那些好奇的人，我在这里注明了这一点。

380
00:23:11,144.84375 --> 00:23:15,65.34375000023283
虽然这是一个可能会让人偏离主要概念的细节，

381
00:23:15,43.1953125 --> 00:23:16,086
但我还是想提一下，

382
00:23:16,086 --> 00:23:18,649.5703125
这样当你在其他地方看到这些讨论时，你就会知道它的来龙去脉。

383
00:23:19,240 --> 00:23:21,377
抛开所有的技术细节，

384
00:23:21,376.99999999976717 --> 00:23:25,450.24999999976717
我们在上一章的概览中了解到，数据在 Transformer 中的流动

385
00:23:25,451.22656249976717 --> 00:23:28,040
并不局限于单个注意力模块。

386
00:23:28,329.453125 --> 00:23:32,849.0234375
首先，数据还会经过其他被称为多层感知器的操作。

387
00:23:32,853.0859375 --> 00:23:34,997.9687500004657
我们会在下一章详细介绍这个。

388
00:23:34,990.625 --> 00:23:39,566.6015625
然后，数据会反复经过这两种操作的多个副本。

389
00:23:39,636.2109375 --> 00:23:43,958.9999999997672
这意味着，一个单词在吸收了一些上下文信息后，

390
00:23:43,959 --> 00:23:47,276
这个更细致的 embedding 仍有更多的机会

391
00:23:47,276 --> 00:23:50,040
受到其周围更为细致环境的影响。

392
00:23:50,603.4375 --> 00:23:52,580.6874999997672
你在网络中越深入，

393
00:23:52,602.0937499997672 --> 00:23:56,450.71093750023283
每个 embedding 从所有其他 embedding 中获取的含义就越多，

394
00:23:56,451.49218750023283 --> 00:23:59,104.00000000023283
这些 embedding 本身也变得越来越复杂，

395
00:23:59,104 --> 00:24:04,599.6171875
我们希望这能有助于编码关于给定输入的更高级别和更抽象的概念，

396
00:24:04,591.296875 --> 00:24:07,445.234375
而不仅仅是描述和语法结构。

397
00:24:07,648.3984375 --> 00:24:11,763
这些概念可以是情感、语调，是否是一首诗，

398
00:24:11,763 --> 00:24:15,130
以及与这个作品相关的基础科学真理等等。

399
00:24:16,700 --> 00:24:22,052
再回到我们的统计，GPT-3 包括 96 个不同的层，

400
00:24:22,052 --> 00:24:27,405
因此关键查询和值参数的总数需要乘以 96，

401
00:24:27,405 --> 00:24:32,049
这使得总数达到将近 580 亿个，

402
00:24:32,049 --> 00:24:34,500
这些参数全部用于各种 attention head。

403
00:24:34,677.890625 --> 00:24:36,410.60937500023283
这确实是一个巨大的数字，

404
00:24:36,417.3671875 --> 00:24:40,940
但它只占网络总计 1750 亿参数的大约三分之一。

405
00:24:41,520 --> 00:24:44,147
所以，尽管注意力模型吸引了所有的关注，

406
00:24:44,147 --> 00:24:48,140
但大部分的参数其实来自那些位于这些步骤之间的模块。

407
00:24:48,560 --> 00:24:51,017
在下一章，我们将更深入地讨论这些模块，

408
00:24:51,017 --> 00:24:53,560
以及更多关于训练过程的信息。

409
00:24:54,120 --> 00:24:57,234.1640625
注意力机制的成功之处

410
00:24:57,250.5703125 --> 00:25:00,988.5546875002328
并非在于它所启动的任何特定类型的行为， 

411
00:25:01,32.18750000023283 --> 00:25:03,998.2656250002328
而在于它极其适合并行运算，

412
00:25:03,998.2656250004657 --> 00:25:08,845.7421875
这意味着你可以使用 GPU 在短时间内完成大量的计算任务。

413
00:25:09,45.1953125 --> 00:25:13,356.99999999976717
在过去十年或二十年的深度学习研究中，我们得到了一个重要启示，

414
00:25:13,357 --> 00:25:17,440
那就是规模的放大似乎可以带来模型性能的巨大定量提升。

415
00:25:17,440 --> 00:25:21,060
因此，适合并行运算的架构具有巨大的优势。

416
00:25:22,040 --> 00:25:25,711.09375
如果你想了解更多关于这个主题的信息，我在视频简介中留下了很多链接。

417
00:25:25,745.8203125 --> 00:25:30,040
特别是，由 Andrej Karpathy 或 Chris Ola 制作的任何内容都非常值得一看。

418
00:25:30,560 --> 00:25:33,763
在这个视频中，我只是想直接介绍现在的注意力机制，

419
00:25:33,763 --> 00:25:36,747
但是如果你对我们是如何达到这里的历程

420
00:25:36,747 --> 00:25:38,985
以及你可能如何为自己重新创新这个想法感到好奇，

421
00:25:38,985 --> 00:25:42,736.3671875
我的朋友 Vivek 最近就发布了几个视频，深入讲解了这个动机。

422
00:25:42,744.21875 --> 00:25:45,746.0000000002328
此外，来自"The Art of the Problem"频道的 Britt Cruz

423
00:25:45,746 --> 00:25:48,460
有一部非常精彩的视频，介绍了大语言模型的历史。

424
00:26:04,960 --> 00:26:09,200
谢谢。
