[
 {
  "translatedText": "Trong chương trước, bạn và tôi đã bắt đầu tìm hiểu hoạt động bên trong của máy biến áp.",
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "translatedText": "Đây là một trong những phần công nghệ quan trọng bên trong các mô hình ngôn ngữ lớn và rất nhiều công cụ khác trong làn sóng AI hiện đại.",
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "translatedText": "Nó lần đầu tiên xuất hiện trong một bài báo nổi tiếng năm 2017 có tên Chú ý là tất cả những gì bạn cần. Trong chương này, bạn và tôi sẽ tìm hiểu cơ chế chú ý này là gì, hình dung cách nó xử lý dữ liệu.",
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "translatedText": "Tóm tắt nhanh, đây là bối cảnh quan trọng mà tôi muốn bạn ghi nhớ.",
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "translatedText": "Mục tiêu của mô hình mà bạn và tôi đang nghiên cứu là tiếp nhận một đoạn văn bản và dự đoán từ nào sẽ xuất hiện tiếp theo.",
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "translatedText": "Văn bản đầu vào được chia thành các phần nhỏ mà chúng ta gọi là mã thông báo và đây thường là các từ hoặc đoạn từ nhưng để giúp bạn và tôi dễ dàng suy nghĩ hơn về các ví dụ trong video này, hãy đơn giản hóa bằng cách giả vờ rằng các mã thông báo là luôn chỉ là lời nói.",
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "translatedText": "Bước đầu tiên trong máy biến áp là liên kết từng mã thông báo với một vectơ chiều cao, cái mà chúng tôi gọi là nhúng.",
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "translatedText": "Ý tưởng quan trọng nhất mà tôi muốn bạn ghi nhớ là làm thế nào các hướng trong không gian chiều cao của tất cả các phần nhúng có thể có này có thể tương ứng với ý nghĩa ngữ nghĩa.",
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "translatedText": "Trong chương trước chúng ta đã thấy một ví dụ về cách hướng có thể tương ứng với giới tính, theo nghĩa là việc thêm một bước nhất định vào không gian này có thể đưa bạn từ việc nhúng danh từ nam tính sang việc nhúng danh từ nữ tính tương ứng.",
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "translatedText": "Đó chỉ là một ví dụ mà bạn có thể tưởng tượng có bao nhiêu hướng khác trong không gian nhiều chiều này có thể tương ứng với nhiều khía cạnh khác của nghĩa của một từ.",
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "translatedText": "Mục đích của máy biến áp là điều chỉnh dần dần các phần nhúng này để chúng không chỉ mã hóa một từ riêng lẻ mà thay vào đó chúng mang ý nghĩa ngữ cảnh phong phú hơn nhiều.",
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "translatedText": "Tôi phải nói trước rằng rất nhiều người thấy cơ chế chú ý, bộ phận quan trọng này trong máy biến áp, rất khó hiểu, vì vậy đừng lo lắng nếu phải mất một thời gian để mọi thứ hiểu rõ.",
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "translatedText": "Tôi nghĩ rằng trước khi chúng ta đi sâu vào chi tiết tính toán và tất cả các phép nhân ma trận, chúng ta nên suy nghĩ về một vài ví dụ về loại hành vi mà chúng ta muốn chú ý kích hoạt.",
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "translatedText": "Hãy xem xét các cụm từ nốt ruồi thực sự của Mỹ, một mol carbon dioxide và lấy sinh thiết nốt ruồi.",
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "translatedText": "Bạn và tôi đều biết rằng từ nốt ruồi có ý nghĩa khác nhau trong mỗi từ này, tùy theo ngữ cảnh.",
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "translatedText": "Nhưng sau bước đầu tiên của biến áp, bước chia nhỏ văn bản và liên kết từng mã thông báo với một vectơ, vectơ liên kết với nốt ruồi sẽ giống nhau trong tất cả các trường hợp này, vì việc nhúng mã thông báo ban đầu này thực sự là một bảng tra cứu không có sự tham khảo đến bối cảnh.",
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "translatedText": "Chỉ ở bước tiếp theo của máy biến áp, các phần nhúng xung quanh mới có cơ hội truyền thông tin vào phần này.",
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "translatedText": "Hình ảnh mà bạn có thể nghĩ đến là có nhiều hướng riêng biệt trong không gian nhúng này mã hóa nhiều ý nghĩa riêng biệt của từ nốt ruồi và khối chú ý được đào tạo bài bản sẽ tính toán những gì bạn cần thêm vào phần nhúng chung để di chuyển nó đến một trong những hướng cụ thể này, như một chức năng của bối cảnh.",
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "translatedText": "Lấy một ví dụ khác, hãy xem xét việc nhúng tháp từ.",
  "input": "To take another example, consider the embedding of the word tower.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "translatedText": "Đây có lẽ là một hướng rất chung chung, không cụ thể nào đó trong không gian, được liên kết với rất nhiều danh từ lớn và cao khác.",
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "translatedText": "Nếu từ này ngay trước Eiffel, bạn có thể tưởng tượng muốn có cơ chế cập nhật vectơ này để nó chỉ theo hướng mã hóa cụ thể hơn tháp Eiffel, có thể tương quan với các vectơ liên quan đến Paris và Pháp và những thứ làm bằng thép.",
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "translatedText": "Nếu trước nó cũng là từ thu nhỏ, thì vectơ phải được cập nhật hơn nữa để nó không còn tương quan với những thứ to lớn, cao lớn.",
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "translatedText": "Tổng quát hơn là chỉ tinh chỉnh nghĩa của một từ, khối chú ý cho phép mô hình di chuyển thông tin được mã hóa từ phần này sang phần nhúng khác, có thể là những thông tin ở khá xa và có khả năng chứa thông tin phong phú hơn nhiều so với chỉ một từ.",
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "translatedText": "Những gì chúng ta đã thấy trong chương trước là sau khi tất cả các vectơ truyền qua mạng, bao gồm nhiều khối chú ý khác nhau, phép tính mà bạn thực hiện để tạo ra dự đoán về mã thông báo tiếp theo hoàn toàn là một hàm của vectơ cuối cùng trong chuỗi.",
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "translatedText": "Ví dụ, hãy tưởng tượng rằng văn bản bạn nhập gần như là toàn bộ một cuốn tiểu thuyết bí ẩn, cho đến điểm gần cuối, nội dung này có nội dung là kẻ sát nhân.",
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "translatedText": "Nếu mô hình dự đoán chính xác từ tiếp theo, thì vectơ cuối cùng trong chuỗi, bắt đầu tồn tại chỉ bằng việc nhúng từ đó, sẽ phải được tất cả các khối chú ý cập nhật để thể hiện nhiều hơn bất kỳ cá nhân nào. từ, bằng cách nào đó mã hóa tất cả thông tin từ cửa sổ ngữ cảnh đầy đủ có liên quan đến việc dự đoán từ tiếp theo.",
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "translatedText": "Tuy nhiên, để thực hiện các bước tính toán, hãy lấy một ví dụ đơn giản hơn nhiều.",
  "input": "To step through the computations, though, let's take a much simpler example.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "translatedText": "Hãy tưởng tượng rằng dữ liệu đầu vào bao gồm cụm từ, một sinh vật có lông màu xanh lam đang lang thang trong khu rừng xanh tươi.",
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "translatedText": "Và hiện tại, giả sử rằng loại cập nhật duy nhất mà chúng ta quan tâm là việc các tính từ điều chỉnh ý nghĩa của danh từ tương ứng của chúng.",
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "translatedText": "Điều tôi sắp mô tả là cái mà chúng ta gọi là một đầu chú ý duy nhất, và sau này chúng ta sẽ thấy khối chú ý bao gồm nhiều đầu khác nhau chạy song song như thế nào.",
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "translatedText": "Một lần nữa, phần nhúng ban đầu cho mỗi từ là một vectơ chiều cao nào đó chỉ mã hóa nghĩa của từ cụ thể đó mà không có ngữ cảnh.",
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "translatedText": "Trên thực tế, điều đó không hoàn toàn đúng.",
  "input": "Actually, that's not quite true.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "translatedText": "Họ cũng mã hóa vị trí của từ.",
  "input": "They also encode the position of the word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "translatedText": "Còn rất nhiều điều để nói về cách các vị trí được mã hóa, nhưng ngay bây giờ, tất cả những gì bạn cần biết là các mục của vectơ này đủ để cho bạn biết từ đó là gì và nó tồn tại ở đâu trong ngữ cảnh.",
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "translatedText": "Hãy tiếp tục và biểu thị các phần nhúng này bằng chữ e.",
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "translatedText": "Mục tiêu là để một loạt các phép tính tạo ra một tập hợp các phần nhúng mới được tinh chỉnh, chẳng hạn như những phần tương ứng với danh từ đã tiếp thu ý nghĩa từ các tính từ tương ứng của chúng.",
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "translatedText": "Và khi chơi trò chơi deep learning, chúng tôi muốn hầu hết các tính toán liên quan trông giống như các tích vectơ ma trận, trong đó các ma trận chứa đầy các trọng số có thể điều chỉnh được, những thứ mà mô hình sẽ học dựa trên dữ liệu.",
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "translatedText": "Để cho rõ ràng, tôi đang tạo ra ví dụ về tính từ cập nhật danh từ chỉ để minh họa loại hành vi mà bạn có thể tưởng tượng rằng một người đang chú ý đang thực hiện.",
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "translatedText": "Cũng như rất nhiều hoạt động học sâu, hành vi thực sự khó phân tích hơn nhiều vì nó dựa trên việc tinh chỉnh và điều chỉnh một số lượng lớn các tham số để giảm thiểu một số hàm chi phí.",
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "translatedText": "Chỉ là khi chúng ta xem qua tất cả các ma trận khác nhau chứa đầy các tham số liên quan đến quá trình này, tôi nghĩ sẽ thực sự hữu ích nếu có một ví dụ tưởng tượng về điều gì đó mà nó có thể làm để giúp mọi thứ cụ thể hơn.",
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "translatedText": "Ở bước đầu tiên của quá trình này, bạn có thể tưởng tượng mỗi danh từ, giống như sinh vật, đặt câu hỏi, này, có tính từ nào ở trước mặt tôi không?",
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "translatedText": "Và đối với những từ có lông tơ và xanh lam, mỗi người có thể trả lời là ừ, tôi là một tính từ và tôi ở vị trí đó.",
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "translatedText": "Câu hỏi đó bằng cách nào đó được mã hóa dưới dạng một vectơ khác, một danh sách các số khác mà chúng tôi gọi là truy vấn cho từ này.",
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "translatedText": "Vectơ truy vấn này mặc dù có kích thước nhỏ hơn nhiều so với vectơ nhúng, chẳng hạn như 128.",
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "translatedText": "Việc tính toán truy vấn này giống như lấy một ma trận nhất định mà tôi sẽ gắn nhãn wq và nhân nó với phép nhúng.",
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "translatedText": "Nén mọi thứ lại một chút, hãy viết vectơ truy vấn đó là q, và sau đó bất cứ khi nào bạn thấy tôi đặt một ma trận bên cạnh một mũi tên như thế này, nó nhằm biểu thị rằng việc nhân ma trận này với vectơ ở đầu mũi tên sẽ cho bạn vectơ tại đầu mũi tên.",
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "translatedText": "Trong trường hợp này, bạn nhân ma trận này với tất cả các phần nhúng trong ngữ cảnh, tạo ra một vectơ truy vấn cho mỗi mã thông báo.",
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "translatedText": "Các mục của ma trận này là các tham số của mô hình, có nghĩa là hành vi thực sự được học từ dữ liệu và trong thực tế, những gì ma trận này thực hiện trong một đầu chú ý cụ thể là một thách thức để phân tích cú pháp.",
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "translatedText": "Nhưng vì lợi ích của chúng ta, hãy tưởng tượng một ví dụ mà chúng ta có thể hy vọng rằng nó sẽ học được, chúng ta sẽ giả sử rằng ma trận truy vấn này ánh xạ các phần nhúng của danh từ theo các hướng nhất định trong không gian truy vấn nhỏ hơn này bằng cách nào đó mã hóa khái niệm tìm kiếm tính từ ở các vị trí trước đó .",
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "translatedText": "Về những gì nó làm với các phần nhúng khác, ai biết được?",
  "input": "As to what it does to other embeddings, who knows?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "translatedText": "Có thể nó đồng thời cố gắng hoàn thành một số mục tiêu khác với những mục tiêu đó.",
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "translatedText": "Hiện tại, chúng tôi đang tập trung vào các danh từ.",
  "input": "Right now, we're laser focused on the nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "translatedText": "Đồng thời, liên kết với điều này là ma trận thứ hai được gọi là ma trận khóa, mà bạn cũng nhân với mỗi phần nhúng.",
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "translatedText": "Điều này tạo ra chuỗi vectơ thứ hai mà chúng ta gọi là khóa.",
  "input": "This produces a second sequence of vectors that we call the keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "translatedText": "Về mặt khái niệm, bạn muốn coi các khóa có khả năng trả lời các truy vấn.",
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "translatedText": "Ma trận khóa này cũng chứa đầy các tham số có thể điều chỉnh được và giống như ma trận truy vấn, nó ánh xạ các vectơ nhúng vào cùng một không gian chiều nhỏ hơn đó.",
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "translatedText": "Bạn coi các khóa giống như các truy vấn bất cứ khi nào chúng liên kết chặt chẽ với nhau.",
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "translatedText": "Trong ví dụ của chúng tôi, bạn sẽ tưởng tượng rằng ma trận khóa ánh xạ các tính từ như bông và xanh lam tới các vectơ được liên kết chặt chẽ với truy vấn do từ tạo ra.",
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "translatedText": "Để đo lường mức độ phù hợp của mỗi khóa với mỗi truy vấn, bạn tính toán tích chấm giữa mỗi cặp khóa-truy vấn có thể có.",
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "translatedText": "Tôi muốn hình dung một lưới chứa đầy các dấu chấm, trong đó các dấu chấm lớn hơn tương ứng với các sản phẩm dấu chấm lớn hơn, những vị trí mà các khóa và truy vấn căn chỉnh.",
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "translatedText": "Đối với ví dụ về danh từ tính từ của chúng ta, nó sẽ trông giống thế này hơn một chút, trong đó nếu các khóa được tạo ra bởi Fluff và Blue thực sự phù hợp chặt chẽ với truy vấn do sinh vật tạo ra, thì tích chấm ở hai điểm này sẽ là một số dương lớn.",
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "translatedText": "Trong biệt ngữ, những người học máy sẽ nói rằng điều này có nghĩa là việc nhúng lông tơ và màu xanh lam có liên quan đến việc nhúng sinh vật.",
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "translatedText": "Ngược lại, tích số chấm giữa khóa của một số từ khác như the và truy vấn dành cho sinh vật sẽ là một giá trị nhỏ hoặc âm nào đó phản ánh không liên quan đến nhau.",
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "translatedText": "Vì vậy, chúng ta có lưới các giá trị này có thể là bất kỳ số thực nào từ âm vô cực đến vô cùng, cho chúng ta điểm về mức độ liên quan của mỗi từ với việc cập nhật nghĩa của mọi từ khác.",
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "translatedText": "Cách chúng tôi sắp sử dụng những điểm số này là lấy một tổng có trọng số nhất định dọc theo mỗi cột, được tính theo mức độ liên quan.",
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "translatedText": "Vì vậy, thay vì có các giá trị nằm trong phạm vi từ vô cực âm đến vô cùng, điều chúng ta muốn là các số trong các cột này nằm trong khoảng từ 0 đến 1 và mỗi cột có tổng bằng 1, như thể chúng là một phân bố xác suất.",
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "translatedText": "Nếu bạn đến từ chương trước, bạn sẽ biết chúng tôi cần phải làm gì.",
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "translatedText": "Chúng tôi tính toán softmax dọc theo mỗi cột này để chuẩn hóa các giá trị.",
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "translatedText": "Trong hình ảnh của chúng tôi, sau khi bạn áp dụng softmax cho tất cả các cột, chúng tôi sẽ điền vào lưới các giá trị chuẩn hóa này.",
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "translatedText": "Tại thời điểm này, bạn có thể an toàn khi coi mỗi cột đều có trọng số tùy theo mức độ liên quan của từ bên trái với giá trị tương ứng ở trên cùng.",
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "translatedText": "Chúng tôi gọi lưới này là một mẫu chú ý.",
  "input": "We call this grid an attention pattern.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "translatedText": "Bây giờ, nếu bạn nhìn vào tờ giấy biến thế ban đầu, có một cách rất nhỏ gọn để họ viết tất cả những điều này ra.",
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "translatedText": "Ở đây, các biến q và k lần lượt biểu thị toàn bộ mảng truy vấn và vectơ khóa, những vectơ nhỏ mà bạn nhận được bằng cách nhân các phần nhúng với truy vấn và ma trận khóa.",
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "translatedText": "Biểu thức ở phần tử số này là một cách thực sự nhỏ gọn để biểu diễn lưới của tất cả các tích số chấm có thể có giữa các cặp khóa và truy vấn.",
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "translatedText": "Một chi tiết kỹ thuật nhỏ mà tôi chưa đề cập đến là để ổn định về số, sẽ rất hữu ích nếu chia tất cả các giá trị này cho căn bậc hai của thứ nguyên trong không gian truy vấn chính đó.",
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "translatedText": "Sau đó, softmax này bao quanh biểu thức đầy đủ được hiểu là áp dụng theo từng cột.",
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "translatedText": "Về thuật ngữ v đó, chúng ta sẽ nói về nó chỉ sau một giây.",
  "input": "As to that v term, we'll talk about it in just a second.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "translatedText": "Trước đó, có một chi tiết kỹ thuật khác mà cho đến nay tôi đã bỏ qua.",
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "translatedText": "Trong quá trình huấn luyện, khi bạn chạy mô hình này trên một ví dụ văn bản nhất định và tất cả các trọng số được điều chỉnh và điều chỉnh một chút để khen thưởng hoặc trừng phạt nó dựa trên xác suất nó gán cho từ đúng tiếp theo trong đoạn văn cao đến mức nào. hóa ra lại làm cho toàn bộ quá trình đào tạo hiệu quả hơn rất nhiều nếu bạn đồng thời dự đoán mọi mã thông báo tiếp theo có thể có sau mỗi chuỗi mã thông báo ban đầu trong đoạn này.",
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "translatedText": "Ví dụ: với cụm từ mà chúng ta đang tập trung vào, nó cũng có thể dự đoán những từ nào theo sau sinh vật và những từ nào theo sau.",
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "translatedText": "Điều này thực sự rất hay, bởi vì nó có nghĩa là nếu không thì một ví dụ huấn luyện đơn lẻ sẽ hoạt động hiệu quả như nhiều ví dụ khác.",
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "translatedText": "Vì mục đích của mô hình chú ý của chúng tôi, điều đó có nghĩa là bạn không bao giờ muốn cho phép những từ sau ảnh hưởng đến những từ trước đó, vì nếu không chúng có thể đưa ra câu trả lời cho những gì tiếp theo.",
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "translatedText": "Điều này có nghĩa là chúng tôi muốn tất cả các điểm này ở đây, những điểm đại diện cho các mã thông báo sau ảnh hưởng đến các điểm trước đó, bằng cách nào đó buộc phải bằng 0.",
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "translatedText": "Điều đơn giản nhất mà bạn có thể nghĩ đến là đặt chúng bằng 0, nhưng nếu bạn làm như vậy thì các cột sẽ không cộng lại thành một nữa, chúng sẽ không được chuẩn hóa.",
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "translatedText": "Vì vậy, thay vào đó, cách phổ biến để thực hiện việc này là trước khi áp dụng softmax, bạn đặt tất cả các mục đó thành âm vô cực.",
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "translatedText": "Nếu bạn làm điều đó, thì sau khi áp dụng softmax, tất cả những thứ đó sẽ chuyển thành 0, nhưng các cột vẫn được chuẩn hóa.",
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "translatedText": "Quá trình này được gọi là mặt nạ.",
  "input": "This process is called masking.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "translatedText": "Có những phiên bản chú ý mà bạn không áp dụng nó, nhưng trong ví dụ GPT của chúng tôi, mặc dù điều này phù hợp hơn trong giai đoạn đào tạo so với việc chạy nó dưới dạng chatbot hoặc thứ gì đó tương tự, bạn vẫn luôn áp dụng việc che giấu này để ngăn chặn các mã thông báo sau ảnh hưởng đến các mã thông báo trước đó.",
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "translatedText": "Một thực tế khác đáng để suy ngẫm về mô hình chú ý này là kích thước của nó bằng bình phương kích thước bối cảnh như thế nào.",
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "translatedText": "Vì vậy, đây là lý do tại sao kích thước ngữ cảnh có thể là một nút cổ chai thực sự lớn đối với các mô hình ngôn ngữ lớn và việc mở rộng quy mô ngữ cảnh là điều không hề nhỏ.",
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "translatedText": "Như bạn tưởng tượng, được thúc đẩy bởi mong muốn có các cửa sổ ngữ cảnh ngày càng lớn hơn, những năm gần đây đã chứng kiến một số biến thể của cơ chế chú ý nhằm làm cho ngữ cảnh có khả năng mở rộng hơn, nhưng ngay tại đây, bạn và tôi đang tập trung vào những điều cơ bản.",
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "translatedText": "Được rồi, tuyệt vời, việc tính toán mẫu này cho phép mô hình suy ra từ nào có liên quan đến từ nào khác.",
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "translatedText": "Bây giờ bạn thực sự cần cập nhật phần nhúng, cho phép các từ truyền thông tin đến bất kỳ từ nào khác có liên quan đến chúng.",
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "translatedText": "Ví dụ: bạn muốn việc nhúng Fluffy bằng cách nào đó gây ra một thay đổi đối với Sinh vật khiến nó di chuyển nó đến một phần khác của không gian nhúng 12.000 chiều này để mã hóa cụ thể hơn một sinh vật Fluffy.",
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "translatedText": "Điều tôi sắp làm ở đây trước tiên là chỉ cho bạn cách đơn giản nhất mà bạn có thể làm điều này, mặc dù có một chút cách để điều này được sửa đổi trong bối cảnh có sự chú ý đa chiều.",
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "translatedText": "Cách đơn giản nhất này là sử dụng ma trận thứ ba, cái mà chúng tôi gọi là ma trận giá trị, mà bạn nhân với việc nhúng từ đầu tiên đó, ví dụ như Fluffy.",
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "translatedText": "Kết quả của việc này là cái mà bạn gọi là vectơ giá trị và đây là thứ bạn thêm vào phần nhúng của từ thứ hai, trong trường hợp này là thứ bạn thêm vào phần nhúng của Sinh vật.",
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "translatedText": "Vì vậy, vectơ giá trị này tồn tại trong cùng một không gian có chiều rất cao như các phần nhúng.",
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "translatedText": "Khi bạn nhân ma trận giá trị này với việc nhúng một từ, bạn có thể nghĩ nó như nói, nếu từ này có liên quan đến việc điều chỉnh ý nghĩa của một cái gì đó khác, thì chính xác thì cần thêm gì vào việc nhúng cái gì đó khác để phản ánh cái này?",
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "translatedText": "Nhìn lại sơ đồ của chúng ta, hãy đặt tất cả các khóa và truy vấn sang một bên, vì sau khi bạn tính toán mẫu chú ý mà bạn đã hoàn thành với các mẫu đó, bạn sẽ lấy ma trận giá trị này và nhân nó với từng phần nhúng đó để tạo ra một chuỗi các vectơ giá trị.",
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "translatedText": "Bạn có thể nghĩ các vectơ giá trị này được liên kết với các khóa tương ứng.",
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "translatedText": "Đối với mỗi cột trong sơ đồ này, bạn nhân từng vectơ giá trị với trọng số tương ứng trong cột đó.",
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "translatedText": "Ví dụ ở đây, khi nhúng Sinh vật, bạn sẽ thêm tỷ lệ lớn các vectơ giá trị cho Fluffy và Blue, trong khi tất cả các vectơ giá trị khác bị loại bỏ hoặc ít nhất gần bằng 0.",
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "translatedText": "Và cuối cùng, cách cập nhật thực sự phần nhúng được liên kết với cột này, trước đó mã hóa một số ý nghĩa không có ngữ cảnh của Sinh vật, bạn cộng tất cả các giá trị đã thay đổi tỷ lệ này vào cột, tạo ra một thay đổi mà bạn muốn thêm, đó là tôi&#39; Tôi sẽ gắn nhãn delta-e, sau đó bạn thêm nó vào phần nhúng ban đầu.",
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "translatedText": "Hy vọng rằng kết quả là một vectơ tinh tế hơn sẽ mã hóa ý nghĩa phong phú hơn về mặt ngữ cảnh, chẳng hạn như ý nghĩa của một sinh vật có lông màu xanh lam.",
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "translatedText": "Và tất nhiên, bạn không chỉ làm điều này với một lần nhúng, bạn áp dụng tổng có trọng số giống nhau trên tất cả các cột trong hình ảnh này, tạo ra một chuỗi các thay đổi, thêm tất cả những thay đổi đó vào các phần nhúng tương ứng, tạo ra một chuỗi đầy đủ các các phần nhúng tinh tế hơn xuất hiện từ khối chú ý.",
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "translatedText": "Thu nhỏ lại, toàn bộ quá trình này là những gì bạn có thể mô tả như một sự chú ý duy nhất.",
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "translatedText": "Như tôi đã mô tả cho đến nay, quy trình này được tham số hóa bằng ba ma trận riêng biệt, tất cả đều chứa các tham số có thể điều chỉnh, khóa, truy vấn và giá trị.",
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "translatedText": "Tôi muốn dành chút thời gian để tiếp tục những gì chúng ta đã bắt đầu ở chương trước, với tính năng ghi điểm trong đó chúng ta đếm tổng số tham số mô hình bằng cách sử dụng các số từ GPT-3.",
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "translatedText": "Mỗi ma trận khóa và truy vấn này có 12.288 cột, khớp với thứ nguyên nhúng và 128 hàng, khớp với thứ nguyên của không gian truy vấn khóa nhỏ hơn đó.",
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "translatedText": "Điều này mang lại cho chúng tôi thêm 1,5 triệu thông số cho mỗi tham số.",
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "translatedText": "Nếu bạn nhìn vào ma trận giá trị đó một cách ngược lại, cách tôi mô tả mọi thứ cho đến nay sẽ gợi ý rằng đó là một ma trận vuông có 12.288 cột và 12.288 hàng, vì cả đầu vào và đầu ra của nó đều nằm trong không gian nhúng rất lớn này.",
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "translatedText": "Nếu đúng, điều đó có nghĩa là có khoảng 150 triệu tham số được thêm vào.",
  "input": "If true, that would mean about 150 million added parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "translatedText": "Và để rõ ràng, bạn có thể làm điều đó.",
  "input": "And to be clear, you could do that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "translatedText": "Bạn có thể dành nhiều tham số cho bản đồ giá trị hơn là cho khóa và truy vấn.",
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "translatedText": "Nhưng trong thực tế, sẽ hiệu quả hơn nhiều nếu thay vào đó bạn thực hiện sao cho số lượng tham số dành cho bản đồ giá trị này giống với số lượng dành cho khóa và truy vấn.",
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "translatedText": "Điều này đặc biệt có liên quan trong cài đặt chạy song song nhiều đầu chú ý.",
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "translatedText": "Giao diện này có nghĩa là bản đồ giá trị được tính thành tích của hai ma trận nhỏ hơn.",
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "translatedText": "Về mặt khái niệm, tôi vẫn khuyến khích bạn suy nghĩ về bản đồ tuyến tính tổng thể, một bản đồ có đầu vào và đầu ra, cả trong không gian nhúng lớn hơn này, chẳng hạn như đưa màu xanh lam vào hướng màu xanh lam mà bạn sẽ thêm vào danh từ.",
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "translatedText": "Chỉ là nó có số lượng hàng nhỏ hơn, thường có cùng kích thước với không gian truy vấn chính.",
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "translatedText": "Điều này có nghĩa là bạn có thể coi nó như ánh xạ các vectơ nhúng lớn xuống một không gian nhỏ hơn nhiều.",
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "translatedText": "Đây không phải là cách đặt tên thông thường, nhưng tôi sẽ gọi đây là ma trận giảm giá trị.",
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "translatedText": "Ma trận thứ hai ánh xạ từ không gian nhỏ hơn này trở lại không gian nhúng, tạo ra các vectơ mà bạn sử dụng để thực hiện các cập nhật thực tế.",
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "translatedText": "Tôi sẽ gọi ma trận này là ma trận tăng giá trị, điều này một lần nữa không mang tính quy ước.",
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "translatedText": "Cách bạn thấy điều này được viết trên hầu hết các tờ báo có vẻ hơi khác một chút.",
  "input": "The way that you would see this written in most papers looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "translatedText": "Tôi sẽ nói về nó trong một phút.",
  "input": "I'll talk about it in a minute.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "translatedText": "Theo tôi, nó có xu hướng khiến mọi thứ trở nên khó hiểu hơn một chút về mặt khái niệm.",
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "translatedText": "Để đưa vào thuật ngữ đại số tuyến tính ở đây, về cơ bản những gì chúng tôi đang làm là hạn chế bản đồ giá trị tổng thể là một phép biến đổi thứ hạng thấp.",
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "translatedText": "Quay trở lại số lượng tham số, cả bốn ma trận này đều có cùng kích thước và cộng tất cả chúng lại chúng ta có được khoảng 6,3 triệu tham số cho một đầu chú ý.",
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "translatedText": "Xin lưu ý nhanh, chính xác hơn một chút, mọi thứ được mô tả cho đến nay đều là thứ mà mọi người gọi là đầu tự chú ý, để phân biệt với một biến thể xuất hiện trong các mô hình khác được gọi là chú ý chéo.",
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "translatedText": "Điều này không liên quan đến ví dụ GPT của chúng tôi, nhưng nếu bạn tò mò, thì sự chú ý chéo liên quan đến các mô hình xử lý hai loại dữ liệu riêng biệt, như văn bản bằng một ngôn ngữ và văn bản bằng một ngôn ngữ khác là một phần của quá trình tạo bản dịch đang diễn ra, hoặc có thể là đầu vào âm thanh của lời nói và phiên âm liên tục.",
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "translatedText": "Một cái đầu gây chú ý chéo trông gần như giống hệt nhau.",
  "input": "A cross-attention head looks almost identical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "translatedText": "Sự khác biệt duy nhất là bản đồ khóa và bản đồ truy vấn hoạt động trên các tập dữ liệu khác nhau.",
  "input": "The only difference is that the key and query maps act on different data sets.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "translatedText": "Ví dụ: trong một mô hình thực hiện dịch thuật, các khóa có thể đến từ một ngôn ngữ, trong khi các truy vấn đến từ một ngôn ngữ khác và mẫu chú ý có thể mô tả những từ nào trong một ngôn ngữ tương ứng với những từ nào trong ngôn ngữ khác.",
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "translatedText": "Và trong cài đặt này thường sẽ không có mặt nạ, vì thực sự không có bất kỳ khái niệm nào về việc mã thông báo sau ảnh hưởng đến mã thông báo trước đó.",
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "translatedText": "Tuy nhiên, hãy tập trung vào sự chú ý của bản thân, nếu bạn hiểu mọi thứ cho đến nay và nếu bạn dừng lại ở đây, bạn sẽ hiểu được bản chất của sự chú ý thực sự là gì.",
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "translatedText": "Tất cả những gì thực sự còn lại đối với chúng tôi là trình bày ý nghĩa của việc bạn thực hiện việc này nhiều lần khác nhau.",
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "translatedText": "Trong ví dụ trung tâm của chúng tôi, chúng tôi tập trung vào tính từ cập nhật danh từ, nhưng tất nhiên có rất nhiều cách khác nhau mà ngữ cảnh có thể ảnh hưởng đến ý nghĩa của một từ.",
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "translatedText": "Nếu từ họ đâm đứng trước từ ô tô thì nó có hàm ý về hình dáng và cấu trúc của chiếc ô tô đó.",
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "translatedText": "Và rất nhiều liên tưởng có thể ít ngữ pháp hơn.",
  "input": "And a lot of associations might be less grammatical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "translatedText": "Nếu từ phù thủy xuất hiện ở bất kỳ đâu trong cùng đoạn văn với Harry, điều đó gợi ý rằng điều này có thể đề cập đến Harry Potter, trong khi thay vào đó, nếu các từ Queen, Sussex và William xuất hiện trong đoạn văn đó, thì có lẽ việc đưa Harry vào thay vào đó nên được cập nhật. để ám chỉ hoàng tử.",
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "translatedText": "Đối với mỗi loại cập nhật theo ngữ cảnh khác nhau mà bạn có thể tưởng tượng, các tham số của các ma trận khóa và truy vấn này sẽ khác nhau để thu hút các mẫu chú ý khác nhau và các tham số của bản đồ giá trị của chúng tôi sẽ khác nhau dựa trên những gì cần được thêm vào phần nhúng.",
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "translatedText": "Và một lần nữa, trên thực tế, hành vi thực sự của những bản đồ này khó diễn giải hơn nhiều, trong đó các trọng số được đặt để làm bất cứ điều gì mà mô hình cần chúng làm để hoàn thành tốt nhất mục tiêu dự đoán mã thông báo tiếp theo.",
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "translatedText": "Như tôi đã nói trước đây, mọi thứ chúng tôi mô tả là một khối chú ý duy nhất và một khối chú ý đầy đủ bên trong máy biến áp bao gồm cái được gọi là chú ý nhiều đầu, trong đó bạn chạy song song nhiều thao tác này, mỗi thao tác có một truy vấn khóa riêng biệt và bản đồ giá trị.",
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "translatedText": "Ví dụ: GPT-3 sử dụng 96 đầu chú ý bên trong mỗi khối.",
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "translatedText": "Vì mỗi câu chuyện đều hơi khó hiểu nên chắc chắn bạn sẽ phải ghi nhớ rất nhiều điều trong đầu.",
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "translatedText": "Nói một cách rõ ràng, điều này có nghĩa là bạn có 96 ma trận khóa và truy vấn riêng biệt tạo ra 96 mẫu chú ý riêng biệt.",
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "translatedText": "Sau đó, mỗi đầu có các ma trận giá trị riêng biệt được sử dụng để tạo ra 96 chuỗi vectơ giá trị.",
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "translatedText": "Tất cả những thứ này được cộng lại với nhau bằng cách sử dụng các mẫu chú ý tương ứng làm trọng số.",
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "translatedText": "Điều này có nghĩa là đối với mỗi vị trí trong ngữ cảnh, mỗi mã thông báo, mỗi đầu trong số này tạo ra một thay đổi được đề xuất để thêm vào phần nhúng ở vị trí đó.",
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "translatedText": "Vì vậy, những gì bạn làm là tổng hợp tất cả những thay đổi được đề xuất đó lại với nhau, một thay đổi cho mỗi đầu và bạn thêm kết quả vào phần nhúng ban đầu của vị trí đó.",
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "translatedText": "Toàn bộ số tiền này ở đây sẽ là một phần của những gì được xuất ra từ khối chú ý nhiều đầu này, một trong những phần nhúng được tinh chỉnh xuất hiện ở đầu bên kia của nó.",
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "translatedText": "Một lần nữa, điều này có rất nhiều điều cần phải suy nghĩ, vì vậy đừng lo lắng nếu bạn phải mất chút thời gian để tìm hiểu.",
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "translatedText": "Ý tưởng tổng thể là bằng cách chạy song song nhiều phần đầu riêng biệt, bạn sẽ cung cấp cho mô hình khả năng tìm hiểu nhiều cách riêng biệt khiến ngữ cảnh thay đổi ý nghĩa.",
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "translatedText": "Tăng tổng số tham số đang chạy của chúng tôi với 96 đầu, mỗi đầu bao gồm biến thể riêng của bốn ma trận này, mỗi khối chú ý nhiều đầu sẽ có khoảng 600 triệu tham số.",
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "translatedText": "Có thêm một điều hơi khó chịu mà tôi thực sự nên đề cập đến cho bất kỳ ai trong số các bạn tiếp tục đọc thêm về máy biến áp.",
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "translatedText": "Bạn còn nhớ tôi đã nói rằng bản đồ giá trị được tính thành hai ma trận riêng biệt này, mà tôi gắn nhãn là ma trận giá trị giảm và ma trận giá trị tăng.",
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "translatedText": "Cách tôi đóng khung mọi thứ sẽ gợi ý rằng bạn nên nhìn thấy cặp ma trận này bên trong mỗi đầu chú ý và bạn hoàn toàn có thể triển khai nó theo cách này.",
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "translatedText": "Đó sẽ là một thiết kế hợp lệ.",
  "input": "That would be a valid design.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "translatedText": "Nhưng cách bạn nhìn thấy điều này được viết trên báo và cách nó được triển khai trong thực tế có vẻ hơi khác một chút.",
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "translatedText": "Tất cả các ma trận tăng giá trị này cho mỗi đầu xuất hiện được ghim lại với nhau trong một ma trận khổng lồ mà chúng tôi gọi là ma trận đầu ra, được liên kết với toàn bộ khối chú ý nhiều đầu.",
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "translatedText": "Và khi bạn thấy mọi người đề cập đến ma trận giá trị cho một đầu chú ý nhất định, họ thường chỉ đề cập đến bước đầu tiên này, bước mà tôi đã gắn nhãn là phép chiếu giá trị xuống không gian nhỏ hơn.",
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "translatedText": "Đối với những ai tò mò, tôi đã để lại ghi chú trên màn hình về điều đó.",
  "input": "For the curious among you, I've left an on-screen note about it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "translatedText": "Đó là một trong những chi tiết có nguy cơ làm xao lãng các điểm khái niệm chính, nhưng tôi muốn nêu ra chỉ để bạn biết nếu bạn đọc về điều này ở các nguồn khác.",
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "translatedText": "Đặt tất cả các sắc thái kỹ thuật sang một bên, trong phần xem trước của chương trước, chúng ta đã thấy cách dữ liệu truyền qua máy biến áp không chỉ truyền qua một khối chú ý duy nhất.",
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "translatedText": "Thứ nhất, nó cũng trải qua các hoạt động khác được gọi là perceptron nhiều lớp.",
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "translatedText": "Chúng ta sẽ nói nhiều hơn về những điều đó trong chương tiếp theo.",
  "input": "We'll talk more about those in the next chapter.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "translatedText": "Và sau đó nó liên tục trải qua rất nhiều bản sao của cả hai hoạt động này.",
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "translatedText": "Điều này có nghĩa là sau khi một từ nhất định thấm nhuần một số ngữ cảnh của nó, sẽ có nhiều cơ hội hơn để việc nhúng nhiều sắc thái hơn này bị ảnh hưởng bởi môi trường xung quanh có nhiều sắc thái hơn của nó.",
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "translatedText": "Bạn càng đi sâu vào mạng, với mỗi phần nhúng ngày càng có nhiều ý nghĩa hơn từ tất cả các phần nhúng khác, bản thân chúng ngày càng có nhiều sắc thái hơn, hy vọng là có khả năng mã hóa các ý tưởng cấp cao hơn và trừu tượng hơn về một nội dung nhất định đầu vào không chỉ là mô tả và cấu trúc ngữ pháp.",
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "translatedText": "Những thứ như tình cảm và giọng điệu, liệu đó có phải là một bài thơ hay không và những sự thật khoa học cơ bản nào có liên quan đến tác phẩm và những thứ tương tự.",
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "translatedText": "Quay lại một lần nữa với quy trình ghi điểm của chúng tôi, GPT-3 bao gồm 96 lớp riêng biệt, do đó, tổng số thông số giá trị và truy vấn chính được nhân với 96 khác, đưa tổng số lên chỉ dưới 58 tỷ thông số riêng biệt dành cho tất cả các đầu chú ý.",
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "translatedText": "Đó là điều chắc chắn rất nhiều, nhưng nó chỉ chiếm khoảng một phần ba trong tổng số 175 tỷ có trong mạng.",
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "translatedText": "Vì vậy, mặc dù mọi sự chú ý đều được chú ý nhưng phần lớn các tham số đều đến từ các khối nằm giữa các bước này.",
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "translatedText": "Trong chương tiếp theo, bạn và tôi sẽ nói nhiều hơn về những khối khác đó cũng như nhiều điều hơn về quá trình đào tạo.",
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "translatedText": "Phần lớn câu chuyện tạo nên sự thành công của cơ chế chú ý không nằm ở bất kỳ loại hành vi cụ thể nào mà nó kích hoạt, mà thực tế là nó có khả năng song song hóa cực kỳ cao, nghĩa là bạn có thể chạy một số lượng lớn các phép tính trong một thời gian ngắn bằng cách sử dụng GPU .",
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "translatedText": "Cho rằng một trong những bài học lớn về học sâu trong một hoặc hai thập kỷ qua là chỉ riêng quy mô đó dường như đã mang lại những cải tiến to lớn về chất lượng trong hiệu suất mô hình, nên có một lợi thế rất lớn đối với các kiến trúc song song hóa cho phép bạn thực hiện điều này.",
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "translatedText": "Nếu bạn muốn tìm hiểu thêm về nội dung này, tôi đã để lại rất nhiều liên kết trong phần mô tả.",
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "translatedText": "Đặc biệt, bất cứ thứ gì do Andrej Karpathy hoặc Chris Ola sản xuất đều có xu hướng là vàng nguyên chất.",
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "translatedText": "Trong video này, tôi chỉ muốn thu hút sự chú ý ở dạng hiện tại, nhưng nếu bạn muốn biết thêm về lịch sử lý do chúng tôi đến đây và cách bạn có thể sáng tạo lại ý tưởng này cho chính mình, bạn tôi Vivek vừa đưa ra một vài video mang lại nhiều động lực hơn.",
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "translatedText": "Ngoài ra, Britt Cruz từ kênh The Art of the problem có một video thực sự hay về lịch sử của các mô hình ngôn ngữ lớn.",
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "translatedText": "Cảm ơn.",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]