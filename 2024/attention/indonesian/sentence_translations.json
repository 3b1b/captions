[
 {
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "translatedText": "Dalam bab terakhir, Anda dan saya mulai melangkah melalui cara kerja internal transformator.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "translatedText": "Ini adalah salah satu bagian penting dari teknologi di dalam model bahasa yang besar, dan banyak alat lain dalam gelombang AI modern.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "translatedText": "Hal ini pertama kali muncul dalam sebuah makalah yang terkenal di tahun 2017 yang berjudul Attention is All You Need, dan dalam bab ini Anda dan saya akan menggali apa itu mekanisme perhatian, memvisualisasikan bagaimana mekanisme ini memproses data.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "translatedText": "Sebagai rangkuman singkat, inilah konteks penting yang saya ingin Anda pikirkan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "translatedText": "Tujuan dari model yang Anda dan saya pelajari adalah untuk mengambil sepotong teks dan memprediksi kata apa yang akan muncul berikutnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "translatedText": "Teks input dipecah menjadi potongan-potongan kecil yang kita sebut token, dan ini sering kali berupa kata atau potongan kata, tetapi untuk membuat contoh dalam video ini lebih mudah untuk Anda dan saya pikirkan, mari kita sederhanakan dengan menganggap token selalu berupa kata.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "translatedText": "Langkah pertama dalam transformator adalah mengasosiasikan setiap token dengan vektor berdimensi tinggi, yang kita sebut sebagai embedding.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "translatedText": "Gagasan terpenting yang saya ingin Anda pikirkan adalah, bagaimana arah dalam ruang dimensi tinggi dari semua kemungkinan penyematan dapat sesuai dengan makna semantik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "translatedText": "Pada bab sebelumnya, kita telah melihat contoh bagaimana arah dapat berhubungan dengan jenis kelamin, dalam arti bahwa menambahkan langkah tertentu dalam ruang ini dapat membawa Anda dari penyematan kata benda maskulin ke penyematan kata benda feminin yang sesuai.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "translatedText": "Itu hanya salah satu contoh yang bisa Anda bayangkan berapa banyak arah lain dalam ruang dimensi tinggi ini yang bisa berhubungan dengan berbagai aspek lain dari makna kata.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "translatedText": "Tujuan dari transformator adalah untuk secara progresif menyesuaikan penyematan ini sehingga tidak hanya menyandikan satu kata saja, tetapi juga menyematkan makna kontekstual yang jauh lebih kaya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "translatedText": "Saya harus mengatakan di depan bahwa banyak orang merasa mekanisme perhatian, bagian penting dalam transformator, sangat membingungkan, jadi jangan khawatir jika perlu waktu untuk meresap ke dalam diri Anda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "translatedText": "Saya pikir sebelum kita menyelami detail komputasi dan semua perkalian matriks, ada baiknya kita memikirkan beberapa contoh untuk jenis perilaku yang kita inginkan untuk diperhatikan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "translatedText": "Pertimbangkan frasa tahi lalat asli Amerika, satu mol karbon dioksida, dan lakukan biopsi tahi lalat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "translatedText": "Anda dan saya tahu bahwa kata tahi lalat memiliki arti yang berbeda dalam setiap kata, berdasarkan konteksnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "translatedText": "Tetapi setelah langkah pertama transformator, yang memecah teks dan mengasosiasikan setiap token dengan vektor, vektor yang diasosiasikan dengan mol akan sama di semua kasus ini, karena penyematan token awal ini secara efektif merupakan tabel pencarian tanpa referensi ke konteks.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "translatedText": "Hanya pada langkah berikutnya dari transformator, baru pada penyematan di sekelilingnya, ada peluang untuk meneruskan informasi ke dalam transformator ini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "translatedText": "Gambaran yang mungkin ada di benak Anda adalah bahwa ada beberapa arah yang berbeda dalam ruang penyematan yang mengkodekan berbagai arti berbeda dari kata mol, dan bahwa blok perhatian yang terlatih dengan baik akan menghitung apa yang perlu Anda tambahkan ke penyematan umum untuk memindahkannya ke salah satu dari arah tertentu ini, sebagai fungsi dari konteksnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "input": "To take another example, consider the embedding of the word tower.",
  "translatedText": "Untuk mengambil contoh lain, pertimbangkan penyematan kata menara.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "translatedText": "Ini mungkin merupakan arah yang sangat umum dan tidak spesifik di dalam ruang, yang terkait dengan banyak kata benda besar dan tinggi lainnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "translatedText": "Jika kata ini langsung didahului oleh Eiffel, Anda dapat membayangkan mekanisme untuk memperbarui vektor ini sehingga menunjuk ke arah yang lebih spesifik mengkodekan menara Eiffel, mungkin berkorelasi dengan vektor yang terkait dengan Paris dan Prancis serta benda-benda yang terbuat dari baja.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "translatedText": "Jika juga didahului oleh kata miniatur, maka vektor harus diperbarui lebih jauh lagi, sehingga tidak lagi berkorelasi dengan benda-benda yang besar dan tinggi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "translatedText": "Secara lebih umum daripada sekadar menyempurnakan arti sebuah kata, blok perhatian memungkinkan model untuk memindahkan informasi yang dikodekan dalam satu penyematan ke penyematan lainnya, yang mungkin saja berjarak cukup jauh, dan berpotensi dengan informasi yang jauh lebih kaya daripada hanya satu kata.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "translatedText": "Apa yang kita lihat pada bab sebelumnya adalah bagaimana setelah semua vektor mengalir melalui jaringan, termasuk banyak blok perhatian yang berbeda, komputasi yang Anda lakukan untuk menghasilkan prediksi token berikutnya sepenuhnya merupakan fungsi dari vektor terakhir dalam urutan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "translatedText": "Bayangkan, misalnya, teks yang Anda masukkan adalah sebagian besar dari keseluruhan novel misteri, sampai ke titik di dekat akhir, yang berbunyi, oleh karena itu, pembunuhnya adalah.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "translatedText": "Jika model akan secara akurat memprediksi kata berikutnya, vektor terakhir dalam urutan tersebut, yang memulai kehidupannya hanya dengan menyematkan kata itu, harus telah diperbarui oleh semua blok perhatian untuk mewakili lebih banyak, lebih banyak daripada kata individual, entah bagaimana mengkodekan semua informasi dari jendela konteks lengkap yang relevan untuk memprediksi kata berikutnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "input": "To step through the computations, though, let's take a much simpler example.",
  "translatedText": "Namun, untuk melangkah lebih jauh dalam perhitungan, mari kita ambil contoh yang lebih sederhana.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "translatedText": "Bayangkan bahwa masukannya mencakup frasa, makhluk biru yang lembut berkeliaran di hutan yang hijau.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "translatedText": "Dan untuk saat ini, anggaplah satu-satunya jenis pembaruan yang kita pedulikan adalah membuat kata sifat menyesuaikan arti kata benda yang sesuai.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "translatedText": "Apa yang akan saya jelaskan adalah apa yang kita sebut sebagai satu kepala perhatian, dan nanti kita akan melihat, bagaimana blok perhatian terdiri atas banyak kepala perhatian yang berbeda, yang berjalan secara paralel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "translatedText": "Sekali lagi, penyematan awal untuk setiap kata adalah beberapa vektor dimensi tinggi yang hanya mengkodekan arti kata tertentu tanpa konteks.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "input": "Actually, that's not quite true.",
  "translatedText": "Sebenarnya, hal itu tidak sepenuhnya benar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "input": "They also encode the position of the word.",
  "translatedText": "Mereka juga menyandikan posisi kata.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "translatedText": "Masih banyak lagi yang dapat dikatakan tentang cara pengkodean posisi, tetapi saat ini, yang perlu Anda ketahui adalah bahwa entri vektor ini sudah cukup untuk memberi tahu Anda apa kata itu dan di mana kata itu berada dalam konteksnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "translatedText": "Mari kita lanjutkan dan tandai penyematan ini dengan huruf e.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "translatedText": "Tujuannya adalah agar serangkaian komputasi menghasilkan serangkaian penyematan baru yang disempurnakan di mana, misalnya, kata benda yang sesuai dengan kata benda tersebut telah menyerap makna dari kata sifat yang sesuai.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "translatedText": "Dan dalam permainan deep learning, kami ingin sebagian besar komputasi yang terlibat terlihat seperti produk vektor-matriks, di mana matriks penuh dengan bobot yang dapat disetel, hal-hal yang akan dipelajari oleh model berdasarkan data.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "translatedText": "Untuk memperjelas, saya membuat contoh kata sifat yang memperbarui kata benda ini hanya untuk mengilustrasikan jenis perilaku yang dapat Anda bayangkan dilakukan oleh seorang kepala yang penuh perhatian.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "translatedText": "Seperti halnya banyak pembelajaran mendalam, perilaku sebenarnya jauh lebih sulit untuk diuraikan karena didasarkan pada penyesuaian dan penyetelan sejumlah besar parameter untuk meminimalkan beberapa fungsi biaya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "translatedText": "Hanya saja, saat kita melangkah melalui semua matriks berbeda yang diisi dengan parameter yang terlibat dalam proses ini, saya pikir akan sangat membantu jika kita memiliki contoh yang bisa dibayangkan tentang sesuatu yang bisa dilakukan untuk membantu membuatnya lebih konkret.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "translatedText": "Untuk langkah pertama dari proses ini, Anda bisa membayangkan setiap kata benda, seperti makhluk, mengajukan pertanyaan, hei, apakah ada kata sifat yang ada di depan saya?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "translatedText": "Dan untuk kata fluffy dan blue, masing-masing bisa menjawab, ya, saya adalah kata sifat dan saya berada di posisi itu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "translatedText": "Pertanyaan itu entah bagaimana dikodekan sebagai vektor lain, daftar angka lain, yang kita sebut kueri untuk kata ini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "translatedText": "Vektor kueri ini memiliki dimensi yang jauh lebih kecil daripada vektor embedding, katakanlah 128.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "translatedText": "Menghitung kueri ini terlihat seperti mengambil matriks tertentu, yang akan saya beri label wq, dan mengalikannya dengan embedding.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "translatedText": "Untuk lebih memperjelas, mari kita tuliskan vektor kueri tersebut sebagai q, lalu kapan pun Anda melihat saya meletakkan matriks di samping panah seperti ini, ini dimaksudkan untuk merepresentasikan bahwa mengalikan matriks ini dengan vektor di awal panah akan menghasilkan vektor di akhir panah.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "translatedText": "Dalam kasus ini, Anda mengalikan matriks ini dengan semua penyematan dalam konteks, menghasilkan satu vektor kueri untuk setiap token.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "translatedText": "Entri dari matriks ini adalah parameter dari model, yang berarti perilaku sebenarnya dipelajari dari data, dan dalam praktiknya, apa yang dilakukan matriks ini di kepala perhatian tertentu sulit untuk diuraikan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "translatedText": "Namun demi kepentingan kita, dengan membayangkan sebuah contoh yang kita harapkan dapat dipelajari, kita anggap saja matriks kueri ini memetakan penyematan kata benda ke arah tertentu dalam ruang kueri yang lebih kecil ini yang entah bagaimana mengkodekan gagasan untuk mencari kata sifat di posisi sebelumnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "input": "As to what it does to other embeddings, who knows?",
  "translatedText": "Mengenai apa yang dilakukannya pada penyematan lainnya, siapa yang tahu?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "translatedText": "Mungkin secara bersamaan mencoba untuk mencapai beberapa tujuan lain dengan itu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "input": "Right now, we're laser focused on the nouns.",
  "translatedText": "Saat ini, kami memfokuskan laser pada kata benda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "translatedText": "Pada saat yang sama, yang terkait dengan ini adalah matriks kedua yang disebut matriks kunci, yang juga Anda kalikan dengan setiap penyematan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "input": "This produces a second sequence of vectors that we call the keys.",
  "translatedText": "Hal ini menghasilkan urutan vektor kedua yang kita sebut sebagai kunci.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "translatedText": "Secara konseptual, Anda ingin memikirkan kunci yang berpotensi menjawab pertanyaan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "translatedText": "Matriks kunci ini juga penuh dengan parameter yang dapat disetel, dan seperti matriks kueri, matriks ini memetakan vektor penyisipan ke ruang dimensi yang lebih kecil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "translatedText": "Anda menganggap kunci sebagai pencocokan kueri setiap kali mereka saling sejajar satu sama lain.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "translatedText": "Dalam contoh kita, Anda dapat membayangkan bahwa matriks kunci memetakan kata sifat seperti fluffy dan blue ke vektor yang selaras dengan kueri yang dihasilkan oleh kata makhluk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "translatedText": "Untuk mengukur seberapa baik setiap kunci cocok dengan setiap kueri, Anda menghitung dot product antara setiap pasangan kunci-kueri yang mungkin.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "translatedText": "Saya suka memvisualisasikan kisi-kisi yang penuh dengan sekumpulan titik, di mana titik-titik yang lebih besar sesuai dengan produk titik yang lebih besar, tempat di mana tombol dan kueri sejajar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "translatedText": "Untuk contoh kata sifat kata benda kita, akan terlihat seperti ini, di mana jika kunci yang dihasilkan oleh fluffy dan blue benar-benar selaras dengan kueri yang dihasilkan oleh creature, maka hasil kali titik di kedua titik tersebut akan menjadi angka positif yang besar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "translatedText": "Dalam istilah pembelajaran mesin, orang-orang akan mengatakan bahwa ini berarti penyematan bulu halus dan biru menghadiri penyematan makhluk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "translatedText": "Sebaliknya, dot product antara kunci untuk beberapa kata lain seperti dan kueri untuk makhluk akan menjadi beberapa nilai kecil atau negatif yang mencerminkan bahwa mereka tidak terkait satu sama lain.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "translatedText": "Jadi, kita memiliki kisi-kisi nilai yang dapat berupa bilangan real dari negatif tak terhingga hingga tak terhingga, yang memberi kita skor untuk seberapa relevan setiap kata untuk memperbarui makna setiap kata lainnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "translatedText": "Cara kita akan menggunakan skor ini adalah dengan mengambil jumlah tertimbang tertentu di setiap kolom, yang dibobotkan berdasarkan relevansi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "translatedText": "Jadi, alih-alih memiliki rentang nilai dari negatif tak terhingga hingga tak terhingga, yang kita inginkan adalah angka-angka dalam kolom-kolom ini berada di antara 0 dan 1, dan untuk setiap kolom dijumlahkan hingga 1, seolah-olah itu adalah distribusi probabilitas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "translatedText": "Jika Anda membaca bab sebelumnya, Anda pasti tahu apa yang harus kita lakukan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "translatedText": "Kami menghitung softmax di sepanjang masing-masing kolom ini untuk menormalkan nilainya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "translatedText": "Pada gambar kita, setelah Anda menerapkan softmax ke semua kolom, kita akan mengisi grid dengan nilai-nilai yang dinormalisasi ini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "translatedText": "Pada titik ini, Anda dapat menganggap setiap kolom sebagai pemberian bobot sesuai dengan seberapa relevan kata di sebelah kiri dengan nilai yang sesuai di bagian atas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "input": "We call this grid an attention pattern.",
  "translatedText": "Kami menyebut kisi-kisi ini sebagai pola perhatian.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "translatedText": "Sekarang, jika Anda melihat kertas transformator asli, ada cara yang sangat ringkas untuk menuliskan semua ini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "translatedText": "Di sini, variabel q dan k masing-masing mewakili larik penuh dari vektor kueri dan kunci, vektor-vektor kecil yang Anda dapatkan dengan mengalikan embedding dengan kueri dan matriks kunci.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "translatedText": "Ekspresi di pembilang ini adalah cara yang sangat ringkas untuk merepresentasikan kisi-kisi dari semua produk titik yang mungkin terjadi antara pasangan kunci dan kueri.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "translatedText": "Detail teknis kecil yang tidak saya sebutkan adalah bahwa untuk stabilitas numerik, akan sangat membantu untuk membagi semua nilai ini dengan akar kuadrat dari dimensi dalam ruang kueri kunci tersebut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "translatedText": "Kemudian, softmax yang melingkari ekspresi penuh ini dimaksudkan untuk dipahami untuk menerapkan kolom demi kolom.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "input": "As to that v term, we'll talk about it in just a second.",
  "translatedText": "Mengenai istilah v, kita akan membicarakannya sebentar lagi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "translatedText": "Sebelum itu, ada satu detail teknis lain yang sejauh ini saya lewati.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "translatedText": "Selama proses pelatihan, ketika Anda menjalankan model ini pada contoh teks yang diberikan, dan semua bobot sedikit disesuaikan dan disetel untuk memberi penghargaan atau hukuman berdasarkan seberapa tinggi probabilitas yang diberikan pada kata yang benar berikutnya dalam bagian tersebut, ternyata membuat seluruh proses pelatihan jauh lebih efisien jika Anda secara bersamaan memintanya untuk memprediksi setiap token berikutnya yang mungkin mengikuti setiap token awal dalam bagian ini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "translatedText": "Misalnya, dengan frasa yang telah kita fokuskan, mungkin juga dapat memprediksi kata apa yang mengikuti makhluk dan kata apa yang mengikuti the.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "translatedText": "Ini sangat bagus, karena ini berarti apa yang seharusnya menjadi contoh pelatihan tunggal, secara efektif bertindak sebagai banyak contoh.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "translatedText": "Untuk tujuan pola perhatian kita, ini berarti bahwa Anda tidak akan pernah membiarkan kata-kata yang muncul belakangan mempengaruhi kata-kata yang muncul sebelumnya, karena jika tidak, kata-kata tersebut bisa memberikan jawaban untuk apa yang akan muncul berikutnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "translatedText": "Artinya, kita ingin semua titik di sini, yang mewakili token-token yang muncul belakangan dan mempengaruhi token-token sebelumnya, entah bagaimana dipaksa menjadi nol.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "translatedText": "Hal paling sederhana yang mungkin Anda pikirkan untuk dilakukan adalah mengaturnya sama dengan nol, tetapi jika Anda melakukan hal tersebut, kolom-kolomnya tidak akan berjumlah satu lagi, mereka tidak akan dinormalisasi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "translatedText": "Jadi, cara umum untuk melakukan ini adalah sebelum menerapkan softmax, Anda mengatur semua entri tersebut menjadi tak terhingga negatif.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "translatedText": "Jika Anda melakukan hal itu, maka setelah menerapkan softmax, semua itu akan berubah menjadi nol, tetapi kolom-kolomnya tetap dinormalisasi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "input": "This process is called masking.",
  "translatedText": "Proses ini disebut masking.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "translatedText": "Ada beberapa versi perhatian di mana Anda tidak menerapkannya, tetapi dalam contoh GPT kami, meskipun ini lebih relevan selama fase pelatihan daripada, katakanlah, menjalankannya sebagai chatbot atau semacamnya, Anda selalu menerapkan penyamaran ini untuk mencegah token yang lebih baru memengaruhi token yang lebih awal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "translatedText": "Fakta lain yang patut direnungkan mengenai pola perhatian ini adalah, bahwa ukurannya sama dengan kuadrat ukuran konteks.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "translatedText": "Jadi, inilah mengapa ukuran konteks dapat menjadi hambatan yang sangat besar untuk model bahasa yang besar, dan meningkatkannya bukanlah hal yang sepele.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "translatedText": "Seperti yang Anda bayangkan, termotivasi oleh keinginan untuk jendela konteks yang lebih besar dan lebih besar, beberapa tahun terakhir ini telah ada beberapa variasi pada mekanisme perhatian yang bertujuan untuk membuat konteks lebih terukur, tetapi di sini, Anda dan saya tetap fokus pada hal-hal mendasar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "translatedText": "Oke, bagus, dengan menghitung pola ini, model dapat menyimpulkan kata mana yang relevan dengan kata lainnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "translatedText": "Sekarang Anda harus benar-benar memperbarui penyematan, sehingga kata-kata dapat meneruskan informasi ke kata-kata lain yang relevan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "translatedText": "Misalnya, Anda ingin penyematan Fluffy entah bagaimana menyebabkan perubahan pada Makhluk yang memindahkannya ke bagian lain dari ruang penyematan 12.000 dimensi ini yang secara lebih spesifik mengkodekan makhluk Fluffy.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "translatedText": "Apa yang akan saya lakukan di sini, pertama-tama saya akan menunjukkan kepada Anda cara yang paling mudah untuk melakukan ini, meskipun ada sedikit cara untuk memodifikasi hal ini dalam konteks perhatian multi-kepala.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "translatedText": "Cara yang paling mudah adalah dengan menggunakan matriks ketiga, yang kami sebut matriks nilai, yang Anda kalikan dengan penyematan kata pertama, misalnya Fluffy.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "translatedText": "Hasil dari ini adalah apa yang Anda sebut sebagai vektor nilai, dan ini adalah sesuatu yang Anda tambahkan ke penyematan kata kedua, dalam hal ini sesuatu yang Anda tambahkan ke penyematan Makhluk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "translatedText": "Jadi, vektor nilai ini berada dalam ruang dimensi yang sangat tinggi yang sama dengan penyematan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "translatedText": "Ketika Anda mengalikan matriks nilai ini dengan penyematan sebuah kata, Anda dapat menganggapnya sebagai mengatakan, jika kata ini relevan untuk menyesuaikan makna sesuatu yang lain, apa yang sebenarnya harus ditambahkan pada penyematan sesuatu yang lain tersebut untuk merefleksikan hal ini?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "translatedText": "Melihat kembali ke diagram kita, mari kita sisihkan semua kunci dan kueri, karena setelah Anda menghitung pola perhatian, Anda selesai dengan itu, maka Anda akan mengambil matriks nilai ini dan mengalikannya dengan setiap penyematan untuk menghasilkan urutan vektor nilai.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "translatedText": "Anda mungkin berpikir bahwa vektor nilai ini terkait dengan kunci yang sesuai.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "translatedText": "Untuk setiap kolom dalam diagram ini, Anda mengalikan setiap vektor nilai dengan bobot yang sesuai di kolom tersebut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "translatedText": "Sebagai contoh di sini, di bawah penyematan Creature, Anda akan menambahkan sebagian besar vektor nilai untuk Fluffy dan Blue, sementara semua vektor nilai lainnya akan dinolkan, atau setidaknya hampir dinolkan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "translatedText": "Dan akhirnya, cara untuk benar-benar memperbarui penyematan yang terkait dengan kolom ini, yang sebelumnya mengkodekan beberapa makna bebas konteks dari Makhluk, Anda menambahkan semua nilai yang diskalakan ulang ini di kolom, menghasilkan perubahan yang ingin Anda tambahkan, yang akan saya beri label delta-e, dan kemudian Anda menambahkannya ke penyematan asli.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "translatedText": "Semoga yang dihasilkan adalah vektor yang lebih halus yang mengkodekan makna yang lebih kaya secara kontekstual, seperti makhluk biru yang lembut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "translatedText": "Dan tentu saja, Anda tidak hanya melakukan ini pada satu embedding, Anda menerapkan jumlah tertimbang yang sama pada semua kolom dalam gambar ini, menghasilkan urutan perubahan, menambahkan semua perubahan tersebut ke embedding yang sesuai, menghasilkan urutan penuh embedding yang lebih halus, yang muncul dari blok perhatian.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "translatedText": "Dengan memperkecil ukuran, seluruh proses ini bisa Anda gambarkan sebagai satu pusat perhatian.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "translatedText": "Seperti yang telah saya jelaskan sejauh ini, proses ini diparameterkan oleh tiga matriks yang berbeda, semuanya diisi dengan parameter yang dapat disetel, kunci, kueri, dan nilai.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "translatedText": "Saya ingin mengambil waktu sejenak untuk melanjutkan apa yang telah kita mulai di bab sebelumnya, dengan pencatatan skor di mana kita menghitung jumlah total parameter model dengan menggunakan angka-angka dari GPT-3.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "translatedText": "Matriks kunci dan kueri ini masing-masing memiliki 12.288 kolom, yang sesuai dengan dimensi penyematan, dan 128 baris, yang sesuai dengan dimensi ruang kueri kunci yang lebih kecil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "translatedText": "Hal ini memberi kita tambahan sekitar 1,5 juta parameter untuk setiap parameter.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "translatedText": "Jika Anda melihat matriks nilai tersebut secara kontras, cara saya menggambarkannya sejauh ini akan menunjukkan bahwa ini adalah matriks persegi yang memiliki 12.288 kolom dan 12.288 baris, karena input dan outputnya berada dalam ruang penyisipan yang sangat besar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "input": "If true, that would mean about 150 million added parameters.",
  "translatedText": "Jika benar, itu berarti sekitar 150 juta parameter yang ditambahkan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "input": "And to be clear, you could do that.",
  "translatedText": "Dan untuk lebih jelasnya, Anda bisa melakukannya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "translatedText": "Anda dapat mencurahkan lebih banyak parameter pada peta nilai daripada kunci dan kueri.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "translatedText": "Namun dalam praktiknya, akan jauh lebih efisien jika Anda membuatnya agar jumlah parameter yang dikhususkan untuk peta nilai ini sama dengan jumlah parameter yang dikhususkan untuk kunci dan kueri.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "translatedText": "Hal ini khususnya relevan dalam pengaturan menjalankan beberapa kepala perhatian secara paralel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "translatedText": "Tampilannya adalah bahwa peta nilai difaktorkan sebagai produk dari dua matriks yang lebih kecil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "translatedText": "Secara konseptual, saya masih akan mendorong Anda untuk memikirkan tentang keseluruhan peta linear, yang memiliki input dan output, keduanya dalam ruang penyematan yang lebih besar ini, misalnya mengambil penyematan warna biru ke arah kebiruan yang akan Anda tambahkan ke kata benda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "translatedText": "Hanya saja, jumlah barisnya lebih sedikit, biasanya berukuran sama dengan ruang kueri kunci.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "translatedText": "Artinya, Anda bisa menganggapnya sebagai pemetaan vektor penyematan yang besar ke ruang yang jauh lebih kecil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "translatedText": "Ini bukanlah penamaan konvensional, tetapi saya akan menyebutnya sebagai matriks penurunan nilai.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "translatedText": "Matriks kedua memetakan dari ruang yang lebih kecil ini kembali ke ruang penyematan, menghasilkan vektor yang Anda gunakan untuk membuat pembaruan yang sebenarnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "translatedText": "Saya akan menyebutnya sebagai matriks nilai naik, yang sekali lagi tidak konvensional.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "input": "The way that you would see this written in most papers looks a little different.",
  "translatedText": "Cara Anda melihat tulisan ini di sebagian besar koran, terlihat sedikit berbeda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "input": "I'll talk about it in a minute.",
  "translatedText": "Saya akan membicarakannya sebentar lagi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "translatedText": "Menurut pendapat saya, hal ini cenderung membuat segala sesuatunya sedikit lebih membingungkan secara konseptual.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "translatedText": "Untuk menggunakan jargon aljabar linier di sini, pada dasarnya yang kita lakukan adalah membatasi peta nilai keseluruhan menjadi transformasi peringkat rendah.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "translatedText": "Kembali ke hitungan parameter, keempat matriks ini memiliki ukuran yang sama, dan dengan menjumlahkan semuanya, kita mendapatkan sekitar 6,3 juta parameter untuk satu attention head.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "translatedText": "Sebagai catatan tambahan, agar sedikit lebih akurat, semua yang dijelaskan sejauh ini adalah apa yang orang sebut sebagai kepala perhatian diri, untuk membedakannya dari variasi yang muncul dalam model lain, yang disebut perhatian silang.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "translatedText": "Hal ini tidak relevan dengan contoh GPT kami, tetapi jika Anda penasaran, perhatian silang melibatkan model yang memproses dua jenis data yang berbeda, seperti teks dalam satu bahasa dan teks dalam bahasa lain yang merupakan bagian dari pembuatan terjemahan yang sedang berlangsung, atau mungkin input audio dari ucapan dan transkripsi yang sedang berlangsung.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "input": "A cross-attention head looks almost identical.",
  "translatedText": "Kepala yang saling silang terlihat hampir sama.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "input": "The only difference is that the key and query maps act on different data sets.",
  "translatedText": "Satu-satunya perbedaan adalah bahwa peta kunci dan kueri bekerja pada set data yang berbeda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "translatedText": "Dalam sebuah model yang melakukan penerjemahan, misalnya, kunci mungkin berasal dari satu bahasa, sementara kueri berasal dari bahasa lain, dan pola perhatian dapat menggambarkan kata mana dari satu bahasa yang sesuai dengan kata mana dalam bahasa lain.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "translatedText": "Dan dalam pengaturan ini biasanya tidak akan ada masking, karena tidak ada gagasan bahwa token yang lebih baru akan memengaruhi token yang lebih awal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "translatedText": "Namun, dengan tetap fokus pada perhatian diri, jika Anda memahami semuanya sejauh ini, dan jika Anda berhenti di sini, Anda akan mendapatkan esensi dari apa sebenarnya perhatian itu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "translatedText": "Yang tersisa bagi kita hanyalah menjabarkan pengertian tentang bagaimana Anda melakukan hal ini berkali-kali.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "translatedText": "Dalam contoh utama kita, kita berfokus pada kata sifat yang memperbarui kata benda, tetapi tentu saja ada banyak cara yang berbeda yang dapat mempengaruhi arti sebuah kata.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "translatedText": "Jika kata mereka menabrak didahului kata mobil, hal ini berimplikasi pada bentuk dan struktur mobil tersebut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "input": "And a lot of associations might be less grammatical.",
  "translatedText": "Dan banyak asosiasi yang mungkin kurang gramatikal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "translatedText": "Jika kata penyihir ada di bagian yang sama dengan Harry, ini menunjukkan bahwa ini mungkin merujuk pada Harry Potter, sedangkan jika kata Queen, Sussex, dan William ada di bagian itu, maka mungkin penyematan Harry harus diperbarui untuk merujuk pada pangeran.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "translatedText": "Untuk setiap jenis pembaruan kontekstual yang berbeda yang dapat Anda bayangkan, parameter matriks kunci dan kueri ini akan berbeda untuk menangkap pola perhatian yang berbeda, dan parameter peta nilai kami akan berbeda berdasarkan apa yang harus ditambahkan ke sematan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "translatedText": "Dan sekali lagi, dalam praktiknya, perilaku sebenarnya dari peta-peta ini jauh lebih sulit untuk ditafsirkan, di mana bobot-bobot diatur untuk melakukan apa pun yang dibutuhkan oleh model untuk mencapai tujuan terbaiknya dalam memprediksi token berikutnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "translatedText": "Seperti yang saya katakan sebelumnya, semua yang kami jelaskan adalah satu kepala perhatian, dan blok perhatian penuh di dalam transformator terdiri dari apa yang disebut perhatian multi-kepala, di mana Anda menjalankan banyak operasi ini secara paralel, masing-masing dengan kueri kunci dan peta nilainya yang berbeda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "translatedText": "GPT-3 misalnya menggunakan 96 kepala perhatian di dalam setiap blok.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "translatedText": "Mengingat masing-masing sudah agak membingungkan, tentu saja banyak hal yang harus dipikirkan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "translatedText": "Untuk lebih jelasnya, ini berarti Anda memiliki 96 matriks kunci dan kueri yang berbeda yang menghasilkan 96 pola perhatian yang berbeda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "translatedText": "Kemudian setiap kepala memiliki matriks nilai yang berbeda yang digunakan untuk menghasilkan 96 urutan vektor nilai.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "translatedText": "Semua ini ditambahkan bersama-sama dengan menggunakan pola perhatian yang sesuai sebagai bobot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "translatedText": "Artinya, untuk setiap posisi dalam konteks, setiap token, setiap kepala ini menghasilkan perubahan yang diusulkan untuk ditambahkan ke penyematan di posisi tersebut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "translatedText": "Jadi, yang Anda lakukan adalah menjumlahkan semua perubahan yang diusulkan, satu untuk setiap kepala, dan menambahkan hasilnya ke penyematan asli posisi tersebut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "translatedText": "Keseluruhan jumlah di sini akan menjadi satu irisan dari apa yang dihasilkan dari blok perhatian berkepala banyak ini, satu dari sekian banyak penyematan halus yang keluar dari ujungnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "translatedText": "Sekali lagi, ini adalah hal yang perlu dipikirkan, jadi jangan khawatir jika perlu waktu untuk meresap.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "translatedText": "Ide keseluruhannya adalah bahwa dengan menjalankan banyak kepala yang berbeda secara paralel, Anda memberikan model kapasitas untuk mempelajari berbagai cara yang berbeda dalam konteks yang mengubah makna.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "translatedText": "Menarik penghitungan berjalan kami untuk jumlah parameter dengan 96 kepala, masing-masing termasuk variasinya sendiri dari empat matriks ini, setiap blok perhatian multi-kepala berakhir dengan sekitar 600 juta parameter.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "translatedText": "Ada satu hal tambahan yang sedikit mengganggu yang harus saya sebutkan bagi Anda yang membaca lebih lanjut tentang transformer.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "translatedText": "Anda ingat bagaimana saya mengatakan bahwa peta nilai diperhitungkan ke dalam dua matriks yang berbeda, yang saya beri label sebagai matriks nilai turun dan nilai naik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "translatedText": "Cara saya membingkai berbagai hal akan menyarankan agar Anda melihat sepasang matriks di dalam setiap kepala perhatian, dan Anda bisa menerapkannya dengan cara ini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "input": "That would be a valid design.",
  "translatedText": "Itu akan menjadi desain yang valid.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "translatedText": "Tetapi, cara Anda melihat hal ini tertulis di kertas dan cara penerapannya dalam praktik, terlihat sedikit berbeda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "translatedText": "Semua matriks nilai untuk setiap kepala ini tampak disatukan dalam satu matriks raksasa yang kita sebut matriks output, yang terkait dengan seluruh blok perhatian multi-kepala.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "translatedText": "Dan ketika Anda melihat orang merujuk ke matriks nilai untuk kepala perhatian tertentu, mereka biasanya hanya merujuk ke langkah pertama ini, langkah yang saya beri label sebagai proyeksi nilai ke bawah ke dalam ruang yang lebih kecil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "input": "For the curious among you, I've left an on-screen note about it.",
  "translatedText": "Bagi Anda yang penasaran, saya telah meninggalkan catatan di layar mengenai hal ini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "translatedText": "Ini adalah salah satu detail yang berisiko mengalihkan perhatian dari poin konseptual utama, tetapi saya ingin menyebutkannya, supaya Anda tahu, jika Anda membaca tentang hal ini di sumber lain.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "translatedText": "Mengesampingkan semua nuansa teknis, dalam pratinjau dari bab sebelumnya, kita telah melihat bagaimana data yang mengalir melalui transformator tidak hanya mengalir melalui satu blok perhatian.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "translatedText": "Untuk satu hal, ini juga melalui operasi lain yang disebut multi-layer perceptron.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "input": "We'll talk more about those in the next chapter.",
  "translatedText": "Kita akan membahas lebih lanjut tentang hal itu di bab berikutnya.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "translatedText": "Dan kemudian berulang kali melalui banyak salinan dari kedua operasi ini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "translatedText": "Artinya, setelah sebuah kata menyerap sebagian dari konteksnya, ada lebih banyak kesempatan untuk penyematan yang lebih bernuansa ini dipengaruhi oleh lingkungannya yang lebih bernuansa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "translatedText": "Semakin jauh Anda masuk ke dalam jaringan, dengan setiap penyematan mengambil lebih banyak makna dari semua penyematan lainnya, yang semakin bernuansa, harapannya adalah bahwa ada kapasitas untuk menyandikan ide yang lebih tinggi dan lebih abstrak tentang input yang diberikan di luar sekadar deskriptor dan struktur tata bahasa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "translatedText": "Hal-hal seperti sentimen dan nada, dan apakah itu sebuah puisi dan kebenaran ilmiah apa yang mendasari karya tersebut, dan hal-hal seperti itu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "translatedText": "Kembali lagi ke pencatatan skor kami, GPT-3 mencakup 96 lapisan yang berbeda, sehingga jumlah total kueri kunci dan parameter nilai dikalikan dengan 96 lainnya, yang membuat jumlah totalnya menjadi kurang dari 58 miliar parameter yang berbeda yang dikhususkan untuk semua perhatian.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "translatedText": "Jumlahnya memang banyak, tetapi hanya sekitar sepertiga dari total 175 miliar yang ada dalam jaringan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "translatedText": "Jadi, meskipun perhatian mendapat semua perhatian, namun sebagian besar parameter berasal dari blok yang berada di antara langkah-langkah ini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "translatedText": "Di bab berikutnya, Anda dan saya akan membahas lebih banyak tentang blok-blok lainnya dan juga lebih banyak lagi tentang proses pelatihan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "translatedText": "Bagian terbesar dari keberhasilan mekanisme perhatian bukanlah jenis perilaku tertentu yang dimungkinkan, tetapi fakta bahwa mekanisme ini sangat dapat diparalelkan, yang berarti Anda dapat menjalankan sejumlah besar komputasi dalam waktu singkat dengan menggunakan GPU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "translatedText": "Mengingat bahwa salah satu pelajaran besar tentang deep learning dalam satu atau dua dekade terakhir adalah bahwa skala saja tampaknya memberikan peningkatan kualitatif yang sangat besar dalam kinerja model, ada keuntungan besar untuk arsitektur paralel yang memungkinkan Anda melakukan hal ini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "translatedText": "Jika Anda ingin mempelajari lebih lanjut tentang hal ini, saya telah meninggalkan banyak tautan dalam deskripsi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "translatedText": "Secara khusus, apa pun yang diproduksi oleh Andrej Karpathy atau Chris Ola cenderung merupakan emas murni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "translatedText": "Dalam video ini, saya hanya ingin memberikan perhatian pada bentuknya yang sekarang, tetapi jika Anda ingin tahu lebih banyak tentang sejarah bagaimana kami sampai di sini dan bagaimana Anda dapat menemukan kembali ide ini untuk diri Anda sendiri, teman saya, Vivek, baru saja mengunggah beberapa video yang memberikan lebih banyak lagi motivasi tersebut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "translatedText": "Selain itu, Britt Cruz dari saluran The Art of the Problem memiliki video yang sangat bagus tentang sejarah model bahasa yang besar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "input": "Thank you.",
  "translatedText": "Terima kasih.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]