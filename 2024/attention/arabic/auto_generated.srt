1
00:00:00,000 --> 00:00:04,019
في الفصل الأخير، بدأنا أنا وأنت في التعرف على الأعمال الداخلية للمحول.

2
00:00:04,560 --> 00:00:07,465
يعد هذا أحد العناصر الأساسية للتكنولوجيا داخل نماذج اللغات الكبيرة، 

3
00:00:07,465 --> 00:00:10,200
والعديد من الأدوات الأخرى في الموجة الحديثة من الذكاء الاصطناعي.

4
00:00:10,980 --> 00:00:14,478
ظهرت لأول مرة على الساحة في ورقة بحثية مشهورة عام 2017 بعنوان 

5
00:00:14,478 --> 00:00:18,089
&quot;الانتباه هو كل ما تحتاجه&quot;، وفي هذا الفصل، سنتعمق أنا 

6
00:00:18,089 --> 00:00:21,700
وأنت في ماهية آلية الانتباه هذه، ونتصور كيفية معالجتها للبيانات.

7
00:00:26,140 --> 00:00:29,540
كملخص سريع، إليك السياق المهم الذي أريدك أن تضعه في الاعتبار.

8
00:00:30,000 --> 00:00:33,196
الهدف من النموذج الذي ندرسه أنا وأنت هو أخذ جزء 

9
00:00:33,196 --> 00:00:36,060
من النص والتنبؤ بالكلمة التي ستأتي بعد ذلك.

10
00:00:36,860 --> 00:00:41,334
يتم تقسيم النص المُدخل إلى أجزاء صغيرة نسميها الرموز المميزة، وهي غالبًا ما تكون 

11
00:00:41,334 --> 00:00:45,753
كلمات أو أجزاء من الكلمات، ولكن فقط لجعل الأمثلة في هذا الفيديو أسهل بالنسبة لي 

12
00:00:45,753 --> 00:00:50,560
ولكم للتفكير فيها، دعونا نبسطها من خلال التظاهر بأن الرموز المميزة هي دائما مجرد كلمات.

13
00:00:51,480 --> 00:00:57,700
الخطوة الأولى في المحول هي ربط كل رمز بمتجه عالي الأبعاد، وهو ما نسميه تضمينه.

14
00:00:57,700 --> 00:01:02,256
الفكرة الأكثر أهمية التي أريدك أن تضعها في ذهنك هي كيف يمكن للاتجاهات في 

15
00:01:02,256 --> 00:01:07,000
هذا الفضاء عالي الأبعاد لجميع التضمينات الممكنة أن تتوافق مع المعنى الدلالي.

16
00:01:07,680 --> 00:01:13,804
رأينا في الفصل الأخير مثالا لكيفية توافق الاتجاه مع الجنس، بمعنى أن إضافة خطوة معينة 

17
00:01:13,804 --> 00:01:19,640
في هذا الفضاء يمكن أن يأخذك من تضمين الاسم المذكر إلى تضمين الاسم المؤنث المقابل.

18
00:01:20,160 --> 00:01:23,870
هذا مجرد مثال واحد يمكنك من خلاله تخيل عدد الاتجاهات الأخرى في هذا الفضاء 

19
00:01:23,870 --> 00:01:27,580
عالي الأبعاد الذي يمكن أن يتوافق مع العديد من الجوانب الأخرى لمعنى الكلمة.

20
00:01:28,800 --> 00:01:33,912
الهدف من المحول هو ضبط هذه التضمينات تدريجيًا بحيث لا تقوم بتشفير 

21
00:01:33,912 --> 00:01:39,180
كلمة فردية فحسب، بل تخبز بدلاً من ذلك معنى سياقيًا أكثر ثراءً بكثير.

22
00:01:40,140 --> 00:01:44,477
يجب أن أقول مقدمًا أن الكثير من الناس يجدون آلية الانتباه، هذه القطعة الرئيسية 

23
00:01:44,477 --> 00:01:48,980
في المحول، مربكة للغاية، لذا لا تقلق إذا استغرق الأمر بعض الوقت حتى تستوعب الأمور.

24
00:01:49,440 --> 00:01:54,333
أعتقد أنه قبل أن نتعمق في التفاصيل الحسابية وجميع عمليات ضرب المصفوفات، 

25
00:01:54,333 --> 00:01:59,160
من المفيد التفكير في بضعة أمثلة لنوع السلوك الذي نريد الاهتمام بتمكينه.

26
00:02:00,140 --> 00:02:03,131
خذ بعين الاعتبار العبارات &quot;الخلد الأمريكي الحقيقي&quot;، 

27
00:02:03,131 --> 00:02:06,220
&quot;مول واحد من ثاني أكسيد الكربون&quot;، وأخذ خزعة من الشامة.

28
00:02:06,700 --> 00:02:10,900
أنا وأنت نعلم أن كلمة شامة لها معاني مختلفة في كل واحدة منها، بناءً على السياق.

29
00:02:11,360 --> 00:02:16,363
ولكن بعد الخطوة الأولى للمحول، الذي يقسم النص ويربط كل رمز بمتجه، 

30
00:02:16,363 --> 00:02:21,140
فإن المتجه المرتبط بالمول سيكون هو نفسه في كل هذه الحالات، لأن 

31
00:02:21,140 --> 00:02:26,220
تضمين الرمز المميز هذا هو في الواقع جدول بحث دون الرجوع إلى السياق.

32
00:02:26,620 --> 00:02:29,930
فقط في الخطوة التالية من المحول تكون للتضمينات 

33
00:02:29,930 --> 00:02:33,100
المحيطة فرصة لتمرير المعلومات إلى هذا المحول.

34
00:02:33,820 --> 00:02:39,952
الصورة التي قد تكون في ذهنك هي أن هناك اتجاهات متعددة ومتميزة في مساحة التضمين هذه التي 

35
00:02:39,952 --> 00:02:45,736
تشفر المعاني المتميزة المتعددة لكلمة mole، وأن كتلة الانتباه المدربة جيدًا تحسب ما 

36
00:02:45,736 --> 00:02:51,800
تحتاج إلى إضافته إلى التضمين العام لنقله إليه أحد هذه الاتجاهات المحددة، كوظيفة للسياق.

37
00:02:53,300 --> 00:02:56,180
لنأخذ مثالاً آخر، فكر في تضمين كلمة برج.

38
00:02:57,060 --> 00:03:00,301
من المفترض أن يكون هذا اتجاهًا عامًا جدًا وغير محدد في 

39
00:03:00,301 --> 00:03:03,720
الفضاء، ويرتبط بالكثير من الأسماء الكبيرة والطويلة الأخرى.

40
00:03:04,020 --> 00:03:08,897
إذا سبقت هذه الكلمة مباشرة كلمة إيفل، فيمكنك أن تتخيل أنك ترغب في تحديث 

41
00:03:08,897 --> 00:03:14,046
الآلية لهذا المتجه بحيث يشير إلى اتجاه يرمز بشكل أكثر تحديدًا إلى برج إيفل، 

42
00:03:14,046 --> 00:03:19,060
وربما يرتبط بالمتجهات المرتبطة بباريس وفرنسا والأشياء المصنوعة من الفولاذ.

43
00:03:19,920 --> 00:03:23,592
إذا سبقتها أيضًا كلمة مصغرة، فيجب تحديث المتجه 

44
00:03:23,592 --> 00:03:27,500
بشكل أكبر، بحيث لا يرتبط بالأشياء الكبيرة الطويلة.

45
00:03:29,480 --> 00:03:34,151
بشكل أكثر عمومية من مجرد تحسين معنى كلمة ما، تسمح كتلة الانتباه للنموذج 

46
00:03:34,151 --> 00:03:38,563
بنقل المعلومات المشفرة في أحد العناصر المضمنة إلى معلومات أخرى، ومن 

47
00:03:38,563 --> 00:03:43,300
المحتمل أن تكون بعيدة جدًا، وربما بمعلومات أكثر ثراءً من مجرد كلمة واحدة.

48
00:03:43,300 --> 00:03:48,127
ما رأيناه في الفصل الأخير هو أنه بعد تدفق جميع المتجهات عبر الشبكة، 

49
00:03:48,127 --> 00:03:52,955
بما في ذلك العديد من كتل الانتباه المختلفة، فإن الحساب الذي تقوم به 

50
00:03:52,955 --> 00:03:58,280
لإنتاج تنبؤ بالرمز المميز التالي هو بالكامل وظيفة المتجه الأخير في التسلسل.

51
00:03:59,100 --> 00:04:03,570
تخيل، على سبيل المثال، أن النص الذي تقوم بإدخاله هو في الغالب رواية غامضة 

52
00:04:03,570 --> 00:04:07,800
بأكملها، وصولاً إلى نقطة قريبة من النهاية، والتي تقول: إذن كان القاتل.

53
00:04:08,400 --> 00:04:13,615
إذا كان النموذج سيتنبأ بدقة بالكلمة التالية، فإن المتجه الأخير في التسلسل، 

54
00:04:13,615 --> 00:04:18,622
والذي بدأ حياته ببساطة بتضمين الكلمة، يجب أن يتم تحديثه بواسطة جميع كتل 

55
00:04:18,622 --> 00:04:23,491
الانتباه لتمثيل أكثر بكثير من أي فرد word، يقوم بطريقة ما بتشفير جميع 

56
00:04:23,491 --> 00:04:28,220
المعلومات من نافذة السياق الكاملة ذات الصلة بالتنبؤ بالكلمة التالية.

57
00:04:29,500 --> 00:04:32,580
ومع ذلك، لكي ننتقل إلى العمليات الحسابية، دعونا نأخذ مثالًا أبسط بكثير.

58
00:04:32,980 --> 00:04:37,960
تخيل أن الإدخال يتضمن العبارة، مخلوق أزرق رقيق يتجول في الغابة الخضراء.

59
00:04:38,460 --> 00:04:42,732
وفي الوقت الحالي، لنفترض أن النوع الوحيد من التحديث الذي 

60
00:04:42,732 --> 00:04:46,780
نهتم به هو جعل الصفات تضبط معاني الأسماء المقابلة لها.

61
00:04:47,000 --> 00:04:51,089
ما أنا على وشك وصفه هو ما نسميه رأسًا واحدًا للانتباه، وسنرى لاحقًا 

62
00:04:51,089 --> 00:04:55,420
كيف تتكون كتلة الانتباه من العديد من الرؤوس المختلفة التي تعمل بالتوازي.

63
00:04:56,140 --> 00:04:59,696
مرة أخرى، التضمين الأولي لكل كلمة هو عبارة عن ناقل عالي 

64
00:04:59,696 --> 00:05:03,380
الأبعاد يقوم فقط بتشفير معنى تلك الكلمة المحددة بدون سياق.

65
00:05:04,000 --> 00:05:05,220
في الواقع، هذا ليس صحيحا تماما.

66
00:05:05,380 --> 00:05:07,640
كما أنها تشفر موضع الكلمة.

67
00:05:07,980 --> 00:05:13,407
هناك الكثير لنقوله عن الطريقة التي يتم بها تشفير المواضع، لكن الآن، كل ما تحتاج إلى 

68
00:05:13,407 --> 00:05:18,900
معرفته هو أن إدخالات هذا المتجه كافية لإخبارك عن ماهية الكلمة ومكان وجودها في السياق.

69
00:05:19,500 --> 00:05:21,660
دعنا نمضي قدمًا ونشير إلى هذه التضمينات بالحرف e.

70
00:05:22,420 --> 00:05:27,988
الهدف هو الحصول على سلسلة من الحسابات تنتج مجموعة جديدة منقحة من التضمينات، حيث، 

71
00:05:27,988 --> 00:05:33,420
على سبيل المثال، تلك المقابلة للأسماء قد استوعبت المعنى من الصفات المقابلة لها.

72
00:05:33,900 --> 00:05:37,117
ومن خلال لعب لعبة التعلم العميق، نريد أن تبدو معظم الحسابات 

73
00:05:37,117 --> 00:05:40,226
المعنية مثل منتجات مصفوفة متجهة، حيث تكون المصفوفات مليئة 

74
00:05:40,226 --> 00:05:43,980
بالأوزان القابلة للضبط، وهي أشياء سيتعلمها النموذج بناءً على البيانات.

75
00:05:44,660 --> 00:05:48,588
لكي أكون واضحًا، أنا أقوم بإعداد هذا المثال للصفات التي تقوم 

76
00:05:48,588 --> 00:05:52,260
بتحديث الأسماء فقط لتوضيح نوع السلوك الذي يمكن أن تتخيله.

77
00:05:52,860 --> 00:05:56,985
كما هو الحال مع الكثير من التعلم العميق، يصعب تحليل السلوك الحقيقي لأنه 

78
00:05:56,985 --> 00:06:01,340
يعتمد على التغيير والتبديل في عدد كبير من المعلمات لتقليل بعض وظائف التكلفة.

79
00:06:01,680 --> 00:06:05,646
إنه فقط بينما ننتقل عبر جميع المصفوفات المختلفة المليئة بالمعلمات 

80
00:06:05,646 --> 00:06:09,493
المتضمنة في هذه العملية، أعتقد أنه من المفيد حقًا أن يكون لدينا 

81
00:06:09,493 --> 00:06:13,220
مثال متخيل لشيء يمكن القيام به للمساعدة في إبقائه أكثر واقعية.

82
00:06:14,140 --> 00:06:17,936
في الخطوة الأولى من هذه العملية، قد تتخيل كل اسم، 

83
00:06:17,936 --> 00:06:21,960
مثل المخلوق، يطرح السؤال، هل هناك أي صفات تجلس أمامي؟

84
00:06:22,160 --> 00:06:27,960
وبالنسبة للكلمات رقيق وأزرق، ليتمكن كل منهما من الإجابة، نعم، أنا صفة وأنا في هذا الموقف.

85
00:06:28,960 --> 00:06:32,342
تم ترميز هذا السؤال بطريقة ما باعتباره متجهًا آخر، أو 

86
00:06:32,342 --> 00:06:36,100
قائمة أخرى من الأرقام، والتي نسميها الاستعلام عن هذه الكلمة.

87
00:06:36,980 --> 00:06:39,638
على الرغم من أن متجه الاستعلام هذا له بُعد أصغر 

88
00:06:39,638 --> 00:06:42,020
بكثير من متجه التضمين، على سبيل المثال 128.

89
00:06:42,940 --> 00:06:49,780
إن حساب هذا الاستعلام يشبه أخذ مصفوفة معينة، والتي سأسميها wq، وضربها في التضمين.

90
00:06:50,960 --> 00:06:55,503
بضغط الأشياء قليلاً، دعنا نكتب متجه الاستعلام هذا كـ q، ثم في أي 

91
00:06:55,503 --> 00:07:00,116
وقت تراني أضع مصفوفة بجوار سهم مثل هذا، من المفترض أن تمثل أن ضرب 

92
00:07:00,116 --> 00:07:04,800
هذه المصفوفة في المتجه في بداية السهم يمنحك المتجه عند نهاية السهم.

93
00:07:05,860 --> 00:07:09,274
في هذه الحالة، يمكنك ضرب هذه المصفوفة بجميع التضمينات الموجودة 

94
00:07:09,274 --> 00:07:12,580
في السياق، مما يؤدي إلى إنتاج متجه استعلام واحد لكل رمز مميز.

95
00:07:13,740 --> 00:07:18,328
مدخلات هذه المصفوفة هي معلمات النموذج، مما يعني أن السلوك الحقيقي يتم تعلمه من 

96
00:07:18,328 --> 00:07:23,440
البيانات، ومن الناحية العملية، فإن ما تفعله هذه المصفوفة في رأس انتباه معين يصعب تحليله.

97
00:07:23,900 --> 00:07:28,435
لكن من أجل مصلحتنا، تخيل مثالًا قد نأمل أن يتعلمه، سنفترض أن مصفوفة 

98
00:07:28,435 --> 00:07:33,171
الاستعلام هذه تحدد تضمينات الأسماء في اتجاهات معينة في مساحة الاستعلام 

99
00:07:33,171 --> 00:07:38,040
الأصغر هذه والتي تشفر بطريقة ما فكرة البحث عن الصفات في المواضع السابقة .

100
00:07:38,780 --> 00:07:41,440
وأما ماذا يفعل بالتضمينات الأخرى فمن يدري؟

101
00:07:41,720 --> 00:07:44,340
ربما يحاول في نفس الوقت تحقيق هدف آخر مع هؤلاء.

102
00:07:44,540 --> 00:07:47,160
في الوقت الحالي، نحن نركز الليزر على الأسماء.

103
00:07:47,280 --> 00:07:50,787
وفي الوقت نفسه، ترتبط بهذا مصفوفة ثانية تسمى المصفوفة 

104
00:07:50,787 --> 00:07:54,620
الرئيسية، والتي يمكنك أيضًا ضربها في كل واحدة من التضمينات.

105
00:07:55,280 --> 00:07:58,500
وينتج عن ذلك سلسلة ثانية من المتجهات التي نسميها المفاتيح.

106
00:07:59,420 --> 00:08:03,140
من الناحية النظرية، تريد أن تفكر في المفاتيح على أنها تجيب على الاستفسارات.

107
00:08:03,840 --> 00:08:07,713
هذه المصفوفة الرئيسية مليئة أيضًا بالمعلمات القابلة للضبط، وكما هو الحال مع مصفوفة 

108
00:08:07,713 --> 00:08:11,400
الاستعلام، فإنها تقوم بتعيين متجهات التضمين إلى نفس المساحة ذات الأبعاد الأصغر.

109
00:08:12,200 --> 00:08:17,020
تعتقد أن المفاتيح مطابقة للاستعلامات عندما تتوافق بشكل وثيق مع بعضها البعض.

110
00:08:17,460 --> 00:08:22,038
في مثالنا، يمكنك أن تتخيل أن المصفوفة الرئيسية تقوم بتعيين الصفات مثل رقيق 

111
00:08:22,038 --> 00:08:26,740
وأزرق إلى المتجهات التي تتماشى بشكل وثيق مع الاستعلام الناتج عن الكلمة مخلوق.

112
00:08:27,200 --> 00:08:30,600
لقياس مدى تطابق كل مفتاح مع كل استعلام، يمكنك 

113
00:08:30,600 --> 00:08:34,000
حساب منتج نقطي بين كل زوج استعلام رئيسي محتمل.

114
00:08:34,480 --> 00:08:38,548
أحب أن أتخيل شبكة مليئة بمجموعة من النقاط، حيث تتوافق النقاط الأكبر مع 

115
00:08:38,548 --> 00:08:42,559
منتجات النقاط الأكبر، والأماكن التي تتماشى فيها المفاتيح والاستعلامات.

116
00:08:43,280 --> 00:08:48,109
بالنسبة لمثال اسم الصفة الخاص بنا، سيبدو هذا أكثر قليلًا مثل هذا، حيث إذا كانت 

117
00:08:48,109 --> 00:08:53,306
المفاتيح التي تم إنتاجها بواسطة رقيق وأزرق تتوافق حقًا بشكل وثيق مع الاستعلام الناتج 

118
00:08:53,306 --> 00:08:58,320
عن المخلوق، فإن منتجات النقاط في هاتين النقطتين ستكون بعض الأرقام الموجبة الكبيرة.

119
00:08:59,100 --> 00:09:02,343
في اللغة، قد يقول الأشخاص الذين يتعلمون الآلة أن هذا يعني 

120
00:09:02,343 --> 00:09:05,420
أن عمليات التضمين الرقيقة والزرقاء تحضر لتضمين المخلوق.

121
00:09:06,040 --> 00:09:11,250
على النقيض من المنتج النقطي بين مفتاح بعض الكلمات الأخرى مثل والاستعلام عن 

122
00:09:11,250 --> 00:09:16,600
المخلوق، سيكون هناك قيمة صغيرة أو سلبية تعكس قيمًا لا علاقة لها ببعضها البعض.

123
00:09:17,700 --> 00:09:23,198
إذن لدينا شبكة من القيم التي يمكن أن تكون أي عدد حقيقي من اللانهاية السالبة 

124
00:09:23,198 --> 00:09:28,480
إلى اللانهاية، مما يمنحنا درجة لمدى صلة كل كلمة بتحديث معنى كل كلمة أخرى.

125
00:09:29,200 --> 00:09:32,490
الطريقة التي نحن على وشك استخدام هذه الدرجات فيها هي 

126
00:09:32,490 --> 00:09:35,780
أخذ مبلغ مرجح معين على طول كل عمود، مرجحًا حسب الصلة.

127
00:09:36,520 --> 00:09:42,056
لذا بدلًا من أن تتراوح القيم من اللانهاية السالبة إلى اللانهاية، ما نريده هو أن تكون 

128
00:09:42,056 --> 00:09:47,528
الأرقام في هذه الأعمدة بين 0 و1، وأن يضيف كل عمود ما يصل إلى 1، كما لو كانت توزيعًا 

129
00:09:47,528 --> 00:09:48,180
احتماليًا.

130
00:09:49,280 --> 00:09:52,220
إذا كنت قادمًا من الفصل الأخير، فأنت تعرف ما يتعين علينا فعله بعد ذلك.

131
00:09:52,620 --> 00:09:57,300
نقوم بحساب softmax على طول كل عمود من هذه الأعمدة لتطبيع القيم.

132
00:10:00,060 --> 00:10:02,991
في صورتنا، بعد تطبيق softmax على جميع الأعمدة، 

133
00:10:02,991 --> 00:10:05,860
سنقوم بملء الشبكة بهذه القيم التي تمت تسويتها.

134
00:10:06,780 --> 00:10:10,680
في هذه المرحلة، يمكنك التفكير بأمان في كل عمود باعتباره يعطي أوزانًا 

135
00:10:10,680 --> 00:10:14,580
وفقًا لمدى صلة الكلمة الموجودة على اليسار بالقيمة المقابلة في الأعلى.

136
00:10:15,080 --> 00:10:16,840
نحن نسمي هذه الشبكة نمط الاهتمام.

137
00:10:18,080 --> 00:10:22,820
الآن، إذا نظرتم إلى ورقة المحولات الأصلية، هناك طريقة مدمجة حقًا لكتابة كل هذا.

138
00:10:23,880 --> 00:10:29,260
هنا يمثل المتغيران q وk المصفوفات الكاملة للاستعلام والمتجهات الرئيسية على التوالي، تلك 

139
00:10:29,260 --> 00:10:34,640
المتجهات الصغيرة التي تحصل عليها عن طريق ضرب التضمينات في الاستعلام والمصفوفات الرئيسية.

140
00:10:35,160 --> 00:10:39,187
يعد هذا التعبير الموجود في البسط طريقة مدمجة حقًا لتمثيل شبكة 

141
00:10:39,187 --> 00:10:43,020
جميع منتجات النقاط الممكنة بين أزواج المفاتيح والاستعلامات.

142
00:10:44,000 --> 00:10:48,980
هناك تفاصيل فنية صغيرة لم أذكرها وهي أنه لتحقيق الاستقرار الرقمي، من المفيد 

143
00:10:48,980 --> 00:10:53,960
تقسيم كل هذه القيم على الجذر التربيعي للبعد في مساحة الاستعلام الرئيسية هذه.

144
00:10:54,480 --> 00:11:00,800
ثم من المفترض أن يتم فهم softmax الملتف حول التعبير الكامل لتطبيق عمود بعمود.

145
00:11:01,640 --> 00:11:04,700
أما بالنسبة لمصطلح v هذا، فسنتحدث عنه خلال ثانية واحدة فقط.

146
00:11:05,020 --> 00:11:08,460
قبل ذلك، هناك تفصيل فني آخر لم أتجاوزه حتى الآن.

147
00:11:09,040 --> 00:11:13,556
أثناء عملية التدريب، عندما تقوم بتشغيل هذا النموذج على مثال نصي معين، ويتم 

148
00:11:13,556 --> 00:11:17,831
تعديل جميع الأوزان قليلاً وضبطها إما لمكافأته أو معاقبته بناءً على مدى 

149
00:11:17,831 --> 00:11:22,407
الاحتمالية العالية التي يعينها للكلمة التالية الحقيقية في المقطع، فإنه اتضح 

150
00:11:22,407 --> 00:11:26,923
أنه يجعل عملية التدريب بأكملها أكثر كفاءة إذا قمت في نفس الوقت بالتنبؤ بكل 

151
00:11:26,923 --> 00:11:31,560
رمز مميز تالٍ محتمل بعد كل سلسلة لاحقة أولية من الرموز المميزة في هذا المقطع.

152
00:11:31,940 --> 00:11:35,620
على سبيل المثال، في العبارة التي ركزنا عليها، قد تتنبأ 

153
00:11:35,620 --> 00:11:39,100
أيضًا بالكلمات التي تتبع المخلوق والكلمات التي تتبع.

154
00:11:39,940 --> 00:11:42,802
هذا أمر لطيف حقًا، لأنه يعني أن ما يمكن أن يكون مثالًا 

155
00:11:42,802 --> 00:11:45,560
تدريبيًا واحدًا يعمل بشكل فعال على العديد من الأمثلة.

156
00:11:46,100 --> 00:11:51,328
لأغراض نمط انتباهنا، فهذا يعني أنك لا تريد أبدًا السماح للكلمات اللاحقة بالتأثير 

157
00:11:51,328 --> 00:11:56,040
على الكلمات السابقة، وإلا فإنها قد تعطي نوعًا ما إجابة لما سيأتي بعد ذلك.

158
00:11:56,560 --> 00:12:00,393
ما يعنيه هذا هو أننا نريد أن يتم إجبار كل هذه النقاط هنا، تلك التي تمثل 

159
00:12:00,393 --> 00:12:04,600
الرموز اللاحقة التي تؤثر على الرموز السابقة، على أن تكون صفرًا بطريقة أو بأخرى.

160
00:12:05,920 --> 00:12:09,170
إن أبسط شيء قد تفكر في القيام به هو جعلها مساوية للصفر، ولكن إذا 

161
00:12:09,170 --> 00:12:12,420
فعلت ذلك فلن يكون مجموع الأعمدة واحدًا بعد الآن، ولن تتم تطبيعها.

162
00:12:13,120 --> 00:12:16,166
لذا بدلاً من ذلك، الطريقة الشائعة للقيام بذلك هي أنه قبل تطبيق 

163
00:12:16,166 --> 00:12:19,020
softmax، يمكنك تعيين كل هذه الإدخالات لتكون سالبة لا نهاية.

164
00:12:19,680 --> 00:12:25,180
إذا قمت بذلك، فبعد تطبيق softmax، ستتحول كل تلك العناصر إلى صفر، لكن تظل الأعمدة طبيعية.

165
00:12:26,000 --> 00:12:27,540
هذه العملية تسمى اخفاء.

166
00:12:27,540 --> 00:12:31,020
هناك إصدارات من الاهتمام لا يمكنك تطبيقها فيها، ولكن في مثال GPT الخاص بنا، 

167
00:12:31,020 --> 00:12:34,454
على الرغم من أن هذا أكثر أهمية خلال مرحلة التدريب مما سيكون عليه، على سبيل 

168
00:12:34,454 --> 00:12:37,888
المثال، تشغيله كبرنامج دردشة آلي أو شيء من هذا القبيل، إلا أنك تقوم دائمًا 

169
00:12:37,888 --> 00:12:41,460
بتطبيقه هذا الإخفاء لمنع الرموز المميزة اللاحقة من التأثير على الرموز السابقة.

170
00:12:42,480 --> 00:12:49,500
هناك حقيقة أخرى تستحق التفكير بشأن نمط الانتباه هذا وهي كيف أن حجمه يساوي مربع حجم السياق.

171
00:12:49,900 --> 00:12:52,649
ولهذا السبب يمكن أن يشكل حجم السياق عائقًا كبيرًا 

172
00:12:52,649 --> 00:12:55,620
لنماذج اللغات الكبيرة، وتوسيع نطاقه ليس بالأمر التافه.

173
00:12:56,300 --> 00:13:00,286
كما تتخيل، بدافع من الرغبة في نوافذ سياق أكبر وأكبر، شهدت السنوات 

174
00:13:00,286 --> 00:13:04,212
الأخيرة بعض الاختلافات في آلية الانتباه التي تهدف إلى جعل السياق 

175
00:13:04,212 --> 00:13:08,320
أكثر قابلية للتطوير، ولكن هنا، أنا وأنت نواصل التركيز على الأساسيات.

176
00:13:10,560 --> 00:13:15,480
حسنًا، رائع، حساب هذا النمط يتيح للنموذج استنتاج الكلمات ذات الصلة بالكلمات الأخرى.

177
00:13:16,020 --> 00:13:19,314
أنت الآن بحاجة إلى تحديث التضمينات فعليًا، مما يسمح 

178
00:13:19,314 --> 00:13:22,800
للكلمات بتمرير المعلومات إلى أي كلمات أخرى ذات صلة بها.

179
00:13:22,800 --> 00:13:26,727
على سبيل المثال، تريد أن يتسبب تضمين Fluffy بطريقة ما في حدوث 

180
00:13:26,727 --> 00:13:30,465
تغيير في المخلوق الذي ينقله إلى جزء مختلف من مساحة التضمين 

181
00:13:30,465 --> 00:13:34,520
ذات 12000 بُعد والتي تقوم بتشفير مخلوق Fluffy بشكل أكثر تحديدًا.

182
00:13:35,460 --> 00:13:39,286
ما سأفعله هنا هو أولًا أن أوضح لك الطريقة الأكثر وضوحًا التي يمكنك من خلالها 

183
00:13:39,286 --> 00:13:43,460
القيام بذلك، على الرغم من وجود طريقة طفيفة لتعديل هذا في سياق الاهتمام متعدد الرؤوس.

184
00:13:44,080 --> 00:13:48,173
الطريقة الأكثر وضوحًا هي استخدام مصفوفة ثالثة، ما نسميه مصفوفة القيمة، 

185
00:13:48,173 --> 00:13:52,440
والتي تقوم بضربها من خلال تضمين تلك الكلمة الأولى، على سبيل المثال Fluffy.

186
00:13:53,300 --> 00:13:57,541
نتيجة هذا هي ما يمكن أن تسميه متجه القيمة، وهذا شيء تضيفه إلى 

187
00:13:57,541 --> 00:14:01,920
تضمين الكلمة الثانية، في هذه الحالة شيء تضيفه إلى تضمين المخلوق.

188
00:14:02,600 --> 00:14:07,000
لذا فإن متجه القيمة هذا يعيش في نفس المساحة ذات الأبعاد العالية جدًا مثل التضمينات.

189
00:14:07,460 --> 00:14:11,835
عندما تقوم بضرب مصفوفة القيمة هذه بتضمين كلمة ما، قد تفكر في 

190
00:14:11,835 --> 00:14:16,210
الأمر كقول، إذا كانت هذه الكلمة ذات صلة بتعديل معنى شيء آخر، 

191
00:14:16,210 --> 00:14:21,160
فما الذي يجب إضافته بالضبط إلى تضمين ذلك الشيء الآخر من أجل عكسه هذا؟

192
00:14:22,140 --> 00:14:26,413
إذا نظرنا إلى الرسم البياني الخاص بنا، فلنضع جانبًا جميع المفاتيح 

193
00:14:26,413 --> 00:14:31,009
والاستعلامات، لأنه بعد حساب نمط الانتباه الذي انتهيت منه، ستأخذ مصفوفة 

194
00:14:31,009 --> 00:14:36,060
القيمة هذه وتضربها في كل واحدة من تلك التضمينات لإنتاج سلسلة من ناقلات القيمة.

195
00:14:37,120 --> 00:14:41,120
قد تعتقد أن متجهات القيمة هذه مرتبطة نوعًا ما بالمفاتيح المقابلة.

196
00:14:42,320 --> 00:14:45,780
بالنسبة لكل عمود في هذا الرسم البياني، يمكنك ضرب 

197
00:14:45,780 --> 00:14:49,240
كل من متجهات القيمة بالوزن المقابل في ذلك العمود.

198
00:14:50,080 --> 00:14:55,691
على سبيل المثال، هنا، ضمن تضمين Creature، ستضيف نسبًا كبيرة من متجهات القيمة لـ Fluffy 

199
00:14:55,691 --> 00:15:01,044
وBlue، بينما يتم التخلص من جميع متجهات القيمة الأخرى، أو على الأقل يتم التخلص منها 

200
00:15:01,044 --> 00:15:01,560
تقريبًا.

201
00:15:02,120 --> 00:15:06,360
وأخيرًا، طريقة التحديث الفعلي للتضمين المرتبط بهذا العمود، وذلك عن طريق 

202
00:15:06,360 --> 00:15:10,601
تشفير بعض المعاني الخالية من السياق لـ Creature، حيث تقوم بإضافة كل هذه 

203
00:15:10,601 --> 00:15:15,902
القيم التي تم إعادة قياسها في العمود، مما ينتج عنه التغيير الذي تريد إضافته، وهو ما أريده.

204
00:15:15,902 --> 00:15:19,260
 سوف نقوم بتسمية delta-e، ثم تضيف ذلك إلى التضمين الأصلي.

205
00:15:19,680 --> 00:15:22,972
نأمل أن تكون النتائج عبارة عن ناقل أكثر دقة يقوم بترميز 

206
00:15:22,972 --> 00:15:26,500
المعنى الأكثر ثراءً من حيث السياق، مثل معنى مخلوق أزرق رقيق.

207
00:15:27,380 --> 00:15:31,255
وبالطبع لا تفعل ذلك على عملية تضمين واحدة فحسب، بل يمكنك تطبيق نفس 

208
00:15:31,255 --> 00:15:35,246
المجموع المرجح عبر جميع الأعمدة في هذه الصورة، مما ينتج عنه سلسلة من 

209
00:15:35,246 --> 00:15:39,237
التغييرات، وإضافة كل هذه التغييرات إلى عمليات التضمين المقابلة، ينتج 

210
00:15:39,237 --> 00:15:43,460
تسلسلًا كاملاً من المزيد من التضمينات الدقيقة التي تخرج من كتلة الانتباه.

211
00:15:44,860 --> 00:15:49,100
بتصغير الصورة، هذه العملية برمتها هي ما يمكن أن تصفه برأس واحد من الاهتمام.

212
00:15:49,600 --> 00:15:54,461
كما وصفت الأمور حتى الآن، يتم تحديد معلمات هذه العملية من خلال ثلاث مصفوفات 

213
00:15:54,461 --> 00:15:58,940
متميزة، كلها مليئة بمعلمات قابلة للضبط، والمفتاح، والاستعلام، والقيمة.

214
00:15:59,500 --> 00:16:03,925
أريد أن أتوقف لحظة لمواصلة ما بدأناه في الفصل الأخير، مع تسجيل النتائج 

215
00:16:03,925 --> 00:16:08,040
حيث نحسب العدد الإجمالي لمعلمات النموذج باستخدام الأرقام من GPT-3.

216
00:16:09,300 --> 00:16:14,381
تحتوي كل من مصفوفات المفاتيح والاستعلام هذه على 12,288 عمودًا، بما يتوافق 

217
00:16:14,381 --> 00:16:19,600
مع بُعد التضمين، و128 صفًا، بما يتوافق مع بُعد مساحة استعلام المفتاح الأصغر.

218
00:16:20,260 --> 00:16:24,220
وهذا يمنحنا 1.5 مليون معلمة إضافية أو نحو ذلك لكل واحدة.

219
00:16:24,860 --> 00:16:30,022
إذا نظرت إلى مصفوفة القيمة تلك على النقيض من ذلك، فإن الطريقة التي وصفت 

220
00:16:30,022 --> 00:16:35,112
بها الأشياء حتى الآن تشير إلى أنها مصفوفة مربعة تحتوي على 12288 عمودًا 

221
00:16:35,112 --> 00:16:40,920
و12288 صفًا، نظرًا لأن مدخلاتها ومخرجاتها تعيش في مساحة التضمين الكبيرة جدًا هذه.

222
00:16:41,500 --> 00:16:45,140
إذا كان هذا صحيحًا، فهذا يعني إضافة حوالي 150 مليون معلمة.

223
00:16:45,660 --> 00:16:47,300
ولكي أكون واضحًا، يمكنك فعل ذلك.

224
00:16:47,420 --> 00:16:51,740
يمكنك تخصيص أوامر ذات حجم أكبر من المعلمات لخريطة القيمة بدلاً من المفتاح والاستعلام.

225
00:16:52,060 --> 00:16:56,410
لكن من الناحية العملية، يكون الأمر أكثر كفاءة إذا قمت بدلاً من ذلك بإجراء ذلك بحيث 

226
00:16:56,410 --> 00:17:00,760
يكون عدد المعلمات المخصصة لخريطة القيمة هذه هو نفس العدد المخصص للمفتاح والاستعلام.

227
00:17:01,460 --> 00:17:05,160
هذا مهم بشكل خاص في إعداد تشغيل رؤوس انتباه متعددة بالتوازي.

228
00:17:06,240 --> 00:17:10,099
الطريقة التي يبدو بها هذا هي أن خريطة القيمة يتم تحليلها كمنتج لمصفوفتين أصغر.

229
00:17:11,180 --> 00:17:15,218
من الناحية النظرية، ما زلت أشجعك على التفكير في الخريطة الخطية الشاملة، 

230
00:17:15,218 --> 00:17:19,368
واحدة تحتوي على المدخلات والمخرجات، سواء في مساحة التضمين الأكبر هذه، على 

231
00:17:19,368 --> 00:17:23,800
سبيل المثال أخذ تضمين اللون الأزرق في اتجاه الزرقة هذا الذي ستضيفه إلى الأسماء.

232
00:17:27,040 --> 00:17:32,760
إنه مجرد عدد أقل من الصفوف، وعادةً ما يكون بنفس حجم مساحة الاستعلام الرئيسية.

233
00:17:33,100 --> 00:17:35,598
ما يعنيه هذا هو أنه يمكنك التفكير في الأمر على أنه 

234
00:17:35,598 --> 00:17:38,440
تعيين لمتجهات التضمين الكبيرة وصولاً إلى مساحة أصغر بكثير.

235
00:17:39,040 --> 00:17:42,700
هذه ليست التسمية التقليدية، ولكنني سأسميها مصفوفة القيمة السفلية.

236
00:17:43,400 --> 00:17:47,013
تقوم المصفوفة الثانية بتعيين خريطة من هذه المساحة الأصغر احتياطيًا إلى مساحة 

237
00:17:47,013 --> 00:17:50,580
التضمين، مما يؤدي إلى إنتاج المتجهات التي تستخدمها لإجراء التحديثات الفعلية.

238
00:17:51,000 --> 00:17:54,740
سأطلق على هذه مصفوفة القيمة الأعلى، والتي مرة أخرى ليست تقليدية.

239
00:17:55,160 --> 00:17:58,080
تبدو الطريقة التي ترى بها هذا مكتوبًا في معظم الصحف مختلفة بعض الشيء.

240
00:17:58,380 --> 00:17:59,520
سأتحدث عن ذلك في دقيقة واحدة.

241
00:17:59,700 --> 00:18:02,540
في رأيي، فإنه يميل إلى جعل الأمور أكثر إرباكًا من الناحية المفاهيمية.

242
00:18:03,260 --> 00:18:06,675
لاستخدام مصطلحات الجبر الخطي هنا، ما نقوم به أساسًا هو 

243
00:18:06,675 --> 00:18:10,340
تقييد خريطة القيمة الإجمالية لتكون تحويلات ذات رتبة منخفضة.

244
00:18:11,420 --> 00:18:16,244
بالعودة إلى عدد المعلمات، جميع هذه المصفوفات الأربع لها نفس الحجم، 

245
00:18:16,244 --> 00:18:20,780
وبجمعها جميعًا نحصل على حوالي 6.3 مليون معلمة لرأس انتباه واحد.

246
00:18:22,040 --> 00:18:25,146
كملاحظة جانبية سريعة، لكي نكون أكثر دقة، كل ما تم وصفه حتى الآن هو 

247
00:18:25,146 --> 00:18:28,300
ما يسميه الناس &quot;رأس الانتباه الذاتي&quot;، لتمييزه عن الاختلاف 

248
00:18:28,300 --> 00:18:31,500
الذي يظهر في النماذج الأخرى والذي يسمى &quot;الانتباه المتبادل&quot;.

249
00:18:32,300 --> 00:18:37,699
هذا ليس له صلة بمثال GPT الخاص بنا، ولكن إذا كنت فضوليًا، فإن الاهتمام المتبادل 

250
00:18:37,699 --> 00:18:43,435
يتضمن نماذج تعالج نوعين مختلفين من البيانات، مثل النص في لغة واحدة والنص في لغة أخرى 

251
00:18:43,435 --> 00:18:49,240
والذي يعد جزءًا من الجيل المستمر من الترجمة، أو ربما إدخال صوتي للكلام والنسخ المستمر.

252
00:18:50,400 --> 00:18:52,700
يبدو رأس الانتباه المتقاطع متطابقًا تقريبًا.

253
00:18:52,980 --> 00:18:57,400
والفرق الوحيد هو أن خرائط المفاتيح والاستعلام تعمل على مجموعات بيانات مختلفة.

254
00:18:57,840 --> 00:19:01,614
في نموذج يقوم بالترجمة، على سبيل المثال، قد تأتي المفاتيح من 

255
00:19:01,614 --> 00:19:05,389
لغة واحدة، في حين تأتي الاستعلامات من لغة أخرى، ويمكن أن يصف 

256
00:19:05,389 --> 00:19:09,660
نمط الانتباه الكلمات من لغة واحدة التي تتوافق مع الكلمات في لغة أخرى.

257
00:19:10,340 --> 00:19:13,389
وفي هذا الإعداد لن يكون هناك عادةً أي إخفاء، نظرًا لعدم وجود 

258
00:19:13,389 --> 00:19:16,340
أي فكرة عن تأثير الرموز المميزة اللاحقة على الرموز السابقة.

259
00:19:17,180 --> 00:19:21,180
ومع ذلك، استمر في التركيز على الاهتمام الذاتي، إذا فهمت كل شيء 

260
00:19:21,180 --> 00:19:25,180
حتى الآن، وإذا توقفت هنا، فسوف تتوصل إلى جوهر الاهتمام الحقيقي.

261
00:19:25,760 --> 00:19:31,440
كل ما تبقى لنا حقًا هو توضيح المعنى الذي تفعل به هذا عدة مرات مختلفة.

262
00:19:32,100 --> 00:19:35,894
في مثالنا المركزي، ركزنا على تحديث الصفات للأسماء، ولكن بالطبع هناك 

263
00:19:35,894 --> 00:19:39,800
الكثير من الطرق المختلفة التي يمكن أن يؤثر بها السياق على معنى الكلمة.

264
00:19:40,360 --> 00:19:43,888
إذا كانت عبارة &quot;تحطموا&quot; سبقت كلمة &quot;سيارة&quot;، 

265
00:19:43,888 --> 00:19:46,520
فإن ذلك يكون له آثار على شكل وبنية تلك السيارة.

266
00:19:47,200 --> 00:19:49,280
وقد تكون الكثير من الارتباطات أقل نحويًا.

267
00:19:49,760 --> 00:19:54,713
إذا كانت كلمة &quot;ساحر&quot; موجودة في أي مكان في نفس المقطع مثل هاري، فهذا يشير 

268
00:19:54,713 --> 00:19:59,427
إلى أن هذا قد يشير إلى هاري بوتر، بينما إذا كانت الكلمات الملكة وساسكس وويليام 

269
00:19:59,427 --> 00:20:04,440
موجودة في هذا المقطع، فربما ينبغي تحديث تضمين هاري بدلاً من ذلك. للإشارة إلى الأمير.

270
00:20:05,040 --> 00:20:09,636
بالنسبة لكل نوع مختلف من التحديث السياقي الذي قد تتخيله، ستكون معلمات هذه 

271
00:20:09,636 --> 00:20:14,419
المصفوفات الرئيسية والاستعلام مختلفة لالتقاط أنماط الاهتمام المختلفة، وستكون 

272
00:20:14,419 --> 00:20:19,140
معلمات خريطة القيمة الخاصة بنا مختلفة بناءً على ما يجب إضافته إلى التضمينات.

273
00:20:19,980 --> 00:20:23,483
ومرة أخرى، من الناحية العملية، يكون تفسير السلوك الحقيقي لهذه الخرائط 

274
00:20:23,483 --> 00:20:26,786
أكثر صعوبة، حيث يتم تعيين الأوزان للقيام بكل ما يحتاج النموذج إلى 

275
00:20:26,786 --> 00:20:30,140
القيام به لتحقيق هدفه المتمثل في التنبؤ بالرمز التالي على أفضل وجه.

276
00:20:31,400 --> 00:20:36,046
كما قلت من قبل، كل ما وصفناه هو رأس واحد للانتباه، وتتكون كتلة الاهتمام 

277
00:20:36,046 --> 00:20:40,950
الكاملة داخل المحول مما يسمى بالانتباه متعدد الرؤوس، حيث تقوم بتشغيل الكثير 

278
00:20:40,950 --> 00:20:45,920
من هذه العمليات بالتوازي، ولكل منها استعلام رئيسي مميز خاص بها وخرائط القيمة.

279
00:20:47,420 --> 00:20:51,700
يستخدم GPT-3 على سبيل المثال 96 رأس انتباه داخل كل كتلة.

280
00:20:52,020 --> 00:20:54,160
مع الأخذ في الاعتبار أن كل واحدة منها مربكة بعض الشيء 

281
00:20:54,160 --> 00:20:56,460
بالفعل، فمن المؤكد أن هناك الكثير مما يجب أن تضعه في رأسك.

282
00:20:56,760 --> 00:21:01,053
فقط لتوضيح كل ذلك بشكل واضح جدًا، هذا يعني أن لديك 96 مفتاحًا 

283
00:21:01,053 --> 00:21:05,000
مميزًا ومصفوفة استعلام تنتج 96 نمطًا متميزًا من الاهتمام.

284
00:21:05,440 --> 00:21:12,180
ثم يكون لكل رأس مصفوفات قيمة مميزة خاصة به تُستخدم لإنتاج 96 سلسلة من متجهات القيمة.

285
00:21:12,460 --> 00:21:16,680
تتم إضافة كل ذلك معًا باستخدام أنماط الاهتمام المقابلة كأوزان.

286
00:21:17,480 --> 00:21:22,317
ما يعنيه هذا هو أنه بالنسبة لكل موضع في السياق، كل رمز مميز، كل واحد من 

287
00:21:22,317 --> 00:21:27,020
هذه الرؤوس ينتج تغييرًا مقترحًا ليتم إضافته إلى التضمين في هذا الموضع.

288
00:21:27,660 --> 00:21:31,497
إذن ما تفعله هو جمع كل هذه التغييرات المقترحة، واحدة 

289
00:21:31,497 --> 00:21:35,480
لكل رأس، وإضافة النتيجة إلى التضمين الأصلي لهذا الموضع.

290
00:21:36,660 --> 00:21:42,198
هذا المجموع بأكمله هنا سيكون شريحة واحدة مما يتم إنتاجه من كتلة الانتباه متعددة 

291
00:21:42,198 --> 00:21:47,460
الرؤوس، شريحة واحدة من تلك التضمينات المكررة التي تنبثق من الطرف الآخر منها.

292
00:21:48,320 --> 00:21:50,248
مرة أخرى، هذا أمر كثير يجب التفكير فيه، لذا لا تقلق 

293
00:21:50,248 --> 00:21:52,140
على الإطلاق إذا استغرق الأمر بعض الوقت حتى تستوعبه.

294
00:21:52,380 --> 00:21:57,221
الفكرة العامة هي أنه من خلال تشغيل العديد من الرؤوس المميزة بالتوازي، فإنك تمنح 

295
00:21:57,221 --> 00:22:01,820
النموذج القدرة على تعلم العديد من الطرق المميزة التي يغير بها السياق المعنى.

296
00:22:03,700 --> 00:22:09,321
وبسحب رصيدنا الجاري لعدد المعلمات إلى 96 رأسًا، يتضمن كل منها نسخته الخاصة من هذه 

297
00:22:09,321 --> 00:22:15,080
المصفوفات الأربع، فإن كل كتلة من الاهتمام متعدد الرؤوس تنتهي بحوالي 600 مليون معلمة.

298
00:22:16,420 --> 00:22:21,800
هناك شيء إضافي مزعج بعض الشيء يجب أن أذكره حقًا لأي منكم يواصل قراءة المزيد عن المحولات.

299
00:22:22,080 --> 00:22:25,731
هل تتذكر كيف قلت إن خريطة القيمة تم تحليلها إلى هاتين المصفوفتين 

300
00:22:25,731 --> 00:22:29,440
المتميزتين، اللتين سميتهما بمصفوفات القيمة السفلية والقيمة الأعلى.

301
00:22:29,960 --> 00:22:34,105
الطريقة التي قمت بها بتأطير الأشياء تشير إلى أنك ترى هذا الزوج من 

302
00:22:34,105 --> 00:22:38,440
المصفوفات داخل كل رأس انتباه، ويمكنك بالتأكيد تنفيذ ذلك بهذه الطريقة.

303
00:22:38,640 --> 00:22:39,920
سيكون ذلك تصميمًا صالحًا.

304
00:22:40,260 --> 00:22:44,920
لكن الطريقة التي ترى بها هذا مكتوبًا في الأوراق وطريقة تنفيذه عمليًا تبدو مختلفة قليلًا.

305
00:22:45,340 --> 00:22:50,589
تظهر كل مصفوفات القيمة المرتفعة لكل رأس مجمعة معًا في مصفوفة عملاقة 

306
00:22:50,589 --> 00:22:56,380
واحدة نسميها مصفوفة الإخراج، المرتبطة بكتلة الانتباه متعددة الرؤوس بأكملها.

307
00:22:56,820 --> 00:23:01,861
وعندما ترى الناس يشيرون إلى مصفوفة القيمة لرأس اهتمام معين، فإنهم عادة ما يشيرون فقط 

308
00:23:01,861 --> 00:23:07,140
إلى هذه الخطوة الأولى، تلك التي كنت أصنفها على أنها إسقاط القيمة لأسفل في المساحة الأصغر.

309
00:23:08,340 --> 00:23:11,040
للفضوليين بينكم، لقد تركت ملاحظة على الشاشة حول هذا الموضوع.

310
00:23:11,260 --> 00:23:14,900
إنها واحدة من تلك التفاصيل التي تنطوي على خطر تشتيت الانتباه عن النقاط المفاهيمية 

311
00:23:14,900 --> 00:23:18,540
الرئيسية، لكنني أريد أن أذكرها فقط حتى تعرف ما إذا كنت قد قرأت عنها في مصادر أخرى.

312
00:23:19,240 --> 00:23:23,699
وبغض النظر عن الفروق الفنية الدقيقة، فقد رأينا في المعاينة من الفصل الأخير 

313
00:23:23,699 --> 00:23:28,040
كيف أن البيانات التي تتدفق عبر المحول لا تتدفق عبر كتلة انتباه واحدة فقط.

314
00:23:28,640 --> 00:23:32,700
لسبب واحد، أنها تمر أيضًا بهذه العمليات الأخرى التي تسمى الإدراك الحسي متعدد الطبقات.

315
00:23:33,120 --> 00:23:34,880
سنتحدث أكثر عن تلك في الفصل التالي.

316
00:23:35,180 --> 00:23:39,320
وبعد ذلك يمر بشكل متكرر عبر العديد من النسخ لكلتا العمليتين.

317
00:23:39,980 --> 00:23:45,010
ما يعنيه هذا هو أنه بعد أن تتشرب كلمة معينة بعضًا من سياقها، هناك 

318
00:23:45,010 --> 00:23:50,040
العديد من الفرص لهذا التضمين الأكثر دقة للتأثر بمحيطها الأكثر دقة.

319
00:23:50,940 --> 00:23:56,422
كلما تقدمت في الشبكة، حيث يأخذ كل تضمين معنى أكثر فأكثر من جميع التضمينات الأخرى، 

320
00:23:56,422 --> 00:24:01,971
والتي تزداد دقة أكثر فأكثر، الأمل هو أن تكون هناك القدرة على تشفير أفكار ذات مستوى 

321
00:24:01,971 --> 00:24:07,320
أعلى وأكثر تجريدًا حول فكرة معينة المدخلات تتجاوز مجرد الواصفات والبنية النحوية.

322
00:24:07,880 --> 00:24:11,630
أشياء مثل المشاعر والنبرة وما إذا كانت قصيدة وما هي الحقائق 

323
00:24:11,630 --> 00:24:15,130
العلمية الأساسية ذات الصلة بالمقال وأشياء من هذا القبيل.

324
00:24:16,700 --> 00:24:22,424
بالعودة مرة أخرى إلى تسجيل النتائج، يتضمن GPT-3 96 طبقة متميزة، لذلك يتم 

325
00:24:22,424 --> 00:24:28,305
ضرب العدد الإجمالي لمعلمات الاستعلام والقيمة الرئيسية في 96 أخرى، مما يجعل 

326
00:24:28,305 --> 00:24:34,500
المجموع الإجمالي أقل بقليل من 58 مليار معلمة متميزة مخصصة لجميع رؤساء الاهتمام.

327
00:24:34,980 --> 00:24:40,940
وهذا أمر مؤكد، لكنه لا يمثل سوى حوالي ثلث إجمالي 175 مليارًا الموجودة في الشبكة.

328
00:24:41,520 --> 00:24:44,799
لذلك، على الرغم من أن الاهتمام يحظى بكل الاهتمام، فإن 

329
00:24:44,799 --> 00:24:48,140
غالبية المعلمات تأتي من الكتل الموجودة بين هذه الخطوات.

330
00:24:48,560 --> 00:24:53,560
في الفصل التالي، سنتحدث أنا وأنت أكثر عن تلك الكتل الأخرى وأيضًا الكثير عن عملية التدريب.

331
00:24:54,120 --> 00:24:58,852
لا يرجع جزء كبير من قصة نجاح آلية الانتباه إلى أي نوع محدد من السلوك الذي 

332
00:24:58,852 --> 00:25:03,647
تتيحه، ولكن حقيقة أنها قابلة للتوازي إلى حد كبير، مما يعني أنه يمكنك تشغيل 

333
00:25:03,647 --> 00:25:08,380
عدد كبير من العمليات الحسابية في وقت قصير باستخدام وحدات معالجة الرسومات .

334
00:25:09,460 --> 00:25:13,272
بالنظر إلى أن أحد الدروس الكبيرة حول التعلم العميق في العقد أو العقدين 

335
00:25:13,272 --> 00:25:16,978
الماضيين هو أن النطاق وحده يبدو أنه يعطي تحسينات نوعية هائلة في أداء 

336
00:25:16,978 --> 00:25:21,060
النموذج، فهناك ميزة كبيرة للبنيات القابلة للتوازي والتي تتيح لك القيام بذلك.

337
00:25:22,040 --> 00:25:25,340
إذا كنت تريد معرفة المزيد عن هذه الأشياء، فقد تركت الكثير من الروابط في الوصف.

338
00:25:25,920 --> 00:25:30,040
على وجه الخصوص، أي شيء ينتجه أندريه كارباثي أو كريس أولا يميل إلى أن يكون ذهبًا خالصًا.

339
00:25:30,560 --> 00:25:34,621
في هذا الفيديو، أردت فقط لفت الانتباه بشكلها الحالي، ولكن إذا كنت مهتمًا بمعرفة 

340
00:25:34,621 --> 00:25:38,580
المزيد عن التاريخ حول كيفية وصولنا إلى هنا وكيف يمكنك إعادة اختراع هذه الفكرة 

341
00:25:38,580 --> 00:25:42,540
بنفسك، فقد وضع صديقي فيفيك بعضًا منها أشرطة الفيديو تعطي الكثير من هذا الدافع.

342
00:25:43,120 --> 00:25:45,815
أيضًا، لدى بريت كروز من قناة The Art of the المشكلة 

343
00:25:45,815 --> 00:25:48,460
مقطع فيديو رائع حقًا عن تاريخ نماذج اللغات الكبيرة.

344
00:26:04,960 --> 00:26:09,200
شكرًا لك.

