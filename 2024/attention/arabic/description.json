[
 {
  "translatedText": "إزالة الغموض عن الاهتمام الذاتي، والرؤوس المتعددة، والانتباه المتبادل.",
  "input": "Demystifying self-attention, multiple heads, and cross-attention.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "بدلاً من قراءة الإعلانات المدعومة، يتم تمويل هذه الدروس مباشرة من قبل المشاهدين: https://3b1b.co/support",
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "أحد أشكال الدعم ذات القيمة نفسها هو ببساطة مشاركة مقاطع الفيديو.",
  "input": "An equally valuable form of support is to simply share the videos.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "موارد أخرى حول المحولات",
  "input": "Other resources about transformers",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "فيديوهات أندريه كارباثي",
  "input": "Andrej Karpathy's videos",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "مشاركات دوائر المحولات بواسطة Anthropic",
  "input": "The Transformer Circuits posts by Anthropic",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://transformer-circuits.pub/2021/framework/index.html",
  "input": "https://transformer-circuits.pub/2021/framework/index.html",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "على وجه الخصوص، فقط بعد أن قرأت هذا المنشور بدأت أفكر في الجمع بين مصفوفات القيمة والمخرجات باعتبارها خريطة مدمجة منخفضة الرتبة من مساحة التضمين إلى نفسها، الأمر الذي، على الأقل في ذهني، جعل الأمور أكثر أهمية أوضح من المصادر الأخرى.",
  "input": "In particular, it was only after I read this post that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "تاريخ نماذج اللغة بقلم بريت كروز، @ArtOfTheProblem ",
  "input": "History of language models by Brit Cruise, @ArtOfTheProblem ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://youtu.be/OFS90-FX6pg",
  "input": "https://youtu.be/OFS90-FX6pg",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "ما هو نموذج اللغة بواسطة @vcubingx ",
  "input": "What is a Language Model by @vcubingx ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "input": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "موقع يحتوي على تمارين متعلقة ببرمجة ML وGPTs",
  "input": "Site with exercises related to ML programming and GPTs",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://www.gptandchill.ai/codingproblems",
  "input": "https://www.gptandchill.ai/codingproblems",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "ورقة مبكرة حول كيفية وجود معنى للاتجاهات في مساحات التضمين:",
  "input": "Early paper on how directions in embedding spaces have meaning:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://arxiv.org/pdf/1301.3781.pdf",
  "input": "https://arxiv.org/pdf/1301.3781.pdf",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "الطوابع الزمنية:",
  "input": "Timestamps:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "0:00 - تلخيص حول التضمينات",
  "input": "0:00 - Recap on embeddings",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "1:39 - أمثلة تحفيزية",
  "input": "1:39 - Motivating examples",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "4:29 – نمط الانتباه",
  "input": "4:29 - The attention pattern",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "11:08 - الإخفاء",
  "input": "11:08 - Masking",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "12:42 - حجم السياق",
  "input": "12:42 - Context size",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "13:10 - القيم",
  "input": "13:10 - Values",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "15:44 - إحصاء المعلمات",
  "input": "15:44 - Counting parameters",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "18:21 - الانتباه المتبادل",
  "input": "18:21 - Cross-attention",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "19:19 - رؤوس متعددة",
  "input": "19:19 - Multiple heads",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "22:16 - مصفوفة الإخراج",
  "input": "22:16 - The output matrix",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "23:19 - التعمق أكثر",
  "input": "23:19 - Going deeper",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "24:54 - النهاية",
  "input": "24:54 - Ending",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 }
]