[
 {
  "translatedText": "في الفصل الأخير، بدأنا أنا وأنت في التعرف على الأعمال الداخلية للمحول.",
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "translatedText": "يعد هذا أحد العناصر الأساسية للتكنولوجيا داخل نماذج اللغات الكبيرة، والعديد من الأدوات الأخرى في الموجة الحديثة من الذكاء الاصطناعي.",
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "translatedText": "ظهرت لأول مرة على الساحة في ورقة بحثية مشهورة عام 2017 بعنوان &quot;الانتباه هو كل ما تحتاجه&quot;، وفي هذا الفصل، سنتعمق أنا وأنت في ماهية آلية الانتباه هذه، ونتصور كيفية معالجتها للبيانات.",
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "translatedText": "كملخص سريع، إليك السياق المهم الذي أريدك أن تضعه في الاعتبار.",
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "translatedText": "الهدف من النموذج الذي ندرسه أنا وأنت هو أخذ جزء من النص والتنبؤ بالكلمة التي ستأتي بعد ذلك.",
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "translatedText": "يتم تقسيم النص المُدخل إلى أجزاء صغيرة نسميها الرموز المميزة، وهي غالبًا ما تكون كلمات أو أجزاء من الكلمات، ولكن فقط لجعل الأمثلة في هذا الفيديو أسهل بالنسبة لي ولكم للتفكير فيها، دعونا نبسطها من خلال التظاهر بأن الرموز المميزة هي دائما مجرد كلمات.",
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "translatedText": "الخطوة الأولى في المحول هي ربط كل رمز بمتجه عالي الأبعاد، وهو ما نسميه تضمينه.",
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "translatedText": "الفكرة الأكثر أهمية التي أريدك أن تضعها في ذهنك هي كيف يمكن للاتجاهات في هذا الفضاء عالي الأبعاد لجميع التضمينات الممكنة أن تتوافق مع المعنى الدلالي.",
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "translatedText": "رأينا في الفصل الأخير مثالا لكيفية توافق الاتجاه مع الجنس، بمعنى أن إضافة خطوة معينة في هذا الفضاء يمكن أن يأخذك من تضمين الاسم المذكر إلى تضمين الاسم المؤنث المقابل.",
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "translatedText": "هذا مجرد مثال واحد يمكنك من خلاله تخيل عدد الاتجاهات الأخرى في هذا الفضاء عالي الأبعاد الذي يمكن أن يتوافق مع العديد من الجوانب الأخرى لمعنى الكلمة.",
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "translatedText": "الهدف من المحول هو ضبط هذه التضمينات تدريجيًا بحيث لا تقوم بتشفير كلمة فردية فحسب، بل تخبز بدلاً من ذلك معنى سياقيًا أكثر ثراءً بكثير.",
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "translatedText": "يجب أن أقول مقدمًا أن الكثير من الناس يجدون آلية الانتباه، هذه القطعة الرئيسية في المحول، مربكة للغاية، لذا لا تقلق إذا استغرق الأمر بعض الوقت حتى تستوعب الأمور.",
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "translatedText": "أعتقد أنه قبل أن نتعمق في التفاصيل الحسابية وجميع عمليات ضرب المصفوفات، من المفيد التفكير في بضعة أمثلة لنوع السلوك الذي نريد الاهتمام بتمكينه.",
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "translatedText": "خذ بعين الاعتبار العبارات &quot;الخلد الأمريكي الحقيقي&quot;، &quot;مول واحد من ثاني أكسيد الكربون&quot;، وأخذ خزعة من الشامة.",
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "translatedText": "أنا وأنت نعلم أن كلمة شامة لها معاني مختلفة في كل واحدة منها، بناءً على السياق.",
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "translatedText": "ولكن بعد الخطوة الأولى للمحول، الذي يقسم النص ويربط كل رمز بمتجه، فإن المتجه المرتبط بالمول سيكون هو نفسه في كل هذه الحالات، لأن تضمين الرمز المميز هذا هو في الواقع جدول بحث دون الرجوع إلى السياق.",
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "translatedText": "فقط في الخطوة التالية من المحول تكون للتضمينات المحيطة فرصة لتمرير المعلومات إلى هذا المحول.",
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "translatedText": "الصورة التي قد تكون في ذهنك هي أن هناك اتجاهات متعددة ومتميزة في مساحة التضمين هذه التي تشفر المعاني المتميزة المتعددة لكلمة mole، وأن كتلة الانتباه المدربة جيدًا تحسب ما تحتاج إلى إضافته إلى التضمين العام لنقله إليه أحد هذه الاتجاهات المحددة، كوظيفة للسياق.",
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "translatedText": "لنأخذ مثالاً آخر، فكر في تضمين كلمة برج.",
  "input": "To take another example, consider the embedding of the word tower.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "translatedText": "من المفترض أن يكون هذا اتجاهًا عامًا جدًا وغير محدد في الفضاء، ويرتبط بالكثير من الأسماء الكبيرة والطويلة الأخرى.",
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "translatedText": "إذا سبقت هذه الكلمة مباشرة كلمة إيفل، فيمكنك أن تتخيل أنك ترغب في تحديث الآلية لهذا المتجه بحيث يشير إلى اتجاه يرمز بشكل أكثر تحديدًا إلى برج إيفل، وربما يرتبط بالمتجهات المرتبطة بباريس وفرنسا والأشياء المصنوعة من الفولاذ.",
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "translatedText": "إذا سبقتها أيضًا كلمة مصغرة، فيجب تحديث المتجه بشكل أكبر، بحيث لا يرتبط بالأشياء الكبيرة الطويلة.",
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "translatedText": "بشكل أكثر عمومية من مجرد تحسين معنى كلمة ما، تسمح كتلة الانتباه للنموذج بنقل المعلومات المشفرة في أحد العناصر المضمنة إلى معلومات أخرى، ومن المحتمل أن تكون بعيدة جدًا، وربما بمعلومات أكثر ثراءً من مجرد كلمة واحدة.",
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "translatedText": "ما رأيناه في الفصل الأخير هو أنه بعد تدفق جميع المتجهات عبر الشبكة، بما في ذلك العديد من كتل الانتباه المختلفة، فإن الحساب الذي تقوم به لإنتاج تنبؤ بالرمز المميز التالي هو بالكامل وظيفة المتجه الأخير في التسلسل.",
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "translatedText": "تخيل، على سبيل المثال، أن النص الذي تقوم بإدخاله هو في الغالب رواية غامضة بأكملها، وصولاً إلى نقطة قريبة من النهاية، والتي تقول: إذن كان القاتل.",
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "translatedText": "إذا كان النموذج سيتنبأ بدقة بالكلمة التالية، فإن المتجه الأخير في التسلسل، والذي بدأ حياته ببساطة بتضمين الكلمة، يجب أن يتم تحديثه بواسطة جميع كتل الانتباه لتمثيل أكثر بكثير من أي فرد word، يقوم بطريقة ما بتشفير جميع المعلومات من نافذة السياق الكاملة ذات الصلة بالتنبؤ بالكلمة التالية.",
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "translatedText": "ومع ذلك، لكي ننتقل إلى العمليات الحسابية، دعونا نأخذ مثالًا أبسط بكثير.",
  "input": "To step through the computations, though, let's take a much simpler example.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "translatedText": "تخيل أن الإدخال يتضمن العبارة، مخلوق أزرق رقيق يتجول في الغابة الخضراء.",
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "translatedText": "وفي الوقت الحالي، لنفترض أن النوع الوحيد من التحديث الذي نهتم به هو جعل الصفات تضبط معاني الأسماء المقابلة لها.",
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "translatedText": "ما أنا على وشك وصفه هو ما نسميه رأسًا واحدًا للانتباه، وسنرى لاحقًا كيف تتكون كتلة الانتباه من العديد من الرؤوس المختلفة التي تعمل بالتوازي.",
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "translatedText": "مرة أخرى، التضمين الأولي لكل كلمة هو عبارة عن ناقل عالي الأبعاد يقوم فقط بتشفير معنى تلك الكلمة المحددة بدون سياق.",
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "translatedText": "في الواقع، هذا ليس صحيحا تماما.",
  "input": "Actually, that's not quite true.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "translatedText": "كما أنها تشفر موضع الكلمة.",
  "input": "They also encode the position of the word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "translatedText": "هناك الكثير لنقوله عن الطريقة التي يتم بها تشفير المواضع، لكن الآن، كل ما تحتاج إلى معرفته هو أن إدخالات هذا المتجه كافية لإخبارك عن ماهية الكلمة ومكان وجودها في السياق.",
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "translatedText": "دعنا نمضي قدمًا ونشير إلى هذه التضمينات بالحرف e.",
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "translatedText": "الهدف هو الحصول على سلسلة من الحسابات تنتج مجموعة جديدة منقحة من التضمينات، حيث، على سبيل المثال، تلك المقابلة للأسماء قد استوعبت المعنى من الصفات المقابلة لها.",
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "translatedText": "ومن خلال لعب لعبة التعلم العميق، نريد أن تبدو معظم الحسابات المعنية مثل منتجات مصفوفة متجهة، حيث تكون المصفوفات مليئة بالأوزان القابلة للضبط، وهي أشياء سيتعلمها النموذج بناءً على البيانات.",
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "translatedText": "لكي أكون واضحًا، أنا أقوم بإعداد هذا المثال للصفات التي تقوم بتحديث الأسماء فقط لتوضيح نوع السلوك الذي يمكن أن تتخيله.",
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "translatedText": "كما هو الحال مع الكثير من التعلم العميق، يصعب تحليل السلوك الحقيقي لأنه يعتمد على التغيير والتبديل في عدد كبير من المعلمات لتقليل بعض وظائف التكلفة.",
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "translatedText": "إنه فقط بينما ننتقل عبر جميع المصفوفات المختلفة المليئة بالمعلمات المتضمنة في هذه العملية، أعتقد أنه من المفيد حقًا أن يكون لدينا مثال متخيل لشيء يمكن القيام به للمساعدة في إبقائه أكثر واقعية.",
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "translatedText": "في الخطوة الأولى من هذه العملية، قد تتخيل كل اسم، مثل المخلوق، يطرح السؤال، هل هناك أي صفات تجلس أمامي؟",
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "translatedText": "وبالنسبة للكلمات رقيق وأزرق، ليتمكن كل منهما من الإجابة، نعم، أنا صفة وأنا في هذا الموقف.",
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "translatedText": "تم ترميز هذا السؤال بطريقة ما باعتباره متجهًا آخر، أو قائمة أخرى من الأرقام، والتي نسميها الاستعلام عن هذه الكلمة.",
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "translatedText": "على الرغم من أن متجه الاستعلام هذا له بُعد أصغر بكثير من متجه التضمين، على سبيل المثال 128.",
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "translatedText": "إن حساب هذا الاستعلام يشبه أخذ مصفوفة معينة، والتي سأسميها wq، وضربها في التضمين.",
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "translatedText": "بضغط الأشياء قليلاً، دعنا نكتب متجه الاستعلام هذا كـ q، ثم في أي وقت تراني أضع مصفوفة بجوار سهم مثل هذا، من المفترض أن تمثل أن ضرب هذه المصفوفة في المتجه في بداية السهم يمنحك المتجه عند نهاية السهم.",
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "translatedText": "في هذه الحالة، يمكنك ضرب هذه المصفوفة بجميع التضمينات الموجودة في السياق، مما يؤدي إلى إنتاج متجه استعلام واحد لكل رمز مميز.",
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "translatedText": "مدخلات هذه المصفوفة هي معلمات النموذج، مما يعني أن السلوك الحقيقي يتم تعلمه من البيانات، ومن الناحية العملية، فإن ما تفعله هذه المصفوفة في رأس انتباه معين يصعب تحليله.",
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "translatedText": "لكن من أجل مصلحتنا، تخيل مثالًا قد نأمل أن يتعلمه، سنفترض أن مصفوفة الاستعلام هذه تحدد تضمينات الأسماء في اتجاهات معينة في مساحة الاستعلام الأصغر هذه والتي تشفر بطريقة ما فكرة البحث عن الصفات في المواضع السابقة .",
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "translatedText": "وأما ماذا يفعل بالتضمينات الأخرى فمن يدري؟",
  "input": "As to what it does to other embeddings, who knows?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "translatedText": "ربما يحاول في نفس الوقت تحقيق هدف آخر مع هؤلاء.",
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "translatedText": "في الوقت الحالي، نحن نركز الليزر على الأسماء.",
  "input": "Right now, we're laser focused on the nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "translatedText": "وفي الوقت نفسه، ترتبط بهذا مصفوفة ثانية تسمى المصفوفة الرئيسية، والتي يمكنك أيضًا ضربها في كل واحدة من التضمينات.",
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "translatedText": "وينتج عن ذلك سلسلة ثانية من المتجهات التي نسميها المفاتيح.",
  "input": "This produces a second sequence of vectors that we call the keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "translatedText": "من الناحية النظرية، تريد أن تفكر في المفاتيح على أنها تجيب على الاستفسارات.",
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "translatedText": "هذه المصفوفة الرئيسية مليئة أيضًا بالمعلمات القابلة للضبط، وكما هو الحال مع مصفوفة الاستعلام، فإنها تقوم بتعيين متجهات التضمين إلى نفس المساحة ذات الأبعاد الأصغر.",
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "translatedText": "تعتقد أن المفاتيح مطابقة للاستعلامات عندما تتوافق بشكل وثيق مع بعضها البعض.",
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "translatedText": "في مثالنا، يمكنك أن تتخيل أن المصفوفة الرئيسية تقوم بتعيين الصفات مثل رقيق وأزرق إلى المتجهات التي تتماشى بشكل وثيق مع الاستعلام الناتج عن الكلمة مخلوق.",
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "translatedText": "لقياس مدى تطابق كل مفتاح مع كل استعلام، يمكنك حساب منتج نقطي بين كل زوج استعلام رئيسي محتمل.",
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "translatedText": "أحب أن أتخيل شبكة مليئة بمجموعة من النقاط، حيث تتوافق النقاط الأكبر مع منتجات النقاط الأكبر، والأماكن التي تتماشى فيها المفاتيح والاستعلامات.",
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "translatedText": "بالنسبة لمثال اسم الصفة الخاص بنا، سيبدو هذا أكثر قليلًا مثل هذا، حيث إذا كانت المفاتيح التي تم إنتاجها بواسطة رقيق وأزرق تتوافق حقًا بشكل وثيق مع الاستعلام الناتج عن المخلوق، فإن منتجات النقاط في هاتين النقطتين ستكون بعض الأرقام الموجبة الكبيرة.",
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "translatedText": "في اللغة، قد يقول الأشخاص الذين يتعلمون الآلة أن هذا يعني أن عمليات التضمين الرقيقة والزرقاء تحضر لتضمين المخلوق.",
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "translatedText": "على النقيض من المنتج النقطي بين مفتاح بعض الكلمات الأخرى مثل والاستعلام عن المخلوق، سيكون هناك قيمة صغيرة أو سلبية تعكس قيمًا لا علاقة لها ببعضها البعض.",
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "translatedText": "إذن لدينا شبكة من القيم التي يمكن أن تكون أي عدد حقيقي من اللانهاية السالبة إلى اللانهاية، مما يمنحنا درجة لمدى صلة كل كلمة بتحديث معنى كل كلمة أخرى.",
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "translatedText": "الطريقة التي نحن على وشك استخدام هذه الدرجات فيها هي أخذ مبلغ مرجح معين على طول كل عمود، مرجحًا حسب الصلة.",
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "translatedText": "لذا بدلًا من أن تتراوح القيم من اللانهاية السالبة إلى اللانهاية، ما نريده هو أن تكون الأرقام في هذه الأعمدة بين 0 و1، وأن يضيف كل عمود ما يصل إلى 1، كما لو كانت توزيعًا احتماليًا.",
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "translatedText": "إذا كنت قادمًا من الفصل الأخير، فأنت تعرف ما يتعين علينا فعله بعد ذلك.",
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "translatedText": "نقوم بحساب softmax على طول كل عمود من هذه الأعمدة لتطبيع القيم.",
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "translatedText": "في صورتنا، بعد تطبيق softmax على جميع الأعمدة، سنقوم بملء الشبكة بهذه القيم التي تمت تسويتها.",
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "translatedText": "في هذه المرحلة، يمكنك التفكير بأمان في كل عمود باعتباره يعطي أوزانًا وفقًا لمدى صلة الكلمة الموجودة على اليسار بالقيمة المقابلة في الأعلى.",
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "translatedText": "نحن نسمي هذه الشبكة نمط الاهتمام.",
  "input": "We call this grid an attention pattern.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "translatedText": "الآن، إذا نظرتم إلى ورقة المحولات الأصلية، هناك طريقة مدمجة حقًا لكتابة كل هذا.",
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "translatedText": "هنا يمثل المتغيران q وk المصفوفات الكاملة للاستعلام والمتجهات الرئيسية على التوالي، تلك المتجهات الصغيرة التي تحصل عليها عن طريق ضرب التضمينات في الاستعلام والمصفوفات الرئيسية.",
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "translatedText": "يعد هذا التعبير الموجود في البسط طريقة مدمجة حقًا لتمثيل شبكة جميع منتجات النقاط الممكنة بين أزواج المفاتيح والاستعلامات.",
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "translatedText": "هناك تفاصيل فنية صغيرة لم أذكرها وهي أنه لتحقيق الاستقرار الرقمي، من المفيد تقسيم كل هذه القيم على الجذر التربيعي للبعد في مساحة الاستعلام الرئيسية هذه.",
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "translatedText": "ثم من المفترض أن يتم فهم softmax الملتف حول التعبير الكامل لتطبيق عمود بعمود.",
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "translatedText": "أما بالنسبة لمصطلح v هذا، فسنتحدث عنه خلال ثانية واحدة فقط.",
  "input": "As to that v term, we'll talk about it in just a second.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "translatedText": "قبل ذلك، هناك تفصيل فني آخر لم أتجاوزه حتى الآن.",
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "translatedText": "أثناء عملية التدريب، عندما تقوم بتشغيل هذا النموذج على مثال نصي معين، ويتم تعديل جميع الأوزان قليلاً وضبطها إما لمكافأته أو معاقبته بناءً على مدى الاحتمالية العالية التي يعينها للكلمة التالية الحقيقية في المقطع، فإنه اتضح أنه يجعل عملية التدريب بأكملها أكثر كفاءة إذا قمت في نفس الوقت بالتنبؤ بكل رمز مميز تالٍ محتمل بعد كل سلسلة لاحقة أولية من الرموز المميزة في هذا المقطع.",
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "translatedText": "على سبيل المثال، في العبارة التي ركزنا عليها، قد تتنبأ أيضًا بالكلمات التي تتبع المخلوق والكلمات التي تتبع.",
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "translatedText": "هذا أمر لطيف حقًا، لأنه يعني أن ما يمكن أن يكون مثالًا تدريبيًا واحدًا يعمل بشكل فعال على العديد من الأمثلة.",
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "translatedText": "لأغراض نمط انتباهنا، فهذا يعني أنك لا تريد أبدًا السماح للكلمات اللاحقة بالتأثير على الكلمات السابقة، وإلا فإنها قد تعطي نوعًا ما إجابة لما سيأتي بعد ذلك.",
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "translatedText": "ما يعنيه هذا هو أننا نريد أن يتم إجبار كل هذه النقاط هنا، تلك التي تمثل الرموز اللاحقة التي تؤثر على الرموز السابقة، على أن تكون صفرًا بطريقة أو بأخرى.",
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "translatedText": "إن أبسط شيء قد تفكر في القيام به هو جعلها مساوية للصفر، ولكن إذا فعلت ذلك فلن يكون مجموع الأعمدة واحدًا بعد الآن، ولن تتم تطبيعها.",
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "translatedText": "لذا بدلاً من ذلك، الطريقة الشائعة للقيام بذلك هي أنه قبل تطبيق softmax، يمكنك تعيين كل هذه الإدخالات لتكون سالبة لا نهاية.",
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "translatedText": "إذا قمت بذلك، فبعد تطبيق softmax، ستتحول كل تلك العناصر إلى صفر، لكن تظل الأعمدة طبيعية.",
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "translatedText": "هذه العملية تسمى اخفاء.",
  "input": "This process is called masking.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "translatedText": "هناك إصدارات من الاهتمام لا يمكنك تطبيقها فيها، ولكن في مثال GPT الخاص بنا، على الرغم من أن هذا أكثر أهمية خلال مرحلة التدريب مما سيكون عليه، على سبيل المثال، تشغيله كبرنامج دردشة آلي أو شيء من هذا القبيل، إلا أنك تقوم دائمًا بتطبيقه هذا الإخفاء لمنع الرموز المميزة اللاحقة من التأثير على الرموز السابقة.",
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "translatedText": "هناك حقيقة أخرى تستحق التفكير بشأن نمط الانتباه هذا وهي كيف أن حجمه يساوي مربع حجم السياق.",
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "translatedText": "ولهذا السبب يمكن أن يشكل حجم السياق عائقًا كبيرًا لنماذج اللغات الكبيرة، وتوسيع نطاقه ليس بالأمر التافه.",
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "translatedText": "كما تتخيل، بدافع من الرغبة في نوافذ سياق أكبر وأكبر، شهدت السنوات الأخيرة بعض الاختلافات في آلية الانتباه التي تهدف إلى جعل السياق أكثر قابلية للتطوير، ولكن هنا، أنا وأنت نواصل التركيز على الأساسيات.",
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "translatedText": "حسنًا، رائع، حساب هذا النمط يتيح للنموذج استنتاج الكلمات ذات الصلة بالكلمات الأخرى.",
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "translatedText": "أنت الآن بحاجة إلى تحديث التضمينات فعليًا، مما يسمح للكلمات بتمرير المعلومات إلى أي كلمات أخرى ذات صلة بها.",
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "translatedText": "على سبيل المثال، تريد أن يتسبب تضمين Fluffy بطريقة ما في حدوث تغيير في المخلوق الذي ينقله إلى جزء مختلف من مساحة التضمين ذات 12000 بُعد والتي تقوم بتشفير مخلوق Fluffy بشكل أكثر تحديدًا.",
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "translatedText": "ما سأفعله هنا هو أولًا أن أوضح لك الطريقة الأكثر وضوحًا التي يمكنك من خلالها القيام بذلك، على الرغم من وجود طريقة طفيفة لتعديل هذا في سياق الاهتمام متعدد الرؤوس.",
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "translatedText": "الطريقة الأكثر وضوحًا هي استخدام مصفوفة ثالثة، ما نسميه مصفوفة القيمة، والتي تقوم بضربها من خلال تضمين تلك الكلمة الأولى، على سبيل المثال Fluffy.",
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "translatedText": "نتيجة هذا هي ما يمكن أن تسميه متجه القيمة، وهذا شيء تضيفه إلى تضمين الكلمة الثانية، في هذه الحالة شيء تضيفه إلى تضمين المخلوق.",
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "translatedText": "لذا فإن متجه القيمة هذا يعيش في نفس المساحة ذات الأبعاد العالية جدًا مثل التضمينات.",
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "translatedText": "عندما تقوم بضرب مصفوفة القيمة هذه بتضمين كلمة ما، قد تفكر في الأمر كقول، إذا كانت هذه الكلمة ذات صلة بتعديل معنى شيء آخر، فما الذي يجب إضافته بالضبط إلى تضمين ذلك الشيء الآخر من أجل عكسه هذا؟",
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "translatedText": "إذا نظرنا إلى الرسم البياني الخاص بنا، فلنضع جانبًا جميع المفاتيح والاستعلامات، لأنه بعد حساب نمط الانتباه الذي انتهيت منه، ستأخذ مصفوفة القيمة هذه وتضربها في كل واحدة من تلك التضمينات لإنتاج سلسلة من ناقلات القيمة.",
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "translatedText": "قد تعتقد أن متجهات القيمة هذه مرتبطة نوعًا ما بالمفاتيح المقابلة.",
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "translatedText": "بالنسبة لكل عمود في هذا الرسم البياني، يمكنك ضرب كل من متجهات القيمة بالوزن المقابل في ذلك العمود.",
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "translatedText": "على سبيل المثال، هنا، ضمن تضمين Creature، ستضيف نسبًا كبيرة من متجهات القيمة لـ Fluffy وBlue، بينما يتم التخلص من جميع متجهات القيمة الأخرى، أو على الأقل يتم التخلص منها تقريبًا.",
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "translatedText": "وأخيرًا، طريقة التحديث الفعلي للتضمين المرتبط بهذا العمود، وذلك عن طريق تشفير بعض المعاني الخالية من السياق لـ Creature، حيث تقوم بإضافة كل هذه القيم التي تم إعادة قياسها في العمود، مما ينتج عنه التغيير الذي تريد إضافته، وهو ما أريده. سوف نقوم بتسمية delta-e، ثم تضيف ذلك إلى التضمين الأصلي.",
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "translatedText": "نأمل أن تكون النتائج عبارة عن ناقل أكثر دقة يقوم بترميز المعنى الأكثر ثراءً من حيث السياق، مثل معنى مخلوق أزرق رقيق.",
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "translatedText": "وبالطبع لا تفعل ذلك على عملية تضمين واحدة فحسب، بل يمكنك تطبيق نفس المجموع المرجح عبر جميع الأعمدة في هذه الصورة، مما ينتج عنه سلسلة من التغييرات، وإضافة كل هذه التغييرات إلى عمليات التضمين المقابلة، ينتج تسلسلًا كاملاً من المزيد من التضمينات الدقيقة التي تخرج من كتلة الانتباه.",
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "translatedText": "بتصغير الصورة، هذه العملية برمتها هي ما يمكن أن تصفه برأس واحد من الاهتمام.",
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "translatedText": "كما وصفت الأمور حتى الآن، يتم تحديد معلمات هذه العملية من خلال ثلاث مصفوفات متميزة، كلها مليئة بمعلمات قابلة للضبط، والمفتاح، والاستعلام، والقيمة.",
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "translatedText": "أريد أن أتوقف لحظة لمواصلة ما بدأناه في الفصل الأخير، مع تسجيل النتائج حيث نحسب العدد الإجمالي لمعلمات النموذج باستخدام الأرقام من GPT-3.",
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "translatedText": "تحتوي كل من مصفوفات المفاتيح والاستعلام هذه على 12,288 عمودًا، بما يتوافق مع بُعد التضمين، و128 صفًا، بما يتوافق مع بُعد مساحة استعلام المفتاح الأصغر.",
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "translatedText": "وهذا يمنحنا 1.5 مليون معلمة إضافية أو نحو ذلك لكل واحدة.",
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "translatedText": "إذا نظرت إلى مصفوفة القيمة تلك على النقيض من ذلك، فإن الطريقة التي وصفت بها الأشياء حتى الآن تشير إلى أنها مصفوفة مربعة تحتوي على 12288 عمودًا و12288 صفًا، نظرًا لأن مدخلاتها ومخرجاتها تعيش في مساحة التضمين الكبيرة جدًا هذه.",
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "translatedText": "إذا كان هذا صحيحًا، فهذا يعني إضافة حوالي 150 مليون معلمة.",
  "input": "If true, that would mean about 150 million added parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "translatedText": "ولكي أكون واضحًا، يمكنك فعل ذلك.",
  "input": "And to be clear, you could do that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "translatedText": "يمكنك تخصيص أوامر ذات حجم أكبر من المعلمات لخريطة القيمة بدلاً من المفتاح والاستعلام.",
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "translatedText": "لكن من الناحية العملية، يكون الأمر أكثر كفاءة إذا قمت بدلاً من ذلك بإجراء ذلك بحيث يكون عدد المعلمات المخصصة لخريطة القيمة هذه هو نفس العدد المخصص للمفتاح والاستعلام.",
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "translatedText": "هذا مهم بشكل خاص في إعداد تشغيل رؤوس انتباه متعددة بالتوازي.",
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "translatedText": "الطريقة التي يبدو بها هذا هي أن خريطة القيمة يتم تحليلها كمنتج لمصفوفتين أصغر.",
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "translatedText": "من الناحية النظرية، ما زلت أشجعك على التفكير في الخريطة الخطية الشاملة، واحدة تحتوي على المدخلات والمخرجات، سواء في مساحة التضمين الأكبر هذه، على سبيل المثال أخذ تضمين اللون الأزرق في اتجاه الزرقة هذا الذي ستضيفه إلى الأسماء.",
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "translatedText": "إنه مجرد عدد أقل من الصفوف، وعادةً ما يكون بنفس حجم مساحة الاستعلام الرئيسية.",
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "translatedText": "ما يعنيه هذا هو أنه يمكنك التفكير في الأمر على أنه تعيين لمتجهات التضمين الكبيرة وصولاً إلى مساحة أصغر بكثير.",
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "translatedText": "هذه ليست التسمية التقليدية، ولكنني سأسميها مصفوفة القيمة السفلية.",
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "translatedText": "تقوم المصفوفة الثانية بتعيين خريطة من هذه المساحة الأصغر احتياطيًا إلى مساحة التضمين، مما يؤدي إلى إنتاج المتجهات التي تستخدمها لإجراء التحديثات الفعلية.",
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "translatedText": "سأطلق على هذه مصفوفة القيمة الأعلى، والتي مرة أخرى ليست تقليدية.",
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "translatedText": "تبدو الطريقة التي ترى بها هذا مكتوبًا في معظم الصحف مختلفة بعض الشيء.",
  "input": "The way that you would see this written in most papers looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "translatedText": "سأتحدث عن ذلك في دقيقة واحدة.",
  "input": "I'll talk about it in a minute.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "translatedText": "في رأيي، فإنه يميل إلى جعل الأمور أكثر إرباكًا من الناحية المفاهيمية.",
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "translatedText": "لاستخدام مصطلحات الجبر الخطي هنا، ما نقوم به أساسًا هو تقييد خريطة القيمة الإجمالية لتكون تحويلات ذات رتبة منخفضة.",
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "translatedText": "بالعودة إلى عدد المعلمات، جميع هذه المصفوفات الأربع لها نفس الحجم، وبجمعها جميعًا نحصل على حوالي 6.3 مليون معلمة لرأس انتباه واحد.",
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "translatedText": "كملاحظة جانبية سريعة، لكي نكون أكثر دقة، كل ما تم وصفه حتى الآن هو ما يسميه الناس &quot;رأس الانتباه الذاتي&quot;، لتمييزه عن الاختلاف الذي يظهر في النماذج الأخرى والذي يسمى &quot;الانتباه المتبادل&quot;.",
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "translatedText": "هذا ليس له صلة بمثال GPT الخاص بنا، ولكن إذا كنت فضوليًا، فإن الاهتمام المتبادل يتضمن نماذج تعالج نوعين مختلفين من البيانات، مثل النص في لغة واحدة والنص في لغة أخرى والذي يعد جزءًا من الجيل المستمر من الترجمة، أو ربما إدخال صوتي للكلام والنسخ المستمر.",
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "translatedText": "يبدو رأس الانتباه المتقاطع متطابقًا تقريبًا.",
  "input": "A cross-attention head looks almost identical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "translatedText": "والفرق الوحيد هو أن خرائط المفاتيح والاستعلام تعمل على مجموعات بيانات مختلفة.",
  "input": "The only difference is that the key and query maps act on different data sets.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "translatedText": "في نموذج يقوم بالترجمة، على سبيل المثال، قد تأتي المفاتيح من لغة واحدة، في حين تأتي الاستعلامات من لغة أخرى، ويمكن أن يصف نمط الانتباه الكلمات من لغة واحدة التي تتوافق مع الكلمات في لغة أخرى.",
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "translatedText": "وفي هذا الإعداد لن يكون هناك عادةً أي إخفاء، نظرًا لعدم وجود أي فكرة عن تأثير الرموز المميزة اللاحقة على الرموز السابقة.",
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "translatedText": "ومع ذلك، استمر في التركيز على الاهتمام الذاتي، إذا فهمت كل شيء حتى الآن، وإذا توقفت هنا، فسوف تتوصل إلى جوهر الاهتمام الحقيقي.",
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "translatedText": "كل ما تبقى لنا حقًا هو توضيح المعنى الذي تفعل به هذا عدة مرات مختلفة.",
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "translatedText": "في مثالنا المركزي، ركزنا على تحديث الصفات للأسماء، ولكن بالطبع هناك الكثير من الطرق المختلفة التي يمكن أن يؤثر بها السياق على معنى الكلمة.",
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "translatedText": "إذا كانت عبارة &quot;تحطموا&quot; سبقت كلمة &quot;سيارة&quot;، فإن ذلك يكون له آثار على شكل وبنية تلك السيارة.",
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "translatedText": "وقد تكون الكثير من الارتباطات أقل نحويًا.",
  "input": "And a lot of associations might be less grammatical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "translatedText": "إذا كانت كلمة &quot;ساحر&quot; موجودة في أي مكان في نفس المقطع مثل هاري، فهذا يشير إلى أن هذا قد يشير إلى هاري بوتر، بينما إذا كانت الكلمات الملكة وساسكس وويليام موجودة في هذا المقطع، فربما ينبغي تحديث تضمين هاري بدلاً من ذلك. للإشارة إلى الأمير.",
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "translatedText": "بالنسبة لكل نوع مختلف من التحديث السياقي الذي قد تتخيله، ستكون معلمات هذه المصفوفات الرئيسية والاستعلام مختلفة لالتقاط أنماط الاهتمام المختلفة، وستكون معلمات خريطة القيمة الخاصة بنا مختلفة بناءً على ما يجب إضافته إلى التضمينات.",
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "translatedText": "ومرة أخرى، من الناحية العملية، يكون تفسير السلوك الحقيقي لهذه الخرائط أكثر صعوبة، حيث يتم تعيين الأوزان للقيام بكل ما يحتاج النموذج إلى القيام به لتحقيق هدفه المتمثل في التنبؤ بالرمز التالي على أفضل وجه.",
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "translatedText": "كما قلت من قبل، كل ما وصفناه هو رأس واحد للانتباه، وتتكون كتلة الاهتمام الكاملة داخل المحول مما يسمى بالانتباه متعدد الرؤوس، حيث تقوم بتشغيل الكثير من هذه العمليات بالتوازي، ولكل منها استعلام رئيسي مميز خاص بها وخرائط القيمة.",
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "translatedText": "يستخدم GPT-3 على سبيل المثال 96 رأس انتباه داخل كل كتلة.",
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "translatedText": "مع الأخذ في الاعتبار أن كل واحدة منها مربكة بعض الشيء بالفعل، فمن المؤكد أن هناك الكثير مما يجب أن تضعه في رأسك.",
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "translatedText": "فقط لتوضيح كل ذلك بشكل واضح جدًا، هذا يعني أن لديك 96 مفتاحًا مميزًا ومصفوفة استعلام تنتج 96 نمطًا متميزًا من الاهتمام.",
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "translatedText": "ثم يكون لكل رأس مصفوفات قيمة مميزة خاصة به تُستخدم لإنتاج 96 سلسلة من متجهات القيمة.",
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "translatedText": "تتم إضافة كل ذلك معًا باستخدام أنماط الاهتمام المقابلة كأوزان.",
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "translatedText": "ما يعنيه هذا هو أنه بالنسبة لكل موضع في السياق، كل رمز مميز، كل واحد من هذه الرؤوس ينتج تغييرًا مقترحًا ليتم إضافته إلى التضمين في هذا الموضع.",
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "translatedText": "إذن ما تفعله هو جمع كل هذه التغييرات المقترحة، واحدة لكل رأس، وإضافة النتيجة إلى التضمين الأصلي لهذا الموضع.",
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "translatedText": "هذا المجموع بأكمله هنا سيكون شريحة واحدة مما يتم إنتاجه من كتلة الانتباه متعددة الرؤوس، شريحة واحدة من تلك التضمينات المكررة التي تنبثق من الطرف الآخر منها.",
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "translatedText": "مرة أخرى، هذا أمر كثير يجب التفكير فيه، لذا لا تقلق على الإطلاق إذا استغرق الأمر بعض الوقت حتى تستوعبه.",
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "translatedText": "الفكرة العامة هي أنه من خلال تشغيل العديد من الرؤوس المميزة بالتوازي، فإنك تمنح النموذج القدرة على تعلم العديد من الطرق المميزة التي يغير بها السياق المعنى.",
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "translatedText": "وبسحب رصيدنا الجاري لعدد المعلمات إلى 96 رأسًا، يتضمن كل منها نسخته الخاصة من هذه المصفوفات الأربع، فإن كل كتلة من الاهتمام متعدد الرؤوس تنتهي بحوالي 600 مليون معلمة.",
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "translatedText": "هناك شيء إضافي مزعج بعض الشيء يجب أن أذكره حقًا لأي منكم يواصل قراءة المزيد عن المحولات.",
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "translatedText": "هل تتذكر كيف قلت إن خريطة القيمة تم تحليلها إلى هاتين المصفوفتين المتميزتين، اللتين سميتهما بمصفوفات القيمة السفلية والقيمة الأعلى.",
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "translatedText": "الطريقة التي قمت بها بتأطير الأشياء تشير إلى أنك ترى هذا الزوج من المصفوفات داخل كل رأس انتباه، ويمكنك بالتأكيد تنفيذ ذلك بهذه الطريقة.",
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "translatedText": "سيكون ذلك تصميمًا صالحًا.",
  "input": "That would be a valid design.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "translatedText": "لكن الطريقة التي ترى بها هذا مكتوبًا في الأوراق وطريقة تنفيذه عمليًا تبدو مختلفة قليلًا.",
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "translatedText": "تظهر كل مصفوفات القيمة المرتفعة لكل رأس مجمعة معًا في مصفوفة عملاقة واحدة نسميها مصفوفة الإخراج، المرتبطة بكتلة الانتباه متعددة الرؤوس بأكملها.",
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "translatedText": "وعندما ترى الناس يشيرون إلى مصفوفة القيمة لرأس اهتمام معين، فإنهم عادة ما يشيرون فقط إلى هذه الخطوة الأولى، تلك التي كنت أصنفها على أنها إسقاط القيمة لأسفل في المساحة الأصغر.",
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "translatedText": "للفضوليين بينكم، لقد تركت ملاحظة على الشاشة حول هذا الموضوع.",
  "input": "For the curious among you, I've left an on-screen note about it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "translatedText": "إنها واحدة من تلك التفاصيل التي تنطوي على خطر تشتيت الانتباه عن النقاط المفاهيمية الرئيسية، لكنني أريد أن أذكرها فقط حتى تعرف ما إذا كنت قد قرأت عنها في مصادر أخرى.",
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "translatedText": "وبغض النظر عن الفروق الفنية الدقيقة، فقد رأينا في المعاينة من الفصل الأخير كيف أن البيانات التي تتدفق عبر المحول لا تتدفق عبر كتلة انتباه واحدة فقط.",
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "translatedText": "لسبب واحد، أنها تمر أيضًا بهذه العمليات الأخرى التي تسمى الإدراك الحسي متعدد الطبقات.",
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "translatedText": "سنتحدث أكثر عن تلك في الفصل التالي.",
  "input": "We'll talk more about those in the next chapter.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "translatedText": "وبعد ذلك يمر بشكل متكرر عبر العديد من النسخ لكلتا العمليتين.",
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "translatedText": "ما يعنيه هذا هو أنه بعد أن تتشرب كلمة معينة بعضًا من سياقها، هناك العديد من الفرص لهذا التضمين الأكثر دقة للتأثر بمحيطها الأكثر دقة.",
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "translatedText": "كلما تقدمت في الشبكة، حيث يأخذ كل تضمين معنى أكثر فأكثر من جميع التضمينات الأخرى، والتي تزداد دقة أكثر فأكثر، الأمل هو أن تكون هناك القدرة على تشفير أفكار ذات مستوى أعلى وأكثر تجريدًا حول فكرة معينة المدخلات تتجاوز مجرد الواصفات والبنية النحوية.",
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "translatedText": "أشياء مثل المشاعر والنبرة وما إذا كانت قصيدة وما هي الحقائق العلمية الأساسية ذات الصلة بالمقال وأشياء من هذا القبيل.",
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "translatedText": "بالعودة مرة أخرى إلى تسجيل النتائج، يتضمن GPT-3 96 طبقة متميزة، لذلك يتم ضرب العدد الإجمالي لمعلمات الاستعلام والقيمة الرئيسية في 96 أخرى، مما يجعل المجموع الإجمالي أقل بقليل من 58 مليار معلمة متميزة مخصصة لجميع رؤساء الاهتمام.",
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "translatedText": "وهذا أمر مؤكد، لكنه لا يمثل سوى حوالي ثلث إجمالي 175 مليارًا الموجودة في الشبكة.",
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "translatedText": "لذلك، على الرغم من أن الاهتمام يحظى بكل الاهتمام، فإن غالبية المعلمات تأتي من الكتل الموجودة بين هذه الخطوات.",
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "translatedText": "في الفصل التالي، سنتحدث أنا وأنت أكثر عن تلك الكتل الأخرى وأيضًا الكثير عن عملية التدريب.",
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "translatedText": "لا يرجع جزء كبير من قصة نجاح آلية الانتباه إلى أي نوع محدد من السلوك الذي تتيحه، ولكن حقيقة أنها قابلة للتوازي إلى حد كبير، مما يعني أنه يمكنك تشغيل عدد كبير من العمليات الحسابية في وقت قصير باستخدام وحدات معالجة الرسومات .",
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "translatedText": "بالنظر إلى أن أحد الدروس الكبيرة حول التعلم العميق في العقد أو العقدين الماضيين هو أن النطاق وحده يبدو أنه يعطي تحسينات نوعية هائلة في أداء النموذج، فهناك ميزة كبيرة للبنيات القابلة للتوازي والتي تتيح لك القيام بذلك.",
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "translatedText": "إذا كنت تريد معرفة المزيد عن هذه الأشياء، فقد تركت الكثير من الروابط في الوصف.",
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "translatedText": "على وجه الخصوص، أي شيء ينتجه أندريه كارباثي أو كريس أولا يميل إلى أن يكون ذهبًا خالصًا.",
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "translatedText": "في هذا الفيديو، أردت فقط لفت الانتباه بشكلها الحالي، ولكن إذا كنت مهتمًا بمعرفة المزيد عن التاريخ حول كيفية وصولنا إلى هنا وكيف يمكنك إعادة اختراع هذه الفكرة بنفسك، فقد وضع صديقي فيفيك بعضًا منها أشرطة الفيديو تعطي الكثير من هذا الدافع.",
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "translatedText": "أيضًا، لدى بريت كروز من قناة The Art of the المشكلة مقطع فيديو رائع حقًا عن تاريخ نماذج اللغات الكبيرة.",
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "translatedText": "شكرًا لك.",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]