[
 {
  "translatedText": "ابهام زدایی از توجه به خود، سرهای متعدد و توجه متقابل.",
  "input": "Demystifying self-attention, multiple heads, and cross-attention.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "به‌جای خواندن آگهی‌های حمایت‌شده، این درس‌ها مستقیماً توسط بینندگان تأمین مالی می‌شوند: https://3b1b.co/support",
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "یک شکل به همان اندازه ارزشمند از پشتیبانی، به اشتراک گذاری ویدیوها است.",
  "input": "An equally valuable form of support is to simply share the videos.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "سایر منابع در مورد ترانسفورماتورها",
  "input": "Other resources about transformers",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "ویدیوهای آندری کارپاتی",
  "input": "Andrej Karpathy's videos",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "پست های The Transformer Circuits توسط Anthropic",
  "input": "The Transformer Circuits posts by Anthropic",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://transformer-circuits.pub/2021/framework/index.html",
  "input": "https://transformer-circuits.pub/2021/framework/index.html",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "به طور خاص، تنها پس از خواندن این پست بود که به ترکیب ماتریس های مقدار و خروجی به عنوان یک نقشه ترکیبی با رتبه پایین از فضای تعبیه شده به خود فکر کردم، که حداقل در ذهن من، همه چیز را بسیار زیاد کرد. واضح تر از سایر منابع",
  "input": "In particular, it was only after I read this post that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "تاریخچه مدل های زبان توسط بریت کروز، @ArtOfTheProblem ",
  "input": "History of language models by Brit Cruise, @ArtOfTheProblem ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://youtu.be/OFS90-FX6pg",
  "input": "https://youtu.be/OFS90-FX6pg",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "مدل زبان توسط @vcubingx چیست ",
  "input": "What is a Language Model by @vcubingx ",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "input": "https://youtu.be/1il-s4mgNdI?si=XaVxj6bsdy3VkgEX",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "سایت با تمرین های مربوط به برنامه نویسی ML و GPT",
  "input": "Site with exercises related to ML programming and GPTs",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://www.gptandchill.ai/codingproblems",
  "input": "https://www.gptandchill.ai/codingproblems",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "مقاله اولیه در مورد اینکه مسیرها در فضاهای تعبیه شده چگونه معنی دارند:",
  "input": "Early paper on how directions in embedding spaces have meaning:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://arxiv.org/pdf/1301.3781.pdf",
  "input": "https://arxiv.org/pdf/1301.3781.pdf",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "مهر زمانی:",
  "input": "Timestamps:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "0:00 - خلاصه ای از جاسازی ها",
  "input": "0:00 - Recap on embeddings",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "1:39 - مثال های انگیزشی",
  "input": "1:39 - Motivating examples",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "4:29 - الگوی توجه",
  "input": "4:29 - The attention pattern",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "11:08 - نقاب زدن",
  "input": "11:08 - Masking",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "12:42 - اندازه زمینه",
  "input": "12:42 - Context size",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "13:10 - ارزش ها",
  "input": "13:10 - Values",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "15:44 - شمارش پارامترها",
  "input": "15:44 - Counting parameters",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "18:21 - توجه متقابل",
  "input": "18:21 - Cross-attention",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "19:19 - چند سر",
  "input": "19:19 - Multiple heads",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "22:16 - ماتریس خروجی",
  "input": "22:16 - The output matrix",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "23:19 - عمیق تر رفتن",
  "input": "23:19 - Going deeper",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "24:54 - پایان",
  "input": "24:54 - Ending",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 }
]