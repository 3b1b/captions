[
 {
  "translatedText": "در فصل آخر، من و شما شروع به قدم گذاشتن در عملکرد داخلی یک ترانسفورماتور کردیم.",
  "input": "In the last chapter, you and I started to step through the internal workings of a transformer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.02
 },
 {
  "translatedText": "این یکی از قطعات کلیدی فناوری در مدل های زبان بزرگ و بسیاری از ابزارهای دیگر در موج مدرن هوش مصنوعی است.",
  "input": "This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.56,
  "end": 10.2
 },
 {
  "translatedText": "این اولین بار در یک مقاله معروف در سال 2017 به نام توجه همه آن چیزی است که نیاز دارید به صحنه آمد و در این فصل من و شما به بررسی این مکانیسم توجه خواهیم پرداخت و نحوه پردازش داده ها را تجسم می کنیم.",
  "input": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 10.98,
  "end": 21.7
 },
 {
  "translatedText": "به عنوان یک جمع بندی سریع، در اینجا زمینه مهمی وجود دارد که می خواهم شما در ذهن داشته باشید.",
  "input": "As a quick recap, here's the important context I want you to have in mind.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.14,
  "end": 29.54
 },
 {
  "translatedText": "هدف از مدلی که من و شما در حال مطالعه آن هستیم این است که یک متن را برداریم و پیش‌بینی کنیم چه کلمه‌ای بعدا می‌آید.",
  "input": "The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 30.0,
  "end": 36.06
 },
 {
  "translatedText": "متن ورودی به تکه‌های کوچکی تقسیم می‌شود که ما آن‌ها را نشانه‌ها می‌نامیم، و اینها اغلب کلمات یا تکه‌هایی از کلمات هستند، اما فقط برای اینکه فکر کردن به مثال‌های این ویدیو برای من و شما راحت‌تر باشد، بیایید با تظاهر به اینکه نشانه‌ها هستند، ساده‌سازی کنیم. همیشه فقط کلمات",
  "input": "The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 36.86,
  "end": 50.56
 },
 {
  "translatedText": "اولین مرحله در یک ترانسفورماتور این است که هر نشانه را با یک بردار با ابعاد بالا مرتبط کنیم، چیزی که ما آن را تعبیه می‌کنیم.",
  "input": "The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.48,
  "end": 57.7
 },
 {
  "translatedText": "مهم‌ترین ایده‌ای که می‌خواهم در ذهن داشته باشید این است که چگونه جهت‌ها در این فضای پربعد همه جاسازی‌های ممکن می‌توانند با معنای معنایی مطابقت داشته باشند.",
  "input": "The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 57.7,
  "end": 67.0
 },
 {
  "translatedText": "در فصل آخر مثالی را دیدیم که چگونه جهت می تواند با جنسیت مطابقت داشته باشد، به این معنا که افزودن یک مرحله خاص در این فضا می تواند شما را از جاسازی یک اسم مذکر به جاسازی اسم مونث مربوطه برساند.",
  "input": "In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 67.68,
  "end": 79.64
 },
 {
  "translatedText": "این تنها یک مثال است که می‌توانید تصور کنید چند جهت دیگر در این فضای پربعد می‌تواند با جنبه‌های متعدد دیگری از معنای یک کلمه مطابقت داشته باشد.",
  "input": "That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 80.16,
  "end": 87.58
 },
 {
  "translatedText": "هدف یک ترانسفورماتور تنظیم تدریجی این تعبیه‌ها به گونه‌ای است که آنها صرفاً یک کلمه را رمزگذاری نمی‌کنند، بلکه در عوض به معنای متنی بسیار غنی‌تری تبدیل می‌شوند.",
  "input": "The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 88.8,
  "end": 99.18
 },
 {
  "translatedText": "باید از قبل بگویم که بسیاری از مردم مکانیسم توجه، این قطعه کلید در ترانسفورماتور را بسیار گیج کننده می دانند، بنابراین اگر کمی طول می کشد تا چیزها در آن فرو بروند، نگران نباشید.",
  "input": "I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.14,
  "end": 108.98
 },
 {
  "translatedText": "من فکر می‌کنم قبل از اینکه به جزئیات محاسباتی و همه ضرب‌های ماتریس بپردازیم، ارزش آن را دارد که به چند مثال برای نوع رفتاری که می‌خواهیم توجه را فعال کنیم، فکر کنیم.",
  "input": "I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.44,
  "end": 119.16
 },
 {
  "translatedText": "عبارات مول واقعی آمریکایی، یک مول دی اکسید کربن را در نظر بگیرید و از خال بیوپسی بگیرید.",
  "input": "Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 120.14,
  "end": 126.22
 },
 {
  "translatedText": "من و شما می دانیم که کلمه خال در هر یک از اینها بر اساس زمینه، معانی مختلفی دارد.",
  "input": "You and I know that the word mole has different meanings in each one of these, based on the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 126.7,
  "end": 130.9
 },
 {
  "translatedText": "اما پس از اولین مرحله ترانسفورماتور، مرحله ای که متن را می شکند و هر نشانه را با یک بردار مرتبط می کند، بردار مرتبط با مول در همه این موارد یکسان خواهد بود، زیرا این جاسازی نشانه اولیه در واقع یک جدول جستجو است. بدون اشاره به زمینه",
  "input": "But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.36,
  "end": 146.22
 },
 {
  "translatedText": "تنها در مرحله بعدی ترانسفورماتور است که جاسازی های اطراف این شانس را دارند که اطلاعات را به این ترانسفورماتور منتقل کنند.",
  "input": "It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.62,
  "end": 153.1
 },
 {
  "translatedText": "تصویری که ممکن است در ذهن داشته باشید این است که چندین جهت متمایز در این فضای جاسازی وجود دارد که معانی متمایز چندگانه کلمه مول را رمزگذاری می کند، و اینکه یک بلوک توجه به خوبی آموزش دیده محاسبه می کند که برای انتقال آن به جاسازی عمومی باید چه چیزی اضافه کنید. یکی از این جهت های خاص، به عنوان تابعی از زمینه.",
  "input": "The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 153.82,
  "end": 171.8
 },
 {
  "translatedText": "برای مثال دیگر، تعبیه کلمه برج را در نظر بگیرید.",
  "input": "To take another example, consider the embedding of the word tower.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 176.18
 },
 {
  "translatedText": "احتمالاً این یک جهت بسیار کلی و غیر خاص در فضا است که با بسیاری از اسم‌های بزرگ و بلند دیگر مرتبط است.",
  "input": "This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 177.06,
  "end": 183.72
 },
 {
  "translatedText": "اگر بلافاصله قبل از این کلمه ایفل قرار می گرفت، می توانید تصور کنید که می خواهید مکانیزم این بردار را به روز کند تا به جهتی اشاره کند که به طور خاص برج ایفل را رمزگذاری می کند، شاید با بردارهای مرتبط با پاریس و فرانسه و چیزهای ساخته شده از فولاد مرتبط باشد.",
  "input": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 184.02,
  "end": 199.06
 },
 {
  "translatedText": "اگر قبل از آن کلمه مینیاتور نیز وجود داشت، باید بردار را حتی بیشتر به روز کرد تا دیگر با چیزهای بزرگ و بلند ارتباط نداشته باشد.",
  "input": "If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.92,
  "end": 207.5
 },
 {
  "translatedText": "به طور کلی تر از صرفاً پالایش معنای یک کلمه، بلوک توجه به مدل اجازه می دهد تا اطلاعات رمزگذاری شده در یک جاسازی را به دیگری منتقل کند، به طور بالقوه مواردی که بسیار دور هستند و احتمالاً با اطلاعاتی که بسیار غنی تر از یک کلمه هستند.",
  "input": "More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.48,
  "end": 223.3
 },
 {
  "translatedText": "چیزی که در فصل گذشته دیدیم این بود که چگونه پس از عبور همه بردارها در شبکه، از جمله بسیاری از بلوک‌های توجه مختلف، محاسباتی که برای تولید پیش‌بینی نشانه بعدی انجام می‌دهید، کاملاً تابعی از آخرین بردار در دنباله است.",
  "input": "What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.3,
  "end": 238.28
 },
 {
  "translatedText": "برای مثال تصور کنید متنی که وارد می‌کنید بیشتر یک رمان معمایی است، تا نقطه‌ای نزدیک به پایان، که می‌خواند، بنابراین قاتل بود.",
  "input": "Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 239.1,
  "end": 247.8
 },
 {
  "translatedText": "اگر مدل قرار است کلمه بعدی را به طور دقیق پیش بینی کند، آن بردار نهایی در دنباله، که زندگی خود را صرفاً با جاسازی کلمه آغاز کرد، باید توسط همه بلوک های توجه به روز شده باشد تا بسیار، بسیار بیشتر از هر فرد دیگری را نشان دهد. کلمه، به نوعی تمام اطلاعات را از پنجره زمینه کامل که برای پیش‌بینی کلمه بعدی مرتبط است، رمزگذاری می‌کند.",
  "input": "If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.4,
  "end": 268.22
 },
 {
  "translatedText": "با این حال، برای گذر از محاسبات، مثالی بسیار ساده‌تر می‌زنیم.",
  "input": "To step through the computations, though, let's take a much simpler example.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.5,
  "end": 272.58
 },
 {
  "translatedText": "تصور کنید که ورودی شامل عبارت باشد، یک موجود آبی کرکی در جنگل سرسبز پرسه می زد.",
  "input": "Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 272.98,
  "end": 277.96
 },
 {
  "translatedText": "و فعلاً، فرض کنید که تنها نوع به‌روزرسانی که به آن اهمیت می‌دهیم این است که صفت‌ها معانی اسم‌های مربوطه خود را تنظیم کنند.",
  "input": "And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.46,
  "end": 286.78
 },
 {
  "translatedText": "چیزی که می‌خواهم توضیح دهم چیزی است که ما آن را یک سر توجه می‌نامیم، و بعداً خواهیم دید که چگونه بلوک توجه از تعداد زیادی سر مختلف به صورت موازی اجرا می‌شود.",
  "input": "What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.42
 },
 {
  "translatedText": "باز هم، جاسازی اولیه برای هر کلمه، برخی از بردارهای با ابعاد بالا است که فقط معنای آن کلمه خاص را بدون زمینه رمزگذاری می کند.",
  "input": "Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.14,
  "end": 303.38
 },
 {
  "translatedText": "در واقع، این کاملا درست نیست.",
  "input": "Actually, that's not quite true.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 304.0,
  "end": 305.22
 },
 {
  "translatedText": "آنها همچنین موقعیت کلمه را رمزگذاری می کنند.",
  "input": "They also encode the position of the word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 305.38,
  "end": 307.64
 },
 {
  "translatedText": "روش‌های رمزگذاری موقعیت‌ها چیزهای بیشتری برای گفتن وجود دارد، اما در حال حاضر، تنها چیزی که باید بدانید این است که ورودی‌های این بردار کافی است تا به شما بگوییم کلمه چیست و کجا در متن وجود دارد.",
  "input": "There's a lot more to say way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.98,
  "end": 318.9
 },
 {
  "translatedText": "بیایید جلوتر برویم و این جاسازی ها را با حرف e نشان دهیم.",
  "input": "Let's go ahead and denote these embeddings with the letter e.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 319.5,
  "end": 321.66
 },
 {
  "translatedText": "هدف این است که مجموعه‌ای از محاسبات مجموعه جدیدی از تعبیه‌ها را تولید کند که برای مثال، آنهایی که با اسم‌ها مطابقت دارند، معنی را از صفت‌های مربوطه خود دریافت کرده باشند.",
  "input": "The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.42,
  "end": 333.42
 },
 {
  "translatedText": "و با انجام بازی یادگیری عمیق، می‌خواهیم بیشتر محاسبات مربوط به محصولات ماتریس-بردار به نظر برسند، جایی که ماتریس‌ها پر از وزن‌های قابل تنظیم هستند، چیزهایی که مدل بر اساس داده‌ها یاد می‌گیرد.",
  "input": "And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tunable weights, things that the model will learn based on data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 333.9,
  "end": 343.98
 },
 {
  "translatedText": "برای واضح بودن، من این مثال از صفت‌هایی که اسم‌ها را به‌روزرسانی می‌کنند، درست می‌کنم تا نوع رفتاری را که می‌توانید تصور کنید یک سر توجه انجام می‌دهد را نشان دهم.",
  "input": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.66,
  "end": 352.26
 },
 {
  "translatedText": "همانند بسیاری از یادگیری‌های عمیق، تجزیه رفتار واقعی بسیار سخت‌تر است، زیرا مبتنی بر بهینه‌سازی و تنظیم تعداد زیادی از پارامترها برای به حداقل رساندن برخی از عملکردهای هزینه است.",
  "input": "As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 352.86,
  "end": 361.34
 },
 {
  "translatedText": "فقط این است که وقتی ما از میان همه ماتریس‌های مختلف پر از پارامترهایی که در این فرآیند دخیل هستند قدم می‌گذاریم، فکر می‌کنم داشتن یک مثال تصوری از چیزی که می‌تواند برای کمک به ملموس‌تر ماندن آن انجام دهد، واقعا مفید است.",
  "input": "It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 361.68,
  "end": 373.22
 },
 {
  "translatedText": "برای اولین مرحله از این فرآیند، ممکن است تصور کنید هر اسم، مانند موجودی، این سوال را می پرسد، هی، آیا صفتی در مقابل من نشسته است؟",
  "input": "For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.14,
  "end": 381.96
 },
 {
  "translatedText": "و برای کلمات کرکی و آبی، به هر کدام که بتواند پاسخ دهد، بله، من یک صفت هستم و در آن موقعیت هستم.",
  "input": "And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.16,
  "end": 387.96
 },
 {
  "translatedText": "این سوال به نوعی به عنوان یک بردار دیگر، لیست دیگری از اعداد، که ما آن را پرس و جو برای این کلمه می نامیم، رمزگذاری می شود.",
  "input": "That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 388.96,
  "end": 396.1
 },
 {
  "translatedText": "این بردار پرس و جو اگرچه ابعاد بسیار کوچکتری نسبت به بردار جاسازی، مثلاً 128 دارد.",
  "input": "This query vector though has a much smaller dimension than the embedding vector, say 128.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.98,
  "end": 402.02
 },
 {
  "translatedText": "محاسبه این پرس و جو مانند گرفتن یک ماتریس خاص است که من آن را wq برچسب گذاری می کنم و آن را در جاسازی ضرب می کنم.",
  "input": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.94,
  "end": 409.78
 },
 {
  "translatedText": "با کمی فشرده کردن چیزها، بیایید آن بردار پرس و جو را به صورت q بنویسیم، و سپس هر زمان که دیدید من یک ماتریس را در کنار فلشی مانند این قرار دادم، به این معنی است که ضرب این ماتریس در بردار در ابتدای فلش به شما بردار را می دهد. انتهای پیکان",
  "input": "Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 410.96,
  "end": 424.8
 },
 {
  "translatedText": "در این حالت، این ماتریس را در همه جاسازی‌های موجود در زمینه ضرب می‌کنید و برای هر توکن یک بردار کوئری تولید می‌کنید.",
  "input": "In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 425.86,
  "end": 432.58
 },
 {
  "translatedText": "ورودی‌های این ماتریس پارامترهای مدل هستند، به این معنی که رفتار واقعی از داده‌ها آموخته می‌شود، و در عمل، آنچه این ماتریس در یک سر توجه خاص انجام می‌دهد، تجزیه و تحلیل چالش برانگیز است.",
  "input": "The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 433.74,
  "end": 443.44
 },
 {
  "translatedText": "اما به خاطر خودمان، با تصور مثالی که امیدواریم یاد بگیرد، فرض می کنیم که این ماتریس پرس و جو جاسازی اسم ها را به جهات خاصی در این فضای پرس و جو کوچکتر ترسیم می کند که به نوعی مفهوم جستجوی صفت ها در موقعیت های قبلی را رمزگذاری می کند. .",
  "input": "But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 443.9,
  "end": 458.04
 },
 {
  "translatedText": "در مورد اینکه با جاسازی های دیگر چه می کند، چه کسی می داند؟",
  "input": "As to what it does to other embeddings, who knows?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.78,
  "end": 461.44
 },
 {
  "translatedText": "شاید به طور همزمان سعی می کند با آن اهداف دیگری را محقق کند.",
  "input": "Maybe it simultaneously tries to accomplish some other goal with those.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 461.72,
  "end": 464.34
 },
 {
  "translatedText": "در حال حاضر، ما روی اسم ها تمرکز لیزری داریم.",
  "input": "Right now, we're laser focused on the nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.54,
  "end": 467.16
 },
 {
  "translatedText": "در عین حال، با این ماتریس دومی به نام ماتریس کلیدی همراه است که آن را نیز در هر یک از جاسازی‌ها ضرب می‌کنید.",
  "input": "At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 467.28,
  "end": 474.62
 },
 {
  "translatedText": "این یک دنباله دوم از بردارها را تولید می کند که ما آنها را کلید می نامیم.",
  "input": "This produces a second sequence of vectors that we call the keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 475.28,
  "end": 478.5
 },
 {
  "translatedText": "از نظر مفهومی، شما می‌خواهید کلیدها را به‌عنوان پاسخ‌دهی بالقوه به پرسش‌ها در نظر بگیرید.",
  "input": "Conceptually, you want to think of the keys as potentially answering the queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 479.42,
  "end": 483.14
 },
 {
  "translatedText": "این ماتریس کلید همچنین پر از پارامترهای قابل تنظیم است و درست مانند ماتریس پرس و جو، بردارهای جاسازی شده را به همان فضای ابعادی کوچکتر نگاشت می کند.",
  "input": "This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.84,
  "end": 491.4
 },
 {
  "translatedText": "شما فکر می کنید که کلیدها هر زمان که دقیقاً با یکدیگر همسو می شوند، با پرس و جوها مطابقت دارند.",
  "input": "You think of the keys as matching the queries whenever they closely align with each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.2,
  "end": 497.02
 },
 {
  "translatedText": "در مثال ما، تصور می‌کنید که ماتریس کلید صفت‌هایی مانند کرکی و آبی را به بردارهایی نگاشت می‌کند که دقیقاً با پرس و جو تولید شده توسط کلمه مخلوق همسو هستند.",
  "input": "In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.46,
  "end": 506.74
 },
 {
  "translatedText": "برای اندازه‌گیری میزان مطابقت هر کلید با هر پرس و جو، یک محصول نقطه‌ای را بین هر جفت کلید پرس و جو ممکن محاسبه می‌کنید.",
  "input": "To measure how well each key matches each query, you compute a dot product between each possible key-query pair.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.2,
  "end": 514.0
 },
 {
  "translatedText": "من دوست دارم شبکه‌ای پر از یک دسته نقطه را تجسم کنم، جایی که نقاط بزرگ‌تر با محصولات نقطه‌ای بزرگ‌تر، مکان‌هایی که کلیدها و پرس و جوها در آن تراز هستند، مطابقت دارند.",
  "input": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.48,
  "end": 522.56
 },
 {
  "translatedText": "برای مثال اسم صفت ما، کمی بیشتر شبیه این به نظر می رسد، جایی که اگر کلیدهای تولید شده توسط کرکی و آبی واقعاً با جستار تولید شده توسط مخلوق مطابقت داشته باشند، آنگاه محصولات نقطه ای در این دو نقطه اعداد مثبت بزرگی خواهند بود.",
  "input": "For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 523.28,
  "end": 538.32
 },
 {
  "translatedText": "در زبان انگلیسی، افراد یادگیری ماشینی می‌گویند که این به معنای تعبیه رنگ‌های کرکی و آبی برای جاسازی موجودات است.",
  "input": "In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.1,
  "end": 545.42
 },
 {
  "translatedText": "بر خلاف حاصل ضرب نقطه ای بین کلید برای یک کلمه دیگر مانند the و پرس و جو برای مخلوق مقداری کوچک یا منفی است که منعکس کننده موارد غیر مرتبط با یکدیگر است.",
  "input": "By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.04,
  "end": 556.6
 },
 {
  "translatedText": "بنابراین ما این شبکه‌ای از مقادیر را داریم که می‌تواند هر عدد واقعی از بی‌نهایت منفی تا بی‌نهایت باشد، که به ما امتیازی برای ارتباط هر کلمه با به‌روزرسانی معنای هر کلمه دیگر می‌دهد.",
  "input": "So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 557.7,
  "end": 568.48
 },
 {
  "translatedText": "روشی که می‌خواهیم از این امتیازها استفاده کنیم، این است که یک جمع وزنی معین در امتداد هر ستون، وزن‌دهی شده بر اساس ربط، در نظر بگیریم.",
  "input": "The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.2,
  "end": 575.78
 },
 {
  "translatedText": "بنابراین به‌جای اینکه مقادیری از بی‌نهایت منفی تا بی‌نهایت داشته باشیم، چیزی که می‌خواهیم این است که اعداد در این ستون‌ها بین 0 و 1 باشند و برای هر ستون تا 1 جمع شود، انگار که یک توزیع احتمال هستند.",
  "input": "So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.52,
  "end": 588.18
 },
 {
  "translatedText": "اگر از فصل آخر وارد می شوید، می دانید که باید چه کار کنیم.",
  "input": "If you're coming in from the last chapter, you know what we need to do then.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 589.28,
  "end": 592.22
 },
 {
  "translatedText": "ما یک Softmax را در امتداد هر یک از این ستون ها محاسبه می کنیم تا مقادیر را عادی کنیم.",
  "input": "We compute a softmax along each one of these columns to normalize the values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.62,
  "end": 597.3
 },
 {
  "translatedText": "در تصویر ما، پس از اعمال softmax به همه ستون‌ها، شبکه را با این مقادیر نرمال شده پر می‌کنیم.",
  "input": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 600.06,
  "end": 605.86
 },
 {
  "translatedText": "در این مرحله شما مطمئن هستید که در مورد هر ستون به عنوان وزن دادن با توجه به ارتباط کلمه سمت چپ با مقدار مربوطه در بالا فکر کنید.",
  "input": "At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 606.78,
  "end": 614.58
 },
 {
  "translatedText": "ما این شبکه را الگوی توجه می نامیم.",
  "input": "We call this grid an attention pattern.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.08,
  "end": 616.84
 },
 {
  "translatedText": "حالا اگر به کاغذ ترانسفورماتور اصلی نگاه کنید، یک روش واقعا فشرده وجود دارد که آنها همه اینها را یادداشت می کنند.",
  "input": "Now if you look at the original transformer paper, there's a really compact way that they write this all down.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 618.08,
  "end": 622.82
 },
 {
  "translatedText": "در اینجا متغیرهای q و k به ترتیب آرایه‌های کامل بردارهای پرس و جو و کلید را نشان می‌دهند، آن بردارهای کوچکی که با ضرب جاسازی‌ها در پرس و جو و ماتریس‌های کلید به دست می‌آورید.",
  "input": "Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 623.88,
  "end": 634.64
 },
 {
  "translatedText": "این عبارت در صورت‌حساب یک روش بسیار فشرده برای نشان دادن شبکه تمام محصولات نقطه‌ای ممکن بین جفت کلید و پرس و جو است.",
  "input": "This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.16,
  "end": 643.02
 },
 {
  "translatedText": "یک جزئیات فنی کوچک که من به آن اشاره نکردم این است که برای ثبات عددی، تقسیم همه این مقادیر بر جذر بعد در فضای پرس و جو کلیدی مفید است.",
  "input": "A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.0,
  "end": 653.96
 },
 {
  "translatedText": "سپس این softmax که در اطراف عبارت کامل پیچیده شده است به این معنی است که ستون به ستون اعمال می شود.",
  "input": "Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.48,
  "end": 660.8
 },
 {
  "translatedText": "در مورد آن عبارت v، ما در یک ثانیه در مورد آن صحبت خواهیم کرد.",
  "input": "As to that v term, we'll talk about it in just a second.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 661.64,
  "end": 664.7
 },
 {
  "translatedText": "قبل از آن، یک جزئیات فنی دیگر وجود دارد که تا کنون از آن صرفنظر کرده ام.",
  "input": "Before that, there's one other technical detail that so far I've skipped.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.02,
  "end": 668.46
 },
 {
  "translatedText": "در طول فرآیند آموزش، وقتی این مدل را بر روی یک مثال متنی خاص اجرا می‌کنید، و همه وزن‌ها کمی تنظیم و تنظیم می‌شوند تا بر اساس احتمال بالایی که به کلمه بعدی واقعی در متن اختصاص می‌دهد، آن را پاداش یا تنبیه کنید. معلوم می‌شود که کل فرآیند آموزش را بسیار کارآمدتر می‌کند اگر همزمان بخواهید هر توکن بعدی ممکن را پس از هر دنباله اولیه نشانه‌ها در این قسمت پیش‌بینی کند.",
  "input": "During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 669.04,
  "end": 691.56
 },
 {
  "translatedText": "برای مثال، با عبارتی که روی آن تمرکز کرده‌ایم، ممکن است پیش‌بینی شود که چه کلماتی پس از مخلوق و چه کلماتی به دنبال آن هستند.",
  "input": "For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 691.94,
  "end": 699.1
 },
 {
  "translatedText": "این واقعاً خوب است، زیرا به این معنی است که آنچه در غیر این صورت یک نمونه آموزشی واحد خواهد بود، به همان اندازه عمل می کند.",
  "input": "This is really nice, because it means what would otherwise be a single training example effectively acts as many.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 699.94,
  "end": 705.56
 },
 {
  "translatedText": "برای اهداف الگوی توجه ما، به این معنی است که شما هرگز نمی خواهید اجازه دهید کلمات بعدی بر کلمات قبلی تأثیر بگذارند، زیرا در غیر این صورت آنها می توانند به نوعی پاسخ چیزهای بعدی را بدهند.",
  "input": "For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 706.1,
  "end": 716.04
 },
 {
  "translatedText": "این به این معنی است که ما می‌خواهیم همه این نقاط در اینجا، آنهایی که نشان‌دهنده نشانه‌های بعدی هستند که بر موارد قبلی تأثیر می‌گذارند، به نحوی مجبور به صفر شوند.",
  "input": "What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 716.56,
  "end": 724.6
 },
 {
  "translatedText": "ساده ترین کاری که ممکن است انجام دهید این است که آنها را برابر با صفر قرار دهید، اما اگر این کار را انجام دهید که ستون ها دیگر به یک جمع نمی شوند، عادی نمی شوند.",
  "input": "The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 725.92,
  "end": 732.42
 },
 {
  "translatedText": "بنابراین، در عوض، یک راه معمول برای انجام این کار این است که قبل از اعمال softmax، همه آن ورودی‌ها را بی‌نهایت منفی تنظیم کنید.",
  "input": "So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 733.12,
  "end": 739.02
 },
 {
  "translatedText": "اگر این کار را انجام دهید، پس از اعمال softmax، همه آن ها به صفر تبدیل می شوند، اما ستون ها عادی می مانند.",
  "input": "If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 739.68,
  "end": 745.18
 },
 {
  "translatedText": "به این فرآیند ماسکینگ می گویند.",
  "input": "This process is called masking.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.0,
  "end": 747.54
 },
 {
  "translatedText": "نسخه‌هایی از توجه وجود دارد که شما آن را به کار نمی‌برید، اما در مثال GPT ما، حتی اگر این موضوع در طول مرحله آموزش بیشتر از زمانی که مثلاً آن را به‌عنوان یک ربات چت یا چیزی شبیه به آن اجرا می‌کنید، مرتبط است، همیشه اعمال می‌کنید. این پوشش برای جلوگیری از تأثیرگذاری توکن‌های بعدی بر روی نشانه‌های قبلی است.",
  "input": "There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.54,
  "end": 761.46
 },
 {
  "translatedText": "واقعیت دیگری که در مورد این الگوی توجه ارزش تأمل دارد این است که چگونه اندازه آن با مربع اندازه زمینه برابر است.",
  "input": "Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.48,
  "end": 769.5
 },
 {
  "translatedText": "بنابراین به همین دلیل است که اندازه زمینه می‌تواند یک گلوگاه واقعا بزرگ برای مدل‌های زبان بزرگ باشد و بزرگ‌کردن آن بی‌اهمیت است.",
  "input": "So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 769.9,
  "end": 775.62
 },
 {
  "translatedText": "همانطور که تصور می کنید، با انگیزه میل به پنجره های زمینه بزرگتر و بزرگتر، در سالهای اخیر تغییراتی در مکانیسم توجه با هدف افزایش مقیاس پذیری زمینه مشاهده شده است، اما در اینجا، من و شما بر روی اصول تمرکز می کنیم.",
  "input": "As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.3,
  "end": 788.32
 },
 {
  "translatedText": "خوب، عالی است، محاسبه این الگو به مدل اجازه می دهد تا بفهمد کدام کلمات با کدام کلمات دیگر مرتبط هستند.",
  "input": "Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 790.56,
  "end": 795.48
 },
 {
  "translatedText": "اکنون باید در واقع جاسازی‌ها را به‌روزرسانی کنید و به کلمات اجازه دهید اطلاعات را به هر کلمه دیگری که مرتبط هستند منتقل کنند.",
  "input": "Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.02,
  "end": 802.8
 },
 {
  "translatedText": "به عنوان مثال، شما می خواهید که تعبیه Fluffy به نحوی باعث تغییر در Creature شود که آن را به قسمت دیگری از این فضای تعبیه شده 12000 بعدی منتقل کند که به طور خاص یک موجود Fluffy را رمزگذاری می کند.",
  "input": "For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.8,
  "end": 814.52
 },
 {
  "translatedText": "کاری که من در اینجا انجام می‌دهم این است که ابتدا ساده‌ترین روشی را که می‌توانید این کار را انجام دهید، به شما نشان می‌دهم، اگرچه روشی جزئی وجود دارد که این روش در چارچوب توجه چند جانبه اصلاح می‌شود.",
  "input": "What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 815.46,
  "end": 823.46
 },
 {
  "translatedText": "ساده‌ترین راه استفاده از ماتریس سوم است، چیزی که ماتریس ارزش می‌نامیم، که آن را در جاسازی کلمه اول ضرب می‌کنید، برای مثال Fluffy.",
  "input": "This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.08,
  "end": 832.44
 },
 {
  "translatedText": "نتیجه این چیزی است که شما آن را بردار مقدار می نامید، و این چیزی است که به تعبیه کلمه دوم اضافه می کنید، در این مورد چیزی را به جاسازی مخلوق اضافه می کنید.",
  "input": "The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 833.3,
  "end": 841.92
 },
 {
  "translatedText": "بنابراین این بردار مقدار در همان فضای بسیار بابعد جاسازی ها زندگی می کند.",
  "input": "So this value vector lives in the same very high-dimensional space as the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 842.6,
  "end": 847.0
 },
 {
  "translatedText": "وقتی این ماتریس مقدار را در تعبیه یک کلمه ضرب می کنید، ممکن است فکر کنید که می گوید، اگر این کلمه با تعدیل معنای چیز دیگری مرتبط است، دقیقاً چه چیزی باید به جاسازی آن چیز دیگر اضافه شود تا منعکس شود. این؟",
  "input": "When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 847.46,
  "end": 861.16
 },
 {
  "translatedText": "با نگاهی به نمودار خود، بیایید همه کلیدها و پرس و جوها را کنار بگذاریم، زیرا پس از محاسبه الگوی توجه که با آن‌ها تمام شد، این ماتریس مقدار را گرفته و در هر یک از آن جاسازی‌ها ضرب می‌کنیم. برای تولید دنباله ای از بردارهای ارزش.",
  "input": "Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 862.14,
  "end": 876.06
 },
 {
  "translatedText": "ممکن است فکر کنید این بردارهای مقدار به نوعی با کلیدهای مربوطه مرتبط هستند.",
  "input": "You might think of these value vectors as being kind of associated with the corresponding keys.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 877.12,
  "end": 881.12
 },
 {
  "translatedText": "برای هر ستون در این نمودار، هر یک از بردارهای مقدار را در وزن مربوطه در آن ستون ضرب می کنید.",
  "input": "For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 882.32,
  "end": 889.24
 },
 {
  "translatedText": "به عنوان مثال، در اینجا، تحت تعبیه Creature، می‌توانید نسبت‌های زیادی از بردارهای مقدار را برای Fluffy و Blue اضافه کنید، در حالی که تمام بردارهای ارزش دیگر صفر می‌شوند یا حداقل تقریباً صفر می‌شوند.",
  "input": "For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 890.08,
  "end": 901.56
 },
 {
  "translatedText": "و سپس در نهایت، روشی برای به روز رسانی واقعی جاسازی مرتبط با این ستون، که قبلاً برخی از معنای بدون متن Creature را رمزگذاری می کرد، همه این مقادیر تغییر مقیاس شده را در ستون با هم جمع می کنید، و تغییری را ایجاد می کنید که می خواهید اضافه کنید، که I&#39; ll delta-e را برچسب گذاری کنید، و سپس آن را به جاسازی اصلی اضافه کنید.",
  "input": "And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 902.12,
  "end": 919.26
 },
 {
  "translatedText": "امیدواریم آنچه که در نتیجه حاصل می‌شود، یک بردار دقیق‌تر باشد که معنای غنی‌تر از نظر بافتی را رمزگذاری می‌کند، مانند موجودی آبی کرکی.",
  "input": "Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 919.68,
  "end": 926.5
 },
 {
  "translatedText": "و البته شما این کار را فقط برای یک جاسازی انجام نمی‌دهید، بلکه جمع وزنی یکسانی را در تمام ستون‌های این تصویر اعمال می‌کنید، دنباله‌ای از تغییرات را ایجاد می‌کنید، همه آن تغییرات را به جاسازی‌های مربوطه اضافه می‌کنید، و یک دنباله کامل از تغییرات را ایجاد می‌کنید. تعبیه‌های دقیق‌تری که از بلوک توجه بیرون می‌آیند.",
  "input": "And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 927.38,
  "end": 943.46
 },
 {
  "translatedText": "با زوم کردن، کل این فرآیند همان چیزی است که شما به عنوان یک سر توجه توصیف می کنید.",
  "input": "Zooming out, this whole process is what you would describe as a single head of attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.86,
  "end": 949.1
 },
 {
  "translatedText": "همانطور که تا کنون موارد را توضیح داده‌ام، این فرآیند توسط سه ماتریس مجزا پارامتربندی می‌شود که همگی با پارامترهای قابل تنظیم، کلید، پرس و جو و مقدار پر شده‌اند.",
  "input": "As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 949.6,
  "end": 958.94
 },
 {
  "translatedText": "می‌خواهم لحظه‌ای وقت بگذارم و کاری را که در فصل آخر شروع کردیم، با حفظ امتیاز که در آن تعداد کل پارامترهای مدل را با استفاده از اعداد GPT-3 می‌شماریم، ادامه دهم.",
  "input": "I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 959.5,
  "end": 968.04
 },
 {
  "translatedText": "این ماتریس های کلید و پرس و جو هر کدام دارای 12288 ستون هستند که با بعد تعبیه شده مطابقت دارند و 128 ردیف که با بعد فضای پرس و جوی کلید کوچکتر مطابقت دارند.",
  "input": "These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 969.3,
  "end": 979.6
 },
 {
  "translatedText": "این به ما 1.5 میلیون یا بیشتر پارامتر اضافی برای هر یک می دهد.",
  "input": "This gives us an additional 1.5 million or so parameters for each one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 980.26,
  "end": 984.22
 },
 {
  "translatedText": "اگر به آن ماتریس مقدار برعکس نگاه کنید، روشی که من تا اینجا توضیح دادم نشان می دهد که این ماتریس مربعی است که دارای 12288 ستون و 12288 سطر است، زیرا ورودی و خروجی هر دو در این فضای جاسازی بسیار بزرگ زندگی می کنند.",
  "input": "If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.86,
  "end": 1000.92
 },
 {
  "translatedText": "اگر درست باشد، حدود 150 میلیون پارامتر اضافه شده است.",
  "input": "If true, that would mean about 150 million added parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1001.5,
  "end": 1005.14
 },
 {
  "translatedText": "و برای واضح بودن، شما می توانید این کار را انجام دهید.",
  "input": "And to be clear, you could do that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.66,
  "end": 1007.3
 },
 {
  "translatedText": "شما می توانید مقادیر بیشتری پارامتر را به نقشه مقدار اختصاص دهید تا کلید و پرس و جو.",
  "input": "You could devote orders of magnitude more parameters to the value map than to the key and query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.42,
  "end": 1011.74
 },
 {
  "translatedText": "اما در عمل، بسیار کارآمدتر است اگر در عوض آن را طوری بسازید که تعداد پارامترهای اختصاص داده شده به این نقشه مقدار با عدد اختصاص داده شده به کلید و پرس و جو یکسان باشد.",
  "input": "But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.06,
  "end": 1020.76
 },
 {
  "translatedText": "این امر به ویژه در تنظیم اجرای موازی سرهای توجه متعدد مرتبط است.",
  "input": "This is especially relevant in the setting of running multiple attention heads in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1021.46,
  "end": 1025.16
 },
 {
  "translatedText": "شکلی که به نظر می رسد این است که نقشه ارزش به عنوان حاصل ضرب دو ماتریس کوچکتر در نظر گرفته می شود.",
  "input": "The way this looks is that the value map is factored as a product of two smaller matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.24,
  "end": 1030.1
 },
 {
  "translatedText": "از نظر مفهومی، من همچنان شما را تشویق می‌کنم که به نقشه خطی کلی فکر کنید، نقشه‌ای با ورودی‌ها و خروجی‌ها، هر دو در این فضای جاسازی بزرگ‌تر، برای مثال تعبیه رنگ آبی را به این جهت آبی که می‌توانید به اسم‌ها اضافه کنید.",
  "input": "Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1043.8
 },
 {
  "translatedText": "فقط این است که تعداد ردیف‌های کمتری است، معمولاً به اندازه فضای جستجوی کلید.",
  "input": "It's just that it's a smaller number of rows, typically the same size as the key query space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.04,
  "end": 1052.76
 },
 {
  "translatedText": "این به این معنی است که می توانید آن را به عنوان نگاشت بردارهای جاسازی بزرگ در فضای بسیار کوچکتر در نظر بگیرید.",
  "input": "What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1053.1,
  "end": 1058.44
 },
 {
  "translatedText": "این نامگذاری مرسوم نیست، اما من آن را ماتریس مقدار پایین می نامم.",
  "input": "This is not the conventional naming, but I'm going to call this the value down matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1059.04,
  "end": 1062.7
 },
 {
  "translatedText": "ماتریس دوم از این فضای کوچکتر به فضای تعبیه شده برمی گردد و بردارهایی را تولید می کند که برای به روز رسانی واقعی استفاده می کنید.",
  "input": "The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1063.4,
  "end": 1070.58
 },
 {
  "translatedText": "من این یکی را ماتریس ارزش بالا می‌نامم که باز هم متعارف نیست.",
  "input": "I'm going to call this one the value up matrix, which again is not conventional.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1071.0,
  "end": 1074.74
 },
 {
  "translatedText": "روشی که این نوشته را در اکثر مقالات مشاهده می کنید کمی متفاوت به نظر می رسد.",
  "input": "The way that you would see this written in most papers looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.16,
  "end": 1078.08
 },
 {
  "translatedText": "یک دقیقه دیگر در مورد آن صحبت خواهم کرد.",
  "input": "I'll talk about it in a minute.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1078.38,
  "end": 1079.52
 },
 {
  "translatedText": "به نظر من، این امر باعث می شود همه چیز از نظر مفهومی گیج کننده تر شود.",
  "input": "In my opinion, it tends to make things a little more conceptually confusing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.7,
  "end": 1082.54
 },
 {
  "translatedText": "برای استفاده از اصطلاحات جبر خطی در اینجا، کاری که ما اساساً انجام می دهیم این است که نقشه ارزش کلی را به یک تبدیل رتبه پایین محدود می کنیم.",
  "input": "To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.26,
  "end": 1090.34
 },
 {
  "translatedText": "با برگشت به تعداد پارامترها، هر چهار این ماتریس دارای اندازه یکسانی هستند و با جمع کردن همه آنها، حدود 6.3 میلیون پارامتر برای یک سر توجه به دست می‌آید.",
  "input": "Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1091.42,
  "end": 1100.78
 },
 {
  "translatedText": "به عنوان یک یادداشت جانبی سریع، برای کمی دقیق تر، همه چیزهایی که تا کنون شرح داده شده است چیزی است که مردم آن را سر توجه به خود می نامند، تا آن را از تنوعی که در مدل های دیگر وجود دارد که به آن توجه متقاطع می گویند متمایز شود.",
  "input": "As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.04,
  "end": 1111.5
 },
 {
  "translatedText": "این به مثال GPT ما مربوط نیست، اما اگر کنجکاو هستید، توجه متقابل شامل مدل‌هایی می‌شود که دو نوع داده متمایز را پردازش می‌کنند، مانند متن در یک زبان و متن در زبان دیگر که بخشی از نسل جاری ترجمه است. یا شاید ورودی صوتی گفتار و رونویسی در حال انجام.",
  "input": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1112.3,
  "end": 1129.24
 },
 {
  "translatedText": "یک سر متقاطع تقریباً یکسان به نظر می رسد.",
  "input": "A cross-attention head looks almost identical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1130.4,
  "end": 1132.7
 },
 {
  "translatedText": "تنها تفاوت این است که نقشه های کلید و پرس و جو بر روی مجموعه داده های مختلف عمل می کنند.",
  "input": "The only difference is that the key and query maps act on different data sets.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1132.98,
  "end": 1137.4
 },
 {
  "translatedText": "برای مثال، در مدلی که ترجمه انجام می دهد، کلیدها ممکن است از یک زبان بیایند، در حالی که پرس و جوها از زبان دیگری می آیند، و الگوی توجه می تواند توصیف کند که کدام کلمه از یک زبان با کدام کلمه در زبان دیگر مطابقت دارد.",
  "input": "In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.84,
  "end": 1149.66
 },
 {
  "translatedText": "و در این تنظیمات معمولاً هیچ پوششی وجود نخواهد داشت، زیرا واقعاً هیچ مفهومی از نشانه‌های بعدی وجود ندارد که بر موارد قبلی تأثیر بگذارند.",
  "input": "And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1150.34,
  "end": 1156.34
 },
 {
  "translatedText": "با این حال، با تمرکز بر توجه خود، اگر تا اینجا همه چیز را درک می کردید، و اگر قرار بود در اینجا متوقف شوید، با جوهره آنچه که توجه واقعاً وجود دارد، از بین می رفتید.",
  "input": "Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1157.18,
  "end": 1165.18
 },
 {
  "translatedText": "تنها چیزی که واقعاً برای ما باقی می‌ماند این است که حسی را که در آن این کار را در زمان‌های مختلف انجام می‌دهید، بیان کنیم.",
  "input": "All that's really left to us is to lay out the sense in which you do this many many different times.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1165.76,
  "end": 1171.44
 },
 {
  "translatedText": "در مثال اصلی خود، ما روی صفت‌هایی که اسم‌ها را به‌روز می‌کنند، تمرکز کردیم، اما البته راه‌های مختلفی وجود دارد که بافت می‌تواند بر معنای یک کلمه تأثیر بگذارد.",
  "input": "In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.1,
  "end": 1179.8
 },
 {
  "translatedText": "اگر کلماتی که آنها تصادف کردند، مقدم بر کلمه ماشین باشد، برای شکل و ساختار آن ماشین تاثیر دارد.",
  "input": "If the words they crashed the preceded the word car, it has implications for the shape and structure of that car.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1180.36,
  "end": 1186.52
 },
 {
  "translatedText": "و بسیاری از تداعی ها ممکن است دستوری کمتری داشته باشند.",
  "input": "And a lot of associations might be less grammatical.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.2,
  "end": 1189.28
 },
 {
  "translatedText": "اگر کلمه جادوگر جایی در همان قسمت هری باشد، نشان می‌دهد که این ممکن است به هری پاتر اشاره داشته باشد، در حالی که اگر به جای آن کلمات ملکه، ساسکس و ویلیام در آن قسمت وجود داشته باشند، شاید جاسازی هری باید به‌روزرسانی شود. رجوع به شاهزاده شود.",
  "input": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.76,
  "end": 1204.44
 },
 {
  "translatedText": "برای هر نوع متفاوتی از به‌روزرسانی زمینه‌ای که ممکن است تصور کنید، پارامترهای این ماتریس‌های کلید و پرس‌وجو برای جلب الگوهای مختلف توجه متفاوت خواهند بود، و پارامترهای نقشه ارزش ما بر اساس آنچه که باید به جاسازی‌ها اضافه شود متفاوت خواهد بود.",
  "input": "For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1205.04,
  "end": 1219.14
 },
 {
  "translatedText": "و باز هم، در عمل، تفسیر رفتار واقعی این نقشه‌ها بسیار دشوارتر است، جایی که وزن‌ها برای انجام هر کاری که مدل به آن‌ها نیاز دارد انجام می‌دهند تا به بهترین شکل به هدف خود یعنی پیش‌بینی نشانه بعدی دست یابد.",
  "input": "And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1219.98,
  "end": 1230.14
 },
 {
  "translatedText": "همانطور که قبلاً گفتم، هر چیزی که توضیح دادیم یک سر توجه است، و یک بلوک توجه کامل در داخل یک ترانسفورماتور شامل چیزی است که به آن توجه چند سر می گویند، که در آن شما تعداد زیادی از این عملیات را به صورت موازی اجرا می کنید، که هر کدام یک جستجوی کلیدی مجزای خود را دارند. و نقشه های ارزشی",
  "input": "As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1231.4,
  "end": 1245.92
 },
 {
  "translatedText": "برای مثال GPT-3 از 96 سر توجه در داخل هر بلوک استفاده می کند.",
  "input": "GPT-3 for example uses 96 attention heads inside each block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1247.42,
  "end": 1251.7
 },
 {
  "translatedText": "با توجه به اینکه هر یک از آنها کمی گیج کننده است، مطمئناً باید در ذهن خود نگه دارید.",
  "input": "Considering that each one is already a bit confusing, it's certainly a lot to hold in your head.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.02,
  "end": 1256.46
 },
 {
  "translatedText": "فقط برای اینکه همه چیز را خیلی واضح بیان کنیم، این بدان معناست که شما 96 کلید متمایز و ماتریس پرس و جو دارید که 96 الگوی توجه متمایز را تولید می کنند.",
  "input": "Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1256.76,
  "end": 1265.0
 },
 {
  "translatedText": "سپس هر هد دارای ماتریس های ارزش مجزای خاص خود است که برای تولید 96 دنباله بردار ارزش استفاده می شود.",
  "input": "Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1265.44,
  "end": 1272.18
 },
 {
  "translatedText": "همه اینها با استفاده از الگوهای توجه مربوطه به عنوان وزن با هم جمع می شوند.",
  "input": "These are all added together using the corresponding attention patterns as weights.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1272.46,
  "end": 1276.68
 },
 {
  "translatedText": "این بدان معناست که برای هر موقعیت در زمینه، هر نشانه، هر یک از این هدها یک تغییر پیشنهادی ایجاد می کند تا به جاسازی در آن موقعیت اضافه شود.",
  "input": "What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1277.48,
  "end": 1287.02
 },
 {
  "translatedText": "بنابراین کاری که انجام می دهید این است که تمام تغییرات پیشنهادی را با هم جمع می کنید، یکی برای هر سر، و نتیجه را به جاسازی اصلی آن موقعیت اضافه می کنید.",
  "input": "So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1287.66,
  "end": 1295.48
 },
 {
  "translatedText": "کل این مجموع در اینجا یک تکه از آنچه از این بلوک توجه چند سر به دست می آید، یک تکه از آن جاسازی های تصفیه شده است که از انتهای دیگر آن بیرون می آید.",
  "input": "This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1296.66,
  "end": 1307.46
 },
 {
  "translatedText": "باز هم، این موضوع جای تامل زیادی دارد، بنابراین اگر کمی طول کشید تا در آن غرق شوید اصلا نگران نباشید.",
  "input": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.32,
  "end": 1312.14
 },
 {
  "translatedText": "ایده کلی این است که با اجرای موازی سرهای متمایز، به مدل این ظرفیت را می‌دهید که روش‌های متمایز زیادی را بیاموزد که متن معنا را تغییر می‌دهد.",
  "input": "The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1312.38,
  "end": 1321.82
 },
 {
  "translatedText": "با بالا بردن آمار تعداد پارامترها با 96 سر، که هر کدام شامل تغییرات خاص خود از این چهار ماتریس است، هر بلوک از توجه چند سر به حدود 600 میلیون پارامتر ختم می شود.",
  "input": "Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1323.7,
  "end": 1335.08
 },
 {
  "translatedText": "یک چیز کمی آزاردهنده اضافه شده است که من واقعاً باید برای هر یک از شما که به خواندن بیشتر در مورد ترانسفورماتورها ادامه می دهید اشاره کنم.",
  "input": "There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.42,
  "end": 1341.8
 },
 {
  "translatedText": "به یاد دارید که چگونه گفتم که نقشه ارزش در این دو ماتریس مجزا قرار می گیرد که من آنها را به عنوان ماتریس های مقدار پایین و ارزش بالا برچسب گذاری کردم.",
  "input": "You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1342.08,
  "end": 1349.44
 },
 {
  "translatedText": "روشی که من چیزها را چارچوب بندی کردم نشان می دهد که شما این جفت ماتریس را در داخل هر سر توجه ببینید و کاملاً می توانید آن را از این طریق پیاده سازی کنید.",
  "input": "The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1349.96,
  "end": 1358.44
 },
 {
  "translatedText": "این یک طرح معتبر خواهد بود.",
  "input": "That would be a valid design.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1358.64,
  "end": 1359.92
 },
 {
  "translatedText": "اما نحوه ای که شما این را می بینید که در مقالات نوشته شده و نحوه اجرای آن در عمل کمی متفاوت به نظر می رسد.",
  "input": "But the way that you see this written in papers and the way that it's implemented in practice looks a little different.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.26,
  "end": 1364.92
 },
 {
  "translatedText": "همه این ماتریس‌های افزایش ارزش برای هر هد در یک ماتریس غول‌پیکر که ماتریس خروجی نامیده می‌شود، منگنه شده به نظر می‌رسند که با کل بلوک توجه چند سر مرتبط است.",
  "input": "All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1365.34,
  "end": 1376.38
 },
 {
  "translatedText": "و وقتی می‌بینید که مردم به ماتریس ارزش برای یک سر توجه معین اشاره می‌کنند، معمولاً فقط به این مرحله اول اشاره می‌کنند، مرحله‌ای که من آن را به عنوان کاهش ارزش در فضای کوچک‌تر برچسب‌گذاری می‌کردم.",
  "input": "And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1376.82,
  "end": 1387.14
 },
 {
  "translatedText": "برای کنجکاوهای شما، یادداشتی روی صفحه در مورد آن گذاشتم.",
  "input": "For the curious among you, I've left an on-screen note about it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1388.34,
  "end": 1391.04
 },
 {
  "translatedText": "این یکی از آن جزئیاتی است که خطر منحرف شدن توجه از نکات مفهومی اصلی را دارد، اما من می‌خواهم آن را بیان کنم تا اگر در منابع دیگر در این مورد مطالعه کردید، بدانید.",
  "input": "It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1391.26,
  "end": 1398.54
 },
 {
  "translatedText": "با کنار گذاشتن تمام نکات ظریف فنی، در پیش‌نمایش فصل آخر دیدیم که چگونه داده‌هایی که از طریق یک ترانسفورماتور جریان می‌یابند، فقط از طریق یک بلوک توجه جریان نمی‌یابند.",
  "input": "Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1399.24,
  "end": 1408.04
 },
 {
  "translatedText": "برای یک چیز، آن را از طریق سایر عملیات به نام پرسپترون چند لایه نیز انجام می دهد.",
  "input": "For one thing, it also goes through these other operations called multi-layer perceptrons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1408.64,
  "end": 1412.7
 },
 {
  "translatedText": "در فصل بعدی بیشتر در مورد آنها صحبت خواهیم کرد.",
  "input": "We'll talk more about those in the next chapter.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1413.12,
  "end": 1414.88
 },
 {
  "translatedText": "و سپس بارها و بارها از طریق بسیاری از کپی های بسیاری از هر دوی این عملیات می گذرد.",
  "input": "And then it repeatedly goes through many many copies of both of these operations.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1415.18,
  "end": 1419.32
 },
 {
  "translatedText": "این به این معنی است که پس از اینکه یک کلمه معین بخشی از بافت خود را در بر می گیرد، شانس بیشتری برای این تعبیه ظریف تر وجود دارد که تحت تأثیر محیط ظریف تر خود قرار گیرد.",
  "input": "What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1419.98,
  "end": 1430.04
 },
 {
  "translatedText": "هر چه در شبکه جلوتر می روید و هر جاسازی از همه جاسازی های دیگر معنی بیشتری پیدا می کند، که خود آنها بیشتر و بیشتر ظریف می شوند، امید این است که ظرفیت رمزگذاری ایده های سطح بالاتر و انتزاعی تر در مورد یک موضوع خاص وجود داشته باشد. ورودی فراتر از توصیفگرها و ساختار دستوری.",
  "input": "The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1430.94,
  "end": 1447.32
 },
 {
  "translatedText": "چیزهایی مانند احساس و لحن و اینکه آیا این یک شعر است و چه حقایق علمی زیربنایی مربوط به قطعه و مواردی از این قبیل است.",
  "input": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1447.88,
  "end": 1455.13
 },
 {
  "translatedText": "اگر یک بار دیگر به امتیازدهی خود برگردیم، GPT-3 شامل 96 لایه مجزا است، بنابراین تعداد کل پارامترهای پرس و جو و ارزش کلیدی در 96 مورد دیگر ضرب می شود که مجموع کل را به کمتر از 58 میلیارد پارامتر متمایز می رساند که به همه پارامترها اختصاص داده شده است. سرهای توجه",
  "input": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1456.7,
  "end": 1474.5
 },
 {
  "translatedText": "این بسیار زیاد است، اما تنها یک سوم از 175 میلیاردی است که در کل شبکه وجود دارد.",
  "input": "That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1474.98,
  "end": 1480.94
 },
 {
  "translatedText": "بنابراین حتی اگر توجه همه توجه را به خود جلب کند، اکثر پارامترها از بلوک هایی می آیند که در بین این مراحل قرار دارند.",
  "input": "So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1481.52,
  "end": 1488.14
 },
 {
  "translatedText": "در فصل بعدی، من و شما بیشتر در مورد آن بلوک های دیگر و همچنین در مورد روند آموزش بیشتر صحبت خواهیم کرد.",
  "input": "In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.56,
  "end": 1493.56
 },
 {
  "translatedText": "بخش بزرگی از داستان موفقیت مکانیسم توجه، رفتار خاصی نیست که آن را قادر می سازد، بلکه این واقعیت است که بسیار موازی پذیر است، به این معنی که شما می توانید تعداد زیادی محاسبات را در مدت زمان کوتاهی با استفاده از GPU انجام دهید. .",
  "input": "A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.12,
  "end": 1508.38
 },
 {
  "translatedText": "با توجه به اینکه یکی از درس‌های مهم در مورد یادگیری عمیق در یکی دو دهه اخیر این بوده است که مقیاس به تنهایی پیشرفت‌های کیفی زیادی را در عملکرد مدل به ارمغان می‌آورد، معماری‌های موازی‌پذیر مزیت بزرگی دارند که به شما اجازه انجام این کار را می‌دهند.",
  "input": "Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1509.46,
  "end": 1521.06
 },
 {
  "translatedText": "اگر می خواهید در مورد این چیزها بیشتر بدانید، لینک های زیادی در توضیحات گذاشته ام.",
  "input": "If you want to learn more about this stuff, I've left lots of links in the description.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1522.04,
  "end": 1525.34
 },
 {
  "translatedText": "به طور خاص، هر چیزی که توسط آندری کارپاتی یا کریس اولا تولید می شود، طلای خالص است.",
  "input": "In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1525.92,
  "end": 1530.04
 },
 {
  "translatedText": "در این ویدیو، من می‌خواستم به شکل کنونی آن توجه را جلب کنم، اما اگر کنجکاو هستید درباره تاریخچه‌ای که چگونه به اینجا رسیده‌ایم و چگونه می‌توانید این ایده را برای خودتان دوباره ابداع کنید، دوست من Vivek فقط یک زوج را مطرح کرد. ویدئوهایی که انگیزه بیشتری را ارائه می دهند.",
  "input": "In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1530.56,
  "end": 1542.54
 },
 {
  "translatedText": "همچنین بریت کروز از کانال The Art of the Problem یک ویدیوی واقعا زیبا در مورد تاریخچه مدل های زبان بزرگ دارد.",
  "input": "Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1543.12,
  "end": 1548.46
 },
 {
  "translatedText": "متشکرم.",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1564.96,
  "end": 1569.2
 }
]