[
 {
  "translatedText": "Nếu bạn đưa cụm từ &quot;Michael Jordan chơi bóng rổ&quot; vào một mô hình ngôn ngữ lớn, và yêu cầu nó dự đoán điều gì sẽ xảy ra tiếp theo, và nó dự đoán đúng bóng rổ, điều này cho thấy rằng ở đâu đó, bên trong hàng trăm tỷ tham số của nó, có chứa kiến thức về một người cụ thể và môn thể thao cụ thể của người đó.",
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "translatedText": "Và tôi nghĩ nhìn chung, bất kỳ ai đã từng sử dụng một trong những mô hình này đều có cảm giác rõ ràng rằng nó ghi nhớ rất nhiều sự kiện.",
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "translatedText": "Vậy câu hỏi hợp lý mà bạn có thể hỏi là: Chính xác thì điều đó diễn ra như thế nào?",
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "translatedText": "Và những sự thật đó tồn tại ở đâu?",
  "input": "And where do those facts live?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "translatedText": "Tháng 12 năm ngoái, một số nhà nghiên cứu từ Google DeepMind đã đăng bài về công trình nghiên cứu câu hỏi này và họ đã sử dụng ví dụ cụ thể này để ghép nối các vận động viên với môn thể thao của họ.",
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "translatedText": "Mặc dù vẫn chưa có lời giải đáp đầy đủ về mặt cơ chế về cách lưu trữ dữ liệu, họ đã có một số kết quả thú vị, bao gồm kết luận chung cấp cao rằng dữ liệu dường như nằm bên trong một phần cụ thể của các mạng này, được gọi một cách hoa mỹ là perceptron nhiều lớp hay viết tắt là MLP.",
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "translatedText": "Trong vài chương gần đây, bạn và tôi đã tìm hiểu sâu hơn về các chi tiết đằng sau bộ chuyển đổi, kiến trúc cơ bản của các mô hình ngôn ngữ lớn và cũng là nền tảng của nhiều AI hiện đại khác.",
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "translatedText": "Trong chương gần đây nhất, chúng ta đã tập trung vào một phần có tên là Sự chú ý.",
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "translatedText": "Bước tiếp theo dành cho bạn và tôi là tìm hiểu sâu hơn về những gì xảy ra bên trong các perceptron nhiều lớp này, tạo nên phần lớn khác của mạng lưới.",
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "translatedText": "Phép tính ở đây thực ra khá đơn giản, đặc biệt là khi bạn so sánh nó với sự chú ý.",
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "translatedText": "Về cơ bản, nó bao gồm một cặp phép nhân ma trận với một phép đơn giản ở giữa.",
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "translatedText": "Tuy nhiên, việc giải thích những phép tính này thực sự có ý nghĩa gì lại là một thách thức vô cùng lớn.",
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "translatedText": "Mục tiêu chính của chúng ta ở đây là thực hiện từng bước tính toán và giúp chúng dễ nhớ, nhưng tôi muốn thực hiện trong bối cảnh đưa ra ví dụ cụ thể về cách một trong những khối này, ít nhất là về nguyên tắc, có thể lưu trữ một sự kiện cụ thể.",
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "translatedText": "Cụ thể hơn, nó sẽ lưu trữ thông tin về việc Michael Jordan chơi bóng rổ.",
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "translatedText": "Tôi nên nói rằng bố cục ở đây được lấy cảm hứng từ cuộc trò chuyện của tôi với một trong những nhà nghiên cứu DeepMind, Neil Nanda.",
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "translatedText": "Về cơ bản, tôi sẽ cho rằng bạn đã xem hai chương cuối cùng hoặc có hiểu biết cơ bản về máy biến áp, nhưng việc ôn lại cũng không sao, vì vậy, đây là lời nhắc nhở nhanh về mạch truyện chung.",
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "translatedText": "Bạn và tôi đã nghiên cứu một mô hình được đào tạo để tiếp nhận một đoạn văn bản và dự đoán nội dung tiếp theo.",
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "translatedText": "Đầu tiên, văn bản đầu vào được chia thành một nhóm mã thông báo, nghĩa là các khối nhỏ thường là các từ hoặc các phần từ nhỏ, và mỗi mã thông báo được liên kết với một vectơ nhiều chiều, tức là một danh sách dài các số.",
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "translatedText": "Chuỗi vectơ này sau đó liên tục trải qua hai loại hoạt động, chú ý, cho phép các vectơ truyền thông tin cho nhau, và sau đó là các perceptron đa lớp, thứ mà chúng ta sẽ tìm hiểu sâu hơn ngày hôm nay, và cũng có một bước chuẩn hóa nhất định ở giữa.",
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "translatedText": "Sau khi chuỗi vectơ đã trải qua rất nhiều lần lặp lại khác nhau của cả hai khối này, cuối cùng, hy vọng là mỗi vectơ đã hấp thụ đủ thông tin, từ ngữ cảnh, tất cả các từ khác trong dữ liệu đầu vào và từ kiến thức chung đã được đưa vào trọng số mô hình thông qua quá trình đào tạo, để có thể sử dụng nó để dự đoán mã thông báo nào sẽ xuất hiện tiếp theo.",
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "translatedText": "Một trong những ý tưởng chính mà tôi muốn bạn ghi nhớ là tất cả các vectơ này đều tồn tại trong một không gian có rất nhiều chiều, và khi bạn nghĩ về không gian đó, các hướng khác nhau có thể mã hóa các loại ý nghĩa khác nhau.",
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "translatedText": "Vì vậy, một ví dụ rất kinh điển mà tôi muốn nhắc lại là nếu bạn nhìn vào danh từ phụ nữ và trừ đi danh từ đàn ông, rồi thực hiện bước nhỏ đó và thêm vào một danh từ nam tính khác, chẳng hạn như chú, bạn sẽ đến một nơi rất, rất gần với danh từ nữ tính tương ứng.",
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "translatedText": "Theo nghĩa này, hướng cụ thể này mã hóa thông tin về giới tính.",
  "input": "In this sense, this particular direction encodes gender information.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "translatedText": "Ý tưởng ở đây là nhiều hướng riêng biệt khác nhau trong không gian có nhiều chiều này có thể tương ứng với các đặc điểm khác mà mô hình muốn thể hiện.",
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "translatedText": "Tuy nhiên, trong máy biến áp, các vectơ này không chỉ mã hóa ý nghĩa của một từ duy nhất.",
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "translatedText": "Khi chúng chảy qua mạng, chúng sẽ hấp thụ ý nghĩa phong phú hơn nhiều dựa trên bối cảnh xung quanh chúng và dựa trên kiến thức của mô hình.",
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "translatedText": "Cuối cùng, mỗi người cần mã hóa một cái gì đó vượt xa ý nghĩa của một từ đơn lẻ, vì nó cần đủ để dự đoán điều gì sẽ xảy ra tiếp theo.",
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "translatedText": "Chúng ta đã thấy các khối chú ý cho phép bạn kết hợp ngữ cảnh như thế nào, nhưng phần lớn các tham số mô hình thực sự nằm bên trong các khối MLP và một suy nghĩ về những gì chúng có thể làm là cung cấp thêm dung lượng để lưu trữ các sự kiện.",
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "translatedText": "Như tôi đã nói, bài học ở đây sẽ tập trung vào ví dụ về đồ chơi bê tông để xem chính xác nó có thể lưu trữ thông tin về việc Michael Jordan chơi bóng rổ như thế nào.",
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "translatedText": "Bây giờ, ví dụ về đồ chơi này sẽ yêu cầu bạn và tôi đưa ra một vài giả định về không gian nhiều chiều đó.",
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "translatedText": "Đầu tiên, chúng ta sẽ giả sử rằng một trong các hướng biểu diễn ý tưởng về tên Michael, sau đó một hướng gần như vuông góc khác biểu diễn ý tưởng về họ Jordan, và sau đó hướng thứ ba sẽ biểu diễn ý tưởng về bóng rổ.",
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "translatedText": "Cụ thể hơn, ý tôi muốn nói là nếu bạn nhìn vào mạng và chọn ra một trong các vectơ đang được xử lý, nếu tích vô hướng của tên Michael này là một, thì đó chính là ý nghĩa của vectơ đang mã hóa ý tưởng về một người có tên đó.",
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "translatedText": "Nếu không, tích vô hướng đó sẽ bằng 0 hoặc âm, nghĩa là vectơ không thực sự thẳng hàng với hướng đó.",
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "translatedText": "Và để đơn giản, chúng ta hãy bỏ qua hoàn toàn câu hỏi rất hợp lý về ý nghĩa của việc tích vô hướng đó lớn hơn một.",
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "translatedText": "Tương tự như vậy, tích vô hướng của nó với các hướng khác sẽ cho bạn biết liệu nó đại diện cho họ Jordan hay tên bóng rổ.",
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "translatedText": "Vì vậy, giả sử một vectơ được dùng để biểu diễn tên đầy đủ, Michael Jordan, thì tích vô hướng của nó với cả hai hướng này sẽ phải bằng một.",
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "translatedText": "Vì văn bản Michael Jordan bao gồm hai mã thông báo khác nhau, điều này cũng có nghĩa là chúng ta phải giả định rằng một khối chú ý trước đó đã truyền thông tin thành công đến vectơ thứ hai trong hai vectơ này để đảm bảo rằng nó có thể mã hóa cả hai tên.",
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "translatedText": "Với tất cả những giả định trên, bây giờ chúng ta hãy đi sâu vào nội dung chính của bài học.",
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "translatedText": "Chuyện gì xảy ra bên trong một perceptron nhiều lớp?",
  "input": "What happens inside a multilayer perceptron?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "translatedText": "Bạn có thể nghĩ đến chuỗi các vectơ này chảy vào khối và nhớ rằng, mỗi vectơ ban đầu được liên kết với một trong các mã thông báo từ văn bản đầu vào.",
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "translatedText": "Điều sẽ xảy ra là mỗi vectơ riêng lẻ từ chuỗi đó sẽ trải qua một loạt các phép toán ngắn, chúng ta sẽ giải nén chúng chỉ trong chốc lát và cuối cùng, chúng ta sẽ nhận được một vectơ khác có cùng kích thước.",
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "translatedText": "Vectơ khác đó sẽ được thêm vào vectơ ban đầu chảy vào và tổng đó là kết quả chảy ra.",
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "translatedText": "Trình tự hoạt động này là thứ bạn áp dụng cho mọi vectơ trong trình tự, liên kết với mọi mã thông báo trong đầu vào và tất cả diễn ra song song.",
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "translatedText": "Đặc biệt, các vectơ không tương tác với nhau trong bước này, chúng đều hoạt động theo cách riêng của mình.",
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "translatedText": "Và đối với bạn và tôi, điều đó thực sự làm cho mọi thứ đơn giản hơn rất nhiều, bởi vì nó có nghĩa là nếu chúng ta hiểu được điều gì xảy ra với chỉ một trong các vectơ thông qua khối này, thì về cơ bản chúng ta sẽ hiểu được điều gì xảy ra với tất cả các vectơ đó.",
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "translatedText": "Khi tôi nói khối này sẽ mã hóa thông tin Michael Jordan chơi bóng rổ, ý tôi là nếu một vectơ mã hóa tên Michael và họ Jordan, thì chuỗi phép tính này sẽ tạo ra thứ gì đó bao gồm hướng bóng rổ đó, tức là thứ sẽ được thêm vào vectơ ở vị trí đó.",
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "translatedText": "Bước đầu tiên của quá trình này trông giống như việc nhân vectơ đó với một ma trận rất lớn.",
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "translatedText": "Không có gì ngạc nhiên, đây chính là học sâu.",
  "input": "No surprises there, this is deep learning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "translatedText": "Và ma trận này, giống như tất cả các ma trận khác mà chúng ta đã thấy, chứa đầy các tham số mô hình được học từ dữ liệu, mà bạn có thể coi như một loạt các nút vặn và mặt số được điều chỉnh và tinh chỉnh để xác định hành vi của mô hình.",
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "translatedText": "Bây giờ, một cách hay để nghĩ về phép nhân ma trận là tưởng tượng mỗi hàng của ma trận đó là một vectơ riêng và lấy một loạt tích vô hướng giữa các hàng đó và vectơ đang được xử lý, mà tôi sẽ dán nhãn là E để nhúng.",
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "translatedText": "Ví dụ, giả sử hàng đầu tiên tình cờ bằng với hướng tên Michael mà chúng ta đang cho là tồn tại.",
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "translatedText": "Điều đó có nghĩa là thành phần đầu tiên trong đầu ra này, tích vô hướng ở đây, sẽ là một nếu vectơ đó mã hóa tên Michael và bằng không hoặc số âm nếu không.",
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "translatedText": "Thậm chí còn thú vị hơn nữa, hãy dành một chút thời gian để suy nghĩ xem điều gì sẽ xảy ra nếu hàng đầu tiên là tên Michael cộng với họ Jordan theo hướng này.",
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "translatedText": "Để đơn giản hơn, tôi xin viết nó thành M cộng với J.",
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "translatedText": "Sau đó, lấy tích vô hướng với phép nhúng E này, mọi thứ phân bố thực sự đẹp, trông giống như M chấm E cộng với J chấm E.",
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "translatedText": "Và hãy lưu ý rằng điều đó có nghĩa là giá trị cuối cùng sẽ là hai nếu vectơ mã hóa tên đầy đủ của Michael Jordan, nếu không thì sẽ là một hoặc một số nhỏ hơn một.",
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "translatedText": "Và đó chỉ là một hàng trong ma trận này.",
  "input": "And that's just one row in this matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "translatedText": "Bạn có thể nghĩ tất cả các hàng khác đều song song đặt ra một số loại câu hỏi khác nhau, thăm dò một số loại tính năng khác nhau của vectơ đang được xử lý.",
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "translatedText": "Bước này thường bao gồm việc thêm một vectơ khác vào đầu ra, chứa đầy đủ các tham số mô hình học được từ dữ liệu.",
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "translatedText": "Vectơ khác này được gọi là độ lệch.",
  "input": "This other vector is known as the bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "translatedText": "Trong ví dụ của chúng ta, tôi muốn bạn tưởng tượng rằng giá trị của độ lệch này trong thành phần đầu tiên là âm một, nghĩa là đầu ra cuối cùng của chúng ta trông giống như tích vô hướng có liên quan, nhưng là âm một.",
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "translatedText": "Bạn có thể hỏi một cách rất hợp lý rằng tại sao tôi muốn bạn cho rằng mô hình đã học được điều này và ngay sau đây bạn sẽ thấy tại sao nó rất rõ ràng và đẹp nếu chúng ta có một giá trị ở đây là dương nếu và chỉ nếu một vectơ mã hóa tên đầy đủ là Michael Jordan, còn nếu không thì sẽ là số không hoặc số âm.",
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "translatedText": "Tổng số hàng trong ma trận này, tương đương với số câu hỏi được đặt ra trong trường hợp của GPT-3, với số lượng mà chúng tôi đang theo dõi, chỉ dưới 50.000.",
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "translatedText": "Trên thực tế, con số này chính xác gấp bốn lần số chiều trong không gian nhúng này.",
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "translatedText": "Đó là một lựa chọn thiết kế.",
  "input": "That's a design choice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "translatedText": "Bạn có thể làm nhiều hơn hoặc ít hơn, nhưng việc làm sạch nhiều lần sẽ thân thiện hơn với phần cứng.",
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "translatedText": "Vì ma trận chứa đầy trọng số này ánh xạ chúng ta vào không gian có nhiều chiều hơn nên tôi sẽ viết tắt là W.",
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "translatedText": "Tôi sẽ tiếp tục dán nhãn vectơ mà chúng ta đang xử lý là E và hãy dán nhãn vectơ sai số này là B lên và đưa tất cả trở lại sơ đồ.",
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "translatedText": "Ở thời điểm này, vấn đề là hoạt động này hoàn toàn tuyến tính, nhưng ngôn ngữ lại là một quá trình không tuyến tính.",
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "translatedText": "Nếu mục nhập mà chúng ta đang đo lường là cao đối với Michael cộng với Jordan, thì nó cũng cần phải được kích hoạt phần nào bởi Michael cộng với Phelps và Alexis cộng với Jordan, mặc dù về mặt khái niệm thì những người này không liên quan đến nhau.",
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "translatedText": "Điều bạn thực sự muốn chỉ là câu trả lời có hoặc không cho tên đầy đủ.",
  "input": "What you really want is a simple yes or no for the full name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "translatedText": "Vì vậy, bước tiếp theo là truyền vectơ trung gian lớn này qua một hàm phi tuyến tính rất đơn giản.",
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "translatedText": "Một lựa chọn phổ biến là lấy tất cả các giá trị âm và ánh xạ chúng thành 0 và giữ nguyên tất cả các giá trị dương.",
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "translatedText": "Và tiếp tục với truyền thống học sâu với những cái tên quá hoa mỹ, hàm rất đơn giản này thường được gọi là đơn vị tuyến tính chỉnh lưu hoặc viết tắt là ReLU.",
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "translatedText": "Đây là hình ảnh biểu đồ.",
  "input": "Here's what the graph looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "translatedText": "Vì vậy, lấy ví dụ tưởng tượng của chúng ta, trong đó mục đầu tiên của vectơ trung gian là một, nếu và chỉ nếu tên đầy đủ là Michael Jordan và bằng không hoặc số âm nếu không, sau khi bạn đưa nó qua ReLU, bạn sẽ có một giá trị rất sạch, trong đó tất cả các giá trị bằng không và giá trị âm chỉ bị cắt thành số không.",
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "translatedText": "Vì vậy, đầu ra này sẽ là một cho tên đầy đủ là Michael Jordan và là 0 cho những trường hợp khác.",
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "translatedText": "Nói cách khác, nó mô phỏng rất trực tiếp hành vi của cổng AND.",
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "translatedText": "Các mô hình thường sử dụng một hàm được sửa đổi đôi chút gọi là JLU, có hình dạng cơ bản giống như vậy nhưng mượt mà hơn một chút.",
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "translatedText": "Nhưng đối với mục đích của chúng ta, mọi thứ sẽ rõ ràng hơn một chút nếu chúng ta chỉ nghĩ về ReLU.",
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "translatedText": "Ngoài ra, khi bạn nghe mọi người nhắc đến các nơ-ron của máy biến áp, nghĩa là họ đang nói về những giá trị này ngay tại đây.",
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "translatedText": "Bất cứ khi nào bạn thấy hình ảnh mạng nơ-ron phổ biến với một lớp chấm và một loạt các đường kết nối với lớp trước đó, mà chúng ta đã có trước đó trong loạt bài này, thì điều đó thường có nghĩa là truyền đạt sự kết hợp của một bước tuyến tính, phép nhân ma trận, theo sau là một số hàm phi tuyến tính theo từng thuật ngữ đơn giản như ReLU.",
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "translatedText": "Bạn có thể nói rằng nơ-ron này hoạt động khi giá trị này dương và không hoạt động nếu giá trị đó bằng không.",
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "translatedText": "Bước tiếp theo trông rất giống với bước đầu tiên.",
  "input": "The next step looks very similar to the first one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "translatedText": "Bạn nhân với một ma trận rất lớn và thêm vào một số hạng thiên vị nhất định.",
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "translatedText": "Trong trường hợp này, số chiều trong đầu ra sẽ giảm xuống bằng kích thước của không gian nhúng đó, vì vậy tôi sẽ tiếp tục và gọi đây là ma trận chiếu xuống.",
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "translatedText": "Và lần này, thay vì nghĩ về từng hàng, thực ra sẽ hay hơn nếu nghĩ về từng cột.",
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "translatedText": "Bạn thấy đấy, một cách khác để bạn có thể ghi nhớ phép nhân ma trận trong đầu là tưởng tượng việc lấy từng cột của ma trận và nhân nó với số hạng tương ứng trong vectơ mà nó đang xử lý và cộng tất cả các cột đã được chia tỷ lệ lại với nhau.",
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "translatedText": "Lý do khiến cách này hay hơn là vì ở đây các cột có cùng kích thước với không gian nhúng, do đó chúng ta có thể coi chúng là các hướng trong không gian đó.",
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "translatedText": "Ví dụ, chúng ta sẽ tưởng tượng rằng mô hình đã học được cách biến cột đầu tiên thành hướng bóng rổ mà chúng ta cho là tồn tại.",
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "translatedText": "Điều đó có nghĩa là khi tế bào thần kinh có liên quan ở vị trí đầu tiên hoạt động, chúng ta sẽ thêm cột này vào kết quả cuối cùng.",
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "translatedText": "Nhưng nếu tế bào thần kinh đó không hoạt động, nếu con số đó bằng không, thì điều này sẽ không có tác dụng gì.",
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "translatedText": "Và không chỉ có bóng rổ.",
  "input": "And it doesn't just have to be basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "translatedText": "Mô hình này cũng có thể tích hợp vào cột này và nhiều tính năng khác mà nó muốn liên kết với thứ gì đó có tên đầy đủ là Michael Jordan.",
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "translatedText": "Và đồng thời, tất cả các cột khác trong ma trận này sẽ cho bạn biết những gì sẽ được thêm vào kết quả cuối cùng nếu nơ-ron tương ứng hoạt động.",
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "translatedText": "Và nếu bạn có sự thiên vị trong trường hợp này, thì đó là thứ bạn chỉ cần thêm vào mỗi lần, bất kể giá trị của nơ-ron.",
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "translatedText": "Bạn có thể tự hỏi điều đó có tác dụng gì.",
  "input": "You might wonder what's that doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "translatedText": "Giống như tất cả các đối tượng có tham số ở đây, thật khó để nói chính xác.",
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "translatedText": "Có thể mạng lưới cần phải thực hiện một số công việc kế toán, nhưng hiện tại bạn có thể thoải mái bỏ qua.",
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "translatedText": "Để ký hiệu của chúng ta trở nên nhỏ gọn hơn một chút, tôi sẽ gọi ma trận lớn W này là xuống và tương tự gọi vectơ thiên vị B là xuống rồi đưa nó trở lại sơ đồ của chúng ta.",
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "translatedText": "Như tôi đã giới thiệu trước đó, những gì bạn làm với kết quả cuối cùng này là thêm nó vào vectơ chảy vào khối tại vị trí đó và bạn sẽ có được kết quả cuối cùng này.",
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "translatedText": "Ví dụ, nếu vectơ chạy vào mã hóa cả tên Michael và họ Jordan, thì vì chuỗi hoạt động này sẽ kích hoạt cổng AND, nó sẽ thêm vào hướng bóng rổ, do đó, những gì xuất hiện sẽ mã hóa tất cả những tên đó lại với nhau.",
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "translatedText": "Và hãy nhớ rằng, đây là một quá trình xảy ra song song với từng vectơ đó.",
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "translatedText": "Cụ thể hơn, khi lấy số GPT-3, điều này có nghĩa là khối này không chỉ có 50.000 nơ-ron mà còn có số lượng mã thông báo trong đầu vào nhiều gấp 50.000 lần.",
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "translatedText": "Vậy là toàn bộ hoạt động đã hoàn tất, hai tích ma trận, mỗi tích có thêm một độ lệch và một hàm cắt đơn giản ở giữa.",
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "translatedText": "Bất kỳ ai đã xem các video trước đó của loạt bài này đều sẽ nhận ra cấu trúc này là loại mạng nơ-ron cơ bản nhất mà chúng ta đã nghiên cứu ở đó.",
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "translatedText": "Trong ví dụ đó, máy tính được đào tạo để nhận dạng chữ số viết tay.",
  "input": "In that example, it was trained to recognize handwritten digits.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "translatedText": "Ở đây, trong bối cảnh của một bộ chuyển đổi cho một mô hình ngôn ngữ lớn, đây là một phần trong một kiến trúc lớn hơn và bất kỳ nỗ lực nào nhằm diễn giải chính xác chức năng của nó đều gắn chặt với ý tưởng mã hóa thông tin thành các vectơ của không gian nhúng nhiều chiều.",
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "translatedText": "Đó là bài học cốt lõi, nhưng tôi muốn lùi lại một bước và suy ngẫm về hai điều khác nhau, điều đầu tiên là một loại sổ sách kế toán, và điều thứ hai liên quan đến một sự thật rất đáng suy ngẫm về các chiều không gian cao hơn mà thực ra tôi không biết cho đến khi tôi tìm hiểu về máy biến áp.",
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "translatedText": "Trong hai chương trước, bạn và tôi đã bắt đầu đếm tổng số tham số trong GPT-3 và xem chính xác vị trí của chúng, vậy nên chúng ta hãy nhanh chóng hoàn thành trò chơi tại đây.",
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "translatedText": "Tôi đã đề cập đến cách ma trận chiếu lên này có chưa đến 50.000 hàng và mỗi hàng khớp với kích thước của không gian nhúng, đối với GPT-3 là 12.288.",
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "translatedText": "Nhân chúng lại với nhau, ta được 604 triệu tham số chỉ dành cho ma trận đó và phép chiếu xuống cũng có cùng số tham số nhưng có hình dạng chuyển vị.",
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "translatedText": "Vì vậy, tổng cộng chúng cung cấp khoảng 1,2 tỷ tham số.",
  "input": "So together, they give about 1.2 billion parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "translatedText": "Vectơ độ lệch cũng chiếm một vài tham số nữa, nhưng nó chỉ chiếm một tỷ lệ nhỏ trong tổng số nên tôi thậm chí sẽ không trình bày nó.",
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "translatedText": "Trong GPT-3, chuỗi vectơ nhúng này chạy qua không chỉ một mà là 96 MLP riêng biệt, do đó tổng số tham số dành cho tất cả các khối này lên tới khoảng 116 tỷ.",
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "translatedText": "Con số này chiếm khoảng 2/3 tổng số tham số trong mạng và khi bạn thêm nó vào mọi thứ chúng ta đã có trước đó, đối với các khối chú ý, nhúng và hủy nhúng, bạn thực sự sẽ nhận được tổng cộng 175 tỷ như đã quảng cáo.",
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "translatedText": "Có lẽ đáng đề cập đến một tập hợp các tham số khác liên quan đến các bước chuẩn hóa mà phần giải thích này đã bỏ qua, nhưng giống như vectơ độ lệch, chúng chỉ chiếm một tỷ lệ rất nhỏ trong tổng số.",
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "translatedText": "Về điểm phản ánh thứ hai, bạn có thể tự hỏi liệu ví dụ đồ chơi trung tâm mà chúng ta đã dành nhiều thời gian cho có phản ánh cách các sự kiện thực sự được lưu trữ trong các mô hình ngôn ngữ lớn thực sự hay không.",
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "translatedText": "Đúng là các hàng của ma trận đầu tiên có thể được coi là các hướng trong không gian nhúng này và điều đó có nghĩa là sự kích hoạt của mỗi nơ-ron sẽ cho bạn biết mức độ một vectơ nhất định liên kết với một hướng cụ thể nào đó.",
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "translatedText": "Các cột của ma trận thứ hai cũng đúng khi cho bạn biết những gì sẽ được thêm vào kết quả nếu nơ-ron đó hoạt động.",
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "translatedText": "Cả hai đều chỉ là những sự thật toán học.",
  "input": "Both of those are just mathematical facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "translatedText": "Tuy nhiên, bằng chứng cho thấy rằng các tế bào thần kinh riêng lẻ rất hiếm khi đại diện cho một đặc điểm sạch duy nhất như Michael Jordan, và thực tế có thể có một lý do rất chính đáng cho trường hợp này, liên quan đến một ý tưởng đang được các nhà nghiên cứu về khả năng diễn giải hiện nay gọi là sự chồng chập.",
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "translatedText": "Đây là giả thuyết có thể giúp giải thích tại sao các mô hình lại đặc biệt khó diễn giải và tại sao chúng có khả năng mở rộng đáng ngạc nhiên.",
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "translatedText": "Ý tưởng cơ bản là nếu bạn có một không gian n chiều và bạn muốn biểu diễn một loạt các đặc điểm khác nhau bằng các hướng vuông góc với nhau trong không gian đó, bạn biết đấy, theo cách đó, nếu bạn thêm một thành phần theo một hướng, nó sẽ không ảnh hưởng đến bất kỳ hướng nào khác, thì số lượng vectơ tối đa bạn có thể phù hợp chỉ là n, tức là số chiều.",
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "translatedText": "Đối với một nhà toán học, đây thực sự là định nghĩa về chiều.",
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "translatedText": "Nhưng điều thú vị là nếu bạn nới lỏng một chút sự hạn chế đó và chấp nhận một số tiếng ồn.",
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "translatedText": "Giả sử bạn cho phép các đặc điểm đó được biểu diễn bằng các vectơ không hoàn toàn vuông góc, mà chỉ gần như vuông góc, có thể cách nhau khoảng 89 đến 91 độ.",
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "translatedText": "Nếu chúng ta ở trong không gian hai hoặc ba chiều thì điều này không tạo ra sự khác biệt.",
  "input": "If we were in two or three dimensions, this makes no difference.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "translatedText": "Điều đó hầu như không cho bạn thêm bất kỳ khoảng trống nào để đưa thêm nhiều vectơ vào, điều này càng khiến cho việc đối với các chiều cao hơn, câu trả lời thay đổi đáng kể trở nên trái ngược với trực giác.",
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "translatedText": "Tôi có thể cho bạn một minh họa thực sự nhanh chóng và đơn giản về điều này bằng cách sử dụng một số Python để tạo danh sách các vectơ 100 chiều, mỗi vectơ được khởi tạo ngẫu nhiên và danh sách này sẽ chứa 10.000 vectơ riêng biệt, do đó có số vectơ nhiều gấp 100 lần số chiều.",
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "translatedText": "Biểu đồ này cho thấy sự phân bố góc giữa các cặp vectơ này.",
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "translatedText": "Vì chúng bắt đầu ngẫu nhiên nên các góc đó có thể nằm trong khoảng từ 0 đến 180 độ, nhưng bạn sẽ nhận thấy rằng, ngay cả đối với các vectơ ngẫu nhiên, vẫn có độ lệch lớn khiến mọi thứ gần hơn với 90 độ.",
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "translatedText": "Sau đó, điều tôi sẽ làm là chạy một quy trình tối ưu hóa nhất định để liên tục thúc đẩy tất cả các vectơ này sao cho chúng trở nên vuông góc với nhau hơn.",
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "translatedText": "Sau khi lặp lại nhiều lần, sự phân bố góc trông như thế này.",
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "translatedText": "Chúng ta thực sự phải phóng to nó ở đây vì tất cả các góc có thể có giữa các cặp vectơ đều nằm trong phạm vi hẹp từ 89 đến 91 độ.",
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "translatedText": "Nhìn chung, một hệ quả của cái gọi là định lý Johnson-Lindenstrauss là số lượng vectơ mà bạn có thể nhồi nhét vào một không gian gần như vuông góc như thế này sẽ tăng theo cấp số nhân theo số chiều.",
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "translatedText": "Điều này rất quan trọng đối với các mô hình ngôn ngữ lớn, có thể được hưởng lợi từ việc liên kết các ý tưởng độc lập với các hướng gần như vuông góc.",
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "translatedText": "Điều này có nghĩa là nó có thể lưu trữ nhiều ý tưởng hơn rất nhiều so với số chiều trong không gian mà nó được phân bổ.",
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "translatedText": "Điều này có thể giải thích một phần tại sao hiệu suất của mô hình dường như tăng trưởng rất tốt theo kích thước.",
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "translatedText": "Một không gian có số chiều gấp 10 lần có thể lưu trữ nhiều hơn gấp 10 lần số ý tưởng độc lập.",
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "translatedText": "Và điều này không chỉ liên quan đến không gian nhúng nơi các vectơ chạy qua mô hình tồn tại mà còn liên quan đến vectơ chứa đầy nơ-ron ở giữa lớp perceptron đa lớp mà chúng ta vừa nghiên cứu.",
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "translatedText": "Nói cách khác, ở kích thước của GPT-3, nó có thể không chỉ thăm dò 50.000 đặc điểm, mà nếu thay vào đó tận dụng khả năng bổ sung khổng lồ này bằng cách sử dụng các hướng gần như vuông góc của không gian, nó có thể thăm dò nhiều đặc điểm hơn nữa của vectơ đang được xử lý.",
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "translatedText": "Nhưng nếu thực hiện được điều đó, điều đó có nghĩa là các đặc điểm riêng lẻ sẽ không thể nhìn thấy được khi một nơ-ron đơn lẻ sáng lên.",
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "translatedText": "Thay vào đó, nó phải trông giống như một sự kết hợp cụ thể nào đó của các tế bào thần kinh, một sự chồng chập.",
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "translatedText": "Đối với bất kỳ ai tò mò muốn tìm hiểu thêm, một thuật ngữ tìm kiếm có liên quan chính ở đây là bộ mã hóa tự động thưa thớt, đây là một công cụ mà một số người giải thích sử dụng để cố gắng trích xuất các tính năng thực sự, ngay cả khi chúng được chồng lên nhau rất nhiều trên tất cả các tế bào thần kinh này.",
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "translatedText": "Tôi sẽ liên kết đến một số bài đăng thực sự tuyệt vời về chủ đề này.",
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "translatedText": "Đến thời điểm này, chúng ta vẫn chưa đề cập đến mọi chi tiết của máy biến áp, nhưng bạn và tôi đã đề cập đến những điểm quan trọng nhất.",
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "translatedText": "Điều chính mà tôi muốn đề cập trong chương tiếp theo là quá trình đào tạo.",
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "translatedText": "Một mặt, câu trả lời ngắn gọn cho cách thức đào tạo hoạt động là tất cả đều dựa trên sự lan truyền ngược, và chúng tôi đã đề cập đến sự lan truyền ngược trong một bối cảnh riêng biệt ở các chương trước trong loạt bài này.",
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "translatedText": "Nhưng vẫn còn nhiều điều cần thảo luận, như hàm chi phí cụ thể được sử dụng cho các mô hình ngôn ngữ, ý tưởng tinh chỉnh bằng cách sử dụng học tăng cường với phản hồi của con người và khái niệm về quy luật tỷ lệ.",
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "translatedText": "Lưu ý nhanh cho những người theo dõi tích cực trong số các bạn, có một số video không liên quan đến máy học mà tôi rất muốn xem hết trước khi viết chương tiếp theo. Có thể sẽ mất một thời gian, nhưng tôi hứa sẽ sớm ra mắt.",
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "translatedText": "Cảm ơn.",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]