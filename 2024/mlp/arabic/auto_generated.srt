1
00:00:00,000 --> 00:00:06,302
إذا قمت بإطعام نموذج لغوي كبير العبارة &quot;مايكل جوردن يلعب كرة السلة&quot;، وجعلته 

2
00:00:06,302 --> 00:00:12,457
يتنبأ بما سيأتي بعد ذلك، ويتنبأ بشكل صحيح بكرة السلة، فهذا يشير إلى أنه في مكان ما، 

3
00:00:12,457 --> 00:00:18,320
داخل مئات المليارات من معلماته، هناك معلومات مدمجة حول شخص معين ورياضته المحددة.

4
00:00:18,940 --> 00:00:22,105
وأعتقد بشكل عام أن أي شخص لعب مع أحد هذه النماذج 

5
00:00:22,105 --> 00:00:25,400
لديه إحساس واضح بأنه حفظ الكثير والكثير من الحقائق.

6
00:00:25,700 --> 00:00:29,160
لذا فإن السؤال المعقول الذي يمكنك طرحه هو، كيف يعمل هذا بالضبط؟

7
00:00:29,160 --> 00:00:31,040
وأين توجد تلك الحقائق؟

8
00:00:35,720 --> 00:00:40,016
في ديسمبر/كانون الأول الماضي، نشر عدد من الباحثين من Google DeepMind أعمالاً 

9
00:00:40,016 --> 00:00:44,480
حول هذا السؤال، وكانوا يستخدمون هذا المثال المحدد لمطابقة الرياضيين مع رياضاتهم.

10
00:00:44,900 --> 00:00:49,232
وعلى الرغم من أن الفهم الميكانيكي الكامل لكيفية تخزين الحقائق لا يزال دون 

11
00:00:49,232 --> 00:00:53,799
حل، فقد توصلوا إلى بعض النتائج الجزئية المثيرة للاهتمام، بما في ذلك الاستنتاج 

12
00:00:53,799 --> 00:00:58,248
العام للغاية على مستوى عالٍ بأن الحقائق يبدو أنها تعيش داخل جزء محدد من هذه 

13
00:00:58,248 --> 00:01:02,640
الشبكات، والمعروف بشكل خيالي باسم المدركات متعددة الطبقات، أو MLPs باختصار.

14
00:01:03,120 --> 00:01:06,042
في الفصول القليلة الماضية، كنا أنا وأنت نبحث في التفاصيل وراء 

15
00:01:06,042 --> 00:01:09,153
المحولات، والهندسة المعمارية التي تقوم عليها نماذج اللغة الكبيرة، 

16
00:01:09,153 --> 00:01:12,500
والتي تقوم عليها أيضًا العديد من أشكال الذكاء الاصطناعي الحديثة الأخرى.

17
00:01:13,060 --> 00:01:16,200
في الفصل الأخير، ركزنا على قطعة تسمى &quot;الانتباه&quot;.

18
00:01:16,840 --> 00:01:20,785
والخطوة التالية بالنسبة لي ولك هي التعمق في تفاصيل ما يحدث داخل 

19
00:01:20,785 --> 00:01:25,040
هذه المدركات متعددة الطبقات، والتي تشكل الجزء الكبير الآخر من الشبكة.

20
00:01:25,680 --> 00:01:30,100
الحساب هنا بسيط نسبيًا في الواقع، خاصة عند مقارنته بالانتباه.

21
00:01:30,560 --> 00:01:34,980
يمكن تلخيص الأمر بشكل أساسي في زوج من مضاعفات المصفوفة مع وجود شيء بسيط بينهما.

22
00:01:35,720 --> 00:01:40,460
ومع ذلك، فإن تفسير ما تفعله هذه الحسابات يعد تحديًا كبيرًا للغاية.

23
00:01:41,560 --> 00:01:45,301
هدفنا الرئيسي هنا هو التقدم عبر العمليات الحسابية وجعلها لا 

24
00:01:45,301 --> 00:01:49,106
تنسى، ولكنني أود أن أفعل ذلك في سياق إظهار مثال محدد حول كيف 

25
00:01:49,106 --> 00:01:53,160
يمكن لأحد هذه الكتل، على الأقل من حيث المبدأ، تخزين حقيقة ملموسة.

26
00:01:53,580 --> 00:01:57,080
على وجه التحديد، سيتم تخزين حقيقة أن مايكل جوردن يلعب كرة السلة.

27
00:01:58,080 --> 00:02:00,693
ينبغي لي أن أذكر أن التصميم هنا مستوحى من محادثة 

28
00:02:00,693 --> 00:02:03,200
أجريتها مع أحد الباحثين في DeepMind، نيل ناندا.

29
00:02:04,060 --> 00:02:09,206
في الغالب، سأفترض أنك إما شاهدت الفصلين الأخيرين، أو أن لديك حسًا أساسيًا 

30
00:02:09,206 --> 00:02:14,700
حول ما هو المحول، ولكن التذكير لا يضر أبدًا، لذا إليك تذكير سريع بالتدفق العام.

31
00:02:15,340 --> 00:02:18,822
لقد قمنا أنا وأنت بدراسة نموذج تم تدريبه على استيعاب 

32
00:02:18,822 --> 00:02:21,320
جزء من النص والتنبؤ بما سيأتي بعد ذلك.

33
00:02:21,720 --> 00:02:26,262
يتم تقسيم نص الإدخال أولاً إلى مجموعة من الرموز، وهو ما يعني أجزاء 

34
00:02:26,262 --> 00:02:30,737
صغيرة تتكون عادةً من كلمات أو قطع صغيرة من الكلمات، ويرتبط كل رمز 

35
00:02:30,737 --> 00:02:35,280
بمتجه عالي الأبعاد، وهو ما يمكن أن نقول عنه قائمة طويلة من الأرقام.

36
00:02:35,840 --> 00:02:41,302
ثم تمر هذه السلسلة من المتجهات بشكل متكرر عبر نوعين من العمليات، الانتباه، 

37
00:02:41,302 --> 00:02:46,619
الذي يسمح للمتجهات بنقل المعلومات بين بعضها البعض، ثم المُدْرِكات متعددة 

38
00:02:46,619 --> 00:02:52,300
الطبقات، وهو الشيء الذي سنتعمق فيه اليوم، وهناك أيضًا خطوة تطبيع معينة بينهما.

39
00:02:53,300 --> 00:02:59,118
بعد أن تتدفق سلسلة المتجهات عبر العديد والعديد من التكرارات المختلفة لكلا الكتلتين، 

40
00:02:59,118 --> 00:03:04,729
في النهاية، يكون الأمل هو أن كل متجه قد استوعب معلومات كافية، سواء من السياق، أو 

41
00:03:04,729 --> 00:03:10,201
من جميع الكلمات الأخرى في المدخلات، وكذلك من المعرفة العامة التي تم تضمينها في 

42
00:03:10,201 --> 00:03:16,020
أوزان النموذج من خلال التدريب، بحيث يمكن استخدامها للتنبؤ بالرمز الذي سيأتي بعد ذلك.

43
00:03:16,860 --> 00:03:20,820
أحد الأفكار الرئيسية التي أريدكم أن تخطروا على بالكم هي أن كل هذه 

44
00:03:20,820 --> 00:03:24,660
المتجهات تعيش في فضاء ذي أبعاد عالية جدًا، وعندما تفكرون في هذا 

45
00:03:24,660 --> 00:03:28,800
الفضاء، فإن الاتجاهات المختلفة يمكن أن تشفر أنواعًا مختلفة من المعنى.

46
00:03:30,120 --> 00:03:35,632
لذا فإن أحد الأمثلة الكلاسيكية التي أحب الرجوع إليها هو كيف إذا نظرت إلى تضمين 

47
00:03:35,632 --> 00:03:40,866
كلمة woman وطرحت تضمين كلمة man، واتخذت هذه الخطوة الصغيرة وأضفتها إلى اسم 

48
00:03:40,866 --> 00:03:46,240
مذكر آخر، مثل العم، فإنك تهبط في مكان قريب جدًا جدًا من الاسم المؤنث المقابل.

49
00:03:46,440 --> 00:03:50,880
وبهذا المعنى، فإن هذا الاتجاه المحدد يشفر معلومات الجنس.

50
00:03:51,640 --> 00:03:55,608
الفكرة هي أن العديد من الاتجاهات الأخرى المتميزة في هذه المساحة 

51
00:03:55,608 --> 00:03:59,640
عالية الأبعاد قد تتوافق مع ميزات أخرى قد يرغب النموذج في تمثيلها.

52
00:04:01,400 --> 00:04:06,180
في المحول، لا تقوم هذه المتجهات بتشفير معنى كلمة واحدة فحسب.

53
00:04:06,680 --> 00:04:10,866
وبينما تتدفق هذه العناصر عبر الشبكة، فإنها تستوعب معنى أكثر ثراءً 

54
00:04:10,866 --> 00:04:15,180
استنادًا إلى كل السياق المحيط بها، وكذلك استنادًا إلى معرفة النموذج.

55
00:04:15,880 --> 00:04:19,762
في نهاية المطاف، يحتاج كل واحد منهم إلى تشفير شيء ما أبعد بكثير من 

56
00:04:19,762 --> 00:04:23,760
معنى كلمة واحدة، حيث يجب أن يكون ذلك كافياً للتنبؤ بما سيأتي بعد ذلك.

57
00:04:24,560 --> 00:04:30,897
لقد رأينا بالفعل كيف تسمح لك كتل الانتباه بتضمين السياق، ولكن غالبية معلمات النموذج 

58
00:04:30,897 --> 00:04:37,536
موجودة بالفعل داخل كتل MLP، وأحد الأفكار حول ما قد تفعله هو أنها توفر سعة إضافية لتخزين 

59
00:04:37,536 --> 00:04:38,140
الحقائق.

60
00:04:38,720 --> 00:04:42,565
كما قلت، الدرس هنا سوف يركز على مثال اللعبة الملموسة 

61
00:04:42,565 --> 00:04:46,120
لكيفية تخزين حقيقة أن مايكل جوردن يلعب كرة السلة.

62
00:04:47,120 --> 00:04:49,560
الآن، هذا المثال البسيط سيتطلب مني ومنك أن نقوم 

63
00:04:49,560 --> 00:04:51,900
ببعض الافتراضات حول تلك المساحة عالية الأبعاد.

64
00:04:52,360 --> 00:04:59,205
أولاً، سنفترض أن أحد الاتجاهات يمثل فكرة الاسم الأول مايكل، ثم يمثل اتجاه 

65
00:04:59,205 --> 00:05:06,420
آخر عمودي تقريبًا فكرة الاسم الأخير جوردان، ثم يمثل اتجاه ثالث فكرة كرة السلة.

66
00:05:07,400 --> 00:05:12,194
لذلك على وجه التحديد، ما أقصده بهذا هو إذا نظرت في الشبكة واخترت أحد 

67
00:05:12,194 --> 00:05:16,919
المتجهات التي تتم معالجتها، إذا كان حاصل ضربه النقطي مع اتجاه الاسم 

68
00:05:16,919 --> 00:05:22,340
الأول مايكل هو واحد، فهذا ما يعنيه أن المتجه يشفر فكرة الشخص بهذا الاسم الأول.

69
00:05:23,800 --> 00:05:26,274
وإلا فإن حاصل الضرب النقطي سيكون صفرًا أو سالبًا، 

70
00:05:26,274 --> 00:05:28,700
مما يعني أن المتجه لا يتوافق حقًا مع هذا الاتجاه.

71
00:05:29,420 --> 00:05:32,473
ومن أجل التبسيط، دعونا نتجاهل تمامًا السؤال المعقول للغاية 

72
00:05:32,473 --> 00:05:35,320
حول ما قد يعنيه إذا كان حاصل الضرب النقطي أكبر من واحد.

73
00:05:36,200 --> 00:05:39,944
وبالمثل، فإن حاصل ضرب النقاط مع هذه الاتجاهات الأخرى 

74
00:05:39,944 --> 00:05:43,760
سيخبرك ما إذا كان يمثل الاسم الأخير جوردان أو كرة سلة.

75
00:05:44,740 --> 00:05:48,553
لذا، لنفترض أن المتجه من المفترض أن يمثل الاسم الكامل، مايكل 

76
00:05:48,553 --> 00:05:52,680
جوردان، فإن حاصل ضربه النقطي في كلا الاتجاهين يجب أن يساوي واحدًا.

77
00:05:53,480 --> 00:05:57,888
نظرًا لأن النص &quot;مايكل جوردان&quot; يمتد على رمزين مختلفين، فهذا 

78
00:05:57,888 --> 00:06:02,551
يعني أيضًا أنه يتعين علينا افتراض أن كتلة انتباه سابقة قد نقلت المعلومات 

79
00:06:02,551 --> 00:06:06,960
بنجاح إلى الثاني من هذين المتجهين لضمان قدرتها على ترميز كلا الاسمين.

80
00:06:07,940 --> 00:06:11,480
مع كل هذه الافتراضات، دعونا الآن ننتقل إلى صلب الدرس.

81
00:06:11,880 --> 00:06:14,980
ماذا يحدث داخل الإدراك المتعدد الطبقات؟

82
00:06:17,100 --> 00:06:21,445
قد تفكر في هذا التسلسل من المتجهات المتدفقة إلى الكتلة، وتذكر 

83
00:06:21,445 --> 00:06:25,580
أن كل متجه كان مرتبطًا في الأصل بأحد الرموز من النص المدخل.

84
00:06:26,080 --> 00:06:31,471
ما سيحدث هو أن كل متجه فردي من هذا التسلسل سيخضع لسلسلة قصيرة من العمليات، 

85
00:06:31,471 --> 00:06:36,360
وسنقوم بتفكيكها في لحظة، وفي النهاية، سنحصل على متجه آخر بنفس البعد.

86
00:06:36,880 --> 00:06:40,166
سيتم إضافة المتجه الآخر إلى المتجه الأصلي الذي تدفق 

87
00:06:40,166 --> 00:06:43,200
للداخل، وهذا المجموع هو النتيجة المتدفقة للخارج.

88
00:06:43,720 --> 00:06:47,703
هذا التسلسل من العمليات هو شيء يمكنك تطبيقه على كل متجه في 

89
00:06:47,703 --> 00:06:51,620
التسلسل، ويرتبط بكل رمز في الإدخال، ويحدث كل ذلك بالتوازي.

90
00:06:52,100 --> 00:06:56,200
على وجه الخصوص، لا تتحدث المتجهات مع بعضها البعض في هذه الخطوة، فكل منها يقوم بعمله الخاص.

91
00:06:56,720 --> 00:07:01,361
وبالنسبة لي ولك، هذا يجعل الأمر في الواقع أبسط بكثير، لأنه يعني أنه إذا فهمنا ما 

92
00:07:01,361 --> 00:07:06,060
يحدث لمتجه واحد فقط من المتجهات عبر هذه الكتلة، فإننا نفهم فعليًا ما يحدث لجميعهم.

93
00:07:07,100 --> 00:07:12,673
عندما أقول أن هذه الكتلة سوف تشفر حقيقة أن مايكل جوردان يلعب كرة السلة، ما أعنيه هو 

94
00:07:12,673 --> 00:07:18,114
أنه إذا تدفق متجه يشفر الاسم الأول مايكل والاسم الأخير جوردان، فإن تسلسل العمليات 

95
00:07:18,114 --> 00:07:24,020
الحسابية هذا سوف ينتج شيئًا يتضمن اتجاه كرة السلة، وهو ما سيضاف إلى المتجه في هذا الموضع.

96
00:07:25,600 --> 00:07:29,700
تبدو الخطوة الأولى في هذه العملية مثل ضرب هذا المتجه بمصفوفة كبيرة جدًا.

97
00:07:30,040 --> 00:07:31,980
لا مفاجآت هنا، هذا هو التعلم العميق.

98
00:07:32,680 --> 00:07:36,300
وهذه المصفوفة، مثل كل المصفوفات الأخرى التي رأيناها، مليئة بمعلمات 

99
00:07:36,300 --> 00:07:40,028
النموذج التي تم تعلمها من البيانات، والتي يمكنك أن تفكر فيها كمجموعة 

100
00:07:40,028 --> 00:07:43,540
من الأزرار والمفاتيح التي يتم تعديلها وضبطها لتحديد سلوك النموذج.

101
00:07:44,500 --> 00:07:48,586
الآن، إحدى الطرق الجيدة للتفكير في عملية ضرب المصفوفة هي تخيل كل صف 

102
00:07:48,586 --> 00:07:52,553
من تلك المصفوفة باعتباره متجهًا خاصًا به، وأخذ مجموعة من المنتجات 

103
00:07:52,553 --> 00:07:56,880
النقطية بين تلك الصفوف والمتجه الذي تتم معالجته، والذي سأسميه E للتضمين.

104
00:07:57,280 --> 00:08:04,040
على سبيل المثال، افترض أن الصف الأول يساوي اتجاه الاسم الأول مايكل الذي نفترض وجوده.

105
00:08:04,320 --> 00:08:09,491
وهذا يعني أن المكون الأول في هذا الإخراج، وهو حاصل الضرب النقطي هنا، سيكون 

106
00:08:09,491 --> 00:08:14,800
واحدًا إذا كان هذا المتجه يشفر الاسم الأول مايكل، وصفرًا أو سالبًا بخلاف ذلك.

107
00:08:15,880 --> 00:08:19,402
الأمر الأكثر متعة هو أن تأخذ لحظة للتفكير فيما قد يعنيه إذا كان هذا 

108
00:08:19,402 --> 00:08:23,080
الصف الأول هو اتجاه الاسم الأول مايكل بالإضافة إلى الاسم الأخير جوردان.

109
00:08:23,700 --> 00:08:27,420
ومن أجل التبسيط، دعني أبدأ وأكتب ذلك على هيئة M زائد J.

110
00:08:28,080 --> 00:08:31,682
بعد ذلك، عند أخذ حاصل نقطي مع هذا التضمين E، تتوزع الأشياء 

111
00:08:31,682 --> 00:08:34,980
بشكل جيد حقًا، فيبدو الأمر مثل M نقطة E زائد J نقطة E.

112
00:08:34,980 --> 00:08:39,705
ولاحظ كيف يعني هذا أن القيمة النهائية ستكون اثنين إذا كان المتجه يشفر 

113
00:08:39,705 --> 00:08:44,700
الاسم الكامل لمايكل جوردان، وإلا فإنها ستكون واحدًا أو شيئًا أصغر من واحد.

114
00:08:45,340 --> 00:08:47,260
وهذا مجرد صف واحد في هذه المصفوفة.

115
00:08:47,600 --> 00:08:51,877
يمكنك أن تفكر في جميع الصفوف الأخرى على التوازي، وتطرح بعض الأنواع الأخرى 

116
00:08:51,877 --> 00:08:56,040
من الأسئلة، وتستكشف بعض الأنواع الأخرى من ميزات المتجه الذي تتم معالجته.

117
00:08:56,700 --> 00:08:59,360
في كثير من الأحيان تتضمن هذه الخطوة أيضًا إضافة متجه آخر إلى 

118
00:08:59,360 --> 00:09:02,240
المخرجات، والذي يكون مليئًا بمعلمات النموذج المستفادة من البيانات.

119
00:09:02,240 --> 00:09:04,560
ويُعرف هذا المتجه الآخر باسم التحيز.

120
00:09:05,180 --> 00:09:10,212
بالنسبة لمثالنا، أريدك أن تتخيل أن قيمة هذا التحيز في هذا المكون الأول هي سالبة 

121
00:09:10,212 --> 00:09:15,560
واحد، مما يعني أن ناتجنا النهائي يبدو مثل حاصل الضرب النقطي ذي الصلة، ولكن ناقص واحد.

122
00:09:16,120 --> 00:09:21,394
قد تسأل بشكل معقول جدًا لماذا أريدك أن تفترض أن النموذج قد تعلم هذا، وفي 

123
00:09:21,394 --> 00:09:26,741
لحظة سترى لماذا يكون نظيفًا وجميلًا جدًا إذا كان لدينا قيمة هنا موجبة إذا 

124
00:09:26,741 --> 00:09:32,160
وفقط إذا قام المتجه بترميز الاسم الكامل مايكل جوردان، وإلا فهو صفر أو سلبي.

125
00:09:33,040 --> 00:09:38,049
العدد الإجمالي للصفوف في هذه المصفوفة، والذي يشبه إلى حد ما عدد الأسئلة 

126
00:09:38,049 --> 00:09:42,780
المطروحة، في حالة GPT-3، التي نتابع أرقامها، يقل قليلاً عن 50 ألفًا.

127
00:09:43,100 --> 00:09:46,640
في الواقع، إنه أربعة أضعاف عدد الأبعاد في مساحة التضمين هذه.

128
00:09:46,920 --> 00:09:47,900
هذا هو اختيار التصميم.

129
00:09:47,940 --> 00:09:52,240
يمكنك جعله أكثر، يمكنك جعله أقل، ولكن وجود مضاعفات نظيفة يميل إلى أن يكون صديقًا للأجهزة.

130
00:09:52,740 --> 00:09:55,721
نظرًا لأن هذه المصفوفة المليئة بالأوزان تنقلنا 

131
00:09:55,721 --> 00:09:59,020
إلى مساحة ذات أبعاد أعلى، فسأعطيها الاختزال W لأعلى.

132
00:09:59,020 --> 00:10:03,198
سأستمر في تسمية المتجه الذي نعالجه بـ E، ودعونا نسمي متجه 

133
00:10:03,198 --> 00:10:07,160
التحيز هذا بـ B ونضع كل ذلك مرة أخرى في الرسم التخطيطي.

134
00:10:09,180 --> 00:10:12,113
في هذه المرحلة، تكمن المشكلة في أن هذه العملية 

135
00:10:12,113 --> 00:10:15,360
خطية بحتة، ولكن اللغة هي عملية غير خطية على الإطلاق.

136
00:10:15,880 --> 00:10:19,777
إذا كان المدخل الذي نقيسه مرتفعًا بالنسبة لمايكل بالإضافة إلى جوردان، فمن 

137
00:10:19,777 --> 00:10:23,675
الضروري أيضًا أن يكون هناك إلى حد ما تأثير عليه من قبل مايكل بالإضافة إلى 

138
00:10:23,675 --> 00:10:28,100
فيلبس وأيضًا أليكسيس بالإضافة إلى جوردان، على الرغم من عدم وجود صلة بينهما مفهوميًا.

139
00:10:28,540 --> 00:10:32,000
ما تريده حقًا هو نعم أو لا بسيطة للاسم الكامل.

140
00:10:32,900 --> 00:10:37,840
والخطوة التالية هي تمرير هذا المتجه الوسيط الكبير من خلال دالة غير خطية بسيطة للغاية.

141
00:10:38,360 --> 00:10:42,199
الخيار الشائع هو الذي يأخذ كل القيم السلبية ويربطها 

142
00:10:42,199 --> 00:10:45,300
بالصفر ويترك كل القيم الإيجابية دون تغيير.

143
00:10:46,440 --> 00:10:51,266
وباستمرار تقليد التعلم العميق للأسماء المبالغ فيها، غالبًا ما تسمى 

144
00:10:51,266 --> 00:10:56,020
هذه الوظيفة البسيطة جدًا بالوحدة الخطية المصححة، أو ReLU للاختصار.

145
00:10:56,020 --> 00:10:57,880
وهذا هو شكل الرسم البياني.

146
00:10:58,300 --> 00:11:03,978
لذا، إذا أخذنا مثالنا المتخيل حيث يكون هذا الإدخال الأول للمتجه الوسيط هو واحد، إذا 

147
00:11:03,978 --> 00:11:09,859
وفقط إذا كان الاسم الكامل هو مايكل جوردان وصفر أو سلبي بخلاف ذلك، بعد تمريره عبر ReLU، 

148
00:11:09,859 --> 00:11:15,740
ينتهي بك الأمر بقيمة نظيفة للغاية حيث يتم اقتصاص جميع القيم الصفرية والسلبية إلى الصفر.

149
00:11:16,100 --> 00:11:19,780
لذا فإن هذا الناتج سيكون واحدًا للاسم الكامل مايكل جوردان وصفرًا بخلاف ذلك.

150
00:11:20,560 --> 00:11:24,120
بعبارة أخرى، فإنه يحاكي بشكل مباشر سلوك بوابة AND.

151
00:11:25,660 --> 00:11:28,839
غالبًا ما تستخدم النماذج وظيفة معدلة قليلاً تسمى JLU، 

152
00:11:28,839 --> 00:11:32,020
والتي لها نفس الشكل الأساسي، ولكنها أكثر سلاسة قليلاً.

153
00:11:32,500 --> 00:11:35,720
ولكن بالنسبة لأغراضنا، فإن الأمر يبدو أكثر نظافة إذا فكرنا فقط في ReLU.

154
00:11:36,740 --> 00:11:40,042
وأيضًا، عندما تسمع الناس يشيرون إلى الخلايا العصبية 

155
00:11:40,042 --> 00:11:42,520
للمحول، فإنهم يتحدثون عن هذه القيم هنا.

156
00:11:42,900 --> 00:11:48,690
عندما ترى صورة الشبكة العصبية الشائعة مع طبقة من النقاط ومجموعة من الخطوط المتصلة 

157
00:11:48,690 --> 00:11:54,692
بالطبقة السابقة، والتي كانت لدينا في وقت سابق من هذه السلسلة، فإن هذا يعني عادةً نقل 

158
00:11:54,692 --> 00:12:00,624
هذا المزيج من الخطوة الخطية، وضرب المصفوفة، متبوعًا ببعض الوظائف غير الخطية البسيطة 

159
00:12:00,624 --> 00:12:01,260
مثل ReLU.

160
00:12:02,500 --> 00:12:05,710
يمكنك القول أن هذه الخلية العصبية نشطة عندما تكون هذه 

161
00:12:05,710 --> 00:12:08,920
القيمة موجبة وأنها غير نشطة إذا كانت هذه القيمة صفرًا.

162
00:12:10,120 --> 00:12:12,380
الخطوة التالية تبدو مشابهة جدًا للخطوة الأولى.

163
00:12:12,560 --> 00:12:16,580
تضرب في مصفوفة كبيرة جدًا وتضيف إليها مصطلح تحيز معين.

164
00:12:16,980 --> 00:12:21,017
في هذه الحالة، يعود عدد الأبعاد في المخرجات إلى حجم 

165
00:12:21,017 --> 00:12:25,520
مساحة التضمين، لذا سأستمر وأسمي هذا مصفوفة الإسقاط السفلي.

166
00:12:26,220 --> 00:12:28,817
وهذه المرة، بدلاً من التفكير في الأمور صفاً تلو 

167
00:12:28,817 --> 00:12:31,360
الآخر، من الأفضل أن نفكر فيها عموداً تلو الآخر.

168
00:12:31,860 --> 00:12:36,343
كما ترى، هناك طريقة أخرى يمكنك من خلالها حفظ عملية ضرب المصفوفات في 

169
00:12:36,343 --> 00:12:40,826
ذهنك وهي أن تتخيل أخذ كل عمود من المصفوفة وضربه بالمصطلح المقابل في 

170
00:12:40,826 --> 00:12:45,640
المتجه الذي تتم معالجته وإضافة كل هذه الأعمدة التي تمت إعادة قياسها معًا.

171
00:12:46,840 --> 00:12:51,342
السبب في أنه من الأفضل التفكير بهذه الطريقة هو أن الأعمدة هنا لها نفس 

172
00:12:51,342 --> 00:12:55,780
أبعاد مساحة التضمين، لذا يمكننا أن نفكر فيها كاتجاهات في تلك المساحة.

173
00:12:56,140 --> 00:12:59,378
على سبيل المثال، سوف نتخيل أن النموذج تعلم كيفية 

174
00:12:59,378 --> 00:13:03,080
تحويل العمود الأول إلى اتجاه كرة السلة الذي نفترض وجوده.

175
00:13:04,180 --> 00:13:07,533
ما يعنيه هذا هو أنه عندما تكون الخلية العصبية ذات الصلة في هذا 

176
00:13:07,533 --> 00:13:10,780
الموضع الأول نشطة، فسوف نضيف هذا العمود إلى النتيجة النهائية.

177
00:13:11,140 --> 00:13:13,560
ولكن إذا كانت تلك الخلية العصبية غير نشطة، وإذا 

178
00:13:13,560 --> 00:13:15,780
كان هذا العدد صفرًا، فلن يكون لهذا أي تأثير.

179
00:13:16,500 --> 00:13:18,060
ولا يجب أن يقتصر الأمر على كرة السلة فقط.

180
00:13:18,220 --> 00:13:21,826
يمكن أن يتكامل النموذج أيضًا مع هذا العمود والعديد من الميزات 

181
00:13:21,826 --> 00:13:25,200
الأخرى التي يريد ربطها بشيء يحمل الاسم الكامل مايكل جوردن.

182
00:13:26,980 --> 00:13:31,643
وفي الوقت نفسه، فإن جميع الأعمدة الأخرى في هذه المصفوفة تخبرك بما 

183
00:13:31,643 --> 00:13:36,660
سيتم إضافته إلى النتيجة النهائية إذا كانت الخلية العصبية المقابلة نشطة.

184
00:13:37,360 --> 00:13:40,563
وإذا كان لديك تحيز في هذه الحالة، فهو شيء تضيفه 

185
00:13:40,563 --> 00:13:43,500
في كل مرة، بغض النظر عن قيم الخلايا العصبية.

186
00:13:44,060 --> 00:13:45,280
ربما تتساءل ماذا يفعل هذا؟

187
00:13:45,540 --> 00:13:47,354
كما هو الحال مع جميع الكائنات المليئة بالمعلمات 

188
00:13:47,354 --> 00:13:49,320
هنا، فمن الصعب نوعًا ما أن نقول ذلك على وجه التحديد.

189
00:13:49,320 --> 00:13:54,380
ربما هناك بعض الأعمال المحاسبية التي تحتاج الشبكة إلى القيام بها، ولكن يمكنك تجاهلها الآن.

190
00:13:54,860 --> 00:13:59,437
ولكي نجعل تدويننا أكثر إحكاما مرة أخرى، فسوف أطلق على هذه المصفوفة الكبيرة 

191
00:13:59,437 --> 00:14:04,260
اسم W، وبالمثل سأسمي متجه التحيز B، وأضعه مرة أخرى في الرسم التخطيطي الخاص بنا.

192
00:14:04,740 --> 00:14:08,873
كما قمت بمعاينته سابقًا، ما تفعله بهذه النتيجة النهائية هو إضافتها إلى 

193
00:14:08,873 --> 00:14:13,240
المتجه الذي تدفق إلى الكتلة في هذا الموضع، وهذا يعطيك هذه النتيجة النهائية.

194
00:14:13,820 --> 00:14:19,160
على سبيل المثال، إذا كان المتجه المتدفق في ترميز كل من الاسم الأول مايكل والاسم 

195
00:14:19,160 --> 00:14:24,433
الأخير جوردان، فبما أن هذه السلسلة من العمليات ستؤدي إلى تشغيل بوابة AND، فسوف 

196
00:14:24,433 --> 00:14:29,240
تضيف اتجاه كرة السلة، وبالتالي فإن ما يخرج سوف يشفر كل هذه العناصر معًا.

197
00:14:29,820 --> 00:14:34,200
وتذكر أن هذه العملية تحدث لكل واحد من هذه المتجهات بالتوازي.

198
00:14:34,800 --> 00:14:39,897
على وجه الخصوص، إذا أخذنا أرقام GPT-3، فهذا يعني أن هذه الكتلة لا تحتوي فقط 

199
00:14:39,897 --> 00:14:44,860
على 50000 خلية عصبية، بل تحتوي أيضًا على 50000 ضعف عدد الرموز في المدخلات.

200
00:14:48,180 --> 00:14:55,180
وهذه هي العملية برمتها، منتجان لمصفوفة، كل منهما مع تحيز مضاف ووظيفة قص بسيطة بينهما.

201
00:14:56,080 --> 00:14:59,469
أي منكم شاهد مقاطع الفيديو السابقة من السلسلة سوف يتعرف على هذا الهيكل 

202
00:14:59,469 --> 00:15:02,620
باعتباره النوع الأكثر أساسية من الشبكات العصبية التي درسناها هناك.

203
00:15:03,080 --> 00:15:06,100
في هذا المثال، تم تدريبه على التعرف على الأرقام المكتوبة بخط اليد.

204
00:15:06,580 --> 00:15:14,880
هنا، في سياق المحول لنموذج لغة كبير، هذا جزء واحد في بنية أكبر وأي محاولة لتفسير ما يفعله 

205
00:15:14,880 --> 00:15:23,180
بالضبط ترتبط ارتباطًا وثيقًا بفكرة ترميز المعلومات في متجهات من مساحة تضمين عالية الأبعاد.

206
00:15:24,260 --> 00:15:29,006
هذا هو الدرس الأساسي، ولكنني أريد أن أتراجع وأفكر في شيئين مختلفين، 

207
00:15:29,006 --> 00:15:33,473
الأول هو نوع من المحاسبة، والثاني يتضمن حقيقة مثيرة للتفكير حول 

208
00:15:33,473 --> 00:15:38,080
الأبعاد العليا التي لم أكن أعرفها في الواقع حتى تعمقت في المحولات.

209
00:15:41,080 --> 00:15:45,992
في الفصلين الأخيرين، بدأنا أنا وأنت في حساب العدد الإجمالي للمعلمات 

210
00:15:45,992 --> 00:15:50,760
في GPT-3 ورؤية مكان وجودها بالضبط، لذا دعنا ننهي اللعبة بسرعة هنا.

211
00:15:51,400 --> 00:15:56,529
لقد ذكرت بالفعل كيف أن مصفوفة الإسقاط هذه تحتوي على ما يقل قليلاً عن 

212
00:15:56,529 --> 00:16:02,180
50000 صف وأن كل صف يطابق حجم مساحة التضمين، والتي بالنسبة لـ GPT-3 هي 12288.

213
00:16:03,240 --> 00:16:08,536
وبضرب هذه القيم معًا، نحصل على 604 مليون معلمة لتلك المصفوفة 

214
00:16:08,536 --> 00:16:13,920
فقط، والإسقاط السفلي له نفس عدد المعلمات فقط مع الشكل المنقول.

215
00:16:14,500 --> 00:16:17,400
وبالتالي، فإنها مجتمعة تعطي حوالي 1.2 مليار معلمة.

216
00:16:18,280 --> 00:16:21,022
يأخذ متجه التحيز أيضًا بعين الاعتبار بضعة معلمات 

217
00:16:21,022 --> 00:16:24,100
أخرى، ولكنها نسبة ضئيلة من الإجمالي، لذا لن أعرضها حتى.

218
00:16:24,660 --> 00:16:31,284
في GPT-3، يتدفق تسلسل متجهات التضمين هذا ليس من خلال كتلة واحدة، بل من خلال 96 كتلة MLP 

219
00:16:31,284 --> 00:16:38,060
مميزة، وبالتالي فإن العدد الإجمالي للمعلمات المخصصة لكل هذه الكتل يصل إلى حوالي 116 مليار.

220
00:16:38,820 --> 00:16:43,086
وهذا يمثل حوالي ثلثي إجمالي المعلمات في الشبكة، وعندما تضيفه إلى كل 

221
00:16:43,086 --> 00:16:47,478
ما كان لدينا من قبل، بالنسبة لكتل الانتباه، والتضمين، وإلغاء التضمين، 

222
00:16:47,478 --> 00:16:51,620
فإنك تحصل بالفعل على المجموع الكلي البالغ 175 مليارًا كما هو معلن.

223
00:16:53,060 --> 00:16:58,450
ربما يجدر بنا أن نذكر أن هناك مجموعة أخرى من المعلمات المرتبطة بخطوات التطبيع التي تم 

224
00:16:58,450 --> 00:17:03,840
تخطيها في هذا التفسير، ولكن مثل متجه التحيز، فإنها تمثل نسبة ضئيلة للغاية من الإجمالي.

225
00:17:05,900 --> 00:17:10,676
أما فيما يتعلق بنقطة التأمل الثانية، فقد تتساءل عما إذا كان هذا المثال المركزي الذي 

226
00:17:10,676 --> 00:17:15,680
أمضينا الكثير من الوقت عليه يعكس كيفية تخزين الحقائق فعليًا في نماذج لغوية كبيرة حقيقية.

227
00:17:16,319 --> 00:17:21,858
من الصحيح أن صفوف تلك المصفوفة الأولى يمكن اعتبارها اتجاهات في مساحة التضمين 

228
00:17:21,858 --> 00:17:27,540
هذه، وهذا يعني أن تنشيط كل خلية عصبية يخبرك بمدى توافق متجه معين مع اتجاه معين.

229
00:17:27,760 --> 00:17:31,050
ومن الصحيح أيضًا أن أعمدة تلك المصفوفة الثانية تخبرك بما 

230
00:17:31,050 --> 00:17:34,340
سيتم إضافته إلى النتيجة إذا كانت تلك الخلية العصبية نشطة.

231
00:17:34,640 --> 00:17:36,800
كلتاهما مجرد حقائق رياضية.

232
00:17:37,740 --> 00:17:42,997
ومع ذلك، تشير الأدلة إلى أن الخلايا العصبية الفردية نادراً ما تمثل ميزة واحدة 

233
00:17:42,997 --> 00:17:48,457
نظيفة مثل مايكل جوردان، وقد يكون هناك في الواقع سبب وجيه للغاية لكون الأمر كذلك، 

234
00:17:48,457 --> 00:17:54,120
وهو مرتبط بفكرة تطفو بين الباحثين في مجال التفسير هذه الأيام والمعروفة باسم التراكب.

235
00:17:54,640 --> 00:17:58,606
هذه فرضية قد تساعد في تفسير سبب صعوبة تفسير النماذج 

236
00:17:58,606 --> 00:18:02,420
بشكل خاص ولماذا يمكن قياسها بشكل جيد على نحو مدهش.

237
00:18:03,500 --> 00:18:08,359
الفكرة الأساسية هي أنه إذا كان لديك مساحة ذات n أبعاد وتريد تمثيل مجموعة من 

238
00:18:08,359 --> 00:18:13,346
الميزات المختلفة باستخدام اتجاهات متعامدة مع بعضها البعض في تلك المساحة، فأنت 

239
00:18:13,346 --> 00:18:18,653
تعلم، بهذه الطريقة إذا أضفت مكونًا في اتجاه واحد، فلن يؤثر ذلك على أي من الاتجاهات 

240
00:18:18,653 --> 00:18:23,960
الأخرى، ثم الحد الأقصى لعدد المتجهات التي يمكنك ملاءمتها هو n فقط، وهو عدد الأبعاد.

241
00:18:24,600 --> 00:18:27,620
بالنسبة لعالم الرياضيات، هذا هو في الواقع تعريف البعد.

242
00:18:28,220 --> 00:18:33,580
لكن الأمر يصبح مثيرا للاهتمام إذا خففت من هذا القيد قليلا وتحمّلت بعض الضوضاء.

243
00:18:34,180 --> 00:18:38,827
لنفترض أنك تسمح بتمثيل هذه الميزات بواسطة متجهات ليست 

244
00:18:38,827 --> 00:18:43,820
عمودية تمامًا، بل هي عمودية تقريبًا، ربما بين 89 و91 درجة.

245
00:18:44,820 --> 00:18:48,020
لو كنا في بعدين أو ثلاثة أبعاد، فهذا لا يشكل أي فرق.

246
00:18:48,260 --> 00:18:52,610
وهذا لا يمنحك أي مساحة إضافية لتناسب المزيد من المتجهات، مما يجعل الأمر 

247
00:18:52,610 --> 00:18:56,780
أكثر مخالفة للحدس حيث تتغير الإجابة بشكل كبير بالنسبة للأبعاد الأعلى.

248
00:18:57,660 --> 00:19:03,193
يمكنني أن أقدم لك مثالًا سريعًا جدًا ومباشرًا لهذا باستخدام بعض Python المتناثر 

249
00:19:03,193 --> 00:19:08,796
الذي سيُنشئ قائمة من المتجهات ذات 100 بُعد، كل منها يتم تهيئته عشوائيًا، وستحتوي 

250
00:19:08,796 --> 00:19:14,400
هذه القائمة على 10000 متجه مميز، أي 100 مرة عدد المتجهات كما هو الحال في الأبعاد.

251
00:19:15,320 --> 00:19:19,900
يوضح الرسم البياني هنا توزيع الزوايا بين أزواج هذه المتجهات.

252
00:19:20,680 --> 00:19:24,301
وبما أنهم بدأوا بشكل عشوائي، فإن هذه الزوايا يمكن أن تكون أي 

253
00:19:24,301 --> 00:19:28,160
شيء من 0 إلى 180 درجة، ولكنك ستلاحظ بالفعل، حتى بالنسبة للمتجهات 

254
00:19:28,160 --> 00:19:31,960
العشوائية، أن هناك تحيزًا كبيرًا للأشياء لتكون أقرب إلى 90 درجة.

255
00:19:32,500 --> 00:19:37,045
ثم ما سأفعله هو تشغيل عملية تحسين معينة تعمل بشكل متكرر على دفع 

256
00:19:37,045 --> 00:19:41,520
كل هذه المتجهات بحيث تحاول أن تصبح أكثر عمودية على بعضها البعض.

257
00:19:42,060 --> 00:19:46,660
بعد تكرار ذلك عدة مرات مختلفة، هذا هو شكل توزيع الزوايا.

258
00:19:47,120 --> 00:19:52,127
يتعين علينا في الواقع تكبير الصورة هنا لأن جميع الزوايا الممكنة 

259
00:19:52,127 --> 00:19:56,900
بين أزواج المتجهات تقع داخل هذا النطاق الضيق بين 89 و91 درجة.

260
00:19:58,020 --> 00:20:04,392
بشكل عام، إحدى نتائج ما يُعرف باسم مبرهنة جونسون-ليندنشتراوس هي أن عدد المتجهات التي 

261
00:20:04,392 --> 00:20:10,840
يمكنك حشرها في فضاء، والتي تكون عمودية تقريبًا مثل هذا، تنمو بشكل كبير مع عدد الأبعاد.

262
00:20:11,960 --> 00:20:15,884
وهذا مهم للغاية بالنسبة لنماذج اللغة الكبيرة، والتي قد 

263
00:20:15,884 --> 00:20:19,880
تستفيد من ربط الأفكار المستقلة باتجاهات متعامدة تقريبًا.

264
00:20:20,000 --> 00:20:26,440
وهذا يعني أنه من الممكن تخزين أفكار كثيرة جدًا تفوق أبعاد المساحة المخصصة لها.

265
00:20:27,320 --> 00:20:31,740
قد يفسر هذا جزئيًا سبب ارتباط أداء النموذج بشكل جيد بالحجم.

266
00:20:32,540 --> 00:20:35,936
إن المساحة التي تحتوي على 10 أضعاف الأبعاد يمكن أن 

267
00:20:35,936 --> 00:20:39,400
تخزن ما يزيد بكثير عن 10 أضعاف عدد الأفكار المستقلة.

268
00:20:40,420 --> 00:20:45,401
وهذا ينطبق ليس فقط على مساحة التضمين حيث تعيش المتجهات المتدفقة عبر النموذج، ولكن أيضًا 

269
00:20:45,401 --> 00:20:50,440
على هذا المتجه المليء بالخلايا العصبية في منتصف الإدراك المتعدد الطبقات الذي درسناه للتو.

270
00:20:50,960 --> 00:20:56,449
وهذا يعني أنه عند أحجام GPT-3، قد لا يكون قادرًا على استكشاف 50 ألف ميزة فقط، ولكن إذا 

271
00:20:56,449 --> 00:21:01,876
استفاد بدلاً من ذلك من هذه القدرة الإضافية الهائلة عن طريق استخدام اتجاهات شبه عمودية 

272
00:21:01,876 --> 00:21:07,240
للمساحة، فقد يكون قادرًا على استكشاف العديد والعديد من ميزات المتجه الذي تتم معالجته.

273
00:21:07,780 --> 00:21:11,216
ولكن إذا كان الأمر كذلك، فإن ما يعنيه هذا هو أن السمات 

274
00:21:11,216 --> 00:21:14,340
الفردية لن تكون مرئية عندما تضيء خلية عصبية واحدة.

275
00:21:14,660 --> 00:21:19,380
سيتعين أن يبدو الأمر وكأنه مزيج محدد من الخلايا العصبية بدلاً من ذلك، أي تراكب.

276
00:21:20,400 --> 00:21:24,542
بالنسبة لأي منكم فضولي لمعرفة المزيد، فإن مصطلح البحث الرئيسي ذي الصلة هنا هو 

277
00:21:24,542 --> 00:21:28,737
sparse autoencoder، وهي أداة يستخدمها بعض الأشخاص المتخصصين في التفسير لمحاولة 

278
00:21:28,737 --> 00:21:32,880
استخراج الميزات الحقيقية، حتى لو كانت متراكبة جدًا على كل هذه الخلايا العصبية.

279
00:21:33,540 --> 00:21:36,800
سأضع رابطًا لبعض المنشورات الأنثروبية الرائعة حول هذا الموضوع.

280
00:21:37,880 --> 00:21:43,300
في هذه المرحلة، لم نلمس كل تفاصيل المحول، لكننا وصلنا إلى النقاط الأكثر أهمية.

281
00:21:43,520 --> 00:21:47,640
الشيء الرئيسي الذي أريد تغطيته في الفصل التالي هو عملية التدريب.

282
00:21:48,460 --> 00:21:52,779
من ناحية أخرى، فإن الإجابة المختصرة لكيفية عمل التدريب هي أن الأمر كله يتعلق بالانتشار 

283
00:21:52,779 --> 00:21:56,900
الخلفي، وقد قمنا بتغطية الانتشار الخلفي في سياق منفصل في الفصول السابقة في السلسلة.

284
00:21:57,220 --> 00:22:02,628
ولكن هناك المزيد لمناقشته، مثل دالة التكلفة المحددة المستخدمة في نماذج اللغة، وفكرة 

285
00:22:02,628 --> 00:22:07,780
الضبط الدقيق باستخدام التعلم المعزز مع ردود الفعل البشرية، ومفهوم قوانين التوسع.

286
00:22:08,960 --> 00:22:12,419
ملاحظة سريعة للمتابعين النشطين بينكم، هناك عدد من مقاطع الفيديو غير 

287
00:22:12,419 --> 00:22:15,980
المرتبطة بالتعلم الآلي والتي أنا متحمس للتعمق فيها قبل أن أقوم بالفصل 

288
00:22:15,980 --> 00:22:20,000
التالي، لذلك قد يستغرق الأمر بعض الوقت، ولكنني أعدك أنه سيأتي في الوقت المناسب.

289
00:22:35,640 --> 00:22:37,920
شكرًا لك.

