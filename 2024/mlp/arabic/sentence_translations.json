[
 {
  "translatedText": "إذا قمت بإطعام نموذج لغوي كبير العبارة &quot;مايكل جوردن يلعب كرة السلة&quot;، وجعلته يتنبأ بما سيأتي بعد ذلك، ويتنبأ بشكل صحيح بكرة السلة، فهذا يشير إلى أنه في مكان ما، داخل مئات المليارات من معلماته، هناك معلومات مدمجة حول شخص معين ورياضته المحددة.",
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "translatedText": "وأعتقد بشكل عام أن أي شخص لعب مع أحد هذه النماذج لديه إحساس واضح بأنه حفظ الكثير والكثير من الحقائق.",
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "translatedText": "لذا فإن السؤال المعقول الذي يمكنك طرحه هو، كيف يعمل هذا بالضبط؟",
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "translatedText": "وأين توجد تلك الحقائق؟",
  "input": "And where do those facts live?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "translatedText": "في ديسمبر/كانون الأول الماضي، نشر عدد من الباحثين من Google DeepMind أعمالاً حول هذا السؤال، وكانوا يستخدمون هذا المثال المحدد لمطابقة الرياضيين مع رياضاتهم.",
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "translatedText": "وعلى الرغم من أن الفهم الميكانيكي الكامل لكيفية تخزين الحقائق لا يزال دون حل، فقد توصلوا إلى بعض النتائج الجزئية المثيرة للاهتمام، بما في ذلك الاستنتاج العام للغاية على مستوى عالٍ بأن الحقائق يبدو أنها تعيش داخل جزء محدد من هذه الشبكات، والمعروف بشكل خيالي باسم المدركات متعددة الطبقات، أو MLPs باختصار.",
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "translatedText": "في الفصول القليلة الماضية، كنا أنا وأنت نبحث في التفاصيل وراء المحولات، والهندسة المعمارية التي تقوم عليها نماذج اللغة الكبيرة، والتي تقوم عليها أيضًا العديد من أشكال الذكاء الاصطناعي الحديثة الأخرى.",
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "translatedText": "في الفصل الأخير، ركزنا على قطعة تسمى &quot;الانتباه&quot;.",
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "translatedText": "والخطوة التالية بالنسبة لي ولك هي التعمق في تفاصيل ما يحدث داخل هذه المدركات متعددة الطبقات، والتي تشكل الجزء الكبير الآخر من الشبكة.",
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "translatedText": "الحساب هنا بسيط نسبيًا في الواقع، خاصة عند مقارنته بالانتباه.",
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "translatedText": "يمكن تلخيص الأمر بشكل أساسي في زوج من مضاعفات المصفوفة مع وجود شيء بسيط بينهما.",
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "translatedText": "ومع ذلك، فإن تفسير ما تفعله هذه الحسابات يعد تحديًا كبيرًا للغاية.",
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "translatedText": "هدفنا الرئيسي هنا هو التقدم عبر العمليات الحسابية وجعلها لا تنسى، ولكنني أود أن أفعل ذلك في سياق إظهار مثال محدد حول كيف يمكن لأحد هذه الكتل، على الأقل من حيث المبدأ، تخزين حقيقة ملموسة.",
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "translatedText": "على وجه التحديد، سيتم تخزين حقيقة أن مايكل جوردن يلعب كرة السلة.",
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "translatedText": "ينبغي لي أن أذكر أن التصميم هنا مستوحى من محادثة أجريتها مع أحد الباحثين في DeepMind، نيل ناندا.",
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "translatedText": "في الغالب، سأفترض أنك إما شاهدت الفصلين الأخيرين، أو أن لديك حسًا أساسيًا حول ما هو المحول، ولكن التذكير لا يضر أبدًا، لذا إليك تذكير سريع بالتدفق العام.",
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "translatedText": "لقد قمنا أنا وأنت بدراسة نموذج تم تدريبه على استيعاب جزء من النص والتنبؤ بما سيأتي بعد ذلك.",
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "translatedText": "يتم تقسيم نص الإدخال أولاً إلى مجموعة من الرموز، وهو ما يعني أجزاء صغيرة تتكون عادةً من كلمات أو قطع صغيرة من الكلمات، ويرتبط كل رمز بمتجه عالي الأبعاد، وهو ما يمكن أن نقول عنه قائمة طويلة من الأرقام.",
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "translatedText": "ثم تمر هذه السلسلة من المتجهات بشكل متكرر عبر نوعين من العمليات، الانتباه، الذي يسمح للمتجهات بنقل المعلومات بين بعضها البعض، ثم المُدْرِكات متعددة الطبقات، وهو الشيء الذي سنتعمق فيه اليوم، وهناك أيضًا خطوة تطبيع معينة بينهما.",
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "translatedText": "بعد أن تتدفق سلسلة المتجهات عبر العديد والعديد من التكرارات المختلفة لكلا الكتلتين، في النهاية، يكون الأمل هو أن كل متجه قد استوعب معلومات كافية، سواء من السياق، أو من جميع الكلمات الأخرى في المدخلات، وكذلك من المعرفة العامة التي تم تضمينها في أوزان النموذج من خلال التدريب، بحيث يمكن استخدامها للتنبؤ بالرمز الذي سيأتي بعد ذلك.",
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "translatedText": "أحد الأفكار الرئيسية التي أريدكم أن تخطروا على بالكم هي أن كل هذه المتجهات تعيش في فضاء ذي أبعاد عالية جدًا، وعندما تفكرون في هذا الفضاء، فإن الاتجاهات المختلفة يمكن أن تشفر أنواعًا مختلفة من المعنى.",
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "translatedText": "لذا فإن أحد الأمثلة الكلاسيكية التي أحب الرجوع إليها هو كيف إذا نظرت إلى تضمين كلمة woman وطرحت تضمين كلمة man، واتخذت هذه الخطوة الصغيرة وأضفتها إلى اسم مذكر آخر، مثل العم، فإنك تهبط في مكان قريب جدًا جدًا من الاسم المؤنث المقابل.",
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "translatedText": "وبهذا المعنى، فإن هذا الاتجاه المحدد يشفر معلومات الجنس.",
  "input": "In this sense, this particular direction encodes gender information.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "translatedText": "الفكرة هي أن العديد من الاتجاهات الأخرى المتميزة في هذه المساحة عالية الأبعاد قد تتوافق مع ميزات أخرى قد يرغب النموذج في تمثيلها.",
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "translatedText": "في المحول، لا تقوم هذه المتجهات بتشفير معنى كلمة واحدة فحسب.",
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "translatedText": "وبينما تتدفق هذه العناصر عبر الشبكة، فإنها تستوعب معنى أكثر ثراءً استنادًا إلى كل السياق المحيط بها، وكذلك استنادًا إلى معرفة النموذج.",
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "translatedText": "في نهاية المطاف، يحتاج كل واحد منهم إلى تشفير شيء ما أبعد بكثير من معنى كلمة واحدة، حيث يجب أن يكون ذلك كافياً للتنبؤ بما سيأتي بعد ذلك.",
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "translatedText": "لقد رأينا بالفعل كيف تسمح لك كتل الانتباه بتضمين السياق، ولكن غالبية معلمات النموذج موجودة بالفعل داخل كتل MLP، وأحد الأفكار حول ما قد تفعله هو أنها توفر سعة إضافية لتخزين الحقائق.",
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "translatedText": "كما قلت، الدرس هنا سوف يركز على مثال اللعبة الملموسة لكيفية تخزين حقيقة أن مايكل جوردن يلعب كرة السلة.",
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "translatedText": "الآن، هذا المثال البسيط سيتطلب مني ومنك أن نقوم ببعض الافتراضات حول تلك المساحة عالية الأبعاد.",
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "translatedText": "أولاً، سنفترض أن أحد الاتجاهات يمثل فكرة الاسم الأول مايكل، ثم يمثل اتجاه آخر عمودي تقريبًا فكرة الاسم الأخير جوردان، ثم يمثل اتجاه ثالث فكرة كرة السلة.",
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "translatedText": "لذلك على وجه التحديد، ما أقصده بهذا هو إذا نظرت في الشبكة واخترت أحد المتجهات التي تتم معالجتها، إذا كان حاصل ضربه النقطي مع اتجاه الاسم الأول مايكل هو واحد، فهذا ما يعنيه أن المتجه يشفر فكرة الشخص بهذا الاسم الأول.",
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "translatedText": "وإلا فإن حاصل الضرب النقطي سيكون صفرًا أو سالبًا، مما يعني أن المتجه لا يتوافق حقًا مع هذا الاتجاه.",
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "translatedText": "ومن أجل التبسيط، دعونا نتجاهل تمامًا السؤال المعقول للغاية حول ما قد يعنيه إذا كان حاصل الضرب النقطي أكبر من واحد.",
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "translatedText": "وبالمثل، فإن حاصل ضرب النقاط مع هذه الاتجاهات الأخرى سيخبرك ما إذا كان يمثل الاسم الأخير جوردان أو كرة سلة.",
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "translatedText": "لذا، لنفترض أن المتجه من المفترض أن يمثل الاسم الكامل، مايكل جوردان، فإن حاصل ضربه النقطي في كلا الاتجاهين يجب أن يساوي واحدًا.",
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "translatedText": "نظرًا لأن النص &quot;مايكل جوردان&quot; يمتد على رمزين مختلفين، فهذا يعني أيضًا أنه يتعين علينا افتراض أن كتلة انتباه سابقة قد نقلت المعلومات بنجاح إلى الثاني من هذين المتجهين لضمان قدرتها على ترميز كلا الاسمين.",
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "translatedText": "مع كل هذه الافتراضات، دعونا الآن ننتقل إلى صلب الدرس.",
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "translatedText": "ماذا يحدث داخل الإدراك المتعدد الطبقات؟",
  "input": "What happens inside a multilayer perceptron?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "translatedText": "قد تفكر في هذا التسلسل من المتجهات المتدفقة إلى الكتلة، وتذكر أن كل متجه كان مرتبطًا في الأصل بأحد الرموز من النص المدخل.",
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "translatedText": "ما سيحدث هو أن كل متجه فردي من هذا التسلسل سيخضع لسلسلة قصيرة من العمليات، وسنقوم بتفكيكها في لحظة، وفي النهاية، سنحصل على متجه آخر بنفس البعد.",
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "translatedText": "سيتم إضافة المتجه الآخر إلى المتجه الأصلي الذي تدفق للداخل، وهذا المجموع هو النتيجة المتدفقة للخارج.",
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "translatedText": "هذا التسلسل من العمليات هو شيء يمكنك تطبيقه على كل متجه في التسلسل، ويرتبط بكل رمز في الإدخال، ويحدث كل ذلك بالتوازي.",
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "translatedText": "على وجه الخصوص، لا تتحدث المتجهات مع بعضها البعض في هذه الخطوة، فكل منها يقوم بعمله الخاص.",
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "translatedText": "وبالنسبة لي ولك، هذا يجعل الأمر في الواقع أبسط بكثير، لأنه يعني أنه إذا فهمنا ما يحدث لمتجه واحد فقط من المتجهات عبر هذه الكتلة، فإننا نفهم فعليًا ما يحدث لجميعهم.",
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "translatedText": "عندما أقول أن هذه الكتلة سوف تشفر حقيقة أن مايكل جوردان يلعب كرة السلة، ما أعنيه هو أنه إذا تدفق متجه يشفر الاسم الأول مايكل والاسم الأخير جوردان، فإن تسلسل العمليات الحسابية هذا سوف ينتج شيئًا يتضمن اتجاه كرة السلة، وهو ما سيضاف إلى المتجه في هذا الموضع.",
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "translatedText": "تبدو الخطوة الأولى في هذه العملية مثل ضرب هذا المتجه بمصفوفة كبيرة جدًا.",
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "translatedText": "لا مفاجآت هنا، هذا هو التعلم العميق.",
  "input": "No surprises there, this is deep learning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "translatedText": "وهذه المصفوفة، مثل كل المصفوفات الأخرى التي رأيناها، مليئة بمعلمات النموذج التي تم تعلمها من البيانات، والتي يمكنك أن تفكر فيها كمجموعة من الأزرار والمفاتيح التي يتم تعديلها وضبطها لتحديد سلوك النموذج.",
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "translatedText": "الآن، إحدى الطرق الجيدة للتفكير في عملية ضرب المصفوفة هي تخيل كل صف من تلك المصفوفة باعتباره متجهًا خاصًا به، وأخذ مجموعة من المنتجات النقطية بين تلك الصفوف والمتجه الذي تتم معالجته، والذي سأسميه E للتضمين.",
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "translatedText": "على سبيل المثال، افترض أن الصف الأول يساوي اتجاه الاسم الأول مايكل الذي نفترض وجوده.",
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "translatedText": "وهذا يعني أن المكون الأول في هذا الإخراج، وهو حاصل الضرب النقطي هنا، سيكون واحدًا إذا كان هذا المتجه يشفر الاسم الأول مايكل، وصفرًا أو سالبًا بخلاف ذلك.",
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "translatedText": "الأمر الأكثر متعة هو أن تأخذ لحظة للتفكير فيما قد يعنيه إذا كان هذا الصف الأول هو اتجاه الاسم الأول مايكل بالإضافة إلى الاسم الأخير جوردان.",
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "translatedText": "ومن أجل التبسيط، دعني أبدأ وأكتب ذلك على هيئة M زائد J.",
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "translatedText": "بعد ذلك، عند أخذ حاصل نقطي مع هذا التضمين E، تتوزع الأشياء بشكل جيد حقًا، فيبدو الأمر مثل M نقطة E زائد J نقطة E.",
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "translatedText": "ولاحظ كيف يعني هذا أن القيمة النهائية ستكون اثنين إذا كان المتجه يشفر الاسم الكامل لمايكل جوردان، وإلا فإنها ستكون واحدًا أو شيئًا أصغر من واحد.",
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "translatedText": "وهذا مجرد صف واحد في هذه المصفوفة.",
  "input": "And that's just one row in this matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "translatedText": "يمكنك أن تفكر في جميع الصفوف الأخرى على التوازي، وتطرح بعض الأنواع الأخرى من الأسئلة، وتستكشف بعض الأنواع الأخرى من ميزات المتجه الذي تتم معالجته.",
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "translatedText": "في كثير من الأحيان تتضمن هذه الخطوة أيضًا إضافة متجه آخر إلى المخرجات، والذي يكون مليئًا بمعلمات النموذج المستفادة من البيانات.",
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "translatedText": "ويُعرف هذا المتجه الآخر باسم التحيز.",
  "input": "This other vector is known as the bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "translatedText": "بالنسبة لمثالنا، أريدك أن تتخيل أن قيمة هذا التحيز في هذا المكون الأول هي سالبة واحد، مما يعني أن ناتجنا النهائي يبدو مثل حاصل الضرب النقطي ذي الصلة، ولكن ناقص واحد.",
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "translatedText": "قد تسأل بشكل معقول جدًا لماذا أريدك أن تفترض أن النموذج قد تعلم هذا، وفي لحظة سترى لماذا يكون نظيفًا وجميلًا جدًا إذا كان لدينا قيمة هنا موجبة إذا وفقط إذا قام المتجه بترميز الاسم الكامل مايكل جوردان، وإلا فهو صفر أو سلبي.",
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "translatedText": "العدد الإجمالي للصفوف في هذه المصفوفة، والذي يشبه إلى حد ما عدد الأسئلة المطروحة، في حالة GPT-3، التي نتابع أرقامها، يقل قليلاً عن 50 ألفًا.",
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "translatedText": "في الواقع، إنه أربعة أضعاف عدد الأبعاد في مساحة التضمين هذه.",
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "translatedText": "هذا هو اختيار التصميم.",
  "input": "That's a design choice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "translatedText": "يمكنك جعله أكثر، يمكنك جعله أقل، ولكن وجود مضاعفات نظيفة يميل إلى أن يكون صديقًا للأجهزة.",
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "translatedText": "نظرًا لأن هذه المصفوفة المليئة بالأوزان تنقلنا إلى مساحة ذات أبعاد أعلى، فسأعطيها الاختزال W لأعلى.",
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "translatedText": "سأستمر في تسمية المتجه الذي نعالجه بـ E، ودعونا نسمي متجه التحيز هذا بـ B ونضع كل ذلك مرة أخرى في الرسم التخطيطي.",
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "translatedText": "في هذه المرحلة، تكمن المشكلة في أن هذه العملية خطية بحتة، ولكن اللغة هي عملية غير خطية على الإطلاق.",
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "translatedText": "إذا كان المدخل الذي نقيسه مرتفعًا بالنسبة لمايكل بالإضافة إلى جوردان، فمن الضروري أيضًا أن يكون هناك إلى حد ما تأثير عليه من قبل مايكل بالإضافة إلى فيلبس وأيضًا أليكسيس بالإضافة إلى جوردان، على الرغم من عدم وجود صلة بينهما مفهوميًا.",
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "translatedText": "ما تريده حقًا هو نعم أو لا بسيطة للاسم الكامل.",
  "input": "What you really want is a simple yes or no for the full name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "translatedText": "والخطوة التالية هي تمرير هذا المتجه الوسيط الكبير من خلال دالة غير خطية بسيطة للغاية.",
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "translatedText": "الخيار الشائع هو الذي يأخذ كل القيم السلبية ويربطها بالصفر ويترك كل القيم الإيجابية دون تغيير.",
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "translatedText": "وباستمرار تقليد التعلم العميق للأسماء المبالغ فيها، غالبًا ما تسمى هذه الوظيفة البسيطة جدًا بالوحدة الخطية المصححة، أو ReLU للاختصار.",
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "translatedText": "وهذا هو شكل الرسم البياني.",
  "input": "Here's what the graph looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "translatedText": "لذا، إذا أخذنا مثالنا المتخيل حيث يكون هذا الإدخال الأول للمتجه الوسيط هو واحد، إذا وفقط إذا كان الاسم الكامل هو مايكل جوردان وصفر أو سلبي بخلاف ذلك، بعد تمريره عبر ReLU، ينتهي بك الأمر بقيمة نظيفة للغاية حيث يتم اقتصاص جميع القيم الصفرية والسلبية إلى الصفر.",
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "translatedText": "لذا فإن هذا الناتج سيكون واحدًا للاسم الكامل مايكل جوردان وصفرًا بخلاف ذلك.",
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "translatedText": "بعبارة أخرى، فإنه يحاكي بشكل مباشر سلوك بوابة AND.",
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "translatedText": "غالبًا ما تستخدم النماذج وظيفة معدلة قليلاً تسمى JLU، والتي لها نفس الشكل الأساسي، ولكنها أكثر سلاسة قليلاً.",
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "translatedText": "ولكن بالنسبة لأغراضنا، فإن الأمر يبدو أكثر نظافة إذا فكرنا فقط في ReLU.",
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "translatedText": "وأيضًا، عندما تسمع الناس يشيرون إلى الخلايا العصبية للمحول، فإنهم يتحدثون عن هذه القيم هنا.",
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "translatedText": "عندما ترى صورة الشبكة العصبية الشائعة مع طبقة من النقاط ومجموعة من الخطوط المتصلة بالطبقة السابقة، والتي كانت لدينا في وقت سابق من هذه السلسلة، فإن هذا يعني عادةً نقل هذا المزيج من الخطوة الخطية، وضرب المصفوفة، متبوعًا ببعض الوظائف غير الخطية البسيطة مثل ReLU.",
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "translatedText": "يمكنك القول أن هذه الخلية العصبية نشطة عندما تكون هذه القيمة موجبة وأنها غير نشطة إذا كانت هذه القيمة صفرًا.",
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "translatedText": "الخطوة التالية تبدو مشابهة جدًا للخطوة الأولى.",
  "input": "The next step looks very similar to the first one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "translatedText": "تضرب في مصفوفة كبيرة جدًا وتضيف إليها مصطلح تحيز معين.",
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "translatedText": "في هذه الحالة، يعود عدد الأبعاد في المخرجات إلى حجم مساحة التضمين، لذا سأستمر وأسمي هذا مصفوفة الإسقاط السفلي.",
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "translatedText": "وهذه المرة، بدلاً من التفكير في الأمور صفاً تلو الآخر، من الأفضل أن نفكر فيها عموداً تلو الآخر.",
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "translatedText": "كما ترى، هناك طريقة أخرى يمكنك من خلالها حفظ عملية ضرب المصفوفات في ذهنك وهي أن تتخيل أخذ كل عمود من المصفوفة وضربه بالمصطلح المقابل في المتجه الذي تتم معالجته وإضافة كل هذه الأعمدة التي تمت إعادة قياسها معًا.",
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "translatedText": "السبب في أنه من الأفضل التفكير بهذه الطريقة هو أن الأعمدة هنا لها نفس أبعاد مساحة التضمين، لذا يمكننا أن نفكر فيها كاتجاهات في تلك المساحة.",
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "translatedText": "على سبيل المثال، سوف نتخيل أن النموذج تعلم كيفية تحويل العمود الأول إلى اتجاه كرة السلة الذي نفترض وجوده.",
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "translatedText": "ما يعنيه هذا هو أنه عندما تكون الخلية العصبية ذات الصلة في هذا الموضع الأول نشطة، فسوف نضيف هذا العمود إلى النتيجة النهائية.",
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "translatedText": "ولكن إذا كانت تلك الخلية العصبية غير نشطة، وإذا كان هذا العدد صفرًا، فلن يكون لهذا أي تأثير.",
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "translatedText": "ولا يجب أن يقتصر الأمر على كرة السلة فقط.",
  "input": "And it doesn't just have to be basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "translatedText": "يمكن أن يتكامل النموذج أيضًا مع هذا العمود والعديد من الميزات الأخرى التي يريد ربطها بشيء يحمل الاسم الكامل مايكل جوردن.",
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "translatedText": "وفي الوقت نفسه، فإن جميع الأعمدة الأخرى في هذه المصفوفة تخبرك بما سيتم إضافته إلى النتيجة النهائية إذا كانت الخلية العصبية المقابلة نشطة.",
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "translatedText": "وإذا كان لديك تحيز في هذه الحالة، فهو شيء تضيفه في كل مرة، بغض النظر عن قيم الخلايا العصبية.",
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "translatedText": "ربما تتساءل ماذا يفعل هذا؟",
  "input": "You might wonder what's that doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "translatedText": "كما هو الحال مع جميع الكائنات المليئة بالمعلمات هنا، فمن الصعب نوعًا ما أن نقول ذلك على وجه التحديد.",
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "translatedText": "ربما هناك بعض الأعمال المحاسبية التي تحتاج الشبكة إلى القيام بها، ولكن يمكنك تجاهلها الآن.",
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "translatedText": "ولكي نجعل تدويننا أكثر إحكاما مرة أخرى، فسوف أطلق على هذه المصفوفة الكبيرة اسم W، وبالمثل سأسمي متجه التحيز B، وأضعه مرة أخرى في الرسم التخطيطي الخاص بنا.",
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "translatedText": "كما قمت بمعاينته سابقًا، ما تفعله بهذه النتيجة النهائية هو إضافتها إلى المتجه الذي تدفق إلى الكتلة في هذا الموضع، وهذا يعطيك هذه النتيجة النهائية.",
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "translatedText": "على سبيل المثال، إذا كان المتجه المتدفق في ترميز كل من الاسم الأول مايكل والاسم الأخير جوردان، فبما أن هذه السلسلة من العمليات ستؤدي إلى تشغيل بوابة AND، فسوف تضيف اتجاه كرة السلة، وبالتالي فإن ما يخرج سوف يشفر كل هذه العناصر معًا.",
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "translatedText": "وتذكر أن هذه العملية تحدث لكل واحد من هذه المتجهات بالتوازي.",
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "translatedText": "على وجه الخصوص، إذا أخذنا أرقام GPT-3، فهذا يعني أن هذه الكتلة لا تحتوي فقط على 50000 خلية عصبية، بل تحتوي أيضًا على 50000 ضعف عدد الرموز في المدخلات.",
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "translatedText": "وهذه هي العملية برمتها، منتجان لمصفوفة، كل منهما مع تحيز مضاف ووظيفة قص بسيطة بينهما.",
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "translatedText": "أي منكم شاهد مقاطع الفيديو السابقة من السلسلة سوف يتعرف على هذا الهيكل باعتباره النوع الأكثر أساسية من الشبكات العصبية التي درسناها هناك.",
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "translatedText": "في هذا المثال، تم تدريبه على التعرف على الأرقام المكتوبة بخط اليد.",
  "input": "In that example, it was trained to recognize handwritten digits.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "translatedText": "هنا، في سياق المحول لنموذج لغة كبير، هذا جزء واحد في بنية أكبر وأي محاولة لتفسير ما يفعله بالضبط ترتبط ارتباطًا وثيقًا بفكرة ترميز المعلومات في متجهات من مساحة تضمين عالية الأبعاد.",
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "translatedText": "هذا هو الدرس الأساسي، ولكنني أريد أن أتراجع وأفكر في شيئين مختلفين، الأول هو نوع من المحاسبة، والثاني يتضمن حقيقة مثيرة للتفكير حول الأبعاد العليا التي لم أكن أعرفها في الواقع حتى تعمقت في المحولات.",
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "translatedText": "في الفصلين الأخيرين، بدأنا أنا وأنت في حساب العدد الإجمالي للمعلمات في GPT-3 ورؤية مكان وجودها بالضبط، لذا دعنا ننهي اللعبة بسرعة هنا.",
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "translatedText": "لقد ذكرت بالفعل كيف أن مصفوفة الإسقاط هذه تحتوي على ما يقل قليلاً عن 50000 صف وأن كل صف يطابق حجم مساحة التضمين، والتي بالنسبة لـ GPT-3 هي 12288.",
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "translatedText": "وبضرب هذه القيم معًا، نحصل على 604 مليون معلمة لتلك المصفوفة فقط، والإسقاط السفلي له نفس عدد المعلمات فقط مع الشكل المنقول.",
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "translatedText": "وبالتالي، فإنها مجتمعة تعطي حوالي 1.2 مليار معلمة.",
  "input": "So together, they give about 1.2 billion parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "translatedText": "يأخذ متجه التحيز أيضًا بعين الاعتبار بضعة معلمات أخرى، ولكنها نسبة ضئيلة من الإجمالي، لذا لن أعرضها حتى.",
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "translatedText": "في GPT-3، يتدفق تسلسل متجهات التضمين هذا ليس من خلال كتلة واحدة، بل من خلال 96 كتلة MLP مميزة، وبالتالي فإن العدد الإجمالي للمعلمات المخصصة لكل هذه الكتل يصل إلى حوالي 116 مليار.",
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "translatedText": "وهذا يمثل حوالي ثلثي إجمالي المعلمات في الشبكة، وعندما تضيفه إلى كل ما كان لدينا من قبل، بالنسبة لكتل الانتباه، والتضمين، وإلغاء التضمين، فإنك تحصل بالفعل على المجموع الكلي البالغ 175 مليارًا كما هو معلن.",
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "translatedText": "ربما يجدر بنا أن نذكر أن هناك مجموعة أخرى من المعلمات المرتبطة بخطوات التطبيع التي تم تخطيها في هذا التفسير، ولكن مثل متجه التحيز، فإنها تمثل نسبة ضئيلة للغاية من الإجمالي.",
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "translatedText": "أما فيما يتعلق بنقطة التأمل الثانية، فقد تتساءل عما إذا كان هذا المثال المركزي الذي أمضينا الكثير من الوقت عليه يعكس كيفية تخزين الحقائق فعليًا في نماذج لغوية كبيرة حقيقية.",
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "translatedText": "من الصحيح أن صفوف تلك المصفوفة الأولى يمكن اعتبارها اتجاهات في مساحة التضمين هذه، وهذا يعني أن تنشيط كل خلية عصبية يخبرك بمدى توافق متجه معين مع اتجاه معين.",
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "translatedText": "ومن الصحيح أيضًا أن أعمدة تلك المصفوفة الثانية تخبرك بما سيتم إضافته إلى النتيجة إذا كانت تلك الخلية العصبية نشطة.",
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "translatedText": "كلتاهما مجرد حقائق رياضية.",
  "input": "Both of those are just mathematical facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "translatedText": "ومع ذلك، تشير الأدلة إلى أن الخلايا العصبية الفردية نادراً ما تمثل ميزة واحدة نظيفة مثل مايكل جوردان، وقد يكون هناك في الواقع سبب وجيه للغاية لكون الأمر كذلك، وهو مرتبط بفكرة تطفو بين الباحثين في مجال التفسير هذه الأيام والمعروفة باسم التراكب.",
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "translatedText": "هذه فرضية قد تساعد في تفسير سبب صعوبة تفسير النماذج بشكل خاص ولماذا يمكن قياسها بشكل جيد على نحو مدهش.",
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "translatedText": "الفكرة الأساسية هي أنه إذا كان لديك مساحة ذات n أبعاد وتريد تمثيل مجموعة من الميزات المختلفة باستخدام اتجاهات متعامدة مع بعضها البعض في تلك المساحة، فأنت تعلم، بهذه الطريقة إذا أضفت مكونًا في اتجاه واحد، فلن يؤثر ذلك على أي من الاتجاهات الأخرى، ثم الحد الأقصى لعدد المتجهات التي يمكنك ملاءمتها هو n فقط، وهو عدد الأبعاد.",
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "translatedText": "بالنسبة لعالم الرياضيات، هذا هو في الواقع تعريف البعد.",
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "translatedText": "لكن الأمر يصبح مثيرا للاهتمام إذا خففت من هذا القيد قليلا وتحمّلت بعض الضوضاء.",
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "translatedText": "لنفترض أنك تسمح بتمثيل هذه الميزات بواسطة متجهات ليست عمودية تمامًا، بل هي عمودية تقريبًا، ربما بين 89 و91 درجة.",
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "translatedText": "لو كنا في بعدين أو ثلاثة أبعاد، فهذا لا يشكل أي فرق.",
  "input": "If we were in two or three dimensions, this makes no difference.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "translatedText": "وهذا لا يمنحك أي مساحة إضافية لتناسب المزيد من المتجهات، مما يجعل الأمر أكثر مخالفة للحدس حيث تتغير الإجابة بشكل كبير بالنسبة للأبعاد الأعلى.",
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "translatedText": "يمكنني أن أقدم لك مثالًا سريعًا جدًا ومباشرًا لهذا باستخدام بعض Python المتناثر الذي سيُنشئ قائمة من المتجهات ذات 100 بُعد، كل منها يتم تهيئته عشوائيًا، وستحتوي هذه القائمة على 10000 متجه مميز، أي 100 مرة عدد المتجهات كما هو الحال في الأبعاد.",
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "translatedText": "يوضح الرسم البياني هنا توزيع الزوايا بين أزواج هذه المتجهات.",
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "translatedText": "وبما أنهم بدأوا بشكل عشوائي، فإن هذه الزوايا يمكن أن تكون أي شيء من 0 إلى 180 درجة، ولكنك ستلاحظ بالفعل، حتى بالنسبة للمتجهات العشوائية، أن هناك تحيزًا كبيرًا للأشياء لتكون أقرب إلى 90 درجة.",
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "translatedText": "ثم ما سأفعله هو تشغيل عملية تحسين معينة تعمل بشكل متكرر على دفع كل هذه المتجهات بحيث تحاول أن تصبح أكثر عمودية على بعضها البعض.",
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "translatedText": "بعد تكرار ذلك عدة مرات مختلفة، هذا هو شكل توزيع الزوايا.",
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "translatedText": "يتعين علينا في الواقع تكبير الصورة هنا لأن جميع الزوايا الممكنة بين أزواج المتجهات تقع داخل هذا النطاق الضيق بين 89 و91 درجة.",
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "translatedText": "بشكل عام، إحدى نتائج ما يُعرف باسم مبرهنة جونسون-ليندنشتراوس هي أن عدد المتجهات التي يمكنك حشرها في فضاء، والتي تكون عمودية تقريبًا مثل هذا، تنمو بشكل كبير مع عدد الأبعاد.",
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "translatedText": "وهذا مهم للغاية بالنسبة لنماذج اللغة الكبيرة، والتي قد تستفيد من ربط الأفكار المستقلة باتجاهات متعامدة تقريبًا.",
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "translatedText": "وهذا يعني أنه من الممكن تخزين أفكار كثيرة جدًا تفوق أبعاد المساحة المخصصة لها.",
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "translatedText": "قد يفسر هذا جزئيًا سبب ارتباط أداء النموذج بشكل جيد بالحجم.",
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "translatedText": "إن المساحة التي تحتوي على 10 أضعاف الأبعاد يمكن أن تخزن ما يزيد بكثير عن 10 أضعاف عدد الأفكار المستقلة.",
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "translatedText": "وهذا ينطبق ليس فقط على مساحة التضمين حيث تعيش المتجهات المتدفقة عبر النموذج، ولكن أيضًا على هذا المتجه المليء بالخلايا العصبية في منتصف الإدراك المتعدد الطبقات الذي درسناه للتو.",
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "translatedText": "وهذا يعني أنه عند أحجام GPT-3، قد لا يكون قادرًا على استكشاف 50 ألف ميزة فقط، ولكن إذا استفاد بدلاً من ذلك من هذه القدرة الإضافية الهائلة عن طريق استخدام اتجاهات شبه عمودية للمساحة، فقد يكون قادرًا على استكشاف العديد والعديد من ميزات المتجه الذي تتم معالجته.",
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "translatedText": "ولكن إذا كان الأمر كذلك، فإن ما يعنيه هذا هو أن السمات الفردية لن تكون مرئية عندما تضيء خلية عصبية واحدة.",
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "translatedText": "سيتعين أن يبدو الأمر وكأنه مزيج محدد من الخلايا العصبية بدلاً من ذلك، أي تراكب.",
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "translatedText": "بالنسبة لأي منكم فضولي لمعرفة المزيد، فإن مصطلح البحث الرئيسي ذي الصلة هنا هو sparse autoencoder، وهي أداة يستخدمها بعض الأشخاص المتخصصين في التفسير لمحاولة استخراج الميزات الحقيقية، حتى لو كانت متراكبة جدًا على كل هذه الخلايا العصبية.",
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "translatedText": "سأضع رابطًا لبعض المنشورات الأنثروبية الرائعة حول هذا الموضوع.",
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "translatedText": "في هذه المرحلة، لم نلمس كل تفاصيل المحول، لكننا وصلنا إلى النقاط الأكثر أهمية.",
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "translatedText": "الشيء الرئيسي الذي أريد تغطيته في الفصل التالي هو عملية التدريب.",
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "translatedText": "من ناحية أخرى، فإن الإجابة المختصرة لكيفية عمل التدريب هي أن الأمر كله يتعلق بالانتشار الخلفي، وقد قمنا بتغطية الانتشار الخلفي في سياق منفصل في الفصول السابقة في السلسلة.",
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "translatedText": "ولكن هناك المزيد لمناقشته، مثل دالة التكلفة المحددة المستخدمة في نماذج اللغة، وفكرة الضبط الدقيق باستخدام التعلم المعزز مع ردود الفعل البشرية، ومفهوم قوانين التوسع.",
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "translatedText": "ملاحظة سريعة للمتابعين النشطين بينكم، هناك عدد من مقاطع الفيديو غير المرتبطة بالتعلم الآلي والتي أنا متحمس للتعمق فيها قبل أن أقوم بالفصل التالي، لذلك قد يستغرق الأمر بعض الوقت، ولكنني أعدك أنه سيأتي في الوقت المناسب.",
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "translatedText": "شكرًا لك.",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]