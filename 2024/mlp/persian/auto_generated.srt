1
00:00:00,000 --> 00:00:04,444
اگر به یک زبان بزرگ این عبارت را به زبان بیاورید، مایکل جردن ورزش خالی را 

2
00:00:04,444 --> 00:00:09,009
انجام می دهد، و شما باید آن را پیش بینی کند که چه اتفاقی می افتد، و بسکتبال 

3
00:00:09,009 --> 00:00:13,694
را به درستی پیش بینی می کند، این نشان می دهد که در جایی، در درون صدها میلیارد 

4
00:00:13,694 --> 00:00:18,320
پارامترش، این ورزش در آن پخته شده است. دانش در مورد یک فرد خاص و ورزش خاص او.

5
00:00:18,940 --> 00:00:22,170
و به طور کلی فکر می‌کنم، هرکسی که با یکی از این مدل‌ها بازی کرده است، 

6
00:00:22,170 --> 00:00:25,400
این حس واضح را دارد که هزاران تن و هزاران واقعیت را به خاطر سپرده است.

7
00:00:25,700 --> 00:00:29,160
بنابراین یک سوال منطقی که می توانید بپرسید این است که دقیقا چگونه کار می کند؟

8
00:00:29,160 --> 00:00:31,040
و این حقایق در کجا زندگی می کنند؟

9
00:00:35,720 --> 00:00:40,131
دسامبر گذشته، چند محقق از Google DeepMind درباره کار روی این سوال پست 

10
00:00:40,131 --> 00:00:44,480
کردند و از این مثال خاص از تطبیق ورزشکاران با ورزش خود استفاده کردند.

11
00:00:44,900 --> 00:00:49,225
و اگرچه درک مکانیکی کامل از نحوه ذخیره حقایق حل نشده باقی مانده است، 

12
00:00:49,225 --> 00:00:53,675
آنها نتایج جزئی جالبی داشتند، از جمله نتیجه گیری سطح بالا بسیار کلی که 

13
00:00:53,675 --> 00:00:58,063
به نظر می رسد حقایق در داخل بخش خاصی از این شبکه ها زندگی می کنند، که 

14
00:00:58,063 --> 00:01:02,640
به صورت خیالی به عنوان چند لایه شناخته می شود. پرسپترون یا به اختصار MLP.

15
00:01:03,120 --> 00:01:07,748
در دو فصل آخر، من و شما جزئیات پشت ترانسفورماتورها، معماری زیربنای مدل‌های 

16
00:01:07,748 --> 00:01:12,500
زبان بزرگ، و همچنین زیربنای بسیاری از هوش مصنوعی مدرن دیگر را بررسی کرده‌ایم.

17
00:01:13,060 --> 00:01:16,200
در فصل اخیر، ما روی قطعه ای به نام توجه تمرکز کردیم.

18
00:01:16,840 --> 00:01:21,050
و قدم بعدی برای من و شما این است که به جزئیات آنچه در داخل این پرسپترون‌های 

19
00:01:21,050 --> 00:01:25,040
چندلایه رخ می‌دهد، بپردازیم، که بخش بزرگ دیگری از شبکه را تشکیل می‌دهند.

20
00:01:25,680 --> 00:01:30,100
محاسبه در اینجا در واقع نسبتاً ساده است، به خصوص زمانی که آن را با توجه مقایسه کنید.

21
00:01:30,560 --> 00:01:34,980
اساساً به یک جفت ضرب ماتریسی با یک چیز ساده در بین آنها خلاصه می شود.

22
00:01:35,720 --> 00:01:40,460
با این حال، تفسیر آنچه این محاسبات انجام می دهند بسیار چالش برانگیز است.

23
00:01:41,560 --> 00:01:45,346
هدف اصلی ما در اینجا این است که در محاسبات قدم بگذاریم و آنها را به یاد ماندنی 

24
00:01:45,346 --> 00:01:49,181
کنیم، اما من می‌خواهم این کار را در چارچوب نشان دادن یک مثال خاص از اینکه چگونه 

25
00:01:49,181 --> 00:01:53,160
یکی از این بلوک‌ها، حداقل در اصل، می‌تواند یک واقعیت ملموس را ذخیره کند، انجام دهم.

26
00:01:53,580 --> 00:01:57,080
به طور خاص، این واقعیت را ذخیره می کند که مایکل جردن بسکتبال بازی می کند.

27
00:01:58,080 --> 00:02:00,515
باید اشاره کنم که چیدمان در اینجا الهام گرفته از 

28
00:02:00,515 --> 00:02:03,200
گفتگوی من با یکی از آن محققان DeepMind، نیل ناندا است.

29
00:02:04,060 --> 00:02:07,656
در بیشتر موارد، من فرض می‌کنم که شما یا دو فصل گذشته را تماشا کرده‌اید، 

30
00:02:07,656 --> 00:02:11,403
یا در غیر این صورت، درک اولیه‌ای از ترانسفورماتور دارید، اما تازه‌کننده‌ها 

31
00:02:11,403 --> 00:02:14,700
هرگز آسیبی نمی‌زنند، بنابراین در اینجا یادآوری سریع جریان کلی است.

32
00:02:15,340 --> 00:02:18,385
من و شما در حال مطالعه مدلی بوده‌ایم که آموزش داده شده 

33
00:02:18,385 --> 00:02:21,320
است تا متنی را بنویسد و اتفاقات بعدی را پیش‌بینی کند.

34
00:02:21,720 --> 00:02:26,425
آن متن ورودی ابتدا به دسته‌ای از نشانه‌ها تقسیم می‌شود، که به معنای تکه‌های 

35
00:02:26,425 --> 00:02:31,007
کوچکی است که معمولاً کلمات یا تکه‌های کوچکی از کلمات هستند، و هر نشانه با 

36
00:02:31,007 --> 00:02:35,280
یک بردار با ابعاد بالا همراه است، که به معنای فهرست طولانی اعداد است.

37
00:02:35,840 --> 00:02:41,214
این دنباله از بردارها سپس به طور مکرر از دو نوع عملیات عبور می کند، توجه، که به 

38
00:02:41,214 --> 00:02:46,790
بردارها اجازه می دهد اطلاعات را بین یکدیگر منتقل کنند، و سپس پرسپترون های چندلایه، 

39
00:02:46,790 --> 00:02:52,300
چیزی که امروز به آن می پردازیم، و همچنین یک مرحله عادی سازی مشخص وجود دارد. در بین

40
00:02:53,300 --> 00:02:58,737
پس از اینکه توالی بردارها در بسیاری از تکرارهای مختلف هر دوی این بلوک ها 

41
00:02:58,737 --> 00:03:04,324
جریان یافت، تا پایان، امید این است که هر بردار اطلاعات کافی را از متن، همه 

42
00:03:04,324 --> 00:03:09,837
کلمات دیگر در ورودی، و همچنین از دانش کلی که از طریق آموزش در وزن های مدل 

43
00:03:09,837 --> 00:03:16,020
پخته شده است، که می توان از آن برای پیش بینی اینکه چه توکن بعدی می آید استفاده کرد.

44
00:03:16,860 --> 00:03:20,732
یکی از ایده‌های کلیدی که می‌خواهم در ذهن خود داشته باشید این است که همه 

45
00:03:20,732 --> 00:03:24,766
این بردارها در یک فضای بسیار بسیار با ابعاد زندگی می‌کنند و وقتی به آن فضا 

46
00:03:24,766 --> 00:03:28,800
فکر می‌کنید، جهت‌های مختلف می‌توانند انواع مختلفی از معنا را رمزگذاری کنند.

47
00:03:30,120 --> 00:03:35,285
بنابراین یک مثال بسیار کلاسیک که دوست دارم به آن اشاره کنم این است که چگونه اگر به 

48
00:03:35,285 --> 00:03:40,700
تعبیه زن نگاه کنید و تعبیه مرد را کم کنید و آن قدم کوچک را بردارید و آن را به اسم مذکر 

49
00:03:40,700 --> 00:03:46,240
دیگری اضافه کنید، چیزی شبیه عمو، فرود می آیید. جایی بسیار بسیار نزدیک به اسم مونث مربوطه.

50
00:03:46,440 --> 00:03:50,880
از این نظر، این جهت خاص اطلاعات جنسیتی را رمزگذاری می کند.

51
00:03:51,640 --> 00:03:55,859
ایده این است که بسیاری از جهت‌های متمایز دیگر در این فضای فوق‌بعدی می‌توانند 

52
00:03:55,859 --> 00:03:59,640
با ویژگی‌های دیگری که مدل ممکن است بخواهد نشان دهد مطابقت داشته باشد.

53
00:04:01,400 --> 00:04:06,180
در یک ترانسفورماتور، این بردارها صرفاً معنای یک کلمه را رمزگذاری نمی کنند.

54
00:04:06,680 --> 00:04:11,048
همانطور که آنها از طریق شبکه جریان می یابند، بر اساس تمام زمینه های اطراف 

55
00:04:11,048 --> 00:04:15,180
خود، و همچنین بر اساس دانش مدل، معنای بسیار غنی تری را دریافت می کنند.

56
00:04:15,880 --> 00:04:19,941
در نهایت، هر یک باید چیزی را بسیار فراتر از معنای یک کلمه رمزگذاری 

57
00:04:19,941 --> 00:04:23,760
کند، زیرا باید برای پیش‌بینی آنچه در آینده خواهد آمد کافی باشد.

58
00:04:24,560 --> 00:04:28,905
ما قبلاً دیده‌ایم که چگونه بلوک‌های توجه به شما اجازه می‌دهند تا زمینه را ترکیب 

59
00:04:28,905 --> 00:04:33,414
کنید، اما اکثر پارامترهای مدل در واقع در داخل بلوک‌های MLP زندگی می‌کنند، و یک فکر 

60
00:04:33,414 --> 00:04:38,140
برای کاری که ممکن است انجام دهند این است که ظرفیت اضافی برای ذخیره حقایق ارائه می‌دهند.

61
00:04:38,720 --> 00:04:42,420
همانطور که گفتم، درس اینجا بر روی نمونه اسباب بازی بتونی تمرکز می کند که 

62
00:04:42,420 --> 00:04:46,120
چگونه می تواند این واقعیت را که مایکل جردن بسکتبال بازی می کند ذخیره کند.

63
00:04:47,120 --> 00:04:49,418
اکنون، این نمونه اسباب بازی مستلزم آن است که من و 

64
00:04:49,418 --> 00:04:51,900
شما چند فرض در مورد آن فضای با ابعاد بالا داشته باشیم.

65
00:04:52,360 --> 00:04:56,893
ابتدا، فرض می کنیم که یکی از جهت ها ایده نام کوچک مایکل را 

66
00:04:56,893 --> 00:05:01,579
نشان می دهد، و سپس جهت تقریباً عمودی دیگری ایده نام خانوادگی 

67
00:05:01,579 --> 00:05:06,420
جردن را نشان می دهد، و سپس جهت سوم ایده بسکتبال را نشان می دهد.

68
00:05:07,400 --> 00:05:12,245
بنابراین به طور خاص، منظور من از این این است که اگر به شبکه نگاه کنید و 

69
00:05:12,245 --> 00:05:17,158
یکی از بردارهای در حال پردازش را بردارید، اگر حاصلضرب نقطه آن با این نام 

70
00:05:17,158 --> 00:05:22,340
کوچک جهت مایکل یکی باشد، رمزگذاری بردار به این معنی است. ایده شخصی با این نام

71
00:05:23,800 --> 00:05:26,135
در غیر این صورت، آن حاصلضرب نقطه صفر یا منفی خواهد 

72
00:05:26,135 --> 00:05:28,700
بود، به این معنی که بردار واقعاً با آن جهت مطابقت ندارد.

73
00:05:29,420 --> 00:05:32,349
و برای سادگی، بیایید این سوال بسیار منطقی را به طور کامل نادیده بگیریم 

74
00:05:32,349 --> 00:05:35,320
که اگر آن محصول نقطه ای بزرگتر از یک بود، چه معنایی می تواند داشته باشد.

75
00:05:36,200 --> 00:05:40,051
به طور مشابه، محصول نقطه‌ای آن با این جهت‌های دیگر به 

76
00:05:40,051 --> 00:05:43,760
شما می‌گوید که آیا نام خانوادگی جردن است یا بسکتبال.

77
00:05:44,740 --> 00:05:48,651
بنابراین فرض کنید یک بردار برای نمایش نام کامل، مایکل جردن، در نظر 

78
00:05:48,651 --> 00:05:52,680
گرفته شده است، پس حاصلضرب نقطه آن با هر دوی این جهت ها باید یکی باشد.

79
00:05:53,480 --> 00:05:58,051
از آنجایی که متن مایکل جردن شامل دو نشانه متفاوت است، این بدان معناست که باید 

80
00:05:58,051 --> 00:06:02,329
فرض کنیم که بلوک توجه قبلی با موفقیت اطلاعات را به دومین بردار از این دو 

81
00:06:02,329 --> 00:06:06,960
بردار منتقل کرده است تا اطمینان حاصل شود که می تواند هر دو نام را رمزگذاری کند.

82
00:06:07,940 --> 00:06:11,480
با همه آن‌ها به عنوان مفروضات، بیایید اکنون به گوشت درس بپردازیم.

83
00:06:11,880 --> 00:06:14,980
درون پرسپترون چندلایه چه اتفاقی می افتد؟

84
00:06:17,100 --> 00:06:21,340
ممکن است به این دنباله از بردارها فکر کنید که در بلوک جریان می یابند، و به 

85
00:06:21,340 --> 00:06:25,580
یاد داشته باشید، هر بردار در ابتدا با یکی از نشانه های متن ورودی مرتبط بود.

86
00:06:26,080 --> 00:06:29,561
اتفاقی که قرار است بیفتد این است که هر بردار منفرد از آن دنباله 

87
00:06:29,561 --> 00:06:32,987
یک سری عملیات کوتاه را انجام می‌دهد، ما آنها را در یک لحظه باز 

88
00:06:32,987 --> 00:06:36,360
می‌کنیم و در پایان، بردار دیگری با همان ابعاد به دست می‌آوریم.

89
00:06:36,880 --> 00:06:40,006
آن بردار دیگر به بردار اصلی که وارد شده اضافه 

90
00:06:40,006 --> 00:06:43,200
می شود، و آن مجموع نتیجه ای است که خارج می شود.

91
00:06:43,720 --> 00:06:47,614
این دنباله از عملیات چیزی است که شما برای هر بردار در دنباله اعمال می 

92
00:06:47,614 --> 00:06:51,620
کنید، با هر نشانه در ورودی مرتبط است، و همه به صورت موازی اتفاق می افتد.

93
00:06:52,100 --> 00:06:54,171
به ویژه، بردارها در این مرحله با یکدیگر صحبت نمی 

94
00:06:54,171 --> 00:06:56,200
کنند، همه آنها به نوعی کار خود را انجام می دهند.

95
00:06:56,720 --> 00:06:59,732
و برای من و شما، این در واقع کار را بسیار ساده‌تر می‌کند، زیرا به این 

96
00:06:59,732 --> 00:07:02,745
معنی است که اگر بفهمیم که فقط برای یکی از بردارها از طریق این بلوک چه 

97
00:07:02,745 --> 00:07:06,060
اتفاقی می‌افتد، به طور موثر متوجه می‌شویم که برای همه آنها چه اتفاقی می‌افتد.

98
00:07:07,100 --> 00:07:11,277
وقتی می‌گویم این بلوک این واقعیت را رمزگذاری می‌کند که مایکل جردن بسکتبال بازی 

99
00:07:11,277 --> 00:07:15,560
می‌کند، منظورم این است که اگر یک بردار در آن جریان داشته باشد که نام مایکل و نام 

100
00:07:15,560 --> 00:07:19,895
خانوادگی جردن را رمزگذاری می‌کند، این دنباله محاسبات چیزی را تولید می‌کند که شامل 

101
00:07:19,895 --> 00:07:24,020
آن جهت بسکتبال می‌شود. که همان چیزی است که به بردار در آن موقعیت اضافه می کند.

102
00:07:25,600 --> 00:07:29,700
اولین مرحله از این فرآیند شبیه ضرب آن بردار در یک ماتریس بسیار بزرگ است.

103
00:07:30,040 --> 00:07:31,980
جای تعجب نیست، این یک یادگیری عمیق است.

104
00:07:32,680 --> 00:07:36,329
و این ماتریس، مانند همه ماتریس‌های دیگری که دیده‌ایم، پر از پارامترهای مدل است که 

105
00:07:36,329 --> 00:07:39,934
از داده‌ها آموخته می‌شوند، که ممکن است آن‌ها را به‌عنوان دسته‌ای از دستگیره‌ها و 

106
00:07:39,934 --> 00:07:43,540
شماره‌گیری‌ها در نظر بگیرید که برای تعیین رفتار مدل بهینه‌سازی و تنظیم می‌شوند. .

107
00:07:44,500 --> 00:07:48,575
حال، یک راه خوب برای فکر کردن در مورد ضرب ماتریس این است که هر سطر از آن ماتریس 

108
00:07:48,575 --> 00:07:52,702
را به عنوان بردار خودش تصور کنید، و دسته ای از محصولات نقطه ای را بین آن سطرها و 

109
00:07:52,702 --> 00:07:56,880
بردار در حال پردازش قرار دهید، که برای جاسازی آن را به عنوان E برچسب گذاری می کنم.

110
00:07:57,280 --> 00:08:00,730
به عنوان مثال، فرض کنید که ردیف اول برابر با نام 

111
00:08:00,730 --> 00:08:04,040
کوچک جهت مایکل است که ما فرض می کنیم وجود دارد.

112
00:08:04,320 --> 00:08:09,401
این بدان معناست که اولین مؤلفه در این خروجی، این محصول نقطه‌ای در اینجا، اگر آن 

113
00:08:09,401 --> 00:08:14,800
بردار نام کوچک مایکل را رمزگذاری کند، یک خواهد بود و در غیر این صورت صفر یا منفی است.

114
00:08:15,880 --> 00:08:19,539
حتی جالب‌تر، لحظه‌ای به این فکر کنید که اگر ردیف اول این نام 

115
00:08:19,539 --> 00:08:23,080
کوچک مایکل به‌علاوه نام خانوادگی جردن باشد، چه معنایی داشت.

116
00:08:23,700 --> 00:08:27,420
و برای سادگی، اجازه دهید ادامه دهم و آن را به صورت M به اضافه J بنویسم.

117
00:08:28,080 --> 00:08:31,530
سپس، با گرفتن یک محصول نقطه ای با این جاسازی E، همه چیز به خوبی 

118
00:08:31,530 --> 00:08:34,980
توزیع می شود، بنابراین به نظر می رسد M نقطه E به اضافه J نقطه E.

119
00:08:34,980 --> 00:08:39,810
و توجه کنید که چگونه این بدان معناست که اگر بردار نام کامل مایکل جردن را رمزگذاری 

120
00:08:39,810 --> 00:08:44,700
کند، مقدار نهایی دو خواهد بود، و در غیر این صورت یک یا چیزی کوچکتر از یک خواهد بود.

121
00:08:45,340 --> 00:08:47,260
و این فقط یک ردیف در این ماتریس است.

122
00:08:47,600 --> 00:08:51,947
ممکن است تمام ردیف‌های دیگر را به‌صورت موازی در نظر بگیرید که انواع دیگری از سؤال‌ها 

123
00:08:51,947 --> 00:08:56,040
را می‌پرسند و در برخی از انواع دیگر ویژگی‌های بردار در حال پردازش تحقیق می‌کنند.

124
00:08:56,700 --> 00:08:59,495
اغلب این مرحله شامل افزودن بردار دیگری به خروجی است که 

125
00:08:59,495 --> 00:09:02,240
پر از پارامترهای مدل است که از داده ها آموخته شده است.

126
00:09:02,240 --> 00:09:04,560
این بردار دیگر به عنوان سوگیری شناخته می شود.

127
00:09:05,180 --> 00:09:10,338
برای مثال، می‌خواهم تصور کنید که مقدار این سوگیری در همان مؤلفه اول منفی است، به 

128
00:09:10,338 --> 00:09:15,560
این معنی که خروجی نهایی ما مانند محصول نقطه‌ای مربوطه به نظر می‌رسد، اما منهای یک.

129
00:09:16,120 --> 00:09:20,056
ممکن است بسیار منطقی بپرسید که چرا می‌خواهم فرض کنید که مدل این را 

130
00:09:20,056 --> 00:09:23,934
یاد گرفته است، و در یک لحظه خواهید دید که چرا اگر مقداری در اینجا 

131
00:09:23,934 --> 00:09:28,987
داشته باشیم مثبت است اگر و فقط اگر یک بردار آن را رمزگذاری کند، بسیار تمیز و خوب است. 

132
00:09:28,987 --> 00:09:32,160
نام کامل مایکل جردن و در غیر این صورت صفر یا منفی است.

133
00:09:33,040 --> 00:09:38,041
تعداد کل ردیف‌های این ماتریس، که چیزی شبیه به تعداد سؤال‌هایی است که پرسیده 

134
00:09:38,041 --> 00:09:42,780
می‌شود، در مورد GPT-3، که تعداد آن را دنبال کرده‌ایم، کمتر از 50000 است.

135
00:09:43,100 --> 00:09:46,640
در واقع، دقیقاً چهار برابر ابعاد این فضای تعبیه شده است.

136
00:09:46,920 --> 00:09:47,900
این یک انتخاب طراحی است.

137
00:09:47,940 --> 00:09:50,127
می‌توانید آن را بیشتر کنید، می‌توانید آن را کمتر کنید، اما 

138
00:09:50,127 --> 00:09:52,240
داشتن یک چندگانه تمیز معمولاً برای سخت‌افزار دوستانه است.

139
00:09:52,740 --> 00:09:55,820
از آنجایی که این ماتریس پر از وزن ما را به فضایی با 

140
00:09:55,820 --> 00:09:59,020
ابعاد بالاتر ترسیم می کند، من به آن مختصر W up می دهم.

141
00:09:59,020 --> 00:10:01,688
من به برچسب گذاری برداری که در حال پردازش آن هستیم به عنوان 

142
00:10:01,688 --> 00:10:04,357
E ادامه می دهم، و اجازه دهید این بردار سوگیری را به عنوان B 

143
00:10:04,357 --> 00:10:07,160
بالا برچسب گذاری کنیم و همه آن را در نمودار به پایین برگردانیم.

144
00:10:09,180 --> 00:10:12,429
در این مرحله، یک مشکل این است که این عملیات کاملاً 

145
00:10:12,429 --> 00:10:15,360
خطی است، اما زبان یک فرآیند بسیار غیر خطی است.

146
00:10:15,880 --> 00:10:19,933
اگر ورودی‌ای که ما اندازه‌گیری می‌کنیم برای مایکل به‌علاوه جردن زیاد 

147
00:10:19,933 --> 00:10:24,046
باشد، باید تا حدودی توسط مایکل به‌علاوه فلپس و همچنین الکسیس به‌علاوه 

148
00:10:24,046 --> 00:10:28,100
جردن، علی‌رغم اینکه از نظر مفهومی به هم مرتبط نیستند، راه‌اندازی شود.

149
00:10:28,540 --> 00:10:32,000
چیزی که واقعاً می خواهید یک بله یا خیر ساده برای نام کامل است.

150
00:10:32,900 --> 00:10:35,344
بنابراین مرحله بعدی این است که این بردار میانی 

151
00:10:35,344 --> 00:10:37,840
بزرگ را از یک تابع غیر خطی بسیار ساده عبور دهیم.

152
00:10:38,360 --> 00:10:41,941
یک انتخاب رایج، انتخابی است که تمام مقادیر منفی را گرفته و آنها 

153
00:10:41,941 --> 00:10:45,300
را به صفر می‌رساند و همه مقادیر مثبت را بدون تغییر می‌گذارد.

154
00:10:46,440 --> 00:10:51,005
و با ادامه سنت یادگیری عمیق اسامی بیش از حد فانتزی، این تابع 

155
00:10:51,005 --> 00:10:56,020
بسیار ساده اغلب واحد خطی اصلاح شده یا به اختصار ReLU نامیده می شود.

156
00:10:56,020 --> 00:10:57,880
در اینجا نمودار به نظر می رسد.

157
00:10:58,300 --> 00:11:04,113
بنابراین با در نظر گرفتن مثال تصوری خود که در آن اولین ورودی بردار میانی یک است، اگر و 

158
00:11:04,113 --> 00:11:09,993
فقط اگر نام کامل مایکل جردن و صفر یا منفی باشد، در غیر این صورت، پس از عبور از ReLU، به 

159
00:11:09,993 --> 00:11:15,740
یک مقدار بسیار تمیز می رسید که در آن همه از مقادیر صفر و منفی فقط به صفر بریده می شود.

160
00:11:16,100 --> 00:11:19,780
بنابراین این خروجی برای نام کامل مایکل جردن یک و در غیر این صورت صفر خواهد بود.

161
00:11:20,560 --> 00:11:24,120
به عبارت دیگر، به طور مستقیم رفتار یک دروازه AND را تقلید می کند.

162
00:11:25,660 --> 00:11:28,839
اغلب مدل‌ها از عملکرد کمی تغییر یافته استفاده می‌کنند که JLU 

163
00:11:28,839 --> 00:11:32,020
نامیده می‌شود، که همان شکل اولیه را دارد، فقط کمی نرم‌تر است.

164
00:11:32,500 --> 00:11:35,720
اما برای اهداف ما، اگر فقط به ReLU فکر کنیم، کمی تمیزتر است.

165
00:11:36,740 --> 00:11:39,729
همچنین، وقتی می شنوید که مردم به نورون های یک ترانسفورماتور 

166
00:11:39,729 --> 00:11:42,520
اشاره می کنند، در اینجا در مورد این مقادیر صحبت می کنند.

167
00:11:42,900 --> 00:11:49,043
هرگاه آن تصویر شبکه عصبی رایج را با لایه ای از نقاط و دسته ای از خطوط متصل به لایه قبلی 

168
00:11:49,043 --> 00:11:55,186
مشاهده کردید، که قبلاً در این سری داشتیم، معمولاً به معنای انتقال این ترکیب از یک مرحله 

169
00:11:55,186 --> 00:12:01,260
خطی، یک ضرب ماتریس، و به دنبال آن است. چند تابع غیر خطی ساده از نظر اصطلاحی مانند ReLU.

170
00:12:02,500 --> 00:12:05,772
شما می گویید که این نورون هر زمان که این مقدار مثبت 

171
00:12:05,772 --> 00:12:08,920
باشد فعال است و اگر آن مقدار صفر باشد غیرفعال است.

172
00:12:10,120 --> 00:12:12,380
مرحله بعدی بسیار شبیه به مرحله اول است.

173
00:12:12,560 --> 00:12:16,580
شما در یک ماتریس بسیار بزرگ ضرب می کنید و یک عبارت تعصب خاصی را اضافه می کنید.

174
00:12:16,980 --> 00:12:21,156
در این مورد، تعداد ابعاد در خروجی به اندازه فضای تعبیه شده کاهش می 

175
00:12:21,156 --> 00:12:25,520
یابد، بنابراین من ادامه می دهم و این را ماتریس طرح ریزی پایین می نامم.

176
00:12:26,220 --> 00:12:28,691
و این بار، به جای اینکه به چیزها ردیف به ردیف فکر 

177
00:12:28,691 --> 00:12:31,360
کنیم، در واقع بهتر است که ستون به ستون به آن فکر کنیم.

178
00:12:31,860 --> 00:12:36,514
می بینید، راه دیگری که می توانید ضرب ماتریس را در ذهن خود نگه دارید این است 

179
00:12:36,514 --> 00:12:41,046
که تصور کنید هر ستون ماتریس را بگیرید و آن را در عبارت مربوطه در بردار که 

180
00:12:41,046 --> 00:12:45,640
پردازش می کند ضرب کنید و همه آن ستون های تغییر مقیاس شده را با هم جمع کنید.

181
00:12:46,840 --> 00:12:51,180
دلیل اینکه بهتر است به این روش فکر کنیم این است که در اینجا ستون ها همان ابعاد فضای 

182
00:12:51,180 --> 00:12:55,780
تعبیه شده را دارند، بنابراین می توانیم آنها را به عنوان جهت هایی در آن فضا در نظر بگیریم.

183
00:12:56,140 --> 00:12:59,665
به عنوان مثال، ما تصور خواهیم کرد که مدل یاد گرفته است که اولین 

184
00:12:59,665 --> 00:13:03,080
ستون را در این جهت بسکتبال که ما فرض می کنیم وجود دارد، بسازد.

185
00:13:04,180 --> 00:13:07,322
معنی آن این است که وقتی نورون مربوطه در آن موقعیت 

186
00:13:07,322 --> 00:13:10,780
اول فعال است، این ستون را به نتیجه نهایی اضافه می کنیم.

187
00:13:11,140 --> 00:13:15,780
اما اگر آن نورون غیرفعال بود، اگر آن عدد صفر بود، هیچ تاثیری نداشت.

188
00:13:16,500 --> 00:13:18,060
و این فقط نباید بسکتبال باشد.

189
00:13:18,220 --> 00:13:21,582
این مدل همچنین می‌تواند در این ستون و بسیاری از ویژگی‌های دیگر که 

190
00:13:21,582 --> 00:13:25,200
می‌خواهد با چیزی که نام کامل مایکل جردن را دارد مرتبط کند، استفاده کند.

191
00:13:26,980 --> 00:13:31,746
و در همان زمان، تمام ستون های دیگر در این ماتریس به شما می گویند 

192
00:13:31,746 --> 00:13:36,660
که اگر نورون مربوطه فعال باشد، چه چیزی به نتیجه نهایی اضافه می شود.

193
00:13:37,360 --> 00:13:40,458
و اگر در این مورد تعصب دارید، این چیزی است که شما فقط 

194
00:13:40,458 --> 00:13:43,500
هر بار بدون توجه به مقادیر نورون آن را اضافه می کنید.

195
00:13:44,060 --> 00:13:45,280
ممکن است تعجب کنید که این چه کار می کند.

196
00:13:45,540 --> 00:13:49,320
مانند تمام اشیاء پر از پارامتر در اینجا، گفتن دقیق آن به نوعی سخت است.

197
00:13:49,320 --> 00:13:51,874
شاید یک مقدار حسابداری وجود داشته باشد که شبکه باید 

198
00:13:51,874 --> 00:13:54,380
انجام دهد، اما می توانید فعلاً آن را نادیده بگیرید.

199
00:13:54,860 --> 00:13:59,618
برای اینکه نماد خود را کمی فشرده‌تر کنیم، این ماتریس بزرگ را W پایین می‌نامم و به 

200
00:13:59,618 --> 00:14:04,260
طور مشابه آن بردار سوگیری B را پایین می‌خوانم و آن را در نمودار خود برمی‌گردانم.

201
00:14:04,740 --> 00:14:07,573
همانطور که قبلاً پیش‌نمایش کردم، کاری که با این نتیجه نهایی انجام 

202
00:14:07,573 --> 00:14:10,363
می‌دهید این است که آن را به برداری که در آن موقعیت به بلوک جریان 

203
00:14:10,363 --> 00:14:13,240
می‌یابد اضافه کنید و این نتیجه نهایی را برای شما به ارمغان می‌آورد.

204
00:14:13,820 --> 00:14:18,706
بنابراین برای مثال، اگر بردار جاری در هر دو نام مایکل و نام خانوادگی جردن را 

205
00:14:18,706 --> 00:14:23,719
رمزگذاری کند، به دلیل اینکه این توالی از عملیات دروازه AND را فعال می‌کند، جهت 

206
00:14:23,719 --> 00:14:29,240
بسکتبال را اضافه می‌کند، بنابراین آنچه بیرون می‌آید همه آن‌ها را با هم رمزگذاری می‌کند.

207
00:14:29,820 --> 00:14:32,080
و به یاد داشته باشید، این فرآیندی است که به طور 

208
00:14:32,080 --> 00:14:34,200
موازی برای هر یک از آن بردارها اتفاق می افتد.

209
00:14:34,800 --> 00:14:39,829
به طور خاص، با گرفتن اعداد GPT-3، به این معنی است که این بلوک نه تنها 

210
00:14:39,829 --> 00:14:44,860
50000 نورون در خود دارد، بلکه 50000 برابر تعداد توکن ها در ورودی دارد.

211
00:14:48,180 --> 00:14:51,680
بنابراین این کل عملیات است، دو محصول ماتریسی، هر کدام 

212
00:14:51,680 --> 00:14:55,180
با یک سوگیری اضافه شده و یک تابع برش ساده در بین آنها.

213
00:14:56,080 --> 00:14:59,263
هر یک از شما که ویدیوهای قبلی این مجموعه را تماشا کردید، این ساختار را به 

214
00:14:59,263 --> 00:15:02,620
عنوان ابتدایی ترین نوع شبکه عصبی که ما در آنجا مطالعه کردیم، تشخیص خواهید داد.

215
00:15:03,080 --> 00:15:06,100
در آن مثال، برای تشخیص ارقام دست نویس آموزش داده شد.

216
00:15:06,580 --> 00:15:11,950
در اینجا، در زمینه یک ترانسفورماتور برای یک مدل زبان بزرگ، این یک قطعه در یک 

217
00:15:11,950 --> 00:15:17,460
معماری بزرگتر است و هر تلاشی برای تفسیر آنچه که دقیقاً انجام می دهد، به شدت با 

218
00:15:17,460 --> 00:15:23,180
ایده رمزگذاری اطلاعات در بردارهای فضای تعبیه شده با ابعاد بالا در هم آمیخته است. .

219
00:15:24,260 --> 00:15:28,866
این درس اصلی است، اما من می خواهم به عقب برگردم و در مورد دو چیز متفاوت فکر کنم، که 

220
00:15:28,866 --> 00:15:33,418
اولی نوعی حسابداری است، و دومی شامل یک واقعیت بسیار قابل تامل در مورد ابعاد بالاتر 

221
00:15:33,418 --> 00:15:38,080
است که من در واقع انجام نداده ام. می دانم تا زمانی که من در ترانسفورماتور حفاری کردم.

222
00:15:41,080 --> 00:15:45,860
در دو فصل آخر، من و شما شروع به شمارش تعداد کل پارامترها در GPT-3 کردیم و دقیقاً 

223
00:15:45,860 --> 00:15:50,760
محل زندگی آنها را دیدیم، بنابراین بیایید به سرعت بازی را در اینجا به پایان برسانیم.

224
00:15:51,400 --> 00:15:56,752
قبلاً اشاره کردم که چگونه این ماتریس پروجکشن به بالا کمتر از 50000 ردیف 

225
00:15:56,752 --> 00:16:02,180
دارد و هر ردیف با اندازه فضای جاسازی مطابقت دارد که برای GPT-3 12288 است.

226
00:16:03,240 --> 00:16:08,732
با ضرب آن‌ها با هم، 604 میلیون پارامتر فقط برای آن ماتریس به ما می‌دهد، 

227
00:16:08,732 --> 00:16:13,920
و پیش‌بینی پایین همان تعداد پارامتر را فقط با یک شکل جابجا شده دارد.

228
00:16:14,500 --> 00:16:17,400
بنابراین با هم، آنها حدود 1.2 میلیارد پارامتر را ارائه می دهند.

229
00:16:18,280 --> 00:16:21,164
بردار بایاس چند پارامتر دیگر را نیز به حساب می‌آورد، اما 

230
00:16:21,164 --> 00:16:24,100
نسبتی ناچیز از کل است، بنابراین من حتی آن را نشان نمی‌دهم.

231
00:16:24,660 --> 00:16:29,003
در GPT-3، این دنباله از بردارهای جاسازی شده از طریق نه یک، 

232
00:16:29,003 --> 00:16:33,495
بلکه 96 MLP مجزا جریان می یابد، بنابراین تعداد کل پارامترهای 

233
00:16:33,495 --> 00:16:38,060
اختصاص داده شده به همه این بلوک ها به حدود 116 میلیارد می رسد.

234
00:16:38,820 --> 00:16:42,788
این تقریباً 2 سوم کل پارامترهای شبکه است، و وقتی آن را به همه 

235
00:16:42,788 --> 00:16:46,948
چیزهایی که قبلاً داشتیم اضافه کنید، برای بلوک‌های توجه، جاسازی و 

236
00:16:46,948 --> 00:16:51,620
عدم تعبیه، در واقع به مجموع کل کل 175 میلیاردی که تبلیغ شده است، می‌رسید.

237
00:16:53,060 --> 00:16:56,598
احتمالاً شایان ذکر است که مجموعه دیگری از پارامترهای مرتبط با آن 

238
00:16:56,598 --> 00:17:00,083
مراحل عادی سازی وجود دارد که این توضیح از آنها صرفنظر کرده است، 

239
00:17:00,083 --> 00:17:03,840
اما مانند بردار بایاس، آنها نسبت بسیار ناچیزی از کل را تشکیل می دهند.

240
00:17:05,900 --> 00:17:09,073
در مورد دومین نکته انعکاس، ممکن است از خود بپرسید که آیا این 

241
00:17:09,073 --> 00:17:12,402
نمونه اسباب بازی مرکزی که ما زمان زیادی را صرف آن کرده ایم نشان 

242
00:17:12,402 --> 00:17:15,680
می دهد که چگونه حقایق در مدل های واقعی زبان بزرگ ذخیره می شوند.

243
00:17:16,319 --> 00:17:20,097
درست است که ردیف‌های آن ماتریس اول را می‌توان به‌عنوان جهت‌هایی در 

244
00:17:20,097 --> 00:17:23,762
این فضای تعبیه‌شده در نظر گرفت، و این بدان معناست که فعال شدن هر 

245
00:17:23,762 --> 00:17:27,540
نورون به شما می‌گوید که یک بردار معین چقدر با جهت خاصی همسو می‌شود.

246
00:17:27,760 --> 00:17:31,107
همچنین درست است که ستون های آن ماتریس دوم به شما می گویند 

247
00:17:31,107 --> 00:17:34,340
که اگر آن نورون فعال باشد چه چیزی به نتیجه اضافه می شود.

248
00:17:34,640 --> 00:17:36,800
هر دوی اینها فقط حقایق ریاضی هستند.

249
00:17:37,740 --> 00:17:41,714
با این حال، شواهد نشان می‌دهد که نورون‌های منفرد به ندرت یک ویژگی 

250
00:17:41,714 --> 00:17:45,869
تمیز مانند مایکل جردن را نشان می‌دهند، و ممکن است در واقع دلیل بسیار 

251
00:17:45,869 --> 00:17:49,904
خوبی برای این موضوع وجود داشته باشد، مربوط به ایده‌ای که این روزها 

252
00:17:49,904 --> 00:17:54,120
در اطراف محققان تفسیرپذیری شناور است که به نام برهم‌نهی شناخته می‌شود.

253
00:17:54,640 --> 00:17:58,393
این فرضیه‌ای است که می‌تواند به توضیح اینکه چرا تفسیر مدل‌ها به‌ویژه 

254
00:17:58,393 --> 00:18:02,420
دشوار است و همچنین چرا مقیاس‌بندی آن‌ها به طرز شگفت‌آوری خوب است، کمک کند.

255
00:18:03,500 --> 00:18:08,522
ایده اصلی این است که اگر یک فضای n بعدی دارید و می‌خواهید مجموعه‌ای از ویژگی‌های 

256
00:18:08,522 --> 00:18:13,606
مختلف را با استفاده از جهت‌هایی که همگی عمود بر یکدیگر هستند در آن فضا نشان دهید، 

257
00:18:13,606 --> 00:18:18,628
می‌دانید که اگر یک جزء را در یک جهت اضافه کنید، هیچ یک از جهات دیگر را تحت تأثیر 

258
00:18:18,628 --> 00:18:23,960
قرار نمی دهد، پس حداکثر تعداد بردارهایی که می توانید قرار دهید فقط n است، تعداد ابعاد.

259
00:18:24,600 --> 00:18:27,620
برای یک ریاضیدان، در واقع، این تعریف بعد است.

260
00:18:28,220 --> 00:18:33,580
اما نکته جالب این است که کمی آن محدودیت را آرام کنید و کمی سر و صدا را تحمل کنید.

261
00:18:34,180 --> 00:18:38,852
فرض کنید اجازه می‌دهید آن ویژگی‌ها با بردارهایی نمایش داده شوند که دقیقاً عمود 

262
00:18:38,852 --> 00:18:43,820
نیستند، آنها فقط تقریباً عمود هستند، شاید بین 89 تا 91 درجه از هم فاصله داشته باشند.

263
00:18:44,820 --> 00:18:48,020
اگر ما در دو یا سه بعدی بودیم، این تفاوتی نمی کند.

264
00:18:48,260 --> 00:18:52,595
این به سختی فضای تکان دادن اضافی را برای قرار دادن بردارهای بیشتری در اختیار شما قرار 

265
00:18:52,595 --> 00:18:56,780
می دهد، که این امر باعث می شود که برای ابعاد بالاتر، پاسخ به طور چشمگیری تغییر کند.

266
00:18:57,660 --> 00:19:01,816
من می توانم یک تصویر بسیار سریع و کثیف از این موضوع را با استفاده از یک 

267
00:19:01,816 --> 00:19:05,972
پایتون خراب به شما ارائه دهم که لیستی از بردارهای 100 بعدی ایجاد می کند 

268
00:19:05,972 --> 00:19:09,955
که هر یک به صورت تصادفی مقداردهی اولیه می شوند و این لیست شامل 10000 

269
00:19:09,955 --> 00:19:14,400
بردار مجزا خواهد بود، بنابراین 100 برابر بردارها. همانطور که ابعاد وجود دارد.

270
00:19:15,320 --> 00:19:19,900
این نمودار دقیقاً در اینجا توزیع زاویه بین جفت این بردارها را نشان می دهد.

271
00:19:20,680 --> 00:19:24,537
بنابراین از آنجایی که آنها به صورت تصادفی شروع شده‌اند، این زاویه‌ها می‌توانند 

272
00:19:24,537 --> 00:19:28,102
از 0 تا 180 درجه باشند، اما متوجه خواهید شد که در حال حاضر، حتی فقط برای 

273
00:19:28,102 --> 00:19:31,960
بردارهای تصادفی، این سوگیری سنگین برای نزدیک‌تر شدن چیزها به 90 درجه وجود دارد.

274
00:19:32,500 --> 00:19:37,061
سپس کاری که من می‌خواهم انجام دهم این است که یک فرآیند بهینه‌سازی خاص را اجرا کنم که به 

275
00:19:37,061 --> 00:19:41,520
طور مکرر همه این بردارها را به‌گونه‌ای هدایت می‌کند که سعی کنند بر یکدیگر عمودتر شوند.

276
00:19:42,060 --> 00:19:46,660
پس از تکرار چندین بار این کار، در اینجا نحوه توزیع زاویه ها به نظر می رسد.

277
00:19:47,120 --> 00:19:51,801
در اینجا باید روی آن زوم کنیم زیرا تمام زوایای ممکن بین 

278
00:19:51,801 --> 00:19:56,900
جفت بردارها در این محدوده باریک بین 89 تا 91 درجه قرار دارند.

279
00:19:58,020 --> 00:20:02,426
به طور کلی، نتیجه چیزی که به عنوان لم جانسون-لیندن اشتراوس شناخته 

280
00:20:02,426 --> 00:20:06,700
می‌شود این است که تعداد بردارهایی که می‌توانید در فضایی تقریباً 

281
00:20:06,700 --> 00:20:10,840
عمود بر هم قرار دهید، با تعداد ابعاد به‌طور تصاعدی رشد می‌کند.

282
00:20:11,960 --> 00:20:15,820
این برای مدل‌های زبان بزرگ، که ممکن است از تداعی ایده‌های 

283
00:20:15,820 --> 00:20:19,880
مستقل با جهت‌های تقریباً عمود بر هم سود ببرند، بسیار مهم است.

284
00:20:20,000 --> 00:20:23,171
این بدان معناست که این امکان برای آن وجود دارد که ایده‌های بسیار 

285
00:20:23,171 --> 00:20:26,440
بسیار بیشتری نسبت به ابعاد موجود در فضای اختصاص داده شده ذخیره کند.

286
00:20:27,320 --> 00:20:29,506
این ممکن است تا حدی توضیح دهد که چرا به نظر می 

287
00:20:29,506 --> 00:20:31,740
رسد عملکرد مدل با اندازه بسیار خوب مقیاس می شود.

288
00:20:32,540 --> 00:20:39,400
فضایی که 10 برابر ابعاد بیشتری دارد، می‌تواند ایده‌های مستقل را بیش از 10 برابر کند.

289
00:20:40,420 --> 00:20:43,777
و این نه تنها به فضای تعبیه‌شده‌ای که در آن بردارهایی که در مدل 

290
00:20:43,777 --> 00:20:47,082
زندگی می‌کنند مربوط می‌شود، بلکه به آن بردار پر از نورون‌ها در 

291
00:20:47,082 --> 00:20:50,440
وسط آن پرسپترون چندلایه‌ای که اخیراً مطالعه کردیم نیز مربوط است.

292
00:20:50,960 --> 00:20:56,229
به عبارت دیگر، در اندازه های GPT-3، ممکن است نه تنها در 50000 ویژگی کاوش کند، 

293
00:20:56,229 --> 00:21:01,498
بلکه اگر در عوض از این ظرفیت افزوده عظیم با استفاده از جهات تقریباً عمودی فضا 

294
00:21:01,498 --> 00:21:07,240
استفاده کند، می تواند در بسیاری از موارد دیگر کاوش کند. ویژگی های بردار در حال پردازش

295
00:21:07,780 --> 00:21:11,030
اما اگر این کار را می‌کرد، معنی آن این بود که ویژگی‌های 

296
00:21:11,030 --> 00:21:14,340
فردی به‌عنوان روشن شدن یک نورون منفرد قابل مشاهده نیستند.

297
00:21:14,660 --> 00:21:19,380
در عوض باید شبیه ترکیب خاصی از نورون ها باشد، یک برهم نهی.

298
00:21:20,400 --> 00:21:24,466
برای هر یک از شما که کنجکاو هستید بیشتر بیاموزید، یک عبارت جستجوی مرتبط کلیدی در اینجا 

299
00:21:24,466 --> 00:21:28,626
رمزگذار خودکار پراکنده است، که ابزاری است که برخی از تفسیرپذیری افراد از آن برای استخراج 

300
00:21:28,626 --> 00:21:32,506
ویژگی‌های واقعی استفاده می‌کنند، حتی اگر آنها خیلی روی همه اینها قرار گرفته باشند. 

301
00:21:32,506 --> 00:21:32,880
نورون ها

302
00:21:33,540 --> 00:21:36,800
من به چند پست واقعا عالی انسان دوستانه در مورد این پیوند خواهم داد.

303
00:21:37,880 --> 00:21:40,564
در این مرحله، ما تمام جزئیات یک ترانسفورماتور را لمس 

304
00:21:40,564 --> 00:21:43,300
نکرده ایم، اما من و شما به مهمترین نکات دست یافته ایم.

305
00:21:43,520 --> 00:21:47,640
نکته اصلی که می خواهم در فصل بعدی به آن بپردازم، فرآیند آموزش است.

306
00:21:48,460 --> 00:21:52,756
از یک طرف، پاسخ کوتاه در مورد نحوه عملکرد آموزش این است که همه اینها پس انتشار است، 

307
00:21:52,756 --> 00:21:56,900
و ما در یک زمینه جداگانه با فصل های قبلی این مجموعه به انتشار پس انتشار پرداختیم.

308
00:21:57,220 --> 00:22:02,500
اما موارد بیشتری برای بحث وجود دارد، مانند تابع هزینه خاص مورد استفاده برای مدل‌های زبان، 

309
00:22:02,500 --> 00:22:07,780
ایده تنظیم دقیق با استفاده از یادگیری تقویتی با بازخورد انسانی، و مفهوم قوانین مقیاس‌بندی.

310
00:22:08,960 --> 00:22:12,733
یادداشت سریع برای فالوورهای فعال در میان شما، تعدادی ویدیوی غیر مرتبط با یادگیری 

311
00:22:12,733 --> 00:22:16,459
ماشینی وجود دارد که من هیجان زده هستم که قبل از ساخت فصل بعدی، دندان هایم را در 

312
00:22:16,459 --> 00:22:20,000
آنها فرو کنم، بنابراین ممکن است مدتی طول بکشد، اما قول می دهم به موقع می آید

313
00:22:35,640 --> 00:22:37,920
متشکرم.

