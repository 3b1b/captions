[
 {
  "translatedText": "اگر به یک زبان بزرگ این عبارت را به زبان بیاورید، مایکل جردن ورزش خالی را انجام می دهد، و شما باید آن را پیش بینی کند که چه اتفاقی می افتد، و بسکتبال را به درستی پیش بینی می کند، این نشان می دهد که در جایی، در درون صدها میلیارد پارامترش، این ورزش در آن پخته شده است. دانش در مورد یک فرد خاص و ورزش خاص او.",
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "translatedText": "و به طور کلی فکر می‌کنم، هرکسی که با یکی از این مدل‌ها بازی کرده است، این حس واضح را دارد که هزاران تن و هزاران واقعیت را به خاطر سپرده است.",
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "translatedText": "بنابراین یک سوال منطقی که می توانید بپرسید این است که دقیقا چگونه کار می کند؟",
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "translatedText": "و این حقایق در کجا زندگی می کنند؟",
  "input": "And where do those facts live?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "translatedText": "دسامبر گذشته، چند محقق از Google DeepMind درباره کار روی این سوال پست کردند و از این مثال خاص از تطبیق ورزشکاران با ورزش خود استفاده کردند.",
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "translatedText": "و اگرچه درک مکانیکی کامل از نحوه ذخیره حقایق حل نشده باقی مانده است، آنها نتایج جزئی جالبی داشتند، از جمله نتیجه گیری سطح بالا بسیار کلی که به نظر می رسد حقایق در داخل بخش خاصی از این شبکه ها زندگی می کنند، که به صورت خیالی به عنوان چند لایه شناخته می شود. پرسپترون یا به اختصار MLP.",
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "translatedText": "در دو فصل آخر، من و شما جزئیات پشت ترانسفورماتورها، معماری زیربنای مدل‌های زبان بزرگ، و همچنین زیربنای بسیاری از هوش مصنوعی مدرن دیگر را بررسی کرده‌ایم.",
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "translatedText": "در فصل اخیر، ما روی قطعه ای به نام توجه تمرکز کردیم.",
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "translatedText": "و قدم بعدی برای من و شما این است که به جزئیات آنچه در داخل این پرسپترون‌های چندلایه رخ می‌دهد، بپردازیم، که بخش بزرگ دیگری از شبکه را تشکیل می‌دهند.",
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "translatedText": "محاسبه در اینجا در واقع نسبتاً ساده است، به خصوص زمانی که آن را با توجه مقایسه کنید.",
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "translatedText": "اساساً به یک جفت ضرب ماتریسی با یک چیز ساده در بین آنها خلاصه می شود.",
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "translatedText": "با این حال، تفسیر آنچه این محاسبات انجام می دهند بسیار چالش برانگیز است.",
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "translatedText": "هدف اصلی ما در اینجا این است که در محاسبات قدم بگذاریم و آنها را به یاد ماندنی کنیم، اما من می‌خواهم این کار را در چارچوب نشان دادن یک مثال خاص از اینکه چگونه یکی از این بلوک‌ها، حداقل در اصل، می‌تواند یک واقعیت ملموس را ذخیره کند، انجام دهم.",
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "translatedText": "به طور خاص، این واقعیت را ذخیره می کند که مایکل جردن بسکتبال بازی می کند.",
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "translatedText": "باید اشاره کنم که چیدمان در اینجا الهام گرفته از گفتگوی من با یکی از آن محققان DeepMind، نیل ناندا است.",
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "translatedText": "در بیشتر موارد، من فرض می‌کنم که شما یا دو فصل گذشته را تماشا کرده‌اید، یا در غیر این صورت، درک اولیه‌ای از ترانسفورماتور دارید، اما تازه‌کننده‌ها هرگز آسیبی نمی‌زنند، بنابراین در اینجا یادآوری سریع جریان کلی است.",
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "translatedText": "من و شما در حال مطالعه مدلی بوده‌ایم که آموزش داده شده است تا متنی را بنویسد و اتفاقات بعدی را پیش‌بینی کند.",
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "translatedText": "آن متن ورودی ابتدا به دسته‌ای از نشانه‌ها تقسیم می‌شود، که به معنای تکه‌های کوچکی است که معمولاً کلمات یا تکه‌های کوچکی از کلمات هستند، و هر نشانه با یک بردار با ابعاد بالا همراه است، که به معنای فهرست طولانی اعداد است.",
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "translatedText": "این دنباله از بردارها سپس به طور مکرر از دو نوع عملیات عبور می کند، توجه، که به بردارها اجازه می دهد اطلاعات را بین یکدیگر منتقل کنند، و سپس پرسپترون های چندلایه، چیزی که امروز به آن می پردازیم، و همچنین یک مرحله عادی سازی مشخص وجود دارد. در بین",
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "translatedText": "پس از اینکه توالی بردارها در بسیاری از تکرارهای مختلف هر دوی این بلوک ها جریان یافت، تا پایان، امید این است که هر بردار اطلاعات کافی را از متن، همه کلمات دیگر در ورودی، و همچنین از دانش کلی که از طریق آموزش در وزن های مدل پخته شده است، که می توان از آن برای پیش بینی اینکه چه توکن بعدی می آید استفاده کرد.",
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "translatedText": "یکی از ایده‌های کلیدی که می‌خواهم در ذهن خود داشته باشید این است که همه این بردارها در یک فضای بسیار بسیار با ابعاد زندگی می‌کنند و وقتی به آن فضا فکر می‌کنید، جهت‌های مختلف می‌توانند انواع مختلفی از معنا را رمزگذاری کنند.",
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "translatedText": "بنابراین یک مثال بسیار کلاسیک که دوست دارم به آن اشاره کنم این است که چگونه اگر به تعبیه زن نگاه کنید و تعبیه مرد را کم کنید و آن قدم کوچک را بردارید و آن را به اسم مذکر دیگری اضافه کنید، چیزی شبیه عمو، فرود می آیید. جایی بسیار بسیار نزدیک به اسم مونث مربوطه.",
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "translatedText": "از این نظر، این جهت خاص اطلاعات جنسیتی را رمزگذاری می کند.",
  "input": "In this sense, this particular direction encodes gender information.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "translatedText": "ایده این است که بسیاری از جهت‌های متمایز دیگر در این فضای فوق‌بعدی می‌توانند با ویژگی‌های دیگری که مدل ممکن است بخواهد نشان دهد مطابقت داشته باشد.",
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "translatedText": "در یک ترانسفورماتور، این بردارها صرفاً معنای یک کلمه را رمزگذاری نمی کنند.",
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "translatedText": "همانطور که آنها از طریق شبکه جریان می یابند، بر اساس تمام زمینه های اطراف خود، و همچنین بر اساس دانش مدل، معنای بسیار غنی تری را دریافت می کنند.",
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "translatedText": "در نهایت، هر یک باید چیزی را بسیار فراتر از معنای یک کلمه رمزگذاری کند، زیرا باید برای پیش‌بینی آنچه در آینده خواهد آمد کافی باشد.",
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "translatedText": "ما قبلاً دیده‌ایم که چگونه بلوک‌های توجه به شما اجازه می‌دهند تا زمینه را ترکیب کنید، اما اکثر پارامترهای مدل در واقع در داخل بلوک‌های MLP زندگی می‌کنند، و یک فکر برای کاری که ممکن است انجام دهند این است که ظرفیت اضافی برای ذخیره حقایق ارائه می‌دهند.",
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "translatedText": "همانطور که گفتم، درس اینجا بر روی نمونه اسباب بازی بتونی تمرکز می کند که چگونه می تواند این واقعیت را که مایکل جردن بسکتبال بازی می کند ذخیره کند.",
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "translatedText": "اکنون، این نمونه اسباب بازی مستلزم آن است که من و شما چند فرض در مورد آن فضای با ابعاد بالا داشته باشیم.",
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "translatedText": "ابتدا، فرض می کنیم که یکی از جهت ها ایده نام کوچک مایکل را نشان می دهد، و سپس جهت تقریباً عمودی دیگری ایده نام خانوادگی جردن را نشان می دهد، و سپس جهت سوم ایده بسکتبال را نشان می دهد.",
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "translatedText": "بنابراین به طور خاص، منظور من از این این است که اگر به شبکه نگاه کنید و یکی از بردارهای در حال پردازش را بردارید، اگر حاصلضرب نقطه آن با این نام کوچک جهت مایکل یکی باشد، رمزگذاری بردار به این معنی است. ایده شخصی با این نام",
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "translatedText": "در غیر این صورت، آن حاصلضرب نقطه صفر یا منفی خواهد بود، به این معنی که بردار واقعاً با آن جهت مطابقت ندارد.",
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "translatedText": "و برای سادگی، بیایید این سوال بسیار منطقی را به طور کامل نادیده بگیریم که اگر آن محصول نقطه ای بزرگتر از یک بود، چه معنایی می تواند داشته باشد.",
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "translatedText": "به طور مشابه، محصول نقطه‌ای آن با این جهت‌های دیگر به شما می‌گوید که آیا نام خانوادگی جردن است یا بسکتبال.",
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "translatedText": "بنابراین فرض کنید یک بردار برای نمایش نام کامل، مایکل جردن، در نظر گرفته شده است، پس حاصلضرب نقطه آن با هر دوی این جهت ها باید یکی باشد.",
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "translatedText": "از آنجایی که متن مایکل جردن شامل دو نشانه متفاوت است، این بدان معناست که باید فرض کنیم که بلوک توجه قبلی با موفقیت اطلاعات را به دومین بردار از این دو بردار منتقل کرده است تا اطمینان حاصل شود که می تواند هر دو نام را رمزگذاری کند.",
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "translatedText": "با همه آن‌ها به عنوان مفروضات، بیایید اکنون به گوشت درس بپردازیم.",
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "translatedText": "درون پرسپترون چندلایه چه اتفاقی می افتد؟",
  "input": "What happens inside a multilayer perceptron?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "translatedText": "ممکن است به این دنباله از بردارها فکر کنید که در بلوک جریان می یابند، و به یاد داشته باشید، هر بردار در ابتدا با یکی از نشانه های متن ورودی مرتبط بود.",
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "translatedText": "اتفاقی که قرار است بیفتد این است که هر بردار منفرد از آن دنباله یک سری عملیات کوتاه را انجام می‌دهد، ما آنها را در یک لحظه باز می‌کنیم و در پایان، بردار دیگری با همان ابعاد به دست می‌آوریم.",
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "translatedText": "آن بردار دیگر به بردار اصلی که وارد شده اضافه می شود، و آن مجموع نتیجه ای است که خارج می شود.",
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "translatedText": "این دنباله از عملیات چیزی است که شما برای هر بردار در دنباله اعمال می کنید، با هر نشانه در ورودی مرتبط است، و همه به صورت موازی اتفاق می افتد.",
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "translatedText": "به ویژه، بردارها در این مرحله با یکدیگر صحبت نمی کنند، همه آنها به نوعی کار خود را انجام می دهند.",
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "translatedText": "و برای من و شما، این در واقع کار را بسیار ساده‌تر می‌کند، زیرا به این معنی است که اگر بفهمیم که فقط برای یکی از بردارها از طریق این بلوک چه اتفاقی می‌افتد، به طور موثر متوجه می‌شویم که برای همه آنها چه اتفاقی می‌افتد.",
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "translatedText": "وقتی می‌گویم این بلوک این واقعیت را رمزگذاری می‌کند که مایکل جردن بسکتبال بازی می‌کند، منظورم این است که اگر یک بردار در آن جریان داشته باشد که نام مایکل و نام خانوادگی جردن را رمزگذاری می‌کند، این دنباله محاسبات چیزی را تولید می‌کند که شامل آن جهت بسکتبال می‌شود. که همان چیزی است که به بردار در آن موقعیت اضافه می کند.",
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "translatedText": "اولین مرحله از این فرآیند شبیه ضرب آن بردار در یک ماتریس بسیار بزرگ است.",
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "translatedText": "جای تعجب نیست، این یک یادگیری عمیق است.",
  "input": "No surprises there, this is deep learning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "translatedText": "و این ماتریس، مانند همه ماتریس‌های دیگری که دیده‌ایم، پر از پارامترهای مدل است که از داده‌ها آموخته می‌شوند، که ممکن است آن‌ها را به‌عنوان دسته‌ای از دستگیره‌ها و شماره‌گیری‌ها در نظر بگیرید که برای تعیین رفتار مدل بهینه‌سازی و تنظیم می‌شوند. .",
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "translatedText": "حال، یک راه خوب برای فکر کردن در مورد ضرب ماتریس این است که هر سطر از آن ماتریس را به عنوان بردار خودش تصور کنید، و دسته ای از محصولات نقطه ای را بین آن سطرها و بردار در حال پردازش قرار دهید، که برای جاسازی آن را به عنوان E برچسب گذاری می کنم.",
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "translatedText": "به عنوان مثال، فرض کنید که ردیف اول برابر با نام کوچک جهت مایکل است که ما فرض می کنیم وجود دارد.",
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "translatedText": "این بدان معناست که اولین مؤلفه در این خروجی، این محصول نقطه‌ای در اینجا، اگر آن بردار نام کوچک مایکل را رمزگذاری کند، یک خواهد بود و در غیر این صورت صفر یا منفی است.",
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "translatedText": "حتی جالب‌تر، لحظه‌ای به این فکر کنید که اگر ردیف اول این نام کوچک مایکل به‌علاوه نام خانوادگی جردن باشد، چه معنایی داشت.",
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "translatedText": "و برای سادگی، اجازه دهید ادامه دهم و آن را به صورت M به اضافه J بنویسم.",
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "translatedText": "سپس، با گرفتن یک محصول نقطه ای با این جاسازی E، همه چیز به خوبی توزیع می شود، بنابراین به نظر می رسد M نقطه E به اضافه J نقطه E.",
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "translatedText": "و توجه کنید که چگونه این بدان معناست که اگر بردار نام کامل مایکل جردن را رمزگذاری کند، مقدار نهایی دو خواهد بود، و در غیر این صورت یک یا چیزی کوچکتر از یک خواهد بود.",
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "translatedText": "و این فقط یک ردیف در این ماتریس است.",
  "input": "And that's just one row in this matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "translatedText": "ممکن است تمام ردیف‌های دیگر را به‌صورت موازی در نظر بگیرید که انواع دیگری از سؤال‌ها را می‌پرسند و در برخی از انواع دیگر ویژگی‌های بردار در حال پردازش تحقیق می‌کنند.",
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "translatedText": "اغلب این مرحله شامل افزودن بردار دیگری به خروجی است که پر از پارامترهای مدل است که از داده ها آموخته شده است.",
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "translatedText": "این بردار دیگر به عنوان سوگیری شناخته می شود.",
  "input": "This other vector is known as the bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "translatedText": "برای مثال، می‌خواهم تصور کنید که مقدار این سوگیری در همان مؤلفه اول منفی است، به این معنی که خروجی نهایی ما مانند محصول نقطه‌ای مربوطه به نظر می‌رسد، اما منهای یک.",
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "translatedText": "ممکن است بسیار منطقی بپرسید که چرا می‌خواهم فرض کنید که مدل این را یاد گرفته است، و در یک لحظه خواهید دید که چرا اگر مقداری در اینجا داشته باشیم مثبت است اگر و فقط اگر یک بردار آن را رمزگذاری کند، بسیار تمیز و خوب است. نام کامل مایکل جردن و در غیر این صورت صفر یا منفی است.",
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "translatedText": "تعداد کل ردیف‌های این ماتریس، که چیزی شبیه به تعداد سؤال‌هایی است که پرسیده می‌شود، در مورد GPT-3، که تعداد آن را دنبال کرده‌ایم، کمتر از 50000 است.",
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "translatedText": "در واقع، دقیقاً چهار برابر ابعاد این فضای تعبیه شده است.",
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "translatedText": "این یک انتخاب طراحی است.",
  "input": "That's a design choice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "translatedText": "می‌توانید آن را بیشتر کنید، می‌توانید آن را کمتر کنید، اما داشتن یک چندگانه تمیز معمولاً برای سخت‌افزار دوستانه است.",
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "translatedText": "از آنجایی که این ماتریس پر از وزن ما را به فضایی با ابعاد بالاتر ترسیم می کند، من به آن مختصر W up می دهم.",
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "translatedText": "من به برچسب گذاری برداری که در حال پردازش آن هستیم به عنوان E ادامه می دهم، و اجازه دهید این بردار سوگیری را به عنوان B بالا برچسب گذاری کنیم و همه آن را در نمودار به پایین برگردانیم.",
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "translatedText": "در این مرحله، یک مشکل این است که این عملیات کاملاً خطی است، اما زبان یک فرآیند بسیار غیر خطی است.",
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "translatedText": "اگر ورودی‌ای که ما اندازه‌گیری می‌کنیم برای مایکل به‌علاوه جردن زیاد باشد، باید تا حدودی توسط مایکل به‌علاوه فلپس و همچنین الکسیس به‌علاوه جردن، علی‌رغم اینکه از نظر مفهومی به هم مرتبط نیستند، راه‌اندازی شود.",
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "translatedText": "چیزی که واقعاً می خواهید یک بله یا خیر ساده برای نام کامل است.",
  "input": "What you really want is a simple yes or no for the full name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "translatedText": "بنابراین مرحله بعدی این است که این بردار میانی بزرگ را از یک تابع غیر خطی بسیار ساده عبور دهیم.",
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "translatedText": "یک انتخاب رایج، انتخابی است که تمام مقادیر منفی را گرفته و آنها را به صفر می‌رساند و همه مقادیر مثبت را بدون تغییر می‌گذارد.",
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "translatedText": "و با ادامه سنت یادگیری عمیق اسامی بیش از حد فانتزی، این تابع بسیار ساده اغلب واحد خطی اصلاح شده یا به اختصار ReLU نامیده می شود.",
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "translatedText": "در اینجا نمودار به نظر می رسد.",
  "input": "Here's what the graph looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "translatedText": "بنابراین با در نظر گرفتن مثال تصوری خود که در آن اولین ورودی بردار میانی یک است، اگر و فقط اگر نام کامل مایکل جردن و صفر یا منفی باشد، در غیر این صورت، پس از عبور از ReLU، به یک مقدار بسیار تمیز می رسید که در آن همه از مقادیر صفر و منفی فقط به صفر بریده می شود.",
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "translatedText": "بنابراین این خروجی برای نام کامل مایکل جردن یک و در غیر این صورت صفر خواهد بود.",
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "translatedText": "به عبارت دیگر، به طور مستقیم رفتار یک دروازه AND را تقلید می کند.",
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "translatedText": "اغلب مدل‌ها از عملکرد کمی تغییر یافته استفاده می‌کنند که JLU نامیده می‌شود، که همان شکل اولیه را دارد، فقط کمی نرم‌تر است.",
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "translatedText": "اما برای اهداف ما، اگر فقط به ReLU فکر کنیم، کمی تمیزتر است.",
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "translatedText": "همچنین، وقتی می شنوید که مردم به نورون های یک ترانسفورماتور اشاره می کنند، در اینجا در مورد این مقادیر صحبت می کنند.",
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "translatedText": "هرگاه آن تصویر شبکه عصبی رایج را با لایه ای از نقاط و دسته ای از خطوط متصل به لایه قبلی مشاهده کردید، که قبلاً در این سری داشتیم، معمولاً به معنای انتقال این ترکیب از یک مرحله خطی، یک ضرب ماتریس، و به دنبال آن است. چند تابع غیر خطی ساده از نظر اصطلاحی مانند ReLU.",
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "translatedText": "شما می گویید که این نورون هر زمان که این مقدار مثبت باشد فعال است و اگر آن مقدار صفر باشد غیرفعال است.",
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "translatedText": "مرحله بعدی بسیار شبیه به مرحله اول است.",
  "input": "The next step looks very similar to the first one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "translatedText": "شما در یک ماتریس بسیار بزرگ ضرب می کنید و یک عبارت تعصب خاصی را اضافه می کنید.",
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "translatedText": "در این مورد، تعداد ابعاد در خروجی به اندازه فضای تعبیه شده کاهش می یابد، بنابراین من ادامه می دهم و این را ماتریس طرح ریزی پایین می نامم.",
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "translatedText": "و این بار، به جای اینکه به چیزها ردیف به ردیف فکر کنیم، در واقع بهتر است که ستون به ستون به آن فکر کنیم.",
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "translatedText": "می بینید، راه دیگری که می توانید ضرب ماتریس را در ذهن خود نگه دارید این است که تصور کنید هر ستون ماتریس را بگیرید و آن را در عبارت مربوطه در بردار که پردازش می کند ضرب کنید و همه آن ستون های تغییر مقیاس شده را با هم جمع کنید.",
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "translatedText": "دلیل اینکه بهتر است به این روش فکر کنیم این است که در اینجا ستون ها همان ابعاد فضای تعبیه شده را دارند، بنابراین می توانیم آنها را به عنوان جهت هایی در آن فضا در نظر بگیریم.",
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "translatedText": "به عنوان مثال، ما تصور خواهیم کرد که مدل یاد گرفته است که اولین ستون را در این جهت بسکتبال که ما فرض می کنیم وجود دارد، بسازد.",
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "translatedText": "معنی آن این است که وقتی نورون مربوطه در آن موقعیت اول فعال است، این ستون را به نتیجه نهایی اضافه می کنیم.",
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "translatedText": "اما اگر آن نورون غیرفعال بود، اگر آن عدد صفر بود، هیچ تاثیری نداشت.",
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "translatedText": "و این فقط نباید بسکتبال باشد.",
  "input": "And it doesn't just have to be basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "translatedText": "این مدل همچنین می‌تواند در این ستون و بسیاری از ویژگی‌های دیگر که می‌خواهد با چیزی که نام کامل مایکل جردن را دارد مرتبط کند، استفاده کند.",
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "translatedText": "و در همان زمان، تمام ستون های دیگر در این ماتریس به شما می گویند که اگر نورون مربوطه فعال باشد، چه چیزی به نتیجه نهایی اضافه می شود.",
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "translatedText": "و اگر در این مورد تعصب دارید، این چیزی است که شما فقط هر بار بدون توجه به مقادیر نورون آن را اضافه می کنید.",
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "translatedText": "ممکن است تعجب کنید که این چه کار می کند.",
  "input": "You might wonder what's that doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "translatedText": "مانند تمام اشیاء پر از پارامتر در اینجا، گفتن دقیق آن به نوعی سخت است.",
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "translatedText": "شاید یک مقدار حسابداری وجود داشته باشد که شبکه باید انجام دهد، اما می توانید فعلاً آن را نادیده بگیرید.",
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "translatedText": "برای اینکه نماد خود را کمی فشرده‌تر کنیم، این ماتریس بزرگ را W پایین می‌نامم و به طور مشابه آن بردار سوگیری B را پایین می‌خوانم و آن را در نمودار خود برمی‌گردانم.",
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "translatedText": "همانطور که قبلاً پیش‌نمایش کردم، کاری که با این نتیجه نهایی انجام می‌دهید این است که آن را به برداری که در آن موقعیت به بلوک جریان می‌یابد اضافه کنید و این نتیجه نهایی را برای شما به ارمغان می‌آورد.",
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "translatedText": "بنابراین برای مثال، اگر بردار جاری در هر دو نام مایکل و نام خانوادگی جردن را رمزگذاری کند، به دلیل اینکه این توالی از عملیات دروازه AND را فعال می‌کند، جهت بسکتبال را اضافه می‌کند، بنابراین آنچه بیرون می‌آید همه آن‌ها را با هم رمزگذاری می‌کند.",
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "translatedText": "و به یاد داشته باشید، این فرآیندی است که به طور موازی برای هر یک از آن بردارها اتفاق می افتد.",
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "translatedText": "به طور خاص، با گرفتن اعداد GPT-3، به این معنی است که این بلوک نه تنها 50000 نورون در خود دارد، بلکه 50000 برابر تعداد توکن ها در ورودی دارد.",
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "translatedText": "بنابراین این کل عملیات است، دو محصول ماتریسی، هر کدام با یک سوگیری اضافه شده و یک تابع برش ساده در بین آنها.",
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "translatedText": "هر یک از شما که ویدیوهای قبلی این مجموعه را تماشا کردید، این ساختار را به عنوان ابتدایی ترین نوع شبکه عصبی که ما در آنجا مطالعه کردیم، تشخیص خواهید داد.",
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "translatedText": "در آن مثال، برای تشخیص ارقام دست نویس آموزش داده شد.",
  "input": "In that example, it was trained to recognize handwritten digits.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "translatedText": "در اینجا، در زمینه یک ترانسفورماتور برای یک مدل زبان بزرگ، این یک قطعه در یک معماری بزرگتر است و هر تلاشی برای تفسیر آنچه که دقیقاً انجام می دهد، به شدت با ایده رمزگذاری اطلاعات در بردارهای فضای تعبیه شده با ابعاد بالا در هم آمیخته است. .",
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "translatedText": "این درس اصلی است، اما من می خواهم به عقب برگردم و در مورد دو چیز متفاوت فکر کنم، که اولی نوعی حسابداری است، و دومی شامل یک واقعیت بسیار قابل تامل در مورد ابعاد بالاتر است که من در واقع انجام نداده ام. می دانم تا زمانی که من در ترانسفورماتور حفاری کردم.",
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "translatedText": "در دو فصل آخر، من و شما شروع به شمارش تعداد کل پارامترها در GPT-3 کردیم و دقیقاً محل زندگی آنها را دیدیم، بنابراین بیایید به سرعت بازی را در اینجا به پایان برسانیم.",
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "translatedText": "قبلاً اشاره کردم که چگونه این ماتریس پروجکشن به بالا کمتر از 50000 ردیف دارد و هر ردیف با اندازه فضای جاسازی مطابقت دارد که برای GPT-3 12288 است.",
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "translatedText": "با ضرب آن‌ها با هم، 604 میلیون پارامتر فقط برای آن ماتریس به ما می‌دهد، و پیش‌بینی پایین همان تعداد پارامتر را فقط با یک شکل جابجا شده دارد.",
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "translatedText": "بنابراین با هم، آنها حدود 1.2 میلیارد پارامتر را ارائه می دهند.",
  "input": "So together, they give about 1.2 billion parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "translatedText": "بردار بایاس چند پارامتر دیگر را نیز به حساب می‌آورد، اما نسبتی ناچیز از کل است، بنابراین من حتی آن را نشان نمی‌دهم.",
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "translatedText": "در GPT-3، این دنباله از بردارهای جاسازی شده از طریق نه یک، بلکه 96 MLP مجزا جریان می یابد، بنابراین تعداد کل پارامترهای اختصاص داده شده به همه این بلوک ها به حدود 116 میلیارد می رسد.",
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "translatedText": "این تقریباً 2 سوم کل پارامترهای شبکه است، و وقتی آن را به همه چیزهایی که قبلاً داشتیم اضافه کنید، برای بلوک‌های توجه، جاسازی و عدم تعبیه، در واقع به مجموع کل کل 175 میلیاردی که تبلیغ شده است، می‌رسید.",
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "translatedText": "احتمالاً شایان ذکر است که مجموعه دیگری از پارامترهای مرتبط با آن مراحل عادی سازی وجود دارد که این توضیح از آنها صرفنظر کرده است، اما مانند بردار بایاس، آنها نسبت بسیار ناچیزی از کل را تشکیل می دهند.",
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "translatedText": "در مورد دومین نکته انعکاس، ممکن است از خود بپرسید که آیا این نمونه اسباب بازی مرکزی که ما زمان زیادی را صرف آن کرده ایم نشان می دهد که چگونه حقایق در مدل های واقعی زبان بزرگ ذخیره می شوند.",
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "translatedText": "درست است که ردیف‌های آن ماتریس اول را می‌توان به‌عنوان جهت‌هایی در این فضای تعبیه‌شده در نظر گرفت، و این بدان معناست که فعال شدن هر نورون به شما می‌گوید که یک بردار معین چقدر با جهت خاصی همسو می‌شود.",
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "translatedText": "همچنین درست است که ستون های آن ماتریس دوم به شما می گویند که اگر آن نورون فعال باشد چه چیزی به نتیجه اضافه می شود.",
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "translatedText": "هر دوی اینها فقط حقایق ریاضی هستند.",
  "input": "Both of those are just mathematical facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "translatedText": "با این حال، شواهد نشان می‌دهد که نورون‌های منفرد به ندرت یک ویژگی تمیز مانند مایکل جردن را نشان می‌دهند، و ممکن است در واقع دلیل بسیار خوبی برای این موضوع وجود داشته باشد، مربوط به ایده‌ای که این روزها در اطراف محققان تفسیرپذیری شناور است که به نام برهم‌نهی شناخته می‌شود.",
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "translatedText": "این فرضیه‌ای است که می‌تواند به توضیح اینکه چرا تفسیر مدل‌ها به‌ویژه دشوار است و همچنین چرا مقیاس‌بندی آن‌ها به طرز شگفت‌آوری خوب است، کمک کند.",
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "translatedText": "ایده اصلی این است که اگر یک فضای n بعدی دارید و می‌خواهید مجموعه‌ای از ویژگی‌های مختلف را با استفاده از جهت‌هایی که همگی عمود بر یکدیگر هستند در آن فضا نشان دهید، می‌دانید که اگر یک جزء را در یک جهت اضافه کنید، هیچ یک از جهات دیگر را تحت تأثیر قرار نمی دهد، پس حداکثر تعداد بردارهایی که می توانید قرار دهید فقط n است، تعداد ابعاد.",
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "translatedText": "برای یک ریاضیدان، در واقع، این تعریف بعد است.",
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "translatedText": "اما نکته جالب این است که کمی آن محدودیت را آرام کنید و کمی سر و صدا را تحمل کنید.",
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "translatedText": "فرض کنید اجازه می‌دهید آن ویژگی‌ها با بردارهایی نمایش داده شوند که دقیقاً عمود نیستند، آنها فقط تقریباً عمود هستند، شاید بین 89 تا 91 درجه از هم فاصله داشته باشند.",
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "translatedText": "اگر ما در دو یا سه بعدی بودیم، این تفاوتی نمی کند.",
  "input": "If we were in two or three dimensions, this makes no difference.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "translatedText": "این به سختی فضای تکان دادن اضافی را برای قرار دادن بردارهای بیشتری در اختیار شما قرار می دهد، که این امر باعث می شود که برای ابعاد بالاتر، پاسخ به طور چشمگیری تغییر کند.",
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "translatedText": "من می توانم یک تصویر بسیار سریع و کثیف از این موضوع را با استفاده از یک پایتون خراب به شما ارائه دهم که لیستی از بردارهای 100 بعدی ایجاد می کند که هر یک به صورت تصادفی مقداردهی اولیه می شوند و این لیست شامل 10000 بردار مجزا خواهد بود، بنابراین 100 برابر بردارها. همانطور که ابعاد وجود دارد.",
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "translatedText": "این نمودار دقیقاً در اینجا توزیع زاویه بین جفت این بردارها را نشان می دهد.",
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "translatedText": "بنابراین از آنجایی که آنها به صورت تصادفی شروع شده‌اند، این زاویه‌ها می‌توانند از 0 تا 180 درجه باشند، اما متوجه خواهید شد که در حال حاضر، حتی فقط برای بردارهای تصادفی، این سوگیری سنگین برای نزدیک‌تر شدن چیزها به 90 درجه وجود دارد.",
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "translatedText": "سپس کاری که من می‌خواهم انجام دهم این است که یک فرآیند بهینه‌سازی خاص را اجرا کنم که به طور مکرر همه این بردارها را به‌گونه‌ای هدایت می‌کند که سعی کنند بر یکدیگر عمودتر شوند.",
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "translatedText": "پس از تکرار چندین بار این کار، در اینجا نحوه توزیع زاویه ها به نظر می رسد.",
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "translatedText": "در اینجا باید روی آن زوم کنیم زیرا تمام زوایای ممکن بین جفت بردارها در این محدوده باریک بین 89 تا 91 درجه قرار دارند.",
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "translatedText": "به طور کلی، نتیجه چیزی که به عنوان لم جانسون-لیندن اشتراوس شناخته می‌شود این است که تعداد بردارهایی که می‌توانید در فضایی تقریباً عمود بر هم قرار دهید، با تعداد ابعاد به‌طور تصاعدی رشد می‌کند.",
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "translatedText": "این برای مدل‌های زبان بزرگ، که ممکن است از تداعی ایده‌های مستقل با جهت‌های تقریباً عمود بر هم سود ببرند، بسیار مهم است.",
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "translatedText": "این بدان معناست که این امکان برای آن وجود دارد که ایده‌های بسیار بسیار بیشتری نسبت به ابعاد موجود در فضای اختصاص داده شده ذخیره کند.",
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "translatedText": "این ممکن است تا حدی توضیح دهد که چرا به نظر می رسد عملکرد مدل با اندازه بسیار خوب مقیاس می شود.",
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "translatedText": "فضایی که 10 برابر ابعاد بیشتری دارد، می‌تواند ایده‌های مستقل را بیش از 10 برابر کند.",
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "translatedText": "و این نه تنها به فضای تعبیه‌شده‌ای که در آن بردارهایی که در مدل زندگی می‌کنند مربوط می‌شود، بلکه به آن بردار پر از نورون‌ها در وسط آن پرسپترون چندلایه‌ای که اخیراً مطالعه کردیم نیز مربوط است.",
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "translatedText": "به عبارت دیگر، در اندازه های GPT-3، ممکن است نه تنها در 50000 ویژگی کاوش کند، بلکه اگر در عوض از این ظرفیت افزوده عظیم با استفاده از جهات تقریباً عمودی فضا استفاده کند، می تواند در بسیاری از موارد دیگر کاوش کند. ویژگی های بردار در حال پردازش",
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "translatedText": "اما اگر این کار را می‌کرد، معنی آن این بود که ویژگی‌های فردی به‌عنوان روشن شدن یک نورون منفرد قابل مشاهده نیستند.",
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "translatedText": "در عوض باید شبیه ترکیب خاصی از نورون ها باشد، یک برهم نهی.",
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "translatedText": "برای هر یک از شما که کنجکاو هستید بیشتر بیاموزید، یک عبارت جستجوی مرتبط کلیدی در اینجا رمزگذار خودکار پراکنده است، که ابزاری است که برخی از تفسیرپذیری افراد از آن برای استخراج ویژگی‌های واقعی استفاده می‌کنند، حتی اگر آنها خیلی روی همه اینها قرار گرفته باشند. نورون ها",
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "translatedText": "من به چند پست واقعا عالی انسان دوستانه در مورد این پیوند خواهم داد.",
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "translatedText": "در این مرحله، ما تمام جزئیات یک ترانسفورماتور را لمس نکرده ایم، اما من و شما به مهمترین نکات دست یافته ایم.",
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "translatedText": "نکته اصلی که می خواهم در فصل بعدی به آن بپردازم، فرآیند آموزش است.",
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "translatedText": "از یک طرف، پاسخ کوتاه در مورد نحوه عملکرد آموزش این است که همه اینها پس انتشار است، و ما در یک زمینه جداگانه با فصل های قبلی این مجموعه به انتشار پس انتشار پرداختیم.",
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "translatedText": "اما موارد بیشتری برای بحث وجود دارد، مانند تابع هزینه خاص مورد استفاده برای مدل‌های زبان، ایده تنظیم دقیق با استفاده از یادگیری تقویتی با بازخورد انسانی، و مفهوم قوانین مقیاس‌بندی.",
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "translatedText": "یادداشت سریع برای فالوورهای فعال در میان شما، تعدادی ویدیوی غیر مرتبط با یادگیری ماشینی وجود دارد که من هیجان زده هستم که قبل از ساخت فصل بعدی، دندان هایم را در آنها فرو کنم، بنابراین ممکن است مدتی طول بکشد، اما قول می دهم به موقع می آید",
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "translatedText": "متشکرم.",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]