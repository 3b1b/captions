[
 {
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "translatedText": "Eğer büyük bir dil modeline \"Michael Jordan boş sporu yapıyor\" cümlesini verip ardından ne geleceğini tahmin etmesini isterseniz ve o da basketbolu doğru tahmin ederse, bu onun yüz milyarlarca parametresinin içinde bir yerlerde belirli bir kişi ve onun belirli sporu hakkında bilgi sahibi olduğunu gösterir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "translatedText": "Ve bence genel olarak, bu modellerden biriyle oynayan herkes, tonlarca ve tonlarca gerçeği ezberlediğini açıkça hissediyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "translatedText": "Bu yüzden sorabileceğiniz makul bir soru, bunun tam olarak nasıl çalıştığıdır?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "input": "And where do those facts live?",
  "translatedText": "Peki bu gerçekler nerede yaşıyor?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "translatedText": "Geçtiğimiz Aralık ayında, Google DeepMind'dan birkaç araştırmacı bu soruyla ilgili çalışmalarını paylaştı ve sporcuları sporlarıyla eşleştirmek gibi özel bir örnek kullandılar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "translatedText": "Gerçeklerin nasıl depolandığına dair tam bir mekanistik anlayış henüz çözülememiş olsa da, gerçeklerin bu ağların çok katmanlı algılayıcılar veya kısaca MLP'ler olarak bilinen belirli bir bölümünde yaşıyor gibi göründüğü çok genel üst düzey sonuç da dahil olmak üzere bazı ilginç kısmi sonuçlar elde ettiler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "translatedText": "Son birkaç bölümde, siz ve ben transformatörlerin, büyük dil modellerinin altında yatan mimarinin ve diğer birçok modern yapay zekanın altında yatan ayrıntılara girdik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "translatedText": "En son bölümde, Attention (Dikkat) adlı bir parçaya odaklanıyorduk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "translatedText": "Sizin ve benim için bir sonraki adım, ağın diğer büyük bölümünü oluşturan bu çok katmanlı algılayıcıların içinde neler olup bittiğinin ayrıntılarına inmektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "translatedText": "Buradaki hesaplama aslında nispeten basittir, özellikle de dikkat ile karşılaştırdığınızda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "translatedText": "Temelde, arada basit bir şey olan bir çift matris çarpımına indirgenir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "translatedText": "Ancak, bu hesaplamaların ne yaptığını yorumlamak son derece zordur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "translatedText": "Buradaki temel amacımız hesaplamaları adım adım inceleyerek akılda kalıcı hale getirmektir, ancak bunu bu bloklardan birinin en azından prensipte somut bir gerçeği nasıl saklayabileceğine dair belirli bir örnek gösterme bağlamında yapmak istiyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "translatedText": "Özellikle de Michael Jordan'ın basketbol oynadığı gerçeğini vurguluyor olacak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "translatedText": "Buradaki planın DeepMind araştırmacılarından Neil Nanda ile yaptığım bir konuşmadan esinlendiğini belirtmeliyim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "translatedText": "Çoğunlukla, ya son iki bölümü izlediğinizi ya da bir transformatörün ne olduğu hakkında temel bir fikriniz olduğunu varsayacağım, ancak tazelemelerin asla zararı olmaz, bu yüzden işte genel akışı hızlı bir şekilde hatırlatıyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "translatedText": "Siz ve ben, bir metin parçasını alıp ardından ne geleceğini tahmin etmek üzere eğitilmiş bir model üzerinde çalışıyoruz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "translatedText": "Bu girdi metni önce bir grup jetona ayrılır, bu da tipik olarak kelimeler veya küçük kelime parçaları olan küçük parçalar anlamına gelir ve her jeton yüksek boyutlu bir vektörle, yani uzun bir sayı listesiyle ilişkilendirilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "translatedText": "Bu vektör dizisi daha sonra tekrar tekrar iki tür işlemden geçer; vektörlerin birbirleri arasında bilgi aktarmasını sağlayan dikkat ve daha sonra bugün inceleyeceğimiz şey olan çok katmanlı algılayıcılar ve ayrıca arada belirli bir normalleştirme adımı vardır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "translatedText": "Vektör dizisi bu blokların her ikisinin de birçok farklı yinelemesinden geçtikten sonra, sonunda, her vektörün hem bağlamdan, girdideki diğer tüm kelimelerden hem de eğitim yoluyla model ağırlıklarına eklenen genel bilgiden, bir sonraki belirtecin ne olacağına dair bir tahmin yapmak için kullanılabilecek kadar bilgi emmiş olması umulur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "translatedText": "Aklınızda bulunmasını istediğim temel fikirlerden biri, tüm bu vektörlerin çok çok yüksek boyutlu bir uzayda yaşadığı ve bu uzay hakkında düşündüğünüzde, farklı yönlerin farklı anlam türlerini kodlayabileceğidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "translatedText": "Tekrar başvurmayı sevdiğim çok klasik bir örnek, kadının gömülmesine bakarsanız ve erkeğin gömülmesini çıkarırsanız ve bu küçük adımı atıp başka bir eril isme, amca gibi bir şeye eklerseniz, karşılık gelen dişil isme çok çok yakın bir yere varırsınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "input": "In this sense, this particular direction encodes gender information.",
  "translatedText": "Bu anlamda, bu özel yön cinsiyet bilgisini kodlamaktadır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "translatedText": "Buradaki fikir, bu süper yüksek boyutlu uzaydaki diğer birçok farklı yönün, modelin temsil etmek isteyebileceği diğer özelliklere karşılık gelebileceğidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "translatedText": "Bir dönüştürücüde, bu vektörler yalnızca tek bir kelimenin anlamını kodlamaz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "translatedText": "Ağ boyunca aktıkça, etraflarındaki tüm bağlama ve modelin bilgisine dayalı olarak çok daha zengin bir anlam kazanırlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "translatedText": "Nihayetinde, her birinin tek bir kelimenin anlamının çok çok ötesinde bir şeyi kodlaması gerekir, çünkü bir sonraki adımın ne olacağını tahmin etmek için yeterli olması gerekir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "translatedText": "Dikkat bloklarının bağlamı dahil etmenize nasıl izin verdiğini zaten gördük, ancak model parametrelerinin çoğu aslında MLP bloklarının içinde yaşıyor ve ne yapıyor olabileceklerine dair bir düşünce, gerçekleri depolamak için ekstra kapasite sunmalarıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "translatedText": "Dediğim gibi, buradaki ders Michael Jordan'ın basketbol oynadığı gerçeğini tam olarak nasıl saklayabileceğine dair somut bir oyuncak örneği üzerinde yoğunlaşacak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "translatedText": "Şimdi, bu oyuncak örnek sizin ve benim bu yüksek boyutlu uzay hakkında birkaç varsayım yapmamızı gerektirecek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "translatedText": "İlk olarak, yönlerden birinin Michael adındaki bir ilk isim fikrini temsil ettiğini ve daha sonra neredeyse dik olan başka bir yönün Jordan soyadı fikrini temsil ettiğini ve daha sonra üçüncü bir yönün basketbol fikrini temsil edeceğini varsayacağız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "translatedText": "Bununla kastettiğim şey, eğer ağa bakarsanız ve işlenen vektörlerden birini koparırsanız, bu ilk isim Michael yönü ile nokta çarpımı bir ise, vektörün bu ilk isme sahip bir kişi fikrini kodladığı anlamına gelecektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "translatedText": "Aksi takdirde, bu nokta çarpımı sıfır veya negatif olur, yani vektör gerçekten o yönle hizalanmaz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "translatedText": "Ve basitlik açısından, nokta çarpımın birden büyük olmasının ne anlama gelebileceği gibi çok makul bir soruyu tamamen göz ardı edelim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "translatedText": "Benzer şekilde, diğer yönlerle nokta çarpımı size Jordan soyadını mı yoksa basketbolu mu temsil ettiğini söyleyecektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "translatedText": "Diyelim ki bir vektör Michael Jordan'ın tam adını temsil ediyor, o zaman bu yönlerin her ikisiyle de nokta çarpımı bir olmalıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "translatedText": "Michael Jordan metni iki farklı simgeyi kapsadığından, bu aynı zamanda daha önceki bir dikkat bloğunun her iki ismi de kodlayabilmesini sağlamak için bu iki vektörden ikincisine başarılı bir şekilde bilgi aktardığını varsaymamız gerektiği anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "translatedText": "Tüm bunlar varsayım olarak kabul edildiğinde, şimdi dersin etine dalalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "input": "What happens inside a multilayer perceptron?",
  "translatedText": "Çok katmanlı bir algılayıcının içinde ne olur?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "translatedText": "Bu vektör dizisinin bloğa aktığını düşünebilirsiniz ve her vektörün başlangıçta giriş metnindeki belirteçlerden biriyle ilişkilendirildiğini hatırlayabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "translatedText": "Olacak olan şey, bu dizideki her bir vektörün kısa bir dizi işlemden geçmesidir, bunları birazdan açacağız ve sonunda aynı boyutta başka bir vektör elde edeceğiz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "translatedText": "Bu diğer vektör, içeri akan orijinal vektöre eklenecek ve bu toplam dışarı akan sonuç olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "translatedText": "Bu işlemler dizisi, girdideki her belirteçle ilişkili olarak dizideki her vektöre uyguladığınız bir şeydir ve hepsi paralel olarak gerçekleşir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "translatedText": "Özellikle, vektörler bu adımda birbirleriyle konuşmazlar, hepsi kendi işlerini yaparlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "translatedText": "Sizin ve benim için bu aslında işi çok daha basit hale getiriyor, çünkü bu blok boyunca vektörlerden sadece birine ne olduğunu anlarsak, hepsine ne olduğunu etkili bir şekilde anlamış oluruz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "translatedText": "Bu bloğun Michael Jordan'ın basketbol oynadığı gerçeğini kodlayacağını söylediğimde, demek istediğim, adı Michael ve soyadı Jordan'ı kodlayan bir vektör gelirse, bu hesaplama dizisi basketbol yönünü içeren bir şey üretecektir, bu da o konumdaki vektöre eklenecek olan şeydir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "translatedText": "Bu sürecin ilk adımı, bu vektörü çok büyük bir matrisle çarpmaya benziyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "input": "No surprises there, this is deep learning.",
  "translatedText": "Sürpriz yok, bu derin öğrenme.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "translatedText": "Ve bu matris, gördüğümüz diğer tüm matrisler gibi, model davranışının ne olduğunu belirlemek için ayarlanan ve ayarlanan bir grup düğme ve kadran olarak düşünebileceğiniz verilerden öğrenilen model parametreleriyle doldurulur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "translatedText": "Şimdi, matris çarpımını düşünmenin güzel bir yolu, bu matrisin her satırını kendi vektörü olarak hayal etmek ve bu satırlar ile işlenen vektör arasında bir grup nokta çarpımı almaktır, bunu gömme için E olarak etiketleyeceğim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "translatedText": "Örneğin, ilk satırın var olduğunu varsaydığımız Michael direction ismine eşit olduğunu varsayalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "translatedText": "Bu, bu çıktıdaki ilk bileşenin, buradaki nokta çarpımının, bu vektör ilk adı Michael'ı kodluyorsa bir, aksi takdirde sıfır veya negatif olacağı anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "translatedText": "Daha da eğlencelisi, bir an için bu ilk sıranın adı Michael artı soyadı Jordan yönünde olsaydı ne anlama geleceğini düşünün.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "translatedText": "Basit olması için bunu M artı J olarak yazmama izin verin.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "translatedText": "Sonra, bu E gömülmesiyle bir nokta çarpımı aldığımızda, işler gerçekten güzel bir şekilde dağılır, böylece M nokta E artı J nokta E gibi görünür.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "translatedText": "Ve bunun, vektör Michael Jordan adını tam olarak kodlarsa nihai değerin iki olacağı ve aksi takdirde bir veya birden küçük bir şey olacağı anlamına geldiğine dikkat edin.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "input": "And that's just one row in this matrix.",
  "translatedText": "Ve bu matriste sadece bir satır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "translatedText": "Diğer tüm satırların paralel olarak başka tür sorular sorduğunu, işlenmekte olan vektörün başka tür özelliklerini araştırdığını düşünebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "translatedText": "Çoğu zaman bu adım, çıktıya verilerden öğrenilen model parametreleriyle dolu başka bir vektörün eklenmesini de içerir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "input": "This other vector is known as the bias.",
  "translatedText": "Bu diğer vektör önyargı olarak bilinir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "translatedText": "Örneğimiz için, ilk bileşendeki bu sapmanın değerinin negatif bir olduğunu hayal etmenizi istiyorum, yani nihai çıktımız ilgili nokta çarpımına benziyor, ancak eksi bir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "translatedText": "Modelin bunu öğrendiğini varsaymanızı neden istediğimi çok makul bir şekilde sorabilirsiniz ve birazdan burada bir vektörün Michael Jordan adını tam olarak kodlaması durumunda pozitif, aksi takdirde sıfır veya negatif olan bir değere sahip olmamızın neden çok temiz ve güzel olduğunu göreceksiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "translatedText": "Sayılarını takip ettiğimiz GPT-3 örneğinde, sorulan soru sayısı gibi bir şey olan bu matristeki toplam satır sayısı 50.000'in biraz altındadır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "translatedText": "Aslında, bu gömme uzayındaki boyut sayısının tam dört katıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "input": "That's a design choice.",
  "translatedText": "Bu bir tasarım tercihi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "translatedText": "Daha fazla yapabilirsiniz, daha az yapabilirsiniz, ancak temiz bir çokluğa sahip olmak donanım için dostça olma eğilimindedir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "translatedText": "Ağırlıklarla dolu bu matris bizi daha yüksek boyutlu bir uzaya eşlediğinden, buna W up kısaltmasını vereceğim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "translatedText": "İşlediğimiz vektörü E olarak etiketlemeye devam edeceğim ve bu önyargı vektörünü B olarak etiketleyelim ve hepsini diyagrama geri koyalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "translatedText": "Bu noktada bir sorun, bu işlemin tamamen doğrusal olması, ancak dilin çok doğrusal olmayan bir süreç olmasıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "translatedText": "Ölçtüğümüz giriş Michael artı Jordan için yüksekse, kavramsal olarak ilgisiz olmalarına rağmen Michael artı Phelps ve Alexis artı Jordan tarafından da bir şekilde tetiklenecektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "input": "What you really want is a simple yes or no for the full name.",
  "translatedText": "Gerçekten istediğiniz şey, tam ad için basit bir evet veya hayır cevabıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "translatedText": "Dolayısıyla bir sonraki adım, bu büyük ara vektörü çok basit bir doğrusal olmayan fonksiyondan geçirmektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "translatedText": "Yaygın bir seçim, tüm negatif değerleri alıp sıfıra eşleyen ve tüm pozitif değerleri değiştirmeden bırakan bir seçimdir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "translatedText": "Ve aşırı süslü isimlerin derin öğrenme geleneğini sürdürerek, bu çok basit fonksiyon genellikle düzeltilmiş doğrusal birim veya kısaca ReLU olarak adlandırılır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "input": "Here's what the graph looks like.",
  "translatedText": "İşte grafik böyle görünüyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "translatedText": "Ara vektörün bu ilk girişinin bir olduğu hayal örneğimizi ele alırsak, tam adı Michael Jordan ise bir, aksi takdirde sıfır veya negatif, ReLU'dan geçirdikten sonra, tüm sıfır ve negatif değerlerin sıfıra kırpıldığı çok temiz bir değer elde edersiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "translatedText": "Dolayısıyla, bu çıktı Michael Jordan tam adı için bir, aksi takdirde sıfır olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "translatedText": "Başka bir deyişle, bir AND kapısının davranışını çok doğrudan taklit eder.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "translatedText": "Genellikle modeller, JLU adı verilen ve aynı temel şekle sahip, sadece biraz daha yumuşak olan biraz değiştirilmiş bir işlev kullanır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "translatedText": "Ancak bizim amaçlarımız doğrultusunda, sadece ReLU'yu düşünürsek bu biraz daha temiz olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "translatedText": "Ayrıca, insanların bir transformatörün nöronlarından bahsettiğini duyduğunuzda, tam burada bu değerlerden bahsediyorlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "translatedText": "Bu seride daha önce gördüğümüz, bir nokta katmanı ve bir önceki katmana bağlanan bir grup çizgi içeren yaygın sinir ağı resmini gördüğünüzde, bu genellikle doğrusal bir adımın, bir matris çarpımının ve ardından ReLU gibi basit bir terimsel doğrusal olmayan işlevin bu kombinasyonunu ifade etmek içindir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "translatedText": "Bu nöronun, bu değer pozitif olduğunda aktif olduğunu ve bu değer sıfır olduğunda inaktif olduğunu söyleyebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "input": "The next step looks very similar to the first one.",
  "translatedText": "Bir sonraki adım ilk adıma çok benziyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "translatedText": "Çok büyük bir matrisle çarparsınız ve belirli bir yanlılık terimi eklersiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "translatedText": "Bu durumda, çıktıdaki boyut sayısı bu gömme uzayının boyutuna geri döner, bu yüzden devam edeceğim ve buna aşağı projeksiyon matrisi diyeceğim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "translatedText": "Ve bu sefer, her şeyi satır satır düşünmek yerine, sütun sütun düşünmek aslında daha güzel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "translatedText": "Matris çarpımını kafanızda tutmanın bir başka yolu da matrisin her bir sütununu alıp işlediği vektördeki ilgili terimle çarptığınızı ve tüm bu yeniden ölçeklendirilmiş sütunları topladığınızı hayal etmektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "translatedText": "Bu şekilde düşünmenin daha güzel olmasının nedeni, burada sütunların gömme uzayı ile aynı boyuta sahip olmasıdır, bu nedenle onları bu uzaydaki yönler olarak düşünebiliriz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "translatedText": "Örneğin, modelin ilk sütunu var olduğunu varsaydığımız bu basketbol yönüne doğru yapmayı öğrendiğini hayal edeceğiz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "translatedText": "Bunun anlamı, ilk pozisyondaki ilgili nöron aktif olduğunda, bu sütunu nihai sonuca ekleyeceğimizdir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "translatedText": "Ancak bu nöron etkin değilse, bu sayı sıfırsa, o zaman bunun hiçbir etkisi olmayacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "input": "And it doesn't just have to be basketball.",
  "translatedText": "Ve bu sadece basketbol olmak zorunda değil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "translatedText": "Model ayrıca bu sütuna ve tam adı Michael Jordan olan bir şeyle ilişkilendirmek istediği diğer birçok özelliği de ekleyebilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "translatedText": "Ve aynı zamanda, bu matristeki diğer tüm sütunlar, ilgili nöronun aktif olması durumunda nihai sonuca neyin ekleneceğini size söyler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "translatedText": "Ve bu durumda bir önyargınız varsa, nöron değerlerinden bağımsız olarak her seferinde eklediğiniz bir şeydir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "input": "You might wonder what's that doing.",
  "translatedText": "Bunun ne işe yaradığını merak edebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "translatedText": "Buradaki tüm parametre dolu nesnelerde olduğu gibi, tam olarak söylemek biraz zor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "translatedText": "Belki ağın yapması gereken bazı muhasebe işlemleri vardır, ancak şimdilik bunu görmezden gelebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "translatedText": "Notasyonumuzu biraz daha kompakt hale getirerek, bu büyük matrise W diyeceğim ve benzer şekilde bu önyargı vektörüne B diyeceğim ve bunu diyagramımıza geri koyacağım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "translatedText": "Daha önce önizlemesini yaptığım gibi, bu nihai sonuçla yaptığınız şey, onu o konumda bloğa akan vektöre eklemektir ve bu da size bu nihai sonucu verir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "translatedText": "Örneğin, içeri akan vektör hem Michael adını hem de Jordan soyadını kodladıysa, bu işlem dizisi AND kapısını tetikleyeceğinden, basketbol yönünü ekleyecektir, böylece ortaya çıkan şey bunların hepsini birlikte kodlayacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "translatedText": "Ve unutmayın, bu vektörlerin her biri için paralel olarak gerçekleşen bir süreçtir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "translatedText": "Özellikle GPT-3 sayılarını ele alırsak, bu blokta sadece 50.000 nöron değil, girdideki belirteç sayısının 50.000 katı var demektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "translatedText": "İşte tüm işlem bu, her birine bir önyargı eklenmiş iki matris ürünü ve arada basit bir kırpma işlevi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "translatedText": "Serinin önceki videolarını izleyenler, bu yapıyı orada incelediğimiz en temel sinir ağı türü olarak tanıyacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "input": "In that example, it was trained to recognize handwritten digits.",
  "translatedText": "Bu örnekte, el yazısı rakamları tanımak üzere eğitilmiştir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "translatedText": "Burada, büyük bir dil modeli için bir dönüştürücü bağlamında, bu daha büyük bir mimarinin bir parçasıdır ve tam olarak ne yaptığını yorumlamaya yönelik herhangi bir girişim, bilgiyi yüksek boyutlu bir gömme uzayının vektörlerine kodlama fikriyle büyük ölçüde iç içe geçmiştir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "translatedText": "Temel ders bu, ancak bir adım geri çekilip iki farklı şey üzerinde düşünmek istiyorum; bunlardan ilki bir tür muhasebe, ikincisi ise transformatörleri araştırana kadar bilmediğim yüksek boyutlarla ilgili çok düşündürücü bir gerçeği içeriyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "translatedText": "Son iki bölümde, siz ve ben GPT-3'teki toplam parametre sayısını saymaya ve tam olarak nerede bulunduklarını görmeye başladık, bu yüzden oyunu burada hızlıca bitirelim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "translatedText": "Bu yukarı izdüşüm matrisinin 50.000 satırdan biraz daha az olduğunu ve her satırın GPT-3 için 12.288 olan gömme uzayının boyutuyla eşleştiğini daha önce belirtmiştim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "translatedText": "Bunları çarparsak, sadece bu matris için 604 milyon parametre elde ederiz ve aşağı izdüşüm de aynı sayıda parametreye sahiptir, sadece şekil değiştirmiştir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "input": "So together, they give about 1.2 billion parameters.",
  "translatedText": "Yani birlikte yaklaşık 1,2 milyar parametre verirler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "translatedText": "Eğilim vektörü ayrıca birkaç parametreyi daha hesaba katar, ancak toplamın önemsiz bir oranıdır, bu yüzden onu göstermeyeceğim bile.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "translatedText": "GPT-3'te bu gömme vektörleri dizisi bir değil 96 farklı MLP'den geçmektedir, dolayısıyla tüm bu bloklara ayrılan toplam parametre sayısı yaklaşık 116 milyara ulaşmaktadır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "translatedText": "Bu, ağdaki toplam parametrelerin yaklaşık üçte ikisidir ve bunu daha önce sahip olduğumuz her şeye eklediğinizde, dikkat blokları, gömme ve gömülmeyi kaldırma için, gerçekten de ilan edildiği gibi 175 milyarlık büyük toplamı elde edersiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "translatedText": "Muhtemelen bu açıklamanın atladığı normalleştirme adımlarıyla ilişkili başka bir parametre setinden bahsetmeye değer, ancak önyargı vektörü gibi, bunlar da toplamın çok önemsiz bir oranını oluşturuyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "translatedText": "İkinci düşünce noktasına gelince, üzerinde bu kadar çok zaman harcadığımız bu merkezi oyuncak örneğinin gerçek büyük dil modellerinde gerçeklerin nasıl depolandığını yansıtıp yansıtmadığını merak ediyor olabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "translatedText": "Bu ilk matrisin satırlarının bu gömme uzayında yönler olarak düşünülebileceği doğrudur ve bu, her nöronun aktivasyonunun size belirli bir vektörün belirli bir yönle ne kadar hizalandığını söylediği anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "translatedText": "Bu ikinci matrisin sütunlarının, söz konusu nöronun aktif olması durumunda sonuca ne ekleneceğini söylediği de doğrudur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "input": "Both of those are just mathematical facts.",
  "translatedText": "Bunların her ikisi de sadece matematiksel gerçeklerdir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "translatedText": "Bununla birlikte, kanıtlar, bireysel nöronların Michael Jordan gibi tek bir temiz özelliği çok nadiren temsil ettiğini göstermektedir ve aslında bu durumun, bugünlerde yorumlanabilirlik araştırmacıları arasında dolaşan ve süperpozisyon olarak bilinen bir fikirle ilgili çok iyi bir nedeni olabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "translatedText": "Bu, hem modellerin yorumlanmasının neden özellikle zor olduğunu hem de neden şaşırtıcı derecede iyi ölçeklendirildiklerini açıklamaya yardımcı olabilecek bir hipotezdir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "translatedText": "Temel fikir, n boyutlu bir uzaya sahipseniz ve bu uzayda birbirine dik olan yönleri kullanarak bir dizi farklı özelliği temsil etmek istiyorsanız, bilirsiniz, bu şekilde bir yöne bir bileşen eklerseniz, diğer yönlerin hiçbirini etkilemez, o zaman sığdırabileceğiniz maksimum vektör sayısı sadece n, boyut sayısıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "translatedText": "Aslında bir matematikçi için bu, boyutun tanımıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "translatedText": "Ancak işin ilginçleştiği nokta, bu kısıtlamayı biraz gevşetir ve biraz gürültüye tolerans gösterirseniz ortaya çıkar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "translatedText": "Bu özelliklerin tam olarak dik olmayan vektörlerle temsil edilmesine izin verdiğinizi varsayalım, bunlar sadece neredeyse dik, belki 89 ila 91 derece arasında.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "input": "If we were in two or three dimensions, this makes no difference.",
  "translatedText": "Eğer iki ya da üç boyutlu olsaydık, bu hiçbir fark yaratmazdı.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "translatedText": "Bu da size daha fazla vektör sığdırmak için neredeyse hiç ekstra kıpırdama alanı bırakmıyor, bu da daha yüksek boyutlar için cevabın dramatik bir şekilde değişmesini daha da mantıksız hale getiriyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "translatedText": "Her biri rastgele başlatılmış 100 boyutlu vektörlerden oluşan bir liste oluşturacak ve bu liste 10.000 farklı vektör içerecek, yani boyutların 100 katı kadar vektör olacak olan bir Python kullanarak bunun gerçekten hızlı ve kirli bir örneğini verebilirim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "translatedText": "Buradaki çizim, bu vektörlerin çiftleri arasındaki açıların dağılımını göstermektedir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "translatedText": "Rastgele başladıkları için, bu açılar 0 ila 180 derece arasında herhangi bir şey olabilir, ancak sadece rastgele vektörler için bile, şeylerin 90 dereceye yakın olması için ağır bir önyargı olduğunu fark edeceksiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "translatedText": "Daha sonra yapacağım şey, tüm bu vektörleri birbirlerine daha dik hale gelmeye çalışacak şekilde iteratif olarak dürtecek belirli bir optimizasyon sürecini çalıştırmaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "translatedText": "Bunu birçok kez tekrarladıktan sonra, işte açıların dağılımı nasıl görünüyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "translatedText": "Aslında burada yakınlaştırmamız gerekiyor çünkü vektör çiftleri arasındaki tüm olası açılar 89 ile 91 derece arasındaki bu dar aralığın içinde yer alıyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "translatedText": "Genel olarak, Johnson-Lindenstrauss lemması olarak bilinen bir şeyin sonucu, bu şekilde neredeyse dik olan bir uzaya tıkıştırabileceğiniz vektörlerin sayısının boyut sayısıyla üstel olarak artmasıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "translatedText": "Bu, bağımsız fikirlerin neredeyse birbirine dik yönlerle ilişkilendirilmesinden fayda sağlayabilecek büyük dil modelleri için çok önemlidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "translatedText": "Bu, kendisine ayrılan alandaki boyutlardan çok daha fazla fikri depolamasının mümkün olduğu anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "translatedText": "Bu, model performansının boyutla neden bu kadar iyi ölçeklendiğini kısmen açıklayabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "translatedText": "Boyutları 10 kat daha fazla olan bir alan, 10 kattan çok daha fazla sayıda bağımsız fikri depolayabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "translatedText": "Ve bu sadece modelden akan vektörlerin yaşadığı gömme uzayı için değil, aynı zamanda az önce incelediğimiz çok katmanlı algılayıcının ortasındaki nöronlarla dolu vektör için de geçerlidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "translatedText": "Yani, GPT-3'ün boyutlarında, sadece 50.000 özelliği araştırmakla kalmayabilir, bunun yerine uzayın neredeyse dik yönlerini kullanarak bu muazzam ek kapasiteden yararlanırsa, işlenmekte olan vektörün çok daha fazla özelliğini araştırabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "translatedText": "Ancak bunu yapıyorsa, bunun anlamı bireysel özelliklerin tek bir nöronun yanması olarak görünmeyeceğidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "translatedText": "Bunun yerine nöronların belirli bir kombinasyonu, bir süperpozisyon gibi görünmesi gerekirdi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "translatedText": "Daha fazla bilgi edinmek isteyenler için buradaki önemli bir arama terimi seyrek oto kodlayıcıdır; bu, bazı yorumlanabilirlik uzmanlarının tüm bu nöronların üzerine çok fazla bindirilmiş olsalar bile gerçek özelliklerin ne olduğunu çıkarmaya çalışmak için kullandıkları bir araçtır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "translatedText": "Bununla ilgili gerçekten harika birkaç antropik yazıya bağlantı vereceğim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "translatedText": "Bu noktada, bir transformatörün her ayrıntısına değinmedik, ancak siz ve ben en önemli noktalara değindik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "translatedText": "Bir sonraki bölümde ele almak istediğim ana konu eğitim sürecidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "translatedText": "Bir yandan, eğitimin nasıl çalıştığına dair kısa cevap, her şeyin geri yayılım olduğudur ve serinin önceki bölümlerinde geri yayılımı ayrı bir bağlamda ele aldık.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "translatedText": "Ancak, dil modelleri için kullanılan özel maliyet fonksiyonu, insan geri bildirimi ile takviyeli öğrenme kullanarak ince ayar yapma fikri ve ölçeklendirme yasaları kavramı gibi tartışılacak daha çok şey var.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "translatedText": "Aranızdaki aktif takipçiler için hızlı bir not, bir sonraki bölümü yapmadan önce dişlerimi batırmak için heyecanlandığım makine öğrenimi ile ilgili olmayan bir dizi video var, bu yüzden biraz zaman alabilir, ancak zamanında geleceğine söz veriyorum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "input": "Thank you.",
  "translatedText": "Teşekkür ederim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]