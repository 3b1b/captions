1
00:00:00,000 --> 00:00:03,162
Ha egy nagy nyelvi modellt azzal a kifejezéssel táplálsz, 

2
00:00:03,162 --> 00:00:07,742
hogy Michael Jordan a blank sportot űzi, és megjósoltatod vele, hogy mi következik, 

3
00:00:07,742 --> 00:00:11,886
és helyesen jósolja meg a kosárlabdát, akkor ez azt sugallja, hogy valahol, 

4
00:00:11,886 --> 00:00:16,302
a több százmilliárd paraméterében valahol egy konkrét személyről és az ő konkrét 

5
00:00:16,302 --> 00:00:18,320
sportágáról szóló tudás van beépítve.

6
00:00:18,940 --> 00:00:22,540
És azt hiszem, általában véve, aki játszott már egy ilyen modellel, 

7
00:00:22,540 --> 00:00:25,400
az egyértelműen érzi, hogy rengeteg tényt megjegyzett.

8
00:00:25,700 --> 00:00:29,160
Tehát egy ésszerű kérdés, amit feltehetünk, hogy pontosan hogyan is működik ez?

9
00:00:29,160 --> 00:00:31,040
És hol élnek ezek a tények?

10
00:00:35,720 --> 00:00:38,797
Tavaly decemberben a Google DeepMind néhány kutatója a kérdéssel 

11
00:00:38,797 --> 00:00:42,159
kapcsolatos munkájáról számolt be, és ezt a konkrét példát használták, 

12
00:00:42,159 --> 00:00:44,480
amikor a sportolókat a sportágukhoz illesztették.

13
00:00:44,900 --> 00:00:49,348
És bár a tények tárolásának teljes mechanisztikus megértése továbbra is megoldatlan, 

14
00:00:49,348 --> 00:00:53,168
érdekes részeredményeket értek el, többek között azt a nagyon általános, 

15
00:00:53,168 --> 00:00:55,994
magas szintű következtetést, hogy a tények úgy tűnik, 

16
00:00:55,994 --> 00:01:00,128
hogy e hálózatok egy bizonyos részében élnek, amelyet fantáziadúsan többrétegű 

17
00:01:00,128 --> 00:01:02,640
perceptronoknak, vagy röviden MLP-knek neveznek.

18
00:01:03,120 --> 00:01:05,919
Az elmúlt néhány fejezetben ön és én a transzformátorok, 

19
00:01:05,919 --> 00:01:09,013
a nagy nyelvi modellek, valamint számos más modern mesterséges 

20
00:01:09,013 --> 00:01:12,500
intelligencia mögött álló architektúra részleteibe ástuk bele magunkat.

21
00:01:13,060 --> 00:01:16,200
A legutóbbi fejezetben a Figyelem című darabra koncentráltunk.

22
00:01:16,840 --> 00:01:20,244
A következő lépés pedig az, hogy ön és én belemerüljünk a részletekbe, 

23
00:01:20,244 --> 00:01:22,882
hogy mi történik ezekben a többrétegű perceptronokban, 

24
00:01:22,882 --> 00:01:25,040
amelyek a hálózat másik nagy részét alkotják.

25
00:01:25,680 --> 00:01:30,100
A számítás itt valójában viszonylag egyszerű, különösen, ha a figyelemhez hasonlítjuk.

26
00:01:30,560 --> 00:01:34,980
Lényegében egy pár mátrixszorzásra fut ki, és egy egyszerű valamire a kettő között.

27
00:01:35,720 --> 00:01:40,460
Azonban rendkívül nagy kihívás értelmezni, hogy mit is csinálnak ezek a számítások.

28
00:01:41,560 --> 00:01:45,910
A fő célunk itt az, hogy végigmenjünk a számításokon, és emlékezetessé tegyük őket, 

29
00:01:45,910 --> 00:01:50,363
de ezt egy konkrét példa bemutatásával szeretném megtenni, hogy az egyik ilyen blokk, 

30
00:01:50,363 --> 00:01:53,160
legalábbis elvileg, hogyan tárolhat egy konkrét tényt.

31
00:01:53,580 --> 00:01:57,080
Konkrétan arról fog szólni, hogy Michael Jordan kosárlabdázik.

32
00:01:58,080 --> 00:02:00,831
Meg kell említenem, hogy az itteni elrendezést egy beszélgetés ihlette, 

33
00:02:00,831 --> 00:02:03,200
amelyet az egyik DeepMind kutatóval, Neil Nandával folytattam.

34
00:02:04,060 --> 00:02:07,401
A legtöbb esetben feltételezem, hogy vagy megnézted az előző két fejezetet, 

35
00:02:07,401 --> 00:02:10,742
vagy egyébként van egy alapvető érzéked ahhoz, hogy mi az a transzformátor, 

36
00:02:10,742 --> 00:02:14,700
de a felfrissítés sosem árt, ezért itt van egy gyors emlékeztető az általános folyamatról.

37
00:02:15,340 --> 00:02:18,584
Ön és én egy olyan modellt tanulmányoztunk, amelyet arra képeztek ki, 

38
00:02:18,584 --> 00:02:21,320
hogy befogadjon egy szöveget, és megjósolja, mi következik.

39
00:02:21,720 --> 00:02:26,900
A bemeneti szöveget először egy csomó tokenre bontjuk, ami kis darabokat jelent, 

40
00:02:26,900 --> 00:02:30,226
amelyek jellemzően szavak vagy szavak kis darabjai, 

41
00:02:30,226 --> 00:02:35,280
és minden tokenhez egy nagydimenziós vektor, azaz számok hosszú listája társul.

42
00:02:35,840 --> 00:02:40,084
Ez a vektorok sorozata aztán ismételten kétféle műveleten megy keresztül: 

43
00:02:40,084 --> 00:02:44,901
a figyelem, amely lehetővé teszi, hogy a vektorok információt adjanak át egymásnak, 

44
00:02:44,901 --> 00:02:48,973
majd a többrétegű perceptronok, az a dolog, amibe ma bele fogunk ásni, 

45
00:02:48,973 --> 00:02:52,300
és van egy bizonyos normalizációs lépés is a kettő között.

46
00:02:53,300 --> 00:02:58,328
Miután a vektorok sorozata mindkét blokk sok-sok különböző iterációján átfutott, 

47
00:02:58,328 --> 00:03:03,294
a végére az a remény, hogy minden egyes vektor elég információt szívott magába, 

48
00:03:03,294 --> 00:03:06,956
mind a kontextusból, mind a bemenet összes többi szavából, 

49
00:03:06,956 --> 00:03:11,550
mind pedig a modell súlyaiba a képzés során beépített általános tudásból, 

50
00:03:11,550 --> 00:03:16,020
hogy felhasználható legyen a következő jelzőre vonatkozó előrejelzéshez.

51
00:03:16,860 --> 00:03:20,478
Az egyik legfontosabb gondolat, amit szeretném, ha észben tartanátok, 

52
00:03:20,478 --> 00:03:24,044
hogy ezek a vektorok egy nagyon-nagyon nagy dimenziójú térben élnek, 

53
00:03:24,044 --> 00:03:28,179
és ha erre a térre gondolunk, a különböző irányok különböző jelentéstartalmakat 

54
00:03:28,179 --> 00:03:28,800
kódolhatnak.

55
00:03:30,120 --> 00:03:33,473
Tehát egy nagyon klasszikus példa, amire szívesen hivatkozom, 

56
00:03:33,473 --> 00:03:37,206
hogy ha megnézzük a nő beágyazását, és kivonjuk a férfi beágyazását, 

57
00:03:37,206 --> 00:03:41,209
és ezt a kis lépést megtesszük, és hozzáadjuk egy másik hímnemű főnévhez, 

58
00:03:41,209 --> 00:03:45,374
például a bácsikához, akkor valahol nagyon-nagyon közel kerülünk a megfelelő 

59
00:03:45,374 --> 00:03:46,240
nőnemű főnévhez.

60
00:03:46,440 --> 00:03:50,880
Ebben az értelemben ez az irány a nemi információt kódolja.

61
00:03:51,640 --> 00:03:55,589
Az elképzelés szerint ebben a szuper magas dimenziójú térben sok más irány is 

62
00:03:55,589 --> 00:03:59,640
megfelelhet más jellemzőknek, amelyeket a modell esetleg reprezentálni szeretne.

63
00:04:01,400 --> 00:04:06,180
Egy transzformátorban azonban ezek a vektorok nem csupán egyetlen szó jelentését kódolják.

64
00:04:06,680 --> 00:04:11,062
Ahogy a hálózaton keresztül áramlanak, sokkal gazdagabb jelentést 

65
00:04:11,062 --> 00:04:15,180
kapnak a körülöttük lévő kontextus és a modell tudása alapján.

66
00:04:15,880 --> 00:04:18,612
Végső soron mindegyiknek kódolnia kell valamit, ami messze, 

67
00:04:18,612 --> 00:04:22,165
messze túlmutat egyetlen szó jelentésén, mivel elegendőnek kell lennie ahhoz, 

68
00:04:22,165 --> 00:04:23,760
hogy megjósolja, mi fog következni.

69
00:04:24,560 --> 00:04:29,104
Azt már láttuk, hogy a figyelemblokkok hogyan teszik lehetővé a kontextus beépítését, 

70
00:04:29,104 --> 00:04:32,855
de a modellparaméterek többsége valójában az MLP-blokkokban található, 

71
00:04:32,855 --> 00:04:35,445
és az egyik gondolat, hogy mit csinálhatnak, az, 

72
00:04:35,445 --> 00:04:38,140
hogy extra kapacitást kínálnak a tények tárolására.

73
00:04:38,720 --> 00:04:42,127
Mint mondtam, a lecke itt a konkrét játékpéldára fog összpontosítani, 

74
00:04:42,127 --> 00:04:46,120
hogy pontosan hogyan lehet tárolni azt a tényt, hogy Michael Jordan kosárlabdázik.

75
00:04:47,120 --> 00:04:49,400
Ez a játékpélda megköveteli, hogy mi ketten tegyünk 

76
00:04:49,400 --> 00:04:51,900
néhány feltételezést a nagydimenziós térrel kapcsolatban.

77
00:04:52,360 --> 00:04:57,547
Először is tegyük fel, hogy az egyik irány a Michael keresztnév gondolatát, 

78
00:04:57,547 --> 00:05:02,461
majd egy másik, majdnem merőleges irány a Jordan vezetéknév gondolatát, 

79
00:05:02,461 --> 00:05:06,420
majd egy harmadik irány a kosárlabda gondolatát képviseli.

80
00:05:07,400 --> 00:05:11,163
Konkrétan tehát azt értem ez alatt, hogy ha megnézzük a hálózatot, 

81
00:05:11,163 --> 00:05:15,824
és kivesszük az egyik feldolgozott vektort, és ha a pontproduktuma ezzel a Michael 

82
00:05:15,824 --> 00:05:20,823
keresztnévvel egy, akkor ez azt jelenti, hogy a vektor az adott keresztnévvel rendelkező 

83
00:05:20,823 --> 00:05:22,340
személy gondolatát kódolja.

84
00:05:23,800 --> 00:05:26,079
Ellenkező esetben a pontproduktum nulla vagy negatív lenne, 

85
00:05:26,079 --> 00:05:28,700
ami azt jelenti, hogy a vektor nem igazán igazodik az adott irányhoz.

86
00:05:29,420 --> 00:05:32,724
És az egyszerűség kedvéért hagyjuk figyelmen kívül azt a nagyon is ésszerű kérdést, 

87
00:05:32,724 --> 00:05:35,320
hogy mit jelentene, ha ez a pontproduktum nagyobb lenne, mint egy.

88
00:05:36,200 --> 00:05:40,358
Hasonlóképpen, a pontproduktuma ezekkel az irányokkal megmondaná, 

89
00:05:40,358 --> 00:05:43,760
hogy a Jordan vagy a kosárlabda vezetéknevet jelöli-e.

90
00:05:44,740 --> 00:05:49,196
Tegyük fel tehát, hogy egy vektor a teljes nevet, Michael Jordan-t hivatott ábrázolni, 

91
00:05:49,196 --> 00:05:52,680
akkor annak a pontproduktumának mindkét iránnyal egynek kell lennie.

92
00:05:53,480 --> 00:05:58,120
Mivel a Michael Jordan szöveg két különböző tokenre terjed ki, ez azt is jelentené, 

93
00:05:58,120 --> 00:06:02,485
hogy feltételeznünk kell, hogy egy korábbi figyelemblokk sikeresen továbbított 

94
00:06:02,485 --> 00:06:06,960
információt a két vektor közül a másodiknak, hogy az mindkét nevet kódolni tudja.

95
00:06:07,940 --> 00:06:10,001
Mindezekkel a feltevésekkel, mint feltételezésekkel, 

96
00:06:10,001 --> 00:06:11,480
most merüljünk bele a lecke lényegébe.

97
00:06:11,880 --> 00:06:14,980
Mi történik egy többrétegű perceptronban?

98
00:06:17,100 --> 00:06:20,862
Gondolhatunk a blokkba áramló vektorok sorozatára, és ne feledjük, 

99
00:06:20,862 --> 00:06:25,580
hogy minden egyes vektor eredetileg a bemeneti szöveg egyik tokenjéhez kapcsolódott.

100
00:06:26,080 --> 00:06:29,561
Az fog történni, hogy a szekvencia minden egyes vektora átmegy 

101
00:06:29,561 --> 00:06:33,043
egy rövid műveletsorozaton, amit egy pillanat múlva kibontunk, 

102
00:06:33,043 --> 00:06:36,360
és a végén egy másik vektort kapunk, ugyanolyan dimenzióval.

103
00:06:36,880 --> 00:06:40,292
A másik vektor hozzáadódik az eredetileg beáramlóhoz, 

104
00:06:40,292 --> 00:06:43,200
és ez az összeg lesz a kifelé áramló eredmény.

105
00:06:43,720 --> 00:06:47,315
Ezt a műveletsorozatot a szekvencia minden egyes vektorára alkalmazza, 

106
00:06:47,315 --> 00:06:51,620
amely a bemenet minden egyes tokenjéhez kapcsolódik, és mindez párhuzamosan történik.

107
00:06:52,100 --> 00:06:54,847
Különösen a vektorok nem beszélnek egymással ebben a lépésben, 

108
00:06:54,847 --> 00:06:56,200
mindegyik a saját dolgát végzi.

109
00:06:56,720 --> 00:07:00,035
És számunkra ez valójában sokkal egyszerűbbé teszi a dolgot, mert ez azt jelenti, 

110
00:07:00,035 --> 00:07:03,310
hogy ha megértjük, hogy mi történik az egyik vektorral ezen a blokkon keresztül, 

111
00:07:03,310 --> 00:07:06,060
akkor gyakorlatilag megértjük, hogy mi történik az összes vektorral.

112
00:07:07,100 --> 00:07:10,038
Amikor azt mondom, hogy ez a blokk azt a tényt fogja kódolni, 

113
00:07:10,038 --> 00:07:14,256
hogy Michael Jordan kosárlabdázik, akkor azt értem ez alatt, hogy ha egy vektor érkezik, 

114
00:07:14,256 --> 00:07:17,242
amely a Michael keresztnevet és a Jordan vezetéknevet kódolja, 

115
00:07:17,242 --> 00:07:19,896
akkor ez a számítási sorozat olyasmit fog eredményezni, 

116
00:07:19,896 --> 00:07:24,020
amely tartalmazza a kosárlabda irányt, ami hozzáadódik a vektorhoz az adott pozícióban.

117
00:07:25,600 --> 00:07:27,606
A folyamat első lépése úgy néz ki, hogy ezt a 

118
00:07:27,606 --> 00:07:29,700
vektort megszorozzuk egy nagyon nagy mátrixszal.

119
00:07:30,040 --> 00:07:31,980
Nincs meglepetés, ez a mélytanulás.

120
00:07:32,680 --> 00:07:36,081
És ez a mátrix, mint az összes többi, amit láttunk, tele van modellparaméterekkel, 

121
00:07:36,081 --> 00:07:38,704
amelyeket az adatokból tanultunk, és amelyeket úgy gondolhatsz, 

122
00:07:38,704 --> 00:07:42,392
mint egy csomó gombot és tárcsát, amelyeket állítgatnak és hangolnak, hogy meghatározzák, 

123
00:07:42,392 --> 00:07:43,540
milyen a modell viselkedése.

124
00:07:44,500 --> 00:07:48,529
A mátrixszorzásról úgy lehet gondolkodni, hogy a mátrix minden sorát 

125
00:07:48,529 --> 00:07:52,441
saját vektorként képzeljük el, és egy csomó pontterméket veszünk a 

126
00:07:52,441 --> 00:07:56,880
sorok és a feldolgozandó vektor között, amit beágyazás esetén E-vel jelölök.

127
00:07:57,280 --> 00:08:00,610
Tegyük fel például, hogy az első sor történetesen megegyezik ezzel 

128
00:08:00,610 --> 00:08:04,040
a Michael irányú keresztnévvel, amelyről feltételezzük, hogy létezik.

129
00:08:04,320 --> 00:08:09,423
Ez azt jelentené, hogy a kimenet első összetevője, ez a pontproduktum itt, 

130
00:08:09,423 --> 00:08:14,800
egy, ha a vektor a Michael keresztnevet kódolja, és nulla vagy negatív, ha nem.

131
00:08:15,880 --> 00:08:19,381
Még szórakoztatóbb, egy pillanatra gondoljon arra, hogy mit jelentene, 

132
00:08:19,381 --> 00:08:23,080
ha az első sorban ez a keresztnév Michael plusz vezetéknév Jordan irányban.

133
00:08:23,700 --> 00:08:27,420
Az egyszerűség kedvéért hadd írjam le ezt úgy, hogy M plusz J.

134
00:08:28,080 --> 00:08:30,980
Ezután, ha pontszorzatot veszünk ezzel az E beágyazással, 

135
00:08:30,980 --> 00:08:34,980
a dolgok nagyon szépen eloszlanak, így úgy néz ki, hogy M pont E plusz J pont E.

136
00:08:34,980 --> 00:08:38,814
És figyeljük meg, hogy ez azt jelenti, hogy a végső érték kettő lenne, 

137
00:08:38,814 --> 00:08:41,568
ha a vektor a teljes Michael Jordan nevet kódolja, 

138
00:08:41,568 --> 00:08:44,700
egyébként pedig egy vagy valami egynél kisebb érték lenne.

139
00:08:45,340 --> 00:08:47,260
És ez csak egy sor ebben a mátrixban.

140
00:08:47,600 --> 00:08:52,882
Gondolhatunk arra, hogy az összes többi sor párhuzamosan másfajta kérdéseket tesz fel, 

141
00:08:52,882 --> 00:08:56,040
a feldolgozott vektor másfajta jellemzőit vizsgálva.

142
00:08:56,700 --> 00:08:59,958
Nagyon gyakran ez a lépés egy másik vektor hozzáadását is jelenti a kimenethez, 

143
00:08:59,958 --> 00:09:02,240
amely tele van az adatokból tanult modellparaméterekkel.

144
00:09:02,240 --> 00:09:04,560
Ez a másik vektor az úgynevezett torzítás.

145
00:09:05,180 --> 00:09:08,708
A példánkhoz azt szeretném, ha elképzelné, hogy az első komponensben 

146
00:09:08,708 --> 00:09:11,367
az előfeszítés értéke negatív egy, ami azt jelenti, 

147
00:09:11,367 --> 00:09:15,560
hogy a végső kimenetünk úgy néz ki, mint a vonatkozó pontproduktum, de mínusz egy.

148
00:09:16,120 --> 00:09:19,381
Joggal kérdezheted, hogy miért akarom, hogy azt feltételezd, 

149
00:09:19,381 --> 00:09:22,856
hogy a modell megtanulta ezt, és egy pillanat múlva látni fogod, 

150
00:09:22,856 --> 00:09:26,652
hogy miért nagyon tiszta és szép, ha van itt egy érték, amely pozitív, 

151
00:09:26,652 --> 00:09:30,449
ha és csak akkor, ha egy vektor kódolja a Michael Jordan teljes nevét, 

152
00:09:30,449 --> 00:09:32,160
és egyébként nulla vagy negatív.

153
00:09:33,040 --> 00:09:37,878
A mátrix összes sorszáma, ami körülbelül a feltett kérdések számát jelenti, 

154
00:09:37,878 --> 00:09:42,780
a GPT-3 esetében, amelynek számait követtük, valamivel kevesebb, mint 50 000.

155
00:09:43,100 --> 00:09:46,640
Valójában ez pontosan négyszerese a beágyazási tér dimenzióinak.

156
00:09:46,920 --> 00:09:47,900
Ez egy tervezési döntés.

157
00:09:47,940 --> 00:09:50,047
Lehetne több is, lehetne kevesebb is, de a tiszta 

158
00:09:50,047 --> 00:09:52,240
többszörös általában barátságos a hardverek számára.

159
00:09:52,740 --> 00:09:57,130
Mivel ez a súlyokkal teli mátrix egy magasabb dimenziós térbe képez le minket, 

160
00:09:57,130 --> 00:09:59,020
ezért a W up rövidítést használom.

161
00:09:59,020 --> 00:10:02,238
Továbbra is E-ként jelölöm a feldolgozott vektort, 

162
00:10:02,238 --> 00:10:07,160
és jelöljük ezt a torzító vektort B-nek, és tegyük vissza mindezt a diagramra.

163
00:10:09,180 --> 00:10:12,865
Ezen a ponton az a probléma, hogy ez a művelet tisztán lineáris, 

164
00:10:12,865 --> 00:10:15,360
de a nyelv egy nagyon nem lineáris folyamat.

165
00:10:15,880 --> 00:10:19,915
Ha az általunk mért belépési arány Michael plusz Jordan esetében magas, 

166
00:10:19,915 --> 00:10:24,512
akkor szükségszerűen Michael plusz Phelps és Alexis plusz Jordan is kiváltja azt, 

167
00:10:24,512 --> 00:10:28,100
annak ellenére, hogy ezek fogalmilag nem kapcsolódnak egymáshoz.

168
00:10:28,540 --> 00:10:32,000
Amit valójában szeretne, az egy egyszerű igen vagy nem a teljes névre.

169
00:10:32,900 --> 00:10:35,389
A következő lépés tehát az, hogy ezt a nagy köztes vektort egy 

170
00:10:35,389 --> 00:10:37,840
nagyon egyszerű nemlineáris függvényen keresztül kell vezetni.

171
00:10:38,360 --> 00:10:42,626
Gyakori az a választás, amely az összes negatív értéket nullára képezi le, 

172
00:10:42,626 --> 00:10:45,300
a pozitív értékeket pedig változatlanul hagyja.

173
00:10:46,440 --> 00:10:50,413
Folytatva a mélytanulás hagyományát a túlságosan divatos elnevezésekkel, 

174
00:10:50,413 --> 00:10:54,387
ezt a nagyon egyszerű függvényt gyakran egyenesített lineáris egységnek, 

175
00:10:54,387 --> 00:10:56,020
vagy röviden ReLU-nak nevezik.

176
00:10:56,020 --> 00:10:57,880
Így néz ki a grafikon.

177
00:10:58,300 --> 00:11:03,316
Tehát, ha a képzeletbeli példánkat vesszük, ahol a köztes vektor első bejegyzése egy, 

178
00:11:03,316 --> 00:11:08,332
ha és csak akkor, ha a teljes név Michael Jordan, egyébként pedig nulla vagy negatív, 

179
00:11:08,332 --> 00:11:11,890
miután átküldjük a ReLU-n, egy nagyon tiszta értéket kapunk, 

180
00:11:11,890 --> 00:11:15,740
ahol az összes nulla és negatív értéket egyszerűen nullára vágjuk.

181
00:11:16,100 --> 00:11:19,780
Tehát ez a kimenet a teljes Michael Jordan név esetén egy, egyébként pedig nulla.

182
00:11:20,560 --> 00:11:24,120
Más szóval, nagyon közvetlenül utánozza egy AND kapu viselkedését.

183
00:11:25,660 --> 00:11:29,054
Gyakran a modellek egy kissé módosított, JLU-nak nevezett funkciót használnak, 

184
00:11:29,054 --> 00:11:32,020
amely ugyanazzal az alapformával rendelkezik, csak egy kicsit simább.

185
00:11:32,500 --> 00:11:35,720
De a mi céljaink szempontjából egy kicsit tisztább, ha csak a ReLU-ra gondolunk.

186
00:11:36,740 --> 00:11:40,173
Amikor az emberek a transzformátor neuronjaira hivatkoznak, 

187
00:11:40,173 --> 00:11:42,520
akkor is ezekről az értékekről beszélnek.

188
00:11:42,900 --> 00:11:47,337
Amikor látod a szokásos neurális hálózatos képet egy pontokból álló réteggel és 

189
00:11:47,337 --> 00:11:50,665
egy csomó vonallal, amelyek az előző réteghez kapcsolódnak, 

190
00:11:50,665 --> 00:11:55,324
amit korábban már láttál ebben a sorozatban, akkor ez általában egy lineáris lépés, 

191
00:11:55,324 --> 00:11:59,984
egy mátrixszorzás kombinációját jelenti, amelyet egy egyszerű nemlineáris függvény, 

192
00:11:59,984 --> 00:12:01,260
például egy ReLU követ.

193
00:12:02,500 --> 00:12:06,758
Azt mondhatnánk, hogy ez a neuron aktív, ha ez az érték pozitív, 

194
00:12:06,758 --> 00:12:08,920
és inaktív, ha ez az érték nulla.

195
00:12:10,120 --> 00:12:12,380
A következő lépés nagyon hasonlít az elsőhöz.

196
00:12:12,560 --> 00:12:16,580
Megszorozzuk egy nagyon nagy mátrixszal, és hozzáadunk egy bizonyos torzító kifejezést.

197
00:12:16,980 --> 00:12:21,921
Ebben az esetben a kimeneti dimenziók száma visszaáll a beágyazási tér méretére, 

198
00:12:21,921 --> 00:12:25,520
ezért ezt a mátrixot lefelé vetítő mátrixnak fogom nevezni.

199
00:12:26,220 --> 00:12:29,108
És ezúttal ahelyett, hogy soronként gondolnánk a dolgokra, 

200
00:12:29,108 --> 00:12:31,360
valójában szebb, ha oszloponként gondolunk rá.

201
00:12:31,860 --> 00:12:36,104
A mátrixszorzás egy másik módja, hogy fejben tartsuk a mátrix szorzását, 

202
00:12:36,104 --> 00:12:40,697
hogy elképzeljük, hogy a mátrix minden egyes oszlopát megszorozzuk a megfelelő 

203
00:12:40,697 --> 00:12:45,640
kifejezéssel a feldolgozott vektorban, és az összes átméretezett oszlopot összeadjuk.

204
00:12:46,840 --> 00:12:51,086
Azért szebb így gondolkodni, mert itt az oszlopoknak ugyanaz a dimenziójuk, 

205
00:12:51,086 --> 00:12:55,780
mint a beágyazási térnek, így úgy gondolhatunk rájuk, mint irányokra abban a térben.

206
00:12:56,140 --> 00:12:58,625
Képzeljük el például, hogy a modell megtanulta, 

207
00:12:58,625 --> 00:13:03,080
hogy az első oszlopot ebbe a kosárlabda irányba, amelyről feltételezzük, hogy létezik.

208
00:13:04,180 --> 00:13:08,097
Ez azt jelenti, hogy amikor a megfelelő neuron az első pozícióban aktív, 

209
00:13:08,097 --> 00:13:10,780
akkor ezt az oszlopot hozzáadjuk a végeredményhez.

210
00:13:11,140 --> 00:13:15,780
De ha ez a neuron inaktív lenne, ha ez a szám nulla lenne, akkor ennek nem lenne hatása.

211
00:13:16,500 --> 00:13:18,060
És ennek nem csak a kosárlabdának kell lennie.

212
00:13:18,220 --> 00:13:21,952
A modell ebbe az oszlopba és sok más olyan tulajdonságot is be tudott sütni, 

213
00:13:21,952 --> 00:13:25,200
amit valamihez akar társítani, aminek a teljes neve Michael Jordan.

214
00:13:26,980 --> 00:13:31,105
Ugyanakkor a mátrix összes többi oszlopa megmondja, 

215
00:13:31,105 --> 00:13:36,660
hogy mi fog hozzáadódni a végeredményhez, ha a megfelelő neuron aktív.

216
00:13:37,360 --> 00:13:40,163
És ha ebben az esetben van egy torzítás, akkor ez olyasvalami, 

217
00:13:40,163 --> 00:13:43,500
amit minden egyes alkalommal hozzáadunk, függetlenül a neuronok értékeitől.

218
00:13:44,060 --> 00:13:45,280
Elgondolkodhatsz, hogy mit csinál ez.

219
00:13:45,540 --> 00:13:49,320
Mint minden paraméterrel töltött objektum esetében, itt is nehéz pontosan megmondani.

220
00:13:49,320 --> 00:13:51,998
Lehet, hogy a hálózatnak van némi könyvelési feladat, 

221
00:13:51,998 --> 00:13:54,380
de egyelőre nyugodtan figyelmen kívül hagyhatja.

222
00:13:54,860 --> 00:13:57,732
Hogy a jelölésünket ismét egy kicsit tömörebbé tegyük, 

223
00:13:57,732 --> 00:14:02,275
ezt a nagy W mátrixot lefelé hívom, és hasonlóképpen lefelé hívom a B torzító vektort, 

224
00:14:02,275 --> 00:14:04,260
és ezt visszahelyezzük a diagramunkba.

225
00:14:04,740 --> 00:14:08,733
Ahogy korábban már említettem, a végeredményt hozzáadjuk a vektorhoz, 

226
00:14:08,733 --> 00:14:13,240
amely az adott pozícióban a blokkba áramlott, és így kapjuk meg a végeredményt.

227
00:14:13,820 --> 00:14:19,355
Tehát például, ha a beáramló vektor a Michael keresztnevet és a Jordan vezetéknevet 

228
00:14:19,355 --> 00:14:23,704
is kódolja, akkor mivel ez a műveletsorozat kiváltja az ÉS kaput, 

229
00:14:23,704 --> 00:14:29,240
összeadja a kosárlabda irányát, így ami kiugrik, az mindezeket együtt fogja kódolni.

230
00:14:29,820 --> 00:14:34,200
És ne feledjük, hogy ez a folyamat minden egyes vektorral párhuzamosan zajlik.

231
00:14:34,800 --> 00:14:37,751
A GPT-3 számokat figyelembe véve ez azt jelenti, 

232
00:14:37,751 --> 00:14:42,631
hogy ebben a blokkban nem csak 50 000 neuron van, hanem 50 000-szer annyi token, 

233
00:14:42,631 --> 00:14:44,860
mint a bemenetben lévő tokenek száma.

234
00:14:48,180 --> 00:14:51,758
Ez tehát a teljes művelet, két mátrixtermék, mindegyikhez hozzáadva 

235
00:14:51,758 --> 00:14:55,180
egy előfeszítést és egy egyszerű vágási függvényt a kettő között.

236
00:14:56,080 --> 00:14:59,474
Aki látta a sorozat korábbi videóit, az felismeri ezt a struktúrát, 

237
00:14:59,474 --> 00:15:02,620
mint a legalapvetőbb neurális hálózatot, amelyet ott tanultunk.

238
00:15:03,080 --> 00:15:06,100
Ebben a példában a kézzel írt számjegyek felismerésére képezték ki.

239
00:15:06,580 --> 00:15:10,885
Itt, egy nagy nyelvi modell transzformátorának kontextusában ez egy nagyobb 

240
00:15:10,885 --> 00:15:14,908
architektúra egyik darabja, és minden kísérlet arra, hogy értelmezzük, 

241
00:15:14,908 --> 00:15:19,044
hogy pontosan mit is csinál, erősen összefonódik azzal az elképzeléssel, 

242
00:15:19,044 --> 00:15:23,180
hogy az információt egy nagydimenziós beágyazási tér vektoraiba kódoljuk.

243
00:15:24,260 --> 00:15:28,381
Ez az alapvető lecke, de szeretnék hátralépni és elgondolkodni két különböző dolgon, 

244
00:15:28,381 --> 00:15:31,727
amelyek közül az első egyfajta könyvelés, a második pedig egy nagyon 

245
00:15:31,727 --> 00:15:35,800
elgondolkodtató tényt tartalmaz a magasabb dimenziókról, amit valójában nem tudtam, 

246
00:15:35,800 --> 00:15:38,080
amíg nem ástam bele magam a transzformátorokba.

247
00:15:41,080 --> 00:15:45,695
Az utolsó két fejezetben elkezdtük számolni a GPT-3 összes paraméterét, 

248
00:15:45,695 --> 00:15:50,760
és megnéztük, hogy pontosan hol is laknak, ezért gyorsan fejezzük be a játékot.

249
00:15:51,400 --> 00:15:56,915
Már említettem, hogy ez a felfelé vetítési mátrix alig kevesebb mint 50 000 sorból áll, 

250
00:15:56,915 --> 00:16:02,180
és hogy minden sor megfelel a beágyazási tér méretének, ami a GPT-3 esetében 12 288.

251
00:16:03,240 --> 00:16:08,332
Ezeket összeszorozva 604 millió paramétert kapunk csak erre a mátrixra, 

252
00:16:08,332 --> 00:16:13,920
és a lefelé vetítés ugyanennyi paramétert tartalmaz, csak transzponált alakkal.

253
00:16:14,500 --> 00:16:17,400
Tehát együttesen körülbelül 1,2 milliárd paramétert adnak.

254
00:16:18,280 --> 00:16:20,694
A torzításvektor még néhány paramétert figyelembe vesz, 

255
00:16:20,694 --> 00:16:24,100
de ez a teljes értéknek csak egy jelentéktelen része, ezért nem is mutatom meg.

256
00:16:24,660 --> 00:16:28,722
A GPT-3-ban a beágyazási vektorok ezen sorozata nem egy, 

257
00:16:28,722 --> 00:16:33,213
hanem 96 különböző MLP-n keresztül folyik, így az összes ilyen 

258
00:16:33,213 --> 00:16:38,060
blokkhoz tartozó paraméterek száma összesen körülbelül 116 milliárd.

259
00:16:38,820 --> 00:16:43,860
Ez a hálózat összes paraméterének körülbelül kétharmada, és ha ezt hozzáadjuk mindahhoz, 

260
00:16:43,860 --> 00:16:48,051
ami korábban volt a figyelemblokkok, a beágyazás és a kiágyazás esetében, 

261
00:16:48,051 --> 00:16:51,620
akkor valóban megkapjuk a hirdetett 175 milliárdos végösszeget.

262
00:16:53,060 --> 00:16:56,058
Valószínűleg érdemes megemlíteni, hogy van egy másik paraméterkészlet is, 

263
00:16:56,058 --> 00:16:59,463
amely a normalizálási lépésekhez kapcsolódik, és amelyet ez a magyarázat kihagyott, 

264
00:16:59,463 --> 00:17:02,988
de a torzítási vektorhoz hasonlóan ezek is csak egy nagyon jelentéktelen részét teszik 

265
00:17:02,988 --> 00:17:03,840
ki a teljes értéknek.

266
00:17:05,900 --> 00:17:08,811
Ami a második gondolatmenetet illeti, talán elgondolkodik azon, 

267
00:17:08,811 --> 00:17:12,268
hogy ez a központi játékpélda, amivel annyi időt töltöttünk, tükrözi-e azt, 

268
00:17:12,268 --> 00:17:15,680
hogy a tényeket valójában hogyan tárolják a valódi nagy nyelvi modellekben.

269
00:17:16,319 --> 00:17:20,680
Igaz, hogy az első mátrix sorai irányoknak tekinthetők ebben a beágyazási térben, 

270
00:17:20,680 --> 00:17:24,402
és ez azt jelenti, hogy az egyes neuronok aktivációja azt mondja meg, 

271
00:17:24,402 --> 00:17:27,540
hogy egy adott vektor mennyire igazodik egy adott irányhoz.

272
00:17:27,760 --> 00:17:30,775
Az is igaz, hogy a második mátrix oszlopai megmondják, 

273
00:17:30,775 --> 00:17:34,340
hogy mi fog hozzáadódni az eredményhez, ha az adott neuron aktív.

274
00:17:34,640 --> 00:17:36,800
Mindkettő csak matematikai tény.

275
00:17:37,740 --> 00:17:41,714
A bizonyítékok azonban arra utalnak, hogy az egyes neuronok nagyon ritkán 

276
00:17:41,714 --> 00:17:45,258
képviselnek egyetlen olyan tiszta jellemzőt, mint Michael Jordan, 

277
00:17:45,258 --> 00:17:49,393
és ennek valójában nagyon jó oka lehet, ami az értelmezhetőséggel foglalkozó 

278
00:17:49,393 --> 00:17:54,120
kutatók körében manapság elterjedt, szuperpozíciónak nevezett elképzeléshez kapcsolódik.

279
00:17:54,640 --> 00:17:57,619
Ez egy olyan hipotézis, amely segíthet megmagyarázni, 

280
00:17:57,619 --> 00:18:02,420
hogy miért különösen nehéz értelmezni a modelleket, és miért skálázódnak meglepően jól.

281
00:18:03,500 --> 00:18:06,508
Az alapötlet az, hogy ha van egy n-dimenziós tér, 

282
00:18:06,508 --> 00:18:11,202
és egy csomó különböző jellemzőt akarsz ábrázolni olyan irányok segítségével, 

283
00:18:11,202 --> 00:18:14,271
amelyek mind merőlegesek egymásra a térben, tudod, 

284
00:18:14,271 --> 00:18:19,506
így ha hozzáadsz egy komponenst az egyik irányban, az nem befolyásolja a többi irányt, 

285
00:18:19,506 --> 00:18:23,960
akkor a maximálisan elhelyezhető vektorok száma csak n, a dimenziók száma.

286
00:18:24,600 --> 00:18:27,620
Egy matematikus számára ez tulajdonképpen a dimenzió definíciója.

287
00:18:28,220 --> 00:18:33,580
Érdekes lesz azonban, ha egy kicsit lazítunk ezen a korláton, és elviselünk némi zajt.

288
00:18:34,180 --> 00:18:37,614
Tegyük fel, hogy ezeket a jellemzőket nem pontosan merőleges, 

289
00:18:37,614 --> 00:18:40,828
hanem csak majdnem merőleges vektorokkal lehet ábrázolni, 

290
00:18:40,828 --> 00:18:43,820
amelyek egymástól 89 és 91 fok között helyezkednek el.

291
00:18:44,820 --> 00:18:48,020
Ha két vagy három dimenzióban lennénk, ez nem számítana.

292
00:18:48,260 --> 00:18:51,866
Ez alig ad extra mozgásteret ahhoz, hogy több vektort illesszünk be, 

293
00:18:51,866 --> 00:18:56,152
ami még inkább ellenkezik azzal, hogy magasabb dimenziók esetén a válasz drámaian 

294
00:18:56,152 --> 00:18:56,780
megváltozik.

295
00:18:57,660 --> 00:19:02,459
Adhatok egy nagyon gyors és piszkos illusztrációt erre egy kis Python segítségével, 

296
00:19:02,459 --> 00:19:05,772
amely létrehoz egy 100 dimenziós vektorokból álló listát, 

297
00:19:05,772 --> 00:19:09,886
mindegyik véletlenszerűen inicializálva, és ez a lista 10.000 különböző 

298
00:19:09,886 --> 00:19:14,400
vektort fog tartalmazni, tehát 100-szor annyi vektort, mint ahány dimenzió van.

299
00:19:15,320 --> 00:19:19,900
Ez a diagram itt mutatja a vektorpárok közötti szögek eloszlását.

300
00:19:20,680 --> 00:19:25,048
Mivel véletlenszerűen indultak, ezek a szögek 0 és 180 fok között bármi lehet, 

301
00:19:25,048 --> 00:19:29,637
de észrevehetitek, hogy még a véletlenszerű vektorok esetében is erős a tendencia, 

302
00:19:29,637 --> 00:19:31,960
hogy a dolgok közelebb vannak a 90 fokhoz.

303
00:19:32,500 --> 00:19:35,952
Ezután egy bizonyos optimalizálási folyamatot fogok futtatni, 

304
00:19:35,952 --> 00:19:38,736
amely iteratív módon eltolja ezeket a vektorokat, 

305
00:19:38,736 --> 00:19:41,520
hogy megpróbáljanak egymásra merőlegesebbek lenni.

306
00:19:42,060 --> 00:19:46,660
Miután ezt többször megismételtük, a szögek eloszlása a következőképpen néz ki.

307
00:19:47,120 --> 00:19:52,049
Valójában nagyítanunk kell, mert a vektorpárok közötti összes 

308
00:19:52,049 --> 00:19:56,900
lehetséges szög a 89 és 91 fok közötti szűk tartományban van.

309
00:19:58,020 --> 00:20:02,834
Általánosságban a Johnson-Lindenstrauss lemma néven ismert tétel egyik következménye, 

310
00:20:02,834 --> 00:20:06,529
hogy a dimenziók számával exponenciálisan nő azon vektorok száma, 

311
00:20:06,529 --> 00:20:10,840
amelyeket egy térbe be lehet zsúfolni, és amelyek közel merőlegesek egymásra.

312
00:20:11,960 --> 00:20:15,999
Ez nagyon fontos a nagy nyelvi modellek esetében, amelyeknek előnyös lehet, 

313
00:20:15,999 --> 00:20:19,880
ha független gondolatokat társítanak egymásra közel merőleges irányokkal.

314
00:20:20,000 --> 00:20:23,577
Ez azt jelenti, hogy sokkal, de sokkal több ötletet tud tárolni, 

315
00:20:23,577 --> 00:20:26,440
mint amennyi dimenzió van a számára kijelölt helyen.

316
00:20:27,320 --> 00:20:29,719
Ez részben megmagyarázhatja, hogy a modell teljesítménye 

317
00:20:29,719 --> 00:20:31,740
miért skálázódik olyan jól a méret függvényében.

318
00:20:32,540 --> 00:20:35,569
Egy tízszer annyi dimenzióval rendelkező tér sokkal, 

319
00:20:35,569 --> 00:20:39,400
de sokkal több, mint tízszer annyi független ötletet képes tárolni.

320
00:20:40,420 --> 00:20:42,774
És ez nem csak arra a beágyazási térre vonatkozik, 

321
00:20:42,774 --> 00:20:46,653
ahol a modellen átáramló vektorok élnek, hanem arra a neuronokkal teli vektorra is, 

322
00:20:46,653 --> 00:20:50,440
amely a többrétegű perceptron közepén található, amelyet az imént tanulmányoztunk.

323
00:20:50,960 --> 00:20:55,140
Vagyis a GPT-3 méreteinél nem csak 50 000 jellemzőt szondázhatna, 

324
00:20:55,140 --> 00:21:00,525
hanem ha ehelyett kihasználná ezt a hatalmas többletkapacitást a tér közel merőleges 

325
00:21:00,525 --> 00:21:04,706
irányainak felhasználásával, akkor a feldolgozandó vektor sokkal, 

326
00:21:04,706 --> 00:21:07,240
de sokkal több jellemzőjét szondázhatná.

327
00:21:07,780 --> 00:21:12,481
De ha ezt tenné, az azt jelentené, hogy az egyes funkciók nem úgy lennének láthatóak, 

328
00:21:12,481 --> 00:21:14,340
mintha egyetlen neuron világítana.

329
00:21:14,660 --> 00:21:18,453
Ehelyett úgy kellene kinéznie, mint a neuronok valamilyen különleges kombinációjának, 

330
00:21:18,453 --> 00:21:19,380
egy szuperpozíciónak.

331
00:21:20,400 --> 00:21:23,590
Aki kíváncsi, hogy többet szeretne megtudni, a legfontosabb releváns keresőszó 

332
00:21:23,590 --> 00:21:25,529
itt a sparse autoencoder, ami egy olyan eszköz, 

333
00:21:25,529 --> 00:21:27,952
amelyet néhány értelmezhetőséggel foglalkozó ember használ, 

334
00:21:27,952 --> 00:21:30,416
hogy megpróbálja kivonni a valódi jellemzőket, még akkor is, 

335
00:21:30,416 --> 00:21:32,880
ha azok nagyon egymásra vannak helyezve az összes neurononon.

336
00:21:33,540 --> 00:21:36,800
Belinkelek néhány igazán nagyszerű antropológiai bejegyzést erről.

337
00:21:37,880 --> 00:21:41,230
Ezen a ponton még nem érintettük a transzformátor minden részletét, 

338
00:21:41,230 --> 00:21:43,300
de a legfontosabb pontokat már eltaláltuk.

339
00:21:43,520 --> 00:21:46,779
A legfontosabb dolog, amivel a következő fejezetben foglalkozni akarok, 

340
00:21:46,779 --> 00:21:47,640
a képzési folyamat.

341
00:21:48,460 --> 00:21:51,733
Egyrészt a rövid válasz arra, hogy hogyan működik a képzés, az, 

342
00:21:51,733 --> 00:21:55,928
hogy az egész backpropagation, és a sorozat korábbi fejezeteiben külön tárgyaltuk 

343
00:21:55,928 --> 00:21:56,900
a backpropagationt.

344
00:21:57,220 --> 00:22:00,903
De van még mit megvitatni, például a nyelvi modellekhez használt speciális 

345
00:22:00,903 --> 00:22:04,931
költségfüggvényt, a finomhangolás gondolatát a megerősített tanulás segítségével, 

346
00:22:04,931 --> 00:22:07,780
emberi visszajelzéssel, és a skálázási törvények fogalmát.

347
00:22:08,960 --> 00:22:12,349
Gyors megjegyzés az aktív követők között, van egy sor nem gépi tanulással 

348
00:22:12,349 --> 00:22:15,785
kapcsolatos videók, hogy én izgatott, hogy süllyessze a fogaimat, mielőtt, 

349
00:22:15,785 --> 00:22:18,717
hogy a következő fejezet, így lehet, hogy egy ideig, de ígérem, 

350
00:22:18,717 --> 00:22:20,000
hogy jön a megfelelő időben.

351
00:22:35,640 --> 00:22:37,920
Köszönöm.

