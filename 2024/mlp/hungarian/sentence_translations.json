[
 {
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "translatedText": "Ha egy nagy nyelvi modellt azzal a kifejezéssel táplálsz, hogy Michael Jordan a blank sportot űzi, és megjósoltatod vele, hogy mi következik, és helyesen jósolja meg a kosárlabdát, akkor ez azt sugallja, hogy valahol, a több százmilliárd paraméterében valahol egy konkrét személyről és az ő konkrét sportágáról szóló tudás van beépítve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "translatedText": "És azt hiszem, általában véve, aki játszott már egy ilyen modellel, az egyértelműen érzi, hogy rengeteg tényt megjegyzett.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "translatedText": "Tehát egy ésszerű kérdés, amit feltehetünk, hogy pontosan hogyan is működik ez?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "input": "And where do those facts live?",
  "translatedText": "És hol élnek ezek a tények?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "translatedText": "Tavaly decemberben a Google DeepMind néhány kutatója a kérdéssel kapcsolatos munkájáról számolt be, és ezt a konkrét példát használták, amikor a sportolókat a sportágukhoz illesztették.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "translatedText": "És bár a tények tárolásának teljes mechanisztikus megértése továbbra is megoldatlan, érdekes részeredményeket értek el, többek között azt a nagyon általános, magas szintű következtetést, hogy a tények úgy tűnik, hogy e hálózatok egy bizonyos részében élnek, amelyet fantáziadúsan többrétegű perceptronoknak, vagy röviden MLP-knek neveznek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "translatedText": "Az elmúlt néhány fejezetben ön és én a transzformátorok, a nagy nyelvi modellek, valamint számos más modern mesterséges intelligencia mögött álló architektúra részleteibe ástuk bele magunkat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "translatedText": "A legutóbbi fejezetben a Figyelem című darabra koncentráltunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "translatedText": "A következő lépés pedig az, hogy ön és én belemerüljünk a részletekbe, hogy mi történik ezekben a többrétegű perceptronokban, amelyek a hálózat másik nagy részét alkotják.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "translatedText": "A számítás itt valójában viszonylag egyszerű, különösen, ha a figyelemhez hasonlítjuk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "translatedText": "Lényegében egy pár mátrixszorzásra fut ki, és egy egyszerű valamire a kettő között.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "translatedText": "Azonban rendkívül nagy kihívás értelmezni, hogy mit is csinálnak ezek a számítások.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "translatedText": "A fő célunk itt az, hogy végigmenjünk a számításokon, és emlékezetessé tegyük őket, de ezt egy konkrét példa bemutatásával szeretném megtenni, hogy az egyik ilyen blokk, legalábbis elvileg, hogyan tárolhat egy konkrét tényt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "translatedText": "Konkrétan arról fog szólni, hogy Michael Jordan kosárlabdázik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "translatedText": "Meg kell említenem, hogy az itteni elrendezést egy beszélgetés ihlette, amelyet az egyik DeepMind kutatóval, Neil Nandával folytattam.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "translatedText": "A legtöbb esetben feltételezem, hogy vagy megnézted az előző két fejezetet, vagy egyébként van egy alapvető érzéked ahhoz, hogy mi az a transzformátor, de a felfrissítés sosem árt, ezért itt van egy gyors emlékeztető az általános folyamatról.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "translatedText": "Ön és én egy olyan modellt tanulmányoztunk, amelyet arra képeztek ki, hogy befogadjon egy szöveget, és megjósolja, mi következik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "translatedText": "A bemeneti szöveget először egy csomó tokenre bontjuk, ami kis darabokat jelent, amelyek jellemzően szavak vagy szavak kis darabjai, és minden tokenhez egy nagydimenziós vektor, azaz számok hosszú listája társul.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "translatedText": "Ez a vektorok sorozata aztán ismételten kétféle műveleten megy keresztül: a figyelem, amely lehetővé teszi, hogy a vektorok információt adjanak át egymásnak, majd a többrétegű perceptronok, az a dolog, amibe ma bele fogunk ásni, és van egy bizonyos normalizációs lépés is a kettő között.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "translatedText": "Miután a vektorok sorozata mindkét blokk sok-sok különböző iterációján átfutott, a végére az a remény, hogy minden egyes vektor elég információt szívott magába, mind a kontextusból, mind a bemenet összes többi szavából, mind pedig a modell súlyaiba a képzés során beépített általános tudásból, hogy felhasználható legyen a következő jelzőre vonatkozó előrejelzéshez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "translatedText": "Az egyik legfontosabb gondolat, amit szeretném, ha észben tartanátok, hogy ezek a vektorok egy nagyon-nagyon nagy dimenziójú térben élnek, és ha erre a térre gondolunk, a különböző irányok különböző jelentéstartalmakat kódolhatnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "translatedText": "Tehát egy nagyon klasszikus példa, amire szívesen hivatkozom, hogy ha megnézzük a nő beágyazását, és kivonjuk a férfi beágyazását, és ezt a kis lépést megtesszük, és hozzáadjuk egy másik hímnemű főnévhez, például a bácsikához, akkor valahol nagyon-nagyon közel kerülünk a megfelelő nőnemű főnévhez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "input": "In this sense, this particular direction encodes gender information.",
  "translatedText": "Ebben az értelemben ez az irány a nemi információt kódolja.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "translatedText": "Az elképzelés szerint ebben a szuper magas dimenziójú térben sok más irány is megfelelhet más jellemzőknek, amelyeket a modell esetleg reprezentálni szeretne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "translatedText": "Egy transzformátorban azonban ezek a vektorok nem csupán egyetlen szó jelentését kódolják.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "translatedText": "Ahogy a hálózaton keresztül áramlanak, sokkal gazdagabb jelentést kapnak a körülöttük lévő kontextus és a modell tudása alapján.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "translatedText": "Végső soron mindegyiknek kódolnia kell valamit, ami messze, messze túlmutat egyetlen szó jelentésén, mivel elegendőnek kell lennie ahhoz, hogy megjósolja, mi fog következni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "translatedText": "Azt már láttuk, hogy a figyelemblokkok hogyan teszik lehetővé a kontextus beépítését, de a modellparaméterek többsége valójában az MLP-blokkokban található, és az egyik gondolat, hogy mit csinálhatnak, az, hogy extra kapacitást kínálnak a tények tárolására.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "translatedText": "Mint mondtam, a lecke itt a konkrét játékpéldára fog összpontosítani, hogy pontosan hogyan lehet tárolni azt a tényt, hogy Michael Jordan kosárlabdázik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "translatedText": "Ez a játékpélda megköveteli, hogy mi ketten tegyünk néhány feltételezést a nagydimenziós térrel kapcsolatban.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "translatedText": "Először is tegyük fel, hogy az egyik irány a Michael keresztnév gondolatát, majd egy másik, majdnem merőleges irány a Jordan vezetéknév gondolatát, majd egy harmadik irány a kosárlabda gondolatát képviseli.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "translatedText": "Konkrétan tehát azt értem ez alatt, hogy ha megnézzük a hálózatot, és kivesszük az egyik feldolgozott vektort, és ha a pontproduktuma ezzel a Michael keresztnévvel egy, akkor ez azt jelenti, hogy a vektor az adott keresztnévvel rendelkező személy gondolatát kódolja.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "translatedText": "Ellenkező esetben a pontproduktum nulla vagy negatív lenne, ami azt jelenti, hogy a vektor nem igazán igazodik az adott irányhoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "translatedText": "És az egyszerűség kedvéért hagyjuk figyelmen kívül azt a nagyon is ésszerű kérdést, hogy mit jelentene, ha ez a pontproduktum nagyobb lenne, mint egy.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "translatedText": "Hasonlóképpen, a pontproduktuma ezekkel az irányokkal megmondaná, hogy a Jordan vagy a kosárlabda vezetéknevet jelöli-e.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "translatedText": "Tegyük fel tehát, hogy egy vektor a teljes nevet, Michael Jordan-t hivatott ábrázolni, akkor annak a pontproduktumának mindkét iránnyal egynek kell lennie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "translatedText": "Mivel a Michael Jordan szöveg két különböző tokenre terjed ki, ez azt is jelentené, hogy feltételeznünk kell, hogy egy korábbi figyelemblokk sikeresen továbbított információt a két vektor közül a másodiknak, hogy az mindkét nevet kódolni tudja.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "translatedText": "Mindezekkel a feltevésekkel, mint feltételezésekkel, most merüljünk bele a lecke lényegébe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "input": "What happens inside a multilayer perceptron?",
  "translatedText": "Mi történik egy többrétegű perceptronban?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "translatedText": "Gondolhatunk a blokkba áramló vektorok sorozatára, és ne feledjük, hogy minden egyes vektor eredetileg a bemeneti szöveg egyik tokenjéhez kapcsolódott.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "translatedText": "Az fog történni, hogy a szekvencia minden egyes vektora átmegy egy rövid műveletsorozaton, amit egy pillanat múlva kibontunk, és a végén egy másik vektort kapunk, ugyanolyan dimenzióval.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "translatedText": "A másik vektor hozzáadódik az eredetileg beáramlóhoz, és ez az összeg lesz a kifelé áramló eredmény.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "translatedText": "Ezt a műveletsorozatot a szekvencia minden egyes vektorára alkalmazza, amely a bemenet minden egyes tokenjéhez kapcsolódik, és mindez párhuzamosan történik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "translatedText": "Különösen a vektorok nem beszélnek egymással ebben a lépésben, mindegyik a saját dolgát végzi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "translatedText": "És számunkra ez valójában sokkal egyszerűbbé teszi a dolgot, mert ez azt jelenti, hogy ha megértjük, hogy mi történik az egyik vektorral ezen a blokkon keresztül, akkor gyakorlatilag megértjük, hogy mi történik az összes vektorral.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "translatedText": "Amikor azt mondom, hogy ez a blokk azt a tényt fogja kódolni, hogy Michael Jordan kosárlabdázik, akkor azt értem ez alatt, hogy ha egy vektor érkezik, amely a Michael keresztnevet és a Jordan vezetéknevet kódolja, akkor ez a számítási sorozat olyasmit fog eredményezni, amely tartalmazza a kosárlabda irányt, ami hozzáadódik a vektorhoz az adott pozícióban.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "translatedText": "A folyamat első lépése úgy néz ki, hogy ezt a vektort megszorozzuk egy nagyon nagy mátrixszal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "input": "No surprises there, this is deep learning.",
  "translatedText": "Nincs meglepetés, ez a mélytanulás.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "translatedText": "És ez a mátrix, mint az összes többi, amit láttunk, tele van modellparaméterekkel, amelyeket az adatokból tanultunk, és amelyeket úgy gondolhatsz, mint egy csomó gombot és tárcsát, amelyeket állítgatnak és hangolnak, hogy meghatározzák, milyen a modell viselkedése.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "translatedText": "A mátrixszorzásról úgy lehet gondolkodni, hogy a mátrix minden sorát saját vektorként képzeljük el, és egy csomó pontterméket veszünk a sorok és a feldolgozandó vektor között, amit beágyazás esetén E-vel jelölök.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "translatedText": "Tegyük fel például, hogy az első sor történetesen megegyezik ezzel a Michael irányú keresztnévvel, amelyről feltételezzük, hogy létezik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "translatedText": "Ez azt jelentené, hogy a kimenet első összetevője, ez a pontproduktum itt, egy, ha a vektor a Michael keresztnevet kódolja, és nulla vagy negatív, ha nem.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "translatedText": "Még szórakoztatóbb, egy pillanatra gondoljon arra, hogy mit jelentene, ha az első sorban ez a keresztnév Michael plusz vezetéknév Jordan irányban.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "translatedText": "Az egyszerűség kedvéért hadd írjam le ezt úgy, hogy M plusz J.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "translatedText": "Ezután, ha pontszorzatot veszünk ezzel az E beágyazással, a dolgok nagyon szépen eloszlanak, így úgy néz ki, hogy M pont E plusz J pont E.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "translatedText": "És figyeljük meg, hogy ez azt jelenti, hogy a végső érték kettő lenne, ha a vektor a teljes Michael Jordan nevet kódolja, egyébként pedig egy vagy valami egynél kisebb érték lenne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "input": "And that's just one row in this matrix.",
  "translatedText": "És ez csak egy sor ebben a mátrixban.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "translatedText": "Gondolhatunk arra, hogy az összes többi sor párhuzamosan másfajta kérdéseket tesz fel, a feldolgozott vektor másfajta jellemzőit vizsgálva.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "translatedText": "Nagyon gyakran ez a lépés egy másik vektor hozzáadását is jelenti a kimenethez, amely tele van az adatokból tanult modellparaméterekkel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "input": "This other vector is known as the bias.",
  "translatedText": "Ez a másik vektor az úgynevezett torzítás.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "translatedText": "A példánkhoz azt szeretném, ha elképzelné, hogy az első komponensben az előfeszítés értéke negatív egy, ami azt jelenti, hogy a végső kimenetünk úgy néz ki, mint a vonatkozó pontproduktum, de mínusz egy.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "translatedText": "Joggal kérdezheted, hogy miért akarom, hogy azt feltételezd, hogy a modell megtanulta ezt, és egy pillanat múlva látni fogod, hogy miért nagyon tiszta és szép, ha van itt egy érték, amely pozitív, ha és csak akkor, ha egy vektor kódolja a Michael Jordan teljes nevét, és egyébként nulla vagy negatív.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "translatedText": "A mátrix összes sorszáma, ami körülbelül a feltett kérdések számát jelenti, a GPT-3 esetében, amelynek számait követtük, valamivel kevesebb, mint 50 000.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "translatedText": "Valójában ez pontosan négyszerese a beágyazási tér dimenzióinak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "input": "That's a design choice.",
  "translatedText": "Ez egy tervezési döntés.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "translatedText": "Lehetne több is, lehetne kevesebb is, de a tiszta többszörös általában barátságos a hardverek számára.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "translatedText": "Mivel ez a súlyokkal teli mátrix egy magasabb dimenziós térbe képez le minket, ezért a W up rövidítést használom.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "translatedText": "Továbbra is E-ként jelölöm a feldolgozott vektort, és jelöljük ezt a torzító vektort B-nek, és tegyük vissza mindezt a diagramra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "translatedText": "Ezen a ponton az a probléma, hogy ez a művelet tisztán lineáris, de a nyelv egy nagyon nem lineáris folyamat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "translatedText": "Ha az általunk mért belépési arány Michael plusz Jordan esetében magas, akkor szükségszerűen Michael plusz Phelps és Alexis plusz Jordan is kiváltja azt, annak ellenére, hogy ezek fogalmilag nem kapcsolódnak egymáshoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "input": "What you really want is a simple yes or no for the full name.",
  "translatedText": "Amit valójában szeretne, az egy egyszerű igen vagy nem a teljes névre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "translatedText": "A következő lépés tehát az, hogy ezt a nagy köztes vektort egy nagyon egyszerű nemlineáris függvényen keresztül kell vezetni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "translatedText": "Gyakori az a választás, amely az összes negatív értéket nullára képezi le, a pozitív értékeket pedig változatlanul hagyja.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "translatedText": "Folytatva a mélytanulás hagyományát a túlságosan divatos elnevezésekkel, ezt a nagyon egyszerű függvényt gyakran egyenesített lineáris egységnek, vagy röviden ReLU-nak nevezik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "input": "Here's what the graph looks like.",
  "translatedText": "Így néz ki a grafikon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "translatedText": "Tehát, ha a képzeletbeli példánkat vesszük, ahol a köztes vektor első bejegyzése egy, ha és csak akkor, ha a teljes név Michael Jordan, egyébként pedig nulla vagy negatív, miután átküldjük a ReLU-n, egy nagyon tiszta értéket kapunk, ahol az összes nulla és negatív értéket egyszerűen nullára vágjuk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "translatedText": "Tehát ez a kimenet a teljes Michael Jordan név esetén egy, egyébként pedig nulla.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "translatedText": "Más szóval, nagyon közvetlenül utánozza egy AND kapu viselkedését.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "translatedText": "Gyakran a modellek egy kissé módosított, JLU-nak nevezett funkciót használnak, amely ugyanazzal az alapformával rendelkezik, csak egy kicsit simább.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "translatedText": "De a mi céljaink szempontjából egy kicsit tisztább, ha csak a ReLU-ra gondolunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "translatedText": "Amikor az emberek a transzformátor neuronjaira hivatkoznak, akkor is ezekről az értékekről beszélnek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "translatedText": "Amikor látod a szokásos neurális hálózatos képet egy pontokból álló réteggel és egy csomó vonallal, amelyek az előző réteghez kapcsolódnak, amit korábban már láttál ebben a sorozatban, akkor ez általában egy lineáris lépés, egy mátrixszorzás kombinációját jelenti, amelyet egy egyszerű nemlineáris függvény, például egy ReLU követ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "translatedText": "Azt mondhatnánk, hogy ez a neuron aktív, ha ez az érték pozitív, és inaktív, ha ez az érték nulla.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "input": "The next step looks very similar to the first one.",
  "translatedText": "A következő lépés nagyon hasonlít az elsőhöz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "translatedText": "Megszorozzuk egy nagyon nagy mátrixszal, és hozzáadunk egy bizonyos torzító kifejezést.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "translatedText": "Ebben az esetben a kimeneti dimenziók száma visszaáll a beágyazási tér méretére, ezért ezt a mátrixot lefelé vetítő mátrixnak fogom nevezni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "translatedText": "És ezúttal ahelyett, hogy soronként gondolnánk a dolgokra, valójában szebb, ha oszloponként gondolunk rá.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "translatedText": "A mátrixszorzás egy másik módja, hogy fejben tartsuk a mátrix szorzását, hogy elképzeljük, hogy a mátrix minden egyes oszlopát megszorozzuk a megfelelő kifejezéssel a feldolgozott vektorban, és az összes átméretezett oszlopot összeadjuk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "translatedText": "Azért szebb így gondolkodni, mert itt az oszlopoknak ugyanaz a dimenziójuk, mint a beágyazási térnek, így úgy gondolhatunk rájuk, mint irányokra abban a térben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "translatedText": "Képzeljük el például, hogy a modell megtanulta, hogy az első oszlopot ebbe a kosárlabda irányba, amelyről feltételezzük, hogy létezik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "translatedText": "Ez azt jelenti, hogy amikor a megfelelő neuron az első pozícióban aktív, akkor ezt az oszlopot hozzáadjuk a végeredményhez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "translatedText": "De ha ez a neuron inaktív lenne, ha ez a szám nulla lenne, akkor ennek nem lenne hatása.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "input": "And it doesn't just have to be basketball.",
  "translatedText": "És ennek nem csak a kosárlabdának kell lennie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "translatedText": "A modell ebbe az oszlopba és sok más olyan tulajdonságot is be tudott sütni, amit valamihez akar társítani, aminek a teljes neve Michael Jordan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "translatedText": "Ugyanakkor a mátrix összes többi oszlopa megmondja, hogy mi fog hozzáadódni a végeredményhez, ha a megfelelő neuron aktív.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "translatedText": "És ha ebben az esetben van egy torzítás, akkor ez olyasvalami, amit minden egyes alkalommal hozzáadunk, függetlenül a neuronok értékeitől.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "input": "You might wonder what's that doing.",
  "translatedText": "Elgondolkodhatsz, hogy mit csinál ez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "translatedText": "Mint minden paraméterrel töltött objektum esetében, itt is nehéz pontosan megmondani.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "translatedText": "Lehet, hogy a hálózatnak van némi könyvelési feladat, de egyelőre nyugodtan figyelmen kívül hagyhatja.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "translatedText": "Hogy a jelölésünket ismét egy kicsit tömörebbé tegyük, ezt a nagy W mátrixot lefelé hívom, és hasonlóképpen lefelé hívom a B torzító vektort, és ezt visszahelyezzük a diagramunkba.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "translatedText": "Ahogy korábban már említettem, a végeredményt hozzáadjuk a vektorhoz, amely az adott pozícióban a blokkba áramlott, és így kapjuk meg a végeredményt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "translatedText": "Tehát például, ha a beáramló vektor a Michael keresztnevet és a Jordan vezetéknevet is kódolja, akkor mivel ez a műveletsorozat kiváltja az ÉS kaput, összeadja a kosárlabda irányát, így ami kiugrik, az mindezeket együtt fogja kódolni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "translatedText": "És ne feledjük, hogy ez a folyamat minden egyes vektorral párhuzamosan zajlik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "translatedText": "A GPT-3 számokat figyelembe véve ez azt jelenti, hogy ebben a blokkban nem csak 50 000 neuron van, hanem 50 000-szer annyi token, mint a bemenetben lévő tokenek száma.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "translatedText": "Ez tehát a teljes művelet, két mátrixtermék, mindegyikhez hozzáadva egy előfeszítést és egy egyszerű vágási függvényt a kettő között.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "translatedText": "Aki látta a sorozat korábbi videóit, az felismeri ezt a struktúrát, mint a legalapvetőbb neurális hálózatot, amelyet ott tanultunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "input": "In that example, it was trained to recognize handwritten digits.",
  "translatedText": "Ebben a példában a kézzel írt számjegyek felismerésére képezték ki.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "translatedText": "Itt, egy nagy nyelvi modell transzformátorának kontextusában ez egy nagyobb architektúra egyik darabja, és minden kísérlet arra, hogy értelmezzük, hogy pontosan mit is csinál, erősen összefonódik azzal az elképzeléssel, hogy az információt egy nagydimenziós beágyazási tér vektoraiba kódoljuk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "translatedText": "Ez az alapvető lecke, de szeretnék hátralépni és elgondolkodni két különböző dolgon, amelyek közül az első egyfajta könyvelés, a második pedig egy nagyon elgondolkodtató tényt tartalmaz a magasabb dimenziókról, amit valójában nem tudtam, amíg nem ástam bele magam a transzformátorokba.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "translatedText": "Az utolsó két fejezetben elkezdtük számolni a GPT-3 összes paraméterét, és megnéztük, hogy pontosan hol is laknak, ezért gyorsan fejezzük be a játékot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "translatedText": "Már említettem, hogy ez a felfelé vetítési mátrix alig kevesebb mint 50 000 sorból áll, és hogy minden sor megfelel a beágyazási tér méretének, ami a GPT-3 esetében 12 288.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "translatedText": "Ezeket összeszorozva 604 millió paramétert kapunk csak erre a mátrixra, és a lefelé vetítés ugyanennyi paramétert tartalmaz, csak transzponált alakkal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "input": "So together, they give about 1.2 billion parameters.",
  "translatedText": "Tehát együttesen körülbelül 1,2 milliárd paramétert adnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "translatedText": "A torzításvektor még néhány paramétert figyelembe vesz, de ez a teljes értéknek csak egy jelentéktelen része, ezért nem is mutatom meg.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "translatedText": "A GPT-3-ban a beágyazási vektorok ezen sorozata nem egy, hanem 96 különböző MLP-n keresztül folyik, így az összes ilyen blokkhoz tartozó paraméterek száma összesen körülbelül 116 milliárd.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "translatedText": "Ez a hálózat összes paraméterének körülbelül kétharmada, és ha ezt hozzáadjuk mindahhoz, ami korábban volt a figyelemblokkok, a beágyazás és a kiágyazás esetében, akkor valóban megkapjuk a hirdetett 175 milliárdos végösszeget.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "translatedText": "Valószínűleg érdemes megemlíteni, hogy van egy másik paraméterkészlet is, amely a normalizálási lépésekhez kapcsolódik, és amelyet ez a magyarázat kihagyott, de a torzítási vektorhoz hasonlóan ezek is csak egy nagyon jelentéktelen részét teszik ki a teljes értéknek.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "translatedText": "Ami a második gondolatmenetet illeti, talán elgondolkodik azon, hogy ez a központi játékpélda, amivel annyi időt töltöttünk, tükrözi-e azt, hogy a tényeket valójában hogyan tárolják a valódi nagy nyelvi modellekben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "translatedText": "Igaz, hogy az első mátrix sorai irányoknak tekinthetők ebben a beágyazási térben, és ez azt jelenti, hogy az egyes neuronok aktivációja azt mondja meg, hogy egy adott vektor mennyire igazodik egy adott irányhoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "translatedText": "Az is igaz, hogy a második mátrix oszlopai megmondják, hogy mi fog hozzáadódni az eredményhez, ha az adott neuron aktív.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "input": "Both of those are just mathematical facts.",
  "translatedText": "Mindkettő csak matematikai tény.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "translatedText": "A bizonyítékok azonban arra utalnak, hogy az egyes neuronok nagyon ritkán képviselnek egyetlen olyan tiszta jellemzőt, mint Michael Jordan, és ennek valójában nagyon jó oka lehet, ami az értelmezhetőséggel foglalkozó kutatók körében manapság elterjedt, szuperpozíciónak nevezett elképzeléshez kapcsolódik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "translatedText": "Ez egy olyan hipotézis, amely segíthet megmagyarázni, hogy miért különösen nehéz értelmezni a modelleket, és miért skálázódnak meglepően jól.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "translatedText": "Az alapötlet az, hogy ha van egy n-dimenziós tér, és egy csomó különböző jellemzőt akarsz ábrázolni olyan irányok segítségével, amelyek mind merőlegesek egymásra a térben, tudod, így ha hozzáadsz egy komponenst az egyik irányban, az nem befolyásolja a többi irányt, akkor a maximálisan elhelyezhető vektorok száma csak n, a dimenziók száma.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "translatedText": "Egy matematikus számára ez tulajdonképpen a dimenzió definíciója.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "translatedText": "Érdekes lesz azonban, ha egy kicsit lazítunk ezen a korláton, és elviselünk némi zajt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "translatedText": "Tegyük fel, hogy ezeket a jellemzőket nem pontosan merőleges, hanem csak majdnem merőleges vektorokkal lehet ábrázolni, amelyek egymástól 89 és 91 fok között helyezkednek el.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "input": "If we were in two or three dimensions, this makes no difference.",
  "translatedText": "Ha két vagy három dimenzióban lennénk, ez nem számítana.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "translatedText": "Ez alig ad extra mozgásteret ahhoz, hogy több vektort illesszünk be, ami még inkább ellenkezik azzal, hogy magasabb dimenziók esetén a válasz drámaian megváltozik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "translatedText": "Adhatok egy nagyon gyors és piszkos illusztrációt erre egy kis Python segítségével, amely létrehoz egy 100 dimenziós vektorokból álló listát, mindegyik véletlenszerűen inicializálva, és ez a lista 10.000 különböző vektort fog tartalmazni, tehát 100-szor annyi vektort, mint ahány dimenzió van.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "translatedText": "Ez a diagram itt mutatja a vektorpárok közötti szögek eloszlását.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "translatedText": "Mivel véletlenszerűen indultak, ezek a szögek 0 és 180 fok között bármi lehet, de észrevehetitek, hogy még a véletlenszerű vektorok esetében is erős a tendencia, hogy a dolgok közelebb vannak a 90 fokhoz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "translatedText": "Ezután egy bizonyos optimalizálási folyamatot fogok futtatni, amely iteratív módon eltolja ezeket a vektorokat, hogy megpróbáljanak egymásra merőlegesebbek lenni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "translatedText": "Miután ezt többször megismételtük, a szögek eloszlása a következőképpen néz ki.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "translatedText": "Valójában nagyítanunk kell, mert a vektorpárok közötti összes lehetséges szög a 89 és 91 fok közötti szűk tartományban van.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "translatedText": "Általánosságban a Johnson-Lindenstrauss lemma néven ismert tétel egyik következménye, hogy a dimenziók számával exponenciálisan nő azon vektorok száma, amelyeket egy térbe be lehet zsúfolni, és amelyek közel merőlegesek egymásra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "translatedText": "Ez nagyon fontos a nagy nyelvi modellek esetében, amelyeknek előnyös lehet, ha független gondolatokat társítanak egymásra közel merőleges irányokkal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "translatedText": "Ez azt jelenti, hogy sokkal, de sokkal több ötletet tud tárolni, mint amennyi dimenzió van a számára kijelölt helyen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "translatedText": "Ez részben megmagyarázhatja, hogy a modell teljesítménye miért skálázódik olyan jól a méret függvényében.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "translatedText": "Egy tízszer annyi dimenzióval rendelkező tér sokkal, de sokkal több, mint tízszer annyi független ötletet képes tárolni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "translatedText": "És ez nem csak arra a beágyazási térre vonatkozik, ahol a modellen átáramló vektorok élnek, hanem arra a neuronokkal teli vektorra is, amely a többrétegű perceptron közepén található, amelyet az imént tanulmányoztunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "translatedText": "Vagyis a GPT-3 méreteinél nem csak 50 000 jellemzőt szondázhatna, hanem ha ehelyett kihasználná ezt a hatalmas többletkapacitást a tér közel merőleges irányainak felhasználásával, akkor a feldolgozandó vektor sokkal, de sokkal több jellemzőjét szondázhatná.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "translatedText": "De ha ezt tenné, az azt jelentené, hogy az egyes funkciók nem úgy lennének láthatóak, mintha egyetlen neuron világítana.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "translatedText": "Ehelyett úgy kellene kinéznie, mint a neuronok valamilyen különleges kombinációjának, egy szuperpozíciónak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "translatedText": "Aki kíváncsi, hogy többet szeretne megtudni, a legfontosabb releváns keresőszó itt a sparse autoencoder, ami egy olyan eszköz, amelyet néhány értelmezhetőséggel foglalkozó ember használ, hogy megpróbálja kivonni a valódi jellemzőket, még akkor is, ha azok nagyon egymásra vannak helyezve az összes neurononon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "translatedText": "Belinkelek néhány igazán nagyszerű antropológiai bejegyzést erről.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "translatedText": "Ezen a ponton még nem érintettük a transzformátor minden részletét, de a legfontosabb pontokat már eltaláltuk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "translatedText": "A legfontosabb dolog, amivel a következő fejezetben foglalkozni akarok, a képzési folyamat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "translatedText": "Egyrészt a rövid válasz arra, hogy hogyan működik a képzés, az, hogy az egész backpropagation, és a sorozat korábbi fejezeteiben külön tárgyaltuk a backpropagationt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "translatedText": "De van még mit megvitatni, például a nyelvi modellekhez használt speciális költségfüggvényt, a finomhangolás gondolatát a megerősített tanulás segítségével, emberi visszajelzéssel, és a skálázási törvények fogalmát.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "translatedText": "Gyors megjegyzés az aktív követők között, van egy sor nem gépi tanulással kapcsolatos videók, hogy én izgatott, hogy süllyessze a fogaimat, mielőtt, hogy a következő fejezet, így lehet, hogy egy ideig, de ígérem, hogy jön a megfelelő időben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "input": "Thank you.",
  "translatedText": "Köszönöm.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]