1
00:00:00,000 --> 00:00:05,058
หากคุณป้อนวลีที่ว่า Michael Jordan กำลังเล่นกีฬาอะไรสักอย่างลงในโมเดลภาษาขนาดใหญ่ 

2
00:00:05,058 --> 00:00:10,301
และให้คุณพยากรณ์สิ่งที่จะเกิดขึ้นต่อไป ซึ่งคุณสามารถพยากรณ์บาสเก็ตบอลได้อย่างถูกต้อง 

3
00:00:10,301 --> 00:00:14,495
นั่นหมายความว่าในบางแห่ง ในพารามิเตอร์นับร้อยพันล้านพารามิเตอร์นั้น 

4
00:00:14,495 --> 00:00:18,320
คุณได้ฝังความรู้เกี่ยวกับบุคคลหนึ่งคนและกีฬาเฉพาะของเขาไว้แล้ว

5
00:00:18,940 --> 00:00:22,170
ฉันคิดว่าโดยทั่วไปแล้ว ใครก็ตามที่เคยเล่นกับโมเดลเหล่านี้มาก่อนจะรับรู

6
00:00:22,170 --> 00:00:25,400
้ได้อย่างชัดเจนว่าโมเดลเหล่านี้จดจำข้อเท็จจริงต่างๆ ไว้ได้เป็นจำนวนมาก

7
00:00:25,700 --> 00:00:29,160
คำถามที่สมเหตุสมผลที่คุณสามารถถามได้ก็คือ มันทำงานอย่างไรกันแน่?

8
00:00:29,160 --> 00:00:31,040
แล้วข้อเท็จจริงเหล่านั้นอยู่ที่ไหน?

9
00:00:35,720 --> 00:00:38,840
ในเดือนธันวาคมที่ผ่านมา นักวิจัยบางคนจาก Google DeepMind 

10
00:00:38,840 --> 00:00:43,220
ได้โพสต์เกี่ยวกับการทำงานเกี่ยวกับคำถามนี้ และพวกเขาใช้ตัวอย่างเฉพาะของการจับคู่

11
00:00:43,220 --> 00:00:44,480
นักกีฬากับกีฬาของพวกเขา

12
00:00:44,900 --> 00:00:49,951
แม้ว่าความเข้าใจเชิงกลไกอย่างสมบูรณ์เกี่ยวกับวิธีการจัดเก็บข้อเท็จจริงยังคงไม่มีคำตอบ 

13
00:00:49,951 --> 00:00:54,357
แต่ก็มีผลลัพธ์บางส่วนที่น่าสนใจ รวมถึงข้อสรุปทั่วไปในระดับสูงที่ว่าข้อเท็จจ

14
00:00:54,357 --> 00:00:58,351
ริงดูเหมือนจะมีชีวิตอยู่ภายในส่วนที่เฉพาะเจาะจงของเครือข่ายเหล่านี้ 

15
00:00:58,351 --> 00:01:02,640
ซึ่งเรียกกันอย่างแปลกประหลาดว่าเพอร์เซพตรอนหลายชั้นหรือเรียกสั้นๆ ว่า MLP

16
00:01:03,120 --> 00:01:07,436
ในสองสามบทที่ผ่านมา คุณและฉันได้เจาะลึกถึงรายละเอียดเบื้องหลังของทรานส์ฟอร์เมอร์ 

17
00:01:07,436 --> 00:01:12,020
สถาปัตยกรรมที่เป็นพื้นฐานของโมเดลภาษาขนาดใหญ่ และยังรวมถึงพื้นฐานของ AI สมัยใหม่อื่นๆ 

18
00:01:12,020 --> 00:01:12,500
อีกมากมาย

19
00:01:13,060 --> 00:01:16,200
ในบทล่าสุดนี้ เรามุ่งเน้นไปที่ชิ้นงานที่เรียกว่า Attention

20
00:01:16,840 --> 00:01:20,940
ขั้นตอนต่อไปสำหรับคุณและฉันคือการเจาะลึกรายละเอียดของสิ่งที่เกิดขึ

21
00:01:20,940 --> 00:01:25,040
้นภายในเพอร์เซพตรอนหลายชั้น ซึ่งเป็นส่วนประกอบขนาดใหญ่ของเครือข่าย

22
00:01:25,680 --> 00:01:30,100
การคำนวณที่นี่ค่อนข้างง่าย โดยเฉพาะเมื่อคุณเปรียบเทียบกับความสนใจ

23
00:01:30,560 --> 00:01:34,980
โดยพื้นฐานแล้ว มันจะสรุปลงเป็นการคูณเมทริกซ์คู่หนึ่งโดยมีอะไรบางอย่างง่ายๆ อยู่ระหว่างนั้น

24
00:01:35,720 --> 00:01:40,460
อย่างไรก็ตาม การตีความว่าการคำนวณเหล่านี้ทำอะไรอยู่ถือเป็นความท้าทายอย่างยิ่ง

25
00:01:41,560 --> 00:01:45,368
เป้าหมายหลักของเราที่นี่คือการก้าวผ่านการคำนวณและทำให้มันน่าจดจำ 

26
00:01:45,368 --> 00:01:49,234
แต่ฉันอยากทำในบริบทของการแสดงตัวอย่างเฉพาะเจาะจงว่าบล็อกเหล่านี้สา

27
00:01:49,234 --> 00:01:53,160
มารถจัดเก็บข้อเท็จจริงที่เป็นรูปธรรมได้อย่างไร อย่างน้อยก็ในหลักการ

28
00:01:53,580 --> 00:01:57,080
โดยเฉพาะอย่างยิ่งจะเป็นการจัดเก็บข้อเท็จจริงที่ว่าไมเคิล จอร์แดนเล่นบาสเก็ตบอล

29
00:01:58,080 --> 00:02:01,743
ฉันควรจะพูดถึงว่าเค้าโครงที่นี่ได้รับแรงบันดาลใจจากบทสนทนาที่ฉันมีกับนักวิจัย 

30
00:02:01,743 --> 00:02:03,200
DeepMind คนหนึ่งชื่อ Neil Nanda

31
00:02:04,060 --> 00:02:07,021
โดยส่วนใหญ่แล้ว ฉันจะถือว่าคุณได้ดูสองบทสุดท้ายไปแล้ว 

32
00:02:07,021 --> 00:02:10,312
หรือไม่ก็คุณน่าจะพอเข้าใจเบื้องต้นว่าทรานส์ฟอร์เมอร์คืออะไร 

33
00:02:10,312 --> 00:02:14,700
แต่การทบทวนก็ไม่เสียหาย ดังนั้น ต่อไปนี้คือคำเตือนสั้นๆ เกี่ยวกับกระบวนการโดยรวม

34
00:02:15,340 --> 00:02:21,320
คุณและฉันได้ศึกษาโมเดลที่ได้รับการฝึกฝนให้รับข้อความและคาดการณ์สิ่งที่จะเกิดขึ้นต่อไป

35
00:02:21,720 --> 00:02:25,482
ข้อความอินพุตนั้นจะถูกแบ่งออกเป็นโทเค็นหลายกลุ่มก่อน 

36
00:02:25,482 --> 00:02:30,381
ซึ่งหมายถึงชิ้นส่วนเล็กๆ ที่โดยทั่วไปแล้วจะเป็นคำหรือชิ้นส่วนคำเล็กๆ 

37
00:02:30,381 --> 00:02:35,280
และแต่ละโทเค็นจะเชื่อมโยงกับเวกเตอร์มิติสูง ซึ่งก็คือรายการตัวเลขยาวๆ

38
00:02:35,840 --> 00:02:40,155
ลำดับของเวกเตอร์เหล่านี้จะส่งผ่านการดำเนินการสองประเภทซ้ำๆ กัน ได้แก่ 

39
00:02:40,155 --> 00:02:44,655
การทำงานที่ต้องการความสนใจ ซึ่งทำให้เวกเตอร์สามารถส่งข้อมูลระหว่างกันได้ 

40
00:02:44,655 --> 00:02:49,032
จากนั้นจึงผ่านเพอร์เซพตรอนหลายชั้น ซึ่งเป็นสิ่งที่เราจะเจาะลึกในวันนี้ 

41
00:02:49,032 --> 00:02:52,300
และยังมีขั้นตอนการทำให้เป็นมาตรฐานอยู่ระหว่างนั้นด้วย

42
00:02:53,300 --> 00:02:59,623
หลังจากลำดับของเวกเตอร์ไหลผ่านการวนซ้ำที่แตกต่างกันหลายต่อหลายครั้งของทั้งสองบล็อกนี้ 

43
00:02:59,623 --> 00:03:05,284
เมื่อสิ้นสุดกระบวนการ ความหวังก็คือว่า เวกเตอร์แต่ละตัวจะได้รับข้อมูลเพียงพอ 

44
00:03:05,284 --> 00:03:10,946
ทั้งจากบริบท คำอื่นๆ ทั้งหมดในอินพุต และจากความรู้ทั่วไปที่รวมอยู่ในน้ำหนักขอ

45
00:03:10,946 --> 00:03:16,020
งโมเดลผ่านการฝึกอบรม ซึ่งสามารถนำไปใช้ทำนายว่าโทเค็นใดที่จะมาต่อไปได้

46
00:03:16,860 --> 00:03:20,072
แนวคิดสำคัญประการหนึ่งที่ฉันอยากให้คุณคิดอยู่ในใจคือ 

47
00:03:20,072 --> 00:03:23,890
เวกเตอร์ทั้งหมดเหล่านี้ต่างก็อาศัยอยู่ในปริภูมิที่มีมิติสูงมาก 

48
00:03:23,890 --> 00:03:28,800
และเมื่อคุณคิดถึงปริภูมินั้น ทิศทางต่างๆ ก็สามารถเข้ารหัสความหมายที่แตกต่างกันได้

49
00:03:30,120 --> 00:03:35,085
ตัวอย่างคลาสสิกมากๆ ที่ฉันชอบอ้างอิงถึงก็คือ หากคุณดูการฝังตัวของผู้หญิง 

50
00:03:35,085 --> 00:03:38,826
และลบการฝังตัวของผู้ชายออก แล้วคุณทำขั้นตอนเล็กๆ น้อยๆ 

51
00:03:38,826 --> 00:03:42,227
นั้นและเพิ่มเข้ากับคำนามเพศชายอีกคำหนึ่ง เช่น ลุง 

52
00:03:42,227 --> 00:03:46,240
คุณจะได้ตำแหน่งที่ใกล้เคียงกับคำนามเพศหญิงที่สอดคล้องกันมาก

53
00:03:46,440 --> 00:03:50,880
ในแง่นี้ ทิศทางเฉพาะนี้จะเข้ารหัสข้อมูลทางเพศ

54
00:03:51,640 --> 00:03:55,640
แนวคิดก็คือทิศทางที่แตกต่างกันอื่นๆ มากมายในพื้นที่มิติสูงส

55
00:03:55,640 --> 00:03:59,640
ุดนี้อาจสอดคล้องกับคุณลักษณะอื่นๆ ที่แบบจำลองอาจต้องการแสดง

56
00:04:01,400 --> 00:04:06,180
ในหม้อแปลง เวกเตอร์เหล่านี้ไม่ได้เข้ารหัสความหมายของคำเดียวเท่านั้น

57
00:04:06,680 --> 00:04:10,898
ขณะที่ข้อมูลไหลผ่านเครือข่าย ข้อมูลจะซึมซับความหมายที่สมบูรณ์ยิ่งขึ้

58
00:04:10,898 --> 00:04:15,180
นโดยอิงจากบริบททั้งหมดรอบตัวข้อมูล และยังอิงจากความรู้ของโมเดลอีกด้วย

59
00:04:15,880 --> 00:04:20,686
ท้ายที่สุดแล้ว ทุกคนจำเป็นต้องเข้ารหัสบางสิ่งบางอย่างที่ไกลเกินกว่าความหมายของคำเดียว 

60
00:04:20,686 --> 00:04:23,760
เนื่องจากต้องเพียงพอที่จะทำนายสิ่งที่จะเกิดขึ้นต่อไปได้

61
00:04:24,560 --> 00:04:28,364
เราได้เห็นแล้วว่าบล็อกความสนใจช่วยให้คุณรวมบริบทเข้าไปได้อย่างไร 

62
00:04:28,364 --> 00:04:31,174
แต่พารามิเตอร์โมเดลส่วนใหญ่จะอยู่ภายในบล็อก MLP 

63
00:04:31,174 --> 00:04:34,569
และสิ่งหนึ่งที่เราคิดว่าพารามิเตอร์เหล่านี้อาจทำอยู่ก็คือ 

64
00:04:34,569 --> 00:04:38,140
พารามิเตอร์เหล่านี้ให้ความจุเพิ่มเติมสำหรับจัดเก็บข้อเท็จจริง

65
00:04:38,720 --> 00:04:42,109
เหมือนที่ผมพูดไป บทเรียนในที่นี้จะเน้นที่ตัวอย่างของเล่นที่เป็นรูปธรรม 

66
00:04:42,109 --> 00:04:46,120
เพื่อแสดงให้เห็นว่าสามารถเก็บข้อเท็จจริงที่ว่าไมเคิล จอร์แดนเล่นบาสเก็ตบอลได้อย่างไร

67
00:04:47,120 --> 00:04:51,900
ตัวอย่างของเล่นนี้จะต้องให้คุณและฉันต้องตั้งสมมติฐานสองสามข้อเกี่ยวกับพื้นที่มิติสูงนี้

68
00:04:52,360 --> 00:04:57,124
ก่อนอื่น เราจะถือว่าทิศทางหนึ่งแสดงถึงแนวคิดเรื่องชื่อไมเคิล 

69
00:04:57,124 --> 00:05:02,826
จากนั้นทิศทางเกือบตั้งฉากอีกทิศทางหนึ่งแสดงถึงแนวคิดเรื่องนามสกุลจอร์แดน 

70
00:05:02,826 --> 00:05:06,420
และทิศทางที่สามจะแสดงถึงแนวคิดเรื่องบาสเก็ตบอล

71
00:05:07,400 --> 00:05:12,334
โดยเฉพาะอย่างยิ่ง สิ่งที่ฉันหมายถึงก็คือ หากคุณดูในเครือข่ายและดึงเวกเตอ

72
00:05:12,334 --> 00:05:16,857
ร์ตัวหนึ่งที่กำลังประมวลผลออกมา หากผลคูณจุดที่มีชื่อจริงของไมเคิล 

73
00:05:16,857 --> 00:05:22,340
ไดเรกชั่นเป็น 1 นั่นคือสิ่งที่เวกเตอร์จะเข้ารหัสความคิดของบุคคลที่มีชื่อจริงนั้น

74
00:05:23,800 --> 00:05:26,250
มิฉะนั้น ผลคูณจุดจะเท่ากับศูนย์หรือเป็นลบ ซึ่งห

75
00:05:26,250 --> 00:05:28,700
มายความว่าเวกเตอร์ไม่ได้เรียงตามทิศทางนั้นจริงๆ

76
00:05:29,420 --> 00:05:32,338
เพื่อความเรียบง่าย เราลองละเลยคำถามที่สมเหตุสม

77
00:05:32,338 --> 00:05:35,320
ผลว่าหากผลคูณจุดมีค่ามากกว่าหนึ่งอาจหมายถึงอะไร

78
00:05:36,200 --> 00:05:39,941
ในทำนองเดียวกัน ผลคูณจุดกับทิศทางอื่นๆ เหล่านี้ 

79
00:05:39,941 --> 00:05:43,760
จะบอกคุณได้ว่าหมายถึงนามสกุลจอร์แดนหรือบาสเก็ตบอล

80
00:05:44,740 --> 00:05:49,055
สมมุติว่าเวกเตอร์หมายถึงชื่อเต็มของไมเคิล จอร์แดน 

81
00:05:49,055 --> 00:05:52,680
ผลคูณจุดกับทิศทางทั้งสองนี้จะต้องเท่ากับ 1

82
00:05:53,480 --> 00:05:57,542
เนื่องจากข้อความ Michael Jordan ครอบคลุมโทเค็นสองแบบที่แตกต่างกัน 

83
00:05:57,542 --> 00:06:02,035
นั่นหมายความว่าเราจะต้องถือว่าบล็อกความสนใจก่อนหน้านี้ได้ส่งข้อมูลไปยังเว

84
00:06:02,035 --> 00:06:06,960
กเตอร์ที่สองจากสองเวกเตอร์นี้สำเร็จ เพื่อให้แน่ใจว่าสามารถเข้ารหัสทั้งสองชื่อได้

85
00:06:07,940 --> 00:06:11,480
เมื่อพิจารณาจากสมมติฐานทั้งหมดแล้ว มาดูเนื้อหาหลักของบทเรียนกันเลย

86
00:06:11,880 --> 00:06:14,980
เกิดอะไรขึ้นภายในเพอร์เซพตรอนหลายชั้น?

87
00:06:17,100 --> 00:06:20,330
คุณอาจคิดถึงลำดับของเวกเตอร์ที่ไหลเข้าไปในบล็อก 

88
00:06:20,330 --> 00:06:25,580
และจำไว้ว่าเวกเตอร์แต่ละตัวถูกเชื่อมโยงกับโทเค็นหนึ่งตัวจากข้อความอินพุตเดิมที

89
00:06:26,080 --> 00:06:31,066
สิ่งที่จะเกิดขึ้นก็คือเวกเตอร์แต่ละตัวจากลำดับนั้นจะต้องผ่านการดำเนินการชุดสั้นๆ 

90
00:06:31,066 --> 00:06:36,360
ซึ่งเราจะแกะมันออกในช่วงเวลาสั้นๆ และในตอนท้าย เราจะได้เวกเตอร์อีกตัวที่มีมิติเดียวกัน

91
00:06:36,880 --> 00:06:40,821
เวกเตอร์อื่นจะถูกเพิ่มเข้ากับเวกเตอร์ดั้งเดิมที่ไหลเข้ามา 

92
00:06:40,821 --> 00:06:43,200
และผลรวมนั้นก็คือผลลัพธ์ที่ไหลออกมา

93
00:06:43,720 --> 00:06:47,430
ลำดับการดำเนินการนี้เป็นสิ่งที่คุณใช้กับเวกเตอร์ทุกตัวในลำดับ 

94
00:06:47,430 --> 00:06:51,620
ซึ่งเชื่อมโยงกับโทเค็นทุกตัวในอินพุต และทั้งหมดนี้จะเกิดขึ้นแบบคู่ขนาน

95
00:06:52,100 --> 00:06:56,200
โดยเฉพาะอย่างยิ่ง เวกเตอร์จะไม่พูดคุยกันในขั้นตอนนี้ พวกมันทุกตัวต่างก็ทำสิ่งของตัวเอง

96
00:06:56,720 --> 00:06:59,800
และสำหรับคุณและฉัน มันทำให้มันง่ายขึ้นมาก เพราะมันหมายความว่าถ้

97
00:06:59,800 --> 00:07:02,588
าเราเข้าใจว่าเกิดอะไรขึ้นกับเวกเตอร์หนึ่งตัวผ่านบล็อกนี้ 

98
00:07:02,588 --> 00:07:06,060
เราก็จะเข้าใจได้อย่างมีประสิทธิภาพว่าเกิดอะไรขึ้นกับเวกเตอร์ทั้งหมดด้วย

99
00:07:07,100 --> 00:07:12,064
เมื่อผมบอกว่าบล็อกนี้จะเข้ารหัสความจริงที่ว่า Michael Jordan เล่นบาสเก็ตบอล 

100
00:07:12,064 --> 00:07:16,899
ผมหมายถึงว่าถ้ามีเวกเตอร์ไหลที่เข้ารหัสชื่อจริง Michael และนามสกุล Jordan 

101
00:07:16,899 --> 00:07:20,688
ลำดับการคำนวณนี้จะสร้างบางอย่างที่รวมทิศทางบาสเก็ตบอลด้วย 

102
00:07:20,688 --> 00:07:24,020
ซึ่งจะเป็นสิ่งที่เพิ่มเข้าไปในเวกเตอร์ในตำแหน่งนั้น

103
00:07:25,600 --> 00:07:29,700
ขั้นตอนแรกของกระบวนการนี้ดูเหมือนว่าการคูณเวกเตอร์นั้นด้วยเมทริกซ์ขนาดใหญ่

104
00:07:30,040 --> 00:07:31,980
ไม่น่าแปลกใจเลย นี่คือการเรียนรู้ที่ลึกซึ้ง

105
00:07:32,680 --> 00:07:35,948
และเมทริกซ์นี้ เช่นเดียวกับเมทริกซ์อื่นๆ ทั้งหมดที่เราเคยเห็น 

106
00:07:35,948 --> 00:07:38,531
เต็มไปด้วยพารามิเตอร์โมเดลที่เรียนรู้มาจากข้อมูล 

107
00:07:38,531 --> 00:07:42,116
ซึ่งคุณอาจคิดว่าเป็นปุ่มและหน้าปัดจำนวนหนึ่งที่ปรับเปลี่ยนและปรับแต่

108
00:07:42,116 --> 00:07:43,540
งเพื่อกำหนดพฤติกรรมของโมเดล

109
00:07:44,500 --> 00:07:47,681
วิธีที่ดีวิธีหนึ่งในการคิดเกี่ยวกับการคูณเมทริกซ์ก็คือ 

110
00:07:47,681 --> 00:07:50,979
ลองนึกภาพว่าแต่ละแถวของเมทริกซ์นั้นเป็นเวกเตอร์ของตัวเอง 

111
00:07:50,979 --> 00:07:55,086
แล้วนำผลคูณจุดจำนวนหนึ่งระหว่างแถวเหล่านั้นกับเวกเตอร์ที่กำลังประมวลผล 

112
00:07:55,086 --> 00:07:56,880
ซึ่งฉันจะเรียกว่า E เพื่อการฝัง

113
00:07:57,280 --> 00:08:04,040
ตัวอย่างเช่น สมมติว่าแถวแรกสุดบังเอิญตรงกับชื่อไมเคิลที่เราสันนิษฐานว่ามีอยู่จริง

114
00:08:04,320 --> 00:08:09,284
นั่นหมายความว่าส่วนประกอบแรกในเอาต์พุตนี้ ผลคูณจุดนี้ จะเป็น 1 

115
00:08:09,284 --> 00:08:14,800
หากเวกเตอร์นั้นเข้ารหัสชื่อจริง ไมเคิล และเป็นศูนย์หรือค่าลบในกรณีอื่น

116
00:08:15,880 --> 00:08:19,888
สนุกยิ่งขึ้นไปอีก ใช้เวลาสักครู่คิดดูว่าจะหมายถึงอะไร 

117
00:08:19,888 --> 00:08:23,080
หากแถวแรกเป็นชื่อไมเคิลบวกกับนามสกุลจอร์แดน

118
00:08:23,700 --> 00:08:27,420
เพื่อความเรียบง่าย ฉันขอเขียนเป็น M บวก J เลย

119
00:08:28,080 --> 00:08:31,247
จากนั้น การใช้ผลคูณจุดที่มีการฝังตัว E จะทำให้สิ่งต่างๆ 

120
00:08:31,247 --> 00:08:34,980
กระจายตัวได้อย่างสวยงาม ดังนั้นจึงมีลักษณะเป็น M จุด E บวก J จุด E

121
00:08:34,980 --> 00:08:39,840
และสังเกตว่านั่นหมายความว่าค่าสูงสุดจะเป็น 2 หากเวกเตอร์เข้ารหัสชื่อเต็มของ 

122
00:08:39,840 --> 00:08:44,700
Michael Jordan และหากไม่ใช่เช่นนั้น ค่าสูงสุดจะเป็น 1 หรือมีค่าที่น้อยกว่า 1

123
00:08:45,340 --> 00:08:47,260
นั่นเป็นเพียงหนึ่งแถวในเมทริกซ์นี้

124
00:08:47,600 --> 00:08:51,717
คุณอาจคิดว่าแถวอื่นๆ ทั้งหมดจะถามคำถามประเภทอื่นๆ ควบคู่กัน 

125
00:08:51,717 --> 00:08:56,040
หรือเจาะลึกคุณลักษณะประเภทอื่นๆ ของเวกเตอร์ที่กำลังประมวลผลอยู่

126
00:08:56,700 --> 00:09:00,024
บ่อยครั้งขั้นตอนนี้ยังเกี่ยวข้องกับการเพิ่มเวกเตอร์อีกตัวหนึ่งลงในเอาต์พุต 

127
00:09:00,024 --> 00:09:02,240
ซึ่งเต็มไปด้วยพารามิเตอร์โมเดลที่เรียนรู้จากข้อมูล

128
00:09:02,240 --> 00:09:04,560
เวกเตอร์อีกตัวหนึ่งนี้เรียกว่า อคติ

129
00:09:05,180 --> 00:09:10,369
สำหรับตัวอย่างของเรา ฉันต้องการให้คุณลองนึกภาพว่าค่าอคติในส่วนประกอบแรกสุดนี้เป็นค่าลบห

130
00:09:10,369 --> 00:09:15,560
นึ่ง ซึ่งหมายความว่าผลลัพธ์สุดท้ายของเราดูเหมือนผลคูณจุดที่เกี่ยวข้อง แต่เป็นค่าลบหนึ่ง

131
00:09:16,120 --> 00:09:21,637
คุณอาจถามได้อย่างสมเหตุสมผลว่าทำไมฉันต้องการให้คุณสันนิษฐานว่าโมเดลได้เรียนรู้สิ่งนี้ 

132
00:09:21,637 --> 00:09:26,963
และในช่วงเวลาสั้นๆ คุณจะเห็นว่าทำไมมันจึงสะอาดและดีมากถ้าเรามีค่าที่เป็นบวกก็ต่อเมื

133
00:09:26,963 --> 00:09:32,160
่อเวกเตอร์เข้ารหัสชื่อเต็มของ Michael Jordan และในกรณีอื่นๆ จะเป็นศูนย์หรือเป็นลบ

134
00:09:33,040 --> 00:09:38,151
จำนวนแถวทั้งหมดในเมทริกซ์นี้ ซึ่งก็เหมือนกับจำนวนคำถามที่ถูกถาม ในกรณีของ 

135
00:09:38,151 --> 00:09:42,780
GPT-3 ซึ่งเราได้ติดตามตัวเลขอยู่นั้น อยู่ที่ต่ำกว่า 50,000 เล็กน้อย

136
00:09:43,100 --> 00:09:46,640
ในความเป็นจริง มันเป็นสี่เท่าของจำนวนมิติในพื้นที่ฝังตัวนี้พอดี

137
00:09:46,920 --> 00:09:47,900
นั่นคือทางเลือกการออกแบบ

138
00:09:47,940 --> 00:09:52,240
คุณสามารถทำให้มันมากขึ้นหรือน้อยลงก็ได้ แต่การมีหลายตัวที่สะอาดมักจะส่งผลดีต่อฮาร์ดแวร์

139
00:09:52,740 --> 00:09:57,003
เนื่องจากเมทริกซ์ที่เต็มไปด้วยน้ำหนักนี้จะนำเราไปสู่พื้นที่มิติที่สูงกว่า 

140
00:09:57,003 --> 00:09:59,020
ดังนั้น ผมจึงจะใช้คำย่อว่า W ขึ้นมา

141
00:09:59,020 --> 00:10:02,776
ฉันจะดำเนินการติดป้ายกำกับเวกเตอร์ที่เรากำลังประมวลผลเป็น E ต่อไป 

142
00:10:02,776 --> 00:10:07,160
จากนั้นเราจะติดป้ายกำกับเวกเตอร์อคตินี้เป็น B ในแนวดิ่งและวางกลับลงในไดอะแกรม

143
00:10:09,180 --> 00:10:12,599
ณ จุดนี้ ปัญหาคือการดำเนินการนี้เป็นเชิงเส้นอย่างแท้จริง 

144
00:10:12,599 --> 00:10:15,360
แต่ภาษาเป็นกระบวนการที่ไม่เป็นเชิงเส้นอย่างมาก

145
00:10:15,880 --> 00:10:19,353
หากรายการที่เรากำลังวัดนั้นสูงสำหรับ Michael และ Jordan 

146
00:10:19,353 --> 00:10:23,261
มันก็จำเป็นต้องถูกกระตุ้นโดย Michael และ Phelps และ Alexis และ 

147
00:10:23,261 --> 00:10:28,100
Jordan ด้วยเช่นกัน แม้ว่าทั้งสองอย่างจะไม่มีความเกี่ยวข้องกันในเชิงแนวคิดก็ตาม

148
00:10:28,540 --> 00:10:32,000
สิ่งที่คุณต้องการจริงๆ คือคำตอบง่ายๆ ว่าใช่หรือไม่สำหรับชื่อเต็ม

149
00:10:32,900 --> 00:10:37,840
ดังนั้นขั้นตอนต่อไปคือการส่งเวกเตอร์กลางขนาดใหญ่ผ่านฟังก์ชันไม่เชิงเส้นที่เรียบง่ายมาก

150
00:10:38,360 --> 00:10:41,794
ทางเลือกทั่วไปคือทางเลือกที่นำค่าลบทั้งหมดมาแมปใ

151
00:10:41,794 --> 00:10:45,300
ห้เป็นศูนย์และปล่อยให้ค่าบวกทั้งหมดไม่เปลี่ยนแปลง

152
00:10:46,440 --> 00:10:50,741
และยังคงใช้ประเพณีการเรียนรู้แบบเจาะลึกที่มีชื่อที่หรูหราเกินจริง 

153
00:10:50,741 --> 00:10:56,020
ฟังก์ชันที่เรียบง่ายมากนี้มักถูกเรียกว่าหน่วยเชิงเส้นแก้ไขหรือเรียกสั้นๆ ว่า ReLU

154
00:10:56,020 --> 00:10:57,880
กราฟมีลักษณะดังนี้

155
00:10:58,300 --> 00:11:04,156
ดังนั้น การใช้ตัวอย่างที่เราจินตนาการขึ้น โดยที่ค่ารายการแรกของเวกเตอร์กลางมีค่าเป็นหนึ่ง 

156
00:11:04,156 --> 00:11:09,037
ก็ต่อเมื่อชื่อเต็มของเขาคือ Michael Jordan และเป็นศูนย์หรือค่าลบในกรณีอื่น 

157
00:11:09,037 --> 00:11:14,828
หลังจากที่คุณส่งค่าผ่าน ReLU แล้ว คุณจะได้ค่าที่สะอาดมากซึ่งค่าศูนย์และค่าลบทั้งหมดจะถูกต

158
00:11:14,828 --> 00:11:15,740
ัดให้เป็นศูนย์

159
00:11:16,100 --> 00:11:19,780
ดังนั้นผลลัพธ์นี้จะต้องเป็นหนึ่งสำหรับชื่อเต็มของไมเคิล จอร์แดน และเป็นศูนย์ในกรณีอื่น

160
00:11:20,560 --> 00:11:24,120
กล่าวอีกนัยหนึ่ง มันเลียนแบบพฤติกรรมของเกต AND โดยตรง

161
00:11:25,660 --> 00:11:28,815
บ่อยครั้งที่โมเดลต่างๆ จะใช้ฟังก์ชั่นที่ดัดแปลงเล็กน้อย เรียกว่า 

162
00:11:28,815 --> 00:11:32,020
JLU ซึ่งมีรูปทรงพื้นฐานเหมือนกัน เพียงแต่มีความนุ่มนวลกว่าเล็กน้อย

163
00:11:32,500 --> 00:11:35,720
แต่สำหรับวัตถุประสงค์ของเรา มันคงจะสะอาดกว่าเล็กน้อยหากเราคิดถึง ReLU เท่านั้น

164
00:11:36,740 --> 00:11:42,520
นอกจากนี้ เมื่อคุณได้ยินผู้คนพูดถึงเซลล์ประสาทของหม้อแปลง พวกเขากำลังพูดถึงค่าเหล่านี้อยู่

165
00:11:42,900 --> 00:11:48,974
เมื่อใดก็ตามที่คุณเห็นภาพเครือข่ายประสาทเทียมทั่วไปที่มีชั้นของจุดและเส้นจำนวนหนึ่งเชื่อม

166
00:11:48,974 --> 00:11:52,796
ต่อไปยังชั้นก่อนหน้า ซึ่งเรามีไว้ก่อนหน้านี้ในซีรีส์นี้ 

167
00:11:52,796 --> 00:11:57,915
โดยทั่วไปแล้วจะหมายถึงการถ่ายทอดการรวมกันของขั้นตอนเชิงเส้น การคูณเมทริกซ์ 

168
00:11:57,915 --> 00:12:01,260
ตามด้วยฟังก์ชันไม่เชิงเส้นแบบจำเพาะเทอม เช่น ReLU

169
00:12:02,500 --> 00:12:08,920
คุณจะบอกได้ว่านิวรอนนี้ทำงานอยู่เมื่อค่านี้เป็นบวก และจะไม่ทำงานหากค่าเป็นศูนย์

170
00:12:10,120 --> 00:12:12,380
ขั้นตอนต่อไปจะดูคล้ายกับขั้นตอนแรกมาก

171
00:12:12,560 --> 00:12:16,580
คุณคูณด้วยเมทริกซ์ที่มีขนาดใหญ่มากและคุณเพิ่มเงื่อนไขอคติบางอย่าง

172
00:12:16,980 --> 00:12:22,165
ในกรณีนี้ จำนวนมิติในเอาต์พุตจะลดลงเหลือเท่ากับขนาดของพื้นที่ฝังตัว 

173
00:12:22,165 --> 00:12:25,520
ดังนั้น ฉันจะเรียกสิ่งนี้ว่าเมทริกซ์การฉายลง

174
00:12:26,220 --> 00:12:31,360
และในครั้งนี้ แทนที่จะคิดแบบแถวต่อแถว จริงๆ แล้วจะดีกว่าหากคิดแบบคอลัมน์ต่อคอลัมน์

175
00:12:31,860 --> 00:12:35,560
คุณจะเห็นว่ามีอีกวิธีหนึ่งในการจดจำการคูณเมทริกซ์ในหัวได้ 

176
00:12:35,560 --> 00:12:40,153
นั่นคือลองจินตนาการถึงการเอาแต่ละคอลัมน์ของเมทริกซ์มาคูณด้วยเทอมที่สอดคล

177
00:12:40,153 --> 00:12:45,640
้องกันในเวกเตอร์ที่กำลังประมวลผล จากนั้นจึงบวกคอลัมน์ที่ปรับขนาดใหม่ทั้งหมดเข้าด้วยกัน

178
00:12:46,840 --> 00:12:52,131
เหตุผลที่ดีกว่าที่จะคิดในลักษณะนี้ก็คือ ในกรณีนี้ คอลัมน์จะมีมิติเดียวกันกับพื้นที่ฝัง 

179
00:12:52,131 --> 00:12:55,780
ดังนั้นเราจึงคิดถึงคอลัมน์เหล่านี้เป็นทิศทางในพื้นที่นั้นได้

180
00:12:56,140 --> 00:12:59,576
ตัวอย่างเช่น เราจะลองจินตนาการว่าโมเดลได้เรียนรู้ที่

181
00:12:59,576 --> 00:13:03,080
จะสร้างคอลัมน์แรกในทิศทางบาสเก็ตบอลที่เราคิดว่ามีอยู่

182
00:13:04,180 --> 00:13:08,246
ซึ่งหมายความว่าเมื่อนิวรอนที่เกี่ยวข้องในตำแหน่งแรกทำงานอยู่ 

183
00:13:08,246 --> 00:13:10,780
เราจะเพิ่มคอลัมน์นี้ลงในผลลัพธ์สุดท้าย

184
00:13:11,140 --> 00:13:15,780
แต่หากนิวรอนนั้นไม่ได้ทำงาน หากตัวเลขนั้นเป็นศูนย์ การกระทำดังกล่าวจะไม่มีผลใดๆ

185
00:13:16,500 --> 00:13:18,060
และมันไม่จำเป็นต้องเป็นแค่บาสเก็ตบอลเท่านั้น

186
00:13:18,220 --> 00:13:21,736
นอกจากนี้ โมเดลดังกล่าวยังสามารถนำไปใช้ในคอลัมน์นี้และฟีเจอร์อื่นๆ 

187
00:13:21,736 --> 00:13:25,200
มากมายที่ต้องการเชื่อมโยงกับบางสิ่งที่มีชื่อเต็มว่า Michael Jordan

188
00:13:26,980 --> 00:13:31,781
ในเวลาเดียวกัน คอลัมน์อื่นๆ ทั้งหมดในเมทริกซ์นี้จะบอกคุณว่าจะม

189
00:13:31,781 --> 00:13:36,660
ีการเพิ่มอะไรลงในผลลัพธ์สุดท้าย หากนิวรอนที่เกี่ยวข้องทำงานอยู่

190
00:13:37,360 --> 00:13:41,615
และหากคุณมีอคติในกรณีนี้ มันเป็นสิ่งที่คุณเพียงแค่เพิ่มเข้าไปทุกครั้ง 

191
00:13:41,615 --> 00:13:43,500
โดยไม่คำนึงถึงค่าของเซลล์ประสาท

192
00:13:44,060 --> 00:13:45,280
คุณอาจสงสัยว่านั่นทำอะไรอยู่

193
00:13:45,540 --> 00:13:49,320
เช่นเดียวกับวัตถุที่เติมพารามิเตอร์ทั้งหมดที่นี่ มันเป็นเรื่องยากที่จะพูดได้ชัดเจน

194
00:13:49,320 --> 00:13:54,380
บางทีเครือข่ายอาจจำเป็นต้องทำบัญชีบางอย่าง แต่คุณสามารถละเลยไปก่อนได้

195
00:13:54,860 --> 00:13:59,464
เพื่อให้สัญลักษณ์ของเรากระชับขึ้นอีกเล็กน้อย ฉันจะเรียกเมทริกซ์ขนาดใหญ่ 

196
00:13:59,464 --> 00:14:04,260
W ลง และเรียกเวกเตอร์อคติ B ลงเช่นเดียวกัน และใส่กลับเข้าไปในไดอะแกรมของเรา

197
00:14:04,740 --> 00:14:08,990
เช่นเดียวกับที่ฉันดูตัวอย่างไว้ก่อนหน้านี้ สิ่งที่คุณต้องทำกับผลลัพธ์สุดท้ายนี้คือการเพิ่

198
00:14:08,990 --> 00:14:13,240
มผลลัพธ์นั้นลงในเวกเตอร์ที่ไหลเข้าไปในบล็อกที่ตำแหน่งนั้น แล้วคุณก็จะได้ผลลัพธ์สุดท้ายนี้

199
00:14:13,820 --> 00:14:19,029
ตัวอย่างเช่น หากเวกเตอร์ที่ไหลเข้ามาเข้ารหัสทั้งชื่อจริง ไมเคิล และนามสกุล 

200
00:14:19,029 --> 00:14:23,474
จอร์แดน ดังนั้น เนื่องจากลำดับการดำเนินการนี้จะทริกเกอร์เกต AND 

201
00:14:23,474 --> 00:14:29,240
และจะเพิ่มทิศทางของบาสเก็ตบอล ดังนั้น สิ่งที่ออกมาจะเข้ารหัสทั้งหมดเหล่านั้นร่วมกัน

202
00:14:29,820 --> 00:14:34,200
และจำไว้ว่านี่คือกระบวนการที่เกิดขึ้นกับเวกเตอร์แต่ละตัวแบบคู่ขนาน

203
00:14:34,800 --> 00:14:40,336
โดยเฉพาะอย่างยิ่ง เมื่อใช้ตัวเลข GPT-3 นั่นหมายความว่าบล็อกนี้ไม่ได้มีเพียงนิวรอน 

204
00:14:40,336 --> 00:14:44,860
50,000 ตัวเท่านั้น แต่ยังมีโทเค็นในอินพุตมากกว่า 50,000 เท่าอีกด้วย

205
00:14:48,180 --> 00:14:51,082
นั่นคือการดำเนินการทั้งหมด ผลิตภัณฑ์เมทริกซ์สองตัว 

206
00:14:51,082 --> 00:14:55,180
โดยแต่ละตัวมีการเพิ่มค่าอคติ และมีฟังก์ชันการตัดแบบง่ายๆ อยู่ระหว่างนั้น

207
00:14:56,080 --> 00:14:59,422
ใครก็ตามที่เคยดูวิดีโอก่อนหน้านี้ของซีรีส์นี้คงจะจำโครงสร้างนี้ได้ดี 

208
00:14:59,422 --> 00:15:02,620
เนื่องจากเป็นเครือข่ายประสาทเทียมขั้นพื้นฐานที่สุดที่เราศึกษากันมา

209
00:15:03,080 --> 00:15:06,100
ในตัวอย่างนั้น มันได้รับการฝึกให้จดจำตัวเลขที่เขียนด้วยลายมือ

210
00:15:06,580 --> 00:15:12,903
ที่นี่ ในบริบทของตัวแปลงสำหรับโมเดลภาษาขนาดใหญ่ นี่คือชิ้นหนึ่งในสถาปัตยกรรมที่ใหญ่กว่า 

211
00:15:12,903 --> 00:15:18,437
และความพยายามใดๆ ที่จะตีความว่าสิ่งนี้กำลังทำอะไรอยู่กันแน่มีความเกี่ยวพันอย่

212
00:15:18,437 --> 00:15:23,180
างมากกับแนวคิดในการเข้ารหัสข้อมูลในเวกเตอร์ของพื้นที่ฝังตัวมิติสูง

213
00:15:24,260 --> 00:15:29,105
นั่นคือบทเรียนสำคัญ แต่ฉันต้องการที่จะถอยกลับมาและไตร่ตรองเกี่ยวกับสองสิ่งที่แตกต่างกัน 

214
00:15:29,105 --> 00:15:33,675
สิ่งแรกคือการทำบัญชี และสิ่งที่สองเกี่ยวข้องกับข้อเท็จจริงที่ชวนคิดมากเกี่ยวกับมิติ

215
00:15:33,675 --> 00:15:38,080
ที่สูงกว่าซึ่งจริงๆ แล้วฉันไม่รู้มาก่อนจนกระทั่งฉันได้ศึกษาเรื่องทรานส์ฟอร์เมอร์

216
00:15:41,080 --> 00:15:46,076
ในสองบทสุดท้าย คุณและฉันเริ่มนับจำนวนพารามิเตอร์ทั้งหมดใน GPT-3 

217
00:15:46,076 --> 00:15:50,760
และดูว่ามันอยู่ที่ไหนแน่ชัด ดังนั้นมาจบเกมกันที่นี่เลยดีกว่า

218
00:15:51,400 --> 00:15:56,705
ฉันกล่าวถึงไปแล้วว่าเมทริกซ์การฉายขึ้นด้านบนมีแถวประมาณ 50,000 

219
00:15:56,705 --> 00:16:02,180
แถว และแต่ละแถวมีขนาดตรงกับพื้นที่ฝัง ซึ่งสำหรับ GPT-3 คือ 12,288

220
00:16:03,240 --> 00:16:06,758
เมื่อคูณค่าเหล่านั้นเข้าด้วยกัน เราจะได้พารามิเตอร์ 604 

221
00:16:06,758 --> 00:16:11,783
ล้านตัวสำหรับเมทริกซ์นั้นเพียงอย่างเดียว และการฉายลงจะมีจำนวนพารามิเตอร์เท่ากัน 

222
00:16:11,783 --> 00:16:13,920
เพียงแต่มีรูปร่างที่สลับตำแหน่งกัน

223
00:16:14,500 --> 00:16:17,400
ดังนั้นเมื่อรวมกันแล้วจะได้พารามิเตอร์ประมาณ 1.2 พันล้านตัว

224
00:16:18,280 --> 00:16:20,503
เวกเตอร์อคติยังคำนึงถึงพารามิเตอร์อีกสองสามตัว 

225
00:16:20,503 --> 00:16:24,100
แต่เป็นสัดส่วนที่เล็กน้อยเมื่อเทียบกับทั้งหมด ดังนั้น ฉันจะไม่แสดงมันด้วยซ้ำ

226
00:16:24,660 --> 00:16:29,450
ใน GPT-3 ลำดับของเวกเตอร์ฝังตัวนี้ไม่ได้ไหลผ่านเพียง 1 แต่ไหลผ่านถึง 

227
00:16:29,450 --> 00:16:33,894
96 MLP ที่แตกต่างกัน ดังนั้น จำนวนพารามิเตอร์ทั้งหมดที่เกี่ยวข้อ

228
00:16:33,894 --> 00:16:38,060
งกับบล็อกทั้งหมดเหล่านี้จึงรวมกันได้ประมาณ 116 พันล้านรายการ

229
00:16:38,820 --> 00:16:42,564
ซึ่งเป็นประมาณ 2 ใน 3 ของพารามิเตอร์ทั้งหมดในเครือข่าย 

230
00:16:42,564 --> 00:16:46,922
และเมื่อคุณเพิ่มทุกอย่างที่เรามีก่อนหน้านี้ สำหรับบล็อกความสนใจ 

231
00:16:46,922 --> 00:16:51,620
การฝัง และการยกเลิกการฝัง คุณจะได้รับยอดรวม 175 พันล้านตามที่โฆษณาไว้

232
00:16:53,060 --> 00:16:56,636
คงจะคุ้มค่าที่จะกล่าวถึงว่ามีชุดพารามิเตอร์อีกชุดหนึ่งที่เกี่ยวข้องกั

233
00:16:56,636 --> 00:17:00,885
บขั้นตอนการทำให้เป็นมาตรฐานซึ่งคำอธิบายนี้ได้ละเว้นไป แต่เช่นเดียวกับเวกเตอร์อคติ 

234
00:17:00,885 --> 00:17:03,840
พารามิเตอร์เหล่านี้คิดเป็นสัดส่วนที่เล็กน้อยมากของทั้งหมด

235
00:17:05,900 --> 00:17:10,759
สำหรับประเด็นที่สองนี้ คุณอาจสงสัยว่าตัวอย่างของเล่นหลักที่เราใช้เวลามากมายในกา

236
00:17:10,759 --> 00:17:15,680
รเรียนรู้นี้ สะท้อนให้เห็นว่าข้อเท็จจริงถูกจัดเก็บในโมเดลภาษาขนาดใหญ่จริงหรือไม่

237
00:17:16,319 --> 00:17:21,111
เป็นเรื่องจริงที่แถวของเมทริกซ์แรกนั้นสามารถคิดได้ว่าเป็นทิศทางในพื้นที่ฝังตัวนี้ 

238
00:17:21,111 --> 00:17:24,851
และนั่นหมายถึงการเปิดใช้งานนิวรอนแต่ละตัวจะบอกคุณว่าเวกเตอร์ที่ก

239
00:17:24,851 --> 00:17:27,540
ำหนดนั้นสอดคล้องกับทิศทางเฉพาะเจาะจงมากเพียงใด

240
00:17:27,760 --> 00:17:31,015
จริงอยู่ที่คอลัมน์ของเมทริกซ์ที่สองจะบอกคุณว่าจ

241
00:17:31,015 --> 00:17:34,340
ะมีการเพิ่มอะไรลงในผลลัพธ์หากนิวรอนนั้นทำงานอยู่

242
00:17:34,640 --> 00:17:36,800
นั่นทั้งสองเป็นเพียงข้อเท็จจริงทางคณิตศาสตร์

243
00:17:37,740 --> 00:17:43,177
อย่างไรก็ตาม หลักฐานชี้ให้เห็นว่าเซลล์ประสาทแต่ละเซลล์แทบจะไม่แสดงลักษณะที่ชัดเจ

244
00:17:43,177 --> 00:17:48,138
นแบบเดียวกับที่ไมเคิล จอร์แดนทำเลย และอาจมีเหตุผลที่ดีมากที่เป็นเช่นนั้น 

245
00:17:48,138 --> 00:17:54,120
ซึ่งเกี่ยวข้องกับแนวคิดที่นักวิจัยด้านการตีความในปัจจุบันนิยมใช้ ซึ่งเรียกว่า การซ้อนทับ

246
00:17:54,640 --> 00:17:59,219
นี่คือสมมติฐานที่อาจช่วยอธิบายได้ว่าเหตุใดแบบจำลองเหล่านี้จึงตีความได้ยากเป็นพิเศษ 

247
00:17:59,219 --> 00:18:02,420
และเหตุใดแบบจำลองเหล่านี้จึงปรับขนาดได้ดีอย่างน่าประหลาดใจ

248
00:18:03,500 --> 00:18:08,632
แนวคิดพื้นฐานก็คือ ถ้าคุณมีปริภูมิ n มิติ และคุณต้องการแสดงคุณลักษณะต่าง 

249
00:18:08,632 --> 00:18:13,765
ๆ มากมายโดยใช้ทิศทางที่ตั้งฉากกันในปริภูมินั้น คุณรู้ไหมว่า ด้วยวิธีนั้น 

250
00:18:13,765 --> 00:18:18,897
หากคุณเพิ่มส่วนประกอบในทิศทางหนึ่ง ส่วนประกอบนั้นจะไม่ส่งผลต่อทิศทางอื่น 

251
00:18:18,897 --> 00:18:23,960
ๆ ดังนั้น จำนวนเวกเตอร์สูงสุดที่คุณใส่ได้คือ n ซึ่งเป็นจำนวนมิติเท่านั้น

252
00:18:24,600 --> 00:18:27,620
สำหรับนักคณิตศาสตร์ จริงๆ แล้วนี่คือคำจำกัดความของมิติ

253
00:18:28,220 --> 00:18:33,580
แต่สิ่งที่น่าสนใจคือหากคุณผ่อนคลายข้อจำกัดนั้นลงสักหน่อยและยอมทนกับเสียงบ้าง

254
00:18:34,180 --> 00:18:39,616
สมมติว่าคุณยอมให้คุณลักษณะเหล่านั้นแสดงด้วยเวกเตอร์ที่ไม่ได้ตั้งฉากกันพอดี 

255
00:18:39,616 --> 00:18:43,820
แต่เกือบจะตั้งฉากกัน โดยมีระยะห่างกันประมาณ 89 ถึง 91 องศา

256
00:18:44,820 --> 00:18:48,020
หากเราอยู่ในสองหรือสามมิติ มันก็ไม่มีความแตกต่างกัน

257
00:18:48,260 --> 00:18:52,192
นั่นทำให้คุณแทบไม่มีช่องว่างเพิ่มขึ้นเลยเพื่อใส่เวกเตอร์เพิ่มเติมเข้าไป 

258
00:18:52,192 --> 00:18:56,780
ซึ่งทำให้ขัดกับสัญชาตญาณมากยิ่งขึ้นว่าสำหรับมิติที่ใหญ่กว่า คำตอบจะเปลี่ยนไปอย่างมาก

259
00:18:57,660 --> 00:19:03,849
ฉันสามารถให้ภาพประกอบที่รวดเร็วและหยาบๆ แก่คุณได้โดยใช้ Python เพื่อสร้างรายการเวกเตอร์ 

260
00:19:03,849 --> 00:19:07,718
100 มิติ โดยที่แต่ละรายการจะถูกกำหนดค่าเริ่มต้นแบบสุ่ม 

261
00:19:07,718 --> 00:19:13,274
และรายการนี้จะมีเวกเตอร์ที่แตกต่างกัน 10,000 รายการ ดังนั้นจึงเป็นเวกเตอร์ 100 

262
00:19:13,274 --> 00:19:14,400
เท่าของจำนวนมิติ

263
00:19:15,320 --> 00:19:19,900
กราฟนี้แสดงการกระจายตัวของมุมระหว่างคู่เวกเตอร์เหล่านี้

264
00:19:20,680 --> 00:19:24,661
เนื่องจากเริ่มต้นแบบสุ่ม มุมต่างๆ เหล่านี้จึงสามารถเป็นอะไรก็ได้ตั้งแต่ 

265
00:19:24,661 --> 00:19:28,255
0 ถึง 180 องศา แต่คุณจะสังเกตเห็นได้ว่า แม้แต่สำหรับเวกเตอร์สุ่ม 

266
00:19:28,255 --> 00:19:31,960
ก็มีความลำเอียงอย่างมากที่จะให้สิ่งต่างๆ อยู่ใกล้กับ 90 องศามากขึ้น

267
00:19:32,500 --> 00:19:36,976
จากนั้นสิ่งที่ฉันจะทำคือรันกระบวนการเพิ่มประสิทธิภาพบางอย่างที่กระ

268
00:19:36,976 --> 00:19:41,520
ตุ้นเวกเตอร์ทั้งหมดเหล่านี้ซ้ำๆ เพื่อให้พยายามตั้งฉากกันมากยิ่งขึ้น

269
00:19:42,060 --> 00:19:46,660
หลังจากทำซ้ำหลายๆ ครั้ง การกระจายของมุมจะมีลักษณะดังต่อไปนี้

270
00:19:47,120 --> 00:19:54,978
เราต้องซูมเข้าไปที่นี่จริงๆ เพราะมุมที่เป็นไปได้ทั้งหมดระหว่างคู่เวกเตอร์จะอยู่ในช่วงแคบๆ 

271
00:19:54,978 --> 00:19:56,900
ระหว่าง 89 ถึง 91 องศา

272
00:19:58,020 --> 00:20:03,205
โดยทั่วไป ผลที่ตามมาของสิ่งที่เรียกว่าเล็มมาจอห์นสัน-ลินเดนชตราส์ ก็คือ 

273
00:20:03,205 --> 00:20:09,615
จำนวนเวกเตอร์ที่คุณสามารถยัดลงในช่องว่างที่เกือบจะตั้งฉากเช่นนี้จะเพิ่มขึ้นแบบเอ็กซ์โพเนน

274
00:20:09,615 --> 00:20:10,840
เชียลตามจำนวนมิติ

275
00:20:11,960 --> 00:20:15,920
สิ่งนี้มีความสำคัญมากสำหรับโมเดลภาษาขนาดใหญ่ ซึ่งอาจได้รับปร

276
00:20:15,920 --> 00:20:19,880
ะโยชน์จากการเชื่อมโยงความคิดอิสระกับทิศทางที่เกือบตั้งฉากกัน

277
00:20:20,000 --> 00:20:26,440
หมายความว่ามันสามารถจัดเก็บไอเดียต่างๆ ได้มากกว่าขนาดในพื้นที่ที่จัดไว้ให้

278
00:20:27,320 --> 00:20:31,740
นี่อาจอธิบายได้บางส่วนว่าเหตุใดประสิทธิภาพของโมเดลจึงดูปรับขนาดได้ดีตามขนาด

279
00:20:32,540 --> 00:20:39,400
พื้นที่ที่มีขนาดมากขึ้น 10 เท่าสามารถจัดเก็บแนวคิดอิสระได้มากกว่า 10 เท่าเลยทีเดียว

280
00:20:40,420 --> 00:20:44,774
ซึ่งสิ่งนี้มีความเกี่ยวข้องไม่เพียงกับพื้นที่ฝังตัวที่เวกเตอร์ไหลผ่านโมเดลเท่านั้น 

281
00:20:44,774 --> 00:20:48,079
แต่ยังเกี่ยวข้องกับเวกเตอร์ที่เต็มไปด้วยเซลล์ประสาทในตรงกลางของ

282
00:20:48,079 --> 00:20:50,440
เพอร์เซพตรอนหลายชั้นที่เราเพิ่งศึกษาไปอีกด้วย

283
00:20:50,960 --> 00:20:56,671
กล่าวอีกนัยหนึ่ง ในขนาดของ GPT-3 มันอาจไม่เพียงแต่ตรวจสอบฟีเจอร์ 50,000 รายการเท่านั้น 

284
00:20:56,671 --> 00:21:02,119
แต่หากมันใช้ประโยชน์จากความจุเพิ่มมหาศาลนี้โดยใช้ทิศทางที่เกือบจะตั้งฉากกับพื้นที่ 

285
00:21:02,119 --> 00:21:07,240
มันก็จะตรวจสอบฟีเจอร์ต่างๆ ของเวกเตอร์ที่กำลังประมวลผลได้เพิ่มมากขึ้นอีกมากมาย

286
00:21:07,780 --> 00:21:14,340
แต่หากทำแบบนั้น แสดงว่าคุณลักษณะแต่ละอย่างจะไม่ปรากฏให้เห็นเมื่อเซลล์ประสาทเดี่ยวส่องสว่าง

287
00:21:14,660 --> 00:21:19,380
มันคงจะต้องดูเหมือนการรวมตัวของเซลล์ประสาทที่เฉพาะเจาะจงมากกว่า ซึ่งเป็นการซ้อนทับ

288
00:21:20,400 --> 00:21:24,633
สำหรับใครก็ตามที่อยากเรียนรู้เพิ่มเติม คำค้นหาที่เกี่ยวข้องที่สำคัญที่นี่คือ 

289
00:21:24,633 --> 00:21:28,756
sparse autoencoder ซึ่งเป็นเครื่องมือที่นักตีความบางส่วนใช้เพื่อพยายามดึงเอ

290
00:21:28,756 --> 00:21:32,880
าคุณสมบัติที่แท้จริงออกมา แม้ว่าจะซ้อนทับกันมากบนนิวรอนทั้งหมดเหล่านี้ก็ตาม

291
00:21:33,540 --> 00:21:36,800
ฉันจะลิงค์ไปยังโพสต์เกี่ยวกับมนุษยวิทยาที่ยอดเยี่ยมสองสามโพสต์เกี่ยวกับเรื่องนี้

292
00:21:37,880 --> 00:21:40,702
ถึงตอนนี้ เราไม่ได้พูดถึงทุกรายละเอียดของหม้อแปลง 

293
00:21:40,702 --> 00:21:43,300
แต่คุณและฉันได้พูดถึงประเด็นที่สำคัญที่สุดแล้ว

294
00:21:43,520 --> 00:21:47,640
สิ่งสำคัญที่ฉันอยากจะพูดถึงในบทถัดไปคือกระบวนการฝึกอบรม

295
00:21:48,460 --> 00:21:52,553
คำตอบสั้นๆ เกี่ยวกับวิธีการทำงานของการฝึกอบรมก็คือ ทั้งหมดเป็นการแบ็กโพรพาเกชั่น 

296
00:21:52,553 --> 00:21:56,900
และเราได้ครอบคลุมเรื่องการแบ็กโพรพาเกชั่นในบริบทที่แยกจากกันในบทก่อนหน้าในชุดบทความนี้

297
00:21:57,220 --> 00:21:59,817
แต่ยังมีเรื่องอื่น ๆ ที่ต้องพูดคุยกันอีก เช่น 

298
00:21:59,817 --> 00:22:03,318
ฟังก์ชันต้นทุนเฉพาะที่ใช้สำหรับโมเดลภาษา แนวคิดในการปรับแต่งโด

299
00:22:03,318 --> 00:22:07,780
ยใช้การเรียนรู้เชิงเสริมแรงด้วยข้อเสนอแนะของมนุษย์ และแนวคิดเรื่องกฎการปรับขนาด

300
00:22:08,960 --> 00:22:11,571
หมายเหตุสั้นๆ สำหรับผู้ติดตามที่กระตือรือร้นในหมู่พวกคุณ 

301
00:22:11,571 --> 00:22:15,235
มีวิดีโอจำนวนหนึ่งที่ไม่เกี่ยวข้องกับการเรียนรู้ของเครื่องซึ่งฉันตื่นเต้นที่จะได

302
00:22:15,235 --> 00:22:18,442
้ดูอย่างตั้งใจก่อนที่จะเริ่มต้นบทต่อไป ดังนั้น อาจจะต้องใช้เวลาสักพัก 

303
00:22:18,442 --> 00:22:20,000
แต่ฉันสัญญาว่าจะมาในเวลาที่เหมาะสม

304
00:22:35,640 --> 00:22:37,920
ขอบคุณ

