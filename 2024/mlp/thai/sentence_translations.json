[
 {
  "translatedText": "หากคุณป้อนวลีที่ว่า Michael Jordan กำลังเล่นกีฬาอะไรสักอย่างลงในโมเดลภาษาขนาดใหญ่ และให้คุณพยากรณ์สิ่งที่จะเกิดขึ้นต่อไป ซึ่งคุณสามารถพยากรณ์บาสเก็ตบอลได้อย่างถูกต้อง นั่นหมายความว่าในบางแห่ง ในพารามิเตอร์นับร้อยพันล้านพารามิเตอร์นั้น คุณได้ฝังความรู้เกี่ยวกับบุคคลหนึ่งคนและกีฬาเฉพาะของเขาไว้แล้ว",
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "translatedText": "ฉันคิดว่าโดยทั่วไปแล้ว ใครก็ตามที่เคยเล่นกับโมเดลเหล่านี้มาก่อนจะรับรู้ได้อย่างชัดเจนว่าโมเดลเหล่านี้จดจำข้อเท็จจริงต่างๆ ไว้ได้เป็นจำนวนมาก",
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "translatedText": "คำถามที่สมเหตุสมผลที่คุณสามารถถามได้ก็คือ มันทำงานอย่างไรกันแน่?",
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "translatedText": "แล้วข้อเท็จจริงเหล่านั้นอยู่ที่ไหน?",
  "input": "And where do those facts live?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "translatedText": "ในเดือนธันวาคมที่ผ่านมา นักวิจัยบางคนจาก Google DeepMind ได้โพสต์เกี่ยวกับการทำงานเกี่ยวกับคำถามนี้ และพวกเขาใช้ตัวอย่างเฉพาะของการจับคู่นักกีฬากับกีฬาของพวกเขา",
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "translatedText": "แม้ว่าความเข้าใจเชิงกลไกอย่างสมบูรณ์เกี่ยวกับวิธีการจัดเก็บข้อเท็จจริงยังคงไม่มีคำตอบ แต่ก็มีผลลัพธ์บางส่วนที่น่าสนใจ รวมถึงข้อสรุปทั่วไปในระดับสูงที่ว่าข้อเท็จจริงดูเหมือนจะมีชีวิตอยู่ภายในส่วนที่เฉพาะเจาะจงของเครือข่ายเหล่านี้ ซึ่งเรียกกันอย่างแปลกประหลาดว่าเพอร์เซพตรอนหลายชั้นหรือเรียกสั้นๆ ว่า MLP",
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "translatedText": "ในสองสามบทที่ผ่านมา คุณและฉันได้เจาะลึกถึงรายละเอียดเบื้องหลังของทรานส์ฟอร์เมอร์ สถาปัตยกรรมที่เป็นพื้นฐานของโมเดลภาษาขนาดใหญ่ และยังรวมถึงพื้นฐานของ AI สมัยใหม่อื่นๆ อีกมากมาย",
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "translatedText": "ในบทล่าสุดนี้ เรามุ่งเน้นไปที่ชิ้นงานที่เรียกว่า Attention",
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "translatedText": "ขั้นตอนต่อไปสำหรับคุณและฉันคือการเจาะลึกรายละเอียดของสิ่งที่เกิดขึ้นภายในเพอร์เซพตรอนหลายชั้น ซึ่งเป็นส่วนประกอบขนาดใหญ่ของเครือข่าย",
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "translatedText": "การคำนวณที่นี่ค่อนข้างง่าย โดยเฉพาะเมื่อคุณเปรียบเทียบกับความสนใจ",
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "translatedText": "โดยพื้นฐานแล้ว มันจะสรุปลงเป็นการคูณเมทริกซ์คู่หนึ่งโดยมีอะไรบางอย่างง่ายๆ อยู่ระหว่างนั้น",
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "translatedText": "อย่างไรก็ตาม การตีความว่าการคำนวณเหล่านี้ทำอะไรอยู่ถือเป็นความท้าทายอย่างยิ่ง",
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "translatedText": "เป้าหมายหลักของเราที่นี่คือการก้าวผ่านการคำนวณและทำให้มันน่าจดจำ แต่ฉันอยากทำในบริบทของการแสดงตัวอย่างเฉพาะเจาะจงว่าบล็อกเหล่านี้สามารถจัดเก็บข้อเท็จจริงที่เป็นรูปธรรมได้อย่างไร อย่างน้อยก็ในหลักการ",
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "translatedText": "โดยเฉพาะอย่างยิ่งจะเป็นการจัดเก็บข้อเท็จจริงที่ว่าไมเคิล จอร์แดนเล่นบาสเก็ตบอล",
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "translatedText": "ฉันควรจะพูดถึงว่าเค้าโครงที่นี่ได้รับแรงบันดาลใจจากบทสนทนาที่ฉันมีกับนักวิจัย DeepMind คนหนึ่งชื่อ Neil Nanda",
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "translatedText": "โดยส่วนใหญ่แล้ว ฉันจะถือว่าคุณได้ดูสองบทสุดท้ายไปแล้ว หรือไม่ก็คุณน่าจะพอเข้าใจเบื้องต้นว่าทรานส์ฟอร์เมอร์คืออะไร แต่การทบทวนก็ไม่เสียหาย ดังนั้น ต่อไปนี้คือคำเตือนสั้นๆ เกี่ยวกับกระบวนการโดยรวม",
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "translatedText": "คุณและฉันได้ศึกษาโมเดลที่ได้รับการฝึกฝนให้รับข้อความและคาดการณ์สิ่งที่จะเกิดขึ้นต่อไป",
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "translatedText": "ข้อความอินพุตนั้นจะถูกแบ่งออกเป็นโทเค็นหลายกลุ่มก่อน ซึ่งหมายถึงชิ้นส่วนเล็กๆ ที่โดยทั่วไปแล้วจะเป็นคำหรือชิ้นส่วนคำเล็กๆ และแต่ละโทเค็นจะเชื่อมโยงกับเวกเตอร์มิติสูง ซึ่งก็คือรายการตัวเลขยาวๆ",
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "translatedText": "ลำดับของเวกเตอร์เหล่านี้จะส่งผ่านการดำเนินการสองประเภทซ้ำๆ กัน ได้แก่ การทำงานที่ต้องการความสนใจ ซึ่งทำให้เวกเตอร์สามารถส่งข้อมูลระหว่างกันได้ จากนั้นจึงผ่านเพอร์เซพตรอนหลายชั้น ซึ่งเป็นสิ่งที่เราจะเจาะลึกในวันนี้ และยังมีขั้นตอนการทำให้เป็นมาตรฐานอยู่ระหว่างนั้นด้วย",
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "translatedText": "หลังจากลำดับของเวกเตอร์ไหลผ่านการวนซ้ำที่แตกต่างกันหลายต่อหลายครั้งของทั้งสองบล็อกนี้ เมื่อสิ้นสุดกระบวนการ ความหวังก็คือว่า เวกเตอร์แต่ละตัวจะได้รับข้อมูลเพียงพอ ทั้งจากบริบท คำอื่นๆ ทั้งหมดในอินพุต และจากความรู้ทั่วไปที่รวมอยู่ในน้ำหนักของโมเดลผ่านการฝึกอบรม ซึ่งสามารถนำไปใช้ทำนายว่าโทเค็นใดที่จะมาต่อไปได้",
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "translatedText": "แนวคิดสำคัญประการหนึ่งที่ฉันอยากให้คุณคิดอยู่ในใจคือ เวกเตอร์ทั้งหมดเหล่านี้ต่างก็อาศัยอยู่ในปริภูมิที่มีมิติสูงมาก และเมื่อคุณคิดถึงปริภูมินั้น ทิศทางต่างๆ ก็สามารถเข้ารหัสความหมายที่แตกต่างกันได้",
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "translatedText": "ตัวอย่างคลาสสิกมากๆ ที่ฉันชอบอ้างอิงถึงก็คือ หากคุณดูการฝังตัวของผู้หญิง และลบการฝังตัวของผู้ชายออก แล้วคุณทำขั้นตอนเล็กๆ น้อยๆ นั้นและเพิ่มเข้ากับคำนามเพศชายอีกคำหนึ่ง เช่น ลุง คุณจะได้ตำแหน่งที่ใกล้เคียงกับคำนามเพศหญิงที่สอดคล้องกันมาก",
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "translatedText": "ในแง่นี้ ทิศทางเฉพาะนี้จะเข้ารหัสข้อมูลทางเพศ",
  "input": "In this sense, this particular direction encodes gender information.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "translatedText": "แนวคิดก็คือทิศทางที่แตกต่างกันอื่นๆ มากมายในพื้นที่มิติสูงสุดนี้อาจสอดคล้องกับคุณลักษณะอื่นๆ ที่แบบจำลองอาจต้องการแสดง",
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "translatedText": "ในหม้อแปลง เวกเตอร์เหล่านี้ไม่ได้เข้ารหัสความหมายของคำเดียวเท่านั้น",
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "translatedText": "ขณะที่ข้อมูลไหลผ่านเครือข่าย ข้อมูลจะซึมซับความหมายที่สมบูรณ์ยิ่งขึ้นโดยอิงจากบริบททั้งหมดรอบตัวข้อมูล และยังอิงจากความรู้ของโมเดลอีกด้วย",
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "translatedText": "ท้ายที่สุดแล้ว ทุกคนจำเป็นต้องเข้ารหัสบางสิ่งบางอย่างที่ไกลเกินกว่าความหมายของคำเดียว เนื่องจากต้องเพียงพอที่จะทำนายสิ่งที่จะเกิดขึ้นต่อไปได้",
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "translatedText": "เราได้เห็นแล้วว่าบล็อกความสนใจช่วยให้คุณรวมบริบทเข้าไปได้อย่างไร แต่พารามิเตอร์โมเดลส่วนใหญ่จะอยู่ภายในบล็อก MLP และสิ่งหนึ่งที่เราคิดว่าพารามิเตอร์เหล่านี้อาจทำอยู่ก็คือ พารามิเตอร์เหล่านี้ให้ความจุเพิ่มเติมสำหรับจัดเก็บข้อเท็จจริง",
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "translatedText": "เหมือนที่ผมพูดไป บทเรียนในที่นี้จะเน้นที่ตัวอย่างของเล่นที่เป็นรูปธรรม เพื่อแสดงให้เห็นว่าสามารถเก็บข้อเท็จจริงที่ว่าไมเคิล จอร์แดนเล่นบาสเก็ตบอลได้อย่างไร",
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "translatedText": "ตัวอย่างของเล่นนี้จะต้องให้คุณและฉันต้องตั้งสมมติฐานสองสามข้อเกี่ยวกับพื้นที่มิติสูงนี้",
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "translatedText": "ก่อนอื่น เราจะถือว่าทิศทางหนึ่งแสดงถึงแนวคิดเรื่องชื่อไมเคิล จากนั้นทิศทางเกือบตั้งฉากอีกทิศทางหนึ่งแสดงถึงแนวคิดเรื่องนามสกุลจอร์แดน และทิศทางที่สามจะแสดงถึงแนวคิดเรื่องบาสเก็ตบอล",
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "translatedText": "โดยเฉพาะอย่างยิ่ง สิ่งที่ฉันหมายถึงก็คือ หากคุณดูในเครือข่ายและดึงเวกเตอร์ตัวหนึ่งที่กำลังประมวลผลออกมา หากผลคูณจุดที่มีชื่อจริงของไมเคิล ไดเรกชั่นเป็น 1 นั่นคือสิ่งที่เวกเตอร์จะเข้ารหัสความคิดของบุคคลที่มีชื่อจริงนั้น",
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "translatedText": "มิฉะนั้น ผลคูณจุดจะเท่ากับศูนย์หรือเป็นลบ ซึ่งหมายความว่าเวกเตอร์ไม่ได้เรียงตามทิศทางนั้นจริงๆ",
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "translatedText": "เพื่อความเรียบง่าย เราลองละเลยคำถามที่สมเหตุสมผลว่าหากผลคูณจุดมีค่ามากกว่าหนึ่งอาจหมายถึงอะไร",
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "translatedText": "ในทำนองเดียวกัน ผลคูณจุดกับทิศทางอื่นๆ เหล่านี้ จะบอกคุณได้ว่าหมายถึงนามสกุลจอร์แดนหรือบาสเก็ตบอล",
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "translatedText": "สมมุติว่าเวกเตอร์หมายถึงชื่อเต็มของไมเคิล จอร์แดน ผลคูณจุดกับทิศทางทั้งสองนี้จะต้องเท่ากับ 1",
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "translatedText": "เนื่องจากข้อความ Michael Jordan ครอบคลุมโทเค็นสองแบบที่แตกต่างกัน นั่นหมายความว่าเราจะต้องถือว่าบล็อกความสนใจก่อนหน้านี้ได้ส่งข้อมูลไปยังเวกเตอร์ที่สองจากสองเวกเตอร์นี้สำเร็จ เพื่อให้แน่ใจว่าสามารถเข้ารหัสทั้งสองชื่อได้",
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "translatedText": "เมื่อพิจารณาจากสมมติฐานทั้งหมดแล้ว มาดูเนื้อหาหลักของบทเรียนกันเลย",
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "translatedText": "เกิดอะไรขึ้นภายในเพอร์เซพตรอนหลายชั้น?",
  "input": "What happens inside a multilayer perceptron?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "translatedText": "คุณอาจคิดถึงลำดับของเวกเตอร์ที่ไหลเข้าไปในบล็อก และจำไว้ว่าเวกเตอร์แต่ละตัวถูกเชื่อมโยงกับโทเค็นหนึ่งตัวจากข้อความอินพุตเดิมที",
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "translatedText": "สิ่งที่จะเกิดขึ้นก็คือเวกเตอร์แต่ละตัวจากลำดับนั้นจะต้องผ่านการดำเนินการชุดสั้นๆ ซึ่งเราจะแกะมันออกในช่วงเวลาสั้นๆ และในตอนท้าย เราจะได้เวกเตอร์อีกตัวที่มีมิติเดียวกัน",
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "translatedText": "เวกเตอร์อื่นจะถูกเพิ่มเข้ากับเวกเตอร์ดั้งเดิมที่ไหลเข้ามา และผลรวมนั้นก็คือผลลัพธ์ที่ไหลออกมา",
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "translatedText": "ลำดับการดำเนินการนี้เป็นสิ่งที่คุณใช้กับเวกเตอร์ทุกตัวในลำดับ ซึ่งเชื่อมโยงกับโทเค็นทุกตัวในอินพุต และทั้งหมดนี้จะเกิดขึ้นแบบคู่ขนาน",
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "translatedText": "โดยเฉพาะอย่างยิ่ง เวกเตอร์จะไม่พูดคุยกันในขั้นตอนนี้ พวกมันทุกตัวต่างก็ทำสิ่งของตัวเอง",
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "translatedText": "และสำหรับคุณและฉัน มันทำให้มันง่ายขึ้นมาก เพราะมันหมายความว่าถ้าเราเข้าใจว่าเกิดอะไรขึ้นกับเวกเตอร์หนึ่งตัวผ่านบล็อกนี้ เราก็จะเข้าใจได้อย่างมีประสิทธิภาพว่าเกิดอะไรขึ้นกับเวกเตอร์ทั้งหมดด้วย",
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "translatedText": "เมื่อผมบอกว่าบล็อกนี้จะเข้ารหัสความจริงที่ว่า Michael Jordan เล่นบาสเก็ตบอล ผมหมายถึงว่าถ้ามีเวกเตอร์ไหลที่เข้ารหัสชื่อจริง Michael และนามสกุล Jordan ลำดับการคำนวณนี้จะสร้างบางอย่างที่รวมทิศทางบาสเก็ตบอลด้วย ซึ่งจะเป็นสิ่งที่เพิ่มเข้าไปในเวกเตอร์ในตำแหน่งนั้น",
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "translatedText": "ขั้นตอนแรกของกระบวนการนี้ดูเหมือนว่าการคูณเวกเตอร์นั้นด้วยเมทริกซ์ขนาดใหญ่",
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "translatedText": "ไม่น่าแปลกใจเลย นี่คือการเรียนรู้ที่ลึกซึ้ง",
  "input": "No surprises there, this is deep learning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "translatedText": "และเมทริกซ์นี้ เช่นเดียวกับเมทริกซ์อื่นๆ ทั้งหมดที่เราเคยเห็น เต็มไปด้วยพารามิเตอร์โมเดลที่เรียนรู้มาจากข้อมูล ซึ่งคุณอาจคิดว่าเป็นปุ่มและหน้าปัดจำนวนหนึ่งที่ปรับเปลี่ยนและปรับแต่งเพื่อกำหนดพฤติกรรมของโมเดล",
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "translatedText": "วิธีที่ดีวิธีหนึ่งในการคิดเกี่ยวกับการคูณเมทริกซ์ก็คือ ลองนึกภาพว่าแต่ละแถวของเมทริกซ์นั้นเป็นเวกเตอร์ของตัวเอง แล้วนำผลคูณจุดจำนวนหนึ่งระหว่างแถวเหล่านั้นกับเวกเตอร์ที่กำลังประมวลผล ซึ่งฉันจะเรียกว่า E เพื่อการฝัง",
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "translatedText": "ตัวอย่างเช่น สมมติว่าแถวแรกสุดบังเอิญตรงกับชื่อไมเคิลที่เราสันนิษฐานว่ามีอยู่จริง",
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "translatedText": "นั่นหมายความว่าส่วนประกอบแรกในเอาต์พุตนี้ ผลคูณจุดนี้ จะเป็น 1 หากเวกเตอร์นั้นเข้ารหัสชื่อจริง ไมเคิล และเป็นศูนย์หรือค่าลบในกรณีอื่น",
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "translatedText": "สนุกยิ่งขึ้นไปอีก ใช้เวลาสักครู่คิดดูว่าจะหมายถึงอะไร หากแถวแรกเป็นชื่อไมเคิลบวกกับนามสกุลจอร์แดน",
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "translatedText": "เพื่อความเรียบง่าย ฉันขอเขียนเป็น M บวก J เลย",
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "translatedText": "จากนั้น การใช้ผลคูณจุดที่มีการฝังตัว E จะทำให้สิ่งต่างๆ กระจายตัวได้อย่างสวยงาม ดังนั้นจึงมีลักษณะเป็น M จุด E บวก J จุด E",
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "translatedText": "และสังเกตว่านั่นหมายความว่าค่าสูงสุดจะเป็น 2 หากเวกเตอร์เข้ารหัสชื่อเต็มของ Michael Jordan และหากไม่ใช่เช่นนั้น ค่าสูงสุดจะเป็น 1 หรือมีค่าที่น้อยกว่า 1",
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "translatedText": "นั่นเป็นเพียงหนึ่งแถวในเมทริกซ์นี้",
  "input": "And that's just one row in this matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "translatedText": "คุณอาจคิดว่าแถวอื่นๆ ทั้งหมดจะถามคำถามประเภทอื่นๆ ควบคู่กัน หรือเจาะลึกคุณลักษณะประเภทอื่นๆ ของเวกเตอร์ที่กำลังประมวลผลอยู่",
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "translatedText": "บ่อยครั้งขั้นตอนนี้ยังเกี่ยวข้องกับการเพิ่มเวกเตอร์อีกตัวหนึ่งลงในเอาต์พุต ซึ่งเต็มไปด้วยพารามิเตอร์โมเดลที่เรียนรู้จากข้อมูล",
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "translatedText": "เวกเตอร์อีกตัวหนึ่งนี้เรียกว่า อคติ",
  "input": "This other vector is known as the bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "translatedText": "สำหรับตัวอย่างของเรา ฉันต้องการให้คุณลองนึกภาพว่าค่าอคติในส่วนประกอบแรกสุดนี้เป็นค่าลบหนึ่ง ซึ่งหมายความว่าผลลัพธ์สุดท้ายของเราดูเหมือนผลคูณจุดที่เกี่ยวข้อง แต่เป็นค่าลบหนึ่ง",
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "translatedText": "คุณอาจถามได้อย่างสมเหตุสมผลว่าทำไมฉันต้องการให้คุณสันนิษฐานว่าโมเดลได้เรียนรู้สิ่งนี้ และในช่วงเวลาสั้นๆ คุณจะเห็นว่าทำไมมันจึงสะอาดและดีมากถ้าเรามีค่าที่เป็นบวกก็ต่อเมื่อเวกเตอร์เข้ารหัสชื่อเต็มของ Michael Jordan และในกรณีอื่นๆ จะเป็นศูนย์หรือเป็นลบ",
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "translatedText": "จำนวนแถวทั้งหมดในเมทริกซ์นี้ ซึ่งก็เหมือนกับจำนวนคำถามที่ถูกถาม ในกรณีของ GPT-3 ซึ่งเราได้ติดตามตัวเลขอยู่นั้น อยู่ที่ต่ำกว่า 50,000 เล็กน้อย",
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "translatedText": "ในความเป็นจริง มันเป็นสี่เท่าของจำนวนมิติในพื้นที่ฝังตัวนี้พอดี",
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "translatedText": "นั่นคือทางเลือกการออกแบบ",
  "input": "That's a design choice.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "translatedText": "คุณสามารถทำให้มันมากขึ้นหรือน้อยลงก็ได้ แต่การมีหลายตัวที่สะอาดมักจะส่งผลดีต่อฮาร์ดแวร์",
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "translatedText": "เนื่องจากเมทริกซ์ที่เต็มไปด้วยน้ำหนักนี้จะนำเราไปสู่พื้นที่มิติที่สูงกว่า ดังนั้น ผมจึงจะใช้คำย่อว่า W ขึ้นมา",
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "translatedText": "ฉันจะดำเนินการติดป้ายกำกับเวกเตอร์ที่เรากำลังประมวลผลเป็น E ต่อไป จากนั้นเราจะติดป้ายกำกับเวกเตอร์อคตินี้เป็น B ในแนวดิ่งและวางกลับลงในไดอะแกรม",
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "translatedText": "ณ จุดนี้ ปัญหาคือการดำเนินการนี้เป็นเชิงเส้นอย่างแท้จริง แต่ภาษาเป็นกระบวนการที่ไม่เป็นเชิงเส้นอย่างมาก",
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "translatedText": "หากรายการที่เรากำลังวัดนั้นสูงสำหรับ Michael และ Jordan มันก็จำเป็นต้องถูกกระตุ้นโดย Michael และ Phelps และ Alexis และ Jordan ด้วยเช่นกัน แม้ว่าทั้งสองอย่างจะไม่มีความเกี่ยวข้องกันในเชิงแนวคิดก็ตาม",
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "translatedText": "สิ่งที่คุณต้องการจริงๆ คือคำตอบง่ายๆ ว่าใช่หรือไม่สำหรับชื่อเต็ม",
  "input": "What you really want is a simple yes or no for the full name.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "translatedText": "ดังนั้นขั้นตอนต่อไปคือการส่งเวกเตอร์กลางขนาดใหญ่ผ่านฟังก์ชันไม่เชิงเส้นที่เรียบง่ายมาก",
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "translatedText": "ทางเลือกทั่วไปคือทางเลือกที่นำค่าลบทั้งหมดมาแมปให้เป็นศูนย์และปล่อยให้ค่าบวกทั้งหมดไม่เปลี่ยนแปลง",
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "translatedText": "และยังคงใช้ประเพณีการเรียนรู้แบบเจาะลึกที่มีชื่อที่หรูหราเกินจริง ฟังก์ชันที่เรียบง่ายมากนี้มักถูกเรียกว่าหน่วยเชิงเส้นแก้ไขหรือเรียกสั้นๆ ว่า ReLU",
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "translatedText": "กราฟมีลักษณะดังนี้",
  "input": "Here's what the graph looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "translatedText": "ดังนั้น การใช้ตัวอย่างที่เราจินตนาการขึ้น โดยที่ค่ารายการแรกของเวกเตอร์กลางมีค่าเป็นหนึ่ง ก็ต่อเมื่อชื่อเต็มของเขาคือ Michael Jordan และเป็นศูนย์หรือค่าลบในกรณีอื่น หลังจากที่คุณส่งค่าผ่าน ReLU แล้ว คุณจะได้ค่าที่สะอาดมากซึ่งค่าศูนย์และค่าลบทั้งหมดจะถูกตัดให้เป็นศูนย์",
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "translatedText": "ดังนั้นผลลัพธ์นี้จะต้องเป็นหนึ่งสำหรับชื่อเต็มของไมเคิล จอร์แดน และเป็นศูนย์ในกรณีอื่น",
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "translatedText": "กล่าวอีกนัยหนึ่ง มันเลียนแบบพฤติกรรมของเกต AND โดยตรง",
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "translatedText": "บ่อยครั้งที่โมเดลต่างๆ จะใช้ฟังก์ชั่นที่ดัดแปลงเล็กน้อย เรียกว่า JLU ซึ่งมีรูปทรงพื้นฐานเหมือนกัน เพียงแต่มีความนุ่มนวลกว่าเล็กน้อย",
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "translatedText": "แต่สำหรับวัตถุประสงค์ของเรา มันคงจะสะอาดกว่าเล็กน้อยหากเราคิดถึง ReLU เท่านั้น",
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "translatedText": "นอกจากนี้ เมื่อคุณได้ยินผู้คนพูดถึงเซลล์ประสาทของหม้อแปลง พวกเขากำลังพูดถึงค่าเหล่านี้อยู่",
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "translatedText": "เมื่อใดก็ตามที่คุณเห็นภาพเครือข่ายประสาทเทียมทั่วไปที่มีชั้นของจุดและเส้นจำนวนหนึ่งเชื่อมต่อไปยังชั้นก่อนหน้า ซึ่งเรามีไว้ก่อนหน้านี้ในซีรีส์นี้ โดยทั่วไปแล้วจะหมายถึงการถ่ายทอดการรวมกันของขั้นตอนเชิงเส้น การคูณเมทริกซ์ ตามด้วยฟังก์ชันไม่เชิงเส้นแบบจำเพาะเทอม เช่น ReLU",
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "translatedText": "คุณจะบอกได้ว่านิวรอนนี้ทำงานอยู่เมื่อค่านี้เป็นบวก และจะไม่ทำงานหากค่าเป็นศูนย์",
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "translatedText": "ขั้นตอนต่อไปจะดูคล้ายกับขั้นตอนแรกมาก",
  "input": "The next step looks very similar to the first one.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "translatedText": "คุณคูณด้วยเมทริกซ์ที่มีขนาดใหญ่มากและคุณเพิ่มเงื่อนไขอคติบางอย่าง",
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "translatedText": "ในกรณีนี้ จำนวนมิติในเอาต์พุตจะลดลงเหลือเท่ากับขนาดของพื้นที่ฝังตัว ดังนั้น ฉันจะเรียกสิ่งนี้ว่าเมทริกซ์การฉายลง",
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "translatedText": "และในครั้งนี้ แทนที่จะคิดแบบแถวต่อแถว จริงๆ แล้วจะดีกว่าหากคิดแบบคอลัมน์ต่อคอลัมน์",
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "translatedText": "คุณจะเห็นว่ามีอีกวิธีหนึ่งในการจดจำการคูณเมทริกซ์ในหัวได้ นั่นคือลองจินตนาการถึงการเอาแต่ละคอลัมน์ของเมทริกซ์มาคูณด้วยเทอมที่สอดคล้องกันในเวกเตอร์ที่กำลังประมวลผล จากนั้นจึงบวกคอลัมน์ที่ปรับขนาดใหม่ทั้งหมดเข้าด้วยกัน",
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "translatedText": "เหตุผลที่ดีกว่าที่จะคิดในลักษณะนี้ก็คือ ในกรณีนี้ คอลัมน์จะมีมิติเดียวกันกับพื้นที่ฝัง ดังนั้นเราจึงคิดถึงคอลัมน์เหล่านี้เป็นทิศทางในพื้นที่นั้นได้",
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "translatedText": "ตัวอย่างเช่น เราจะลองจินตนาการว่าโมเดลได้เรียนรู้ที่จะสร้างคอลัมน์แรกในทิศทางบาสเก็ตบอลที่เราคิดว่ามีอยู่",
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "translatedText": "ซึ่งหมายความว่าเมื่อนิวรอนที่เกี่ยวข้องในตำแหน่งแรกทำงานอยู่ เราจะเพิ่มคอลัมน์นี้ลงในผลลัพธ์สุดท้าย",
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "translatedText": "แต่หากนิวรอนนั้นไม่ได้ทำงาน หากตัวเลขนั้นเป็นศูนย์ การกระทำดังกล่าวจะไม่มีผลใดๆ",
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "translatedText": "และมันไม่จำเป็นต้องเป็นแค่บาสเก็ตบอลเท่านั้น",
  "input": "And it doesn't just have to be basketball.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "translatedText": "นอกจากนี้ โมเดลดังกล่าวยังสามารถนำไปใช้ในคอลัมน์นี้และฟีเจอร์อื่นๆ มากมายที่ต้องการเชื่อมโยงกับบางสิ่งที่มีชื่อเต็มว่า Michael Jordan",
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "translatedText": "ในเวลาเดียวกัน คอลัมน์อื่นๆ ทั้งหมดในเมทริกซ์นี้จะบอกคุณว่าจะมีการเพิ่มอะไรลงในผลลัพธ์สุดท้าย หากนิวรอนที่เกี่ยวข้องทำงานอยู่",
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "translatedText": "และหากคุณมีอคติในกรณีนี้ มันเป็นสิ่งที่คุณเพียงแค่เพิ่มเข้าไปทุกครั้ง โดยไม่คำนึงถึงค่าของเซลล์ประสาท",
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "translatedText": "คุณอาจสงสัยว่านั่นทำอะไรอยู่",
  "input": "You might wonder what's that doing.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "translatedText": "เช่นเดียวกับวัตถุที่เติมพารามิเตอร์ทั้งหมดที่นี่ มันเป็นเรื่องยากที่จะพูดได้ชัดเจน",
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "translatedText": "บางทีเครือข่ายอาจจำเป็นต้องทำบัญชีบางอย่าง แต่คุณสามารถละเลยไปก่อนได้",
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "translatedText": "เพื่อให้สัญลักษณ์ของเรากระชับขึ้นอีกเล็กน้อย ฉันจะเรียกเมทริกซ์ขนาดใหญ่ W ลง และเรียกเวกเตอร์อคติ B ลงเช่นเดียวกัน และใส่กลับเข้าไปในไดอะแกรมของเรา",
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "translatedText": "เช่นเดียวกับที่ฉันดูตัวอย่างไว้ก่อนหน้านี้ สิ่งที่คุณต้องทำกับผลลัพธ์สุดท้ายนี้คือการเพิ่มผลลัพธ์นั้นลงในเวกเตอร์ที่ไหลเข้าไปในบล็อกที่ตำแหน่งนั้น แล้วคุณก็จะได้ผลลัพธ์สุดท้ายนี้",
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "translatedText": "ตัวอย่างเช่น หากเวกเตอร์ที่ไหลเข้ามาเข้ารหัสทั้งชื่อจริง ไมเคิล และนามสกุล จอร์แดน ดังนั้น เนื่องจากลำดับการดำเนินการนี้จะทริกเกอร์เกต AND และจะเพิ่มทิศทางของบาสเก็ตบอล ดังนั้น สิ่งที่ออกมาจะเข้ารหัสทั้งหมดเหล่านั้นร่วมกัน",
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "translatedText": "และจำไว้ว่านี่คือกระบวนการที่เกิดขึ้นกับเวกเตอร์แต่ละตัวแบบคู่ขนาน",
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "translatedText": "โดยเฉพาะอย่างยิ่ง เมื่อใช้ตัวเลข GPT-3 นั่นหมายความว่าบล็อกนี้ไม่ได้มีเพียงนิวรอน 50,000 ตัวเท่านั้น แต่ยังมีโทเค็นในอินพุตมากกว่า 50,000 เท่าอีกด้วย",
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "translatedText": "นั่นคือการดำเนินการทั้งหมด ผลิตภัณฑ์เมทริกซ์สองตัว โดยแต่ละตัวมีการเพิ่มค่าอคติ และมีฟังก์ชันการตัดแบบง่ายๆ อยู่ระหว่างนั้น",
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "translatedText": "ใครก็ตามที่เคยดูวิดีโอก่อนหน้านี้ของซีรีส์นี้คงจะจำโครงสร้างนี้ได้ดี เนื่องจากเป็นเครือข่ายประสาทเทียมขั้นพื้นฐานที่สุดที่เราศึกษากันมา",
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "translatedText": "ในตัวอย่างนั้น มันได้รับการฝึกให้จดจำตัวเลขที่เขียนด้วยลายมือ",
  "input": "In that example, it was trained to recognize handwritten digits.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "translatedText": "ที่นี่ ในบริบทของตัวแปลงสำหรับโมเดลภาษาขนาดใหญ่ นี่คือชิ้นหนึ่งในสถาปัตยกรรมที่ใหญ่กว่า และความพยายามใดๆ ที่จะตีความว่าสิ่งนี้กำลังทำอะไรอยู่กันแน่มีความเกี่ยวพันอย่างมากกับแนวคิดในการเข้ารหัสข้อมูลในเวกเตอร์ของพื้นที่ฝังตัวมิติสูง",
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "translatedText": "นั่นคือบทเรียนสำคัญ แต่ฉันต้องการที่จะถอยกลับมาและไตร่ตรองเกี่ยวกับสองสิ่งที่แตกต่างกัน สิ่งแรกคือการทำบัญชี และสิ่งที่สองเกี่ยวข้องกับข้อเท็จจริงที่ชวนคิดมากเกี่ยวกับมิติที่สูงกว่าซึ่งจริงๆ แล้วฉันไม่รู้มาก่อนจนกระทั่งฉันได้ศึกษาเรื่องทรานส์ฟอร์เมอร์",
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "translatedText": "ในสองบทสุดท้าย คุณและฉันเริ่มนับจำนวนพารามิเตอร์ทั้งหมดใน GPT-3 และดูว่ามันอยู่ที่ไหนแน่ชัด ดังนั้นมาจบเกมกันที่นี่เลยดีกว่า",
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "translatedText": "ฉันกล่าวถึงไปแล้วว่าเมทริกซ์การฉายขึ้นด้านบนมีแถวประมาณ 50,000 แถว และแต่ละแถวมีขนาดตรงกับพื้นที่ฝัง ซึ่งสำหรับ GPT-3 คือ 12,288",
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "translatedText": "เมื่อคูณค่าเหล่านั้นเข้าด้วยกัน เราจะได้พารามิเตอร์ 604 ล้านตัวสำหรับเมทริกซ์นั้นเพียงอย่างเดียว และการฉายลงจะมีจำนวนพารามิเตอร์เท่ากัน เพียงแต่มีรูปร่างที่สลับตำแหน่งกัน",
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "translatedText": "ดังนั้นเมื่อรวมกันแล้วจะได้พารามิเตอร์ประมาณ 1.2 พันล้านตัว",
  "input": "So together, they give about 1.2 billion parameters.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "translatedText": "เวกเตอร์อคติยังคำนึงถึงพารามิเตอร์อีกสองสามตัว แต่เป็นสัดส่วนที่เล็กน้อยเมื่อเทียบกับทั้งหมด ดังนั้น ฉันจะไม่แสดงมันด้วยซ้ำ",
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "translatedText": "ใน GPT-3 ลำดับของเวกเตอร์ฝังตัวนี้ไม่ได้ไหลผ่านเพียง 1 แต่ไหลผ่านถึง 96 MLP ที่แตกต่างกัน ดังนั้น จำนวนพารามิเตอร์ทั้งหมดที่เกี่ยวข้องกับบล็อกทั้งหมดเหล่านี้จึงรวมกันได้ประมาณ 116 พันล้านรายการ",
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "translatedText": "ซึ่งเป็นประมาณ 2 ใน 3 ของพารามิเตอร์ทั้งหมดในเครือข่าย และเมื่อคุณเพิ่มทุกอย่างที่เรามีก่อนหน้านี้ สำหรับบล็อกความสนใจ การฝัง และการยกเลิกการฝัง คุณจะได้รับยอดรวม 175 พันล้านตามที่โฆษณาไว้",
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "translatedText": "คงจะคุ้มค่าที่จะกล่าวถึงว่ามีชุดพารามิเตอร์อีกชุดหนึ่งที่เกี่ยวข้องกับขั้นตอนการทำให้เป็นมาตรฐานซึ่งคำอธิบายนี้ได้ละเว้นไป แต่เช่นเดียวกับเวกเตอร์อคติ พารามิเตอร์เหล่านี้คิดเป็นสัดส่วนที่เล็กน้อยมากของทั้งหมด",
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "translatedText": "สำหรับประเด็นที่สองนี้ คุณอาจสงสัยว่าตัวอย่างของเล่นหลักที่เราใช้เวลามากมายในการเรียนรู้นี้ สะท้อนให้เห็นว่าข้อเท็จจริงถูกจัดเก็บในโมเดลภาษาขนาดใหญ่จริงหรือไม่",
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "translatedText": "เป็นเรื่องจริงที่แถวของเมทริกซ์แรกนั้นสามารถคิดได้ว่าเป็นทิศทางในพื้นที่ฝังตัวนี้ และนั่นหมายถึงการเปิดใช้งานนิวรอนแต่ละตัวจะบอกคุณว่าเวกเตอร์ที่กำหนดนั้นสอดคล้องกับทิศทางเฉพาะเจาะจงมากเพียงใด",
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "translatedText": "จริงอยู่ที่คอลัมน์ของเมทริกซ์ที่สองจะบอกคุณว่าจะมีการเพิ่มอะไรลงในผลลัพธ์หากนิวรอนนั้นทำงานอยู่",
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "translatedText": "นั่นทั้งสองเป็นเพียงข้อเท็จจริงทางคณิตศาสตร์",
  "input": "Both of those are just mathematical facts.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "translatedText": "อย่างไรก็ตาม หลักฐานชี้ให้เห็นว่าเซลล์ประสาทแต่ละเซลล์แทบจะไม่แสดงลักษณะที่ชัดเจนแบบเดียวกับที่ไมเคิล จอร์แดนทำเลย และอาจมีเหตุผลที่ดีมากที่เป็นเช่นนั้น ซึ่งเกี่ยวข้องกับแนวคิดที่นักวิจัยด้านการตีความในปัจจุบันนิยมใช้ ซึ่งเรียกว่า การซ้อนทับ",
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "translatedText": "นี่คือสมมติฐานที่อาจช่วยอธิบายได้ว่าเหตุใดแบบจำลองเหล่านี้จึงตีความได้ยากเป็นพิเศษ และเหตุใดแบบจำลองเหล่านี้จึงปรับขนาดได้ดีอย่างน่าประหลาดใจ",
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "translatedText": "แนวคิดพื้นฐานก็คือ ถ้าคุณมีปริภูมิ n มิติ และคุณต้องการแสดงคุณลักษณะต่าง ๆ มากมายโดยใช้ทิศทางที่ตั้งฉากกันในปริภูมินั้น คุณรู้ไหมว่า ด้วยวิธีนั้น หากคุณเพิ่มส่วนประกอบในทิศทางหนึ่ง ส่วนประกอบนั้นจะไม่ส่งผลต่อทิศทางอื่น ๆ ดังนั้น จำนวนเวกเตอร์สูงสุดที่คุณใส่ได้คือ n ซึ่งเป็นจำนวนมิติเท่านั้น",
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "translatedText": "สำหรับนักคณิตศาสตร์ จริงๆ แล้วนี่คือคำจำกัดความของมิติ",
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "translatedText": "แต่สิ่งที่น่าสนใจคือหากคุณผ่อนคลายข้อจำกัดนั้นลงสักหน่อยและยอมทนกับเสียงบ้าง",
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "translatedText": "สมมติว่าคุณยอมให้คุณลักษณะเหล่านั้นแสดงด้วยเวกเตอร์ที่ไม่ได้ตั้งฉากกันพอดี แต่เกือบจะตั้งฉากกัน โดยมีระยะห่างกันประมาณ 89 ถึง 91 องศา",
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "translatedText": "หากเราอยู่ในสองหรือสามมิติ มันก็ไม่มีความแตกต่างกัน",
  "input": "If we were in two or three dimensions, this makes no difference.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "translatedText": "นั่นทำให้คุณแทบไม่มีช่องว่างเพิ่มขึ้นเลยเพื่อใส่เวกเตอร์เพิ่มเติมเข้าไป ซึ่งทำให้ขัดกับสัญชาตญาณมากยิ่งขึ้นว่าสำหรับมิติที่ใหญ่กว่า คำตอบจะเปลี่ยนไปอย่างมาก",
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "translatedText": "ฉันสามารถให้ภาพประกอบที่รวดเร็วและหยาบๆ แก่คุณได้โดยใช้ Python เพื่อสร้างรายการเวกเตอร์ 100 มิติ โดยที่แต่ละรายการจะถูกกำหนดค่าเริ่มต้นแบบสุ่ม และรายการนี้จะมีเวกเตอร์ที่แตกต่างกัน 10,000 รายการ ดังนั้นจึงเป็นเวกเตอร์ 100 เท่าของจำนวนมิติ",
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "translatedText": "กราฟนี้แสดงการกระจายตัวของมุมระหว่างคู่เวกเตอร์เหล่านี้",
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "translatedText": "เนื่องจากเริ่มต้นแบบสุ่ม มุมต่างๆ เหล่านี้จึงสามารถเป็นอะไรก็ได้ตั้งแต่ 0 ถึง 180 องศา แต่คุณจะสังเกตเห็นได้ว่า แม้แต่สำหรับเวกเตอร์สุ่ม ก็มีความลำเอียงอย่างมากที่จะให้สิ่งต่างๆ อยู่ใกล้กับ 90 องศามากขึ้น",
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "translatedText": "จากนั้นสิ่งที่ฉันจะทำคือรันกระบวนการเพิ่มประสิทธิภาพบางอย่างที่กระตุ้นเวกเตอร์ทั้งหมดเหล่านี้ซ้ำๆ เพื่อให้พยายามตั้งฉากกันมากยิ่งขึ้น",
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "translatedText": "หลังจากทำซ้ำหลายๆ ครั้ง การกระจายของมุมจะมีลักษณะดังต่อไปนี้",
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "translatedText": "เราต้องซูมเข้าไปที่นี่จริงๆ เพราะมุมที่เป็นไปได้ทั้งหมดระหว่างคู่เวกเตอร์จะอยู่ในช่วงแคบๆ ระหว่าง 89 ถึง 91 องศา",
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "translatedText": "โดยทั่วไป ผลที่ตามมาของสิ่งที่เรียกว่าเล็มมาจอห์นสัน-ลินเดนชตราส์ ก็คือ จำนวนเวกเตอร์ที่คุณสามารถยัดลงในช่องว่างที่เกือบจะตั้งฉากเช่นนี้จะเพิ่มขึ้นแบบเอ็กซ์โพเนนเชียลตามจำนวนมิติ",
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "translatedText": "สิ่งนี้มีความสำคัญมากสำหรับโมเดลภาษาขนาดใหญ่ ซึ่งอาจได้รับประโยชน์จากการเชื่อมโยงความคิดอิสระกับทิศทางที่เกือบตั้งฉากกัน",
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "translatedText": "หมายความว่ามันสามารถจัดเก็บไอเดียต่างๆ ได้มากกว่าขนาดในพื้นที่ที่จัดไว้ให้",
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "translatedText": "นี่อาจอธิบายได้บางส่วนว่าเหตุใดประสิทธิภาพของโมเดลจึงดูปรับขนาดได้ดีตามขนาด",
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "translatedText": "พื้นที่ที่มีขนาดมากขึ้น 10 เท่าสามารถจัดเก็บแนวคิดอิสระได้มากกว่า 10 เท่าเลยทีเดียว",
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "translatedText": "ซึ่งสิ่งนี้มีความเกี่ยวข้องไม่เพียงกับพื้นที่ฝังตัวที่เวกเตอร์ไหลผ่านโมเดลเท่านั้น แต่ยังเกี่ยวข้องกับเวกเตอร์ที่เต็มไปด้วยเซลล์ประสาทในตรงกลางของเพอร์เซพตรอนหลายชั้นที่เราเพิ่งศึกษาไปอีกด้วย",
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "translatedText": "กล่าวอีกนัยหนึ่ง ในขนาดของ GPT-3 มันอาจไม่เพียงแต่ตรวจสอบฟีเจอร์ 50,000 รายการเท่านั้น แต่หากมันใช้ประโยชน์จากความจุเพิ่มมหาศาลนี้โดยใช้ทิศทางที่เกือบจะตั้งฉากกับพื้นที่ มันก็จะตรวจสอบฟีเจอร์ต่างๆ ของเวกเตอร์ที่กำลังประมวลผลได้เพิ่มมากขึ้นอีกมากมาย",
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "translatedText": "แต่หากทำแบบนั้น แสดงว่าคุณลักษณะแต่ละอย่างจะไม่ปรากฏให้เห็นเมื่อเซลล์ประสาทเดี่ยวส่องสว่าง",
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "translatedText": "มันคงจะต้องดูเหมือนการรวมตัวของเซลล์ประสาทที่เฉพาะเจาะจงมากกว่า ซึ่งเป็นการซ้อนทับ",
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "translatedText": "สำหรับใครก็ตามที่อยากเรียนรู้เพิ่มเติม คำค้นหาที่เกี่ยวข้องที่สำคัญที่นี่คือ sparse autoencoder ซึ่งเป็นเครื่องมือที่นักตีความบางส่วนใช้เพื่อพยายามดึงเอาคุณสมบัติที่แท้จริงออกมา แม้ว่าจะซ้อนทับกันมากบนนิวรอนทั้งหมดเหล่านี้ก็ตาม",
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "translatedText": "ฉันจะลิงค์ไปยังโพสต์เกี่ยวกับมนุษยวิทยาที่ยอดเยี่ยมสองสามโพสต์เกี่ยวกับเรื่องนี้",
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "translatedText": "ถึงตอนนี้ เราไม่ได้พูดถึงทุกรายละเอียดของหม้อแปลง แต่คุณและฉันได้พูดถึงประเด็นที่สำคัญที่สุดแล้ว",
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "translatedText": "สิ่งสำคัญที่ฉันอยากจะพูดถึงในบทถัดไปคือกระบวนการฝึกอบรม",
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "translatedText": "คำตอบสั้นๆ เกี่ยวกับวิธีการทำงานของการฝึกอบรมก็คือ ทั้งหมดเป็นการแบ็กโพรพาเกชั่น และเราได้ครอบคลุมเรื่องการแบ็กโพรพาเกชั่นในบริบทที่แยกจากกันในบทก่อนหน้าในชุดบทความนี้",
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "translatedText": "แต่ยังมีเรื่องอื่น ๆ ที่ต้องพูดคุยกันอีก เช่น ฟังก์ชันต้นทุนเฉพาะที่ใช้สำหรับโมเดลภาษา แนวคิดในการปรับแต่งโดยใช้การเรียนรู้เชิงเสริมแรงด้วยข้อเสนอแนะของมนุษย์ และแนวคิดเรื่องกฎการปรับขนาด",
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "translatedText": "หมายเหตุสั้นๆ สำหรับผู้ติดตามที่กระตือรือร้นในหมู่พวกคุณ มีวิดีโอจำนวนหนึ่งที่ไม่เกี่ยวข้องกับการเรียนรู้ของเครื่องซึ่งฉันตื่นเต้นที่จะได้ดูอย่างตั้งใจก่อนที่จะเริ่มต้นบทต่อไป ดังนั้น อาจจะต้องใช้เวลาสักพัก แต่ฉันสัญญาว่าจะมาในเวลาที่เหมาะสม",
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "translatedText": "ขอบคุณ",
  "input": "Thank you.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]