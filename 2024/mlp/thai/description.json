[
 {
  "translatedText": "การแกะกล่องเพอร์เซพตรอนหลายชั้นในหม้อแปลง และวิธีการจัดเก็บข้อเท็จจริง",
  "input": "Unpacking the multilayer perceptrons in a transformer, and how they may store facts",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "แทนที่จะอ่านโฆษณาที่ได้รับการสนับสนุน บทเรียนเหล่านี้ได้รับเงินทุนโดยตรงจากผู้ชม: https://3b1b.co/support",
  "input": "Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "รูปแบบการสนับสนุนที่มีคุณค่าเท่าเทียมกันคือการแชร์วิดีโอ",
  "input": "An equally valuable form of support is to share the videos.",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "โพสต์ในฟอรั่ม AI Alignment จากนักวิจัย Deepmind ที่อ้างอิงในช่วงเริ่มต้นของวิดีโอ:",
  "input": "AI Alignment forum post from the Deepmind researchers referenced at the video's start:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall",
  "input": "https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "โพสต์เกี่ยวกับ Anthropic เกี่ยวกับการซ้อนทับที่อ้างอิงใกล้จะจบ:",
  "input": "Anthropic posts about superposition referenced near the end:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://transformer-circuits.pub/2022/toy_model/index.html",
  "input": "https://transformer-circuits.pub/2022/toy_model/index.html",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "https://transformer-circuits.pub/2023/monosemantic-features",
  "input": "https://transformer-circuits.pub/2023/monosemantic-features",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "ส่วนต่างๆ:",
  "input": "Sections:",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "0:00 - ข้อเท็จจริงใน LLM อยู่ที่ไหน",
  "input": "0:00 - Where facts in LLMs live",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "2:15 - ทบทวนเรื่องหม้อแปลงแบบรวดเร็ว",
  "input": "2:15 - Quick refresher on transformers",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "4:39 - สมมติฐานสำหรับตัวอย่างของเล่นของเรา",
  "input": "4:39 - Assumptions for our toy example",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "6:07 - ภายในเพอร์เซ็ปตรอนหลายชั้น",
  "input": "6:07 - Inside a multilayer perceptron",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "15:38 - การนับพารามิเตอร์",
  "input": "15:38 - Counting parameters",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "17:04 - การซ้อนทับ",
  "input": "17:04 - Superposition",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "21:37 - ต่อไป",
  "input": "21:37 - Up next",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 },
 {
  "translatedText": "",
  "input": "",
  "model": "google_nmt",
  "n_reviews": 0
 }
]