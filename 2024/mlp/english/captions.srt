1
00:00:00,000 --> 00:00:05,038
If you feed a large language model the phrase, Michael Jordan plays the sport of blank, 

2
00:00:05,038 --> 00:00:09,560
and you have it predict what comes next, and it correctly predicts basketball, 

3
00:00:09,560 --> 00:00:14,255
this would suggest that somewhere, inside its hundreds of billions of parameters, 

4
00:00:14,255 --> 00:00:18,320
it's baked in knowledge about a specific person and his specific sport.

5
00:00:18,940 --> 00:00:22,146
And I think in general, anyone who's played around with one of these 

6
00:00:22,146 --> 00:00:25,400
models has the clear sense that it's memorized tons and tons of facts.

7
00:00:25,700 --> 00:00:29,160
So a reasonable question you could ask is, how exactly does that work?

8
00:00:29,160 --> 00:00:31,040
And where do those facts live?

9
00:00:35,720 --> 00:00:40,385
Last December, a few researchers from Google DeepMind posted about work on this question, 

10
00:00:40,385 --> 00:00:44,480
and they were using this specific example of matching athletes to their sports.

11
00:00:44,900 --> 00:00:49,824
And although a full mechanistic understanding of how facts are stored remains unsolved, 

12
00:00:49,824 --> 00:00:54,357
they had some interesting partial results, including the very general high-level 

13
00:00:54,357 --> 00:00:58,890
conclusion that the facts seem to live inside a specific part of these networks, 

14
00:00:58,890 --> 00:01:02,640
known fancifully as the multi-layer perceptrons, or MLPs for short.

15
00:01:03,120 --> 00:01:06,262
In the last couple of chapters, you and I have been digging into 

16
00:01:06,262 --> 00:01:10,324
the details behind transformers, the architecture underlying large language models, 

17
00:01:10,324 --> 00:01:12,500
and also underlying a lot of other modern AI.

18
00:01:13,060 --> 00:01:16,200
In the most recent chapter, we were focusing on a piece called Attention.

19
00:01:16,840 --> 00:01:20,964
And the next step for you and me is to dig into the details of what happens inside 

20
00:01:20,964 --> 00:01:25,040
these multi-layer perceptrons, which make up the other big portion of the network.

21
00:01:25,680 --> 00:01:28,074
The computation here is actually relatively simple, 

22
00:01:28,074 --> 00:01:30,100
especially when you compare it to attention.

23
00:01:30,560 --> 00:01:32,656
It boils down essentially to a pair of matrix 

24
00:01:32,656 --> 00:01:34,980
multiplications with a simple something in between.

25
00:01:35,720 --> 00:01:40,460
However, interpreting what these computations are doing is exceedingly challenging.

26
00:01:41,560 --> 00:01:45,666
Our main goal here is to step through the computations and make them memorable, 

27
00:01:45,666 --> 00:01:49,464
but I'd like to do it in the context of showing a specific example of how 

28
00:01:49,464 --> 00:01:53,160
one of these blocks could, at least in principle, store a concrete fact.

29
00:01:53,580 --> 00:01:57,080
Specifically, it'll be storing the fact that Michael Jordan plays basketball.

30
00:01:58,080 --> 00:02:00,768
I should mention the layout here is inspired by a conversation 

31
00:02:00,768 --> 00:02:03,200
I had with one of those DeepMind researchers, Neil Nanda.

32
00:02:04,060 --> 00:02:08,038
For the most part, I will assume that you've either watched the last two chapters, 

33
00:02:08,038 --> 00:02:11,057
or otherwise you have a basic sense for what a transformer is, 

34
00:02:11,057 --> 00:02:14,700
but refreshers never hurt, so here's the quick reminder of the overall flow.

35
00:02:15,340 --> 00:02:18,246
You and I have been studying a model that's trained 

36
00:02:18,246 --> 00:02:21,320
to take in a piece of text and predict what comes next.

37
00:02:21,720 --> 00:02:24,965
That input text is first broken into a bunch of tokens, 

38
00:02:24,965 --> 00:02:29,485
which means little chunks that are typically words or little pieces of words, 

39
00:02:29,485 --> 00:02:33,020
and each token is associated with a high-dimensional vector, 

40
00:02:33,020 --> 00:02:35,280
which is to say a long list of numbers.

41
00:02:35,840 --> 00:02:40,318
This sequence of vectors then repeatedly passes through two kinds of operation, 

42
00:02:40,318 --> 00:02:44,629
attention, which allows the vectors to pass information between one another, 

43
00:02:44,629 --> 00:02:49,108
and then the multilayer perceptrons, the thing that we're gonna dig into today, 

44
00:02:49,108 --> 00:02:52,300
and also there's a certain normalization step in between.

45
00:02:53,300 --> 00:02:56,431
After the sequence of vectors has flowed through many, 

46
00:02:56,431 --> 00:03:00,019
many different iterations of both of these blocks, by the end, 

47
00:03:00,019 --> 00:03:04,916
the hope is that each vector has soaked up enough information, both from the context, 

48
00:03:04,916 --> 00:03:09,357
all of the other words in the input, and also from the general knowledge that 

49
00:03:09,357 --> 00:03:12,261
was baked into the model weights through training, 

50
00:03:12,261 --> 00:03:16,020
that it can be used to make a prediction of what token comes next.

51
00:03:16,860 --> 00:03:20,682
One of the key ideas that I want you to have in your mind is that all of 

52
00:03:20,682 --> 00:03:23,772
these vectors live in a very, very high-dimensional space, 

53
00:03:23,772 --> 00:03:27,909
and when you think about that space, different directions can encode different 

54
00:03:27,909 --> 00:03:28,800
kinds of meaning.

55
00:03:30,120 --> 00:03:34,096
So a very classic example that I like to refer back to is how if you look 

56
00:03:34,096 --> 00:03:37,374
at the embedding of woman and subtract the embedding of man, 

57
00:03:37,374 --> 00:03:41,242
and you take that little step and you add it to another masculine noun, 

58
00:03:41,242 --> 00:03:43,768
something like uncle, you land somewhere very, 

59
00:03:43,768 --> 00:03:46,240
very close to the corresponding feminine noun.

60
00:03:46,440 --> 00:03:50,880
In this sense, this particular direction encodes gender information.

61
00:03:51,640 --> 00:03:55,614
The idea is that many other distinct directions in this super high-dimensional 

62
00:03:55,614 --> 00:03:59,640
space could correspond to other features that the model might want to represent.

63
00:04:01,400 --> 00:04:06,180
In a transformer, these vectors don't merely encode the meaning of a single word, though.

64
00:04:06,680 --> 00:04:10,988
As they flow through the network, they imbibe a much richer meaning based 

65
00:04:10,988 --> 00:04:15,180
on all the context around them, and also based on the model's knowledge.

66
00:04:15,880 --> 00:04:18,506
Ultimately, each one needs to encode something far, 

67
00:04:18,506 --> 00:04:22,345
far beyond the meaning of a single word, since it needs to be sufficient to 

68
00:04:22,345 --> 00:04:23,760
predict what will come next.

69
00:04:24,560 --> 00:04:28,480
We've already seen how attention blocks let you incorporate context, 

70
00:04:28,480 --> 00:04:32,798
but a majority of the model parameters actually live inside the MLP blocks, 

71
00:04:32,798 --> 00:04:37,287
and one thought for what they might be doing is that they offer extra capacity 

72
00:04:37,287 --> 00:04:38,140
to store facts.

73
00:04:38,720 --> 00:04:42,345
Like I said, the lesson here is gonna center on the concrete toy example 

74
00:04:42,345 --> 00:04:46,120
of how exactly it could store the fact that Michael Jordan plays basketball.

75
00:04:47,120 --> 00:04:49,530
Now, this toy example is gonna require that you and I make 

76
00:04:49,530 --> 00:04:51,900
a couple of assumptions about that high-dimensional space.

77
00:04:52,360 --> 00:04:56,991
First, we'll suppose that one of the directions represents the idea of a first name 

78
00:04:56,991 --> 00:05:01,623
Michael, and then another nearly perpendicular direction represents the idea of the 

79
00:05:01,623 --> 00:05:06,420
last name Jordan, and then yet a third direction will represent the idea of basketball.

80
00:05:07,400 --> 00:05:11,121
So specifically, what I mean by this is if you look in the network and 

81
00:05:11,121 --> 00:05:13,742
you pluck out one of the vectors being processed, 

82
00:05:13,742 --> 00:05:17,202
if its dot product with this first name Michael direction is one, 

83
00:05:17,202 --> 00:05:20,872
that's what it would mean for the vector to be encoding the idea of a 

84
00:05:20,872 --> 00:05:22,340
person with that first name.

85
00:05:23,800 --> 00:05:26,143
Otherwise, that dot product would be zero or negative, 

86
00:05:26,143 --> 00:05:28,700
meaning the vector doesn't really align with that direction.

87
00:05:29,420 --> 00:05:32,217
And for simplicity, let's completely ignore the very reasonable 

88
00:05:32,217 --> 00:05:35,320
question of what it might mean if that dot product was bigger than one.

89
00:05:36,200 --> 00:05:39,831
Similarly, its dot product with these other directions would 

90
00:05:39,831 --> 00:05:43,760
tell you whether it represents the last name Jordan or basketball.

91
00:05:44,740 --> 00:05:48,791
So let's say a vector is meant to represent the full name, Michael Jordan, 

92
00:05:48,791 --> 00:05:52,680
then its dot product with both of these directions would have to be one.

93
00:05:53,480 --> 00:05:56,658
Since the text Michael Jordan spans two different tokens, 

94
00:05:56,658 --> 00:06:01,480
this would also mean we have to assume that an earlier attention block has successfully 

95
00:06:01,480 --> 00:06:05,973
passed information to the second of these two vectors so as to ensure that it can 

96
00:06:05,973 --> 00:06:06,960
encode both names.

97
00:06:07,940 --> 00:06:11,480
With all of those as the assumptions, let's now dive into the meat of the lesson.

98
00:06:11,880 --> 00:06:14,980
What happens inside a multilayer perceptron?

99
00:06:17,100 --> 00:06:21,366
You might think of this sequence of vectors flowing into the block, and remember, 

100
00:06:21,366 --> 00:06:25,580
each vector was originally associated with one of the tokens from the input text.

101
00:06:26,080 --> 00:06:29,442
What's gonna happen is that each individual vector from that sequence 

102
00:06:29,442 --> 00:06:33,237
goes through a short series of operations, we'll unpack them in just a moment, 

103
00:06:33,237 --> 00:06:36,360
and at the end, we'll get another vector with the same dimension.

104
00:06:36,880 --> 00:06:40,999
That other vector is gonna get added to the original one that flowed in, 

105
00:06:40,999 --> 00:06:43,200
and that sum is the result flowing out.

106
00:06:43,720 --> 00:06:47,946
This sequence of operations is something you apply to every vector in the sequence, 

107
00:06:47,946 --> 00:06:51,620
associated with every token in the input, and it all happens in parallel.

108
00:06:52,100 --> 00:06:54,605
In particular, the vectors don't talk to each other in this step, 

109
00:06:54,605 --> 00:06:56,200
they're all kind of doing their own thing.

110
00:06:56,720 --> 00:06:59,349
And for you and me, that actually makes it a lot simpler, 

111
00:06:59,349 --> 00:07:02,342
because it means if we understand what happens to just one of the 

112
00:07:02,342 --> 00:07:06,060
vectors through this block, we effectively understand what happens to all of them.

113
00:07:07,100 --> 00:07:11,380
When I say this block is gonna encode the fact that Michael Jordan plays basketball, 

114
00:07:11,380 --> 00:07:15,509
what I mean is that if a vector flows in that encodes first name Michael and last 

115
00:07:15,509 --> 00:07:19,789
name Jordan, then this sequence of computations will produce something that includes 

116
00:07:19,789 --> 00:07:24,020
that direction basketball, which is what will add on to the vector in that position.

117
00:07:25,600 --> 00:07:29,700
The first step of this process looks like multiplying that vector by a very big matrix.

118
00:07:30,040 --> 00:07:31,980
No surprises there, this is deep learning.

119
00:07:32,680 --> 00:07:35,235
And this matrix, like all of the other ones we've seen, 

120
00:07:35,235 --> 00:07:37,973
is filled with model parameters that are learned from data, 

121
00:07:37,973 --> 00:07:41,441
which you might think of as a bunch of knobs and dials that get tweaked and 

122
00:07:41,441 --> 00:07:43,540
tuned to determine what the model behavior is.

123
00:07:44,500 --> 00:07:48,678
Now, one nice way to think about matrix multiplication is to imagine each row of 

124
00:07:48,678 --> 00:07:52,804
that matrix as being its own vector, and taking a bunch of dot products between 

125
00:07:52,804 --> 00:07:56,880
those rows and the vector being processed, which I'll label as E for embedding.

126
00:07:57,280 --> 00:08:00,576
For example, suppose that very first row happened to equal 

127
00:08:00,576 --> 00:08:04,040
this first name Michael direction that we're presuming exists.

128
00:08:04,320 --> 00:08:09,411
That would mean that the first component in this output, this dot product right here, 

129
00:08:09,411 --> 00:08:12,964
would be one if that vector encodes the first name Michael, 

130
00:08:12,964 --> 00:08:14,800
and zero or negative otherwise.

131
00:08:15,880 --> 00:08:19,505
Even more fun, take a moment to think about what it would mean if that 

132
00:08:19,505 --> 00:08:23,080
first row was this first name Michael plus last name Jordan direction.

133
00:08:23,700 --> 00:08:27,420
And for simplicity, let me go ahead and write that down as M plus J.

134
00:08:28,080 --> 00:08:30,931
Then, taking a dot product with this embedding E, 

135
00:08:30,931 --> 00:08:34,980
things distribute really nicely, so it looks like M dot E plus J dot E.

136
00:08:34,980 --> 00:08:39,782
And notice how that means the ultimate value would be two if the vector encodes the 

137
00:08:39,782 --> 00:08:44,700
full name Michael Jordan, and otherwise it would be one or something smaller than one.

138
00:08:45,340 --> 00:08:47,260
And that's just one row in this matrix.

139
00:08:47,600 --> 00:08:51,871
You might think of all of the other rows as in parallel asking some other kinds of 

140
00:08:51,871 --> 00:08:56,040
questions, probing at some other sorts of features of the vector being processed.

141
00:08:56,700 --> 00:08:59,916
Very often this step also involves adding another vector to the output, 

142
00:08:59,916 --> 00:09:02,240
which is full of model parameters learned from data.

143
00:09:02,240 --> 00:09:04,560
This other vector is known as the bias.

144
00:09:05,180 --> 00:09:08,567
For our example, I want you to imagine that the value of this 

145
00:09:08,567 --> 00:09:11,353
bias in that very first component is negative one, 

146
00:09:11,353 --> 00:09:15,560
meaning our final output looks like that relevant dot product, but minus one.

147
00:09:16,120 --> 00:09:19,991
You might very reasonably ask why I would want you to assume that the 

148
00:09:19,991 --> 00:09:23,918
model has learned this, and in a moment you'll see why it's very clean 

149
00:09:23,918 --> 00:09:28,067
and nice if we have a value here which is positive if and only if a vector 

150
00:09:28,067 --> 00:09:32,160
encodes the full name Michael Jordan, and otherwise it's zero or negative.

151
00:09:33,040 --> 00:09:36,268
The total number of rows in this matrix, which is something 

152
00:09:36,268 --> 00:09:39,712
like the number of questions being asked, in the case of GPT-3, 

153
00:09:39,712 --> 00:09:42,780
whose numbers we've been following, is just under 50,000.

154
00:09:43,100 --> 00:09:46,640
In fact, it's exactly four times the number of dimensions in this embedding space.

155
00:09:46,920 --> 00:09:47,900
That's a design choice.

156
00:09:47,940 --> 00:09:49,816
You could make it more, you could make it less, 

157
00:09:49,816 --> 00:09:52,240
but having a clean multiple tends to be friendly for hardware.

158
00:09:52,740 --> 00:09:56,945
Since this matrix full of weights maps us into a higher dimensional space, 

159
00:09:56,945 --> 00:09:59,020
I'm gonna give it the shorthand W up.

160
00:09:59,020 --> 00:10:02,334
I'll continue labeling the vector we're processing as E, 

161
00:10:02,334 --> 00:10:07,160
and let's label this bias vector as B up and put that all back down in the diagram.

162
00:10:09,180 --> 00:10:12,956
At this point, a problem is that this operation is purely linear, 

163
00:10:12,956 --> 00:10:15,360
but language is a very non-linear process.

164
00:10:15,880 --> 00:10:19,778
If the entry that we're measuring is high for Michael plus Jordan, 

165
00:10:19,778 --> 00:10:23,910
it would also necessarily be somewhat triggered by Michael plus Phelps 

166
00:10:23,910 --> 00:10:28,100
and also Alexis plus Jordan, despite those being unrelated conceptually.

167
00:10:28,540 --> 00:10:32,000
What you really want is a simple yes or no for the full name.

168
00:10:32,900 --> 00:10:35,443
So the next step is to pass this large intermediate 

169
00:10:35,443 --> 00:10:37,840
vector through a very simple non-linear function.

170
00:10:38,360 --> 00:10:41,803
A common choice is one that takes all of the negative values and 

171
00:10:41,803 --> 00:10:45,300
maps them to zero and leaves all of the positive values unchanged.

172
00:10:46,440 --> 00:10:50,744
And continuing with the deep learning tradition of overly fancy names, 

173
00:10:50,744 --> 00:10:56,020
this very simple function is often called the rectified linear unit, or ReLU for short.

174
00:10:56,020 --> 00:10:57,880
Here's what the graph looks like.

175
00:10:58,300 --> 00:11:03,372
So taking our imagined example where this first entry of the intermediate vector is one, 

176
00:11:03,372 --> 00:11:07,874
if and only if the full name is Michael Jordan and zero or negative otherwise, 

177
00:11:07,874 --> 00:11:12,263
after you pass it through the ReLU, you end up with a very clean value where 

178
00:11:12,263 --> 00:11:15,740
all of the zero and negative values just get clipped to zero.

179
00:11:16,100 --> 00:11:19,780
So this output would be one for the full name Michael Jordan and zero otherwise.

180
00:11:20,560 --> 00:11:24,120
In other words, it very directly mimics the behavior of an AND gate.

181
00:11:25,660 --> 00:11:29,252
Often models will use a slightly modified function that's called the JLU, 

182
00:11:29,252 --> 00:11:32,020
which has the same basic shape, it's just a bit smoother.

183
00:11:32,500 --> 00:11:35,720
But for our purposes, it's a little bit cleaner if we only think about the ReLU.

184
00:11:36,740 --> 00:11:40,146
Also, when you hear people refer to the neurons of a transformer, 

185
00:11:40,146 --> 00:11:42,520
they're talking about these values right here.

186
00:11:42,900 --> 00:11:47,390
Whenever you see that common neural network picture with a layer of dots and a 

187
00:11:47,390 --> 00:11:52,278
bunch of lines connecting to the previous layer, which we had earlier in this series, 

188
00:11:52,278 --> 00:11:56,144
that's typically meant to convey this combination of a linear step, 

189
00:11:56,144 --> 00:12:01,260
a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.

190
00:12:02,500 --> 00:12:05,818
You would say that this neuron is active whenever this value 

191
00:12:05,818 --> 00:12:08,920
is positive and that it's inactive if that value is zero.

192
00:12:10,120 --> 00:12:12,380
The next step looks very similar to the first one.

193
00:12:12,560 --> 00:12:16,580
You multiply by a very large matrix and you add on a certain bias term.

194
00:12:16,980 --> 00:12:21,147
In this case, the number of dimensions in the output is back down to the size of 

195
00:12:21,147 --> 00:12:25,520
that embedding space, so I'm gonna go ahead and call this the down projection matrix.

196
00:12:26,220 --> 00:12:28,907
And this time, instead of thinking of things row by row, 

197
00:12:28,907 --> 00:12:31,360
it's actually nicer to think of it column by column.

198
00:12:31,860 --> 00:12:36,252
You see, another way that you can hold matrix multiplication in your head is to 

199
00:12:36,252 --> 00:12:40,698
imagine taking each column of the matrix and multiplying it by the corresponding 

200
00:12:40,698 --> 00:12:45,640
term in the vector that it's processing and adding together all of those rescaled columns.

201
00:12:46,840 --> 00:12:51,361
The reason it's nicer to think about this way is because here the columns have the same 

202
00:12:51,361 --> 00:12:55,780
dimension as the embedding space, so we can think of them as directions in that space.

203
00:12:56,140 --> 00:12:59,685
For instance, we will imagine that the model has learned to make that 

204
00:12:59,685 --> 00:13:03,080
first column into this basketball direction that we suppose exists.

205
00:13:04,180 --> 00:13:08,450
What that would mean is that when the relevant neuron in that first position is active, 

206
00:13:08,450 --> 00:13:10,780
we'll be adding this column to the final result.

207
00:13:11,140 --> 00:13:15,780
But if that neuron was inactive, if that number was zero, then this would have no effect.

208
00:13:16,500 --> 00:13:18,060
And it doesn't just have to be basketball.

209
00:13:18,220 --> 00:13:21,638
The model could also bake into this column and many other features that 

210
00:13:21,638 --> 00:13:25,200
it wants to associate with something that has the full name Michael Jordan.

211
00:13:26,980 --> 00:13:31,851
And at the same time, all of the other columns in this matrix are telling you 

212
00:13:31,851 --> 00:13:36,660
what will be added to the final result if the corresponding neuron is active.

213
00:13:37,360 --> 00:13:40,454
And if you have a bias in this case, it's something that you're 

214
00:13:40,454 --> 00:13:43,500
just adding every single time, regardless of the neuron values.

215
00:13:44,060 --> 00:13:45,280
You might wonder what's that doing.

216
00:13:45,540 --> 00:13:49,320
As with all parameter-filled objects here, it's kind of hard to say exactly.

217
00:13:49,320 --> 00:13:52,287
Maybe there's some bookkeeping that the network needs to do, 

218
00:13:52,287 --> 00:13:54,380
but you can feel free to ignore it for now.

219
00:13:54,860 --> 00:13:57,738
Making our notation a little more compact again, 

220
00:13:57,738 --> 00:14:02,438
I'll call this big matrix W down and similarly call that bias vector B down and 

221
00:14:02,438 --> 00:14:04,260
put that back into our diagram.

222
00:14:04,740 --> 00:14:09,118
Like I previewed earlier, what you do with this final result is add it to the vector 

223
00:14:09,118 --> 00:14:13,240
that flowed into the block at that position and that gets you this final result.

224
00:14:13,820 --> 00:14:19,060
So for example, if the vector flowing in encoded both first name Michael and last name 

225
00:14:19,060 --> 00:14:23,698
Jordan, then because this sequence of operations will trigger that AND gate, 

226
00:14:23,698 --> 00:14:28,697
it will add on the basketball direction, so what pops out will encode all of those 

227
00:14:28,697 --> 00:14:29,240
together.

228
00:14:29,820 --> 00:14:34,200
And remember, this is a process happening to every one of those vectors in parallel.

229
00:14:34,800 --> 00:14:39,767
In particular, taking the GPT-3 numbers, it means that this block doesn't just 

230
00:14:39,767 --> 00:14:44,860
have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.

231
00:14:48,180 --> 00:14:51,356
So that is the entire operation, two matrix products, 

232
00:14:51,356 --> 00:14:55,180
each with a bias added and a simple clipping function in between.

233
00:14:56,080 --> 00:14:59,415
Any of you who watched the earlier videos of the series will recognize this 

234
00:14:59,415 --> 00:15:02,620
structure as the most basic kind of neural network that we studied there.

235
00:15:03,080 --> 00:15:06,100
In that example, it was trained to recognize handwritten digits.

236
00:15:06,580 --> 00:15:10,804
Over here, in the context of a transformer for a large language model, 

237
00:15:10,804 --> 00:15:15,088
this is one piece in a larger architecture and any attempt to interpret 

238
00:15:15,088 --> 00:15:19,431
what exactly it's doing is heavily intertwined with the idea of encoding 

239
00:15:19,431 --> 00:15:23,180
information into vectors of a high-dimensional embedding space.

240
00:15:24,260 --> 00:15:28,600
That is the core lesson, but I do wanna step back and reflect on two different things, 

241
00:15:28,600 --> 00:15:32,043
the first of which is a kind of bookkeeping, and the second of which 

242
00:15:32,043 --> 00:15:35,435
involves a very thought-provoking fact about higher dimensions that 

243
00:15:35,435 --> 00:15:38,080
I actually didn't know until I dug into transformers.

244
00:15:41,080 --> 00:15:45,947
In the last two chapters, you and I started counting up the total number of parameters 

245
00:15:45,947 --> 00:15:50,760
in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.

246
00:15:51,400 --> 00:15:56,790
I already mentioned how this up projection matrix has just under 50,000 rows and 

247
00:15:56,790 --> 00:16:02,180
that each row matches the size of the embedding space, which for GPT-3 is 12,288.

248
00:16:03,240 --> 00:16:08,517
Multiplying those together, it gives us 604 million parameters just for that matrix, 

249
00:16:08,517 --> 00:16:13,920
and the down projection has the same number of parameters just with a transposed shape.

250
00:16:14,500 --> 00:16:17,400
So together, they give about 1.2 billion parameters.

251
00:16:18,280 --> 00:16:20,885
The bias vector also accounts for a couple more parameters, 

252
00:16:20,885 --> 00:16:24,100
but it's a trivial proportion of the total, so I'm not even gonna show it.

253
00:16:24,660 --> 00:16:29,612
In GPT-3, this sequence of embedding vectors flows through not one, 

254
00:16:29,612 --> 00:16:34,273
but 96 distinct MLPs, so the total number of parameters devoted 

255
00:16:34,273 --> 00:16:38,060
to all of these blocks adds up to about 116 billion.

256
00:16:38,820 --> 00:16:42,177
This is around 2 thirds of the total parameters in the network, 

257
00:16:42,177 --> 00:16:46,374
and when you add it to everything that we had before, for the attention blocks, 

258
00:16:46,374 --> 00:16:50,465
the embedding, and the unembedding, you do indeed get that grand total of 175 

259
00:16:50,465 --> 00:16:51,620
billion as advertised.

260
00:16:53,060 --> 00:16:56,637
It's probably worth mentioning there's another set of parameters associated 

261
00:16:56,637 --> 00:16:59,979
with those normalization steps that this explanation has skipped over, 

262
00:16:59,979 --> 00:17:03,840
but like the bias vector, they account for a very trivial proportion of the total.

263
00:17:05,900 --> 00:17:09,160
As to that second point of reflection, you might be wondering if 

264
00:17:09,160 --> 00:17:12,219
this central toy example we've been spending so much time on 

265
00:17:12,219 --> 00:17:15,680
reflects how facts are actually stored in real large language models.

266
00:17:16,319 --> 00:17:19,768
It is true that the rows of that first matrix can be thought of as 

267
00:17:19,768 --> 00:17:23,576
directions in this embedding space, and that means the activation of each 

268
00:17:23,576 --> 00:17:27,540
neuron tells you how much a given vector aligns with some specific direction.

269
00:17:27,760 --> 00:17:30,968
It's also true that the columns of that second matrix tell 

270
00:17:30,968 --> 00:17:34,340
you what will be added to the result if that neuron is active.

271
00:17:34,640 --> 00:17:36,800
Both of those are just mathematical facts.

272
00:17:37,740 --> 00:17:41,806
However, the evidence does suggest that individual neurons very rarely 

273
00:17:41,806 --> 00:17:44,899
represent a single clean feature like Michael Jordan, 

274
00:17:44,899 --> 00:17:48,507
and there may actually be a very good reason this is the case, 

275
00:17:48,507 --> 00:17:52,516
related to an idea floating around interpretability researchers these 

276
00:17:52,516 --> 00:17:54,120
days known as superposition.

277
00:17:54,640 --> 00:17:58,557
This is a hypothesis that might help to explain both why the models are 

278
00:17:58,557 --> 00:18:02,420
especially hard to interpret and also why they scale surprisingly well.

279
00:18:03,500 --> 00:18:07,386
The basic idea is that if you have an n-dimensional space and you wanna 

280
00:18:07,386 --> 00:18:11,165
represent a bunch of different features using directions that are all 

281
00:18:11,165 --> 00:18:14,080
perpendicular to one another in that space, you know, 

282
00:18:14,080 --> 00:18:16,780
that way if you add a component in one direction, 

283
00:18:16,780 --> 00:18:19,479
it doesn't influence any of the other directions, 

284
00:18:19,479 --> 00:18:23,960
then the maximum number of vectors you can fit is only n, the number of dimensions.

285
00:18:24,600 --> 00:18:27,620
To a mathematician, actually, this is the definition of dimension.

286
00:18:28,220 --> 00:18:30,873
But where it gets interesting is if you relax that 

287
00:18:30,873 --> 00:18:33,580
constraint a little bit and you tolerate some noise.

288
00:18:34,180 --> 00:18:38,709
Say you allow those features to be represented by vectors that aren't exactly 

289
00:18:38,709 --> 00:18:43,820
perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.

290
00:18:44,820 --> 00:18:48,020
If we were in two or three dimensions, this makes no difference.

291
00:18:48,260 --> 00:18:51,608
That gives you hardly any extra wiggle room to fit more vectors in, 

292
00:18:51,608 --> 00:18:55,204
which makes it all the more counterintuitive that for higher dimensions, 

293
00:18:55,204 --> 00:18:56,780
the answer changes dramatically.

294
00:18:57,660 --> 00:19:01,845
I can give you a really quick and dirty illustration of this using some 

295
00:19:01,845 --> 00:19:06,088
scrappy Python that's going to create a list of 100-dimensional vectors, 

296
00:19:06,088 --> 00:19:11,319
each one initialized randomly, and this list is going to contain 10,000 distinct vectors, 

297
00:19:11,319 --> 00:19:14,400
so 100 times as many vectors as there are dimensions.

298
00:19:15,320 --> 00:19:19,900
This plot right here shows the distribution of angles between pairs of these vectors.

299
00:19:20,680 --> 00:19:25,393
So because they started at random, those angles could be anything from 0 to 180 degrees, 

300
00:19:25,393 --> 00:19:28,676
but you'll notice that already, even just for random vectors, 

301
00:19:28,676 --> 00:19:31,960
there's this heavy bias for things to be closer to 90 degrees.

302
00:19:32,500 --> 00:19:37,169
Then what I'm going to do is run a certain optimization process that iteratively nudges 

303
00:19:37,169 --> 00:19:41,520
all of these vectors so that they try to become more perpendicular to one another.

304
00:19:42,060 --> 00:19:44,533
After repeating this many different times, here's 

305
00:19:44,533 --> 00:19:46,660
what the distribution of angles looks like.

306
00:19:47,120 --> 00:19:51,819
We have to actually zoom in on it here because all of the possible angles 

307
00:19:51,819 --> 00:19:56,900
between pairs of vectors sit inside this narrow range between 89 and 91 degrees.

308
00:19:58,020 --> 00:20:02,217
In general, a consequence of something known as the Johnson-Lindenstrauss 

309
00:20:02,217 --> 00:20:06,642
lemma is that the number of vectors you can cram into a space that are nearly 

310
00:20:06,642 --> 00:20:10,840
perpendicular like this grows exponentially with the number of dimensions.

311
00:20:11,960 --> 00:20:14,820
This is very significant for large language models, 

312
00:20:14,820 --> 00:20:18,505
which might benefit from associating independent ideas with nearly 

313
00:20:18,505 --> 00:20:19,880
perpendicular directions.

314
00:20:20,000 --> 00:20:22,596
It means that it's possible for it to store many, 

315
00:20:22,596 --> 00:20:26,440
many more ideas than there are dimensions in the space that it's allotted.

316
00:20:27,320 --> 00:20:31,740
This might partially explain why model performance seems to scale so well with size.

317
00:20:32,540 --> 00:20:36,316
A space that has 10 times as many dimensions can store way, 

318
00:20:36,316 --> 00:20:39,400
way more than 10 times as many independent ideas.

319
00:20:40,420 --> 00:20:43,871
And this is relevant not just to that embedding space where the vectors 

320
00:20:43,871 --> 00:20:47,323
flowing through the model live, but also to that vector full of neurons 

321
00:20:47,323 --> 00:20:50,440
in the middle of that multilayer perceptron that we just studied.

322
00:20:50,960 --> 00:20:56,113
That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, 

323
00:20:56,113 --> 00:20:59,978
but if it instead leveraged this enormous added capacity by using 

324
00:20:59,978 --> 00:21:04,370
nearly perpendicular directions of the space, it could be probing at many, 

325
00:21:04,370 --> 00:21:07,240
many more features of the vector being processed.

326
00:21:07,780 --> 00:21:10,926
But if it was doing that, what it means is that individual 

327
00:21:10,926 --> 00:21:14,340
features aren't gonna be visible as a single neuron lighting up.

328
00:21:14,660 --> 00:21:19,380
It would have to look like some specific combination of neurons instead, a superposition.

329
00:21:20,400 --> 00:21:24,315
For any of you curious to learn more, a key relevant search term here is sparse 

330
00:21:24,315 --> 00:21:28,426
autoencoder, which is a tool that some of the interpretability people use to try to 

331
00:21:28,426 --> 00:21:32,488
extract what the true features are, even if they're very superimposed on all these 

332
00:21:32,488 --> 00:21:32,880
neurons.

333
00:21:33,540 --> 00:21:36,800
I'll link to a couple really great anthropic posts all about this.

334
00:21:37,880 --> 00:21:40,970
At this point, we haven't touched every detail of a transformer, 

335
00:21:40,970 --> 00:21:43,300
but you and I have hit the most important points.

336
00:21:43,520 --> 00:21:47,640
The main thing that I wanna cover in a next chapter is the training process.

337
00:21:48,460 --> 00:21:51,929
On the one hand, the short answer for how training works is that it's all 

338
00:21:51,929 --> 00:21:55,821
backpropagation, and we covered backpropagation in a separate context with earlier 

339
00:21:55,821 --> 00:21:56,900
chapters in the series.

340
00:21:57,220 --> 00:22:02,034
But there is more to discuss, like the specific cost function used for language models, 

341
00:22:02,034 --> 00:22:06,083
the idea of fine-tuning using reinforcement learning with human feedback, 

342
00:22:06,083 --> 00:22:07,780
and the notion of scaling laws.

343
00:22:08,960 --> 00:22:11,113
Quick note for the active followers among you, 

344
00:22:11,113 --> 00:22:14,686
there are a number of non-machine learning-related videos that I'm excited to 

345
00:22:14,686 --> 00:22:18,167
sink my teeth into before I make that next chapter, so it might be a while, 

346
00:22:18,167 --> 00:22:20,000
but I do promise it'll come in due time.

347
00:22:35,640 --> 00:22:37,920
Thank you.

