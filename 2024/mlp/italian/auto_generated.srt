1
00:00:00,000 --> 00:00:03,459
Se a un modello linguistico di grandi dimensioni dai in pasto la frase 

2
00:00:03,459 --> 00:00:07,308
"Michael Jordan gioca a basket" e gli chiedi di prevedere cosa succederà dopo, 

3
00:00:07,308 --> 00:00:11,498
e lui predice correttamente la pallacanestro, questo suggerisce che da qualche parte, 

4
00:00:11,498 --> 00:00:14,324
all'interno delle sue centinaia di miliardi di parametri, 

5
00:00:14,324 --> 00:00:18,320
è stata inserita la conoscenza di una persona specifica e del suo sport specifico.

6
00:00:18,940 --> 00:00:22,086
E credo che, in generale, chiunque abbia giocato con uno di questi modelli 

7
00:00:22,086 --> 00:00:25,400
abbia la netta sensazione di aver memorizzato tonnellate e tonnellate di fatti.

8
00:00:25,700 --> 00:00:29,160
Quindi una domanda ragionevole da porsi è: come funziona esattamente?

9
00:00:29,160 --> 00:00:31,040
E dove vivono questi fatti?

10
00:00:35,720 --> 00:00:38,468
Lo scorso dicembre, alcuni ricercatori di Google DeepMind hanno 

11
00:00:38,468 --> 00:00:41,517
pubblicato un articolo sul lavoro svolto in merito a questa questione, 

12
00:00:41,517 --> 00:00:44,480
utilizzando l'esempio specifico di abbinare gli atleti ai loro sport.

13
00:00:44,900 --> 00:00:49,161
Sebbene la comprensione meccanica del modo in cui i fatti vengono memorizzati rimanga 

14
00:00:49,161 --> 00:00:52,679
irrisolta, sono stati ottenuti alcuni risultati parziali interessanti, 

15
00:00:52,679 --> 00:00:57,040
tra cui la conclusione generale di alto livello che i fatti sembrano vivere all'interno 

16
00:00:57,040 --> 00:01:01,252
di una parte specifica di queste reti, conosciute in modo fantasioso come perceptron 

17
00:01:01,252 --> 00:01:02,640
multistrato, o MLP in breve.

18
00:01:03,120 --> 00:01:07,365
Negli ultimi due capitoli, io e te abbiamo approfondito i dettagli dei trasformatori, 

19
00:01:07,365 --> 00:01:10,278
l'architettura alla base dei modelli linguistici di grandi 

20
00:01:10,278 --> 00:01:12,500
dimensioni e anche di molte altre AI moderne.

21
00:01:13,060 --> 00:01:16,200
Nell'ultimo capitolo ci siamo concentrati su un pezzo chiamato Attenzione.

22
00:01:16,840 --> 00:01:19,544
Il passo successivo, per te e per me, è quello di approfondire 

23
00:01:19,544 --> 00:01:22,764
i dettagli di ciò che accade all'interno di questi percettori multistrato, 

24
00:01:22,764 --> 00:01:25,040
che costituiscono l'altra grande porzione della rete.

25
00:01:25,680 --> 00:01:28,049
Il calcolo in questo caso è relativamente semplice, 

26
00:01:28,049 --> 00:01:30,100
soprattutto se lo paragoniamo all'attenzione.

27
00:01:30,560 --> 00:01:33,030
Si riduce essenzialmente a una coppia di moltiplicazioni 

28
00:01:33,030 --> 00:01:34,980
di matrici con un semplice qualcosa in mezzo.

29
00:01:35,720 --> 00:01:40,460
Tuttavia, l'interpretazione di ciò che fanno questi calcoli è estremamente impegnativa.

30
00:01:41,560 --> 00:01:45,872
Il nostro obiettivo principale è quello di illustrare i calcoli e renderli memorabili, 

31
00:01:45,872 --> 00:01:50,185
ma vorrei farlo mostrando un esempio specifico di come uno di questi blocchi potrebbe, 

32
00:01:50,185 --> 00:01:53,160
almeno in linea di principio, memorizzare un fatto concreto.

33
00:01:53,580 --> 00:01:57,080
Nello specifico, si tratterà di memorizzare il fatto che Michael Jordan gioca a basket.

34
00:01:58,080 --> 00:02:00,392
Devo dire che il layout di questo sito è ispirato a una 

35
00:02:00,392 --> 00:02:03,200
conversazione avuta con uno dei ricercatori di DeepMind, Neil Nanda.

36
00:02:04,060 --> 00:02:07,404
Per la maggior parte, darò per scontato che tu abbia guardato gli ultimi due 

37
00:02:07,404 --> 00:02:10,834
capitoli o che comunque tu abbia un'idea di base di cosa sia un trasformatore, 

38
00:02:10,834 --> 00:02:14,700
ma una rinfrescata non fa mai male, quindi ecco un rapido promemoria del flusso generale.

39
00:02:15,340 --> 00:02:18,241
Io e te abbiamo studiato un modello addestrato a 

40
00:02:18,241 --> 00:02:21,320
recepire un testo e a prevedere cosa succederà dopo.

41
00:02:21,720 --> 00:02:25,561
Il testo in ingresso viene prima suddiviso in una serie di token, 

42
00:02:25,561 --> 00:02:29,518
ovvero piccoli pezzi che sono tipicamente parole o pezzi di parole, 

43
00:02:29,518 --> 00:02:33,359
e ogni token viene associato a un vettore ad alta dimensionalità, 

44
00:02:33,359 --> 00:02:35,280
ovvero un lungo elenco di numeri.

45
00:02:35,840 --> 00:02:40,877
Questa sequenza di vettori passa poi ripetutamente attraverso due tipi di operazioni: 

46
00:02:40,877 --> 00:02:45,095
l'attenzione, che permette ai vettori di passare informazioni tra loro, 

47
00:02:45,095 --> 00:02:49,078
e poi i percettori multistrato, l'elemento che approfondiremo oggi, 

48
00:02:49,078 --> 00:02:52,300
e c'è anche una certa fase di normalizzazione in mezzo.

49
00:02:53,300 --> 00:02:56,282
Dopo che la sequenza di vettori ha attraversato molte, 

50
00:02:56,282 --> 00:02:58,885
molte iterazioni diverse di questi due blocchi, 

51
00:02:58,885 --> 00:03:03,060
alla fine si spera che ogni vettore abbia assorbito abbastanza informazioni, 

52
00:03:03,060 --> 00:03:06,530
sia dal contesto, da tutte le altre parole presenti nell'input, 

53
00:03:06,530 --> 00:03:11,194
sia dalla conoscenza generale che è stata incorporata nei pesi del modello attraverso 

54
00:03:11,194 --> 00:03:16,020
l'addestramento, da poter essere utilizzato per fare una previsione del token successivo.

55
00:03:16,860 --> 00:03:20,652
Una delle idee chiave che voglio che tu abbia in mente è che tutti questi 

56
00:03:20,652 --> 00:03:24,597
vettori vivono in uno spazio molto, molto alto e dimensionale e quando pensi 

57
00:03:24,597 --> 00:03:28,800
a questo spazio, direzioni diverse possono codificare diversi tipi di significato.

58
00:03:30,120 --> 00:03:33,898
Un esempio molto classico a cui mi piace fare riferimento è il fatto che se si 

59
00:03:33,898 --> 00:03:37,486
considera l'incorporazione di donna e si sottrae l'incorporazione di uomo, 

60
00:03:37,486 --> 00:03:41,121
e si fa quel piccolo passo e lo si aggiunge a un altro sostantivo maschile, 

61
00:03:41,121 --> 00:03:43,752
come ad esempio zio, si arriva da qualche parte molto, 

62
00:03:43,752 --> 00:03:46,240
molto vicino al sostantivo femminile corrispondente.

63
00:03:46,440 --> 00:03:50,880
In questo senso, questa particolare direzione codifica informazioni di genere.

64
00:03:51,640 --> 00:03:55,318
L'idea è che molte altre direzioni distinte in questo spazio super-dimensionale 

65
00:03:55,318 --> 00:03:58,996
potrebbero corrispondere ad altre caratteristiche che il modello potrebbe voler 

66
00:03:58,996 --> 00:03:59,640
rappresentare.

67
00:04:01,400 --> 00:04:03,967
In un trasformatore, però, questi vettori non si limitano 

68
00:04:03,967 --> 00:04:06,180
a codificare il significato di una singola parola.

69
00:04:06,680 --> 00:04:10,872
Mentre scorrono nella rete, acquisiscono un significato molto più ricco, 

70
00:04:10,872 --> 00:04:15,180
basato su tutto il contesto che li circonda e sulla conoscenza del modello.

71
00:04:15,880 --> 00:04:19,865
In definitiva, ognuno di essi deve codificare qualcosa che va ben oltre il significato 

72
00:04:19,865 --> 00:04:23,760
di una singola parola, poiché deve essere sufficiente a prevedere ciò che verrà dopo.

73
00:04:24,560 --> 00:04:29,053
Abbiamo già visto come i blocchi di attenzione ti permettano di incorporare il contesto, 

74
00:04:29,053 --> 00:04:33,546
ma la maggior parte dei parametri del modello vive in realtà all'interno dei blocchi MLP 

75
00:04:33,546 --> 00:04:37,837
e un'idea su cosa potrebbero fare è che offrono una capacità extra per memorizzare i 

76
00:04:37,837 --> 00:04:38,140
fatti.

77
00:04:38,720 --> 00:04:42,290
Come ho detto, la lezione si concentrerà sull'esempio concreto di un 

78
00:04:42,290 --> 00:04:46,120
giocattolo che può memorizzare il fatto che Michael Jordan gioca a basket.

79
00:04:47,120 --> 00:04:49,633
Ora, questo esempio giocattolo richiede che io e te facciamo 

80
00:04:49,633 --> 00:04:51,900
un paio di ipotesi su questo spazio ad alta dimensione.

81
00:04:52,360 --> 00:04:57,837
Per prima cosa, supponiamo che una delle direzioni rappresenti l'idea di un nome Michael, 

82
00:04:57,837 --> 00:05:03,011
e che un'altra direzione quasi perpendicolare rappresenti l'idea del cognome Jordan, 

83
00:05:03,011 --> 00:05:06,420
e che una terza direzione rappresenti l'idea del basket.

84
00:05:07,400 --> 00:05:12,176
Nello specifico, ciò che intendo dire è che se si esamina la rete e si estrae uno dei 

85
00:05:12,176 --> 00:05:16,841
vettori in fase di elaborazione, se il prodotto del punto con la direzione del nome 

86
00:05:16,841 --> 00:05:21,784
Michael è pari a uno, significa che il vettore sta codificando l'idea di una persona con 

87
00:05:21,784 --> 00:05:22,340
quel nome.

88
00:05:23,800 --> 00:05:25,894
Altrimenti, il prodotto del punto sarebbe zero o negativo, 

89
00:05:25,894 --> 00:05:28,700
il che significa che il vettore non è realmente allineato con quella direzione.

90
00:05:29,420 --> 00:05:32,265
E per semplicità, ignoriamo completamente la ragionevole domanda su 

91
00:05:32,265 --> 00:05:35,320
cosa potrebbe significare se il prodotto dei punti fosse maggiore di uno.

92
00:05:36,200 --> 00:05:39,883
Allo stesso modo, il prodotto del punto con queste altre 

93
00:05:39,883 --> 00:05:43,760
direzioni ti dirà se rappresenta il cognome Jordan o basket.

94
00:05:44,740 --> 00:05:48,786
Quindi, supponiamo che un vettore rappresenti il nome completo Michael Jordan, 

95
00:05:48,786 --> 00:05:52,680
allora il suo prodotto del punto con entrambe le direzioni dovrà essere uno.

96
00:05:53,480 --> 00:05:56,884
Dal momento che il testo Michael Jordan si estende su due token diversi, 

97
00:05:56,884 --> 00:06:00,150
ciò significa anche che dobbiamo supporre che un blocco di attenzione 

98
00:06:00,150 --> 00:06:04,254
precedente abbia passato con successo le informazioni al secondo di questi due vettori, 

99
00:06:04,254 --> 00:06:06,960
in modo da garantire che possa codificare entrambi i nomi.

100
00:06:07,940 --> 00:06:11,480
Partendo da questi presupposti, entriamo ora nel vivo della lezione.

101
00:06:11,880 --> 00:06:14,980
Cosa succede all'interno di un percettrone multistrato?

102
00:06:17,100 --> 00:06:21,364
Potresti pensare a questa sequenza di vettori che confluiscono nel blocco e ricordare 

103
00:06:21,364 --> 00:06:25,580
che ogni vettore era originariamente associato a uno dei token del testo in ingresso.

104
00:06:26,080 --> 00:06:29,642
Ciò che accadrà è che ogni singolo vettore di questa sequenza passerà 

105
00:06:29,642 --> 00:06:33,102
attraverso una breve serie di operazioni, che spiegheremo tra poco, 

106
00:06:33,102 --> 00:06:36,360
e alla fine otterremo un altro vettore con la stessa dimensione.

107
00:06:36,880 --> 00:06:39,918
L'altro vettore verrà aggiunto a quello originale 

108
00:06:39,918 --> 00:06:43,200
che è entrato e la somma sarà il risultato che uscirà.

109
00:06:43,720 --> 00:06:48,034
Questa sequenza di operazioni viene applicata a ogni vettore della sequenza, 

110
00:06:48,034 --> 00:06:51,620
associato a ogni token dell'input, e tutto avviene in parallelo.

111
00:06:52,100 --> 00:06:54,861
In particolare, in questa fase i vettori non si parlano tra loro, 

112
00:06:54,861 --> 00:06:56,200
ma fanno ognuno le proprie cose.

113
00:06:56,720 --> 00:06:59,483
E per te e per me, questo rende tutto molto più semplice, 

114
00:06:59,483 --> 00:07:02,724
perché significa che se capiamo cosa succede a uno solo dei vettori 

115
00:07:02,724 --> 00:07:06,060
attraverso questo blocco, capiamo effettivamente cosa succede a tutti.

116
00:07:07,100 --> 00:07:11,824
Quando dico che questo blocco codificherà il fatto che Michael Jordan gioca a basket, 

117
00:07:11,824 --> 00:07:16,658
intendo dire che se arriva un vettore che codifica il nome Michael e il cognome Jordan, 

118
00:07:16,658 --> 00:07:21,328
allora questa sequenza di calcoli produrrà qualcosa che include la direzione basket, 

119
00:07:21,328 --> 00:07:24,020
che si aggiungerà al vettore in quella posizione.

120
00:07:25,600 --> 00:07:27,527
Il primo passo di questo processo consiste nel 

121
00:07:27,527 --> 00:07:29,700
moltiplicare il vettore per una matrice molto grande.

122
00:07:30,040 --> 00:07:31,980
Non c'è da sorprendersi: si tratta di deep learning.

123
00:07:32,680 --> 00:07:35,059
Questa matrice, come tutte le altre che abbiamo visto, 

124
00:07:35,059 --> 00:07:37,785
è piena di parametri del modello che vengono appresi dai dati, 

125
00:07:37,785 --> 00:07:41,203
e che potremmo considerare come un insieme di manopole e quadranti che vengono 

126
00:07:41,203 --> 00:07:43,540
regolati per determinare il comportamento del modello.

127
00:07:44,500 --> 00:07:48,737
Ora, un modo simpatico di pensare alla moltiplicazione matriciale è quello di immaginare 

128
00:07:48,737 --> 00:07:52,737
ogni riga della matrice come un proprio vettore e di fare un insieme di prodotti di 

129
00:07:52,737 --> 00:07:56,880
punti tra queste righe e il vettore da elaborare, che etichetterò come E per embedding.

130
00:07:57,280 --> 00:08:01,091
Ad esempio, supponiamo che la prima riga corrisponda 

131
00:08:01,091 --> 00:08:04,040
al nome di Michael che presumiamo esista.

132
00:08:04,320 --> 00:08:09,905
Ciò significa che il primo componente di questo risultato, questo prodotto di punti qui, 

133
00:08:09,905 --> 00:08:14,800
sarà uno se il vettore codifica il nome Michael, e zero o negativo altrimenti.

134
00:08:15,880 --> 00:08:19,449
Ancora più divertente, pensa a cosa significherebbe se la 

135
00:08:19,449 --> 00:08:23,080
prima fila fosse composta da nome Michael e cognome Jordan.

136
00:08:23,700 --> 00:08:27,420
E per semplicità, permettimi di scriverlo come M più J.

137
00:08:28,080 --> 00:08:31,119
Poi, facendo un prodotto di punti con questo incorporamento E, 

138
00:08:31,119 --> 00:08:34,980
le cose si distribuiscono molto bene, quindi sembra che M punto E più J punto E.

139
00:08:34,980 --> 00:08:38,130
E nota come questo significhi che il valore finale sarebbe 

140
00:08:38,130 --> 00:08:41,335
due se il vettore codifica il nome completo Michael Jordan, 

141
00:08:41,335 --> 00:08:44,700
mentre altrimenti sarebbe uno o qualcosa di più piccolo di uno.

142
00:08:45,340 --> 00:08:47,260
E questa è solo una riga di questa matrice.

143
00:08:47,600 --> 00:08:51,845
Si potrebbe pensare che tutte le altre righe facciano parallelamente altri tipi di 

144
00:08:51,845 --> 00:08:56,040
domande, sondando altri tipi di caratteristiche del vettore che si sta elaborando.

145
00:08:56,700 --> 00:09:00,133
Molto spesso questo passaggio comporta anche l'aggiunta di un altro vettore all'output, 

146
00:09:00,133 --> 00:09:02,240
che è pieno di parametri del modello appresi dai dati.

147
00:09:02,240 --> 00:09:04,560
Quest'altro vettore è noto come bias.

148
00:09:05,180 --> 00:09:08,568
Per il nostro esempio, voglio che tu immagini che il valore di 

149
00:09:08,568 --> 00:09:11,096
questo bias nel primo componente sia negativo, 

150
00:09:11,096 --> 00:09:15,560
ovvero che il nostro risultato finale assomigli al prodotto dei punti, ma meno uno.

151
00:09:16,120 --> 00:09:20,075
Potresti ragionevolmente chiederti perché voglio che tu dia per scontato 

152
00:09:20,075 --> 00:09:23,923
che il modello abbia imparato questo, e tra poco vedrai perché è molto 

153
00:09:23,923 --> 00:09:27,770
pulito e piacevole se abbiamo un valore che è positivo se e solo se un 

154
00:09:27,770 --> 00:09:32,160
vettore codifica il nome completo Michael Jordan, e altrimenti è zero o negativo.

155
00:09:33,040 --> 00:09:36,286
Il numero totale di righe di questa matrice, che corrisponde 

156
00:09:36,286 --> 00:09:39,480
al numero di domande che vengono poste, nel caso del GPT-3, 

157
00:09:39,480 --> 00:09:42,780
di cui abbiamo seguito i numeri, è di poco inferiore a 50.000.

158
00:09:43,100 --> 00:09:44,887
In effetti, è esattamente quattro volte il numero 

159
00:09:44,887 --> 00:09:46,640
di dimensioni di questo spazio di incorporazione.

160
00:09:46,920 --> 00:09:47,900
È una scelta di design.

161
00:09:47,940 --> 00:09:49,921
Si può fare di più, si può fare di meno, ma avere un 

162
00:09:49,921 --> 00:09:52,240
multiplo pulito tende ad essere più amichevole per l'hardware.

163
00:09:52,740 --> 00:09:55,825
Dal momento che questa matrice piena di pesi ci mappa in 

164
00:09:55,825 --> 00:09:59,020
uno spazio dimensionale superiore, le darò il nome di W up.

165
00:09:59,020 --> 00:10:03,195
Continuerò a etichettare il vettore che stiamo elaborando come E e etichetterò 

166
00:10:03,195 --> 00:10:07,160
questo vettore di polarizzazione come B e riporterò il tutto nel diagramma.

167
00:10:09,180 --> 00:10:12,847
A questo punto, un problema è che questa operazione è puramente lineare, 

168
00:10:12,847 --> 00:10:15,360
ma il linguaggio è un processo molto poco lineare.

169
00:10:15,880 --> 00:10:20,138
Se l'ingresso che stiamo misurando è elevato per Michael più Jordan, 

170
00:10:20,138 --> 00:10:25,199
sarà necessariamente attivato anche da Michael più Phelps e da Alexis più Jordan, 

171
00:10:25,199 --> 00:10:28,100
nonostante non siano concettualmente correlati.

172
00:10:28,540 --> 00:10:32,000
Quello che vuoi è un semplice sì o no per il nome completo.

173
00:10:32,900 --> 00:10:35,465
Il passo successivo consiste nel far passare questo grande vettore 

174
00:10:35,465 --> 00:10:37,840
intermedio attraverso una funzione non lineare molto semplice.

175
00:10:38,360 --> 00:10:42,857
Una scelta comune è quella che prende tutti i valori negativi e li mappa a zero, 

176
00:10:42,857 --> 00:10:45,300
lasciando invariati tutti i valori positivi.

177
00:10:46,440 --> 00:10:50,994
E continuando con la tradizione dell'apprendimento profondo di nomi troppo fantasiosi, 

178
00:10:50,994 --> 00:10:55,182
questa funzione molto semplice viene spesso chiamata unità lineare rettificata, 

179
00:10:55,182 --> 00:10:56,020
o ReLU in breve.

180
00:10:56,020 --> 00:10:57,880
Ecco come appare il grafico.

181
00:10:58,300 --> 00:11:02,542
Quindi, prendendo il nostro esempio immaginario in cui la prima voce del vettore 

182
00:11:02,542 --> 00:11:06,889
intermedio è uno, se e solo se il nome completo è Michael Jordan e zero o negativo 

183
00:11:06,889 --> 00:11:09,979
in caso contrario, dopo averlo passato attraverso il ReLU, 

184
00:11:09,979 --> 00:11:14,168
si ottiene un valore molto pulito in cui tutti i valori zero e negativi vengono 

185
00:11:14,168 --> 00:11:15,740
semplicemente tagliati a zero.

186
00:11:16,100 --> 00:11:19,780
Quindi questo risultato sarà uno per il nome completo Michael Jordan e zero altrimenti.

187
00:11:20,560 --> 00:11:24,120
In altre parole, imita in modo molto diretto il comportamento di una porta AND.

188
00:11:25,660 --> 00:11:28,722
Spesso i modelli utilizzano una funzione leggermente modificata, 

189
00:11:28,722 --> 00:11:32,020
chiamata JLU, che ha la stessa forma di base, solo un po' più morbida.

190
00:11:32,500 --> 00:11:35,720
Ma per i nostri scopi, è un po' più pulito se pensiamo solo al ReLU.

191
00:11:36,740 --> 00:11:40,613
Inoltre, quando senti parlare dei neuroni di un trasformatore, 

192
00:11:40,613 --> 00:11:42,520
stai parlando di questi valori.

193
00:11:42,900 --> 00:11:47,464
Ogni volta che vedi la comune immagine di una rete neurale con uno strato di punti e un 

194
00:11:47,464 --> 00:11:50,420
gruppo di linee che si collegano allo strato precedente, 

195
00:11:50,420 --> 00:11:53,013
come abbiamo visto in precedenza in questa serie, 

196
00:11:53,013 --> 00:11:57,681
in genere si tratta di una combinazione di passi lineari, una moltiplicazione di matrice, 

197
00:11:57,681 --> 00:12:01,260
seguita da una semplice funzione non lineare a termine come una ReLU.

198
00:12:02,500 --> 00:12:05,822
Si potrebbe dire che questo neurone è attivo quando questo 

199
00:12:05,822 --> 00:12:08,920
valore è positivo e che è inattivo se il valore è zero.

200
00:12:10,120 --> 00:12:12,380
Il passo successivo è molto simile al primo.

201
00:12:12,560 --> 00:12:14,657
Si moltiplica per una matrice molto grande e si 

202
00:12:14,657 --> 00:12:16,580
aggiunge un certo termine di polarizzazione.

203
00:12:16,980 --> 00:12:21,250
In questo caso, il numero di dimensioni dell'output si riduce alle dimensioni dello 

204
00:12:21,250 --> 00:12:25,520
spazio di incorporazione, quindi la chiameremo matrice di proiezione verso il basso.

205
00:12:26,220 --> 00:12:29,135
E questa volta, invece di pensare alle cose riga per riga, 

206
00:12:29,135 --> 00:12:31,360
è più bello pensare alla colonna per colonna.

207
00:12:31,860 --> 00:12:36,552
Vedi, un altro modo per memorizzare la moltiplicazione matriciale è immaginare 

208
00:12:36,552 --> 00:12:40,710
di prendere ogni colonna della matrice e moltiplicarla per il termine 

209
00:12:40,710 --> 00:12:45,640
corrispondente nel vettore che sta elaborando e sommare tutte le colonne riscalate.

210
00:12:46,840 --> 00:12:49,820
Il motivo per cui è più bello pensare a questo modo è che in questo 

211
00:12:49,820 --> 00:12:53,106
caso le colonne hanno la stessa dimensione dello spazio di incorporamento, 

212
00:12:53,106 --> 00:12:55,780
quindi possiamo considerarle come direzioni in quello spazio.

213
00:12:56,140 --> 00:12:59,583
Per esempio, immaginiamo che il modello abbia imparato a fare la 

214
00:12:59,583 --> 00:13:03,080
prima colonna in questa direzione di basket che supponiamo esista.

215
00:13:04,180 --> 00:13:08,324
Ciò significa che quando il neurone in questione nella prima posizione è attivo, 

216
00:13:08,324 --> 00:13:10,780
aggiungeremo questa colonna al risultato finale.

217
00:13:11,140 --> 00:13:14,278
Ma se quel neurone fosse inattivo, se quel numero fosse pari a zero, 

218
00:13:14,278 --> 00:13:15,780
allora non avrebbe alcun effetto.

219
00:13:16,500 --> 00:13:18,060
E non si tratta solo di basket.

220
00:13:18,220 --> 00:13:21,891
Il modello potrebbe anche inserire in questa colonna molte altre caratteristiche 

221
00:13:21,891 --> 00:13:25,200
che vuole associare a qualcosa che ha il nome completo di Michael Jordan.

222
00:13:26,980 --> 00:13:31,885
Allo stesso tempo, tutte le altre colonne di questa matrice ti dicono cosa 

223
00:13:31,885 --> 00:13:36,660
verrà aggiunto al risultato finale se il neurone corrispondente è attivo.

224
00:13:37,360 --> 00:13:41,329
In questo caso, se hai un pregiudizio, è qualcosa che aggiungi ogni volta, 

225
00:13:41,329 --> 00:13:43,500
indipendentemente dai valori dei neuroni.

226
00:13:44,060 --> 00:13:45,280
Potresti chiederti cosa ci fa questo.

227
00:13:45,540 --> 00:13:49,320
Come per tutti gli oggetti pieni di parametri, è difficile dirlo con precisione.

228
00:13:49,320 --> 00:13:51,943
Forse la rete ha bisogno di fare un po' di contabilità, 

229
00:13:51,943 --> 00:13:54,380
ma per il momento puoi sentirti libero di ignorarla.

230
00:13:54,860 --> 00:13:57,612
Per rendere la nostra notazione un po' più compatta, 

231
00:13:57,612 --> 00:14:00,780
chiamerò questa grande matrice W e allo stesso modo chiamerò 

232
00:14:00,780 --> 00:14:04,260
il vettore di polarizzazione B e lo riporterò nel nostro diagramma.

233
00:14:04,740 --> 00:14:08,935
Come ho visto in anteprima, il risultato finale viene sommato al vettore che 

234
00:14:08,935 --> 00:14:13,240
è confluito nel blocco in quella posizione, ottenendo così il risultato finale.

235
00:14:13,820 --> 00:14:17,754
Quindi, ad esempio, se il vettore che arriva codifica sia il nome Michael 

236
00:14:17,754 --> 00:14:22,540
che il cognome Jordan, allora, poiché questa sequenza di operazioni innesca la porta AND, 

237
00:14:22,540 --> 00:14:26,262
aggiungerà la direzione della pallacanestro, in modo che il risultato 

238
00:14:26,262 --> 00:14:29,240
sarà quello di codificare tutti questi elementi insieme.

239
00:14:29,820 --> 00:14:34,200
E ricorda, questo è un processo che avviene per ognuno di questi vettori in parallelo.

240
00:14:34,800 --> 00:14:39,624
In particolare, prendendo i numeri GPT-3, significa che questo blocco non ha solo 

241
00:14:39,624 --> 00:14:44,860
50.000 neuroni al suo interno, ma ha 50.000 volte il numero di token presenti nell'input.

242
00:14:48,180 --> 00:14:51,010
Questa è l'intera operazione: due prodotti matriciali, 

243
00:14:51,010 --> 00:14:55,180
ciascuno con l'aggiunta di un bias e una semplice funzione di ritaglio nel mezzo.

244
00:14:56,080 --> 00:14:59,411
Chiunque di voi abbia guardato i video precedenti della serie riconoscerà questa 

245
00:14:59,411 --> 00:15:02,620
struttura come il tipo più elementare di rete neurale che abbiamo studiato lì.

246
00:15:03,080 --> 00:15:06,100
In quell'esempio, è stato addestrato a riconoscere le cifre scritte a mano.

247
00:15:06,580 --> 00:15:10,903
Qui, nel contesto di un trasformatore per un modello linguistico di grandi dimensioni, 

248
00:15:10,903 --> 00:15:15,327
si tratta di un pezzo di un'architettura più ampia e qualsiasi tentativo di interpretare 

249
00:15:15,327 --> 00:15:19,452
cosa stia facendo esattamente è fortemente intrecciato con l'idea di codificare le 

250
00:15:19,452 --> 00:15:23,180
informazioni in vettori di uno spazio di incorporamento ad alta dimensione.

251
00:15:24,260 --> 00:15:27,865
Questa è la lezione principale, ma voglio fare un passo indietro e riflettere 

252
00:15:27,865 --> 00:15:31,285
su due cose diverse, la prima delle quali è una sorta di contabilità e la 

253
00:15:31,285 --> 00:15:34,613
seconda riguarda un fatto molto interessante sulle dimensioni superiori 

254
00:15:34,613 --> 00:15:38,080
che in realtà non conoscevo fino a quando non ho scavato nei trasformatori.

255
00:15:41,080 --> 00:15:45,489
Negli ultimi due capitoli, io e te abbiamo iniziato a contare il numero totale di 

256
00:15:45,489 --> 00:15:48,608
parametri in GPT-3 e a vedere esattamente dove risiedono, 

257
00:15:48,608 --> 00:15:50,760
quindi finiamo rapidamente il gioco qui.

258
00:15:51,400 --> 00:15:54,993
Ho già detto che questa matrice di proiezione ha poco meno di 

259
00:15:54,993 --> 00:16:00,151
50.000 righe e che ogni riga corrisponde alla dimensione dello spazio di incorporazione, 

260
00:16:00,151 --> 00:16:02,180
che per il GPT-3 è di 12.288 righe.

261
00:16:03,240 --> 00:16:08,292
Moltiplicandoli insieme, si ottengono 604 milioni di parametri solo per quella matrice, 

262
00:16:08,292 --> 00:16:11,680
mentre la proiezione verso il basso ha lo stesso numero di 

263
00:16:11,680 --> 00:16:13,920
parametri solo con una forma trasposta.

264
00:16:14,500 --> 00:16:17,400
Quindi, insieme, forniscono circa 1,2 miliardi di parametri.

265
00:16:18,280 --> 00:16:20,720
Il vettore bias tiene conto anche di un altro paio di parametri, 

266
00:16:20,720 --> 00:16:24,100
ma si tratta di una percentuale insignificante del totale, quindi non lo mostrerò nemmeno.

267
00:16:24,660 --> 00:16:30,154
Nel GPT-3, questa sequenza di vettori di incorporamento passa attraverso non uno, 

268
00:16:30,154 --> 00:16:34,576
ma 96 MLP distinti, quindi il numero totale di parametri dedicati 

269
00:16:34,576 --> 00:16:38,060
a tutti questi blocchi ammonta a circa 116 miliardi.

270
00:16:38,820 --> 00:16:42,126
Si tratta di circa 2 terzi dei parametri totali della rete e, 

271
00:16:42,126 --> 00:16:45,913
sommandoli a tutto ciò che avevamo prima, per i blocchi di attenzione, 

272
00:16:45,913 --> 00:16:50,073
l'incorporazione e la disincarnazione, si ottiene effettivamente un totale di 

273
00:16:50,073 --> 00:16:51,620
175 miliardi come annunciato.

274
00:16:53,060 --> 00:16:56,507
Vale la pena ricordare che c'è un'altra serie di parametri associati a 

275
00:16:56,507 --> 00:16:59,906
questi passaggi di normalizzazione che questa spiegazione ha saltato, 

276
00:16:59,906 --> 00:17:03,840
ma come il vettore bias, rappresentano una parte molto insignificante del totale.

277
00:17:05,900 --> 00:17:07,870
Per quanto riguarda il secondo punto di riflessione, 

278
00:17:07,870 --> 00:17:11,068
potresti chiederti se questo esempio giocattolo centrale a cui abbiamo dedicato tanto 

279
00:17:11,068 --> 00:17:14,229
tempo riflette il modo in cui i fatti vengono effettivamente memorizzati nei modelli 

280
00:17:14,229 --> 00:17:15,680
linguistici reali di grandi dimensioni.

281
00:17:16,319 --> 00:17:20,106
È vero che le righe della prima matrice possono essere considerate come direzioni 

282
00:17:20,106 --> 00:17:23,661
in questo spazio di incorporazione e ciò significa che l'attivazione di ogni 

283
00:17:23,661 --> 00:17:27,540
neurone indica quanto un determinato vettore si allinea con una direzione specifica.

284
00:17:27,760 --> 00:17:31,078
È anche vero che le colonne della seconda matrice indicano 

285
00:17:31,078 --> 00:17:34,340
cosa verrà aggiunto al risultato se quel neurone è attivo.

286
00:17:34,640 --> 00:17:36,800
Entrambi sono solo fatti matematici.

287
00:17:37,740 --> 00:17:42,026
Tuttavia, le prove suggeriscono che i singoli neuroni molto raramente rappresentano 

288
00:17:42,026 --> 00:17:45,343
una singola caratteristica pulita come quella di Michael Jordan, 

289
00:17:45,343 --> 00:17:48,455
e potrebbe esserci un'ottima ragione per cui questo avviene, 

290
00:17:48,455 --> 00:17:52,844
legata a un'idea che circola in questi giorni tra i ricercatori sull'interpretabilità 

291
00:17:52,844 --> 00:17:54,120
nota come superposizione.

292
00:17:54,640 --> 00:17:58,168
Questa è un'ipotesi che potrebbe aiutare a spiegare sia perché i modelli sono 

293
00:17:58,168 --> 00:18:02,148
particolarmente difficili da interpretare, sia perché hanno una scala sorprendentemente 

294
00:18:02,148 --> 00:18:02,420
buona.

295
00:18:03,500 --> 00:18:07,725
L'idea di base è che se disponi di uno spazio a n dimensioni e vuoi rappresentare 

296
00:18:07,725 --> 00:18:11,539
un gruppo di caratteristiche diverse utilizzando direzioni che sono tutte 

297
00:18:11,539 --> 00:18:15,662
perpendicolari tra loro in quello spazio, in modo che se aggiungi un componente 

298
00:18:15,662 --> 00:18:19,424
in una direzione, questo non influisca su nessuna delle altre direzioni, 

299
00:18:19,424 --> 00:18:23,960
allora il numero massimo di vettori che puoi inserire è solo n, il numero di dimensioni.

300
00:18:24,600 --> 00:18:27,620
Per un matematico, in realtà, questa è la definizione di dimensione.

301
00:18:28,220 --> 00:18:30,844
Ma la cosa si fa interessante se si allenta un 

302
00:18:30,844 --> 00:18:33,580
po' questo vincolo e si tollera un po' di rumore.

303
00:18:34,180 --> 00:18:37,129
Diciamo che permetti a queste caratteristiche di essere 

304
00:18:37,129 --> 00:18:40,606
rappresentate da vettori che non sono esattamente perpendicolari, 

305
00:18:40,606 --> 00:18:43,820
ma solo quasi perpendicolari, magari tra gli 89 e i 91 gradi.

306
00:18:44,820 --> 00:18:48,020
Se fossimo in due o tre dimensioni, non farebbe alcuna differenza.

307
00:18:48,260 --> 00:18:51,971
Questo non ti dà quasi nessun margine di manovra aggiuntivo per inserire altri vettori, 

308
00:18:51,971 --> 00:18:55,388
il che rende ancora più controintuitivo il fatto che per dimensioni più elevate, 

309
00:18:55,388 --> 00:18:56,780
la risposta cambia drasticamente.

310
00:18:57,660 --> 00:19:01,900
Posso darti un'illustrazione molto veloce e sporca di questo utilizzando un 

311
00:19:01,900 --> 00:19:05,918
po' di Python scrauso che creerà un elenco di vettori a 100 dimensioni, 

312
00:19:05,918 --> 00:19:10,828
ognuno inizializzato in modo casuale, e questo elenco conterrà 10.000 vettori distinti, 

313
00:19:10,828 --> 00:19:14,400
quindi un numero di vettori 100 volte superiore alle dimensioni.

314
00:19:15,320 --> 00:19:19,900
Questo grafico mostra la distribuzione degli angoli tra le coppie di questi vettori.

315
00:19:20,680 --> 00:19:24,977
Dato che sono partiti a caso, gli angoli possono essere qualsiasi cosa, 

316
00:19:24,977 --> 00:19:29,035
da 0 a 180 gradi, ma noterai che, anche solo per i vettori casuali, 

317
00:19:29,035 --> 00:19:31,960
c'è una forte tendenza ad avvicinarsi a 90 gradi.

318
00:19:32,500 --> 00:19:36,928
Poi eseguirò un certo processo di ottimizzazione che sposta iterativamente tutti 

319
00:19:36,928 --> 00:19:41,520
questi vettori in modo che cerchino di diventare più perpendicolari l'uno all'altro.

320
00:19:42,060 --> 00:19:44,476
Dopo aver ripetuto questa operazione diverse volte, 

321
00:19:44,476 --> 00:19:46,660
ecco come appare la distribuzione degli angoli.

322
00:19:47,120 --> 00:19:51,916
Dobbiamo ingrandire l'immagine perché tutti i possibili angoli tra coppie di 

323
00:19:51,916 --> 00:19:56,900
vettori si trovano all'interno di questo ristretto intervallo tra 89 e 91 gradi.

324
00:19:58,020 --> 00:20:01,615
In generale, una conseguenza del cosiddetto lemma di 

325
00:20:01,615 --> 00:20:06,363
Johnson-Lindenstrauss è che il numero di vettori quasi perpendicolari 

326
00:20:06,363 --> 00:20:10,840
in uno spazio cresce esponenzialmente con il numero di dimensioni.

327
00:20:11,960 --> 00:20:15,292
Questo è molto significativo per i modelli linguistici di grandi dimensioni, 

328
00:20:15,292 --> 00:20:17,802
che potrebbero trarre vantaggio dall'associazione di idee 

329
00:20:17,802 --> 00:20:19,880
indipendenti con direzioni quasi perpendicolari.

330
00:20:20,000 --> 00:20:22,522
Significa che è possibile immagazzinare molte, 

331
00:20:22,522 --> 00:20:26,440
molte più idee di quante siano le dimensioni dello spazio a disposizione.

332
00:20:27,320 --> 00:20:29,510
Questo potrebbe spiegare in parte perché le prestazioni 

333
00:20:29,510 --> 00:20:31,740
del modello sembrano scalare così bene con le dimensioni.

334
00:20:32,540 --> 00:20:36,145
Uno spazio con un numero di dimensioni 10 volte superiore può contenere 

335
00:20:36,145 --> 00:20:39,400
un numero di idee indipendenti molto, molto superiore a 10 volte.

336
00:20:40,420 --> 00:20:43,834
E questo è rilevante non solo per lo spazio di incorporazione in cui vivono 

337
00:20:43,834 --> 00:20:47,159
i vettori che attraversano il modello, ma anche per quel vettore pieno di 

338
00:20:47,159 --> 00:20:50,440
neuroni al centro del perceptron multistrato che abbiamo appena studiato.

339
00:20:50,960 --> 00:20:54,777
In altre parole, alle dimensioni di GPT-3, potrebbe non limitarsi a 

340
00:20:54,777 --> 00:20:58,707
sondare 50.000 caratteristiche, ma se invece sfruttasse questa enorme 

341
00:20:58,707 --> 00:21:03,029
capacità aggiuntiva utilizzando direzioni quasi perpendicolari dello spazio, 

342
00:21:03,029 --> 00:21:07,240
potrebbe sondare molte, molte più caratteristiche del vettore da elaborare.

343
00:21:07,780 --> 00:21:11,137
Ma se lo facesse, significherebbe che le singole caratteristiche 

344
00:21:11,137 --> 00:21:14,340
non sarebbero visibili come un singolo neurone che si accende.

345
00:21:14,660 --> 00:21:19,380
Dovrebbe invece assomigliare a una combinazione specifica di neuroni, una sovrapposizione.

346
00:21:20,400 --> 00:21:24,457
Per chi fosse curioso di saperne di più, un termine di ricerca chiave è sparse 

347
00:21:24,457 --> 00:21:28,771
autoencoder, uno strumento che alcuni interpreti utilizzano per cercare di estrarre 

348
00:21:28,771 --> 00:21:32,880
le vere caratteristiche, anche se sono molto sovrapposte a tutti questi neuroni.

349
00:21:33,540 --> 00:21:36,800
Ti linko un paio di post antropici molto belli su questo argomento.

350
00:21:37,880 --> 00:21:41,114
A questo punto, non abbiamo toccato tutti i dettagli di un trasformatore, 

351
00:21:41,114 --> 00:21:43,300
ma io e te abbiamo toccato i punti più importanti.

352
00:21:43,520 --> 00:21:45,964
L'aspetto principale che voglio trattare nel prossimo 

353
00:21:45,964 --> 00:21:47,640
capitolo è il processo di formazione.

354
00:21:48,460 --> 00:21:51,244
Da un lato, la risposta breve su come funziona l'addestramento è 

355
00:21:51,244 --> 00:21:54,329
che si tratta di backpropagation, e abbiamo trattato la backpropagation 

356
00:21:54,329 --> 00:21:56,900
in un contesto separato nei capitoli precedenti della serie.

357
00:21:57,220 --> 00:22:00,517
Ma c'è molto altro da discutere, come la funzione di costo specifica 

358
00:22:00,517 --> 00:22:04,005
utilizzata per i modelli linguistici, l'idea della messa a punto tramite 

359
00:22:04,005 --> 00:22:07,780
l'apprendimento per rinforzo con feedback umano e la nozione di leggi di scala.

360
00:22:08,960 --> 00:22:10,934
Una nota veloce per i seguaci attivi tra di voi: 

361
00:22:10,934 --> 00:22:13,633
ci sono una serie di video non legati all'apprendimento automatico 

362
00:22:13,633 --> 00:22:17,098
in cui sono entusiasta di affondare i denti prima di realizzare il prossimo capitolo, 

363
00:22:17,098 --> 00:22:20,000
quindi potrebbe volerci un po', ma prometto che arriverà a tempo debito.

364
00:22:35,640 --> 00:22:37,920
Grazie.

