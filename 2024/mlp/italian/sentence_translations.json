[
 {
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "translatedText": "Se a un modello linguistico di grandi dimensioni dai in pasto la frase \"Michael Jordan gioca a basket\" e gli chiedi di prevedere cosa succederà dopo, e lui predice correttamente la pallacanestro, questo suggerisce che da qualche parte, all'interno delle sue centinaia di miliardi di parametri, è stata inserita la conoscenza di una persona specifica e del suo sport specifico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "translatedText": "E credo che, in generale, chiunque abbia giocato con uno di questi modelli abbia la netta sensazione di aver memorizzato tonnellate e tonnellate di fatti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "translatedText": "Quindi una domanda ragionevole da porsi è: come funziona esattamente?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "input": "And where do those facts live?",
  "translatedText": "E dove vivono questi fatti?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "translatedText": "Lo scorso dicembre, alcuni ricercatori di Google DeepMind hanno pubblicato un articolo sul lavoro svolto in merito a questa questione, utilizzando l'esempio specifico di abbinare gli atleti ai loro sport.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "translatedText": "Sebbene la comprensione meccanica del modo in cui i fatti vengono memorizzati rimanga irrisolta, sono stati ottenuti alcuni risultati parziali interessanti, tra cui la conclusione generale di alto livello che i fatti sembrano vivere all'interno di una parte specifica di queste reti, conosciute in modo fantasioso come perceptron multistrato, o MLP in breve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "translatedText": "Negli ultimi due capitoli, io e te abbiamo approfondito i dettagli dei trasformatori, l'architettura alla base dei modelli linguistici di grandi dimensioni e anche di molte altre AI moderne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "translatedText": "Nell'ultimo capitolo ci siamo concentrati su un pezzo chiamato Attenzione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "translatedText": "Il passo successivo, per te e per me, è quello di approfondire i dettagli di ciò che accade all'interno di questi percettori multistrato, che costituiscono l'altra grande porzione della rete.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "translatedText": "Il calcolo in questo caso è relativamente semplice, soprattutto se lo paragoniamo all'attenzione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "translatedText": "Si riduce essenzialmente a una coppia di moltiplicazioni di matrici con un semplice qualcosa in mezzo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "translatedText": "Tuttavia, l'interpretazione di ciò che fanno questi calcoli è estremamente impegnativa.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "translatedText": "Il nostro obiettivo principale è quello di illustrare i calcoli e renderli memorabili, ma vorrei farlo mostrando un esempio specifico di come uno di questi blocchi potrebbe, almeno in linea di principio, memorizzare un fatto concreto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "translatedText": "Nello specifico, si tratterà di memorizzare il fatto che Michael Jordan gioca a basket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "translatedText": "Devo dire che il layout di questo sito è ispirato a una conversazione avuta con uno dei ricercatori di DeepMind, Neil Nanda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "translatedText": "Per la maggior parte, darò per scontato che tu abbia guardato gli ultimi due capitoli o che comunque tu abbia un'idea di base di cosa sia un trasformatore, ma una rinfrescata non fa mai male, quindi ecco un rapido promemoria del flusso generale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "translatedText": "Io e te abbiamo studiato un modello addestrato a recepire un testo e a prevedere cosa succederà dopo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "translatedText": "Il testo in ingresso viene prima suddiviso in una serie di token, ovvero piccoli pezzi che sono tipicamente parole o pezzi di parole, e ogni token viene associato a un vettore ad alta dimensionalità, ovvero un lungo elenco di numeri.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "translatedText": "Questa sequenza di vettori passa poi ripetutamente attraverso due tipi di operazioni: l'attenzione, che permette ai vettori di passare informazioni tra loro, e poi i percettori multistrato, l'elemento che approfondiremo oggi, e c'è anche una certa fase di normalizzazione in mezzo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "translatedText": "Dopo che la sequenza di vettori ha attraversato molte, molte iterazioni diverse di questi due blocchi, alla fine si spera che ogni vettore abbia assorbito abbastanza informazioni, sia dal contesto, da tutte le altre parole presenti nell'input, sia dalla conoscenza generale che è stata incorporata nei pesi del modello attraverso l'addestramento, da poter essere utilizzato per fare una previsione del token successivo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "translatedText": "Una delle idee chiave che voglio che tu abbia in mente è che tutti questi vettori vivono in uno spazio molto, molto alto e dimensionale e quando pensi a questo spazio, direzioni diverse possono codificare diversi tipi di significato.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "translatedText": "Un esempio molto classico a cui mi piace fare riferimento è il fatto che se si considera l'incorporazione di donna e si sottrae l'incorporazione di uomo, e si fa quel piccolo passo e lo si aggiunge a un altro sostantivo maschile, come ad esempio zio, si arriva da qualche parte molto, molto vicino al sostantivo femminile corrispondente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "input": "In this sense, this particular direction encodes gender information.",
  "translatedText": "In questo senso, questa particolare direzione codifica informazioni di genere.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "translatedText": "L'idea è che molte altre direzioni distinte in questo spazio super-dimensionale potrebbero corrispondere ad altre caratteristiche che il modello potrebbe voler rappresentare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "translatedText": "In un trasformatore, però, questi vettori non si limitano a codificare il significato di una singola parola.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "translatedText": "Mentre scorrono nella rete, acquisiscono un significato molto più ricco, basato su tutto il contesto che li circonda e sulla conoscenza del modello.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "translatedText": "In definitiva, ognuno di essi deve codificare qualcosa che va ben oltre il significato di una singola parola, poiché deve essere sufficiente a prevedere ciò che verrà dopo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "translatedText": "Abbiamo già visto come i blocchi di attenzione ti permettano di incorporare il contesto, ma la maggior parte dei parametri del modello vive in realtà all'interno dei blocchi MLP e un'idea su cosa potrebbero fare è che offrono una capacità extra per memorizzare i fatti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "translatedText": "Come ho detto, la lezione si concentrerà sull'esempio concreto di un giocattolo che può memorizzare il fatto che Michael Jordan gioca a basket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "translatedText": "Ora, questo esempio giocattolo richiede che io e te facciamo un paio di ipotesi su questo spazio ad alta dimensione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "translatedText": "Per prima cosa, supponiamo che una delle direzioni rappresenti l'idea di un nome Michael, e che un'altra direzione quasi perpendicolare rappresenti l'idea del cognome Jordan, e che una terza direzione rappresenti l'idea del basket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "translatedText": "Nello specifico, ciò che intendo dire è che se si esamina la rete e si estrae uno dei vettori in fase di elaborazione, se il prodotto del punto con la direzione del nome Michael è pari a uno, significa che il vettore sta codificando l'idea di una persona con quel nome.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "translatedText": "Altrimenti, il prodotto del punto sarebbe zero o negativo, il che significa che il vettore non è realmente allineato con quella direzione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "translatedText": "E per semplicità, ignoriamo completamente la ragionevole domanda su cosa potrebbe significare se il prodotto dei punti fosse maggiore di uno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "translatedText": "Allo stesso modo, il prodotto del punto con queste altre direzioni ti dirà se rappresenta il cognome Jordan o basket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "translatedText": "Quindi, supponiamo che un vettore rappresenti il nome completo Michael Jordan, allora il suo prodotto del punto con entrambe le direzioni dovrà essere uno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "translatedText": "Dal momento che il testo Michael Jordan si estende su due token diversi, ciò significa anche che dobbiamo supporre che un blocco di attenzione precedente abbia passato con successo le informazioni al secondo di questi due vettori, in modo da garantire che possa codificare entrambi i nomi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "translatedText": "Partendo da questi presupposti, entriamo ora nel vivo della lezione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "input": "What happens inside a multilayer perceptron?",
  "translatedText": "Cosa succede all'interno di un percettrone multistrato?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "translatedText": "Potresti pensare a questa sequenza di vettori che confluiscono nel blocco e ricordare che ogni vettore era originariamente associato a uno dei token del testo in ingresso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "translatedText": "Ciò che accadrà è che ogni singolo vettore di questa sequenza passerà attraverso una breve serie di operazioni, che spiegheremo tra poco, e alla fine otterremo un altro vettore con la stessa dimensione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "translatedText": "L'altro vettore verrà aggiunto a quello originale che è entrato e la somma sarà il risultato che uscirà.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "translatedText": "Questa sequenza di operazioni viene applicata a ogni vettore della sequenza, associato a ogni token dell'input, e tutto avviene in parallelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "translatedText": "In particolare, in questa fase i vettori non si parlano tra loro, ma fanno ognuno le proprie cose.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "translatedText": "E per te e per me, questo rende tutto molto più semplice, perché significa che se capiamo cosa succede a uno solo dei vettori attraverso questo blocco, capiamo effettivamente cosa succede a tutti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "translatedText": "Quando dico che questo blocco codificherà il fatto che Michael Jordan gioca a basket, intendo dire che se arriva un vettore che codifica il nome Michael e il cognome Jordan, allora questa sequenza di calcoli produrrà qualcosa che include la direzione basket, che si aggiungerà al vettore in quella posizione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "translatedText": "Il primo passo di questo processo consiste nel moltiplicare il vettore per una matrice molto grande.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "input": "No surprises there, this is deep learning.",
  "translatedText": "Non c'è da sorprendersi: si tratta di deep learning.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "translatedText": "Questa matrice, come tutte le altre che abbiamo visto, è piena di parametri del modello che vengono appresi dai dati, e che potremmo considerare come un insieme di manopole e quadranti che vengono regolati per determinare il comportamento del modello.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "translatedText": "Ora, un modo simpatico di pensare alla moltiplicazione matriciale è quello di immaginare ogni riga della matrice come un proprio vettore e di fare un insieme di prodotti di punti tra queste righe e il vettore da elaborare, che etichetterò come E per embedding.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "translatedText": "Ad esempio, supponiamo che la prima riga corrisponda al nome di Michael che presumiamo esista.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "translatedText": "Ciò significa che il primo componente di questo risultato, questo prodotto di punti qui, sarà uno se il vettore codifica il nome Michael, e zero o negativo altrimenti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "translatedText": "Ancora più divertente, pensa a cosa significherebbe se la prima fila fosse composta da nome Michael e cognome Jordan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "translatedText": "E per semplicità, permettimi di scriverlo come M più J.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "translatedText": "Poi, facendo un prodotto di punti con questo incorporamento E, le cose si distribuiscono molto bene, quindi sembra che M punto E più J punto E.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "translatedText": "E nota come questo significhi che il valore finale sarebbe due se il vettore codifica il nome completo Michael Jordan, mentre altrimenti sarebbe uno o qualcosa di più piccolo di uno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "input": "And that's just one row in this matrix.",
  "translatedText": "E questa è solo una riga di questa matrice.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "translatedText": "Si potrebbe pensare che tutte le altre righe facciano parallelamente altri tipi di domande, sondando altri tipi di caratteristiche del vettore che si sta elaborando.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "translatedText": "Molto spesso questo passaggio comporta anche l'aggiunta di un altro vettore all'output, che è pieno di parametri del modello appresi dai dati.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "input": "This other vector is known as the bias.",
  "translatedText": "Quest'altro vettore è noto come bias.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "translatedText": "Per il nostro esempio, voglio che tu immagini che il valore di questo bias nel primo componente sia negativo, ovvero che il nostro risultato finale assomigli al prodotto dei punti, ma meno uno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "translatedText": "Potresti ragionevolmente chiederti perché voglio che tu dia per scontato che il modello abbia imparato questo, e tra poco vedrai perché è molto pulito e piacevole se abbiamo un valore che è positivo se e solo se un vettore codifica il nome completo Michael Jordan, e altrimenti è zero o negativo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "translatedText": "Il numero totale di righe di questa matrice, che corrisponde al numero di domande che vengono poste, nel caso del GPT-3, di cui abbiamo seguito i numeri, è di poco inferiore a 50.000.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "translatedText": "In effetti, è esattamente quattro volte il numero di dimensioni di questo spazio di incorporazione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "input": "That's a design choice.",
  "translatedText": "È una scelta di design.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "translatedText": "Si può fare di più, si può fare di meno, ma avere un multiplo pulito tende ad essere più amichevole per l'hardware.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "translatedText": "Dal momento che questa matrice piena di pesi ci mappa in uno spazio dimensionale superiore, le darò il nome di W up.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "translatedText": "Continuerò a etichettare il vettore che stiamo elaborando come E e etichetterò questo vettore di polarizzazione come B e riporterò il tutto nel diagramma.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "translatedText": "A questo punto, un problema è che questa operazione è puramente lineare, ma il linguaggio è un processo molto poco lineare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "translatedText": "Se l'ingresso che stiamo misurando è elevato per Michael più Jordan, sarà necessariamente attivato anche da Michael più Phelps e da Alexis più Jordan, nonostante non siano concettualmente correlati.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "input": "What you really want is a simple yes or no for the full name.",
  "translatedText": "Quello che vuoi è un semplice sì o no per il nome completo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "translatedText": "Il passo successivo consiste nel far passare questo grande vettore intermedio attraverso una funzione non lineare molto semplice.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "translatedText": "Una scelta comune è quella che prende tutti i valori negativi e li mappa a zero, lasciando invariati tutti i valori positivi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "translatedText": "E continuando con la tradizione dell'apprendimento profondo di nomi troppo fantasiosi, questa funzione molto semplice viene spesso chiamata unità lineare rettificata, o ReLU in breve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "input": "Here's what the graph looks like.",
  "translatedText": "Ecco come appare il grafico.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "translatedText": "Quindi, prendendo il nostro esempio immaginario in cui la prima voce del vettore intermedio è uno, se e solo se il nome completo è Michael Jordan e zero o negativo in caso contrario, dopo averlo passato attraverso il ReLU, si ottiene un valore molto pulito in cui tutti i valori zero e negativi vengono semplicemente tagliati a zero.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "translatedText": "Quindi questo risultato sarà uno per il nome completo Michael Jordan e zero altrimenti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "translatedText": "In altre parole, imita in modo molto diretto il comportamento di una porta AND.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "translatedText": "Spesso i modelli utilizzano una funzione leggermente modificata, chiamata JLU, che ha la stessa forma di base, solo un po' più morbida.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "translatedText": "Ma per i nostri scopi, è un po' più pulito se pensiamo solo al ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "translatedText": "Inoltre, quando senti parlare dei neuroni di un trasformatore, stai parlando di questi valori.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "translatedText": "Ogni volta che vedi la comune immagine di una rete neurale con uno strato di punti e un gruppo di linee che si collegano allo strato precedente, come abbiamo visto in precedenza in questa serie, in genere si tratta di una combinazione di passi lineari, una moltiplicazione di matrice, seguita da una semplice funzione non lineare a termine come una ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "translatedText": "Si potrebbe dire che questo neurone è attivo quando questo valore è positivo e che è inattivo se il valore è zero.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "input": "The next step looks very similar to the first one.",
  "translatedText": "Il passo successivo è molto simile al primo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "translatedText": "Si moltiplica per una matrice molto grande e si aggiunge un certo termine di polarizzazione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "translatedText": "In questo caso, il numero di dimensioni dell'output si riduce alle dimensioni dello spazio di incorporazione, quindi la chiameremo matrice di proiezione verso il basso.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "translatedText": "E questa volta, invece di pensare alle cose riga per riga, è più bello pensare alla colonna per colonna.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "translatedText": "Vedi, un altro modo per memorizzare la moltiplicazione matriciale è immaginare di prendere ogni colonna della matrice e moltiplicarla per il termine corrispondente nel vettore che sta elaborando e sommare tutte le colonne riscalate.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "translatedText": "Il motivo per cui è più bello pensare a questo modo è che in questo caso le colonne hanno la stessa dimensione dello spazio di incorporamento, quindi possiamo considerarle come direzioni in quello spazio.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "translatedText": "Per esempio, immaginiamo che il modello abbia imparato a fare la prima colonna in questa direzione di basket che supponiamo esista.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "translatedText": "Ciò significa che quando il neurone in questione nella prima posizione è attivo, aggiungeremo questa colonna al risultato finale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "translatedText": "Ma se quel neurone fosse inattivo, se quel numero fosse pari a zero, allora non avrebbe alcun effetto.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "input": "And it doesn't just have to be basketball.",
  "translatedText": "E non si tratta solo di basket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "translatedText": "Il modello potrebbe anche inserire in questa colonna molte altre caratteristiche che vuole associare a qualcosa che ha il nome completo di Michael Jordan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "translatedText": "Allo stesso tempo, tutte le altre colonne di questa matrice ti dicono cosa verrà aggiunto al risultato finale se il neurone corrispondente è attivo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "translatedText": "In questo caso, se hai un pregiudizio, è qualcosa che aggiungi ogni volta, indipendentemente dai valori dei neuroni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "input": "You might wonder what's that doing.",
  "translatedText": "Potresti chiederti cosa ci fa questo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "translatedText": "Come per tutti gli oggetti pieni di parametri, è difficile dirlo con precisione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "translatedText": "Forse la rete ha bisogno di fare un po' di contabilità, ma per il momento puoi sentirti libero di ignorarla.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "translatedText": "Per rendere la nostra notazione un po' più compatta, chiamerò questa grande matrice W e allo stesso modo chiamerò il vettore di polarizzazione B e lo riporterò nel nostro diagramma.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "translatedText": "Come ho visto in anteprima, il risultato finale viene sommato al vettore che è confluito nel blocco in quella posizione, ottenendo così il risultato finale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "translatedText": "Quindi, ad esempio, se il vettore che arriva codifica sia il nome Michael che il cognome Jordan, allora, poiché questa sequenza di operazioni innesca la porta AND, aggiungerà la direzione della pallacanestro, in modo che il risultato sarà quello di codificare tutti questi elementi insieme.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "translatedText": "E ricorda, questo è un processo che avviene per ognuno di questi vettori in parallelo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "translatedText": "In particolare, prendendo i numeri GPT-3, significa che questo blocco non ha solo 50.000 neuroni al suo interno, ma ha 50.000 volte il numero di token presenti nell'input.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "translatedText": "Questa è l'intera operazione: due prodotti matriciali, ciascuno con l'aggiunta di un bias e una semplice funzione di ritaglio nel mezzo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "translatedText": "Chiunque di voi abbia guardato i video precedenti della serie riconoscerà questa struttura come il tipo più elementare di rete neurale che abbiamo studiato lì.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "input": "In that example, it was trained to recognize handwritten digits.",
  "translatedText": "In quell'esempio, è stato addestrato a riconoscere le cifre scritte a mano.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "translatedText": "Qui, nel contesto di un trasformatore per un modello linguistico di grandi dimensioni, si tratta di un pezzo di un'architettura più ampia e qualsiasi tentativo di interpretare cosa stia facendo esattamente è fortemente intrecciato con l'idea di codificare le informazioni in vettori di uno spazio di incorporamento ad alta dimensione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "translatedText": "Questa è la lezione principale, ma voglio fare un passo indietro e riflettere su due cose diverse, la prima delle quali è una sorta di contabilità e la seconda riguarda un fatto molto interessante sulle dimensioni superiori che in realtà non conoscevo fino a quando non ho scavato nei trasformatori.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "translatedText": "Negli ultimi due capitoli, io e te abbiamo iniziato a contare il numero totale di parametri in GPT-3 e a vedere esattamente dove risiedono, quindi finiamo rapidamente il gioco qui.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "translatedText": "Ho già detto che questa matrice di proiezione ha poco meno di 50.000 righe e che ogni riga corrisponde alla dimensione dello spazio di incorporazione, che per il GPT-3 è di 12.288 righe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "translatedText": "Moltiplicandoli insieme, si ottengono 604 milioni di parametri solo per quella matrice, mentre la proiezione verso il basso ha lo stesso numero di parametri solo con una forma trasposta.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "input": "So together, they give about 1.2 billion parameters.",
  "translatedText": "Quindi, insieme, forniscono circa 1,2 miliardi di parametri.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "translatedText": "Il vettore bias tiene conto anche di un altro paio di parametri, ma si tratta di una percentuale insignificante del totale, quindi non lo mostrerò nemmeno.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "translatedText": "Nel GPT-3, questa sequenza di vettori di incorporamento passa attraverso non uno, ma 96 MLP distinti, quindi il numero totale di parametri dedicati a tutti questi blocchi ammonta a circa 116 miliardi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "translatedText": "Si tratta di circa 2 terzi dei parametri totali della rete e, sommandoli a tutto ciò che avevamo prima, per i blocchi di attenzione, l'incorporazione e la disincarnazione, si ottiene effettivamente un totale di 175 miliardi come annunciato.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "translatedText": "Vale la pena ricordare che c'è un'altra serie di parametri associati a questi passaggi di normalizzazione che questa spiegazione ha saltato, ma come il vettore bias, rappresentano una parte molto insignificante del totale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "translatedText": "Per quanto riguarda il secondo punto di riflessione, potresti chiederti se questo esempio giocattolo centrale a cui abbiamo dedicato tanto tempo riflette il modo in cui i fatti vengono effettivamente memorizzati nei modelli linguistici reali di grandi dimensioni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "translatedText": "È vero che le righe della prima matrice possono essere considerate come direzioni in questo spazio di incorporazione e ciò significa che l'attivazione di ogni neurone indica quanto un determinato vettore si allinea con una direzione specifica.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "translatedText": "È anche vero che le colonne della seconda matrice indicano cosa verrà aggiunto al risultato se quel neurone è attivo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "input": "Both of those are just mathematical facts.",
  "translatedText": "Entrambi sono solo fatti matematici.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "translatedText": "Tuttavia, le prove suggeriscono che i singoli neuroni molto raramente rappresentano una singola caratteristica pulita come quella di Michael Jordan, e potrebbe esserci un'ottima ragione per cui questo avviene, legata a un'idea che circola in questi giorni tra i ricercatori sull'interpretabilità nota come superposizione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "translatedText": "Questa è un'ipotesi che potrebbe aiutare a spiegare sia perché i modelli sono particolarmente difficili da interpretare, sia perché hanno una scala sorprendentemente buona.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "translatedText": "L'idea di base è che se disponi di uno spazio a n dimensioni e vuoi rappresentare un gruppo di caratteristiche diverse utilizzando direzioni che sono tutte perpendicolari tra loro in quello spazio, in modo che se aggiungi un componente in una direzione, questo non influisca su nessuna delle altre direzioni, allora il numero massimo di vettori che puoi inserire è solo n, il numero di dimensioni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "translatedText": "Per un matematico, in realtà, questa è la definizione di dimensione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "translatedText": "Ma la cosa si fa interessante se si allenta un po' questo vincolo e si tollera un po' di rumore.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "translatedText": "Diciamo che permetti a queste caratteristiche di essere rappresentate da vettori che non sono esattamente perpendicolari, ma solo quasi perpendicolari, magari tra gli 89 e i 91 gradi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "input": "If we were in two or three dimensions, this makes no difference.",
  "translatedText": "Se fossimo in due o tre dimensioni, non farebbe alcuna differenza.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "translatedText": "Questo non ti dà quasi nessun margine di manovra aggiuntivo per inserire altri vettori, il che rende ancora più controintuitivo il fatto che per dimensioni più elevate, la risposta cambia drasticamente.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "translatedText": "Posso darti un'illustrazione molto veloce e sporca di questo utilizzando un po' di Python scrauso che creerà un elenco di vettori a 100 dimensioni, ognuno inizializzato in modo casuale, e questo elenco conterrà 10.000 vettori distinti, quindi un numero di vettori 100 volte superiore alle dimensioni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "translatedText": "Questo grafico mostra la distribuzione degli angoli tra le coppie di questi vettori.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "translatedText": "Dato che sono partiti a caso, gli angoli possono essere qualsiasi cosa, da 0 a 180 gradi, ma noterai che, anche solo per i vettori casuali, c'è una forte tendenza ad avvicinarsi a 90 gradi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "translatedText": "Poi eseguirò un certo processo di ottimizzazione che sposta iterativamente tutti questi vettori in modo che cerchino di diventare più perpendicolari l'uno all'altro.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "translatedText": "Dopo aver ripetuto questa operazione diverse volte, ecco come appare la distribuzione degli angoli.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "translatedText": "Dobbiamo ingrandire l'immagine perché tutti i possibili angoli tra coppie di vettori si trovano all'interno di questo ristretto intervallo tra 89 e 91 gradi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "translatedText": "In generale, una conseguenza del cosiddetto lemma di Johnson-Lindenstrauss è che il numero di vettori quasi perpendicolari in uno spazio cresce esponenzialmente con il numero di dimensioni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "translatedText": "Questo è molto significativo per i modelli linguistici di grandi dimensioni, che potrebbero trarre vantaggio dall'associazione di idee indipendenti con direzioni quasi perpendicolari.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "translatedText": "Significa che è possibile immagazzinare molte, molte più idee di quante siano le dimensioni dello spazio a disposizione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "translatedText": "Questo potrebbe spiegare in parte perché le prestazioni del modello sembrano scalare così bene con le dimensioni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "translatedText": "Uno spazio con un numero di dimensioni 10 volte superiore può contenere un numero di idee indipendenti molto, molto superiore a 10 volte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "translatedText": "E questo è rilevante non solo per lo spazio di incorporazione in cui vivono i vettori che attraversano il modello, ma anche per quel vettore pieno di neuroni al centro del perceptron multistrato che abbiamo appena studiato.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "translatedText": "In altre parole, alle dimensioni di GPT-3, potrebbe non limitarsi a sondare 50.000 caratteristiche, ma se invece sfruttasse questa enorme capacità aggiuntiva utilizzando direzioni quasi perpendicolari dello spazio, potrebbe sondare molte, molte più caratteristiche del vettore da elaborare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "translatedText": "Ma se lo facesse, significherebbe che le singole caratteristiche non sarebbero visibili come un singolo neurone che si accende.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "translatedText": "Dovrebbe invece assomigliare a una combinazione specifica di neuroni, una sovrapposizione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "translatedText": "Per chi fosse curioso di saperne di più, un termine di ricerca chiave è sparse autoencoder, uno strumento che alcuni interpreti utilizzano per cercare di estrarre le vere caratteristiche, anche se sono molto sovrapposte a tutti questi neuroni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "translatedText": "Ti linko un paio di post antropici molto belli su questo argomento.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "translatedText": "A questo punto, non abbiamo toccato tutti i dettagli di un trasformatore, ma io e te abbiamo toccato i punti più importanti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "translatedText": "L'aspetto principale che voglio trattare nel prossimo capitolo è il processo di formazione.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "translatedText": "Da un lato, la risposta breve su come funziona l'addestramento è che si tratta di backpropagation, e abbiamo trattato la backpropagation in un contesto separato nei capitoli precedenti della serie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "translatedText": "Ma c'è molto altro da discutere, come la funzione di costo specifica utilizzata per i modelli linguistici, l'idea della messa a punto tramite l'apprendimento per rinforzo con feedback umano e la nozione di leggi di scala.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "translatedText": "Una nota veloce per i seguaci attivi tra di voi: ci sono una serie di video non legati all'apprendimento automatico in cui sono entusiasta di affondare i denti prima di realizzare il prossimo capitolo, quindi potrebbe volerci un po', ma prometto che arriverà a tempo debito.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "input": "Thank you.",
  "translatedText": "Grazie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]