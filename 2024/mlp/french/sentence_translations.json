[
 {
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "translatedText": "Si tu donnes à un grand modèle de langage la phrase \"Michael Jordan joue au basket-ball\", et que tu lui demandes de prédire ce qui va suivre, et qu'il prédit correctement le basket-ball, cela suggère que quelque part, dans ses centaines de milliards de paramètres, il a intégré des connaissances sur une personne spécifique et sur son sport spécifique.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "translatedText": "Et je pense qu'en général, toute personne qui a joué avec l'un de ces modèles a la nette impression qu'il a mémorisé des tonnes et des tonnes de faits.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "translatedText": "On peut donc raisonnablement se demander comment cela fonctionne exactement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "input": "And where do those facts live?",
  "translatedText": "Et où vivent ces faits ?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "translatedText": "En décembre dernier, quelques chercheurs de Google DeepMind ont publié des travaux sur cette question, et ils utilisaient cet exemple précis d'appariement des athlètes à leur sport.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "translatedText": "Et bien qu'une compréhension mécanique complète de la façon dont les faits sont stockés reste à faire, ils ont obtenu quelques résultats partiels intéressants, y compris la conclusion très générale de haut niveau que les faits semblent vivre à l'intérieur d'une partie spécifique de ces réseaux, connus sous le nom fantaisiste de perceptrons multicouches, ou MLP en abrégé.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "translatedText": "Dans les derniers chapitres, toi et moi avons creusé les détails derrière les transformateurs, l'architecture qui sous-tend les grands modèles de langage, et qui sous-tend également beaucoup d'autres IA modernes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "translatedText": "Dans le chapitre le plus récent, nous nous sommes concentrés sur une pièce appelée Attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "translatedText": "Et la prochaine étape pour toi et moi est de creuser les détails de ce qui se passe à l'intérieur de ces perceptrons multicouches, qui constituent l'autre grande partie du réseau.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "translatedText": "Le calcul ici est en fait relativement simple, surtout si tu le compares à l'attention.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "translatedText": "Cela se résume essentiellement à une paire de multiplications de matrices avec un simple quelque chose entre les deux.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "translatedText": "Cependant, l'interprétation de ce que font ces calculs est extrêmement difficile.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "translatedText": "Notre objectif principal ici est de présenter les calculs et de les rendre mémorables, mais j'aimerais le faire dans le contexte d'un exemple spécifique de la façon dont l'un de ces blocs pourrait, au moins en principe, stocker un fait concret.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "translatedText": "Plus précisément, il s'agira de stocker le fait que Michael Jordan joue au basket-ball.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "translatedText": "Je dois préciser que la mise en page ici est inspirée d'une conversation que j'ai eue avec l'un de ces chercheurs de DeepMind, Neil Nanda.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "translatedText": "Pour l'essentiel, je partirai du principe que tu as regardé les deux derniers chapitres ou que tu as une idée générale de ce qu'est un transformateur, mais les rafraîchissements ne font jamais de mal, alors voici un petit rappel du déroulement général.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "translatedText": "Toi et moi avons étudié un modèle qui a été entraîné à prendre un morceau de texte et à prédire ce qui va suivre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "translatedText": "Ce texte d'entrée est d'abord décomposé en un ensemble de jetons, c'est-à-dire de petits morceaux qui sont typiquement des mots ou de petits morceaux de mots, et chaque jeton est associé à un vecteur à haute dimension, c'est-à-dire à une longue liste de nombres.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "translatedText": "Cette séquence de vecteurs passe ensuite de façon répétée par deux types d'opérations, l'attention, qui permet aux vecteurs de se transmettre des informations entre eux, puis les perceptrons multicouches, le truc que nous allons creuser aujourd'hui, et il y a aussi une certaine étape de normalisation entre les deux.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "translatedText": "Après que la séquence de vecteurs a traversé de nombreuses itérations différentes de ces deux blocs, on espère qu'à la fin, chaque vecteur a absorbé suffisamment d'informations, à la fois du contexte, de tous les autres mots de l'entrée, et aussi de la connaissance générale qui a été incorporée dans les poids du modèle par l'entraînement, pour qu'il puisse être utilisé pour faire une prédiction du jeton suivant.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "translatedText": "L'une des idées clés que je veux que tu aies à l'esprit est que tous ces vecteurs vivent dans un espace à très, très haute dimension, et lorsque tu penses à cet espace, différentes directions peuvent encoder différents types de signification.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "translatedText": "Un exemple très classique auquel j'aime me référer est que si tu regardes l'intégration de la femme et que tu soustrais l'intégration de l'homme, et que tu fais ce petit pas et que tu l'ajoutes à un autre nom masculin, quelque chose comme l'oncle, tu atterris quelque part très, très près du nom féminin correspondant.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "input": "In this sense, this particular direction encodes gender information.",
  "translatedText": "En ce sens, cette direction particulière encode des informations sur le genre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "translatedText": "L'idée est que de nombreuses autres directions distinctes dans cet espace à très haute dimension pourraient correspondre à d'autres caractéristiques que le modèle pourrait vouloir représenter.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "translatedText": "Dans un transformateur, ces vecteurs ne se contentent pas d'encoder le sens d'un seul mot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "translatedText": "Au fur et à mesure qu'ils circulent dans le réseau, ils s'imprègnent d'une signification beaucoup plus riche basée sur tout le contexte qui les entoure, et aussi sur les connaissances du modèle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "translatedText": "En fin de compte, chacun doit coder quelque chose qui va bien au-delà de la signification d'un seul mot, puisqu'il doit être suffisant pour prédire ce qui va suivre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "translatedText": "Nous avons déjà vu comment les blocs d'attention te permettent d'intégrer le contexte, mais la majorité des paramètres du modèle se trouvent en fait à l'intérieur des blocs MLP, et l'on peut penser qu'ils offrent une capacité supplémentaire pour stocker des faits.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "translatedText": "Comme je l'ai dit, la leçon ici va être centrée sur l'exemple concret du jouet qui pourrait stocker le fait que Michael Jordan joue au basket-ball.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "translatedText": "Maintenant, cet exemple de jouet va exiger que toi et moi fassions quelques hypothèses sur cet espace à haute dimension.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "translatedText": "Tout d'abord, nous supposerons qu'une des directions représente l'idée d'un prénom Michael, puis qu'une autre direction presque perpendiculaire représente l'idée du nom de famille Jordan, et enfin qu'une troisième direction encore représentera l'idée du basket-ball.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "translatedText": "Plus précisément, ce que je veux dire par là, c'est que si tu regardes dans le réseau et que tu extrais l'un des vecteurs traités, si son produit en points avec la direction du prénom Michael est égal à un, cela signifie que le vecteur codifie l'idée d'une personne portant ce prénom.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "translatedText": "Sinon, le produit de ce point serait nul ou négatif, ce qui signifierait que le vecteur ne s'aligne pas vraiment sur cette direction.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "translatedText": "Et pour simplifier, ignorons complètement la question très raisonnable de savoir ce que cela pourrait signifier si ce produit de points était supérieur à un.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "translatedText": "De même, son produit en points avec ces autres directions te permettrait de savoir s'il représente le nom de famille Jordan ou basket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "translatedText": "Disons qu'un vecteur est censé représenter le nom complet, Michael Jordan, et que son produit en points avec ces deux directions doit être égal à un.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "translatedText": "Étant donné que le texte Michael Jordan s'étend sur deux tokens différents, cela signifie également que nous devons supposer qu'un bloc d'attention antérieur a réussi à transmettre des informations au second de ces deux vecteurs afin de s'assurer qu'il peut encoder les deux noms.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "translatedText": "Avec tous ces éléments comme hypothèses, plongeons maintenant dans la chair de la leçon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "input": "What happens inside a multilayer perceptron?",
  "translatedText": "Que se passe-t-il à l'intérieur d'un perceptron multicouche ?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "translatedText": "Tu peux penser à cette séquence de vecteurs qui s'écoulent dans le bloc, et te rappeler que chaque vecteur était à l'origine associé à l'un des tokens du texte d'entrée.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "translatedText": "Ce qui va se passer, c'est que chaque vecteur individuel de cette séquence va passer par une courte série d'opérations, que nous allons décortiquer dans un instant, et à la fin, nous obtiendrons un autre vecteur avec la même dimension.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "translatedText": "Cet autre vecteur va s'ajouter au vecteur initial qui est entré, et cette somme est le résultat qui sort.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "translatedText": "Cette séquence d'opérations est quelque chose que tu appliques à chaque vecteur de la séquence, associé à chaque jeton de l'entrée, et tout cela se passe en parallèle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "translatedText": "En particulier, les vecteurs ne se parlent pas entre eux dans cette étape, ils font tous un peu ce qu'ils veulent.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "translatedText": "Et pour toi et moi, cela rend les choses beaucoup plus simples, car cela signifie que si nous comprenons ce qui arrive à un seul des vecteurs à travers ce bloc, nous comprenons effectivement ce qui arrive à tous les vecteurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "translatedText": "Lorsque je dis que ce bloc va coder le fait que Michael Jordan joue au basket-ball, ce que je veux dire c'est que si un vecteur entre qui code le prénom Michael et le nom Jordan, alors cette séquence de calculs produira quelque chose qui inclut la direction basket-ball, ce qui s'ajoutera au vecteur à cette position.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "translatedText": "La première étape de ce processus consiste à multiplier ce vecteur par une très grande matrice.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "input": "No surprises there, this is deep learning.",
  "translatedText": "Il n'y a pas de surprise, il s'agit d'apprentissage en profondeur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "translatedText": "Et cette matrice, comme toutes celles que nous avons vues, est remplie de paramètres de modèle appris à partir des données, que tu peux considérer comme un ensemble de boutons et de cadrans qui sont réglés et ajustés pour déterminer le comportement du modèle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "translatedText": "Une bonne façon d'envisager la multiplication matricielle est d'imaginer que chaque ligne de cette matrice est son propre vecteur, et de faire un tas de produits de points entre ces lignes et le vecteur traité, que j'appellerai E pour embedding (intégration).",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "translatedText": "Par exemple, supposons que la toute première ligne corresponde à ce prénom Michael direction dont nous supposons qu'il existe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "translatedText": "Cela signifie que la première composante de ce résultat, ce produit de point ici, serait un si ce vecteur code le prénom Michael, et zéro ou négatif dans le cas contraire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "translatedText": "Encore plus amusant, prends un moment pour réfléchir à ce que cela signifierait si cette première rangée était cette direction prénom Michael plus nom de famille Jordan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "translatedText": "Et pour plus de simplicité, permets-moi d'aller de l'avant et d'écrire cela sous la forme M plus J.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "translatedText": "Ensuite, en prenant un produit de point avec cet encastrement E, les choses se répartissent vraiment bien, de sorte que cela ressemble à M point E plus J point E.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "translatedText": "Et remarque que cela signifie que la valeur ultime sera deux si le vecteur code le nom complet Michael Jordan, et sinon ce sera un ou quelque chose de plus petit que un.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "input": "And that's just one row in this matrix.",
  "translatedText": "Et ce n'est qu'une seule ligne de cette matrice.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "translatedText": "Tu peux considérer que toutes les autres lignes posent en parallèle d'autres types de questions, qu'elles sondent d'autres types de caractéristiques du vecteur en cours de traitement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "translatedText": "Très souvent, cette étape consiste également à ajouter un autre vecteur à la sortie, qui est plein de paramètres du modèle appris à partir des données.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "input": "This other vector is known as the bias.",
  "translatedText": "Cet autre vecteur est appelé le biais.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "translatedText": "Pour notre exemple, je veux que tu imagines que la valeur de ce biais dans ce tout premier composant est négative de un, ce qui signifie que notre résultat final ressemble à ce produit de points pertinent, mais moins un.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "translatedText": "Tu pourrais très raisonnablement demander pourquoi je voudrais que tu supposes que le modèle a appris cela, et dans un instant tu verras pourquoi c'est très propre et agréable si nous avons une valeur ici qui est positive si et seulement si un vecteur encode le nom complet Michael Jordan, et sinon c'est zéro ou négatif.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "translatedText": "Le nombre total de lignes de cette matrice, qui correspond en quelque sorte au nombre de questions posées, dans le cas du GPT-3, dont nous avons suivi les chiffres, est d'un peu moins de 50 000.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "translatedText": "En fait, c'est exactement quatre fois le nombre de dimensions de cet espace d'intégration.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "input": "That's a design choice.",
  "translatedText": "C'est un choix de conception.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "translatedText": "Tu pourrais en faire plus, tu pourrais en faire moins, mais le fait d'avoir un multiple propre a tendance à être convivial pour le matériel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "translatedText": "Comme cette matrice pleine de poids nous place dans un espace de dimension supérieure, je vais lui donner le nom abrégé de W up.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "translatedText": "Je vais continuer à étiqueter le vecteur que nous traitons comme E, et étiqueter ce vecteur de biais comme B et remettre tout cela dans le diagramme.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "translatedText": "À ce stade, un problème se pose : cette opération est purement linéaire, mais la langue est un processus très non linéaire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "translatedText": "Si l'entrée que nous mesurons est élevée pour Michael plus Jordan, elle sera aussi nécessairement déclenchée par Michael plus Phelps et Alexis plus Jordan, même si ces deux éléments n'ont aucun lien conceptuel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "input": "What you really want is a simple yes or no for the full name.",
  "translatedText": "Ce que tu veux vraiment, c'est un simple oui ou non pour le nom complet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "translatedText": "L'étape suivante consiste donc à faire passer ce grand vecteur intermédiaire par une fonction non linéaire très simple.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "translatedText": "Un choix courant est celui qui prend toutes les valeurs négatives et les fait correspondre à zéro et laisse toutes les valeurs positives inchangées.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "translatedText": "Et pour continuer avec la tradition de l'apprentissage profond qui consiste à utiliser des noms trop fantaisistes, cette fonction très simple est souvent appelée l'unité linéaire rectifiée, ou ReLU en abrégé.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "input": "Here's what the graph looks like.",
  "translatedText": "Voici à quoi ressemble le graphique.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "translatedText": "Donc, en prenant notre exemple imaginé où cette première entrée du vecteur intermédiaire est un, si et seulement si le nom complet est Michael Jordan et zéro ou négatif sinon, après l'avoir fait passer par la ReLU, tu te retrouves avec une valeur très propre où toutes les valeurs zéro et négatives sont simplement écrêtées à zéro.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "translatedText": "Ce résultat serait donc un pour le nom complet Michael Jordan et zéro sinon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "translatedText": "En d'autres termes, il imite très directement le comportement d'une porte ET.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "translatedText": "Souvent, les modèles utilisent une fonction légèrement modifiée appelée JLU, qui a la même forme de base, elle est juste un peu plus lisse.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "translatedText": "Mais pour ce qui nous concerne, c'est un peu plus propre si nous ne pensons qu'au ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "translatedText": "Aussi, lorsque tu entends les gens faire référence aux neurones d'un transformateur, ils parlent de ces valeurs ici même.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "translatedText": "Chaque fois que tu vois l'image d'un réseau neuronal avec une couche de points et une série de lignes reliées à la couche précédente, comme nous l'avons vu plus tôt dans cette série, c'est généralement pour exprimer cette combinaison d'une étape linéaire, une multiplication de matrice, suivie d'une fonction non linéaire simple par terme comme une ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "translatedText": "Tu dirais que ce neurone est actif chaque fois que cette valeur est positive et qu'il est inactif si cette valeur est nulle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "input": "The next step looks very similar to the first one.",
  "translatedText": "L'étape suivante ressemble beaucoup à la première.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "translatedText": "Tu multiplies par une très grande matrice et tu ajoutes un certain terme de biais.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "translatedText": "Dans ce cas, le nombre de dimensions dans la sortie est ramené à la taille de l'espace d'intégration, je vais donc l'appeler la matrice de projection vers le bas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "translatedText": "Et cette fois, au lieu de penser aux choses ligne par ligne, il est en fait plus agréable d'y penser colonne par colonne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "translatedText": "Tu vois, une autre façon de retenir la multiplication matricielle dans ta tête est d'imaginer que l'on prend chaque colonne de la matrice et qu'on la multiplie par le terme correspondant dans le vecteur qu'elle traite, puis que l'on additionne toutes ces colonnes remises à l'échelle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "translatedText": "La raison pour laquelle il est plus agréable de penser de cette façon est qu'ici les colonnes ont la même dimension que l'espace d'intégration, nous pouvons donc les considérer comme des directions dans cet espace.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "translatedText": "Par exemple, nous imaginerons que le modèle a appris à faire cette première colonne dans cette direction de basket que nous supposons exister.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "translatedText": "Cela signifie que lorsque le neurone correspondant à cette première position est actif, nous ajouterons cette colonne au résultat final.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "translatedText": "Mais si ce neurone était inactif, si ce nombre était nul, alors cela n'aurait aucun effet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "input": "And it doesn't just have to be basketball.",
  "translatedText": "Et il ne s'agit pas seulement de basket-ball.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "translatedText": "Le modèle pourrait également intégrer cette colonne et bien d'autres caractéristiques qu'il souhaite associer à quelque chose qui porte le nom complet de Michael Jordan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "translatedText": "Et en même temps, toutes les autres colonnes de cette matrice t'indiquent ce qui sera ajouté au résultat final si le neurone correspondant est actif.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "translatedText": "Et si tu as un biais dans ce cas, c'est quelque chose que tu ajoutes à chaque fois, quelles que soient les valeurs des neurones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "input": "You might wonder what's that doing.",
  "translatedText": "Tu peux te demander ce que cela fait.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "translatedText": "Comme pour tous les objets remplis de paramètres ici, c'est un peu difficile à dire exactement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "translatedText": "Il y a peut-être une comptabilité que le réseau doit faire, mais tu peux te sentir libre de l'ignorer pour l'instant.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "translatedText": "Pour rendre notre notation un peu plus compacte, j'appellerai cette grande matrice W vers le bas et, de la même façon, j'appellerai ce vecteur de biais B vers le bas et je le remettrai dans notre diagramme.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "translatedText": "Comme je l'ai indiqué plus tôt, ce que tu fais avec ce résultat final, c'est l'ajouter au vecteur qui est entré dans le bloc à cette position et cela te donne ce résultat final.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "translatedText": "Par exemple, si le vecteur qui entre codait à la fois le prénom Michael et le nom Jordan, cette séquence d'opérations déclenchera la porte ET et ajoutera la direction du ballon de basket, de sorte que le vecteur qui sort codera tous ces éléments ensemble.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "translatedText": "Et n'oublie pas qu'il s'agit d'un processus qui se déroule en parallèle pour chacun de ces vecteurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "translatedText": "En particulier, si l'on prend les chiffres du GPT-3, cela signifie que ce bloc ne contient pas seulement 50 000 neurones, mais aussi 50 000 fois le nombre de jetons de l'entrée.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "translatedText": "Voilà donc toute l'opération, deux produits matriciels, chacun avec un biais ajouté et une simple fonction d'écrêtage entre les deux.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "translatedText": "Tous ceux d'entre vous qui ont regardé les vidéos précédentes de la série reconnaîtront cette structure comme le type de réseau neuronal le plus basique que nous avons étudié là-bas.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "input": "In that example, it was trained to recognize handwritten digits.",
  "translatedText": "Dans cet exemple, il a été entraîné à reconnaître des chiffres manuscrits.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "translatedText": "Ici, dans le contexte d'un transformateur pour un grand modèle de langue, il s'agit d'une pièce d'une architecture plus vaste et toute tentative d'interprétation de ce qu'il fait exactement est fortement liée à l'idée d'encoder des informations dans des vecteurs d'un espace d'encastrement à haute dimension.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "translatedText": "C'est la leçon principale, mais je veux prendre du recul et réfléchir à deux choses différentes, la première étant une sorte de comptabilité, et la seconde impliquant un fait très stimulant sur les dimensions supérieures que je ne connaissais pas jusqu'à ce que je me penche sur les transformateurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "translatedText": "Dans les deux derniers chapitres, toi et moi avons commencé à compter le nombre total de paramètres dans GPT-3 et à voir exactement où ils se trouvent, alors terminons rapidement le jeu ici.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "translatedText": "J'ai déjà mentionné comment cette matrice de projection vers le haut a un peu moins de 50 000 lignes et que chaque ligne correspond à la taille de l'espace d'intégration, qui pour GPT-3 est de 12 288.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "translatedText": "En les multipliant, on obtient 604 millions de paramètres pour cette seule matrice, et la projection vers le bas a le même nombre de paramètres, mais avec une forme transposée.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "input": "So together, they give about 1.2 billion parameters.",
  "translatedText": "Ensemble, ils donnent donc environ 1,2 milliard de paramètres.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "translatedText": "Le vecteur de biais tient également compte de quelques paramètres supplémentaires, mais c'est une proportion triviale du total, alors je ne vais même pas le montrer.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "translatedText": "Dans GPT-3, cette séquence de vecteurs d'intégration passe par non pas un, mais 96 MLP distincts, de sorte que le nombre total de paramètres consacrés à tous ces blocs s'élève à environ 116 milliards.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "translatedText": "Cela représente environ deux tiers des paramètres totaux du réseau, et lorsque tu les ajoutes à tout ce que nous avions auparavant, pour les blocs d'attention, l'encastrement et le désencastrement, tu obtiens effectivement ce grand total de 175 milliards, comme cela a été annoncé.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "translatedText": "Il est probablement utile de mentionner qu'il existe un autre ensemble de paramètres associés à ces étapes de normalisation que cette explication n'a pas abordé, mais comme le vecteur de biais, ils représentent une proportion très insignifiante du total.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "translatedText": "En ce qui concerne le deuxième point de réflexion, tu te demandes peut-être si cet exemple central sur lequel nous avons passé tant de temps reflète la façon dont les faits sont réellement stockés dans les grands modèles de langage.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "translatedText": "Il est vrai que les lignes de cette première matrice peuvent être considérées comme des directions dans cet espace d'intégration, et cela signifie que l'activation de chaque neurone t'indique dans quelle mesure un vecteur donné s'aligne sur une direction spécifique.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "translatedText": "Il est également vrai que les colonnes de cette deuxième matrice t'indiquent ce qui sera ajouté au résultat si ce neurone est actif.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "input": "Both of those are just mathematical facts.",
  "translatedText": "Dans les deux cas, il s'agit simplement de faits mathématiques.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "translatedText": "Cependant, les preuves suggèrent que les neurones individuels représentent très rarement une seule caractéristique propre comme Michael Jordan, et il pourrait en fait y avoir une très bonne raison pour que ce soit le cas, liée à une idée qui flotte autour des chercheurs en interprétabilité ces jours-ci, connue sous le nom de superposition.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "translatedText": "C'est une hypothèse qui pourrait aider à expliquer à la fois pourquoi les modèles sont particulièrement difficiles à interpréter et pourquoi ils s'adaptent étonnamment bien.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "translatedText": "L'idée de base est que si tu as un espace à n dimensions et que tu veux représenter un tas de caractéristiques différentes en utilisant des directions qui sont toutes perpendiculaires les unes aux autres dans cet espace, tu sais, de façon à ce que si tu ajoutes un composant dans une direction, il n'influence aucune des autres directions, alors le nombre maximum de vecteurs que tu peux faire tenir est seulement n, le nombre de dimensions.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "translatedText": "Pour un mathématicien, en fait, c'est la définition de la dimension.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "translatedText": "Mais là où ça devient intéressant, c'est si tu relâches un peu cette contrainte et que tu tolères un peu de bruit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "translatedText": "Disons que tu permets à ces caractéristiques d'être représentées par des vecteurs qui ne sont pas exactement perpendiculaires, ils sont juste presque perpendiculaires, peut-être entre 89 et 91 degrés d'écart.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "input": "If we were in two or three dimensions, this makes no difference.",
  "translatedText": "Si nous étions en deux ou trois dimensions, cela ne fait aucune différence.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "translatedText": "Cela te laisse à peine une marge de manœuvre supplémentaire pour insérer plus de vecteurs, ce qui rend d'autant plus contre-intuitif le fait que pour des dimensions plus élevées, la réponse change radicalement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "translatedText": "Je peux te donner une illustration vraiment rapide et sale de ceci en utilisant un peu de Python qui va créer une liste de vecteurs à 100 dimensions, chacun initialisé de façon aléatoire, et cette liste va contenir 10 000 vecteurs distincts, donc 100 fois plus de vecteurs qu'il n'y a de dimensions.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "translatedText": "Ce graphique ici montre la distribution des angles entre les paires de ces vecteurs.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "translatedText": "Comme ils ont commencé au hasard, ces angles peuvent être compris entre 0 et 180 degrés, mais tu remarqueras que déjà, même pour les vecteurs aléatoires, il y a une forte tendance à ce que les choses soient plus proches de 90 degrés.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "translatedText": "Ensuite, je vais lancer un processus d'optimisation qui, de façon itérative, modifie tous ces vecteurs pour qu'ils deviennent plus perpendiculaires les uns par rapport aux autres.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "translatedText": "Après avoir répété cette opération plusieurs fois, voici à quoi ressemble la répartition des angles.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "translatedText": "Nous devons en fait faire un zoom ici parce que tous les angles possibles entre les paires de vecteurs se situent à l'intérieur de cette plage étroite entre 89 et 91 degrés.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "translatedText": "En général, une conséquence de ce qu'on appelle le lemme de Johnson-Lindenstrauss est que le nombre de vecteurs que tu peux entasser dans un espace et qui sont presque perpendiculaires comme celui-ci croît de façon exponentielle avec le nombre de dimensions.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "translatedText": "Ceci est très significatif pour les grands modèles de langue, qui pourraient bénéficier de l'association d'idées indépendantes avec des directions presque perpendiculaires.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "translatedText": "Cela signifie qu'il est possible de stocker beaucoup, beaucoup plus d'idées qu'il n'y a de dimensions dans l'espace qui lui est alloué.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "translatedText": "Cela pourrait expliquer en partie pourquoi la performance du modèle semble s'adapter si bien à la taille.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "translatedText": "Un espace qui a 10 fois plus de dimensions peut stocker beaucoup, beaucoup plus que 10 fois plus d'idées indépendantes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "translatedText": "Et cela ne concerne pas seulement cet espace d'intégration où vivent les vecteurs qui circulent dans le modèle, mais aussi ce vecteur plein de neurones au milieu de ce perceptron multicouche que nous venons d'étudier.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "translatedText": "En d'autres termes, aux dimensions du GPT-3, il pourrait ne pas se contenter de sonder 50 000 caractéristiques, mais s'il tirait parti de cette énorme capacité supplémentaire en utilisant des directions presque perpendiculaires de l'espace, il pourrait sonder beaucoup, beaucoup plus de caractéristiques du vecteur en cours de traitement.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "translatedText": "Mais si c'est le cas, cela signifie que les caractéristiques individuelles ne seront pas visibles sous la forme d'un seul neurone qui s'allume.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "translatedText": "Il faudrait qu'il ressemble plutôt à une combinaison spécifique de neurones, une superposition.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "translatedText": "Pour ceux d'entre vous qui sont curieux d'en savoir plus, un terme de recherche clé est sparse autoencoder, qui est un outil utilisé par certains spécialistes de l'interprétabilité pour essayer d'extraire les vraies caractéristiques, même si elles sont très superposées sur tous ces neurones.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "translatedText": "Je vais te donner un lien vers deux excellents articles sur l'anthropologie qui traitent de ce sujet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "translatedText": "À ce stade, nous n'avons pas abordé tous les détails d'un transformateur, mais toi et moi avons touché les points les plus importants.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "translatedText": "La principale chose que je veux aborder dans un prochain chapitre est le processus de formation.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "translatedText": "D'une part, la réponse courte à la question de savoir comment fonctionne la formation est qu'il s'agit de rétropropagation, et nous avons abordé la rétropagation dans un contexte distinct avec les chapitres précédents de la série.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "translatedText": "Mais il y a plus à discuter, comme la fonction de coût spécifique utilisée pour les modèles de langage, l'idée d'un réglage fin à l'aide de l'apprentissage par renforcement avec un retour d'information humain, et la notion de lois de mise à l'échelle.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "translatedText": "Petite note pour les adeptes actifs parmi vous, il y a un certain nombre de vidéos non liées à l'apprentissage machine que j'ai hâte de me mettre sous la dent avant de faire ce prochain chapitre, donc ça risque de prendre du temps, mais je promets que ça viendra en temps et en heure.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "input": "Thank you.",
  "translatedText": "Merci.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]