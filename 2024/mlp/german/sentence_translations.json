[
 {
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "translatedText": "Wenn du ein großes Sprachmodell mit dem Satz \"Michael Jordan spielt den Sport Blank\" fütterst und es vorhersagen lässt, was als Nächstes kommt, und es korrekt Basketball vorhersagt, würde das darauf hindeuten, dass es irgendwo in seinen Hunderten von Milliarden von Parametern Wissen über eine bestimmte Person und ihren speziellen Sport eingebaut hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "translatedText": "Und ich denke, dass jeder, der schon einmal mit einem dieser Modelle gespielt hat, das Gefühl hat, dass es sich tonnenweise Fakten gemerkt hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "translatedText": "Eine vernünftige Frage, die du dir stellen könntest, ist: Wie genau funktioniert das?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "input": "And where do those facts live?",
  "translatedText": "Und wo leben diese Fakten?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "translatedText": "Im Dezember letzten Jahres berichteten einige Forscher von Google DeepMind über ihre Arbeit zu dieser Frage und nutzten dabei das Beispiel der Zuordnung von Sportlern zu ihren Sportarten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "translatedText": "Und obwohl ein vollständiges mechanistisches Verständnis darüber, wie Fakten gespeichert werden, nach wie vor ungelöst ist, kamen sie zu einigen interessanten Teilergebnissen, einschließlich der sehr allgemeinen Schlussfolgerung, dass die Fakten in einem bestimmten Teil dieser Netze zu leben scheinen, die phantasievoll als mehrschichtige Perzeptrons oder kurz MLPs bezeichnet werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "translatedText": "In den letzten Kapiteln haben wir uns mit den Details der Transformatoren beschäftigt, der Architektur, die großen Sprachmodellen zugrunde liegt, und auch mit vielen anderen Aspekten der modernen KI.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "translatedText": "Im letzten Kapitel haben wir uns mit einem Stück namens Attention beschäftigt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "translatedText": "Der nächste Schritt für dich und mich ist es, die Details zu erforschen, was in diesen mehrschichtigen Perzeptronen passiert, die den anderen großen Teil des Netzwerks ausmachen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "translatedText": "Die Berechnung ist eigentlich relativ einfach, vor allem wenn du sie mit der Aufmerksamkeit vergleichst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "translatedText": "Im Grunde genommen handelt es sich um zwei Matrixmultiplikationen mit einem einfachen Etwas dazwischen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "translatedText": "Allerdings ist es äußerst schwierig zu verstehen, was diese Berechnungen bewirken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "translatedText": "Unser Hauptziel hier ist es, die Berechnungen durchzugehen und sie einprägsam zu machen, aber ich möchte ein konkretes Beispiel dafür zeigen, wie einer dieser Blöcke zumindest im Prinzip eine konkrete Tatsache speichern könnte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "translatedText": "Konkret geht es darum, die Tatsache zu speichern, dass Michael Jordan Basketball spielt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "translatedText": "Ich sollte erwähnen, dass das Layout hier von einem Gespräch inspiriert ist, das ich mit einem der DeepMind-Forscher, Neil Nanda, hatte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "translatedText": "In den meisten Fällen gehe ich davon aus, dass du entweder die letzten beiden Kapitel gesehen hast oder ein grundlegendes Gefühl dafür hast, was ein Transformator ist, aber eine Auffrischung kann nie schaden, also hier eine kurze Erinnerung an den allgemeinen Ablauf.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "translatedText": "Du und ich haben ein Modell untersucht, das darauf trainiert ist, einen Text aufzunehmen und vorherzusagen, was als nächstes kommt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "translatedText": "Der Eingabetext wird zunächst in eine Reihe von Token zerlegt, d.h. in kleine Stücke, die typischerweise Wörter oder kleine Wortteile sind, und jedes Token wird mit einem hochdimensionalen Vektor verknüpft, d.h. einer langen Liste von Zahlen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "translatedText": "Diese Sequenz von Vektoren durchläuft dann immer wieder zwei Arten von Operationen: die Aufmerksamkeit, die es den Vektoren ermöglicht, Informationen untereinander weiterzugeben, und dann die mehrschichtigen Perzeptronen, die wir heute genauer unter die Lupe nehmen werden, und dazwischen gibt es noch einen gewissen Normalisierungsschritt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "translatedText": "Nachdem die Sequenz von Vektoren viele, viele verschiedene Iterationen dieser beiden Blöcke durchlaufen hat, besteht die Hoffnung, dass jeder Vektor am Ende genug Informationen aus dem Kontext, allen anderen Wörtern in der Eingabe und auch aus dem allgemeinen Wissen, das durch das Training in die Modellgewichte eingeflossen ist, aufgesaugt hat, um eine Vorhersage über das nächste Token zu machen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "translatedText": "Eine der wichtigsten Ideen, die ich dir mit auf den Weg geben möchte, ist, dass all diese Vektoren in einem sehr, sehr hochdimensionalen Raum leben, und wenn du über diesen Raum nachdenkst, können verschiedene Richtungen verschiedene Arten von Bedeutung kodieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "translatedText": "Ein klassisches Beispiel, auf das ich immer wieder gerne zurückgreife, ist, dass man, wenn man die Einbettung von Frau betrachtet und die Einbettung von Mann abzieht, diesen kleinen Schritt macht und ihn zu einem anderen männlichen Substantiv hinzufügt, z. B. Onkel, dann landet man sehr, sehr nahe am entsprechenden weiblichen Substantiv.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "input": "In this sense, this particular direction encodes gender information.",
  "translatedText": "In diesem Sinne kodiert diese besondere Richtung Geschlechterinformationen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "translatedText": "Die Idee ist, dass viele andere Richtungen in diesem hochdimensionalen Raum anderen Merkmalen entsprechen könnten, die das Modell darstellen möchte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "translatedText": "In einem Transformator kodieren diese Vektoren aber nicht nur die Bedeutung eines einzelnen Wortes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "translatedText": "Während sie durch das Netzwerk fließen, erhalten sie eine viel reichhaltigere Bedeutung, die auf dem gesamten Kontext um sie herum und auch auf dem Wissen des Modells basiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "translatedText": "Letztlich muss jeder etwas kodieren, das weit über die Bedeutung eines einzelnen Wortes hinausgeht, denn es muss ausreichen, um vorherzusagen, was als Nächstes kommen wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "translatedText": "Wir haben bereits gesehen, wie du mit Hilfe von Aufmerksamkeitsblöcken Kontext einbeziehen kannst, aber ein Großteil der Modellparameter befindet sich in den MLP-Blöcken, und ein Gedanke ist, dass sie zusätzliche Kapazität zum Speichern von Fakten bieten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "translatedText": "Wie ich schon sagte, wird sich die Lektion hier auf das konkrete Spielzeugbeispiel konzentrieren, wie genau es die Tatsache speichern könnte, dass Michael Jordan Basketball spielt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "translatedText": "Für dieses Beispiel müssen wir ein paar Annahmen über den hochdimensionalen Raum treffen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "translatedText": "Zunächst nehmen wir an, dass eine der Richtungen die Vorstellung eines Vornamens Michael repräsentiert, eine andere, fast senkrechte Richtung steht für die Vorstellung des Nachnamens Jordan und eine dritte Richtung für die Vorstellung von Basketball.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "translatedText": "Was ich damit meine, ist, wenn du dir das Netzwerk ansiehst und einen der verarbeiteten Vektoren herauspickst und sein Punktprodukt mit dem Vornamen Michael eins ist, bedeutet das, dass der Vektor die Idee einer Person mit diesem Vornamen kodiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "translatedText": "Andernfalls wäre das Punktprodukt null oder negativ, was bedeutet, dass der Vektor nicht wirklich mit dieser Richtung übereinstimmt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "translatedText": "Und der Einfachheit halber lassen wir die sehr vernünftige Frage, was es bedeuten würde, wenn das Punktprodukt größer als eins wäre, völlig außer Acht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "translatedText": "Auch das Punktprodukt mit den anderen Richtungen würde dir sagen, ob es den Nachnamen Jordan oder Basketball darstellt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "translatedText": "Angenommen, ein Vektor soll den vollen Namen Michael Jordan repräsentieren, dann muss sein Punktprodukt mit beiden Richtungen eins sein.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "translatedText": "Da sich der Text Michael Jordan über zwei verschiedene Token erstreckt, bedeutet dies auch, dass wir davon ausgehen müssen, dass ein früherer Aufmerksamkeitsblock erfolgreich Informationen an den zweiten dieser beiden Vektoren weitergegeben hat, um sicherzustellen, dass er beide Namen kodieren kann.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "translatedText": "Mit all diesen Voraussetzungen können wir nun zum Kern der Lektion vordringen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "input": "What happens inside a multilayer perceptron?",
  "translatedText": "Was passiert im Inneren eines mehrschichtigen Perzeptrons?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "translatedText": "Du kannst dir vorstellen, dass diese Folge von Vektoren in den Block fließt, und denk daran, dass jeder Vektor ursprünglich mit einem der Token aus dem Eingabetext verbunden war.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "translatedText": "Was passieren wird, ist, dass jeder einzelne Vektor aus dieser Sequenz eine kurze Reihe von Operationen durchläuft, die wir gleich auspacken werden, und am Ende erhalten wir einen weiteren Vektor mit derselben Dimension.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "translatedText": "Der andere Vektor wird zu dem ursprünglichen Vektor addiert, der hineingeflossen ist, und diese Summe ist das Ergebnis, das herausfließt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "translatedText": "Diese Abfolge von Operationen wendest du auf jeden Vektor in der Sequenz an, der mit jedem Token in der Eingabe verbunden ist, und das alles geschieht parallel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "translatedText": "Vor allem reden die Vektoren in diesem Schritt nicht miteinander, sondern machen alle irgendwie ihr eigenes Ding.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "translatedText": "Das macht es für dich und mich viel einfacher, denn wenn wir verstehen, was mit einem der Vektoren in diesem Block passiert, verstehen wir auch, was mit allen Vektoren passiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "translatedText": "Wenn ich sage, dass dieser Block die Tatsache kodiert, dass Michael Jordan Basketball spielt, dann meine ich damit, dass, wenn ein Vektor einfließt, der den Vornamen Michael und den Nachnamen Jordan kodiert, diese Sequenz von Berechnungen etwas erzeugen wird, das die Richtung Basketball enthält, was dem Vektor an dieser Stelle hinzugefügt wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "translatedText": "Der erste Schritt dieses Prozesses sieht so aus, dass dieser Vektor mit einer sehr großen Matrix multipliziert wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "input": "No surprises there, this is deep learning.",
  "translatedText": "Das ist keine Überraschung, das ist Deep Learning.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "translatedText": "Und diese Matrix ist, wie alle anderen, die wir gesehen haben, mit Modellparametern gefüllt, die aus den Daten gelernt wurden und die du dir als eine Reihe von Knöpfen und Reglern vorstellen kannst, die eingestellt werden, um das Verhalten des Modells zu bestimmen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "translatedText": "Eine schöne Art, sich die Matrixmultiplikation vorzustellen, ist, sich jede Zeile der Matrix als eigenen Vektor vorzustellen und eine Reihe von Punktprodukten zwischen diesen Zeilen und dem zu verarbeitenden Vektor zu bilden, den ich mit E für Embedding bezeichne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "translatedText": "Nehmen wir zum Beispiel an, dass die erste Zeile zufällig mit dem Vornamen Michael übereinstimmt, von dem wir annehmen, dass er existiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "translatedText": "Das würde bedeuten, dass die erste Komponente in dieser Ausgabe, dieses Punktprodukt hier, eins ist, wenn der Vektor den Vornamen Michael kodiert, und ansonsten null oder negativ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "translatedText": "Noch mehr Spaß macht es, wenn du dir überlegst, was es bedeuten würde, wenn die erste Reihe aus dem Vornamen Michael und dem Nachnamen Jordan bestehen würde.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "translatedText": "Der Einfachheit halber schreibe ich das mal als M plus J auf.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "translatedText": "Wenn du dann ein Punktprodukt mit dieser Einbettung E bildest, verteilen sich die Dinge sehr schön, so dass es wie M Punkt E plus J Punkt E aussieht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "translatedText": "Das bedeutet, dass der endgültige Wert zwei ist, wenn der Vektor den vollen Namen Michael Jordan kodiert, und ansonsten wäre er eins oder etwas kleiner als eins.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "input": "And that's just one row in this matrix.",
  "translatedText": "Und das ist nur eine Zeile in dieser Matrix.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "translatedText": "Du könntest dir vorstellen, dass alle anderen Zeilen parallel dazu andere Fragen stellen und andere Merkmale des zu verarbeitenden Vektors untersuchen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "translatedText": "Sehr oft wird bei diesem Schritt auch ein weiterer Vektor zur Ausgabe hinzugefügt, der die aus den Daten gelernten Modellparameter enthält.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "input": "This other vector is known as the bias.",
  "translatedText": "Dieser andere Vektor wird als Bias bezeichnet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "translatedText": "Für unser Beispiel möchte ich, dass du dir vorstellst, dass der Wert dieser Verzerrung in der allerersten Komponente negativ ist, d.h. unsere endgültige Ausgabe sieht aus wie das relevante Punktprodukt, aber minus eins.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "translatedText": "Du wirst dich vielleicht fragen, warum ich möchte, dass du davon ausgehst, dass das Modell dies gelernt hat. Gleich wirst du sehen, warum es sehr sauber und schön ist, wenn wir hier einen Wert haben, der nur dann positiv ist, wenn ein Vektor den vollen Namen Michael Jordan kodiert, und ansonsten null oder negativ ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "translatedText": "Die Gesamtzahl der Zeilen in dieser Matrix, die in etwa der Anzahl der gestellten Fragen entspricht, beträgt im Fall des GPT-3, dessen Zahlen wir verfolgt haben, knapp 50.000.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "translatedText": "Tatsächlich ist sie genau viermal so groß wie die Anzahl der Dimensionen in diesem Einbettungsraum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "input": "That's a design choice.",
  "translatedText": "Das ist eine Design-Entscheidung.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "translatedText": "Du kannst es mehr oder weniger machen, aber ein sauberes Multiple ist in der Regel günstiger für die Hardware.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "translatedText": "Da diese Matrix voller Gewichte uns in einen höherdimensionalen Raum abbildet, gebe ich ihr die Kurzform W up.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "translatedText": "Ich beschrifte den Vektor, den wir bearbeiten, weiterhin mit E, und diesen Vorspannungsvektor beschriften wir mit B und tragen das alles wieder in das Diagramm ein.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "translatedText": "Ein Problem dabei ist, dass dieser Vorgang rein linear ist, Sprache aber ein sehr nicht-linearer Prozess ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "translatedText": "Wenn der Eintritt, den wir messen, für Michael plus Jordan hoch ist, wird er zwangsläufig auch durch Michael plus Phelps und Alexis plus Jordan ausgelöst, auch wenn diese konzeptionell nicht miteinander verbunden sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "input": "What you really want is a simple yes or no for the full name.",
  "translatedText": "Was du wirklich willst, ist ein einfaches Ja oder Nein für den vollständigen Namen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "translatedText": "Der nächste Schritt besteht also darin, diesen großen Zwischenvektor durch eine sehr einfache nichtlineare Funktion zu leiten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "translatedText": "Eine gängige Wahl ist die, die alle negativen Werte auf Null setzt und alle positiven Werte unverändert lässt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "translatedText": "Und um der Tradition des Deep Learning mit allzu ausgefallenen Namen gerecht zu werden, wird diese sehr einfache Funktion oft als gleichgerichtete lineare Einheit oder kurz ReLU bezeichnet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "input": "Here's what the graph looks like.",
  "translatedText": "So sieht das Diagramm aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "translatedText": "In unserem Beispiel, in dem der erste Eintrag des Zwischenvektors eine Eins ist, wenn der vollständige Name Michael Jordan lautet, und ansonsten eine Null oder ein negativer Wert, erhältst du nach dem Durchlaufen der ReLU einen sehr sauberen Wert, bei dem alle Nullen und negativen Werte einfach auf Null gekappt werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "translatedText": "Die Ausgabe wäre also eins für den vollen Namen Michael Jordan und sonst null.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "translatedText": "Mit anderen Worten: Es ahmt das Verhalten eines UND-Gatters sehr direkt nach.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "translatedText": "Oft verwenden Modelle eine leicht abgewandelte Funktion namens JLU, die dieselbe Grundform hat, nur etwas glatter ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "translatedText": "Aber für unsere Zwecke ist es ein bisschen sauberer, wenn wir nur an die ReLU denken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "translatedText": "Wenn du von den Neuronen eines Transformators sprichst, meinst du diese Werte hier.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "translatedText": "Wenn du das übliche Bild eines neuronalen Netzwerks mit einer Schicht aus Punkten und einem Bündel von Linien siehst, die mit der vorherigen Schicht verbunden sind, wie wir es in dieser Reihe bereits hatten, dann soll das normalerweise die Kombination aus einem linearen Schritt, einer Matrixmultiplikation, gefolgt von einer einfachen, nichtlinearen Funktion wie einer ReLU darstellen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "translatedText": "Du würdest sagen, dass dieses Neuron aktiv ist, wenn dieser Wert positiv ist, und dass es inaktiv ist, wenn dieser Wert Null ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "input": "The next step looks very similar to the first one.",
  "translatedText": "Der nächste Schritt sieht dem ersten sehr ähnlich.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "translatedText": "Du multiplizierst mit einer sehr großen Matrix und fügst einen bestimmten Bias-Term hinzu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "translatedText": "In diesem Fall ist die Anzahl der Dimensionen in der Ausgabe wieder auf die Größe des Einbettungsraums reduziert, also nenne ich das hier die Abwärtsprojektionsmatrix.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "translatedText": "Und dieses Mal ist es schöner, die Dinge nicht Zeile für Zeile, sondern Spalte für Spalte zu betrachten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "translatedText": "Eine andere Möglichkeit, die Matrixmultiplikation im Kopf zu behalten, besteht darin, dir vorzustellen, dass du jede Spalte der Matrix mit dem entsprechenden Term des Vektors multiplizierst, den sie verarbeitet, und dann alle umskalierten Spalten addierst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "translatedText": "Der Grund dafür, dass diese Denkweise schöner ist, liegt darin, dass die Spalten hier die gleiche Dimension wie der Einbettungsraum haben, sodass wir sie als Richtungen in diesem Raum betrachten können.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "translatedText": "Wir stellen uns zum Beispiel vor, dass das Modell gelernt hat, die erste Spalte in die von uns angenommene Basketballrichtung zu bringen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "translatedText": "Das würde bedeuten, dass wir diese Spalte zum Endergebnis hinzufügen, wenn das entsprechende Neuron an dieser ersten Position aktiv ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "translatedText": "Aber wenn das Neuron inaktiv wäre, wenn die Zahl Null wäre, dann hätte das keinen Effekt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "input": "And it doesn't just have to be basketball.",
  "translatedText": "Und das muss nicht nur beim Basketball sein.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "translatedText": "Das Modell könnte auch in diese Spalte und viele andere Merkmale einbacken, die es mit etwas verbinden will, das den vollen Namen Michael Jordan trägt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "translatedText": "Und gleichzeitig sagen dir alle anderen Spalten in dieser Matrix, was zum Endergebnis hinzugefügt wird, wenn das entsprechende Neuron aktiv ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "translatedText": "Und wenn du in diesem Fall eine Verzerrung hast, ist das etwas, das du jedes Mal hinzufügst, unabhängig von den Neuronenwerten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "input": "You might wonder what's that doing.",
  "translatedText": "Du fragst dich vielleicht, was das soll.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "translatedText": "Wie bei allen Objekten, die mit Parametern gefüllt sind, ist es schwer, das genau zu sagen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "translatedText": "Vielleicht muss das Netzwerk ein paar Buchhaltungsaufgaben erledigen, aber das kannst du erst einmal ignorieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "translatedText": "Um unsere Notation wieder etwas kompakter zu machen, nenne ich diese große Matrix W unten und nenne den Bias-Vektor B unten und füge ihn wieder in unser Diagramm ein.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "translatedText": "Wie ich vorhin schon angedeutet habe, fügst du das Endergebnis zu dem Vektor hinzu, der an dieser Stelle in den Block geflossen ist, und so erhältst du das Endergebnis.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "translatedText": "Wenn zum Beispiel der einfließende Vektor sowohl den Vornamen Michael als auch den Nachnamen Jordan kodiert, dann wird diese Folge von Operationen das UND-Gatter auslösen und die Richtung des Basketballs addieren, so dass das, was herauskommt, alles zusammen kodiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "translatedText": "Und denk daran, dass dieser Prozess mit jedem einzelnen dieser Vektoren parallel abläuft.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "translatedText": "Nimmt man die GPT-3-Zahlen, bedeutet das, dass dieser Block nicht nur 50.000 Neuronen enthält, sondern auch die 50.000-fache Anzahl von Token in der Eingabe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "translatedText": "Das ist also die gesamte Operation: zwei Matrixprodukte, zu denen jeweils eine Vorspannung hinzugefügt wird, und dazwischen eine einfache Clipping-Funktion.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "translatedText": "Diejenigen von euch, die die früheren Videos der Reihe gesehen haben, werden diese Struktur als die grundlegendste Art von neuronalem Netz erkennen, die wir dort untersucht haben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "input": "In that example, it was trained to recognize handwritten digits.",
  "translatedText": "In diesem Beispiel wurde es darauf trainiert, handgeschriebene Ziffern zu erkennen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "translatedText": "Hier, im Kontext eines Transformators für ein großes Sprachmodell, ist dies ein Teil einer größeren Architektur, und jeder Versuch, zu interpretieren, was genau er tut, ist stark mit der Idee der Kodierung von Informationen in Vektoren eines hochdimensionalen Einbettungsraums verwoben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "translatedText": "Das ist die wichtigste Lektion, aber ich möchte einen Schritt zurücktreten und über zwei verschiedene Dinge nachdenken. Das erste ist eine Art Buchhaltung und das zweite beinhaltet eine sehr nachdenklich stimmende Tatsache über höhere Dimensionen, die ich nicht kannte, bis ich mich mit Transformatoren beschäftigte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "translatedText": "In den letzten beiden Kapiteln haben du und ich angefangen, die Gesamtzahl der Parameter in GPT-3 zu zählen und genau zu sehen, wo sie sich befinden, also lass uns das Spiel hier schnell beenden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "translatedText": "Ich habe bereits erwähnt, dass diese Projektionsmatrix knapp 50.000 Zeilen hat und dass jede Zeile der Größe des Einbettungsraums entspricht, der für GPT-3 12.288 beträgt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "translatedText": "Multipliziert man diese Werte miteinander, erhält man 604 Millionen Parameter allein für diese Matrix, und die Abwärtsprojektion hat die gleiche Anzahl von Parametern, nur mit einer transponierten Form.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "input": "So together, they give about 1.2 billion parameters.",
  "translatedText": "Zusammen ergeben sie also etwa 1,2 Milliarden Parameter.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "translatedText": "Der Bias-Vektor berücksichtigt noch ein paar weitere Parameter, aber das ist ein trivialer Anteil an der Gesamtsumme, deshalb zeige ich ihn gar nicht erst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "translatedText": "In GPT-3 durchläuft diese Sequenz von Einbettungsvektoren nicht nur eine, sondern 96 verschiedene MLPs, so dass sich die Gesamtzahl der Parameter für alle diese Blöcke auf etwa 116 Milliarden beläuft.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "translatedText": "Das sind etwa 2 Drittel der gesamten Parameter im Netzwerk, und wenn du sie zu den Aufmerksamkeitsblöcken, der Einbettung und der Entbettung addierst, erhältst du tatsächlich die angekündigte Gesamtsumme von 175 Milliarden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "translatedText": "Es ist wahrscheinlich erwähnenswert, dass es eine weitere Gruppe von Parametern gibt, die mit diesen Normalisierungsschritten verbunden sind und die in dieser Erklärung übersprungen wurden, aber genau wie der Bias-Vektor machen sie nur einen sehr geringen Teil der Gesamtmenge aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "translatedText": "Was den zweiten Punkt betrifft, so fragst du dich vielleicht, ob das zentrale Spielzeugbeispiel, auf das wir so viel Zeit verwendet haben, widerspiegelt, wie Fakten in echten großen Sprachmodellen tatsächlich gespeichert werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "translatedText": "Es stimmt, dass die Zeilen dieser ersten Matrix als Richtungen in diesem Einbettungsraum betrachtet werden können, und das bedeutet, dass die Aktivierung jedes Neurons angibt, wie sehr ein bestimmter Vektor mit einer bestimmten Richtung übereinstimmt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "translatedText": "Es stimmt auch, dass die Spalten dieser zweiten Matrix dir sagen, was zu dem Ergebnis hinzugefügt wird, wenn das Neuron aktiv ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "input": "Both of those are just mathematical facts.",
  "translatedText": "Beides sind nur mathematische Fakten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "translatedText": "Es gibt jedoch Hinweise darauf, dass einzelne Neuronen nur sehr selten ein einzelnes sauberes Merkmal wie Michael Jordan repräsentieren, und es könnte tatsächlich einen sehr guten Grund dafür geben, der mit einer Idee zusammenhängt, die heutzutage unter Interpretationsforschern als Superposition bekannt ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "translatedText": "Diese Hypothese könnte erklären, warum die Modelle besonders schwer zu interpretieren sind und warum sie erstaunlich gut skalieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "translatedText": "Die Grundidee ist: Wenn du einen n-dimensionalen Raum hast und eine Reihe von verschiedenen Merkmalen durch Richtungen darstellen willst, die alle senkrecht zueinander in diesem Raum stehen, so dass eine Komponente in einer Richtung keinen Einfluss auf die anderen Richtungen hat, dann ist die maximale Anzahl der Vektoren, die du unterbringen kannst, nur n, die Anzahl der Dimensionen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "translatedText": "Für einen Mathematiker ist das die Definition von Dimension.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "translatedText": "Interessant wird es aber, wenn du diese Einschränkung ein wenig lockerst und etwas Lärm tolerierst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "translatedText": "Angenommen, du erlaubst, dass diese Merkmale durch Vektoren dargestellt werden, die nicht genau senkrecht sind, sondern nur fast senkrecht, vielleicht zwischen 89 und 91 Grad auseinander.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "input": "If we were in two or three dimensions, this makes no difference.",
  "translatedText": "Wenn wir uns in zwei oder drei Dimensionen befinden, macht das keinen Unterschied.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "translatedText": "Das gibt dir kaum zusätzlichen Spielraum, um mehr Vektoren einzubauen. Umso erstaunlicher ist es, dass sich die Antwort für höhere Dimensionen dramatisch ändert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "translatedText": "Ich kann dir eine schnelle und schmutzige Veranschaulichung geben, indem ich eine Liste von 100-dimensionalen Vektoren erstelle, die alle zufällig initialisiert werden. Diese Liste wird 10.000 verschiedene Vektoren enthalten, also 100 Mal so viele Vektoren wie es Dimensionen gibt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "translatedText": "Diese Grafik hier zeigt die Verteilung der Winkel zwischen den Paaren dieser Vektoren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "translatedText": "Da sie mit einem Zufallsvektor begonnen haben, können diese Winkel zwischen 0 und 180 Grad liegen, aber du wirst feststellen, dass schon bei den Zufallsvektoren eine starke Tendenz besteht, dass die Winkel näher an 90 Grad liegen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "translatedText": "Dann führe ich einen bestimmten Optimierungsprozess durch, der all diese Vektoren iterativ so verschiebt, dass sie möglichst senkrecht zueinander stehen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "translatedText": "Nachdem du dies viele Male wiederholt hast, sieht die Verteilung der Winkel folgendermaßen aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "translatedText": "Wir müssen hier tatsächlich heranzoomen, weil alle möglichen Winkel zwischen Vektorenpaaren in diesem engen Bereich zwischen 89 und 91 Grad liegen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "translatedText": "Das Johnson-Lindenstrauss-Lemma besagt, dass die Anzahl der Vektoren, die man in einem Raum unterbringen kann und die nahezu senkrecht zueinander stehen, exponentiell mit der Anzahl der Dimensionen wächst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "translatedText": "Dies ist sehr wichtig für große Sprachmodelle, die davon profitieren können, dass unabhängige Ideen mit fast senkrechten Richtungen verbunden werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "translatedText": "Das bedeutet, dass es möglich ist, viel, viel mehr Ideen zu speichern, als es Dimensionen in dem ihm zugewiesenen Raum gibt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "translatedText": "Das könnte teilweise erklären, warum die Leistung des Modells so gut mit der Größe zu skalieren scheint.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "translatedText": "Ein Raum, der 10 Mal so viele Dimensionen hat, kann viel, viel mehr als 10 Mal so viele unabhängige Ideen speichern.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "translatedText": "Und das gilt nicht nur für den Einbettungsraum, in dem die Vektoren leben, die durch das Modell fließen, sondern auch für den Vektor voller Neuronen in der Mitte des mehrschichtigen Perzeptrons, das wir gerade untersucht haben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "translatedText": "Das heißt, bei der Größe von GPT-3 könnte es nicht nur 50.000 Merkmale untersuchen, sondern wenn es stattdessen diese enorme zusätzliche Kapazität nutzen würde, indem es fast senkrechte Richtungen des Raums verwendet, könnte es viel, viel mehr Merkmale des zu verarbeitenden Vektors untersuchen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "translatedText": "Aber wenn das der Fall wäre, würde das bedeuten, dass einzelne Merkmale nicht als ein einzelnes aufleuchtendes Neuron sichtbar sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "translatedText": "Stattdessen müsste es wie eine bestimmte Kombination von Neuronen aussehen, eine Überlagerung.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "translatedText": "Für alle, die mehr wissen wollen: Ein wichtiger Suchbegriff ist \"Sparse Autoencoder\", ein Werkzeug, mit dem einige Leute, die sich mit der Interpretierbarkeit beschäftigen, versuchen, die wahren Merkmale zu extrahieren, auch wenn sie von all diesen Neuronen überlagert werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "translatedText": "Ich verlinke mal ein paar wirklich tolle Anthropic-Beiträge zu diesem Thema.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "translatedText": "An dieser Stelle haben wir noch nicht alle Details eines Transformators behandelt, aber wir haben die wichtigsten Punkte angesprochen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "translatedText": "Das Wichtigste, das ich in einem nächsten Kapitel behandeln möchte, ist der Ausbildungsprozess.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "translatedText": "Die kurze Antwort auf die Frage, wie das Training funktioniert, ist, dass es sich dabei um Backpropagation handelt, und wir haben die Backpropagation in einem anderen Zusammenhang in früheren Kapiteln der Reihe behandelt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "translatedText": "Aber es gibt noch mehr zu besprechen, z. B. die spezifische Kostenfunktion, die für Sprachmodelle verwendet wird, die Idee der Feinabstimmung durch Verstärkungslernen mit menschlichem Feedback und das Konzept der Skalierungsgesetze.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "translatedText": "Kurzer Hinweis für die aktiven Follower unter euch: Es gibt eine Reihe von Videos, die nichts mit maschinellem Lernen zu tun haben und in die ich mich noch vertiefen möchte, bevor ich das nächste Kapitel schreibe. Es könnte also noch eine Weile dauern, aber ich verspreche, dass es zu gegebener Zeit kommen wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "input": "Thank you.",
  "translatedText": "Vielen Dank!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]