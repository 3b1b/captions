1
00:00:00,000 --> 00:00:04,692
Якщо ви дасте великій мовній моделі фразу "Майкл Джордан грає в бейсбол" 

2
00:00:04,692 --> 00:00:09,963
і попросите її передбачити, що буде далі, і вона правильно передбачить баскетбол, 

3
00:00:09,963 --> 00:00:14,720
це означатиме, що десь всередині її сотень мільярдів параметрів закладено 

4
00:00:14,720 --> 00:00:18,320
знання про конкретну людину та її конкретний вид спорту.

5
00:00:18,940 --> 00:00:22,060
І я думаю, що кожен, хто бавився з однією з цих моделей, 

6
00:00:22,060 --> 00:00:25,400
має чітке відчуття, що він запам'ятовує тонни і тонни фактів.

7
00:00:25,700 --> 00:00:29,160
Тож резонне питання, яке ви можете поставити, - як саме це працює?

8
00:00:29,160 --> 00:00:31,040
І де живуть ці факти?

9
00:00:35,720 --> 00:00:38,794
У грудні минулого року кілька дослідників з Google DeepMind опублікували 

10
00:00:38,794 --> 00:00:41,700
повідомлення про роботу над цим питанням, і вони використовували цей 

11
00:00:41,700 --> 00:00:44,480
конкретний приклад зіставлення спортсменів з їхніми видами спорту.

12
00:00:44,900 --> 00:00:49,812
І хоча повне механістичне розуміння того, як зберігаються факти, залишається невирішеним, 

13
00:00:49,812 --> 00:00:52,432
вони отримали деякі цікаві часткові результати, 

14
00:00:52,432 --> 00:00:56,253
включаючи дуже загальний висновок високого рівня, що факти, здається, 

15
00:00:56,253 --> 00:01:00,893
живуть всередині певної частини цих мереж, відомих під химерною назвою багатошарових 

16
00:01:00,893 --> 00:01:02,640
перцептронів, або скорочено MLP.

17
00:01:03,120 --> 00:01:07,699
В останніх двох розділах ми з вами розглянули деталі трансформаторів, архітектуру, 

18
00:01:07,699 --> 00:01:12,500
що лежить в основі великих мовних моделей, а також в основі багатьох інших сучасних ШІ.

19
00:01:13,060 --> 00:01:16,200
В останньому розділі ми зосередилися на творі під назвою "Увага".

20
00:01:16,840 --> 00:01:19,966
І наступний крок для нас з вами - заглибитися в деталі того, 

21
00:01:19,966 --> 00:01:22,938
що відбувається всередині цих багатошарових перцептронів, 

22
00:01:22,938 --> 00:01:25,040
які складають іншу велику частину мережі.

23
00:01:25,680 --> 00:01:30,100
Обчислення тут насправді відносно прості, особливо якщо порівнювати їх з увагою.

24
00:01:30,560 --> 00:01:34,980
По суті, це зводиться до пари матричних множень з простим чимось середнім між ними.

25
00:01:35,720 --> 00:01:40,460
Однак інтерпретувати те, що роблять ці обчислення, надзвичайно складно.

26
00:01:41,560 --> 00:01:44,908
Наша головна мета тут - пройти через обчислення і зробити їх такими, 

27
00:01:44,908 --> 00:01:48,694
що запам'ятовуються, але я хотів би зробити це в контексті показу конкретного 

28
00:01:48,694 --> 00:01:51,898
прикладу того, як один з цих блоків міг би, принаймні в принципі, 

29
00:01:51,898 --> 00:01:53,160
зберігати конкретний факт.

30
00:01:53,580 --> 00:01:57,080
Зокрема, він зберігатиме інформацію про те, що Майкл Джордан грає в баскетбол.

31
00:01:58,080 --> 00:02:00,469
Маю зазначити, що макет тут натхненний розмовою, 

32
00:02:00,469 --> 00:02:03,200
яку я мав з одним із дослідників DeepMind, Нілом Нандою.

33
00:02:04,060 --> 00:02:07,345
Здебільшого я припускаю, що ви або дивилися попередні два розділи, 

34
00:02:07,345 --> 00:02:10,139
або маєте базове уявлення про те, що таке трансформатор, 

35
00:02:10,139 --> 00:02:13,719
але освіжити пам'ять ніколи не завадить, тож ось коротке нагадування про 

36
00:02:13,719 --> 00:02:14,700
загальний хід подій.

37
00:02:15,340 --> 00:02:21,320
Ми з вами вивчали модель, яка навчена сприймати шматок тексту і передбачати, що буде далі.

38
00:02:21,720 --> 00:02:26,689
Вхідний текст спочатку розбивається на купу токенів, тобто невеликих фрагментів, 

39
00:02:26,689 --> 00:02:30,064
які зазвичай є словами або невеликими шматочками слів, 

40
00:02:30,064 --> 00:02:35,280
і кожен токен асоціюється з вектором високої розмірності, тобто довгим списком чисел.

41
00:02:35,840 --> 00:02:41,389
Потім ця послідовність векторів багаторазово проходить через два види операцій - увагу, 

42
00:02:41,389 --> 00:02:44,984
яка дозволяє векторам передавати інформацію один одному, 

43
00:02:44,984 --> 00:02:49,525
а потім багатошарові перцептрони, те, що ми сьогодні будемо розглядати, 

44
00:02:49,525 --> 00:02:52,300
а також між ними є певний етап нормалізації.

45
00:02:53,300 --> 00:02:57,024
Після того, як послідовність векторів пройшла через багато, 

46
00:02:57,024 --> 00:03:00,687
багато різних ітерацій обох цих блоків, можна сподіватися, 

47
00:03:00,687 --> 00:03:04,535
що кожен вектор ввібрав достатньо інформації, як з контексту, 

48
00:03:04,535 --> 00:03:07,701
всіх інших слів на вході, так і з загальних знань, 

49
00:03:07,701 --> 00:03:10,929
які були закладені у вагах моделі під час навчання, 

50
00:03:10,929 --> 00:03:16,020
щоб його можна було використати для прогнозування того, який токен буде наступним.

51
00:03:16,860 --> 00:03:20,711
Одна з ключових ідей, яку я хочу, щоб ви запам'ятали, полягає в тому, 

52
00:03:20,711 --> 00:03:24,233
що всі ці вектори живуть у дуже, дуже багатовимірному просторі, 

53
00:03:24,233 --> 00:03:28,800
і коли ви думаєте про цей простір, різні напрямки можуть кодувати різні види сенсу.

54
00:03:30,120 --> 00:03:33,344
Дуже класичний приклад, до якого я люблю повертатися, - це те, 

55
00:03:33,344 --> 00:03:37,386
що якщо ви подивитеся на вбудованість жінки і віднімете вбудованість чоловіка, 

56
00:03:37,386 --> 00:03:41,378
зробите цей маленький крок і додасте його до іншого іменника чоловічого роду, 

57
00:03:41,378 --> 00:03:45,523
наприклад, дядько, ви опинитеся десь дуже, дуже близько до відповідного іменника 

58
00:03:45,523 --> 00:03:46,240
жіночого роду.

59
00:03:46,440 --> 00:03:50,880
У цьому сенсі саме цей напрямок кодує гендерну інформацію.

60
00:03:51,640 --> 00:03:55,762
Ідея полягає в тому, що багато інших чітких напрямків у цьому надвимірному просторі 

61
00:03:55,762 --> 00:03:59,640
можуть відповідати іншим особливостям, які модель, можливо, захоче відобразити.

62
00:04:01,400 --> 00:04:06,180
Однак у трансформаторі ці вектори не просто кодують значення одного слова.

63
00:04:06,680 --> 00:04:11,321
Коли вони протікають через мережу, вони вбирають в себе набагато багатше значення, 

64
00:04:11,321 --> 00:04:15,180
засноване на всьому контексті навколо них, а також на знаннях моделі.

65
00:04:15,880 --> 00:04:20,072
Зрештою, кожне з них має кодувати щось набагато більше, ніж значення одного слова, 

66
00:04:20,072 --> 00:04:23,760
оскільки воно має бути достатнім для того, щоб передбачити, що буде далі.

67
00:04:24,560 --> 00:04:27,845
Ми вже бачили, як блоки уваги дозволяють включати контекст, 

68
00:04:27,845 --> 00:04:31,733
але більшість параметрів моделі насправді живуть всередині блоків MLP, 

69
00:04:31,733 --> 00:04:35,073
і одна з версій того, що вони можуть робити, полягає в тому, 

70
00:04:35,073 --> 00:04:38,140
що вони надають додаткову ємність для зберігання фактів.

71
00:04:38,720 --> 00:04:42,660
Як я вже казав, урок буде зосереджений на конкретному прикладі іграшки і на тому, 

72
00:04:42,660 --> 00:04:46,120
як саме вона може зберігати той факт, що Майкл Джордан грає в баскетбол.

73
00:04:47,120 --> 00:04:49,596
Цей приклад з іграшкою вимагатиме від нас з вами зробити 

74
00:04:49,596 --> 00:04:51,900
кілька припущень щодо цього багатовимірного простору.

75
00:04:52,360 --> 00:04:57,301
Спочатку припустимо, що один з напрямків представляє ідею імені Майкл, 

76
00:04:57,301 --> 00:05:02,800
потім інший майже перпендикулярний напрямок представляє ідею прізвища Джордан, 

77
00:05:02,800 --> 00:05:06,420
а третій напрямок буде представляти ідею баскетболу.

78
00:05:07,400 --> 00:05:12,425
Конкретно я маю на увазі, що якщо ви подивитеся в мережі і витягнете один 

79
00:05:12,425 --> 00:05:18,401
з оброблюваних векторів, якщо його точковий добуток з цим ім'ям Майкл дорівнює одиниці, 

80
00:05:18,401 --> 00:05:22,340
це буде означати, що вектор кодує ідею людини з цим ім'ям.

81
00:05:23,800 --> 00:05:26,209
Інакше цей точковий добуток був би нульовим або від'ємним, 

82
00:05:26,209 --> 00:05:28,700
що означає, що вектор насправді не співпадає з цим напрямком.

83
00:05:29,420 --> 00:05:32,448
І для простоти, давайте повністю проігноруємо цілком резонне питання про те, 

84
00:05:32,448 --> 00:05:35,320
що це могло б означати, якби цей точковий продукт був більшим за одиницю.

85
00:05:36,200 --> 00:05:40,711
Аналогічно, його крапковий продукт із цими іншими напрямками підкаже вам, 

86
00:05:40,711 --> 00:05:43,760
чи представляє він прізвище Джордан, чи баскетбол.

87
00:05:44,740 --> 00:05:48,486
Отже, скажімо, вектор має представляти повне ім'я Майкла Джордана, 

88
00:05:48,486 --> 00:05:52,680
тоді його точковий добуток з обома цими напрямками має дорівнювати одиниці.

89
00:05:53,480 --> 00:05:57,837
Оскільки текст Michael Jordan охоплює дві різні лексеми, це також означає, 

90
00:05:57,837 --> 00:06:02,369
що ми повинні припустити, що попередній блок уваги успішно передав інформацію 

91
00:06:02,369 --> 00:06:06,960
до другого з цих двох векторів, щоб забезпечити можливість кодування обох імен.

92
00:06:07,940 --> 00:06:11,480
З усіма цими припущеннями, давайте зануримося в суть уроку.

93
00:06:11,880 --> 00:06:14,980
Що відбувається всередині багатошарового персептрона?

94
00:06:17,100 --> 00:06:21,633
Ви можете уявити собі цю послідовність векторів, що вливаються у блок, і пам'ятайте, 

95
00:06:21,633 --> 00:06:25,580
що кожен вектор спочатку був пов'язаний з однією з лексем вхідного тексту.

96
00:06:26,080 --> 00:06:31,220
Кожен окремий вектор з цієї послідовності проходить через коротку серію операцій, 

97
00:06:31,220 --> 00:06:36,360
ми розпаковуємо їх за мить, і в кінці отримуємо інший вектор з тією ж розмірністю.

98
00:06:36,880 --> 00:06:39,818
Цей інший вектор буде додано до початкового вектора, 

99
00:06:39,818 --> 00:06:43,200
який увійшов, і ця сума буде результатом, який вийде назовні.

100
00:06:43,720 --> 00:06:48,081
Ця послідовність операцій - це те, що ви застосовуєте до кожного вектора в послідовності, 

101
00:06:48,081 --> 00:06:51,620
пов'язаного з кожною лексемою на вході, і все це відбувається паралельно.

102
00:06:52,100 --> 00:06:56,200
Зокрема, на цьому кроці вектори не розмовляють один з одним, кожен з них робить щось своє.

103
00:06:56,720 --> 00:06:59,485
І для нас з вами це насправді робить все набагато простішим, 

104
00:06:59,485 --> 00:07:02,568
тому що це означає, що якщо ми розуміємо, що відбувається з одним з 

105
00:07:02,568 --> 00:07:06,060
векторів через цей блок, ми фактично розуміємо, що відбувається з усіма ними.

106
00:07:07,100 --> 00:07:11,885
Коли я кажу, що цей блок буде кодувати той факт, що Майкл Джордан грає в баскетбол, 

107
00:07:11,885 --> 00:07:16,670
я маю на увазі, що якщо подається вектор, який кодує ім'я Майкл і прізвище Джордан, 

108
00:07:16,670 --> 00:07:21,456
то ця послідовність обчислень створить щось, що включає в себе напрямок баскетболу, 

109
00:07:21,456 --> 00:07:24,020
і саме це додасться до вектора в цій позиції.

110
00:07:25,600 --> 00:07:29,700
Перший крок цього процесу виглядає як множення вектора на дуже велику матрицю.

111
00:07:30,040 --> 00:07:31,980
Ніяких сюрпризів, це глибоке навчання.

112
00:07:32,680 --> 00:07:36,085
І ця матриця, як і всі інші, які ми бачили, заповнена параметрами моделі, 

113
00:07:36,085 --> 00:07:39,858
які вивчаються на основі даних, що можна уявити собі як купу ручок і циферблатів, 

114
00:07:39,858 --> 00:07:43,540
які підлаштовуються і налаштовуються, щоб визначити, якою буде поведінка моделі.

115
00:07:44,500 --> 00:07:48,533
Тепер, один з гарних способів подумати про множення матриць - це уявити 

116
00:07:48,533 --> 00:07:52,678
кожен рядок матриці як власний вектор, і взяти купу точкових добутків між 

117
00:07:52,678 --> 00:07:56,880
цими рядками і оброблюваним вектором, який я позначу як E для вбудовування.

118
00:07:57,280 --> 00:08:02,005
Наприклад, припустимо, що перший рядок збігся з напрямком імені Майкла, 

119
00:08:02,005 --> 00:08:04,040
який, як ми припускаємо, існує.

120
00:08:04,320 --> 00:08:09,083
Це означає, що перший компонент цього результату, цього точкового добутку, 

121
00:08:09,083 --> 00:08:14,800
буде одиницею, якщо вектор кодує ім'я Майкл, і нулем або від'ємним у протилежному випадку.

122
00:08:15,880 --> 00:08:19,636
А ще веселіше - подумайте, що б це означало, якби в першому 

123
00:08:19,636 --> 00:08:23,080
рядку було ім'я Майкл плюс прізвище Джордан у напрямку.

124
00:08:23,700 --> 00:08:27,420
І для простоти, дозвольте мені записати це як M плюс J.

125
00:08:28,080 --> 00:08:30,921
Потім, якщо взяти крапковий продукт з цим вкладанням E, 

126
00:08:30,921 --> 00:08:34,980
все розподіляється дуже добре, так що це виглядає як M крапка E плюс J крапка E.

127
00:08:34,980 --> 00:08:38,998
Зверніть увагу, що це означає, що кінцеве значення буде дорівнювати двом, 

128
00:08:38,998 --> 00:08:43,668
якщо вектор кодує повне ім'я Майкла Джордана, а в іншому випадку - одиниці або чомусь 

129
00:08:43,668 --> 00:08:44,700
меншому за одиницю.

130
00:08:45,340 --> 00:08:47,260
І це лише один рядок у цій матриці.

131
00:08:47,600 --> 00:08:52,347
Ви можете уявити, що всі інші рядки паралельно ставлять інші запитання, 

132
00:08:52,347 --> 00:08:56,040
досліджуючи інші особливості вектора, який обробляється.

133
00:08:56,700 --> 00:08:59,889
Дуже часто цей крок також передбачає додавання ще одного вектора на виході, 

134
00:08:59,889 --> 00:09:02,240
який містить параметри моделі, отримані на основі даних.

135
00:09:02,240 --> 00:09:04,560
Цей інший вектор відомий як упередженість.

136
00:09:05,180 --> 00:09:08,794
Для нашого прикладу я хочу, щоб ви уявили, що значення цього зміщення 

137
00:09:08,794 --> 00:09:12,048
в першому компоненті є від'ємним, тобто наш кінцевий результат 

138
00:09:12,048 --> 00:09:15,560
виглядає як відповідний точковий добуток, але з від'ємним значенням.

139
00:09:16,120 --> 00:09:20,552
Ви можете резонно запитати, чому я хочу, щоб ви вважали, що модель навчилася цьому, 

140
00:09:20,552 --> 00:09:24,614
і за мить ви побачите, чому це дуже чисто і красиво, якщо ми маємо значення, 

141
00:09:24,614 --> 00:09:28,888
яке є додатним тоді і тільки тоді, коли вектор кодує повне ім'я Майкла Джордана, 

142
00:09:28,888 --> 00:09:32,160
а в іншому випадку воно дорівнює нулю або від'ємному значенню.

143
00:09:33,040 --> 00:09:37,938
Загальна кількість рядків у цій матриці, яка є чимось на кшталт кількості поставлених 

144
00:09:37,938 --> 00:09:42,780
запитань, у випадку GPT-3, цифри якого ми відстежували, становить трохи менше 50 000.

145
00:09:43,100 --> 00:09:45,193
Насправді це рівно в чотири рази більше, ніж кількість 

146
00:09:45,193 --> 00:09:46,640
вимірів у цьому просторі вбудовування.

147
00:09:46,920 --> 00:09:47,900
Це дизайнерський вибір.

148
00:09:47,940 --> 00:09:49,759
Ви можете зробити його більшим, можете зробити меншим, 

149
00:09:49,759 --> 00:09:52,240
але чисте кратне число, як правило, сприятливе для апаратного забезпечення.

150
00:09:52,740 --> 00:09:57,329
Оскільки ця матриця, повна ваг, відображає нас у простір вищої розмірності, 

151
00:09:57,329 --> 00:09:59,020
я позначу її скорочено W up.

152
00:09:59,020 --> 00:10:02,609
Я продовжую позначати вектор, який ми обробляємо, як E, 

153
00:10:02,609 --> 00:10:07,160
а цей вектор зсуву позначимо як B вгору і повернемо все це на діаграму.

154
00:10:09,180 --> 00:10:13,300
На цьому етапі проблема полягає в тому, що ця операція є суто лінійною, 

155
00:10:13,300 --> 00:10:15,360
але мова є дуже нелінійним процесом.

156
00:10:15,880 --> 00:10:19,578
Якщо показник, який ми вимірюємо, є високим для Майкла плюс Джордан, 

157
00:10:19,578 --> 00:10:23,115
він також обов'язково буде дещо підвищений для Майкла плюс Фелпс, 

158
00:10:23,115 --> 00:10:25,688
а також Алексіс плюс Джордан, незважаючи на те, 

159
00:10:25,688 --> 00:10:28,100
що вони концептуально не пов'язані між собою.

160
00:10:28,540 --> 00:10:32,000
Насправді вам потрібна проста відповідь "так" або "ні" на повне ім'я.

161
00:10:32,900 --> 00:10:35,195
Отже, наступний крок - пропустити цей великий 

162
00:10:35,195 --> 00:10:37,840
проміжний вектор через дуже просту нелінійну функцію.

163
00:10:38,360 --> 00:10:43,148
Найпоширеніший вибір - це вибір, який бере всі від'ємні значення і прирівнює їх до нуля, 

164
00:10:43,148 --> 00:10:45,300
а всі додатні значення залишає без змін.

165
00:10:46,440 --> 00:10:50,657
І продовжуючи традицію глибокого навчання щодо надто вигадливих назв, 

166
00:10:50,657 --> 00:10:56,020
цю дуже просту функцію часто називають випрямленою лінійною одиницею, або скорочено ReLU.

167
00:10:56,020 --> 00:10:57,880
Ось як виглядає графік.

168
00:10:58,300 --> 00:11:03,056
Отже, якщо взяти наш уявний приклад, де перший запис проміжного вектора дорівнює одиниці, 

169
00:11:03,056 --> 00:11:07,548
якщо і тільки якщо повне ім'я Майкл Джордан, і нулю або від'ємному значенню в іншому 

170
00:11:07,548 --> 00:11:10,613
випадку, то після того, як ви пропустите його через ReLU, 

171
00:11:10,613 --> 00:11:14,683
ви отримаєте дуже чисте значення, де всі нульові та від'ємні значення просто 

172
00:11:14,683 --> 00:11:15,740
обрізаються до нуля.

173
00:11:16,100 --> 00:11:17,867
Таким чином, цей вихід буде одиницею для повного 

174
00:11:17,867 --> 00:11:19,780
імені Майкла Джордана і нулем у протилежному випадку.

175
00:11:20,560 --> 00:11:24,120
Іншими словами, він безпосередньо імітує поведінку воріт AND.

176
00:11:25,660 --> 00:11:29,294
Часто моделі використовують дещо модифіковану функцію, яка називається JLU, 

177
00:11:29,294 --> 00:11:32,020
що має ту ж саму базову форму, тільки трохи більш плавну.

178
00:11:32,500 --> 00:11:35,720
Але для наших цілей буде трохи чистіше, якщо ми будемо думати лише про Революцію Гідності.

179
00:11:36,740 --> 00:11:40,470
Крім того, коли ви чуєте, як люди говорять про нейрони трансформатора, 

180
00:11:40,470 --> 00:11:42,520
вони говорять про ці цінності саме тут.

181
00:11:42,900 --> 00:11:47,547
Коли ви бачите звичайне зображення нейронної мережі з шаром точок і купою ліній, 

182
00:11:47,547 --> 00:11:51,965
що з'єднуються з попереднім шаром, яке ми розглядали раніше в цій серії, це, 

183
00:11:51,965 --> 00:11:56,670
як правило, має на меті передати комбінацію лінійного кроку, матричного множення, 

184
00:11:56,670 --> 00:12:01,260
за яким слідує якась проста нелінійна функція, така як ReLU, з простими членами.

185
00:12:02,500 --> 00:12:06,114
Можна сказати, що цей нейрон активний, коли це значення позитивне, 

186
00:12:06,114 --> 00:12:08,920
і що він неактивний, якщо це значення дорівнює нулю.

187
00:12:10,120 --> 00:12:12,380
Наступний крок дуже схожий на перший.

188
00:12:12,560 --> 00:12:16,580
Ви множите на дуже велику матрицю і додаєте певний член зміщення.

189
00:12:16,980 --> 00:12:21,067
У цьому випадку кількість вимірів на виході зменшується до розміру 

190
00:12:21,067 --> 00:12:25,520
цього простору вбудовування, тому я називатиму її матрицею проекції вниз.

191
00:12:26,220 --> 00:12:28,875
І цього разу, замість того, щоб думати про речі ряд за рядом, 

192
00:12:28,875 --> 00:12:31,360
насправді приємніше думати про них стовпчик за стовпчиком.

193
00:12:31,860 --> 00:12:36,244
Бачите, ще один спосіб тримати в голові множення матриць - це уявити, 

194
00:12:36,244 --> 00:12:41,318
що ви берете кожен стовпець матриці, множите його на відповідний член у векторі, 

195
00:12:41,318 --> 00:12:45,640
який він обробляє, і складаєте всі ці перемасштабовані стовпці разом.

196
00:12:46,840 --> 00:12:49,693
Причина, по якій приємніше думати саме так, полягає в тому, 

197
00:12:49,693 --> 00:12:52,784
що тут колонки мають той самий вимір, що і простір вбудовування, 

198
00:12:52,784 --> 00:12:55,780
тому ми можемо думати про них як про напрямки в цьому просторі.

199
00:12:56,140 --> 00:12:59,638
Наприклад, уявімо, що модель навчилася робити перший стовпчик 

200
00:12:59,638 --> 00:13:03,080
у цьому баскетбольному напрямку, який, як ми вважаємо, існує.

201
00:13:04,180 --> 00:13:08,151
Це означає, що коли відповідний нейрон у першій позиції буде активним, 

202
00:13:08,151 --> 00:13:10,780
ми додамо цей стовпчик до кінцевого результату.

203
00:13:11,140 --> 00:13:14,327
Але якби цей нейрон був неактивним, якби це число дорівнювало нулю, 

204
00:13:14,327 --> 00:13:15,780
то це не мало б жодного ефекту.

205
00:13:16,500 --> 00:13:18,060
І це не обов'язково має бути лише баскетбол.

206
00:13:18,220 --> 00:13:21,735
Модель також може вписати в цю колонку і багато інших особливостей, 

207
00:13:21,735 --> 00:13:25,200
які вона хоче асоціювати з чимось, що має повне ім'я Майкл Джордан.

208
00:13:26,980 --> 00:13:31,305
І в той же час, всі інші стовпчики в цій матриці говорять вам, 

209
00:13:31,305 --> 00:13:36,660
що буде додано до кінцевого результату, якщо відповідний нейрон буде активним.

210
00:13:37,360 --> 00:13:40,268
І якщо у вас є упередження в цьому випадку, то це те, 

211
00:13:40,268 --> 00:13:43,500
що ви просто додаєте щоразу, незалежно від значень нейронів.

212
00:13:44,060 --> 00:13:45,280
Ви можете здивуватися, що це робить.

213
00:13:45,540 --> 00:13:49,320
Як і у випадку з усіма об'єктами з параметрами, тут важко сказати точно.

214
00:13:49,320 --> 00:13:54,380
Можливо, мережа має вести якусь бухгалтерію, але поки що ви можете її ігнорувати.

215
00:13:54,860 --> 00:13:57,913
Щоб знову зробити нашу нотацію трохи компактнішою, 

216
00:13:57,913 --> 00:14:02,703
я назву цю велику матрицю W вниз і так само назву вектор зсуву B вниз і поверну 

217
00:14:02,703 --> 00:14:04,260
їх назад на нашу діаграму.

218
00:14:04,740 --> 00:14:09,295
Як я вже показував раніше, з цим кінцевим результатом ви додаєте його до вектора, 

219
00:14:09,295 --> 00:14:13,240
який потрапив у блок у цій позиції, і отримуєте цей кінцевий результат.

220
00:14:13,820 --> 00:14:19,220
Так, наприклад, якщо вектор, що надходить, містить ім'я Майкл та прізвище Джордан, 

221
00:14:19,220 --> 00:14:23,058
то оскільки ця послідовність операцій запустить вентиль І, 

222
00:14:23,058 --> 00:14:27,483
він додасть напрямок баскетбольного м'яча, і те, що вийде на екран, 

223
00:14:27,483 --> 00:14:29,240
буде кодувати все це разом.

224
00:14:29,820 --> 00:14:34,200
І пам'ятайте, що цей процес відбувається з кожним з цих векторів паралельно.

225
00:14:34,800 --> 00:14:38,430
Зокрема, якщо взяти цифри GPT-3, то це означає, 

226
00:14:38,430 --> 00:14:44,860
що в цьому блоці не просто 50 000 нейронів, а в 50 000 разів більше токенів на вході.

227
00:14:48,180 --> 00:14:52,947
Ось і вся операція: два матричні добутки, кожен з яких додається зі зміщенням, 

228
00:14:52,947 --> 00:14:55,180
і проста функція відсікання між ними.

229
00:14:56,080 --> 00:14:58,551
Кожен з вас, хто дивився попередні відео серії, 

230
00:14:58,551 --> 00:15:02,620
впізнає цю структуру як найпростіший тип нейронної мережі, який ми там вивчали.

231
00:15:03,080 --> 00:15:06,100
У цьому прикладі його навчили розпізнавати цифри, написані від руки.

232
00:15:06,580 --> 00:15:10,513
Тут, у контексті трансформатора для великої мовної моделі, 

233
00:15:10,513 --> 00:15:15,246
це лише частина великої архітектури, і будь-яка спроба інтерпретувати, 

234
00:15:15,246 --> 00:15:20,579
що саме він робить, тісно переплітається з ідеєю кодування інформації у вектори 

235
00:15:20,579 --> 00:15:23,180
високорозмірного простору вбудовування.

236
00:15:24,260 --> 00:15:29,086
Це основний урок, але я хочу зробити крок назад і поміркувати над двома різними речами, 

237
00:15:29,086 --> 00:15:33,747
перша з яких є своєрідною бухгалтерією, а друга включає в себе дуже цікавий факт про 

238
00:15:33,747 --> 00:15:38,080
вищі виміри, про який я насправді не знав, поки не заглибився в трансформатори.

239
00:15:41,080 --> 00:15:46,032
У попередніх двох розділах ми з вами почали підраховувати загальну кількість параметрів 

240
00:15:46,032 --> 00:15:50,760
у GPT-3 і бачити, де саме вони живуть, тож давайте швиденько закінчимо гру на цьому.

241
00:15:51,400 --> 00:15:56,443
Я вже згадував, що ця матриця висхідної проекції має трохи менше 50 000 рядків, 

242
00:15:56,443 --> 00:16:00,162
і що кожен рядок відповідає розміру простору вбудовування, 

243
00:16:00,162 --> 00:16:02,180
який для GPT-3 становить 12 288.

244
00:16:03,240 --> 00:16:08,711
Помноживши їх разом, ми отримаємо 604 мільйони параметрів тільки для цієї матриці, 

245
00:16:08,711 --> 00:16:13,920
а проекція вниз має таку ж кількість параметрів, тільки з переставленою формою.

246
00:16:14,500 --> 00:16:17,400
Тож разом вони дають близько 1,2 мільярда параметрів.

247
00:16:18,280 --> 00:16:20,528
Вектор зміщення також враховує ще пару параметрів, 

248
00:16:20,528 --> 00:16:24,100
але це тривіальна частка від загальної суми, тому я навіть не буду її показувати.

249
00:16:24,660 --> 00:16:29,737
У GPT-3 ця послідовність векторів вбудовування проходить не через один, 

250
00:16:29,737 --> 00:16:33,898
а через 96 різних MLP, тому загальна кількість параметрів, 

251
00:16:33,898 --> 00:16:38,060
присвячених усім цим блокам, складає близько 116 мільярдів.

252
00:16:38,820 --> 00:16:43,728
Це приблизно 2 третини всіх параметрів мережі, і коли ви додасте їх до всього, 

253
00:16:43,728 --> 00:16:47,643
що ми мали раніше, до блоків уваги, вбудовування та вилучення, 

254
00:16:47,643 --> 00:16:51,620
ви дійсно отримаєте 175 мільярдів, як і було заявлено в рекламі.

255
00:16:53,060 --> 00:16:55,874
Можливо, варто згадати, що існує ще один набір параметрів, 

256
00:16:55,874 --> 00:16:59,547
пов'язаних з тими кроками нормалізації, які ми пропустили в цьому поясненні, 

257
00:16:59,547 --> 00:17:03,840
але, як і вектор зміщення, вони становлять дуже тривіальну частку від загальної кількості.

258
00:17:05,900 --> 00:17:08,320
Щодо другої точки роздумів, вам може бути цікаво, 

259
00:17:08,320 --> 00:17:11,758
чи цей центральний приклад іграшки, на який ми витратили стільки часу, 

260
00:17:11,758 --> 00:17:15,680
відображає те, як факти насправді зберігаються в реальних великих мовних моделях.

261
00:17:16,319 --> 00:17:20,003
Дійсно, рядки цієї першої матриці можна розглядати як напрямки в 

262
00:17:20,003 --> 00:17:24,593
цьому просторі вбудовування, а це означає, що активація кожного нейрона показує, 

263
00:17:24,593 --> 00:17:27,540
наскільки даний вектор співпадає з певним напрямком.

264
00:17:27,760 --> 00:17:30,893
Також вірно, що стовпці другої матриці показують, 

265
00:17:30,893 --> 00:17:34,340
що буде додано до результату, якщо цей нейрон активний.

266
00:17:34,640 --> 00:17:36,800
І те, і інше - просто математичні факти.

267
00:17:37,740 --> 00:17:43,050
Однак дані свідчать про те, що окремі нейрони дуже рідко представляють єдину чисту 

268
00:17:43,050 --> 00:17:48,489
функцію, як Майкл Джордан, і на це може бути дуже вагома причина, пов'язана з ідеєю, 

269
00:17:48,489 --> 00:17:54,120
яка в наш час поширюється серед дослідників інтерпретативності, відомою як суперпозиція.

270
00:17:54,640 --> 00:17:57,307
Це гіпотеза, яка може допомогти пояснити як те, 

271
00:17:57,307 --> 00:18:00,252
чому моделі особливо важко інтерпретувати, так і те, 

272
00:18:00,252 --> 00:18:02,420
чому вони напрочуд добре масштабуються.

273
00:18:03,500 --> 00:18:07,680
Основна ідея полягає в тому, що якщо у вас є n-вимірний простір і ви хочете 

274
00:18:07,680 --> 00:18:11,200
представити купу різних характеристик, використовуючи напрямки, 

275
00:18:11,200 --> 00:18:14,500
які перпендикулярні один до одного в цьому просторі, тобто, 

276
00:18:14,500 --> 00:18:18,790
якщо ви додаєте компонент в одному напрямку, він не впливає на інші напрямки, 

277
00:18:18,790 --> 00:18:22,970
то максимальна кількість векторів, яку ви можете вмістити, дорівнює лише n, 

278
00:18:22,970 --> 00:18:23,960
кількості вимірів.

279
00:18:24,600 --> 00:18:27,620
Для математика, власне, це і є визначенням розмірності.

280
00:18:28,220 --> 00:18:33,580
Але цікаво стає тоді, коли ви трохи послаблюєте ці обмеження і терпимо ставитеся до шуму.

281
00:18:34,180 --> 00:18:39,679
Скажімо, ви дозволяєте представити ці елементи векторами, які не зовсім перпендикулярні, 

282
00:18:39,679 --> 00:18:43,820
але майже перпендикулярні, можливо, під кутом між 89 і 91 градусом.

283
00:18:44,820 --> 00:18:48,020
Якби ми були у двох чи трьох вимірах, це не мало б ніякого значення.

284
00:18:48,260 --> 00:18:50,681
Це майже не дає вам додаткового простору для маневру, 

285
00:18:50,681 --> 00:18:54,044
щоб вписати більше векторів, що робить ще більш контрінтуїтивним той факт, 

286
00:18:54,044 --> 00:18:56,780
що при більших розмірностях відповідь кардинально змінюється.

287
00:18:57,660 --> 00:19:02,955
Я можу дати вам дуже швидку і брудну ілюстрацію цього за допомогою уривчастого Python, 

288
00:19:02,955 --> 00:19:07,399
який створить список 100-вимірних векторів, кожен з яких ініціалізується 

289
00:19:07,399 --> 00:19:11,538
випадковим чином, і цей список буде містити 10 000 різних векторів, 

290
00:19:11,538 --> 00:19:14,400
тобто в 100 разів більше векторів, ніж вимірів.

291
00:19:15,320 --> 00:19:19,900
На цьому графіку показано розподіл кутів між парами цих векторів.

292
00:19:20,680 --> 00:19:25,850
Оскільки вони починали навмання, ці кути могли бути будь-якими - від 0 до 180 градусів, 

293
00:19:25,850 --> 00:19:29,668
але ви помітите, що навіть для випадкових векторів існує сильний 

294
00:19:29,668 --> 00:19:31,960
зсув у бік кутів ближче до 90 градусів.

295
00:19:32,500 --> 00:19:35,434
Потім я збираюся запустити певний процес оптимізації, 

296
00:19:35,434 --> 00:19:37,988
який ітеративно підштовхує всі ці вектори так, 

297
00:19:37,988 --> 00:19:41,520
щоб вони намагалися стати більш перпендикулярними один до одного.

298
00:19:42,060 --> 00:19:46,660
Повторивши це багато разів, ось як виглядає розподіл кутів.

299
00:19:47,120 --> 00:19:52,079
Тут ми повинні збільшити масштаб, оскільки всі можливі кути між парами 

300
00:19:52,079 --> 00:19:56,900
векторів знаходяться в цьому вузькому діапазоні між 89 і 91 градусом.

301
00:19:58,020 --> 00:20:01,947
Загалом, наслідком леми Джонсона-Лінденштрауса є те, 

302
00:20:01,947 --> 00:20:07,579
що кількість майже перпендикулярних векторів, які можна втиснути в простір, 

303
00:20:07,579 --> 00:20:10,840
зростає експоненціально з кількістю вимірів.

304
00:20:11,960 --> 00:20:15,771
Це дуже важливо для великих мовних моделей, які можуть виграти, 

305
00:20:15,771 --> 00:20:19,880
якщо пов'язувати незалежні ідеї з майже перпендикулярними напрямками.

306
00:20:20,000 --> 00:20:23,680
Це означає, що він може зберігати набагато більше ідей, 

307
00:20:23,680 --> 00:20:26,440
ніж є вимірів у відведеному йому просторі.

308
00:20:27,320 --> 00:20:29,376
Це може частково пояснити, чому продуктивність 

309
00:20:29,376 --> 00:20:31,740
моделі так добре масштабується зі збільшенням розміру.

310
00:20:32,540 --> 00:20:36,381
Простір, який має в 10 разів більше вимірів, може зберігати набагато, 

311
00:20:36,381 --> 00:20:39,400
набагато більше, ніж в 10 разів більше незалежних ідей.

312
00:20:40,420 --> 00:20:43,864
І це стосується не лише простору вбудовування, де живуть вектори, 

313
00:20:43,864 --> 00:20:47,256
що протікають через модель, але й вектора, наповненого нейронами 

314
00:20:47,256 --> 00:20:50,440
в середині багатошарового персептрона, який ми щойно вивчали.

315
00:20:50,960 --> 00:20:55,475
Тобто, при розмірах GPT-3, він міг би не просто зондувати 50 000 елементів, 

316
00:20:55,475 --> 00:20:59,218
але якби він використовував цю величезну додаткову потужність, 

317
00:20:59,218 --> 00:21:02,546
використовуючи майже перпендикулярні напрямки простору, 

318
00:21:02,546 --> 00:21:07,240
він міг би зондувати набагато, набагато більше елементів оброблюваного вектору.

319
00:21:07,780 --> 00:21:11,264
Але якщо він це робить, то це означає, що індивідуальні особливості 

320
00:21:11,264 --> 00:21:14,340
не будуть видимими у вигляді окремих нейронів, що світяться.

321
00:21:14,660 --> 00:21:19,380
Натомість він мав би виглядати як певна комбінація нейронів, суперпозиція.

322
00:21:20,400 --> 00:21:24,592
Для тих, кому цікаво дізнатися більше, ключовим пошуковим терміном тут є розріджений 

323
00:21:24,592 --> 00:21:28,292
автокодер - інструмент, який деякі фахівці з інтерпретації використовують, 

324
00:21:28,292 --> 00:21:32,485
щоб спробувати виокремити справжні ознаки, навіть якщо вони дуже накладені на всі ці 

325
00:21:32,485 --> 00:21:32,880
нейрони.

326
00:21:33,540 --> 00:21:36,800
Я дам посилання на кілька чудових антропологічних постів про це.

327
00:21:37,880 --> 00:21:41,050
На цьому етапі ми не торкнулися кожної деталі трансформатора, 

328
00:21:41,050 --> 00:21:43,300
але ми з вами зачепили найважливіші моменти.

329
00:21:43,520 --> 00:21:47,640
Головне, про що я хочу розповісти в наступному розділі, - це процес навчання.

330
00:21:48,460 --> 00:21:51,147
З одного боку, коротка відповідь на питання, як працює тренінг, 

331
00:21:51,147 --> 00:21:53,162
полягає в тому, що все це - зворотне поширення, 

332
00:21:53,162 --> 00:21:56,900
і ми розглядали зворотне поширення в окремому контексті в попередніх розділах цієї серії.

333
00:21:57,220 --> 00:22:00,280
Але є ще багато чого обговорити, наприклад, специфічну функцію витрат, 

334
00:22:00,280 --> 00:22:03,685
що використовується для мовних моделей, ідею точного налаштування за допомогою 

335
00:22:03,685 --> 00:22:06,142
навчання з підкріпленням і зворотним зв'язком з людиною, 

336
00:22:06,142 --> 00:22:07,780
а також поняття законів масштабування.

337
00:22:08,960 --> 00:22:13,140
Для активних підписників: є кілька відео, не пов'язаних з машинним навчанням, 

338
00:22:13,140 --> 00:22:16,462
які я хочу переглянути перед тим, як написати наступну главу, 

339
00:22:16,462 --> 00:22:20,000
тому це може зайняти деякий час, але я обіцяю, що все буде вчасно.

340
00:22:35,640 --> 00:22:37,920
Дякую.

