[
 {
  "input": "If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball, this would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport.",
  "translatedText": "Якщо ви дасте великій мовній моделі фразу \"Майкл Джордан грає в бейсбол\" і попросите її передбачити, що буде далі, і вона правильно передбачить баскетбол, це означатиме, що десь всередині її сотень мільярдів параметрів закладено знання про конкретну людину та її конкретний вид спорту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 18.32
 },
 {
  "input": "And I think in general, anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts.",
  "translatedText": "І я думаю, що кожен, хто бавився з однією з цих моделей, має чітке відчуття, що він запам'ятовує тонни і тонни фактів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 18.94,
  "end": 25.4
 },
 {
  "input": "So a reasonable question you could ask is, how exactly does that work?",
  "translatedText": "Тож резонне питання, яке ви можете поставити, - як саме це працює?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 25.7,
  "end": 29.16
 },
 {
  "input": "And where do those facts live?",
  "translatedText": "І де живуть ці факти?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 29.16,
  "end": 31.04
 },
 {
  "input": "Last December, a few researchers from Google DeepMind posted about work on this question, and they were using this specific example of matching athletes to their sports.",
  "translatedText": "У грудні минулого року кілька дослідників з Google DeepMind опублікували повідомлення про роботу над цим питанням, і вони використовували цей конкретний приклад зіставлення спортсменів з їхніми видами спорту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 35.72,
  "end": 44.48
 },
 {
  "input": "And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short.",
  "translatedText": "І хоча повне механістичне розуміння того, як зберігаються факти, залишається невирішеним, вони отримали деякі цікаві часткові результати, включаючи дуже загальний висновок високого рівня, що факти, здається, живуть всередині певної частини цих мереж, відомих під химерною назвою багатошарових перцептронів, або скорочено MLP.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 44.9,
  "end": 62.64
 },
 {
  "input": "In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI.",
  "translatedText": "В останніх двох розділах ми з вами розглянули деталі трансформаторів, архітектуру, що лежить в основі великих мовних моделей, а також в основі багатьох інших сучасних ШІ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 63.12,
  "end": 72.5
 },
 {
  "input": "In the most recent chapter, we were focusing on a piece called Attention.",
  "translatedText": "В останньому розділі ми зосередилися на творі під назвою \"Увага\".",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 73.06,
  "end": 76.2
 },
 {
  "input": "And the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network.",
  "translatedText": "І наступний крок для нас з вами - заглибитися в деталі того, що відбувається всередині цих багатошарових перцептронів, які складають іншу велику частину мережі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.84,
  "end": 85.04
 },
 {
  "input": "The computation here is actually relatively simple, especially when you compare it to attention.",
  "translatedText": "Обчислення тут насправді відносно прості, особливо якщо порівнювати їх з увагою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.68,
  "end": 90.1
 },
 {
  "input": "It boils down essentially to a pair of matrix multiplications with a simple something in between.",
  "translatedText": "По суті, це зводиться до пари матричних множень з простим чимось середнім між ними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 90.56,
  "end": 94.98
 },
 {
  "input": "However, interpreting what these computations are doing is exceedingly challenging.",
  "translatedText": "Однак інтерпретувати те, що роблять ці обчислення, надзвичайно складно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.72,
  "end": 100.46
 },
 {
  "input": "Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact.",
  "translatedText": "Наша головна мета тут - пройти через обчислення і зробити їх такими, що запам'ятовуються, але я хотів би зробити це в контексті показу конкретного прикладу того, як один з цих блоків міг би, принаймні в принципі, зберігати конкретний факт.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 101.56,
  "end": 113.16
 },
 {
  "input": "Specifically, it'll be storing the fact that Michael Jordan plays basketball.",
  "translatedText": "Зокрема, він зберігатиме інформацію про те, що Майкл Джордан грає в баскетбол.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.58,
  "end": 117.08
 },
 {
  "input": "I should mention the layout here is inspired by a conversation I had with one of those DeepMind researchers, Neil Nanda.",
  "translatedText": "Маю зазначити, що макет тут натхненний розмовою, яку я мав з одним із дослідників DeepMind, Нілом Нандою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 118.08,
  "end": 123.2
 },
 {
  "input": "For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow.",
  "translatedText": "Здебільшого я припускаю, що ви або дивилися попередні два розділи, або маєте базове уявлення про те, що таке трансформатор, але освіжити пам'ять ніколи не завадить, тож ось коротке нагадування про загальний хід подій.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.06,
  "end": 134.7
 },
 {
  "input": "You and I have been studying a model that's trained to take in a piece of text and predict what comes next.",
  "translatedText": "Ми з вами вивчали модель, яка навчена сприймати шматок тексту і передбачати, що буде далі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 135.34,
  "end": 141.32
 },
 {
  "input": "That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.",
  "translatedText": "Вхідний текст спочатку розбивається на купу токенів, тобто невеликих фрагментів, які зазвичай є словами або невеликими шматочками слів, і кожен токен асоціюється з вектором високої розмірності, тобто довгим списком чисел.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.72,
  "end": 155.28
 },
 {
  "input": "This sequence of vectors then repeatedly passes through two kinds of operation, attention, which allows the vectors to pass information between one another, and then the multilayer perceptrons, the thing that we're gonna dig into today, and also there's a certain normalization step in between.",
  "translatedText": "Потім ця послідовність векторів багаторазово проходить через два види операцій - увагу, яка дозволяє векторам передавати інформацію один одному, а потім багатошарові перцептрони, те, що ми сьогодні будемо розглядати, а також між ними є певний етап нормалізації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.84,
  "end": 172.3
 },
 {
  "input": "After the sequence of vectors has flowed through many, many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words in the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.",
  "translatedText": "Після того, як послідовність векторів пройшла через багато, багато різних ітерацій обох цих блоків, можна сподіватися, що кожен вектор ввібрав достатньо інформації, як з контексту, всіх інших слів на вході, так і з загальних знань, які були закладені у вагах моделі під час навчання, щоб його можна було використати для прогнозування того, який токен буде наступним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 173.3,
  "end": 196.02
 },
 {
  "input": "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very, very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning.",
  "translatedText": "Одна з ключових ідей, яку я хочу, щоб ви запам'ятали, полягає в тому, що всі ці вектори живуть у дуже, дуже багатовимірному просторі, і коли ви думаєте про цей простір, різні напрямки можуть кодувати різні види сенсу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 196.86,
  "end": 208.8
 },
 {
  "input": "So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very, very close to the corresponding feminine noun.",
  "translatedText": "Дуже класичний приклад, до якого я люблю повертатися, - це те, що якщо ви подивитеся на вбудованість жінки і віднімете вбудованість чоловіка, зробите цей маленький крок і додасте його до іншого іменника чоловічого роду, наприклад, дядько, ви опинитеся десь дуже, дуже близько до відповідного іменника жіночого роду.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 210.12,
  "end": 226.24
 },
 {
  "input": "In this sense, this particular direction encodes gender information.",
  "translatedText": "У цьому сенсі саме цей напрямок кодує гендерну інформацію.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.44,
  "end": 230.88
 },
 {
  "input": "The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent.",
  "translatedText": "Ідея полягає в тому, що багато інших чітких напрямків у цьому надвимірному просторі можуть відповідати іншим особливостям, які модель, можливо, захоче відобразити.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.64,
  "end": 239.64
 },
 {
  "input": "In a transformer, these vectors don't merely encode the meaning of a single word, though.",
  "translatedText": "Однак у трансформаторі ці вектори не просто кодують значення одного слова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.4,
  "end": 246.18
 },
 {
  "input": "As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge.",
  "translatedText": "Коли вони протікають через мережу, вони вбирають в себе набагато багатше значення, засноване на всьому контексті навколо них, а також на знаннях моделі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.68,
  "end": 255.18
 },
 {
  "input": "Ultimately, each one needs to encode something far, far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next.",
  "translatedText": "Зрештою, кожне з них має кодувати щось набагато більше, ніж значення одного слова, оскільки воно має бути достатнім для того, щоб передбачити, що буде далі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.88,
  "end": 263.76
 },
 {
  "input": "We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts.",
  "translatedText": "Ми вже бачили, як блоки уваги дозволяють включати контекст, але більшість параметрів моделі насправді живуть всередині блоків MLP, і одна з версій того, що вони можуть робити, полягає в тому, що вони надають додаткову ємність для зберігання фактів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 264.56,
  "end": 278.14
 },
 {
  "input": "Like I said, the lesson here is gonna center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball.",
  "translatedText": "Як я вже казав, урок буде зосереджений на конкретному прикладі іграшки і на тому, як саме вона може зберігати той факт, що Майкл Джордан грає в баскетбол.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 278.72,
  "end": 286.12
 },
 {
  "input": "Now, this toy example is gonna require that you and I make a couple of assumptions about that high-dimensional space.",
  "translatedText": "Цей приклад з іграшкою вимагатиме від нас з вами зробити кілька припущень щодо цього багатовимірного простору.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.12,
  "end": 291.9
 },
 {
  "input": "First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball.",
  "translatedText": "Спочатку припустимо, що один з напрямків представляє ідею імені Майкл, потім інший майже перпендикулярний напрямок представляє ідею прізвища Джордан, а третій напрямок буде представляти ідею баскетболу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 292.36,
  "end": 306.42
 },
 {
  "input": "So specifically, what I mean by this is if you look in the network and you pluck out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name.",
  "translatedText": "Конкретно я маю на увазі, що якщо ви подивитеся в мережі і витягнете один з оброблюваних векторів, якщо його точковий добуток з цим ім'ям Майкл дорівнює одиниці, це буде означати, що вектор кодує ідею людини з цим ім'ям.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.4,
  "end": 322.34
 },
 {
  "input": "Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.",
  "translatedText": "Інакше цей точковий добуток був би нульовим або від'ємним, що означає, що вектор насправді не співпадає з цим напрямком.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.8,
  "end": 328.7
 },
 {
  "input": "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one.",
  "translatedText": "І для простоти, давайте повністю проігноруємо цілком резонне питання про те, що це могло б означати, якби цей точковий продукт був більшим за одиницю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 329.42,
  "end": 335.32
 },
 {
  "input": "Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan or basketball.",
  "translatedText": "Аналогічно, його крапковий продукт із цими іншими напрямками підкаже вам, чи представляє він прізвище Джордан, чи баскетбол.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.2,
  "end": 343.76
 },
 {
  "input": "So let's say a vector is meant to represent the full name, Michael Jordan, then its dot product with both of these directions would have to be one.",
  "translatedText": "Отже, скажімо, вектор має представляти повне ім'я Майкла Джордана, тоді його точковий добуток з обома цими напрямками має дорівнювати одиниці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.74,
  "end": 352.68
 },
 {
  "input": "Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names.",
  "translatedText": "Оскільки текст Michael Jordan охоплює дві різні лексеми, це також означає, що ми повинні припустити, що попередній блок уваги успішно передав інформацію до другого з цих двох векторів, щоб забезпечити можливість кодування обох імен.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.48,
  "end": 366.96
 },
 {
  "input": "With all of those as the assumptions, let's now dive into the meat of the lesson.",
  "translatedText": "З усіма цими припущеннями, давайте зануримося в суть уроку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.94,
  "end": 371.48
 },
 {
  "input": "What happens inside a multilayer perceptron?",
  "translatedText": "Що відбувається всередині багатошарового персептрона?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.88,
  "end": 374.98
 },
 {
  "input": "You might think of this sequence of vectors flowing into the block, and remember, each vector was originally associated with one of the tokens from the input text.",
  "translatedText": "Ви можете уявити собі цю послідовність векторів, що вливаються у блок, і пам'ятайте, що кожен вектор спочатку був пов'язаний з однією з лексем вхідного тексту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.1,
  "end": 385.58
 },
 {
  "input": "What's gonna happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end, we'll get another vector with the same dimension.",
  "translatedText": "Кожен окремий вектор з цієї послідовності проходить через коротку серію операцій, ми розпаковуємо їх за мить, і в кінці отримуємо інший вектор з тією ж розмірністю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.08,
  "end": 396.36
 },
 {
  "input": "That other vector is gonna get added to the original one that flowed in, and that sum is the result flowing out.",
  "translatedText": "Цей інший вектор буде додано до початкового вектора, який увійшов, і ця сума буде результатом, який вийде назовні.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.88,
  "end": 403.2
 },
 {
  "input": "This sequence of operations is something you apply to every vector in the sequence, associated with every token in the input, and it all happens in parallel.",
  "translatedText": "Ця послідовність операцій - це те, що ви застосовуєте до кожного вектора в послідовності, пов'язаного з кожною лексемою на вході, і все це відбувається паралельно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.72,
  "end": 411.62
 },
 {
  "input": "In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing.",
  "translatedText": "Зокрема, на цьому кроці вектори не розмовляють один з одним, кожен з них робить щось своє.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.1,
  "end": 416.2
 },
 {
  "input": "And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them.",
  "translatedText": "І для нас з вами це насправді робить все набагато простішим, тому що це означає, що якщо ми розуміємо, що відбувається з одним з векторів через цей блок, ми фактично розуміємо, що відбувається з усіма ними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.72,
  "end": 426.06
 },
 {
  "input": "When I say this block is gonna encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what will add on to the vector in that position.",
  "translatedText": "Коли я кажу, що цей блок буде кодувати той факт, що Майкл Джордан грає в баскетбол, я маю на увазі, що якщо подається вектор, який кодує ім'я Майкл і прізвище Джордан, то ця послідовність обчислень створить щось, що включає в себе напрямок баскетболу, і саме це додасться до вектора в цій позиції.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.1,
  "end": 444.02
 },
 {
  "input": "The first step of this process looks like multiplying that vector by a very big matrix.",
  "translatedText": "Перший крок цього процесу виглядає як множення вектора на дуже велику матрицю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.6,
  "end": 449.7
 },
 {
  "input": "No surprises there, this is deep learning.",
  "translatedText": "Ніяких сюрпризів, це глибоке навчання.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 450.04,
  "end": 451.98
 },
 {
  "input": "And this matrix, like all of the other ones we've seen, is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is.",
  "translatedText": "І ця матриця, як і всі інші, які ми бачили, заповнена параметрами моделі, які вивчаються на основі даних, що можна уявити собі як купу ручок і циферблатів, які підлаштовуються і налаштовуються, щоб визначити, якою буде поведінка моделі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.68,
  "end": 463.54
 },
 {
  "input": "Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector, and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding.",
  "translatedText": "Тепер, один з гарних способів подумати про множення матриць - це уявити кожен рядок матриці як власний вектор, і взяти купу точкових добутків між цими рядками і оброблюваним вектором, який я позначу як E для вбудовування.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 464.5,
  "end": 476.88
 },
 {
  "input": "For example, suppose that very first row happened to equal this first name Michael direction that we're presuming exists.",
  "translatedText": "Наприклад, припустимо, що перший рядок збігся з напрямком імені Майкла, який, як ми припускаємо, існує.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 477.28,
  "end": 484.04
 },
 {
  "input": "That would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael, and zero or negative otherwise.",
  "translatedText": "Це означає, що перший компонент цього результату, цього точкового добутку, буде одиницею, якщо вектор кодує ім'я Майкл, і нулем або від'ємним у протилежному випадку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.32,
  "end": 494.8
 },
 {
  "input": "Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction.",
  "translatedText": "А ще веселіше - подумайте, що б це означало, якби в першому рядку було ім'я Майкл плюс прізвище Джордан у напрямку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.88,
  "end": 503.08
 },
 {
  "input": "And for simplicity, let me go ahead and write that down as M plus J.",
  "translatedText": "І для простоти, дозвольте мені записати це як M плюс J.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.7,
  "end": 507.42
 },
 {
  "input": "Then, taking a dot product with this embedding E, things distribute really nicely, so it looks like M dot E plus J dot E.",
  "translatedText": "Потім, якщо взяти крапковий продукт з цим вкладанням E, все розподіляється дуже добре, так що це виглядає як M крапка E плюс J крапка E.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.08,
  "end": 514.98
 },
 {
  "input": "And notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan, and otherwise it would be one or something smaller than one.",
  "translatedText": "Зверніть увагу, що це означає, що кінцеве значення буде дорівнювати двом, якщо вектор кодує повне ім'я Майкла Джордана, а в іншому випадку - одиниці або чомусь меншому за одиницю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.98,
  "end": 524.7
 },
 {
  "input": "And that's just one row in this matrix.",
  "translatedText": "І це лише один рядок у цій матриці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.34,
  "end": 527.26
 },
 {
  "input": "You might think of all of the other rows as in parallel asking some other kinds of questions, probing at some other sorts of features of the vector being processed.",
  "translatedText": "Ви можете уявити, що всі інші рядки паралельно ставлять інші запитання, досліджуючи інші особливості вектора, який обробляється.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 527.6,
  "end": 536.04
 },
 {
  "input": "Very often this step also involves adding another vector to the output, which is full of model parameters learned from data.",
  "translatedText": "Дуже часто цей крок також передбачає додавання ще одного вектора на виході, який містить параметри моделі, отримані на основі даних.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.7,
  "end": 542.24
 },
 {
  "input": "This other vector is known as the bias.",
  "translatedText": "Цей інший вектор відомий як упередженість.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 542.24,
  "end": 544.56
 },
 {
  "input": "For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one.",
  "translatedText": "Для нашого прикладу я хочу, щоб ви уявили, що значення цього зміщення в першому компоненті є від'ємним, тобто наш кінцевий результат виглядає як відповідний точковий добуток, але з від'ємним значенням.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.18,
  "end": 555.56
 },
 {
  "input": "You might very reasonably ask why I would want you to assume that the model has learned this, and in a moment you'll see why it's very clean and nice if we have a value here which is positive if and only if a vector encodes the full name Michael Jordan, and otherwise it's zero or negative.",
  "translatedText": "Ви можете резонно запитати, чому я хочу, щоб ви вважали, що модель навчилася цьому, і за мить ви побачите, чому це дуже чисто і красиво, якщо ми маємо значення, яке є додатним тоді і тільки тоді, коли вектор кодує повне ім'я Майкла Джордана, а в іншому випадку воно дорівнює нулю або від'ємному значенню.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 556.12,
  "end": 572.16
 },
 {
  "input": "The total number of rows in this matrix, which is something like the number of questions being asked, in the case of GPT-3, whose numbers we've been following, is just under 50,000.",
  "translatedText": "Загальна кількість рядків у цій матриці, яка є чимось на кшталт кількості поставлених запитань, у випадку GPT-3, цифри якого ми відстежували, становить трохи менше 50 000.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.04,
  "end": 582.78
 },
 {
  "input": "In fact, it's exactly four times the number of dimensions in this embedding space.",
  "translatedText": "Насправді це рівно в чотири рази більше, ніж кількість вимірів у цьому просторі вбудовування.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.1,
  "end": 586.64
 },
 {
  "input": "That's a design choice.",
  "translatedText": "Це дизайнерський вибір.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.92,
  "end": 587.9
 },
 {
  "input": "You could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware.",
  "translatedText": "Ви можете зробити його більшим, можете зробити меншим, але чисте кратне число, як правило, сприятливе для апаратного забезпечення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.94,
  "end": 592.24
 },
 {
  "input": "Since this matrix full of weights maps us into a higher dimensional space, I'm gonna give it the shorthand W up.",
  "translatedText": "Оскільки ця матриця, повна ваг, відображає нас у простір вищої розмірності, я позначу її скорочено W up.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.74,
  "end": 599.02
 },
 {
  "input": "I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram.",
  "translatedText": "Я продовжую позначати вектор, який ми обробляємо, як E, а цей вектор зсуву позначимо як B вгору і повернемо все це на діаграму.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.02,
  "end": 607.16
 },
 {
  "input": "At this point, a problem is that this operation is purely linear, but language is a very non-linear process.",
  "translatedText": "На цьому етапі проблема полягає в тому, що ця операція є суто лінійною, але мова є дуже нелінійним процесом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.18,
  "end": 615.36
 },
 {
  "input": "If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually.",
  "translatedText": "Якщо показник, який ми вимірюємо, є високим для Майкла плюс Джордан, він також обов'язково буде дещо підвищений для Майкла плюс Фелпс, а також Алексіс плюс Джордан, незважаючи на те, що вони концептуально не пов'язані між собою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 615.88,
  "end": 628.1
 },
 {
  "input": "What you really want is a simple yes or no for the full name.",
  "translatedText": "Насправді вам потрібна проста відповідь \"так\" або \"ні\" на повне ім'я.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.54,
  "end": 632.0
 },
 {
  "input": "So the next step is to pass this large intermediate vector through a very simple non-linear function.",
  "translatedText": "Отже, наступний крок - пропустити цей великий проміжний вектор через дуже просту нелінійну функцію.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 632.9,
  "end": 637.84
 },
 {
  "input": "A common choice is one that takes all of the negative values and maps them to zero and leaves all of the positive values unchanged.",
  "translatedText": "Найпоширеніший вибір - це вибір, який бере всі від'ємні значення і прирівнює їх до нуля, а всі додатні значення залишає без змін.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 638.36,
  "end": 645.3
 },
 {
  "input": "And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or ReLU for short.",
  "translatedText": "І продовжуючи традицію глибокого навчання щодо надто вигадливих назв, цю дуже просту функцію часто називають випрямленою лінійною одиницею, або скорочено ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 646.44,
  "end": 656.02
 },
 {
  "input": "Here's what the graph looks like.",
  "translatedText": "Ось як виглядає графік.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.02,
  "end": 657.88
 },
 {
  "input": "So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan and zero or negative otherwise, after you pass it through the ReLU, you end up with a very clean value where all of the zero and negative values just get clipped to zero.",
  "translatedText": "Отже, якщо взяти наш уявний приклад, де перший запис проміжного вектора дорівнює одиниці, якщо і тільки якщо повне ім'я Майкл Джордан, і нулю або від'ємному значенню в іншому випадку, то після того, як ви пропустите його через ReLU, ви отримаєте дуже чисте значення, де всі нульові та від'ємні значення просто обрізаються до нуля.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.3,
  "end": 675.74
 },
 {
  "input": "So this output would be one for the full name Michael Jordan and zero otherwise.",
  "translatedText": "Таким чином, цей вихід буде одиницею для повного імені Майкла Джордана і нулем у протилежному випадку.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 676.1,
  "end": 679.78
 },
 {
  "input": "In other words, it very directly mimics the behavior of an AND gate.",
  "translatedText": "Іншими словами, він безпосередньо імітує поведінку воріт AND.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 680.56,
  "end": 684.12
 },
 {
  "input": "Often models will use a slightly modified function that's called the JLU, which has the same basic shape, it's just a bit smoother.",
  "translatedText": "Часто моделі використовують дещо модифіковану функцію, яка називається JLU, що має ту ж саму базову форму, тільки трохи більш плавну.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.66,
  "end": 692.02
 },
 {
  "input": "But for our purposes, it's a little bit cleaner if we only think about the ReLU.",
  "translatedText": "Але для наших цілей буде трохи чистіше, якщо ми будемо думати лише про Революцію Гідності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.5,
  "end": 695.72
 },
 {
  "input": "Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.",
  "translatedText": "Крім того, коли ви чуєте, як люди говорять про нейрони трансформатора, вони говорять про ці цінності саме тут.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.74,
  "end": 702.52
 },
 {
  "input": "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple term-wise nonlinear function like a ReLU.",
  "translatedText": "Коли ви бачите звичайне зображення нейронної мережі з шаром точок і купою ліній, що з'єднуються з попереднім шаром, яке ми розглядали раніше в цій серії, це, як правило, має на меті передати комбінацію лінійного кроку, матричного множення, за яким слідує якась проста нелінійна функція, така як ReLU, з простими членами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 702.9,
  "end": 721.26
 },
 {
  "input": "You would say that this neuron is active whenever this value is positive and that it's inactive if that value is zero.",
  "translatedText": "Можна сказати, що цей нейрон активний, коли це значення позитивне, і що він неактивний, якщо це значення дорівнює нулю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.5,
  "end": 728.92
 },
 {
  "input": "The next step looks very similar to the first one.",
  "translatedText": "Наступний крок дуже схожий на перший.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.12,
  "end": 732.38
 },
 {
  "input": "You multiply by a very large matrix and you add on a certain bias term.",
  "translatedText": "Ви множите на дуже велику матрицю і додаєте певний член зміщення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.56,
  "end": 736.58
 },
 {
  "input": "In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm gonna go ahead and call this the down projection matrix.",
  "translatedText": "У цьому випадку кількість вимірів на виході зменшується до розміру цього простору вбудовування, тому я називатиму її матрицею проекції вниз.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 736.98,
  "end": 745.52
 },
 {
  "input": "And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column.",
  "translatedText": "І цього разу, замість того, щоб думати про речі ряд за рядом, насправді приємніше думати про них стовпчик за стовпчиком.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 746.22,
  "end": 751.36
 },
 {
  "input": "You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns.",
  "translatedText": "Бачите, ще один спосіб тримати в голові множення матриць - це уявити, що ви берете кожен стовпець матриці, множите його на відповідний член у векторі, який він обробляє, і складаєте всі ці перемасштабовані стовпці разом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 751.86,
  "end": 765.64
 },
 {
  "input": "The reason it's nicer to think about this way is because here the columns have the same dimension as the embedding space, so we can think of them as directions in that space.",
  "translatedText": "Причина, по якій приємніше думати саме так, полягає в тому, що тут колонки мають той самий вимір, що і простір вбудовування, тому ми можемо думати про них як про напрямки в цьому просторі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.84,
  "end": 775.78
 },
 {
  "input": "For instance, we will imagine that the model has learned to make that first column into this basketball direction that we suppose exists.",
  "translatedText": "Наприклад, уявімо, що модель навчилася робити перший стовпчик у цьому баскетбольному напрямку, який, як ми вважаємо, існує.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 776.14,
  "end": 783.08
 },
 {
  "input": "What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result.",
  "translatedText": "Це означає, що коли відповідний нейрон у першій позиції буде активним, ми додамо цей стовпчик до кінцевого результату.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 784.18,
  "end": 790.78
 },
 {
  "input": "But if that neuron was inactive, if that number was zero, then this would have no effect.",
  "translatedText": "Але якби цей нейрон був неактивним, якби це число дорівнювало нулю, то це не мало б жодного ефекту.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 791.14,
  "end": 795.78
 },
 {
  "input": "And it doesn't just have to be basketball.",
  "translatedText": "І це не обов'язково має бути лише баскетбол.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 796.5,
  "end": 798.06
 },
 {
  "input": "The model could also bake into this column and many other features that it wants to associate with something that has the full name Michael Jordan.",
  "translatedText": "Модель також може вписати в цю колонку і багато інших особливостей, які вона хоче асоціювати з чимось, що має повне ім'я Майкл Джордан.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 798.22,
  "end": 805.2
 },
 {
  "input": "And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active.",
  "translatedText": "І в той же час, всі інші стовпчики в цій матриці говорять вам, що буде додано до кінцевого результату, якщо відповідний нейрон буде активним.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.98,
  "end": 816.66
 },
 {
  "input": "And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values.",
  "translatedText": "І якщо у вас є упередження в цьому випадку, то це те, що ви просто додаєте щоразу, незалежно від значень нейронів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 817.36,
  "end": 823.5
 },
 {
  "input": "You might wonder what's that doing.",
  "translatedText": "Ви можете здивуватися, що це робить.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 824.06,
  "end": 825.28
 },
 {
  "input": "As with all parameter-filled objects here, it's kind of hard to say exactly.",
  "translatedText": "Як і у випадку з усіма об'єктами з параметрами, тут важко сказати точно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 825.54,
  "end": 829.32
 },
 {
  "input": "Maybe there's some bookkeeping that the network needs to do, but you can feel free to ignore it for now.",
  "translatedText": "Можливо, мережа має вести якусь бухгалтерію, але поки що ви можете її ігнорувати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 829.32,
  "end": 834.38
 },
 {
  "input": "Making our notation a little more compact again, I'll call this big matrix W down and similarly call that bias vector B down and put that back into our diagram.",
  "translatedText": "Щоб знову зробити нашу нотацію трохи компактнішою, я назву цю велику матрицю W вниз і так само назву вектор зсуву B вниз і поверну їх назад на нашу діаграму.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.86,
  "end": 844.26
 },
 {
  "input": "Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position and that gets you this final result.",
  "translatedText": "Як я вже показував раніше, з цим кінцевим результатом ви додаєте його до вектора, який потрапив у блок у цій позиції, і отримуєте цей кінцевий результат.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 844.74,
  "end": 853.24
 },
 {
  "input": "So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction, so what pops out will encode all of those together.",
  "translatedText": "Так, наприклад, якщо вектор, що надходить, містить ім'я Майкл та прізвище Джордан, то оскільки ця послідовність операцій запустить вентиль І, він додасть напрямок баскетбольного м'яча, і те, що вийде на екран, буде кодувати все це разом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 853.82,
  "end": 869.24
 },
 {
  "input": "And remember, this is a process happening to every one of those vectors in parallel.",
  "translatedText": "І пам'ятайте, що цей процес відбувається з кожним з цих векторів паралельно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 869.82,
  "end": 874.2
 },
 {
  "input": "In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input.",
  "translatedText": "Зокрема, якщо взяти цифри GPT-3, то це означає, що в цьому блоці не просто 50 000 нейронів, а в 50 000 разів більше токенів на вході.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 874.8,
  "end": 884.86
 },
 {
  "input": "So that is the entire operation, two matrix products, each with a bias added and a simple clipping function in between.",
  "translatedText": "Ось і вся операція: два матричні добутки, кожен з яких додається зі зміщенням, і проста функція відсікання між ними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.18,
  "end": 895.18
 },
 {
  "input": "Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there.",
  "translatedText": "Кожен з вас, хто дивився попередні відео серії, впізнає цю структуру як найпростіший тип нейронної мережі, який ми там вивчали.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 896.08,
  "end": 902.62
 },
 {
  "input": "In that example, it was trained to recognize handwritten digits.",
  "translatedText": "У цьому прикладі його навчили розпізнавати цифри, написані від руки.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 903.08,
  "end": 906.1
 },
 {
  "input": "Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high-dimensional embedding space.",
  "translatedText": "Тут, у контексті трансформатора для великої мовної моделі, це лише частина великої архітектури, і будь-яка спроба інтерпретувати, що саме він робить, тісно переплітається з ідеєю кодування інформації у вектори високорозмірного простору вбудовування.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 906.58,
  "end": 923.18
 },
 {
  "input": "That is the core lesson, but I do wanna step back and reflect on two different things, the first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into transformers.",
  "translatedText": "Це основний урок, але я хочу зробити крок назад і поміркувати над двома різними речами, перша з яких є своєрідною бухгалтерією, а друга включає в себе дуже цікавий факт про вищі виміри, про який я насправді не знав, поки не заглибився в трансформатори.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.26,
  "end": 938.08
 },
 {
  "input": "In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live, so let's quickly finish up the game here.",
  "translatedText": "У попередніх двох розділах ми з вами почали підраховувати загальну кількість параметрів у GPT-3 і бачити, де саме вони живуть, тож давайте швиденько закінчимо гру на цьому.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.08,
  "end": 950.76
 },
 {
  "input": "I already mentioned how this up projection matrix has just under 50,000 rows and that each row matches the size of the embedding space, which for GPT-3 is 12,288.",
  "translatedText": "Я вже згадував, що ця матриця висхідної проекції має трохи менше 50 000 рядків, і що кожен рядок відповідає розміру простору вбудовування, який для GPT-3 становить 12 288.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 951.4,
  "end": 962.18
 },
 {
  "input": "Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transposed shape.",
  "translatedText": "Помноживши їх разом, ми отримаємо 604 мільйони параметрів тільки для цієї матриці, а проекція вниз має таку ж кількість параметрів, тільки з переставленою формою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 963.24,
  "end": 973.92
 },
 {
  "input": "So together, they give about 1.2 billion parameters.",
  "translatedText": "Тож разом вони дають близько 1,2 мільярда параметрів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 974.5,
  "end": 977.4
 },
 {
  "input": "The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even gonna show it.",
  "translatedText": "Вектор зміщення також враховує ще пару параметрів, але це тривіальна частка від загальної суми, тому я навіть не буду її показувати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 978.28,
  "end": 984.1
 },
 {
  "input": "In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion.",
  "translatedText": "У GPT-3 ця послідовність векторів вбудовування проходить не через один, а через 96 різних MLP, тому загальна кількість параметрів, присвячених усім цим блокам, складає близько 116 мільярдів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 984.66,
  "end": 998.06
 },
 {
  "input": "This is around 2 thirds of the total parameters in the network, and when you add it to everything that we had before, for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised.",
  "translatedText": "Це приблизно 2 третини всіх параметрів мережі, і коли ви додасте їх до всього, що ми мали раніше, до блоків уваги, вбудовування та вилучення, ви дійсно отримаєте 175 мільярдів, як і було заявлено в рекламі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.82,
  "end": 1011.62
 },
 {
  "input": "It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total.",
  "translatedText": "Можливо, варто згадати, що існує ще один набір параметрів, пов'язаних з тими кроками нормалізації, які ми пропустили в цьому поясненні, але, як і вектор зміщення, вони становлять дуже тривіальну частку від загальної кількості.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1013.06,
  "end": 1023.84
 },
 {
  "input": "As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real large language models.",
  "translatedText": "Щодо другої точки роздумів, вам може бути цікаво, чи цей центральний приклад іграшки, на який ми витратили стільки часу, відображає те, як факти насправді зберігаються в реальних великих мовних моделях.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1025.9,
  "end": 1035.68
 },
 {
  "input": "It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction.",
  "translatedText": "Дійсно, рядки цієї першої матриці можна розглядати як напрямки в цьому просторі вбудовування, а це означає, що активація кожного нейрона показує, наскільки даний вектор співпадає з певним напрямком.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1036.32,
  "end": 1047.54
 },
 {
  "input": "It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active.",
  "translatedText": "Також вірно, що стовпці другої матриці показують, що буде додано до результату, якщо цей нейрон активний.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.76,
  "end": 1054.34
 },
 {
  "input": "Both of those are just mathematical facts.",
  "translatedText": "І те, і інше - просто математичні факти.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.64,
  "end": 1056.8
 },
 {
  "input": "However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan, and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.",
  "translatedText": "Однак дані свідчать про те, що окремі нейрони дуже рідко представляють єдину чисту функцію, як Майкл Джордан, і на це може бути дуже вагома причина, пов'язана з ідеєю, яка в наш час поширюється серед дослідників інтерпретативності, відомою як суперпозиція.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1057.74,
  "end": 1074.12
 },
 {
  "input": "This is a hypothesis that might help to explain both why the models are especially hard to interpret and also why they scale surprisingly well.",
  "translatedText": "Це гіпотеза, яка може допомогти пояснити як те, чому моделі особливо важко інтерпретувати, так і те, чому вони напрочуд добре масштабуються.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1074.64,
  "end": 1082.42
 },
 {
  "input": "The basic idea is that if you have an n-dimensional space and you wanna represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction, it doesn't influence any of the other directions, then the maximum number of vectors you can fit is only n, the number of dimensions.",
  "translatedText": "Основна ідея полягає в тому, що якщо у вас є n-вимірний простір і ви хочете представити купу різних характеристик, використовуючи напрямки, які перпендикулярні один до одного в цьому просторі, тобто, якщо ви додаєте компонент в одному напрямку, він не впливає на інші напрямки, то максимальна кількість векторів, яку ви можете вмістити, дорівнює лише n, кількості вимірів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1083.5,
  "end": 1103.96
 },
 {
  "input": "To a mathematician, actually, this is the definition of dimension.",
  "translatedText": "Для математика, власне, це і є визначенням розмірності.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1104.6,
  "end": 1107.62
 },
 {
  "input": "But where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.",
  "translatedText": "Але цікаво стає тоді, коли ви трохи послаблюєте ці обмеження і терпимо ставитеся до шуму.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1113.58
 },
 {
  "input": "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart.",
  "translatedText": "Скажімо, ви дозволяєте представити ці елементи векторами, які не зовсім перпендикулярні, але майже перпендикулярні, можливо, під кутом між 89 і 91 градусом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1114.18,
  "end": 1123.82
 },
 {
  "input": "If we were in two or three dimensions, this makes no difference.",
  "translatedText": "Якби ми були у двох чи трьох вимірах, це не мало б ніякого значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1124.82,
  "end": 1128.02
 },
 {
  "input": "That gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions, the answer changes dramatically.",
  "translatedText": "Це майже не дає вам додаткового простору для маневру, щоб вписати більше векторів, що робить ще більш контрінтуїтивним той факт, що при більших розмірностях відповідь кардинально змінюється.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1128.26,
  "end": 1136.78
 },
 {
  "input": "I can give you a really quick and dirty illustration of this using some scrappy Python that's going to create a list of 100-dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions.",
  "translatedText": "Я можу дати вам дуже швидку і брудну ілюстрацію цього за допомогою уривчастого Python, який створить список 100-вимірних векторів, кожен з яких ініціалізується випадковим чином, і цей список буде містити 10 000 різних векторів, тобто в 100 разів більше векторів, ніж вимірів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1137.66,
  "end": 1154.4
 },
 {
  "input": "This plot right here shows the distribution of angles between pairs of these vectors.",
  "translatedText": "На цьому графіку показано розподіл кутів між парами цих векторів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.32,
  "end": 1159.9
 },
 {
  "input": "So because they started at random, those angles could be anything from 0 to 180 degrees, but you'll notice that already, even just for random vectors, there's this heavy bias for things to be closer to 90 degrees.",
  "translatedText": "Оскільки вони починали навмання, ці кути могли бути будь-якими - від 0 до 180 градусів, але ви помітите, що навіть для випадкових векторів існує сильний зсув у бік кутів ближче до 90 градусів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1160.68,
  "end": 1171.96
 },
 {
  "input": "Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another.",
  "translatedText": "Потім я збираюся запустити певний процес оптимізації, який ітеративно підштовхує всі ці вектори так, щоб вони намагалися стати більш перпендикулярними один до одного.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1172.5,
  "end": 1181.52
 },
 {
  "input": "After repeating this many different times, here's what the distribution of angles looks like.",
  "translatedText": "Повторивши це багато разів, ось як виглядає розподіл кутів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1182.06,
  "end": 1186.66
 },
 {
  "input": "We have to actually zoom in on it here because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees.",
  "translatedText": "Тут ми повинні збільшити масштаб, оскільки всі можливі кути між парами векторів знаходяться в цьому вузькому діапазоні між 89 і 91 градусом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1187.12,
  "end": 1196.9
 },
 {
  "input": "In general, a consequence of something known as the Johnson-Lindenstrauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this grows exponentially with the number of dimensions.",
  "translatedText": "Загалом, наслідком леми Джонсона-Лінденштрауса є те, що кількість майже перпендикулярних векторів, які можна втиснути в простір, зростає експоненціально з кількістю вимірів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.02,
  "end": 1210.84
 },
 {
  "input": "This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions.",
  "translatedText": "Це дуже важливо для великих мовних моделей, які можуть виграти, якщо пов'язувати незалежні ідеї з майже перпендикулярними напрямками.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1211.96,
  "end": 1219.88
 },
 {
  "input": "It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted.",
  "translatedText": "Це означає, що він може зберігати набагато більше ідей, ніж є вимірів у відведеному йому просторі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1220.0,
  "end": 1226.44
 },
 {
  "input": "This might partially explain why model performance seems to scale so well with size.",
  "translatedText": "Це може частково пояснити, чому продуктивність моделі так добре масштабується зі збільшенням розміру.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1227.32,
  "end": 1231.74
 },
 {
  "input": "A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas.",
  "translatedText": "Простір, який має в 10 разів більше вимірів, може зберігати набагато, набагато більше, ніж в 10 разів більше незалежних ідей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1232.54,
  "end": 1239.4
 },
 {
  "input": "And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multilayer perceptron that we just studied.",
  "translatedText": "І це стосується не лише простору вбудовування, де живуть вектори, що протікають через модель, але й вектора, наповненого нейронами в середині багатошарового персептрона, який ми щойно вивчали.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1240.42,
  "end": 1250.44
 },
 {
  "input": "That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed.",
  "translatedText": "Тобто, при розмірах GPT-3, він міг би не просто зондувати 50 000 елементів, але якби він використовував цю величезну додаткову потужність, використовуючи майже перпендикулярні напрямки простору, він міг би зондувати набагато, набагато більше елементів оброблюваного вектору.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.96,
  "end": 1267.24
 },
 {
  "input": "But if it was doing that, what it means is that individual features aren't gonna be visible as a single neuron lighting up.",
  "translatedText": "Але якщо він це робить, то це означає, що індивідуальні особливості не будуть видимими у вигляді окремих нейронів, що світяться.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1267.78,
  "end": 1274.34
 },
 {
  "input": "It would have to look like some specific combination of neurons instead, a superposition.",
  "translatedText": "Натомість він мав би виглядати як певна комбінація нейронів, суперпозиція.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1274.66,
  "end": 1279.38
 },
 {
  "input": "For any of you curious to learn more, a key relevant search term here is sparse autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're very superimposed on all these neurons.",
  "translatedText": "Для тих, кому цікаво дізнатися більше, ключовим пошуковим терміном тут є розріджений автокодер - інструмент, який деякі фахівці з інтерпретації використовують, щоб спробувати виокремити справжні ознаки, навіть якщо вони дуже накладені на всі ці нейрони.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1280.4,
  "end": 1292.88
 },
 {
  "input": "I'll link to a couple really great anthropic posts all about this.",
  "translatedText": "Я дам посилання на кілька чудових антропологічних постів про це.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1293.54,
  "end": 1296.8
 },
 {
  "input": "At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points.",
  "translatedText": "На цьому етапі ми не торкнулися кожної деталі трансформатора, але ми з вами зачепили найважливіші моменти.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1297.88,
  "end": 1303.3
 },
 {
  "input": "The main thing that I wanna cover in a next chapter is the training process.",
  "translatedText": "Головне, про що я хочу розповісти в наступному розділі, - це процес навчання.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1303.52,
  "end": 1307.64
 },
 {
  "input": "On the one hand, the short answer for how training works is that it's all backpropagation, and we covered backpropagation in a separate context with earlier chapters in the series.",
  "translatedText": "З одного боку, коротка відповідь на питання, як працює тренінг, полягає в тому, що все це - зворотне поширення, і ми розглядали зворотне поширення в окремому контексті в попередніх розділах цієї серії.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1308.46,
  "end": 1316.9
 },
 {
  "input": "But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning using reinforcement learning with human feedback, and the notion of scaling laws.",
  "translatedText": "Але є ще багато чого обговорити, наприклад, специфічну функцію витрат, що використовується для мовних моделей, ідею точного налаштування за допомогою навчання з підкріпленням і зворотним зв'язком з людиною, а також поняття законів масштабування.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.22,
  "end": 1327.78
 },
 {
  "input": "Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sink my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time.",
  "translatedText": "Для активних підписників: є кілька відео, не пов'язаних з машинним навчанням, які я хочу переглянути перед тим, як написати наступну главу, тому це може зайняти деякий час, але я обіцяю, що все буде вчасно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1328.96,
  "end": 1340.0
 },
 {
  "input": "Thank you.",
  "translatedText": "Дякую.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1355.64,
  "end": 1357.92
 }
]